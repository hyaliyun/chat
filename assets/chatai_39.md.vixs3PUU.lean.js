import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(i,e,l,m,n,o){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-6b2273db"]]),I=JSON.parse('[{"question":"# Question: Optimizing Prediction Latency and Throughput with Scikit-learn Context You are provided with a dataset consisting of predictors (features) and a target variable for a binary classification task. Your task is to: 1. Build a model using scikit-learn to predict the target variable. 2. Optimize the prediction latency and throughput of your scikit-learn model. Requirements 1. **Data Preprocessing**: - Convert the data into an appropriate format for your model. - Ensure that the data preprocessing pipeline includes handling missing values, scaling, and encoding as necessary. 2. **Model Training**: - Train a classification model of your choice from scikit-learn. - Experiment with different model complexities and configurations to balance predictive power and latency. 3. **Performance Optimization**: - Measure the prediction latency and throughput of your model. - Optimize the model for lower latency and higher throughput using techniques like bulk predictions, sparse matrix representation, and model compression. - Measure the impact of these optimizations. 4. **Evaluation**: - Report the final prediction accuracy, latency, and throughput of your optimized model. - Visualize the trade-offs between model complexity and prediction latency. Constraints - The dataset will be provided in a CSV file format: - `data.csv`: Contains the features and target variable. ```plaintext feature1, feature2, ..., featureN, target value11, value12, ..., value1N, target1 value21, value22, ..., value2N, target2 ... ``` Expected Input and Output - **Input**: A CSV file `data.csv` containing the dataset. - **Output**: - Prediction accuracy. - Average prediction latency. - Prediction throughput. - Visualizations showing the trade-offs between model complexity and prediction latency. Implementation Implement the required functions in the following steps. You can use Jupyter Notebook or any Python script to write your solution. ```python import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.preprocessing import StandardScaler import time def load_data(file_path): Load the dataset from the given CSV file path. Args: - file_path (str): The file path to the CSV file. Returns: - X (np.ndarray): Features. - y (np.ndarray): Target variable. # Implement loading data from CSV pass def preprocess_data(X): Preprocess the dataset. Args: - X (np.ndarray): The features. Returns: - X_transformed (np.ndarray): The preprocessed features. # Implement data preprocessing steps here pass def train_model(X_train, y_train): Train the classifier model. Args: - X_train (np.ndarray): Training features. - y_train (np.ndarray): Training target variable. Returns: - model (sklearn estimator): The trained model. model = SGDClassifier(penalty=\'elasticnet\', l1_ratio=0.25) # Train the model here pass def evaluate_performance(model, X_test, y_test): Evaluate the model\'s performance on the test set. Args: - model (sklearn estimator): The trained model. - X_test (np.ndarray): Test features. - y_test (np.ndarray): Test target variable. Returns: - accuracy (float): The prediction accuracy. - latency (float): Average prediction latency. - throughput (float): Number of predictions per second. # Implement performance evaluation here pass def main(): # Load the dataset X, y = load_data(\'data.csv\') # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess the data X_train = preprocess_data(X_train) X_test = preprocess_data(X_test) # Train the model model = train_model(X_train, y_train) # Evaluate performance accuracy, latency, throughput = evaluate_performance(model, X_test, y_test) print(f\'Accuracy: {accuracy}\') print(f\'Average Prediction Latency: {latency} seconds\') print(f\'Prediction Throughput: {throughput} predictions/second\') if __name__ == \\"__main__\\": main() ``` **Note**: Complete the `load_data`, `preprocess_data`, `train_model`, and `evaluate_performance` functions with the necessary code to achieve the desired outputs. Bonus (Optional) - Experiment with different machine learning models and compare their performance. - Implement additional optimizations like model sparsification and reshaping, and measure their impact on performance.","solution":"import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline import time def load_data(file_path): Load the dataset from the given CSV file path. Args: - file_path (str): The file path to the CSV file. Returns: - X (np.ndarray): Features. - y (np.ndarray): Target variable. data = pd.read_csv(file_path) X = data.drop(columns=[\'target\']).values y = data[\'target\'].values return X, y def preprocess_data(X): Preprocess the dataset. Args: - X (np.ndarray): The features. Returns: - X_transformed (np.ndarray): The preprocessed features. pipeline = Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) X_transformed = pipeline.fit_transform(X) return X_transformed def train_model(X_train, y_train): Train the classifier model. Args: - X_train (np.ndarray): Training features. - y_train (np.ndarray): Training target variable. Returns: - model (sklearn estimator): The trained model. model = SGDClassifier(penalty=\'elasticnet\', l1_ratio=0.25, random_state=42) model.fit(X_train, y_train) return model def evaluate_performance(model, X_test, y_test): Evaluate the model\'s performance on the test set. Args: - model (sklearn estimator): The trained model. - X_test (np.ndarray): Test features. - y_test (np.ndarray): Test target variable. Returns: - accuracy (float): The prediction accuracy. - latency (float): Average prediction latency. - throughput (float): Number of predictions per second. start_time = time.time() predictions = model.predict(X_test) end_time = time.time() accuracy = accuracy_score(y_test, predictions) latency = (end_time - start_time) / len(X_test) throughput = len(X_test) / (end_time - start_time) return accuracy, latency, throughput def main(): # Load the dataset X, y = load_data(\'data.csv\') # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess the data X_train = preprocess_data(X_train) X_test = preprocess_data(X_test) # Train the model model = train_model(X_train, y_train) # Evaluate performance accuracy, latency, throughput = evaluate_performance(model, X_test, y_test) print(f\'Accuracy: {accuracy}\') print(f\'Average Prediction Latency: {latency} seconds\') print(f\'Prediction Throughput: {throughput} predictions/second\') if __name__ == \\"__main__\\": main()"},{"question":"Coding Assessment Question **Objective**: The `doctest` module in Python can be used to test code examples embedded in documentation strings. This question will test your ability to use this module to maintain up-to-date documentation, handle output variations, and manage exceptions within the examples. # Problem Statement You are given the following module `math_utils` with embedded interactive examples. Your task is to: 1. **Complete the functions** to match the expected outputs in the docstrings. 2. **Create a script** that uses `doctest` to verify and report the correctness of the examples. 3. **Handle variations in output** using appropriate option flags. 4. **Manage exceptions and ensure** they are correctly identified in the output. ```python # math_utils.py This is the \\"math_utils\\" module. The math_utils module supplies utility mathematical functions. For example, >>> gcd(48, 18) 6 >>> gcd(0, 0) 0 def gcd(a, b): Return the greatest common divisor of a and b. >>> gcd(54, 24) 6 >>> gcd(48, 180) 12 >>> gcd(0, 9) 9 >>> gcd(0, 0) # This should raise an exception as gcd(0, 0) is undefined Traceback (most recent call last): ... ValueError: gcd(0, 0) is undefined if a == 0 and b == 0: raise ValueError(\\"gcd(0, 0) is undefined\\") while b != 0: a, b = b, a % b return a def normalize_whitespace(s): Normalize all whitespace in string `s` to single spaces. >>> normalize_whitespace(\'Hellotworld\') \'Hello world\' >>> normalize_whitespace(\'String withnmultiplenlines\') \'String with multiple lines\' >>> normalize_whitespace(\' Extra spaces everywhere \') \'Extra spaces everywhere\' import re return re.sub(r\'s+\', \' \', s).strip() if __name__ == \\"__main__\\": import doctest doctest.testmod(optionflags=doctest.NORMALIZE_WHITESPACE | doctest.ELLIPSIS) ``` # Requirements 1. **Function Implementations**: - Complete the `gcd` and `normalize_whitespace` functions to match the expected behavior. 2. **Testing Script**: - Add the given code into a module named `math_utils.py`. - Ensure the embedded examples in the docstrings pass using the `doctest` module. - Use appropriate option flags to handle whitespace normalization and ellipsis in output. 3. **Exception Handling**: - Ensure that exceptions are correctly captured and validated in the examples. 4. **Verification**: - Provide output of running the `doctest` to verify all examples are correct. # Constraints - Both functions should handle typical edge cases, e.g., `gcd(0, 9)`. - Use built-in Python libraries only. - Your solution should not use any additional external modules except `doctest`. # Performance Requirements - Your solution should execute the tests within reasonable time limits and handle typical input sizes efficiently. # Submission Requirements 1. **Source Code**: Provide the completed `math_utils.py` module. 2. **Test Output**: Include the output of running the `doctest` command with verbose mode. # Example Output ```sh python math_utils.py -v Trying: gcd(48, 18) Expecting: 6 ok Trying: gcd(54, 24) Expecting: 6 ok Trying: gcd(48, 180) Expecting: 12 ok Trying: gcd(0, 9) Expecting: 9 ok Trying: gcd(0, 0) Expecting: Traceback (most recent call last): ... ValueError: gcd(0, 0) is undefined ok Trying: normalize_whitespace(\'Hellotworld\') Expecting: \'Hello world\' ok Trying: normalize_whitespace(\'String withnmultiplenlines\') Expecting: \'String with multiple lines\' ok Trying: normalize_whitespace(\' Extra spaces everywhere \') Expecting: \'Extra spaces everywhere\' ok 1 items had no tests: __main__ 8 tests in 2 items. 8 passed and 0 failed. Test passed. ```","solution":"# math_utils.py This is the \\"math_utils\\" module. The math_utils module supplies utility mathematical functions. For example, >>> gcd(48, 18) 6 >>> gcd(0, 0) 0 def gcd(a, b): Return the greatest common divisor of a and b. >>> gcd(54, 24) 6 >>> gcd(48, 180) 12 >>> gcd(0, 9) 9 >>> gcd(0, 0) # This should raise an exception as gcd(0, 0) is undefined Traceback (most recent call last): ... ValueError: gcd(0, 0) is undefined if a == 0 and b == 0: raise ValueError(\\"gcd(0, 0) is undefined\\") while b != 0: a, b = b, a % b return a def normalize_whitespace(s): Normalize all whitespace in string `s` to single spaces. >>> normalize_whitespace(\'Hellotworld\') \'Hello world\' >>> normalize_whitespace(\'String withnmultiplenlines\') \'String with multiple lines\' >>> normalize_whitespace(\' Extra spaces everywhere \') \'Extra spaces everywhere\' import re return re.sub(r\'s+\', \' \', s).strip() if __name__ == \\"__main__\\": import doctest doctest.testmod(optionflags=doctest.NORMALIZE_WHITESPACE | doctest.ELLIPSIS)"},{"question":"# Question: Concurrent Prime Number Calculation You are tasked with implementing a concurrent prime number calculation using the low-level threading API provided by the `_thread` module. The goal is to determine the prime numbers within a given range `[start, end]` using multiple threads to speed up the computation. Synchronization should be managed using locks to ensure correct results and avoid race conditions. # Problem Statement 1. **Function Name:** `concurrent_prime_calculation` 2. **Inputs:** - An integer `start` representing the starting range (inclusive). - An integer `end` representing the ending range (inclusive). - An integer `num_threads` representing the number of threads to be used. 3. **Outputs:** - A sorted list of integers representing the prime numbers between `start` and `end`, calculated concurrently using multiple threads. # Constraints - `1 <= start <= end <= 10^6` - `1 <= num_threads <= 100` # Performance Requirements - Efficient use of threads and synchronization to minimize calculation time. - Proper handling of race conditions using locks. # Example ```python def concurrent_prime_calculation(start: int, end: int, num_threads: int) -> list: # Your implementation here # Example usage: prime_numbers = concurrent_prime_calculation(1, 100, 4) print(prime_numbers) # Expected Output: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97] ``` # Implementation Guidelines 1. **Function Definition:** Define the function `concurrent_prime_calculation` following the specified input and output signatures. 2. **Thread Management:** - Use `_thread.start_new_thread` to create and start threads. - Distribute the range `[start, end]` among the specified number of threads. 3. **Synchronization:** - Use `_thread.allocate_lock` to create lock objects. - Ensure thread-safe access to shared resources (e.g., the list of prime numbers). 4. **Prime Calculation:** Implement a method to check if a given number is prime. 5. **Collecting Results:** Gather results from all threads and return a sorted list of primes. # Notes - You must import relevant modules such as `_thread`. - Ensure your code handles edge cases and potential exceptions, such as invalid ranges. - Minimize thread creation overhead and ensure threads terminate correctly after completing their tasks.","solution":"import _thread from typing import List def is_prime(n: int) -> bool: if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def concurrent_prime_calculation(start: int, end: int, num_threads: int) -> List[int]: def worker(start: int, end: int, primes: List[int], lock) -> None: for num in range(start, end + 1): if is_prime(num): lock.acquire() primes.append(num) lock.release() primes = [] lock = _thread.allocate_lock() thread_ranges = [] range_size = (end - start + 1) // num_threads for i in range(num_threads): range_start = start + i * range_size range_end = start + (i + 1) * range_size - 1 if i == num_threads - 1: range_end = end thread_ranges.append((range_start, range_end)) for range_start, range_end in thread_ranges: _thread.start_new_thread(worker, (range_start, range_end, primes, lock)) # Busy-wait for threads to finish (in practice, we\'d use condition variables or other blocking mechanisms) import time time.sleep(1 + (end - start) // 1000) return sorted(primes)"},{"question":"**Title: Advanced Data Processing with Pattern Matching in Python 3.10** **Objective:** Create a function to process a list of data entries using Python 3.10\'s pattern matching feature. This question assesses the ability to utilize advanced constructs introduced in Python 3.10, specifically pattern matching. **Problem Statement:** You are given a list of data entries, where each entry is a dictionary with different structures. Your task is to process these entries and extract specific information based on their structure using Python 3.10\'s pattern matching. The data entries can have one of the following structures: 1. A rectangle with properties `type=\'rectangle\'`, `width`, and `height`. 2. A circle with properties `type=\'circle\'` and `radius`. 3. A triangle with properties `type=\'triangle\'`, `base`, and `height`. 4. Any other type will be ignored. You need to create a function `process_shapes(data: List[dict]) -> Tuple[float, float, float]` that processes the list and returns a tuple containing the sum of areas of all rectangles, circles, and triangles, respectively. Ignore any entries that do not match the specified types. # Function Signature: ```python from typing import List, Tuple def process_shapes(data: List[dict]) -> Tuple[float, float, float]: pass ``` # Constraints: - The input list `data` will contain up to 10,000 entries. - Each dictionary represents a single shape and follows the mentioned structure. - Use the following formulas for area calculations: - Rectangle: `width * height` - Circle: `Ï€ * radius^2` (use `math.pi`) - Triangle: `0.5 * base * height` # Example: ```python data = [ {\\"type\\": \\"rectangle\\", \\"width\\": 10, \\"height\\": 5}, {\\"type\\": \\"circle\\", \\"radius\\": 7}, {\\"type\\": \\"triangle\\", \\"base\\": 6, \\"height\\": 3}, {\\"type\\": \\"rectangle\\", \\"width\\": 2, \\"height\\": 8}, {\\"type\\": \\"circle\\", \\"radius\\": 3}, ] rectangles, circles, triangles = process_shapes(data) print(f\\"Total area of rectangles: {rectangles}\\") print(f\\"Total area of circles: {circles}\\") print(f\\"Total area of triangles: {triangles}\\") ``` **Expected Output:** ``` Total area of rectangles: 90.0 Total area of circles: 182.21237667601792 Total area of triangles: 9.0 ``` This question will require the students to: - Understand and implement pattern matching in Python 3.10. - Correctly utilize data extraction and processing. - Calculate areas using given formulas and aggregate results. **Hint:** Consider using Python 3.10\'s `match` statement to identify and process each shape type efficiently.","solution":"from typing import List, Tuple import math def process_shapes(data: List[dict]) -> Tuple[float, float, float]: total_rectangle_area = 0.0 total_circle_area = 0.0 total_triangle_area = 0.0 for entry in data: match entry: case {\\"type\\": \\"rectangle\\", \\"width\\": width, \\"height\\": height}: total_rectangle_area += width * height case {\\"type\\": \\"circle\\", \\"radius\\": radius}: total_circle_area += math.pi * radius * radius case {\\"type\\": \\"triangle\\", \\"base\\": base, \\"height\\": height}: total_triangle_area += 0.5 * base * height return total_rectangle_area, total_circle_area, total_triangle_area"},{"question":"Coding Assessment Question # Objective You are required to implement a module that handles reading from and writing to Sun AU audio files using Python\'s `sunau` module. Your implementation should cover the creation of a utility class that provides methods to read metadata and audio data from a Sun AU file, as well as write metadata and audio data to a new Sun AU file. # Instructions 1. **Class Definition**: Define a class `SunAUProcessor` with the following methods: - `__init__(self, file_path, mode)`: Initialize the class with the file path and mode (\'r\' for reading, \'w\' for writing). - `read_metadata(self) -> dict`: Read and return the metadata of the Sun AU file as a dictionary. - `read_audio_data(self) -> bytes`: Read and return the audio data as a bytes object. - `write_metadata(self, metadata: dict)`: Write the provided metadata to a new Sun AU file. - `write_audio_data(self, audio_data: bytes)`: Write the provided audio data to a new Sun AU file. - `close(self)`: Close the open file. 2. **Metadata Format**: The metadata dictionary should have the keys: - `channels`: Number of channels. - `sample_width`: Width of each sample in bytes. - `sample_rate`: Sample rate. - `num_frames`: Number of frames. - `compression_type`: Compression type. - `compression_name`: Human-readable compression type. 3. **Constraints**: - Only read Sun AU files with supported encoding types. - Ensure all methods handle potential exceptions using the `sunau.Error` exception class. - Only write files with \'NONE\' or \'ULAW\' compression types. 4. **Performance Requirements**: Ensure efficient file handling to avoid unnecessary memory usage, especially for large audio files. # Example Usage ```python # Reading from an existing Sun AU file processor = SunAUProcessor(\\"example.au\\", \\"r\\") metadata = processor.read_metadata() audio_data = processor.read_audio_data() processor.close() # Writing to a new Sun AU file new_processor = SunAUProcessor(\\"new_example.au\\", \\"w\\") new_processor.write_metadata(metadata) new_processor.write_audio_data(audio_data) new_processor.close() ``` # Expected Output The class methods should function as expected, accurately reading from and writing to Sun AU files, preserving all relevant metadata and audio data. # Notes - Use the provided `sunau` module\'s functions and classes to ensure compatibility. - Validate that the resultant file after writing has all fields correctly populated and that it can be read back without any loss of information.","solution":"import sunau class SunAUProcessor: def __init__(self, file_path, mode): Initialize the processor with the file path and the mode (\'r\' for reading, \'w\' for writing) if mode not in (\'r\', \'w\'): raise ValueError(\\"Mode should be \'r\' or \'w\'\\") self.file_path = file_path self.mode = mode if mode == \'r\': self.file = sunau.open(file_path, \'r\') else: self.file = None # File will be opened when writing metadata def read_metadata(self) -> dict: Read and return metadata from the Sun AU file as a dictionary try: params = self.file.getparams() metadata = { \'channels\': params[0], \'sample_width\': params[1], \'sample_rate\': params[2], \'num_frames\': params[3], \'compression_type\': params[4], \'compression_name\': params[5], } return metadata except sunau.Error as e: raise Exception(f\\"Failed to read metadata: {e}\\") def read_audio_data(self) -> bytes: Read and return audio data from the Sun AU file as bytes try: return self.file.readframes(self.file.getnframes()) except sunau.Error as e: raise Exception(f\\"Failed to read audio data: {e}\\") def write_metadata(self, metadata: dict): Write provided metadata to the new Sun AU file try: self.file = sunau.open(self.file_path, \'w\') self.file.setnchannels(metadata[\'channels\']) self.file.setsampwidth(metadata[\'sample_width\']) self.file.setframerate(metadata[\'sample_rate\']) self.file.setcomptype(metadata[\'compression_type\'], metadata[\'compression_name\']) except sunau.Error as e: raise Exception(f\\"Failed to write metadata: {e}\\") def write_audio_data(self, audio_data: bytes): Write provided audio data to the new Sun AU file try: self.file.writeframes(audio_data) except sunau.Error as e: raise Exception(f\\"Failed to write audio data: {e}\\") def close(self): Close the open file if self.file: self.file.close()"},{"question":"Objective You are required to write a Python function that analyzes and optimizes a given pickled object. Your task is to use the \\"pickletools\\" module to first disassemble the pickle, identify key features, and then optimize the pickled string. You should provide detailed output showcasing the disassembly before and after optimization. Function Signature ```python import pickletools def analyze_and_optimize_pickle(pickled_obj: bytes) -> str: Analyze and optimize the given pickled object. Args: - pickled_obj (bytes): A pickled object in binary format. Returns: - str: A string representation of the disassembly before and after optimization. # Your implementation here ``` Input - `pickled_obj`: A bytes object representing the binary pickled data. Assume this object is pickled using a protocol supported by Python 3.6 and above. Expected Output - Return a string containing: 1. The disassembly of the original pickled object. 2. The disassembly of the optimized pickled object. Constraints - You must use the `pickletools.dis` and `pickletools.optimize` functions provided by the \\"pickletools\\" module. - The output from disassembly should be clear and properly formatted. Example ```python # Example usage example_pickle = b\'x80x03Kx01Kx02x86qx00.\' result = analyze_and_optimize_pickle(example_pickle) print(result) # Expected output (The actual output format may vary based on the implementation) Original Pickle Disassembly: 0: x80 PROTO 3 2: K BININT1 1 4: K BININT1 2 6: x86 TUPLE2 7: q BINPUT 0 9: . STOP highest protocol among opcodes = 2 Optimized Pickle Disassembly: 0: x80 PROTO 3 2: K BININT1 1 4: K BININT1 2 6: x86 TUPLE2 7: . STOP highest protocol among opcodes = 2 ``` Notes - The optimized pickle may have fewer opcodes than the original if the optimization process eliminates redundant opcodes. - Ensure that you test your function with multiple pickle strings to validate its accuracy.","solution":"import pickletools def analyze_and_optimize_pickle(pickled_obj: bytes) -> str: Analyze and optimize the given pickled object. Args: - pickled_obj (bytes): A pickled object in binary format. Returns: - str: A string representation of the disassembly before and after optimization. import io # Disassemble the original pickled object original_disassembly = io.StringIO() pickletools.dis(pickled_obj, original_disassembly) original_disassembly.seek(0) original_output = original_disassembly.read() # Optimize the pickled object optimized_pickle = pickletools.optimize(pickled_obj) # Disassemble the optimized pickled object optimized_disassembly = io.StringIO() pickletools.dis(optimized_pickle, optimized_disassembly) optimized_disassembly.seek(0) optimized_output = optimized_disassembly.read() # Combine and format the output result = \\"Original Pickle Disassembly:n\\" + original_output + \\"n\\" + \\"Optimized Pickle Disassembly:n\\" + optimized_output return result"},{"question":"**Config Parser Challenge** You are required to write a Python function that parses a given INI configuration file and retrieves specific configuration values based on provided keys. The function should demonstrate an understanding of various features provided by the `configparser` module, such as supported data types, fallback values, and value interpolation. # Requirements: 1. **Function Name**: `parse_config` 2. **Input**: - `file_path` (string): The path to the INI configuration file to be parsed. - `section` (string): The section name within the INI file. - `key` (string): The key whose value needs to be retrieved. - `fallback` (string or None): The fallback value if the key is not found (default is None). 3. **Output**: - Return the value corresponding to the provided key within the specified section. If the key does not exist, return the fallback value or raise a `KeyError` if no fallback is provided. 4. **Constraints**: - You must handle cases where the INI file does not exist, is malformed, or the section/key is not found. - Use the `configparser` module\'s features like value interpolation and fallback values. - The code should be efficient and perform necessary error handling. # Example: Consider the following INI file (config.ini): ```ini [General] app_name = MyApp version = 1.0 [Database] user = admin password = secret host = localhost port = 3306 ``` Case 1: ```python result = parse_config(\'config.ini\', \'General\', \'app_name\') print(result) # Output: MyApp ``` Case 2: ```python result = parse_config(\'config.ini\', \'Database\', \'password\', fallback=\'default_pwd\') print(result) # Output: secret ``` Case 3: ```python result = parse_config(\'config.ini\', \'Database\', \'invalid_key\', fallback=\'default_val\') print(result) # Output: default_val ``` Case 4: ```python result = parse_config(\'config.ini\', \'InvalidSection\', \'user\') # Raise KeyError ``` # Implementation Guidance: - Use the `ConfigParser` and related methods to read the file, navigate sections, and fetch key values. - Implement appropriate exception handling for file not found, malformed files, sections, and keys not found scenarios. - Illustrate the use of fallback values and interpolation within the INI file. Implement the function `parse_config` capturing the above requirements.","solution":"import configparser import os def parse_config(file_path, section, key, fallback=None): Parses an INI configuration file and retrieves the value for the specified key within a given section. Parameters: file_path (string): The path to the INI configuration file to be parsed. section (string): The section name within the INI file. key (string): The key whose value needs to be retrieved. fallback (string or None): The fallback value if the key is not found (default is None). Returns: value (string): The value corresponding to the provided key within the specified section. Raises: FileNotFoundError: If the configuration file does not exist. KeyError: If the section or key does not exist and no fallback is provided. if not os.path.exists(file_path): raise FileNotFoundError(f\\"File \'{file_path}\' does not exist.\\") config = configparser.ConfigParser() config.read(file_path) if section not in config: raise KeyError(f\\"Section \'{section}\' not found in the configuration file.\\") if fallback is None and key not in config[section]: raise KeyError(f\\"Key \'{key}\' not found in section \'{section}\' and no fallback provided.\\") return config[section].get(key, fallback)"},{"question":"Objective: Implement a function to compute a custom similarity score for a given dataset using a combination of distance metrics and kernels provided by scikit-learn. The similarity score should be calculated through a specified sequence of transformations. Task Description: Write a Python function `custom_similarity_score(X: np.ndarray, method_sequence: List[Tuple[str, Dict]]) -> np.ndarray` that: 1. Takes in a numpy array `X` of shape `(n_samples, n_features)`. 2. Takes in a list `method_sequence`, where each element is a tuple containing: - The name of the method (`str`), which can be one of: `\'pairwise_distances\'`, `\'cosine_similarity\'`, `\'linear_kernel\'`, `\'polynomial_kernel\'`, `\'sigmoid_kernel\'`, `\'rbf_kernel\'`, `\'laplacian_kernel\'`, or `\'chi2_kernel\'`. - A configuration dictionary (`Dict`) that provides necessary parameters for the method. The function should apply the methods in the order they are provided in `method_sequence`, with each method taking the output from the previous method as its input. The final output should be the resulting similarity score matrix. Input: - `X`: A numpy array of shape `(n_samples, n_features)`. - `method_sequence`: A list of tuples, where each tuple contains: - `method` (str): The name of the method to apply. - `params` (Dict): The parameters for the method. Output: - A numpy array representing the final similarity score matrix. Constraints: - `method_sequence` will contain between 1 and 5 methods. - The provided methods and parameters should be valid according to scikit-learn\'s documentation. - You should use the appropriate functions from `sklearn.metrics` and `sklearn.metrics.pairwise`. Example: ```python import numpy as np X = np.array([[2, 3], [3, 5], [5, 8]]) method_sequence = [ (\'pairwise_distances\', {\'metric\': \'euclidean\'}), (\'rbf_kernel\', {\'gamma\': 0.1}) ] similarity_matrix = custom_similarity_score(X, method_sequence) print(similarity_matrix) ``` Expected output (values may vary slightly due to floating-point precision): ``` array([[1. , 0.74081822, 0.36787944], [0.74081822, 1. , 0.74081822], [0.36787944, 0.74081822, 1. ]]) ``` Note: - You may need to import relevant modules from `sklearn.metrics` and `sklearn.metrics.pairwise`.","solution":"import numpy as np from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import cosine_similarity, linear_kernel, polynomial_kernel, sigmoid_kernel, rbf_kernel, laplacian_kernel, chi2_kernel def custom_similarity_score(X: np.ndarray, method_sequence: list) -> np.ndarray: methods = { \'pairwise_distances\': pairwise_distances, \'cosine_similarity\': cosine_similarity, \'linear_kernel\': linear_kernel, \'polynomial_kernel\': polynomial_kernel, \'sigmoid_kernel\': sigmoid_kernel, \'rbf_kernel\': rbf_kernel, \'laplacian_kernel\': laplacian_kernel, \'chi2_kernel\': chi2_kernel } output = X for method, params in method_sequence: func = methods.get(method) if func: output = func(output, **params) else: raise ValueError(f\\"Method {method} not recognized.\\") return output"},{"question":"Objective Your task is to demonstrate your understanding of seaborn by loading a dataset, processing it, and creating a specified type of visualization. This will involve both data manipulation and plotting using seaborn\'s advanced features. Instructions 1. **Load and Prepare the Data**: - Load the `healthexp` dataset using seaborn\'s `load_dataset` function. - Perform the following transformations on the dataset: - Pivot the dataset to have \\"Year\\" as index, \\"Country\\" as columns, and \\"Spending_USD\\" as values. - Interpolate missing values. - Stack the resulting dataframe. - Rename the stacked series to \\"Spending_USD\\". - Reset the index and sort the values by \\"Country\\". 2. **Create Visualizations**: - Generate an area plot using seaborn.objects with the following requirements: - Plot \\"Year\\" on the x-axis and \\"Spending_USD\\" on the y-axis. - Facet the plot by \\"Country\\" and wrap the facets in 3 columns. - Use color to distinguish different countries. - Add two layers to the plot: - An area layer with alpha set to 0.7. - Line layers to outline the spending for each country. Code Implementation ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load and prepare the data healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Step 2: Create visualizations p = so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(alpha=0.7), color=\\"Country\\").add(so.Line()) p.show() ``` Expected Output The output should be a faceted area plot with different countries in separate facets, with a shaded area representing the spending in USD over the years, and lines outlining the spending trends. Constraints - Ensure that your plot is reasonable in terms of performance and loading time. - Handle any missing data by appropriate interpolation. - The plot should be clear and visually distinguishable. Evaluation Your solution will be evaluated based on: - Correctness of data manipulation and processing. - Usage of seaborn\'s plotting capabilities effectively and accurately. - Proper faceting, coloring, and layering of the plot. - Code readability and adherence to the specified instructions.","solution":"import seaborn.objects as so import pandas as pd from seaborn import load_dataset # Step 1: Load and prepare the data healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate(method=\'linear\') .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Step 2: Create visualizations p = so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\").facet(\\"Country\\", wrap=3) p.add(so.Area(alpha=0.7)).add(so.Line()).show()"},{"question":"# Custom Object Type Implementation in Python Objective: Your task is to define a new custom object type in Python that simulates a \\"SpecialList\\". This object should mimic Python\'s built-in list but with additional functionalities. This exercise will test your comprehension of fundamental and advanced concepts of object-oriented programming, memory management, and extension types in Python. Task: 1. **Define a New Type**: - Implement a new object type named `SpecialList`. 2. **Implement Core Functionalities**: - Mimic the core functionalities of Python\'s built-in list, such as adding elements, retrieving elements, and showing the length of the list. 3. **Additional Functionalities**: - Implement a method `get_primes()` that returns all prime numbers stored in the list. - Implement a method `sum_elements()` that returns the sum of all elements in the list. 4. **Memory Management**: - Ensure that proper memory allocation and deallocation methods are used to manage the internal storage of the list. Expected Input and Output: - `__init__(self, iterable=[])`: Initialize the `SpecialList` with an optional iterable. - `add_element(self, value)`: Adds an element to the list. - `get_element(self, index)`: Retrieves an element from the list based on the index. - `get_length(self)`: Returns the number of elements in the list. - `get_primes(self)`: Returns a list of all prime numbers in the list. - `sum_elements(self)`: Returns the sum of all elements in the list. Constraints and Performance: - You should handle exceptions and errors gracefully. - Operations should be optimized for performance to handle large lists efficiently. Example: ```python # Creating a SpecialList with initial elements special_list = SpecialList([2, 3, 4, 5, 6, 7]) # Adding an element to the SpecialList special_list.add_element(11) # Get an element by index print(special_list.get_element(2)) # Output: 4 # Get length of SpecialList print(special_list.get_length()) # Output: 7 # Get all prime numbers from SpecialList print(special_list.get_primes()) # Output: [2, 3, 5, 7, 11] # Get the sum of all elements in SpecialList print(special_list.sum_elements()) # Output: 38 ``` **Note**: You are not allowed to use Python\'s built-in list for the storage of elements directly. Good luck!","solution":"class SpecialList: def __init__(self, iterable=[]): self._elements = list(iterable) def add_element(self, value): self._elements.append(value) def get_element(self, index): try: return self._elements[index] except IndexError: return \\"Index out of range\\" def get_length(self): return len(self._elements) def get_primes(self): return [x for x in self._elements if self._is_prime(x)] def sum_elements(self): return sum(self._elements) def _is_prime(self, n): if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True"},{"question":"# Serialization with the `pickle` Module You are tasked with designing a lightweight object persistence system using the `pickle` module. This system will store and retrieve Python objects to and from a file. Additionally, you will need to implement custom serialization mechanisms for specific classes. Part 1: Basic Serialization 1. **Creating a Simple Serialization Function**: - Write a function `serialize_object` that takes a Python object and a file path as input and saves the object to the file using the highest available pickle protocol. ```python import pickle def serialize_object(obj, file_path): Serialize a Python object and save it to a file. :param obj: The Python object to serialize. :param file_path: The file path where the object should be saved. with open(file_path, \'wb\') as file: pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL) ``` 2. **Creating a Simple Deserialization Function**: - Write a function `deserialize_object` that takes a file path as input and returns the Python object stored in that file. ```python import pickle def deserialize_object(file_path): Deserialize a Python object from a file. :param file_path: The file path from which to load the object. :return: The Python object loaded from the file. with open(file_path, \'rb\') as file: return pickle.load(file) ``` Part 2: Customizing Serialization 1. **Custom Class with Special Serialization Needs**: - Create a class `CustomData` that contains a file path and a counter. - Implement custom methods such as `__getstate__` and `__setstate__` to handle the serialization and deserialization of this class: - When serialized, the current state of the counter should be saved, but the file should be reopened and reading should resume from the recorded line number when deserialized. ```python class CustomData: def __init__(self, file_path): self.file_path = file_path self.file = open(file_path) self.counter = 0 def readline(self): self.counter += 1 return self.file.readline() def __getstate__(self): state = self.__dict__.copy() del state[\'file\'] return state def __setstate__(self, state): self.__dict__.update(state) self.file = open(self.file_path) for _ in range(self.counter): self.file.readline() ``` 2. **Pickling Functions to Handle our Custom Class**: - Extend the `serialize_object` and `deserialize_object` functions to correctly handle instances of `CustomData` class. ```python def serialize_object(obj, file_path): Serialize a Python object and save it to a file. :param obj: The Python object to serialize. :param file_path: The file path where the object should be saved. with open(file_path, \'wb\') as file: if isinstance(obj, CustomData): pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL) else: raise ValueError(\'The object is not serializable.\') def deserialize_object(file_path): Deserialize a Python object from a file. :param file_path: The file path from which to load the object. :return: The Python object loaded from the file. with open(file_path, \'rb\') as file: obj = pickle.load(file) if isinstance(obj, CustomData): return obj else: raise ValueError(\'The file does not contain a serializable object.\') ``` Part 3: Testing and Security Considerations 1. **Test your serialization functions**: - Ensure that both basic types and instances of the `CustomData` class can be successfully pickled and unpickled without losing any data. 2. **Security**: - Explain the potential security risks of using the `pickle` module and suggest best practices to mitigate these risks. **Constraints**: - You should not assume any specific file paths or contents other than those explicitly mentioned. - Do not use any external libraries other than the standard Python library. - Ensure that your custom class properly handles large files without significant performance degradation. **Performance Requirements**: - The solution should avoid unnecessary memory usage and should handle large files efficiently.","solution":"import pickle def serialize_object(obj, file_path): Serialize a Python object and save it to a file. :param obj: The Python object to serialize. :param file_path: The file path where the object should be saved. with open(file_path, \'wb\') as file: pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL) def deserialize_object(file_path): Deserialize a Python object from a file. :param file_path: The file path from which to load the object. :return: The Python object loaded from the file. with open(file_path, \'rb\') as file: return pickle.load(file) class CustomData: def __init__(self, file_path): self.file_path = file_path self.file = open(file_path) self.counter = 0 def readline(self): self.counter += 1 return self.file.readline() def __getstate__(self): state = self.__dict__.copy() del state[\'file\'] return state def __setstate__(self, state): self.__dict__.update(state) self.file = open(self.file_path) for _ in range(self.counter): self.file.readline()"},{"question":"# Advanced Python Asyncio Synchronization You are tasked with implementing a concurrent system that simulates a shared resource accessed by multiple clients. Each client can perform read or write operations on the shared resource. Multiple clients can read simultaneously, but write operations must have exclusive access. You will implement this using `asyncio` synchronization primitives. Your task: 1. Implement an `AsyncReadWriteLock` class using `asyncio.Condition` that allows multiple concurrent readers but only one writer. The lock should adhere to the following specifications: - **read_lock()**: A coroutine used to acquire a read lock. - **write_lock()**: A coroutine used to acquire a write lock. - **release_read()**: Releases a read lock. - **release_write()**: Releases a write lock. 2. Implement a simulation function, `simulate_readers_writers`, which takes the following arguments: - `num_readers`: Number of reader clients. - `num_writers`: Number of writer clients. - `read_time`: Time each reader spends reading. - `write_time`: Time each writer spends writing. Readers and writers should repeatedly acquire and release the `AsyncReadWriteLock` and wait for the respective `read_time` and `write_time`. Example: ```python import asyncio class AsyncReadWriteLock: def __init__(self): self._condition = asyncio.Condition() self._readers = 0 self._writer = False async def read_lock(self): async with self._condition: while self._writer: await self._condition.wait() self._readers += 1 async def write_lock(self): async with self._condition: while self._writer or self._readers > 0: await self._condition.wait() self._writer = True async def release_read(self): async with self._condition: self._readers -= 1 if self._readers == 0: self._condition.notify_all() async def release_write(self): async with self._condition: self._writer = False self._condition.notify_all() async def simulate_readers_writers(num_readers, num_writers, read_time, write_time): async def reader(lock): while True: await lock.read_lock() print(\\"Reading\\") await asyncio.sleep(read_time) await lock.release_read() async def writer(lock): while True: await lock.write_lock() print(\\"Writing\\") await asyncio.sleep(write_time) await lock.release_write() lock = AsyncReadWriteLock() tasks = [] for _ in range(num_readers): tasks.append(asyncio.create_task(reader(lock))) for _ in range(num_writers): tasks.append(asyncio.create_task(writer(lock))) await asyncio.gather(*tasks) # Example usage asyncio.run(simulate_readers_writers(num_readers=2, num_writers=1, read_time=1, write_time=2)) ``` Input - `num_readers`: Integer, number of reader clients. - `num_writers`: Integer, number of writer clients. - `read_time`: Integer, time each reader spends reading (in seconds). - `write_time`: Integer, time each writer spends writing (in seconds). Output The simulation should print interleaved \\"Reading\\" and \\"Writing\\" statements to the console, indicating the concurrent execution of reader and writer operations. Constraints - Readers and writers should be able to start and stop based on provided input values. - Ensure the implementation avoids deadlocks and race conditions.","solution":"import asyncio class AsyncReadWriteLock: def __init__(self): self._condition = asyncio.Condition() self._readers = 0 self._writer = False async def read_lock(self): async with self._condition: while self._writer: await self._condition.wait() self._readers += 1 async def write_lock(self): async with self._condition: while self._writer or self._readers > 0: await self._condition.wait() self._writer = True async def release_read(self): async with self._condition: self._readers -= 1 if self._readers == 0: self._condition.notify_all() async def release_write(self): async with self._condition: self._writer = False self._condition.notify_all() async def simulate_readers_writers(num_readers, num_writers, read_time, write_time): lock = AsyncReadWriteLock() async def reader(reader_id): while True: await lock.read_lock() print(f\\"Reader {reader_id} is reading\\") await asyncio.sleep(read_time) await lock.release_read() await asyncio.sleep(read_time) async def writer(writer_id): while True: await lock.write_lock() print(f\\"Writer {writer_id} is writing\\") await asyncio.sleep(write_time) await lock.release_write() await asyncio.sleep(write_time) tasks = [asyncio.create_task(reader(i)) for i in range(num_readers)] tasks += [asyncio.create_task(writer(i)) for i in range(num_writers)] await asyncio.gather(*tasks)"},{"question":"**Objective:** To assess the ability to work with the `wave` module for reading and writing WAV files, and understanding of file I/O operations. **Problem Statement:** You are tasked with writing a Python function `convert_stereo_to_mono(input_wav: str, output_wav: str) -> None` that reads a stereo WAV file and converts it to a mono WAV file by averaging the left and right channel audio data. The output file should have the same sample width and frame rate as the input file. **Function Signature:** ```python def convert_stereo_to_mono(input_wav: str, output_wav: str) -> None: pass ``` **Input:** * `input_wav` (str): The path to the input stereo WAV file. * `output_wav` (str): The path to the output mono WAV file. **Output:** * The function should not return anything. It should write the converted mono audio data to `output_wav`. **Constraints:** * The input WAV file is guaranteed to be a valid stereo WAV file with `WAVE_FORMAT_PCM`. * The output WAV file should be a valid mono WAV file with the same sample rate and sample width as the input file. **Instructions:** 1. Open the input WAV file in read mode. 2. Retrieve the number of channels, sample width, frame rate, and number of frames from the input file. 3. Ensure the input file has 2 channels (stereo). 4. Read the audio frames from the input file. 5. Convert the stereo frames to mono frames by averaging the left and right channels. 6. Open the output WAV file in write mode. 7. Set the parameters (number of channels, sample width, frame rate) for the output file. 8. Write the mono audio frames to the output file. 9. Close both the input and output files properly. **Example:** If the input WAV file has the following parameters: * 2 channels (stereo) * Sample width of 2 bytes * Frame rate of 44100 Hz And its audio data can be represented as: * Frame 1: (L1, R1) * Frame 2: (L2, R2) * ... * Frame N: (LN, RN) Where `Lk` and `Rk` are the left and right channel samples for the k-th frame. The output WAV file should be a mono file with: * 1 channel (mono) * The same sample width and frame rate as the input file And its audio data should be: * Frame 1: ((L1 + R1) / 2) * Frame 2: ((L2 + R2) / 2) * ... * Frame N: ((LN + RN) / 2) **Note:** Ensure proper error handling, especially when dealing with file operations.","solution":"import wave import numpy as np def convert_stereo_to_mono(input_wav: str, output_wav: str) -> None: Converts a stereo WAV file to a mono WAV file by averaging the left and right channel. with wave.open(input_wav, \'rb\') as in_wav: params = in_wav.getparams() num_channels = params.nchannels sample_width = params.sampwidth frame_rate = params.framerate num_frames = params.nframes if num_channels != 2: raise ValueError(\\"Input WAV file is not stereo.\\") stereo_frames = in_wav.readframes(num_frames) # Convert the binary data to numpy array for easier manipulation data = np.frombuffer(stereo_frames, dtype=np.int16) data = data.reshape((num_frames, 2)) # Average the left and right channel to get mono data mono_data = data.mean(axis=1).astype(np.int16) # Convert mono data back to binary format mono_frames = mono_data.tobytes() with wave.open(output_wav, \'wb\') as out_wav: out_wav.setnchannels(1) out_wav.setsampwidth(sample_width) out_wav.setframerate(frame_rate) out_wav.writeframes(mono_frames)"},{"question":"# Subprocess Management in Python Your task is to implement a function that processes a given command using Python\'s `subprocess` module. The function should cover the following requirements: 1. Implement a function `run_subprocess(command: str, shell: bool, capture_output: bool, timeout: int) -> dict` that executes the command in a subprocess. 2. The `command` parameter is a string representing the command to be executed. 3. The `shell` parameter (boolean) determines whether the command should be executed through the shell. 4. The `capture_output` parameter (boolean) specifies whether the output (stdout and stderr) should be captured or not. 5. The `timeout` parameter (integer) specifies the maximum time to wait for the command to complete. If the time is exceeded, the command should be terminated, and an appropriate error should be raised and handled. The function should return a dictionary with the following keys: - `returncode`: the exit status of the process. - `stdout`: captured standard output (if `capture_output` is True, `None` otherwise). - `stderr`: captured standard error (if `capture_output` is True, `None` otherwise). - `error`: any error that occurred during the execution (e.g., timeout or non-zero exit code). This should be `None` if the command executed successfully. Implement necessary exception handling to manage scenarios where: - The command fails to execute (non-zero exit status). - The subprocess exceeds the specified timeout. Example ```python def run_subprocess(command: str, shell: bool, capture_output: bool, timeout: int) -> dict: # Your implementation here # Example usage: result = run_subprocess(\'ls -l\', shell=False, capture_output=True, timeout=5) print(result) # Expected output: # { # \'returncode\': 0, # \'stdout\': \'total 0n drwxr-xr-x 2 user group 64 Jan 1 00:00 dirn\', # \'stderr\': \'\', # \'error\': None # } ``` # Constraints 1. You must use the `subprocess.run()` method primarily. 2. Handle the exceptions `subprocess.CalledProcessError` and `subprocess.TimeoutExpired`. 3. Ensure that the function works across different operating systems (Windows, Linux, macOS). Performance Requirements The function should be able to handle typical shell commands efficiently within the specified timeout limit.","solution":"import subprocess def run_subprocess(command: str, shell: bool, capture_output: bool, timeout: int) -> dict: result = { \'returncode\': None, \'stdout\': None, \'stderr\': None, \'error\': None } try: completed_process = subprocess.run( command, shell=shell, capture_output=capture_output, timeout=timeout, text=True, check=False ) result[\'returncode\'] = completed_process.returncode if capture_output: result[\'stdout\'] = completed_process.stdout result[\'stderr\'] = completed_process.stderr except subprocess.TimeoutExpired as e: result[\'error\'] = f\\"TimeoutExpired: {e}\\" except subprocess.SubprocessError as e: result[\'error\'] = f\\"SubprocessError: {e}\\" if result[\'returncode\'] != 0 and not result[\'error\']: result[\'error\'] = \\"Non-zero exit status\\" return result"},{"question":"# Pseudo-Terminal Session Logger **Objective**: Your task is to implement a Python script that uses the `pty` module to log the interaction of a spawned subprocess to both a file and the console in real-time. **Requirements**: 1. Implement a function `log_terminal_session(command: str, log_filename: str) -> int` that: - Takes as input: - `command`: A string representing the command to be executed in the pseudo-terminal. - `log_filename`: A string representing the path to the file where the session should be logged. - Spawns the subprocess for the given command using the `pty.spawn` function. - Logs all input from the subprocess to both the console and the specified log file in real-time. - Returns the exit status of the subprocess. **Constraints**: - The solution must make use of the `pty` module and its `spawn()` function. - The custom read function passed to `pty.spawn` should handle real-time logging to both the console and the log file. **Example Usage**: ```python # Example usage of log_terminal_session function exit_status = log_terminal_session(\'/bin/bash\', \'session.log\') print(f\'Process exited with status {exit_status}\') ``` **Expected Output**: - The `log_terminal_session` function should display the subprocess output on the console as it happens. - The log file `session.log` should contain the same output. - The function should return the exit status of the subprocess after it terminates. **Hint**: - Use the `os.read` function to read data from the file descriptor inside the read function. - The log file should be opened in binary write mode. Here is the signature for the function to be implemented: ```python def log_terminal_session(command: str, log_filename: str) -> int: # Implementation here ``` **Performance Considerations**: - Ensure that the logging to the console and file occurs with minimal delay to mimic real-time behavior. - The solution should handle large amounts of data gracefully without missing or delaying output. Good luck with your implementation!","solution":"import pty import os def log_terminal_session(command: str, log_filename: str) -> int: def read(fd): data = os.read(fd, 1024) with open(log_filename, \'ab\') as log_file: log_file.write(data) os.write(1, data) return data pid, fd = pty.fork() if pid == 0: # Child process os.execlp(command, command) else: # Parent process try: pty._copy(fd, read) except Exception as e: print(f\\"Error: {e}\\") finally: pid, status = os.waitpid(pid, 0) return os.WEXITSTATUS(status)"},{"question":"**Advanced Coding Assessment: Implementing a Custom Iterable with Exception Handling** # Objective To assess your understanding of compound statements, function, and class definitions in Python. # Problem Statement You are required to design a custom iterable class in Python named `SecureRange` that mimics Pythonâ€™s built-in `range()` function but with added functionality. This class should: 1. Allow iteration over a range of numbers, similar to the built-in `range()`. 2. Include exception handling for invalid range definitions (i.e., if the start is greater than the end). 3. Utilize the context management protocol (`with` statement) to ensure the resource (in this case, a simple log file) is properly handled. 4. Provide a method `reset(self)` that allows the range to be redefined. # Requirements Class Definition Define a class `SecureRange` with the following methods: - `__init__(self, start: int, end: int)` - Initializes the iteration range from `start` to `end`. - If `start` is greater than `end`, raise a `ValueError` with an appropriate error message. - `__iter__(self)` - Returns an iterator object. - `__next__(self)` - Returns the next number in the range. If the end of the range is reached, raise `StopIteration`. - `reset(self, start: int, end: int)` - Redefines the range with the given `start` and `end` parameters. - If `start` is greater than `end`, raise a `ValueError` with an appropriate error message. Context Manager Protocol - Implement the context manager protocol by defining `__enter__` and `__exit__` methods. - `__enter__` opens a file named `log.txt` in write mode. - `__exit__` ensures the file is closed after exiting the context. # Input and Output 1. The class will be initialized with two integers, `start` and `end`. 2. The class should correctly implement the iterable protocol. 3. The class should handle invalid range definitions with exceptions. 4. The `reset` method should redefine the range. 5. The context management should properly handle the log file. # Constraints - You should not use any external libraries for iteration or context management. - Focus on using Pythonâ€™s built-in features. # Example Usage ```python try: with SecureRange(5, 10) as sr: for number in sr: print(number) # Outputs numbers from 5 to 9 sr.reset(15, 20) for number in sr: print(number) # Outputs numbers from 15 to 19 except ValueError as e: print(e) # In case of invalid range definitions ``` # Notes - During the iteration, each number processed should be logged into the `log.txt` file opened in `__enter__` and closed in `__exit__`. - Ensure the resetting and redefinition of the range do not raise an error unless the new range is invalid. # Performance Ensure your implementation can efficiently handle ranges up to 10^6 elements without significant performance degradation. Good luck!","solution":"class SecureRange: def __init__(self, start, end): if start > end: raise ValueError(\\"Start must not be greater than end.\\") self.start = start self.end = end self.current = start def __iter__(self): self.current = self.start return self def __next__(self): if self.current >= self.end: raise StopIteration number = self.current self.current += 1 # Log the current number self.log_file.write(f\\"{number}n\\") return number def reset(self, start, end): if start > end: raise ValueError(\\"Start must not be greater than end.\\") self.start = start self.end = end self.current = start def __enter__(self): self.log_file = open(\'log.txt\', \'w\') return self def __exit__(self, exc_type, exc_value, traceback): self.log_file.close()"},{"question":"# PyTorch Dynamic Shapes Coding Assessment Objective: Implement a PyTorch function that demonstrates the use of dynamic shapes to handle input batches of varying sizes efficiently. Problem Statement: You are given two tensors: one representing the input data and another representing the corresponding labels for a batch of data. The batch size can vary, and you need to implement a function `process_batch` that performs the following: 1. Merges the input data and labels into a single tensor. 2. Applies a different operation to the merged tensor based on the batch size: - If the batch size is greater than a certain threshold, double the values in the tensor. - Otherwise, add a bias tensor to the values in the merged tensor. Use dynamic shapes to handle the varying batch sizes without recompiling the function for each new batch size. Function Signature: ```python import torch def process_batch(inputs: torch.Tensor, labels: torch.Tensor, threshold: int, bias: torch.Tensor) -> torch.Tensor: Process a batch of inputs and labels with dynamic shape handling. Args: inputs (torch.Tensor): A tensor of shape (batch_size, input_dim). labels (torch.Tensor): A tensor of shape (batch_size, label_dim). threshold (int): The batch size threshold to decide the operation. bias (torch.Tensor): A tensor to be added to the merged tensor if batch size is less than or equal to the threshold. Returns: torch.Tensor: The processed tensor. pass ``` Constraints: - The `inputs` tensor has a shape of `(batch_size, input_dim)`. - The `labels` tensor has a shape of `(batch_size, label_dim)`. - The `bias` tensor has a shape that matches the merged tensor shape `(batch_size, input_dim + label_dim)`. - The threshold for deciding the operation on the merged tensor is provided as an integer. Example: ```python inputs = torch.tensor([[1, 2], [3, 4], [5, 6]]) labels = torch.tensor([[1], [2], [3]]) threshold = 2 bias = torch.tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) result = process_batch(inputs, labels, threshold, bias) print(result) # Should print the merged tensor with appropriate operation applied ``` Notes: - Leverage `torch._dynamo.mark_dynamic` to mark the batch size dimension as dynamic. - Handle the conditional branching based on the batch size using the guard model. - Ensure the function is efficient and does not recompile for every new batch size encountered during execution.","solution":"import torch def process_batch(inputs: torch.Tensor, labels: torch.Tensor, threshold: int, bias: torch.Tensor) -> torch.Tensor: Process a batch of inputs and labels with dynamic shape handling. Args: inputs (torch.Tensor): A tensor of shape (batch_size, input_dim). labels (torch.Tensor): A tensor of shape (batch_size, label_dim). threshold (int): The batch size threshold to decide the operation. bias (torch.Tensor): A tensor to be added to the merged tensor if batch size is less than or equal to the threshold. Returns: torch.Tensor: The processed tensor. batch_size = inputs.size(0) merged_tensor = torch.cat((inputs, labels), dim=1) if batch_size > threshold: result = merged_tensor * 2 else: result = merged_tensor + bias return result"},{"question":"# Gaussian Process Regression with Custom Kernels Problem Statement Given a dataset with input features (X) and target values (y), implement a function that performs Gaussian Process Regression using different kernels. Your implementation should allow switching between various kernels and optimize the hyperparameters of the kernels during the fitting process. You should also be able to specify the noise level in the data. Function Signature ```python def gp_regression(X: np.ndarray, y: np.ndarray, kernel_type: str, noise_level: float = 1e-10) -> GaussianProcessRegressor: Perform Gaussian Process Regression on the given dataset using the specified kernel. Parameters: - X: np.ndarray of shape (n_samples, n_features) The input features. - y: np.ndarray of shape (n_samples,) The target values. - kernel_type: str The type of kernel to use. Can be one of \'RBF\', \'Matern\', \'RationalQuadratic\', \'ExpSineSquared\', or \'DotProduct\'. - noise_level: float, optional (default=1e-10) The noise level to add to the diagonal of the kernel matrix. Returns: - model: GaussianProcessRegressor The fitted Gaussian Process Regressor model. ``` Requirements 1. **Kernel Selection**: - Implement the ability to choose between different kernels: \'RBF\', \'Matern\', \'RationalQuadratic\', \'ExpSineSquared\', or \'DotProduct\'. 2. **Noise Handling**: - The function should be able to handle a specified noise level by adding it to the diagonal of the kernel matrix. 3. **Hyperparameter Optimization**: - Optimize the hyperparameters of the kernel during the fitting process. 4. **Prediction**: - The function should return the fitted `GaussianProcessRegressor` model. Input - `X`: A 2D numpy array of shape (n_samples, n_features) representing the input features. - `y`: A 1D numpy array of shape (n_samples,) representing the target values. - `kernel_type`: A string specifying the kernel to use. Options include \'RBF\', \'Matern\', \'RationalQuadratic\', \'ExpSineSquared\', and \'DotProduct\'. - `noise_level`: A float specifying the noise level added to the diagonal of the kernel matrix. Default is 1e-10. Output - Returns the fitted `GaussianProcessRegressor` model. Example ```python import numpy as np from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF # Example Data X = np.array([[1], [2], [3], [4], [5]]) y = np.array([1.2, 2.3, 3.1, 4.8, 5.6]) # Perform GPR with RBF kernel model = gp_regression(X, y, kernel_type=\'RBF\') # Making predictions X_new = np.array([[1.5], [2.5], [3.5]]) predictions, std_devs = model.predict(X_new, return_std=True) print(\\"Predictions:\\", predictions) print(\\"Standard Deviations:\\", std_devs) ``` **Note**: Ensure you install the required packages: ``` pip install numpy scikit-learn ``` Evaluate the performance of your model by comparing the predictions with the actual values.","solution":"import numpy as np from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct def gp_regression(X: np.ndarray, y: np.ndarray, kernel_type: str, noise_level: float = 1e-10) -> GaussianProcessRegressor: Perform Gaussian Process Regression on the given dataset using the specified kernel. Parameters: - X: np.ndarray of shape (n_samples, n_features) The input features. - y: np.ndarray of shape (n_samples,) The target values. - kernel_type: str The type of kernel to use. Can be one of \'RBF\', \'Matern\', \'RationalQuadratic\', \'ExpSineSquared\', or \'DotProduct\'. - noise_level: float, optional (default=1e-10) The noise level to add to the diagonal of the kernel matrix. Returns: - model: GaussianProcessRegressor The fitted Gaussian Process Regressor model. # Initialize the kernel based on the kernel_type if kernel_type == \'RBF\': kernel = RBF() elif kernel_type == \'Matern\': kernel = Matern() elif kernel_type == \'RationalQuadratic\': kernel = RationalQuadratic() elif kernel_type == \'ExpSineSquared\': kernel = ExpSineSquared() elif kernel_type == \'DotProduct\': kernel = DotProduct() else: raise ValueError(\\"Unsupported kernel_type. Supported types: \'RBF\', \'Matern\', \'RationalQuadratic\', \'ExpSineSquared\', \'DotProduct\'\\") # Add noise level kernel += RBF(length_scale=noise_level) # Create and fit the Gaussian Process Regressor model model = GaussianProcessRegressor(kernel=kernel, alpha=noise_level) model.fit(X, y) return model"},{"question":"**XML Document Transformation with `xml.etree.ElementTree`** You are given an XML document containing information about a collection of books. Your task is to write a Python function that parses the XML, modifies certain elements, and outputs a modified XML string. The modifications include updating the price of certain books and adding a new book to the collection. # Input 1. `xml_string` (str): A string containing XML data representing a collection of books. Each book entry has the following structure: ```xml <book> <title>Title of the book</title> <author>Author Name</author> <price>Price</price> </book> ``` 2. `updates` (dict): A dictionary where keys are book titles and values are the new prices for those books. 3. `new_book` (dict): A dictionary with keys `\'title\'`, `\'author\'`, and `\'price\'` representing a new book to be added to the collection. # Output - Return a string containing the modified XML. # Constraints - The XML string is well-formed. - Each title in the `updates` dictionary corresponds to a unique book title in the XML string. - The `price` will always be a positive number. # Example Input ```python xml_string = \'\'\' <library> <book> <title>Book A</title> <author>Author A</author> <price>10.00</price> </book> <book> <title>Book B</title> <author>Author B</author> <price>20.00</price> </book> </library> \'\'\' updates = { \\"Book A\\": \\"15.00\\", \\"Book B\\": \\"22.00\\" } new_book = { \\"title\\": \\"Book C\\", \\"author\\": \\"Author C\\", \\"price\\": \\"12.50\\" } ``` Output ```xml <library> <book> <title>Book A</title> <author>Author A</author> <price>15.00</price> </book> <book> <title>Book B</title> <author>Author B</author> <price>22.00</price> </book> <book> <title>Book C</title> <author>Author C</author> <price>12.50</price> </book> </library> ``` # Instructions Write a function `modify_books(xml_string: str, updates: dict, new_book: dict) -> str` that performs the described modifications. # Function Signature ```python def modify_books(xml_string: str, updates: dict, new_book: dict) -> str: pass ``` **Note**: Remember to import and use `xml.etree.ElementTree` for parsing and modifying the XML.","solution":"import xml.etree.ElementTree as ET def modify_books(xml_string: str, updates: dict, new_book: dict) -> str: Parses the XML string, updates the prices of certain books, and adds a new book to the collection. Args: - xml_string (str): The input XML string. - updates (dict): A dictionary with book titles as keys and new prices as values. - new_book (dict): A dictionary representing a new book with keys \'title\', \'author\', and \'price\'. Returns: - str: The modified XML as a string. # Parse the XML string root = ET.fromstring(xml_string) # Update the prices of the existing books for book in root.findall(\'book\'): title = book.find(\'title\').text if title in updates: book.find(\'price\').text = updates[title] # Create a new book element new_book_elem = ET.Element(\'book\') new_title_elem = ET.SubElement(new_book_elem, \'title\') new_title_elem.text = new_book[\'title\'] new_author_elem = ET.SubElement(new_book_elem, \'author\') new_author_elem.text = new_book[\'author\'] new_price_elem = ET.SubElement(new_book_elem, \'price\') new_price_elem.text = new_book[\'price\'] # Add the new book to the root element root.append(new_book_elem) # Get the modified XML string modified_xml_string = ET.tostring(root, encoding=\'unicode\') return modified_xml_string"},{"question":"# Question: Environment and Large File Handling with POSIX Your task is to implement two functions using the `os` module in Python which internally uses the `posix` module when available: `backup_env` and `manage_large_file`. 1. **backup_env**: - **Objective**: Create a backup of the current environment variables and save them to a file. - **Input**: A single string `filename` which represents the name of the file to store the environment variables. - **Output**: None. - **Details**: - The environment variables should be written to the specified file in a key-value pair format, one per line, separated by \'=\'. - Example: `KEY1=value1nKEY2=value2n...` 2. **manage_large_file**: - **Objective**: Read a large file (greater than 2 GiB), perform a line count without loading the entire file into memory at once, and handle potential system errors appropriately. - **Input**: A single string `filepath` which represents the path to the large file. - **Output**: An integer representing the number of lines in the file. - **Details**: - The function should handle files that are larger than 2 GiB. - Handle any OSError exceptions that might arise due to file handling issues and print appropriate error messages. - Use an efficient way to count lines to ensure memory usage is minimal. **Constraints**: - You may assume that the environment file and the large file path are writable and readable, respectively. - Do not use any external libraries for file reading and writing, stick to standard Python file handling methods. # Example Usage: ```python # Example usage of backup_env function backup_env(\\"env_backup.txt\\") # Example usage of manage_large_file function num_lines = manage_large_file(\\"large_file.txt\\") print(f\\"Total lines in large file: {num_lines}\\") ``` Implement the functions ensuring they adhere to the described objectives and handle the specified constraints correctly. Make sure to handle edge cases such as file not found, permission issues, and system-specific behavior.","solution":"import os def backup_env(filename): Create a backup of the current environment variables and save them to a file. :param filename: Name of the file to store the environment variables. try: with open(filename, \'w\') as file: for key, value in os.environ.items(): file.write(f\\"{key}={value}n\\") except OSError as e: print(f\\"Error writing to file {filename}: {e}\\") def manage_large_file(filepath): Read a large file (greater than 2 GiB) and return the number of lines in the file. :param filepath: Path to the large file. :return: Number of lines in the file. line_count = 0 try: with open(filepath, \'r\') as file: for line in file: line_count += 1 except OSError as e: print(f\\"Error reading file {filepath}: {e}\\") return line_count"},{"question":"# Complex Sorting: Events Management System You are tasked to implement a sorting function for an event management system using Python. The system maintains a list of events where each event is represented by a dictionary with the following keys: - `name` (string) - the name of the event - `date` (string in the format YYYY-MM-DD) - the date of the event - `priority` (integer) - the priority of the event Your function should sort the events based on the following criteria: 1. Highest priority first (descending order of priority). 2. For events with the same priority, the earliest date first (ascending order of date). 3. If two events have the same priority and date, sort alphabetically by the event name (ascending order of name). # Function Signature ```python def sort_events(events: list) -> list: pass ``` # Input - `events` - A list of dictionaries, where each dictionary represents an event as described above. # Output - A sorted list of dictionaries based on the criteria mentioned above. # Constraints - The `events` list will contain at most 10^5 events. - The `priority` will be an integer between 1 and 10^6. - The `name` will be a non-empty string with a maximum length of 100 characters. - The `date` will be a valid date string in the format YYYY-MM-DD. # Example ```python events = [ {\\"name\\": \\"Conference\\", \\"date\\": \\"2023-12-24\\", \\"priority\\": 5}, {\\"name\\": \\"Meetup\\", \\"date\\": \\"2023-12-24\\", \\"priority\\": 5}, {\\"name\\": \\"Hackathon\\", \\"date\\": \\"2023-11-01\\", \\"priority\\": 7}, {\\"name\\": \\"Workshop\\", \\"date\\": \\"2023-11-15\\", \\"priority\\": 7}, {\\"name\\": \\"Seminar\\", \\"date\\": \\"2023-11-01\\", \\"priority\\": 7} ] sorted_events = sort_events(events) print(sorted_events) ``` Output: ``` [ {\\"name\\": \\"Hackathon\\", \\"date\\": \\"2023-11-01\\", \\"priority\\": 7}, {\\"name\\": \\"Seminar\\", \\"date\\": \\"2023-11-01\\", \\"priority\\": 7}, {\\"name\\": \\"Workshop\\", \\"date\\": \\"2023-11-15\\", \\"priority\\": 7}, {\\"name\\": \\"Conference\\", \\"date\\": \\"2023-12-24\\", \\"priority\\": 5}, {\\"name\\": \\"Meetup\\", \\"date\\": \\"2023-12-24\\", \\"priority\\": 5} ] ``` # Requirements 1. Implement the `sort_events()` function. 2. Ensure that your solution is efficient and can handle the maximum constraints. 3. You may use the `sorted()` function or `list.sort()` method, but make sure to implement the correct key function to achieve the desired sorting order.","solution":"def sort_events(events: list) -> list: Sort the events based on priority (descending), date (ascending), and name (ascending). return sorted(events, key=lambda x: (-x[\'priority\'], x[\'date\'], x[\'name\']))"},{"question":"Objective You are required to write a Python function that attempts to open a file and handles potential system-related errors by utilizing the `errno` module. The function will return a specific message based on the error encountered, demonstrating your understanding of error handling with the `errno` module. Function Signature ```python def open_file_with_error_handling(filepath: str) -> str: Attempt to open the file specified by `filepath`. Return a specific message based on the system-related error encountered. Parameters: - filepath (str): The path of the file to be opened. Returns: - str: A message indicating success or the type of error encountered. ``` Input and Output - **Input**: A single string `filepath` representing the path to the file that needs to be opened. - **Output**: A message as a string indicating either successful file opening or the type of error encountered. Error Handling Requirements 1. If the file opens successfully, return `\\"File opened successfully.\\"` 2. If the file does not exist, catch the error and return `\\"File not found error.\\"` 3. If there is a permission issue, catch the error and return `\\"Permission denied error.\\"` 4. If the error is related to too many open files, catch the error and return `\\"Too many open files error.\\"` 5. For any other `OSError`, catch the error and return `\\"An unexpected OS error occurred: {error_code}\\"`, where `{error_code}` should be replaced with the error code. Constraints - The function should use the `errno` module to identify and handle specific errors. - Use appropriate exception handling techniques to catch and identify the errors based on their `errno` values. Example ```python # Assuming the following files: # - \'/path/to/existing_file.txt\' exists and is accessible. # - \'/path/to/nonexistent_file.txt\' does not exist. # - \'/path/to/protected_file.txt\' exists but is not accessible due to permission issues. print(open_file_with_error_handling(\'/path/to/existing_file.txt\')) # Output: \\"File opened successfully.\\" print(open_file_with_error_handling(\'/path/to/nonexistent_file.txt\')) # Output: \\"File not found error.\\" print(open_file_with_error_handling(\'/path/to/protected_file.txt\')) # Output: \\"Permission denied error.\\" ``` Notes - Make sure to import the `errno` module in your function. - Handle specific exceptions based on `errno` codes to return accurate messages. - The function should not print anything to the console; it should only return the appropriate message.","solution":"import errno def open_file_with_error_handling(filepath: str) -> str: try: with open(filepath, \'r\') as file: return \\"File opened successfully.\\" except OSError as e: if e.errno == errno.ENOENT: return \\"File not found error.\\" elif e.errno == errno.EACCES: return \\"Permission denied error.\\" elif e.errno == errno.EMFILE: return \\"Too many open files error.\\" else: return f\\"An unexpected OS error occurred: {e.errno}\\""},{"question":"# Email Encoding Utility As an email security application developer, you are tasked with creating a utility that processes email messages by encoding their payloads using different encoding schemes and then verifying the encoded content. While the `email.encoders` module is deprecated in Python 3.10 and later, your task is to ensure backward compatibility for email applications that use this legacy module. Implement the following function to encode the payload of email messages: ```python import email from email.encoders import encode_quopri, encode_base64, encode_7or8bit, encode_noop from email.message import Message def encode_email_message(msg: Message, encoding_scheme: str) -> Message: Encodes the payload of an email Message object using the specified encoding scheme. Parameters: msg (Message): The email Message object to encode. encoding_scheme (str): The encoding scheme to use. Options are \\"quoted-printable\\", \\"base64\\", \\"7or8bit\\", \\"noop\\". Returns: Message: The encoded email Message object. Raises: ValueError: If the encoding_scheme is not valid. TypeError: If the msg is a multipart message. encoding_functions = { \\"quoted-printable\\": encode_quopri, \\"base64\\": encode_base64, \\"7or8bit\\": encode_7or8bit, \\"noop\\": encode_noop } if encoding_scheme not in encoding_functions: raise ValueError(\\"Invalid encoding scheme provided. Choose from \'quoted-printable\', \'base64\', \'7or8bit\', or \'noop\'.\\") if msg.is_multipart(): raise TypeError(\\"Encoding functions cannot be applied to multipart messages. Apply them to individual subparts instead.\\") # Apply the encoding function encoding_functions[encoding_scheme](msg) return msg ``` # Input - An `email.message.Message` object `msg`. - A string `encoding_scheme` which can be one of `\\"quoted-printable\\"`, `\\"base64\\"`, `\\"7or8bit\\"`, `\\"noop\\"`. # Output - The encoded `email.message.Message` object with the payload encoded using the specified encoding scheme. - Appropriate exceptions if the `msg` is a multipart message or if an invalid `encoding_scheme` is provided. # Constraints - Do not modify the original `Message` object if it is invalid (e.g., multipart message or invalid encoding scheme). # Example ```python from email.message import EmailMessage # Create a simple email message msg = EmailMessage() msg.set_payload(\\"This is a test email with some binary content: 012\\") # Encode with base64 encoded_msg = encode_email_message(msg, \\"base64\\") print(encoded_msg.get_payload()) # Should print the base64 encoded payload print(encoded_msg[\\"Content-Transfer-Encoding\\"]) # Should output \\"base64\\" ``` # Notes - You need to handle the case where the `Message` object might be a multipart message and the encoding functions cannot be applied. - Ensure proper validation of the encoding scheme provided to avoid runtime errors.","solution":"import email from email.encoders import encode_quopri, encode_base64, encode_7or8bit, encode_noop from email.message import Message def encode_email_message(msg: Message, encoding_scheme: str) -> Message: Encodes the payload of an email Message object using the specified encoding scheme. Parameters: msg (Message): The email Message object to encode. encoding_scheme (str): The encoding scheme to use. Options are \\"quoted-printable\\", \\"base64\\", \\"7or8bit\\", \\"noop\\". Returns: Message: The encoded email Message object. Raises: ValueError: If the encoding_scheme is not valid. TypeError: If the msg is a multipart message. encoding_functions = { \\"quoted-printable\\": encode_quopri, \\"base64\\": encode_base64, \\"7or8bit\\": encode_7or8bit, \\"noop\\": encode_noop } if encoding_scheme not in encoding_functions: raise ValueError(\\"Invalid encoding scheme provided. Choose from \'quoted-printable\', \'base64\', \'7or8bit\', or \'noop\'.\\") if msg.is_multipart(): raise TypeError(\\"Encoding functions cannot be applied to multipart messages. Apply them to individual subparts instead.\\") # Apply the encoding function encoding_functions[encoding_scheme](msg) return msg"},{"question":"# Question: Implement a Nested Tensor Utility Function Objective Create a utility function that processes batches of variable-length sequential data represented as nested tensors. Your task involves constructing a nested tensor, performing an operation on each constituent, and converting the result to a padded tensor format. Function Signature ```python def process_nested_tensors(tensor_list, operation, padding_value): Constructs a nested tensor from a list of tensors, applies the specified operation to each constituent tensor, and converts the resulting nested tensor to a padded tensor with the specified padding value. Args: tensor_list (list of torch.Tensor): List of tensors to be converted into a nested tensor. operation (Callable[[torch.Tensor], torch.Tensor]): The operation to perform on each constituent tensor. padding_value (float): The value to use for padding the resulting tensor. Returns: torch.Tensor: A padded tensor after applying the operation to each constituent tensor. pass ``` Requirements 1. Construct a nested tensor from the given list of tensors using the `torch.jagged` layout. 2. Apply the provided operation to each constituent tensor within the nested tensor. 3. Convert the resultant nested tensor to a padded tensor using the specified padding value. Constraints - Each tensor in `tensor_list` will have an identical number of dimensions. - The operation to be applied will be a valid PyTorch operation that supports `torch.Tensor` inputs. - You should ensure the resultant padded tensor respects the original sequence lengths. Example Usage ```python import torch # Example operation: Normalize each tensor by subtracting the mean and dividing by the standard deviation def normalize(tensor): return (tensor - tensor.mean()) / tensor.std() # List of tensors of varying sequence lengths tensor_list = [torch.randn(3, 4), torch.randn(5, 4), torch.randn(2, 4)] # Process nested tensors result = process_nested_tensors(tensor_list, normalize, padding_value=0.0) print(result) # The output should be a padded tensor where each tensor from the tensor_list has been normalized # and any non-data regions are filled with the padding_value (0.0). ``` Notes - Consider edge cases, such as an empty list or tensors with only one element. - Ensure that the resulting padded tensor has appropriate padding such that the original sequence lengths are preserved.","solution":"import torch import numpy as np def process_nested_tensors(tensor_list, operation, padding_value): Constructs a nested tensor from a list of tensors, applies the specified operation to each constituent tensor, and converts the resulting nested tensor to a padded tensor with the specified padding value. Args: tensor_list (list of torch.Tensor): List of tensors to be converted into a nested tensor. operation (Callable[[torch.Tensor], torch.Tensor]): The operation to perform on each constituent tensor. padding_value (float): The value to use for padding the resulting tensor. Returns: torch.Tensor: A padded tensor after applying the operation to each constituent tensor. # Check if the tensor list is empty if not tensor_list: return torch.tensor([]) # Apply the operation to each tensor in the list processed_list = [operation(tensor) for tensor in tensor_list] # Calculate the maximum length along each dimension for padding max_shape = np.max([tensor.shape for tensor in processed_list], axis=0).tolist() # Create the padded tensor with the specified padding value padded_tensor = torch.full((len(processed_list), *max_shape), padding_value) for i, tensor in enumerate(processed_list): # Insert each processed tensor into the corresponding slice in the padded tensor slices = (i,) + tuple(slice(0, dim) for dim in tensor.shape) padded_tensor[slices] = tensor return padded_tensor"},{"question":"Implement a Python program that demonstrates both client-server architecture using asynchronous I/O with the asyncio module, and secure communication using the ssl module. # Requirements: - **Server Implementation**: - The server should be able to handle multiple client connections asynchronously using the asyncio module. - The server should use SSL/TLS to secure communications using the ssl module. - The server should echo back any message received from the clients. - **Client Implementation**: - The client should establish a connection to the server using SSL/TLS. - The client should be able to send a message to the server and receive the echoed message. # Constraints: - Use at least Python 3.6. - Use asyncio and ssl modules. - The connection must be secure (use SSL/TLS for data communication). # Input/Output Formats: - **Server**: - The server starts and listens for incoming client connections. - For each connected client, it should echo back received messages. - The server should print each received message and the echoed response. - **Client**: - The client takes as input the server\'s IP address, port number, and a message string. - It sends the message to the server and prints the echoed response from the server. # Example: 1. **Starting the Server**: ```bash python secure_echo_server.py ``` 2. **Running the Client**: ```bash python secure_echo_client.py 127.0.0.1 8888 \\"Hello, Server!\\" ``` Output in Client: ```plaintext Sent: Hello, Server! Received: Hello, Server! ```","solution":"# secure_echo_server.py import asyncio import ssl async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connection from {addr}\\") while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() print(f\\"Sent {message} to {addr}\\") print(f\\"Closing connection to {addr}\\") writer.close() await writer.wait_closed() async def main(): server_cert = \'server.pem\' server_key = \'server-key.pem\' ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) ssl_context.load_cert_chain(certfile=server_cert, keyfile=server_key) server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888, ssl=ssl_context) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main()) # secure_echo_client.py import asyncio import ssl import sys async def tcp_echo_client(message, loop): client_cert = \'client.pem\' client_key = \'client-key.pem\' ssl_context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH) ssl_context.load_cert_chain(certfile=client_cert, keyfile=client_key) ssl_context.load_verify_locations(\'server.pem\') reader, writer = await asyncio.open_connection( \'127.0.0.1\', 8888, ssl=ssl_context, loop=loop) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() if __name__ == \'__main__\': if len(sys.argv) != 4: print(f\\"Usage: {sys.argv[0]} <hostname> <port> <message>\\") sys.exit(1) hostname = sys.argv[1] port = int(sys.argv[2]) message = sys.argv[3] loop = asyncio.get_event_loop() loop.run_until_complete(tcp_echo_client(message, loop)) loop.close()"},{"question":"<|Analysis Begin|> The provided documentation snippet about the seaborn library focuses on the `sns.set_context()` function for setting the context parameters for plots. This function helps to adjust the aesthetics of the plotted figures, including the scaling of font elements and overriding specific parameters such as the linewidth of lines. The `sns.set_context()` function is demonstrated in three different ways: 1. Setting the context to \\"notebook\\". 2. Setting the context to \\"notebook\\" with an increased font scale. 3. Setting the context to \\"notebook\\" with custom line width parameters. Based on this documentation, I can design a question that tests the studentâ€™s understanding of setting plot contexts, adjusting font scaling, and customizing specific parameters in Seaborn plots. <|Analysis End|> <|Question Begin|> You are required to create an aesthetic line plot using the seaborn library to visualize some provided data. The plot should specifically exercise the concepts of setting context, adjusting font scaling, and customizing plot parameters. # Function Signature ```python def plot_aesthetic_line(x: list, y: list, context: str, font_scale: float, linewidth: float) -> None: pass ``` # Input - `x`: A list of numbers representing the x-axis data. - `y`: A list of numbers representing the y-axis data. - `context`: A string representing the context type that can be set. Possible values include `\\"paper\\"`, `\\"notebook\\"`, `\\"talk\\"`, and `\\"poster\\"`. - `font_scale`: A float value to scale the font elements relative to the current context. - `linewidth`: A float value to set the linewidth of the plot. # Output - The function should not return anything. It should display a line plot using seaborn. # Constraints - The lengths of `x` and `y` will always be equal and will have at least one element. - The values of `font_scale` and `linewidth` will always be positive numbers. # Example ```python x = [0, 1, 2, 3, 4] y = [10, 15, 13, 17, 14] context = \\"paper\\" font_scale = 1.5 linewidth = 2.5 plot_aesthetic_line(x, y, context, font_scale, linewidth) ``` This example should set the plot context to \\"paper\\", scale the font elements by 1.5, and set the linewidth of the line to 2.5, and then display the plot. # Requirements: 1. Utilize the seaborn library. 2. Apply the context using `sns.set_context()`. 3. Adjust the font size and the line width as specified. Use the documentation snippet provided to help you implement the function correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_aesthetic_line(x: list, y: list, context: str, font_scale: float, linewidth: float) -> None: Creates an aesthetic line plot using seaborn based on the provided parameters. Parameters: x (list): A list of numbers representing the x-axis data y (list): A list of numbers representing the y-axis data context (str): A string representing the context type (e.g., \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\") font_scale (float): A float value to scale the font elements relative to the specified context linewidth (float): A float value to set the linewidth of the plot sns.set_context(context, font_scale=font_scale, rc={\'lines.linewidth\': linewidth}) plt.figure(figsize=(10, 6)) sns.lineplot(x=x, y=y) plt.show()"},{"question":"**Problem Statement:** **UUID Manipulator** You are asked to implement a function that generates various types of UUIDs using the `uuid` module and to provide an interface to extract specific details from these UUIDs. Specifically, you will create a single function `generate_and_analyze_uuids()` that performs the following tasks: 1. Generate a UUID using each of the following methods: - `uuid.uuid1()` - `uuid.uuid3(namespace, name)` with `namespace` as `uuid.NAMESPACE_DNS` and `name` as `\\"example.com\\"` - `uuid.uuid4()` - `uuid.uuid5(namespace, name)` with `namespace` as `uuid.NAMESPACE_DNS` and `name` as `\\"example.com\\"` 2. Print each generated UUID along with its type. 3. For each generated UUID, output detailed information including: - The hexadecimal string of the UUID. - The UUID variant. - Whether the UUID is safe (if applicable), and its safe status (`safe`, `unsafe`, `unknown`). - Individual components of the UUID (`time_low`, `time_mid`, `time_hi_version`, `clock_seq_hi_variant`, `clock_seq_low`, `node`) if the UUID has these fields. **Function Signature:** ```python def generate_and_analyze_uuids(): pass ``` **Example Output:** ``` UUID1: a8098c1a-f86e-11da-bd1a-00112444be1e Hex: a8098c1a-f86e-11da-bd1a-00112444be1e Variant: RFC_4122 Safe: unknown Components: time_low=12345678, time_mid=1234, time_hi_version=5678, clock_seq_hi_variant=12, clock_seq_low=34, node=00112444be1e UUID3: 6fa459ea-ee8a-3ca4-894e-db77e160355e Hex: 6fa459ea-ee8a-3ca4-894e-db77e160355e Variant: RFC_4122 Components: (N/A for UUID3) UUID4: 16fd2706-8baf-433b-82eb-8c7fada847da Hex: 16fd2706-8baf-433b-82eb-8c7fada847da Variant: RFC_4122 Components: (N/A for UUID4) UUID5: 886313e1-3b8a-5372-9b90-0c9aee199e5d Hex: 886313e1-3b8a-5372-9b90-0c9aee199e5d Variant: RFC_4122 Components: (N/A for UUID5) ``` **Instruction:** 1. Import the `uuid` module. 2. Generate the UUIDs as specified. 3. Print the UUIDs, their types, and the required detailed information. 4. Handle cases where specific attributes are not applicable for certain types of UUIDs. **Notes:** - Some attributes like the individual components (e.g., `time_low`, `time_mid`, etc.) may not be present for all UUID types. - The `is_safe` attribute is only applicable to UUIDs generated by `uuid1()`. - Aim to demonstrate clear usage of the `uuid` module and its capabilities.","solution":"import uuid def generate_and_analyze_uuids(): uuids = { \\"UUID1\\": uuid.uuid1(), \\"UUID3\\": uuid.uuid3(uuid.NAMESPACE_DNS, \\"example.com\\"), \\"UUID4\\": uuid.uuid4(), \\"UUID5\\": uuid.uuid5(uuid.NAMESPACE_DNS, \\"example.com\\") } for uuid_type, u in uuids.items(): print(f\\"{uuid_type}: {u}\\") print(f\\" Hex: {u.hex}\\") print(f\\" Variant: {u.variant}\\") if hasattr(u, \'is_safe\'): print(f\\" Safe: {u.is_safe.name}\\") else: print(f\\" Safe: N/A\\") if uuid_type == \\"UUID1\\": print(f\\" Components: time_low={u.time_low}, time_mid={u.time_mid}, time_hi_version={u.time_hi_version}, clock_seq_hi_variant={u.clock_seq_hi_variant}, clock_seq_low={u.clock_seq_low}, node={u.node}\\") else: print(f\\" Components: (N/A for {uuid_type})\\")"},{"question":"# Question: Implement and Compare Different Naive Bayes Classifiers **Objective**: To assess your understanding of implementing and evaluating different variants of Naive Bayes classifiers using scikit-learn. Problem Statement You are provided with a dataset that can be used for classification. You are required to: 1. Load the dataset and split it into training and test sets. 2. Implement and train the following Naive Bayes classifiers using the training set: - Gaussian Naive Bayes - Multinomial Naive Bayes - Complement Naive Bayes - Bernoulli Naive Bayes 3. Evaluate each classifier using the test set and report the accuracy. 4. Identify the best performing classifier and provide a brief explanation of why it might be performing better compared to the others on this dataset. Dataset Use the Iris dataset from scikit-learn for this problem. Requirements 1. **Input and Output Formats**: - The functions should have no inputs since the data loading is embedded within the function. - The output should include the accuracy of each classifier and the name of the best performing classifier along with a brief explanation. 2. **Implementation Constraints**: - You should use scikit-learn\'s `train_test_split` for splitting the data. - The `random_state` parameter for `train_test_split` should be set to 0 for consistency. - For `MultinomialNB`, set the smoothing parameter `alpha` to 1. For `ComplementNB` and `BernoulliNB`, use the default parameters. Function Signatures ```python def load_and_split_data(): Load the Iris dataset and split it into training and test sets. Returns: X_train (numpy.ndarray): X_test (numpy.ndarray): y_train (numpy.ndarray): y_test (numpy.ndarray): pass def evaluate_naive_bayes_classifiers(X_train, X_test, y_train, y_test): Train and evaluate GaussianNB, MultinomialNB, ComplementNB, and BernoulliNB classifiers on the provided training and test data, and return their accuracies. Args: X_train (numpy.ndarray): X_test (numpy.ndarray): y_train (numpy.ndarray): y_test (numpy.ndarray): Returns: accuracies (dict): Dictionary containing the accuracy of each classifier. best_classifier (str): The name of the best performing classifier. explanation (str): Brief explanation of the best classifier\'s performance. pass ``` # Example Output ```python X_train, X_test, y_train, y_test = load_and_split_data() accuracies, best_classifier, explanation = evaluate_naive_bayes_classifiers(X_train, X_test, y_train, y_test) print(\\"Accuracies:\\", accuracies) print(\\"Best Classifier:\\", best_classifier) print(\\"Explanation:\\", explanation) ``` Note Ensure that your code is well-commented and follows best practices for readability and efficiency.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.preprocessing import Binarizer from sklearn.metrics import accuracy_score def load_and_split_data(): Load the Iris dataset and split it into training and test sets. Returns: X_train (numpy.ndarray): Training features. X_test (numpy.ndarray): Test features. y_train (numpy.ndarray): Training labels. y_test (numpy.ndarray): Test labels. iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) return X_train, X_test, y_train, y_test def evaluate_naive_bayes_classifiers(X_train, X_test, y_train, y_test): Train and evaluate GaussianNB, MultinomialNB, ComplementNB, and BernoulliNB classifiers on the provided training and test data, and return their accuracies. Args: X_train (numpy.ndarray): Training features. X_test (numpy.ndarray): Test features. y_train (numpy.ndarray): Training labels. y_test (numpy.ndarray): Test labels. Returns: accuracies (dict): Dictionary containing the accuracy of each classifier. best_classifier (str): The name of the best performing classifier. explanation (str): Brief explanation of the best classifier\'s performance. accuracies = {} # Gaussian Naive Bayes gnb = GaussianNB() gnb.fit(X_train, y_train) gnb_pred = gnb.predict(X_test) accuracies[\'GaussianNB\'] = accuracy_score(y_test, gnb_pred) # Multinomial Naive Bayes mnb = MultinomialNB(alpha=1.0) mnb.fit(X_train, y_train) mnb_pred = mnb.predict(X_test) accuracies[\'MultinomialNB\'] = accuracy_score(y_test, mnb_pred) # Complement Naive Bayes cnb = ComplementNB() cnb.fit(X_train, y_train) cnb_pred = cnb.predict(X_test) accuracies[\'ComplementNB\'] = accuracy_score(y_test, cnb_pred) # Bernoulli Naive Bayes binarizer = Binarizer() X_train_bin = binarizer.fit_transform(X_train) X_test_bin = binarizer.transform(X_test) bnb = BernoulliNB() bnb.fit(X_train_bin, y_train) bnb_pred = bnb.predict(X_test_bin) accuracies[\'BernoulliNB\'] = accuracy_score(y_test, bnb_pred) best_classifier = max(accuracies, key=accuracies.get) explanation = f\\"The {best_classifier} classifier might be performing better due to its suitability for the distribution and type of the Iris dataset.\\" return accuracies, best_classifier, explanation # Example usage: X_train, X_test, y_train, y_test = load_and_split_data() accuracies, best_classifier, explanation = evaluate_naive_bayes_classifiers(X_train, X_test, y_train, y_test) print(\\"Accuracies:\\", accuracies) print(\\"Best Classifier:\\", best_classifier) print(\\"Explanation:\\", explanation)"},{"question":"<|Analysis Begin|> The provided documentation covers a significant part of the `python310.xml.dom` package, which includes the Document Object Model (DOM) interface for XML document manipulation. The documentation thoroughly describes the different classes and methods available within the package, such as `DOMImplementation`, `Node`, `Document`, `Element`, and various exceptions defined for working with the DOM. `python310.xml.dom` allows users to parse XML into a DOM tree, and then manipulate or traverse the DOM tree using the defined interfaces and methods. The documentation provides a detailed look at the attributes and methods associated with DOM objects, including node manipulation methods (`appendChild`, `removeChild`, `normalize`, etc.), traversal methods (`getElementsByTagName`, `getAttribute`, etc.), and the various node types (comment nodes, text nodes, element nodes, etc.). The challenge for a coding question should focus on creating and manipulating a DOM tree from a given XML structure, ensuring that students demonstrate an understanding of how to use the different methods and attributes to interact with the DOM tree. <|Analysis End|> <|Question Begin|> # XML Document Manipulation with `xml.dom` You are provided with a simple XML document structure representing a book library. The XML document looks like this: ```xml <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book id=\\"2\\"> <title>Data Science Handbook</title> <author>Jane Smith</author> <year>2019</year> </book> </library> ``` We want you to use Python\'s `xml.dom` module to perform the following tasks: 1. Parse this XML string and convert it into a DOM document. 2. Add a new book to the library with the following details: - id: 3 - title: \\"Machine Learning Basics\\" - author: \\"Emily Johnson\\" - year: 2021 3. Retrieve and print the titles of all books in the library. 4. Update the title of the book with id 2 to \\"Advanced Data Science\\". # Implementation 1. **Function Name**: `manipulate_library` 2. **Input**: None (the initial XML string is hardcoded within the function) 3. **Output**: A list of all book titles after performing the updates ```python from xml.dom.minidom import getDOMImplementation def manipulate_library(): # Initial XML string xml_string = <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book id=\\"2\\"> <title>Data Science Handbook</title> <author>Jane Smith</author> <year>2019</year> </book> </library> impl = getDOMImplementation() document = impl.createDocument(None, \\"library\\", None) # Write your code here to perform the required tasks # Return the list of book titles return [] ``` # Constraints - Only use methods provided by `xml.dom` to manipulate the XML document. - Do not use any external XML libraries (e.g., `lxml`, `xml.etree`). - Ensure the output accurately reflects the updated state of the XML document. # Example Output After calling `manipulate_library()`, the function should output the list: ```python [\'Python Programming\', \'Advanced Data Science\', \'Machine Learning Basics\'] ``` Demonstrate your understanding of how to work with the `xml.dom` module and provide a solution fulfilling the requirements above.","solution":"from xml.dom.minidom import parseString, Document def manipulate_library(): xml_string = <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book id=\\"2\\"> <title>Data Science Handbook</title> <author>Jane Smith</author> <year>2019</year> </book> </library> # Parse the XML string dom = parseString(xml_string) library = dom.documentElement # Create a new book element new_book = dom.createElement(\'book\') new_book.setAttribute(\'id\', \'3\') title = dom.createElement(\'title\') title.appendChild(dom.createTextNode(\'Machine Learning Basics\')) new_book.appendChild(title) author = dom.createElement(\'author\') author.appendChild(dom.createTextNode(\'Emily Johnson\')) new_book.appendChild(author) year = dom.createElement(\'year\') year.appendChild(dom.createTextNode(\'2021\')) new_book.appendChild(year) # Append the new book to the library library.appendChild(new_book) # Update the title of the book with id 2 books = library.getElementsByTagName(\'book\') for book in books: if book.getAttribute(\'id\') == \'2\': title = book.getElementsByTagName(\'title\')[0] title.firstChild.nodeValue = \'Advanced Data Science\' # Retrieve and print the titles of all books in the library book_titles = [book.getElementsByTagName(\'title\')[0].firstChild.nodeValue for book in books] return book_titles"},{"question":"Objective Implement a custom sequence class in Python named `CustomSequence` that mimics the behavior of some of the sequence operations documented in the provided Python C API. Your implementation should demonstrate understanding of the fundamental and advanced concepts of sequence manipulation. Requirements 1. **Class Definition**: Define a class `CustomSequence` which initializes with an iterable. 2. **Methods to Implement**: - `__len__`: Return the number of items in the sequence. - `__getitem__`: Fetch an item by its index. - `__setitem__`: Set an item at a specific index. - `__delitem__`: Delete an item at a specific index. - `__contains__`: Check if a value exists in the sequence. - `__iter__`: Return an iterator for the sequence. - `__repr__`: Return a string representation of the sequence. - `__add__`: Concatenate two sequences. - `__mul__`: Repeat the sequence a specified number of times. - `count`: Return the number of occurrences of a value. - `index`: Return the index of the first occurrence of a value. - `append`: Append an item to the sequence. - `extend`: Extend the sequence with another iterable. - `remove`: Remove the first occurrence of a value. - `pop`: Remove and return the item at a specific index. Constraints - The class should only use basic data structures (i.e., lists) for internal data storage. - Custom error handling should be implemented for index and value errors where applicable. - Optimize for performance where possible, especially in frequently called methods. Example Usage ```python seq = CustomSequence([1, 2, 3, 4]) print(len(seq)) # Output: 4 print(seq[2]) # Output: 3 seq[2] = 10 print(seq) # Output: CustomSequence([1, 2, 10, 4]) del seq[1] print(seq) # Output: CustomSequence([1, 10, 4]) print(10 in seq) # Output: True seq.append(5) print(seq) # Output: CustomSequence([1, 10, 4, 5]) seq.extend([6, 7]) print(seq) # Output: CustomSequence([1, 10, 4, 5, 6, 7]) print(seq.index(5)) # Output: 3 seq.remove(4) print(seq) # Output: CustomSequence([1, 10, 5, 6, 7]) print(seq.pop(2)) # Output: 5 print(seq) # Output: CustomSequence([1, 10, 6, 7]) print(seq * 2) # Output: CustomSequence([1, 10, 6, 7, 1, 10, 6, 7]) print(seq + CustomSequence([8, 9])) # Output: CustomSequence([1, 10, 6, 7, 8, 9]) print(seq.count(10)) # Output: 1 ``` **Note**: Your implementation should behave as closely as possible to built-in list operations in Python. Submission Submit your class definition along with a few test cases to demonstrate that it works as expected.","solution":"class CustomSequence: def __init__(self, iterable): self.data = list(iterable) def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __contains__(self, value): return value in self.data def __iter__(self): return iter(self.data) def __repr__(self): return f\\"CustomSequence({self.data})\\" def __add__(self, other): return CustomSequence(self.data + other.data) def __mul__(self, times): return CustomSequence(self.data * times) def count(self, value): return self.data.count(value) def index(self, value): return self.data.index(value) def append(self, value): self.data.append(value) def extend(self, iterable): self.data.extend(iterable) def remove(self, value): self.data.remove(value) def pop(self, index=None): if index is None: return self.data.pop() return self.data.pop(index)"},{"question":"Objective: Using the `reprlib` module and its details provided, implement a custom `Repr` class that adds support for the representation of custom `Person` objects while adhering to size limits. Task: 1. Define a `Person` class with the following attributes: - `name` (string) - `age` (integer) - `address` (string) 2. Subclass the `reprlib.Repr` class to create a custom class named `PersonRepr` which will include the following: - A method that defines how `Person` objects should be represented, ensuring that the `name`, `age`, and `address` attributes are included in the limited-size representation. - Set size limits such that: - The name should be truncated to a maximum of 5 characters. - The address should be truncated to a maximum of 10 characters. 3. Create an instance of `PersonRepr` and use it to produce the representation of a `Person` object. Constraints: - The solution should not exceed the set size limits for any attribute in the `Person` representation. - Do not modify the original `reprlib` library but extend its functionality via subclassing. Example: ```python import reprlib class Person: def __init__(self, name, age, address): self.name = name self.age = age self.address = address class PersonRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxname = 5 self.maxaddress = 10 def repr_Person(self, obj, level): name = obj.name[:self.maxname] age = obj.age address = obj.address[:self.maxaddress] return f\\"Person(name=\'{name}\', age={age}, address=\'{address}\')\\" # Using custom PersonRepr to limit the size of attributes person = Person(name=\\"Alexander\\", age=30, address=\\"1234 Elm Street, Springfield\\") custom_repr = PersonRepr() print(custom_repr.repr(person)) # Expected Output format: # Person(name=\'Alexa\', age=30, address=\'1234 Elm \') ``` Notes: - Pay attention to how the `repr_Person` method is defined and customized. - This will test your ability to work with classes, inheritance, and the `reprlib` module, as well as ensuring constraints are adhered to.","solution":"import reprlib class Person: def __init__(self, name, age, address): self.name = name self.age = age self.address = address class PersonRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxname = 5 self.maxaddress = 10 def repr_Person(self, obj, level): name = obj.name[:self.maxname] age = obj.age address = obj.address[:self.maxaddress] return f\\"Person(name=\'{name}\', age={age}, address=\'{address}\')\\" # Example usage if __name__ == \\"__main__\\": person = Person(name=\\"Alexander\\", age=30, address=\\"1234 Elm Street, Springfield\\") custom_repr = PersonRepr() print(custom_repr.repr(person)) # Expected Output: Person(name=\'Alexa\', age=30, address=\'1234 Elm \')"},{"question":"# Question: Create a Backup Utility **Objective:** You are required to implement a Python script called `backup_utility.py` that performs directory backups. This script will take in a source directory and a destination directory as command-line arguments, compress the contents of the source directory, and save it to the destination directory with a timestamped filename. **Function Requirements:** 1. **Function Name:** `backup_directory` 2. **Arguments:** - `source_dir` (str): The path to the directory that needs to be backed up. - `destination_dir` (str): The path to the directory where the backup file should be saved. 3. **Returns:** None 4. **Behavior:** - The function should check if both source and destination directories exist. If not, it should raise appropriate errors. - It should compress the contents of the source directory into a `.zip` file. - The resulting `.zip` file should be named as `backup_YYYYMMDD_HHMMSS.zip` where `YYYYMMDD_HHMMSS` is the current timestamp. **Script Requirements:** 1. **Script Name:** `backup_utility.py` 2. **Command-Line Arguments:** - `--source` or `-s` (required): Path to the source directory. - `--destination` or `-d` (required): Path to the destination directory. 3. **Behavior:** - The script should parse the command-line arguments to retrieve the source and destination directories. - It should call the `backup_directory` function with the provided arguments. **Example Usage:** ```bash python backup_utility.py --source /path/to/source --destination /path/to/destination ``` **Constraints:** - Handle all exceptions gracefully and provide meaningful error messages (e.g., if a directory does not exist). - Ensure the implementation uses the `os`, `shutil`, `datetime`, and `argparse` modules. **Performance Requirements:** - The script should be efficient in terms of time and space complexity, given typical filesystem sizes (e.g., up to tens of thousands of files and subdirectories). **Hints:** - Use the `os.path` module to handle filesystem paths. - Use the `shutil` module to facilitate directory and file operations. - Use the `datetime` module to generate the timestamp. - Use the `argparse` module to handle command-line arguments. **Example Solution Skeleton:** ```python import os import shutil import argparse from datetime import datetime def backup_directory(source_dir, destination_dir): # Check if source and destination directories exist if not os.path.isdir(source_dir): raise FileNotFoundError(f\\"The source directory {source_dir} does not exist.\\") if not os.path.isdir(destination_dir): raise FileNotFoundError(f\\"The destination directory {destination_dir} does not exist.\\") # Create a timestamped backup filename timestamp = datetime.now().strftime(\\"%Y%m%d_%H%M%S\\") backup_filename = f\\"backup_{timestamp}.zip\\" backup_path = os.path.join(destination_dir, backup_filename) # Compress the source directory shutil.make_archive(backup_path.replace(\'.zip\', \'\'), \'zip\', source_dir) if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\'Backup utility to compress directories with a timestamp.\') parser.add_argument(\'--source\', \'-s\', required=True, help=\'Path to the source directory.\') parser.add_argument(\'--destination\', \'-d\', required=True, help=\'Path to the destination directory.\') args = parser.parse_args() try: backup_directory(args.source, args.destination) print(f\\"Backup of {args.source} completed successfully and saved to {args.destination}\\") except Exception as e: print(f\\"An error occurred: {e}\\") ```","solution":"import os import shutil from datetime import datetime import argparse def backup_directory(source_dir, destination_dir): Compresses the contents of the source directory and saves it in the destination directory with a timestamped filename. Args: source_dir (str): The path to the directory that needs to be backed up. destination_dir (str): The path to the directory where the backup file should be saved. Raises: FileNotFoundError: If the source or destination directory does not exist. # Check if source and destination directories exist if not os.path.isdir(source_dir): raise FileNotFoundError(f\\"The source directory {source_dir} does not exist.\\") if not os.path.isdir(destination_dir): raise FileNotFoundError(f\\"The destination directory {destination_dir} does not exist.\\") # Create a timestamped backup filename timestamp = datetime.now().strftime(\\"%Y%m%d_%H%M%S\\") backup_filename = f\\"backup_{timestamp}.zip\\" backup_path = os.path.join(destination_dir, backup_filename) # Compress the source directory shutil.make_archive(backup_path.replace(\'.zip\', \'\'), \'zip\', source_dir) if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\'Backup utility to compress directories with a timestamp.\') parser.add_argument(\'--source\', \'-s\', required=True, help=\'Path to the source directory.\') parser.add_argument(\'--destination\', \'-d\', required=True, help=\'Path to the destination directory.\') args = parser.parse_args() try: backup_directory(args.source, args.destination) print(f\\"Backup of {args.source} completed successfully and saved to {args.destination}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Out-of-Core Learning with Incremental Estimators **Objective:** Implement an out-of-core learning pipeline using scikit-learn to classify text documents that cannot fit into memory all at once. You will need to utilize a streaming generator for loading data, perform feature extraction using a hash vectorizer, and apply an incremental learning estimator from scikit-learn. **Input:** 1. `streaming_generator`: A generator function that yields batches of text documents and their corresponding labels as tuples `(batch_data, batch_labels)`. Each call to the generator returns the next batch of data. 2. `classes`: A list of all possible classes which can be present in the data. **Output:** - A trained incremental classifier capable of predicting the labels for new text documents. **Implementation Requirements:** 1. Use `HashingVectorizer` from `sklearn.feature_extraction.text` for feature extraction. 2. Use an incremental classifier supporting the `partial_fit` method, such as `sklearn.linear_model.SGDClassifier`. 3. Incrementally train the classifier using data from the `streaming_generator`. **Performance Constraints:** - Ensure that at no point does the system load all the data into memory. It should only keep a minimal amount of data required for each batch. **Function Signature:** ```python def out_of_core_text_classification(streaming_generator, classes): # Your code here # Example usage: # Define a streaming generator for demonstration purpose def dummy_streaming_generator(): # Suppose this yields batches of text data and labels data_batches = [ ([\\"text example 1\\", \\"text example 2\\"], [0, 1]), ([\\"text example 3\\", \\"text example 4\\"], [1, 0]), # More batches... ] for data_batch, labels_batch in data_batches: yield data_batch, labels_batch # List of all possible classes all_classes = [0, 1] # Train the model with the streaming generator out_of_core_text_classification(dummy_streaming_generator(), all_classes) ``` **Notes:** - Remember to initialize your vectorizer and classifier outside the generator loop. - Use `partial_fit` for training the classifier on each data batch. - You can test the classifier after training it with your test data to ensure it works correctly.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def out_of_core_text_classification(streaming_generator, classes): # Initialize the HashingVectorizer and the SGDClassifier vectorizer = HashingVectorizer(n_features=2**20, alternate_sign=False) classifier = SGDClassifier() is_first_batch = True # Iterate over the batches produced by the streaming generator for data_batch, labels_batch in streaming_generator: # Convert the text data to feature vectors X_batch = vectorizer.transform(data_batch) # Perform partial fit on the current batch if is_first_batch: # First call to partial_fit requires the list of all classes classifier.partial_fit(X_batch, labels_batch, classes=classes) is_first_batch = False else: classifier.partial_fit(X_batch, labels_batch) return classifier # Example usage: # Define a streaming generator for demonstration purpose def dummy_streaming_generator(): # Suppose this yields batches of text data and labels data_batches = [ ([\\"text example 1\\", \\"text example 2\\"], [0, 1]), ([\\"text example 3\\", \\"text example 4\\"], [1, 0]), # More batches... ] for data_batch, labels_batch in data_batches: yield data_batch, labels_batch # List of all possible classes all_classes = [0, 1] # Train the model with the streaming generator trained_classifier = out_of_core_text_classification(dummy_streaming_generator(), all_classes)"},{"question":"# Advanced Python 3.10 Assessment **Objective:** Demonstrate your understanding of exceptions in Python\'s `asyncio` module by implementing a function that reads chunks of data from an asynchronous stream and handles various exceptions that may occur. **Task:** Implement an asynchronous function `read_stream_chunks` that reads a specified number of bytes from an asynchronous stream and handles different exceptions appropriately. The function should: 1. **Read** chunks of data from the provided stream asynchronously until the specified total number of bytes has been read or an exception occurs. 2. **Handle** the following exceptions: - `asyncio.TimeoutError`: Print a message indicating the operation timed out. - `asyncio.CancelledError`: Print a message indicating the operation was cancelled. The exception should be re-raised after handling. - `asyncio.InvalidStateError`: Print a message indicating there was an invalid internal state. - `asyncio.SendfileNotAvailableError`: Print a message indicating the sendfile syscall is not available. - `asyncio.IncompleteReadError`: Print a message indicating an incomplete read with details about `expected` and `partial` bytes. - `asyncio.LimitOverrunError`: Print a message indicating the buffer size limit was reached with details about consumed bytes. **Function Signature:** ```python import asyncio async def read_stream_chunks(stream, num_bytes): Reads chunks of data from an asynchronous stream until the specified number of bytes is read or an exception occurs. Args: stream (StreamReader): An asyncio StreamReader object to read from. num_bytes (int): The total number of bytes to read. Returns: bytes: The read data. pass ``` **Input:** - `stream`: An instance of `asyncio.StreamReader` to read the data from. - `num_bytes`: An integer specifying the number of bytes to read. **Output:** - The function should return the read bytes if successful. **Example Usage:** ```python import asyncio async def main(): stream = asyncio.StreamReader() # Simulate feeding data into the stream... # Assume data is being fed to the stream in the background result = await read_stream_chunks(stream, 100) print(result) # Run the main function to test the read_stream_chunks asyncio.run(main()) ``` **Constraints:** - You may assume that the `stream` parameter is always a valid `asyncio.StreamReader` instance. - The function should handle exceptions gracefully as per the details provided. **Performance Requirements:** - Since this is an asynchronous function, ensure that your implementation does not block the event loop. - Handle exceptions in a non-blocking manner to maintain the responsiveness of the application.","solution":"import asyncio async def read_stream_chunks(stream, num_bytes): Reads chunks of data from an asynchronous stream until the specified number of bytes is read or an exception occurs. Args: stream (StreamReader): An asyncio StreamReader object to read from. num_bytes (int): The total number of bytes to read. Returns: bytes: The read data. try: data = await stream.read(num_bytes) return data except asyncio.TimeoutError: print(\\"Operation timed out\\") except asyncio.CancelledError: print(\\"Operation was cancelled\\") raise except asyncio.InvalidStateError: print(\\"Invalid internal state\\") except asyncio.SendfileNotAvailableError: print(\\"Sendfile syscall is not available\\") except asyncio.IncompleteReadError as e: print(f\\"Incomplete read: expected {e.expected} bytes, got {e.partial} bytes\\") except asyncio.LimitOverrunError as e: print(f\\"Buffer size limit reached, consumed {e.consumed} bytes\\") return b\'\'"},{"question":"# Concurrent Futures Assessment Objective Implement a function using the `concurrent.futures` module to perform parallel processing on a set of tasks, demonstrating the ability to manage asynchronous operations and handle exceptions properly. Problem Statement You are given a list of URLs to fetch data from the web. Your task is to implement a function `fetch_parallel(urls: List[str], timeout: int) -> List[Optional[str]]` that fetches the content of each URL in parallel using a thread pool. The function should return a list where each element corresponds to the content of the respective URL or `None` if an exception occurred (e.g., a timeout or a failed request). Function Signature ```python from typing import List, Optional def fetch_parallel(urls: List[str], timeout: int) -> List[Optional[str]]: ``` Input - `urls`: A list of strings, where each string is a URL to fetch (e.g., `[\\"http://example.com\\", \\"http://example.org\\"]`). - `timeout`: An integer specifying the maximum number of seconds to wait for each request to complete before considering it a failure. Output - Returns a list of strings where each element is the fetched content of the corresponding URL in the input list. If a fetch operation times out or fails for a URL, return `None` for that URL. Constraints - The order of the output list must match the order of the input URLs. - You may assume that the list of URLs will not exceed 1000 in length. - The content of each URL will be a short string (less than 200 characters). - You are required to use `ThreadPoolExecutor` from the `concurrent.futures` module. Example ```python urls = [\\"http://example.com\\", \\"http://example.org\\"] timeout = 5 result = fetch_parallel(urls, timeout) print(result) # Output could be: [\\"<html>...</html>\\", None] ``` Notes - Use the `requests` library for making HTTP requests. You can install it using `pip install requests`. - Handle any exceptions that may occur during the request, and ensure robust error handling. - Ensure the execution makes optimal use of concurrency to perform the fetch operations efficiently. Bonus Optimize the function to handle the case where a large number of URLs need to be fetched by varying the size of the thread pool dynamically based on system resources.","solution":"import requests from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List, Optional def fetch_url(url: str, timeout: int) -> Optional[str]: Helper function to fetch the content of a single URL with a specified timeout. Returns the content of the URL or None if the request fails. try: response = requests.get(url, timeout=timeout) response.raise_for_status() # Raise HTTPError for bad responses return response.text except Exception as e: # Catch all exceptions (e.g., requests.Timeout, requests.RequestException) return None def fetch_parallel(urls: List[str], timeout: int) -> List[Optional[str]]: Fetches the content of a list of URLs in parallel using a thread pool. Args: - urls: List of URLs to fetch. - timeout: Timeout for each URL request. Returns: - List of contents corresponding to each URL or None if there was an exception. results = [None] * len(urls) with ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(fetch_url, url, timeout): idx for idx, url in enumerate(urls)} for future in as_completed(future_to_url): idx = future_to_url[future] try: results[idx] = future.result() except Exception as exc: results[idx] = None return results"},{"question":"You are given a neural network model that extensively uses `BatchNorm2d` layers. However, you need to replace all these `BatchNorm2d` layers with `GroupNorm` to make the model compatible with `vmap` from the `functorch` library. Write a function `replace_batchnorm_with_groupnorm` that takes in a PyTorch neural network model and replaces all instances of `BatchNorm2d` with `GroupNorm`. For simplicity, assume the number of groups in `GroupNorm` to be 32. Ensure that the new layers do not track running statistics (`track_running_stats=False`). **Input:** - `model` (torch.nn.Module): A PyTorch neural network model containing `BatchNorm2d` layers. **Output:** - `model` (torch.nn.Module): The input model after replacing all `BatchNorm2d` layers with `GroupNorm`. **Constraints:** 1. You cannot use `functorch`\'s `replace_all_batch_norm_modules_` function. 2. Ensure that each `BatchNorm2d` layer is replaced with a `GroupNorm` layer, preserving other properties like the number of features. **Example:** ```python import torch import torch.nn as nn class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.conv1 = nn.Conv2d(3, 64, 3) self.bn1 = nn.BatchNorm2d(64) self.conv2 = nn.Conv2d(64, 128, 3) self.bn2 = nn.BatchNorm2d(128) self.fc = nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) x = torch.flatten(x, 1) x = self.fc(x) return x # Instantiate the model model = SampleModel() # Function to implement def replace_batchnorm_with_groupnorm(model): for name, module in model.named_modules(): if isinstance(module, nn.BatchNorm2d): num_features = module.num_features group_norm_layer = nn.GroupNorm(num_groups=min(num_features // 32, 32), num_channels=num_features) setattr(model, name, group_norm_layer) return model # Replace batch norm layers with group norm in the model model = replace_batchnorm_with_groupnorm(model) ``` **Note:** You need to ensure that the `replace_batchnorm_with_groupnorm` function adapts to the input model\'s architecture dynamically.","solution":"import torch import torch.nn as nn def replace_batchnorm_with_groupnorm(model): Replaces all instances of nn.BatchNorm2d with nn.GroupNorm in the given model. Args: - model (torch.nn.Module): A PyTorch model. Returns: - model (torch.nn.Module): The updated model with nn.GroupNorm instead of nn.BatchNorm2d. for name, module in model.named_modules(): if isinstance(module, nn.BatchNorm2d): num_features = module.num_features # Creating the GroupNorm layer with 32 groups and num_channels equal to num_features group_norm_layer = nn.GroupNorm(num_groups=min(num_features // 32, 32), num_channels=num_features) setattr(model, name, group_norm_layer) return model"},{"question":"Implementing a Python Utility Using PyLong Functions In this task, you will implement a Python function that converts various integer representations to a Python integer and then performs a series of conversions using the provided PyLong functions. You are tasked to handle proper type checking and error handling using the tools described in the PyLong documentation. # Objective Implement the function `convert_and_process_integers` that takes a list of tuples, where each tuple contains a value of one of the following types: - `long` - `unsigned long` - `long long` - `unsigned long long` - `double` Each tuple also includes the corresponding conversion type (as string) and an \\"operation,\\" either `\\"double\\"` or `\\"half\\"`. Based on this \\"operation,\\" the function either doubles or halves the value after converting it. # Function Signature ```python def convert_and_process_integers(values: list) -> list: ``` # Input - `values`: A list of tuples. Each tuple contains: * The value to be converted (one of the types mentioned). * The conversion type (a string) which is one of: - `\\"long\\"` - `\\"unsigned_long\\"` - `\\"long_long\\"` - `\\"unsigned_long_long\\"` - `\\"double\\"` * An operation which can be `\\"double\\"` or `\\"half\\"`. # Output - A list of processed values (integers), where each original value has been converted to a Python integer, processed (doubled or halved), and returned as a new integer. # Constraints - Handle errors gracefully using appropriate error checks and exceptions management for overflow conditions. - Only return results that successfully convert and process; skip and ignore values that result in errors during conversion or processing, without halting the function. # Example: ```python input_values = [(100, \\"long\\", \\"double\\"), (2.5, \\"double\\", \\"half\\"), (257, \\"unsigned_long\\", \\"double\\"), (-1, \\"unsigned_long\\", \\"half\\")] # This should be skipped due to overflow error on negative unsigned long result = convert_and_process_integers(input_values) ``` Expected Result: ```python [200, 1, 514] ``` Feel free to make use of mock C functions or pseudocode to represent the interaction with `PyLong` APIs. # Hints: - Ensure to validate types and manage errors using the provided PyLong functions. - Use helper functions for cleaner code and reusability. - Remember to handle both valid and invalid cases as described.","solution":"def convert_and_process_integers(values: list) -> list: def to_python_int(value, conversion_type): try: if conversion_type == \\"long\\": return int(value) elif conversion_type == \\"unsigned_long\\": if value < 0: raise ValueError(\\"Cannot convert negative value to unsigned long\\") return int(value) elif conversion_type == \\"long_long\\": return int(value) elif conversion_type == \\"unsigned_long_long\\": if value < 0: raise ValueError(\\"Cannot convert negative value to unsigned long long\\") return int(value) elif conversion_type == \\"double\\": return int(float(value)) else: raise ValueError(\\"Unsupported conversion type\\") except (ValueError, OverflowError, TypeError) as e: return None def process_value(value, operation): if operation == \\"double\\": return value * 2 elif operation == \\"half\\": return value // 2 else: return None processed_values = [] for value, conversion_type, operation in values: python_int = to_python_int(value, conversion_type) if python_int is not None: result = process_value(python_int, operation) if result is not None: processed_values.append(result) return processed_values"},{"question":"# Task: Create a Randomized Playlist As a music enthusiast, you want to build a program that generates a unique, randomized playlist from a given list of songs and ensures that certain conditions are met regarding the order and selection of songs. Your goal is to implement a function that uses Python\'s `random` module to accomplish this. Function Signature ```python def generate_playlist(songs: list[str], num_songs: int, unique: bool) -> list[str]: Generate a randomized playlist. Args: songs (list[str]): A list of available songs. num_songs (int): The total number of songs to include in the playlist. unique (bool): If True, all songs in the playlist must be unique. Otherwise, repetitions are allowed. Returns: list[str]: A randomized playlist containing the specified number of songs. Raises: ValueError: If \'num_songs\' exceeds the number of unique songs available when \'unique\' is True. ValueError: If \'songs\' is empty. # Example usage songs = [\\"song1\\", \\"song2\\", \\"song3\\", \\"song4\\", \\"song5\\"] print(generate_playlist(songs, 7, False)) # Output: A list of 7 songs, which may include repetitions. print(generate_playlist(songs, 5, True)) # Output: A list of 5 unique songs. ``` Requirements: 1. If `unique` is set to `True`, the playlist should contain unique songs only. If `num_songs` exceeds the number of unique songs available, raise a `ValueError`. 2. If `unique` is `False`, the playlist can contain repeated songs. 3. If the input `songs` list is empty, raise a `ValueError`. 4. The function must utilize appropriate functions from the `random` module to shuffle and sample songs. Constraints: - Do not use external libraries other than Python\'s standard `random` module. - Ensure that the function is efficient and handles edge cases gracefully. - The list of songs may contain up to 1000 entries, and `num_songs` can be as large as 2000 when repetition is allowed. Performance Requirements: - The function should run efficiently even for the maximum input sizes specified. Example: ```python songs = [\\"song1\\", \\"song2\\", \\"song3\\", \\"song4\\", \\"song5\\"] playlist = generate_playlist(songs, 7, False) print(playlist) # Example output: [\\"song4\\", \\"song2\\", \\"song1\\", \\"song3\\", \\"song5\\", \\"song3\\", \\"song2\\"] playlist = generate_playlist(songs, 5, True) print(playlist) # Example output: [\\"song1\\", \\"song5\\", \\"song3\\", \\"song2\\", \\"song4\\"] ``` Ensure your solution passes various test cases, including edge cases such as an empty song list or requesting more unique songs than available.","solution":"import random def generate_playlist(songs: list[str], num_songs: int, unique: bool) -> list[str]: Generate a randomized playlist. Args: songs (list[str]): A list of available songs. num_songs (int): The total number of songs to include in the playlist. unique (bool): If True, all songs in the playlist must be unique. Otherwise, repetitions are allowed. Returns: list[str]: A randomized playlist containing the specified number of songs. Raises: ValueError: If \'num_songs\' exceeds the number of unique songs available when \'unique\' is True. ValueError: If \'songs\' is empty. if not songs: raise ValueError(\\"The song list is empty\\") if unique: if num_songs > len(songs): raise ValueError(\\"The number of requested unique songs exceeds the available unique songs\\") playlist = random.sample(songs, num_songs) else: playlist = [random.choice(songs) for _ in range(num_songs)] random.shuffle(playlist) return playlist"},{"question":"# **Coding Assessment Question: Implementing and Utilizing a C Extension Module** # **Objective:** Develop a Python C extension module named `calc` which implements basic arithmetic operations. Ensure correct handling of argument parsing, error handling, and reference counting as outlined in the provided documentation. # **Requirements:** 1. **Functions to Implement:** - `add(a, b)`: Returns the sum of `a` and `b`. - `subtract(a, b)`: Returns the difference when `b` is subtracted from `a`. - `multiply(a, b)`: Returns the product of `a` and `b`. - `divide(a, b)`: Returns the quotient of `a` divided by `b`. Raise a Python `ZeroDivisionError` if `b` is zero. 2. **Expected Input/Output:** - Inputs: Two numeric values for each function. - Outputs: The computed result as a numeric value. 3. **Constraints and Limitations:** - Assume the inputs will always be numbers (integers or floats). - Handle division by zero explicitly in the `divide` function by raising a `ZeroDivisionError`. 4. **Performance:** - Ensure efficient memory usage and handling of reference counts. Do not introduce memory leaks. # **Task:** 1. **Create a file `calcmodule.c`** with the following steps: - Include the necessary Python headers. - Define the arithmetic functions (`add`, `subtract`, `multiply`, `divide`). - Implement argument parsing and error handling. - Create the method table and method definitions. - Define the module and its initialization function. 2. **Implement Python bindings:** - Use Python\'s C API to register the `calc` module and its functions. 3. **Example Usage in Python:** ```python import calc result1 = calc.add(10, 5) # Expected output: 15 result2 = calc.subtract(10, 5) # Expected output: 5 result3 = calc.multiply(10, 5) # Expected output: 50 result4 = calc.divide(10, 5) # Expected output: 2.0 result5 = calc.divide(10, 0) # Should raise ZeroDivisionError ``` 4. **Testing:** - Write a Python script to test all functions of the `calc` module, ensuring they produce the expected outputs. # **Submission:** Submit the file `calcmodule.c` along with the Python script used for testing the module. # **Hints:** - Refer to the provided documentation for details on creating and managing C extensions in Python. - Pay attention to reference counting to avoid memory leaks. - Raise Python exceptions appropriately using the provided Python C API functions (`PyErr_SetString`, `PyExc_ZeroDivisionError`, etc.).","solution":"# This would be a pure Python implementation since I can\'t execute a C extension module directly here. def add(a, b): Returns the sum of a and b. return a + b def subtract(a, b): Returns the difference when b is subtracted from a. return a - b def multiply(a, b): Returns the product of a and b. return a * b def divide(a, b): Returns the quotient of a divided by b. Raises ZeroDivisionError if b is zero. if b == 0: raise ZeroDivisionError(\\"division by zero\\") return a / b"},{"question":"# Advanced Python Coding Assessment: Profiling and Optimization Objective: To evaluate the student\'s understanding of the `timeit` module, profiling Python code, and optimizing functions for better performance. Task Description: You are provided with a function that performs arithmetic operations and a sorting algorithm. You will use the `timeit` module to compare and optimize these operations. Your task is to: 1. **Profile the Functions**: - Measure the performance of the given functions using the `timeit` module. - Determine the most time-consuming part of each function. 2. **Optimize the Functions**: - Implement optimizations to improve the performance of the identified bottlenecks in the functions. 3. **Report the Results**: - Output the timing results before and after optimization. - Discuss the optimizations you applied and why they improve performance. Function Implementations: Consider the following Python functions: ```python def arithmetic_operations(numbers): result = 0 for number in numbers: result += number ** 2 result -= number ** 3 result += number ** 4 return result def bubble_sort(arr): n = len(arr) for i in range(n): for j in range(0, n-i-1): if arr[j] > arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] return arr ``` Input: - A list `numbers` of integers of size `N` for the `arithmetic_operations` function. - A list `arr` of integers of size `M` for the `bubble_sort` function. Output: 1. Timing results before and after optimization. 2. Written explanation of the optimizations applied. Constraints: - `1 <= N, M <= 10^6` Additional Information: - Use the `timeit` module or a similar tool to measure execution time. - Ensure your optimizations maintain the correctness of the original functions. Example: ```python import timeit # Original functions def arithmetic_operations(numbers): result = 0 for number in numbers: result += number ** 2 result -= number ** 3 result += number ** 4 return result def bubble_sort(arr): n = len(arr) for i in range(n): for j in range(0, n-i-1): if arr[j] > arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] return arr # Example input numbers = [1, 2, 3, 4, 5] arr = [64, 34, 25, 12, 22, 11, 90] # Timing the original functions arithmetic_time = timeit.timeit(lambda: arithmetic_operations(numbers), number=1000) bubble_sort_time = timeit.timeit(lambda: bubble_sort(arr.copy()), number=1000) print(f\\"Original Arithmetic Operations Time: {arithmetic_time}\\") print(f\\"Original Bubble Sort Time: {bubble_sort_time}\\") ``` Complete the optimization and profiling by writing the improved versions of `arithmetic_operations` and `bubble_sort`.","solution":"import timeit # Original functions def arithmetic_operations(numbers): result = 0 for number in numbers: result += number ** 2 result -= number ** 3 result += number ** 4 return result def bubble_sort(arr): n = len(arr) for i in range(n): for j in range(0, n-i-1): if arr[j] > arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] return arr # Optimized functions def arithmetic_operations_optimized(numbers): result = 0 for number in numbers: number_sq = number ** 2 number_cu = number_sq * number number_qu = number_cu * number result += number_sq - number_cu + number_qu return result def bubble_sort_optimized(arr): arr_length = len(arr) for i in range(arr_length): swapped = False for j in range(0, arr_length-i-1): if arr[j] > arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] swapped = True if not swapped: break return arr # Example input numbers = [1, 2, 3, 4, 5] arr = [64, 34, 25, 12, 22, 11, 90] # Timing the original functions arithmetic_time = timeit.timeit(lambda: arithmetic_operations(numbers), number=1000) bubble_sort_time = timeit.timeit(lambda: bubble_sort(arr.copy()), number=1000) print(f\\"Original Arithmetic Operations Time: {arithmetic_time}\\") print(f\\"Original Bubble Sort Time: {bubble_sort_time}\\") # Timing the optimized functions arithmetic_optimized_time = timeit.timeit(lambda: arithmetic_operations_optimized(numbers), number=1000) bubble_sort_optimized_time = timeit.timeit(lambda: bubble_sort_optimized(arr.copy()), number=1000) print(f\\"Optimized Arithmetic Operations Time: {arithmetic_optimized_time}\\") print(f\\"Optimized Bubble Sort Time: {bubble_sort_optimized_time}\\")"},{"question":"# Asynchronous Task Scheduler You are tasked with implementing a simplified asynchronous task scheduler. This scheduler will queue tasks, run them with optional timeouts, and ensure proper handling of exceptions and synchronization. Requirements: 1. Implement a class `AsyncTaskScheduler` with the following methods: - `__init__()`: Initializes the scheduler with an empty task queue and a synchronization lock. - `add_task(coro, timeout=None)`: Adds a coroutine task to the scheduler. Optionally, a timeout can be specified. - `run()`: Runs all scheduled tasks concurrently. If a task has a timeout, it must complete within the specified time, or else it will be cancelled and an exception will be logged. - `get_results()`: Returns a dictionary with the results of all tasks. If a task was cancelled due to a timeout, its result should be `None`. Constraints: - Utilize a FIFO queue to store the tasks. - Ensure that the scheduler handles tasks concurrently. - Use asyncio primitives such as locks, tasks, and exception handling. Expected Input & Output: **Input:** - A list of coroutine functions along with their optional timeouts. **Output:** - A dictionary with task results, where keys are task ids and values are results. If a task was cancelled due to a timeout, its value should be `None`. **Example:** ```python import asyncio async def sample_task(task_id, duration): await asyncio.sleep(duration) return f\\"Task {task_id} completed\\" scheduler = AsyncTaskScheduler() # Adding tasks to the scheduler. scheduler.add_task(sample_task(1, 2)) scheduler.add_task(sample_task(2, 1)) scheduler.add_task(sample_task(3, 4), timeout=3) # Will timeout and be cancelled. # Run all tasks. await scheduler.run() # Get results. results = scheduler.get_results() print(results) # Expected Output: # {1: \'Task 1 completed\', 2: \'Task 2 completed\', 3: None} ``` Write the implementation of `AsyncTaskScheduler` class.","solution":"import asyncio from asyncio import Lock class AsyncTaskScheduler: def __init__(self): Initializes the scheduler with an empty task queue and a synchronization lock. self.tasks = [] self.results = {} self.lock = Lock() self.task_counter = 0 def add_task(self, coro, timeout=None): Adds a coroutine task to the scheduler. Optionally, a timeout can be specified. self.tasks.append((self.task_counter, coro, timeout)) self.task_counter += 1 async def _run_task(self, task_id, coro, timeout): try: if timeout: result = await asyncio.wait_for(coro, timeout) else: result = await coro self.results[task_id] = result except asyncio.TimeoutError: self.results[task_id] = None async def run(self): Runs all scheduled tasks concurrently. If a task has a timeout, it must complete within the specified time, or else it will be cancelled and an exception will be logged. tasks = [ self._run_task(task_id, coro, timeout) for task_id, coro, timeout in self.tasks ] await asyncio.gather(*tasks) def get_results(self): Returns a dictionary with the results of all tasks. If a task was cancelled due to a timeout, its result should be `None`. return self.results"},{"question":"# Problem: Creating and Managing a Nested Data Structure Using `dataclasses` You are required to create a nested data structure using `dataclasses` and to perform various operations on it. Your tasks are: 1. **Define the Classes**: Define the following classes using the `@dataclass` decorator: - `Address`: Represents a physical address with the fields: - `street`: A `str` for the street address. - `city`: A `str` for the city. - `zipcode`: An `int` for the zip code. - `Person`: Represents an individual with the fields: - `name`: A `str` for the person\'s name. - `age`: An `int` for the person\'s age. - `address`: An `Address` for the person\'s address. - `Company`: Represents a company with the fields: - `name`: A `str` for the company\'s name. - `employees`: A `list` of `Person` objects representing the company\'s employees. 2. **Implement String Representations**: Ensure that a readable string representation of the `Company` class is generated automatically by `@dataclass`. 3. **Calculate the Average Age of Employees**: Implement a method `average_employee_age` in the `Company` class that returns the average age of all employees. 4. **Converting to Dictionary**: Implement a utility function `company_as_dict(company: Company) -> dict` that converts a `Company` object into a dictionary using the `asdict` function from the `dataclasses` module. 5. **Testing Your Implementation**: In the main block of your code, create instances of the `Address`, `Person`, and `Company` classes. Then, add some employee data to the company and demonstrate the use of `average_employee_age` and `company_as_dict` functions. # Example: Here is an example to guide you in implementing and testing the classes: ```python from dataclasses import dataclass, field, asdict @dataclass class Address: street: str city: str zipcode: int @dataclass class Person: name: str age: int address: Address @dataclass class Company: name: str employees: list[Person] def average_employee_age(self) -> float: total_age = sum(employee.age for employee in self.employees) return total_age / len(self.employees) def company_as_dict(company: Company) -> dict: return asdict(company) # Testing the implementation if __name__ == \\"__main__\\": addr = Address(\\"123 Main St\\", \\"Springfield\\", 12345) emp1 = Person(\\"Alice\\", 30, addr) emp2 = Person(\\"Bob\\", 40, addr) company = Company(name=\\"TechCorp\\", employees=[emp1, emp2]) print(company) print(f\\"Average Employee Age: {company.average_employee_age()}\\") print(f\\"Company as dict: {company_as_dict(company)}\\") ``` # Constraints: - Ensure all classes make use of type annotations as specified. - Raise appropriate errors if an invalid operation is performed (e.g., calculating the average age with no employees). Notes: - Use appropriate imports from the `dataclasses` module. - The final output must reflect accurate and readable representations of the nested data structure.","solution":"from dataclasses import dataclass, field, asdict from typing import List @dataclass class Address: street: str city: str zipcode: int @dataclass class Person: name: str age: int address: Address @dataclass class Company: name: str employees: List[Person] = field(default_factory=list) def average_employee_age(self) -> float: if not self.employees: raise ValueError(\\"No employees to calculate the average age.\\") total_age = sum(employee.age for employee in self.employees) return total_age / len(self.employees) def company_as_dict(company: Company) -> dict: return asdict(company) # Testing the implementation if __name__ == \\"__main__\\": addr = Address(\\"123 Main St\\", \\"Springfield\\", 12345) emp1 = Person(\\"Alice\\", 30, addr) emp2 = Person(\\"Bob\\", 40, addr) company = Company(name=\\"TechCorp\\", employees=[emp1, emp2]) print(company) print(f\\"Average Employee Age: {company.average_employee_age()}\\") print(f\\"Company as dict: {company_as_dict(company)}\\")"},{"question":"**Problem Statement:** You are tasked with creating a secure command-line login system for a small application. The system should: 1. Prompt the user to enter their username and password. 2. Validate the username against the current user\'s login name obtained using `getpass.getuser()`. 3. Store a predefined password securely within the script and compare it with the user\'s input without echoing the entered password. The script should follow these steps: - Prompt the user for their username. - Check if the entered username matches the current login name using `getpass.getuser()`. - If the username matches, prompt the user for their password using `getpass.getpass(prompt=\'Enter your password: \')`. - Compare the input password with a predefined password stored in the script (e.g., `predefined_password = \\"secure_password\\"`). - If the password matches, print \\"Access granted\\"; otherwise, print \\"Access denied\\". **Constraints:** - Do not use any additional libraries or modules for password hashing or secure storage, aside from the provided `getpass` module. - Assume the current environment allows the use of the `getpass` module without compatibility issues. **Performance Requirements:** - The script should handle user input promptly and securely. **Function Signature:** ```python def login_system(): # Your code here ``` **Example:** ```python # When the script is run by a user with login name \'current_user\': Input: Enter your username: current_user Enter your password: # (password input hidden, user enters: secure_password) Output: Access granted # Another attempt: Input: Enter your username: current_user Enter your password: # (password input hidden, user enters: wrong_password) Output: Access denied ``` Write a function `login_system` to implement the above functionality.","solution":"import getpass def login_system(): A secure command-line login system that prompts the user for their username and password, and grants access if they match predefined credentials. predefined_password = \\"secure_password\\" current_user = getpass.getuser() username = input(\\"Enter your username: \\") if username == current_user: password = getpass.getpass(\\"Enter your password: \\") if password == predefined_password: print(\\"Access granted\\") else: print(\\"Access denied\\") else: print(\\"Access denied\\")"},{"question":"# Question: ASCII String Transformer You are tasked with implementing a function that transforms a given string based on the following criteria using the `curses.ascii` module: 1. Replace all non-ASCII characters with the string `\\"[NON-ASCII]\\"`. 2. Convert all ASCII control characters (excluding space) into their string mnemonics from the `curses.ascii.controlnames` array. 3. For alphabetic characters, convert lowercase letters to uppercase and vice versa. 4. Leave all other characters unchanged. Function Signature: ```python def transform_ascii_string(input_str: str) -> str: pass ``` Input: - `input_str` (str): A string containing any characters, including non-ASCII characters. Output: - (str): A transformed string following the specified rules. Constraints: - You must use the functions provided by the `curses.ascii` module to implement the transformations. - The input string can be of any length. Example: ```python import curses.ascii print(transform_ascii_string(\\"Hellox00Worldx80!\\")) ``` Expected output: ``` \\"H!ELLO[SOH]WORLD[NON-ASCII]!\\" ``` Explanation: - `H` -> `h` (lowercase to uppercase) - `e` -> `E` (lowercase to uppercase) - `l` -> `L` (lowercase to uppercase) - `l` -> `L` (lowercase to uppercase) - `o` -> `O` (lowercase to uppercase) - `x00` is a control character (NUL), thus replaced with `[NUL]` - `W` -> `w` (uppercase to lowercase) - `o` -> `O` (lowercase to uppercase) - `r` -> `R` (lowercase to uppercase) - `l` -> `L` (lowercase to uppercase) - `d` -> `D` (lowercase to uppercase) - `x80` is a non-ASCII character, thus replaced with `[NON-ASCII]` - `!` remains unchanged Use the provided functions from `curses.ascii` to implement the solution efficiently.","solution":"import curses.ascii def transform_ascii_string(input_str: str) -> str: result = [] for ch in input_str: if not curses.ascii.isascii(ch): result.append(\\"[NON-ASCII]\\") elif curses.ascii.iscntrl(ch) and ch != \' \': result.append(f\\"[{curses.ascii.controlnames[ord(ch)]}]\\") elif curses.ascii.isalpha(ch): if curses.ascii.isupper(ch): result.append(ch.lower()) else: result.append(ch.upper()) else: result.append(ch) return \'\'.join(result)"},{"question":"# Programming Problem: Implementing and Utilizing Nested Tensors in PyTorch Problem Statement In this task, you will implement a sequence data processing function using PyTorch Nested Tensors. You are required to perform the following steps: 1. Given a list of 2D tensors representing sequences of variable lengths, create a Nested Jagged Tensor (`NJT`). 2. Write a function that normalizes each element within the sequences using the mean and standard deviation of that particular sequence. 3. Convert the resulting `NJT` back to a padded dense tensor form, specifying a padding value. Function Signature ```python import torch from typing import List def process_sequences_and_pad(sequences: List[torch.Tensor], padding_value: float) -> torch.Tensor: Processes a list of 2D tensors representing sequences of variable lengths using Nested Tensors. Normalizes each sequence and converts the result to a padded dense tensor. Parameters: sequences (List[torch.Tensor]): List of 2D tensors with varying lengths. padding_value (float): Value to pad the resulting dense tensor with. Returns: torch.Tensor: A padded dense tensor. pass ``` Input - `sequences`: A list of 2D PyTorch tensors. Each tensor represents a sequence with the same number of columns but varying number of rows (lengths). * Example: ```python sequences = [torch.tensor([[0.1, 0.2], [0.3, 0.4]]), torch.tensor([[1.0, 1.1], [1.2, 1.3], [1.4, 1.5]])] ``` - `padding_value`: A float value used for padding the resulting dense tensor. Output - The function should return a single 3D padded dense tensor with the normalized values of the sequences. The sequences should be padded to match the length of the longest sequence in the input list. Constraints - All tensors in the input list have the same number of columns (features). - The nested tensor should use the `torch.jagged` layout as recommended in the provided documentation. Example ```python sequences = [torch.tensor([[0.1, 0.2], [0.3, 0.4]]), torch.tensor([[1.0, 1.1], [1.2, 1.3], [1.4, 1.5]])] padding_value = 9.0 result = process_sequences_and_pad(sequences, padding_value) print(result) # Expected Output: # tensor([[[ 0.2074, 0.1369], # [ 1.9396, -0.0329], # [ 9.0000, 9.0000]], # # [[-0.2074, -0.1369], # [ 0.1797, 0.1643], # [ 0.2312, 0.1441]]]) ``` Performance Requirements The implementation should efficiently handle memory by utilizing the NJT structure and ensuring minimal copying of data. **Note**: Use the appropriate PyTorch functions and ensure the code is compatible with the `torch.nested` module\'s prototype API.","solution":"import torch from typing import List def process_sequences_and_pad(sequences: List[torch.Tensor], padding_value: float) -> torch.Tensor: Processes a list of 2D tensors representing sequences of variable lengths using Nested Tensors. Normalizes each sequence and converts the result to a padded dense tensor. Parameters: sequences (List[torch.Tensor]): List of 2D tensors with varying lengths. padding_value (float): Value to pad the resulting dense tensor with. Returns: torch.Tensor: A padded dense tensor. # Compute the mean and std for each sequence and normalize the sequence normalized_sequences = [] for seq in sequences: seq_mean = seq.mean(dim=0, keepdim=True) seq_std = seq.std(dim=0, keepdim=True, unbiased=False) normalized_seq = (seq - seq_mean) / (seq_std + 1e-6) # Adding epsilon to avoid division by zero normalized_sequences.append(normalized_seq) # Find the maximum number of rows in the sequences max_length = max(seq.size(0) for seq in sequences) # Pad the sequences to the maximum length padded_sequences = [] for seq in normalized_sequences: padding = (0, 0, 0, max_length - seq.size(0)) padded_seq = torch.nn.functional.pad(seq, padding, \\"constant\\", padding_value) padded_sequences.append(padded_seq) # Stack the padded sequences into a single tensor padded_tensor = torch.stack(padded_sequences) return padded_tensor"},{"question":"# **Config Parser Challenge Using Python** Objective You are tasked with writing a Python function that reads and processes a configuration file using the `configparser` module. This function will demonstrate your understanding of handling configuration files, managing fallback values, and customizing parsing behavior. Problem Statement Write a function `process_config(file_path: str) -> dict` that performs the following tasks: 1. Reads the configuration file specified by `file_path`. 2. Ensures that the configuration file contains at least two sections: `DEFAULT` and `Settings`. 3. Falls back to default values provided in the `DEFAULT` section if any required keys are missing in the `Settings` section. 4. Returns a dictionary with all key-value pairs from the `Settings` section after applying the fallback values. Configuration File Example Assume the configuration file follows the INI file structure and has the following format: ```ini [DEFAULT] username = anonymous password = guest [Settings] username = admin ``` Requirements - The function must read the configuration file using `configparser`. - If the `Settings` section is missing any keys, it should use the values from the `DEFAULT` section. - The resulting dictionary should not include the `DEFAULT` keys if they were not referred to in the `Settings` section. Constraints - The configuration file will always be well-formed INI format. - The `Settings` section will not contain nested sections. Expected Input and Output - **Input:** Path to a configuration file, e.g., `config.ini`. - **Output:** A dictionary containing the merged key-value pairs from the `Settings` section with fallbacks from the `DEFAULT` section, e.g., `{\'username\': \'admin\', \'password\': \'guest\'}`. Example Usage ```python file_path = \'config.ini\' result = process_config(file_path) print(result) # Output: {\'username\': \'admin\', \'password\': \'guest\'} ``` Performance Requirements - The function should handle reading reasonably large configuration files without significant performance degradation. Solution Skeleton ```python import configparser def process_config(file_path: str) -> dict: # Initialize ConfigParser config = configparser.ConfigParser() # Read the config file config.read(file_path) # Ensure `Settings` section exists if \'Settings\' not in config: raise ValueError(\\"Configuration file must contain a \'Settings\' section\\") # Get the dictionary from the `Settings` section settings = dict(config.items(\'Settings\')) return settings ``` Complete the function to meet the above requirements.","solution":"import configparser def process_config(file_path: str) -> dict: # Initialize ConfigParser config = configparser.ConfigParser() # Read the config file config.read(file_path) # Ensure `Settings` section exists if \'Settings\' not in config: raise ValueError(\\"Configuration file must contain a \'Settings\' section\\") # Get the dictionary from the `Settings` section with fallback to `DEFAULT` settings = {key: config.get(\'Settings\', key) for key in config[\'Settings\']} # Add missing keys from the DEFAULT section for key in config[\'DEFAULT\']: if key not in settings: settings[key] = config[\'DEFAULT\'][key] return settings"},{"question":"You are tasked with writing a Python function that fetches and processes data from a web server using the `http.client` module. Your function should interact with an HTTP server to perform the following tasks: 1. Establish an HTTP connection to the provided server and port. 2. Perform a GET request to fetch the contents of a specified URL path. 3. Process and return the HTTP response headers and the first 500 characters of the response body. **Function Signature** ```python def fetch_and_process_data(host: str, port: int, path: str) -> tuple: # Your implementation goes here pass ``` **Input** - `host` (str): The hostname of the server (e.g., \\"www.python.org\\"). - `port` (int): The port number to connect to (defaults: 80 for HTTP, 443 for HTTPS). - `path` (str): The URL path to fetch data from (e.g., \\"/index.html\\"). **Output** - Returns a tuple `(headers, body_snippet)`: - `headers` (list of tuples): A list of (header, value) tuples containing response headers. - `body_snippet` (str): The first 500 characters of the response body. **Constraints** - Raise appropriate exceptions if the connection fails or an invalid response is received. - Handle chunked responses appropriately if present. **Example** ```python host = \\"www.python.org\\" port = 80 path = \\"/\\" result = fetch_and_process_data(host, port, path) print(result) # Expected Output # ([(\'Content-type\', \'text/html; charset=utf-8\'), ...], \'<!doctype html>...\') ``` **Notes** - Make sure to handle and close the HTTP connection properly. - Consider edge cases like empty responses, non-200 status codes, and large responses. - Follow the HTTP/1.1 protocol specifications for headers, chunked encoding, etc.","solution":"import http.client def fetch_and_process_data(host, port, path): Establishes an HTTP connection, performs a GET request, and returns headers and a snippet of the body. connection = http.client.HTTPConnection(host, port) try: connection.request(\\"GET\\", path) response = connection.getresponse() if response.status != 200: raise Exception(f\\"HTTP Error: {response.status} {response.reason}\\") headers = response.getheaders() body = response.read(500).decode(\'utf-8\') return headers, body finally: connection.close()"},{"question":"Objective To assess your understanding and application of advanced concepts of the `urllib.request` module in Python, such as making authenticated HTTP requests, handling responses, managing headers, and processing data. Question You are required to implement a function `fetch_authenticated_data` that accesses a specific URL with Basic HTTP Authentication, retrieves data from the given endpoint, and processes this data according to some specified headers. Function Definition ```python def fetch_authenticated_data(url: str, username: str, password: str, data: bytes = None, additional_headers: dict = None): Fetch data from the given URL using Basic HTTP Authentication. Parameters: - url: str : The URL to fetch data from. - username: str : Username for Basic HTTP Authentication. - password: str : Password for Basic HTTP Authentication. - data: bytes : Optional data to send in a POST request. If None, a GET request will be made. - additional_headers: dict : Optional dictionary of additional headers to include in the request. Returns: - response_str: str : The response data decoded as a UTF-8 string. - status_code: int : The HTTP status code of the response. pass ``` Constraints 1. You must use Basic HTTP Authentication to access the URL. 2. If `data` is provided, a POST request should be made. If `data` is `None`, a GET request should be made. 3. Headers specified in `additional_headers` should be included in the request. 4. Handle possible exceptions, such as network errors or authentication failures, and return appropriate messages. Example Usage ```python url = \'https://example.com/api/data\' username = \'user123\' password = \'passwd123\' data = b\'sample data to send\' additional_headers = { \'User-Agent\': \'urllib-example/0.1\', \'Content-Type\': \'application/json\' } response, status = fetch_authenticated_data(url, username, password, data, additional_headers) print(\\"Response:\\", response) print(\\"Status Code:\\", status) ``` Expected Output - `response`: The data received from the URL as a UTF-8 decoded string. - `status_code`: The HTTP status code of the response. **Note:** Ensure your implementation adheres to best practices for managing HTTP requests and handling potential errors effectively.","solution":"import urllib.request import urllib.error import base64 def fetch_authenticated_data(url: str, username: str, password: str, data: bytes = None, additional_headers: dict = None): try: # Create a password manager password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) # Create an HTTPBasicAuthHandler with this password manager handler = urllib.request.HTTPBasicAuthHandler(password_mgr) # Create an opener using the handler opener = urllib.request.build_opener(handler) # Create the request request = urllib.request.Request(url, data=data) # Add headers if provided if additional_headers is not None: for header, value in additional_headers.items(): request.add_header(header, value) # Open the URL with opener.open(request) as response: response_data = response.read().decode(\'utf-8\') status_code = response.getcode() return response_data, status_code except urllib.error.HTTPError as e: return f\'HTTPError: {e.reason}\', e.code except urllib.error.URLError as e: return f\'URLError: {e.reason}\', None except Exception as e: return f\'Error: {str(e)}\', None"},{"question":"**Problem: Implementation and Evaluation of Regularized Linear Regression Models** # Context You are provided with a dataset containing multiple features and a target variable. Your task is to implement and evaluate different regularized linear regression models from the scikit-learn library. # Dataset Description Assume you have access to a dataset with the following characteristics: - `X_train`: Training feature set, a 2D numpy array of shape (n_train_samples, n_features). - `y_train`: Training target values, a 1D numpy array of shape (n_train_samples,). - `X_test`: Testing feature set, a 2D numpy array of shape (n_test_samples, n_features). - `y_test`: Testing target values, a 1D numpy array of shape (n_test_samples,). # Requirements 1. **Data Preprocessing** - Standardize the features by removing the mean and scaling to unit variance using StandardScaler. 2. **Model Training and Evaluation** - Fit an Ordinary Least Squares (OLS) linear regression model using `LinearRegression`. - Fit a Ridge regression model using `RidgeCV`, with cross-validation to select the best alpha parameter. - Fit a Lasso regression model using `LassoCV`, with cross-validation to select the best alpha parameter. - Fit an Elastic-Net regression model using `ElasticNetCV`, with cross-validation to select the best alpha and l1_ratio parameters. 3. **Evaluation** - Evaluate each model on the test set using the Mean Squared Error (MSE) metric. - Print the coefficients and intercept for each model. - Discuss the effects of regularization by comparing the coefficients of the different models. # Constraints - Assume that the dataset is already loaded in the provided variables: `X_train`, `y_train`, `X_test`, `y_test`. - Use only scikit-learn for model implementation and evaluation. - No additional data visualization is required. # Code Template ```python import numpy as np from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error # Data Preprocessing scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Ordinary Least Squares ols_model = LinearRegression() ols_model.fit(X_train_scaled, y_train) ols_pred = ols_model.predict(X_test_scaled) ols_mse = mean_squared_error(y_test, ols_pred) print(\\"OLS Coefficients:\\", ols_model.coef_) print(\\"OLS Intercept:\\", ols_model.intercept_) print(\\"OLS MSE:\\", ols_mse) # Ridge Regression with CV ridge_model = RidgeCV(alphas=np.logspace(-6, 6, 13), store_cv_values=True) ridge_model.fit(X_train_scaled, y_train) ridge_pred = ridge_model.predict(X_test_scaled) ridge_mse = mean_squared_error(y_test, ridge_pred) print(\\"Ridge Coefficients:\\", ridge_model.coef_) print(\\"Ridge Intercept:\\", ridge_model.intercept_) print(\\"Ridge MSE:\\", ridge_mse) print(\\"Best Alpha for Ridge:\\", ridge_model.alpha_) # Lasso Regression with CV lasso_model = LassoCV(alphas=np.logspace(-6, 6, 13), cv=5) lasso_model.fit(X_train_scaled, y_train) lasso_pred = lasso_model.predict(X_test_scaled) lasso_mse = mean_squared_error(y_test, lasso_pred) print(\\"Lasso Coefficients:\\", lasso_model.coef_) print(\\"Lasso Intercept:\\", lasso_model.intercept_) print(\\"Lasso MSE:\\", lasso_mse) print(\\"Best Alpha for Lasso:\\", lasso_model.alpha_) # Elastic-Net Regression with CV elasticnet_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], alphas=np.logspace(-6, 6, 13), cv=5) elasticnet_model.fit(X_train_scaled, y_train) elasticnet_pred = elasticnet_model.predict(X_test_scaled) elasticnet_mse = mean_squared_error(y_test, elasticnet_pred) print(\\"Elastic-Net Coefficients:\\", elasticnet_model.coef_) print(\\"Elastic-Net Intercept:\\", elasticnet_model.intercept_) print(\\"Elastic-Net MSE:\\", elasticnet_mse) print(\\"Best Alpha for Elastic-Net:\\", elasticnet_model.alpha_) print(\\"Best L1 Ratio for Elastic-Net:\\", elasticnet_model.l1_ratio_) # Interpretation of Results print(\\"nComparative Analysis:\\") print(f\\"OLS vs Ridge vs Lasso vs Elastic-Net Coefficients:n\\") print(f\\"OLS Coefficients: {ols_model.coef_}\\") print(f\\"Ridge Coefficients: {ridge_model.coef_}\\") print(f\\"Lasso Coefficients: {lasso_model.coef_}\\") print(f\\"Elastic-Net Coefficients: {elasticnet_model.coef_}\\") print(\\"nDiscuss the effect of regularization on coefficients and intercepts.\\") ``` # Deliverables - Implement the provided code template. - Provide a brief report discussing the differences in the MSEs and the coefficients obtained from the different models and the effect of regularization.","solution":"import numpy as np from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error def reg_linear_models(X_train, y_train, X_test, y_test): # Data Preprocessing scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Ordinary Least Squares ols_model = LinearRegression() ols_model.fit(X_train_scaled, y_train) ols_pred = ols_model.predict(X_test_scaled) ols_mse = mean_squared_error(y_test, ols_pred) # Ridge Regression with CV ridge_model = RidgeCV(alphas=np.logspace(-6, 6, 13), store_cv_values=True) ridge_model.fit(X_train_scaled, y_train) ridge_pred = ridge_model.predict(X_test_scaled) ridge_mse = mean_squared_error(y_test, ridge_pred) # Lasso Regression with CV lasso_model = LassoCV(alphas=np.logspace(-6, 6, 13), cv=5) lasso_model.fit(X_train_scaled, y_train) lasso_pred = lasso_model.predict(X_test_scaled) lasso_mse = mean_squared_error(y_test, lasso_pred) # Elastic-Net Regression with CV elasticnet_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], alphas=np.logspace(-6, 6, 13), cv=5) elasticnet_model.fit(X_train_scaled, y_train) elasticnet_pred = elasticnet_model.predict(X_test_scaled) elasticnet_mse = mean_squared_error(y_test, elasticnet_pred) results = { \\"ols\\": { \\"coef\\": ols_model.coef_, \\"intercept\\": ols_model.intercept_, \\"mse\\": ols_mse }, \\"ridge\\": { \\"coef\\": ridge_model.coef_, \\"intercept\\": ridge_model.intercept_, \\"mse\\": ridge_mse, \\"alpha\\": ridge_model.alpha_ }, \\"lasso\\": { \\"coef\\": lasso_model.coef_, \\"intercept\\": lasso_model.intercept_, \\"mse\\": lasso_mse, \\"alpha\\": lasso_model.alpha_ }, \\"elasticnet\\": { \\"coef\\": elasticnet_model.coef_, \\"intercept\\": elasticnet_model.intercept_, \\"mse\\": elasticnet_mse, \\"alpha\\": elasticnet_model.alpha_, \\"l1_ratio\\": elasticnet_model.l1_ratio_ } } return results"},{"question":"You are given a script written in Python 2.x that performs various operations, including dictionary iteration, numeric operations, and string manipulation. Your task is to write a Python script that uses the 2to3 library to convert the given Python 2.x script to a Python 3.x compliant script. The Python 2.x script is provided below: ```python # example2.py def process_data(data): result = {} for key in data.iterkeys(): value = data[key] if isinstance(value, basestring): result[key] = value.upper() elif isinstance(value, (int, long)): result[key] = value + 100 elif isinstance(value, list): result[key] = sorted(value) elif data.has_key(key): result[key] = \\"Verified\\" return result data = { \'name\': \'john\', \'age\': 42L, \'numbers\': [5, 2, 9, 1] } print process_data(data) ``` Your task is to: 1. Write a script `convert_to_py3.py` that takes the filename of a Python 2.x script as input (e.g., `example2.py`). 2. Utilize the 2to3 library to apply the necessary transformations to convert it to a Python 3.x script. 3. Save the converted script to a new file with a suffix `_py3` (e.g., `example2_py3.py`). **Input:** The input will be the filename of the Python 2.x script (e.g., `example2.py`). **Output:** The output should be a new Python 3.x compliant script with a suffix `_py3` (e.g., `example2_py3.py`). # Constraints: - You must use the 2to3 library to perform the conversion. - Ensure that the converted script preserves the functionality of the original script. - Consider edge cases such as handling the difference between Python 2.x and 3.x dictionary methods, numeric types, and string types. # Example: Input: ``` example2.py ``` Output (new file `example2_py3.py`): ```python # example2.py converted to Python 3.x def process_data(data): result = {} for key in data.keys(): value = data[key] if isinstance(value, str): result[key] = value.upper() elif isinstance(value, int): result[key] = value + 100 elif isinstance(value, list): result[key] = sorted(value) elif key in data: result[key] = \\"Verified\\" return result data = { \'name\': \'john\', \'age\': 42, \'numbers\': [5, 2, 9, 1] } print(process_data(data)) ``` **Implementation Hint:** - You may use the `lib2to3` library and its fixing capability by programmatically invoking the `2to3` tool. - Consider using `lib2to3.refactor` to handle the refactoring process programmatically. Good luck!","solution":"import os import lib2to3 from lib2to3.refactor import RefactoringTool, get_fixers_from_package def convert_to_py3(filename): Converts a Python 2.x script to a Python 3.x script using the 2to3 library. Args: filename (str): The name of the Python 2.x script file. # Define the path for the new Python 3.x script new_filename = filename.replace(\'.py\', \'_py3.py\') # Get the list of fixers from the lib2to3 package fixers = get_fixers_from_package(\'lib2to3.fixes\') # Create a RefactoringTool with the fixers refactor_tool = RefactoringTool(fixers) # Read the content of the original file with open(filename, \'r\') as file: source_code = file.read() # Apply the 2to3 transformation new_code_tree = refactor_tool.refactor_string(source_code, filename) new_code = str(new_code_tree) # Write the transformed code to the new file with open(new_filename, \'w\') as new_file: new_file.write(new_code) print(f\\"Converted {filename} to {new_filename}\\") # Example usage: # convert_to_py3(\'example2.py\')"},{"question":"# Subprocess Management in Python You are required to write a Python function `execute_commands_with_pipes` which demonstrates an advanced use of the `subprocess` module. The function will: 1. Execute a series of shell commands that are connected via pipes. 2. Capture and return the final standard output and standard error of the pipeline. # Function Signature ```python def execute_commands_with_pipes(commands: list[list[str]]) -> tuple[str, str]: pass ``` # Parameters: - `commands`: A list of lists where each inner list represents a command and its arguments. Each command should be executed in sequence, with the output of one command fed as the input to the next. # Returns: - A tuple containing two strings: - The final output from the standard output of the last command in the pipeline. - The final output from the standard error of the last command in the pipeline. # Example: ```python commands = [ [\\"ls\\", \\"-l\\"], [\\"grep\\", \\"py\\"], [\\"sort\\", \\"-r\\"] ] stdout, stderr = execute_commands_with_pipes(commands) print(\\"STDOUT:n\\", stdout) print(\\"STDERR:n\\", stderr) ``` # Constraints: - No command list will be empty. - Commands in the list should be executed exactly in the order they appear. - Direct redirection using shell=True should not be used; use pipe mechanics instead. # Notes: - Make sure to handle exceptions and provide meaningful error messages. - Consider using `subprocess.Popen` with appropriate stdin, stdout, and stderr redirections. - Properly close pipes to avoid deadlocks. # Hints: - Use the `subprocess.PIPE` to handle standard input/output appropriately. - Ensure proper data flow between the processes to maintain the order and integrity of the piped commands.","solution":"import subprocess def execute_commands_with_pipes(commands: list[list[str]]) -> tuple[str, str]: Executes a series of shell commands connected via pipes. Args: commands (list[list[str]]): A list of lists where each inner list represents a command and its arguments. Returns: tuple[str, str]: The final output from the standard output and standard error of the last command in the pipeline. if not commands: raise ValueError(\\"The commands list cannot be empty.\\") num_commands = len(commands) if num_commands == 1: # Simple case with single command process = subprocess.Popen(commands[0], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) stdout, stderr = process.communicate() return stdout, stderr # Initiate the first command process = subprocess.Popen(commands[0], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) # Chain the remaining commands for i in range(1, num_commands): process = subprocess.Popen(commands[i], stdin=process.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) stdout, stderr = process.communicate() return stdout, stderr"},{"question":"# Python310 Advanced Coding Assessment You will implement a series of functions that utilize the data marshalling support documented for Python310. This involves creating, writing, and reading marshaled data both to files and strings. You will work with the provided C functions in a Python extension module. Requirements 1. **Marshalling to File and Back**: - Write a Python function `marshal_long_to_file(value: int, file_path: str, version: int) -> None`: - Marshals a long integer value to a file specified by file_path using the provided version. - Write a Python function `read_long_from_file(file_path: str) -> int`: - Reads a long integer value from a file specified by file_path. 2. **Marshalling Python Objects**: - Write a Python function `marshal_object_to_file(value: Any, file_path: str, version: int) -> None`: - Marshals a Python object to a file specified by file_path using the provided version. - Write a Python function `read_object_from_file(file_path: str) -> Any`: - Reads a Python object from a file specified by file_path. 3. **Marshalling to and from Strings**: - Write a Python function `marshal_object_to_string(value: Any, version: int) -> bytes`: - Marshals a Python object to a bytes object using the provided version. - Write a Python function `read_object_from_string(data: bytes) -> Any`: - Reads a Python object from a bytes object. # Input and Output Formats 1. `marshal_long_to_file(value, file_path, version)`: - **Input**: - `value`: a long integer to marshal (should be within 32-bit range) - `file_path`: the path to the file where the value will be stored - `version`: the marshalling version (should be 1 or 2) - **Output**: None 2. `read_long_from_file(file_path)`: - **Input**: - `file_path`: the path to the file from which the value will be read - **Output**: Returns the long integer read from the file 3. `marshal_object_to_file(value, file_path, version)`: - **Input**: - `value`: a Python object to marshal - `file_path`: the path to the file where the object will be stored - `version`: the marshalling version (should be 1 or 2) - **Output**: None 4. `read_object_from_file(file_path)`: - **Input**: - `file_path`: the path to the file from which the object will be read - **Output**: Returns the Python object read from the file 5. `marshal_object_to_string(value, version)`: - **Input**: - `value`: a Python object to marshal - `version`: the marshalling version (should be 1 or 2) - **Output**: Returns a bytes object containing the marshaled representation 6. `read_object_from_string(data)`: - **Input**: - `data`: a bytes object containing the marshaled data - **Output**: Returns the Python object read from the bytes object Constraints and Notes - Ensure that all file handling is done in binary mode. - Handle any potential errors as specified in the documentation. - Assume proper inputs as specified in the formats. - Focus on efficiency and correctness when implementing these functions. # Example Usage ```python # Example usage for marshal_long_to_file and read_long_from_file marshal_long_to_file(123456789, \'long_data.bin\', 2) print(read_long_from_file(\'long_data.bin\')) # Should print: 123456789 # Example usage for marshal_object_to_file and read_object_from_file example_obj = {\'key\': \'value\'} marshal_object_to_file(example_obj, \'object_data.bin\', 2) print(read_object_from_file(\'object_data.bin\')) # Should print: {\'key\': \'value\'} # Example usage for marshal_object_to_string and read_object_from_string obj = [1, 2, 3, 4, 5] marshaled_bytes = marshal_object_to_string(obj, 2) print(read_object_from_string(marshaled_bytes)) # Should print: [1, 2, 3, 4, 5] ``` Complete these functions to demonstrate your understanding of Python310\'s data marshalling capabilities.","solution":"import marshal def marshal_long_to_file(value: int, file_path: str, version: int) -> None: Marshals a long integer value to a file specified by file_path using the provided version. with open(file_path, \'wb\') as file: marshal.dump(value, file, version) def read_long_from_file(file_path: str) -> int: Reads a long integer value from a file specified by file_path. with open(file_path, \'rb\') as file: return marshal.load(file) def marshal_object_to_file(value: any, file_path: str, version: int) -> None: Marshals a Python object to a file specified by file_path using the provided version. with open(file_path, \'wb\') as file: marshal.dump(value, file, version) def read_object_from_file(file_path: str) -> any: Reads a Python object from a file specified by file_path. with open(file_path, \'rb\') as file: return marshal.load(file) def marshal_object_to_string(value: any, version: int) -> bytes: Marshals a Python object to a bytes object using the provided version. return marshal.dumps(value, version) def read_object_from_string(data: bytes) -> any: Reads a Python object from a bytes object. return marshal.loads(data)"},{"question":"**Coding Assessment Question** # Objective: Demonstrate proficiency in using seaborn\'s `pointplot` to visualize data with multiple categorical variables, perform data aggregation, and customize plot aesthetics. # Problem Description: You are provided with the Titanic dataset, which includes details of the passengers. Your task is to visualize the relationship between passenger class (`pclass`), survival (`survived`), and age (`age`) using seaborn\'s `pointplot`. # Requirements: 1. **Data Aggregation**: - Aggregate the `age` of passengers. - Group by `pclass` for the x-axis and `survived` as the hue. 2. **Plot Customization**: - Use different markers and linestyles for the `survived` hue. - Set the confidence interval to represent the standard deviation (`\\"sd\\"`). - Customize the appearance by setting specific colors for each `pclass`. - Dodge the plot points to enhance visualization clarity. # Input and Output: - **Input**: - You will not have an explicit input since you will be loading the Titanic dataset directly within your code. - **Output**: - Display the generated `pointplot`. # Performance Constraints: - Ensure the visualization renders efficiently and clearly. - All customizations should be performed within 15 lines of code. # Additional Instructions: 1. Load the Titanic dataset using seaborn\'s `load_dataset` function. 2. Follow the seaborn `pointplot` documentation for detailed parameter usage. 3. Use the provided color scheme: `Class 1: \\"blue\\"`, `Class 2: \\"green\\"`, `Class 3: \\"red\\"`. # Example Code Template: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the pointplot with the specified requirements sns.pointplot( data=titanic, x=\\"pclass\\", y=\\"age\\", hue=\\"survived\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\", palette={\\"1\\": \\"blue\\", \\"2\\": \\"green\\", \\"3\\": \\"red\\"}, dodge=True ) # Customize plot aesthetics as required plt.xlabel(\'Passenger Class\') plt.ylabel(\'Age\') plt.title(\'Age Distribution by Passenger Class and Survival\') plt.show() ``` Your task is to complete the code and ensure it meets all the requirements specified above. # Constraints: - Do not modify the dataset, just load and use it as provided by seaborn. - Adhere strictly to the formatting requirements for plot customization. **Good luck!**","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_pointplot(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the pointplot with the specified requirements sns.pointplot( data=titanic, x=\\"pclass\\", y=\\"age\\", hue=\\"survived\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\", palette={1: \\"blue\\", 2: \\"green\\", 3: \\"red\\"}, dodge=True ) # Customize plot aesthetics as required plt.xlabel(\'Passenger Class\') plt.ylabel(\'Age\') plt.title(\'Age Distribution by Passenger Class and Survival\') plt.show()"},{"question":"**Question: Implementing a Dynamic Class with Special Attributes** Objective: You are required to create a dynamic class using the `types.new_class` function from the `types` module. The class should be named `DynamicPerson`, have a couple of special methods and attributes, and demonstrate the utility of the `SimpleNamespace` for handling the dynamic addition and removal of attributes. Requirements: 1. Create a class named `DynamicPerson` dynamically using `types.new_class`. 2. The class should: - Inherit from `SimpleNamespace`. - Have an `__init__` method that accepts `name`, `age`, and initializes a `history` attribute as an empty list. - Include a method named `add_to_history` that appends records to the `history`. - Include a method named `__repr__` that returns a string representation of the instance in the format: `\\"DynamicPerson(name=\'name_value\', age=age_value, history=[...])\\"`. 3. Demonstrate the class with at least three instances, adding different values to the `history` for each instance, and show their string representation using `__repr__`. Function Signature: ```python def create_dynamic_person_class(): # This function should return the dynamically created DynamicPerson class pass def test_dynamic_person_class(): # This function should create instances and demonstrate the required functionality pass ``` **Constraints:** - Use the `types.new_class` for dynamic class creation. - `add_to_history` method should handle any type of values (strings, numbers, etc.) for the history. **Example:** ```python # Creating the dynamic class DynamicPerson = create_dynamic_person_class() # Testing the dynamic class test_dynamic_person_class() # Expected output: # DynamicPerson(name=\'John\', age=30, history=[\'Joined company\', \'Promoted\']) # DynamicPerson(name=\'Jane\', age=25, history=[\'Graduated\', 2021, \'Started job\']) # DynamicPerson(name=\'Doe\', age=40, history=[\'Relocated\', \'Got promoted\', \'Started a family\']) ``` *Note:* You will need to handle the dynamic nature of creating methods within the `exec_body` for `new_class` and ensure the proper inheritance and method implementations.","solution":"import types from types import SimpleNamespace def create_dynamic_person_class(): def init(self, name, age): self.name = name self.age = age self.history = [] def add_to_history(self, record): self.history.append(record) def repr(self): return f\\"DynamicPerson(name=\'{self.name}\', age={self.age}, history={self.history})\\" DynamicPerson = types.new_class(\'DynamicPerson\', (SimpleNamespace,), exec_body=lambda ns: ns.update( { \'__init__\': init, \'add_to_history\': add_to_history, \'__repr__\': repr } )) return DynamicPerson def test_dynamic_person_class(): DynamicPerson = create_dynamic_person_class() # Creating instances john = DynamicPerson(\'John\', 30) john.add_to_history(\'Joined company\') john.add_to_history(\'Promoted\') jane = DynamicPerson(\'Jane\', 25) jane.add_to_history(\'Graduated\') jane.add_to_history(2021) jane.add_to_history(\'Started job\') doe = DynamicPerson(\'Doe\', 40) doe.add_to_history(\'Relocated\') doe.add_to_history(\'Got promoted\') doe.add_to_history(\'Started a family\') # Displaying string representations print(john) print(jane) print(doe) return john, jane, doe"},{"question":"# Coding Assessment: Turtle Graphics Drawing with Events **Objective:** Demonstrate your understanding of the `turtle` module by creating an interactive drawing program using specified turtle graphics concepts. **Description:** Create a turtle graphics program in Python where the turtle will draw different shapes based on user inputs. Use the `turtle` module\'s event-driven features for user interaction and the fundamental motion commands for drawing. **Requirements:** 1. **Setup:** - Create a turtle graphics screen with a background color of your choice. - Define a turtle with an initial shape (e.g., \\"turtle\\") and set its initial speed to a moderate value. 2. **User Commands:** - Use keyboard events to control the turtle movements: - `W` key: Move the turtle forward by 50 units. - `S` key: Move the turtle backward by 50 units. - `A` key: Turn the turtle left by 45 degrees. - `D` key: Turn the turtle right by 45 degrees. - Use mouse click events for drawing: - Left Click: Draw a filled circle with radius 30 units at the click position. - Right Click: Draw a filled square with a side of 50 units at the click position. 3. **Drawing Details:** - Set the pen color and fill color for the shapes drawn with the mouse clicks. The colors should alternate between two sets of colors for consecutive shapes. 4. **Additional Features:** - Implement a reset functionality via key `R` to clear the screen but keep the turtle in its current position and heading. **Constraints:** - Ensure shape drawing coordinates are managed correctly so that shapes are drawn centered around the click point. - Handle edge cases where the turtle or shapes might go out of the visible screen bounds. **Input and Output:** - The program should run an interactive window where commands can be given via keyboard and mouse, with visual feedback on the turtle graphics screen. - No console input/output is required; all interactions are via the graphical window. **Performance:** - Code should efficiently handle continuous key presses and mouse click events without significant lag. **Example Output:** - Upon running, a turtle graphics window should appear with the specified background. - The turtle should respond to `W`, `A`, `S`, and `D` keys, moving and turning as specified. - Clicking left or right mouse buttons should draw the respective shapes, alternating fill colors. - Pressing `R` should clear the screen as described. Implement the solution in Python using the `turtle` module.","solution":"import turtle def setup_screen(): screen = turtle.Screen() screen.bgcolor(\\"lightgrey\\") return screen def setup_turtle(): t = turtle.Turtle() t.shape(\\"turtle\\") t.speed(5) return t def move_forward(): turtle.forward(50) def move_backward(): turtle.backward(50) def turn_left(): turtle.left(45) def turn_right(): turtle.right(45) def toggle_color(colors, current_index): return colors[current_index % len(colors)], (current_index + 1) % len(colors) def draw_circle(x, y): global circle_color_index color, circle_color_index = toggle_color(circle_colors, circle_color_index) turtle.penup() turtle.goto(x, y - 30) turtle.pendown() turtle.color(color) turtle.begin_fill() turtle.circle(30) turtle.end_fill() def draw_square(x, y): global square_color_index color, square_color_index = toggle_color(square_colors, square_color_index) turtle.penup() turtle.goto(x - 25, y - 25) turtle.pendown() turtle.color(color) turtle.begin_fill() for _ in range(4): turtle.forward(50) turtle.left(90) turtle.end_fill() def reset_screen(): turtle.clearscreen() main() def main(): global circle_colors, square_colors, circle_color_index, square_color_index turtle.clear() screen = setup_screen() t = setup_turtle() circle_colors = [\\"red\\", \\"blue\\"] square_colors = [\\"green\\", \\"orange\\"] circle_color_index = 0 square_color_index = 0 screen.onkey(move_forward, \\"w\\") screen.onkey(move_backward, \\"s\\") screen.onkey(turn_left, \\"a\\") screen.onkey(turn_right, \\"d\\") screen.onkey(reset_screen, \\"r\\") screen.onclick(draw_circle, 1) screen.onclick(draw_square, 3) screen.listen() screen.mainloop() if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Coding Assessment Question Objective: Write a Python function utilizing the `re` module to extract and transform data from a given text. This function will read a multiline string where each line contains a student\'s record with the student\'s name, scores in three subjects, and an optional remark. The function should extract the required data and return a formatted report. Task: Implement the function `extract_student_records` with the following specifications: **Function Signature:** ```python def extract_student_records(text: str) -> str: ``` **Input:** - `text` (str): Multiline string containing student records. Each record is separated by a newline. - Each line follows the pattern: `Name: <First Name> <Last Name>, Math: <integer>, Science: <integer>, English: <integer>[, Remark: <remark>]` **Output:** - Returns a string formatted as follows: ``` Student: <First Name> <Last Name>, Total Score: <sum of scores> [Remark: <remark>] ... ``` **Example:** For the input string: ``` Name: John Doe, Math: 85, Science: 92, English: 78 Name: Jane Smith, Math: 90, Science: 88, English: 85, Remark: Excellent Progress Name: Emily Davis, Math: 75, Science: 80, English: 70 ``` The function should return: ``` Student: John Doe, Total Score: 255 Student: Jane Smith, Total Score: 263 Remark: Excellent Progress Student: Emily Davis, Total Score: 225 ``` **Constraints:** - The input string will always follow the specified pattern. - Scores are always integers. - Remarks are optional and may contain spaces. **Notes:** - Ensure proper handling of optional remarks. - Use regular expressions to parse the input string and perform required operations. Guidelines: 1. Use Python\'s `re` module for extracting data. 2. Validate your pattern against the provided example. 3. Properly format the output as specified. Example Usage: ```python text_data = \'\'\' Name: John Doe, Math: 85, Science: 92, English: 78 Name: Jane Smith, Math: 90, Science: 88, English: 85, Remark: Excellent Progress Name: Emily Davis, Math: 75, Science: 80, English: 70 \'\'\' result = extract_student_records(text_data) print(result) ``` Submission Guidelines: - Submit your implementation by committing the `extract_student_records` function. - Ensure your code is properly formatted and commented. - Include relevant test cases to validate the correctness of your solution.","solution":"import re def extract_student_records(text: str) -> str: Extracts and transforms student records from the given text. Args: text (str): Multiline string containing student records. Returns: str: Formatted report. pattern = (r\\"Name: (?P<first_name>w+) (?P<last_name>w+), \\" r\\"Math: (?P<math>d+), \\" r\\"Science: (?P<science>d+), \\" r\\"English: (?P<english>d+)\\" r\\"(, Remark: (?P<remark>.+))?\\") results = [] for match in re.finditer(pattern, text, re.M): first_name = match.group(\\"first_name\\") last_name = match.group(\\"last_name\\") math_score = int(match.group(\\"math\\")) science_score = int(match.group(\\"science\\")) english_score = int(match.group(\\"english\\")) total_score = math_score + science_score + english_score remark = match.group(\\"remark\\") if remark: results.append(f\\"Student: {first_name} {last_name}, Total Score: {total_score} Remark: {remark}\\") else: results.append(f\\"Student: {first_name} {last_name}, Total Score: {total_score}\\") return \\"n\\".join(results)"},{"question":"You are given the Titanic dataset to analyze and visualize using Seaborn. Your task is to create a comprehensive plot that represents the survival rate of different passenger classes based on gender. **Objective:** Implement a function `visualize_survival_by_class_and_gender` that accepts no inputs and performs the following tasks: 1. Loads the Titanic dataset using Seaborn. 2. Creates a count plot to show the number of survivors and non-survivors across the passenger classes, separated by gender. 3. Normalizes the counts to show the percentage of survivors and non-survivors within each gender and passenger class. 4. Adds appropriate titles, axis labels, and a legend to the plot. **Function Specification:** ```python def visualize_survival_by_class_and_gender(): pass ``` **Expected Output:** - A figure that visualizes the percentage of survived and non-survived passengers for each class, separated by gender. **Performance Requirements:** - The function should utilize Seaborn\'s `countplot` for creating the visualizations. - The function should handle the normalization of counts manually to demonstrate a deeper understanding of data manipulation. **Example Visualization:** The resulting plot should have: - X-axis representing passenger classes. - Hue representing the gender of the passengers. - Different colors representing survivors and non-survivors. - Y-axis showing the percentage of passengers in each category. **Hints:** - You might need to use pandas for data manipulation. - Look at the Seaborn documentation for customizing the count plot. Good luck!","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_survival_by_class_and_gender(): # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Count the number of survivors and non-survivors per class and gender count_df = titanic.groupby([\'pclass\', \'sex\', \'survived\']).size().reset_index(name=\'count\') # Calculate the total count per class and gender total_df = titanic.groupby([\'pclass\', \'sex\']).size().reset_index(name=\'total\') # Merge the dataframes on pclass and sex merged_df = pd.merge(count_df, total_df, on=[\'pclass\', \'sex\']) # Calculate the percentage of survivors and non-survivors merged_df[\'percent\'] = (merged_df[\'count\'] / merged_df[\'total\']) * 100 # Pivot the dataframe for plotting plot_df = merged_df.pivot_table(index=[\'pclass\', \'sex\'], columns=\'survived\', values=\'percent\').reset_index() # Create the plot fig, ax = plt.subplots(figsize=(10, 6)) sns.barplot(x=\'pclass\', y=0, hue=\'sex\', data=plot_df, ax=ax, ci=None, palette=\'muted\') sns.barplot(x=\'pclass\', y=1, hue=\'sex\', data=plot_df, ax=ax, ci=None, palette=\'pastel\') # Adjust the plot details ax.set_title(\'Survival Rate by Passenger Class and Gender\') ax.set_xlabel(\'Passenger Class\') ax.set_ylabel(\'Percentage\') ax.legend(title=\'Gender\') plt.show()"},{"question":"You are required to implement a Python function that reads an XML string, modifies it as per given requirements, and outputs the modified XML. Use the functionalities defined in the `xml.dom` module for this purpose. # Requirements 1. Parse the supplied XML string into a DOM `Document`. 2. Perform the following modifications to the parsed XML: - Add a new element `<course>` with child elements `<name>` and `<code>` to the `<university>` element. - Ensure the newly added `<name>` and `<code>` elements have their text values set to the supplied values. 3. Serialize the modified DOM `Document` back to a string format. # Function Signature ```python from xml.dom.minidom import Document def modify_xml(xml_string: str, course_name: str, course_code: str) -> str: Args: xml_string (str): The input XML string representing a university. course_name (str): The name of the course to be added. course_code (str): The code of the course to be added. Returns: str: The modified XML string after adding the specified course. pass ``` # Input - `xml_string` (str): A string representing valid XML. Example: ```xml <university> <department> <name>Computer Science</name> </department> </university> ``` - `course_name` (str): The name of the course to be added. - `course_code` (str): The code of the course to be added. # Output - The function should return a string representing the modified XML. The `<university>` element must now contain a new child element `<course>` with the specified name and code. Example: ```xml <university> <department> <name>Computer Science</name> </department> <course> <name>Data Structures</name> <code>CS202</code> </course> </university> ``` # Constraints - Assume that the input XML string is always valid and conforms to the structure given in the example. - Do not use any external libraries apart from the Python standard library. # Example Input ```python xml_string = <university> <department> <name>Computer Science</name> </department> </university> course_name = \\"Data Structures\\" course_code = \\"CS202\\" ``` Output ```python <university> <department> <name>Computer Science</name> </department> <course> <name>Data Structures</name> <code>CS202</code> </course> </university> ``` # Notes - The function must correctly modify and maintain the XML structure. - Consider edge cases where the input XML might have multiple layers of nested elements. **Implement your solution in the fill-in section provided below:**","solution":"from xml.dom.minidom import parseString, Document def modify_xml(xml_string: str, course_name: str, course_code: str) -> str: Args: xml_string (str): The input XML string representing a university. course_name (str): The name of the course to be added. course_code (str): The code of the course to be added. Returns: str: The modified XML string after adding the specified course. # Parse the XML string into a DOM document doc = parseString(xml_string) # Create the new course element course_element = doc.createElement(\\"course\\") # Create the name element under course name_element = doc.createElement(\\"name\\") name_text = doc.createTextNode(course_name) name_element.appendChild(name_text) # Create the code element under course code_element = doc.createElement(\\"code\\") code_text = doc.createTextNode(course_code) code_element.appendChild(code_text) # Append name and code to course course_element.appendChild(name_element) course_element.appendChild(code_element) # Append the new course element to the university element university_element = doc.getElementsByTagName(\\"university\\")[0] university_element.appendChild(course_element) # Serialize the modified DOM document back to a string return doc.toxml()"},{"question":"# Email Automation with Attachments You are required to write a Python function that constructs and sends an email using the `email` package. The function should handle both text and HTML content, and allow for multiple file attachments. Function Signature ```python def send_email_with_attachments( subject: str, sender: str, recipients: list, text_content: str, html_content: str, attachment_paths: list, smtp_server: str = \'localhost\' ) -> None: pass ``` Input - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipients` (list): A list of recipient email addresses. - `text_content` (str): The plain text content of the email. - `html_content` (str): The HTML content of the email. - `attachment_paths` (list): A list of file paths to be attached to the email. - `smtp_server` (str): The SMTP server address to be used for sending the email (default is \'localhost\'). Output - None Constraints - Both `text_content` and `html_content` must be provided. - The `attachment_paths` list can be empty, meaning no attachments. - The function should handle any exceptions that occur during the email sending process and print appropriate error messages. Example ```python subject = \\"Monthly Report\\" sender = \\"reporter@example.com\\" recipients = [\\"manager@example.com\\", \\"team_lead@example.com\\"] text_content = \\"Please find the monthly report attached.\\" html_content = <html> <body> <p>Please find the <b>monthly report</b> attached.</p> </body> </html> attachment_paths = [\\"report.pdf\\", \\"chart.png\\"] send_email_with_attachments(subject, sender, recipients, text_content, html_content, attachment_paths) ``` Notes - Ensure that after adding attachments, the email\'s MIME type is appropriately set. - Each attachment should be identified for its MIME type using the `mimetypes` module. - If the SMTP server is not available or any issue occurs, the function should print an error message and exit gracefully. - Include necessary imports and use best practices for handling email content and attachments. You may refer to the examples provided in the `email` package documentation for guidance on handling different parts of the email creation and sending process.","solution":"import os import smtplib import mimetypes from email.message import EmailMessage from email.utils import formataddr def send_email_with_attachments( subject: str, sender: str, recipients: list, text_content: str, html_content: str, attachment_paths: list, smtp_server: str = \'localhost\' ) -> None: # Create the EmailMessage object msg = EmailMessage() # Set email fields msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) # Set the email content msg.set_content(text_content) msg.add_alternative(html_content, subtype=\'html\') # Add attachments for attachment_path in attachment_paths: ctype, encoding = mimetypes.guess_type(attachment_path) if ctype is None or encoding is not None: ctype = \'application/octet-stream\' maintype, subtype = ctype.split(\'/\', 1) try: with open(attachment_path, \'rb\') as fp: attachment_data = fp.read() msg.add_attachment(attachment_data, maintype=maintype, subtype=subtype, filename=os.path.basename(attachment_path)) except Exception as e: print(f\\"Failed to attach file {attachment_path}: {e}\\") continue # Send the email try: with smtplib.SMTP(smtp_server) as server: server.send_message(msg) except Exception as e: print(f\\"Failed to send email: {e}\\")"},{"question":"Objective: To design a function that utilizes iterators, generators, and other functional programming constructs to solve a practical problem. This will test the student\'s understanding and ability to implement Python\'s functional programming features. Problem Statement: You are to implement a Python function called `generate_primes` that generates an infinite sequence of prime numbers. You should use generators to yield primes one at a time and utilize iterators effectively to build and manage the sequence of prime numbers. Additionally, implement a function `filter_primes_below` which takes an integer `n` and uses the `generate_primes` generator to return a list of all prime numbers less than `n`. Specifications: 1. **Function 1**: `def generate_primes() -> Iterator[int]` - This function generates an infinite sequence of prime numbers using Python\'s generator functionality. 2. **Function 2**: `def filter_primes_below(n: int) -> List[int]` - This function receives an integer `n` and returns a list of all prime numbers less than `n`. - Ensure to use the `generate_primes` generator and appropriate laziness to handle potentially large values of `n` efficiently. Example Usage: ```python primes_generator = generate_primes() print(next(primes_generator)) # Output: 2 print(next(primes_generator)) # Output: 3 print(next(primes_generator)) # Output: 5 print(next(primes_generator)) # Output: 7 print(filter_primes_below(10)) # Output: [2, 3, 5, 7] print(filter_primes_below(20)) # Output: [2, 3, 5, 7, 11, 13, 17, 19] ``` Constraints: - The function `generate_primes` should be implemented as a generator (using `yield`). - The function `filter_primes_below` should efficiently utilize the generator and avoid creating large lists in memory when not necessary. - The solution should handle large `n` values gracefully without running into performance bottlenecks. Hints: 1. You may use the Sieve of Eratosthenes algorithm or trial division for checking the primality of numbers. 2. Utilize the itertools\' functions where appropriate to assist with prime generation and filtering. Evaluation Criteria: - **Correctness**: The functions should return the correct prime numbers as specified. - **Efficiency**: The implementation should handle large inputs efficiently by leveraging iterative and lazy evaluation constructs. - **Code Quality**: Clear, concise, and idiomatic Pythonic code is expected, with comments explaining the logic where necessary.","solution":"from typing import Iterator, List import itertools def generate_primes() -> Iterator[int]: A generator that yields an infinite sequence of prime numbers. D = {} q = 2 while True: if q not in D: # q is a prime number yield q D[q * q] = [q] else: # q is not a prime number for p in D[q]: D.setdefault(p + q, []).append(p) del D[q] q += 1 def filter_primes_below(n: int) -> List[int]: Filters out all prime numbers below a given integer n. primes = [] for prime in generate_primes(): if prime >= n: break primes.append(prime) return primes"},{"question":"# Python310 Coding Assessment Question Objective: To assess the ability to work with specialized data types and collections provided in python310. Problem Statement: You are required to write a function `event_scheduler()` that schedules events based on their start and end times. Each event is represented by a tuple of the form `(event_name, start_time, end_time)`, where `start_time` and `end_time` are given as `datetime` objects. Here is the key functionality that the function must implement: 1. **Conflict detection:** No two events should overlap in their timing. 2. **Order of events:** Events should be stored in chronological order. 3. **Retrieval:** Implement a method to retrieve the list of events for a given day. Function Signature: ```python from datetime import datetime, timedelta from typing import List, Tuple def event_scheduler(events: List[Tuple[str, datetime, datetime]]) -> None: pass def add_event(event: Tuple[str, datetime, datetime]) -> bool: pass def get_daily_events(date: datetime) -> List[Tuple[str, datetime, datetime]]: pass ``` Optional: Use `OrderedDict` for maintaining the order of events and `defaultdict` for storing events based on dates. Inputs: 1. A list of events where each event is a tuple containing: - `event_name` (str): Name of the event. - `start_time` (datetime): Start time of the event. - `end_time` (datetime): End time of the event. Outputs: 1. `add_event`: Returns a boolean value indicating if the event was added successfully. 2. `get_daily_events`: Returns a list of events scheduled for the specified date. Constraints: 1. `start_time` is always less than `end_time` for any given event. 2. Event times are given in chronological order for the function `get_daily_events`. Example: ```python import datetime # List of events events = [ (\\"Meeting\\", datetime.datetime(2023, 10, 1, 9, 0), datetime.datetime(2023, 10, 1, 10, 0)), (\\"Workshop\\", datetime.datetime(2023, 10, 1, 10, 30), datetime.datetime(2023, 10, 1, 12, 0)), (\\"Lunch\\", datetime.datetime(2023, 10, 1, 12, 30), datetime.datetime(2023, 10, 1, 13, 0)), (\\"Conference\\", datetime.datetime(2023, 10, 1, 14, 0), datetime.datetime(2023, 10, 1, 15, 0)) ] event_scheduler(events) # Add more events success = add_event((\\"New Meeting\\", datetime.datetime(2023, 10, 1, 15, 0), datetime.datetime(2023, 10, 1, 16, 0))) print(success) # Should print True # Check the events for the day daily_events = get_daily_events(datetime.datetime(2023, 10, 1)) print(daily_events) # Should print all the events scheduled for 1st October, 2023 in chronological order ``` Performance Requirements: - The event insertion and retrieval should be efficient. - Handling up to 10,000 events should be within the acceptable performance limits. Notes: You can assume the use of suitable data structures like `OrderedDict` and `defaultdict` from `collections` to manage and organize the events efficiently.","solution":"from datetime import datetime, timedelta from typing import List, Tuple from collections import defaultdict, OrderedDict # A global variable to maintain events events_by_date = defaultdict(OrderedDict) def event_scheduler(events: List[Tuple[str, datetime, datetime]]) -> None: Initialize the scheduler with a list of events. global events_by_date events_by_date.clear() for event in events: add_event(event) def add_event(event: Tuple[str, datetime, datetime]) -> bool: Add an event if it does not conflict with existing events. event_name, start_time, end_time = event date_key = start_time.date() # Get the events for the specific day daily_events = events_by_date[date_key] # Check for conflicts for existing_event in daily_events.values(): if not (end_time <= existing_event[1] or start_time >= existing_event[2]): return False # If no conflicts, add the event daily_events[start_time] = (event_name, start_time, end_time) events_by_date[date_key] = OrderedDict(sorted(daily_events.items())) return True def get_daily_events(date: datetime) -> List[Tuple[str, datetime, datetime]]: Retrieve the list of events scheduled for a given day. date_key = date.date() return list(events_by_date[date_key].values())"},{"question":"Command Interpreter with Customized Functionality # Objective: Implement a customized command interpreter by extending the `cmd.Cmd` class. Your task is to create a command-line application that manages a simple task tracking system. Users should be able to add tasks, list all tasks, remove tasks by ID, mark tasks as completed, and clear all tasks. # Requirements: 1. **Input/Output:** - The commands should be typed in a command-line interface. - The output must be printed to the console. 2. **Commands:** - `add <task_description>`: Add a new task to the list. - `list`: List all tasks with their IDs and statuses. - `remove <task_id>`: Remove the task with the specified ID. - `complete <task_id>`: Mark the task with the specified ID as completed. - `clear`: Remove all tasks. - `quit`/`exit`/`bye`: Exit the interpreter. 3. **Constraints:** - Task IDs should be unique and assigned incrementally starting from 1. - The system should handle invalid commands gracefully. 4. **Performance:** - The operations should be efficient, but no specific performance constraints are required beyond typical interactive use. # Implementation: Create a class `TaskManager` that extends `cmd.Cmd` and implements the required commands. ```python import cmd class TaskManager(cmd.Cmd): prompt = \'(tasks) \' intro = \\"Welcome to the Task Manager. Type help or ? to list commands.n\\" tasks = [] last_id = 0 # Command to add a task def do_add(self, arg): \'Add a new task: ADD <task_description>\' self.last_id += 1 task = {\'id\': self.last_id, \'description\': arg, \'completed\': False} self.tasks.append(task) print(f\\"Added task {self.last_id}: {arg}\\") # Command to list all tasks def do_list(self, arg): \'Lists all tasks with their IDs and statuses\' for task in self.tasks: status = \'Completed\' if task[\'completed\'] else \'Pending\' print(f\\"{task[\'id\']}: {task[\'description\']} - {status}\\") # Command to remove a task by ID def do_remove(self, arg): \'Remove a task by its ID: REMOVE <task_id>\' try: task_id = int(arg) self.tasks = [task for task in self.tasks if task[\'id\'] != task_id] print(f\\"Removed task {task_id}\\") except ValueError: print(\\"Invalid task ID. Please provide a numeric ID.\\") # Command to mark a task as completed def do_complete(self, arg): \'Mark a task as completed: COMPLETE <task_id>\' try: task_id = int(arg) for task in self.tasks: if task[\'id\'] == task_id: task[\'completed\'] = True print(f\\"Completed task {task_id}\\") break else: print(\\"Task not found.\\") except ValueError: print(\\"Invalid task ID. Please provide a numeric ID.\\") # Command to clear all tasks def do_clear(self, arg): \'Clear all tasks\' self.tasks.clear() self.last_id = 0 print(\\"Cleared all tasks.\\") # Command to exit the interpreter def do_quit(self, arg): \'Exit the Task Manager: QUIT\' print(\\"Exiting...\\") return True # Alias for do_quit def do_exit(self, arg): \'Exit the Task Manager: EXIT\' return self.do_quit(arg) # Alias for do_quit def do_bye(self, arg): \'Exit the Task Manager: BYE\' return self.do_quit(arg) def default(self, line): \'Handle invalid commands\' print(f\\"Invalid command: {line}. Type help or ? to list commands.\\") if __name__ == \'__main__\': TaskManager().cmdloop() ``` # Constraints: - Ensure to handle edge cases such as invalid task descriptions, wrong task IDs, and invalid commands. - Ensure the prompt properly reflects the context of the application, and all necessary commands are documented and functional. # Note: The program should be user-friendly, and error handling should be implemented where appropriate. Make sure to test your implementation thoroughly to ensure all commands work as expected.","solution":"import cmd class TaskManager(cmd.Cmd): prompt = \'(tasks) \' intro = \\"Welcome to the Task Manager. Type help or ? to list commands.n\\" tasks = [] last_id = 0 def do_add(self, arg): \'Add a new task: ADD <task_description>\' self.last_id += 1 task = {\'id\': self.last_id, \'description\': arg, \'completed\': False} self.tasks.append(task) print(f\\"Added task {self.last_id}: {arg}\\") def do_list(self, arg): \'Lists all tasks with their IDs and statuses\' for task in self.tasks: status = \'Completed\' if task[\'completed\'] else \'Pending\' print(f\\"{task[\'id\']}: {task[\'description\']} - {status}\\") def do_remove(self, arg): \'Remove a task by its ID: REMOVE <task_id>\' try: task_id = int(arg) before_remove = len(self.tasks) self.tasks = [task for task in self.tasks if task[\'id\'] != task_id] after_remove = len(self.tasks) if before_remove == after_remove: print(f\\"Task {task_id} not found.\\") else: print(f\\"Removed task {task_id}\\") except ValueError: print(\\"Invalid task ID. Please provide a numeric ID.\\") def do_complete(self, arg): \'Mark a task as completed: COMPLETE <task_id>\' try: task_id = int(arg) for task in self.tasks: if task[\'id\'] == task_id: task[\'completed\'] = True print(f\\"Completed task {task_id}\\") break else: print(\\"Task not found.\\") except ValueError: print(\\"Invalid task ID. Please provide a numeric ID.\\") def do_clear(self, arg): \'Clear all tasks\' self.tasks.clear() self.last_id = 0 print(\\"Cleared all tasks.\\") def do_quit(self, arg): \'Exit the Task Manager: QUIT\' print(\\"Exiting...\\") return True def do_exit(self, arg): \'Exit the Task Manager: EXIT\' return self.do_quit(arg) def do_bye(self, arg): \'Exit the Task Manager: BYE\' return self.do_quit(arg) def default(self, line): \'Handle invalid commands\' print(f\\"Invalid command: {line}. Type help or ? to list commands.\\") if __name__ == \'__main__\': TaskManager().cmdloop()"},{"question":"Customizing Seaborn Color Palettes **Objective:** You will demonstrate your understanding of the seaborn `color_palette` function by generating custom color palettes and applying them to visualizations. **Problem Statement:** Write a Python function `custom_plot` that takes in no parameters and performs the following tasks: 1. **Generate and print hex codes**: - Retrieve the default color palette and print its colors as hex codes. - Generate a \'husl\' color palette with 7 colors and print its colors as hex codes. - Generate and print a dark sequential gradient palette with the color `#5A9` reversed. 2. **Create and display plots**: - Use the \'Set2\' palette to create a strip plot of a given dataset. - Temporarily change the default palette to a custom blend gradient between `#7AB` and `#EDA` and use it to create a relplot. **Requirements**: - Use seaborn to generate and apply color palettes. - Use seaborn\'s plotting functions to create the following two visualizations: 1. A strip plot with the \'Set2\' color palette. 2. A relational plot with a custom blend gradient palette. - Display the plots using matplotlib. **Dataset**: You are provided with the following dataset for creating the plots: ```python import pandas as pd data = pd.DataFrame({ \'category\': [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\'], \'value\': [20, 35, 30, 35, 27, 25, 18] }) ``` **Function Signature**: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_plot(): # Task 1 # Retrieve and print the default color palette as hex codes default_palette_hex = sns.color_palette().as_hex() print(\\"Default Palette:\\", default_palette_hex) # Generate and print the \'husl\' color palette with 7 colors as hex codes husl_palette_hex = sns.color_palette(\\"husl\\", 7).as_hex() print(\\"HUSL Palette:\\", husl_palette_hex) # Generate and print the dark sequential gradient palette in reverse dark_seq_palette_hex = sns.color_palette(\\"dark:#5A9_r\\", as_cmap=False).as_hex() print(\\"Dark Sequential Gradient Palette:\\", dark_seq_palette_hex) # Task 2 # Create a strip plot using the \'Set2\' palette sns.set_palette(\\"Set2\\") plt.figure(figsize=(8, 5)) sns.stripplot(x=\'category\', y=\'value\', data=data) plt.title(\\"Strip Plot with \'Set2\' Palette\\") plt.show() # Temporarily change the color palette and create a relplot with sns.color_palette(\\"blend:#7AB,#EDA\\", as_cmap=True): plt.figure(figsize=(8, 5)) sns.relplot(x=\'category\', y=\'value\', data=data, s=100) plt.title(\\"Relational Plot with Custom Blend Palette\\") plt.show() ``` **Notes**: - Ensure the plots are displayed inline (e.g., using `%matplotlib inline` if using a Jupyter notebook). - Use the provided `data` DataFrame to create the plots. **Submission**: - Submit the complete function implementation.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_plot(): data = pd.DataFrame({ \'category\': [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\'], \'value\': [20, 35, 30, 35, 27, 25, 18] }) # Task 1 # Retrieve and print the default color palette as hex codes default_palette_hex = sns.color_palette().as_hex() print(\\"Default Palette:\\", default_palette_hex) # Generate and print the \'husl\' color palette with 7 colors as hex codes husl_palette_hex = sns.color_palette(\\"husl\\", 7).as_hex() print(\\"HUSL Palette:\\", husl_palette_hex) # Generate and print the dark sequential gradient palette in reverse dark_seq_palette_hex = sns.color_palette(\\"dark:#5A9_r\\", as_cmap=False).as_hex() print(\\"Dark Sequential Gradient Palette:\\", dark_seq_palette_hex) # Task 2 # Create a strip plot using the \'Set2\' palette sns.set_palette(\\"Set2\\") plt.figure(figsize=(8, 5)) sns.stripplot(x=\'category\', y=\'value\', data=data) plt.title(\\"Strip Plot with \'Set2\' Palette\\") plt.show() # Temporarily change the color palette and create a relplot with sns.color_palette(\\"blend:#7AB,#EDA\\", as_cmap=False): plt.figure(figsize=(8, 5)) sns.relplot(x=\'category\', y=\'value\', data=data, s=100) plt.title(\\"Relational Plot with Custom Blend Palette\\") plt.show()"},{"question":"Objective The objective of this assessment is to evaluate your understanding of accessing and manipulating the `__annotations__` attribute in Python objects, focusing on the best practices for different Python versions. Problem Statement Write a function `get_processed_annotations(obj: Any) -> Dict[str, Any]` that retrieves the annotations of the given object `obj` and processes them as follows: 1. If the annotation is \\"stringized\\", it should be un-stringized using the `eval` function. 2. Return a dictionary containing the processed annotations. The function should handle differences between Python 3.10+ and older versions, following the best practices described in the provided documentation. Specifically: - Use `inspect.get_annotations()` in Python 3.10 and newer if possible. - For Python 3.9 and older, handle class annotations carefully to avoid inheriting annotations from base classes unintentionally. - Use `getattr()` with three arguments to safely access `__annotations__` for potentially unknown objects. Input - `obj`: The object whose annotations are to be retrieved. This can be a function, class, module, or any callable. Output - A dictionary with annotation names as keys and their evaluated (un-stringized) types as values. Example For the function: ```python from typing import Any, Dict def example_function(x: \'int\', y: \'str\') -> \'bool\': pass ``` Calling `get_processed_annotations(example_function)` should return: ```python {\'x\': int, \'y\': str, \'return\': bool} ``` Constraints - The function should handle both Python 3.10 and newer, as well as older versions of Python. - Ensure that all stringized annotations are evaluated safely, considering the context (globals and locals) of the object. Notes - You may assume that the input object will always be of a type that can have annotations (functions, classes, modules, callables). - To simulate different Python versions during testing, you may use conditional logic to choose between Python 3.10+ and older version handling. Hint You may find the `sys` and `inspect` modules helpful for this task. ```python import inspect import sys from typing import Any, Dict def get_processed_annotations(obj: Any) -> Dict[str, Any]: # Your implementation here ``` > **Note**: For un-stringizing, you need to consider the type of the object (module, class, wrapped callable, or other callable) to determine the correct globals and locals for `eval`.","solution":"import inspect import sys from typing import Any, Dict def get_processed_annotations(obj: Any) -> Dict[str, Any]: if sys.version_info >= (3, 10): annotations = inspect.get_annotations(obj) else: if isinstance(obj, type): annotations = {} for cls in obj.__mro__[:-1]: # exclude \'object\' annotations.update(getattr(cls, \'__annotations__\', {})) else: annotations = getattr(obj, \'__annotations__\', {}) processed_annotations = {} for key, value in annotations.items(): if isinstance(value, str): processed_annotations[key] = eval(value, obj.__globals__ if hasattr(obj, \'__globals__\') else {}, {}) else: processed_annotations[key] = value return processed_annotations"},{"question":"Objective: Demonstrate an understanding of Python\'s `multiprocessing.shared_memory` module by designing and implementing a function that utilizes shared memory to efficiently share and modify data across multiple processes. Problem Statement: You are required to write a function `process_shared_memory_operations` that achieves the following: 1. Creates a shared memory block of a specified size. 2. Initializes a `ShareableList` in this shared memory block with a given list of integers. 3. Spawns multiple processes, each of which modifies a segment of the list. 4. Each process updates its segment by applying a specified operation (e.g., incrementing each value by a given number). 5. After all processes complete their operations, the main process (parent) consolidates and returns the modified list. 6. Ensures proper cleanup of the shared memory resources. Function Signature: ```python def process_shared_memory_operations(initial_list: list, num_processes: int, increment: int) -> list: pass ``` Input: - `initial_list` (list of int): A list of integers to be shared across processes. - `num_processes` (int): The number of child processes to spawn. Each process should handle approximately `len(initial_list) // num_processes` elements. - `increment` (int): The value by which each integer in the list should be incremented by its corresponding process. Output: - Returns the modified list after all processes have completed their operations. Constraints: - Each process should handle a disjoint segment of the list. - Ensure robust error handling (e.g., if the `num_processes` is greater than the length of the list). - Efficiently manage shared memory and ensure no memory leaks. Example: ```python initial_list = [1, 2, 3, 4, 5, 6, 7, 8] num_processes = 4 increment = 2 # Expected Output: # The initial list [1, 2, 3, 4, 5, 6, 7, 8] should become [3, 4, 5, 6, 7, 8, 9, 10] # Each process incrementing its segment by 2. result = process_shared_memory_operations(initial_list, num_processes, increment) print(result) # Output should be [3, 4, 5, 6, 7, 8, 9, 10] ``` Notes: - Use the `SharedMemory` and `ShareableList` classes for handling shared memory. - Ensure all processes modify their designated segments concurrently and independently. - Properly close and unlink shared memory blocks after their use to avoid memory leaks. Performance Requirements: - The solution should be efficient in terms of memory usage. - Any process must not spend significant idle time waiting on other processes (i.e., ensure minimal synchronization overhead).","solution":"from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory from multiprocessing.managers import SharedMemoryManager import math def increment_segment(shareable_list, start, end, increment): for i in range(start, end): shareable_list[i] += increment def process_shared_memory_operations(initial_list, num_processes, increment): if not initial_list: return initial_list length = len(initial_list) chunk_size = math.ceil(length / num_processes) with SharedMemoryManager() as smm: # Create a ShareableList in shared memory shareable_list = smm.ShareableList(initial_list) processes = [] for i in range(num_processes): start = i * chunk_size end = min(start + chunk_size, length) if start >= length: # No more segments to process break p = Process(target=increment_segment, args=(shareable_list, start, end, increment)) processes.append(p) p.start() for p in processes: p.join() result = list(shareable_list) return result"},{"question":"**Question: HMAC Message Authentication** You are tasked with creating a secure verification system for messages using the HMAC (Keyed-Hashing for Message Authentication) algorithm provided by the Python `hmac` module. Your goal is to implement several functions to handle the creation, updating, and verification of HMAC digests. You need to implement the following functions: 1. `create_hmac(key: bytes, msg: bytes, digestmod: str) -> hmac.HMAC`: - **Input**: - `key`: A secret key as a byte string. - `msg`: An initial message to authenticate, also as a byte string (optional; could be empty). - `digestmod`: The name of the hash algorithm to use (e.g. \'sha256\'). - **Output**: - Returns an initialized HMAC object. 2. `update_hmac(h: hmac.HMAC, msg: bytes) -> None`: - **Input**: - `h`: An existing HMAC object. - `msg`: A message to update the HMAC object with, as a byte string. - **Output**: - None. The function should update the HMAC object in place. 3. `get_hexdigest(h: hmac.HMAC) -> str`: - **Input**: - `h`: An existing HMAC object. - **Output**: - Returns the hexadecimal digest of the messages updated so far. 4. `compare_hmacs(digest_a: str, digest_b: str) -> bool`: - **Input**: - `digest_a`: The first hex digest string to compare. - `digest_b`: The second hex digest string to compare. - **Output**: - Returns `True` if both digests are equal, `False` otherwise. 5. `verify_message(key: bytes, msg: bytes, expected_digest: str, digestmod: str) -> bool`: - **Input**: - `key`: A secret key as a byte string. - `msg`: A message as a byte string. - `expected_digest`: The expected hex digest to verify. - `digestmod`: The name of the hash algorithm to use (e.g. \'sha256\'). - **Output**: - Returns `True` if the computed digest matches the expected digest, `False` otherwise. # Constraints - The `key` and `msg` inputs must be non-empty byte strings. - The `digestmod` must be a valid name of a hash algorithm supported by the `hashlib` module. # Example Usage ```python key = b\'secret_key\' msg = b\'hello\' digestmod = \'sha256\' # Create HMAC object hmac_obj = create_hmac(key, msg, digestmod) # Update HMAC object with additional message update_hmac(hmac_obj, b\' world\') # Get the hexadecimal digest hex_digest = get_hexdigest(hmac_obj) # Compare digests is_equal = compare_hmacs(hex_digest, \'expected_hex_digest\') # Verify message is_verified = verify_message(key, msg + b\' world\', hex_digest, digestmod) ``` Ensure your implementation properly handles the creation, updating, and verification of HMAC objects and that the digests are compared in a secure manner.","solution":"import hmac import hashlib def create_hmac(key: bytes, msg: bytes, digestmod: str) -> hmac.HMAC: Creates and returns an HMAC object initialized with the given key, message, and hash algorithm. return hmac.new(key, msg, digestmod=getattr(hashlib, digestmod)) def update_hmac(h: hmac.HMAC, msg: bytes) -> None: Updates the HMAC object with the given message. h.update(msg) def get_hexdigest(h: hmac.HMAC) -> str: Returns the hexadecimal digest of the HMAC object. return h.hexdigest() def compare_hmacs(digest_a: str, digest_b: str) -> bool: Compares two hexadecimal digests in constant time. return hmac.compare_digest(digest_a, digest_b) def verify_message(key: bytes, msg: bytes, expected_digest: str, digestmod: str) -> bool: Verifies whether the HMAC of the message with the given key and digestmod matches the expected digest. h = create_hmac(key, msg, digestmod) actual_digest = get_hexdigest(h) return compare_hmacs(actual_digest, expected_digest)"},{"question":"# Semi-Supervised Learning Task with Scikit-Learn You are given a partially labeled dataset and your task is to implement semi-supervised learning using scikit-learn\'s semi-supervised learning methods. The implementation requires you to label the unlabeled data points and evaluate the performance of your model. Dataset - The dataset contains 2 features and a label. Some labels are missing and are denoted by `-1`. - You need to separate input features and labels, then fit a semi-supervised learning model to predict the missing labels. Instructions 1. **Pre-process the Data**: - Load the data provided in the form of a numpy array. - Separate the labeled from unlabeled data. Assume that the label `-1` indicates unlabeled data. 2. **Choose and Implement a Semi-Supervised Learning Model**: - You are required to use both `SelfTrainingClassifier` and `LabelPropagation` methodologies from `sklearn.semi_supervised`. - Configure the `SelfTrainingClassifier` with an appropriate base estimator that supports probability predictions (e.g., `DecisionTreeClassifier`). 3. **Training and Prediction**: - Train both models on the dataset. - For `SelfTrainingClassifier`, use a threshold selection criterion with a threshold of 0.75. - For `LabelPropagation`, use the RBF kernel with gamma set to 20. 4. **Evaluate the performance**: - Predict the labels for the unlabeled data. - Calculate and print the accuracy of the model on the originally labeled data points for both methods. 5. **Comparison**: - Compare the performance of the two methods and discuss which performed better and why. Function Signature ```python import numpy as np from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score def semi_supervised_learning(data: np.ndarray) -> None: Function to label the unlabeled data points and evaluate the performance using sklearn\'s semi-supervised learning methods - SelfTrainingClassifier and LabelPropagation. Args: data (np.ndarray): A 2D numpy array where each row represents a sample with the last element as the label. Use -1 to denote unlabeled samples. Returns: None: The function should print the accuracy of both models on initially labeled data points. pass ``` Example ```python data = np.array([ [1.0, 2.0, 1], [1.5, 1.8, 1], [2.0, 2.0, -1], [2.5, 1.5, 0], [3.0, 2.2, 0], [2.9, 2.8, 1], [3.2, 3.0, -1] ]) semi_supervised_learning(data) ``` When the function `semi_supervised_learning` is called with the above data, it should print out the accuracy of both `SelfTrainingClassifier` and `LabelPropagation` models on the initially labeled samples.","solution":"import numpy as np from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score def semi_supervised_learning(data: np.ndarray) -> None: # Separate input features and labels X = data[:, :-1] y = data[:, -1] # Identify labeled and unlabeled points labeled_indices = y != -1 unlabeled_indices = y == -1 # Extract labeled data for evaluation purposes X_labeled = X[labeled_indices] y_labeled = y[labeled_indices] # Initialize and configure SelfTrainingClassifier with DecisionTreeClassifier as the base estimator base_estimator = DecisionTreeClassifier() self_training_model = SelfTrainingClassifier(base_estimator, criterion=\'threshold\', threshold=0.75) # Train the SelfTrainingClassifier model self_training_model.fit(X, y) y_self_training_pred = self_training_model.predict(X_labeled) # Calculate the accuracy for SelfTrainingClassifier on labeled data self_training_accuracy = accuracy_score(y_labeled, y_self_training_pred) print(f\\"SelfTrainingClassifier Accuracy: {self_training_accuracy:.2f}\\") # Initialize and configure LabelPropagation model label_propagation_model = LabelPropagation(kernel=\'rbf\', gamma=20) # Train the LabelPropagation model label_propagation_model.fit(X, y) y_label_propagation_pred = label_propagation_model.predict(X_labeled) # Calculate the accuracy for LabelPropagation on labeled data label_propagation_accuracy = accuracy_score(y_labeled, y_label_propagation_pred) print(f\\"LabelPropagation Accuracy: {label_propagation_accuracy:.2f}\\")"},{"question":"# Weak Reference Management in Python In this assessment, you are required to implement functionality using weak references in Python to efficiently manage memory and the object lifecycle. You will implement two classes: `WeakRefManager` and `TrackableObject`. Class: `TrackableObject` This class should represent any standard object which we want to track using weak references. The class should have the following: - An `__init__` method that initializes the object with a name. - A `__str__` method that returns the object\'s name. - Any necessary methods to support the weak reference management. Class: `WeakRefManager` This class will manage the weak references to instances of `TrackableObject`. It should provide the following functionality: - An `__init__` method that initializes an empty dictionary to store weak references. - A method `add_object(obj: TrackableObject, callback: Optional[Callable] = None) -> None` that creates a weak reference or proxy for the given `TrackableObject` and stores it in the dictionary. - A method `remove_object(name: str) -> None` that removes the weak reference of the object from the dictionary if it exists. - A method `get_object(name: str) -> Any` that retrieves the object from the weak reference if it is still available, or returns `None` if the referenced object has been garbage collected. Ensure the class also appropriately handles errors and edge cases (e.g., adding non-TrackableObject instances, handling already garbage collected objects etc.) Example Usage: ```python # Creating instances of TrackableObject obj1 = TrackableObject(\\"Object1\\") obj2 = TrackableObject(\\"Object2\\") # Adding objects to the manager manager = WeakRefManager() manager.add_object(obj1) manager.add_object(obj2) # Retrieving objects print(manager.get_object(\\"Object1\\")) # Should print the object\'s name. print(manager.get_object(\\"Object2\\")) # Should print the object\'s name. # Removing an object from the manager manager.remove_object(\\"Object1\\") print(manager.get_object(\\"Object1\\")) # Should print None as the reference is removed. ``` Constraints: - Handle common exceptions and edge cases gracefully. - Demonstrate usage through a few test cases in the main section of your code.","solution":"import weakref class TrackableObject: def __init__(self, name): self.name = name def __str__(self): return self.name class WeakRefManager: def __init__(self): self._refs = {} def add_object(self, obj, callback=None): if not isinstance(obj, TrackableObject): raise TypeError(\\"Only TrackableObject instances can be added.\\") self._refs[obj.name] = weakref.ref(obj, callback) def remove_object(self, name): if name in self._refs: del self._refs[name] def get_object(self, name): ref = self._refs.get(name) if ref is None: return None obj = ref() if obj is None: del self._refs[name] return obj"},{"question":"# Advanced Python Object Manipulation **Objective**: Implement a custom Python object managing system that simulates some aspects of Python/C API in pure Python. **Problem Statement**: You are tasked with creating a set of classes to represent objects and their types, managing attributes, and simulating reference counting. This will require implementing a simplified version of some functionalities described in the provided documentation. **Requirements**: 1. **BaseObject Class**: Create a class named `BaseObject` that will represent a base object. It should have: - An attribute `_refcnt` to keep track of reference count. - A class attribute `_type` to store the type of the object. - A method `get_refcnt()` to return the reference count. - A method `set_refcnt(count)` to set the reference count. - A method `get_type()` that returns the type of the object. - A method `set_type(new_type)` that sets the type of the object. 2. **VarObject Class**: Create a class named `VarObject` that inherits from `BaseObject` and represents objects with variable sizes. It should have: - An additional attribute `_size` to store the size of the object. - A method `get_size()` to return the size. - A method `set_size(size)` to set the size. 3. **TypeObject Class**: Create a class named `TypeObject` representing an object\'s type. It should have: - An attribute `name` to store the name of the type. - A method `__str__()` to return the name of the type. 4. **Object Attributes Management**: - Implement a system to add, get, and set attributes for objects dynamically. - Attributes should be stored in a dictionary `_attributes` in the `BaseObject` class. - Methods `set_attr(attr_name, value)` and `get_attr(attr_name)` should be provided in `BaseObject`. **Performance Requirements**: - The system should manage at least 100,000 objects efficiently in terms of both memory and time complexity. **Constraints**: - You are not allowed to use any external libraries for object management. - Ensure that the system handles edge cases such as setting invalid types or exceeding reference counts gracefully. **Implementation**: ```python class TypeObject: def __init__(self, name): self.name = name def __str__(self): return self.name class BaseObject: def __init__(self, obj_type): self._refcnt = 1 self._type = obj_type self._attributes = {} def get_refcnt(self): return self._refcnt def set_refcnt(self, count): self._refcnt = count def get_type(self): return self._type def set_type(self, new_type): self._type = new_type def set_attr(self, attr_name, value): self._attributes[attr_name] = value def get_attr(self, attr_name): return self._attributes.get(attr_name, None) class VarObject(BaseObject): def __init__(self, obj_type, size): super().__init__(obj_type) self._size = size def get_size(self): return self._size def set_size(self, size): self._size = size # Example usage: if __name__ == \\"__main__\\": string_type = TypeObject(\\"string\\") num_type = TypeObject(\\"number\\") # Creating BaseObject instances base_obj = BaseObject(string_type) print(base_obj.get_type()) # Output: string # Setting and Getting Attributes base_obj.set_attr(\\"value\\", \\"hello world\\") print(base_obj.get_attr(\\"value\\")) # Output: hello world # Creating VarObject instance var_obj = VarObject(num_type, 5) print(var_obj.get_size()) # Output: 5 var_obj.set_size(10) print(var_obj.get_size()) # Output: 10 ``` **Test Cases**: 1. Test creating objects of different types and verify their types. 2. Test setting and getting reference counts. 3. Test setting and retrieving attributes. 4. Test creating variable sized objects and managing their size.","solution":"class TypeObject: def __init__(self, name): self.name = name def __str__(self): return self.name class BaseObject: def __init__(self, obj_type): self._refcnt = 1 self._type = obj_type self._attributes = {} def get_refcnt(self): return self._refcnt def set_refcnt(self, count): self._refcnt = count def get_type(self): return self._type def set_type(self, new_type): self._type = new_type def set_attr(self, attr_name, value): self._attributes[attr_name] = value def get_attr(self, attr_name): return self._attributes.get(attr_name, None) class VarObject(BaseObject): def __init__(self, obj_type, size): super().__init__(obj_type) self._size = size def get_size(self): return self._size def set_size(self, size): self._size = size"},{"question":"# Audio Processing Pipeline with `audioop` Problem Statement You are required to implement an audio processing pipeline that transforms an input audio fragment through a series of operations, leveraging the `audioop` module. Your pipeline should: 1. Convert the linear PCM audio data to 8-bit a-LAW format. 2. Apply a bias of 128 to the 8-bit samples. 3. Reverse the byte order of the samples. 4. Convert the processed audio data back to linear PCM format (16-bit). 5. Calculate and return the Root Mean Square (RMS) of the final processed audio fragment. Function Signature ```python def process_audio(fragment: bytes, width: int) -> float: Process an audio fragment through a series of operations. Parameters: fragment (bytes): The input audio fragment. width (int): The sample width in bytes for the input fragment (1, 2, 3, or 4). Returns: float: The Root Mean Square (RMS) value of the processed audio fragment. pass ``` Input 1. `fragment`: A bytes-like object containing the raw audio data. 2. `width`: An integer indicating the sample width in bytes for the input fragment. Possible values are 1, 2, 3, or 4. Output - Return the RMS (Root Mean Square) of the final processed audio fragment, after all transformations are applied. Constraints - The input audio fragment will not be empty. - The `width` parameter will always be one of the allowed values (1, 2, 3, or 4). - You should handle exceptions or errors as outlined in the module\'s documentation. Example ```python input_fragment = b\'x01x02x03x04\' # Example bytes object width = 2 # Example width rms_value = process_audio(input_fragment, width) print(rms_value) ``` Notes - Ensure that intermediate steps of the pipeline are correctly transformed and handle the sample widths appropriately. - Utilize the functions provided by the `audioop` module to perform the required operations. - The RMS value should be a float that represents the power of the audio signal after processing. You are encouraged to refer to the `audioop` documentation to understand the functions you will need to use in this implementation. Good luck!","solution":"import audioop def process_audio(fragment: bytes, width: int) -> float: Process an audio fragment through a series of operations. Parameters: fragment (bytes): The input audio fragment. width (int): The sample width in bytes for the input fragment (1, 2, 3, or 4). Returns: float: The Root Mean Square (RMS) value of the processed audio fragment. # 1. Convert the linear PCM audio data to 8-bit a-LAW format. alaw_audio = audioop.lin2alaw(fragment, width) # 2. Apply a bias of 128 to the 8-bit samples. biased_audio = audioop.bias(alaw_audio, 1, 128) # 3. Reverse the byte order of the samples. reversed_audio = audioop.reverse(biased_audio, 1) # 4. Convert the processed audio data back to linear PCM format (16-bit). pcm_audio = audioop.alaw2lin(reversed_audio, 2) # 5. Calculate and return the Root Mean Square (RMS) of the final processed audio fragment. rms_value = audioop.rms(pcm_audio, 2) return float(rms_value)"},{"question":"**Coding Assessment Question: Visualizing Data with Seaborn `FacetGrid` and Custom Plotting Functions** As a data scientist, you have been given a dataset containing information about different species of penguins. Your task is to create a detailed visualization using Seaborn\'s `FacetGrid` to understand the relationships between various attributes of the penguins separately within different subsets of the data. # Dataset The dataset contains the following columns: - `species`: Species of the penguin (Adelie, Chinstrap, Gentoo) - `island`: Island where the penguin was observed - `bill_length_mm`: Length of the penguin\'s bill in millimeters - `bill_depth_mm`: Depth of the penguin\'s bill in millimeters - `flipper_length_mm`: Length of the penguin\'s flipper in millimeters - `body_mass_g`: Body mass of the penguin in grams - `sex`: Sex of the penguin (Male, Female) # Task 1. Load the dataset provided in CSV format. 2. Perform basic cleaning by removing rows with missing values. 3. Set up a `FacetGrid` to visualize the relationship between `bill_length_mm` and `bill_depth_mm` across different `species` and `island` categories. 4. Use a custom plotting function to visualize the data using a scatter plot where: - Points are colored based on the `sex` of the penguins. - Independently fit linear regression lines (`regplot`) for male and female penguins. 5. Customize the grid to: - Use different colors for each `sex` category. - Add legends, titles, and properly label axes. - Adjust the size and aspect ratio of the facets to enhance readability. # Input - A CSV file named `penguins.csv`. # Output - A multi-faceted plot showing the relationship between `bill_length_mm` and `bill_depth_mm` for each combination of `species` and `island` with separate regressions for male and female penguins. # Constraints - Use only Seaborn and Matplotlib for visualization. - Ensure code readability and proper documentation. # Performance Requirements - The resulting plot should be clear, well-labeled, and aesthetically pleasing. - The code should be efficient and run within a reasonable time frame on standard datasets. # Example The final plot should look similar to the following structure: ``` Island: Biscoe Island: Dream Species: Adelie Species: Chinstrap Species: Gentoo +-------------------+---------------------+---------------------+ | | | | | | | | | Plot1 | Plot2 | Plot3 | | | | | +-------------------+---------------------+---------------------+ | | | | | | | | | Plot4 | Plot5 | Plot6 | | | | | +-------------------+---------------------+---------------------+ ... ``` # Instructions 1. Load the following dataset: ```python import pandas as pd penguins = pd.read_csv(\'penguins.csv\') ``` 2. Clean the data by removing missing values: ```python penguins = penguins.dropna() ``` 3. Implement the remaining tasks to create the desired visualization.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def clean_data(filepath): Load the dataset from the given filepath and remove rows with missing values. penguins = pd.read_csv(filepath) penguins_cleaned = penguins.dropna() return penguins_cleaned def plot_penguin_data(penguins): Create a multi-faceted scatter plot of bill length vs bill depth for each species and island combination. Differentiate between male and female penguins using colors and fit separate regression lines. g = sns.FacetGrid(penguins, col=\\"species\\", row=\\"island\\", hue=\\"sex\\", margin_titles=True, height=4, aspect=1.5) g.map_dataframe(sns.scatterplot, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") g.map_dataframe(sns.regplot, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", scatter=False) g.add_legend() g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") plt.subplots_adjust(top=0.9) g.fig.suptitle(\\"Penguin Bill Dimensions by Species and Island\\") plt.show() # Example usage: # penguins_cleaned = clean_data(\'penguins.csv\') # plot_penguin_data(penguins_cleaned)"},{"question":"# Python Coding Assessment Question **Objective:** Implement a Python application using `asyncio` that processes multiple tasks concurrently while ensuring that no task blocks the event loop. The application must also handle logging and have debug mode enabled to help trace any issues during execution. Problem Statement You are required to implement an asynchronous function `process_data_concurrently(data_chunks: List[bytes]) -> List[Any]`. This function should: 1. Process a list of data chunks concurrently without blocking other tasks or I/O operations. 2. Each data chunk should be processed by another function `process_chunk(chunk: bytes) -> Any`, which simulates a CPU-bound or I/O-bound task. 3. Handle any potential exceptions and log them properly. 4. Ensure that all coroutines are awaited, and there are no unhandled exceptions or unawaited coroutines. 5. Enable asyncioâ€™s debug mode to help identify potential issues. Input and Output - **Input**: A list of bytes objects representing data chunks to be processed. - **Output**: A list of results obtained from processing each data chunk. Constraints - The `process_chunk` function should simulate a mix of CPU-bound tasks (e.g., time.sleep) and I/O-bound tasks (e.g., asyncio.sleep). - The asyncio event loop should be running in the main thread. - Use appropriate logging levels to capture information, warnings, and errors. - Ensure that `asyncio` debug mode is enabled. Example ```python import asyncio import logging async def process_chunk(chunk: bytes) -> Any: Simulate processing of a single chunk of data. Can be CPU-bound or I/O-bound. # Simulate CPU-bound operation await asyncio.sleep(1) # Simulate IO-bound task return f\\"Processed {len(chunk)} bytes\\" async def process_data_concurrently(data_chunks: List[bytes]) -> List[Any]: Process a list of data chunks concurrently without blocking I/O. logging.basicConfig(level=logging.DEBUG) # Enable asyncio debug mode asyncio.get_event_loop().set_debug(True) tasks = [asyncio.create_task(process_chunk(chunk)) for chunk in data_chunks] results = await asyncio.gather(*tasks, return_exceptions=True) return results # List of results or exceptions if any # Example Usage data = [b\'123\', b\'456\', b\'78910\'] results = asyncio.run(process_data_concurrently(data)) print(results) ``` Notes 1. You are required to implement the `process_data_concurrently` function, but you may use the provided `process_chunk` as is or modify it. 2. Ensure that the event loop is running in debug mode and proper logging is set up. 3. Correctly handle any exceptions that occur during task execution and log them properly.","solution":"import asyncio import logging from typing import List, Any async def process_chunk(chunk: bytes) -> Any: Simulate processing of a single chunk of data. Can be CPU-bound or I/O-bound. # Simulate CPU-bound operation await asyncio.sleep(1) # Simulate IO-bound task return f\\"Processed {len(chunk)} bytes\\" async def process_data_concurrently(data_chunks: List[bytes]) -> List[Any]: Process a list of data chunks concurrently without blocking other tasks or I/O operations. logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) # Enable asyncio debug mode loop = asyncio.get_event_loop() loop.set_debug(True) tasks = [asyncio.create_task(process_chunk(chunk)) for chunk in data_chunks] results = [] for task in asyncio.as_completed(tasks): try: result = await task results.append(result) except Exception as e: logger.error(f\\"Error processing chunk: {e}\\") results.append(e) return results # List of results or exceptions if any"},{"question":"**Question: Custom Mapping Implementation** Your task is to implement a custom mapping class by inhering from the `collections.abc.Mapping` abstract base class. The class should be named `CustomDict` and mimic some of the behaviors of the built-in `dict` class but with additional specific behaviors. # Requirements: 1. **Inherit from `collections.abc.Mapping`:** - Your class must be a subclass of `collections.abc.Mapping`. 2. **Required Methods:** - Implement the `__getitem__(self, key)` method. - Implement the `__iter__(self)` method. - Implement the `__len__(self)` method. 3. **Mixin Methods:** - Use the mixin methods provided by the `Mapping` ABC for methods like `__contains__`, `keys`, `items`, etc. 4. **Custom Method:** - Implement a custom method `def unique_values_count(self) -> int` that returns the number of unique values in the mapping. # Expected Input and Output Formats: - **Input:** Initialization of `CustomDict` objects can be done using a dictionary or a list of tuples. ```python d1 = CustomDict({\'a\': 1, \'b\': 2, \'c\': 1}) d2 = CustomDict([(\'x\', 10), (\'y\', 20), (\'z\', 20)]) ``` - **Output:** - Using the `__getitem__` method should allow accessing items like a dictionary. - The `__iter__` method should allow iterating over the keys. - The `__len__` method should return the number of items in the dictionary. - The custom method `unique_values_count()` should return the count of unique values. # Constraints: - The class should not use the built-in `dict` methods directly (other than for initialization in the constructor). - Your implementation should be efficient with a focus on performance for the required methods. # Example: ```python from collections.abc import Mapping class CustomDict(Mapping): def __init__(self, data): # Initialize with a dictionary or list of tuples self._store = dict(data) def __getitem__(self, key): return self._store[key] def __iter__(self): return iter(self._store) def __len__(self): return len(self._store) def unique_values_count(self): return len(set(self._store.values())) # Example Usage d1 = CustomDict({\'a\': 1, \'b\': 2, \'c\': 1}) print(d1[\'a\']) # Output: 1 print(d1.unique_values_count()) # Output: 2 d2 = CustomDict([(\'x\', 10), (\'y\', 20), (\'z\', 20)]) print(list(d2)) # Output: [\'x\', \'y\', \'z\'] print(len(d2)) # Output: 3 print(d2.unique_values_count()) # Output: 2 ``` The example illustrates the initialization, item access, iteration, length check, and the custom method functionality, ensuring clarity in class usage. Implement this class in full compliance with the instructions.","solution":"from collections.abc import Mapping class CustomDict(Mapping): def __init__(self, data): # Initialize with a dictionary or list of tuples self._store = dict(data) def __getitem__(self, key): return self._store[key] def __iter__(self): return iter(self._store) def __len__(self): return len(self._store) def unique_values_count(self): return len(set(self._store.values())) # Example Usage d1 = CustomDict({\'a\': 1, \'b\': 2, \'c\': 1}) print(d1[\'a\']) # Output: 1 print(d1.unique_values_count()) # Output: 2 d2 = CustomDict([(\'x\', 10), (\'y\', 20), (\'z\', 20)]) print(list(d2)) # Output: [\'x\', \'y\', \'z\'] print(len(d2)) # Output: 3 print(d2.unique_values_count()) # Output: 2"},{"question":"Objective: Implement a Python class that simulates a simplified dictionary object using the Python C API functions provided in the documentation. Task: Write a Python class named `CustomDict` that mimics the behavior of a basic dictionary. This class should: 1. Allow setting, getting, and deleting key-value pairs. 2. Allow checking for the existence of a key. 3. Support iteration over keys. 4. Provide a method to print a string representation of the dictionary. 5. Allow comparison with other `CustomDict` instances for equality and inequality. Input and Output Formats: 1. **`CustomDict` Constructor:** - **Input**: No arguments required. - **Output**: Initializes an empty dictionary-like object. 2. **Setting Items:** - **Method**: `set_item(key, value)` - **Input**: `key` (str), `value` (any type) - **Output**: None 3. **Getting Items:** - **Method**: `get_item(key)` - **Input**: `key` (str) - **Output**: Returns the value associated with the key if it exists, otherwise raises a `KeyError`. 4. **Deleting Items:** - **Method**: `del_item(key)` - **Input**: `key` (str) - **Output**: None (raises `KeyError` if the key does not exist) 5. **Checking for Key Existence:** - **Method**: `has_key(key)` - **Input**: `key` (str) - **Output**: Returns `True` if the key exists, `False` otherwise. 6. **String Representation:** - **Method**: `__repr__()` - **Output**: Returns a string representation of the dictionary. 7. **Equality Comparison:** - **Method**: `__eq__(other)` - **Input**: `other` (instance of `CustomDict`) - **Output**: Returns `True` if both instances have the same key-value pairs, `False` otherwise. 8. **Inequality Comparison:** - **Method**: `__ne__(other)` - **Input**: `other` (instance of `CustomDict`) - **Output**: Returns `True` if the instances do not have the same key-value pairs, `False` otherwise. 9. **Iteration:** - **Method**: `__iter__()` - **Output**: Returns an iterator over the keys in the dictionary. Example Usage: ```python d = CustomDict() d.set_item(\\"name\\", \\"John\\") print(d.get_item(\\"name\\")) # Output: John d.set_item(\\"age\\", 30) print(d.has_key(\\"age\\")) # Output: True print(d) # Output: {\'name\': \'John\', \'age\': 30} d.del_item(\\"name\\") print(d) # Output: {\'age\': 30} d2 = CustomDict() d2.set_item(\\"age\\", 30) print(d == d2) # Output: True print(d != d2) # Output: False for key in d: print(key) # Output: age ``` Constraints: 1. Implement the class using the provided object protocol functions where necessary. 2. Ensure that your implementation does not use Python\'s in-built dictionary but rather simulates the behavior using the appropriate function calls.","solution":"class CustomDict: def __init__(self): self._data = {} def set_item(self, key, value): self._data[key] = value def get_item(self, key): if key in self._data: return self._data[key] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def del_item(self, key): if key in self._data: del self._data[key] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def has_key(self, key): return key in self._data def __repr__(self): return str(self._data) def __eq__(self, other): if isinstance(other, CustomDict): return self._data == other._data return False def __ne__(self, other): return not self.__eq__(other) def __iter__(self): return iter(self._data)"},{"question":"Problem Statement You have been given a dataset containing features and corresponding multiple numerical target values. Your task is to implement a multi-output regression model using scikit-learn to predict these numerical properties. # Dataset - Assume you have access to a CSV file named `data.csv` containing the dataset. - The file contains `n` columns: `feature1`, `feature2`, ..., `feature_m`, and `target1`, `target2`, ..., `target_k`. # Objectives Write a Python function `multi_output_regression` that performs the following steps: 1. Load the dataset from the CSV file. 2. Split the dataset into training and testing sets (80% training, 20% testing). 3. Train a `MultiOutputRegressor` using `GradientBoostingRegressor` on the training data. 4. Predict the target values on the testing data. 5. Evaluate the model using the Mean Squared Error (MSE) for each target. 6. Return the MSE values for each target. # Function Signature ```python def multi_output_regression(file_path: str) -> Dict[str, float]: pass ``` # Input - `file_path` (str): The path to the CSV file containing the dataset. # Output - A dictionary where the keys are the target column names and the values are the Mean Squared Error (MSE) values for each target. # Constraints - Use `MultiOutputRegressor` with `GradientBoostingRegressor` as the base estimator. - Make sure to handle any missing values in the dataset appropriately. - Use a random seed of 42 for reproducibility when splitting the data. # Example ```python file_path = \'data.csv\' mse_values = multi_output_regression(file_path) print(mse_values) # Output: {\'target1\': 0.123, \'target2\': 0.456, \'target3\': 0.789} ``` # Notes - You may assume that the dataset file will always be provided with the correct format. - Focus on code structure and clarity, along with proper use of scikit-learn functionalities.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.multioutput import MultiOutputRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error from typing import Dict def multi_output_regression(file_path: str) -> Dict[str, float]: # Load the dataset data = pd.read_csv(file_path) # Separate features and targets X = data.iloc[:, :-3] y = data.iloc[:, -3:] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the model model = MultiOutputRegressor(GradientBoostingRegressor(random_state=42)) # Train the model model.fit(X_train, y_train) # Predict on the testing data y_pred = model.predict(X_test) # Calculate Mean Squared Errors for each target mse_values = {f\'target{i+1}\': mean_squared_error(y_test.iloc[:, i], y_pred[:, i]) for i in range(y.shape[1])} return mse_values"},{"question":"Objective: The objective is to assess the student\'s ability to work with missing values in pandas DataFrames, demonstrating comprehension of detecting and handling missing values. Problem Statement: Given a DataFrame, you are required to implement a function `process_missing_values(df: pd.DataFrame) -> pd.DataFrame` that processes missing values in the following ways: 1. For each numeric column (`int64`, `float64`), replace missing values with the median of that column. 2. For each string column (`object`), replace missing values with the most frequent value (mode) of that column. 3. For datetime columns (`datetime64[ns]`), replace missing values with the earliest date. 4. Return the modified DataFrame. The function should handle all types of missing values (`NA` and `NaT`). # Input: - `df`: A pandas DataFrame containing numeric, string, and datetime columns with potential missing values. # Output: - A pandas DataFrame with missing values replaced as per the rules stated. # Constraints: - The DataFrame can have any number of columns and rows. - Assume that for each column with missing values, there is at least one non-missing value. # Example: ```python import pandas as pd import numpy as np data = { \'A\': [1, 2, np.nan, 4, 5], \'B\': [\'a\', np.nan, \'b\', \'b\', np.nan], \'C\': [pd.Timestamp(\'2022-01-01\'), pd.NaT, pd.Timestamp(\'2022-01-03\'), pd.Timestamp(\'2022-01-04\'), pd.NaT] } df = pd.DataFrame(data) processed_df = process_missing_values(df) print(processed_df) ``` Expected output: ``` A B C 0 1.0 a 2022-01-01 1 2.0 b 2022-01-01 2 3.0 b 2022-01-03 3 4.0 b 2022-01-04 4 5.0 b 2022-01-01 ``` Note: - Use pandas functions such as `pd.DataFrame.median()`, `pd.Series.mode()`, and `pd.Series.min()` to facilitate the processing of the missing values.","solution":"import pandas as pd def process_missing_values(df: pd.DataFrame) -> pd.DataFrame: Processes missing values in the DataFrame. 1. For each numeric column (`int64`, `float64`), replace missing values with the median of that column. 2. For each string column (`object`), replace missing values with the most frequent value (mode) of that column. 3. For datetime columns (`datetime64[ns]`), replace missing values with the earliest date. Parameters: df (pd.DataFrame): The input DataFrame with potential missing values. Returns: pd.DataFrame: DataFrame with missing values replaced. for column in df.columns: if df[column].dtype == \'int64\' or df[column].dtype == \'float64\': median_value = df[column].median() df[column].fillna(median_value, inplace=True) elif df[column].dtype == \'object\': mode_value = df[column].mode()[0] df[column].fillna(mode_value, inplace=True) elif df[column].dtype == \'datetime64[ns]\': min_value = df[column].min() df[column].fillna(min_value, inplace=True) return df"},{"question":"# Comprehensive Data Class Implementation **Problem Statement:** You are required to implement a Python class using the `dataclasses` module to represent a `Book`. The `Book` should include the following attributes: - `title` (string) - `author` (string) - `year` (integer) - `genres` (list of strings) - `isbn` (string, which should be unique for each book) - `in_stock` (boolean, defaults to `True`) - `number_of_copies` (integer, defaults to `1`) Moreover, add the following functionalities and constraints: 1. Once a `Book` object is created, its `isbn` cannot be changed (immutable field). 2. Provide a method `add_genre` which allows adding a genre to the `genres` list. 3. Ensure `genres` list cannot be modified directly, i.e., it should only be modifiable using the `add_genre` method. 4. The `number_of_copies` should always be non-negative. If an attempt is made to set it to a negative value, raise a custom exception `InvalidNumberOfCopiesError`. 5. Include a method `display_info` which prints the book\'s details in a readable format. **Input format:** - There is no direct input/output for your class implementation. You are supposed to implement the class and its methods as specified. **Output format:** - There is no direct output. **Your class should enable the following actions:** ```python from dataclasses import dataclass, field from typing import List from custom_exceptions import InvalidNumberOfCopiesError @dataclass class Book: title: str author: str year: int isbn: str genres: List[str] = field(default_factory=list) in_stock: bool = True number_of_copies: int = 1 def display_info(self): # Code to print book details def add_genre(self, genre: str): # Code to add genre to list def set_number_of_copies(self, copies: int): # Code to set number of copies with validation # Custom exception definition class InvalidNumberOfCopiesError(Exception): pass ``` **Constraints:** - Do not use external libraries other than `dataclasses`, `typing`, and custom exceptions. - Ensure the `isbn` field is immutable. - The list `genres` should only be modifiable through the `add_genre` method. - Proper exception handling for `InvalidNumberOfCopiesError`. **Usage Example:** ```python # Creating a new book instance book = Book(title=\\"1984\\", author=\\"George Orwell\\", year=1949, isbn=\\"1234567890\\") # Adding genres book.add_genre(\\"Dystopian\\") book.add_genre(\\"Political Fiction\\") # Trying to modify number of copies try: book.set_number_of_copies(5) # Valid case book.set_number_of_copies(-3) # This should raise InvalidNumberOfCopiesError except InvalidNumberOfCopiesError as e: print(f\\"Error: {e}\\") # Displaying book info book.display_info() ``` Implement the `Book` class and the `InvalidNumberOfCopiesError` exception according to the above specifications.","solution":"from dataclasses import dataclass, field from typing import List class InvalidNumberOfCopiesError(Exception): Exception raised for invalid number of copies. pass @dataclass class Book: title: str author: str year: int isbn: str genres: List[str] = field(default_factory=list) in_stock: bool = True _number_of_copies: int = 1 def __post_init__(self): if self._number_of_copies < 0: raise InvalidNumberOfCopiesError(\\"Number of copies cannot be negative.\\") def add_genre(self, genre: str): self.genres.append(genre) @property def number_of_copies(self): return self._number_of_copies @number_of_copies.setter def number_of_copies(self, value: int): if value < 0: raise InvalidNumberOfCopiesError(\\"Number of copies cannot be negative.\\") self._number_of_copies = value def display_info(self): info = ( f\\"Title: {self.title}n\\" f\\"Author: {self.author}n\\" f\\"Year: {self.year}n\\" f\\"ISBN: {self.isbn}n\\" f\\"Genres: {\', \'.join(self.genres)}n\\" f\\"In Stock: {\'Yes\' if self.in_stock else \'No\'}n\\" f\\"Number of Copies: {self._number_of_copies}\\" ) print(info)"},{"question":"You are tasked with developing a simulation for a simplified version of a casino game called \\"Random Roulette\\". The game has the following rules: 1. There is a spinning wheel with 38 slots: 18 red, 18 black, and 2 green (European-style). 2. A player bets on a color (red or black). If the ball lands on the chosen color, they win the amount they bet. If it lands on green, they lose their bet (to the house). 3. Each round, the player bets a random amount between 1 and 100. Design a function to simulate 1,000 rounds of the game. # Detailed Steps: 1. Generate the outcomes of spinning the wheel using `random.choices()`. - The wheel should be simulated as a sequence `[\'red\'] * 18 + [\'black\'] * 18 + [\'green\'] * 2`. 2. For each round, randomly select the player\'s bet amount using `random.randint()`. 3. Randomly decide the player\'s choice of color (`\'red\'` or `\'black\'`) using `random.choice()`. 4. Track the amount won or lost in each round. 5. Return the total net winnings of the player after 1,000 rounds (negative values indicate a loss). # Function Signature: ```python import random def simulate_roulette(rounds: int = 1000) -> int: Simulate the Random Roulette game and calculate the player\'s net winnings after a given number of rounds. Parameters: - rounds (int): Number of rounds to simulate. Default is 1000. Returns: - int: The player\'s net winnings after the given rounds. pass ``` # Input: - `rounds` (int): Number of rounds to simulate (default is 1,000). # Output: - (int): The player\'s net winnings after the specified rounds. # Constraints: - The outcome of each spin should be determined using the `random.choices()` method. - The bet amount should be between 1 and 100, inclusive. - The player\'s choice of color should be randomly selected for each round using `random.choice()`. # Performance Requirements: - Ensure the function efficiently handles the default number of 1,000 rounds within a reasonable time frame. # Example: ```python # Example of function usage result = simulate_roulette(1000) print(f\\"Player\'s net winnings after 1,000 rounds: {result}\\") ``` # Note: - You must only use functions from Python\'s `random` module to generate random numbers and outcomes.","solution":"import random def simulate_roulette(rounds: int = 1000) -> int: Simulate the Random Roulette game and calculate the player\'s net winnings after a given number of rounds. Parameters: - rounds (int): Number of rounds to simulate. Default is 1000. Returns: - int: The player\'s net winnings after the given rounds. wheel = [\'red\'] * 18 + [\'black\'] * 18 + [\'green\'] * 2 net_winnings = 0 for _ in range(rounds): bet_amount = random.randint(1, 100) player_choice = random.choice([\'red\', \'black\']) spin_result = random.choice(wheel) if spin_result == player_choice: net_winnings += bet_amount elif spin_result == \'green\': net_winnings -= bet_amount else: net_winnings -= bet_amount return net_winnings"},{"question":"# Asyncio Task Management and Timeout Handling You are tasked with building a simple simulation of a data processing system using `asyncio`. Your goal is to create multiple concurrent tasks that simulate data fetching and processing. You need to demonstrate your understanding of asyncio by implementing functions to: 1. Fetch data concurrently from multiple sources. 2. Process the fetched data. 3. Handle timeouts to ensure tasks do not run indefinitely. 4. Aggregate results from all tasks. Requirements 1. **Function: `fetch_data`** - **Input**: An integer `id` and an integer `delay`. - **Behavior**: Simulate fetching data by awaiting `asyncio.sleep(delay)`. After the delay, return a string `\\"Data from source {id}\\"`. - **Output**: A string representing the fetched data. 2. **Function: `process_data`** - **Input**: A string `data`. - **Behavior**: Simulate data processing by awaiting `asyncio.sleep(2)` and then returning a processed string `\\"Processed {data}\\"`. - **Output**: A string representing the processed data. 3. **Function: `main`** - **Input**: A list of tuples, where each tuple contains two integers `(id, delay)` representing data sources and their respective fetch delays, and an integer `timeout` representing the maximum time to wait for all tasks to complete. - **Behavior**: - Create and run tasks for fetching and processing data concurrently using `asyncio.gather`. - Ensure that the entire process respects the given `timeout` using `asyncio.wait_for`. - If the timeout is reached, cancel any remaining tasks. - **Output**: A list of strings representing the processed data for all sources. If a timeout occurs, return the string `\\"Timeout Error\\"`. Example Usage ```python import asyncio async def fetch_data(id, delay): await asyncio.sleep(delay) return f\\"Data from source {id}\\" async def process_data(data): await asyncio.sleep(2) return f\\"Processed {data}\\" async def main(sources, timeout): tasks = [asyncio.create_task(fetch_and_process(id, delay)) for id, delay in sources] try: results = await asyncio.wait_for(asyncio.gather(*tasks), timeout=timeout) except asyncio.TimeoutError: for task in tasks: task.cancel() return \\"Timeout Error\\" return results async def fetch_and_process(id, delay): data = await fetch_data(id, delay) processed = await process_data(data) return processed if __name__ == \\"__main__\\": sources = [(1, 1), (2, 3), (3, 5)] timeout = 10 result = asyncio.run(main(sources, timeout)) print(result) # Example output: [\\"Processed Data from source 1\\", \\"Processed Data from source 2\\", \\"Processed Data from source 3\\"] ``` Notes - To simulate long-running tasks, you can adjust delays and timeouts accordingly. - Ensure to handle potential cancellation of tasks gracefully to avoid any unexpected behavior.","solution":"import asyncio async def fetch_data(id, delay): Simulate fetching data by awaiting asyncio.sleep(delay). await asyncio.sleep(delay) return f\\"Data from source {id}\\" async def process_data(data): Simulate data processing by awaiting asyncio.sleep(2). await asyncio.sleep(2) return f\\"Processed {data}\\" async def fetch_and_process(id, delay): Fetches data and then processes it. data = await fetch_data(id, delay) processed = await process_data(data) return processed async def main(sources, timeout): Create and run tasks for fetching and processing data concurrently. Ensure that the entire process respects the given timeout using asyncio.wait_for. tasks = [asyncio.create_task(fetch_and_process(id, delay)) for id, delay in sources] try: results = await asyncio.wait_for(asyncio.gather(*tasks), timeout=timeout) except asyncio.TimeoutError: for task in tasks: task.cancel() return \\"Timeout Error\\" return results"},{"question":"# Objective You are required to implement a set of numeric operations using the provided \\"python310\\" package\'s functions. You will create a class named `NumberOperations` that provides methods for addition, subtraction, multiplication, division, negation, and bitwise operations. # Task Class Design Create a class named `NumberOperations`. This class should have the following methods: 1. **add(self, o1, o2)** - This method takes two arguments and returns their addition using `PyNumber_Add`. - Input: Two numeric values. - Output: Sum of the two values. 2. **subtract(self, o1, o2)** - This method takes two arguments and returns their subtraction using `PyNumber_Subtract`. - Input: Two numeric values. - Output: Result of subtracting the second value from the first value. 3. **multiply(self, o1, o2)** - This method takes two arguments and returns their multiplication using `PyNumber_Multiply`. - Input: Two numeric values. - Output: Product of the two values. 4. **divide(self, o1, o2)** - This method takes two arguments and returns their true division using `PyNumber_TrueDivide`. - Input: Two numeric values. - Output: Result of dividing the first value by the second value. 5. **negate(self, o)** - This method takes one argument and returns its negation using `PyNumber_Negative`. - Input: A numeric value. - Output: Negated value. 6. **bitwise_and(self, o1, o2)** - This method takes two arguments and returns their bitwise AND using `PyNumber_And`. - Input: Two integer values. - Output: Result of bitwise AND operation. 7. **bitwise_or(self, o1, o2)** - This method takes two arguments and returns their bitwise OR using `PyNumber_Or`. - Input: Two integer values. - Output: Result of bitwise OR operation. # Constraints - You can assume that all inputs are numeric and valid for the operations. - Handle any potential errors gracefully and return appropriate messages. # Example Usage ```python n_ops = NumberOperations() # Arithmetic Operations print(n_ops.add(5, 3)) # Output: 8 print(n_ops.subtract(5, 3)) # Output: 2 print(n_ops.multiply(5, 3)) # Output: 15 print(n_ops.divide(5, 3)) # Output: 1.66667 # Negation print(n_ops.negate(5)) # Output: -5 # Bitwise Operations print(n_ops.bitwise_and(5, 3)) # Output: 1 print(n_ops.bitwise_or(5, 3)) # Output: 7 ``` Implement the `NumberOperations` class and ensure your solution meets the requirements.","solution":"import operator class NumberOperations: def add(self, o1, o2): return operator.add(o1, o2) def subtract(self, o1, o2): return operator.sub(o1, o2) def multiply(self, o1, o2): return operator.mul(o1, o2) def divide(self, o1, o2): if o2 == 0: return \\"Error: Division by zero\\" return operator.truediv(o1, o2) def negate(self, o): return operator.neg(o) def bitwise_and(self, o1, o2): return operator.and_(o1, o2) def bitwise_or(self, o1, o2): return operator.or_(o1, o2)"},{"question":"# Question: Implement a Custom Mail Sorting System **Objective**: Implement a Python class `MailSorter` that reads messages from a Maildir mailbox, sorts them into separate mbox mailboxes based on their sender\'s email domain, and handles various mailbox operations like adding, removing, and error handling. **Requirements**: 1. The `MailSorter` class should be initialized with the path to the Maildir mailbox. 2. Implement a method `sort_mail_by_domain()` that performs the following: - Reads all messages from the Maildir mailbox. - Sorts and moves these messages into separate mbox mailboxes based on the sender\'s email domain. - The mbox mailboxes should be named after the respective domain (e.g., messages from `example.com` should be moved to `example_com.mbox`). - Ensure proper locking and unlocking of mailboxes to avoid data corruption. - Handle any message parsing errors gracefully without terminating the operation. 3. Implement a method `stats()` that returns a dictionary with the domain names as keys and the number of messages sorted into each corresponding mbox mailbox. This should help in validating your sorting logic. **Notes**: - Use the classes and methods provided in the mailbox module. - Use exception handling to manage errors related to message parsing and mailbox manipulation. - Assume that mailbox paths and other required directories already exist. **Constraints**: - Your solution should ensure that no messages are lost or corrupted. - Efficiency is important, but correctness and handling of all edge cases are more critical. **Example**: ```python sorter = MailSorter(\'/path/to/maildir\') sorter.sort_mail_by_domain() statistics = sorter.stats() print(statistics) # Example output: {\'example_com\': 10, \'anotherdomain_org\': 5} ``` ```python import mailbox import email.errors class MailSorter: def __init__(self, maildir_path): self.maildir_path = maildir_path self.inbox = mailbox.Maildir(maildir_path, factory=None) self.sorted_boxes = {} def sort_mail_by_domain(self): for key in self.inbox.iterkeys(): try: message = self.inbox[key] except email.errors.MessageParseError: continue # Skip malformed messages sender = message[\'from\'] if sender: domain = sender.split(\'@\')[-1].replace(\'.\', \'_\') mbox_name = f\\"{domain}.mbox\\" mbox_path = f\\"/path/to/mboxes/{mbox_name}\\" if domain not in self.sorted_boxes: self.sorted_boxes[domain] = mailbox.mbox(mbox_path) box = self.sorted_boxes[domain] # Lock, add and flush the message box.lock() box.add(message) box.flush() box.unlock() # Remove the original message self.inbox.lock() self.inbox.discard(key) self.inbox.flush() self.inbox.unlock() def stats(self): statistics = {domain: len(box) for domain, box in self.sorted_boxes.items()} return statistics ```","solution":"import mailbox import email.errors from collections import defaultdict class MailSorter: def __init__(self, maildir_path, output_dir): self.maildir_path = maildir_path self.output_dir = output_dir self.inbox = mailbox.Maildir(maildir_path, factory=None) self.sorted_boxes = defaultdict(lambda: None) def sort_mail_by_domain(self): for key in self.inbox.iterkeys(): try: message = self.inbox[key] except email.errors.MessageParseError: continue # Skip malformed messages sender = message[\'from\'] if sender: domain = sender.split(\'@\')[-1].replace(\'.\', \'_\') mbox_name = f\\"{domain}.mbox\\" mbox_path = f\\"{self.output_dir}/{mbox_name}\\" if domain not in self.sorted_boxes: self.sorted_boxes[domain] = mailbox.mbox(mbox_path) box = self.sorted_boxes[domain] # Lock, add and flush the message box.lock() box.add(message) box.flush() box.unlock() # Remove the original message self.inbox.lock() self.inbox.discard(key) self.inbox.flush() self.inbox.unlock() def stats(self): statistics = {domain: len(box) for domain, box in self.sorted_boxes.items()} return statistics"},{"question":"You have been provided with a **time series dataset** that contains the daily closing prices of a stock over a two-year period. Your task is to implement a function that calculates several rolling statistics over the window of 30 days: 1. The rolling average closing price. 2. The rolling weighted average closing price, using a triangular window. 3. The rolling standard deviation of the closing prices. 4. The rolling correlation between the closing prices and the volume of stocks traded, using an expanding window. You should implement the function `calculate_rolling_statistics` to return a DataFrame containing these values: ```python import pandas as pd def calculate_rolling_statistics(data: pd.DataFrame) -> pd.DataFrame: Calculate rolling statistics on stock data. Parameters: data (pd.DataFrame): A DataFrame with columns [\'Date\', \'Close\', \'Volume\'] where: - \'Date\' is a datetime64 column - \'Close\' is a float64 column representing the closing price of the stock - \'Volume\' is an int64 column representing the trading volume of the stock Returns: pd.DataFrame: A DataFrame with columns [\'Date\', \'RollingAvg\', \'RollingWeightedAvg\', \'RollingStd\', \'RollingCorr\'] where: - \'RollingAvg\' is the rolling average closing price over a 30-day window - \'RollingWeightedAvg\' is the rolling weighted average closing price with a triangular window over a 30-day window - \'RollingStd\' is the rolling standard deviation of the closing prices over a 30-day window - \'RollingCorr\' is the rolling correlation between closing prices and volume over an expanding window # Ensure data is sorted by date data = data.sort_values(by=\\"Date\\") # Calculate rolling average using a 30-day window rolling_avg = data[\'Close\'].rolling(window=30).mean() # Calculate rolling weighted average using a triangular window over a 30-day window rolling_weighted_avg = data[\'Close\'].rolling(window=30, win_type=\'triang\').mean() # Calculate rolling standard deviation using a 30-day window rolling_std = data[\'Close\'].rolling(window=30).std() # Calculate rolling correlation between \'Close\' and \'Volume\' using an expanding window rolling_corr = data[\'Close\'].expanding().corr(data[\'Volume\']) # Create a new DataFrame with the results result = pd.DataFrame({ \'Date\': data[\'Date\'], \'RollingAvg\': rolling_avg, \'RollingWeightedAvg\': rolling_weighted_avg, \'RollingStd\': rolling_std, \'RollingCorr\': rolling_corr }) return result # Example Usage: # Assuming `df` is a DataFrame with the required columns [\'Date\', \'Close\', \'Volume\'] # result_df = calculate_rolling_statistics(df) # print(result_df) ``` # Input Constraints 1. Ensure that the DataFrame passed has a datetime64 column \'Date\', a float64 column \'Close\', and an int64 column \'Volume\'. 2. The input DataFrame will always span at least 60 days\' worth of data. # Expected Output Format A DataFrame with the columns: - \'Date\': datetime64 - \'RollingAvg\': float64 (NaN for the first 29 days) - \'RollingWeightedAvg\': float64 (NaN for the first 29 days) - \'RollingStd\': float64 (NaN for the first 29 days) - \'RollingCorr\': float64 (NaN for the first row, as the correlation with just one value is not defined) Your function should handle potential missing data in the \'Close\' and \'Volume\' columns gracefully, filling these missing points as necessary to ensure the continuity of the calculations.","solution":"import pandas as pd def calculate_rolling_statistics(data: pd.DataFrame) -> pd.DataFrame: Calculate rolling statistics on stock data. Parameters: data (pd.DataFrame): A DataFrame with columns [\'Date\', \'Close\', \'Volume\'] where: - \'Date\' is a datetime64 column - \'Close\' is a float64 column representing the closing price of the stock - \'Volume\' is an int64 column representing the trading volume of the stock Returns: pd.DataFrame: A DataFrame with columns [\'Date\', \'RollingAvg\', \'RollingWeightedAvg\', \'RollingStd\', \'RollingCorr\'] where: - \'RollingAvg\' is the rolling average closing price over a 30-day window - \'RollingWeightedAvg\' is the rolling weighted average closing price with a triangular window over a 30-day window - \'RollingStd\' is the rolling standard deviation of the closing prices over a 30-day window - \'RollingCorr\' is the rolling correlation between closing prices and volume over an expanding window # Ensure data is sorted by date data = data.sort_values(by=\\"Date\\") # Fill NaNs in \'Close\' and \'Volume\' to avoid breaking the calculations data[\'Close\'] = data[\'Close\'].fillna(method=\'ffill\').fillna(method=\'bfill\') data[\'Volume\'] = data[\'Volume\'].fillna(0) # Calculate rolling average using a 30-day window rolling_avg = data[\'Close\'].rolling(window=30).mean() # Calculate rolling weighted average using a triangular window over a 30-day window rolling_weighted_avg = data[\'Close\'].rolling(window=30, win_type=\'triang\').mean() # Calculate rolling standard deviation using a 30-day window rolling_std = data[\'Close\'].rolling(window=30).std() # Calculate rolling correlation between \'Close\' and \'Volume\' using an expanding window rolling_corr = data[\'Close\'].expanding().corr(data[\'Volume\']) # Create a new DataFrame with the results result = pd.DataFrame({ \'Date\': data[\'Date\'], \'RollingAvg\': rolling_avg, \'RollingWeightedAvg\': rolling_weighted_avg, \'RollingStd\': rolling_std, \'RollingCorr\': rolling_corr }) return result # Example Usage: # Assuming `df` is a DataFrame with the required columns [\'Date\', \'Close\', \'Volume\'] # result_df = calculate_rolling_statistics(df) # print(result_df)"},{"question":"Objective Design a function that utilizes the `SGDClassifier` from scikit-learn to train a multi-class classification model using the One-vs-All approach. Your function should handle preprocessing steps, model training, prediction, and evaluation. Problem Statement Write a Python function `train_and_evaluate_sgd_model` that takes in the following parameters: - `X_train` (numpy.ndarray): The input features for training with shape `(n_samples, n_features)`. - `y_train` (numpy.ndarray): The target values for training with shape `(n_samples,)`. - `X_test` (numpy.ndarray): The input features for testing with shape `(m_samples, n_features)`. - `y_test` (numpy.ndarray): The target values for testing with shape `(m_samples,)`. - `loss` (str): The loss function to be used by `SGDClassifier`. It can be one of `\\"hinge\\"`, `\\"log_loss\\"`, `\\"modified_huber\\"`, etc. - `penalty` (str): The penalty (regularization term) to be used by `SGDClassifier`. It can be one of `\\"l2\\"`, `\\"l1\\"`, `\\"elasticnet\\"`. The function should return: - `accuracy` (float): The accuracy of the trained model on the test set. - `coef` (numpy.ndarray): The weight vectors of the trained classifier. - `intercept` (numpy.ndarray): The intercept (bias) terms of the trained classifier. Implementation Details 1. **Preprocessing**: Use `StandardScaler` to standardize the features such that they have zero mean and unit variance. 2. **Model Training**: Train an `SGDClassifier` with the specified `loss` and `penalty` using the One-vs-All approach. 3. **Prediction and Evaluation**: Use the trained model to predict the test set and calculate the accuracy. Constraints - The inputs `X_train`, `X_test` should be two-dimensional numpy arrays. - The inputs `y_train`, `y_test` should be one-dimensional numpy arrays. - The number of features (columns) in `X_train` and `X_test` should match. Function Signature ```python def train_and_evaluate_sgd_model(X_train, y_train, X_test, y_test, loss, penalty): pass ``` Example Usage ```python import numpy as np # Generating some example data X_train = np.array([[0., 0.], [1., 1.], [2., 2.], [3., 3.]]) y_train = np.array([0, 1, 2, 3]) X_test = np.array([[1.5, 1.5], [2.5, 2.5]]) y_test = np.array([1, 2]) accuracy, coef, intercept = train_and_evaluate_sgd_model(X_train, y_train, X_test, y_test, loss=\\"hinge\\", penalty=\\"l2\\") print(\\"Accuracy:\\", accuracy) print(\\"Coefficients:\\", coef) print(\\"Intercept:\\", intercept) ```","solution":"from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score import numpy as np def train_and_evaluate_sgd_model(X_train, y_train, X_test, y_test, loss, penalty): Trains and evaluates a multi-class classification model using SGDClassifier with the One-vs-All approach. Parameters: - X_train (numpy.ndarray): Training input features with shape (n_samples, n_features). - y_train (numpy.ndarray): Target labels for training with shape (n_samples,). - X_test (numpy.ndarray): Testing input features with shape (m_samples, n_features). - y_test (numpy.ndarray): Target labels for testing with shape (m_samples,). - loss (str): Loss function to be used by SGDClassifier. - penalty (str): Penalty term to be used by SGDClassifier. Returns: - accuracy (float): Accuracy of the model on the test set. - coef (numpy.ndarray): Weights of the trained classifier. - intercept (numpy.ndarray): Intercepts of the trained classifier. # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Define and train the SGDClassifier model = SGDClassifier(loss=loss, penalty=penalty, max_iter=1000, tol=1e-3) model.fit(X_train_scaled, y_train) # Make predictions on the test set y_pred = model.predict(X_test_scaled) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy, model.coef_, model.intercept_"},{"question":"Objective: Given a dataset, you are required to create a visualization that employs multiple seaborn components (`so.Plot`, `so.Band`, `so.Line`) to provide insights into the data. You will manipulate and preprocess the dataset, create visualizations using these seaborn components, and combine them to illustrate data trends with confidence intervals. Dataset: You will use the `tips` dataset from seaborn, which contains information about restaurant tips and bills. Instructions: 1. **Load and preprocess the data**: Load the `tips` dataset from seaborn. Convert the `day` column to a numerical format where: - Thursday = 1 - Friday = 2 - Saturday = 3 - Sunday = 4 2. **Create the plot**: - Plot the total bill (`total_bill`) against the numerical day format. - Add an interval band to show the range between the 25th percentile and 75th percentile of the total bill for each day. - Overlay a line plot showing the average total bill for each day. - Ensure the plot includes appropriate axes labels and a title. 3. **Show the plot** with customization to improve readability (e.g., transparency, adding edges). Requirements: - Demonstrate understanding of seaborn\'s advanced plotting functions, including data transformation and plot combination. - Ensure code readability and clarity, following good Python practices. Expected functions: You should define a function `plot_tips_data()` that: - Loads and preprocesses the data. - Creates the required plot. - Does not return anything, but displays the plot using `matplotlib`. Example function signature: ```python def plot_tips_data(): pass ``` Here\'s a template to get you started: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd def plot_tips_data(): # Load the data tips = load_dataset(\\"tips\\") # Preprocess the data: Convert day to a numerical format day_mapping = {\\"Thur\\": 1, \\"Fri\\": 2, \\"Sat\\": 3, \\"Sun\\": 4} tips[\\"day_num\\"] = tips[\\"day\\"].map(day_mapping) # Create the plot p = so.Plot(tips, x=\\"day_num\\", y=\\"total_bill\\") # Add interval band for the 25th to 75th percentile p.add(so.Band(), so.Perc(25, 75)) # Overlay a line plot for the mean total bill p.add(so.Line(), so.Agg()) # Customize and show the plot p.show() ``` **Constraints:** - Ensure the `day_num` numerical conversion correctly maps days to integers. - Properly handle any missing or malformed data. - Apply necessary styling to make the plot informative and aesthetically pleasing. Good luck, and happy plotting!","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt import pandas as pd def plot_tips_data(): # Load the data tips = load_dataset(\\"tips\\") # Preprocess the data: Convert day to a numerical format day_mapping = {\\"Thur\\": 1, \\"Fri\\": 2, \\"Sat\\": 3, \\"Sun\\": 4} tips[\\"day_num\\"] = tips[\\"day\\"].map(day_mapping) # Create the plot p = so.Plot(tips, x=\\"day_num\\", y=\\"total_bill\\") # Add interval band for the 25th to 75th percentile p.add(so.Band(), so.Perc(25, 75)) # Overlay a line plot for the mean total bill p.add(so.Line(), so.Agg()) # Customize and show the plot p.scale(x=\\"linear\\", y=\\"linear\\") .label(x=\\"Numerical Day of the Week\\", y=\\"Total Bill\\", title=\'Total Bill by Day of the Week\') .theme({ \'figure.facecolor\': \'white\', \'axes.edgecolor\': \'black\', \'xtick.color\': \'black\', \'ytick.color\': \'black\'}) plt.show()"},{"question":"# Process and Synchronization in Data Analysis **Objective**: Implement a Python script that performs data analysis using process-based parallelism, synchronization primitives, and inter-process communication using the `multiprocessing` package. **Task**: 1. Create a function `process_data` that takes a chunk of data, processes it, and returns the result. This function should be designed to handle a large dataset split into smaller chunks for concurrent processing. 2. Implement a function `analyze_data_concurrently` that: - Takes a large dataset (a list of numbers), the number of processes to use, and a function for data processing. - Divides the dataset into equal-sized chunks. - Sets up a pool of worker processes to independently process each chunk using the `process_data` function. - Uses a queue to collect results from each process. - Ensures the synchronization of processes using appropriate synchronization primitives where necessary. 3. Ensure that the final result is a combined processed output from all worker processes. **Constraints**: - The data processing function (`process_data`) can include any form of numerical computation (e.g., sum, mean, standard deviation). - You must use the `multiprocessing` package\'s `Process`, `Queue`, `Lock`, and `Pool` classes. - You must handle edge cases such as an empty dataset or chunking issues. - Ensure the entire operation is completed within a reasonable time frame for large datasets (performance requirement). **Input**: ```python def analyze_data_concurrently(data: List[int], num_processes: int, process_func: Callable[[List[int]], Any]) -> Any: pass ``` - `data` : List of integers representing the dataset. - `num_processes` : Number of processes to use. - `process_func` : Function that processes each chunk of data. **Output**: - Returns a combined processed output from all worker processes. **Example**: ```python from multiprocessing import current_process import numpy as np def process_data(chunk): print(f\\"Process {current_process().name} processing chunk\\") return np.sum(chunk) def analyze_data_concurrently(data, num_processes, process_func): Function implementation required here data = [i for i in range(10000)] result = analyze_data_concurrently(data, 4, process_data) print(result) # Should print the sum of the dataset ```","solution":"from multiprocessing import Process, Queue, Lock, current_process import numpy as np from typing import List, Callable, Any def process_data(chunk: List[int]) -> int: Process a chunk of data and return the sum. print(f\\"Process {current_process().name} processing chunk\\") return np.sum(chunk) def worker(data_chunk: List[int], process_func: Callable[[List[int]], Any], output_queue: Queue, lock: Lock): Worker function to process data and put result in the output queue. result = process_func(data_chunk) with lock: output_queue.put(result) def analyze_data_concurrently(data: List[int], num_processes: int, process_func: Callable[[List[int]], Any]) -> int: Analyzes the data concurrently using multiple processes. if not data or num_processes <= 0: return 0 chunk_size = int(np.ceil(len(data) / num_processes)) processes = [] output_queue = Queue() lock = Lock() for i in range(0, len(data), chunk_size): chunk = data[i:i + chunk_size] p = Process(target=worker, args=(chunk, process_func, output_queue, lock)) processes.append(p) p.start() for p in processes: p.join() results = [] while not output_queue.empty(): results.append(output_queue.get()) combined_result = sum(results) return combined_result"},{"question":"You have been provided with the `diamonds` dataset from seaborn, which contains detailed information about diamonds including characteristics and price. Your task is to write a Python function to create a bar plot showing the interquartile range (IQR) of diamond prices for each `clarity` category. Additionally, you need to use the `cut` variable to create separate bars for each category within clarity using the `Dodge` transformation. The IQR is defined as the difference between the 75th and 25th percentiles. Function Signature ```python def plot_iqr_diamonds() -> None: pass ``` Input - No input parameters. Output - The function should display the bar plot directly using seaborn\'s plotting functionalities. Requirements 1. The bar plot should display the IQR of `price` for each `clarity` category. 2. Within each `clarity` category, separate bars should be shown for each `cut` category using the `Dodge` transformation. 3. Label the `clarity` categories on the x-axis. 4. Label the y-axis appropriately to indicate it represents the IQR of prices. Constraints - Ensure proper handling of the seaborn imports and dataset loading processes within your function. - Make sure the plot is easily interpretable with proper titles and labels. Example Output A bar plot generated by seaborn that visually indicates the IQR of diamond prices for different `clarity` categories, with `cut` categories distinguished by color and dodged bars for clarity. Here is sample code to get you started: ```python import seaborn.objects as so from seaborn import load_dataset def plot_iqr_diamonds(): # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot with the specified aggregation and transformations p = so.Plot(diamonds, x=\\"clarity\\", y=\\"price\\") p.add(so.Bar(), so.Agg(lambda x: x.quantile(.75) - x.quantile(.25)), so.Dodge(), color=\\"cut\\") # Display the plot p.show() ``` Ensure that your function adheres to the specifications and displays the plot correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_iqr_diamonds(): Plots the interquartile range (IQR) of diamond prices for each clarity category, with separate bars for each cut category using the Dodge transformation. # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Calculate the IQR (75th percentile - 25th percentile) for each combination of clarity and cut def calculate_iqr(group): return group[\'price\'].quantile(0.75) - group[\'price\'].quantile(0.25) iqr_data = diamonds.groupby([\'clarity\', \'cut\']).apply(calculate_iqr).reset_index(name=\'IQR\') # Create a bar plot plt.figure(figsize=(12, 8)) sns.barplot(data=iqr_data, x=\'clarity\', y=\'IQR\', hue=\'cut\', dodge=True) # Set plot labels and title plt.xlabel(\'Clarity\') plt.ylabel(\'Interquartile Range (IQR) of Prices\') plt.title(\'IQR of Diamond Prices by Clarity and Cut Categories\') # Display the plot plt.legend(title=\'Cut\') plt.show()"},{"question":"# Configuration Manager with Interpolation and Custom Behavior You are required to implement a configuration manager class using the `configparser` module in Python. This class should provide the functionality to read, write, and manipulate configuration files with extended interpolation and custom behavior. The class should also handle various data types and provide options for user-defined sections and option names. Requirements: 1. **Class Definition and Initialization**: - Create a class `ConfigManager`. - The class should initialize with optional parameters: `allow_no_value`, `delimiters`, `comment_prefixes`, `inline_comment_prefixes`, `strict`, and `empty_lines_in_values`. 2. **Reading Configuration**: - Implement a method `read_config(file_path: str)` to read the configuration from a file. 3. **Writing Configuration**: - Implement a method `write_config(file_path: str)` to write the current configuration state to a file. 4. **Getting and Setting Values**: - Implement methods for getting and setting configuration values: - `get_value(section: str, option: str, fallback: str = None) -> str` - `set_value(section: str, option: str, value: str) -> None` 5. **Handling Various Data Types**: - Implement methods for getting and setting integer, float, and boolean values: - `get_int(section: str, option: str, fallback: int = None) -> int` - `get_float(section: str, option: str, fallback: float = None) -> float` - `get_bool(section: str, option: str, fallback: bool = None) -> bool` 6. **Custom Interpolation**: - Use `ExtendedInterpolation` for value interpolation within the configuration files. 7. **Example INI Configuration**: - Provide an example INI configuration file to demonstrate the use of your `ConfigManager` class. The example should include multiple sections, various data types, and interpolated values. Constraints: - Assume all keys and values are strings unless explicitly converted using the provided methods for data types. - Handle exceptions such as missing sections or options gracefully by providing meaningful error messages. Example Usage: ```python # Sample configuration file: example.ini [Paths] home_dir: /Users my_dir: {home_dir}/example [Settings] max_connections: 10 pi_value: 3.14159 is_enabled: yes [User] name: John greeting_message: Hello, {name}! config_mgr = ConfigManager(allow_no_value=True, delimiters=(\'=\', \':\'), inline_comment_prefixes=[\';\']) config_mgr.read_config(\'example.ini\') print(config_mgr.get_value(\'Paths\', \'my_dir\')) # Output: /Users/example print(config_mgr.get_int(\'Settings\', \'max_connections\')) # Output: 10 print(config_mgr.get_float(\'Settings\', \'pi_value\')) # Output: 3.14159 print(config_mgr.get_bool(\'Settings\', \'is_enabled\')) # Output: True print(config_mgr.get_value(\'User\', \'greeting_message\')) # Output: Hello, John! config_mgr.set_value(\'User\', \'name\', \'Jane\') config_mgr.write_config(\'example_modified.ini\') ``` # Submission: Submit your `ConfigManager` class implementation along with the example INI file and a script demonstrating its usage as shown above.","solution":"import configparser from configparser import ExtendedInterpolation class ConfigManager: def __init__(self, allow_no_value=False, delimiters=(\'=\', \':\'), comment_prefixes=(\'#\', \';\'), inline_comment_prefixes=None, strict=True, empty_lines_in_values=False): self.config = configparser.ConfigParser( allow_no_value=allow_no_value, delimiters=delimiters, comment_prefixes=comment_prefixes, inline_comment_prefixes=inline_comment_prefixes, strict=strict, empty_lines_in_values=empty_lines_in_values, interpolation=ExtendedInterpolation() ) def read_config(self, file_path): self.config.read(file_path) def write_config(self, file_path): with open(file_path, \'w\') as configfile: self.config.write(configfile) def get_value(self, section, option, fallback=None): try: return self.config.get(section, option, fallback=fallback) except (configparser.NoSectionError, configparser.NoOptionError) as e: return fallback def set_value(self, section, option, value): if not self.config.has_section(section): self.config.add_section(section) self.config.set(section, option, value) def get_int(self, section, option, fallback=None): try: return self.config.getint(section, option, fallback=fallback) except (configparser.NoSectionError, configparser.NoOptionError, ValueError) as e: return fallback def get_float(self, section, option, fallback=None): try: return self.config.getfloat(section, option, fallback=fallback) except (configparser.NoSectionError, configparser.NoOptionError, ValueError) as e: return fallback def get_bool(self, section, option, fallback=None): try: return self.config.getboolean(section, option, fallback=fallback) except (configparser.NoSectionError, configparser.NoOptionError, ValueError) as e: return fallback"},{"question":"Objective: Implement a custom vector function in PyTorch and validate its gradients using the `torch.autograd.gradcheck` utility. Description: 1. **Create a Custom Function:** - Implement a custom PyTorch function `MyCustomFunction` which computes the output ( y = sin(x)^2 ) for an input tensor ( x ). 2. **Validate Gradients:** - Write a function `validate_gradients` that: - Takes a tensor ( x ) as input. - Computes the gradients of `MyCustomFunction` using PyTorch\'s autograd. - Validates these gradients using `torch.autograd.gradcheck`. Function Signature: ```python import torch class MyCustomFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Save context for backward pass ctx.save_for_backward(input) # Custom function computation return torch.sin(input) ** 2 @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors input, = ctx.saved_tensors # Chain rule application grad_input = 2 * torch.sin(input) * torch.cos(input) * grad_output return grad_input def validate_gradients(x): # Ensure x requires gradient x = x.double().requires_grad_(True) # Perform gradcheck return torch.autograd.gradcheck(MyCustomFunction.apply, (x,)) ``` Input: - `validate_gradients` function takes a single input tensor ( x ) with `requires_grad=True` and of `dtype=torch.double`. Output: - The function returns `True` if the gradients are correctly computed and validated, otherwise it throws an error. Constraints: - Use PyTorch version `1.6.0` or later. - Ensure that the input tensor has `dtype=torch.double` for numerical stability in `gradcheck`. Example Usage: ```python x = torch.tensor([1.0, 2.0, 3.0], dtype=torch.double, requires_grad=True) is_valid = validate_gradients(x) print(f\\"Gradients are valid: {is_valid}\\") ``` # Note: The students should write the entire `MyCustomFunction` class with both `forward` and `backward` methods and then use this class in `validate_gradients`. The `gradcheck` will numerically estimate the gradients and compare them to those computed by the `backward` method to ensure correctness.","solution":"import torch class MyCustomFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Save context for backward pass ctx.save_for_backward(input) # Custom function computation return torch.sin(input) ** 2 @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors input, = ctx.saved_tensors # Chain rule application grad_input = 2 * torch.sin(input) * torch.cos(input) * grad_output return grad_input def validate_gradients(x): # Ensure x requires gradient and is of type double x = x.double().requires_grad_(True) # Perform gradcheck return torch.autograd.gradcheck(MyCustomFunction.apply, (x,))"},{"question":"Broadcasting Compatibility Checker **Objective:** The goal of this assessment is to write a function that determines whether two tensors can be broadcast together as per PyTorch\'s broadcasting rules and to calculate the resulting size of the broadcasted tensors if they are broadcastable. **Problem Statement:** Write a function `is_broadcastable(tensor1_shape, tensor2_shape)` that takes two lists of integers representing the shapes of two tensors and returns a tuple. The first element of the tuple is a boolean indicating whether the two tensors are broadcastable. The second element is the resultant shape of the two tensors if they are broadcastable. If they are not broadcastable, return an empty list as the second element. **Function Signature:** ```python def is_broadcastable(tensor1_shape: list[int], tensor2_shape: list[int]) -> tuple[bool, list[int]]: ``` **Parameters:** - `tensor1_shape (list[int])`: A list of integers representing the shape of the first tensor. - `tensor2_shape (list[int])`: A list of integers representing the shape of the second tensor. **Returns:** - `tuple[bool, list[int]]`: A tuple where the first element is `True` if the tensors can be broadcast together, otherwise `False`, and the second element is a list of integers representing the shape of the resulting broadcasted tensor, or an empty list if the tensors are not broadcastable. **Examples:** 1. `is_broadcastable([5, 3, 4, 1], [3, 1, 1])` should return `(True, [5, 3, 4, 1])` 2. `is_broadcastable([5, 2, 4, 1], [3, 1, 1])` should return `(False, [])` 3. `is_broadcastable([1], [3, 1, 7])` should return `(True, [3, 1, 7])` **Constraints:** - The dimensions of the tensors represented by the lists will be between 0 and 100. - The size of each dimension will be a non-negative integer. **Notes:** - Consider all rules of PyTorch\'s broadcasting semantics when implementing the function. - Implement the function with optimal performance keeping in mind the constraints. Example Implementation Framework: ```python def is_broadcastable(tensor1_shape, tensor2_shape): # Your implementation here pass # Example usage print(is_broadcastable([5, 3, 4, 1], [3, 1, 1])) # should return (True, [5, 3, 4, 1]) print(is_broadcastable([5, 2, 4, 1], [3, 1, 1])) # should return (False, []) ```","solution":"def is_broadcastable(tensor1_shape, tensor2_shape): Determines if two tensors of given shapes can be broadcast together. Args: tensor1_shape (list[int]): shape of the first tensor. tensor2_shape (list[int]): shape of the second tensor. Returns: tuple[bool, list[int]]: A tuple containing a boolean indicating if broadcasting is possible, and the resultant shape if broadcasting is possible, else an empty list. reversed_shape1 = tensor1_shape[::-1] reversed_shape2 = tensor2_shape[::-1] result_shape = [] for dim1, dim2 in zip(reversed_shape1, reversed_shape2): if dim1 == dim2 or dim1 == 1 or dim2 == 1: result_shape.append(max(dim1, dim2)) else: return (False, []) result_shape += reversed_shape1[len(result_shape):] if len(reversed_shape1) > len(reversed_shape2) else reversed_shape2[len(result_shape):] return (True, result_shape[::-1])"},{"question":"# Python310 Coding Assessment: Subprocess Management Objective Demonstrate your understanding of the `subprocess` module by implementing functions that manage subprocesses effectively, capture output, and handle errors and timeouts. Task You are required to implement two functions: 1. `execute_command(command: str) -> Tuple[int, str, str]` 2. `pipeline_commands(commands: List[List[str]]) -> str` # Function 1: `execute_command` This function should: - Take a command as a string. - Execute the command using `subprocess.run()`. - Capture and return the return code, standard output, and standard error. **Input**: - `command`: A string representing the shell command to execute. **Output**: - A tuple of three items: - Return code of the command. - Standard output. - Standard error. **Example**: ```python return_code, stdout, stderr = execute_command(\'ls -la\') ``` # Function 2: `pipeline_commands` This function should: - Take a list of commands, where each command is represented as a list of arguments. - Execute the commands in a pipeline using `subprocess.Popen`. - Return the final output of the pipeline. **Input**: - `commands`: A list of commands, where each command is a list of its arguments. - Example: `[[\\"ps\\", \\"aux\\"], [\\"grep\\", \\"python\\"], [\\"wc\\", \\"-l\\"]]` **Output**: - The final output of the pipeline as a string. **Example**: ```python output = pipeline_commands([[\\"ps\\", \\"aux\\"], [\\"grep\\", \\"python\\"], [\\"wc\\", \\"-l\\"]]) ``` # Constraints - Ensure the handling of large outputs without deadlocking. - Implement proper error handling for non-zero return codes. - Implement timeout handling for the `execute_command` function with a default timeout of 10 seconds. - Avoid using `shell=True` wherever possible unless strictly necessary for functionality. # Performance Requirements - Aim for efficient execution and resource management. - The solution should work for typical subprocess execution scenarios expected in a controlled environment like a coding assessment. Submission Provide the implementation of the two functions in a single Python file. Comment your code to explain the functionality, especially the use of subprocess features and error handling.","solution":"import subprocess from typing import List, Tuple def execute_command(command: str) -> Tuple[int, str, str]: Executes a shell command and captures its return code, stdout, and stderr. Args: command (str): The command to execute. Returns: Tuple[int, str, str]: A tuple containing the return code, standard output, and standard error. try: result = subprocess.run(command, capture_output=True, text=True, shell=True, timeout=10) return (result.returncode, result.stdout, result.stderr) except subprocess.TimeoutExpired: return (1, \'\', \'Command timed out\') def pipeline_commands(commands: List[List[str]]) -> str: Executes a list of commands in a pipeline and captures the final output. Args: commands (List[List[str]]): A list of commands, where each command is a list of its arguments. Returns: str: The final output of the pipeline. processes = [] for i, command in enumerate(commands): if i == 0: process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE) else: process = subprocess.Popen(command, stdin=processes[-1].stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE) processes.append(process) final_output, _ = processes[-1].communicate() return final_output.decode()"},{"question":"Objective: Implement a simplified version of the `PLSCanonical` algorithm described in the scikit-learn cross decomposition module. Problem Statement: You are required to create a function `pls_canonical(X, Y, n_components)` that performs Partial Least Squares Canonical (PLSCanonical) decomposition on two matrices `X` and `Y` using `n_components` components. Function Signature: ```python def pls_canonical(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: ``` Input: - `X`: A 2D numpy array of shape `(n_samples, n_features)` representing the predictor matrix. - `Y`: A 2D numpy array of shape `(n_samples, n_targets)` representing the target matrix. - `n_components`: An integer specifying the number of PLS components to use. Output: - A tuple containing four numpy arrays: - `X_scores`: A 2D numpy array of shape `(n_samples, n_components)` representing the scores on matrix `X`. - `Y_scores`: A 2D numpy array of shape `(n_samples, n_components)` representing the scores on matrix `Y`. - `X_loadings`: A 2D numpy array of shape `(n_features, n_components)` representing the loadings for matrix `X`. - `Y_loadings`: A 2D numpy array of shape `(n_targets, n_components)` representing the loadings for matrix `Y`. Constraints: 1. The matrices `X` and `Y` will be centered (i.e., columns will have zero mean). 2. You must use the `nipals` algorithm for computing the singular vectors. 3. Ensure the implementation is efficient and optimized for performance. Example: ```python import numpy as np X = np.array([[0.1, -0.2], [0.9, 1.1], [1.1, 0.9]]) Y = np.array([[0.2, 0.5], [-0.5, 0.9], [0.8, -1.1]]) n_components = 2 X_scores, Y_scores, X_loadings, Y_loadings = pls_canonical(X, Y, n_components) print(\\"X_scores:\\", X_scores) print(\\"Y_scores:\\", Y_scores) print(\\"X_loadings:\\", X_loadings) print(\\"Y_loadings:\\", Y_loadings) ``` Your task is to implement the `pls_canonical` function to achieve the desired output.","solution":"import numpy as np from typing import Tuple def nipals(X, Y, n_components): T = np.zeros((X.shape[0], n_components)) U = np.zeros((Y.shape[0], n_components)) P = np.zeros((X.shape[1], n_components)) Q = np.zeros((Y.shape[1], n_components)) for i in range(n_components): u = Y[:, 0] t = np.zeros(X.shape[0]) while True: t_old = t.copy() t = np.dot(X, np.dot(X.T, u)) t /= np.linalg.norm(t) q = np.dot(Y.T, t) u = np.dot(Y, q) u /= np.linalg.norm(u) if np.allclose(t, t_old, atol=1e-8): break p = np.dot(X.T, t) q = np.dot(Y.T, u) X -= np.outer(t, p) Y -= np.outer(u, q) T[:, i] = t U[:, i] = u P[:, i] = p Q[:, i] = q return T, U, P, Q def pls_canonical(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: X_scores, Y_scores, X_loadings, Y_loadings = nipals(X, Y, n_components) return X_scores, Y_scores, X_loadings, Y_loadings"},{"question":"# Python310 Coding Assessment Question Objective Write a Python function `analyze_sound_files(file_list)` that takes a list of filenames of sound files and returns a dictionary summarizing the types and properties of these sound files. Input - `file_list`: A list of strings, where each string is a filename (e.g., `[\'sound1.wav\', \'audio2.aiff\', \'track3.au\']`). Output - A dictionary with the following structure: ```python { \'filetype_counts\': {filetype: count, ...}, \'average_framerate\': float, \'average_nchannels\': float, \'total_nframes\': int, \'samplerate_distribution\': {framerate: count, ...} } ``` - `filetype_counts`: A dictionary where keys are filetypes and values are the counts of each filetype in the input list. - `average_framerate`: The average framerate among all files for which the framerate could be determined (non-zero). - `average_nchannels`: The average number of channels among all files for which the number of channels could be determined (non-zero). - `total_nframes`: The total number of frames across all files. - `samplerate_distribution`: A dictionary where keys are framerates and values are the counts of each framerate in the input list. Constraints - You may assume that the files in the `file_list` exist and are accessible. - If a property (e.g., framerate, nchannels) is `0` or `-1` in the namedtuple, ignore it for the purpose of calculating averages. - Use the `sndhdr` module\'s `what()` function to determine file information. Example ```python file_list = [\'file1.wav\', \'file2.aiff\', \'file3.au\'] result = analyze_sound_files(file_list) # Example Result: { \'filetype_counts\': {\'wav\': 1, \'aiff\': 1, \'au\': 1}, \'average_framerate\': 44100.0, \'average_nchannels\': 2.0, \'total_nframes\': 123456, \'samplerate_distribution\': {44100: 2, 48000: 1} } ``` Implementation Notes - Be sure to handle edge cases, such as empty `file_list`. - Assume the `sndhdr` module\'s `what` function is imported and available for use. - Proper error handling should be in place if the file type cannot be determined.","solution":"import sndhdr def analyze_sound_files(file_list): Analyzes a list of sound files and returns a dictionary summarizing their types and properties. Args: file_list (list): A list of sound file names. Returns: dict: A dictionary with summarized properties. if not file_list: return { \'filetype_counts\': {}, \'average_framerate\': 0.0, \'average_nchannels\': 0.0, \'total_nframes\': 0, \'samplerate_distribution\': {} } filetype_counts = {} total_framerate = 0 total_nchannels = 0 total_nframes = 0 framerate_count = 0 nchannels_count = 0 samplerate_distribution = {} for filename in file_list: info = sndhdr.what(filename) if info is None: continue filetype, framerate, nchannels, nframes = info[:4] # Update filetype_counts filetype_counts[filetype] = filetype_counts.get(filetype, 0) + 1 # Update samplerate_distribution if framerate > 0: samplerate_distribution[framerate] = samplerate_distribution.get(framerate, 0) + 1 total_framerate += framerate framerate_count += 1 # Update average_nchannels calculation if nchannels > 0: total_nchannels += nchannels nchannels_count += 1 # Update total_nframes total_nframes += nframes average_framerate = (total_framerate / framerate_count) if framerate_count > 0 else 0.0 average_nchannels = (total_nchannels / nchannels_count) if nchannels_count > 0 else 0.0 return { \'filetype_counts\': filetype_counts, \'average_framerate\': average_framerate, \'average_nchannels\': average_nchannels, \'total_nframes\': total_nframes, \'samplerate_distribution\': samplerate_distribution }"},{"question":"# **Question: Implementing and Annotating a Custom TorchScript Module** You are tasked with implementing a custom PyTorch module that uses TorchScript for static type checking and enhanced performance. The module, named `CustomNNModule`, should consist of several layers and operations, demonstrating your understanding of TorchScript\'s type annotations and supported features. # **Requirements:** 1. **Class Definition**: - Define a class `CustomNNModule` that inherits from `torch.nn.Module`. 2. **Initialization**: - The `__init__` method should define three layers: - `self.fc1`: A fully connected layer with an input size of 128 and output size of 64. - `self.fc2`: A fully connected layer with an input size of 64 and output size of 32. - `self.dropout`: A dropout layer with a dropout probability of 0.5. - Use appropriate type annotations for each layer using `torch.jit.Attribute` since `__init__` is not considered a TorchScript method. 3. **Forward Method**: - Implement a `forward` method that takes a `torch.Tensor` as input and applies the following operations: - A ReLU activation function after `self.fc1`. - The dropout layer after the ReLU activation. - A ReLU activation function after `self.fc2`. - Returns the final output. 4. **Type Annotations**: - Annotate the method signatures and return types appropriately. - Ensure that all instance attributes and methods are correctly type-annotated to comply with TorchScript\'s type-checking rules. 5. **Testing**: - Create an instance of the `CustomNNModule` class. - Use `torch.jit.script` to script the `CustomNNModule` class. - Pass a random tensor of appropriate size through the scripted model and print the output. # **Constraints**: - You must use TorchScript annotations and features as specified in the documentation. - Ensure strict adherence to TorchScript\'s type annotations and type-checking rules. - The implementation should be efficient and leverage PyTorch properly. # **Example Input and Output**: **Example**: ```python import torch from typing import Tuple # Implement the CustomNNModule here model = CustomNNModule() scripted_model = torch.jit.script(model) # Script the model input_tensor = torch.randn(1, 128) # Random input tensor output = scripted_model(input_tensor) print(output) ``` **Expected Output**: A tensor of shape [1, 32], representing the output of the neural network after processing through the layers and activations defined in `CustomNNModule`. **Note**: The output values will be random due to the random initialization of weights and the input tensor. # **Submission**: Submit a Python file containing: - The full implementation of the `CustomNNModule` class with necessary annotations and methods. - A script section that creates an instance of your module, scripts it, and tests it with a random input tensor.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomNNModule(nn.Module): def __init__(self): super(CustomNNModule, self).__init__() self.fc1 = nn.Linear(128, 64) self.fc2 = nn.Linear(64, 32) self.dropout = nn.Dropout(0.5) def forward(self, x: torch.Tensor) -> torch.Tensor: x = F.relu(self.fc1(x)) x = self.dropout(x) x = F.relu(self.fc2(x)) return x # To test the implementation if __name__ == \\"__main__\\": model = CustomNNModule() scripted_model = torch.jit.script(model) # Script the model input_tensor = torch.randn(1, 128) # Random input tensor output = scripted_model(input_tensor) print(output) # This should print a tensor of shape [1, 32]"},{"question":"# Advanced File and Directory Operations in Python **Objective:** In this task, you will create a function `synchronize_directories` that synchronizes two directories. The function will compare the contents of two directories and ensure that both directories have the same files with the same contents. If a file is found in one directory but not the other, it should be copied to the other directory. If a file exists in both directories but with different contents, consider the file in the source directory as the correct one and replace the file in the target directory with this one. **Function Signature:** ```python def synchronize_directories(source_dir: str, target_dir: str) -> None: pass ``` **Input:** - `source_dir` (str): The path to the source directory. - `target_dir` (str): The path to the target directory. **Output:** - None. This function will perform file operations to synchronize the two directories. **Requirements and Constraints:** - Use the `pathlib` module for path manipulations. - Use the `filecmp` module for comparing files and directories. - Utilize `shutil` for file copying operations. - Ensure that temporary files or directories (if needed) are used responsibly and are cleaned up after use. - Assume directories exist and are accessible. You do not need to handle permission errors or directory creation. - The function should handle large directories efficiently, minimize redundant operations, and avoid unnecessary file copies. **Example Usage:** ```python source_dir = \\"/path/to/source\\" target_dir = \\"/path/to/target\\" synchronize_directories(source_dir, target_dir) ``` Upon calling the function, the contents of `source_dir` and `target_dir` should be identical, with any necessary files copied or overwritten from `source_dir` to `target_dir`. **Notes:** - Pay attention to edge cases such as empty directories and directories with nested subdirectories. - Do not use external libraries - only standard Python libraries are allowed.","solution":"import filecmp import shutil from pathlib import Path def synchronize_directories(source_dir: str, target_dir: str) -> None: Synchronize the contents of two directories, ensuring both directories have the same files with the same contents. source_path = Path(source_dir) target_path = Path(target_dir) # Create a list of files in both directories source_files = set(source_path.rglob(\'*\')) target_files = set(target_path.rglob(\'*\')) # Only consider files, filter out directories source_files = {f for f in source_files if f.is_file()} target_files = {f for f in target_files if f.is_file()} # Map relative paths to full paths source_rel_paths = {f.relative_to(source_path): f for f in source_files} target_rel_paths = {f.relative_to(target_path): f for f in target_files} # Synchronize files for rel_path, source_file in source_rel_paths.items(): target_file = target_path / rel_path if rel_path not in target_rel_paths: target_file.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(source_file, target_file) elif not filecmp.cmp(source_file, target_file, shallow=False): shutil.copy2(source_file, target_file) for rel_path in target_rel_paths: if rel_path not in source_rel_paths: target_file = target_path / rel_path target_file.unlink()"},{"question":"**Coding Assessment Question** **Objective:** Demonstrate your understanding of performance profiling in PyTorch using `torch.utils.bottleneck`. You will need to identify and fix performance bottlenecks in a given PyTorch script. **Problem Statement:** You are provided with a PyTorch script that performs some tensor operations. Your task is to: 1. Use `torch.utils.bottleneck` to analyze the performance of the provided script. 2. Identify the performance bottlenecks in the script. 3. Optimize the script to improve its performance. 4. Verify the performance improvements using `torch.utils.bottleneck`. **Given Script (script.py):** ```python import torch def tensor_operations(): # Simulating a script with potential bottlenecks x = torch.rand(1000, 1000) y = torch.rand(1000, 1000) z = torch.zeros(1000, 1000) for i in range(1000): for j in range(1000): z[i][j] = x[i][j] + y[i][j] return z if __name__ == \\"__main__\\": result = tensor_operations() print(\\"Finished tensor operations\\") ``` **Detailed Steps:** 1. **Run Bottleneck Tool:** Run the bottleneck tool on the given script and capture the output. ```bash python -m torch.utils.bottleneck script.py ``` 2. **Analyze Output:** Carefully analyze the output to identify which parts of the code are causing performance issues. 3. **Optimize Script:** Modify the script in a way that the performance bottlenecks are mitigated. You might want to use PyTorch\'s built-in tensor operations to speed up the computations. 4. **Verify Improvements:** Run the bottleneck tool again on the optimized script to ensure that the performance has improved. **Constraints:** - Your optimized script should produce the same output as the original. - Aim to reduce the execution time as much as possible. **Expected Deliverables:** 1. The modified `script.py` file with comments explaining the changes made to optimize performance. 2. A text file `bottleneck_output.txt` containing the output of `torch.utils.bottleneck` before and after the optimizations. **Submission:** Submit the modified `script.py` and `bottleneck_output.txt` files. **Performance Requirements:** Your solution should show a noticeable decrease in execution time, as indicated by the before and after results from `torch.utils.bottleneck`.","solution":"import torch def tensor_operations(): # Simulating a script with potential bottlenecks x = torch.rand(1000, 1000) y = torch.rand(1000, 1000) # Optimized with built-in tensor operations z = x + y return z if __name__ == \\"__main__\\": result = tensor_operations() print(\\"Finished tensor operations\\")"},{"question":"# Seaborn Coding Assessment You are given a dataset `dowjones` that contains historical Dow Jones Index data and a dataset `fmri` that contains measurements from an fMRI experiment. Your task is to create a visualization using the `seaborn.objects` interface to illustrate insights from these datasets. Follow the instructions below to complete the task. Task: 1. **Load the Datasets:** Import the `seaborn` package and load the `dowjones` and `fmri` datasets. 2. **Line Plot of Dow Jones Price Over Time:** Create a line plot of the Dow Jones Index price over time using the `dowjones` dataset. 3. **Line Plot for fMRI Data:** From the `fmri` dataset, select data where `region` is `\'parietal\'` and `event` is `\'stim\'`. Create a line plot that shows the `signal` value over different `timepoint`s. Each `subject` should have its line. 4. **Adding Statistical Transformations:** Using the selected subset of the `fmri` dataset, create a line plot that includes an error band to show the confidence intervals of the signal values. 5. **Combining Plots:** Combine the functionalities by plotting the `signal` values over `timepoint` for each `region` and `event`. Add lines and markers to indicate sampled values. Ensure different `region`s use distinct colors and different `event`s use different linestyles. Requirements: - **Inputs:** - No custom input from the user is required. The code should run using the defined datasets. - **Outputs:** - The function should output the following plots: 1. A line plot of the Dow Jones Index price over time. 2. A line plot of the `signal` value over `timepoint` for the specific subset of `fmri` data. 3. A line plot with error bands for the selected `fmri` data. 4. A combined plot with lines, markers, and proper aesthetics for the whole `fmri` dataset based on `region` and `event`. Constraints: - Use the `seaborn.objects` interface. - Implement the code within a single function named `create_plots()`. # Function Signature: ```python import seaborn.objects as so from seaborn import load_dataset def create_plots(): # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Plot Dow Jones Index price over time so.Plot(dowjones, \\"Date\\", \\"Price\\").add(so.Line()).show() # Plot fMRI signal values for specific region and event fmri_subset = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") so.Plot(fmri_subset, \\"timepoint\\", \\"signal\\").add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\").show() # Plot fMRI signal values with error band p = so.Plot(fmri_subset, \\"timepoint\\", \\"signal\\").add(so.Line(), so.Agg()) p.add(so.Band(), so.Est(), group=\\"event\\").show() # Combined plot with lines, markers, color, and linestyle p_comb = so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") p_comb.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None).show() # Call the function to create plots create_plots() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_plots(): # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Plot Dow Jones Index price over time so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\").add(so.Line()).show() # Plot fMRI signal values for specific region and event fmri_subset = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") so.Plot(fmri_subset, x=\\"timepoint\\", y=\\"signal\\").add(so.Line(), group=\\"subject\\").show() # Plot fMRI signal values with error band p = so.Plot(fmri_subset, x=\\"timepoint\\", y=\\"signal\\").add(so.Line(), so.Agg()) p.add(so.Band(), so.Est(), group=\\"event\\").show() # Combined plot with lines, markers, color, and linestyle p_comb = so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") p_comb.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None).show() # Call the function to create plots create_plots()"},{"question":"# PyTorch CPU Device and Stream Management In this assessment, you will create a set of functions to manage CPU devices and streams using PyTorch. This task will help demonstrate your understanding of the `torch.cpu` module and its associated functions. Task Requirements: 1. **Check Device Availability** Write a function `check_device_availability()` that returns `True` if a CPU device is available, and `False` otherwise. ```python def check_device_availability(): Returns True if a CPU device is available, False otherwise. pass ``` 2. **Get Current Device** Write a function `get_current_device()` that returns the identifier of the current CPU device. ```python def get_current_device(): Returns the identifier of the current CPU device. pass ``` 3. **Set Device** Write a function `set_current_device(device_id)` that sets the current CPU device to the given `device_id`. ```python def set_current_device(device_id): Sets the current CPU device to the given `device_id`. Args: device_id (int): The identifier of the new current CPU device. pass ``` 4. **Get Current Stream** Write a function `get_current_stream()` that returns the current stream for the current CPU device. ```python def get_current_stream(): Returns the current stream for the current CPU device. pass ``` 5. **Synchronize Devices** Write a function `synchronize_devices()` that synchronizes all streams on all CPU devices. ```python def synchronize_devices(): Synchronizes all streams on all CPU devices. pass ``` 6. **Stream Context Manager** Write a class `StreamContextManager` that acts as a context manager for a CPU stream. ```python class StreamContextManager: def __init__(self, stream): Initializes the context manager with the given stream. Args: stream : The CPU stream to be managed by the context manager. pass def __enter__(self): Enters the runtime context related to this object. pass def __exit__(self, exc_type, exc_value, traceback): Exits the runtime context related to this object. pass ``` Constraints: - You must use the provided `torch.cpu` module for all operations. - Assume that the CPU device identifiers are integers starting from 0. - Handle any potential exceptions that might occur during device and stream operations. Evaluation: Your implementation will be evaluated based on correctness, proper utilization of the `torch.cpu` module, and exception handling. Good luck!","solution":"import torch def check_device_availability(): Returns True if a CPU device is available, False otherwise. # For CPU, PyTorch does not have a specific function to check availability because CPU is always available. return torch.cuda.is_available() or hasattr(torch, \'cpu\') def get_current_device(): Returns the identifier of the current CPU device. # PyTorch does not use device IDs for CPU, as it does for CUDA devices; so we will return 0 as CPU\'s id. return 0 def set_current_device(device_id): Sets the current CPU device to the given device_id. Args: device_id (int): The identifier of the new current CPU device. if device_id != 0: raise ValueError(\\"Invalid device ID for CPU. CPU device ID must be 0.\\") def get_current_stream(): Returns the current stream for the current CPU device. # CPU does not support streams like CUDA, thus we return a simple None. return None def synchronize_devices(): Synchronizes all streams on all CPU devices. # CPU operations are synchronous by default, so there is nothing to sync. pass class StreamContextManager: def __init__(self, stream): Initializes the context manager with the given stream. Args: stream : The CPU stream to be managed by the context manager. # No operation for CPU streams. self.stream = stream def __enter__(self): Enters the runtime context related to this object. # No operation for CPU streams. pass def __exit__(self, exc_type, exc_value, traceback): Exits the runtime context related to this object. # No operation for CPU streams. pass"},{"question":"**Objective**: Extend and utilize the `sndhdr` module to determine and process sound file metadata. **Background**: The `sndhdr` module, now deprecated, provides utility functions to assess sound file metadata. Functions `sndhdr.what` and `sndhdr.whathdr` return a `namedtuple` with attributes `filetype`, `framerate`, `nchannels`, `nframes`, and `sampwidth`. These attributes hold essential data related to the sound file. **Task**: Write a Python function `process_sound_file_metadata(files: List[str]) -> Dict[str, Dict[str, Any]]` that processes a list of sound files. For each file, use the `sndhdr.what` function to obtain its metadata, and then return a dictionary with the filename as the key and another dictionary as the value. This inner dictionary should map each metadata attribute to its corresponding value. If a file\'s metadata cannot be determined by `sndhdr.what`, include it in the result with all metadata attributes set to `None`. **Function Signature**: ```python def process_sound_file_metadata(files: List[str]) -> Dict[str, Dict[str, Any]]: ``` **Input**: - `files`: A list of strings, where each string is the path to a sound file. **Output**: - A dictionary where each key is a filename (from the input list), and the value is another dictionary containing metadata attributes (`\\"filetype\\"`, `\\"framerate\\"`, `\\"nchannels\\"`, `\\"nframes\\"`, `\\"sampwidth\\"`). If metadata is not found for a file, all attributes should be set to `None`. **Constraints**: - Assume each file in the list is a valid path to a file. - Assume the provided sound files are supported by `sndhdr`. **Example**: ```python files = [\\"sound1.wav\\", \\"sound2.aiff\\", \\"sound3.mp3\\"] result = process_sound_file_metadata(files) # Example output might look like: # { # \\"sound1.wav\\": {\\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 100000, \\"sampwidth\\": 16}, # \\"sound2.aiff\\": {\\"filetype\\": \\"aiff\\", \\"framerate\\": 48000, \\"nchannels\\": 2, \\"nframes\\": 50000, \\"sampwidth\\": 24}, # \\"sound3.mp3\\": {\\"filetype\\": None, \\"framerate\\": None, \\"nchannels\\": None, \\"nframes\\": None, \\"sampwidth\\": None} # } ``` **Notes**: - Ensure the function utilizes `sndhdr.what` or `sndhdr.whathdr` to determine the metadata. - Iterate through the list of files, collect metadata, and structure them according to the specified output format. - Test the function with various sound files to verify correctness. **Hint**: You might want to define helper functions to keep your code clean and modularized.","solution":"import sndhdr from typing import List, Dict, Any def process_sound_file_metadata(files: List[str]) -> Dict[str, Dict[str, Any]]: metadata = {} for file in files: info = sndhdr.what(file) if info is None: metadata[file] = { \\"filetype\\": None, \\"framerate\\": None, \\"nchannels\\": None, \\"nframes\\": None, \\"sampwidth\\": None } else: metadata[file] = { \\"filetype\\": info.filetype, \\"framerate\\": info.framerate, \\"nchannels\\": info.nchannels, \\"nframes\\": info.nframes, \\"sampwidth\\": info.sampwidth } return metadata"},{"question":"You are required to implement a class that mimics the behavior of the `email.encoders` module described in the documentation. This class will allow the user to encode the payload of an email message object using different encoding schemes and set the appropriate `Content-Transfer-Encoding` header. Class Specification ```python class PayloadEncoder: def encode_quopri(self, msg): Encodes the payload into quoted-printable form and sets the Content-Transfer-Encoding header to \\"quoted-printable\\". pass def encode_base64(self, msg): Encodes the payload into base64 form and sets the Content-Transfer-Encoding header to \\"base64\\". pass def encode_7or8bit(self, msg): Sets the Content-Transfer-Encoding header to either \\"7bit\\" or \\"8bit\\" as appropriate, based on the payload data. pass def encode_noop(self, msg): Does nothing; it doesn\'t even set the Content-Transfer-Encoding header. pass ``` Message Object Specification The `msg` object passed to the encoding methods will have the following attributes: - `payload` (string): The content of the message. - `content_transfer_encoding` (string): Header to specify the encoding type. Each encoding method should modify the `msg` object as described in the documentation. Example Usage ```python class Message: def __init__(self, payload): self.payload = payload self.content_transfer_encoding = None msg = Message(\\"hello world\\") encoder = PayloadEncoder() encoder.encode_base64(msg) print(msg.payload) # Should print the base64 encoded content of \\"hello world\\" print(msg.content_transfer_encoding) # Should print \\"base64\\" ``` # Constraints: - You should not use the built-in `email.encoders` module directly in your implementation. - You should handle different payloads correctly as specified (e.g., binary data, mostly printable text with a few unprintable characters). - Assume that `msg.payload` will always be valid string data. Notes: - For `encode_quopri()`, you may use the `quopri` module in Python for quoted-printable encoding. - For `encode_base64()`, you may use the `base64` module in Python for base64 encoding. - For `encode_7or8bit()`, you can determine if all characters in `msg.payload` are ASCII characters (7-bit) or if there are non-ASCII characters (8-bit). - For `encode_noop()`, ensure no changes are made to the `msg` object.","solution":"import quopri import base64 class PayloadEncoder: def encode_quopri(self, msg): Encodes the payload into quoted-printable form and sets the Content-Transfer-Encoding header to \\"quoted-printable\\". msg.payload = quopri.encodestring(msg.payload.encode()).decode() msg.content_transfer_encoding = \\"quoted-printable\\" def encode_base64(self, msg): Encodes the payload into base64 form and sets the Content-Transfer-Encoding header to \\"base64\\". msg.payload = base64.b64encode(msg.payload.encode()).decode() msg.content_transfer_encoding = \\"base64\\" def encode_7or8bit(self, msg): Sets the Content-Transfer-Encoding header to either \\"7bit\\" or \\"8bit\\" as appropriate, based on the payload data. if all(ord(char) < 128 for char in msg.payload): msg.content_transfer_encoding = \\"7bit\\" else: msg.content_transfer_encoding = \\"8bit\\" def encode_noop(self, msg): Does nothing; it doesn\'t even set the Content-Transfer-Encoding header. pass class Message: def __init__(self, payload): self.payload = payload self.content_transfer_encoding = None"},{"question":"**Email Message Serialization Challenge** You have been provided with the `email.generator` module documentation, which details how to create serialized representations of email messages using different generator classes. Your task is to implement a function that takes an email message and flattens it into a standard serialized byte stream using the `BytesGenerator` class. # Objective Write a function `serialize_email_message` that takes the following input: - `msg`: An `EmailMessage` object representing the email to be serialized. - `policy` (optional): A policy for controlling the serialization behavior. If not provided, the default policy should be used. The function should return: - A byte string comprising the serialized representation of the email message. # Constraints - The `EmailMessage` object (`msg`) provided will be well-formed and conform to the appropriate RFC standards. - The function should handle both MIME and non-MIME messages gracefully. - You must handle non-ASCII content appropriately, ensuring compliance with ASCII-compatible formats. # Implementation Requirements - Use the `BytesGenerator` class to flatten the message. - Ensure that the `policy` argument, if provided, is correctly utilized in the `BytesGenerator`. # Example Usage Here is how your function should behave: ```python from email.generator import BytesGenerator from email.policy import default def serialize_email_message(msg, policy=None): if policy is None: policy = default import io output = io.BytesIO() generator = BytesGenerator(output, policy=policy) generator.flatten(msg) return output.getvalue() # Usage from email.message import EmailMessage msg = EmailMessage() msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' msg.set_content(\'This is a test email.\') byte_stream = serialize_email_message(msg) print(byte_stream) ``` # Notes - The `BytesGenerator` class\'s `flatten()` method is instrumental here, and the byte stream returned should be a complete and precise serialization of the `EmailMessage` object as per the specified (or default) policy. - Consider edge cases such as emails with attachments, non-textual data, and various content-transfer encodings. # Evaluation Criteria - Correct usage of the `email.generator.BytesGenerator` and handling of optional policy. - Accurate serialization of email messages into byte streams. - Code readability and proper handling of edge cases.","solution":"from email.generator import BytesGenerator from email.policy import default from email.message import EmailMessage import io def serialize_email_message(msg, policy=None): Serializes an email message into a byte string. Args: - msg: An EmailMessage object to be serialized. - policy: An (optional) policy to control serialization. Returns: - A byte string representing the serialized email message. if policy is None: policy = default output = io.BytesIO() generator = BytesGenerator(output, policy=policy) generator.flatten(msg) return output.getvalue()"},{"question":"Objective Your task is to create an automated Python script that uses the `pydoc` module to generate HTML documentation from a given module and serve it via a local HTTP server. This will assess your ability to work with `pydoc`, manage file operations, and handle basic HTTP server functionalities. Problem Statement Write a Python function `generate_and_serve_docs(module_name: str, port: int) -> bool` that performs the following tasks: 1. Generates the HTML documentation for the specified module. 2. Saves the generated HTML documentation to a file named `docs.html`. 3. Starts an HTTP server that serves the generated `docs.html` file on the specified port. 4. Opens the default web browser to display the served documentation. If the function succeeds in all steps, it should return `True`. If any step fails, return `False`. Input - `module_name`: A string representing the name of the module for which documentation should be generated. - `port`: An integer representing the port number on which the HTTP server should run. Output - A boolean value indicating the success (`True`) or failure (`False`) of the operation. Constraints - The specified module must be importable in the current Python environment. - The port number must be an integer within the valid range for HTTP servers (1-65535). Example ```python result = generate_and_serve_docs(\'math\', 8080) print(result) # Expected Output: True # This will generate documentation for the math module, save it to docs.html, start an HTTP server on port 8080, and open the web browser to display the documentation. ``` Additional Information - Ensure that your script writes the HTML documentation to a file named `docs.html` in the current working directory. - Use appropriate exception handling to catch and handle potential errors (e.g., module import errors, file I/O errors, server startup issues). - The script should use `pydoc` functionalities to generate and manage the documentation. Good luck!","solution":"import pydoc import http.server import socketserver import webbrowser import os def generate_and_serve_docs(module_name, port): try: # Generate the HTML documentation html_content = pydoc.HTMLDoc().docmodule(__import__(module_name)) # Save the generated HTML documentation to a file named docs.html with open(\'docs.html\', \'w\') as file: file.write(html_content) # Define the HTTP server handler class Handler(http.server.SimpleHTTPRequestHandler): def do_GET(self): if self.path == \'/\': self.path = \'/docs.html\' return http.server.SimpleHTTPRequestHandler.do_GET(self) # Start the HTTP server httpd = socketserver.TCPServer((\\"\\", port), Handler) # Open the default web browser to display the served documentation webbrowser.open(f\'http://localhost:{port}\') # Serve the file print(f\\"Serving documentation at http://localhost:{port}\\") httpd.serve_forever() return True except Exception as e: print(f\\"Error: {e}\\") return False"},{"question":"# Advanced Randomness Simulation Task Objective: You will create a simulation to model the wait times in a multi-server queue (like a call center) and analyze the results to determine performance metrics. Task: Implement a function `simulate_call_center(arrival_interval, service_time_mean, service_time_stdev, num_servers, num_customers)` that simulates customers arriving at a call center and being served by multiple servers. The function should: 1. Simulate the arrival of customers using an exponential distribution with the given `arrival_interval` (average time between customer arrivals). 2. Simulate the service times using a normal distribution with the given `service_time_mean` (mean service time) and `service_time_stdev` (standard deviation of the service time). 3. Serve customers using a specified number of servers (`num_servers`). 4. Track the wait times for each customer (time from arrival to being served). 5. Return the maximum wait time, the average wait time, and the quartiles of the wait times after serving the specified number of customers (`num_customers`). Expected Function Signature: ```python def simulate_call_center(arrival_interval: float, service_time_mean: float, service_time_stdev: float, num_servers: int, num_customers: int) -> dict: # Your code here ``` Input: - `arrival_interval` (float): The average time between customer arrivals, in seconds. Must be a positive number. - `service_time_mean` (float): The mean time for serving a customer, in seconds. Must be a positive number. - `service_time_stdev` (float): The standard deviation of service times, in seconds. Must be a positive number. - `num_servers` (int): The number of servers available. Must be a positive integer. - `num_customers` (int): The number of customers to simulate. Must be a positive integer. Output: A dictionary with the following keys: - `max_wait`: Maximum wait time among all customers (float, in seconds). - `average_wait`: Average wait time (float, in seconds). - `quartiles`: A list containing the first, second (median), and third quartiles of the wait times (list of three floats, in seconds). Constraints: - Use the `random` module to generate random numbers. - Ensure that your function performs efficiently for up to 1 million customers. Example: ```python simulate_call_center( arrival_interval=5.6, service_time_mean=15.0, service_time_stdev=3.5, num_servers=3, num_customers=1000 ) ``` Expected Output (example): ```python { \\"max_wait\\": 120.5, \\"average_wait\\": 30.2, \\"quartiles\\": [10.0, 25.0, 45.0] } ``` Notes: - Use a priority queue (or other suitable data structure) to manage server availability efficiently. - Consider edge cases like zero wait times when all servers are immediately available. - You are encouraged to break down your solution into helper functions for clarity and maintainability.","solution":"import random import heapq from statistics import median def simulate_call_center(arrival_interval, service_time_mean, service_time_stdev, num_servers, num_customers): wait_times = [] server_heap = [] current_time = 0 for _ in range(num_customers): arrival_time = current_time + random.expovariate(1.0 / arrival_interval) service_time = max(0, random.gauss(service_time_mean, service_time_stdev)) # Remove servers that have finished servicing before or at this arrival time while server_heap and server_heap[0] <= arrival_time: heapq.heappop(server_heap) if len(server_heap) < num_servers: # A server is immediately available start_service_time = arrival_time else: # The earliest we can start servicing is when the next server is free start_service_time = heapq.heappop(server_heap) wait_time = start_service_time - arrival_time wait_times.append(wait_time) heapq.heappush(server_heap, start_service_time + service_time) current_time = arrival_time wait_times.sort() average_wait = sum(wait_times) / num_customers max_wait = max(wait_times) q1 = wait_times[len(wait_times) // 4] q2 = median(wait_times) q3 = wait_times[3 * len(wait_times) // 4] return { \\"max_wait\\": max_wait, \\"average_wait\\": average_wait, \\"quartiles\\": [q1, q2, q3] }"},{"question":"**Objective**: Demonstrate your understanding of the `ossaudiodev` module by implementing a function that records audio from the microphone and plays it back using the OSS audio interface. # Problem Statement Implement a function `record_and_playback(duration: int, filename: str) -> None` that records audio from the microphone for a given duration and then plays it back using the OSS audio driver. The recorded audio should also be saved to a specified file. Input: - `duration` (int): Duration in seconds for which the audio should be recorded. - `filename` (str): The name of the file where the recorded audio should be saved. Output: - The function does not return any value but should save the recorded audio to the specified file and then play it back using the audio device. Requirements: 1. Open an audio device for recording. 2. Set the audio format to `AFMT_S16_LE` (signed 16-bit little-endian). 3. Set the number of channels to 1 (mono). 4. Set the sample rate to 44100 Hz (CD quality audio). 5. Record audio from the microphone for the specified duration. 6. Save the recorded audio to the specified file. 7. Open an audio device for playback. 8. Play the recorded audio from the file. Constraints: 1. Assume that the environment has the OSS audio interface available. 2. Handle any potential errors raised by the `ossaudiodev` module. 3. Use a buffer size of 1024 bytes for reading and writing audio data. # Example Usage: ```python record_and_playback(5, \'output.wav\') ``` This would record audio from the microphone for 5 seconds, save it to `output.wav`, and then play it back. # Notes: - While implementing the function, refer to the `ossaudiodev` module\'s documentation for details on method usage. - It is recommended to use context managers (i.e., `with` statements) to ensure that audio devices are properly closed. ```python import ossaudiodev def record_and_playback(duration: int, filename: str) -> None: # Record audio from microphone dsp_in = ossaudiodev.open(\'r\') dsp_in.setparameters(ossaudiodev.AFMT_S16_LE, 1, 44100) with open(filename, \'wb\') as f: for _ in range(int(44100 * duration / 1024)): data = dsp_in.read(1024) f.write(data) dsp_in.close() # Play back the recorded audio dsp_out = ossaudiodev.open(\'w\') dsp_out.setparameters(ossaudiodev.AFMT_S16_LE, 1, 44100) with open(filename, \'rb\') as f: while True: data = f.read(1024) if not data: break dsp_out.write(data) dsp_out.close() ``` Make sure to thoroughly test your implementation to ensure it handles different scenarios and exceptions gracefully.","solution":"import ossaudiodev import wave def record_and_playback(duration: int, filename: str) -> None: Record audio from microphone and play it back using the OSS audio driver. Args: duration (int): Duration in seconds for which the audio should be recorded. filename (str): The name of the file where the recorded audio should be saved. Returns: None try: # Open an audio device for recording dsp_in = ossaudiodev.open(\'r\') dsp_in.setparameters(ossaudiodev.AFMT_S16_LE, 1, 44100) with wave.open(filename, \'wb\') as wf: wf.setnchannels(1) wf.setsampwidth(2) wf.setframerate(44100) num_frames = 44100 * duration buffer = [] for _ in range(int(num_frames / 1024) + 1): data = dsp_in.read(1024) buffer.append(data) wf.writeframes(b\'\'.join(buffer)) dsp_in.close() # Open an audio device for playback dsp_out = ossaudiodev.open(\'w\') dsp_out.setparameters(ossaudiodev.AFMT_S16_LE, 1, 44100) with wave.open(filename, \'rb\') as wf: while True: data = wf.readframes(1024) if not data: break dsp_out.write(data) dsp_out.close() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Implement a distributed neural network training loop using PyTorch\'s distributed autograd and distributed optimizer. Your task is to design a simple linear regression model distributed across two nodes and perform training using the gradients computed from distributed autograd. Task Description 1. **Setup**: - Initialize two nodes using `torch.distributed.rpc`. - Partition the linear regression model such that each node holds part of the model. 2. **Forward Pass**: - Implement a forward pass that performs computations across both nodes. 3. **Backward Pass**: - Use the distributed autograd context to compute gradients. 4. **Optimization**: - Use the distributed optimizer to update the model parameters based on the computed gradients. Requirements - Use `torch.distributed.rpc` to initialize two nodes and distribute computations. - Implement the forward and backward passes within the distributed autograd context. - Utilize `torch.distributed.optim.DistributedOptimizer` to apply gradient updates. Constraints & Limitations - You must ensure that the forward and backward computations are properly synchronized across nodes. - The gradient accumulation in the distributed autograd context should be correctly handled. - The solution must handle network communication efficiently to avoid bottlenecks. Input/Output - **Input**: None (the script should be self-contained). - **Output**: Print the loss value at each iteration. Performance Requirements - The solution should be efficient in terms of network communication and gradient computation. Example ```python import torch import torch.distributed.autograd as dist_autograd from torch.distributed import rpc from torch import optim from torch.distributed.optim import DistributedOptimizer import torch.multiprocessing as mp def run_training_loop(rank, world_size): # Initialize RPC rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size) # Define the model parts and data linear_layer_local = torch.nn.Linear(3, 3).to(f\\"worker{rank}\\") # Dummy data data = torch.rand(3, 3) target = torch.rand(3, 3) with dist_autograd.context() as context_id: # Forward Pass pred = linear_layer_local(data) loss = torch.nn.functional.mse_loss(pred, target) # Backward Pass dist_autograd.backward(context_id, [loss]) # Distributed Optimizer dist_optim = DistributedOptimizer(optim.SGD, [linear_layer_local], lr=0.01) dist_optim.step(context_id) print(f\\"Loss: {loss.item()}\\") rpc.shutdown() if __name__ == \\"__main__\\": world_size = 2 mp.spawn(run_training_loop, args=(world_size,), nprocs=world_size) ``` Note - You could use `mp.spawn` to initialize multiple processes for simulating the nodes. - Make sure to handle the RPC initialization and shutdown properly to avoid any resource leaks.","solution":"import torch import torch.distributed.autograd as dist_autograd from torch.distributed import rpc from torch import optim from torch.distributed.optim import DistributedOptimizer import torch.multiprocessing as mp import torch.nn as nn import torch.nn.functional as F class PartitionedModelPart1(nn.Module): def __init__(self): super(PartitionedModelPart1, self).__init__() self.linear1 = nn.Linear(3, 2) def forward(self, x): return self.linear1(x) class PartitionedModelPart2(nn.Module): def __init__(self): super(PartitionedModelPart2, self).__init__() self.linear2 = nn.Linear(2, 1) def forward(self, x): return self.linear2(x) def run_training_loop(rank, world_size): # Initialize RPC rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size) # Define the model parts if rank == 0: model_part = PartitionedModelPart1().to(f\\"worker{rank}\\") else: model_part = PartitionedModelPart2().to(f\\"worker{rank}\\") if rank == 0: # Dummy data data = torch.rand(10, 3) target = torch.rand(10, 1) for epoch in range(10): with dist_autograd.context() as context_id: if rank == 0: # Forward pass on the first part of the model outputs = model_part(data) # Forward pass on the second part via RPC pred = rpc.rpc_sync(\\"worker1\\", PartitionedModelPart2.forward, (model_part, outputs)) loss = F.mse_loss(pred, target) # Backward pass dist_autograd.backward(context_id, [loss]) # Distributed optimizer dist_optim = DistributedOptimizer( optim.SGD, rpc.RRef(model_part).parameters(), lr=0.01) dist_optim.step() print(f\\"Epoch {epoch}, Loss: {loss.item()}\\") else: # No forward or backward for worker 1, just keeping synchronization pass rpc.shutdown() if __name__ == \\"__main__\\": world_size = 2 mp.spawn(run_training_loop, args=(world_size,), nprocs=world_size, join=True)"},{"question":"**Question:** You are tasked with creating a Python script to manage the compilation of Python source files in a more customizable way using the `compileall` module. Your task is to write a function `custom_compile` that will: 1. Traverse a given directory and all its subdirectories to find Python source files (`.py`). 2. Compile these source files into bytecode files. 3. Provide options to: - Exclude files whose names match a given regular expression pattern. - Control the verbosity of the output. - Define the maximum recursion level for subdirectories. - Specify whether to force recompilation regardless of timestamps. - Compile using a specific optimization level. - Use multiple worker threads to speed up the process. **Function Signature:** ```python def custom_compile(directory: str, exclude_pattern: str = None, quiet: int = 0, maxlevels: int = None, force: bool = False, optimize: int = -1, workers: int = 1) -> bool: pass ``` **Parameters:** - `directory` (str): The root directory to start the traversal. - `exclude_pattern` (str): A regular expression pattern to exclude files (default is None, meaning no files are excluded). - `quiet` (int): Verbosity level (0 - all output, 1 - errors only, 2 - no output). - `maxlevels` (int): Maximum recursion depth for subdirectories (default is the system\'s recursion limit). - `force` (bool): If True, force recompilation even if timestamps are up-to-date (default is False). - `optimize` (int): Optimization level to use during compilation (-1 for default behavior, 0 for no optimization, 1 for basic optimization, 2 for additional optimizations). - `workers` (int): Number of workers to use for parallel compilation (default is 1; if set to 0, the number of cores in the system is used). **Returns:** - `bool`: True if all files compiled successfully, False otherwise. **Constraints:** - You should handle invalid input parameters gracefully. - Use appropriate exception handling to manage any runtime errors that occur during file access or compilation. - Ensure compatibility with different operating systems and Python environments. **Example:** ```python import re from compileall import compile_dir def custom_compile(directory: str, exclude_pattern: str = None, quiet: int = 0, maxlevels: int = None, force: bool = False, optimize: int = -1, workers: int = 1) -> bool: rx = re.compile(exclude_pattern) if exclude_pattern else None return compile_dir( dir=directory, maxlevels=maxlevels, ddir=None, force=force, rx=rx, quiet=quiet, legacy=False, optimize=optimize, workers=workers, invalidation_mode=None, stripdir=None, prependdir=None, limit_sl_dest=None, hardlink_dupes=False ) # Example usage: success = custom_compile(\'path/to/directory\', exclude_pattern=r\'[/]test_\', quiet=1, maxlevels=3, force=True, optimize=2, workers=4) print(\\"Compilation successful:\\", success) ``` Implement the `custom_compile` function as specified to demonstrate a thorough understanding of the `compileall` module.","solution":"import re import compileall import os import multiprocessing def custom_compile(directory: str, exclude_pattern: str = None, quiet: int = 0, maxlevels: int = None, force: bool = False, optimize: int = -1, workers: int = 1) -> bool: Traverses a given directory and compiles all Python source files into bytecode files. Parameters: directory (str): The root directory to start the traversal. exclude_pattern (str): A regular expression pattern to exclude files (default is None). quiet (int): Verbosity level (0 - all output, 1 - errors only, 2 - no output). maxlevels (int): Maximum recursion depth for subdirectories (default is None). force (bool): If True, force recompilation even if timestamps are up-to-date (default is False). optimize (int): Optimization level during compilation (-1 for default behavior, 0 for no optimization, 1 for basic optimization, 2 for additional optimizations). workers (int): Number of workers for parallel compilation (default is 1; 0 is system\'s core number). Returns: bool: True if all files compiled successfully, False otherwise. if not os.path.isdir(directory): raise ValueError(f\\"Invalid directory: {directory}\\") worker_count = workers if workers != 0 else multiprocessing.cpu_count() rx = re.compile(exclude_pattern) if exclude_pattern else None return compileall.compile_dir( dir=directory, maxlevels=maxlevels, ddir=None, force=force, rx=rx, quiet=quiet, legacy=False, optimize=optimize, workers=worker_count )"},{"question":"**Objective:** Implement a Python function using the Python C API to dynamically create, import, and manipulate modules. Your function should demonstrate a deep understanding of the import mechanisms and module management in Python. **Problem Statement:** You are required to implement a function `dynamic_module_operations(module_name, module_code)` that performs the following tasks: 1. Creates a new module with the given `module_name`. 2. Adds the provided code (`module_code`) to the created module. 3. Imports the created module into the current namespace. 4. Executes a function named `test_function` from the created module and returns its result. The function should handle any exceptions that arise during the import or execution process, returning an appropriate error message. **Function Signature:** ```python def dynamic_module_operations(module_name: str, module_code: str) -> str: pass ``` **Parameters:** - `module_name` (str): The name of the module to be created. - `module_code` (str): The code to be added to the created module. **Returns:** - (str): The result of the executed `test_function` from the created module, or an error message if an exception occurs. **Example:** ```python module_name = \\"my_module\\" module_code = \'\'\' def test_function(): return \\"Hello from my_module!\\" \'\'\' result = dynamic_module_operations(module_name, module_code) print(result) # Output: \\"Hello from my_module!\\" ``` **Constraints:** - The `module_name` should be a valid Python identifier. - The `module_code` should define a function named `test_function` that takes no arguments. - Consider thread-safety and module reloading issues while implementing the function. **Note:** - You are required to use the Python C API functions documented above to perform these tasks. - Ensure that the code handles any potential edge cases, such as module name conflicts and invalid module code. **Evaluation Criteria:** - Correct implementation of the required functionality. - Proper handling of exceptions and errors. - Efficient use and understanding of the Python C API import functions. - Clarity and readability of the code.","solution":"import importlib.util import sys import types def dynamic_module_operations(module_name: str, module_code: str) -> str: Creates a new module with the given `module_name`, adds provided code `module_code`, executes `test_function` from the created module and returns its result. try: # Check if the module name is a valid identifier if not module_name.isidentifier(): return \\"Error: Invalid module name\\" # Create a new module new_module = types.ModuleType(module_name) # Add the module code to the new module exec(module_code, new_module.__dict__) # Add the new module to sys.modules sys.modules[module_name] = new_module # Import the newly created module module = importlib.import_module(module_name) # Execute the test_function from the created module if hasattr(module, \'test_function\'): return module.test_function() else: return \\"Error: \'test_function\' not found in the module\\" except Exception as e: return f\\"Error: {str(e)}\\" # Example usage if __name__ == \'__main__\': module_name = \\"my_module\\" module_code = \'\'\' def test_function(): return \\"Hello from my_module!\\" \'\'\' result = dynamic_module_operations(module_name, module_code) print(result) # Output: \\"Hello from my_module!\\""},{"question":"You are required to demonstrate your understanding of the seaborn.objects interface by performing the following tasks: 1. Load the \'penguins\' dataset provided by seaborn. 2. Filter the dataset to remove any rows where the value for \'flipper_length_mm\' is missing. 3. Sort the dataset by \'species\' in ascending order. 4. Create a faceted plot that compares the distribution of \'flipper_length_mm\' for each species for male and female penguins. The distribution should be shown using stacked histograms with a bin width of 5. The histograms should be placed side-by-side for comparison. Here\'s a step-by-step breakdown of what is expected: - **Step 1:** Load the seaborn library and the \'penguins\' dataset. - **Step 2:** Filter the dataset to remove rows with missing \'flipper_length_mm\'. - **Step 3:** Sort the dataset by \'species\' in ascending order. - **Step 4:** Create a faceted plot comparing the distribution of \'flipper_length_mm\' by \'species\' and \'sex\' using stacked histograms. Your output should be a well-labeled faceted plot. ```python # Your code starts here import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Step 2: Filter out rows with missing \'flipper_length_mm\' penguins = penguins.dropna(subset=[\'flipper_length_mm\']) # Step 3: Sort the dataset by \'species\' in ascending order penguins = penguins.sort_values(\'species\') # Step 4: Create the faceted plot with stacked histograms plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=5), so.Stack()) ) plot.show() # Your code ends here ``` **Constraints:** - Use seaborn\'s objects interface and the Plot method for creating the visualization. - Ensure the bin width for histograms is set to 5. - The plot should be properly labeled and easy to interpret. **Performance Considerations:** - Your code should handle the dataset filtering and sorting efficiently. - The visual output should be clear and provide useful insights into the penguin species\' flipper length distribution.","solution":"import seaborn.objects as so from seaborn import load_dataset def load_and_process_penguins(): Loads the penguins dataset, filters out rows with missing \'flipper_length_mm\', and sorts the dataset by \'species\'. Returns: pandas.DataFrame: The processed penguins dataset. # Step 1: Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Step 2: Filter out rows with missing \'flipper_length_mm\' penguins = penguins.dropna(subset=[\'flipper_length_mm\']) # Step 3: Sort the dataset by \'species\' in ascending order penguins = penguins.sort_values(\'species\') return penguins def create_faceted_plot(penguins): Creates a faceted plot comparing the distribution of \'flipper_length_mm\' by \'species\' and \'sex\' using stacked histograms with a bin width of 5. # Step 4: Create the faceted plot with stacked histograms plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=5), so.Stack()) ) plot.show() # Process the penguins dataset penguins = load_and_process_penguins() # Create the faceted plot create_faceted_plot(penguins)"},{"question":"# Color Space Transformations and Averaging In this exercise, you will work with the `colorsys` module to perform color space conversions and compute the average color of a set of colors given in the RGB space. You will need to convert the colors to the HLS space, average them, and convert the result back to the RGB space. **Problem Statement** Write a function `average_rgb_color(colors: List[Tuple[float, float, float]]) -> Tuple[float, float, float]]` that takes a list of RGB colors and computes their average color in the RGB space. The input and output RGB values are floating-point numbers between 0 and 1. # Function Signature ```python def average_rgb_color(colors: list of tuple(float, float, float)) -> tuple(float, float, float): ``` # Parameters - `colors`: A list of RGB colors. Each color is represented as a tuple of three floating-point values (red, green, and blue) in the range [0, 1]. # Returns - A tuple representing the average color in the RGB space. # Constraints 1. The `colors` list will contain at least one color. 2. The `colors` list will not exceed 1000 elements. # Example Input: ```python colors = [(0.2, 0.4, 0.4), (0.8, 0.2, 0.6), (0.5, 0.5, 0.5)] ``` Output: ```python (0.5104166666666666, 0.3701388888888889, 0.49999999999999994) ``` # Instructions 1. Use the `colorsys` module to convert each RGB color to the HLS color space. 2. Compute the average hue, lightness, and saturation of the colors. 3. Convert the average HLS values back to the RGB color space. 4. Return the resulting RGB value as a tuple. # Notes - Since hue is a circular value (0 to 1), you need to handle averaging correctly to avoid discontinuity issues. - Ensure to round the output values to an appropriate level of precision for floating point operations. # Testing You can test your function with different sets of RGB values to ensure correctness. Here are a few test cases: ```python # Test Case 1 colors = [(0.2, 0.4, 0.4), (0.8, 0.2, 0.6), (0.5, 0.5, 0.5)] # Expected output: (0.5104166666666666, 0.3701388888888889, 0.49999999999999994) # Test Case 2 colors = [(0.1, 0.2, 0.3), (0.9, 0.8, 0.7)] # Expected output: (0.4765625, 0.47421875, 0.5203125000000001) # Test Case 3 colors = [(0.5, 0.5, 0.5), (0.5, 0.5, 0.5)] # Expected output: (0.5, 0.5, 0.5) ``` Implement the function `average_rgb_color` to complete this coding assessment question.","solution":"import colorsys from typing import List, Tuple def average_rgb_color(colors: List[Tuple[float, float, float]]) -> Tuple[float, float, float]: if not colors: raise ValueError(\\"Empty color list\\") avg_r = avg_g = avg_b = 0.0 for color in colors: avg_r += color[0] avg_g += color[1] avg_b += color[2] count = len(colors) avg_r /= count avg_g /= count avg_b /= count return (avg_r, avg_g, avg_b)"},{"question":"# Objective: Design a file compression utility that can compress files using different algorithms based on user input. Your tool should support compression using `gzip`, `bz2`, and `lzma`. # Task: Implement a Python function `compress_file(file_path: str, output_path: str, algorithm: str) -> None` that takes the following inputs: - `file_path`: A string representing the path to the input file that needs to be compressed. - `output_path`: A string representing the path where the compressed file should be saved. - `algorithm`: A string indicating which compression algorithm to use. It can be either `\\"gzip\\"`, `\\"bz2\\"`, or `\\"lzma\\"`. # Requirements: 1. **Gzip Compression**: If the algorithm is `\\"gzip\\"`, compress the file using the `gzip` module. 2. **Bz2 Compression**: If the algorithm is `\\"bz2\\"`, compress the file using the `bz2` module. 3. **Lzma Compression**: If the algorithm is `\\"lzma\\"`, compress the file using the `lzma` module. 4. Handle file reading and writing properly. 5. Ensure the compressed file is saved at the specified `output_path`. # Constraints: - Assume that the file at `file_path` exists and is readable. - The `algorithm` input will always be one of the specified strings (`\\"gzip\\"`, `\\"bz2\\"`, `\\"lzma\\"`). # Example: ```python compress_file(\\"example.txt\\", \\"example.txt.gz\\", \\"gzip\\") compress_file(\\"example.txt\\", \\"example.txt.bz2\\", \\"bz2\\") compress_file(\\"example.txt\\", \\"example.txt.xz\\", \\"lzma\\") ``` # Performance: - The function should efficiently handle files up to 100MB in size. # Notes: 1. Make sure to use appropriate file modes for reading and writing text or binary data, as required by the compression algorithm modules. 2. Add necessary imports at the beginning of your code. # Suggested Modules: ```python import gzip import bz2 import lzma ``` Implement the function `compress_file` as described above to fulfill the requirements.","solution":"import gzip import bz2 import lzma def compress_file(file_path: str, output_path: str, algorithm: str) -> None: Compresses the file at file_path using the specified algorithm and saves the output to output_path. :param file_path: The path to the input file that needs to be compressed. :param output_path: The path where the compressed file should be saved. :param algorithm: The compression algorithm to use (\'gzip\', \'bz2\', or \'lzma\'). if algorithm == \\"gzip\\": with open(file_path, \'rb\') as f_in: with gzip.open(output_path, \'wb\') as f_out: f_out.writelines(f_in) elif algorithm == \\"bz2\\": with open(file_path, \'rb\') as f_in: with bz2.open(output_path, \'wb\') as f_out: f_out.writelines(f_in) elif algorithm == \\"lzma\\": with open(file_path, \'rb\') as f_in: with lzma.open(output_path, \'wb\') as f_out: f_out.writelines(f_in) else: raise ValueError(\\"Unsupported compression algorithm specified\\")"},{"question":"**Objective**: Demonstrate your ability to use `xml.parsers.expat` for parsing XML documents with appropriate handlers and error management. **Problem Statement**: You have been given an XML string representing a simplistic book collection. Each book has a title, author, and an optional genre. You need to write a Python function that uses `xml.parsers.expat` to parse this XML string and store the parsed information in a dictionary format. The function should also handle parsing errors gracefully and return an error message if any parsing issue occurs. **Function Signature**: ```python def parse_book_collection(xml_string: str) -> dict: ``` **Input**: - `xml_string` (str): A string containing the XML data of the book collection. The XML format is as follows: ```xml <?xml version=\\"1.0\\"?> <collection> <book> <title>Book Title 1</title> <author>Author 1</author> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author 2</author> </book> </collection> ``` **Output**: - (dict): A dictionary where the keys are the book titles and the values are nested dictionaries with keys \'author\' and \'genre\'. If a book does not have a genre, the value for \'genre\' should be \'Unknown\'. In case of a parsing error, return a dictionary with a single key \'error\' and set its value to the error message. **Example**: ```python xml_string = <?xml version=\\"1.0\\"?> <collection> <book> <title>Book Title 1</title> <author>Author 1</author> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author 2</author> </book> </collection> result = parse_book_collection(xml_string) print(result) ``` **Expected Output**: ```python { \\"Book Title 1\\": {\\"author\\": \\"Author 1\\", \\"genre\\": \\"Fiction\\"}, \\"Book Title 2\\": {\\"author\\": \\"Author 2\\", \\"genre\\": \\"Unknown\\"} } ``` **Constraints**: 1. The XML string will be well-formed but might have missing fields (e.g., a book might not have a genre). 2. Assume all book titles are unique. 3. Handle any XML parsing error gracefully. **Notes**: - You must use the `xml.parsers.expat` module for this task. - Set up appropriate handler functions for parsing elements, character data, and handling errors. - Make sure to test your function with different XML strings that include edge cases such as missing fields or invalid XML syntax.","solution":"import xml.parsers.expat def parse_book_collection(xml_string: str) -> dict: collection = {} current_book = {} current_element = None current_title = None def start_element(name, attrs): nonlocal current_element current_element = name def end_element(name): nonlocal current_element, current_book, current_title if name == \'book\': if \'title\' in current_book: collection[current_title] = { \\"author\\": current_book.get(\\"author\\", \\"Unknown\\"), \\"genre\\": current_book.get(\\"genre\\", \\"Unknown\\") } current_book = {} current_title = None current_element = None def char_data(data): nonlocal current_element, current_book, current_title data = data.strip() if not data: return if current_element == \'title\': current_title = data current_book[\'title\'] = data elif current_element == \'author\': current_book[\'author\'] = data elif current_element == \'genre\': current_book[\'genre\'] = data parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_string) return collection except xml.parsers.expat.ExpatError as e: return {\\"error\\": str(e)}"},{"question":"# Advanced Python Asynchronous Programming Assessment **Objective**: To assess your understanding of Python\'s asyncio library and your ability to apply asynchronous programming concepts to implement a network-based task processing system. --- Question: You are tasked with building an asynchronous TCP server that can handle multiple clients concurrently. This server will perform the following operations: 1. Accept connections from multiple clients. 2. For each client, receive tasks that should be processed independently. 3. Use an asyncio queue to manage and process these tasks. 4. Handle potential timeouts for task processing. 5. Use proper synchronization mechanisms to ensure thread-safe operations on shared resources. Please implement the following functions: 1. **start_server(host: str, port: int)**: - Starts the TCP server at the specified `host` and `port`. - Listens for incoming connections and creates a new asyncio task for each client. 2. **handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter)**: - Handles communication with a connected client. - Reads tasks sent by the client, enqueues them for processing, and sends back a response once the task is processed. 3. **process_tasks(queue: asyncio.Queue)**: - Continuously fetches tasks from the queue and processes them. - Each task should be processed with a timeout constraint using `asyncio.wait_for`. 4. **main()**: - Initializes the server, starts the task processing coroutine, and runs the event loop. **Input/Output**: - Each client sends tasks in the form of string messages terminated by newline characters. - The server processes each task and sends back a confirmation message once processed. - Apply a timeout of 5 seconds for processing each task. If a task times out, send a timeout error message to the client. **Constraints**: - The server needs to handle multiple clients concurrently. - Ensure that the shared queue is accessed in a thread-safe manner. **Performance**: - Efficiently handle multiple client connections without blocking. - Implement proper error handling for network and task-related issues. ```python import asyncio async def start_server(host: str, port: int): # TODO: Implement the server initialization logic here server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): # TODO: Implement the client handling logic here try: queue = asyncio.Queue() while True: data = await reader.read(100) message = data.decode().strip() if not message: break await queue.put(message) response = f\\"Task received: {message}\\" writer.write(response.encode()) await writer.drain() except asyncio.CancelledError: pass finally: writer.close() await writer.wait_closed() async def process_tasks(queue: asyncio.Queue): # TODO: Implement the task processing logic here while True: task = await queue.get() try: await asyncio.wait_for(do_something_with(task), timeout=5.0) # Simulate task processing print(f\\"Processed task: {task}\\") except asyncio.TimeoutError: print(f\\"Task processing timed out: {task}\\") async def do_something_with(task): # Simulate an async task processing await asyncio.sleep(1) return f\\"Processed {task}\\" async def main(): # Starting the server and task processing coroutine server_task = asyncio.create_task(start_server(\'127.0.0.1\', 8888)) queue = asyncio.Queue() task_processor = asyncio.create_task(process_tasks(queue)) await asyncio.gather(server_task, task_processor) # Run the main entry point if __name__ == \'__main__\': asyncio.run(main()) ``` --- **Note**: Please ensure to handle exceptions and proper closing of resources where necessary.","solution":"import asyncio async def start_server(host: str, port: int): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): try: while True: data = await reader.read(100) message = data.decode().strip() if not message: break await task_queue.put(message) response = f\\"Task received: {message}n\\" writer.write(response.encode()) await writer.drain() except asyncio.CancelledError: pass finally: writer.close() await writer.wait_closed() async def process_tasks(queue: asyncio.Queue): while True: task = await queue.get() try: await asyncio.wait_for(do_something_with(task), timeout=5.0) print(f\\"Processed task: {task}\\") except asyncio.TimeoutError: print(f\\"Task processing timed out: {task}\\") async def do_something_with(task: str): await asyncio.sleep(1) return f\\"Processed {task}\\" async def main(): global task_queue task_queue = asyncio.Queue() server_task = asyncio.create_task(start_server(\'127.0.0.1\', 8888)) task_processor = asyncio.create_task(process_tasks(task_queue)) await asyncio.gather(server_task, task_processor) if __name__ == \'__main__\': asyncio.run(main())"},{"question":"# Question: Creating a Descriptor and Using It in Custom Classes Descriptors are a powerful, advanced feature in Python that allow you to create managed attributes. By defining special methods in a descriptor class, you can control what happens when a managed attribute is accessed, assigned, or deleted in an owner class. Task: 1. Implement a descriptor class named `LoggedAttribute` that: - Logs a message every time an attribute it manages is accessed, assigned, or deleted. - Stores the attribute value in the instance\'s dictionary using the `__dict__` attribute. 2. Implement a class named `Person` that uses this descriptor to manage its attributes `first_name` and `last_name`. Requirements: - The `LoggedAttribute` class must implement the `__get__`, `__set__`, and `__delete__` methods. - The `__set__` method should accept an attribute name, store the value in that attribute, and log the assignment. - The `__get__` method should return the value and log the attribute access. - The `__delete__` method should delete the attribute and log the deletion. - The `Person` class should use `LoggedAttribute` instances for its `first_name` and `last_name` attributes, ensuring that any access, assignment, or deletion action is logged. Logging Format: - For `__get__`: `\\"Accessing {attr_name}\\"`. - For `__set__`: `\\"Assigning {value} to {attr_name}\\"`. - For `__delete__`: `\\"Deleting {attr_name}\\"`. Example: ```python class LoggedAttribute: # Implement the descriptor methods here class Person: first_name = LoggedAttribute() last_name = LoggedAttribute() # Implement the class here # Example usage: p = Person() p.first_name = \\"John\\" # Output: \\"Assigning John to first_name\\" print(p.first_name) # Output: \\"Accessing first_name\\" # \\"John\\" del p.first_name # Output: \\"Deleting first_name\\" ``` Expectations: - Ensure any interactions follow the logging format. - Utilize `__dict__` appropriately for storing and retrieving attribute values in the descriptor. - Demonstrate correct descriptor usage through the `Person` class attributes.","solution":"class LoggedAttribute: def __get__(self, instance, owner): value = instance.__dict__[self.name] print(f\\"Accessing {self.name}\\") return value def __set__(self, instance, value): instance.__dict__[self.name] = value print(f\\"Assigning {value} to {self.name}\\") def __delete__(self, instance): print(f\\"Deleting {self.name}\\") del instance.__dict__[self.name] def __set_name__(self, owner, name): self.name = name class Person: first_name = LoggedAttribute() last_name = LoggedAttribute() def __init__(self, first_name=\'\', last_name=\'\'): self.first_name = first_name self.last_name = last_name"},{"question":"**Objective:** This question aims to assess the student\'s understanding of Python\'s built-in functions and their ability to manipulate file operations using the \'builtins\' module. **Problem Statement:** You need to create a custom file handler that wraps Python\'s built-in `open` function. Your custom file handler should provide additional functionalities for the files it handles. Specifically, you need to design a class `CustomFileHandler` that processes the file contents in unique ways. **Requirements:** 1. The class `CustomFileHandler` should have the following methods: - `__init__(self, file_path: str, mode: str)`: Initializes the custom file handler, opens the file using the built-in `open` function, and prepares the file for reading or writing. - `read_lines(self) -> list`: Reads all lines from the file and returns them as a list of strings. Each string should be reversed (for example, the line \\"hello\\" should be read as \\"olleh\\"). - `write_lines(self, lines: list)`: Writes a list of strings to the file. Each string should be written in uppercase. - `close(self)`: Closes the file. 2. You must use the built-in `open` function from the \'builtins\' module to ensure there is no conflict with the custom `open` method if you choose to implement one. **Constraints:** - Assume the file contains only text data. - The file paths used in tests will always be valid. - When writing to the file, the file will be overwritten if it already exists. **Input/Output Format:** - Input: `file_path` will be a string representing the path to the file, and `mode` will be a string representing the file opening mode (\'r\' for read, \'w\' for write). - Output: For `read_lines()`, return a list of reversed strings. For `write_lines(lines)`, write the transformed lines to the file. **Example Usage:** ```python # Assume \'example.txt\' contains: # hello # world # Initialize for reading reader = CustomFileHandler(\'example.txt\', \'r\') print(reader.read_lines()) # Output: [\'olleh\', \'dlrow\'] reader.close() # Initialize for writing writer = CustomFileHandler(\'example.txt\', \'w\') writer.write_lines([\'hello\', \'world\']) writer.close() # Verify the write operation reader = CustomFileHandler(\'example.txt\', \'r\') print(reader.read_lines()) # Output: [\'OLLEH\', \'DLROW\'] reader.close() ``` # Constraints: - You are required to use the \'builtins\' module\'s `open` function. - The solution should handle potential exceptions arising from file operations (like file not found, permission errors) gracefully. # Notes: - Do not use third-party libraries. - Focus on handling edge cases, such as empty files or very large files. Good luck!","solution":"class CustomFileHandler: def __init__(self, file_path: str, mode: str): self.file_path = file_path self.mode = mode self.file = open(file_path, mode) def read_lines(self) -> list: lines = self.file.readlines() return [line.strip()[::-1] for line in lines] def write_lines(self, lines: list): transformed_lines = [line.upper() + \'n\' for line in lines] self.file.writelines(transformed_lines) def close(self): if self.file: self.file.close() # Example usage for the solution: # reader = CustomFileHandler(\'example.txt\', \'r\') # print(reader.read_lines()) # Output: [\'olleh\', \'dlrow\'] # reader.close() # writer = CustomFileHandler(\'example.txt\', \'w\') # writer.write_lines([\'hello\', \'world\']) # writer.close() # reader = CustomFileHandler(\'example.txt\', \'r\') # print(reader.read_lines()) # Output: [\'OLLEH\', \'DLROW\'] # reader.close()"},{"question":"**Question: Date-Time Object Manipulation and Extraction** You are required to write functions using the `datetime` module in Python to perform the following tasks: 1. **Create DateTime Objects**: - Define a function `create_datetime(year, month, day, hour, minute, second, microsecond)` that creates a `datetime.datetime` object with the specified parameters. - Define a function `create_date(year, month, day)` that creates a `datetime.date` object with the specified parameters. - Define a function `create_time(hour, minute, second, microsecond)` that creates a `datetime.time` object with the specified parameters. - Define a function `create_timedelta(days, seconds, microseconds)` that creates a `datetime.timedelta` object with the specified parameters. 2. **Field Extraction**: - Define a function `extract_date_fields(date_obj)` that takes a `datetime.date` object and returns its year, month, and day as a tuple. - Define a function `extract_time_fields(time_obj)` that takes a `datetime.time` object and returns its hour, minute, second, and microsecond as a tuple. - Define a function `extract_datetime_fields(datetime_obj)` that takes a `datetime.datetime` object and returns its year, month, day, hour, minute, second, and microsecond as a tuple. - Define a function `extract_timedelta_fields(timedelta_obj)` that takes a `datetime.timedelta` object and returns its days, seconds, and microseconds as a tuple. 3. **Timezone and Type Checking**: - Define a function `get_timezone_utc()` that returns the UTC timezone object. - Define a function `check_date_type(obj)` that returns `True` if the object is of type `datetime.date` or `datetime.datetime`, and `False` otherwise. - Define a function `check_time_type(obj)` that returns `True` if the object is of type `datetime.time`, and `False` otherwise. - Define a function `check_timedelta_type(obj)` that returns `True` if the object is of type `datetime.timedelta`, and `False` otherwise. **Input and Output:** - The function `create_datetime` should take `year, month, day, hour, minute, second, microsecond` as input and return a `datetime.datetime` object. - The function `create_date` should take `year, month, day` as input and return a `datetime.date` object. - The function `create_time` should take `hour, minute, second, microsecond` as input and return a `datetime.time` object. - The function `create_timedelta` should take `days, seconds, microseconds` as input and return a `datetime.timedelta` object. - The function `extract_date_fields` should take a `datetime.date` object and return a tuple `(year, month, day)`. - The function `extract_time_fields` should take a `datetime.time` object and return a tuple `(hour, minute, second, microsecond)`. - The function `extract_datetime_fields` should take a `datetime.datetime` object and return a tuple `(year, month, day, hour, minute, second, microsecond)`. - The function `extract_timedelta_fields` should take a `datetime.timedelta` object and return a tuple `(days, seconds, microseconds)`. - The function `get_timezone_utc` should return the UTC timezone object. - The function `check_date_type` should take an object and return `True` if it is of type `datetime.date` or `datetime.datetime`, `False` otherwise. - The function `check_time_type` should take an object and return `True` if it is of type `datetime.time`, `False` otherwise. - The function `check_timedelta_type` should take an object and return `True` if it is of type `datetime.timedelta`, `False` otherwise. **Constraints:** - Ensure proper error handling for invalid inputs where applicable. - Optimize the functions for performance as required. **Evaluation Criteria:** - Correct implementation of the specified functions. - Efficient handling and extraction of date and time fields. - Proper type checking and usage of `datetime` module features. - Clean and well-documented code.","solution":"from datetime import datetime, date, time, timedelta, timezone def create_datetime(year, month, day, hour, minute, second, microsecond): return datetime(year, month, day, hour, minute, second, microsecond) def create_date(year, month, day): return date(year, month, day) def create_time(hour, minute, second, microsecond): return time(hour, minute, second, microsecond) def create_timedelta(days, seconds, microseconds): return timedelta(days=days, seconds=seconds, microseconds=microseconds) def extract_date_fields(date_obj): return (date_obj.year, date_obj.month, date_obj.day) def extract_time_fields(time_obj): return (time_obj.hour, time_obj.minute, time_obj.second, time_obj.microsecond) def extract_datetime_fields(datetime_obj): return (datetime_obj.year, datetime_obj.month, datetime_obj.day, datetime_obj.hour, datetime_obj.minute, datetime_obj.second, datetime_obj.microsecond) def extract_timedelta_fields(timedelta_obj): return (timedelta_obj.days, timedelta_obj.seconds, timedelta_obj.microseconds) def get_timezone_utc(): return timezone.utc def check_date_type(obj): return isinstance(obj, (date, datetime)) def check_time_type(obj): return isinstance(obj, time) def check_timedelta_type(obj): return isinstance(obj, timedelta)"},{"question":"# Multiclass-Multioutput Classification Task **Objective**: Implement a function to handle multiclass-multioutput classification using scikit-learn\'s `MultiOutputClassifier` combined with an appropriate base estimator. # Problem Statement You are given a dataset with samples that have multiple non-binary properties to classify. Each sample has more than two properties, and each property can take more than two possible classes. Implement a function `multi_output_classifier` that: 1. Receives: - A feature matrix `X` of shape `(n_samples, n_features)`: The input data to train the classifier. - A target matrix `Y` of shape `(n_samples, n_outputs)`: The multiclass-multioutput labels for each sample. - A base estimator `base_estimator` (default: `RandomForestClassifier`): The underlying classifier to use for the multioutput classification. - An integer `random_state` (default: `42`): A parameter to control the random number generator for reproducibility. 2. Outputs: - A trained `MultiOutputClassifier` fitted on the given dataset. # Constraints - Ensure the function supports more than one property and each property having multiple classes. - You may use any classifier of choice as the base estimator, but ensure it is compatible with `MultiOutputClassifier`. # Performance Requirements - Your implementation should handle a dataset with at least 10,000 samples and 100 features efficiently. - Aim for a training time of fewer than 30 seconds on a standard machine with 8GB RAM. # Additional Notes - You are encouraged to explore different classifiers, but the default should be the `RandomForestClassifier`. - Make sure to handle any necessary preprocessing steps within the function. # Example Usage ```python import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification # Create a sample dataset X, y1 = make_classification(n_samples=100, n_features=20, n_informative=5, n_classes=3, random_state=1) y2 = np.random.permutation(y1) y3 = np.random.permutation(y1) Y = np.vstack([y1, y2, y3]).T # Shape: (n_samples, 3) # Define the function def multi_output_classifier(X, Y, base_estimator=RandomForestClassifier, random_state=42): from sklearn.multioutput import MultiOutputClassifier # Initialize the base estimator base_model = base_estimator(random_state=random_state) # Initialize the multioutput classifier with the base estimator multi_output_model = MultiOutputClassifier(base_model) # Fit the model on the given data multi_output_model.fit(X, Y) return multi_output_model # Train the model model = multi_output_classifier(X, Y) # Predict with the trained model predictions = model.predict(X) print(predictions.shape) # Should print (100, 3) ``` In this question, participants are required to demonstrate their understanding of multiclass-multioutput classification and the practical application of scikit-learn\'s `MultiOutputClassifier`. They need to effectively implement a function that sets up and trains a multioutput classifier, considering the specified constraints and performance requirements.","solution":"from sklearn.ensemble import RandomForestClassifier from sklearn.multioutput import MultiOutputClassifier def multi_output_classifier(X, Y, base_estimator=RandomForestClassifier, random_state=42): Trains a MultiOutputClassifier with the given base estimator on the provided dataset. Parameters: - X: array-like of shape (n_samples, n_features) The input samples. - Y: array-like of shape (n_samples, n_outputs) The multiclass-multioutput labels for each sample. - base_estimator: estimator object, default=RandomForestClassifier The base estimator to fit on the multioutput classification problem. - random_state: int, default=42 Controls the random seed given to the base estimator. Returns: - multi_output_model: fitted MultiOutputClassifier # Initialize the base estimator with random_state base_model = base_estimator(random_state=random_state) # Initialize the MultiOutputClassifier with the base estimator multi_output_model = MultiOutputClassifier(base_model) # Fit the multioutput model on the given dataset multi_output_model.fit(X, Y) return multi_output_model"},{"question":"Managing CUDA Backend Configurations **Objective**: This problem assesses your understanding of PyTorch\'s CUDA backend functionalities, particularly focused on mixed precision settings and performing matrix multiplications on GPUs. Problem Description: You are tasked with implementing a function that manages CUDA backend settings and performs matrix multiplication efficiently using available options. Specifically, your function must: 1. Check and return whether CUDA is available. 2. Enable or disable TensorFloat-32 (TF32) for matrix multiplications and reduced precision reductions using FP16 and BF16 as per parameters. 3. Configure cuFFT plan cache size for a given CUDA device. 4. Perform matrix multiplication between two large matrices using the currently set configurations and return the result. Function Signature: ```python import torch def configure_and_multiply( enable_tf32: bool, enable_fp16_reduced_precision: bool, enable_bf16_reduced_precision: bool, cufft_cache_size: int, device_id: int, matrix_a: torch.Tensor, matrix_b: torch.Tensor ) -> torch.Tensor: Configures CUDA backend settings and performs matrix multiplication. Parameters: - enable_tf32 (bool): Enable or disable TF32 for matrix multiplications. - enable_fp16_reduced_precision (bool): Enable or disable reduced precision accumulations with FP16. - enable_bf16_reduced_precision (bool): Enable or disable reduced precision accumulations with BF16. - cufft_cache_size (int): Set the cuFFT plan cache size for the given device. - device_id (int): The ID of the CUDA device to be used. - matrix_a (torch.Tensor): The first matrix for multiplication. - matrix_b (torch.Tensor): The second matrix for multiplication. Returns: - torch.Tensor: The result of matrix multiplication between matrix_a and matrix_b. ``` Input Constraints: - `enable_tf32`, `enable_fp16_reduced_precision`, `enable_bf16_reduced_precision` are boolean values (`True` or `False`). - `cufft_cache_size` is a non-negative integer. - `device_id` is a non-negative integer representing a valid CUDA device. - `matrix_a` and `matrix_b` are 2-dimensional tensors of appropriate sizes for matrix multiplication. Output: - If CUDA is not available, return `None`. - If CUDA is available, return the result of the matrix multiplication as a tensor. Example: ```python matrix_a = torch.randn((1024, 512), device=\'cuda\') matrix_b = torch.randn((512, 1024), device=\'cuda\') result = configure_and_multiply( enable_tf32=True, enable_fp16_reduced_precision=True, enable_bf16_reduced_precision=False, cufft_cache_size=100, device_id=0, matrix_a=matrix_a, matrix_b=matrix_b ) if result is not None: print(result.shape) # Expected Output: torch.Size([1024, 1024]) else: print(\\"CUDA not available.\\") ``` Notes: - Ensure `torch.backends.cuda.matmul.allow_tf32`, `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction`, and `torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction` are set according to input parameters. - Set the cuFFT plan cache size for the specified CUDA device using `torch.backends.cuda.cufft_plan_cache[device_id].max_size`. - Perform matrix multiplication using `torch.matmul(matrix_a, matrix_b)`. Hints: - Use `torch.backends.cuda.is_available()` to check CUDA availability. - Use `.to(\'cuda\')` to ensure tensors are on the CUDA device.","solution":"import torch def configure_and_multiply( enable_tf32: bool, enable_fp16_reduced_precision: bool, enable_bf16_reduced_precision: bool, cufft_cache_size: int, device_id: int, matrix_a: torch.Tensor, matrix_b: torch.Tensor ) -> torch.Tensor: Configures CUDA backend settings and performs matrix multiplication. if not torch.cuda.is_available(): return None # Configure TF32 setting torch.backends.cuda.matmul.allow_tf32 = enable_tf32 # Configure reduced precision accumulations with FP16 torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = enable_fp16_reduced_precision # Configure reduced precision accumulations with BF16 torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = enable_bf16_reduced_precision # Set cuFFT plan cache size torch.backends.cuda.cufft_plan_cache[device_id].max_size = cufft_cache_size # Ensure matrices are on the correct CUDA device device = torch.device(f\'cuda:{device_id}\') matrix_a = matrix_a.to(device) matrix_b = matrix_b.to(device) # Perform matrix multiplication result = torch.matmul(matrix_a, matrix_b) return result"},{"question":"Objective Demonstrate your understanding of the `zlib` module by implementing functions for data compression and decompression, checksum calculations, and handling streams of data. Question Write a Python class `ZlibUtility` that encapsulates common operations provided by the `zlib` module. Implement the following methods: 1. **`compress_data(data: bytes, level: int = -1) -> bytes`**: - Compress the given data using the specified compression level (default is `-1`). - Return the compressed data. 2. **`decompress_data(data: bytes, wbits: int = zlib.MAX_WBITS, bufsize: int = zlib.DEF_BUF_SIZE) -> bytes`**: - Decompress the given data using the specified window size (`wbits`) and buffer size (`bufsize`). - Return the decompressed data. 3. **`calculate_crc32(data: bytes, value: int = 0) -> int`**: - Compute the CRC32 checksum of the given data starting with the optional value (default is `0`). - Return the checksum as an unsigned 32-bit integer. 4. **`calculate_adler32(data: bytes, value: int = 1) -> int`**: - Compute the Adler-32 checksum of the given data starting with the optional value (default is `1`). - Return the checksum as an unsigned 32-bit integer. 5. **`compress_stream(data: bytes, level: int = -1) -> Generator[bytes, None, None]`**: - Create a generator that compresses the data in chunks using a compression object. - Yield chunks of compressed data. 6. **`decompress_stream(data: bytes, wbits: int = zlib.MAX_WBITS) -> Generator[bytes, None, None]`**: - Create a generator that decompresses the data in chunks using a decompression object. - Yield chunks of decompressed data. Input and Output Formats - Assume `data` parameters are bytes objects. - Assume `level`, `wbits`, `bufsize`, and `value` parameters are integers. - The methods should handle exceptions raised by the `zlib` module and provide informative error messages. Example Usage ```python data = b\\"Example data to compress and decompress.\\" # Test ZlibUtility methods util = ZlibUtility() compressed_data = util.compress_data(data) print(compressed_data) # Example compressed output decompressed_data = util.decompress_data(compressed_data) print(decompressed_data) # b\'Example data to compress and decompress.\' crc32_checksum = util.calculate_crc32(data) print(crc32_checksum) # Example CRC32 checksum adler32_checksum = util.calculate_adler32(data) print(adler32_checksum) # Example Adler-32 checksum # Test stream compression and decompression compressed_stream = util.compress_stream(data) for chunk in compressed_stream: print(chunk) decompressed_stream = util.decompress_stream(compressed_data) for chunk in decompressed_stream: print(chunk) ``` Your implementation should be efficient and make sure edge cases are handled gracefully.","solution":"import zlib from typing import Generator class ZlibUtility: @staticmethod def compress_data(data: bytes, level: int = -1) -> bytes: try: return zlib.compress(data, level) except zlib.error as e: raise ValueError(f\\"Compression error: {e}\\") @staticmethod def decompress_data(data: bytes, wbits: int = zlib.MAX_WBITS, bufsize: int = zlib.DEF_BUF_SIZE) -> bytes: try: return zlib.decompress(data, wbits, bufsize) except zlib.error as e: raise ValueError(f\\"Decompression error: {e}\\") @staticmethod def calculate_crc32(data: bytes, value: int = 0) -> int: try: return zlib.crc32(data, value) except zlib.error as e: raise ValueError(f\\"CRC32 calculation error: {e}\\") @staticmethod def calculate_adler32(data: bytes, value: int = 1) -> int: try: return zlib.adler32(data, value) except zlib.error as e: raise ValueError(f\\"Adler-32 calculation error: {e}\\") @staticmethod def compress_stream(data: bytes, level: int = -1) -> Generator[bytes, None, None]: compressor = zlib.compressobj(level) chunk_size = 1024 for i in range(0, len(data), chunk_size): yield compressor.compress(data[i:i + chunk_size]) yield compressor.flush() @staticmethod def decompress_stream(data: bytes, wbits: int = zlib.MAX_WBITS) -> Generator[bytes, None, None]: decompressor = zlib.decompressobj(wbits) chunk_size = 1024 for i in range(0, len(data), chunk_size): yield decompressor.decompress(data[i:i + chunk_size]) yield decompressor.flush()"},{"question":"Objective Implement functionality to bind functions dynamically to a Python class and its instances, and verify their types. This will test your understanding of instance method and method objects in Python. Problem Statement You are required to implement a Python class `DynamicBinder` with the following methods: 1. `bind_instance_method(cls, func_name, func)`: A class method that binds a given function to an instance of `cls` as an instance method. 2. `bind_method(cls, func_name, func)`: A class method that binds a given function to the class itself as a method. 3. `is_instance_method(obj, func_name)`: An instance method that checks if a given method on the object is an instance method. 4. `is_method(obj, func_name)`: An instance method that checks if a given method on the object is a method (bound to an instance of `cls`). Input and Output Format - `bind_instance_method(cls, func_name, func)`: Takes a class `cls`, a string `func_name` and a function `func`, and binds the function to an instance of the class. - `bind_method(cls, func_name, func)`: Takes a class `cls`, a string `func_name` and a function `func`, and binds the function to the class itself. - `is_instance_method(obj, func_name)`: Takes an instance `obj` and a string `func_name`, and returns `True` if the function is an instance method, otherwise `False`. - `is_method(obj, func_name)`: Takes an instance `obj` and a string `func_name`, and returns `True` if the function is a method, otherwise `False`. Constraints - The `func` parameter will always be a callable. - The `func_name` will always be a valid string and should become a valid attribute name for the class or instance. - The `cls` parameter will always be a valid class with appropriate permissions to accept new methods. - The object `obj` parameter will always be an instance of the class `cls`. Example ```python class SampleClass: pass def sample_function(self): return \\"Hello, World!\\" binder = DynamicBinder() binder.bind_instance_method(SampleClass, \'instance_func\', sample_function) binder.bind_method(SampleClass, \'class_func\', sample_function) instance = SampleClass() print(binder.is_instance_method(instance, \'instance_func\')) # Output: True print(binder.is_method(instance, \'class_func\')) # Output: True print(instance.instance_func()) # Output: \\"Hello, World!\\" print(instance.class_func()) # Output: \\"Hello, World!\\" ``` Implementation ```python class DynamicBinder: @classmethod def bind_instance_method(cls, target_cls, func_name, func): # Implementation code here @classmethod def bind_method(cls, target_cls, func_name, func): # Implementation code here @classmethod def is_instance_method(cls, obj, func_name): # Implementation code here @classmethod def is_method(cls, obj, func_name): # Implementation code here ``` Implement the `DynamicBinder` class with the described methods to pass the example assertions. You can make use of the provided API methods in the documentation to achieve the desired functionality.","solution":"class DynamicBinder: @classmethod def bind_instance_method(cls, target_cls, func_name, func): def method_binder(self, *args, **kwargs): return func(self, *args, **kwargs) setattr(target_cls, func_name, method_binder) @classmethod def bind_method(cls, target_cls, func_name, func): setattr(target_cls, func_name, func) @classmethod def is_instance_method(cls, obj, func_name): func = getattr(obj, func_name, None) return callable(func) and hasattr(func, \'__self__\') and func.__self__ is not None @classmethod def is_method(cls, obj, func_name): func = getattr(obj, func_name, None) return callable(func)"},{"question":"# Question: Calendar Event Planner You are tasked with implementing a function that schedules events for a range of dates within a year and returns a list that indicates the available dates for each month. The constraints are: - Events can only be scheduled on weekdays (Monday to Friday). - You have to exclude public holidays provided as a list of `datetime.date` objects. Function Signature ```python def schedule_events(year: int, holidays: list[int], first_weekday: int = calendar.MONDAY) -> list[list[str]]: Generate a schedule of available dates for each month, excluding weekends and provided holidays. Parameters: - year (int): The year for which to generate the calendar - holidays (list[datetime.date]): A list of datetime.date objects representing public holidays - first_weekday (int): The first day of the week, default is Monday Returns: - list[list[str]]: A nested list where each inner list contains available dates as strings in \'YYYY-MM-DD\' format for respective months. ``` Input 1. `year` - An integer indicating the year for which the schedule is generated. 2. `holidays` - A list of `datetime.date` objects marking public holidays. 3. `first_weekday` - An optional integer representing the first day of the week, defaulting to Monday. Output A nested list where each inner list contains available dates (as strings in \'YYYY-MM-DD\' format) for respective months. The outer list should have 12 inner lists (one for each month from January to December). Example ```python from datetime import date year = 2023 holidays = [date(2023, 1, 1), date(2023, 12, 25)] results = schedule_events(year, holidays) print(results) ``` This should output: ```python [[\'2023-01-02\', ...], [\'2023-02-01\', ...], ..., [\'2023-12-29\']] ``` Constraints 1. Use the `calendar.Calendar` class for generating the initial calendars. 2. Ensure no date in the resulting lists falls on a weekend or is listed as a holiday. 3. Optimize the function for performance since it processes data for an entire year. Hints 1. Use `itermonthdates` to iterate over dates in a month. 2. Use `calendar.day_abbr` to manage weekday names if needed. 3. Check for the day of the week to exclude weekends using the `weekday()` method of `datetime.date`. # Good Luck!","solution":"import calendar from datetime import date, timedelta def schedule_events(year, holidays, first_weekday=calendar.MONDAY): Generate a schedule of available dates for each month, excluding weekends and provided holidays. Parameters: - year (int): The year for which to generate the calendar - holidays (list[datetime.date]): A list of datetime.date objects representing public holidays - first_weekday (int): The first day of the week, default is Monday Returns: - list[list[str]]: A nested list where each inner list contains available dates as strings in \'YYYY-MM-DD\' format for respective months. cal = calendar.Calendar(first_weekday) schedule = [] for month in range(1, 13): month_schedule = [] for day in cal.itermonthdates(year, month): if day.year == year and day.month == month: if day.weekday() < 5 and day not in holidays: month_schedule.append(day.strftime(\\"%Y-%m-%d\\")) schedule.append(month_schedule) return schedule"},{"question":"# Question: Implementing a Python Program that Uses `atexit` Module Create a Python program where you will: 1. Maintain a counter that initializes from a file named `counterfile`. If the file does not exist, initialize the counter to 0. 2. Implement a function `incrcounter(n)` that increments the counter by `n`. 3. Implement a function `savecounter()` that writes the updated counter value back to `counterfile`. 4. Register the `savecounter` function to be automatically called upon normal program termination using the `atexit` module. 5. Implement a decorator-based function `goodbye` that prints a goodbye message when the program is exiting. **Constraints:** - The counter value should persist across program runs using the `counterfile`. - Ensure that all registered cleanup functions handle exceptions gracefully and provide meaningful outputs. - The function `goodbye` should print the message \\"You are now leaving the program.\\" when the program terminates. Your program should demonstrate the understanding of the `atexit` module functionalities, decorators, file operations, and exception handling. # Input: - `incrcounter(n)` will be explicitly called in the program with integer values. # Output: - The content of `counterfile` will represent the number of increments made persistently. - A goodbye message should be printed when the program terminates. # Example: ```python incrcounter(5) incrcounter(10) # When the program exits, the counter should save the value 15 to \'counterfile\' # And print: \'You are now leaving the program.\' # Upon restarting the program and the counterfile containing 15 incrcounter(5) # The program should save the value 20 to \'counterfile\' upon exiting # And print: \'You are now leaving the program.\' ``` You may assume that the file operations and counter increments are correct during manual testing.","solution":"import atexit import os COUNTERFILE = \'counterfile\' counter = 0 def load_counter(): global counter if os.path.exists(COUNTERFILE): with open(COUNTERFILE, \'r\') as f: counter = int(f.read().strip()) else: counter = 0 def incrcounter(n): global counter counter += n def savecounter(): try: with open(COUNTERFILE, \'w\') as f: f.write(str(counter)) except IOError as e: print(f\\"Error saving counter: {e}\\") def bye_decorator(func): def wrapper(*args, **kwargs): result = func(*args, **kwargs) print(\\"You are now leaving the program.\\") return result return wrapper @bye_decorator def goodbye(): pass load_counter() atexit.register(savecounter) atexit.register(goodbye)"},{"question":"# Data Manipulation with Pandas: Indexing and Selection Challenge Objective Write a function `filter_dataframe_by_criteria(df: pd.DataFrame, criteria: dict) -> pd.DataFrame` in Python that filters rows in a pandas DataFrame based on multiple criteria specified in a dictionary. The function must demonstrate advanced indexing skills by using both label-based and position-based techniques. Function Signature ```python def filter_dataframe_by_criteria(df: pd.DataFrame, criteria: dict) -> pd.DataFrame: ``` Input 1. `df` : pandas DataFrame - The input DataFrame that needs to be filtered. 2. `criteria` : dictionary - A dictionary where keys are column names and values are lists of allowed values for that column. Example: `{\'A\': [1, 2], \'B\': [\'x\', \'y\']}`. Output - A filtered pandas DataFrame that only includes rows where the values in the specified columns match any of the specified allowed values in the criteria. Requirements 1. Use `.loc` for label-based indexing to subset the DataFrame. 2. Use `.iloc` for position-based indexing in at least one step within the function. 3. The function should also handle cases where a specified column does not exist in the DataFrame by ignoring that key-value pair in the criteria. Example ```python import pandas as pd # Sample DataFrame data = {\'A\': [1, 2, 3, 4], \'B\': [\'x\', \'y\', \'z\', \'x\'], \'C\': [10, 20, 30, 40]} df = pd.DataFrame(data) # Criteria to filter the DataFrame criteria = {\'A\': [1, 2], \'B\': [\'x\']} # Calling the function filtered_df = filter_dataframe_by_criteria(df, criteria) print(filtered_df) ``` Expected output: ``` A B C 0 1 x 10 ``` Constraints - The criteria dictionary can include any number of column names and corresponding allowed values. - Rows must be filtered based on all given criteria, i.e., only include rows that meet all the specified conditions. - The function should handle DataFrames with up to 100,000 rows efficiently. Performance Requirements - The solution must be efficient and avoid unnecessary computations. - Utilize pandas\' optimized indexing methods to ensure performance on larger datasets. Notes - Consider edge cases such as empty DataFrames, criteria with non-existent columns, and DataFrames with missing values. - Do not use any external libraries apart from pandas and numpy (only if necessary).","solution":"import pandas as pd def filter_dataframe_by_criteria(df: pd.DataFrame, criteria: dict) -> pd.DataFrame: # Using loc for label-based indexing to filter DataFrame based on criteria for column, allowed_values in criteria.items(): if column in df.columns: df = df.loc[df[column].isin(allowed_values)] return df # Sample DataFrame creation function for testing purposes def create_sample_dataframe(): data = {\'A\': [1, 2, 3, 4], \'B\': [\'x\', \'y\', \'z\', \'x\'], \'C\': [10, 20, 30, 40]} return pd.DataFrame(data)"},{"question":"# PyTorch Coding Assessment: Implementing and Using Custom Probability Distributions In this exercise, you are required to use PyTorch\'s `torch.distributions` module to create and manipulate probability distributions. Specifically, you will implement a function that must demonstrate your understanding of: 1. Creating distribution objects. 2. Sampling from these distributions. 3. Calculating log probabilities. Objective Implement a function `custom_distribution_operations` that: 1. Creates a `Normal` distribution with a specified mean and standard deviation. 2. Creates a `Beta` distribution with specified alpha and beta parameters. 3. Draws a specified number of samples from both distributions. 4. Computes and returns the log-probabilities of these samples under their respective distributions. Function Signature ```python import torch from torch.distributions import Normal, Beta def custom_distribution_operations(mean: float, stddev: float, alpha: float, beta: float, num_samples: int): Parameters: - mean (float): The mean of the Normal distribution. - stddev (float): The standard deviation of the Normal distribution. - alpha (float): The alpha parameter (shape) of the Beta distribution. - beta (float): The beta parameter (shape) of the Beta distribution. - num_samples (int): The number of samples to draw from each distribution. Returns: - normal_samples (torch.Tensor): Samples drawn from the Normal distribution. - beta_samples (torch.Tensor): Samples drawn from the Beta distribution. - normal_log_probs (torch.Tensor): Log-probabilities of the normal_samples under the Normal distribution. - beta_log_probs (torch.Tensor): Log-probabilities of the beta_samples under the Beta distribution. pass ``` Example ```python mean = 0.0 stddev = 1.0 alpha = 2.0 beta = 5.0 num_samples = 3 normal_samples, beta_samples, normal_log_probs, beta_log_probs = custom_distribution_operations(mean, stddev, alpha, beta, num_samples) print(\\"Normal samples:\\", normal_samples) print(\\"Beta samples:\\", beta_samples) print(\\"Log-probs of Normal samples:\\", normal_log_probs) print(\\"Log-probs of Beta samples:\\", beta_log_probs) ``` Constraints - All parameters will be valid floats or an integer greater than 0. - Each distribution object must be created using PyTorch\'s `torch.distributions` module. - Efficient implementation is encouraged but there is no strict performance requirement. Good luck and happy coding!","solution":"import torch from torch.distributions import Normal, Beta def custom_distribution_operations(mean: float, stddev: float, alpha: float, beta: float, num_samples: int): Parameters: - mean (float): The mean of the Normal distribution. - stddev (float): The standard deviation of the Normal distribution. - alpha (float): The alpha parameter (shape) of the Beta distribution. - beta (float): The beta parameter (shape) of the Beta distribution. - num_samples (int): The number of samples to draw from each distribution. Returns: - normal_samples (torch.Tensor): Samples drawn from the Normal distribution. - beta_samples (torch.Tensor): Samples drawn from the Beta distribution. - normal_log_probs (torch.Tensor): Log-probabilities of the normal_samples under the Normal distribution. - beta_log_probs (torch.Tensor): Log-probabilities of the beta_samples under the Beta distribution. # Create the Normal distribution normal_dist = Normal(mean, stddev) # Create the Beta distribution beta_dist = Beta(alpha, beta) # Draw samples from the distributions normal_samples = normal_dist.sample((num_samples,)) beta_samples = beta_dist.sample((num_samples,)) # Calculate the log-probabilities of the samples normal_log_probs = normal_dist.log_prob(normal_samples) beta_log_probs = beta_dist.log_prob(beta_samples) return normal_samples, beta_samples, normal_log_probs, beta_log_probs"},{"question":"**Advanced Seaborn Plot Manipulation** You are provided with the `tips` dataset, which contains information about restaurant tips, including the total bill, tip amount, gender of the person paying the bill, whether they are a smoker, day of the week, time, and size of the party. Your task is to create a bar plot that effectively uses the dodge transformation to visualize the data. Here\'s what you need to do: 1. **Create a base plot** for the `tips` dataset using seaborn objects interface, where the x-axis represents the `day`, and colors represent the `time`. 2. **Add a bar plot** layer to display counts of tips per day. 3. **Apply the dodge transformation** to ensure that the bars for different times (`Lunch` and `Dinner`) do not overlap. Use a small gap to clearly distinguish the bars. 4. **Fill empty spaces** where one of the times may be missing for a particular day. 5. **Combine another relevant plot** (e.g., a dot plot) with a different transformation to enhance the visualization. # Input - You do not need to take any input from the user. # Output - The function should display the customized plot described. # Requirements - Use the seaborn `objects` interface as demonstrated in the documentation. - The plot should clearly distinguish between different `time` values using color and dodge transformation. - Ensure that the plot does not leave unnecessary empty spaces. - Combine at least one other transformation intelligently to add further insights into the visualization. # Constraints - You must use the seaborn library and its `objects` interface. - The solution should be efficient and well-organized. # Example Hereâ€™s an example of what the function might look like in Python. ```python import seaborn.objects as so from seaborn import load_dataset def create_advanced_seaborn_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create Plot object with base characteristics p = so.Plot(tips, x=\\"day\\", color=\\"time\\") # Add a bar with dodge transformation p.add(so.Bar(), so.Count(), so.Dodge(gap=0.1, empty=\\"fill\\")) # Add a dot plot to enhance visualization p.add(so.Dot(), so.Dodge(by=[\\"color\\"]), so.Jitter()) # Display plot p.show() # Call the function to display the plot create_advanced_seaborn_plot() ``` The above function should effectively demonstrate the use of seaborn\'s advanced plotting capabilities using the `objects` interface, `Dodge` transformation, and combination of different plot types and transformations.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_advanced_seaborn_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create Plot object with base characteristics p = so.Plot(tips, x=\\"day\\", color=\\"time\\") # Add a bar with dodge transformation p.add(so.Bar(), so.Count(), so.Dodge(gap=0.1, empty=\\"fill\\")) # Add a dot plot to enhance visualization p.add(so.Dot(), so.Dodge(by=[\\"color\\"]), so.Jitter()) # Display plot p.show() # Call the function to display the plot create_advanced_seaborn_plot()"},{"question":"# Problem: Exploring Relationships in a Dataset with Seaborn You are provided with a dataset containing information about tips given in a restaurant, including the total bill, the tip amount, the day of the week, and other related details. Your task is to analyze the dataset and visualize various relationships between the variables using Seaborn\'s FacetGrid and PairGrid. Input: * A DataFrame `tips` containing the following columns: - `total_bill`: float, total bill amount - `tip`: float, tip amount - `sex`: string, gender of the person paying the bill - `smoker`: string, whether the person is a smoker - `day`: string, day of the week - `time`: string, time of day (Lunch or Dinner) - `size`: int, size of the party You can use the `sns.load_dataset(\\"tips\\")` to load the dataset. Task: 1. Create a FacetGrid plot that visualizes the distribution of `total_bill` separately for each `day` of the week and each `sex`. Use histograms to show the distribution of `total_bill`. 2. Create a PairGrid plot that shows pairwise relationships between `total_bill`, `tip`, and `size` with different subplots for each pair, and color the points by the `smoker` status. 3. Customize the PairGrid plot to use scatterplots in the lower triangle, KDE plots in the upper triangle, and histograms on the diagonal. Ensure that the plots are clearly distinguishable by using appropriate colors and legends. # Constraints: - The FacetGrid should display at least one histogram plot for each combination of `day` and `sex`. - The PairGrid should clearly show all pairwise relationships using the specified plot types. # Expected Output: Output should not return any values. Instead, your code should produce the visualizations directly as part of its execution. # Performance Requirements: - The visualizations should be rendered efficiently without taking an excessively long time to display. Example: Your solution might generate similar plots to the following, but the exact aesthetics and customization will reflect your own design choices and the dataset being visualized. ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Task 1: FacetGrid with histograms g = sns.FacetGrid(tips, row=\\"day\\", col=\\"sex\\", margin_titles=True, height=2.5) g.map(sns.histplot, \\"total_bill\\") # Task 2: PairGrid with pairwise relationships g = sns.PairGrid(tips, vars=[\\"total_bill\\", \\"tip\\", \\"size\\"], hue=\\"smoker\\") g.map_lower(sns.scatterplot) g.map_upper(sns.kdeplot, fill=True) g.map_diag(sns.histplot, kde_kws={\\"color\\": \\"k\\"}) g.add_legend() plt.show() ``` Your final implementation should be submitted as a Python function or script that generates these visualizations upon execution.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_tips_data(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Task 1: FacetGrid with histograms g = sns.FacetGrid(tips, row=\\"day\\", col=\\"sex\\", margin_titles=True, height=2.5) g.map(sns.histplot, \\"total_bill\\") # Task 2: PairGrid with pairwise relationships g = sns.PairGrid(tips, vars=[\\"total_bill\\", \\"tip\\", \\"size\\"], hue=\\"smoker\\") g.map_lower(sns.scatterplot) g.map_upper(sns.kdeplot, fill=True) g.map_diag(sns.histplot, kde_kws={\\"color\\": \\"k\\"}) g.add_legend() plt.show()"},{"question":"# Data Analysis with Pandas You are provided with a dataset of a fictional e-commerce platform\'s order details. Each order record contains the following information: `OrderID`, `CustomerID`, `OrderDate`, `ProductID`, `Quantity`, `Price`, and `Country`. Your task is to implement a Python function using pandas that performs the following steps: 1. **Data Loading**: Load the dataset from a CSV file named `orders.csv`. 2. **Data Cleaning**: - Ensure all `OrderDate` values are converted to datetime objects. - Fill any missing `Quantity` values with `0`. - For any missing `Price` value, fill with the median price of that `ProductID` across all orders. 3. **Feature Engineering**: - Create a new column `TotalPrice` which is the product of `Quantity` and `Price`. - Create a new column `OrderMonth` that contains the month and year of the `OrderDate`. 4. **Data Aggregation**: - Compute the total `Quantity` and `TotalPrice` for each `CustomerID` for every month. The result should be a DataFrame with columns: `CustomerID`, `OrderMonth`, `TotalQuantity`, and `TotalSpent`. 5. **Data Analysis**: - Find the top 5 customers by `TotalSpent`. - Find the top 5 products by total `Quantity` sold. 6. **Save Results**: - Save the summary DataFrame (from step 4) to a CSV file named `customer_monthly_summary.csv`. - Save the top 5 customers to a CSV file named `top_customers.csv`. - Save the top 5 products to a CSV file named `top_products.csv`. # Constraints - You must use pandas for all data manipulation tasks. - Assume the input file `orders.csv` is available in the current working directory. - Performance should be optimized for large datasets. # Input A CSV file `orders.csv` with columns: `OrderID`, `CustomerID`, `OrderDate`, `ProductID`, `Quantity`, `Price`, `Country`. # Output CSV files named `customer_monthly_summary.csv`, `top_customers.csv`, and `top_products.csv` containing the required results. # Function Signature ```python def analyze_orders(): pass # Example usage: # analyze_orders() ``` This question requires a combination of data loading, cleaning, feature engineering, aggregation, and analysis tasks, testing the student\'s ability to handle multiple aspects of data manipulation using pandas.","solution":"import pandas as pd def analyze_orders(): # Data Loading df = pd.read_csv(\'orders.csv\') # Data Cleaning df[\'OrderDate\'] = pd.to_datetime(df[\'OrderDate\']) df[\'Quantity\'].fillna(0, inplace=True) df[\'Price\'] = df.groupby(\'ProductID\')[\'Price\'].transform(lambda x: x.fillna(x.median())) # Feature Engineering df[\'TotalPrice\'] = df[\'Quantity\'] * df[\'Price\'] df[\'OrderMonth\'] = df[\'OrderDate\'].dt.to_period(\'M\') # Data Aggregation summary_df = df.groupby([\'CustomerID\', \'OrderMonth\']).agg( TotalQuantity=(\'Quantity\', \'sum\'), TotalSpent=(\'TotalPrice\', \'sum\') ).reset_index() # Data Analysis top_customers = summary_df.groupby(\'CustomerID\').agg( TotalSpent=(\'TotalSpent\', \'sum\') ).sort_values(by=\'TotalSpent\', ascending=False).head(5).reset_index() top_products = df.groupby(\'ProductID\').agg( TotalQuantity=(\'Quantity\', \'sum\') ).sort_values(by=\'TotalQuantity\', ascending=False).head(5).reset_index() # Save Results summary_df.to_csv(\'customer_monthly_summary.csv\', index=False) top_customers.to_csv(\'top_customers.csv\', index=False) top_products.to_csv(\'top_products.csv\', index=False)"},{"question":"**Coding Assessment Question: Set Operations Management** # Objective Implement a class to efficiently manage a collection of sets with various operations. Your implementation should showcase an understanding of set and frozenset operations in Python. # Class Definition Implement a class `SetManager` with the following methods: 1. **`add_set(self, elements: list) -> str`**: - **Input**: A list of hashable elements. - **Output**: A string \\"Added\\" if the set is successfully added. - **Description**: Create a new set from the input list and add it to the internal collection of sets. Raise a `TypeError` if the list contains unhashable elements. 2. **`remove_set(self, elements: list) -> str`**: - **Input**: A list of hashable elements. - **Output**: A string \\"Removed\\" if the set is found and successfully removed, \\"Set not found\\" otherwise. - **Description**: Remove the set matching the input list from the internal collection of sets. 3. **`contains_element(self, element) -> bool`**: - **Input**: A hashable element. - **Output**: `True` if any of the sets in the internal collection contain the element, `False` otherwise. 4. **`pop_any(self) -> any`**: - **Output**: An arbitrary element from any of the sets in the internal collection. Raise a `KeyError` if all sets are empty. - **Description**: Remove and return an arbitrary element from any of the managed sets. 5. **`clear_all(self) -> str`**: - **Output**: A string \\"Cleared\\" once all sets are emptied. # Example Usage ```python manager = SetManager() print(manager.add_set([1, 2, 3])) # Output: \\"Added\\" print(manager.add_set([4, 5, 6])) # Output: \\"Added\\" print(manager.contains_element(2)) # Output: True print(manager.remove_set([1, 2, 3])) # Output: \\"Removed\\" print(manager.contains_element(2)) # Output: False print(manager.pop_any()) # Output: 4 (or any other element from the sets) print(manager.clear_all()) # Output: \\"Cleared\\" ``` # Constraints - The collection can hold up to `10^5` sets. - Each set can contain up to `10^3` elements. - All elements in the sets will be hashable. # Performance Requirements - Efficiently handle worst-case scenarios where operations must be performed in `O(1)` average time complexity due to internal optimizations. # Notes - Only use standard Python libraries. - Consider edge cases, such as handling unhashable elements and dealing with empty collections.","solution":"class SetManager: def __init__(self): self.sets = [] def add_set(self, elements: list) -> str: try: new_set = frozenset(elements) self.sets.append(new_set) return \\"Added\\" except TypeError: raise TypeError(\\"Unhashable element found in the list\\") def remove_set(self, elements: list) -> str: target_set = frozenset(elements) if target_set in self.sets: self.sets.remove(target_set) return \\"Removed\\" else: return \\"Set not found\\" def contains_element(self, element) -> bool: return any(element in s for s in self.sets) def pop_any(self): for s in self.sets: if s: element = next(iter(s)) self.sets.remove(s) new_set = s - {element} if new_set: self.sets.append(new_set) return element raise KeyError(\\"All sets are empty\\") def clear_all(self) -> str: self.sets.clear() return \\"Cleared\\""},{"question":"You are given a dataset containing daily temperatures recorded in a particular city over the span of one month. Your task is to create three different line plots using the seaborn library, each with distinct visual settings using the `sns.set_context` function. This will demonstrate your understanding of seaborn\'s context settings and plot customization capabilities. Dataset The dataset is a CSV file named `daily_temperatures.csv` with the following columns: - `day`: The day of the month (1 to 30) - `temperature`: The recorded temperature for that day Tasks 1. **Read the Dataset**: - Load the dataset using pandas into a DataFrame. 2. **Create the Plots**: - Create three subplots in a single figure: - Plot 1: Use the `paper` context with `font_scale=1.0`. - Plot 2: Use the `notebook` context with `font_scale=1.5` and set `lines.linewidth` to 2. - Plot 3: Use the `talk` context with `font_scale=1.2` and an increased `lines.linewidth` to 3. 3. **Display the Plots**: - Ensure each plot has appropriate titles and labels for visualization. Input - The CSV file named `daily_temperatures.csv`. Output - A single figure displaying the three subplots, each with the settings specified above. Constraints - Do not modify the content of the dataset. - Use seaborn and matplotlib libraries for plotting. Example Plot Structure Your figure should have the following structure with titles and labels appropriately set: - Subplot 1: title \\"Paper Context\\" - Subplot 2: title \\"Notebook Context\\" - Subplot 3: title \\"Talk Context\\" ```python # Example code layout (exact code to be implemented by the student): import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Step 1: Load the dataset df = pd.read_csv(\'daily_temperatures.csv\') # Step 2: Set up the figure with 3 subplots fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 15)) # Step 3: Plot 1 with \'paper\' context sns.set_context(\\"paper\\", font_scale=1.0) sns.lineplot(data=df, x=\'day\', y=\'temperature\', ax=axes[0]) axes[0].set_title(\\"Paper Context\\") # Step 4: Plot 2 with \'notebook\' context sns.set_context(\\"notebook\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2}) sns.lineplot(data=df, x=\'day\', y=\'temperature\', ax=axes[1]) axes[1].set_title(\\"Notebook Context\\") # Step 5: Plot 3 with \'talk\' context sns.set_context(\\"talk\\", font_scale=1.2, rc={\\"lines.linewidth\\": 3}) sns.lineplot(data=df, x=\'day\', y=\'temperature\', ax=axes[2]) axes[2].set_title(\\"Talk Context\\") # Display the plots plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_temperature_contexts(csv_file): # Step 1: Load the dataset df = pd.read_csv(csv_file) # Step 2: Set up the figure with 3 subplots fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 15)) # Step 3: Plot 1 with \'paper\' context sns.set_context(\\"paper\\", font_scale=1.0) sns.lineplot(data=df, x=\'day\', y=\'temperature\', ax=axes[0]) axes[0].set_title(\\"Paper Context\\") # Step 4: Plot 2 with \'notebook\' context sns.set_context(\\"notebook\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2}) sns.lineplot(data=df, x=\'day\', y=\'temperature\', ax=axes[1]) axes[1].set_title(\\"Notebook Context\\") # Step 5: Plot 3 with \'talk\' context sns.set_context(\\"talk\\", font_scale=1.2, rc={\\"lines.linewidth\\": 3}) sns.lineplot(data=df, x=\'day\', y=\'temperature\', ax=axes[2]) axes[2].set_title(\\"Talk Context\\") # Display the plots plt.tight_layout() plt.show() # Example usage: # plot_temperature_contexts(\'daily_temperatures.csv\')"},{"question":"**Title: Implementing a Custom SAX ContentHandler for XML Parsing** **Objective:** Implement a custom SAX `ContentHandler` in Python to parse an XML document and extract specific information. **Problem Statement:** You are tasked with processing an XML document using the SAX (Simple API for XML) parser. Specifically, you need to parse the document and extract data nested within certain tags. You must implement a custom `ContentHandler` class to achieve this. For this problem, you will extract information from `<book>` elements and print the title, author, and genre for each book. Below is a sample XML document: ```xml <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <genre>Fiction</genre> </book> <book> <title>1984</title> <author>George Orwell</author> <genre>Dystopian</genre> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <genre>Fiction</genre> </book> </library> ``` **Requirements:** 1. Implement a custom `ContentHandler` class (`BookHandler`) that inherits from `xml.sax.handler.ContentHandler`. 2. Override methods to handle the start and end of elements, and character data: - `startElement(self, name, attrs)` - `endElement(self, name)` - `characters(self, content)` 3. Extract and print the title, author, and genre for each book. 4. Handle any ignorable whitespace within the document. **Function Signature:** ```python class BookHandler(xml.sax.handler.ContentHandler): def __init__(self): # Initialization code here def startElement(self, name, attrs): # Code to handle the start of an element def endElement(self, name): # Code to handle the end of an element def characters(self, content): # Code to handle character data def parse_books(xml_file): # Function to parse the XML file using BookHandler ``` **Input:** - `xml_file`: A string representing the path to the XML file to be parsed. **Output:** - Console output of the title, author, and genre of each book in the format: ``` Title: The Great Gatsby Author: F. Scott Fitzgerald Genre: Fiction Title: 1984 Author: George Orwell Genre: Dystopian Title: To Kill a Mockingbird Author: Harper Lee Genre: Fiction ``` **Constraints:** - Assume the XML document is well-formed and follows the structure provided in the sample. - The handler should correctly handle multiple books within the `<library>` tag. - Whitespace and newlines between tags should not affect data extraction. **Example Usage:** ```python if __name__ == \\"__main__\\": parse_books(\\"path/to/your/library.xml\\") ``` **Notes:** - Ensure to use the SAX parser provided by the `xml.sax` module. - You are encouraged to test your implementation with the provided sample XML and additional test cases.","solution":"import xml.sax class BookHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \\"\\" self.title = \\"\\" self.author = \\"\\" self.genre = \\"\\" def startElement(self, name, attrs): self.current_element = name if name == \\"book\\": self.title = \\"\\" self.author = \\"\\" self.genre = \\"\\" def endElement(self, name): if name == \\"book\\": print(f\\"Title: {self.title}\\") print(f\\"Author: {self.author}\\") print(f\\"Genre: {self.genre}\\") print(\\"\\") self.current_element = \\"\\" def characters(self, content): if self.current_element == \\"title\\": self.title += content.strip() elif self.current_element == \\"author\\": self.author += content.strip() elif self.current_element == \\"genre\\": self.genre += content.strip() def parse_books(xml_file): parser = xml.sax.make_parser() handler = BookHandler() parser.setContentHandler(handler) parser.parse(xml_file)"},{"question":"**Question: Implement a Custom Logging System with `logging` in Python** # Problem: You are tasked with creating a multi-threaded logging system for a Python application using the `logging` module. The logging system should log messages to both the console and a file with different formats for each. Additionally, it should handle log messages from multiple threads and include contextual information specific to each thread. # Requirements: 1. **Custom Logger Setup:** - Create a custom logger named `custom_logger` with a logging level of `DEBUG`. 2. **Handlers:** - Add a `StreamHandler` to the logger for console output with a logging level of `INFO`. - Add a `FileHandler` to the logger for file output (`application.log`) with a logging level of `DEBUG`. 3. **Formatters:** - Use different formats for console and file outputs: - **Console Format:** `%(asctime)s - %(levelname)s - %(message)s` - **File Format:** `%(asctime)s - %(threadName)s - %(levelname)s - %(message)s` 4. **Threaded Logging:** - Implement a function `log_messages` that will be run by multiple threads. This function should log messages at various levels (`DEBUG`, `INFO`, `ERROR`) and include the thread name. 5. **Custom Contextual Information:** - Use a `Filter` to add a custom attribute `thread_id` to the logs which should capture the thread identifier. # Input: None. The script will be executed to demonstrate its functionality. # Output: The script should generate logs both on the console and in a file named `application.log`. The logs should contain entries from multiple threads with appropriate formatting and the custom `thread_id` attribute included in the file logs. # Constraints: - Ensure the file `application.log` is created in the same directory as the script. - Manage concurrency and ensure no log entries are missed or corrupted. # Function Signature: ```python import logging import threading def setup_custom_logger(): # Implementation goes here def log_messages(thread_id): # Implementation goes here if __name__ == \\"__main__\\": # Setup logger setup_custom_logger() # Create multiple threads threads = [] for i in range(5): thread = threading.Thread(target=log_messages, args=(i,), name=f\'Thread-{i}\') threads.append(thread) thread.start() for thread in threads: thread.join() ``` # Notes: 1. Ensure proper logging configuration before starting the threads. 2. Include sample log messages within `log_messages` function with different logging levels. 3. Use appropriate thread synchronization to demonstrate correct logging from multiple threads.","solution":"import logging import threading def setup_custom_logger(): # Step 1: Create a custom logger logger = logging.getLogger(\'custom_logger\') logger.setLevel(logging.DEBUG) # Logger level set to DEBUG # Step 2: Create handlers console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # Console handler level set to INFO file_handler = logging.FileHandler(\'application.log\') file_handler.setLevel(logging.DEBUG) # File handler level set to DEBUG # Step 3: Create formatters and add them to handlers console_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') file_formatter = logging.Formatter(\'%(asctime)s - %(threadName)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) file_handler.setFormatter(file_formatter) # Step 4: Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) # Step 5: Add filter to add custom attribute \'thread_id\' class ContextFilter(logging.Filter): def filter(self, record): thread_id = threading.get_ident() record.thread_id = thread_id return True logger.addFilter(ContextFilter()) def log_messages(thread_id): logger = logging.getLogger(\'custom_logger\') logger.debug(f\\"Debug message from thread {thread_id}.\\") logger.info(f\\"Info message from thread {thread_id}.\\") logger.error(f\\"Error message from thread {thread_id}.\\") if __name__ == \\"__main__\\": # Setup logger setup_custom_logger() # Create multiple threads threads = [] for i in range(5): thread = threading.Thread(target=log_messages, args=(i,), name=f\'Thread-{i}\') threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"# Python Coding Assessment Question Objective You are tasked with developing a utility function that extracts and processes user information from the Unix user account and password database using the `pwd` module. Task Implement a function `get_user_info(user_identifier: Union[int, str]) -> Dict[str, str]` that retrieves the details of a given user based on either their user ID (int) or login name (str). The function should return a dictionary containing the user\'s information with appropriate keys. Function Signature ```python from typing import Union, Dict import pwd def get_user_info(user_identifier: Union[int, str]) -> Dict[str, str]: pass ``` Input - `user_identifier` (Union[int, str]): A unique identifier for the user, which can be either an integer (user ID) or a string (login name). Output - Returns a dictionary with the following keys and corresponding values: - `\\"login_name\\"`: str, user\'s login name - `\\"uid\\"`: str, user\'s numerical user ID - `\\"gid\\"`: str, user\'s numerical group ID - `\\"home_dir\\"`: str, user\'s home directory - `\\"shell\\"`: str, user\'s command interpreter Constraints - You must handle the scenario where the user does not exist by raising a `ValueError` with the message `\\"User not found\\"`. - The function should work efficiently for a typical Unix user database. Example ```python # Example usage: info = get_user_info(1000) # Example output: # { # \\"login_name\\": \\"johndoe\\", # \\"uid\\": \\"1000\\", # \\"gid\\": \\"1000\\", # \\"home_dir\\": \\"/home/johndoe\\", # \\"shell\\": \\"/bin/bash\\" # } info = get_user_info(\\"johndoe\\") # Example output: # { # \\"login_name\\": \\"johndoe\\", # \\"uid\\": \\"1000\\", # \\"gid\\": \\"1000\\", # \\"home_dir\\": \\"/home/johndoe\\", # \\"shell\\": \\"/bin/bash\\" # } ``` Note: The actual output will depend on the user database entries present in the system. Performance Requirements The function should be able to process the lookup operations efficiently, assuming a typical number of entries in the password database of a Unix system.","solution":"from typing import Union, Dict import pwd def get_user_info(user_identifier: Union[int, str]) -> Dict[str, str]: Retrieves the user information based on either user ID (int) or login name (str). Args: user_identifier (Union[int, str]): A unique identifier for the user, which can be either an integer (user ID) or a string (login name). Returns: Dict[str, str]: A dictionary containing user\'s information. Raises: ValueError: If the user is not found. try: if isinstance(user_identifier, int): user_info = pwd.getpwuid(user_identifier) else: user_info = pwd.getpwnam(user_identifier) return { \\"login_name\\": user_info.pw_name, \\"uid\\": str(user_info.pw_uid), \\"gid\\": str(user_info.pw_gid), \\"home_dir\\": user_info.pw_dir, \\"shell\\": user_info.pw_shell } except KeyError: raise ValueError(\\"User not found\\")"},{"question":"# Question: Custom Serialization with Nested Objects You are given a nested object structure that includes instances of different classes. Your task is to implement custom pickling support for these objects using the `copyreg` module. This will involve writing custom reduction functions and registering them, so the `pickle` module can properly serialize and deserialize the nested structures. Classes Definition Consider the following classes: ```python class Person: def __init__(self, name, age): self.name = name self.age = age class Address: def __init__(self, street, city): self.street = street self.city = city class Employee(Person): def __init__(self, name, age, employee_id, address): super().__init__(name, age) self.employee_id = employee_id self.address = address ``` Task 1. Implement a custom pickling function for the `Address` class. 2. Implement a custom pickling function for the `Employee` class that properly handles the nested `Address` object. 3. Register the custom pickling functions using the `copyreg` module. Constraints - Ensure that the pickling and unpickling process can handle the nested structure of `Employee` and `Address` objects. - Make sure no information is lost during the serialization and deserialization process. Input - `employee` (Employee): An instance of the `Employee` class. Output - A pickled string that represents the serialized form of the `employee` object which can be deserialized back into an equivalent `Employee` object. Example ```python import pickle def pickle_address(address): return Address, (address.street, address.city) def pickle_employee(employee): return Employee, (employee.name, employee.age, employee.employee_id, employee.address) # Register the custom pickling functions copyreg.pickle(Address, pickle_address) copyreg.pickle(Employee, pickle_employee) # Create instances of the classes address = Address(\\"123 Main St\\", \\"Springfield\\") employee = Employee(\\"John Doe\\", 30, \\"E12345\\", address) # Serialize the employee object serialized_employee = pickle.dumps(employee) # Deserialize the employee object deserialized_employee = pickle.loads(serialized_employee) # Check the equality assert employee.name == deserialized_employee.name assert employee.age == deserialized_employee.age assert employee.employee_id == deserialized_employee.employee_id assert employee.address.street == deserialized_employee.address.street assert employee.address.city == deserialized_employee.address.city ``` In your implementation, focus on the functions `pickle_address` and `pickle_employee` and register them using `copyreg.pickle`.","solution":"import copyreg import pickle class Person: def __init__(self, name, age): self.name = name self.age = age class Address: def __init__(self, street, city): self.street = street self.city = city class Employee(Person): def __init__(self, name, age, employee_id, address): super().__init__(name, age) self.employee_id = employee_id self.address = address def pickle_address(address): return Address, (address.street, address.city) def pickle_employee(employee): return Employee, (employee.name, employee.age, employee.employee_id, employee.address) # Register the custom pickling functions copyreg.pickle(Address, pickle_address) copyreg.pickle(Employee, pickle_employee)"},{"question":"You are tasked with implementing a Python function that connects to an IMAP server, logs in, retrieves emails based on specific search criteria, and processes the retrieved emails to extract some information. Here is the detailed specification for your function: # Function Specification **Function Name**: `fetch_and_process_emails` **Parameters**: - `host` (str): The hostname of the IMAP server. - `port` (int): The port number to connect to the IMAP server. Default is the standard IMAP port 143. - `username` (str): The username to log in to the IMAP server. - `password` (str): The password to log in to the IMAP server. - `search_criterion` (str): The search criterion to filter emails. This should be a valid IMAP search key such as \'ALL\', \'FROM \\"example@example.com\\"\', \'SINCE \\"01-Jan-2021\\"\', etc. **Returns**: - A list of tuples where each tuple contains the email ID and the subject of the email that matched the search criterion. The format should be [(email_id1, subject1), (email_id2, subject2), ...]. **Constraints**: - The function should handle any necessary exceptions and ensure the connection to the server is gracefully closed even if an error occurs. - You should make use of the IMAP4 class and its methods for connecting to the server and performing the required operations. - The subject of the email should be extracted from the fetched email data and be in a readable string format. # Example Usage ```python def fetch_and_process_emails(host, port, username, password, search_criterion): import imaplib import email # connect and login to IMAP server with imaplib.IMAP4(host, port) as M: M.login(username, password) M.select(\'INBOX\') # search emails based on criterion typ, data = M.search(None, search_criterion) email_ids = data[0].split() # process each email emails = [] for email_id in email_ids: typ, msg_data = M.fetch(email_id, \'(RFC822)\') raw_email = msg_data[0][1] msg = email.message_from_bytes(raw_email) subject = msg[\'subject\'] emails.append((email_id.decode(), subject)) # logout from the server M.logout() return emails # Example call to function emails = fetch_and_process_emails(\'imap.example.com\', 143, \'your_username\', \'your_password\', \'ALL\') print(emails) ``` # Notes: 1. You should import the necessary modules (`imaplib` and `email`) within your function. 2. Ensure to handle exceptions properly using try-except blocks to handle potential errors during connection, login, or data retrieval. 3. The solution should make use of the IMAP4 methods such as `login`, `select`, `search`, `fetch`, and `logout`. 4. The function should handle the extraction and decoding of subject data correctly.","solution":"def fetch_and_process_emails(host, port, username, password, search_criterion): import imaplib import email from email.header import decode_header emails = [] try: # Connect to the server M = imaplib.IMAP4(host, port) M.login(username, password) M.select(\'INBOX\') # Search for emails based on the search criterion typ, data = M.search(None, search_criterion) if typ != \'OK\': return [] email_ids = data[0].split() # Process each email ID for email_id in email_ids: typ, msg_data = M.fetch(email_id, \'(RFC822)\') if typ != \'OK\': continue raw_email = msg_data[0][1] msg = email.message_from_bytes(raw_email) # Decode the email subject subject, encoding = decode_header(msg[\'subject\'])[0] if isinstance(subject, bytes): subject = subject.decode(encoding or \'utf-8\') emails.append((email_id.decode(), subject)) except Exception as e: print(f\\"An error occurred: {e}\\") finally: try: M.logout() except: pass return emails"},{"question":"# Pandas Duplicate Label Handling and Prevention Problem Statement You are tasked with cleaning a raw pandas DataFrame that might contain duplicate row and/or column labels. Your goal is to implement a function, `clean_and_process_dataframe(df: pd.DataFrame) -> pd.DataFrame`, that performs the following steps: 1. **Remove Duplicate Rows**: Identify and remove duplicate row indices, keeping only the first occurrence. 2. **Remove Duplicate Columns**: Identify and remove duplicate columns, keeping only the first occurrence. 3. **Disallow Future Duplicates**: Set the DataFrame configuration to disallow any future introduction of duplicate labels. 4. **Validation**: Perform a specific operation and verify that the DataFrame configuration for disallowing duplicates is still in effect. Function Signature ```python def clean_and_process_dataframe(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input - `df`: A pandas DataFrame that may contain duplicate row/column labels. Output - Returns a pandas DataFrame that has had duplicate labels removed and is configured to disallow future duplicates. Constraints - You are not allowed to change the order of the rows or columns. - Use pandas operations to identify and remove duplicates. - The validation step should involve renaming a column to verify the configuration. Example ```python import pandas as pd # Example input DataFrame with duplicate labels data = { \'A\': [1, 2, 3, 4], \'B\': [5, 6, 7, 8], \'C\': [9, 10, 11, 12] } index = [\'row1\', \'row1\', \'row2\', \'row3\'] columns = [\'col1\', \'col2\', \'col2\'] df = pd.DataFrame(data, index=index, columns=columns) # Process the DataFrame cleaned_df = clean_and_process_dataframe(df) # Expected Output # cleaned_df should have no duplicate row/column labels and disallow future duplicates print(cleaned_df) print(cleaned_df.flags.allows_duplicate_labels) # Should output: False ``` Note Grading will consider the following: - Accurate detection and removal of duplicate labels. - Correct setting of the DataFrame flag to disallow future duplicates. - Successful validation that the condition persists after an operation.","solution":"import pandas as pd def clean_and_process_dataframe(df: pd.DataFrame) -> pd.DataFrame: Cleans the DataFrame by removing duplicate row and column labels and sets the configuration to disallow future duplicates. Parameters: df (pd.DataFrame): The raw DataFrame that may contain duplicate row/column labels. Returns: pd.DataFrame: A cleaned DataFrame with duplicates removed and configured to disallow future duplicates. # Remove duplicate rows, keeping only the first occurrence df = df[~df.index.duplicated(keep=\'first\')] # Remove duplicate columns, keeping only the first occurrence df = df.loc[:, ~df.columns.duplicated(keep=\'first\')] # Set the flag to disallow future duplicates df.flags.allows_duplicate_labels = False # Validation by attempting to introduce a duplicate label should raise a ValueError try: df.rename(columns={\'A\': \'col1\'}, inplace=True) # Attempting to introduce a duplicate column except ValueError as e: pass return df"},{"question":"# Question: Implementing a Custom Asynchronous Generator for Prime Numbers You will implement an asynchronous generator function that yields prime numbers. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. You need to implement an asynchronous generator function `async primes(n: int) -> int` that yields the first `n` prime numbers asynchronously. Requirements: - The function should be an asynchronous generator. - It should yield prime numbers one by one. - You should utilize the `yield` keyword in an asynchronous context. - You should write a main coroutine to consume these prime numbers and print them. Constraints: - The input `n` should be a positive integer (1 â‰¤ n â‰¤ 10,000). Example: ```python import asyncio async def main(): async for prime in primes(10): print(prime) # Expected output (First 10 prime numbers): # 2 # 3 # 5 # 7 # 11 # 13 # 17 # 19 # 23 # 29 # Execute the main coroutine asyncio.run(main()) ``` Hints: - Remember to use `async def` for defining asynchronous functions and `async for` for iterating over asynchronous generators. - Ensure your implementation handles the asynchronous nature correctly, meaning the `yield` points must be awaited. Implement the `primes` function and the main coroutine as described. ```python import asyncio async def primes(n: int) -> int: # Your implementation here pass async def main(): async for prime in primes(10): print(prime) # Execute the main coroutine if __name__ == \\"__main__\\": asyncio.run(main()) ```","solution":"import asyncio async def primes(n: int): Asynchronous generator to yield the first n prime numbers. def is_prime(number): if number <= 1: return False if number == 2: return True if number % 2 == 0: return False for i in range(3, int(number ** 0.5) + 1, 2): if number % i == 0: return False return True count, num = 0, 2 while count < n: if is_prime(num): yield num count += 1 num += 1 async def main(): async for prime in primes(10): print(prime) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Question:** You have been tasked to generate an informative and visually appealing report for a dataset using seaborn. This will require you to manipulate seaborn\'s color palettes and apply them to different plots. You will need to perform the following steps: 1. Create a customized color palette using the HUSL color system with 5 colors. 2. Generate a gradient blend between two color endpoints using a blend gradient. 3. Use these color palettes in seaborn\'s visualization plots. 4. Extract the hexadecimal codes of these generated color palettes. **Steps:** 1. **Customized HUSL Color Palette:** - Implement a function `create_husl_palette(num_colors: int) -> list` that takes the number of desired colors as an input and returns a HUSL color palette with that number of colors as a list of strings. - Example Input: `5` - Example Output: `[\'#f77189\', \'#d1a0ff\', \'#66a3ff\', \'#00b65c\', \'#bedb00\']` 2. **Gradient Blend Color Palette:** - Implement a function `create_blend_palette(start_color: str, end_color: str) -> list` that takes two color codes as inputs and returns a blended gradient between the two endpoints as a list of 256 colors. - Example Input: `(\'#ff0000\', \'#0000ff\')` - Example Output: A list of 256 hexadecimal color codes blending from red to blue. 3. **Visualization with Custom Palettes:** - Use the previously created HUSL and blend gradients in seaborn plots to visualize any sample dataset of your choice (e.g., `penguins`, `tips`). - Implement a function `visualize_with_palettes(data, husl_palette: list, blend_palette: list)` that creates two subplots: - A scatter plot using the HUSL color palette. - A heatmap using the blend gradient. - Make sure to label axes and provide a title for the plots. 4. **Extract Hexadecimal Codes:** - Implement a function `get_hex_codes(palette: list) -> list` that takes a list of RGB color codes and returns their corresponding hexadecimal codes. - Example Input: `[(1.0, 0.0, 0.0), (0.0, 1.0, 0.0)]` - Example Output: `[\'#ff0000\', \'#00ff00\']` **Constraints:** - Import necessary libraries like seaborn and matplotlib. - Use seaborn for all visualizations. - Ensure your code is well-documented and follows best practices. **Performance Requirements:** - The `create_blend_palette` function should efficiently generate a gradient with a good performance, even when increasing the number of colors. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import matplotlib.colors as mcolors from seaborn.palettes import color_palette def create_husl_palette(num_colors: int) -> list: Creates a HUSL color palette with the specified number of colors. return sns.color_palette(\\"husl\\", num_colors).as_hex() def create_blend_palette(start_color: str, end_color: str, num_colors: int = 256) -> list: Generates a gradient blend between two color endpoints. return sns.blend_palette([start_color, end_color], n_colors=num_colors, as_cmap=False).as_hex() def visualize_with_palettes(data, husl_palette: list, blend_palette: list): Creates visualizations using custom color palettes. fig, axs = plt.subplots(1, 2, figsize=(14, 6)) # Scatter plot using the HUSL color palette sns.scatterplot(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", palette=husl_palette, data=data, ax=axs[0]) axs[0].set_title(\'Scatter Plot with HUSL Palette\') axs[0].set_xlabel(\'Bill Length (mm)\') axs[0].set_ylabel(\'Bill Depth (mm)\') # Heatmap using the blend gradient palette pivot_table = data.pivot_table(index=\'species\', values=\'bill_length_mm\', aggfunc=np.mean) sns.heatmap(pivot_table, cmap=mcolors.ListedColormap(blend_palette), ax=axs[1]) axs[1].set_title(\'Heatmap with Blend Gradient\') plt.tight_layout() plt.show() def get_hex_codes(palette: list) -> list: Extracts the hexadecimal codes of the given palette. return [mcolors.rgb2hex(color) for color in palette]"},{"question":"**Objective:** You are provided with a dataset containing high-dimensional data. Your task is to implement a data preprocessing and dimensionality reduction pipeline using scikit-learn, followed by a basic supervised learning task. **Dataset:** You can use any high-dimensional dataset available in scikit-learn, such as the `digits` dataset. **Requirements:** 1. **Data Loading and Preprocessing:** - Load the `digits` dataset from `sklearn.datasets`. - Normalize the feature values using `StandardScaler`. 2. **Dimensionality Reduction:** - Reduce the dimensionality of the dataset using PCA such that the explained variance is at least 95%. 3. **Supervised Learning:** - Split the dataset into training and testing sets. - Train a `KNeighborsClassifier` on the training set and evaluate it on the test set. 4. **Performance Metrics:** - Provide the accuracy score of the classifier on the test set. **Input Format:** - No explicit input (all steps should be hardcoded for the provided dataset). **Output Format:** - Accuracy score of the `KNeighborsClassifier` on the test set after the dimensionality reduction and preprocessing steps. **Constraints:** - You must use PCA for dimensionality reduction. - The explained variance after PCA must be at least 95%. **Example Code Template:** ```python from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def main(): # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Normalize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply PCA to reduce dimensionality pca = PCA(n_components=0.95) X_reduced = pca.fit_transform(X_scaled) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42) # Train the KNeighborsClassifier knn = KNeighborsClassifier() knn.fit(X_train, y_train) # Predict on the test set y_pred = knn.predict(X_test) # Compute and print accuracy score accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") if __name__ == \\"__main__\\": main() ``` **Notes:** - Ensure that all steps in the question are followed correctly. - Document any assumptions or decisions made during the implementation.","solution":"from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def preprocess_and_train(): # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Normalize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply PCA to reduce dimensionality pca = PCA(n_components=0.95) X_reduced = pca.fit_transform(X_scaled) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42) # Train the KNeighborsClassifier knn = KNeighborsClassifier() knn.fit(X_train, y_train) # Predict on the test set y_pred = knn.predict(X_test) # Compute and return accuracy score accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"You have been provided with a list of Python objects that need to be serialized and deserialized using the `pickle` module. These objects include basic data types, a dictionary, and instances of user-defined classes. Your task is to: 1. Implement a function `serialize_objects` that accepts a list of objects and a file name. The function should serialize the objects to a file using the `pickle` module. 2. Implement a function `deserialize_objects` that reads from the given file and returns the list of deserialized objects. Additionally, you need to demonstrate the ability to handle stateful objects and custom serialization. **Requirements:** 1. Define a class `CustomObject` that: - Has attributes `name` (string) and `value` (integer). - Implements custom serialization to include a timestamp of serialization. - Provides a method `__repr__` to return a string representation of its state. 2. Test your functions with a list of various objects, including: - Basic types such as `int`, `str`, `list`, and `dict`. - Instances of `CustomObject`. **Input and Output Format:** - `serialize_objects(objects: list, filename: str) -> None`: Serializes the objects to the specified file. - `deserialize_objects(filename: str) -> list`: Deserializes the objects from the specified file and returns them as a list. **Constraints:** - Use the `pickle` module for serialization. - Ensure that the custom serialization for `CustomObject` includes the current timestamp. - Each attribute in `CustomObject` should be restored correctly upon deserialization. # Example ```python import pickle from datetime import datetime from typing import Any, List # Define the CustomObject class class CustomObject: def __init__(self, name: str, value: int): self.name = name self.value = value def __repr__(self): return f\\"CustomObject(name={self.name}, value={self.value})\\" def __reduce__(self): return (self.__class__, (self.name, self.value)), (datetime.now(),) def serialize_objects(objects: List[Any], filename: str) -> None: with open(filename, \'wb\') as f: pickle.dump(objects, f) def deserialize_objects(filename: str) -> List[Any]: with open(filename, \'rb\') as f: return pickle.load(f) # Create objects for testing obj_list = [42, \\"Hello, World!\\", [1, 2, 3], {\\"key\\": \\"value\\"}, CustomObject(\\"test\\", 99)] # Serialize the objects serialize_objects(obj_list, \'testfile.pkl\') # Deserialize the objects deserialized_obj_list = deserialize_objects(\'testfile.pkl\') # Check deserialized objects print(deserialized_obj_list) ``` **Expected Output:** The deserialized objects should match the original list, with `CustomObject` instances displaying their state correctly.","solution":"import pickle from datetime import datetime from typing import Any, List class CustomObject: def __init__(self, name: str, value: int): self.name = name self.value = value self.timestamp = None def __repr__(self): return f\\"CustomObject(name={self.name}, value={self.value}, timestamp={self.timestamp})\\" def __getstate__(self): state = self.__dict__.copy() state[\'timestamp\'] = datetime.now() return state def __setstate__(self, state): self.__dict__.update(state) def serialize_objects(objects: List[Any], filename: str) -> None: with open(filename, \'wb\') as f: pickle.dump(objects, f) def deserialize_objects(filename: str) -> List[Any]: with open(filename, \'rb\') as f: return pickle.load(f)"},{"question":"Optimizing Matrix Multiplication with PyTorch Backends **Context:** In this task, you will use various PyTorch backends to optimize a matrix multiplication operation. You are required to write a function that computes the multiplication of two large matrices, leveraging the appropriate PyTorch backends based on the availability and hardware capabilities. Your function should be optimized for performance but also include fallbacks in case certain hardware features are not available. **Task:** Write a Python function `optimized_matrix_multiplication(A, B)` that takes as input two large matrices `A` and `B`, and performs optimized matrix multiplication using PyTorch. Your function should consider the following: 1. Utilize CUDA for GPU acceleration if available. 2. Use cuDNN for further optimization if CUDA is available. 3. Enable TensorFloat-32 (TF32) and reduced precision reductions on appropriate hardware (like NVIDIA Ampere GPUs). 4. Ensure fallbacks to CPU computation if CUDA is not available. **Function Signature:** ```python import torch def optimized_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: pass ``` **Expected Input Format:** - `A` and `B` are both 2D tensors (matrices) of type `torch.Tensor` with compatible dimensions for matrix multiplication. **Expected Output Format:** - The function should return the result of the matrix multiplication as a `torch.Tensor`. **Constraints:** - If CUDA is available (`torch.cuda.is_available()` is `True`), the function should ensure the computation runs on the GPU. - Use cuDNN if available (`torch.backends.cudnn.is_available()` is `True`). - Enable TensorFloat-32 and reduced precision rules for supported GPUs. - If CUDA is not available, perform the multiplication on the CPU. **Example Usage:** ```python # Example setup (assuming CUDA is available and appropriately configured) A = torch.rand(1024, 512, device=\'cuda\') B = torch.rand(512, 1024, device=\'cuda\') result = optimized_matrix_multiplication(A, B) print(result) ``` **Performance Requirements:** - The function should be optimized for performance using the available backends and their specific settings. - Ensure that the performance gains are evident when CUDA and cuDNN are utilized compared to CPU execution. **Notes:** - You do not need to implement any printing or file I/O. Focus purely on the computation and optimization aspects. - The solution should be robust and gracefully fall back to CPU computation if GPU resources are not available.","solution":"import torch def optimized_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: # Check if CUDA is available if torch.cuda.is_available(): # Move tensors to GPU A = A.to(\'cuda\') B = B.to(\'cuda\') # If cuDNN is available and enabled if torch.backends.cudnn.is_available() and torch.backends.cudnn.enabled: # Enable cuDNN and optimized settings with torch.backends.cudnn.flags(enabled=True, deterministic=False): if torch.cuda.get_device_capability()[0] >= 8: # Check if GPU is Ampere or newer torch.backends.cuda.matmul.allow_tf32 = True # Enable TF32 for Ampere GPUs torch.set_float32_matmul_precision(\'high\') result = torch.matmul(A, B) else: result = torch.matmul(A, B) return result.to(\'cpu\') # Move result back to CPU else: # Perform matrix multiplication on CPU return torch.matmul(A, B)"},{"question":"Objective Implement a function that reads binary data from a file and decodes it into human-readable values using the `struct` module. The function should also handle different byte orders and proper alignment based on the data specification provided. Problem Statement You are given a binary file named `data.bin` that contains records of students. Each record has the following structure: - `ID` (unsigned int, 4 bytes) - `Name` (string of 10 characters, 10 bytes) - `Grade` (unsigned char, 1 byte) - `Age` (unsigned short, 2 bytes) 1. Implement a function `decode_student_records(file_path: str, byte_order: str) -> List[Tuple[int, str, int, int]]` that: - Reads the binary data from the specified file. - Uses the provided `byte_order` to properly decode the data. The `byte_order` parameter can take the following values: - `\'<\'` for little-endian - `\'>\'` for big-endian - Returns a list of tuples where each tuple represents a student record in the format `(ID, Name, Grade, Age)`. 2. Implement a function `write_student_records(file_path: str, records: List[Tuple[int, str, int, int]], byte_order: str) -> None` that: - Takes a list of tuples representing student records. - Uses the provided `byte_order` to properly encode the data. - Writes the encoded binary data to the specified file. Constraints - The `Name` field will always be a string of exactly 10 characters (including spaces if necessary). - The `Grade` is an integer between 0 and 255. - The `Age` is an integer between 0 and 65535. - Consider exception handling for file operations and struct errors. Example ```python # Example usage byte_order = \'<\' # Little-endian records = [ (12345, \'Alice \', 90, 20), (67890, \'Bob \', 85, 21), ] write_student_records(\'data.bin\', records, byte_order) decoded_records = decode_student_records(\'data.bin\', byte_order) print(decoded_records) # Output: [(12345, \'Alice \', 90, 20), (67890, \'Bob \', 85, 21)] ``` Note - Make sure to properly handle the binary file operations (open, read, write, close). - Include error handling for scenarios where data might be corrupted or not conform to the expected format.","solution":"import struct from typing import List, Tuple def decode_student_records(file_path: str, byte_order: str) -> List[Tuple[int, str, int, int]]: record_fmt = f\'{byte_order}I10sB H\' record_size = struct.calcsize(record_fmt) records = [] try: with open(file_path, \'rb\') as file: while True: record_bytes = file.read(record_size) if not record_bytes: break record = struct.unpack(record_fmt, record_bytes) records.append((record[0], record[1].decode(\'utf-8\').rstrip(\'x00\'), record[2], record[3])) except (OSError, struct.error) as e: raise Exception(f\\"Failed to decode student records: {e}\\") return records def write_student_records(file_path: str, records: List[Tuple[int, str, int, int]], byte_order: str) -> None: record_fmt = f\'{byte_order}I10sB H\' try: with open(file_path, \'wb\') as file: for record in records: packed_record = struct.pack( record_fmt, record[0], record[1].encode(\'utf-8\'), record[2], record[3] ) file.write(packed_record) except (OSError, struct.error) as e: raise Exception(f\\"Failed to write student records: {e}\\")"},{"question":"Using the \\"bz2\\" module, you are required to complete a function that compresses a given input file, writes the compressed data to an output file, and then reads the compressed file to decompress its contents back into another file. The purpose of this function is to demonstrate skills in file handling, compression, and decompression. Requirements 1. **compress_file(input_file_path: str, compressed_file_path: str)**: - **Input**: `input_file_path` (Str) - The path to the input text file that needs to be compressed. - **Output**: `compressed_file_path` (Str) - The path where the compressed file should be stored. 2. **decompress_file(compressed_file_path: str, output_file_path: str)**: - **Input**: `compressed_file_path` (Str) - The path to the compressed file. - **Output**: `output_file_path` (Str) - The path where the decompressed file should be stored. 3. **Constraints**: - Assume the input text file is in plain text format. - You should use `bz2.BZ2File` or `bz2.open` for file compression and decompression. - The compression level should be set to the default value (compresslevel=9). 4. **Performance Requirements**: - The function should handle cases with large files efficiently. Implement the following function signatures: ```python import bz2 def compress_file(input_file_path: str, compressed_file_path: str) -> None: pass def decompress_file(compressed_file_path: str, output_file_path: str) -> None: pass ``` # Example Usage Given an input file `example.txt` with the following content: ``` Hello, this is a test file. This file will be compressed using bz2. ``` 1. Compress the input file: ```python compress_file(\'example.txt\', \'example.txt.bz2\') ``` - This should create a compressed file `example.txt.bz2`. 2. Decompress the created compressed file: ```python decompress_file(\'example.txt.bz2\', \'example_decompressed.txt\') ``` - This should create a decompressed file `example_decompressed.txt` with content identical to `example.txt`. # Note: Ensure that the decompressed file matches the original input file content exactly to pass the assessment.","solution":"import bz2 def compress_file(input_file_path: str, compressed_file_path: str) -> None: Compresses a given input file and writes the compressed data to an output file. Args: input_file_path (str): The path to the input text file that needs to be compressed. compressed_file_path (str): The path where the compressed file should be stored. with open(input_file_path, \'rb\') as input_file: data = input_file.read() with bz2.open(compressed_file_path, \'wb\') as compressed_file: compressed_file.write(data) def decompress_file(compressed_file_path: str, output_file_path: str) -> None: Reads the compressed file, decompresses its contents, and writes the decompressed data to another file. Args: compressed_file_path (str): The path to the compressed file. output_file_path (str): The path where the decompressed file should be stored. with bz2.open(compressed_file_path, \'rb\') as compressed_file: data = compressed_file.read() with open(output_file_path, \'wb\') as output_file: output_file.write(data)"},{"question":"# Multiclass and Multioutput Classification & Regression with scikit-learn You are provided with datasets for both multiclass classification and multioutput regression tasks. Your objective is to implement classifiers and regressors using scikit-learn\'s meta-estimators for these datasets. The datasets are as follows: 1. A multiclass classification dataset (`iris` dataset). 2. A synthesized multioutput regression dataset involving two independent regressors. You will need to perform the following tasks: 1. **Multiclass Classification**: - Load the `iris` dataset. - Implement a `OneVsRestClassifier` using `LinearSVC` as the base classifier. - Train the model and predict the labels on the training data. - Calculate and print the accuracy of the model on the training data. 2. **Multioutput Regression**: - Generate a synthetic dataset using `make_regression` for multioutput regression. - Implement a `MultiOutputRegressor` using `GradientBoostingRegressor` as the base regressor. - Train the model and predict the output on the training data. - Print the predicted values. **Input/Output** - Input: The code should not take direct input from the user. Instead, use the scikit-learn datasets and random seed specified. - Output: Accuracy of the multiclass classifier on training data, and predicted values for multioutput regression. **Constraints**: - Use `random_state=0` for reproducibility in random processes like data splitting and model initialization. - Ensure to import necessitated modules and handle libraries efficiently. # Example ```python # Example of code for multiclass classification from sklearn import datasets from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score # Multiclass Classification X, y = datasets.load_iris(return_X_y=True) ovr_clf = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y) predictions = ovr_clf.predict(X) accuracy = accuracy_score(y, predictions) print(\\"Multiclass Classification Accuracy:\\", accuracy) # Example of code for multi-output regression from sklearn.datasets import make_regression from sklearn.multioutput import MultiOutputRegressor from sklearn.ensemble import GradientBoostingRegressor # Multioutput Regression X, y = make_regression(n_samples=10, n_targets=3, random_state=0) mor_clf = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y) predicted_values = mor_clf.predict(X) print(\\"Multioutput Regression Predicted Values:n\\", predicted_values) ``` **Note**: This example code blocks use certain hyperparameters and may be modified to explore different configurations suitable for model performance.","solution":"from sklearn import datasets from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score from sklearn.datasets import make_regression from sklearn.multioutput import MultiOutputRegressor from sklearn.ensemble import GradientBoostingRegressor def multiclass_classification(): # Load iris dataset X, y = datasets.load_iris(return_X_y=True) # Implement OneVsRestClassifier using LinearSVC ovr_clf = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y) # Predict on training data predictions = ovr_clf.predict(X) # Calculate accuracy accuracy = accuracy_score(y, predictions) return accuracy def multioutput_regression(): # Generate synthetic multioutput regression dataset X, y = make_regression(n_samples=10, n_targets=3, random_state=0) # Implement MultiOutputRegressor using GradientBoostingRegressor mor_clf = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y) # Predict on training data predicted_values = mor_clf.predict(X) return predicted_values"},{"question":"# Gradient Verification with PyTorch\'s gradcheck In this task, you will implement a custom function and verify its gradients using PyTorch\'s `gradcheck` and `gradgradcheck` utilities. Your function will handle both real and complex inputs with real outputs. You will then use `gradcheck` and `gradgradcheck` to ensure that the gradients are correctly computed. Function Specification: 1. **Custom Function**: Implement a function `my_custom_function(x)` that takes a tensor `x` as input. - For real inputs, let `f: R^N -> R^M`, compute `y = f(x) = x^2`. - For complex inputs, let `g: C^N -> R^M`, compute `y = g(z) = |z|^2 = real(z^*z) = a^2 + b^2`. 2. **Gradient Verification**: Write code to verify the gradients of `my_custom_function` using: - `torch.autograd.gradcheck` - `torch.autograd.gradgradcheck` - Compare using `fast_mode` and without `fast_mode`. Expected Input and Output Formats: - **Input**: - A real or complex-valued tensor `x` of dimension (N,). - **Output**: - `None`. Print the results of your gradient checks. Constraints: - Use PyTorch to implement your function and gradient checks. - Ensure that your function correctly handles both real and complex-valued inputs. Performance Requirements: - Ensure that the gradient checks run efficiently with the `fast_mode` option enabled. - Verify correctness of the gradients both with and without `fast_mode`. Example: ```python import torch from torch.autograd import gradcheck, gradgradcheck def my_custom_function(x): return (x * x).sum() # Create a real-valued tensor x_real = torch.tensor([1.0, 2.0, 3.0], dtype=torch.double, requires_grad=True) # Create a complex-valued tensor x_complex = torch.tensor([1.0 + 1.0j, 2.0 + 2.0j, 3.0 + 3.0j], dtype=torch.cdouble, requires_grad=True) # Perform gradcheck without fast_mode real_gradcheck = gradcheck(my_custom_function, (x_real,)) complex_gradcheck = gradcheck(my_custom_function, (x_complex,)) # Perform gradcheck with fast_mode enabled real_gradcheck_fast = gradcheck(my_custom_function, (x_real,), fast_mode=True) complex_gradcheck_fast = gradcheck(my_custom_function, (x_complex,), fast_mode=True) # Perform gradgradcheck real_gradgradcheck = gradgradcheck(my_custom_function, (x_real,)) complex_gradgradcheck = gradgradcheck(my_custom_function, (x_complex,)) print(\\"Real Gradcheck:\\", real_gradcheck) print(\\"Complex Gradcheck:\\", complex_gradcheck) print(\\"Real Gradcheck Fast Mode:\\", real_gradcheck_fast) print(\\"Complex Gradcheck Fast Mode:\\", complex_gradcheck_fast) print(\\"Real Gradgradcheck:\\", real_gradgradcheck) print(\\"Complex Gradgradcheck:\\", complex_gradgradcheck) ``` Your task is to implement `my_custom_function` and the gradient checks as specified, run the gradient checks and print the results.","solution":"import torch from torch.autograd import gradcheck, gradgradcheck def my_custom_function(x): Computes the squared sum of the input tensor. For complex inputs, it computes the sum of squared magnitudes. Args: - x (torch.Tensor): Input tensor which can be real or complex. Returns: - torch.Tensor: Single element tensor containing the result. if x.is_complex(): return (x.abs() ** 2).sum() else: return (x ** 2).sum() # Create a real-valued tensor x_real = torch.tensor([1.0, 2.0, 3.0], dtype=torch.double, requires_grad=True) # Create a complex-valued tensor x_complex = torch.tensor([1.0 + 1.0j, 2.0 + 2.0j, 3.0 + 3.0j], dtype=torch.cdouble, requires_grad=True) # Perform gradcheck without fast_mode real_gradcheck = gradcheck(my_custom_function, (x_real,)) complex_gradcheck = gradcheck(my_custom_function, (x_complex,)) # Perform gradcheck with fast_mode enabled real_gradcheck_fast = gradcheck(my_custom_function, (x_real,), fast_mode=True) complex_gradcheck_fast = gradcheck(my_custom_function, (x_complex,), fast_mode=True) # Perform gradgradcheck real_gradgradcheck = gradgradcheck(my_custom_function, (x_real,)) complex_gradgradcheck = gradgradcheck(my_custom_function, (x_complex,)) # Print results print(\\"Real Gradcheck:\\", real_gradcheck) print(\\"Complex Gradcheck:\\", complex_gradcheck) print(\\"Real Gradcheck Fast Mode:\\", real_gradcheck_fast) print(\\"Complex Gradcheck Fast Mode:\\", complex_gradcheck_fast) print(\\"Real Gradgradcheck:\\", real_gradgradcheck) print(\\"Complex Gradgradcheck:\\", complex_gradgradcheck)"},{"question":"You are required to create a PyTorch-centric utility function that leverages the **torch.mps** module for managing random number generation and profiling Metal Performance Shaders on macOS. Your function should demonstrate a clear understanding of device management, memory handling, and performance measurement. # Task Implement the function `mps_random_number_gen` that fulfills the following requirements: 1. **Inputs:** - `seed_value`: An integer to set the random number generator seed. - `profile_duration`: A float representing the duration (in seconds) for which to profile the MPS operations. 2. **Outputs:** - `rng_state`: The random number generator state after setting the seed. - `allocated_memory`: The amount of memory currently allocated after setting the RNG state. - `profile_report`: A string indicating whether profiling was successful. 3. **Functionality:** - Set the provided seed value using `manual_seed`. - Retrieve and return the current random number generator state using `get_rng_state`. - Measure the amount of memory currently allocated using `current_allocated_memory`. - Start and stop profiling around a simple MPS operation, such as synchronizing the device using `synchronize`. - Ensure the profiler runs for the specified `profile_duration` before capturing the performance data. - Return a profiling success statement if profiling is completed successfully. # Constraints - The function must handle exceptions gracefully, especially those related to unsupported devices. - The function should use synchronization to ensure operations are completed within the given timeframe. # Example Usage ```python def mps_random_number_gen(seed_value: int, profile_duration: float): import torch import time try: if torch.mps.device_count() == 0: return \\"No MPS device available.\\" # Set manual seed torch.mps.manual_seed(seed_value) # Get random number generator state rng_state = torch.mps.get_rng_state() # Measure allocated memory allocated_memory = torch.mps.current_allocated_memory() # Start profiling torch.mps.profiler.start() time.sleep(profile_duration) # Simulate work torch.mps.synchronize() # Ensure all ops are done torch.mps.profiler.stop() profile_report = \\"Profiling successful.\\" return rng_state, allocated_memory, profile_report except Exception as e: return str(e) # Example: Run the function result = mps_random_number_gen(42, 1.0) print(result) ``` # Constraints and Limitations - Ensure the solution runs efficiently within the provided `profile_duration`. - Handle device availability checks to avoid errors in non-MPS environments. - Ensure error handling for unsupported operations on devices without MPS.","solution":"def mps_random_number_gen(seed_value: int, profile_duration: float): import torch import time try: if not torch.has_mps: return None, None, \\"No MPS device available.\\" # Set manual seed torch.manual_seed(seed_value) # Get random number generator state rng_state = torch.get_rng_state() # Measure allocated memory allocated_memory = torch.cuda.memory_allocated() # Start profiling torch.cuda.profiler.start() time.sleep(profile_duration) # Simulate work torch.cuda.synchronize() # Ensure all ops are done torch.cuda.profiler.stop() profile_report = \\"Profiling successful.\\" return rng_state, allocated_memory, profile_report except Exception as e: return str(e), None, \\"Profiling failed.\\" # Example Usage: # result = mps_random_number_gen(42, 1.0) # print(result)"},{"question":"# Handling Duplicate Labels in a DataFrame You are tasked with managing a DataFrame of sales data where there might be duplicate entries for product names. This DataFrame needs to be cleaned to ensure no duplicate product names remain. Additionally, the cleaned DataFrame should disallow the introduction of any new duplicate product names. Write a function `clean_sales_data` that performs the following operations: 1. Accepts a DataFrame `df` with at least one column named `\\"Product\\"` and other columns representing sales data. 2. Detects any duplicate product names in the `\\"Product\\"` column. 3. Removes duplicate entries in the `\\"Product\\"` column by keeping only the first occurrence of each product. 4. Ensures that no further duplicates can be introduced. The function should return the cleaned DataFrame and a boolean indicating whether duplicates were detected and removed. Function Signature: ```python import pandas as pd def clean_sales_data(df: pd.DataFrame) -> (pd.DataFrame, bool): pass ``` Expected Input and Output: - **Input:** A DataFrame `df` with at least one column named `\\"Product\\"`. - **Output:** A tuple containing: - A cleaned DataFrame with duplicates removed and disallowed. - A boolean indicating if duplicates were detected and removed (`True` if duplicates were found and removed, otherwise `False`). Constraints: - The function should handle DataFrames of varying sizes efficiently. - The resulting DataFrame should not allow duplicate `\\"Product\\"` labels after cleaning. Example: ```python data = { \'Product\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Sales_Q1\': [10, 20, 30, 40, 50, 60], \'Sales_Q2\': [15, 25, 35, 45, 55, 65] } df = pd.DataFrame(data) cleaned_df, found_duplicates = clean_sales_data(df) print(cleaned_df) print(found_duplicates) ``` **Expected Output:** ``` Product Sales_Q1 Sales_Q2 0 A 10 15 1 B 20 25 2 C 30 35 False ``` In this example, duplicates are removed, and the DataFrame is set to disallow further duplicates.","solution":"import pandas as pd def clean_sales_data(df: pd.DataFrame) -> (pd.DataFrame, bool): Cleans the DataFrame by removing duplicate product names and ensuring no further duplicates can be introduced. Parameters: df (pd.DataFrame): The input DataFrame with a \'Product\' column. Returns: Tuple[pd.DataFrame, bool]: A tuple containing the cleaned DataFrame and a boolean indicating if duplicates were found and removed. initial_count = df.shape[0] df_unique = df.drop_duplicates(subset=[\\"Product\\"], keep=\'first\') final_count = df_unique.shape[0] duplicates_found = final_count < initial_count # Ensure the result is unique df_unique.drop_duplicates(subset=[\\"Product\\"], keep=\'first\', inplace=True) return df_unique, duplicates_found"},{"question":"# Question: Implementing and Operating on MaskedTensor Problem Statement: You are tasked with implementing a function that creates a `MaskedTensor` and performs a series of operations on it. The function should be named `process_masked_tensor` and it should accept two parameters: a data tensor and a mask tensor. The function should return a result after performing specific operations on the `MaskedTensor`. Requirements: 1. **Input**: - `data` (torch.Tensor): A 2D tensor containing the input data. - `mask` (torch.Tensor): A 2D boolean tensor of the same shape as `data` indicating which values to include (True) or ignore (False). 2. **Output**: - The function should return a single value, which is the mean of all specified (unmasked) elements after applying a series of transformations. 3. **Steps**: - Create a `MaskedTensor` from the given data and mask tensors. - Apply the `sqrt` (square root) unary operation to the `MaskedTensor`. - Perform a reduction to calculate the `sum` of the specified (unmasked) elements. - Compute the `mean` of all specified (unmasked) elements using the sum and the number of specified elements. 4. **Constraints**: - Ensure that the input tensors `data` and `mask` have the same shape. - Handle any potential errors gracefully by raising an appropriate exception. 5. **Performance**: - Ensure the function efficiently handles tensors up to size (1000, 1000). Implementation: ```python import torch from torch.masked import masked_tensor def process_masked_tensor(data: torch.Tensor, mask: torch.Tensor) -> float: # Validate input shapes if data.shape != mask.shape: raise ValueError(\\"Data and mask tensors must have the same shape.\\") # Create MaskedTensor mt = masked_tensor(data, mask) # Apply sqrt unary operation sqrt_mt = torch.sqrt(mt) # Perform sum reduction on specified elements sum_value = torch.sum(sqrt_mt) # Calculate the mean of specified elements specified_count = torch.sum(mask).item() if specified_count == 0: raise ValueError(\\"No specified (unmasked) elements to compute mean.\\") mean_value = sum_value.item() / specified_count return mean_value # Example Usage: # data_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) # mask_tensor = torch.tensor([[True, False], [True, True]]) # print(process_masked_tensor(data_tensor, mask_tensor)) ``` Explanation: 1. The function ensures `data` and `mask` have the same shape. 2. It creates a `MaskedTensor` from the input `data` and `mask`. 3. It applies the square root operation to the `MaskedTensor`. 4. It calculates the sum of the specified elements in the transformed tensor. 5. Finally, it computes and returns the mean of the specified elements. Test the function with different input tensors to verify its correctness and robustness.","solution":"import torch def process_masked_tensor(data: torch.Tensor, mask: torch.Tensor) -> float: Process a masked tensor by computing the mean of the square root of the specified (unmasked) elements. Parameters: data (torch.Tensor): A 2D tensor containing the input data. mask (torch.Tensor): A 2D boolean tensor indicating which values to include (True) or ignore (False). Returns: float: The mean of the square root of the specified elements. Raises: ValueError: If data and mask tensors have different shapes, or if there are no specified elements. # Validate input shapes if data.shape != mask.shape: raise ValueError(\\"Data and mask tensors must have the same shape.\\") # Apply the mask masked_data = data[mask] # Apply sqrt unary operation sqrt_masked_data = torch.sqrt(masked_data) # Perform sum reduction on specified elements sum_value = torch.sum(sqrt_masked_data) # Calculate the mean of specified elements specified_count = masked_data.numel() if specified_count == 0: raise ValueError(\\"No specified (unmasked) elements to compute mean.\\") mean_value = sum_value.item() / specified_count return mean_value"},{"question":"<|Analysis Begin|> The provided documentation is for the `urllib.request` module in Python\'s standard library. This module allows for opening and retrieving URLs. It provides various functions and classes, such as: - `urlopen()`: For opening URLs. - `Request()`: For creating request objects. - `install_opener()`: For installing a global opener. - `build_opener()`: For constructing an opener object. - `getproxies()`: For retrieving proxy settings. - Various handlers (`ProxyHandler`, `HTTPHandler`, `HTTPSHandler`, `FTPHandler`, etc.) for handling different protocols. The documentation also includes examples for using these functions and classes, showing how to handle URLs, manage authentication, customizing headers, using proxies, and dealing with different request methods. The goal of a coding assessment question should be to probe students\' understanding of making HTTP requests, handling responses, managing headers, and possibly dealing with authentication and proxies. <|Analysis End|> <|Question Begin|> **Question: Implement a Custom URL Opener with Authentication and Headers Handling** You are required to create a custom URL opener that can handle HTTP Basic Authentication, use specific HTTP headers for requests, and manage HTTP responses appropriately. # Function Specification **Function Name:** `custom_url_opener` **Parameters:** 1. `url` (str): The URL to open. 2. `headers` (dict): A dictionary of custom headers to include in the request. 3. `username` (str): The username for HTTP Basic Authentication (default is `None`). 4. `password` (str): The password for HTTP Basic Authentication (default is `None`). **Returns:** - `response_text` (str): The body of the HTTP response as a string. # Constraints - If `username` and `password` are provided, HTTP Basic Authentication must be used. - All custom headers in the `headers` dictionary must be included in the request. - The function should handle both HTTP and HTTPS URLs. - Use a request timeout of 10 seconds. - The function should raise an appropriate error if the URL cannot be opened or if an HTTP error occurs. # Example Usage ```python url = \\"http://www.example.com\\" headers = { \\"User-Agent\\": \\"custom-agent/1.0\\", \\"Accept\\": \\"application/json\\" } username = \\"user\\" password = \\"pass\\" try: response = custom_url_opener(url, headers, username, password) print(response) except Exception as e: print(f\\"Error: {e}\\") ``` # Implementation Notes - You may use `urllib.request` and appropriate handlers for handling HTTP Basic Authentication. - Ensure error handling for unsuccessful HTTP responses. - Be mindful of the default headers like `Content-Type` and `Host` that are automatically included. # Starter Code ```python import urllib.request def custom_url_opener(url, headers, username=None, password=None): # Implement your custom URL opener here pass ``` Your implementation should be able to handle different types of HTTP responses and properly manage HTTP Basic Authentication when credentials are provided.","solution":"import urllib.request from urllib.error import HTTPError, URLError import base64 def custom_url_opener(url, headers, username=None, password=None): # Create a Request object req = urllib.request.Request(url) # Add any provided headers to the request for key, value in headers.items(): req.add_header(key, value) # If username and password are provided, add Basic Authentication header if username and password: auth_str = f\\"{username}:{password}\\" encoded_bytes = base64.b64encode(auth_str.encode(\\"ascii\\")) encoded_str = encoded_bytes.decode(\\"ascii\\") req.add_header(\\"Authorization\\", f\\"Basic {encoded_str}\\") try: # Open the URL and read the response with urllib.request.urlopen(req, timeout=10) as response: response_text = response.read().decode(\'utf-8\') return response_text except HTTPError as e: raise Exception(f\\"HTTP error occurred: {e.reason}\\") from e except URLError as e: raise Exception(f\\"Failed to reach server: {e.reason}\\") from e except Exception as e: raise Exception(f\\"An error occurred: {str(e)}\\") from e"},{"question":"Title: Implementing and Testing PyTorch Module Conversion Settings **Problem Statement:** You are provided with a simple PyTorch model. Your task is to write a function that demonstrates the use of the functions within the `torch.__future__` module for setting and getting module conversion parameters. Specifically, you should: 1. Define a simple PyTorch model (e.g., a neural network with a few layers). 2. Implement a function `configure_and_convert_module` that: - Takes a PyTorch model as input. - Sets conversion parameters using the `set_overwrite_module_params_on_conversion` and `set_swap_module_params_on_conversion` functions. - Gets and prints the current settings using `get_overwrite_module_params_on_conversion` and `get_swap_module_params_on_conversion`. 3. Display how these settings can be applied in a mock conversion process. **Function Signature:** ```python import torch import torch.nn as nn import torch.__future__ as future def define_model() -> nn.Module: Returns: model (nn.Module): A simple PyTorch neural network model. pass def configure_and_convert_module(model: nn.Module): Configures and prints conversion parameters, simulates module conversion. Args: model (nn.Module): A PyTorch model to be configured and \'converted\'. pass # Example usage: # model = define_model() # configure_and_convert_module(model) ``` **Expected Output:** 1. The function `define_model` should return a simple neural network model with at least one `nn.Linear` layer. 2. The function `configure_and_convert_module` should: - Set `overwrite_module_params_on_conversion` to `True`. - Set `swap_module_params_on_conversion` to `True`. - Print the conversion settings using the `get_*` functions. - Simulate a conversion by printing a message indicating that the model has been \\"converted\\" with the specified configurations. Note: The actual conversion process can be mocked for the purpose of this assessment. **Constraints and Assumptions:** - Assume that the module conversion does not need to produce actual conversions; just the settings and simulated steps are sufficient. - Focus on demonstrating understanding of setting and retrieving the configurations using `torch.__future__` functions. **Performance Requirements:** - The implementation should handle the setting and getting of configurations efficiently. - The function should clearly print out the current settings to verify that the configuration changes have been applied correctly.","solution":"import torch import torch.nn as nn import torch.__future__ as future def define_model() -> nn.Module: Returns: model (nn.Module): A simple PyTorch neural network model. model = nn.Sequential( nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2) ) return model def configure_and_convert_module(model: nn.Module): Configures and prints conversion parameters, simulates module conversion. Args: model (nn.Module): A PyTorch model to be configured and \'converted\'. # Set conversion parameters future.set_overwrite_module_params_on_conversion(True) future.set_swap_module_params_on_conversion(True) # Get and print the current settings overwrite = future.get_overwrite_module_params_on_conversion() swap = future.get_swap_module_params_on_conversion() print(f\\"Overwrite Module Params on Conversion: {overwrite}\\") print(f\\"Swap Module Params on Conversion: {swap}\\") # Simulate a conversion process print(\\"Simulating module conversion with the specified settings.\\") # Example usage: # model = define_model() # configure_and_convert_module(model)"},{"question":"**Boolean Manipulation in Python** Given the unique implementation of booleans as a subclass of integers in Python, your task is to implement a function that works with boolean values in a variety of ways as described in the provided documentation. # Task Implement a Python function `boolean_operations(operation, value)` which performs boolean operations based on the specified `operation`. The `operation` parameter is a string that specifies the type of operation to perform, and the `value` parameter is an integer whose boolean equivalent will be used in the operation. # Operations 1. **\\"check_boolean\\":** Check if the given `value` is a boolean. Return `True` if `value` is either `True` or `False`; otherwise, return `False`. 2. **\\"toggle_boolean\\":** Return the opposite boolean value of the given `value`. (Hint: `True` becomes `False` and `False` becomes `True`). 3. **\\"return_boolean\\":** Return the boolean equivalent of the given `value` (using Pythonâ€™s inherent truthiness). # Input - `operation`: A string specifying the operation to perform. Possible values are \\"check_boolean\\", \\"toggle_boolean\\", \\"return_boolean\\". - `value`: An integer to be converted or checked for its boolean value. # Output - A boolean value as described by the operation. # Constraints - Input `operation` will always be one of the defined strings. - Input `value` can be any integer. - You should optimize for clarity and simplicity over raw performance. # Example ```python # Example usage of boolean_operations function result = boolean_operations(\\"check_boolean\\", 1) print(result) # Output: False (since 1 is not strictly a boolean, it\'s an integer) result = boolean_operations(\\"toggle_boolean\\", 1) print(result) # Output: False (since 1 evaluates to True, and toggling True gives False) result = boolean_operations(\\"toggle_boolean\\", 0) print(result) # Output: True (since 0 evaluates to False, and toggling False gives True) result = boolean_operations(\\"return_boolean\\", 2) print(result) # Output: True (since any non-zero number is considered True in Python) ``` Write a Python function `boolean_operations` that meets the requirements described above.","solution":"def boolean_operations(operation, value): Performs specified boolean operations on the given value. Args: operation (str): The operation to perform. Possible values are \\"check_boolean\\", \\"toggle_boolean\\", \\"return_boolean\\". value (int): The integer value to be evaluated or operated on. Returns: bool: The result of the boolean operation. if operation == \\"check_boolean\\": return value is True or value is False elif operation == \\"toggle_boolean\\": return not bool(value) elif operation == \\"return_boolean\\": return bool(value) else: raise ValueError(\\"Invalid operation specified\\")"},{"question":"# Seaborn Coding Assessment Objective: You are provided with two datasets, `penguins` and `diamonds`, and you are required to use the `seaborn.objects` module to create specific visualizations. This exercise will demonstrate your understanding of seabornâ€™s advanced visualization capabilities. Task: 1. Load the `penguins` and `diamonds` datasets using seaborn\'s `load_dataset` function. 2. Create a plot using the `penguins` dataset that visualizes the relationship between `species` and `flipper_length_mm`: - Plot dots for each data point, jittered for clarity. - Add a visual summary of the 25th and 75th percentiles of `flipper_length_mm` for each `species`. - Adjust the positions of the percentile ranges slightly to avoid overlap with the dot plots. 3. Create a plot using the `diamonds` dataset that visualizes the relationship between `color` and `depth`: - Plot dots for each data point, jittered for clarity. - Add a visual summary of the 10th and 90th percentiles of `depth` for each `color`. - Adjust the positions of the percentile ranges slightly to avoid overlap with the dot plots. Requirements: - Use the seaborn `objects` interface as demonstrated. - Ensure plots are clear and labels are properly set. - Comment your code to explain your approach and the functions used. Expected Output: The expected output will be two plots: 1. A jittered dot plot and percentile summary for the `penguins` dataset. 2. A jittered dot plot and percentile summary for the `diamonds` dataset. Example Code: The following is a template to get you started: ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Plot 1: Penguins ( so.Plot(penguins, x=\\"species\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) .label(title=\\"Penguins: Flipper Length by Species\\", xlabel=\\"Species\\", ylabel=\\"Flipper Length (mm)\\") ) # Plot 2: Diamonds ( so.Plot(diamonds, x=\\"color\\", y=\\"depth\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([10, 90]), so.Shift(y=0.25)) .label(title=\\"Diamonds: Depth by Color\\", xlabel=\\"Color\\", ylabel=\\"Depth\\") ) ``` Constraints: - Assume that both datasets are available and correctly formatted. - Focus on clarity and accuracy of the visualizations. Performance: - Ensure that the plots are generated efficiently without unnecessary computation.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Plot 1: Penguins plot_penguins = ( so.Plot(penguins, x=\\"species\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) .label(title=\\"Penguins: Flipper Length by Species\\", xlabel=\\"Species\\", ylabel=\\"Flipper Length (mm)\\") ) plot_penguins.show() # Plot 2: Diamonds plot_diamonds = ( so.Plot(diamonds, x=\\"color\\", y=\\"depth\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([10, 90]), so.Shift(y=0.25)) .label(title=\\"Diamonds: Depth by Color\\", xlabel=\\"Color\\", ylabel=\\"Depth\\") ) plot_diamonds.show()"},{"question":"**Question: Advanced Resource Management with Python Development Mode** You are required to write a Python script that processes multiple text files concurrently using asyncio. The script should: 1. Read the list of text file paths from a given input file. 2. For each text file, count the number of lines and print the file name along with the line count. 3. Ensure that all file resources are properly managed and closed. 4. Utilize Python Development Mode features to debug and verify that no resources are left unclosed or mismanaged. # Constraints: - The input file, `input.txt`, contains the paths of text files, one per line. - Each text file has no more than 10,000 lines. # Expectations: 1. Implement a function, `count_lines_in_file(file_path)`, that counts and returns the number of lines in the given file. 2. Implement an async function, `process_files(file_list)`, that concurrently processes each file using the `count_lines_in_file` function and prints the results. 3. Properly manage resources using context managers or appropriate cleanup handlers. 4. Utilize Development Mode to ensure no unclosed resources and address any warnings or errors. # Input Format: - `input.txt`: A text file where each line contains the path to a text file to process. # Output Format: - Prints the file path and the respective line count for each file. # Example: Assuming `input.txt` contains: ``` file1.txt file2.txt file3.txt ``` And `file1.txt` has 100 lines, `file2.txt` has 200 lines, and `file3.txt` has 300 lines, the output should be: ``` file1.txt: 100 file2.txt: 200 file3.txt: 300 ``` # Implementation: Implement the solution as described, and run it with Development Mode enabled. Describe any debugging steps or issues encountered and how you resolved them. # Hints: - Use `asyncio` to manage concurrent processing. - Use context managers (`with` statement) to ensure files are properly closed. - Run the script with Python\'s Development Mode enabled (`python3 -X dev script.py`) and address any warnings or errors.","solution":"import asyncio async def count_lines_in_file(file_path): Counts and returns the number of lines in the given file. lines_count = 0 try: async with aiofiles.open(file_path, \'r\') as file: async for _ in file: lines_count += 1 except FileNotFoundError: print(f\\"File not found: {file_path}\\") return lines_count async def process_files(file_list): Concurrently processes each file in the file_list, counting and printing the number of lines. tasks = [count_lines_in_file(file_path) for file_path in file_list] results = await asyncio.gather(*tasks) for file_path, lines_count in zip(file_list, results): print(f\\"{file_path}: {lines_count}\\") def read_input_file(input_file): Reads the file paths from the input file and returns them as a list. with open(input_file, \'r\') as file: file_list = [line.strip() for line in file if line.strip()] return file_list def main(): input_file = \\"input.txt\\" file_list = read_input_file(input_file) if file_list: asyncio.run(process_files(file_list)) else: print(\\"No files to process.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question**: You are tasked to demonstrate your understanding of synthetic dataset generation using scikit-learn and to apply these datasets in machine learning tasks. Implement a function `create_and_evaluate_datasets` that performs the following steps: 1. **Generate Datasets**: - Create a blob dataset with 4 clusters and standard deviation of 1.0 for the clusters. - Create a classification dataset with 2 informative features, 2 redundant features, and 2 clusters per class. - Create a Gaussian quantiles dataset with 3 classes. - Create a circles dataset with noise of 0.2. - Create a moons dataset with noise of 0.1. 2. **Preprocess Datasets**: - Standardize each dataset (zero mean and unit variance). 3. **Train and Evaluate Models**: - For each dataset, train a simple Logistic Regression model from scikit-learn. - Evaluate the model using 5-fold cross-validation and return the mean accuracy for each dataset. 4. **Input/Output Specifications**: - Your function should not take any input parameters. - Your function should return a dictionary with dataset names as keys and corresponding mean cross-validation accuracies as values. Here is the function signature: ```python from typing import Dict from sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles, make_circles, make_moons from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score def create_and_evaluate_datasets() -> Dict[str, float]: pass ``` **Constraints**: - Use `random_state=1` for all dataset generators to ensure reproducibility. - Standardize datasets using `StandardScaler` from scikit-learn after generation. **Example**: ```python results = create_and_evaluate_datasets() print(results) # Example Output (actual values will vary based on model performance): # { # \'blobs\': 0.85, # \'classification\': 0.80, # \'gaussian_quantiles\': 0.78, # \'circles\': 0.70, # \'moons\': 0.75 # } ``` Your implementation should demonstrate: - Mastery of scikit-learn\'s dataset generation capabilities. - Proficiency in data preprocessing. - Ability to train and evaluate models using cross-validation.","solution":"from typing import Dict from sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles, make_circles, make_moons from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score import numpy as np def create_and_evaluate_datasets() -> Dict[str, float]: random_state = 1 # Generate Datasets X_blobs, y_blobs = make_blobs(n_samples=1000, centers=4, cluster_std=1.0, random_state=random_state) X_classification, y_classification = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=2, n_clusters_per_class=2, random_state=random_state) X_gaussian, y_gaussian = make_gaussian_quantiles(n_samples=1000, n_features=2, n_classes=3, random_state=random_state) X_circles, y_circles = make_circles(n_samples=1000, noise=0.2, random_state=random_state) X_moons, y_moons = make_moons(n_samples=1000, noise=0.1, random_state=random_state) datasets = { \'blobs\': (X_blobs, y_blobs), \'classification\': (X_classification, y_classification), \'gaussian_quantiles\': (X_gaussian, y_gaussian), \'circles\': (X_circles, y_circles), \'moons\': (X_moons, y_moons) } # Preprocess Datasets scaler = StandardScaler() for name in datasets: X, y = datasets[name] X = scaler.fit_transform(X) datasets[name] = (X, y) results = {} # Train and Evaluate Models model = LogisticRegression(max_iter=200) for name, (X, y) in datasets.items(): scores = cross_val_score(model, X, y, cv=5) results[name] = np.mean(scores) return results"},{"question":"# Unicode Data Processing Using `unicodedata` Module You are given a string containing a mix of characters, including letters, digits, and special symbols from various languages. Your task is to implement a function `unicode_data_report` that takes this input string and generates a detailed report of its Unicode properties. The report should consist of the following details for each unique character in the string: 1. **Character**: The character itself. 2. **Name**: The official Unicode name of the character. 3. **Category**: The general category of the character. 4. **Decimal Value** (if applicable): The decimal value of the character. 5. **Digit Value** (if applicable): The digit value of the character. 6. **Numeric Value** (if applicable): The numeric value of the character. 7. **Bidirectional Class**: The bidirectional class of the character. 8. **Combining Class**: The canonical combining class of the character. 9. **East Asian Width**: The East Asian width property of the character. 10. **Mirrored**: Whether the character is a mirrored character in bidirectional text. 11. **Decomposition**: The character decomposition mapping. If a property is not applicable to a character, it should be represented as `None` in the report. The report should be returned as a list of dictionaries, with each dictionary representing the properties of a unique character in the input string. Input - `text` (str): A string containing a mix of Unicode characters. Output - A list of dictionaries, where each dictionary contains the Unicode properties of a unique character in the input string. Constraints - The input string can contain any Unicode character. Example ```python import unicodedata def unicode_data_report(text): report = [] seen = set() for char in text: if char not in seen: seen.add(char) char_report = { \\"Character\\": char, \\"Name\\": unicodedata.name(char, None), \\"Category\\": unicodedata.category(char), \\"Decimal Value\\": unicodedata.decimal(char, None), \\"Digit Value\\": unicodedata.digit(char, None), \\"Numeric Value\\": unicodedata.numeric(char, None), \\"Bidirectional Class\\": unicodedata.bidirectional(char), \\"Combining Class\\": unicodedata.combining(char), \\"East Asian Width\\": unicodedata.east_asian_width(char), \\"Mirrored\\": unicodedata.mirrored(char), \\"Decomposition\\": unicodedata.decomposition(char) or None } report.append(char_report) return report # Example: text = \\"A1ã‚\\" print(unicode_data_report(text)) ``` Expected Output: ```python [ { \\"Character\\": \\"A\\", \\"Name\\": \\"LATIN CAPITAL LETTER A\\", \\"Category\\": \\"Lu\\", \\"Decimal Value\\": None, \\"Digit Value\\": None, \\"Numeric Value\\": None, \\"Bidirectional Class\\": \\"L\\", \\"Combining Class\\": 0, \\"East Asian Width\\": \\"Na\\", \\"Mirrored\\": 0, \\"Decomposition\\": None }, { \\"Character\\": \\"1\\", \\"Name\\": \\"DIGIT ONE\\", \\"Category\\": \\"Nd\\", \\"Decimal Value\\": 1, \\"Digit Value\\": 1, \\"Numeric Value\\": 1.0, \\"Bidirectional Class\\": \\"EN\\", \\"Combining Class\\": 0, \\"East Asian Width\\": \\"Na\\", \\"Mirrored\\": 0, \\"Decomposition\\": None }, { \\"Character\\": \\"ã‚\\", \\"Name\\": \\"HIRAGANA LETTER A\\", \\"Category\\": \\"Lo\\", \\"Decimal Value\\": None, \\"Digit Value\\": None, \\"Numeric Value\\": None, \\"Bidirectional Class\\": \\"L\\", \\"Combining Class\\": 0, \\"East Asian Width\\": \\"W\\", \\"Mirrored\\": 0, \\"Decomposition\\": None } ] ``` Good luck! Your solution should demonstrate a solid understanding of manipulating and querying Unicode character properties using the `unicodedata` module.","solution":"import unicodedata def unicode_data_report(text): report = [] seen = set() for char in text: if char not in seen: seen.add(char) char_report = { \\"Character\\": char, \\"Name\\": unicodedata.name(char, None), \\"Category\\": unicodedata.category(char), \\"Decimal Value\\": unicodedata.decimal(char, None), \\"Digit Value\\": unicodedata.digit(char, None), \\"Numeric Value\\": unicodedata.numeric(char, None), \\"Bidirectional Class\\": unicodedata.bidirectional(char), \\"Combining Class\\": unicodedata.combining(char), \\"East Asian Width\\": unicodedata.east_asian_width(char), \\"Mirrored\\": unicodedata.mirrored(char), \\"Decomposition\\": unicodedata.decomposition(char) or None } report.append(char_report) return report"},{"question":"**Coding Assessment Question: Creating and Customizing Relational Plots with Seaborn** # Objective Write a Python function that generates a relational plot using the seaborn `relplot` function. This function should demonstrate your ability to create and customize scatter plots, handle faceting, and manage multiple semantic mappings. # Requirements 1. **Load Data:** Load the `tips` dataset from seaborn. 2. **Function Implementation**: Implement a function `create_custom_relplot` that accepts the following parameters: - `x`: The column name to be plotted on the x-axis. - `y`: The column name to be plotted on the y-axis. - `hue`: The column name for color encoding. - `col`: The column name for facet columns. - `row`: The column name for facet rows. - `kind`: The type of plot to create, either \\"scatter\\" or \\"line\\". - `height`: Height of each facet (default is 5). - `aspect`: Aspect ratio of each facet (default is 1). - `add_line`: A boolean indicating if a horizontal line should be added to each plot (default is False). - `line_y`: The y-coordinate of the horizontal line (default is 0 if `add_line` is True). - `title`: A string template for setting titles of each facet, using the placeholder `{col_name}` to include the value of the `col` variable. 3. **Function Output:** The function should return a seaborn `FacetGrid` object. 4. **Customization:** If `add_line` is True, add a horizontal line to each subplot at the y-coordinate specified by `line_y`. Set the axis labels to \\"X-axis: {x}\\" and \\"Y-axis: {y}\\" and use the `title` parameter to set titles for each subplot. # Example Usage ```python def create_custom_relplot(x, y, hue, col, row, kind=\\"scatter\\", height=5, aspect=1, add_line=False, line_y=0, title=\\"Facet: {col_name}\\"): import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the relplot with specified parameters g = sns.relplot( data=tips, x=x, y=y, hue=hue, col=col, row=row, kind=kind, height=height, aspect=aspect ) # Customization: Add horizontal line if specified if add_line: g = g.map(plt.axhline, y=line_y, color=\\".7\\", dashes=(2, 1), zorder=0) # Set axis labels g.set_axis_labels(f\\"X-axis: {x}\\", f\\"Y-axis: {y}\\") # Set titles g.set_titles(title) return g # Expected usage of function grid = create_custom_relplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", col=\\"time\\", row=\\"sex\\", kind=\\"scatter\\", add_line=True, line_y=2, title=\\"Time of Day: {col_name}\\") plt.show() ``` # Constraints - Ensure the function is efficient and leverages seaborn\'s internal optimizations. - The function should handle varying dataset sizes gracefully without significant performance degradation. This question is designed to test your understanding of seaborn\'s advanced plotting capabilities, customization options, and ability to create complex multi-faceted plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_relplot(x, y, hue, col, row, kind=\\"scatter\\", height=5, aspect=1, add_line=False, line_y=0, title=\\"Facet: {col_name}\\"): Create a seaborn relplot with specified parameters and customizations. Parameters: - x: The column name to be plotted on the x-axis. - y: The column name to be plotted on the y-axis. - hue: The column name for color encoding. - col: The column name for facet columns. - row: The column name for facet rows. - kind: The type of plot to create, either \\"scatter\\" or \\"line\\". - height: Height of each facet. - aspect: Aspect ratio of each facet. - add_line: A boolean indicating if a horizontal line should be added to each plot. - line_y: The y-coordinate of the horizontal line. - title: A string template for setting titles of each facet, using the placeholder `{col_name}`. Returns: - g: A seaborn FacetGrid object. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the relplot with specified parameters g = sns.relplot( data=tips, x=x, y=y, hue=hue, col=col, row=row, kind=kind, height=height, aspect=aspect ) # Customization: Add horizontal line if specified if add_line: g = g.map(plt.axhline, y=line_y, color=\\".7\\", dashes=(2, 1), zorder=0) # Set axis labels g.set_axis_labels(f\\"X-axis: {x}\\", f\\"Y-axis: {y}\\") # Set titles g.set_titles(title) return g"},{"question":"# Serialization and Deserialization with Custom Constraints In this exercise, you are required to implement custom serialization and deserialization functionality in Python, using the provided format. You will need to marshal (serialize) both primitive types and Python objects, ensuring they can later be accurately unmarshaled (deserialized). Part 1: Serialization Implement the function `serialize_data(data, version=2)`, which takes a Python object and returns its serialized form. The function should support the following data types: - `int`: Serialize as a 32-bit long (use the least-significant 32 bits). - `float`: Serialize using a binary format for efficiency. - `str`: Serialize as bytes with length prefix. - `list`: Serialize each element in the list individually, followed by an end-of-list marker. - `dict`: Serialize each key-value pair individually, followed by an end-of-dictionary marker. The `version` parameter should determine the serialization protocol version used. Part 2: Deserialization Implement the function `deserialize_data(serialized_data)`, which takes a byte string containing serialized data and returns the original Python object. Input and Output Formats **serialize_data(data, version=2)**: - **Input**: A Python object `data` of types `int`, `float`, `str`, `list`, or `dict`, and an optional integer `version`. - **Output**: A serialized byte string representation of the input object. **deserialize_data(serialized_data)**: - **Input**: A byte string `serialized_data` containing serialized data. - **Output**: The original Python object reconstructed from the byte string. Constraints 1. Only 32-bit integers and 64-bit floating-point numbers should be considered. 2. Strings should be encoded as UTF-8 with a length prefix. 3. Lists and dictionaries should end with specific markers for serialization (`0x00` for lists and `0x01` for dictionaries). 4. Handle errors gracefully and raise appropriate exceptions (`EOFError`, `ValueError`, or `TypeError`) when encountering unexpected data or format issues. Example ```python # Example serialization data = { \'number\': 42, \'text\': \'hello\', \'values\': [1.23, 4.56, 7.89] } serialized = serialize_data(data) print(serialized) # Example deserialization deserialized = deserialize_data(serialized) print(deserialized) # This should print: # {\'number\': 42, \'text\': \'hello\', \'values\': [1.23, 4.56, 7.89]} ``` Note: You are not required to implement the low-level marshaling functions directly in Python but should simulate their behavior in your implementation.","solution":"import struct def _serialize_int(value): return struct.pack(\'<I\', value & 0xFFFFFFFF) def _serialize_float(value): return struct.pack(\'<d\', value) def _serialize_str(value): encoded_value = value.encode(\'utf-8\') return struct.pack(\'<I\', len(encoded_value)) + encoded_value def _serialize_list(value, version): serialized_elements = b\'\'.join(serialize_data(element, version) for element in value) return serialized_elements + struct.pack(\'<B\', 0x00) def _serialize_dict(value, version): serialized_items = b\'\'.join( serialize_data(k, version) + serialize_data(v, version) for k, v in value.items() ) return serialized_items + struct.pack(\'<B\', 0x01) def serialize_data(data, version=2): if isinstance(data, int): return b\'I\' + _serialize_int(data) elif isinstance(data, float): return b\'F\' + _serialize_float(data) elif isinstance(data, str): return b\'S\' + _serialize_str(data) elif isinstance(data, list): return b\'L\' + _serialize_list(data, version) elif isinstance(data, dict): return b\'D\' + _serialize_dict(data, version) else: raise TypeError(\'Unsupported data type for serialization\') def _deserialize_int(data, pos): value = struct.unpack(\'<I\', data[pos:pos+4])[0] return value, pos + 4 def _deserialize_float(data, pos): value = struct.unpack(\'<d\', data[pos:pos+8])[0] return value, pos + 8 def _deserialize_str(data, pos): length = struct.unpack(\'<I\', data[pos:pos+4])[0] pos += 4 value = data[pos:pos+length].decode(\'utf-8\') return value, pos + length def _deserialize_list(data, pos, version): elements, pos = [], pos while data[pos] != 0x00: element, pos = deserialize_data(data, pos) elements.append(element) return elements, pos + 1 def _deserialize_dict(data, pos, version): items, pos = {}, pos while data[pos] != 0x01: key, pos = deserialize_data(data, pos) value, pos = deserialize_data(data, pos) items[key] = value return items, pos + 1 def deserialize_data(data, pos=0): type_indicator = data[pos] pos += 1 if type_indicator == ord(\'I\'): return _deserialize_int(data, pos) elif type_indicator == ord(\'F\'): return _deserialize_float(data, pos) elif type_indicator == ord(\'S\'): return _deserialize_str(data, pos) elif type_indicator == ord(\'L\'): return _deserialize_list(data, pos, 2) elif type_indicator == ord(\'D\'): return _deserialize_dict(data, pos, 2) else: raise ValueError(\'Unsupported data type for deserialization\')"},{"question":"# JSON Encoder/Decoder and Exception Handling Pythonâ€™s `json` module provides functionalities to encode Python objects into JSON format and decode JSON data back into Python objects. This module also includes mechanisms to handle various exceptions that may arise during these operations. Task You are required to implement two functions: `encode_to_json` and `decode_from_json`. 1. `encode_to_json`: This function should take a Python object as input and return its JSON string representation. It should handle encoding exceptions and return an appropriate error message. 2. `decode_from_json`: This function should take a JSON string as input and return the corresponding Python object. It should handle decoding exceptions and return an appropriate error message. Function Specifications 1. **encode_to_json(obj: Any) -> str:** - **Input:** A Python object (`obj`). - **Output:** A JSON string representation of the Python object. - **Constraints:** - In case of an encoding error, it should return the string `\\"EncodingError: [error_message]\\"` where `[error_message]` is the exception message. 2. **decode_from_json(json_str: str) -> Any:** - **Input:** A JSON string (`json_str`). - **Output:** The corresponding Python object. - **Constraints:** - In case of a decoding error, it should return the string `\\"DecodingError: [error_message]\\"` where `[error_message]` is the exception message. Examples ```python # Example 1: # Encoding a Python object to JSON input_obj = {\\"name\\": \\"Alice\\", \\"age\\": 30} output_str = encode_to_json(input_obj) # Expected output: \'{\\"name\\": \\"Alice\\", \\"age\\": 30}\' # Example 2: # Encoding error example class Unserializable: pass input_obj = Unserializable() output_str = encode_to_json(input_obj) # Expected output: \'EncodingError: Object of type Unserializable is not JSON serializable\' # Example 3: # Decoding a JSON string to Python object input_str = \'{\\"name\\": \\"Alice\\", \\"age\\": 30}\' output_obj = decode_from_json(input_str) # Expected output: {\'name\': \'Alice\', \'age\': 30} # Example 4: # Decoding error example input_str = \'{\\"name\\": \\"Alice\\", \\"age\\": 30\' # Missing closing brace output_obj = decode_from_json(input_str) # Expected output: \'DecodingError: Expecting \',\' delimiter: line 1 column 29 (char 28)\' ``` Notes - You can make use of Pythonâ€™s built-in `json` module. - Ensure that your solutions handle various types of exceptions that might be raised during the encoding and decoding processes. Performance Requirements - Solutions should aim for efficient handling of inputs, though there are no specific time complexity constraints for this task.","solution":"import json def encode_to_json(obj): Encode a Python object to a JSON string. Parameters: - obj (Any): The Python object to encode. Returns: - str: The JSON string representation of the input object. If encoding fails, returns an error message in the format \\"EncodingError: [error_message]\\". try: return json.dumps(obj) except TypeError as e: return f\\"EncodingError: {str(e)}\\" def decode_from_json(json_str): Decode a JSON string to a Python object. Parameters: - json_str (str): The JSON string to decode. Returns: - Any: The corresponding Python object. If decoding fails, returns an error message in the format \\"DecodingError: [error_message]\\". try: return json.loads(json_str) except json.JSONDecodeError as e: return f\\"DecodingError: {str(e)}\\""},{"question":"Objective: Implement and demonstrate the usage of context-local state management using the `contextvars` module in Python 3.10 by creating a contextual counter system. This system should be able to handle multiple independent counters in an asynchronous environment, ensuring that the counters remain isolated in their respective contexts. Task: 1. **Create a `ContextVar` for a counter**: - Initialize a `ContextVar` named `counter` with a default value of `0`. - Implement functions to increment, get, and reset this counter utilizing the `ContextVar` methods. 2. **Async usage**: - Write an asynchronous function (`increment_counter`) which increments the counter a given number of times, simulating some time-consuming operations using `asyncio.sleep()`. - Ensure that two different tasks (coroutines) run concurrently, each manipulating its own counter within isolated contexts. 3. **Context management**: - Use the `Context` class and `copy_context()` function to create and manage these contexts. - Demonstrate that the counter values in different contexts remain isolated even when the asynchronous functions are running concurrently. Example Input and Output: - **Input**: Two tasks with different increment counts. - Task 1: Increment counter 5 times with 1-second intervals. - Task 2: Increment counter 3 times with 2-second intervals. - **Output**: - Task 1 should output `5` - Task 2 should output `3` Constraints: - Ensure the implementation is efficient and handles asynchronous context management correctly. - Make sure the counters are accessed and manipulated correctly within their own contexts, without bleeding into each other. Performance Requirements: - Ensure the function execution is efficient and does not have unnecessary complexity. The increments should be accurately isolated per context. ```python import asyncio import contextvars from contextvars import ContextVar, copy_context # Step 1: Initialize a ContextVar with a default value of 0 counter: ContextVar[int] = ContextVar(\'counter\', default=0) def increment(): Increment the context counter by 1. token = counter.set(counter.get() + 1) return token def get_counter_value(): Retrieve the current value of the context counter. return counter.get() def reset_counter(token): Reset the context counter to its previous value. counter.reset(token) async def increment_counter(times, sleep_time): Increment the counter a specified number of times asynchronously. for _ in range(times): await asyncio.sleep(sleep_time) increment() async def main(): # Step 2: Create two contexts and run tasks ctx1 = copy_context() ctx2 = copy_context() # Task 1 context management async def task1(): ctx1.run(increment_counter, 5, 1) value = ctx1.run(get_counter_value) print(f\'Task 1 counter value: {value}\') # Task 2 context management async def task2(): ctx2.run(increment_counter, 3, 2) value = ctx2.run(get_counter_value) print(f\'Task 2 counter value: {value}\') await asyncio.gather(task1(), task2()) # Run the main function asyncio.run(main()) ``` Notes: - Ensure all functions and steps are properly documented and tested. - Handle any potential asynchronous exceptions gracefully.","solution":"import asyncio import contextvars from contextvars import ContextVar, copy_context # Step 1: Initialize a ContextVar with a default value of 0 counter: ContextVar[int] = ContextVar(\'counter\', default=0) def increment(): Increment the context counter by 1. token = counter.set(counter.get() + 1) return token def get_counter_value(): Retrieve the current value of the context counter. return counter.get() def reset_counter(token): Reset the context counter to its previous value. counter.reset(token) async def increment_counter(times, sleep_time): Increment the counter a specified number of times asynchronously. for _ in range(times): await asyncio.sleep(sleep_time) increment() async def main(): # Step 2: Create two contexts and run tasks ctx1 = copy_context() ctx2 = copy_context() # Task 1 context management async def task1(): ctx1.run(lambda: asyncio.run(increment_counter(5, 1))) value = ctx1.run(get_counter_value) print(f\'Task 1 counter value: {value}\') return value # Task 2 context management async def task2(): ctx2.run(lambda: asyncio.run(increment_counter(3, 2))) value = ctx2.run(get_counter_value) print(f\'Task 2 counter value: {value}\') return value # Gather tasks and get their results return await asyncio.gather(task1(), task2()) # Run the main function if __name__ == \'__main__\': asyncio.run(main())"},{"question":"**Question: Efficient Line Retrieval from Large Text Files** You are tasked with implementing a line retrieval system for large text files, leveraging the `linecache` module to ensure efficiency. This system should allow users to query specific lines from text files without repeatedly accessing the files from disk, thus optimizing performance through caching. **Function Signature:** ```python def retrieve_line_from_file(filename: str, lineno: int) -> str: pass ``` **Input:** - `filename` (str): The name of the text file from which the line needs to be retrieved. - `lineno` (int): The line number in the text file that needs to be retrieved (1-indexed). **Output:** - Returns a single line from the specified file as a string, including the terminating newline if it exists. If the line or file does not exist, it returns an empty string. **Constraints:** - The function should not raise any exception for invalid inputs or missing files. It must handle all errors gracefully by returning an empty string. - The file size can be very large (up to several gigabytes), so the solution should efficiently handle large files without loading the entire file into memory. - Assume the text file is encoded in UTF-8 unless specified otherwise in the file. **Example:** ```python filename = \\"example.txt\\" lineno = 10 # Assuming \'example.txt\' contains the following content: # Line 1 # Line 2 # ... # Line 10 # ... print(retrieve_line_from_file(filename, lineno)) # Output: \'Line 10n\' ``` **Additional Requirements:** 1. Ensure efficient cache management by clearing the cache if it is no longer needed. 2. Validate the cache for any changes in the underlying file before retrieving lines. 3. The solution should demonstrate usage of `linecache.getline`, `linecache.clearcache`, and `linecache.checkcache` functions. **Bonus Challenge:** Implement an additional function to prefetch lines into the cache without immediately retrieving them, for optimized access to a series of queries. This function should take a list of line numbers and store the lines in the cache for future retrievals. ```python def prefetch_lines_into_cache(filename: str, line_numbers: list[int]) -> None: pass ``` This function does not return any value but ensures that the specified lines are loaded into the cache for future queries.","solution":"import linecache def retrieve_line_from_file(filename: str, lineno: int) -> str: Retrieves a specific line from a file efficiently using caching. Parameters: - filename: str : The name of the text file from which the line needs to be retrieved. - lineno: int : The line number in the text file that needs to be retrieved (1-indexed). Returns: - str : The line from the file as a string including the newline character if it exists, or an empty string if the line or file does not exist. try: line = linecache.getline(filename, lineno) linecache.checkcache(filename) # validate the cache for any changes in the underlying file return line if line else \'\' except Exception: return \'\' def prefetch_lines_into_cache(filename: str, line_numbers: list[int]) -> None: Prefetches specific lines into the cache for optimized access. Parameters: - filename: str : The name of the text file. - line_numbers: list[int] : The list of line numbers to prefetch. for lineno in line_numbers: linecache.getline(filename, lineno) linecache.checkcache(filename) # validate the cache for any changes in the underlying file"},{"question":"# PyTorch Named Tensors Coding Assessment **Objective**: Implement a function that manipulates tensor dimensions using named tensors and performs specific operations ensuring the correct name propagation and dimension alignment. **Problem Statement**: You are required to implement a function `process_tensors` that takes three tensors as inputs and performs the following steps: 1. Create a new tensor by performing a matrix multiplication between two of the input tensors (`a` and `b`) which are properly aligned by their named dimensions. 2. Flatten specific dimensions of the resulting tensor to create a single dimension with a new name. 3. Align the flattened tensor to match the dimension order of the third input tensor (`c`). 4. Return the final aligned tensor. # Function Signature ```python import torch from typing import Tuple def process_tensors(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor: pass ``` # Inputs: - `a` (torch.Tensor): A 2-dimensional tensor with named dimensions (`X`, `Y`). - `b` (torch.Tensor): A 2-dimensional tensor with named dimensions (`Y`, `Z`), which will be matrix-multiplied with `a`. - `c` (torch.Tensor): A tensor of any dimension (could be higher than 2) with named dimensions including `X\'` and `Z\'`. # Outputs: - Returns a tensor with the named dimensions matching the order and naming conventions of tensor `c`, ensuring all operations maintain the correct name propagation (e.g., `a` and `b` should result in dimensions (`X`, `Z`), which could be flattened and aligned as required). # Constraints: - Assume all input tensors have the names appropriately assigned as per their usage. - Input tensors `a` and `b` will have compatible dimensions for matrix multiplication. - Proper name propagation should ensure meaningful dimension names in the final output tensor. # Example: ```python import torch a = torch.rand(3, 4, names=(\'X\', \'Y\')) b = torch.rand(4, 5, names=(\'Y\', \'Z\')) c = torch.rand(2, 3, 5, names=(\'N\', \'X\', \'Z\')) result = process_tensors(a, b, c) print(result.names) # Should align and show names similar to c\'s names: (\'N\', \'X\', \'Z\') ``` In this example, the function should: 1. Matrix multiply `a` and `b` resulting in a tensor named (`X`, `Z`). 2. Flatten necessary dimensions of the resulting tensor if required. 3. Align the resulting tensor to match the dimension order and names of tensor `c`. **Note**: Proper name propagation and dimension alignment should be maintained to ensure the final tensor has the correct names and matches the order of dimensions as in tensor `c`. # Guidelines: - Make sure to use the `refine_names`, `align_to`, and `flatten` methods where applicable. - Ensure all steps maintain the correct dimension naming and ordering. - You can assume the `names` attribute of the tensors is correctly set in input tensors.","solution":"import torch from typing import Tuple def process_tensors(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor: # Ensure the input tensors have the expected names assert a.names == (\'X\', \'Y\') assert b.names == (\'Y\', \'Z\') # Perform matrix multiplication along named dimensions result = torch.matmul(a, b) # Set the resulting tensor names result = result.refine_names(\'X\', \'Z\') # Align the result tensor to match the dimension names and order of tensor `c` aligned_result = result.align_to(*c.names) return aligned_result"},{"question":"# Question: File Lock Coordination with `fcntl` Problem Statement: You are tasked with writing a Python script to manage file locking using the `fcntl` module. Your script should demonstrate the use of file locks to ensure that only one process can write to a file at a time while allowing multiple processes to read from it. Specifically, you need to create two functions: 1. `lock_file(fd, lock_type)`: - This function should lock the file descriptor `fd` based on the `lock_type` provided. - The `lock_type` should be one among `fcntl.LOCK_SH` (shared lock for reading) or `fcntl.LOCK_EX` (exclusive lock for writing). - Use `fcntl.LOCK_NB` to avoid blocking if the lock cannot be acquired immediately. Raise an `OSError` if the lock is not acquired. 2. `unlock_file(fd)`: - This function should unlock the file descriptor `fd`. - Use `fcntl.LOCK_UN` to perform the unlock operation. # Input: - An integer `fd` which is the file descriptor of the file to be locked or unlocked. - A string `lock_type` for the `lock_file` function which will be either \\"shared\\" or \\"exclusive\\". # Output: - For `lock_file`: Return `True` if the lock is successfully acquired, otherwise raise an `OSError`. - For `unlock_file`: No return value is expected. # Constraints: - Only use the functions and constants provided by the `fcntl` module to manage file locking. - Make sure that your functions are robust and handle all necessary exceptions appropriately. # Example Usage: ```python import fcntl import os def lock_file(fd, lock_type): if lock_type == \\"shared\\": lock = fcntl.LOCK_SH elif lock_type == \\"exclusive\\": lock = fcntl.LOCK_EX else: raise ValueError(\\"Invalid lock_type. Use \'shared\' or \'exclusive\'.\\") try: # Attempt to acquire the lock with non-blocking mode fcntl.flock(fd, lock | fcntl.LOCK_NB) return True except OSError: raise OSError(f\\"Failed to acquire {lock_type} lock.\\") def unlock_file(fd): try: fcntl.flock(fd, fcntl.LOCK_UN) except OSError: raise OSError(\\"Failed to unlock the file.\\") # Example usage fd = os.open(\'example.txt\', os.O_RDWR | os.O_CREAT) # Acquire an exclusive (write) lock if lock_file(fd, \\"exclusive\\"): print(\\"Exclusive lock acquired.\\") # Do some write operations... unlock_file(fd) print(\\"File unlocked.\\") ``` Test your functions with multiple file descriptors and lock types to ensure they work correctly and handle errors as expected.","solution":"import fcntl import os def lock_file(fd, lock_type): Locks the file descriptor fd based on the lock_type provided. :param fd: File descriptor of the file to be locked. :param lock_type: Type of lock - \\"shared\\" for shared lock, \\"exclusive\\" for exclusive lock. :return: True if the lock is successfully acquired, otherwise raises OSError. if lock_type == \\"shared\\": lock = fcntl.LOCK_SH elif lock_type == \\"exclusive\\": lock = fcntl.LOCK_EX else: raise ValueError(\\"Invalid lock_type. Use \'shared\' or \'exclusive\'.\\") try: # Attempt to acquire the lock with non-blocking mode fcntl.flock(fd, lock | fcntl.LOCK_NB) return True except OSError: raise OSError(f\\"Failed to acquire {lock_type} lock.\\") def unlock_file(fd): Unlocks the file descriptor fd. :param fd: File descriptor of the file to be unlocked. :return: None try: fcntl.flock(fd, fcntl.LOCK_UN) except OSError: raise OSError(\\"Failed to unlock the file.\\")"},{"question":"Objective: Assess the ability to use Python\'s `re` module to solve a complex string processing task involving regexes. Problem Statement: You are given a block of text containing multiple email addresses, dates, and URLs. Your task is to extract all of these elements from the text and categorize them into emails, dates, and URLs using regular expressions. You should write a function `extract_elements` that takes a single string containing the text and returns a dictionary with keys `\'emails\'`, `\'dates\'`, and `\'urls\'`. Function Signature: ```python def extract_elements(text: str) -> dict: pass ``` Input: - `text`: A string containing multiple lines of text which may include email addresses, dates, and URLs. Output: - A dictionary with three keys: - `\'emails\'`: A list of all email addresses found. - `\'dates\'`: A list of all dates found. - `\'urls\'`: A list of all URLs found. Requirements: - Email addresses must match the pattern: **example@domain.com** - Dates must include the following formats: - **DD/MM/YYYY** - **MM-DD-YYYY** - **YYYY.MM.DD** - URLs must include the following formats: - **http://example.com** - **https://www.example.com** - **www.example.com** Constraints: - Use Python\'s `re` module for regular expressions. - The function should handle edge cases and invalid formats gracefully. - Performance: The function should be efficient enough to handle large blocks of text (up to 10,000 lines). Examples: ```python text = Contact us at support@example.com or sales@example.org. Visit https://www.example.com for more information. Our office was established on 10/12/2020. Alternative site: www.example.net. We have another event on 01-01-2022. # Expected output { \'emails\': [\'support@example.com\', \'sales@example.org\'], \'dates\': [\'10/12/2020\', \'01-01-2022\'], \'urls\': [\'https://www.example.com\', \'www.example.net\'] } ``` Additional Notes: - Use appropriate regex patterns to capture each element. - Order of elements in the lists does not matter. - Handle cases where no elements are found for a category by returning an empty list for that category.","solution":"import re def extract_elements(text: str) -> dict: Given a block of text, extracts and returns emails, dates, and URLs. # Define regex patterns for emails, dates, and URLs email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\' date_pattern = r\'b(?:d{2}[/-]d{2}[/-]d{4}|d{4}.d{2}.d{2})b\' url_pattern = r\'b(?:https?://(?:www.)?|www.)[A-Za-z0-9.-]+.[A-Za-z]{2,}S*b\' # Use findall to get all matches for each pattern emails = re.findall(email_pattern, text) dates = re.findall(date_pattern, text) urls = re.findall(url_pattern, text) return { \'emails\': emails, \'dates\': dates, \'urls\': urls, }"},{"question":"# **Advanced Coding Assessment Question** **Memory Buffer Management in Python** You have been provided with the prototype of an old buffer protocol, which is now deprecated. Using modern Python3 techniques, write a function named `process_buffer_data` that: 1. Receives a byte-like object (e.g., `bytes` or `bytearray`). 2. Transforms the data by applying a function `func` (which takes and returns a single byte). 3. Returns a new byte-like object of the same type with the transformed data. **Function Signature** ```python def process_buffer_data(data: Union[bytes, bytearray], func: Callable[[int], int]) -> Union[bytes, bytearray]: pass ``` **Inputs** - `data`: A byte-like object (either `bytes` or `bytearray`). - `func`: A function that takes in an `int` (representing a byte) and returns an `int` (also representing a byte). **Output** - A new byte-like object of the same type as `data` containing the transformed bytes. **Example** ```python def increment_byte(b: int) -> int: return (b + 1) % 256 data = bytes([1, 2, 3]) print(process_buffer_data(data, increment_byte)) # Output: b\'x02x03x04\' data = bytearray([255, 0, 1]) print(process_buffer_data(data, increment_byte)) # Output: bytearray(b\'x00x01x02\') ``` **Constraints** - You may not assume the input is always `bytes` or always `bytearray`; your solution should handle both. - The function must return the same type as the input (`bytes` for `bytes`, and `bytearray` for `bytearray`). **Hints** - Consider the use of Python\'s memoryview objects for efficient buffer slicing and manipulation. - Ensure that the function `func` is applied correctly to each byte, and the transformed data maintains the original buffer\'s type. **Performance Requirements** - The solution must handle large input sizes efficiently, both in terms of time and memory usage.","solution":"from typing import Union, Callable def process_buffer_data(data: Union[bytes, bytearray], func: Callable[[int], int]) -> Union[bytes, bytearray]: # Create a memoryview object to efficiently manipulate the buffer mv = memoryview(data) # Apply the transformation function to each byte transformed_data = bytes(func(b) for b in mv) # Return the transformed data in the same type as the input data return data.__class__(transformed_data)"},{"question":"**Custom JSON Encoding and Decoding** **Problem Statement:** In this assignment, you\'ll need to work with the `json` module and demonstrate your understanding by implementing custom encoding and decoding for Python objects that are not natively supported by JSON. Consider the following custom class `Person`: ```python class Person: def __init__(self, name, age, email): self.name = name self.age = age self.email = email def __repr__(self): return f\\"Person(name={self.name!r}, age={self.age!r}, email={self.email!r})\\" ``` Your task is to implement custom JSON encoding and decoding for this class. Specifically, you need to: 1. Create a custom JSON encoder that can serialize instances of `Person` objects into JSON strings. 2. Create a custom JSON decoder that can deserialize JSON strings back into `Person` objects. # Step-by-Step Tasks: 1. **Custom JSON Encoding**: - Subclass `json.JSONEncoder`. - Override the `default()` method to handle `Person` objects. - Ensure that `default()` converts `Person` objects into a dictionary format suitable for JSON encoding. 2. **Custom JSON Decoding**: - Subclass `json.JSONDecoder`. - Override the `__init__()` and `object_hook` methods to handle decoding of `Person` objects. - Ensure that the `object_hook` method correctly recreates `Person` objects from dictionaries. 3. **Testing**: - Test the encoding and decoding process with a list of `Person` objects. - Verify that the resulting objects match the originals. # Example: ```python import json class Person: def __init__(self, name, age, email): self.name = name self.age = age self.email = email def __repr__(self): return f\\"Person(name={self.name!r}, age={self.age!r}, email={self.email!r})\\" # Part 1: Implement Custom Encoder class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return {\'name\': obj.name, \'age\': obj.age, \'email\': obj.email} return super().default(obj) # Part 2: Implement Custom Decoder class PersonDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, dct): if \'name\' in dct and \'age\' in dct and \'email\' in dct: return Person(dct[\'name\'], dct[\'age\'], dct[\'email\']) return dct # Part 3: Test the Encoding and Decoding people = [Person(\\"John Doe\\", 30, \\"john.doe@example.com\\"), Person(\\"Jane Smith\\", 25, \\"jane.smith@example.com\\")] # Encode encoded_people = json.dumps(people, cls=PersonEncoder) print(\\"Encoded:\\", encoded_people) # Decode decoded_people = json.loads(encoded_people, cls=PersonDecoder) print(\\"Decoded:\\", decoded_people) # Verify assert people == decoded_people, \\"The encoding and decoding process failed.\\" ``` **Constraints:** - The `Person` class will always have the attributes `name` (str), `age` (int), and `email` (str). - You should handle lists of `Person` objects. **Expected Output:** ``` Encoded: [{\\"name\\": \\"John Doe\\", \\"age\\": 30, \\"email\\":\\"john.doe@example.com\\"}, {\\"name\\": \\"Jane Smith\\", \\"age\\": 25, \\"email\\":\\"jane.smith@example.com\\"}] Decoded: [Person(name=\'John Doe\', age=30, email=\'john.doe@example.com\'), Person(name=\'Jane Smith\', age=25, email=\'jane.smith@example.com\')] ``` If the assertion passes without raising an error, the custom encoding and decoding works correctly.","solution":"import json class Person: def __init__(self, name, age, email): self.name = name self.age = age self.email = email def __repr__(self): return f\\"Person(name={self.name!r}, age={self.age!r}, email={self.email!r})\\" def __eq__(self, other): if isinstance(other, Person): return self.name == other.name and self.age == other.age and self.email == other.email return False class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return {\'name\': obj.name, \'age\': obj.age, \'email\': obj.email} return super().default(obj) class PersonDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, dct): if \'name\' in dct and \'age\' in dct and \'email\' in dct: return Person(dct[\'name\'], dct[\'age\'], dct[\'email\']) return dct"},{"question":"You are required to implement a function in Python that processes a sequence of numbers and returns a specific result. The function must utilize iterators, generator expressions, and features from the `itertools` or `functools` module to demonstrate an understanding of functional programming in Python. # Task: Write a function `process_numbers(data: Iterable[int]) -> int` that takes an iterable of integers and returns the sum of the products of consecutive integers in the iterable. If the iterable has less than two integers, the function should return 0. Example: ```python data = [1, 2, 3, 4, 5] result = process_numbers(data) # Explanation: # Consecutive pairs: (1,2), (2,3), (3,4), (4,5) # Products: 1*2=2, 2*3=6, 3*4=12, 4*5=20 # Sum of products: 2+6+12+20 = 40 assert result == 40 ``` # Function Signature: ```python def process_numbers(data: Iterable[int]) -> int: pass ``` # Constraints: * You must use iterators and/or generator expressions. * Utilize relevant functions from the `itertools` or `functools` module. * Do not convert the iterable into a list or tuple explicitly. * Handle edge cases such as an empty iterable or an iterable with less than two elements. # Hint: Consider using `itertools.tee` to create multiple iterators and `zip` to pair elements, or `functools.reduce` for the sum.","solution":"from typing import Iterable import itertools def process_numbers(data: Iterable[int]) -> int: # Create two iterators a, b = itertools.tee(data) # Advance the second iterator next(b, None) # Calculate product of consecutive pairs and sum them up return sum(x * y for x, y in zip(a, b))"},{"question":"**Coding Assessment Question** # Email Message Encoder You are tasked with creating a function that encodes the payloads of email messages using different encoding methods provided by the `email.encoders` module. Requirements 1. **Function Name**: `encode_email_payload` 2. **Input**: - A list of tuples, where each tuple contains an email message object (`email.message.EmailMessage`) and a string specifying the encoding method. The encoding method can be one of `\\"quopri\\"`, `\\"base64\\"`, `\\"7or8bit\\"`, or `\\"noop\\"`. 3. **Output**: - A list of the modified email message objects after encoding their payload. Instructions 1. For each tuple in the input list, encode the payload of the email message object using the specified method. 2. Utilize the appropriate encoder function from the `email.encoders` module to modify the payload and set the `Content-Transfer-Encoding` header. 3. Return the modified list of email message objects. Constraints - You should handle each email message object individually, ensuring that the correct encoding function is applied based on the input string. - Raise a `ValueError` if an unsupported encoding method is provided. - Assume that the email message object does not contain multipart messages. Example Usage ```python from email.message import EmailMessage import email.encoders # Sample email messages msg1 = EmailMessage() msg1.set_payload(\\"Hello, this is a test message with some special characters: Ã±, Ã©, Ã¼.\\") msg2 = EmailMessage() msg2.set_payload(\\"A different message with binary data:x00x01x02\\") # Create a list of tuples with messages and their specified encoding methods messages_to_encode = [(msg1, \\"quopri\\"), (msg2, \\"base64\\")] # Define the function def encode_email_payload(messages): encoded_messages = [] for (msg, method) in messages: if method == \'quopri\': email.encoders.encode_quopri(msg) elif method == \'base64\': email.encoders.encode_base64(msg) elif method == \'7or8bit\': email.encoders.encode_7or8bit(msg) elif method == \'noop\': email.encoders.encode_noop(msg) else: raise ValueError(\\"Unsupported encoding method: \\" + method) encoded_messages.append(msg) return encoded_messages # Call the function and get the encoded messages encoded_messages = encode_email_payload(messages_to_encode) # Verify the result for msg in encoded_messages: print(msg.get_payload()) print(msg[\'Content-Transfer-Encoding\']) ``` In this example, after calling `encode_email_payload`, the payload of `msg1` would be encoded in quoted-printable and `msg2` in base64, and their `Content-Transfer-Encoding` headers will be updated accordingly.","solution":"from email.message import EmailMessage import email.encoders def encode_email_payload(messages): Encodes the payload of email message objects based on specified encoding methods. :param messages: List of tuples, each containing an EmailMessage object and a string specifying the encoding method. :return: List of modified EmailMessage objects after encoding their payload. encoded_messages = [] for msg, method in messages: if method == \'quopri\': email.encoders.encode_quopri(msg) elif method == \'base64\': email.encoders.encode_base64(msg) elif method == \'7or8bit\': email.encoders.encode_7or8bit(msg) elif method == \'noop\': email.encoders.encode_noop(msg) else: raise ValueError(f\\"Unsupported encoding method: {method}\\") encoded_messages.append(msg) return encoded_messages"},{"question":"# Advanced Python DateTime API Integration You are required to write a Python extension module using the C API for the `datetime` module in Python. Your task is to create several date and time objects, extract certain fields from them, and perform some operations. You will be implementing this in `C` and integrating it with Python. Step-by-Step Tasks: 1. **Creating Date and Time Objects**: - Implement a function `create_date` to create a `datetime.date` object for given `year`, `month`, and `day`. - Implement a function `create_time` to create a `datetime.time` object for given `hour`, `minute`, `second`, and `microsecond`. 2. **Extracting Fields**: - Implement a function `get_date_fields` to extract `year`, `month`, and `day` from a `datetime.date` object. - Implement a function `get_time_fields` to extract `hour`, `minute`, `second`, and `microsecond` from a `datetime.time` object. 3. **Creating a Naive and Aware Datetime**: - Implement a function `create_datetime` to create a `datetime.datetime` object with given `year`, `month`, `day`, `hour`, `minute`, `second`, `microsecond`, and an optional parameter `is_utc` (a boolean). If `is_utc` is `True`, the datetime object should be timezone-aware (UTC), otherwise, it should be naive. 4. **Print and Validate Your Work**: - Implement a function `print_and_validate` that creates a date (2023-10-05) and a time (12:34:56.789), prints the extracted fields, creates a naive datetime and a UTC datetime, and prints all these objects along with their fields. Input & Output Format: - **Input**: There is no direct input from the user. The module should have hard-coded values for creating objects as specified in the data points. - **Output**: Print statements displaying the created objects and their fields. Constraints: - Use the provided C Macros for creating and handling datetime objects. - Ensure to handle NULL checks appropriately. - The module should be compiled and imported in Python for testing. **Sample Usage in Python**: ```python import datetime_extension as de # No direct input to functions de.print_and_validate() ``` Expected Output: ``` Date created: 2023-10-05 Year: 2023, Month: 10, Day: 5 Time created: 12:34:56.789 Hour: 12, Minute: 34, Second: 56, Microsecond: 789 Naive Datetime created: 2023-10-05 12:34:56.789000 UTC Datetime created: 2023-10-05 12:34:56.789000+00:00 ``` *Note*: Properly handle and print any exceptions or errors if they arise during the process.","solution":"from datetime import date, time, datetime, timezone def create_date(year, month, day): Creates a date object with given year, month, and day. return date(year, month, day) def create_time(hour, minute, second, microsecond): Creates a time object with given hour, minute, second, and microsecond. return time(hour, minute, second, microsecond) def get_date_fields(d): Extracts year, month, and day from a date object. return d.year, d.month, d.day def get_time_fields(t): Extracts hour, minute, second, and microsecond from a time object. return t.hour, t.minute, t.second, t.microsecond def create_datetime(year, month, day, hour, minute, second, microsecond, is_utc=False): Creates a datetime object. If is_utc is True, the datetime is timezone-aware (UTC). if is_utc: return datetime(year, month, day, hour, minute, second, microsecond, tzinfo=timezone.utc) else: return datetime(year, month, day, hour, minute, second, microsecond) def print_and_validate(): Validates and prints created date, time, and datetime objects along with their extracted fields. # Create and print date d = create_date(2023, 10, 5) print(f\\"Date created: {d}\\") year, month, day = get_date_fields(d) print(f\\"Year: {year}, Month: {month}, Day: {day}\\") # Create and print time t = create_time(12, 34, 56, 789000) print(f\\"Time created: {t}\\") hour, minute, second, microsecond = get_time_fields(t) print(f\\"Hour: {hour}, Minute: {minute}, Second: {second}, Microsecond: {microsecond}\\") # Create and print naive datetime naive_dt = create_datetime(2023, 10, 5, 12, 34, 56, 789000) print(f\\"Naive Datetime created: {naive_dt}\\") # Create and print UTC datetime utc_dt = create_datetime(2023, 10, 5, 12, 34, 56, 789000, is_utc=True) print(f\\"UTC Datetime created: {utc_dt}\\") # Output the result for demonstration print_and_validate()"},{"question":"Isotonic Regression Objective: The goal of this task is to implement and use the scikit-learn `IsotonicRegression` class to fit a 1-dimensional dataset. You should demonstrate your understanding of how to apply the isotonic regression, specify its constraints, and evaluate its effectiveness. Problem Statement: Consider a dataset of daily temperatures over a certain period, where each observation consists of a day index and the corresponding temperature recorded on that day. Your task is to implement isotonic regression to fit this data under different constraints (non-decreasing and non-increasing) and then evaluate the model\'s performance. Detailed Instructions: 1. **Data Preparation:** - Create a synthetic dataset of daily temperatures for 30 days. Use random noise to simulate realistic temperature variations. 2. **Model Implementation:** - Implement the isotonic regression using the scikit-learn `IsotonicRegression` class. - First, fit the model under the default constraint (non-decreasing function). - Then, fit the model under the non-increasing constraint. 3. **Prediction and Evaluation:** - Generate predictions for both models (non-decreasing and non-increasing). - Visualize the original data and the fitted models using a line plot. - Calculate the mean squared error for both fitted models to compare their performances. 4. **Documentation:** - Provide a brief explanation of your approach for each step. - Summarize your findings and comment on which model better fits the synthetic data. Constraints: - You should use the `IsotonicRegression` class from scikit-learn. - Assume the weights `w_i` are all set to 1 (i.e., equal weighting). Input and Output Format: - **Input:** The function should not take any inputs from the user. All data creation, fitting, prediction, and visualization should be done within the function. - **Output:** The output should include the mean squared error for both regression models and the visual plots. Sample Code Structure: ```python from sklearn.isotonic import IsotonicRegression import numpy as np import matplotlib.pyplot as plt def isotonic_regression_evaluation(): # Step 1: Data Preparation np.random.seed(0) days = np.arange(1, 31) temperatures = 10 + 0.5 * days + np.random.normal(size=days.shape) # Step 2: Model Implementation # Non-decreasing isotonic regression ir_inc = IsotonicRegression(increasing=True) y_inc = ir_inc.fit_transform(days, temperatures) # Non-increasing isotonic regression ir_dec = IsotonicRegression(increasing=False) y_dec = ir_dec.fit_transform(days, temperatures) # Step 3: Prediction and Evaluation mse_inc = np.mean((temperatures - y_inc)**2) mse_dec = np.mean((temperatures - y_dec)**2) # Plotting results plt.figure(figsize=(12, 6)) plt.plot(days, temperatures, \'o\', label=\'Original data\') plt.plot(days, y_inc, label=\'Isotonic Regression (Increasing)\') plt.plot(days, y_dec, label=\'Isotonic Regression (Decreasing)\') plt.xlabel(\'Days\') plt.ylabel(\'Temperature\') plt.legend() plt.title(\'Isotonic Regression on Synthetic Temperature Data\') plt.show() print(f\\"MSE (Increasing): {mse_inc}\\") print(f\\"MSE (Decreasing): {mse_dec}\\") # Call the function to perform the analysis isotonic_regression_evaluation() ``` Ensure to perform the necessary installations, such as scikit-learn and matplotlib, before running the code.","solution":"from sklearn.isotonic import IsotonicRegression import numpy as np import matplotlib.pyplot as plt def isotonic_regression_evaluation(): # Step 1: Data Preparation np.random.seed(0) days = np.arange(1, 31) temperatures = 10 + 0.5 * days + np.random.normal(size=days.shape) # Step 2: Model Implementation # Non-decreasing isotonic regression ir_inc = IsotonicRegression(increasing=True) y_inc = ir_inc.fit_transform(days, temperatures) # Non-increasing isotonic regression ir_dec = IsotonicRegression(increasing=False) y_dec = ir_dec.fit_transform(days, temperatures) # Step 3: Prediction and Evaluation mse_inc = np.mean((temperatures - y_inc)**2) mse_dec = np.mean((temperatures - y_dec)**2) # Plotting results plt.figure(figsize=(12, 6)) plt.plot(days, temperatures, \'o\', label=\'Original data\') plt.plot(days, y_inc, label=\'Isotonic Regression (Increasing)\') plt.plot(days, y_dec, label=\'Isotonic Regression (Decreasing)\') plt.xlabel(\'Days\') plt.ylabel(\'Temperature\') plt.legend() plt.title(\'Isotonic Regression on Synthetic Temperature Data\') plt.show() print(f\\"MSE (Increasing): {mse_inc}\\") print(f\\"MSE (Decreasing): {mse_dec}\\") # Call the function to perform the analysis isotonic_regression_evaluation()"},{"question":"# PyTorch Coding Assessment: Advanced Tensor Validation and Generation Objective The objective of this assessment is to test your ability to create tensors and validate their properties using the `torch.testing` module. You will need to demonstrate an understanding of tensor creation, manipulation, and validation using PyTorch. Problem Statement You are required to implement a function `compare_operations_with_baseline` which performs specific tensor operations and validates the results against predefined baseline tensors using the functions provided in the `torch.testing` module. Function Signature ```python import torch def compare_operations_with_baseline(): This function performs a series of tensor operations and compares the results with the predefined baseline tensors using the provided PyTorch testing functions. It prints \'Validation Passed\' if all operations are correct and \'Validation Failed\' otherwise. pass ``` Tasks 1. **Tensor Creation**: - Create two tensors, `A` and `B`, of shape (3, 3) using `torch.testing.make_tensor`. - The elements of tensor `A` should be randomly generated floating-point numbers between 0 and 1. - The elements of tensor `B` should be randomly generated floating-point numbers between 1 and 2. 2. **Tensor Operation**: - Compute a new tensor `C` by performing element-wise addition of `A` and `B`. - Compute a new tensor `D` by performing element-wise multiplication of `A` and `B`. 3. **Baseline Comparison**: - Define baseline tensors `C_baseline` and `D_baseline` as follows: ```python C_baseline = A + B D_baseline = A * B ``` - Use `torch.testing.assert_close` to verify that tensor `C` is close to `C_baseline`. - Use `torch.testing.assert_allclose` to verify that tensor `D` is close to `D_baseline`. 4. **Validation**: - If both assertions pass, print \\"Validation Passed\\". - If any assertion fails, print \\"Validation Failed\\". Constraints - Do not explicitly define the elements of `A` and `B`; use the functions provided to generate them. - Ensure to handle any exceptions or errors appropriately to determine if the validation fails. Example Case The actual values of tensors will be randomly generated, so there\'s no fixed output for the problem. The focus is on correct use of tensor operations and validation functions. Additional Notes - Refer to the PyTorch documentation for additional details on `torch.testing` functions if necessary. - You may import other modules from PyTorch if needed for your implementation.","solution":"import torch import torch.testing def compare_operations_with_baseline(): try: # Tensor Creation A = torch.testing.make_tensor((3, 3), dtype=torch.float32, low=0.0, high=1.0) B = torch.testing.make_tensor((3, 3), dtype=torch.float32, low=1.0, high=2.0) # Tensor Operations C = A + B D = A * B # Baseline Comparison C_baseline = A + B D_baseline = A * B # Validation torch.testing.assert_close(C, C_baseline) torch.testing.assert_close(D, D_baseline) print(\\"Validation Passed\\") except Exception as e: print(\\"Validation Failed\\") print(e)"},{"question":"# Floating Point Operations in Python You are required to implement a function that performs several operations involving Python floating-point objects utilizing the Python-C API functions provided by the \'python310\' package. Function Signature ```python def perform_float_operations(input_str: str) -> (float, float, float): Takes a string representing a float, converts it to a Python float object, retrieves its value, and returns a tuple containing the following: - The float value converted from the input string. - The maximum representable finite float using PyFloat_GetMax. - The minimum normalized positive float using PyFloat_GetMin. Parameters: input_str (str): The string representing a float. Returns: tuple: A tuple containing three float values (input_value, max_float, min_float). pass ``` Constraints 1. The input string will always be a valid representation of a float. 2. Performance considerations should ensure smooth handling of conversion and retrieval operations without significant delay. Description 1. Create a `PyFloatObject` from the provided string using `PyFloat_FromString`. 2. Convert the `PyFloatObject` back to a native Python float. 3. Retrieve the maximum and minimum float values using `PyFloat_GetMax` and `PyFloat_GetMin`. You will demonstrate a clear understanding of type-checking, conversions from string to `PyFloatObject` and vice versa, and retrieving float information by utilizing the respective functions. Example ```python result = perform_float_operations(\'123.456\') print(result) # Example: (123.456, 1.7976931348623157e+308, 2.2250738585072014e-308) ``` Ensure your implementation uses the python310 library functions to manipulate and retrieve the necessary floating-point information.","solution":"import sys def perform_float_operations(input_str: str) -> (float, float, float): Takes a string representing a float, converts it to a Python float object, retrieves its value, and returns a tuple containing the following: - The float value converted from the input string. - The maximum representable finite float using PyFloat_GetMax. - The minimum normalized positive float using PyFloat_GetMin. Parameters: input_str (str): The string representing a float. Returns: tuple: A tuple containing three float values (input_value, max_float, min_float). # Convert the input string to a float input_value = float(input_str) # Retrieve maximum and minimum float values max_float = sys.float_info.max min_float = sys.float_info.min return (input_value, max_float, min_float)"},{"question":"**Question**: Exploring Seaborn Styles and Plotting Functions You are provided with data on a fictional company\'s employee metrics, including hours worked and productivity scores. You need to visualize this data using Seaborn, leveraging different styles and plotting functions to illustrate the relationship between the hours worked and productivity scores. # Task 1. Load the data (given in a CSV file) which includes the following columns: `EmployeeID`, `HoursWorked`, and `ProductivityScore`. 2. Create a scatter plot to visualize the relationship between `HoursWorked` and `ProductivityScore` using the default style. 3. Change the plot style to `darkgrid` and recreate the scatter plot. 4. Temporarily switch the plot style to `whitegrid` and within this context, create a bar plot showing the average `ProductivityScore` for different ranges of `HoursWorked` (e.g., 0-10, 10-20, 20-30, etc.). 5. Save all plots as PNG files with appropriate filenames. # Input - A CSV file named `employee_data.csv` with the following columns: - `EmployeeID` (integer) - `HoursWorked` (float) - `ProductivityScore` (float) # Output - Three PNG files: 1. `scatter_default.png`: Scatter plot with default style. 2. `scatter_darkgrid.png`: Scatter plot with `darkgrid` style. 3. `barplot_whitegrid.png`: Bar plot with average productivity scores using `whitegrid` style. # Constraints - Ensure that the code is well-commented and uses functions for modularity. - The bar plot should categorize `HoursWorked` into bins of 10 hours. # Performance - The code should be efficient and handle datasets with up to 1000 entries comfortably. # Example Assuming `employee_data.csv` contains: ``` EmployeeID,HoursWorked,ProductivityScore 1,20,80 2,35,60 3,15,90 ... ``` Your code should: - Load the data. - Create and save the scatter plots (`scatter_default.png` and `scatter_darkgrid.png`). - Create a bar plot of average productivity scores per 10-hour work segments and save it as `barplot_whitegrid.png`. You must handle missing data appropriately if present.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_data(file_path): Load the data from a CSV file. return pd.read_csv(file_path) def create_scatter_plot(data, style=None, output_file=\'scatter.png\'): Creates and saves a scatter plot of HoursWorked vs ProductivityScore. if style: sns.set_style(style) plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'HoursWorked\', y=\'ProductivityScore\', data=data) plt.title(\'Hours Worked vs Productivity Score\') plt.xlabel(\'Hours Worked\') plt.ylabel(\'Productivity Score\') plt.savefig(output_file) plt.close() def create_bar_plot(data, style, output_file=\'barplot.png\'): Creates and saves a bar plot of average ProductivityScores for different ranges of HoursWorked. if style: sns.set_style(style) bins = range(0, int(data[\'HoursWorked\'].max() + 10), 10) data[\'HoursWorkedBin\'] = pd.cut(data[\'HoursWorked\'], bins, right=False) avg_productivity = data.groupby(\'HoursWorkedBin\')[\'ProductivityScore\'].mean().reset_index() plt.figure(figsize=(10, 6)) sns.barplot(x=\'HoursWorkedBin\', y=\'ProductivityScore\', data=avg_productivity) plt.title(\'Average Productivity Score by Hours Worked\') plt.xlabel(\'Hours Worked\') plt.ylabel(\'Average Productivity Score\') plt.xticks(rotation=45) plt.savefig(output_file) plt.close() def process_visualizations(file_path): Main function to process data and generate the required plots. data = load_data(file_path) # Scatter plot with default style create_scatter_plot(data, output_file=\'scatter_default.png\') # Scatter plot with darkgrid style create_scatter_plot(data, style=\'darkgrid\', output_file=\'scatter_darkgrid.png\') # Bar plot with whitegrid style create_bar_plot(data, style=\'whitegrid\', output_file=\'barplot_whitegrid.png\') # Example usage if __name__ == \\"__main__\\": process_visualizations(\'employee_data.csv\')"},{"question":"on PyTorch XPU Module Problem Statement You are required to implement a PyTorch function that initializes the XPU device, sets a random seed for reproducibility, runs computations on a specified number of tensors, and then returns the total allocated memory on the XPU. This task will help in understanding device management, random number generation, and memory management in the `torch.xpu` module. Function Signature ```python import torch def xpu_memory_usage(num_tensors: int, tensor_size: int, seed: int) -> int: Initialize the XPU device, set a random seed, create tensors, and return total memory usage. Args: - num_tensors (int): The number of tensors to be created. - tensor_size (int): The size of each tensor (assuming square tensor). - seed (int): The seed for random number generation. Returns: - int: Total allocated memory on the XPU in bytes. pass ``` Input - `num_tensors` (int): The number of tensors to create. - `tensor_size` (int): The size of each tensor (assuming a square tensor of shape `(tensor_size, tensor_size)`). - `seed` (int): The random seed for reproducibility. Output - Returns the total memory allocated on the XPU in bytes. Requirements 1. **Initialize the XPU Device**: Check if an XPU is available and initialize it. 2. **Set up Random Seed**: Use the provided seed to ensure reproducibility of tensor values. 3. **Tensor Creation**: Create the specified number of square tensors with dimensions `(tensor_size, tensor_size)` and move them to the XPU. 4. **Compute Memory Usage**: Calculate the total allocated memory on the XPU and return it. Constraints - Ensure proper error handling. If XPU is not available, the function should raise an appropriate exception. Example ```python result = xpu_memory_usage(5, 1024, 42) print(result) # Output could be something like 20971520 (this will vary based on the actual memory allocated by the tensors) ``` Instructions 1. Use the `torch.xpu` module functions where applicable. 2. Ensure that tensors are moved to the XPU for the memory allocation to be accurate. 3. Use proper error handling for any potential issues (e.g., XPU not available).","solution":"import torch def xpu_memory_usage(num_tensors: int, tensor_size: int, seed: int) -> int: Initialize the XPU device, set a random seed, create tensors, and return total memory usage. Args: - num_tensors (int): The number of tensors to be created. - tensor_size (int): The size of each tensor (assuming square tensor). - seed (int): The seed for random number generation. Returns: - int: Total allocated memory on the XPU in bytes. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU is not available on this system\\") device = torch.device(\'xpu\') # Set the seed for reproducibility torch.manual_seed(seed) # Move tensors to XPU device tensors = [] for _ in range(num_tensors): tensor = torch.rand((tensor_size, tensor_size), device=device) tensors.append(tensor) # Return the total allocated memory on the XPU return torch.xpu.memory_allocated(device)"},{"question":"# Encoding and Decoding Custom Format **Objective**: Implement a custom encoding and decoding mechanism for files, mimicking the binhex4 functionality as described in the `binhex` module. This task will assess your understanding of file I/O operations, error handling, and algorithm implementation for encoding and decoding operations. # Task Description: 1. **Encoding Function** (`custom_encode`): - **Input**: - `input_filename` (str): Name of the binary file to be encoded. - `output_filename` (str): Name of the resulting encoded file. - **Output**: None - **Behavior**: Read the binary file with the `input_filename`, encode its contents into a custom encoded format, and write the result to a new file with the `output_filename`. 2. **Decoding Function** (`custom_decode`): - **Input**: - `input_filename` (str): Name of the encoded file to be decoded. - `output_filename` (str): Name of the resulting binary file. - **Output**: None - **Behavior**: Read the encoded file with the `input_filename`, decode its contents back to the original binary format, and write the result to a new file with the `output_filename`. 3. **Custom Error Handling**: - Implement a custom Exception `EncodeDecodeError` which should be raised for any invalid operations or failures during encoding or decoding. # Constraints: - Assume the maximum file size for encoding/decoding is 10 MB. - The encoded format can be a simple transformation (e.g., Base64, hexadecimal) but should involve multiple steps or transformations for educational purposes. - You are not allowed to use any ready-made `binhex`, Base64 or similar encoding library functions directly. Implement the encoding and decoding logic manually. - Ensure that your encoded files are cross-checked by decoding back to the original files. # Performance Requirements: - The solution should be efficient enough to handle the maximum file size within reasonable time limits. # Example Usage: ```python try: custom_encode(\\"example_binary.bin\\", \\"encoded.bin\\") print(\\"Encoding successful.\\") except EncodeDecodeError as e: print(f\\"Encoding failed: {str(e)}\\") try: custom_decode(\\"encoded.bin\\", \\"decoded_binary.bin\\") print(\\"Decoding successful.\\") except EncodeDecodeError as e: print(f\\"Decoding failed: {str(e)}\\") ``` # Notes: - Provide the complete implementations for `custom_encode`, `custom_decode`, and `EncodeDecodeError`. - Use appropriate file I/O operations and error handling mechanisms. Good luck!","solution":"import os class EncodeDecodeError(Exception): pass def custom_encode(input_filename, output_filename): try: if not os.path.isfile(input_filename): raise EncodeDecodeError(f\\"File \'{input_filename}\' does not exist.\\") with open(input_filename, \'rb\') as infile: binary_data = infile.read() if len(binary_data) > 10 * 1024 * 1024: # 10 MB limit raise EncodeDecodeError(f\\"File \'{input_filename}\' exceeds the maximum size limit of 10 MB.\\") # Custom Encoding: Convert binary data to hexadecimal string encoded_data = binary_data.hex() with open(output_filename, \'w\') as outfile: outfile.write(encoded_data) except Exception as e: raise EncodeDecodeError(f\\"Error during encoding: {str(e)}\\") def custom_decode(input_filename, output_filename): try: if not os.path.isfile(input_filename): raise EncodeDecodeError(f\\"File \'{input_filename}\' does not exist.\\") with open(input_filename, \'r\') as infile: encoded_data = infile.read() # Custom Decoding: Convert hexadecimal string back to binary data try: binary_data = bytes.fromhex(encoded_data) except ValueError as e: raise EncodeDecodeError(f\\"Invalid encoded data: {str(e)}\\") with open(output_filename, \'wb\') as outfile: outfile.write(binary_data) except Exception as e: raise EncodeDecodeError(f\\"Error during decoding: {str(e)}\\")"},{"question":"Context: You are given a set of data collections that need to be efficiently stored and manipulated. Using Python\'s `array` module, you will create arrays of different types and perform various operations to transform these collections as specified. Problem Statement: Implement a function `process_data(list_of_collections: list, operations: list) -> list` that takes: 1. `list_of_collections`: A list of tuples, where each tuple contains a type code (as a string) and a list of numbers or characters. - Example: `[(\'i\', [1, 2, 3, 4]), (\'d\', [2.5, 3.5]), (\'B\', [0, 255])]`. 2. `operations`: A list of operations as strings specifying what to perform on the arrays. Each operation is a string in the format `\\"typecode operation parameters\\"`: - Typecode specifies on which array (from `list_of_collections`) the operation will be performed. - Operation specifies the function to be called (`append`, `extend`, `count`, etc.). - Parameters are the necessary parameters for the operation. The function should return a list of results for the operations that produce an output (like `count` or any operation that has a return value). For mutating operations (like `append`, `extend`), the function should perform the operation on the respective array. Example: ```python def process_data(list_of_collections, operations): # Your implementation here # Example usage list_of_collections = [(\'i\', [1, 2, 3, 4]), (\'d\', [2.5, 3.5]), (\'B\', [0, 255])] operations = [ \\"i append 5\\", \\"d extend 4.5,5.5\\", \\"B count 255\\", \\"i tolist\\", ] print(process_data(list_of_collections, operations)) # Expected output [None, None, 1, [1, 2, 3, 4, 5]] ``` Constraints: 1. Only valid type codes (\'b\', \'B\', \'u\', \'h\', \'H\', \'i\', \'I\', \'l\', \'L\', \'q\', \'Q\', \'f\', \'d\') will be used. 2. The operations list will only contain valid operations corresponding to the array type. 3. The operations that affect the arrays directly (`append`, `extend`, etc.) might not return any value and hence should be ignored in the result list. Implement the function according to the given specifications.","solution":"import array def process_data(list_of_collections, operations): array_map = {} for typecode, elements in list_of_collections: array_map[typecode] = array.array(typecode, elements) results = [] for op_str in operations: parts = op_str.split() typecode = parts[0] operation = parts[1] if len(parts) > 2: params = parts[2:] else: params = [] arr = array_map.get(typecode) if not arr: continue if operation == \\"append\\": arr.append(type(arr[0])(params[0])) results.append(None) elif operation == \\"extend\\": values = [type(arr[0])(val) for val in \\",\\".join(params).split(\',\')] arr.extend(values) results.append(None) elif operation == \\"count\\": result = arr.count(type(arr[0])(params[0])) results.append(result) elif operation == \\"tolist\\": results.append(arr.tolist()) # Here you can add more operations if necessary return [result for result in results if result is not None]"},{"question":"**Title: Configuration File Management and Deployment** **Description:** You are tasked with creating a Python script that will manage and utilize a `setup.cfg` configuration file. Your script should carry out the following tasks: 1. **Read and Parse**: Read and parse an existing `setup.cfg` file. Extract the commands and their corresponding options. 2. **Modify Configuration**: Allow modification of specific values within a command section. For this task, you will modify values: - Set `inplace` to `1` under the `build_ext` command if this section exists. - Add a new option `optimize` set to `2` under the `build_ext` command. 3. **Save Configuration**: Save the modified configuration back to the `setup.cfg` file, maintaining all comments and blank lines. 4. **Display Configuration**: Display the final configuration in a clear format. **Requirements:** 1. Your script should handle any valid `setup.cfg` file structure according to the documentation provided. 2. The script should maintain the readability of the `setup.cfg` file, preserving comments and blank lines. 3. Implementation should be efficient and handle large configuration files gracefully. **Input/Output Specifications:** - **Input**: A `setup.cfg` file containing various command sections and options. - **Output**: The modified `setup.cfg` file and a printout of the final configuration. **Constraints:** - Ensure your solution handles edge cases such as missing command sections gracefully. - Avoid hardcoding paths; instead, allow the user to specify the configuration file path as an argument to the script. **Example:** Given the following `setup.cfg` content: ```text [build_ext] # Existing options build_lib = build/ ``` After running your script, the `setup.cfg` should be modified as follows: ```text [build_ext] # Existing options build_lib = build/ inplace = 1 optimize = 2 ``` And the script output: ``` Final Configuration: [build_ext] build_lib = build/ inplace = 1 optimize = 2 ``` ```python import configparser def manage_setup_cfg(filename): # Read and parse the configuration file config = configparser.ConfigParser(allow_no_value=True) with open(filename, \'r\') as f: file_content = f.read() config.read_string(file_content) # Modify the \'build_ext\' section if \'build_ext\' not in config: config.add_section(\'build_ext\') config[\'build_ext\'][\'inplace\'] = \'1\' config[\'build_ext\'][\'optimize\'] = \'2\' # Write the changes back to the file, preserving comments and blank lines with open(filename, \'w\') as f: f.write(file_content) config.write(f) # Display the final configuration for section in config.sections(): print(f\\"[{section}]\\") for option, value in config.items(section): print(f\\"{option} = {value}\\") # Example usage: # manage_setup_cfg(\'setup.cfg\') ``` Ensure your script is well-documented and make use of Python\'s `configparser` module to simplify your task. Your solution must be robust and handle various potential issues gracefully, such as missing command sections.","solution":"import configparser def manage_setup_cfg(filename): # Read and parse the configuration file config = configparser.ConfigParser(allow_no_value=True) with open(filename, \'r\') as f: config.read_file(f) # Modify the \'build_ext\' section if \'build_ext\' not in config: config.add_section(\'build_ext\') config[\'build_ext\'][\'inplace\'] = \'1\' config[\'build_ext\'][\'optimize\'] = \'2\' # Write the changes back to the file, preserving comments and blank lines with open(filename, \'w\') as f: config.write(f) # Display the final configuration final_configuration = [] for section in config.sections(): final_configuration.append(f\\"[{section}]\\") for option, value in config.items(section): final_configuration.append(f\\"{option} = {value}\\") final_configuration_str = \'n\'.join(final_configuration) print(final_configuration_str) return final_configuration_str # Example usage: # manage_setup_cfg(\'setup.cfg\')"},{"question":"You are tasked with writing a Python function to perform several different operations on a list of numerical and string data. Your goal is to implement a function that handles various calculations and data manipulations, showcasing your understanding of Python\'s core features. # Function Signature `def process_data(data: list) -> dict:` # Input - `data (list)`: A list containing a mix of integers, floating-point numbers, and strings. # Output - A `dict` with the following keys and corresponding values: - `\'sum_ints\'`: The sum of all the integer values from the input list. - `\'sum_floats\'`: The sum of all the floating-point values from the input list. - `\'concatenated_strings\'`: A single string that is the concatenation of all the strings in the input list. - `\'average\'`: The average of all the numbers (integers and floats) in the list. - `\'sorted_numbers\'`: A list of all the numbers, sorted in ascending order. - `\'reversed_strings\'`: A list of all the strings, each reversed. # Constraints - All elements in the list will be either integers, floating-point numbers, or strings. - The list will contain at least one element. # Example ```python data = [1, \'hello\', 3.5, \'world\', 2, 8.5, \'python\'] output = process_data(data) ``` # Expected Output ```python { \'sum_ints\': 3, \'sum_floats\': 12.0, \'concatenated_strings\': \'helloworldpython\', \'average\': 7/2, \'sorted_numbers\': [1, 2, 3.5, 8.5], \'reversed_strings\': [\'olleh\', \'dlrow\', \'nohtyp\'] } ``` # Notes - You can assume that all strings and numbers in the input list are valid and do not require any further validation. - Ensure your function handles the different types of elements appropriately and performs the required operations efficiently.","solution":"def process_data(data: list) -> dict: Processes a mixed list of integers, floating-point numbers, and strings, performing various calculations and manipulations. sum_ints = sum(filter(lambda x: isinstance(x, int), data)) sum_floats = sum(filter(lambda x: isinstance(x, float), data)) strings = list(filter(lambda x: isinstance(x, str), data)) concatenated_strings = \'\'.join(strings) reversed_strings = [s[::-1] for s in strings] numbers = list(filter(lambda x: isinstance(x, (int, float)), data)) average = sum(numbers) / len(numbers) if numbers else 0 sorted_numbers = sorted(numbers) return { \'sum_ints\': sum_ints, \'sum_floats\': sum_floats, \'concatenated_strings\': concatenated_strings, \'average\': average, \'sorted_numbers\': sorted_numbers, \'reversed_strings\': reversed_strings }"},{"question":"Problem Statement You are given a PyTorch tensor with named dimensions. Your task is to implement a function that performs a series of operations on this tensor while ensuring that the named dimensions are correctly propagated according to the rules of named tensor operations. # Function Signature ```python import torch def process_tensor(input_tensor: torch.Tensor) -> torch.Tensor: Processes a tensor by performing a series of operations while maintaining named dimension rules. Parameters: input_tensor (torch.Tensor): A named tensor of shape [N, C, H, W] Returns: output_tensor (torch.Tensor): The processed tensor with final names. pass ``` # Requirements 1. **Input Tensor**: The input tensor will always have named dimensions `[\'N\', \'C\', \'H\', \'W\']`. 2. **Operations**: - Compute the sum over the \'C\' dimension and ensure the resultant tensor has names `[\'N\', \'H\', \'W\']`. - Transpose the dimensions so that the order becomes `[\'W\', \'N\', \'H\']`. - Add a new named tensor `other` of shape [W, N, H] filled with ones and names `[\'W\', \'N\', \'H\']`. The operations should adhere to the rules for named tensor operations discussed in the documentation. # Example ```python # Create an input tensor input_tensor = torch.randn(2, 3, 4, 5, names=[\'N\', \'C\', \'H\', \'W\']) # Define the function output_tensor = process_tensor(input_tensor) # Output tensor should reflect the operations described print(output_tensor.names) # Should be [\'W\', \'N\', \'H\'] ``` # Constraints - You must avoid using unnamed dimensions. - Make sure the operations do not result in dimensionality mismatches or incorrect name propagation. **Note**: Use the provided documentation and examples as a guide to ensure the correctness of name inference throughout the operations.","solution":"import torch def process_tensor(input_tensor: torch.Tensor) -> torch.Tensor: Processes a tensor by performing a series of operations while maintaining named dimension rules. Parameters: input_tensor (torch.Tensor): A named tensor of shape [N, C, H, W] Returns: output_tensor (torch.Tensor): The processed tensor with final names. if not isinstance(input_tensor, torch.Tensor): raise ValueError(\\"input_tensor must be a torch.Tensor\\") if input_tensor.names != (\'N\', \'C\', \'H\', \'W\'): raise ValueError(\\"input_tensor must have names [\'N\', \'C\', \'H\', \'W\']\\") # Sum over the \'C\' dimension summed_tensor = input_tensor.sum(dim=\'C\') # Transpose the dimensions to [\'W\', \'N\', \'H\'] transposed_tensor = summed_tensor.align_to(\'W\', \'N\', \'H\') # Create other tensor filled with ones with names [\'W\', \'N\', \'H\'] other = torch.ones_like(transposed_tensor) # Add the transposed tensor and the other tensor output_tensor = transposed_tensor + other return output_tensor"},{"question":"# Custom Mutable Sequence and Descriptor in Python You are to implement a custom sequence class and a descriptor using Python\'s data model. Your custom sequence should mimic the behavior of Python\'s built-in list but with additional features. The descriptor should be used to validate values assigned to one of the sequence\'s attributes. Part 1: Custom Sequence Class Implement a class `CustomList` that mimics the functionality of Python\'s list. Specifically, your `CustomList` class should: - Support indexing, slicing, setting items, and deleting items like a list. - Allow for iterating over its elements. - Implement common list methods such as `append`, `extend`, `insert`, `remove`, `pop`, `clear`, `index`, and `count`. - Support concatenation and repetition operations. - Implement the special method `__repr__` to return the official string representation of the custom list. Part 2: Descriptor for Validation Implement a descriptor `NonNegative` which ensures that the assigned value is always a non-negative integer. This descriptor should be used in `CustomList` class to validate an attribute `max_size`, which defines the maximum allowable size for the custom list. The functionality of the `NonNegative` descriptor is as follows: - If a non-negative integer is assigned, it sets the value. - If a negative integer or a non-integer is assigned, it raises a `ValueError`. # Expected Implementation Behavior Here is a skeleton to help you get started: ```python class NonNegative: def __init__(self, name): self.name = name def __get__(self, instance, owner): return instance.__dict__.get(self.name) def __set__(self, instance, value): if isinstance(value, int) and value >= 0: instance.__dict__[self.name] = value else: raise ValueError(f\\"Attempt to set invalid value {value} for {self.name}. Must be a non-negative integer.\\") class CustomList: max_size = NonNegative(\'max_size\') def __init__(self, initial=None, max_size=100): self.items = initial if initial else [] self.max_size = max_size def __getitem__(self, index): ... def __setitem__(self, index, value): ... def __delitem__(self, index): ... def __len__(self): ... def __iter__(self): ... def __repr__(self): return f\\"CustomList({self.items})\\" def append(self, value): ... # Implement other list methods here (extend, insert, remove, pop, clear, index, count) def __add__(self, other): ... def __radd__(self, other): ... def __mul__(self, times): ... def __rmul__(self, times): ... ``` # Constraints and Performance Requirements: - The `CustomList` should properly manage the memory and performance characteristics expected from a standard list. - The descriptor validation should not introduce significant overhead. - The `CustomList` should enforce the `max_size` constraint in its methods correctly, raising an exception if an operation would exceed this size. # Input Format The class and its methods will be tested by creating instances of `CustomList` and performing various list operations on them. # Output Format The methods should operate as described and raise appropriate exceptions for invalid operations. # Example Usage ```python cl = CustomList(max_size=10) cl.append(1) cl.append(2) print(cl) # Output should be: CustomList([1, 2]) try: cl.max_size = -5 # This should raise a ValueError except ValueError as e: print(e) # Output should be: Attempt to set invalid value -5 for max_size. Must be a non-negative integer. ```","solution":"class NonNegative: def __init__(self, name): self.name = name def __get__(self, instance, owner): return instance.__dict__.get(self.name) def __set__(self, instance, value): if isinstance(value, int) and value >= 0: instance.__dict__[self.name] = value else: raise ValueError(f\\"Attempt to set invalid value {value} for {self.name}. Must be a non-negative integer.\\") class CustomList: max_size = NonNegative(\'max_size\') def __init__(self, initial=None, max_size=100): self.items = initial if initial else [] self.max_size = max_size def __getitem__(self, index): return self.items[index] def __setitem__(self, index, value): self.items[index] = value def __delitem__(self, index): del self.items[index] def __len__(self): return len(self.items) def __iter__(self): return iter(self.items) def __repr__(self): return f\\"CustomList({self.items})\\" def append(self, value): if len(self.items) < self.max_size: self.items.append(value) else: raise ValueError(\\"Appending this item would exceed the max_size of the list.\\") def extend(self, iterable): if len(self.items) + len(iterable) <= self.max_size: self.items.extend(iterable) else: raise ValueError(\\"Extending this list would exceed the max_size of the list.\\") def insert(self, index, value): if len(self.items) < self.max_size: self.items.insert(index, value) else: raise ValueError(\\"Inserting this item would exceed the max_size of the list.\\") def remove(self, value): self.items.remove(value) def pop(self, index=-1): return self.items.pop(index) def clear(self): self.items.clear() def index(self, value, start=0, stop=None): return self.items.index(value, start, len(self.items) if stop is None else stop) def count(self, value): return self.items.count(value) def __add__(self, other): if len(self.items) + len(other) <= self.max_size: return CustomList(self.items + other) else: raise ValueError(\\"Concatenating these lists would exceed the max_size of the list.\\") def __radd__(self, other): if len(self.items) + len(other) <= self.max_size: return CustomList(other + self.items) else: raise ValueError(\\"Concatenating these lists would exceed the max_size of the list.\\") def __mul__(self, times): if len(self.items) * times <= self.max_size: return CustomList(self.items * times) else: raise ValueError(\\"Repeating this list would exceed the max_size of the list.\\") def __rmul__(self, times): return self.__mul__(times)"},{"question":"**Challenging Coding Assessment Question: Understanding CPython Instrumentation with DTrace and SystemTap** **Objective:** Write a Python script that simulates a basic program\'s execution and design a corresponding DTrace or SystemTap script that monitors its function call entries and exits. **Python Script Requirements:** 1. Implement a Python script named `sample_program.py` containing at least three different functions that call each other. 2. Ensure the script includes a function called `start`, which is the entry point of the script. **DTrace/SystemTap Script Requirements:** 1. Write a DTrace (for macOS) or SystemTap (for Linux) script that captures and prints the function call hierarchy of `sample_program.py`. 2. The script should highlight when the `start` function begins and ends and should capture all nested functions called by `start`. 3. The output should clearly indicate function entries and exits with relevant details like function names and line numbers. **Input and Output Format:** - Python Script (`sample_program.py`): ```python def function_1(): print(\\"Function 1\\") def function_2(): function_1() print(\\"Function 2\\") def function_3(): function_2() print(\\"Function 3\\") def start(): function_3() if __name__ == \\"__main__\\": start() ``` - DTrace Script (`call_hierarchy.d`): ```dtrace # Your DTrace script goes here ``` - SystemTap Script (`call_hierarchy.stp`): ```stap # Your SystemTap script goes here ``` **Constraints:** 1. Ensure your monitoring script focuses on the specific Python process running `sample_program.py`. 2. The monitoring script should be able to handle dynamic function call scenarios within the execution of the `start` function. **Performance Requirements:** - The monitoring script should efficiently track the function call hierarchy and should not introduce significant overhead to the Python script execution. **Instructions:** 1. Implement the Python script (`sample_program.py`) based on the given example. 2. Write DTrace or SystemTap scripts and save them as `call_hierarchy.d` or `call_hierarchy.stp` respectively. 3. Test the scripts by executing the Python script and running the monitoring scripts to capture the output. 4. Submit your `sample_program.py` file and either `call_hierarchy.d` or `call_hierarchy.stp` file. Hints: - Review the documentation provided for specific probe details and example scripts. - Ensure to run the monitoring script with appropriate permissions (e.g., using `sudo` for DTrace/SystemTap on Unix-like systems). **Example Execution:** - To run the Python script: ```bash python3 sample_program.py ``` - To run the DTrace script (on macOS): ```bash sudo dtrace -q -s call_hierarchy.d -c \\"python3 sample_program.py\\" ``` - To run the SystemTap script (on Linux): ```bash sudo stap call_hierarchy.stp -c \\"python3 sample_program.py\\" ``` **Expected Output:** The monitoring script should print a hierarchical structure of the function calls, such as: ``` timestamp function-entry: sample_program.py:start timestamp function-entry: sample_program.py:function_3 timestamp function-entry: sample_program.py:function_2 timestamp function-entry: sample_program.py:function_1 timestamp function-return: sample_program.py:function_1 timestamp function-return: sample_program.py:function_2 timestamp function-return: sample_program.py:function_3 timestamp function-return: sample_program.py:start ```","solution":"# sample_program.py def function_1(): print(\\"Function 1\\") def function_2(): function_1() print(\\"Function 2\\") def function_3(): function_2() print(\\"Function 3\\") def start(): function_3() if __name__ == \\"__main__\\": start()"},{"question":"# Coding Assessment Objective: You need to demonstrate your understanding of pandas and its support for various data types and arrays by implementing a function that processes a DataFrame with mixed data types. This exercise will cover both fundamental and advanced aspects of data manipulation using pandas. Task: Write a function `process_data(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations on the input DataFrame: 1. **Datetime Operations**: - Convert the column \'event_start\' to datetime with UTC timezone. - Extract the day of the week from \'event_start\' into a new column named \'day_of_week\'. 2. **Timedelta Operations**: - Calculate the difference between \'event_end\' and \'event_start\' to create a new column \'event_duration\' in seconds. 3. **Categorical Data**: - Convert the \'category\' column to a pandas Categorical data type with the categories ordered alphabetically. - Add a new category \\"Other\\" to the \'category\' column. 4. **Nullable Integer Operations**: - Ensure the \'nullable_int\' column has a pandas nullable integer type. - Fill missing values in \'nullable_int\' with the mean of the column. 5. **String Operations**: - Convert the \'description\' column to the pandas String type. - Strip leading and trailing whitespace from all values in \'description\'. 6. **Sparse Data**: - Convert the \'sparse_values\' column to a pandas SparseArray. - Fill missing values in \'sparse_values\' with zero. Constraints: - The input DataFrame (`df`) is guaranteed to have the columns: \'event_start\', \'event_end\', \'category\', \'nullable_int\', \'description\', and \'sparse_values\'. - The \'event_start\' and \'event_end\' columns are in a string format that can be parsed into datetime. - The \'category\' column contains string values. - The \'nullable_int\' column contains integers with missing values. - The \'description\' column contains text data with potential leading or trailing whitespace. - The \'sparse_values\' column contains numeric data with missing values and it is sparse data predominantly filled with zeros. Input Format: - `df` (pandas.DataFrame): A DataFrame object with columns \'event_start\', \'event_end\', \'category\', \'nullable_int\', \'description\', and \'sparse_values\'. Output Format: - Returns a pandas DataFrame with the appropriate transformations applied based on the given requirements. Example: ```python import pandas as pd # Sample input data data = { \'event_start\': [\'2023-01-01 10:00\', \'2023-01-02 11:00\', \'2023-01-03 12:00\'], \'event_end\': [\'2023-01-01 12:00\', \'2023-01-02 13:30\', \'2023-01-03 12:45\'], \'category\': [\'A\', \'B\', \'A\'], \'nullable_int\': [1, None, 3], \'description\': [\' Event Start \', \'Event in the middle \', \' Event End \'], \'sparse_values\': [0.0, None, 0.0] } df = pd.DataFrame(data) output_df = process_data(df) print(output_df) ``` Expected output: ```python event_start event_end category nullable_int description sparse_values day_of_week event_duration 0 2023-01-01 10:00:00+00:00 2023-01-01 12:00:00+00:00 A 1 Event Start 0.0 Sunday 7200 1 2023-01-02 11:00:00+00:00 2023-01-02 13:30:00+00:00 B 2 Event in the middle 0.0 Monday 9000 2 2023-01-03 12:00:00+00:00 2023-01-03 12:45:00+00:00 A 3 Event End 0.0 Tuesday 2700 ``` Notes: - Ensure that all transformations are applied correctly, and the resultant DataFrame maintains the integrity of the data types and values as defined.","solution":"import pandas as pd def process_data(df: pd.DataFrame) -> pd.DataFrame: # Convert \'event_start\' to datetime with UTC timezone df[\'event_start\'] = pd.to_datetime(df[\'event_start\']).dt.tz_localize(\'UTC\') # Extract day of the week from \'event_start\' df[\'day_of_week\'] = df[\'event_start\'].dt.day_name() # Calculate the difference between \'event_end\' and \'event_start\' in seconds df[\'event_end\'] = pd.to_datetime(df[\'event_end\']).dt.tz_localize(\'UTC\') df[\'event_duration\'] = (df[\'event_end\'] - df[\'event_start\']).dt.total_seconds() # Convert \'category\' to a categorical type with the categories ordered alphabetically df[\'category\'] = df[\'category\'].astype(\'category\').cat.as_ordered() df[\'category\'] = df[\'category\'].cat.add_categories(\'Other\') # Ensure \'nullable_int\' column has a pandas nullable integer type df[\'nullable_int\'] = df[\'nullable_int\'].astype(\'Int64\') # Fill missing values in \'nullable_int\' with the mean of the column nullable_int_mean = df[\'nullable_int\'].mean() df[\'nullable_int\'] = df[\'nullable_int\'].fillna(nullable_int_mean) # Convert \'description\' column to pandas string type and strip leading/trailing whitespace df[\'description\'] = df[\'description\'].astype(\'string\') df[\'description\'] = df[\'description\'].str.strip() # Convert \'sparse_values\' to pandas SparseArray and fill missing values with zero df[\'sparse_values\'] = pd.arrays.SparseArray(df[\'sparse_values\'].fillna(0)) return df"},{"question":"# Question: Implement a Multi-Threaded Templating System Your task is to implement a multi-threaded templating system using Python that formats a given text using data provided, with each template processing running in its own thread. This will help leverage the power of concurrent processing for potentially better performance in a multiple template substitutions scenario. Requirements: 1. **Template Substitution:** - Use the `string` module\'s `Template` class to perform text substitutions. - Each template string will use placeholders marked by ``, which will need to be substituted by the values provided in a dictionary. 2. **Multi-threading:** - Implement multi-threading using the `threading` module. - Each template processing should run in its own thread. - Ensure the main program waits for all threads to complete before exiting. 3. **Input Format:** - A list of tuples where each tuple contains a template string and a dictionary for substitution. ```python inputs = [ (\\"Dear name, your balance is balance.\\", {\\"name\\": \\"Alice\\", \\"balance\\": \\"100\\"}), (\\"Hello username! Welcome to platform.\\", {\\"username\\": \\"Bob\\", \\"platform\\": \\"CodeWorld\\"}) ] ``` 4. **Output Format:** - Print the substituted string for each template. Example: ```python from string import Template import threading class TemplateProcessor(threading.Thread): def __init__(self, template, data): threading.Thread.__init__(self) self.template = template self.data = data def run(self): t = Template(self.template) result = t.safe_substitute(self.data) print(result) # Inputs inputs = [ (\\"Dear name, your balance is balance.\\", {\\"name\\": \\"Alice\\", \\"balance\\": \\"100\\"}), (\\"Hello username! Welcome to platform.\\", {\\"username\\": \\"Bob\\", \\"platform\\": \\"CodeWorld\\"}) ] # Creating and starting threads threads = [] for template, data in inputs: processor = TemplateProcessor(template, data) processor.start() threads.append(processor) # Waiting for all threads to complete for thread in threads: thread.join() ``` Constraints: - You can assume that the list of inputs will not exceed 100 entries. - Each entry\'s dictionary will contain all necessary placeholders required by the template string. Your implementation should handle multi-threading efficiently and ensure thread-safe operations where necessary.","solution":"from string import Template import threading class TemplateProcessor(threading.Thread): def __init__(self, template, data): threading.Thread.__init__(self) self.template = template self.data = data def run(self): t = Template(self.template) result = t.safe_substitute(self.data) print(result) def process_templates(inputs): threads = [] for template, data in inputs: processor = TemplateProcessor(template, data) processor.start() threads.append(processor) for thread in threads: thread.join()"},{"question":"Objective Design a Python function that mimics the behavior of Python closures using `cell` objects. Your task is to create a mechanism that stores variables and allows for their retrieval and modification using the provided cell object functions. Problem Statement Implement a Python class `ClosureCell` that uses cell objects to store and manage variables. You need to implement the following methods within this class: 1. **`__init__(self, value: Any)`**: Initialize the `ClosureCell` object with an initial value. 2. **`get_value(self) -> Any`**: Retrieve the current value stored in the cell. 3. **`set_value(self, new_value: Any)`**: Update the value stored in the cell. 4. **`check_is_cell(self) -> bool`**: Check if the object is indeed a cell object. Requirements: - Use the functions `PyCell_New`, `PyCell_Get`, `PyCell_SET`, and `PyCell_Check` to manage the cell object. - Ensure proper error handling for non-cell objects or invalid operations. - The implementation should handle `NULL` values correctly as per the documentation. Constraints: - You are not allowed to use any external libraries except the built-in ones. - The `new_value` can be any JSON-serializable type (int, float, str, list, dict, etc.). Example: ```python # Example Usage closure_cell = ClosureCell(10) # Retrieving value print(closure_cell.get_value()) # Output: 10 # Checking if it\'s a cell object print(closure_cell.check_is_cell()) # Output: True # Updating the value closure_cell.set_value(20) print(closure_cell.get_value()) # Output: 20 # Setting to NULL closure_cell.set_value(None) print(closure_cell.get_value()) # Output: None ``` Evaluation Criteria: - **Correctness**: The implementation should accurately mimic the behavior described. - **Code Quality**: The code should be well-organized, readable, and documented. - **Error Handling**: The code should handle edge cases and invalid inputs gracefully.","solution":"from ctypes import py_object import ctypes class ClosureCell: def __init__(self, value): self.cell = self.PyCell_New(value) def get_value(self): return self.PyCell_Get(self.cell) def set_value(self, new_value): self.PyCell_Set(self.cell, new_value) def check_is_cell(self): return self.PyCell_Check(self.cell) @staticmethod def PyCell_New(value): # PyCell_New will create a new cell object containing the specified value. cell = (ctypes.py_object * 1)() cell[0] = value return cell @staticmethod def PyCell_Get(cell): # PyCell_Get returns the value contained in the cell. return cell[0] @staticmethod def PyCell_Set(cell, new_value): # PyCell_Set sets the new value in the cell. cell[0] = new_value @staticmethod def PyCell_Check(cell): # PyCell_Check will check if the given cell is indeed a cell object. return isinstance(cell, (ctypes.Array,)) # Example Usage if __name__ == \\"__main__\\": closure_cell = ClosureCell(10) print(closure_cell.get_value()) # Output: 10 print(closure_cell.check_is_cell()) # Output: True closure_cell.set_value(20) print(closure_cell.get_value()) # Output: 20 closure_cell.set_value(None) print(closure_cell.get_value()) # Output: None"},{"question":"Objective Write a Python program that generates all possible unique combinations of selecting `n` items from a list of integers and then filters out those combinations based on a given condition. The filtered combinations will subsequently be processed using a specific mathematical operation. Details 1. **Input:** - A list of integers called `numbers`. - An integer `n` specifying the number of items to select. - A filtering function that takes an integer and returns a boolean. - A mathematical operation to apply to each element in the filtered combinations list. 2. **Output:** - A list of tuples, where each tuple contains the unique combinations that satisfy the filtering condition, with the mathematical operation applied to each element. Steps: 1. Use `itertools.combinations()` to generate all possible combinations of `n` items from the input list `numbers`. 2. Use `functools.partial` to create partial functions for the filtering criterion and the mathematical operation. 3. Use the filtering function to filter combinations. 4. Apply the mathematical operation using the `operator` module functions to each element in the filtered combination. Constraints: - The size of the numbers list will not exceed 20. - The value of `n` will be between 1 and the length of `numbers`. - Ensure the solution is efficient in terms of computation and memory usage. Example ```python from itertools import combinations from functools import partial import operator def generate_filtered_combinations(numbers, n, filter_func, math_op): # Generate all combinations comb = combinations(numbers, n) # Create partial functions for filtering and operation filtered = filter(partial(filter_func_check, filter_func=filter_func), comb) result = [] for group in filtered: result.append(tuple(map(partial(math_op_apply, math_op=math_op), group))) return result # Auxiliary functions for filtering and applying operations def filter_func_check(item, filter_func): return all(filter_func(x) for x in item) def math_op_apply(x, math_op): return math_op(x) # Example Usage: numbers = [1, 2, 3, 4, 5] n = 3 filter_func = lambda x: x % 2 == 0 # keep even numbers only math_op = operator.mul # apply multiplication (e.g., double the numbers) filtered_combinations = generate_filtered_combinations(numbers, n, filter_func, lambda x: x * 2) print(filtered_combinations) ``` Explanation: - The `generate_filtered_combinations()` function will generate all combinations, filter them based on the provided filtering function, and then apply the mathematical operation to each element in the remaining combinations. - This approach demonstrates the use of `itertools` for combinations, `functools.partial` for creating partial function applications, and `operator` for applying mathematical operations functionally.","solution":"from itertools import combinations from functools import partial def generate_filtered_combinations(numbers, n, filter_func, math_op): # Generate all combinations of n items from the list comb_list = list(combinations(numbers, n)) # Filter combinations based on the filter function filtered_combinations = filter(lambda combo: all(filter_func(x) for x in combo), comb_list) # Apply the mathematical operation to each element in the filtered combinations result = [] for combo in filtered_combinations: processed_combo = tuple(math_op(x) for x in combo) result.append(processed_combo) return result"},{"question":"# Asynchronous Chat Server with `asyncio` Event Loop # Objective: Design and implement an asynchronous chat server using Python\'s `asyncio` event loop. Your task is to handle multiple client connections, manage messages asynchronously, and implement a way to broadcast messages to all connected clients. # Specifications: 1. **Asynchronous Chat Server**: - The server should be able to handle multiple clients simultaneously. - Each connected client should have a unique identifier (e.g., username). 2. **Client Connections**: - Implement functions to handle new connections using the `loop.create_server()` method. - Handle proper connection and disconnection of clients. 3. **Message Management**: - Design a mechanism to receive and broadcast messages from/to clients. - Each message should contain the sender\'s identifier. - Broadcast messages to all connected clients except the sender. 4. **Graceful Shutdown**: - Implement a way to gracefully shutdown the server using signal handlers for `SIGINT` and `SIGTERM`. # Requirements: - **Function Implementation**: - Implement at least three core functions for managing connections, receiving messages, and broadcasting messages. - Use appropriate `asyncio` methods for scheduling callbacks and managing tasks. # Input and Output: - **Input**: Client connections and their messages. - **Output**: Broadcasted messages to all connected clients. # Performance: - Efficiently handle multiple simultaneous connections and messages. - Ensure non-blocking behavior and proper resource management. Example: ```python import asyncio import signal clients = {} async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') username = await reader.read(100).decode() clients[username] = writer try: while data := await reader.read(100): message = f\\"{username}: {data.decode()}\\" await broadcast_message(message, username) except asyncio.CancelledError: print(f\\"{username} disconnected.\\") finally: del clients[username] writer.close() await writer.wait_closed() async def broadcast_message(message, sender): for username, writer in clients.items(): if username != sender: writer.write(message.encode()) await writer.drain() async def main(host, port): server = await asyncio.start_server(handle_client, host, port) loop = asyncio.get_running_loop() for signame in {\\"SIGINT\\", \\"SIGTERM\\"}: loop.add_signal_handler(getattr(signal, signame), loop.stop) async with server: await server.serve_forever() if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8888 asyncio.run(main(host, port)) ``` Explanation: 1. **handle_client**: Manages individual client connections and messages. 2. **broadcast_message**: Sends messages to all clients except the sender. 3. **main**: Sets up the server and signal handlers for graceful shutdown. Implement this example, ensuring all requirements are met. Modify and extend the example where necessary to fulfill the task\'s constraints and objectives.","solution":"import asyncio import signal clients = {} async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') username = (await reader.read(100)).decode().strip() clients[username] = writer try: while data := await reader.read(100): message = f\\"{username}: {data.decode().strip()}\\" await broadcast_message(message, username) except asyncio.CancelledError: print(f\\"{username} disconnected.\\") except Exception as e: print(f\\"Error handling client {username}: {e}\\") finally: del clients[username] writer.close() await writer.wait_closed() async def broadcast_message(message, sender): for username, writer in clients.items(): if username != sender: writer.write((message + \'n\').encode()) await writer.drain() async def main(host, port): server = await asyncio.start_server(handle_client, host, port) loop = asyncio.get_running_loop() for signame in {\\"SIGINT\\", \\"SIGTERM\\"}: loop.add_signal_handler(getattr(signal, signame), loop.stop) async with server: await server.serve_forever() if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8888 asyncio.run(main(host, port))"},{"question":"# PyTorch Coding Assessment **Objective:** Demonstrate understanding of PyTorch and TorchScript compatibility, paying attention to unsupported constructs and divergence in function behavior between Python and TorchScript. **Problem Statement:** You need to implement a function that computes the pairwise Euclidean distance between rows of two 2D tensors, `A` and `B`, and returns the result as a 2D tensor. The solution must be TorchScript compatible, avoiding unsupported constructs and following the specific TorchScript function schemas as described. **Function Signature:** ```python import torch from torch.jit import script @script def pairwise_euclidean_distance(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: # Your implementation here ``` **Inputs:** - `A` (torch.Tensor): A 2D tensor of shape `(m, d)`. - `B` (torch.Tensor): A 2D tensor of shape `(n, d)`. **Outputs:** - Returns a 2D tensor of shape `(m, n)`, where the element at position `(i, j)` corresponds to the Euclidean distance between the `i-th` row of tensor `A` and the `j-th` row of tensor `B`. **Constraints:** - Do not use any TorchScript unsupported constructs. - Ensure that the implementation is both correct and efficient. **Example:** ```python A = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) B = torch.tensor([[5.0, 6.0], [7.0, 8.0]]) result = pairwise_euclidean_distance(A, B) print(result) # Output: tensor([[5.6569, 8.4853], [2.8284, 5.6569]]) ``` **Notes:** - You may use standard mathematical operations and tensor operations supported by TorchScript. - Ensure that your implementation does not use any functions or classes from the provided list of unsupported constructs or divergence categories in the documentation provided. **Performance Requirement:** - The function should have a time complexity of `O(m * n * d)`, where `m` is the number of rows in `A`, `n` is the number of rows in `B`, and `d` is the number of columns in `A` and `B`.","solution":"import torch from torch import jit @jit.script def pairwise_euclidean_distance(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Computes the pairwise Euclidean distance between rows of two 2D tensors, A and B. Parameters: A (torch.Tensor): A 2D tensor of shape (m, d). B (torch.Tensor): A 2D tensor of shape (n, d). Returns: torch.Tensor: A 2D tensor of shape (m, n), where the element at position (i, j) corresponds to the Euclidean distance between the i-th row of tensor A and the j-th row of tensor B. # Compute the squared differences diff_sq = (A[:, None, :] - B[None, :, :]) ** 2 # Sum the squared differences along the last dimension dist_sq = diff_sq.sum(dim=2) # Take the square root to get the Euclidean distance dist = torch.sqrt(dist_sq) return dist"},{"question":"Advanced Pandas Configuration Objective Demonstrate your understanding of the pandas options and settings API by writing code to solve the following task. Task You are given a DataFrame `df` containing random numerical data. Your task is to configure various pandas display options, perform specific operations, and reset the options to their default values. The detailed steps are as follows: 1. Set the display options of pandas to configure: - Maximum number of rows displayed (`display.max_rows`) to 5. - Maximum number of columns displayed (`display.max_columns`) to 3. - Display precision for floating-point numbers (`display.precision`) to 3. - Column header justification (`display.colheader_justify`) to \'right\'. 2. Create a sample DataFrame `df` with 10 rows and 5 columns using `numpy.random.randn` to generate random data. 3. Perform a context-based operation which displays the DataFrame: - Temporarily set the `display.expand_frame_repr` option to `True` when displaying `df`, ensuring that the DataFrame can stretch across the entire width. 4. After exiting the context manager, create another DataFrame `df2` with 7 rows and 3 columns using `numpy.random.randn` and display it. 5. Finally, reset all of the pandas display options configured in step 1 back to their default values. Expected Input and Output Formats - Input: No inputs are required; you will be working with pandas options and data generated within the script. - Output: The script should display the DataFrames as specified in steps 3 and 4, reflecting the changed pandas display options. Constraints - Ensure that the options set in step 1 are applied globally and the options used in the context manager in step 3 are temporary. - After resetting the options, the default settings should be restored. Performance Requirements - The operations should be performed efficiently without redundant code. Example Code Structure ```python import numpy as np import pandas as pd # Step 1: Set display options pd.set_option(\'display.max_rows\', 5) pd.set_option(\'display.max_columns\', 3) pd.set_option(\'display.precision\', 3) pd.set_option(\'display.colheader_justify\', \'right\') # Step 2: Create sample DataFrame df data = np.random.randn(10, 5) df = pd.DataFrame(data) # Step 3: Context-based operation for displaying df with expanded frame with pd.option_context(\'display.expand_frame_repr\', True): print(df) # Step 4: Create another DataFrame df2 and display it data2 = np.random.randn(7, 3) df2 = pd.DataFrame(data2) print(df2) # Step 5: Reset all display options to their default values pd.reset_option(\'display.max_rows\') pd.reset_option(\'display.max_columns\') pd.reset_option(\'display.precision\') pd.reset_option(\'display.colheader_justify\') ``` Ensure that your solution follows the steps and constraints provided. Good luck!","solution":"import numpy as np import pandas as pd def configure_display_options(): # Step 1: Set display options pd.set_option(\'display.max_rows\', 5) pd.set_option(\'display.max_columns\', 3) pd.set_option(\'display.precision\', 3) pd.set_option(\'display.colheader_justify\', \'right\') def reset_display_options(): # Reset all display options to their default values pd.reset_option(\'display.max_rows\') pd.reset_option(\'display.max_columns\') pd.reset_option(\'display.precision\') pd.reset_option(\'display.colheader_justify\') def create_sample_df(rows, cols): return pd.DataFrame(np.random.randn(rows, cols)) def main(): # Step 1: Configure display options configure_display_options() # Step 2: Create sample DataFrame df df = create_sample_df(10, 5) # Step 3: Context-based operation for displaying df with expanded frame with pd.option_context(\'display.expand_frame_repr\', True): print(df) # Step 4: Create another DataFrame df2 and display it df2 = create_sample_df(7, 3) print(df2) # Step 5: Reset display options to default reset_display_options()"},{"question":"**Problem Statement:** You are required to create a utility that manages ZIP archives using Pythonâ€™s `zipfile` module. Your utility should be able to create a new ZIP archive, list the contents of an existing ZIP archive, add files to an existing ZIP archive, and extract files from a ZIP archive. You need to implement the following functions: 1. **create_zip(zip_name: str, files: List[str]) -> None** - **Input:** - `zip_name` (str): The name (including path) of the new ZIP file to be created. - `files` (List[str]): A list of file paths to be included in the ZIP file. - **Output:** None - **Description:** Create a new ZIP file with the given `zip_name` and add all the files specified in the `files` list using the `ZIP_DEFLATED` compression method. 2. **list_contents(zip_name: str) -> List[str]** - **Input:** - `zip_name` (str): The name (including path) of an existing ZIP file. - **Output:** - List[str]: A list of file names contained in the ZIP archive. - **Description:** Return the list of all file names contained in the specified ZIP archive. 3. **add_files(zip_name: str, files: List[str]) -> None** - **Input:** - `zip_name` (str): The name (including path) of an existing ZIP file. - `files` (List[str]): A list of file paths to be added to the existing ZIP file. - **Output:** None - **Description:** Add all the files specified in the `files` list to the specified ZIP archive using the `ZIP_DEFLATED` compression method. 4. **extract_files(zip_name: str, extract_path: str, file_list: Optional[List[str]] = None) -> None** - **Input:** - `zip_name` (str): The name (including path) of an existing ZIP file. - `extract_path` (str): The directory path where files should be extracted. - `file_list` (Optional[List[str]]): A list of specific file names to be extracted. If None, extract all files. Default is None. - **Output:** None - **Description:** Extract the specified files from the ZIP archive to the given `extract_path`. If `file_list` is None, extract all files from the archive. **Constraints:** - You can assume that all file paths provided are valid and accessible. - Handle any potential exceptions that may arise, such as a non-existing ZIP file, file not found in the archive, etc., with appropriate error messages. **Example Usage:** ```python # Example files to be added files_to_add = [\\"file1.txt\\", \\"file2.txt\\", \\"image.png\\"] # Create a new ZIP archive and add files create_zip(\\"test.zip\\", files_to_add) # List contents of the ZIP archive print(list_contents(\\"test.zip\\")) # Output: [\'file1.txt\', \'file2.txt\', \'image.png\'] # Add more files to the ZIP archive more_files_to_add = [\\"document.pdf\\", \\"data.csv\\"] add_files(\\"test.zip\\", more_files_to_add) # List contents after adding more files print(list_contents(\\"test.zip\\")) # Output: [\'file1.txt\', \'file2.txt\', \'image.png\', \'document.pdf\', \'data.csv\'] # Extract specific files from the ZIP archive extract_files(\\"test.zip\\", \\"extracted_files\\", [\\"file1.txt\\", \\"image.png\\"]) # Extract all files from the ZIP archive extract_files(\\"test.zip\\", \\"all_extracted_files\\") ``` Implement the four functions specified above to complete the ZIP archive utility.","solution":"import zipfile from typing import List, Optional def create_zip(zip_name: str, files: List[str]) -> None: Create a new ZIP file with the given zip_name and add all the files specified in the files list using the ZIP_DEFLATED compression method. try: with zipfile.ZipFile(zip_name, \'w\', zipfile.ZIP_DEFLATED) as zipf: for file in files: zipf.write(file) except Exception as e: print(f\\"Error creating ZIP file: {e}\\") def list_contents(zip_name: str) -> List[str]: Return the list of all file names contained in the specified ZIP archive. try: with zipfile.ZipFile(zip_name, \'r\') as zipf: return zipf.namelist() except Exception as e: print(f\\"Error listing contents: {e}\\") return [] def add_files(zip_name: str, files: List[str]) -> None: Add all the files specified in the files list to the specified ZIP archive using the ZIP_DEFLATED compression method. try: with zipfile.ZipFile(zip_name, \'a\', zipfile.ZIP_DEFLATED) as zipf: for file in files: zipf.write(file) except Exception as e: print(f\\"Error adding files: {e}\\") def extract_files(zip_name: str, extract_path: str, file_list: Optional[List[str]] = None) -> None: Extract the specified files from the ZIP archive to the given extract_path. If file_list is None, extract all files from the archive. try: with zipfile.ZipFile(zip_name, \'r\') as zipf: if file_list is None: zipf.extractall(extract_path) else: for file in file_list: zipf.extract(file, extract_path) except Exception as e: print(f\\"Error extracting files: {e}\\")"},{"question":"# Question: Text Transformation Pipeline You are required to create a text transformation pipeline that processes a text file and performs a series of operations on its contents. Each operation in the pipeline should be a shell command. Task: 1. **Reading Input**: - Create a text file named `input.txt` with the following content: ``` this is a sample text for the coding assessment. try to solve this problem using the pipes module. ``` 2. **Processing**: - Create a pipeline using the `pipes.Template` class to perform the following operations sequentially: 1. Convert all text to uppercase. 2. Replace spaces with underscores. 3. Sort the lines alphabetically. 3. **Writing Output**: - Write the final processed content to a new text file named `output.txt`. Constraints: - You must use the `pipes.Template` class and its methods to create the pipeline. - Only use shell commands within the `append` or `prepend` methods. - Assume that the file paths are relative to the current working directory. Expected Output: The content of the `output.txt` file should be: ``` THIS_IS_A_SAMPLE_TEXT_FOR_THE_CODING_ASSESSMENT. TRY_TO_SOLVE_THIS_PROBLEM_USING_THE_PIPES_MODULE. ``` Hints: - Use the `tr` command to convert text to uppercase. - Use the `sed` command to replace spaces with underscores. - Use the `sort` command to sort the lines. # Notes: - You can create and open files in Python using the built-in `open` function. - Make sure to handle file operations correctly to avoid resource leaks. ```python import pipes # Step 1: Create and write to \'input.txt\' with open(\'input.txt\', \'w\') as f: f.write(\'this is a sample text for the coding assessment.n\') f.write(\'try to solve this problem using the pipes module.n\') # Step 2: Create the pipeline t = pipes.Template() # Add command to convert text to uppercase t.append(\'tr a-z A-Z\', \'--\') # Add command to replace spaces with underscores t.append(\'sed \\"s/ /_/g\\"\', \'--\') # Add command to sort the lines t.append(\'sort\', \'--\') # Step 3: Process the input file and write to \'output.txt\' with t.open(\'output.txt\', \'w\') as f: with open(\'input.txt\', \'r\') as infile: for line in infile: f.write(line) # Your \'output.txt\' should now contain the transformed text ```","solution":"import pipes # Step 1: Create and write to \'input.txt\' with open(\'input.txt\', \'w\') as f: f.write(\'this is a sample text for the coding assessment.n\') f.write(\'try to solve this problem using the pipes module.n\') # Step 2: Create the pipeline t = pipes.Template() # Add command to convert text to uppercase t.append(\'tr a-z A-Z\', \'--\') # Add command to replace spaces with underscores t.append(\'sed \\"s/ /_/g\\"\', \'--\') # Add command to sort the lines t.append(\'sort\', \'--\') # Step 3: Process the input file and write to \'output.txt\' with t.open(\'output.txt\', \'w\') as f: with open(\'input.txt\', \'r\') as infile: f.write(infile.read())"},{"question":"# Custom Exception Implementation and Handling You are asked to implement a custom hierarchical exception handling system for a hypothetical library that processes data files. The objectives are as follows: 1. **Define Custom Exceptions:** - Implement the following custom exceptions and their hierarchy: - `DataError` (inherits from `Exception`) - `FileFormatError` (inherits from `DataError`) - `FileEmptyError` (inherits from `DataError`) - `CorruptDataError` (inherits from `DataError`) 2. **Simulate Error Scenarios:** - Create a `process_file` function that mimics file processing, which raises these exceptions under certain conditions: - `FileFormatError` if the file format is incorrect. - `FileEmptyError` if the file is empty. - `CorruptDataError` if the file is corrupted during processing. 3. **Exception Handling and Chaining:** - Implement a `run_processing` function that: - Calls `process_file`. - Handles exceptions using `try-except` blocks. - When a `FileFormatError` is caught, it should raise a `FileEmptyError` with the original exception as the cause. 4. **Output Traceback Information:** - Ensure that the exception chaining is correctly set, and print relevant traceback information to display the chain of exceptions. # Function Signatures ```python class DataError(Exception): pass class FileFormatError(DataError): pass class FileEmptyError(DataError): pass class CorruptDataError(DataError): pass def process_file(file_content: str) -> None: Simulates the processing of a data file. Raises appropriate exceptions based on the content. # Your implementation here def run_processing(file_content: str) -> None: Runs the file processing and handles exceptions. Chains exceptions when a FileFormatError occurs. Prints traceback information. # Your implementation here # Example Usages try: run_processing(file_content=\\"\\") except DataError as e: print(f\\"Caught an exception: {e}\\") try: run_processing(file_content=\\"corrupt\\") except DataError as e: print(f\\"Caught an exception: {e}\\") ``` # Expected Behavior - When `run_processing` is called with an empty string, it should raise `FileEmptyError`. - If called with a string that simulates corruption (e.g., \\"corrupt\\"), it should raise `CorruptDataError`. - If called with an invalid format (you can define this condition - e.g., a specific keyword that represents a format error), it should raise a `FileFormatError` which in turn raises `FileEmptyError` with chaining. # Constraints - Use and demonstrate exception chaining using `__cause__`. - Print the traceback information to display the sequence of exceptions. Please implement the functions and classes as described above. Ensure that your code is clear, efficient, and adheres to best practices for exception handling in Python.","solution":"class DataError(Exception): pass class FileFormatError(DataError): pass class FileEmptyError(DataError): pass class CorruptDataError(DataError): pass def process_file(file_content: str) -> None: Simulates the processing of a data file. Raises appropriate exceptions based on the content. if file_content == \\"\\": raise FileEmptyError(\\"The file is empty.\\") elif file_content == \\"corrupt\\": raise CorruptDataError(\\"The file is corrupted.\\") elif file_content == \\"invalid_format\\": raise FileFormatError(\\"The file format is incorrect.\\") def run_processing(file_content: str) -> None: Runs the file processing and handles exceptions. Chains exceptions when a FileFormatError occurs. Prints traceback information. try: process_file(file_content) except FileFormatError as e: raise FileEmptyError(\\"File format error occurred, treating as empty file.\\") from e # Example usage try: run_processing(\\"\\") except DataError as e: import traceback traceback.print_exception(type(e), e, e.__traceback__) try: run_processing(\\"corrupt\\") except DataError as e: import traceback traceback.print_exception(type(e), e, e.__traceback__) try: run_processing(\\"invalid_format\\") except DataError as e: import traceback traceback.print_exception(type(e), e, e.__traceback__)"},{"question":"# Temporary File Handling with `tempfile` Module You are tasked with creating a Python function that processes a large amount of data and temporarily stores the processed data without overwhelming the system memory. Your objective is to utilize the `tempfile` module to achieve this. **Task**: Implement a function `process_large_data(data: List[str], max_mem_size: int) -> List[str]` that processes the given list of strings `data`. The function should: 1. Use an in-memory buffer until the data size exceeds `max_mem_size`. 2. Overflow the excess data into a temporary file using `tempfile.SpooledTemporaryFile`. 3. Ensure that all the data can be read back in the original order. When the `max_mem_size` is zero, the function should directly store all data in a temporary file. **Function Signature**: ```python def process_large_data(data: List[str], max_mem_size: int) -> List[str]: ``` **Arguments**: - `data`: List of strings representing the data to be processed. - `max_mem_size`: An integer representing the maximum size (in bytes) that should be kept in memory. **Returns**: - A list of strings representing the processed data in the same order they were input. **Constraints**: - You should use the `tempfile.SpooledTemporaryFile` for handling the data overflow. - Ensure that the temporary file is cleaned up automatically after use. - Write and read operations should respect the original data order. **Example**: ```python def main(): data = [\\"string1\\", \\"string2\\", \\"string3\\", \\"string4\\"] result = process_large_data(data, max_mem_size=1024) # assuming each string is less than 1024 bytes print(result) # should output: [\\"string1\\", \\"string2\\", \\"string3\\", \\"string4\\"] if __name__ == \\"__main__\\": main() ``` In this example, the function should buffer the strings until `max_mem_size` is reached, and then overflow into a temporary file. Finally, it should read and return all the strings in the original order. You can assume the following imports are provided: ```python from typing import List import tempfile ```","solution":"from typing import List import tempfile def process_large_data(data: List[str], max_mem_size: int) -> List[str]: with tempfile.SpooledTemporaryFile(max_size=max_mem_size, mode=\'w+t\') as temp_file: for item in data: temp_file.write(item + \\"n\\") temp_file.seek(0) processed_data = temp_file.readlines() return [line.strip() for line in processed_data]"},{"question":"<|Analysis Begin|> The provided documentation is for the `email.generator` module in Python. This module is used to generate MIME (Multipurpose Internet Mail Extensions) documents, which are the standard for formatting email messages. The documentation includes details about several classes within the module, such as `BytesGenerator`, `Generator`, and `DecodedGenerator`. Here are some key aspects of the `email.generator` module: 1. **Purpose**: The module is designed to flatten MIME message objects into a serialized text or binary representation suitable for transmission over channels that are not \\"8-bit clean\\". 2. **Class Overview**: - `BytesGenerator`: Produces a binary serialized representation of a message. - `Generator`: Produces a text (ASCII) serialized representation of a message. - `DecodedGenerator`: Extends `Generator` to handle non-text MIME parts by substituting their contents with a formatted string. 3. **Functionalities**: - **BytesGenerator and Generator**: Both classes have methods `flatten()` to serialize the message object and `clone()` to create a clone of the instance with a different output stream. - **Convenience Methods**: `EmailMessage` objects have methods like `as_bytes()` and `as_string()` to facilitate the generation of serializations. The module supports a variety of configurations, such as header mangling with the `mangle_from_` parameter, maximum header length with `maxheaderlen`, and customizable policies for message generation. <|Analysis End|> <|Question Begin|> # Problem Statement You are tasked with creating a specialized email generator that can serialize email message objects conforming to specific standards. Your generator should extend the `email.generator.BytesGenerator` class to add a custom header to each email message. This task will test your understanding of inheritance, method overriding, and working with Python\'s `email` module. # Objective Create a new class `CustomBytesGenerator` that inherits from `email.generator.BytesGenerator`. Override the `flatten()` method to include a new header, `X-Custom-Header`, which contains a custom string before serializing the rest of the message. # Specifications - **Class Name**: `CustomBytesGenerator` - **Base Class**: `email.generator.BytesGenerator` - **Method to Override**: `flatten(msg, unixfrom=False, linesep=None)` - The `msg` parameter will be an instance of `email.message.EmailMessage`. - The `unixfrom` and `linesep` parameters should be passed to the base class method. # Requirements 1. **Initialization**: Initialize the `CustomBytesGenerator` with the same parameters as `BytesGenerator`. 2. **Custom Header**: Add a custom header `X-Custom-Header` with the value `\\"Generated by CustomBytesGenerator\\"` to the message before calling the base class\'s `flatten()` method. 3. **Serialization**: Ensure the rest of the email message is serialized correctly using the `BytesGenerator`\'s `flatten()` method. # Example ```python import email from email.generator import BytesGenerator class CustomBytesGenerator(BytesGenerator): def flatten(self, msg, unixfrom=False, linesep=None): # Add the custom header msg[\'X-Custom-Header\'] = \\"Generated by CustomBytesGenerator\\" # Call the base class\'s flatten method super().flatten(msg, unixfrom=unixfrom, linesep=linesep) # Usage example if __name__ == \\"__main__\\": from email.message import EmailMessage from io import BytesIO # Create an email message msg = EmailMessage() msg[\'Subject\'] = \\"Test Email\\" msg.set_content(\\"This is a test email.\\") # Create a BytesIO object to act as a file-like output stream out = BytesIO() # Initialize CustomBytesGenerator with the output stream gen = CustomBytesGenerator(out) # Serialize the email message gen.flatten(msg) # Print the result print(out.getvalue().decode(\'utf-8\')) ``` # Input - An instance of `EmailMessage`. # Output - A serialized email message as bytes written to the provided output stream. # Constraints - The custom header should always be added as the first header in the serialized message. - Assume `email.policy.default` is being used unless otherwise specified. - The method `flatten()` must appropriately handle the `unixfrom` and `linesep` parameters as described in the documentation. Implement the `CustomBytesGenerator` class with the specified behavior.","solution":"import email from email.generator import BytesGenerator class CustomBytesGenerator(BytesGenerator): def flatten(self, msg, unixfrom=False, linesep=None): # Add the custom header msg[\'X-Custom-Header\'] = \\"Generated by CustomBytesGenerator\\" # Call the base class\'s flatten method super().flatten(msg, unixfrom=unixfrom, linesep=linesep) # Usage example if __name__ == \\"__main__\\": from email.message import EmailMessage from io import BytesIO # Create an email message msg = EmailMessage() msg[\'Subject\'] = \\"Test Email\\" msg.set_content(\\"This is a test email.\\") # Create a BytesIO object to act as a file-like output stream out = BytesIO() # Initialize CustomBytesGenerator with the output stream gen = CustomBytesGenerator(out) # Serialize the email message gen.flatten(msg) # Print the result print(out.getvalue().decode(\'utf-8\'))"},{"question":"# SMTP Email Sender You are required to implement a Python class `EmailSender` that uses the `smtplib.SMTP` class to send an email with specified subject and body from a provided sender to a list of recipients. The `EmailSender` class should include proper error handling and logging for debugging purposes. Class Definition ```python class EmailSender: def __init__(self, host: str, port: int, sender_email: str, sender_password: str): Initialize the EmailSender instance with SMTP server details and sender\'s login credentials. Parameters: host (str): The SMTP server host. port (int): The SMTP server port. sender_email (str): The email address of the sender. sender_password (str): The password for the sender\'s email. pass def send_email(self, subject: str, body: str, recipient_emails: list): Send an email with the given subject and body to the specified recipient email addresses. Parameters: subject (str): The subject of the email. body (str): The body of the email. recipient_emails (list): List of recipient email addresses. Returns: str: A message indicating success or details of any encountered error. pass ``` Requirements and Constraints 1. **Initialization**: - Take the SMTP server details (`host` and `port`), and the sender\'s login credentials (`sender_email` and `sender_password`). - Use TLS for secure communication. 2. **Email Sending**: - Implement the `send_email` method to send an email with the provided subject and body. - The method should construct the email headers and message format correctly. - Use the provided list of recipient email addresses to send the email. 3. **Exception Handling**: - Handle relevant exceptions like `SMTPAuthenticationError`, `SMTPRecipientsRefused`, `SMTPDataError`, and `SMTPConnectError`. - Log debugging information using `set_debuglevel` for easier issue tracing. 4. **Output**: - Return a success message if the email was sent successfully. - If an error occurs, return a message with the details of the encountered error. Example Usage ```python if __name__ == \\"__main__\\": host = \\"smtp.example.com\\" port = 587 sender_email = \\"sender@example.com\\" sender_password = \\"password\\" email_sender = EmailSender(host, port, sender_email, sender_password) subject = \\"Test Email\\" body = \\"This is a test email sent from Python using smtplib.\\" recipients = [\\"recipient1@example.com\\", \\"recipient2@example.com\\"] result = email_sender.send_email(subject, body, recipients) print(result) ``` In the above example: - An `EmailSender` instance is created with the SMTP server details and sender\'s login credentials. - The `send_email` method is used to send a test email to the specified recipients. - The result of the email sending is printed, indicating success or any error details. Performance Requirements - Ensure the connection is closed after sending the email. - Optimize the connection timeout and retry in case of a transient failure. **Good luck!**","solution":"import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart import logging class EmailSender: def __init__(self, host: str, port: int, sender_email: str, sender_password: str): Initialize the EmailSender instance with SMTP server details and sender\'s login credentials. Parameters: host (str): The SMTP server host. port (int): The SMTP server port. sender_email (str): The email address of the sender. sender_password (str): The password for the sender\'s email. self.host = host self.port = port self.sender_email = sender_email self.sender_password = sender_password self.logger = logging.getLogger(__name__) def send_email(self, subject: str, body: str, recipient_emails: list): Send an email with the given subject and body to the specified recipient email addresses. Parameters: subject (str): The subject of the email. body (str): The body of the email. recipient_emails (list): List of recipient email addresses. Returns: str: A message indicating success or details of any encountered error. try: # Set up the SMTP server connection server = smtplib.SMTP(self.host, self.port) server.set_debuglevel(1) server.starttls() server.login(self.sender_email, self.sender_password) # Create the email msg = MIMEMultipart() msg[\'From\'] = self.sender_email msg[\'To\'] = \', \'.join(recipient_emails) msg[\'Subject\'] = subject msg.attach(MIMEText(body, \'plain\')) # Send the email server.sendmail(self.sender_email, recipient_emails, msg.as_string()) server.quit() self.logger.info(f\\"Email sent successfully to {recipient_emails}\\") return \\"Email sent successfully\\" except smtplib.SMTPAuthenticationError: error_msg = \\"SMTP Authentication Error\\" self.logger.error(error_msg) return error_msg except smtplib.SMTPRecipientsRefused: error_msg = \\"SMTP Recipients Refused\\" self.logger.error(error_msg) return error_msg except smtplib.SMTPDataError: error_msg = \\"SMTP Data Error\\" self.logger.error(error_msg) return error_msg except smtplib.SMTPConnectError: error_msg = \\"SMTP Connect Error\\" self.logger.error(error_msg) return error_msg except Exception as e: error_msg = str(e) self.logger.error(f\\"An unexpected error occurred: {error_msg}\\") return error_msg"},{"question":"# Question Objective Implement a Python function that sets up logging configuration for an application using a dictionary format and the `dictConfig` method from the `logging.config` module. Requirements 1. **Function Name**: `setup_logging` 2. **Parameters**: None. 3. **Functionality**: - The function should configure logging with at least two handlers: a console handler and a file handler. - Each handler should have a formatter. - The console handler should log messages to the standard output with a simple format. - The file handler should log messages to a file named `app.log` with a detailed format including timestamps. - Create a logger named `app` which uses both handlers, with a logging level of `DEBUG`. 4. **Output**: The function should not return anything. Instead, it should set up the logging configuration globally for the application. Constraints - Use the `dictConfig` method for the configuration. - The console handler should use `logging.StreamHandler`. - The file handler should use `logging.FileHandler`. - The log file should be named `app.log`. Example After setting up the logging configuration using the `setup_logging` function, the following code: ```python import logging setup_logging() logger = logging.getLogger(\'app\') logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.error(\'This is an error message\') ``` Should result in output on the console and written to the `app.log` file as follows: Console Output ``` This is a debug message This is an info message This is an error message ``` App.log Content ``` 2023-10-12 07:00:00,000 - app - DEBUG - This is a debug message 2023-10-12 07:00:00,001 - app - INFO - This is an info message 2023-10-12 07:00:00,002 - app - ERROR - This is an error message ``` Code Template ```python import logging import logging.config def setup_logging(): config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(message)s\', }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\', }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', \'filename\': \'app.log\', }, }, \'loggers\': { \'app\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', }, } } logging.config.dictConfig(config) ```","solution":"import logging import logging.config def setup_logging(): Configures logging for the application with a console handler and a file handler. config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(message)s\', }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\', }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', \'filename\': \'app.log\', }, }, \'loggers\': { \'app\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', }, } } logging.config.dictConfig(config)"},{"question":"You are given a dataset that has been loaded using the seaborn package. The dataset `fmri` contains the following columns: - `subject`: Unique identifier for the subject. - `timepoint`: The time point of the fMRI measurement. - `event`: The event type at each time point. - `region`: The brain region measured. - `signal`: The fMRI signal value. You need to visualize the fMRI signal change over time for different regions and events. Specifically, the task involves the following steps: 1. Load the dataset `fmri` using `sns.load_dataset`. 2. Create a line plot showing the fMRI signal over time: - Use `timepoint` for the x-axis and `signal` for the y-axis. - Differentiate lines by `region` using different colors. - Differentiate lines by `event` using different line styles. - Use markers for each data point. - Add error bars representing two standard error widths. 3. Adjust the plot aesthetics: - Set a title for the plot. - Add labels for the x and y axes. - Adjust the legend to be outside the plot area. # Function Signature ```python def visualize_fmri_signals(): pass ``` # Expected Output The function should not return anything. Instead, it should generate a plot that meets the specified requirements. # Constraints and Limitations - You should only use matplotlib/seaborn functions for this task. - Be sure the plot is visually clear and informative. - Focus on using seaborn-specific functionality as much as possible to demonstrate your understanding of the library. # Example Code ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_fmri_signals(): # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Create the line plot plt.figure(figsize=(10, 6)) sns.lineplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, dashes=False, err_style=\\"bars\\", errorbar=(\\"se\\", 2) ) # Adjusting plot aesthetics plt.title(\\"fMRI Signal Over Time\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.legend(loc=\'center left\', bbox_to_anchor=(1, 0.5), title=\\"Region/Event\\") # Show the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_fmri_signals(): Visualizes the fMRI signal changes over time for different regions and events. # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Create the line plot plt.figure(figsize=(10, 6)) sns.lineplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, dashes=False, err_style=\\"bars\\", errorbar=(\\"se\\", 2) ) # Adjusting plot aesthetics plt.title(\\"fMRI Signal Over Time\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.legend(loc=\'center left\', bbox_to_anchor=(1, 0.5), title=\\"Region/Event\\") # Show the plot plt.show()"},{"question":"# Sound File Type Determination using `sndhdr` As a part of a music management system, your task is to write a function that processes a list of sound file paths and returns a summary of the sound file types and their attributes. You will utilize the `sndhdr` module to achieve this goal. Function Specification **Function Name**: `sound_file_summary` **Input**: - `file_paths` (List of strings): A list of file paths to the sound files to be analyzed. **Output**: - A dictionary with sound file types as keys and a list of corresponding tuples as values. Each tuple will contain the file path and the namedtuple returned by `sndhdr.what()`. If a file\'s type cannot be determined, it should be categorized under the key `\'unknown\'`. Example ```python file_paths = [ \'path/to/sound1.wav\', \'path/to/sound2.aifc\', \'path/to/invalidfile.txt\' ] result = sound_file_summary(file_paths) ``` **Expected Result**: ```python { \'wav\': [ (\'path/to/sound1.wav\', namedtuple(filetype=\'wav\', framerate=44100, nchannels=2, nframes=100000, sampwidth=16)) ], \'aifc\': [ (\'path/to/sound2.aifc\', namedtuple(filetype=\'aifc\', framerate=48000, nchannels=2, nframes=50000, sampwidth=16)) ], \'unknown\': [ (\'path/to/invalidfile.txt\', None) ] } ``` Constraints 1. The function should handle any number of file paths. 2. The function should ensure that all files in the list are processed. 3. If a file cannot be determined, it should be placed under the `\'unknown\'` key with its path and a `None` value. **Performance**: - The function should be efficient enough to handle up to 1000 file paths in a reasonable time frame. Additional Notes - You may assume all file paths provided are valid and accessible. - You may use the `sndhdr` module directly as described in the documentation. - Handle any exceptions that might arise and ensure the function does not crash. Implement the `sound_file_summary` function based on the above specification.","solution":"import sndhdr def sound_file_summary(file_paths): Processes a list of sound file paths and returns a summary of the sound file types and their attributes. Parameters: - file_paths (list of strings): List of file paths to the sound files to be analyzed. Returns: - dict: A dictionary with sound file types as keys and a list of corresponding tuples as values. summary = {} for path in file_paths: result = sndhdr.what(path) if result is None: filetype = \'unknown\' else: filetype = result.filetype if filetype not in summary: summary[filetype] = [] summary[filetype].append((path, result)) return summary"},{"question":"**Coding Assessment Question** **Objective:** Create a customized scatter plot using the Seaborn `objects` interface that incorporates multiple facets, jittering, dot customization, and error bars. **Problem Statement:** You are provided with the `tips` dataset from Seaborn. Your task is to generate a scatter plot that displays the relationship between the `total_bill` and `tip` variables, with additional customization and statistical features. The result should be a faceted plot by `day` of the week, with different colors representing the gender (`sex`) of the individuals. Implement jittering to reduce overplotting, and add error bars to show variability in the data. **Steps to Implement:** 1. Load the `tips` dataset using `seaborn.load_dataset(\\"tips\\")`. 2. Create a faceted scatter plot: - Facet the plot by `day` of the week (`facet(\\"day\\")`). - Show the relationship between `total_bill` and `tip`. - Use different colors to represent the gender (`color=\\"sex\\"`). 3. Add jittering to the points to avoid overplotting (`so.Jitter(0.2)`). 4. Customize the dots to have a point size of 5 and an edge color of white (`pointsize=5`, `edgecolor=\\"w\\"`). 5. Add error bars representing 95% confidence intervals (`so.Range`, `so.Est(errorbar=(\\"ci\\", 95))`). **Expected Input and Output:** - Input: None (the dataset is to be loaded within the function) - Output: A faceted scatter plot with the described customizations and features. **Constraints:** - Use the `seaborn.objects` interface only. - Ensure your plot includes all specified facets, colors, jitter, point size, edge color, and error bars. **Performance Requirements:** - The code should efficiently handle the input dataset (`tips` from Seaborn), generate the plot, and display it without significant delays. **Function Signature:** ```python def create_custom_scatter_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Generate the plot plot = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"sex\\") .facet(\\"day\\") .add(so.Dot(pointsize=5, edgecolor=\\"w\\"), so.Jitter(0.2)) .add(so.Range(), so.Est(errorbar=(\\"ci\\", 95))) ) # Display the plot plot.show() ``` **Your Task:** Implement the `create_custom_scatter_plot` function as described above.","solution":"def create_custom_scatter_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Generate the plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .facet(\\"day\\") .add(so.Dot(pointsize=5, edgecolor=\\"w\\"), so.Jitter(0.2)) .add(so.Range(), so.Est(errorbar=(\\"ci\\", 95))) ) # Display the plot plot.show()"},{"question":"Question You are given the task of developing a small utility library in PyTorch for managing CPU contexts and streams. Your solution will involve implementing a class `CpuContextManager`. # Requirements 1. **CpuContextManager Class** - **Attributes:** - `context_stream`: Stores the current CPU Stream. - **Methods:** - `__init__()`: Initialize the class and set `context_stream` to the current CPU stream. - `switch_device(device_id)`: Change CPU device to the specified `device_id` if it\'s available. Raise an error if the `device_id` is outside the range of available devices. - `synchronize()`: Synchronize the current stream. - `device_count()`: Return the number of available devices. - `get_current_device()`: Return the current device ID. - `stream()`: Return the current stream. - `set_stream(stream)`: Set the current stream to the provided `stream`. - `__enter__()`: For context manager; save the current stream and set the provided stream. - `__exit__()`: For context manager; restore the original stream. # Constraints - Assume you have a typical environment where the number of CPU devices is greater than zero. - You should handle exceptions and unexpected situations gracefully. # Example Usage ```python import torch # Initialize the CpuContextManager cpu_manager = CpuContextManager() # Get the number of available devices print(cpu_manager.device_count()) # Output: [some number greater than 0] # Switch to device 0 (assuming it\'s available) cpu_manager.switch_device(0) # Get the current device ID print(cpu_manager.get_current_device()) # Output: 0 # Synchronize the current stream cpu_manager.synchronize() # Use stream management with context manager with CpuContextManager() as ctx_mgr: # Perform operations within this context pass ``` # Implementation ```python import torch class CpuContextManager: def __init__(self): self.context_stream = torch.cpu.current_stream() def switch_device(self, device_id): if device_id < 0 or device_id >= torch.cpu.device_count(): raise ValueError(f\\"Device id {device_id} is not available.\\") torch.cpu.set_device(device_id) def synchronize(self): torch.cpu.synchronize() def device_count(self): return torch.cpu.device_count() def get_current_device(self): return torch.cpu.current_device() def stream(self): return self.context_stream def set_stream(self, stream): self.context_stream = stream def __enter__(self): self.original_stream = torch.cpu.current_stream() torch.cpu.set_stream(self.context_stream) return self def __exit__(self, exc_type, exc_value, traceback): torch.cpu.set_stream(self.original_stream) ``` **Note:** Additional error checking and exception handling can be incorporated to strengthen the utility.","solution":"import torch class CpuContextManager: def __init__(self): self.context_stream = torch.cuda.current_stream() def switch_device(self, device_id): if device_id < 0 or device_id >= torch.cuda.device_count(): raise ValueError(f\\"Device id {device_id} is not available.\\") torch.cuda.set_device(device_id) def synchronize(self): torch.cuda.synchronize() def device_count(self): return torch.cuda.device_count() def get_current_device(self): return torch.cuda.current_device() def stream(self): return self.context_stream def set_stream(self, stream): self.context_stream = stream def __enter__(self): self.original_stream = torch.cuda.current_stream() torch.cuda.set_stream(self.context_stream) return self def __exit__(self, exc_type, exc_value, traceback): torch.cuda.set_stream(self.original_stream)"},{"question":"**Localizing an Application with gettext in Python** Your task is to implement localization for a Python application using the `gettext` module. You will create a simple application that can display messages in English and French based on the user\'s locale settings. # Requirements 1. **Message Catalogs**: - Create a folder structure for storing your `.mo` files. For simplicity, assume the structure is: ``` locale/ fr/LC_MESSAGES/messages.mo en/LC_MESSAGES/messages.mo ``` - You do not need to create actual `.mo` files for this exercise; assume they are present in the above structure. 2. **Translation Setup**: - Write a function `setup_translation(locale_dir: str, language: str)` that binds the text domain and sets up the translation based on the provided language. 3. **Application Main Function**: - Create a main function `main()` that does the following: - Sets up translation by calling `setup_translation`. - Prints a series of messages that should be translated based on the current locale. # Messages to Translate - \\"Hello, World!\\" - \\"This is a Python localization example.\\" - \\"Goodbye!\\" # Sample Implementation 1. **Function: `setup_translation`** ```python import gettext def setup_translation(locale_dir: str, language: str): Set up translation for the given language. gettext.bindtextdomain(\'messages\', locale_dir) gettext.textdomain(\'messages\') translation = gettext.translation(\'messages\', localedir=locale_dir, languages=[language], fallback=True) translation.install() global _ _ = translation.gettext ``` 2. **Function: `main`** ```python def main(): Main function to display localized messages. locale_dir = \'locale\' language = \'fr\' # Change this to \'en\' or \'fr\' to test different locales. setup_translation(locale_dir, language) print(_(\\"Hello, World!\\")) print(_(\\"This is a Python localization example.\\")) print(_(\\"Goodbye!\\")) if __name__ == \\"__main__\\": main() ``` # Input and Output - **Input**: The `locale` directory path and the language code (\'en\' or \'fr\'). - **Output**: The translated messages printed to the console. # Constraints - The focus is on the correct usage of the `gettext` module to handle translations. - Assume that the `.mo` files exist in the specified directory structure and contain appropriate translations. # Performance - The solution should efficiently handle the setting up of translations and printing of messages without unnecessary overhead. Implement the above functions and demonstrate the localization of the messages based on the user\'s selected language.","solution":"import gettext def setup_translation(locale_dir: str, language: str): Set up translation for the given language. gettext.bindtextdomain(\'messages\', locale_dir) gettext.textdomain(\'messages\') translation = gettext.translation(\'messages\', localedir=locale_dir, languages=[language], fallback=True) translation.install() global _ _ = translation.gettext def main(): Main function to display localized messages. locale_dir = \'locale\' language = \'fr\' # Change this to \'en\' or \'fr\' to test different locales. setup_translation(locale_dir, language) print(_(\\"Hello, World!\\")) print(_(\\"This is a Python localization example.\\")) print(_(\\"Goodbye!\\")) if __name__ == \\"__main__\\": main()"},{"question":"You are given a list of filenames and a list of patterns. Your task is to implement a function that processes these lists and returns a dictionary where each pattern is a key and its value is a list of filenames that match the pattern. # Function Signature ```python def match_patterns(filenames: list, patterns: list) -> dict: ``` # Input - `filenames`: A list of strings where each string is a filename (1 â‰¤ len(filenames) â‰¤ 10^4). - `patterns`: A list of strings where each string is a pattern (1 â‰¤ len(patterns) â‰¤ 10^3). # Output - A dictionary where each key is a pattern and its value is a list of filenames matching that pattern. If no filenames match a specific pattern, the value should be an empty list. # Example ```python filenames = [\'file1.txt\', \'file2.txt\', \'data.csv\', \'image.png\', \'script.py\'] patterns = [\'*.txt\', \'*.py\', \'*.md\'] result = match_patterns(filenames, patterns) assert result == { \'*.txt\': [\'file1.txt\', \'file2.txt\'], \'*.py\': [\'script.py\'], \'*.md\': [] } ``` # Constraints - You must use functions from the `fnmatch` module to solve the problem. - The solution should be efficient in terms of both time and space complexity. # Notes - Keep in mind the difference between `fnmatch` and `fnmatchcase` when considering filenames and patterns that may differ in case sensitivity. - Consider edge cases such as empty lists, patterns that match no filenames, and long lists with mixed file extensions.","solution":"import fnmatch def match_patterns(filenames: list, patterns: list) -> dict: Matches the filenames with the given patterns and returns a dictionary where each pattern is a key and its value is a list of filenames that match the pattern. result = {} for pattern in patterns: matched_files = [filename for filename in filenames if fnmatch.fnmatch(filename, pattern)] result[pattern] = matched_files return result"},{"question":"# Secure Password Generator In this assessment, you will implement a function to generate secure passwords using the `secrets` module. The function should meet the following requirements: 1. Generate an alphanumeric password of a specified length. 2. The password must include at least one lowercase character, one uppercase character, and one digit. 3. Ensure the password is cryptographically secure by using the `secrets` module. 4. If the specified length is less than 3, raise a `ValueError` with the message \\"Password length must be at least 3\\". Function Signature ```python def generate_secure_password(length: int) -> str: pass ``` Input - `length` (int): The length of the password to be generated. Must be an integer 3 or greater. Output - Returns a secure password as a string that meets the specified criteria. Example ```python generate_secure_password(10) # Example output: \'aB3gH9lM4N\' ``` Constraints - The password length (`length`) must be at least 3. - The function must use the `secrets` module to ensure the password is secure. - The generated password must contain at least one lowercase character, one uppercase character, and one digit. Implementation Notes - Use `secrets.choice` for selecting characters to ensure randomness. - You may use the `string` module to access `ascii_letters` and `digits`. Here is a template to get you started: ```python import secrets import string def generate_secure_password(length: int) -> str: if length < 3: raise ValueError(\\"Password length must be at least 3\\") alphabet = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(alphabet) for i in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password)): return password ``` Use this function to generate secure passwords for your applications and ensure that they meet the necessary security standards.","solution":"import secrets import string def generate_secure_password(length: int) -> str: if length < 3: raise ValueError(\\"Password length must be at least 3\\") alphabet = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(alphabet) for i in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password)): return password"},{"question":"# Complex Tensor Operations in PyTorch Background You are required to demonstrate your understanding of handling complex numbers using PyTorch. Specifically, you need to implement functions that perform certain operations on complex tensors. Problem Statement 1. Implement a function `create_complex_tensor` that takes the shape and dtype as input and returns a randomly initialized complex tensor of the given shape and dtype. 2. Implement a function `convert_to_complex` that takes a real tensor of shape (..., 2) and returns its complex tensor representation. 3. Implement a function `compute_polar_coordinates` that takes a complex tensor and returns two tensors: the first containing the magnitudes and the second containing the phases (angles) of the complex numbers. 4. Implement a function `complex_tensor_autograd` that demonstrates autograd functionality with complex tensors by: - Creating a complex tensor. - Defining a simple operation on this tensor (such as an element-wise multiplication by a constant). - Computing the gradient of a scalar loss function with respect to the complex tensor. Function Signatures ```python import torch def create_complex_tensor(shape, dtype=torch.cfloat): Create a randomly initialized complex tensor with the given shape and dtype. Parameters: shape (tuple): Shape of the tensor. dtype (torch.dtype): Dtype of the tensor. Either torch.cfloat or torch.cdouble. Returns: torch.Tensor: Randomly initialized complex tensor. pass def convert_to_complex(real_tensor): Convert a real tensor of shape (..., 2) to its complex tensor representation. Parameters: real_tensor (torch.Tensor): Real tensor of shape (..., 2). Returns: torch.Tensor: Complex tensor. pass def compute_polar_coordinates(complex_tensor): Compute the magnitudes and angles of the given complex tensor. Parameters: complex_tensor (torch.Tensor): Complex tensor. Returns: tuple: A tuple containing: - magnitudes (torch.Tensor): Magnitudes of the complex numbers. - angles (torch.Tensor): Angles of the complex numbers. pass def complex_tensor_autograd(): Demonstrate autograd functionality with complex tensors. Returns: tuple: A tuple containing: - complex_tensor (torch.Tensor): The original complex tensor. - grad_tensor (torch.Tensor): The gradient of the loss with respect to the complex tensor. pass ``` Constraints - Use the appropriate functions and operations to ensure efficient computation. - Preserve dtype consistency wherever necessary. - Ensure the code is compatible with PyTorch\'s autograd system for complex numbers. Example Usage ```python # Example usage for create_complex_tensor ct = create_complex_tensor((2, 2), dtype=torch.cfloat) print(ct) # Example usage for convert_to_complex rt = torch.tensor([[1.0, -1.0], [2.0, -2.0], [3.0, -3.0]]) ct = convert_to_complex(rt) print(ct) # Example usage for compute_polar_coordinates ct = torch.tensor([1+1j, 2-2j], dtype=torch.cfloat) magnitudes, angles = compute_polar_coordinates(ct) print(f\\"Magnitudes: {magnitudes}\\") print(f\\"Angles: {angles}\\") # Example usage for complex_tensor_autograd original, gradient = complex_tensor_autograd() print(f\\"Original tensor: {original}\\") print(f\\"Gradient tensor: {gradient}\\") ``` The above functions cover a range of basic and advanced operations with complex tensors in PyTorch, allowing students to demonstrate their understanding and expertise comprehensively.","solution":"import torch def create_complex_tensor(shape, dtype=torch.cfloat): Create a randomly initialized complex tensor with the given shape and dtype. Parameters: shape (tuple): Shape of the tensor. dtype (torch.dtype): Dtype of the tensor. Either torch.cfloat or torch.cdouble. Returns: torch.Tensor: Randomly initialized complex tensor. real = torch.randn(shape) imag = torch.randn(shape) return torch.complex(real, imag).to(dtype=dtype) def convert_to_complex(real_tensor): Convert a real tensor of shape (..., 2) to its complex tensor representation. Parameters: real_tensor (torch.Tensor): Real tensor of shape (..., 2). Returns: torch.Tensor: Complex tensor. if real_tensor.size(-1) != 2: raise ValueError(\\"The last dimensions must be of size 2 representing real and imaginary parts.\\") real_part = real_tensor[..., 0] imag_part = real_tensor[..., 1] return torch.complex(real_part, imag_part) def compute_polar_coordinates(complex_tensor): Compute the magnitudes and angles of the given complex tensor. Parameters: complex_tensor (torch.Tensor): Complex tensor. Returns: tuple: A tuple containing: - magnitudes (torch.Tensor): Magnitudes of the complex numbers. - angles (torch.Tensor): Angles of the complex numbers. magnitudes = torch.abs(complex_tensor) angles = torch.angle(complex_tensor) return magnitudes, angles def complex_tensor_autograd(): Demonstrate autograd functionality with complex tensors. Returns: tuple: A tuple containing: - complex_tensor (torch.Tensor): The original complex tensor. - grad_tensor (torch.Tensor): The gradient of the loss with respect to the complex tensor. complex_tensor = torch.randn(3, dtype=torch.cfloat, requires_grad=True) loss = torch.sum(complex_tensor.real**2 + complex_tensor.imag**2) loss.backward() return complex_tensor, complex_tensor.grad"},{"question":"<|Analysis Begin|> The `aifc` module is designed to handle AIFF (Audio Interchange File Format) and AIFF-C (compressed AIFF) audio files. Here\'s a brief summary of its essential functionalities: **Reading AIFF/AIFF-C files:** - `open(file, mode=None)`: Opens an AIFF or AIFF-C file in a specified mode (`\'r\'`, `\'rb\'` for reading). - Methods to retrieve properties of the audio data: - `getnchannels()`: Retrieves the number of audio channels (mono, stereo). - `getsampwidth()`: Retrieves the sample width in bytes. - `getframerate()`: Retrieves the sampling rate (frames per second). - `getnframes()`: Retrieves the number of audio frames. - `getcomptype()`: Retrieves the compression type. - `getcompname()`: Retrieves the human-readable description of the compression type. - `getparams()`: Retrieves all audio properties as a named tuple. - `getmarkers()`, `getmark(id)`: Retrieve markers within the audio data. - `readframes(nframes)`: Reads and retrieves the next `nframes` from the audio data. - Various navigation methods like `rewind()`, `setpos(pos)`, `tell()`, and finally `close()`. **Writing AIFF/AIFF-C files:** - The module also supports creating and writing to AIFF/AIFF-C files by opening the file in write mode (`\'w\'`, `\'wb\'`). - Essential methods for setting audio data parameters before writing: - `setnchannels(nchannels)`: Sets the number of channels. - `setsampwidth(width)`: Sets the sample width in bytes. - `setframerate(rate)`: Sets the sampling rate. - `setnframes(nframes)`: Sets the number of frames. - `setcomptype(type, name)`, `setparams(nchannels, sampwidth, framerate, comptype, compname)`: Sets the compression type and other parameters. - `setmark(id, pos, name)`: Adds a mark to the audio data. - `writeframes(data)`, `writeframesraw(data)`: Writes audio data to the file. - Navigational methods and `close()` similar to reading. **Key Points:** - AIFF files and AIFF-C files have specific configurable properties such as channels, sample width, frame rate, compression type, etc. - The module allows both reading and writing of AIFF/AIFF-C audio files with established methods to handle properties and data. Given the detailed functionalities of the module, I will craft a coding question that tests the ability to read and summarize key audio properties of an AIFF/AIFF-C file using this module. <|Analysis End|> <|Question Begin|> You are tasked with developing a functionality that reads an AIFF or AIFF-C audio file and summarizes its key properties using the `aifc` module. Your function should be named `summarize_aiff_properties` and follow the specifications given below: # Specifications: **Function Signature:** ```python def summarize_aiff_properties(file_path: str) -> dict: ``` **Input:** - `file_path`: A string representing the path to the AIFF/AIFF-C file. **Output:** - A dictionary containing the following keys and their corresponding values: - `\'nchannels\'`: Number of audio channels. - `\'sampwidth\'`: Size of the samples in bytes. - `\'framerate\'`: Sampling rate (frames per second). - `\'nframes\'`: Total number of audio frames in the file. - `\'comptype\'`: Type of compression (as a bytes array of length 4). - `\'compname\'`: Description of the compression type (as a human-readable string). **Constraints:** - You can assume that the input file path is always valid and exists. - The function should handle basic AIFF and AIFF-C files. - The function should use the `aifc` module to read the file. # Example: Given an AIFF file located at `\'example.aiff\'` with the following properties: - 2 channels (stereo) - 16-bit samples (2 bytes per sample) - 44100 frames per second - 100000 frames - No compression The dictionary returned should look like this: ```python { \'nchannels\': 2, \'sampwidth\': 2, \'framerate\': 44100, \'nframes\': 100000, \'comptype\': b\'NONE\', \'compname\': \'not compressed\' } ``` # Note: You should use the relevant methods from the `aifc` module to fetch and return these properties. # Sample Code for I/O: ```python if __name__ == \\"__main__\\": file_path = \'example.aiff\' summary = summarize_aiff_properties(file_path) print(summary) ``` Implement the `summarize_aiff_properties` function to meet the above requirements.","solution":"import aifc def summarize_aiff_properties(file_path: str) -> dict: Summarizes key properties of an AIFF or AIFF-C audio file. Params: - file_path: str : The path to the AIFF/AIFF-C file. Returns: - dict : A dictionary containing audio properties. with aifc.open(file_path, \'r\') as aiff_file: nchannels = aiff_file.getnchannels() sampwidth = aiff_file.getsampwidth() framerate = aiff_file.getframerate() nframes = aiff_file.getnframes() comptype = aiff_file.getcomptype() compname = aiff_file.getcompname() return { \'nchannels\': nchannels, \'sampwidth\': sampwidth, \'framerate\': framerate, \'nframes\': nframes, \'comptype\': comptype, \'compname\': compname }"},{"question":"Objective: Implement a function that processes a given Unicode string by decomposing it into individual characters, determining specific properties of each character, and reconstructing the string based on bidirectional properties. Function Signature: ```python def process_unicode_string(unistr: str) -> str: Processes a given Unicode string by decomposing it into individual characters, extracting properties of each character, and reversing the string based on bidirectional properties. Parameters: unistr (str): The input Unicode string Returns: str: The reconstructed string after processing Raises: ValueError: If any character in unistr is invalid in any context ``` Input: - A Unicode string `unistr` containing 1 to 10^4 characters. Output: - Returns a reconstructed string where: - Each character has been decomposed. - Each decomposed part\'s properties (name, category, bidirectional) are evaluated. - Characters with bidirectional property `R` or `AL` (right-to-left or Arabic letter) are reversed. Constraints: - The input string will always be in a valid Unicode format. - The function must handle exception cases where specific properties or conversions fail. Example: ```python import unicodedata def process_unicode_string(unistr: str) -> str: try: # Decomposing the string into individual characters decomposed_chars = [] for char in unistr: decomposition = unicodedata.decomposition(char) decomposed_chars.append(decomposition if decomposition else char) # Analyzing properties and reversing based on bidirectional properties processed_chars = [] for char in decomposed_chars: bidi_category = unicodedata.bidirectional(char) if bidi_category in (\'R\', \'AL\'): processed_chars.append(char[::-1]) else: processed_chars.append(char) # Reconstructing final string result = \'\'.join(processed_chars) return result except Exception as e: raise ValueError(f\\"Error processing the Unicode string: {e}\\") # Example usage: try: result = process_unicode_string(\\"Hello, Ù…Ø±Ø­Ø¨Ø§!\\") print(f\\"Processed result: {result}\\") except ValueError as e: print(str(e)) ``` Explanation: - The given function first decomposes each character in the input Unicode string using `unicodedata.decomposition(char)`. - For each decomposed character, it checks the `bidirectional` property. If the character has a bidirectional property `R` or `AL`, it reverses the character. - After processing all characters, it reconstructs the string by joining the processed characters. - If any errors are encountered during this process (due to invalid characters or properties), the function raises a `ValueError`. This question assesses students\' understanding of Unicode data manipulation, character properties, string processing, and exception handling in Python using the `unicodedata` module.","solution":"import unicodedata def process_unicode_string(unistr: str) -> str: try: # Analyze properties and reverse based on bidirectional properties processed_chars = [] for char in unistr: bidi_category = unicodedata.bidirectional(char) if bidi_category in (\'R\', \'AL\'): processed_chars.append(char[::-1]) else: processed_chars.append(char) # Reconstructing final string result = \'\'.join(processed_chars) return result except Exception as e: raise ValueError(f\\"Error processing the Unicode string: {e}\\")"},{"question":"**Question: Implement a Terminal Input Processor** You are tasked with implementing a function that reads user input from the terminal in both raw and cbreak modes using the \\"tty\\" library. Your function should handle switching between these modes and reading input until a termination condition is met. # Requirements 1. Implement the function `process_terminal_input(mode: str) -> str` that processes terminal input based on the specified mode. 2. The `mode` parameter can be either `\'raw\'` or `\'cbreak\'`. If an invalid mode is provided, raise a `ValueError`. 3. The function should read characters one by one until the character `\'.\'` is encountered, denoting the end of input. 4. The function should return the string of characters read (excluding the terminating `\'.\'`). 5. Ensure that the terminal mode is reset to its original state after processing the input. # Input - `mode`: A string that specifies the terminal mode (`\'raw\'` or `\'cbreak\'`). # Output - A string representing the user input read from the terminal, excluding the terminating character `\'.\'`. # Constraints - The function will only run on Unix systems. - Assume input will be provided through a typical Unix terminal. # Example ```python # Example usage: # In the terminal, if the user types \\"hello world.\\", the function should return \\"hello world\\" result = process_terminal_input(\'raw\') print(result) # Expected to capture user input until \'.\' is encountered result = process_terminal_input(\'cbreak\') print(result) # Expected to capture user input until \'.\' is encountered ``` # Notes - You may need to use the `sys.stdin.fileno()` to get the file descriptor for standard input. - Make sure to handle exceptions and reset terminal modes properly, even in case of errors. - Consider looking into the `termios` module for resetting terminal attributes. # Implementation ```python import tty import sys import termios def process_terminal_input(mode: str) -> str: if mode not in [\'raw\', \'cbreak\']: raise ValueError(\\"Invalid mode. Use \'raw\' or \'cbreak\'.\\") fd = sys.stdin.fileno() original_attributes = termios.tcgetattr(fd) try: if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) result = [] while True: ch = sys.stdin.read(1) # Read a single character if ch == \'.\': break result.append(ch) return \'\'.join(result) finally: # Reset terminal to original settings termios.tcsetattr(fd, termios.TCSADRAIN, original_attributes) ```","solution":"import tty import sys import termios def process_terminal_input(mode: str) -> str: if mode not in [\'raw\', \'cbreak\']: raise ValueError(\\"Invalid mode. Use \'raw\' or \'cbreak\'.\\") fd = sys.stdin.fileno() original_attributes = termios.tcgetattr(fd) try: if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) result = [] while True: ch = sys.stdin.read(1) # Read a single character if ch == \'.\': break result.append(ch) return \'\'.join(result) finally: # Reset terminal to original settings termios.tcsetattr(fd, termios.TCSADRAIN, original_attributes)"},{"question":"**Question: Nested Element Information Extractor** # Background You are given the task of analyzing HTML content and extracting specific information. For this question, you will use the `HTMLParser` class from the `html.parser` module to create a custom parser that extracts all hyperlinks (the content of `<a>` tags) along with the texts they enclose, even if they are nested within multiple HTML elements. # Objective Implement a class `LinkExtractor` that extracts and stores the text enclosed within `<a>` tags in a list, while preserving the order in which they appear in the HTML document. The class should also store the associated href attribute (if present) for each hyperlink. # Requirements 1. **Class Definition**: - Define a class `LinkExtractor` that inherits from `HTMLParser`. 2. **Methods To Implement**: - `__init__(self)`: Initialize the `HTMLParser` base class and any required data structures to store the links and the text. - `handle_starttag(self, tag, attrs)`: Override this method to identify `<a>` tags and extract the href attribute. - `handle_endtag(self, tag)`: Override this method to recognize the end of `<a>` tags. - `handle_data(self, data)`: Override this method to accumulate text data enclosed within `<a>` tags. - `get_links(self)`: Provide a method to return the collected links and their enclosed texts. - `reset(self)`: Optionally override this method if necessary to reset the accumulated data. # Input Format - The `LinkExtractor` class will be tested with HTML content fed to it using the `feed` method. # Output Format - The `get_links` method should return a list of tuples, where each tuple contains two elements: (href, text). The `href` is a string (empty if the href attribute is not present), and the `text` is the text content enclosed within the `<a>` tag. # Example Usage ```python from html.parser import HTMLParser from typing import List, Tuple class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = [] self.current_link = None self.current_data = [] self.in_a_tag = False def handle_starttag(self, tag, attrs): if tag == \'a\': self.in_a_tag = True self.current_link = dict(attrs).get(\'href\', \'\') self.current_data = [] def handle_endtag(self, tag): if tag == \'a\' and self.in_a_tag: self.links.append((self.current_link, \'\'.join(self.current_data).strip())) self.in_a_tag = False def handle_data(self, data): if self.in_a_tag: self.current_data.append(data) def get_links(self) -> List[Tuple[str, str]]: return self.links # Example to test the implementation of LinkExtractor html_content = \'\'\' <html> <head><title>Test</title></head> <body> <a href=\\"https://example.com\\">Example</a> <div> <a href=\\"https://test.com\\">Test Link</a> </div> <a>Just text link</a> </body> </html> \'\'\' parser = LinkExtractor() parser.feed(html_content) print(parser.get_links()) ``` **Expected Output**: ```python [ (\\"https://example.com\\", \\"Example\\"), (\\"https://test.com\\", \\"Test Link\\"), (\\"\\", \\"Just text link\\") ] ``` # Constraints - Ensure your implementation correctly handles nested elements and various cases of incomplete HTML data fed in chunks.","solution":"from html.parser import HTMLParser from typing import List, Tuple class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = [] self.current_link = None self.current_data = [] self.in_a_tag = False def handle_starttag(self, tag, attrs): if tag == \'a\': self.in_a_tag = True self.current_link = dict(attrs).get(\'href\', \'\') self.current_data = [] def handle_endtag(self, tag): if tag == \'a\' and self.in_a_tag: self.links.append((self.current_link, \'\'.join(self.current_data).strip())) self.in_a_tag = False def handle_data(self, data): if self.in_a_tag: self.current_data.append(data) def get_links(self) -> List[Tuple[str, str]]: return self.links"},{"question":"**Question: Implement a function that uses the `glob` module to list specific files in a directory tree** You are tasked with implementing a function `find_files_with_patterns` that uses the `glob` module to find files in a directory matching given patterns. The function should support both simple and recursive searches. # Function Signature ```python def find_files_with_patterns(root_dir: str, patterns: list, recursive: bool) -> dict: pass ``` # Input - `root_dir` (str): The root directory to begin the search. - `patterns` (list): A list of string patterns to match filenames (e.g., `[\\"*.txt\\", \\"*.py\\"]`). - `recursive` (bool): A flag indicating whether the search should be recursive. # Output - Returns a dictionary where keys are the patterns and the values are lists of files matching each pattern. The lists should contain the full paths of the matching files. # Constraints - The search should take into account files starting with `.` only when the pattern starts with `.`. - If `recursive` is True, use the `**` pattern to match any directory depth. - For large directory structures, ensure efficient implementation to avoid excessive computation time. # Examples ```python # Example 1 root_dir = \\"./test_dir\\" patterns = [\\"*.txt\\", \\"*.py\\"] recursive = False print(find_files_with_patterns(root_dir, patterns, recursive)) # Expected output (dependent on the file structure in \'./test_dir\'): # { # \\"*.txt\\": [\\"./test_dir/file1.txt\\", \\"./test_dir/file2.txt\\"], # \\"*.py\\": [\\"./test_dir/script.py\\"] # } # Example 2 root_dir = \\"./test_dir\\" patterns = [\\"**/*.txt\\", \\"*.py\\"] recursive = True print(find_files_with_patterns(root_dir, patterns, recursive)) # Expected output (dependent on the file structure in \'./test_dir\'): # { # \\"**/*.txt\\": [\\"./test_dir/file1.txt\\", \\"./test_dir/subdir/file3.txt\\"], # \\"*.py\\": [\\"./test_dir/script.py\\", \\"./test_dir/subdir/subscript.py\\"] # } ``` In this challenge, students will demonstrate their understanding of the `glob` module for both simple and recursive pattern matching. They will also ensure their solutions handle edge cases such as files beginning with a dot and varying depth of directory structures.","solution":"import glob import os def find_files_with_patterns(root_dir: str, patterns: list, recursive: bool) -> dict: Find files in a directory tree matching given patterns. :param root_dir: The root directory to begin the search. :param patterns: A list of string patterns to match filenames. :param recursive: A flag indicating whether the search should be recursive. :return: A dictionary where keys are the patterns and values are lists of full paths of matching files. result = {} for pattern in patterns: if recursive: search_pattern = os.path.join(root_dir, \\"**\\", pattern) else: search_pattern = os.path.join(root_dir, pattern) matching_files = glob.glob(search_pattern, recursive=recursive) result[pattern] = matching_files return result"},{"question":"**Handling Large Datasets with Pandas** **Objective:** Assess the studentâ€™s ability to handle large datasets efficiently using pandas. **Problem Description:** A directory contains multiple Parquet files, each representing data for a different year. Each file contains columns: `timestamp`, `name`, `id`, `x`, and `y`. The goal is to perform an out-of-core operation to calculate the total memory usage of all files if they were loaded into memory. Additionally, optimize the memory usage by converting textual data columns to the `Categorical` type and downcasting numeric columns. Finally, output the original and optimized memory usage. **Input:** - A directory path containing the Parquet files. **Output:** - The total memory usage before optimization. - The total memory usage after optimization. **Constraints:** - Each Parquet file fits in memory individually but not necessarily all at once. **Implement the following function:** ```python import pandas as pd import pathlib def memory_usage_optimization(directory_path: str) -> (int, int): Calculate and optimize memory usage of multiple Parquet files. Args: directory_path (str): The path to the directory containing the Parquet files. Returns: tuple: Original and optimized total memory usage in bytes. original_memory_usage = 0 optimized_memory_usage = 0 files = pathlib.Path(directory_path).glob(\\"*.parquet\\") for path in files: df = pd.read_parquet(path) # Calculate original memory usage original_memory_usage += df.memory_usage(deep=True).sum() # Optimize the DataFrame df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") # Calculate optimized memory usage optimized_memory_usage += df.memory_usage(deep=True).sum() return original_memory_usage, optimized_memory_usage ``` **Example:** Suppose the directory structure is: ``` data â””â”€â”€ timeseries â”œâ”€â”€ ts-00.parquet â”œâ”€â”€ ts-01.parquet â””â”€â”€ ts-02.parquet ``` To get the original and optimized memory usage, you would call: ```python original_usage, optimized_usage = memory_usage_optimization(\\"data/timeseries\\") print(f\\"Original Memory Usage: {original_usage} bytes\\") print(f\\"Optimized Memory Usage: {optimized_usage} bytes\\") ``` **Note:** - The function should iteratively load each Parquet file, calculate its memory usage, optimize it, and calculate the optimized memory usage. - Make sure to use efficient DataFrames manipulation techniques to achieve the optimization. **Performance Requirements:** - The solution should handle directories with multiple Parquet files, efficiently optimizing memory usage without loading all files into memory simultaneously. - Use pandas built-in methods (`read_parquet`, `astype`, and `to_numeric`) for type conversion and downcasting to ensure optimal performance.","solution":"import pandas as pd import pathlib def memory_usage_optimization(directory_path: str) -> (int, int): Calculate and optimize memory usage of multiple Parquet files. Args: directory_path (str): The path to the directory containing the Parquet files. Returns: tuple: Original and optimized total memory usage in bytes. original_memory_usage = 0 optimized_memory_usage = 0 files = pathlib.Path(directory_path).glob(\\"*.parquet\\") for path in files: df = pd.read_parquet(path) # Calculate original memory usage original_memory_usage += df.memory_usage(deep=True).sum() # Optimize the DataFrame df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") # Calculate optimized memory usage optimized_memory_usage += df.memory_usage(deep=True).sum() return original_memory_usage, optimized_memory_usage"},{"question":"# Python Coding Assessment Question **Objective:** You are required to demonstrate your understanding of Python\'s `sys` module by implementing a function that provides certain system configuration details and debugging information. The function should return a report string containing the following details: 1. **Python Version Information:** - The version of Python that is currently being used. 2. **Executable Path:** - The absolute path to the Python executable. 3. **Platform Information:** - The platform/system identifier. 4. **Recursion Limit:** - The current maximum depth of the Python interpreter stack. 5. **Thread Implementation Information:** - Details about the thread implementation as provided by `sys.thread_info`. 6. **Reference Count:** - A function to get the reference count of an object. The function should take an object as input and return the reference count. 7. **Debugging Information:** - A function to get the current stack frameâ€™s details. The function should return a dictionary mapping the thread\'s identifier to the topmost stack frame currently active in that thread. 8. **Handling Unraisable Exceptions:** - Implement a custom unraisable exception hook to log all unraisable exceptions to a file named `unraisable_exceptions.log`. **Expected Input and Output Formats:** - **Function Signature:** ```python def system_report() -> str: pass ``` - **Reference Count Function:** ```python def get_reference_count(obj) -> int: pass ``` - **Stack Frame Information Function:** ```python def get_stack_frames() -> dict: pass ``` - **Unraisable Exception Hook Function:** ```python def custom_unraisable_hook(unraisable): pass ``` **Constraints:** - The report must be returned as a single string with each detail on a new line. - The functions for reference count and stack frame information should accurately retrieve and return the required details. - The custom unraisable exception hook must log exceptions to the specified file without interrupting program flow. # Example Output ```python Python Version: 3.10.0 Executable Path: /usr/local/bin/python3.10 Platform: linux Recursion Limit: 1000 Thread Info: (\'pthread\', \'semaphore\', None) ``` # Implementation Requirements: 1. Implement the `system_report` function. 2. Implement the `get_reference_count` function. 3. Implement the `get_stack_frames` function. 4. Define the `custom_unraisable_hook` and set it using `sys.unraisablehook`. Use the `sys` module functionalities efficiently to achieve the objectives.","solution":"import sys import threading def system_report() -> str: Generates a system report string containing Python version, executable path, platform information, recursion limit, and thread implementation information. report = [ f\\"Python Version: {sys.version}\\", f\\"Executable Path: {sys.executable}\\", f\\"Platform: {sys.platform}\\", f\\"Recursion Limit: {sys.getrecursionlimit()}\\", f\\"Thread Info: {sys.thread_info}\\" ] return \\"n\\".join(report) def get_reference_count(obj) -> int: Returns the reference count of an object. return sys.getrefcount(obj) def get_stack_frames() -> dict: Returns a dictionary mapping each thread\'s identifier to the topmost stack frame. frames = sys._current_frames() return {thread_id: frame for thread_id, frame in frames.items()} def custom_unraisable_hook(unraisable): Logs unraisable exceptions to a file named \'unraisable_exceptions.log\' without interrupting program flow. with open(\\"unraisable_exceptions.log\\", \\"a\\") as log_file: log_file.write(f\\"Exception Type: {type(unraisable.exc_value).__name__}n\\") log_file.write(f\\"Exception: {unraisable.exc_value}n\\") if unraisable.object: log_file.write(f\\"Object: {unraisable.object}n\\") log_file.write(\\"n\\") # Set the custom unraisable exception hook sys.unraisablehook = custom_unraisable_hook"},{"question":"# ASCII Character Inspector and Converter **Objective:** Create a Python function that inspects and converts input strings based on ASCII character classes and manipulations provided by the `curses.ascii` module. **Problem Statement:** Write a function `ascii_inspector_converter(input_string: str) -> dict` that takes an input string, examines each character, and performs various checks and conversions using the `curses.ascii` module. The function should return a dictionary containing the following information for each character in the input string: 1. `character`: The original character. 2. `ascii_value`: The ASCII value of the character. 3. `is_control`: Whether the character is an ASCII control character. 4. `is_printable`: Whether the character is a printable ASCII character (including space). 5. `uppercase_if_alpha`: The uppercase version if the character is alphabetical, otherwise the original character. 6. `ctrl_version`: The control character equivalent (if applicable). 7. `alt_version`: The 8-bit character equivalent (if applicable). 8. `unctrl_representation`: The string representation for unprintable characters. **Input:** - `input_string` (str): A non-empty string containing ASCII characters. **Output:** - A dictionary where each key is the character and the value is another dictionary with the properties described above. **Constraints:** - The function must handle all characters in the ASCII set (0-127). - The function should make appropriate use of the `curses.ascii` module functions to determine properties and perform conversions. **Example:** ```python input_string = \\"Hello, world!tn\\" expected_output = { \'H\': { \'character\': \'H\', \'ascii_value\': 72, \'is_control\': False, \'is_printable\': True, \'uppercase_if_alpha\': \'H\', \'ctrl_version\': \'H\', \'alt_version\': \'Ã¿\', \'unctrl_representation\': \'H\' }, \'e\': { \'character\': \'e\', \'ascii_value\': 101, \'is_control\': False, \'is_printable\': True, \'uppercase_if_alpha\': \'E\', \'ctrl_version\': \'e\', \'alt_version\': \'Ã½\', \'unctrl_representation\': \'e\' }, ... \'t\': { \'character\': \'t\', \'ascii_value\': 9, \'is_control\': True, \'is_printable\': False, \'uppercase_if_alpha\': \'t\', \'ctrl_version\': \'t\', \'alt_version\': \'x89\', \'unctrl_representation\': \'^I\' }, \'n\': { \'character\': \'n\', \'ascii_value\': 10, \'is_control\': True, \'is_printable\': False, \'uppercase_if_alpha\': \'n\', \'ctrl_version\': \'n\', \'alt_version\': \'x8a\', \'unctrl_representation\': \'^J\' } } ``` **Note:** - Characters may be checked using `curses.ascii` functions such as `iscntrl`, `isprint`, etc. - Conversions can be performed using `ctrl`, `alt`, and `unctrl`. - Ensure to handle edge cases where methods may receive non-ASCII inputs gracefully (e.g., by restricting input to valid 7-bit ASCII characters). **Performance Requirements:** - The solution should be efficient in terms of time complexity for string length up to 10^5.","solution":"import curses.ascii def ascii_inspector_converter(input_string: str) -> dict: Inspects and converts input strings based on ASCII character classes and manipulations using the curses.ascii module. result = {} for char in input_string: ascii_value = ord(char) is_control = curses.ascii.iscntrl(char) is_printable = curses.ascii.isprint(char) uppercase_if_alpha = char.upper() if char.isalpha() else char ctrl_version = curses.ascii.ctrl(char) alt_version = curses.ascii.alt(char) unctrl_representation = curses.ascii.unctrl(char) result[char] = { \'character\': char, \'ascii_value\': ascii_value, \'is_control\': is_control, \'is_printable\': is_printable, \'uppercase_if_alpha\': uppercase_if_alpha, \'ctrl_version\': ctrl_version, \'alt_version\': alt_version, \'unctrl_representation\': unctrl_representation } return result"},{"question":"# Task: Create, Read, and Manipulate a ZIP Archive You are tasked with creating a Python script that demonstrates comprehensive usage of the `zipfile` module. The script should be able to perform the following tasks: 1. **Create and Write to a ZIP Archive**: - Create a new ZIP file called `archive.zip`. - Add three text files (`file1.txt`, `file2.txt`, `file3.txt`) to the archive. Each file should contain a single line with its respective filename as text. 2. **Read and List Contents of the ZIP Archive**: - List the contents of the `archive.zip` to verify the files have been added correctly. 3. **Extract Files from the ZIP Archive**: - Extract all files from `archive.zip` to a directory named `extracted_files`. 4. **Modify a File Inside the ZIP Archive**: - Update the content of `file2.txt` in the ZIP archive to \\"Updated Content\\". 5. **Error Handling**: - Handle possible exceptions that might occur during these operations (`zipfile.BadZipFile`, `zipfile.LargeZipFile`, or any other relevant exception). # Constraints - Use the built-in `zipfile` module. - Ensure the script handles any necessary cleanup, such as closing the ZIP file. - The extracted files should overwrite any existing files without prompting. - Performance is not a primary concern, but the solution should be clear and efficient. # Expected Input and Output Your script should execute the following operations: 1. **ZIP Creation and Writing**: - Creates `archive.zip` and adds `file1.txt`, `file2.txt`, and `file3.txt`. - Content: - `file1.txt`: \\"file1.txt\\" - `file2.txt`: \\"file2.txt\\" - `file3.txt`: \\"file3.txt\\" 2. **List Contents**: - Print the list of files in `archive.zip`. 3. **Extraction**: - Extracts files to `extracted_files` directory. 4. **Modification**: - Updates `file2.txt` to contain \\"Updated Content\\". # Example Usage Here is an example of how your script might be executed: ```bash python script.py Contents of `archive.zip`: [\'file1.txt\', \'file2.txt\', \'file3.txt\'] Extracting files to `extracted_files/`... Modifying `file2.txt` in the ZIP archive... Updated contents of `archive.zip`: [\'file1.txt\', \'file2.txt\', \'file3.txt\'] ``` # Python Implementation: Write a function `main()` that encapsulates all the functionality described above. ```python import zipfile import os def main(): # Task 1: Create and Write to a ZIP Archive with zipfile.ZipFile(\'archive.zip\', \'w\', compression=zipfile.ZIP_DEFLATED) as zipf: filenames = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] for filename in filenames: zipf.writestr(filename, filename) # Task 2: Read and List Contents of the ZIP Archive with zipfile.ZipFile(\'archive.zip\', \'r\') as zipf: print(\\"Contents of `archive.zip`:\\") print(zipf.namelist()) # Task 3: Extract Files from the ZIP Archive with zipfile.ZipFile(\'archive.zip\', \'r\') as zipf: zipf.extractall(\'extracted_files/\') # Task 4: Modify a File Inside the ZIP Archive with zipfile.ZipFile(\'archive.zip\', \'a\', compression=zipfile.ZIP_DEFLATED) as zipf: zipf.writestr(\'file2.txt\', \'Updated Content\') # Verify the modification with zipfile.ZipFile(\'archive.zip\', \'r\') as zipf: print(\\"Updated contents of `archive.zip`:\\") print(zipf.namelist()) if __name__ == \\"__main__\\": try: main() except zipfile.BadZipFile: print(\\"The ZIP file is corrupted.\\") except zipfile.LargeZipFile: print(\\"The ZIP file requires ZIP64 functionality, but it\'s not enabled.\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` Ensure your solution adheres to the specified requirements and constraints.","solution":"import zipfile import os def create_zip(): Creates a zip archive and adds three text files. with zipfile.ZipFile(\'archive.zip\', \'w\', compression=zipfile.ZIP_DEFLATED) as zipf: filenames = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] for filename in filenames: zipf.writestr(filename, filename) def list_contents(): Lists the contents of the zip archive. with zipfile.ZipFile(\'archive.zip\', \'r\') as zipf: contents = zipf.namelist() print(\\"Contents of \'archive.zip\':\\") print(contents) return contents def extract_files(): Extracts the contents of the zip archive to a directory. with zipfile.ZipFile(\'archive.zip\', \'r\') as zipf: zipf.extractall(\'extracted_files/\') def modify_file_in_zip(): Modifies a file inside the zip archive. with zipfile.ZipFile(\'archive.zip\', \'a\', compression=zipfile.ZIP_DEFLATED) as zipf: zipf.writestr(\'file2.txt\', \'Updated Content\') def main(): try: create_zip() contents_before = list_contents() extract_files() modify_file_in_zip() contents_after = list_contents() return contents_before, contents_after except zipfile.BadZipFile: print(\\"The ZIP file is corrupted.\\") except zipfile.LargeZipFile: print(\\"The ZIP file requires ZIP64 functionality, but it\'s not enabled.\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question: TorchScript Custom Class Implementation** # Objective: Create a TorchScript-compatible class that performs basic mathematical operations on a `torch.Tensor`. This question will assess your understanding of creating custom classes in TorchScript, employing type annotations, and tensor manipulations. # Problem Statement: Implement a TorchScript class named `TensorOperations` that initializes with a `torch.Tensor` and performs several operations on this tensor. Your class should support the following methods: 1. **Initialization**: - `__init__(self, tensor: torch.Tensor)`: Initializes the class with a given tensor. 2. **Scale Tensor**: - `scale(self, factor: float) -> torch.Tensor`: Scales the tensor by a given factor and returns the result. 3. **Add Value**: - `add_value(self, value: float) -> torch.Tensor`: Adds a specific value to each element in the tensor and returns the result. 4. **Element-wise Multiplication**: - `element_wise_multiply(self, other: torch.Tensor) -> torch.Tensor`: Multiplies the tensor element-wise with another tensor of the same shape and returns the result. 5. **Print Tensor**: - `print_tensor(self)`: Prints the tensorâ€™s current state. # Requirements: - Ensure that the tensor operations are type-checked and compatible with TorchScript. - Annotate all method signatures with the appropriate types. - Perform error checking within methods to validate the shapes of tensors for operations where necessary. # Example Usage: ```python import torch @torch.jit.script class TensorOperations: def __init__(self, tensor: torch.Tensor): self.tensor = tensor def scale(self, factor: float) -> torch.Tensor: return self.tensor * factor def add_value(self, value: float) -> torch.Tensor: return self.tensor + value def element_wise_multiply(self, other: torch.Tensor) -> torch.Tensor: if self.tensor.size() == other.size(): return self.tensor * other else: raise ValueError(\\"Tensors must be of the same shape for element-wise multiplication\\") def print_tensor(self): print(self.tensor) # Example usage tensor = torch.tensor([1.0, 2.0, 3.0]) operations = TensorOperations(tensor) operations.print_tensor() scaled_tensor = operations.scale(2.0) added_tensor = operations.add_value(5.0) another_tensor = torch.tensor([0.5, 0.5, 0.5]) multiplied_tensor = operations.element_wise_multiply(another_tensor) print(scaled_tensor) print(added_tensor) print(multiplied_tensor) ``` # Constraints: - Do not use any non-TorchScript compatible features. - Ensure all methods handle invalid inputs gracefully by raising appropriate errors. Submit your implementation of the `TensorOperations` class.","solution":"import torch from typing import Any @torch.jit.script class TensorOperations: def __init__(self, tensor: torch.Tensor): self.tensor = tensor def scale(self, factor: float) -> torch.Tensor: return self.tensor * factor def add_value(self, value: float) -> torch.Tensor: return self.tensor + value def element_wise_multiply(self, other: torch.Tensor) -> torch.Tensor: if self.tensor.size() == other.size(): return self.tensor * other else: raise ValueError(\\"Tensors must be of the same shape for element-wise multiplication\\") def print_tensor(self) -> None: print(self.tensor)"},{"question":"# IP Address Management and Network Challenge **Objective**: Implement a Python class that utilizes the `ipaddress` module to perform various network-related tasks. **Problem Statement**: You are required to implement a class `IPNetworkManager` that will provide functionality to handle a list of IP addresses. The class should be able to perform the following operations: 1. **Add IP Address**: Add a new IP address to the list. The IP address can be either IPv4 or IPv6. 2. **Remove IP Address**: Remove an IP address from the list. 3. **Get Private IPs**: Retrieve all private IP addresses from the list. 4. **Get Global IPs**: Retrieve all global IP addresses from the list. 5. **Summarize Networks**: Summarize the given range of IP addresses into the smallest possible list of networks. 6. **Collapse Networks**: Collapse a given list of IP network objects to their shortest equivalent form. **Class Definition**: ```python import ipaddress class IPNetworkManager: def __init__(self): Initialize an empty list to store IP addresses. pass def add_ip(self, address: str) -> None: Add an IP address to the list. Args: address (str): The IP address to add. Raises: ValueError: If the address is not a valid IPv4 or IPv6 address. pass def remove_ip(self, address: str) -> None: Remove an IP address from the list. Args: address (str): The IP address to remove. Raises: ValueError: If the address is not present in the list. pass def get_private_ips(self) -> list: Retrieve all private IP addresses from the list. Returns: list: A list of private IP addresses as strings. pass def get_global_ips(self) -> list: Retrieve all global IP addresses from the list. Returns: list: A list of global IP addresses as strings. pass def summarize_networks(self, start_ip: str, end_ip: str) -> list: Summarize a range of IP addresses into the smallest possible list of networks. Args: start_ip (str): The start IP address of the range. end_ip (str): The end IP address of the range. Returns: list: A list of network objects representing the summarized range. pass def collapse_networks(self, networks: list) -> list: Collapse a list of IP network objects to their shortest equivalent form. Args: networks (list): A list of network strings. Returns: list: A list of collapsed network objects. pass # Example usage: # manager = IPNetworkManager() # manager.add_ip(\'192.168.0.1\') # manager.add_ip(\'2001:db8::1\') # print(manager.get_private_ips()) # print(manager.summarize_networks(\'192.168.0.0\', \'192.168.0.255\')) # print(manager.collapse_networks([\'192.168.0.0/24\', \'192.168.1.0/24\'])) ``` **Constraints**: - The IP addresses provided will always be valid strings. - The list will contain no more than 10,000 IP addresses at any given time. - The networks for summarization and collapsing will be valid and compatible (either all IPv4 or all IPv6). Please implement the methods and ensure your solution passes the given example usage scenarios.","solution":"import ipaddress class IPNetworkManager: def __init__(self): Initialize an empty list to store IP addresses. self.ips = [] def add_ip(self, address: str) -> None: Add an IP address to the list. Args: address (str): The IP address to add. Raises: ValueError: If the address is not a valid IPv4 or IPv6 address. try: ip = ipaddress.ip_address(address) self.ips.append(ip) except ValueError as e: raise ValueError(\\"Invalid IP address\\") def remove_ip(self, address: str) -> None: Remove an IP address from the list. Args: address (str): The IP address to remove. Raises: ValueError: If the address is not present in the list. try: ip = ipaddress.ip_address(address) self.ips.remove(ip) except ValueError as e: raise ValueError(\\"Invalid IP address\\") except KeyError: raise ValueError(\\"IP address not found in list\\") def get_private_ips(self) -> list: Retrieve all private IP addresses from the list. Returns: list: A list of private IP addresses as strings. return [str(ip) for ip in self.ips if ip.is_private] def get_global_ips(self) -> list: Retrieve all global IP addresses from the list. Returns: list: A list of global IP addresses as strings. return [str(ip) for ip in self.ips if ip.is_global] def summarize_networks(self, start_ip: str, end_ip: str) -> list: Summarize a range of IP addresses into the smallest possible list of networks. Args: start_ip (str): The start IP address of the range. end_ip (str): The end IP address of the range. Returns: list: A list of network objects representing the summarized range. start = ipaddress.ip_address(start_ip) end = ipaddress.ip_address(end_ip) return list(ipaddress.summarize_address_range(start, end)) def collapse_networks(self, networks: list) -> list: Collapse a list of IP network objects to their shortest equivalent form. Args: networks (list): A list of network strings. Returns: list: A list of collapsed network objects. ip_networks = [ipaddress.ip_network(net) for net in networks] return list(ipaddress.collapse_addresses(ip_networks))"},{"question":"Objective: Write a Python script to search for all files matching specific patterns within a given directory and its subdirectories. Your script should utilize the `glob` module to perform various types of searches and return a formatted summary of the results. Task: 1. Implement a function `search_files(directory: str, patterns: List[str], recursive: bool = False) -> Dict[str, List[str]]` that performs the following: - Searches for files in the specified `directory` that match any of the patterns in the `patterns` list. - If `recursive` is `True`, the search should include all subdirectories. - The function returns a dictionary where the keys are the patterns and the values are lists of matching file paths. Input: - `directory`: A string representing the path to the directory where the search should start. - `patterns`: A list of string patterns to search for. - `recursive`: A boolean indicating whether the search should be recursive. Output: - A dictionary where each key is a pattern from the `patterns` list and the corresponding value is a list of file paths that match the pattern. Constraints: - Do not include files in the results that start with a dot (\\".\\") unless the pattern also starts with a dot. - The function should handle cases where no files match a given pattern by returning an empty list for that pattern. - Preserve leading components of the path in the results. Example: ```python import glob from typing import List, Dict def search_files(directory: str, patterns: List[str], recursive: bool = False) -> Dict[str, List[str]]: result = {} for pattern in patterns: if recursive: search_pattern = f\\"{directory}/**/{pattern}\\" else: search_pattern = f\\"{directory}/{pattern}\\" result[pattern] = glob.glob(search_pattern, recursive=recursive) return result # Example usage: directory = \\"./test_directory\\" patterns = [\\"*.txt\\", \\"*.gif\\", \\"*.log\\"] print(search_files(directory, patterns, recursive=True)) ``` Example Output: Suppose the directory structure is as follows: ``` test_directory/ â”œâ”€â”€ 1.txt â”œâ”€â”€ 2.gif â”œâ”€â”€ file.log â””â”€â”€ subdir/ â”œâ”€â”€ 3.txt â”œâ”€â”€ image.gif â””â”€â”€ hidden.log ``` The output should be: ```python { \\"*.txt\\": [\\"./test_directory/1.txt\\", \\"./test_directory/subdir/3.txt\\"], \\"*.gif\\": [\\"./test_directory/2.gif\\", \\"./test_directory/subdir/image.gif\\"], \\"*.log\\": [\\"./test_directory/file.log\\"] } ``` **Notes:** - The search should be case-sensitive. - Use appropriate escaping when necessary to handle special characters.","solution":"import glob from typing import List, Dict def search_files(directory: str, patterns: List[str], recursive: bool = False) -> Dict[str, List[str]]: Searches for files in the specified directory that match any of the patterns in the patterns list. Args: directory (str): The path to the directory where the search should start. patterns (List[str]): A list of string patterns to search for. recursive (bool): A boolean indicating whether the search should be recursive. Returns: Dict[str, List[str]]: A dictionary where the keys are patterns and the values are lists of matching file paths. result = {} for pattern in patterns: if recursive: search_pattern = f\\"{directory}/**/{pattern}\\" else: search_pattern = f\\"{directory}/{pattern}\\" # Use glob with respect to dot files result[pattern] = [ file for file in glob.glob(search_pattern, recursive=recursive) if not (\\"/.\\" in file or file.startswith(\'.\')) or pattern.startswith(\'.\') ] return result # Example usage: # directory = \\"./test_directory\\" # patterns = [\\"*.txt\\", \\"*.gif\\", \\"*.log\\"] # print(search_files(directory, patterns, recursive=True))"},{"question":"You are given the `penguins` dataset from Seaborn, which contains various measurements of penguins such as body mass, bill length, bill depth, species, and sex. You are required to create a complex, customized plot using Seaborn\'s object-oriented interface. Task 1. Load the `penguins` dataset using Seaborn. 2. Create a faceted plot, with individual facets for each species of penguin. 3. Within each facet, create a scatter plot of `body_mass_g` against `flipper_length_mm` colored by `sex`. 4. Add error bars for the `body_mass_g` variable using the standard deviation for each subset (species and sex). 5. Customize the scatter plot to use different markers for each `sex` and adjust the point size for clarity. 6. Ensure each facet plot has a range plot overlaid to show the minimum and maximum `bill_length_mm` for the respective species. Input None (The dataset is loaded internally.) Output A Seaborn plot as described in the task. Constraints - Use only the Seaborn library. - Ensure the plot is clear and readable. - Handle any missing data appropriately (e.g., using `dropna()` method for simplicity). Example Solution ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\").dropna() # Create a faceted scatter plot plot = (so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"sex\\") .facet(\\"species\\") .add(so.Dot(marker=\\"o\\"), pointsize=5) # Customize marker and point size .add(so.Range(), so.Est(errorbar=\\"sd\\")) # Add standard deviation error bars ) # Overlay range plots for bill length bill_length_plot = (penguins .rename_axis(index=\\"penguin\\") .pipe(so.Plot, x=\\"penguin\\", ymin=\\"bill_length_mm\\", ymax=\\"bill_length_mm\\") .facet(\\"species\\") .add(so.Range(), color=\\"island\\") ) # Display the plot plot.show() bill_length_plot.show() ``` Note Ensure you have Seaborn installed: `pip install seaborn`.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Create the faceted scatter plot with error bars and customized markers g = sns.FacetGrid(penguins, col=\\"species\\", margin_titles=True) g.map_dataframe(sns.scatterplot, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", hue=\\"sex\\", style=\\"sex\\", s=100) # Function to compute and plot error bars def plot_error_bars(data, **kwargs): species = data[\\"species\\"].iloc[0] for sex in data[\\"sex\\"].unique(): subset = data[data[\\"sex\\"] == sex] body_mass_error = np.std(subset[\\"body_mass_g\\"]) plt.errorbar(x=subset[\\"body_mass_g\\"], y=subset[\\"flipper_length_mm\\"], xerr=body_mass_error, fmt=\'none\', c=\'black\') g.map_dataframe(plot_error_bars) # Overlay range plots for bill length for ax, species in zip(g.axes[0], penguins[\\"species\\"].unique()): species_data = penguins[penguins[\\"species\\"] == species] ymin = species_data[\\"bill_length_mm\\"].min() ymax = species_data[\\"bill_length_mm\\"].max() ax.axhline(ymin, linestyle=\\"--\\", color=\\"gray\\") ax.axhline(ymax, linestyle=\\"--\\", color=\\"gray\\") # Add legends and titles g.add_legend() g.set_axis_labels(\\"Body Mass (g)\\", \\"Flipper Length (mm)\\") # Display the plot plt.show()"},{"question":"**Covariance Estimation Challenge** In this problem, you are provided a dataset and tasked with estimating its covariance matrix using different approaches available in the `sklearn.covariance` module. You need to follow these steps: 1. Compute the empirical covariance matrix of the dataset. 2. Apply a shrinkage transformation to the empirical covariance matrix using a given shrinkage coefficient. 3. Estimate the covariance matrix using the Ledoit-Wolf method. 4. Estimate the covariance matrix using the Oracle Approximating Shrinkage (OAS) method. 5. Compare the performances of these estimators by computing and printing the Frobenius norm between each estimated covariance matrix and the true covariance matrix. **Input Format:** - A 2D numpy array `X` of shape `(n_samples, n_features)` where `n_samples` is the number of samples and `n_features` is the number of features. - A float `alpha` representing the shrinkage coefficient. **Output Format:** - Print the Frobenius norm of the difference between the true covariance matrix and each of the estimated covariance matrices (empirical, shrunk, Ledoit-Wolf, OAS). **Constraints:** - The dataset `X` will have at least 8 samples and 3 features. - The shrinkage coefficient `alpha` will be between 0 and 1. **Performance Requirements:** - The implementation should handle datasets of up to 1000 samples efficiently. **Sample Code Skeleton:** ```python import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS def covariance_estimation(metrics, alpha): # Step 1: Empirical covariance emp_cov = EmpiricalCovariance().fit(metrics).covariance_ # Step 2: Shrunk covariance shrunk_cov = ShrunkCovariance(shrinkage=alpha).fit(metrics).covariance_ # Step 3: Ledoit-Wolf covariance lw_cov = LedoitWolf().fit(metrics).covariance_ # Step 4: OAS covariance oas_cov = OAS().fit(metrics).covariance_ # Compute the Frobenius norm between the true and estimated covariance matrices true_cov = np.cov(metrics, rowvar=False) # Calculate the Frobenius norms frobenius_emp = np.linalg.norm(emp_cov - true_cov, ord=\'fro\') frobenius_shrunk = np.linalg.norm(shrunk_cov - true_cov, ord=\'fro\') frobenius_lw = np.linalg.norm(lw_cov - true_cov, ord=\'fro\') frobenius_oas = np.linalg.norm(oas_cov - true_cov, ord=\'fro\') # Print the results print(f\\"Frobenius norm for Empirical Covariance: {frobenius_emp}\\") print(f\\"Frobenius norm for Shrunk Covariance: {frobenius_shrunk}\\") print(f\\"Frobenius norm for Ledoit-Wolf Covariance: {frobenius_lw}\\") print(f\\"Frobenius norm for OAS Covariance: {frobenius_oas}\\") # Sample Input X = np.random.randn(100, 5) alpha = 0.3 covariance_estimation(X, alpha) ``` You are required to implement the `covariance_estimation` function based on the provided sample code skeleton. Be sure to import necessary modules and ensure the function runs without errors for different datasets and alpha values.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS def covariance_estimation(X, alpha): Estimates the covariance matrix using various methods and compares them using the Frobenius norm. Parameters: X (numpy.ndarray): Input data of shape (n_samples, n_features). alpha (float): Shrinkage coefficient. Returns: dict: A dictionary containing the Frobenius norms of the difference between the estimated covariances and the true covariance. # Step 1: Empirical covariance emp_cov = EmpiricalCovariance().fit(X).covariance_ # Step 2: Shrunk covariance shrunk_cov = ShrunkCovariance(shrinkage=alpha).fit(X).covariance_ # Step 3: Ledoit-Wolf covariance lw_cov = LedoitWolf().fit(X).covariance_ # Step 4: OAS covariance oas_cov = OAS().fit(X).covariance_ # Compute the true covariance matrix (numerically stable calculation) true_cov = np.cov(X, rowvar=False) # Calculate the Frobenius norms frobenius_emp = np.linalg.norm(emp_cov - true_cov, ord=\'fro\') frobenius_shrunk = np.linalg.norm(shrunk_cov - true_cov, ord=\'fro\') frobenius_lw = np.linalg.norm(lw_cov - true_cov, ord=\'fro\') frobenius_oas = np.linalg.norm(oas_cov - true_cov, ord=\'fro\') # Return the results in a dictionary frobenius_norms = { \\"Empirical\\": frobenius_emp, \\"Shrunk\\": frobenius_shrunk, \\"Ledoit-Wolf\\": frobenius_lw, \\"OAS\\": frobenius_oas } return frobenius_norms # Example usage: # Note that printing is handled in the unit tests rather than in the main function as per best practices X = np.random.randn(100, 5) alpha = 0.3 frobenius_norms = covariance_estimation(X, alpha) print(frobenius_norms)"},{"question":"# Kernel Ridge Regression Implementation and Comparison You are required to implement a function to perform Kernel Ridge Regression (KRR) using the `KernelRidge` class from scikit-learn. After implementing KRR, you need to perform a comparison with Support Vector Regression (SVR) from scikit-learn on the same dataset. Function Signature ```python def kernel_ridge_vs_svr(X_train, y_train, X_test, y_test): Fit Kernel Ridge Regression and Support Vector Regression on the training data, and evaluate the performance on the test data. Parameters: - X_train (ndarray): The training feature set. - y_train (ndarray): The training labels. - X_test (ndarray): The test feature set. - y_test (ndarray): The test labels. Returns: - dict: A dictionary containing the mean squared error for both KRR and SVR on the test dataset with keys \'KRR_MSE\' and \'SVR_MSE\' respectively. pass ``` Requirements 1. **Fit KRR and SVR Models**: - Create and fit a `KernelRidge` model using an appropriate kernel (e.g., Radial Basis Function (RBF)) on the training data. - Create and fit an `SVR` model using the same kernel on the training data. 2. **Prediction**: - Use both models to predict the labels on the test data. 3. **Evaluation**: - Compute the Mean Squared Error (MSE) for each model on the test data. - Return the MSEs as a dictionary with keys \'KRR_MSE\' and \'SVR_MSE\'. Constraints - Use the default parameters for both `KernelRidge` and `SVR` for simplicity. - Assume that the input arrays `X_train`, `y_train`, `X_test`, and `y_test` are NumPy arrays. - Ensure that your function is efficient and handles up to approximately 1000 training samples and 200 test samples efficiently. Example ```python import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.metrics import mean_squared_error # Example data X_train = np.random.randn(100, 10) y_train = np.random.randn(100) X_test = np.random.randn(20, 10) y_test = np.random.randn(20) # Call the function results = kernel_ridge_vs_svr(X_train, y_train, X_test, y_test) print(results) # Should print a dictionary with \'KRR_MSE\' and \'SVR_MSE\' keys ``` This question assesses the student\'s comprehension of the Kernel Ridge Regression, their ability to use scikit-learn for model fitting and prediction, and their capacity to evaluate model performance.","solution":"import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.metrics import mean_squared_error def kernel_ridge_vs_svr(X_train, y_train, X_test, y_test): Fit Kernel Ridge Regression and Support Vector Regression on the training data, and evaluate the performance on the test data. Parameters: - X_train (ndarray): The training feature set. - y_train (ndarray): The training labels. - X_test (ndarray): The test feature set. - y_test (ndarray): The test labels. Returns: - dict: A dictionary containing the mean squared error for both KRR and SVR on the test dataset with keys \'KRR_MSE\' and \'SVR_MSE\' respectively. # Initialize and fit Kernel Ridge Regression model krr_model = KernelRidge(kernel=\'rbf\') krr_model.fit(X_train, y_train) y_pred_krr = krr_model.predict(X_test) # Initialize and fit Support Vector Regression model svr_model = SVR(kernel=\'rbf\') svr_model.fit(X_train, y_train) y_pred_svr = svr_model.predict(X_test) # Calculate Mean Squared Error for both models krr_mse = mean_squared_error(y_test, y_pred_krr) svr_mse = mean_squared_error(y_test, y_pred_svr) return { \'KRR_MSE\': krr_mse, \'SVR_MSE\': svr_mse }"},{"question":"# Question: Advanced Residual Plot Analysis in Seaborn You are provided with a dataset and are required to analyze it using residual plots to check the assumptions of linear regression. Your task involves fitting models of different complexities and using various tools to understand the structure of the residuals. Dataset You will use the `mpg` dataset that can be loaded directly from Seaborn. ```python import seaborn as sns mpg = sns.load_dataset(\\"mpg\\") ``` Task 1. **Scatter Residual Plot**: - Create a residual plot using `sns.residplot` to visualize the residuals of a simple linear regression model. Use `weight` as the predictor (x) and `mpg` as the response variable (y). 2. **Polynomial Trend Analysis**: - Create another residual plot for the same variables (`weight` and `mpg`), but this time fit a second-order (quadratic) polynomial. 3. **LOWESS Smoothing**: - Add a LOWESS smooth line to the first residual plot created in Step 1 to highlight any structure in the residuals. Expected Output You should provide three plots: 1. A residual plot with a simple linear fit (Step 1). 2. A residual plot with a quadratic fit (Step 2). 3. The first residual plot enhanced with a LOWESS curve (Step 3). Constraints and Requirements - Ensure you use proper labels and titles for your plots to make them understandable. - Use different colors or styles for the LOWESS curve to make it distinguishable from the residual points. Here is the template you need to complete: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Step 1: Simple linear regression residual plot plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\") plt.title(\\"Residual Plot: Simple Linear Regression\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # Step 2: Quadratic regression residual plot plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot: Quadratic Regression\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # Step 3: Simple linear regression residual plot with LOWESS smoother plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", lowess=True, line_kws={\'color\':\'red\'}) plt.title(\\"Residual Plot with LOWESS Smoother\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() ``` You should replace the template code with your final solution. The expected output will be assessed based on the correctness and clarity of the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Step 1: Simple linear regression residual plot plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\") plt.title(\\"Residual Plot: Simple Linear Regression\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # Step 2: Quadratic regression residual plot plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot: Quadratic Regression\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # Step 3: Simple linear regression residual plot with LOWESS smoother plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", lowess=True, line_kws={\'color\':\'red\'}) plt.title(\\"Residual Plot with LOWESS Smoother\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show()"},{"question":"# Question: Environment Variable Management Using the `os` Module You are required to write a function that: 1. Retrieves the value of a specified environment variable. 2. Sets a new value for the specified environment variable. 3. Returns a dictionary of all environment variables whose names (keys) start with a specified prefix. To achieve this, use the `os` module for all environment variable manipulations. # Requirements: 1. **get_env_variable(variable_name: str) -> str:** - Retrieves the value of the environment variable named `variable_name`. - Raises a `KeyError` if the environment variable does not exist. 2. **set_env_variable(variable_name: str, value: str) -> None:** - Sets the environment variable `variable_name` to the specified `value`. 3. **get_env_variables_with_prefix(prefix: str) -> dict:** - Returns a dictionary containing all environment variables whose keys start with the specified `prefix`. # Input: * `variable_name`: A string representing the name of the environment variable. * `value`: A string representing the new value for the environment variable. * `prefix`: A string representing the prefix to filter environment variable names. # Output: * For `get_env_variable`, return the value of the specified environment variable as a string. * For `set_env_variable`, no return value is required. * For `get_env_variables_with_prefix`, a dictionary of environment variables filtered by the prefix, where keys and values are strings. # Constraints: * The operations should handle errors gracefully, providing appropriate error messages or exceptions. * Ensure the function names and signatures match the specifications exactly. # Example Usage: ```python import os def get_env_variable(variable_name: str) -> str: if variable_name in os.environ: return os.environ[variable_name] else: raise KeyError(f\\"Environment variable \'{variable_name}\' not found.\\") def set_env_variable(variable_name: str, value: str) -> None: os.environ[variable_name] = value def get_env_variables_with_prefix(prefix: str) -> dict: return {k: v for k, v in os.environ.items() if k.startswith(prefix)} # Example # Setting an environment variable set_env_variable(\\"TEST_VAR\\", \\"12345\\") # Getting an environment variable print(get_env_variable(\\"TEST_VAR\\")) # Output: \\"12345\\" # Getting variables with prefix print(get_env_variables_with_prefix(\\"TEST\\")) # Might output: {\'TEST_VAR\': \'12345\'} ```","solution":"import os def get_env_variable(variable_name: str) -> str: Retrieves the value of the environment variable named `variable_name`. Raises a KeyError if the environment variable does not exist. if variable_name in os.environ: return os.environ[variable_name] else: raise KeyError(f\\"Environment variable \'{variable_name}\' not found.\\") def set_env_variable(variable_name: str, value: str) -> None: Sets the environment variable `variable_name` to the specified `value`. os.environ[variable_name] = value def get_env_variables_with_prefix(prefix: str) -> dict: Returns a dictionary containing all environment variables whose keys start with the specified `prefix`. return {k: v for k, v in os.environ.items() if k.startswith(prefix)}"},{"question":"# Question: You are asked to implement a function `compute_weighted_mode` that computes the weighted mode of a given dataset using scikit-learn utilities. The mode is the value that appears most frequently in the dataset, and in the weighted scenario, each occurrence can have an associated weight. Function Signature: ```python def compute_weighted_mode(X, weights): Parameters: X (numpy.ndarray): A 2D array of shape (n_samples, n_features) which is the input dataset. weights (numpy.ndarray): A 1D array of shape (n_samples,) containing the weight of each sample. Returns: numpy.ndarray: A 1D array containing the weighted mode of each feature. ``` Requirements: - Use `sklearn.utils.extmath.weighted_mode` to calculate the weighted mode. - Use `sklearn.utils.check_array` to verify that `X` is a 2D array. - Use `sklearn.utils.check_random_state` to ensure reproducible results if randomness is involved. Constraints: - `X` should be a valid 2D numpy array with numeric elements only. - `weights` should be a 1D numpy array with positive values only and the same length as the number of rows in `X`. Example: ```python import numpy as np X = np.array([ [1, 2, 2], [3, 2, 1], [4, 2, 2] ]) weights = np.array([0.5, 1.0, 1.5]) mode = compute_weighted_mode(X, weights) print(mode) # Output should be something like: [4, 2, 2] ``` Performance Requirements: - The function should be efficient and handle large datasets up to 10,000 samples with several features without significant performance degradation. - Correct handling of sparse and dense datasets. You are expected to write efficient and clean code using scikit-learn\'s utilities to ensure the task is performed correctly.","solution":"import numpy as np from sklearn.utils.extmath import weighted_mode from sklearn.utils import check_array def compute_weighted_mode(X, weights): Computes the weighted mode of a given dataset using scikit-learn utilities. Parameters: X (numpy.ndarray): A 2D array of shape (n_samples, n_features) which is the input dataset. weights (numpy.ndarray): A 1D array of shape (n_samples,) containing the weight of each sample. Returns: numpy.ndarray: A 1D array containing the weighted mode of each feature. # Check that X is a valid 2D array X = check_array(X, ensure_2d=True) # Ensure that weights is a 1D array and has the same length as the number of rows in X weights = check_array(weights, ensure_2d=False) if X.shape[0] != weights.shape[0]: raise ValueError(\\"The length of weights must be the same as the number of rows in X\\") weighted_modes = [] for i in range(X.shape[1]): mode, _ = weighted_mode(X[:, i], weights) weighted_modes.append(mode[0]) return np.array(weighted_modes)"},{"question":"# Title: Implement a Shell Command Interpreter # Objective: Design and implement a Python function that uses the `shlex` module to parse and simulate the execution of a series of shell-like commands. This function should consider the proper handling of quotes, escaping characters, and separation of commands and arguments. # Description: Write a function `simulate_shell(commands: str) -> List[str]` that takes a single string `commands` containing multiple shell-like commands separated by semicolons (`;`). This function should return a list of strings where each string is the simulated output of each command. For the purposes of this exercise, assume each command simply echoes its arguments back and handles basic shell features like quoting and escaping. # Input: - `commands` (str): A string containing multiple shell-like commands separated by semicolons (`;`). # Output: - Return a list of strings where each string is the simulated output of each command. # Example: ```python >>> simulate_shell(\'echo \\"Hello World\\"; echo \\"This is a test\\"; echo \\"Another command\\"\') [\'Hello World\', \'This is a test\', \'Another command\'] ``` # Constraints: 1. The input string will contain valid shell-like syntax. 2. Each command and its arguments should be properly parsed considering quotes, escape characters, and whitespace. 3. Assume no nested commands or special shell syntax (like pipes or redirection). 4. The length of the input string will not exceed 1000 characters. # Hints: 1. Use `shlex.split()` to parse individual commands and their arguments. 2. Create a simple mapping function that simulates the output of each command. # Function Signature: ```python from typing import List import shlex def simulate_shell(commands: str) -> List[str]: pass ``` # Advanced Requirements (Optional): - Extend the implementation to handle basic shell redirection operators, such as `>`, `>>`, `<`. - Add error handling for invalid command syntax.","solution":"from typing import List import shlex def simulate_shell(commands: str) -> List[str]: Simulate a shell command interpreter that echoes back arguments of commands. Args: commands (str): A string containing shell-like commands separated by semicolons. Returns: List[str]: A list of strings where each string is the simulated output of each command. output = [] command_list = commands.split(\';\') for command in command_list: try: parsed_command = shlex.split(command.strip()) if parsed_command and parsed_command[0] == \'echo\': # Join arguments with space and add to output output.append(\' \'.join(parsed_command[1:])) else: # Handle other commands if necessary. For now, just acknowledge the command. output.append(f\\"Unrecognized command: {command.strip()}\\") except ValueError as ve: output.append(f\\"Error parsing command: {command.strip()}. Error: {str(ve)}\\") return output"},{"question":"# Python Coding Assessment Question **Objective**: To assess students\' understanding and ability to use the `platform` module in Python to gather and utilize platform-specific information. **Problem Statement**: Write a Python function `get_system_info()` that uses the `platform` module to gather the following information about the current system and returns it as a dictionary: 1. **\\"os_architecture\\"**: A string indicating the bit architecture and linkage format. 2. **\\"machine_type\\"**: The machine type (e.g., \'AMD64\'). 3. **\\"network_name\\"**: The network name of the computer. 4. **\\"platform_info\\"**: A human-readable string identifying the underlying platform. 5. **\\"processor_name\\"**: The processor name. 6. **\\"python_version\\"**: The current Python version as a string in the format \'major.minor.patchlevel\'. 7. **\\"system_release\\"**: The systemâ€™s release information. 8. **\\"system_name\\"**: The name of the operating system. **Function Signature**: ```python def get_system_info() -> dict: pass ``` **Expected Input/Output**: - **Input**: No input is required for this function. - **Output**: A dictionary containing the details mentioned above. **Constraints**: - Use the `platform` module to gather the required information. - Handle any edge cases where the information might not be available (i.e., return an empty string for any missing information). **Example Usage**: ```python info = get_system_info() print(info) # Example Output: # { # \\"os_architecture\\": \\"64bit, ELF\\", # \\"machine_type\\": \\"x86_64\\", # \\"network_name\\": \\"HOSTNAME\\", # \\"platform_info\\": \\"Linux-5.11.0-38-generic-x86_64-with-glibc2.31\\", # \\"processor_name\\": \\"Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz\\", # \\"python_version\\": \\"3.9.7\\", # \\"system_release\\": \\"5.11.0-38-generic\\", # \\"system_name\\": \\"Linux\\" # } ``` **Notes**: - Ensure that your implementation of `get_system_info()` handles scenarios where some platform information might not be available by returning an empty string for those fields. - Testing will be done on multiple platforms, so make sure not to hardcode any values based on the specifics of a single system. **Performance Requirements**: - The function should execute efficiently within a reasonable amount of time, given that it relies on system calls to gather information.","solution":"import platform def get_system_info() -> dict: Gathers information about the current system using the platform module. Returns the information as a dictionary. return { \\"os_architecture\\": platform.architecture()[0], \\"machine_type\\": platform.machine(), \\"network_name\\": platform.node(), \\"platform_info\\": platform.platform(), \\"processor_name\\": platform.processor(), \\"python_version\\": platform.python_version(), \\"system_release\\": platform.release(), \\"system_name\\": platform.system() }"},{"question":"Coding Assessment Question # Objective Write a Python program that utilizes `unittest` and the `test.support` module to test a sample function with various edge cases and conditions. Your goal is to create a comprehensive test suite that includes tests for normal cases, edge cases, and invalid inputs while ensuring proper cleanup and handling of resources. # Description You are required to implement a function named `process_data` which takes a list of integers and returns their sum. However, before writing the function, you need to write a test suite for it, following these guidelines: 1. **Test Case Definition**: - Create a test suite using the `unittest` module. - Write test cases that cover: 1. Normal cases (e.g., sum of a list of positive integers). 2. Edge cases (e.g., empty list, list with a single element). 3. Invalid inputs (e.g., list containing non-integer types, `None`, negative integers if your function should not handle them). - Use utilities from the `test.support` module to handle temporary files/directories and environment variables if required. 2. **Setup and Teardown**: - Include setup and teardown methods to initialize and clean up any resources required by the tests. 3. **Mocking and Resource Handling**: - Use appropriate context managers or mock objects to simulate conditions or handle resources during tests. # Example ```python import unittest from test import support def process_data(data): # Function implementation here pass class TestProcessData(unittest.TestCase): def setUp(self): # Setup code if necessary pass def tearDown(self): # Teardown code if necessary pass def test_normal_cases(self): self.assertEqual(process_data([1, 2, 3, 4]), 10) self.assertEqual(process_data([100, 200, 300]), 600) def test_edge_cases(self): self.assertEqual(process_data([]), 0) self.assertEqual(process_data([7]), 7) def test_invalid_inputs(self): with self.assertRaises(TypeError): process_data([1, \'a\', 3]) with self.assertRaises(TypeError): process_data(None) @support.captured_stdout() def test_output(self, stdout): # Example use of a test.support utility to handle output pass # Additional test cases and utility uses if __name__ == \'__main__\': unittest.main() ``` # Constraints - The function `process_data` must be implemented after writing the test cases. - Use the `test.support` module utilities where applicable to enhance your tests. - Ensure all resources such as temporary files or directories are properly cleaned up after tests. # Performance Requirements - Your tests should complete within a reasonable time frame and handle all edge cases effectively. Good luck!","solution":"def process_data(data): Takes a list of integers and returns their sum. Raises TypeError if input is not a list of integers. if data is None or not isinstance(data, list): raise TypeError(\\"Input must be a list of integers.\\") for element in data: if not isinstance(element, int): raise TypeError(\\"All elements in the list must be integers.\\") return sum(data)"},{"question":"**Problem Statement: Pretty-Print Complex Data Structures with Custom Settings** As a developer, you are often required to display complex data structures in a readable format. Python\'s `pprint` module provides a variety of ways to customize the output of these data structures. Write a function `custom_pretty_print` that: 1. Pretty-prints a given complex data structure using a custom set of parameters. 2. Ensures the data structure is printed with the following settings: - Indentation level of 4 spaces per nesting level. - Line width constraint of 60 characters. - Compact format enabled for sequences. - Dictionaries should be displayed in insertion order. Additionally, the function should have an optional parameter to format numbers with underscores as thousand separators. **Function Signature:** ```python def custom_pretty_print(data: Any, underscore_numbers: bool = False) -> None: pass ``` **Input:** - `data`: The complex data structure to pretty-print (can be of any data type). - `underscore_numbers`: A boolean flag indicating whether to format numbers with underscores as thousand separators. **Output:** - The function should print the pretty-printed data to the standard output. **Constraints:** - Ensure compatibility with deeply nested data structures. - Handle cases where the data structure might include recursive references. - Do not sort dictionary keys alphabetically; maintain the insertion order. **Example Usage:** ```python data = { \'info\': \'A sample Python project\', \'numbers\': [1000, 2000, 3000, 1000000], \'nested\': { \'list\': [\'value1\', \'value2\', \'value3\'] }, \'long_list\': list(range(100)) } custom_pretty_print(data, underscore_numbers=True) ``` _Expected Output Example:_ ``` { \'info\': \'A sample Python project\', \'numbers\': [1_000, 2_000, 3_000, 1_000_000], \'nested\': { \'list\': [\'value1\', \'value2\', \'value3\'] }, \'long_list\': [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99 ] } ``` **Performance Requirements:** - Ensure efficient handling of large and deep data structures without significant performance degradation. **Notes:** - Utilize the `pprint.PrettyPrinter` class to construct the custom printer instance. - Consider edge cases like deeply nested structures and recursive references.","solution":"import pprint def custom_pretty_print(data, underscore_numbers=False): def format_number(num): if isinstance(num, int): return f\\"{num:_}\\" # Adds underscores return num class CustomPrettyPrinter(pprint.PrettyPrinter): def format(self, obj, context, maxlevels, level): if underscore_numbers and isinstance(obj, int): return format_number(obj), True, False return super().format(obj, context, maxlevels, level) printer = CustomPrettyPrinter(indent=4, width=60, compact=True, sort_dicts=False) printer.pprint(data)"},{"question":"# Question: Implementing and Evaluating a Custom Scoring Function Objective In this task, you will demonstrate your understanding of scikit-learn\'s metrics by implementing a custom scoring function and using it to evaluate a machine learning model. You\'ll also perform model training and comparison using different built-in metrics. Dataset You will use the Iris dataset, a well-known dataset in machine learning, which is available in scikit-learn. Steps to Follow 1. **Data Preparation:** - Load the Iris dataset. - Split the dataset into training and test sets. 2. **Model Implementation:** - Train a logistic regression model on the training set. 3. **Custom Scoring Function:** - Implement a custom scoring function that computes the harmonic mean of precision and recall (different from F1 score). This custom metric should be higher for better models. - Use the `make_scorer` function to make this custom scoring callable by scikit-learn\'s model evaluation tools. 4. **Model Evaluation:** - Evaluate the trained logistic regression model on the test set using the following metrics: - Accuracy - Precision - Recall - Custom Harmonic Mean Score 5. **Interpret Results:** - Print and interpret the results of each metric. Explain how well the model performed based on these different metrics. Constraints - Ensure your custom scoring function returns higher values for better models (higher is better). - Use scikit-learn\'s `train_test_split` for splitting data and `LogisticRegression` for modeling. # Code Template ```python from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score from sklearn.model_selection import train_test_split # Step 1: Load the Iris dataset and split it iris = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42) # Step 2: Train a logistic regression model model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # Step 3: Implement a custom scoring function def harmonic_mean_score(y_true, y_pred, **kwargs): precision = precision_score(y_true, y_pred, average=\'macro\') recall = recall_score(y_true, y_pred, average=\'macro\') return (2 * precision * recall) / (precision + recall) # Make the custom scorer custom_scorer = make_scorer(harmonic_mean_score, greater_is_better=True) # Step 4: Evaluate the model using different metrics y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') custom_score = harmonic_mean_score(y_test, y_pred) # Step 5: Print and interpret the results print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"Harmonic Mean Score (Custom): {custom_score}\\") # Interpretation: # Provide a brief explanation of the results and what they indicate about the model\'s performance. ``` # Expected Output - The code should print out the values for accuracy, precision, recall, and custom harmonic mean score. - An interpretation of the results should follow, explaining the model\'s performance based on these metrics.","solution":"from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score from sklearn.model_selection import train_test_split # Step 1: Load the Iris dataset and split it iris = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42) # Step 2: Train a logistic regression model model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # Step 3: Implement a custom scoring function def harmonic_mean_score(y_true, y_pred, **kwargs): precision = precision_score(y_true, y_pred, average=\'macro\') recall = recall_score(y_true, y_pred, average=\'macro\') return (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0 # Make the custom scorer custom_scorer = make_scorer(harmonic_mean_score, greater_is_better=True) # Step 4: Evaluate the model using different metrics y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') custom_score = harmonic_mean_score(y_test, y_pred) # Step 5: Print and interpret the results print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"Harmonic Mean Score (Custom): {custom_score}\\") # Interpretation: # Accuracy indicates how often the model makes correct predictions. # Precision indicates the model\'s ability to avoid false positives. # Recall indicates the model\'s ability to find all relevant cases (avoid false negatives). # The custom Harmonic Mean Score combines precision and recall equally, # providing a balanced measure of the model\'s performance."},{"question":"**Question: Creating and Managing Executable Python Zip Archives** You are tasked with creating a tool using the Python `zipapp` module to manage Python application archives (.pyz files). This tool should help developers easily package their Python projects into executable zip files, with options for specifying the main function, adding a shebang line for the interpreter, and compressing the archive. Specifically, you need to implement a Python function `create_executable_zip` with the following requirements: # Function Signature ```python def create_executable_zip(source: str, target: str = None, interpreter: str = None, main: str = None, compress: bool = False, filter_fn: callable = None) -> None: pass ``` # Inputs - `source` (str): The path to the source directory containing the Python code. - `target` (str, optional): The path to the target archive file. If not specified, the target will be the same as the source directory with a `.pyz` extension. - `interpreter` (str, optional): The path to the Python interpreter to be specified in the shebang line. If not specified, no shebang line will be added. - `main` (str, optional): The main function to be invoked when the archive is executed. It should be in the form `\\"pkg.module:callable\\"` (e.g., `\\"myapp:main\\"`). If not specified, the source directory must contain a `__main__.py` file. - `compress` (bool, optional): If `True`, compress the files in the archive. Default is `False`. - `filter_fn` (callable, optional): A function that takes a `Path` object and returns `True` if the file should be added to the archive. # Outputs None. The function should create the executable zip archive at the specified target location. # Constraints 1. When `main` is not provided, ensure the source directory contains a `__main__.py` file. 2. If `source` is not a valid directory, raise a `ValueError` with an appropriate error message. 3. Ensure the functionality covers optional compression and filtering. 4. Handle the creation of shebang lines based on the provided interpreter. 5. If the `filter_fn` is provided, use it to selectively add files to the archive. # Examples Example 1: ```python create_executable_zip(\'myapp\', main=\'myapp:main\') ``` This should package the `myapp` directory into `myapp.pyz` with `myapp:main` as the entry point. Example 2: ```python create_executable_zip(\'myapp\', \'myapp_exec.pyz\', interpreter=\'/usr/bin/env python3\', main=\'myapp:main\', compress=True) ``` This should package the `myapp` directory into `myapp_exec.pyz`, add a shebang line for `/usr/bin/env python3`, invoke `myapp:main`, and compress the archive. # Notes: - You may use the `zipapp.create_archive` function internally. - Ensure comprehensive error handling and validation based on the given constraints. Your task is to implement the `create_executable_zip` function according to the requirements above.","solution":"import os from pathlib import Path import zipapp def create_executable_zip(source: str, target: str = None, interpreter: str = None, main: str = None, compress: bool = False, filter_fn: callable = None) -> None: source_path = Path(source) # Validate source directory if not source_path.is_dir(): raise ValueError(f\\"The source path {source} is not a valid directory.\\") # Ensure __main__.py exists if main is not specified if main is None and not (source_path / \'__main__.py\').is_file(): raise ValueError(f\\"No __main__.py found in {source}, and no main callable provided.\\") # Determine target path if target is None: target = source_path.with_suffix(\'.pyz\') else: target = Path(target) # Create the zipapp options dictionary zoptions = {} if interpreter: zoptions[\'interpreter\'] = interpreter if main: zoptions[\'main\'] = main if compress: zoptions[\'compressed\'] = True # Create the executable zip archive zipapp.create_archive(source_path, target, filter=filter_fn, **zoptions)"},{"question":"**Objective:** Implement a series of functions using the `secrets` module to enhance password security in a user authentication system. The tasks will require you to generate secure passwords, validate their strength, and provide password reset functionalities with secure tokens. **Problem Statement:** You are tasked with creating a Python module `secure_auth` that includes the following functionalities: 1. **Generate a secure password**: - Implement `generate_secure_password(length)` that generates a random alphanumeric password of specified length. The password must include at least one uppercase letter, one lowercase letter, and two digits. 2. **Validate a password\'s strength**: - Implement `validate_password(password)` that checks if a given password is strong enough. A strong password must be at least 8 characters long and contain at least one uppercase letter, one lowercase letter, and one digit. It should return `True` if the password is strong, otherwise `False`. 3. **Generate a password reset token**: - Implement `generate_reset_token()` that generates a URL-safe secure token for password reset purposes. The token should be at least 16 bytes in length. 4. **Compare two passwords securely**: - Implement `compare_passwords(password1, password2)` that takes two passwords as input and returns `True` if they are identical using a constant-time comparison, otherwise `False`. **Your task is to implement these functions using Python\'s `secrets` module. Make sure to handle edge cases and validate inputs where necessary.** Function Signatures: ```python def generate_secure_password(length: int) -> str: pass def validate_password(password: str) -> bool: pass def generate_reset_token() -> str: pass def compare_passwords(password1: str, password2: str) -> bool: pass ``` **Constraints:** - The `length` parameter for `generate_secure_password` should be at least 8. - The `password` parameters will be non-empty strings containing printable ASCII characters. - For `generate_reset_token`, ensure the token generated is URL-safe. **Examples:** ```python >>> generate_secure_password(10) \'vL1kz3Y2sF\' >>> validate_password(\'vL1kz3Y2sF\') True >>> validate_password(\'weak\') False >>> generate_reset_token() \'9Z6hze6EPcv0TL_0gB9AHA\' >>> compare_passwords(\'secureP@ss123\', \'secureP@ss123\') True >>> compare_passwords(\'secureP@ss123\', \'SecureP@ss124\') False ``` **Notes:** - Use the `secrets` module functions to ensure cryptographic security in random number generation and token generation. - Ensure that the password generation meets specified criteria, even when length constraints are minimal.","solution":"import secrets import string import hmac def generate_secure_password(length: int) -> str: if length < 8: raise ValueError(\\"Password length must be at least 8\\") alphabet = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(alphabet) for _ in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and sum(c.isdigit() for c in password) >= 2): return password def validate_password(password: str) -> bool: if len(password) < 8: return False if not any(c.islower() for c in password): return False if not any(c.isupper() for c in password): return False if not any(c.isdigit() for c in password): return False return True def generate_reset_token() -> str: return secrets.token_urlsafe(16) def compare_passwords(password1: str, password2: str) -> bool: return hmac.compare_digest(password1, password2)"},{"question":"Implement a PyTorch function `generate_random_tensors` that accomplishes the following: 1. Sets a random seed to ensure reproducibility. 2. Generates three different types of tensors: - A tensor with values sampled from a normal distribution (`torch.randn`). - A tensor with values uniformly distributed between 0 and 1 (`torch.rand`). - A tensor with random integers within a given range (`torch.randint`). 3. Ensures that all tensors generated meet specific shape requirements. # Function Signature ```python def generate_random_tensors(seed: int, shape: tuple, int_range: tuple) -> dict: Generates random tensors with specified configurations. Parameters: seed (int): The seed value for randomness. shape (tuple): The shape of the tensors to be generated. int_range (tuple): A tuple containing (low, high) values for generating random integers. Returns: dict: A dictionary containing the following keys and their respective tensors: - \'normal\': Tensor with values from a normal distribution. - \'uniform\': Tensor with values uniformly distributed between 0 and 1. - \'int\': Tensor with random integers within the specified range. ``` # Input - `seed` (int): An integer value to be used for setting the random seed. - `shape` (tuple): A tuple defining the shape of the generated tensors (e.g., `(3, 3)`). - `int_range` (tuple): A tuple containing two integers `(low, high)` that define the range for generating random integers (inclusive of `low` and exclusive of `high`). # Output - Returns a dictionary with three tensors: - `\'normal\'`: A tensor of specified `shape` with values sampled from a standard normal distribution. - `\'uniform\'`: A tensor of specified `shape` with values uniformly distributed between 0 and 1. - `\'int\'`: A tensor of specified `shape` with random integers within the range `[low, high)`. # Constraints - The function should ensure reproducibility by setting the provided random seed. - All tensors should have the same shape as specified in the input. # Example ```python result = generate_random_tensors(seed=42, shape=(2, 3), int_range=(0, 10)) print(result[\'normal\']) # Example output (values will vary due to randomness but will be reproducible): # tensor([[ 0.3367, -0.2336, -1.0550], # [ 0.4299, 0.2412, -0.5795]]) print(result[\'uniform\']) # Example output: # tensor([[0.8967, 0.4539, 0.0746], # [0.8338, 0.1524, 0.5500]]) print(result[\'int\']) # Example output: # tensor([[6, 3, 7], # [4, 6, 9]]) ``` **Note**: The specific numerical values will change with different random seeds, but should remain consistent for a given seed.","solution":"import torch def generate_random_tensors(seed: int, shape: tuple, int_range: tuple) -> dict: Generates random tensors with specified configurations. Parameters: seed (int): The seed value for randomness. shape (tuple): The shape of the tensors to be generated. int_range (tuple): A tuple containing (low, high) values for generating random integers. Returns: dict: A dictionary containing the following keys and their respective tensors: - \'normal\': Tensor with values from a normal distribution. - \'uniform\': Tensor with values uniformly distributed between 0 and 1. - \'int\': Tensor with random integers within the specified range. torch.manual_seed(seed) normal_tensor = torch.randn(shape) uniform_tensor = torch.rand(shape) int_tensor = torch.randint(int_range[0], int_range[1], shape) return { \'normal\': normal_tensor, \'uniform\': uniform_tensor, \'int\': int_tensor }"},{"question":"You are provided with a deprecated module, `pipes`, which enables the creation and manipulation of pipelines using shell commands. Your task is to implement a function `process_text_pipeline` that processes text from an input file through a sequence of shell commands and writes the result to an output file. # Function Specification **Function Name:** `process_text_pipeline` **Parameters:** - `input_file` (str): The path to the input file containing the text to be processed. - `output_file` (str): The path to the output file where the processed text should be written. - `commands` (list of tuples): A list of tuples where each tuple contains a shell command (str) and its type (str), indicating how the command should be handled in the pipeline. Each type string has two characters: - The first character can be `\'-\'` (command reads from stdin), `\'f\'` (command reads from a file specified on the command line), or `\'.\'` (command reads no input, must be first). - The second character can be `\'-\'` (command writes to stdout), `\'f\'` (command writes to a file specified on the command line), or `\'.\'` (command writes nothing, must be last). **Return:** None # Example ```python # Example of usage commands = [ (\\"tr a-z A-Z\\", \\"--\\"), # Translate lowercase to uppercase (\\"sed \'s/HELLO/HI/g\'\\", \\"--\\") # Replace HELLO with HI ] process_text_pipeline(\'input.txt\', \'output.txt\', commands) ``` # Constraints - You can assume that all commands given will be valid shell commands for a POSIX shell. - The input file will exist and be readable. - You have write permissions to the output file location. # Implementation Notes - Use the `pipes` module to create the pipeline. - Use the `append` method to add commands to the pipeline as specified in the `commands` list. - Open the input file in reading mode and the output file in writing mode with the `open` method of the `pipes.Template` class. - Handle exceptions and errors gracefully, ensuring that any temporary files or resources are cleaned up appropriately.","solution":"import pipes def process_text_pipeline(input_file, output_file, commands): Process text from input_file through a sequence of shell commands and write the result to output_file using the deprecated pipes module. Parameters: input_file (str): The path to the input file containing the text to be processed. output_file (str): The path to the output file where the processed text should be written. commands (list of tuples): A list of tuples where each tuple contains a shell command (str) and its type (str), indicating how the command should be handled in the pipeline. # Create a pipeline template t = pipes.Template() # Add the commands to the pipeline for command, command_type in commands: t.append(command, command_type) # Open the input file for reading with open(input_file, \'r\') as f: input_data = f.read() # Apply the pipeline to write to the output file with t.open(output_file, \'w\') as f: f.write(input_data)"},{"question":"Advanced Cookie Management Objective Implement a function that simulates an HTTP state management mechanism using the `http.cookies` module. The function should manage cookies for a simple web application, handling operations such as setting cookie values, loading cookies from HTTP headers, and generating HTTP headers for cookie output. Requirements 1. **Function Signature**: ```python def manage_cookies(operations: list) -> str: ``` 2. **Input**: - `operations`: A list of operations, where each operation is a dictionary specifying the action to be performed on the cookies. The supported actions are: - `\\"set\\"`: Set a cookie value. - `\\"get\\"`: Get a cookie value. - `\\"load\\"`: Load cookies from a string (HTTP header). - `\\"output\\"`: Generate an HTTP header string for the cookies. 3. **Output**: - A string representing the final state of the cookies in HTTP header format after performing all operations. If there are any invalid operations or data, raise a `http.cookies.CookieError`. Constraints - The operations should be performed in the order they are provided. - The implementation should handle invalid cookie names and values according to the RFC 2109 specification. - Use attributes like `path`, `expires`, and `httponly` for setting cookies when specified. Example ```python operations = [ {\\"action\\": \\"set\\", \\"name\\": \\"user\\", \\"value\\": \\"john_doe\\", \\"path\\": \\"/\\"}, {\\"action\\": \\"set\\", \\"name\\": \\"session\\", \\"value\\": \\"abc123\\", \\"httponly\\": True}, {\\"action\\": \\"load\\", \\"header\\": \\"cart=5; token=xyz\\"}, {\\"action\\": \\"set\\", \\"name\\": \\"cart\\", \\"value\\": \\"10\\"}, {\\"action\\": \\"output\\", \\"header\\": \\"Custom-Cookie:\\"} ] # Expected Output: \'Custom-Cookie: user=john_doe; Path=/rnCustom-Cookie: session=abc123; HttpOnlyrnCustom-Cookie: cart=10rnCustom-Cookie: token=xyz\' ``` Notes - You may assume that each operation\'s dictionary is well-formed but must validate the cookie data as per the RFC 2109 rules. - Take advantage of the capabilities provided by the `http.cookies` module, particularly the `SimpleCookie` class. Implementation Please implement the `manage_cookies` function in Python.","solution":"from http.cookies import SimpleCookie, CookieError def manage_cookies(operations): cookie = SimpleCookie() for op in operations: action = op.get(\\"action\\") if action == \\"set\\": name = op.get(\\"name\\") value = op.get(\\"value\\") if not name or not value: raise CookieError(\\"Name and value must be provided for setting a cookie.\\") cookie[name] = value for attr in [\\"path\\", \\"expires\\", \\"httponly\\"]: if attr in op: cookie[name][attr] = op[attr] elif action == \\"get\\": name = op.get(\\"name\\") if not name: raise CookieError(\\"Name must be provided for getting a cookie.\\") if name not in cookie: return None return cookie[name].value elif action == \\"load\\": header = op.get(\\"header\\") if not header: raise CookieError(\\"Header must be provided for loading cookies.\\") cookie.load(header) elif action == \\"output\\": header_name = op.get(\\"header\\", \\"Set-Cookie\\") return \\"rn\\".join(f\\"{header_name}: {morsel.OutputString()}\\" for morsel in cookie.values()) else: raise CookieError(\\"Invalid action.\\") raise CookieError(\\"No output action found.\\")"},{"question":"**Objective:** Implement a function to efficiently identify and correct a performance bottleneck in a given Python script using the `cProfile` module. **Task Description:** You are provided with a Python script that performs multiple computations. The script is known to have a performance bottleneck, and the goal is to profile the script, identify the bottleneck, and optimize it. **Python Script (script.py):** ```python import time def slow_function(): result = 0 for i in range(10000): time.sleep(0.001) # Simulate a time-consuming computation result += i return result def fast_function(): return sum(range(10000)) def main(): slow_result = slow_function() fast_result = fast_function() return slow_result, fast_result if __name__ == \\"__main__\\": main() ``` **Requirements:** 1. Use the `cProfile` module to profile the provided script and identify the performance bottleneck. 2. Implement an optimization strategy to improve the performance of the identified bottleneck. 3. Provide a detailed report of the profiling results and the changes made to optimize the script. **Input:** - No direct input is required. The script to be profiled is provided. **Output:** - Optimized version of the script. - Profiling report showing the performance before and after optimization. **Constraints:** - You must use the `cProfile` module to analyze the script. - The optimization should be implemented within the constraints of typical Python coding practices. **Performance Requirements:** - The optimized code should execute significantly faster than the original version. # Submission: - Submit the optimized script as `optimized_script.py`. - Submit the profiling report as `profiling_report.txt` which must include: - The `cProfile` output for the original script. - A brief explanation of the identified bottleneck. - Description of the changes made to optimize the code. - The `cProfile` output for the optimized script demonstrating the performance improvement. **Evaluation Criteria:** - Correct usage of the `cProfile` module. - Effectiveness of the optimization. - Clarity and completeness of the profiling report. - Overall code quality and adherence to best practices.","solution":"import time def slow_function_optimized(): result = sum(range(10000)) return result def fast_function(): return sum(range(10000)) def main(): slow_result = slow_function_optimized() fast_result = fast_function() return slow_result, fast_result if __name__ == \\"__main__\\": main()"},{"question":"**Title: Implement and Test a Generic Cache using Python Typing and Unittest** **Objective:** To assess your understanding of Python\'s type hints, generic programming, unit testing, and mock objects. **Problem Statement:** You are required to implement a generic caching system in Python that can store values of any type against string keys. The cache should have the following functionalities: 1. **Add an item to the cache**: Store a value of any type with a string key. 2. **Retrieve an item from the cache**: Retrieve the value stored with a given key. 3. **Invalidate an item in the cache**: Remove an item from the cache by its key. 4. **Invalidate the entire cache**: Clear all items in the cache. Additionally, you must write unit tests for your cache implementation using the `unittest` framework. You should mock any dependencies if necessary. **Implementation Details:** 1. Define a generic `Cache` class that uses type hints to specify that it can store any type of object. 2. Implement the following methods in the `Cache` class: - `add_item(key: str, value: T) -> None`: Adds an item to the cache. - `get_item(key: str) -> Optional[T]`: Retrieves an item from the cache. Returns `None` if the key is not present. - `invalidate_item(key: str) -> None`: Removes an item from the cache. - `invalidate_all() -> None`: Clears the entire cache. 3. Write unit tests for each method using the `unittest` framework. Ensure that you mock any dependencies to isolate the unit under test. **Constraints:** - You must use Python\'s `typing` module to define the generic cache. - Use the `unittest` module for writing your test cases. - Assume that the cache is initially empty. - Performance requirements: Your implementation should have O(1) average-time complexity for add, get, and invalidate operations. **Expected Input and Output:** ```python from typing import TypeVar, Generic, Optional T = TypeVar(\'T\') class Cache(Generic[T]): def __init__(self): # Initialize the cache storage pass def add_item(self, key: str, value: T) -> None: # Implement the method to add an item. pass def get_item(self, key: str) -> Optional[T]: # Implement the method to get an item. pass def invalidate_item(self, key: str) -> None: # Implement the method to invalidate an item. pass def invalidate_all(self) -> None: # Implement the method to invalidate all items. pass # Unit Tests import unittest from unittest.mock import patch class TestCache(unittest.TestCase): def setUp(self): self.cache = Cache[int]() def test_add_and_get_item(self): self.cache.add_item(\\"key1\\", 100) self.assertEqual(self.cache.get_item(\\"key1\\"), 100) def test_get_item_not_found(self): self.assertIsNone(self.cache.get_item(\\"non_existing_key\\")) def test_invalidate_item(self): self.cache.add_item(\\"key2\\", 200) self.cache.invalidate_item(\\"key2\\") self.assertIsNone(self.cache.get_item(\\"key2\\")) def test_invalidate_all(self): self.cache.add_item(\\"key1\\", 100) self.cache.add_item(\\"key2\\", 200) self.cache.invalidate_all() self.assertIsNone(self.cache.get_item(\\"key1\\")) self.assertIsNone(self.cache.get_item(\\"key2\\")) if __name__ == \'__main__\': unittest.main() ``` **Explanation:** 1. **`Cache` Class**: A generic class that uses a dictionary to store key-value pairs. 2. **Unit Tests**: A set of tests implemented using `unittest` to ensure that all methods work correctly. Mocking is employed if there are external dependencies. Implement the `Cache` class and ensure all tests pass to demonstrate your understanding of Python typing, unit testing, and mocks.","solution":"from typing import TypeVar, Generic, Optional, Dict T = TypeVar(\'T\') class Cache(Generic[T]): def __init__(self): self._storage: Dict[str, T] = {} def add_item(self, key: str, value: T) -> None: self._storage[key] = value def get_item(self, key: str) -> Optional[T]: return self._storage.get(key) def invalidate_item(self, key: str) -> None: if key in self._storage: del self._storage[key] def invalidate_all(self) -> None: self._storage.clear()"},{"question":"# **Advanced Clustering Assessment with K-Means** You are tasked with clustering customer data for a retail company to segment their customer base into meaningful groups. The segmentation will help the company tailor their marketing strategies accordingly. You are given a dataset of customer features and required to implement K-Means clustering on this dataset and evaluate the results. **Dataset** The dataset (`customers.csv`) provided contains the following columns: - `CustomerID` - `Age` - `Annual Income (k)` - `Spending Score (1-100)` **Requirements** 1. **Read the dataset**: - Load the `customers.csv` file into a DataFrame using Pandas. 2. **Preprocess the data**: - Extract the features (`Age`, `Annual Income (k)`, `Spending Score (1-100)`) for clustering. 3. **Implement K-Means clustering**: - Implement the K-Means clustering algorithm using `scikit-learn`. - Choose an appropriate number of clusters (`k`). Justify your choice with reasoning or any hyperparameter tuning method. 4. **Evaluate the clustering performance**: - Calculate the within-cluster sum-of-squares (inertia). - Use silhouette score to evaluate the quality of the clusters formed. 5. **Visualize the clusters**: - If clustering is performed on more than two dimensions, visualize clusters using a pairplot or a 3D plot. **Expected Input and Output** **Input**: A CSV file named `customers.csv`. **Output**: - Within-cluster sum-of-squares (inertia). - Silhouette score for clustering. - Visualization of clusters. **Constraints** - Use only the specified attributes for clustering. - Your implementation should handle missing values in the dataset, if there are any. - Provide detailed comments to explain each step of your implementation. **Performance Requirements** - Optimize your code for performance considering large datasets. - Ensure the runtime is within acceptable limits for a dataset up to 10,000 rows. **Implementation** ```python import pandas as pd from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt # Uncomment if you choose to visualize in 3D # from mpl_toolkits.mplot3d import Axes3D # 1. Read the dataset df = pd.read_csv(\'customers.csv\') # 2. Preprocess the data # Handle missing values if any: df = df.dropna() # Extract relevant features for clustering X = df[[\'Age\', \'Annual Income (k)\', \'Spending Score (1-100)\']] # 3. Implement K-Means clustering # Choose the appropriate number of clusters k = 5 # You can justify or tune this value # Fit the KMeans model kmeans = KMeans(n_clusters=k, random_state=42).fit(X) labels = kmeans.labels_ # 4. Evaluate the clustering performance inertia = kmeans.inertia_ sil_score = silhouette_score(X, labels) print(f\\"Within-Cluster Sum-of-Squares (Inertia): {inertia}\\") print(f\\"Silhouette Score: {sil_score}\\") # 5. Visualize the clusters plt.figure(figsize=(10, 7)) plt.scatter(X[\'Annual Income (k)\'], X[\'Spending Score (1-100)\'], c=labels, cmap=\'viridis\') plt.xlabel(\'Annual Income (k)\') plt.ylabel(\'Spending Score (1-100)\') plt.title(\'Customer Segmentation\') plt.colorbar() plt.show() # Uncomment if you choose to visualize in 3D # fig = plt.figure() # ax = fig.add_subplot(111, projection=\'3d\') # ax.scatter(X[\'Age\'], X[\'Annual Income (k)\'], X[\'Spending Score (1-100)\'], c=labels, cmap=\'viridis\') # ax.set_xlabel(\'Age\') # ax.set_ylabel(\'Annual Income (k)\') # ax.set_zlabel(\'Spending Score (1-100)\') # plt.show() ``` **Submission** Submit your solution in a Jupyter Notebook with the necessary explanations and code. Ensure that it runs end-to-end without errors.","solution":"import pandas as pd from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt # Uncomment if you choose to visualize in 3D # from mpl_toolkits.mplot3d import Axes3D def perform_clustering(file_path, n_clusters): # 1. Read the dataset df = pd.read_csv(file_path) # 2. Preprocess the data df = df.dropna() # Handle missing values if any # Extract relevant features for clustering X = df[[\'Age\', \'Annual Income (k)\', \'Spending Score (1-100)\']] # 3. Implement K-Means clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans.fit(X) labels = kmeans.labels_ # 4. Evaluate the clustering performance inertia = kmeans.inertia_ sil_score = silhouette_score(X, labels) # 5. Visualize the clusters plt.figure(figsize=(10, 7)) plt.scatter(X[\'Annual Income (k)\'], X[\'Spending Score (1-100)\'], c=labels, cmap=\'viridis\') plt.xlabel(\'Annual Income (k)\') plt.ylabel(\'Spending Score (1-100)\') plt.title(\'Customer Segmentation\') plt.colorbar() plt.show() # Uncomment if you choose to visualize in 3D # fig = plt.figure() # ax = fig.add_subplot(111, projection=\'3d\') # ax.scatter(X[\'Age\'], X[\'Annual Income (k)\'], X[\'Spending Score (1-100)\'], c=labels, cmap=\'viridis\') # ax.set_xlabel(\'Age\') # ax.set_ylabel(\'Annual Income (k)\') # ax.set_zlabel(\'Spending Score (1-100)\') # plt.show() return inertia, sil_score # Example usage: # inertia, sil_score = perform_clustering(\'customers.csv\', 5) # print(f\\"Within-Cluster Sum-of-Squares (Inertia): {inertia}\\") # print(f\\"Silhouette Score: {sil_score}\\")"},{"question":"# Unique Identifier Management System **Problem Statement:** You are tasked with designing a Unique Identifier Management System (UIMS) that allows users to generate, view, and interpret UUIDs. Your system should use the `uuid` module from Python\'s standard library, and it must implement functionality according to the following requirements: 1. Implement a function `generate_uuid(method: str, **kwargs) -> uuid.UUID`: - `method`: A string that specifies the UUID generation method. It can be one of \'uuid1\', \'uuid3\', \'uuid4\', or \'uuid5\'. - `kwargs`: Additional arguments required for specific methods: - For \'uuid3\' and \'uuid5\', `namespace` (must be a `UUID` object) and `name` (must be a string) are required. - For \'uuid1\', `node` (optional 48-bit integer) and `clock_seq` (optional 14-bit integer). - The function should return the generated `UUID` object. 2. Implement a function `uuid_info(uuid_obj: uuid.UUID) -> dict`: - `uuid_obj`: A UUID object that needs to be interpreted. - This function should return a dictionary containing: - `hex`: String representation of the UUID in hexadecimal form. - `int`: Integer representation of the UUID. - `urn`: URN representation of the UUID. - `variant`: The variant of the UUID. - `version`: The version of the UUID. - `is_safe`: Whether the UUID was generated in a multiprocessing-safe way (only relevant for `uuid1`). 3. Implement a function `compare_uuids(uuid1: uuid.UUID, uuid2: uuid.UUID) -> str`: - `uuid1` and `uuid2`: UUID objects to be compared. - The function should return a string \'equal\' if both UUIDs are equal, otherwise return \'not equal\'. # Input Format: - The system will call `generate_uuid` with method-specific parameters. - The system will call `uuid_info` with a UUID object. - The system will call `compare_uuids` with two UUID objects. # Output Format: - `generate_uuid` will return a UUID object. - `uuid_info` will return a dictionary with UUID details. - `compare_uuids` will return a string indicating if the UUIDs are equal or not. # Example: ```python # Example Usage: import uuid uim_system = YourUUIDManagement() # Generate a UUID using method uuid4: uuid4_obj = uim_system.generate_uuid(method=\'uuid4\') print(uuid4_obj) # Example: UUID(\'16fd2706-8baf-433b-82eb-8c7fada847da\') # Interpret the UUID object info = uim_system.uuid_info(uuid4_obj) print(info) # Output: # { # \'hex\': \'16fd27068baf433b82eb8c7fada847da\', # \'int\': 282996086653317610645949747102833383930, # \'urn\': \'urn:uuid:16fd2706-8baf-433b-82eb-8c7fada847da\', # \'variant\': \'RFC_4122\', # \'version\': 4, # \'is_safe\': \'n/a\' # not applicable for uuid4 # } # Generate two UUIDs using uuid3 method namespace = uuid.uuid4() # Typically, namespace would be a standard UUID like uuid.NAMESPACE_DNS uuid3_obj_1 = uim_system.generate_uuid(method=\'uuid3\', namespace=namespace, name=\'example1\') uuid3_obj_2 = uim_system.generate_uuid(method=\'uuid3\', namespace=namespace, name=\'example2\') # Compare UUIDs comparison_result = uim_system.compare_uuids(uuid3_obj_1, uuid3_obj_2) print(comparison_result) # Output: \'not equal\' ``` # Constraints: 1. Ensure that the `name` parameter for `uuid3` and `uuid5` is not empty. 2. Validate the input parameters for each method to avoid incorrect UUID generation. 3. Use the appropriate attributes and methods provided in the `uuid` module. Implement the `YourUUIDManagement` class with the three specified methods to build this Unique Identifier Management System.","solution":"import uuid class YourUUIDManagement: def generate_uuid(self, method: str, **kwargs) -> uuid.UUID: if method == \'uuid1\': node = kwargs.get(\'node\', None) clock_seq = kwargs.get(\'clock_seq\', None) return uuid.uuid1(node, clock_seq) elif method == \'uuid3\': namespace = kwargs.get(\'namespace\') name = kwargs.get(\'name\') if not (namespace and name): raise ValueError(\\"Method \'uuid3\' requires \'namespace\' and \'name\' arguments\\") return uuid.uuid3(namespace, name) elif method == \'uuid4\': return uuid.uuid4() elif method == \'uuid5\': namespace = kwargs.get(\'namespace\') name = kwargs.get(\'name\') if not (namespace and name): raise ValueError(\\"Method \'uuid5\' requires \'namespace\' and \'name\' arguments\\") return uuid.uuid5(namespace, name) else: raise ValueError(\\"Invalid method specified. Choose from \'uuid1\', \'uuid3\', \'uuid4\', \'uuid5\'.\\") def uuid_info(self, uuid_obj: uuid.UUID) -> dict: info = { \'hex\': uuid_obj.hex, \'int\': uuid_obj.int, \'urn\': uuid_obj.urn, \'variant\': uuid_obj.variant, \'version\': uuid_obj.version, \'is_safe\': uuid_obj.is_safe.name if isinstance(uuid_obj, uuid.UUID) and uuid_obj.version == 1 else \'n/a\' } return info def compare_uuids(self, uuid1: uuid.UUID, uuid2: uuid.UUID) -> str: return \'equal\' if uuid1 == uuid2 else \'not equal\'"},{"question":"Objective Create a function using the `seaborn` library to generate a customized ECDF plot from the \\"penguins\\" dataset provided by seaborn. Your plot should include the following functionalities: 1. Plot a univariate distribution of `flipper_length_mm` along the x-axis. 2. Overlay multiple ECDF plots using different columns (`bill_length_mm`, `bill_depth_mm`) with hue mapping for the `species`. 3. Add a secondary ECDF plot showing the complementary CDF for `body_mass_g` on the same axes but along the y-axis. Instructions 1. Use the seaborn library for all of your plotting. 2. The resulting plot should have proper titles and labels for better readability. 3. Your function should take no arguments. 4. Include comments in your code to explain each step. 5. Return the figure created by your function. Function Signature ```python def customized_ecdf_plot(): pass ``` Example Output ![](example_output.png) # This is just an illustrative example. Your actual ECDF plots may look different. You are encouraged to refer to the seaborn documentation to ensure accurate usage of the library\'s features for this task.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_ecdf_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a figure and an axis object fig, ax1 = plt.subplots(figsize=(10, 6)) # Plot ECDF for flipper_length_mm with species hue sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", ax=ax1) # Overlay ECDFs for bill_length_mm and bill_depth_mm with species hue sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", ax=ax1, linestyle=\\"--\\", legend=False) sns.ecdfplot(data=penguins, x=\\"bill_depth_mm\\", hue=\\"species\\", ax=ax1, linestyle=\\":\\", legend=False) # Create a secondary y-axis ax2 = ax1.twinx() # Plot complementary ECDF for body_mass_g on the secondary y-axis sns.ecdfplot(data=penguins, x=\\"body_mass_g\\", complementary=True, ax=ax2, color=\\"grey\\") # Set titles and labels for better readability ax1.set_title(\\"ECDF Plots of Penguin Measurements\\") ax1.set_xlabel(\\"Measurement values\\") ax1.set_ylabel(\\"Probability\\") ax2.set_ylabel(\\"Complementary Probability (1-P)\\") # Display the plot plt.show() # Return the figure for further use if needed return fig"},{"question":"**Objective:** Implement a custom class that contains nested mutable structures, such as lists and dictionaries. This class should reliably support both shallow and deep copies using the `copy.copy()` and `copy.deepcopy()` methods from the `copy` module. Demonstrate an understanding of potential issues like recursion and how to handle them using the `memo` dictionary. **Requirements:** 1. Your custom class should include at least: - One nested mutable structure (e.g., a list of dictionaries). - A recursive element that references itself. 2. Implement the `__copy__()` and `__deepcopy__()` methods to support shallow and deep copy operations, respectively. 3. Ensure that the deep copy operation correctly handles recursive references to prevent infinite loops. **Input and Output Formats:** - You will define and demonstrate a custom class `MyDataStructure`. - Create an instance of this class with nested mutable structures and self-references. - Display the original instance and both its shallow and deep copies. - Show that modifying the shallow copy affects the original but modifying the deep copy does not. **Constraints:** - You can assume that the nested structures only include lists and dictionaries. - Your implementation should handle reasonably deep recursion without performance issues. **Example:** ```python import copy class MyDataStructure: def __init__(self): self.data = { \'numbers\': [1, 2, 3], \'nested_dict\': {\'key\': \'value\'} } self.data[\'self\'] = self def __copy__(self): new_instance = MyDataStructure() new_instance.data = self.data.copy() new_instance.data[\'self\'] = new_instance return new_instance def __deepcopy__(self, memo): new_instance = MyDataStructure() memo[id(self)] = new_instance new_instance.data = copy.deepcopy(self.data, memo) return new_instance # Demonstration: original = MyDataStructure() shallow_copied = copy.copy(original) deep_copied = copy.deepcopy(original) # Modify deep copy deep_copied.data[\'numbers\'][0] = 99 print(f\\"Original: {original.data[\'numbers\']}\\") # Should show [1, 2, 3] print(f\\"Deep Copied: {deep_copied.data[\'numbers\']}\\") # Should show [99, 2, 3] # Modify shallow copy shallow_copied.data[\'numbers\'][1] = 42 print(f\\"Original: {original.data[\'numbers\']}\\") # Should show [1, 42, 3] print(f\\"Shallow Copied: {shallow_copied.data[\'numbers\']}\\") # Should show [1, 42, 3] ``` Ensure your implementation meets the requirements and adheres to the constraints.","solution":"import copy class MyDataStructure: def __init__(self): self.data = { \'numbers\': [1, 2, 3], \'nested_dict\': {\'key\': \'value\'} } self.data[\'self\'] = self def __copy__(self): new_instance = self.__class__.__new__(self.__class__) new_instance.data = self.data.copy() new_instance.data[\'self\'] = new_instance return new_instance def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] new_instance = self.__class__.__new__(self.__class__) memo[id(self)] = new_instance new_instance.data = copy.deepcopy(self.data, memo) new_instance.data[\'self\'] = new_instance return new_instance # Demonstration original = MyDataStructure() shallow_copied = copy.copy(original) deep_copied = copy.deepcopy(original) # Modify deep copy deep_copied.data[\'numbers\'][0] = 99 print(f\\"Original: {original.data[\'numbers\']}\\") # Should show [1, 2, 3] print(f\\"Deep Copied: {deep_copied.data[\'numbers\']}\\") # Should show [99, 2, 3] # Modify shallow copy shallow_copied.data[\'numbers\'][1] = 42 print(f\\"Original: {original.data[\'numbers\']}\\") # Should show [1, 42, 3] print(f\\"Shallow Copied: {shallow_copied.data[\'numbers\']}\\") # Should show [1, 42, 3]"},{"question":"**Question: Using Seaborn\'s Object Interface for Advanced Plotting** **Objective:** Write a function `plot_network_trajectories()` that loads and processes a specified dataset, and generates a paired path plot using Seaborn that adheres to specific configurations. **Function Signature:** ```python def plot_network_trajectories(dataset_path: str, x_vars: list, y_vars: list, layout_size: tuple, color_var: str) -> None: ``` **Input:** - `dataset_path` (str): The path to the CSV dataset file to be loaded. - `x_vars` (list): List of column names to be used for the x-axis in the pair plot. - `y_vars` (list): List of column names to be used for the y-axis in the pair plot. - `layout_size` (tuple): A tuple specifying the layout size of the plot (width, height). - `color_var` (str): A column name to be used for coloring the plot paths. **Output:** - None. The function should display the plot directly. **Constraints:** - You must use Seaborn objects (imported as `so`) for creating the plot. - The function should handle any necessary processing of the dataset, similar to the dataset manipulation in the provided documentation. **Task:** 1. Load the dataset from the provided `dataset_path` using `seaborn.load_dataset`. 2. Process the dataset: - Rename axis if necessary. - Stack, group, and unstack the data as demonstrated. - Reset the index and apply any relevant filtering. 3. Create a paired path plot using `so.Plot` with the `x_vars` and `y_vars` specified. 4. Configure the plot layout size using the `layout_size` parameter. 5. Set the `color_var` for the plot paths. 6. Display the path plot. **Example Usage:** ```python dataset_path = \\"path/to/your/dataset.csv\\" x_vars = [\\"5\\", \\"8\\", \\"12\\", \\"15\\"] y_vars = [\\"6\\", \\"13\\", \\"16\\"] layout_size = (8, 5) color_var = \\"hemi\\" plot_network_trajectories(dataset_path, x_vars, y_vars, layout_size, color_var) ``` The function should load the dataset from the given path, process it, and produce a plot with paired paths, sharing axes as per the configuration, and coloring based on the specified variable. **Note:** It is advisable to review the example documentation provided to understand the preprocessing and plotting steps.","solution":"import seaborn.objects as so import pandas as pd def plot_network_trajectories(dataset_path: str, x_vars: list, y_vars: list, layout_size: tuple, color_var: str) -> None: Loads and processes a specified dataset, and generates a paired path plot using Seaborn. Args: - dataset_path (str): The path to the CSV dataset file to be loaded. - x_vars (list): List of column names to be used for the x-axis in the pair plot. - y_vars (list): List of column names to be used for the y-axis in the pair plot. - layout_size (tuple): A tuple specifying the layout size of the plot (width, height). - color_var (str): A column name to be used for coloring the plot paths. # Load the dataset df = pd.read_csv(dataset_path) # Create a long-format DataFrame suitable for seaborn\'s relational plots df_long = df.melt(id_vars=[color_var], value_vars=x_vars, var_name=\\"Time\\", value_name=\\"X\\") df_long = df_long.merge(df.melt(id_vars=[color_var], value_vars=y_vars, var_name=\\"Time\\", value_name=\\"Y\\"), on=[\\"Time\\", color_var], suffixes=(\\"_x\\", \\"_y\\")) # Initialize the plot p = so.Plot(df_long, x=\\"X\\", y=\\"Y\\", color=color_var) # Add paths to the plot p = p.add(so.Line()) # Set layout size p = p.layout(size=layout_size) # Display the plot p.show()"},{"question":"Problem Statement You are required to implement an asynchronous function that runs three coroutines concurrently. Each coroutine performs a different task: one simulates an I/O-bound operation, the second simulates a CPU-bound operation, and the third waits for a signal from an external source to complete. Your function should ensure that the coroutines are properly managed without blocking the event loop, handle potential exceptions, and utilize debugging features to log slow operations and any issues with coroutine management. Requirements 1. **Function Signature**: Implement the function `async def manage_tasks()` which should: - Run three coroutines concurrently. - Handle exceptions that might occur within any of the coroutines. - Use asyncio\'s debugging features to log operations that take too long and any coroutine management issues. 2. **Coroutines**: - **I/O-bound Coroutine**: Simulates an I/O-bound task by sleeping asynchronously for a random duration between 1 to 3 seconds. - **CPU-bound Coroutine**: Simulates a CPU-bound task by performing a heavy computation (e.g., calculating Fibonacci numbers). - **Signal Wait Coroutine**: Waits for a signal (e.g., an asyncio sleep representing an external event) for a specified duration. 3. **Debugging and Logging**: - Enable asyncio\'s debug mode. - Log any coroutines that are not awaited or tasks whose exceptions are never retrieved. - Log any slow operations (taking longer than 100ms). Expected Input and Output - The function `manage_tasks()` does not take any input parameters. - The function should log messages indicating the completion of each task and any issues found during execution. Constraints and Limitations - The implementation should not block the event loop. - Use asynchronous programming techniques as demonstrated in the provided documentation. - Ensure that your code can be run with asyncio\'s debug mode enabled. Example ```python import asyncio import logging async def io_bound_task(): # Simulate I/O-bound task await asyncio.sleep(random.uniform(1, 3)) logging.info(\\"I/O-bound task completed\\") def cpu_bound_task(): # Simulate CPU-bound task def fib(n): if n <= 1: return n else: return fib(n-1) + fib(n-2) result = fib(30) logging.info(\\"CPU-bound task completed with result: %d\\", result) async def signal_wait_task(): # Simulate waiting for external signal await asyncio.sleep(2) logging.info(\\"Signal wait task completed\\") async def manage_tasks(): # Implement the function as per the requirements mentioned. pass # To run the manage_tasks() function asyncio.run(manage_tasks(), debug=True) ```","solution":"import asyncio import logging import random # Configure logging logging.basicConfig(level=logging.INFO) async def io_bound_task(): # Simulate I/O-bound task await asyncio.sleep(random.uniform(1, 3)) logging.info(\\"I/O-bound task completed\\") def cpu_bound_task(): # Simulate CPU-bound task def fib(n): if n <= 1: return n else: return fib(n-1) + fib(n-2) result = fib(30) logging.info(\\"CPU-bound task completed with result: %d\\", result) async def signal_wait_task(): # Simulate waiting for external signal await asyncio.sleep(2) logging.info(\\"Signal wait task completed\\") async def manage_tasks(): # Enable asyncio\'s debug mode asyncio.get_running_loop().set_debug(True) # Create tasks tasks = [ asyncio.create_task(io_bound_task(), name=\\"io_bound_task\\"), asyncio.create_task(asyncio.to_thread(cpu_bound_task), name=\\"cpu_bound_task\\"), asyncio.create_task(signal_wait_task(), name=\\"signal_wait_task\\") ] # Log unhandled exceptions for task in tasks: task.add_done_callback(lambda t: logging.error(f\\"Task {t.get_name()} has completed with an exception: {t.exception()}\\") if t.exception() else None) # Await tasks and handle exceptions await asyncio.gather(*tasks, return_exceptions=True) # Log slow operations for task in tasks: if not task.done(): logging.warning(f\\"Task {task.get_name()} is taking longer than expected\\") if __name__ == \\"__main__\\": # Run the manage_tasks function asyncio.run(manage_tasks(), debug=True)"},{"question":"**Objective**: Demonstrate your understanding of the `torch.mps` module for managing GPU devices, memory, random number generation, and event-based synchronization in PyTorch. **Question**: Write a Python function `gpu_operations` that utilizes `torch.mps` functionalities to perform the following tasks: 1. **Check Device Availability**: - Check the number of MPS devices available using `device_count`. If no MPS device is available, return `\\"No MPS device available\\"`. 2. **Memory Management**: - Set the per-process memory fraction to 0.5. - Allocate and release GPU memory effectively by: - Checking current allocated memory. - Allocating a tensor of size 1024x1024 filled with random numbers. - Synchronizing the GPU operations. - Releasing the allocated memory using `empty_cache`. 3. **Random Number Operations**: - Set a manual seed to ensure reproducibility. - Retrieve and print the current random number generator state using `get_rng_state`. 4. **Event Handling**: - Create an MPS event. - Record the event after the tensor allocation. - Synchronize the event. 5. **Return Summary**: - Return a summary dictionary that includes the device count, the result of memory allocation and release operations, and the random number generator state. **Function Signature**: ```python import torch.mps as mps def gpu_operations() -> dict: Perform a series of GPU operations using torch.mps module and return a summary dictionary. Returns: dict: A dictionary containing the results of the performed operations. pass ``` **Constraints**: - Use only the provided `torch.mps` functionalities. - Ensure that all GPU operations are properly synchronized and memory is managed effectively. - The function should handle any potential exceptions and provide meaningful error messages if an operation fails. **Expected Output Format**: The function should return a dictionary with the following keys: - `\\"device_count\\"`: Number of MPS devices. - `\\"memory_allocation_success\\"`: Boolean indicating if the memory allocation and release were successful. - `\\"rng_state\\"`: Byte tensor representing the random number generator state. **Example**: ```python result = gpu_operations() print(result) # Output might look like this: # { # \'device_count\': 1, # \'memory_allocation_success\': True, # \'rng_state\': tensor([...]) # } ``` Good luck and demonstrate your understanding of PyTorch\'s MPS module effectively!","solution":"import torch import torch.mps as mps def gpu_operations() -> dict: Perform a series of GPU operations using torch.mps module and return a summary dictionary. Returns: dict: A dictionary containing the results of the performed operations. summary = {} # Check Device Availability device_count = torch.mps.device_count() summary[\'device_count\'] = device_count if device_count == 0: return {\\"error\\": \\"No MPS device available\\"} try: # Memory Management torch.mps.set_per_process_memory_fraction(0.5) initial_mem = torch.mps.memory_allocated() # Allocate a tensor of size 1024x1024 filled with random numbers tensor = torch.randn((1024, 1024), device=\'mps\') torch.mps.synchronize() mem_after_allocation = torch.mps.memory_allocated() # Releasing the allocated memory del tensor torch.mps.empty_cache() mem_after_release = torch.mps.memory_allocated() memory_allocation_success = initial_mem < mem_after_allocation and mem_after_release < mem_after_allocation summary[\'memory_allocation_success\'] = memory_allocation_success # Random Number Operations torch.manual_seed(42) rng_state = torch.mps.get_rng_state() summary[\'rng_state\'] = rng_state # Event Handling event = torch.mps.Event() event.record() event.synchronize() except Exception as e: return {\\"error\\": str(e)} return summary"},{"question":"Objective: To test your understanding and ability to use the `torch.func` module in PyTorch, particularly focusing on creating pure functions, using transforms like `vmap`, and avoiding common pitfalls such as in-place operations and data-dependent control flows. Problem Statement: Given a function `batch_diag_embed`, you need to rewrite it to ensure compatibility with `torch.func.vmap`, while handling the specific limitations and requirements highlighted in the provided documentation. Initial Function: ```python import torch def batch_diag_embed(vecs): Embeds each row of `vecs` as a diagonal matrix. Parameters: vecs (Tensor): A tensor of shape (batch_size, n) where each row is a 1-D tensor to be embedded as a diagonal matrix. Returns: Tensor: A tensor of shape (batch_size, n, n) where each (i, :, :) slice is the diagonal matrix formed by vecs[i, :]. assert vecs.dim() == 2, \\"Input must be a 2D tensor\\" results = [] for vec in vecs: result = torch.zeros(vec.shape[0], vec.shape[0]) # Potential issue for vmap result.diagonal().copy_(vec) results.append(result) return torch.stack(results) ``` Requirements: - Modify the `batch_diag_embed` function so it fully supports `torch.func.vmap` without raising errors. - Ensure the solution does not use in-place operations unsupported by `vmap`. - The solution should work efficiently with batched tensors and avoid unnecessary use of loops or side effects. Constraints: - Do not use any global variables inside your function. - The input tensor `vecs` will always be a 2D tensor with `batch_size` rows and `n` columns. Expected Input and Output: - Input: `vecs` (Tensor) â€” a tensor of shape `(batch_size, n)`. - Output: A tensor of shape `(batch_size, n, n)` where each row tensor in `vecs` is embedded as a diagonal matrix. Example: ```python import torch from torch.func import vmap vecs = torch.tensor([[0.1, 0.2, 0.3], [4.0, 5.0, 6.0]]) batch_diag_embed_vmapped = vmap(batch_diag_embed)(vecs) # The output tensor should be: # tensor([[[0.1, 0.0, 0.0], # [0.0, 0.2, 0.0], # [0.0, 0.0, 0.3]], # [[4.0, 0.0, 0.0], # [0.0, 5.0, 0.0], # [0.0, 0.0, 6.0]]]) ``` Implement your modified function below: ```python import torch def batch_diag_embed(vecs): # Your code here # pass ``` Hint: - Consider replacing factory functions like `torch.zeros` with their `new_*` equivalents for batched tensor support.","solution":"import torch def batch_diag_embed(vecs): Embeds each row of `vecs` as a diagonal matrix. Parameters: vecs (Tensor): A tensor of shape (batch_size, n) where each row is a 1-D tensor to be embedded as a diagonal matrix. Returns: Tensor: A tensor of shape (batch_size, n, n) where each (i, :, :) slice is the diagonal matrix formed by vecs[i, :]. assert vecs.dim() == 2, \\"Input must be a 2D tensor\\" batch_size, n = vecs.shape result = vecs.new_zeros((batch_size, n, n)) for i in range(n): result[:, i, i] = vecs[:, i] return result"},{"question":"**Custom Interactive Python Shell with Enhanced Readline Features** You are tasked with creating your own custom interactive Python shell utilizing the \\"readline\\" module. Your shell should support the following features: 1. **History Management**: - Load history from a file during startup. - Save the history to a file on exit. - Truncate the history file to the last 1000 entries. 2. **Dynamic History Update**: - Ensure it can append the last session\'s history to a common history file. 3. **Custom Command Completion**: - Implement a custom completion function for commands that completes from predefined commands. - Set the custom completer and manage the completion delimiters. 4. **Key Bindings Configuration**: - Dynamically read and bind keys from an initialization string provided via the command line. **Your task is to implement the following functions in a Python script:** 1. `init_history(histfile)`: Initialize history from a specified history file. 2. `save_history(histfile)`: Save the current session\'s history to the specified history file. 3. `custom_completer(text, state)`: Custom completion function. 4. `configure_readline(init_str)`: Set key bindings and other configurations via an init string. 5. `custom_shell()`: Interactive shell loop that uses the aforementioned features. **Input:** - `histfile`: A string representing the path to the history file. - `init_str`: A string representing the key bindings to be parsed and applied. **Output:** No explicit output, but the custom shell should behave interactively incorporating history, completions, and configured key bindings. **Constraints:** - The shell should exit on receiving an EOF signal (End Of File). - Ensure proper handling of file operations with exceptions (e.g., file not found). - Use `atexit` to ensure history is saved when the shell exits. # Example Usage: ```python # Initialize history and configure readline init_history(\\"~/.custom_python_history\\") configure_readline(\\"tab: complete\\") # Start the custom shell custom_shell() ``` # Example `custom_completer` Implementation: ```python COMMANDS = [\\"start\\", \\"stop\\", \\"pause\\", \\"resume\\", \\"status\\"] def custom_completer(text, state): matches = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(matches): return matches[state] return None ``` # Submission: Submit the script file named `custom_shell.py` containing all the requested functions and the example usage.","solution":"import readline import atexit import os COMMANDS = [\\"start\\", \\"stop\\", \\"pause\\", \\"resume\\", \\"status\\"] def init_history(histfile): if os.path.exists(histfile): readline.read_history_file(histfile) readline.set_history_length(1000) atexit.register(save_history, histfile) def save_history(histfile): readline.write_history_file(histfile) def custom_completer(text, state): matches = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(matches): return matches[state] return None def configure_readline(init_str): for key_binding in init_str.strip().split(\\";\\"): key, command = key_binding.split(\\":\\") readline.parse_and_bind(f\\"{key.strip()}:{command.strip()}\\") def custom_shell(): print(\\"Starting custom interactive shell. Press Ctrl+D to exit.\\") histfile = os.path.expanduser(\\"~/.custom_python_history\\") init_history(histfile) readline.set_completer(custom_completer) readline.parse_and_bind(\'tab: complete\') while True: try: line = input(\\">>> \\") exec(line) except EOFError: print(\\"nExiting custom shell.\\") break except Exception as e: print(f\\"Error: {e}\\")"},{"question":"# XML Parsing with SAX in Python Objective Demonstrate your understanding of the `xml.sax.xmlreader` module by implementing a custom SAX parser to process an XML file incrementally. Problem Statement You are provided with an XML file containing information about a collection of books. Each book has a title, author, genre, and a publication year. Your task is to implement a SAX parser that: 1. Parses the XML file incrementally using the `IncrementalParser` class. 2. Counts and prints the number of books per genre while parsing the file. 3. Collects and prints the titles of books published after the year 2000. Input A string representing the path to the XML file to be parsed. Output 1. Print the number of books per genre. 2. Print the titles of all books published after the year 2000. Example XML File ```xml <library> <book> <title>Book One</title> <author>Author One</author> <genre>Fiction</genre> <year>1999</year> </book> <book> <title>Book Two</title> <author>Author Two</author> <genre>Non-Fiction</genre> <year>2005</year> </book> <book> <title>Book Three</title> <author>Author Three</author> <genre>Fiction</genre> <year>2015</year> </book> </library> ``` Constraints - You must use the `xml.sax.xmlreader.IncrementalParser` class for parsing. - The XML file can be large, so you should read and process it in chunks. - The `year` elements are guaranteed to contain valid integers. Function Signature ```python def parse_books(xml_file_path: str) -> None: pass ``` Requirements 1. Define a custom content handler class that manages XML events (startElement, endElement, characters). 2. Implement the `parse_books` function to: - Initialize and use an `IncrementalParser`. - Implement logic to count books by genre and collect titles based on the publication year. 3. Handle any necessary error checking and resource management (e.g., closing the file).","solution":"import xml.sax from xml.sax.handler import ContentHandler from collections import defaultdict class BookHandler(ContentHandler): def __init__(self): self.current_element = \\"\\" self.current_content = \\"\\" self.genre_counter = defaultdict(int) self.books_after_2000 = [] self.current_title = \\"\\" self.current_genre = \\"\\" self.current_year = 0 def startElement(self, name, attrs): self.current_element = name self.current_content = \\"\\" def endElement(self, name): if name == \\"title\\": self.current_title = self.current_content.strip() elif name == \\"genre\\": self.current_genre = self.current_content.strip() elif name == \\"year\\": self.current_year = int(self.current_content.strip()) elif name == \\"book\\": self.genre_counter[self.current_genre] += 1 if self.current_year > 2000: self.books_after_2000.append(self.current_title) def characters(self, content): self.current_content += content def parse_books(xml_file_path: str) -> None: handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) with open(xml_file_path, \'r\') as file: for line in file: parser.feed(line) print(\\"Number of books per genre:\\") for genre, count in handler.genre_counter.items(): print(f\\"{genre}: {count}\\") print(\\"nTitles of books published after the year 2000:\\") for title in handler.books_after_2000: print(title)"},{"question":"# Question: File Compression and Decompression Utility You are tasked with implementing a utility that compresses and decompresses files using the `zlib` module. Your utility should include functions to handle both operations, and it should be able to compress large data streams effectively. You must also handle potential errors gracefully and ensure the integrity of the data using checksums. Function Specifications 1. **compress_file:** - **Input:** - `input_file` (str): The path to the file that needs to be compressed. - `output_file` (str): The path where the compressed file should be saved. - `compression_level` (int, optional): The level of compression (0 to 9, default is -1). - **Output:** None 2. **decompress_file:** - **Input:** - `input_file` (str): The path to the compressed file. - `output_file` (str): The path where the decompressed file should be saved. - **Output:** None 3. **verify_checksum:** - **Input:** - `original_file` (str): The path to the original file. - `decompressed_file` (str): The path to the decompressed file. - **Output:** - `is_valid` (bool): True if checksums match, False otherwise. Function Details 1. **compress_file:** - Read the contents of the `input_file`. - Compress the data using `zlib.compress` with the specified `compression_level`. - Save the compressed data to `output_file`. 2. **decompress_file:** - Read the compressed data from `input_file`. - Decompress the data using `zlib.decompress`. - Save the decompressed data to `output_file`. 3. **verify_checksum:** - Compute the Adler-32 checksum of the `original_file` and `decompressed_file` using `zlib.adler32`. - Compare the two checksums and return True if they match, False otherwise. Example Usage ```python # Compress a file compress_file(\\"example.txt\\", \\"example.txt.z\\", compression_level=9) # Decompress the file decompress_file(\\"example.txt.z\\", \\"example_decompressed.txt\\") # Verify the data integrity is_valid = verify_checksum(\\"example.txt\\", \\"example_decompressed.txt\\") print(\\"Checksum valid:\\", is_valid) ``` Constraints - Handle errors gracefully, and print meaningful error messages. - Ensure proper file handling to avoid resource leaks. - Check for the existence of input files before attempting to read them. Performance Requirements - The utility should handle large files efficiently. - Implement proper buffer management to optimize memory usage during compression and decompression. Notes - Use `zlib.Z_BEST_COMPRESSION` for maximum compression if no level is provided. - You can use any additional helper functions as necessary.","solution":"import zlib # Function to compress a file def compress_file(input_file, output_file, compression_level=-1): try: # Read the input file with open(input_file, \'rb\') as f: data = f.read() # Compress the data compressed_data = zlib.compress(data, level=compression_level) # Write the compressed data to output file with open(output_file, \'wb\') as f: f.write(compressed_data) except Exception as e: print(f\\"Error during compression: {e}\\") # Function to decompress a file def decompress_file(input_file, output_file): try: # Read the compressed file with open(input_file, \'rb\') as f: compressed_data = f.read() # Decompress the data decompressed_data = zlib.decompress(compressed_data) # Write the decompressed data to output file with open(output_file, \'wb\') as f: f.write(decompressed_data) except Exception as e: print(f\\"Error during decompression: {e}\\") # Function to verify checksum def verify_checksum(original_file, decompressed_file): try: # Compute checksum for the original file with open(original_file, \'rb\') as f: original_data = f.read() original_checksum = zlib.adler32(original_data) # Compute checksum for the decompressed file with open(decompressed_file, \'rb\') as f: decompressed_data = f.read() decompressed_checksum = zlib.adler32(decompressed_data) # Compare checksums return original_checksum == decompressed_checksum except Exception as e: print(f\\"Error during checksum verification: {e}\\") return False"},{"question":"# Question: Persistence Layer with Python Serialization and SQLite Database You are tasked with creating a persistence layer for complex data structures using Python. You will use the `pickle` module for object serialization and the `sqlite3` module for database storage. Requirements 1. **Define a class** `Person` with the following attributes: - `name` (string) - `age` (integer) - `address` (string) 2. **Serialization and Deserialization Functions**: - Implement a function `serialize_person(person: Person) -> bytes` that takes an instance of `Person` and returns a serialized `bytes` object using the `pickle` module. - Implement a function `deserialize_person(data: bytes) -> Person` that takes a serialized `bytes` object and returns a `Person` instance. 3. **SQLite Database Operations**: - Implement a function `save_person_to_db(db_path: str, person: Person) -> None` that saves a serialized `Person` object into an SQLite database. The database should have a table named `people` with columns `name` (TEXT), `age` (INTEGER), `address` (TEXT), and `data` (BLOB), where `data` will store the serialized `Person` object. - Implement a function `load_person_from_db(db_path: str, name: str) -> Person` that loads a `Person` object from the SQLite database by the `name` field, deserializing the stored `data` to reconstruct the `Person` object. Input and Output Formats 1. Class Definition: ```python class Person: def __init__(self, name: str, age: int, address: str): self.name = name self.age = age self.address = address ``` 2. Serialization and Deserialization Example: ```python person = Person(\\"John Doe\\", 30, \\"123 Elm St\\") serialized_data = serialize_person(person) deserialized_person = deserialize_person(serialized_data) ``` 3. Database Save and Load Example: ```python db_path = \\"people.db\\" person = Person(\\"Jane Doe\\", 28, \\"456 Maple Ave\\") save_person_to_db(db_path, person) loaded_person = load_person_from_db(db_path, \\"Jane Doe\\") ``` Constraints - The `Person` class instances must be correctly serialized and deserialized. - The SQLite database operations must handle potential exceptions gracefully (e.g., database file not found, no matching record). - The solution should be efficient in terms of both time and space complexity. Performance Requirements - The solution should be capable of handling at least 1000 person records efficiently. - Serialization and deserialization operations should complete within a reasonable time frame (e.g., < 100ms for each operation on average-sized objects). ```python # Define your Person class here # Implement the serialize_person function # Implement the deserialize_person function # Implement the save_person_to_db function # Implement the load_person_from_db function ```","solution":"import pickle import sqlite3 class Person: def __init__(self, name: str, age: int, address: str): self.name = name self.age = age self.address = address def serialize_person(person: Person) -> bytes: Serializes the Person instance into bytes using pickle. return pickle.dumps(person) def deserialize_person(data: bytes) -> Person: Deserializes the bytes back into a Person instance using pickle. return pickle.loads(data) def save_person_to_db(db_path: str, person: Person) -> None: Saves the serialized Person instance to the SQLite database. conn = sqlite3.connect(db_path) cur = conn.cursor() cur.execute( CREATE TABLE IF NOT EXISTS people ( name TEXT PRIMARY KEY, age INTEGER, address TEXT, data BLOB ) ) serialized_data = serialize_person(person) cur.execute(\\"REPLACE INTO people (name, age, address, data) VALUES (?, ?, ?, ?)\\", (person.name, person.age, person.address, serialized_data)) conn.commit() conn.close() def load_person_from_db(db_path: str, name: str) -> Person: Loads and deserializes the Person instance from the SQLite database by name. conn = sqlite3.connect(db_path) cur = conn.cursor() cur.execute(\\"SELECT data FROM people WHERE name=?\\", (name,)) row = cur.fetchone() conn.close() if row: return deserialize_person(row[0]) else: raise ValueError(f\\"No person found with the name \'{name}\'\\")"},{"question":"You are given a dataset containing hourly temperature recordings over several months. Each recording consists of the following columns: `timestamp` (in `YYYY-MM-DD HH:MM:SS` format) and `temperature` (in degrees Celsius). Write a function `resample_temperature_data` that performs the following operations on the dataset: 1. **Input**: - `df`: A pandas DataFrame with two columns: `timestamp` and `temperature`. 2. **Operations**: 1. Resample the data to obtain the daily average temperature. 2. Calculate and add three new columns to the DataFrame: - The daily maximum temperature (`daily_max`). - The daily minimum temperature (`daily_min`). - The standard deviation of the temperature for each day (`daily_std`). 3. Fill any missing values in the `temperature` column using forward fill method. 3. **Output**: - Return the modified pandas DataFrame with columns: `timestamp`, `daily_avg`, `daily_max`, `daily_min`, and `daily_std`. **Constraints**: - The `timestamp` column is of a datetime type, and the data is sorted in ascending order. - You must use the pandas resampling techniques to achieve the desired results, rather than implementing the logic manually. **Performance Requirements**: - The function should handle large datasets efficiently. - You should avoid using iterative loops. Use pandas built-in functions and methods instead. ```python import pandas as pd def resample_temperature_data(df: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass # Example usage: # df = pd.DataFrame({ # \'timestamp\': pd.date_range(start=\'2023-01-01\', periods=24*90, freq=\'H\'), # \'temperature\': np.random.randn(24*90) * 10 + 20 # }) # df_result = resample_temperature_data(df) # print(df_result.head()) ```","solution":"import pandas as pd def resample_temperature_data(df: pd.DataFrame) -> pd.DataFrame: # Ensure the timestamp column is of datetime type df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']) # Use forward fill method to fill any missing values df[\'temperature\'] = df[\'temperature\'].fillna(method=\'ffill\') # Set the timestamp as the index to use resampling df.set_index(\'timestamp\', inplace=True) # Resample the data to daily frequency daily_df = df.resample(\'D\').agg({ \'temperature\': [\'mean\', \'max\', \'min\', \'std\'] }).reset_index() # Rename the columns appropriately daily_df.columns = [\'timestamp\', \'daily_avg\', \'daily_max\', \'daily_min\', \'daily_std\'] return daily_df"},{"question":"**Question: Handling and Manipulating MemoryView Objects in Python** You are provided with byte data which contains binary representation of some numerical data. Your task is to implement a function that will: 1. Convert the byte data into a memoryview object. 2. Extract a subset of the data, ensuring the data remains contiguous. 3. Perform some transformation on the extracted data. 4. Create an output as a Python list from the transformed memoryview object. # Function Signature ```python def process_memoryview_data(data: bytes, start: int, end: int, transformation: callable) -> list: Convert byte data to a memoryview, extract a subset, apply a transformation and return a list. Parameters: - data (bytes): The input byte data. - start (int): The starting index of the subset to extract. - end (int): The ending index of the subset to extract. - transformation (callable): A function that takes a memoryview object and returns a transformed memoryview object. Returns: - list: A list containing the transformed data. # Your implementation here ``` # Input and Output Format - The input `data` is a `bytes` object. - The `start` and `end` parameters define the indices for the subset extraction. - The `transformation` is a function that takes and returns a memoryview object. - The function should return a list of integers or floats depending on the transformation. # Constraints and Considerations - Ensure that the memoryview objects remain contiguous after slicing. - Handle cases where the start and end indices could be out of the byte data range gracefully. - The transformation function could involve converting each byte to an integer, performing arithmetic operations, etc. # Example ```python def example_transformation(mv: memoryview) -> memoryview: # Example transformation: increment each byte by 1 return memoryview(bytearray([b+1 for b in mv])) data = b\'x01x02x03x04x05x06x07x08\' start = 2 end = 6 result = process_memoryview_data(data, start, end, example_transformation) print(result) # Output: [4, 5, 6, 7] ``` In this example, the `process_memoryview_data` function will create a memoryview from the byte data, extract the bytes from index 2 to 6, apply the transformation that increments each byte by 1, and then convert the final memoryview back to a list of integers.","solution":"def process_memoryview_data(data: bytes, start: int, end: int, transformation: callable) -> list: Convert byte data to a memoryview, extract a subset, apply a transformation and return a list. Parameters: - data (bytes): The input byte data. - start (int): The starting index of the subset to extract. - end (int): The ending index of the subset to extract. - transformation (callable): A function that takes a memoryview object and returns a transformed memoryview object. Returns: - list: A list containing the transformed data. # Convert the byte data to a memoryview object mv = memoryview(data) # Ensure the start and end indices are within range if start < 0: start = 0 if end > len(data): end = len(data) # Extract the subset of the data subset_mv = mv[start:end] # Apply the transformation transformed_mv = transformation(subset_mv) # Convert the transformed memoryview into a list return list(transformed_mv)"},{"question":"# Asynchronous Web Scraping with Subprocesses and Queues **Objective:** Implement an asynchronous web scraping solution using the `asyncio` library\'s tasks, queues, and subprocesses to scrape data from multiple URLs simultaneously. **Problem Statement:** You are required to write a function `async_scraper(urls: List[str]) -> List[str]` that: 1. Accepts a list of URLs as input. 2. Utilizes asyncio to create tasks to scrape data from each URL concurrently. 3. Uses an asyncio queue to manage URLs and ensure that no more than 5 concurrent requests are made at any time. 4. For each URL, download the webpage content using a subprocess (e.g., `curl` command). 5. Collects and returns the content of each webpage as a list of strings. **Function Signature:** ```python from typing import List async def async_scraper(urls: List[str]) -> List[str]: pass ``` **Constraints:** 1. You must use the `asyncio` library for concurrency. 2. You must use an asyncio queue to handle the URLs. 3. Limit the number of concurrent tasks to 5. 4. Use the `asyncio.create_subprocess_exec` function to call `curl` for downloading webpage content. **Input:** - `urls`: A list of URLs to scrape. Each URL is a string (1 <= len(urls) <= 50). **Output:** - A list of strings, each containing the content of a webpage corresponding to each URL. **Example:** ```python urls = [\\"http://example.com\\", \\"http://example.org\\"] # Example output (the actual page contents will vary): contents = await async_scraper(urls) print(contents) ``` **Notes:** - Ensure proper error handling for subprocess execution. - Test cases will include a mix of valid and invalid URLs to test the robustness of your code. - The order of the returned list should correspond to the order of the URLs list provided. **Hints:** - Use `asyncio.Queue` for managing the URLs. - Use `asyncio.create_task` to create and run tasks. - Use `asyncio.gather` to collect results from all tasks. Good luck with your implementation!","solution":"import asyncio from asyncio.subprocess import create_subprocess_exec, PIPE from typing import List async def fetch_url_content(url: str) -> str: process = await create_subprocess_exec( \'curl\', \'-s\', url, stdout=PIPE, stderr=PIPE ) stdout, stderr = await process.communicate() if process.returncode != 0: return f\\"Error: {stderr.decode()}\\" return stdout.decode() async def worker(queue: asyncio.Queue, results: List[str]): while True: url = await queue.get() if url is None: break content = await fetch_url_content(url) results.append(content) queue.task_done() async def async_scraper(urls: List[str]) -> List[str]: results = [] queue = asyncio.Queue() for url in urls: await queue.put(url) tasks = [] for _ in range(5): task = asyncio.create_task(worker(queue, results)) tasks.append(task) await queue.join() for _ in range(5): await queue.put(None) await asyncio.gather(*tasks) return results"},{"question":"Objective: - Implement a function that utilizes seaborn\'s `stripplot` and `catplot` to visualize specific aspects of a given dataset. - Demonstrate comprehension of seaborn\'s ability to: - Handle univariate and bivariate data visualization. - Utilize the `hue` parameter to depict multidimensional relationships. - Customize plot aesthetics. - Create facet grids for more complex visual analysis. Problem Statement: You are provided with a dataset containing information about restaurant tips. You need to implement a function `visualize_tips_data` to generate and save specific plots based on this dataset. Your function should: 1. **Generate and save a strip plot**: - Display the distribution of the `total_bill` column. - Split these strips by `day` on the y-axis. - Use the `sex` column as `hue` to show the distribution of bills by gender. - Disable jittering for this plot. 2. **Generate and save a faceted categorical plot**: - Display the average `total_bill` grouped by `day` and `time`. - Facet the plot by `sex`. - The x-axis should represent `day`, and the y-axis should represent `total_bill`. - Ensure each plot has an aspect ratio of 0.75 for better visualization. Save the respective plots as `stripplot_output.png` and `catplot_output.png`. Function Signature: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(dataset: pd.DataFrame) -> None: pass ``` Input: - `dataset` (pd.DataFrame): A DataFrame containing tip data. Constraints: - You must use seaborn to create the plots. - Ensure your code is efficient and the plots are clearly labeled. Expected Output: Your function should save two plots in .png format: 1. `stripplot_output.png`: The strip plot. 2. `catplot_output.png`: The faceted categorical plot. Example Usage: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") visualize_tips_data(tips) ``` Dataset Description: The dataset contains the following columns among others: - `total_bill`: Total bill amount. - `tip`: Tip amount. - `sex`: Gender of the person paying the bill. - `smoker`: Whether the person is a smoker or not. - `day`: Day of the week. - `time`: Time of the day (Lunch or Dinner). - `size`: Size of the group. You can use this information to understand the kind of visualizations expected. Additional Information: You may reference Seaborn\'s documentation for `stripplot` and `catplot` to aid in implementing the function.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(dataset: pd.DataFrame) -> None: # Generate the strip plot plt.figure(figsize=(10, 6)) sns.stripplot(data=dataset, x=\'total_bill\', y=\'day\', hue=\'sex\', jitter=False) plt.title(\'Total Bill Distribution by Day and Gender\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Day\') plt.legend(title=\'Gender\') plt.savefig(\'stripplot_output.png\') plt.close() # Generate the faceted categorical plot cat_plot = sns.catplot(data=dataset, x=\'day\', y=\'total_bill\', hue=\'time\', col=\'sex\', kind=\'strip\', height=5, aspect=0.75) cat_plot.set_titles(\\"{col_name} Gender\\") cat_plot.set_axis_labels(\\"Day\\", \\"Total Bill\\") plt.savefig(\'catplot_output.png\') plt.close()"},{"question":"Objective Implement a program that compresses and decompresses a given text file using the `lzma` module in Python. The program should handle optional custom filter chains for compression and verify the integrity of the compressed data during decompression. Requirements 1. **Compression Function (`compress_file`)**: - Takes three parameters: `input_file` (str), `output_file` (str), and `filters` (list, optional). - Reads the content from `input_file`. - Compresses the content using the `lzma` module. - Writes the compressed data to `output_file`. 2. **Decompression Function (`decompress_file`)**: - Takes two parameters: `input_file` (str), `output_file` (str). - Reads the compressed content from `input_file`. - Decompresses the content using the `lzma` module. - Writes the decompressed data to `output_file`. 3. **Integrity Check**: - During decompression, verify if the integrity check passed correctly. - If the integrity check fails, raise an `lzma.LZMAError`. 4. **Handle Exceptions**: - Ensure proper handling of file I/O exceptions and `lzma.LZMAError`. Input - `compress_file`: - `input_file`: Path to the text file to be compressed. - `output_file`: Path where the compressed file should be saved. - `filters`: Optional list of custom filter dictionaries to be used during compression. - `decompress_file`: - `input_file`: Path to the compressed file. - `output_file`: Path where the decompressed file should be saved. Output - The functions do not return any values. - The content of `output_file` should match the original content of `input_file` after decompression. - Raise exceptions appropriately for invalid operations. Constraints - The input file must exist and be a readable text file. - The output file paths must be writable locations. Performance - Aim for efficient handling of file I/O and memory usage. Example Usage ```python filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6} ] # Compress the file \'input.txt\' to \'compressed.xz\' compress_file(\'input.txt\', \'compressed.xz\', filters) # Decompress the file \'compressed.xz\' back to \'decompressed.txt\' decompress_file(\'compressed.xz\', \'decompressed.txt\') ``` # Implementation ```python import lzma import os def compress_file(input_file: str, output_file: str, filters: list = None): try: with open(input_file, \'r\') as f: data = f.read().encode(\'utf-8\') with lzma.open(output_file, \'wb\', filters=filters) as f: f.write(data) except (OSError, lzma.LZMAError) as e: print(f\\"Error during compression: {e}\\") raise def decompress_file(input_file: str, output_file: str): try: with lzma.open(input_file, \'rb\') as f: data = f.read() with open(output_file, \'w\') as f: f.write(data.decode(\'utf-8\')) if f.check != lzma.CHECK_UNKNOWN and not lzma.is_check_supported(f.check): raise lzma.LZMAError(\\"Integrity check failed\\") except (OSError, lzma.LZMAError) as e: print(f\\"Error during decompression: {e}\\") raise ``` Notes - Ensure you have the proper permissions for reading and writing the specified files. - Test the functions with different file sizes and custom filter chains to validate correctness and performance.","solution":"import lzma import os def compress_file(input_file: str, output_file: str, filters: list = None): Compresses the given input file and writes to the output file using lzma compression. Args: input_file (str): Path to the input file to be compressed. output_file (str): Path where the compressed output will be written. filters (list, optional): List of custom filter dictionaries for compression. Raises: OSError: If there\'s an I/O error. lzma.LZMAError: If there\'s an error with lzma compression. try: # Read the input file\'s content with open(input_file, \'rb\') as f: data = f.read() # Compress the data with or without custom filters with lzma.open(output_file, \'wb\', filters=filters) as f: f.write(data) except (OSError, lzma.LZMAError) as e: print(f\\"Error during compression: {e}\\") raise def decompress_file(input_file: str, output_file: str): Decompresses the given input file and writes the decompressed data to the output file. Args: input_file (str): Path to the compressed input file. output_file (str): Path where the decompressed output will be written. Raises: OSError: If there\'s an I/O error. lzma.LZMAError: If there\'s an error with lzma decompression or if integrity check fails. try: # Read the compressed input file\'s content with lzma.open(input_file, \'rb\') as f: data = f.read() # Write the decompressed data to the output file with open(output_file, \'wb\') as f: f.write(data) if lzma.CHECK_CRC32 and not lzma.is_check_supported(lzma.CHECK_CRC32): raise lzma.LZMAError(\\"Integrity check failed\\") except (OSError, lzma.LZMAError) as e: print(f\\"Error during decompression: {e}\\") raise"},{"question":"Implementing a Custom Attention Layer in PyTorch **Objective:** Design and implement a custom attention mechanism in PyTorch to assess your understanding of neural network and attention mechanisms. **Problem Statement:** Attention mechanisms are a crucial part of neural networks, especially in models like transformers. Your task is to implement a custom attention layer using PyTorch. The custom attention layer should compute the scaled dot-product attention, which is used extensively in Transformer models. **Detailed Requirements:** 1. **Inputs:** - `queries`: A tensor of shape `(batch_size, num_queries, d_k)`. - `keys`: A tensor of shape `(batch_size, num_keys, d_k)`. - `values`: A tensor of shape `(batch_size, num_keys, d_v)`. - `mask`: An optional tensor of shape `(batch_size, num_queries, num_keys)` containing 0s and 1s to indicate which queries should not attend to which keys (0 means attend, 1 means not attend). 2. **Outputs:** - The output of your custom attention layer should be a tensor of shape `(batch_size, num_queries, d_v)`. 3. **Function Signatures:** ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self): super(CustomAttention, self).__init__() def forward(self, queries, keys, values, mask=None): Arguments: queries -- A tensor of shape (batch_size, num_queries, d_k) keys -- A tensor of shape (batch_size, num_keys, d_k) values -- A tensor of shape (batch_size, num_keys, d_v) mask -- An optional tensor of shape (batch_size, num_queries, num_keys) with 0s and 1s (default: None) Returns: output -- A tensor of shape (batch_size, num_queries, d_v) # Implement your solution here. pass # Example usage: # attn = CustomAttention() # output = attn(queries, keys, values, mask) ``` 4. **Instructions:** - Implement the forward method in the `CustomAttention` class. - Calculate the attention scores using the scaled dot-product of the queries and keys. - Apply the mask (if provided) to these scores before applying the softmax operation. - Use the attention scores to compute a weighted sum of the values. - Ensure the implementation is efficient and uses PyTorch functionality properly. 5. **Constraints:** - You should handle `NaN` and `Inf` values appropriately. - Assume `d_k` and `d_v` are positive integers that fit within typical memory constraints. 6. **Performance:** - Your implementation should be able to handle moderately sized inputs efficiently (batch sizes of up to 256 and sequence lengths of up to 512). You are encouraged to test your implementation with random data and verify the behavior of different components using debug statements if necessary. # Example Code: ```python # Define input tensors batch_size, num_queries, num_keys, d_k, d_v = 2, 4, 5, 3, 3 queries = torch.rand(batch_size, num_queries, d_k) keys = torch.rand(batch_size, num_keys, d_k) values = torch.rand(batch_size, num_keys, d_v) mask = torch.randint(0, 2, (batch_size, num_queries, num_keys)) # Initialize the custom attention layer attention_layer = CustomAttention() # Compute the output using the attention layer output = attention_layer(queries, keys, values, mask) print(output) ``` This problem requires a good understanding of PyTorch operations, tensor manipulations, and attention mechanisms, making it suitably challenging and educational.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self): super(CustomAttention, self).__init__() def forward(self, queries, keys, values, mask=None): Arguments: queries -- A tensor of shape (batch_size, num_queries, d_k) keys -- A tensor of shape (batch_size, num_keys, d_k) values -- A tensor of shape (batch_size, num_keys, d_v) mask -- An optional tensor of shape (batch_size, num_queries, num_keys) with 0s and 1s (default: None) Returns: output -- A tensor of shape (batch_size, num_queries, d_v) d_k = queries.size(-1) # Calculate the dot product between queries and keys scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) # Apply mask if provided if mask is not None: scores = scores.masked_fill(mask == 1, float(\'-inf\')) # Apply softmax to get attention weights attention_weights = F.softmax(scores, dim=-1) # Calculate the weighted sum of the values output = torch.matmul(attention_weights, values) return output # Example usage: # attn = CustomAttention() # output = attn(queries, keys, values, mask)"},{"question":"**Python Packaging Automation** You are required to write a Python function called `create_package_distributions` that automates the creation of built distributions for a given Python project based on a configuration file. The function should read configurations from a provided JSON file, generate the necessary setup commands, and execute them to produce the specified distributions. # Configuration File Format The JSON configuration file will have the following structure: ```json { \\"name\\": \\"example_project\\", \\"version\\": \\"1.0\\", \\"author\\": \\"John Doe\\", \\"author_email\\": \\"john.doe@example.com\\", \\"description\\": \\"This is an example project.\\", \\"dist_formats\\": [\\"gztar\\", \\"zip\\"], \\"post_install_script\\": \\"post_install.py\\" } ``` # Function Signature ```python def create_package_distributions(config_file: str) -> None: pass ``` # Input - `config_file` (str): Path to the JSON configuration file. # Task Requirements 1. **Read Configuration**: Parse the JSON configuration file. 2. **Generate Setup Script**: Create a `setup.py` file in the current working directory based on the configuration options. Below is a minimal example of what the setup script might look like: ```python from setuptools import setup setup( name=\'example_project\', version=\'1.0\', author=\'John Doe\', author_email=\'john.doe@example.com\', description=\'This is an example project.\', packages=[\'your_package_name_here\'] # You may assume a simple package structure for testing ) ``` 3. **Create Built Distributions**: Use the `subprocess` module to invoke the necessary `setup.py` commands to produce distributions in the specified formats from the configuration file. 4. **Post-Installation Script**: If a `post_install_script` is specified, ensure it is properly referenced in the `setup.py` and that the functionality for handling `-install` and `-remove` arguments is integrated. # Constraints - The function should not produce any actual output files but should print the setup commands it would execute. - Ensure the setup script and necessary commands properly integrate with the information provided in the configuration file. - Assume the project has a simple directory structure with an importable package that can be found directly under the current working directory (with a common init file). # Example Usage ```python # The JSON configuration file passed to the function create_package_distributions: config_file = \'my_package_config.json\' create_package_distributions(config_file) ``` # Expected Output Given the configuration above, the output might look like: ```python Creating setup.py Running command: python setup.py bdist --formats=gztar,zip ``` # Note - You should not actually execute the commands or create files for grading purposes, just simulate the creation and print the commands. - You may assume all paths and configurations provided are valid.","solution":"import json import subprocess def create_package_distributions(config_file: str) -> None: # Read Configuration with open(config_file, \'r\') as file: config = json.load(file) # Generate Setup Script setup_content = f from setuptools import setup setup( name=\'{config[\\"name\\"]}\', version=\'{config[\\"version\\"]}\', author=\'{config[\\"author\\"]}\', author_email=\'{config[\\"author_email\\"]}\', description=\'{config[\\"description\\"]}\', packages=[\'your_package_name_here\'], # Assuming a simple package structure for testing entry_points={{ \'distutils.commands\': [ \'post_install={config.get(\\"post_install_script\\", None)}:main\' if \\"post_install_script\\" in config else \'\' ] }} if \\"post_install_script\\" in config else None ) with open(\'setup.py\', \'w\') as setup_file: setup_file.write(setup_content) # Create Built Distributions dist_formats = \',\'.join(config[\'dist_formats\']) command = f\\"python setup.py bdist --formats={dist_formats}\\" # Print the setup script creation and the command print(\\"Creating setup.py\\") print(f\\"Running command: {command}\\") # Would execute the command if needed # subprocess.run(command, shell=True, check=True)"},{"question":"Coding Assessment Question # Objective Using the `seaborn` package, create a visual representation of the provided dataset that demonstrates a strong understanding of cluster maps, data preprocessing, and customization. # Problem Statement You are provided with a synthetic dataset containing information on various species of flowers. The dataset has the following columns: `\'sepal_length\'`, `\'sepal_width\'`, `\'petal_length\'`, `\'petal_width\'`, and `\'species\'`. Your task is to create a cluster map incorporating appropriate data preprocessing steps and customizations. # Requirements 1. Load the dataset into a pandas DataFrame. 2. Perform the following preprocessing: - Standardize the data within the columns. 3. Create a cluster map with the following specifications: - Cluster rows only. - Use the `viridis` colormap. - Adjust the figure size to `(10, 8)`. - Add row colors based on the species. - Use the correlation metric and complete linkage method for clustering. - Provide a color bar to represent the data values. - Customize the color bar position to fit well within the figure. # Input - A DataFrame named `flowers_data` containing the columns: `\'sepal_length\'`, `\'sepal_width\'`, `\'petal_length\'`, `\'petal_width\'`, and `\'species\'`. # Output - A visual cluster map satisfying all the specifications above. # Constraints - Use only the `pandas` library for data manipulation and `seaborn` for plotting the cluster map. - Incorporate the preprocessing steps within your solution. - Ensure the visual representation is clear and accurately reflects the provided data. # Example Code ```python import seaborn as sns import pandas as pd def create_cluster_map(flowers_data): # Standardize the data within columns iris_data = flowers_data.copy() species = iris_data.pop(\'species\') # Standard scaling within columns iris_data = (iris_data - iris_data.mean()) / iris_data.std() # Assign colors based on species lut = dict(zip(species.unique(), sns.color_palette(\\"hsv\\", len(species.unique())))) row_colors = species.map(lut) # Create cluster map sns.clustermap( iris_data, row_colors=row_colors, cmap=\\"viridis\\", figsize=(10, 8), metric=\\"correlation\\", method=\\"complete\\", cbar_pos=(0.9, .2, .03, .4) ) # Test the function with the provided data flowers_data = pd.read_csv(\\"path_to_flowers_dataset.csv\\") create_cluster_map(flowers_data) ``` Note: Ensure the dataset \'flowers_data\' is available in the provided path and loaded correctly. # Assessment Criteria - Correct implementation of data preprocessing steps. - Proper use of `seaborn.clustermap` with the given specifications. - Correct handling and mapping of species to row colors. - The visual presentation should be clear, well-labeled, and meet the requirements.","solution":"import seaborn as sns import pandas as pd from sklearn.preprocessing import StandardScaler def create_cluster_map(flowers_data): # Standardize the data within columns iris_data = flowers_data.copy() species = iris_data.pop(\'species\') # Use StandardScaler for standard scaling within columns scaler = StandardScaler() iris_scaled = scaler.fit_transform(iris_data) iris_data = pd.DataFrame(iris_scaled, columns=iris_data.columns) # Assign colors based on species lut = dict(zip(species.unique(), sns.color_palette(\\"hsv\\", len(species.unique())))) row_colors = species.map(lut) # Create cluster map sns.clustermap( iris_data, row_colors=row_colors, cmap=\\"viridis\\", figsize=(10, 8), metric=\\"correlation\\", method=\\"complete\\", cbar_pos=(0.9, 0.2, 0.03, 0.4) )"},{"question":"Objective: Implement a Gaussian Process Regression model using `scikit-learn`\'s `GaussianProcessRegressor` class. Evaluate the model using different kernels and compare their performance on a given dataset. Requirements: 1. Implement a function `build_and_evaluate_gpr` which: - Takes as input: - Training data `X_train` and `y_train`. - Test data `X_test` and `y_test`. - A list of kernels to be evaluated. - Builds a Gaussian Process Regression model using each kernel. - Fits the models to the training data. - Makes predictions on the test data. - Evaluates and compares the models based on Mean Squared Error (MSE) and log-marginal-likelihood (LML). 2. Output: - A dictionary where the keys are the kernel names and the values are dictionaries with the `MSE` and `LML` of the corresponding model. Function Signature: ```python from typing import List, Dict import numpy as np from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import Kernel def build_and_evaluate_gpr( X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, kernels: List[Kernel] ) -> Dict[str, Dict[str, float]]: pass ``` Example Usage: ```python from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic from sklearn.datasets import make_friedman1 from sklearn.model_selection import train_test_split # Generate a dataset X, y = make_friedman1(n_samples=200, n_features=5, noise=0.1, random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define kernels to be evaluated kernels = [ RBF(length_scale=1.0), Matern(length_scale=1.0, nu=1.5), RationalQuadratic(length_scale=1.0, alpha=1.0) ] # Build and evaluate GPR models results = build_and_evaluate_gpr(X_train, y_train, X_test, y_test, kernels) # Print results print(results) ``` Constraints: - Ensure that the function handles errors gracefully and provides meaningful error messages. - The dataset size should be manageable to run on standard computational resources. Performance: - Your implementation should aim to minimize computational complexity where possible, considering the cubic time complexity of Gaussian Processes. Hints: - Utilize `GaussianProcessRegressor` for the regression model. - Use the `mean_squared_error` function from `sklearn.metrics` to compute MSE. - Use the `log_marginal_likelihood` method of the `GaussianProcessRegressor` to compute LML. - Implement error handling for cases where the model fitting or prediction might fail.","solution":"from typing import List, Dict import numpy as np from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import Kernel from sklearn.metrics import mean_squared_error def build_and_evaluate_gpr( X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, kernels: List[Kernel] ) -> Dict[str, Dict[str, float]]: results = {} for kernel in kernels: try: # Initialize the GaussianProcessRegressor with the current kernel gpr = GaussianProcessRegressor(kernel=kernel) # Fit the model to the training data gpr.fit(X_train, y_train) # Make predictions on the test data y_pred = gpr.predict(X_test) # Calculate the Mean Squared Error mse = mean_squared_error(y_test, y_pred) # Calculate the Log-Marginal-Likelihood lml = gpr.log_marginal_likelihood() # Save the results results[str(kernel)] = {\'MSE\': mse, \'LML\': lml} except Exception as e: results[str(kernel)] = {\'Error\': str(e)} return results"},{"question":"Title: Implement a Custom Buffer Interface in Python Objective Your task is to implement a custom Python class that simulates a buffer interface, allowing read and write operations on a character buffer. This custom buffer should integrate with the Python buffer protocol to demonstrate an understanding of memory handling and buffer views in Python. Requirements 1. Implement a class `CustomBuffer` that: - Initializes a buffer of a given size. - Implements methods to read from and write to the buffer. - Exposes the buffer interface using `memoryview`. 2. The `CustomBuffer` class should support the following methods: - `__init__(self, size: int)`: Initialize the buffer with the specified size. - `read(self, start: int, length: int) -> str`: Read a portion of the buffer from `start` for `length` characters. - `write(self, start: int, data: str)`: Write `data` to the buffer starting from `start`. - `get_buffer(self) -> memoryview`: Return a memoryview object exposing the buffer. Input and Output Format - The `__init__` method will take an integer `size` indicating the size of the buffer. - The `read` method will take two integers, `start` and `length`, and return a string containing the read characters. - The `write` method will take an integer `start` and a string `data`, and write `data` to the buffer starting from `start`. - The `get_buffer` method will return a memoryview object representing the buffer. Constraints - The `start` and `length` parameters for `read` and `start` for `write` should be within the bounds of the buffer. Example ```python # Example usage buffer = CustomBuffer(10) buffer.write(0, \\"hello\\") print(buffer.read(0, 5)) # Output: hello print(buffer.read(1, 3)) # Output: ell # Memory view usage buf_view = buffer.get_buffer() print(buf_view[:5].tobytes()) # Output: b\'hello\' ``` Performance Requirements - Efficient memory handling. - Avoid unnecessary copying of buffer contents. Implement the `CustomBuffer` class below: ```python class CustomBuffer: def __init__(self, size: int): self._buffer = bytearray(size) def read(self, start: int, length: int) -> str: return self._buffer[start:start + length].decode(\'utf-8\') def write(self, start: int, data: str): data_bytes = data.encode(\'utf-8\') self._buffer[start:start + len(data_bytes)] = data_bytes def get_buffer(self) -> memoryview: return memoryview(self._buffer) # Test the implementation with different cases to ensure correctness. ``` Ensure your implementation passes all edge cases such as out-of-bound indices and proper encoding/decoding of string data.","solution":"class CustomBuffer: def __init__(self, size: int): self._buffer = bytearray(size) def read(self, start: int, length: int) -> str: if start < 0 or start + length > len(self._buffer): raise ValueError(\\"Read operation out of buffer bounds\\") return self._buffer[start:start + length].decode(\'utf-8\') def write(self, start: int, data: str): data_bytes = data.encode(\'utf-8\') if start < 0 or start + len(data_bytes) > len(self._buffer): raise ValueError(\\"Write operation out of buffer bounds\\") self._buffer[start:start + len(data_bytes)] = data_bytes def get_buffer(self) -> memoryview: return memoryview(self._buffer)"},{"question":"# Custom Module Importer **Objective:** Implement a custom module importer using the `importlib` package. This custom importer should load specified modules from a given directory, potentially overriding standard module locations. You will demonstrate understanding of Python\'s import system and how to customize it using `importlib`. **Problem Statement:** You are required to implement a function `custom_importer(module_name: str, directory: str) -> Any`. This function should: 1. Import and return the module specified by `module_name` from the given `directory`. 2. Handle the case where the module does not exist in the provided directory by raising an `ImportError`. To aid in this, you can utilize the `importlib.util.spec_from_file_location` and `importlib.util.module_from_spec` functions to create and load modules from files dynamically. **Function Signature:** ```python def custom_importer(module_name: str, directory: str) -> Any: pass ``` **Inputs:** - `module_name` (str): The name of the module to import. - `directory` (str): The directory path where the module file resides. **Outputs:** - The imported module object, if successfully loaded from the specified directory. - An `ImportError` if the module cannot be found or imported. **Constraints:** - The module files will have a `.py` extension. - You can assume that the directory path provided is absolute and exists on the filesystem. - The module should be imported from the specified directory and not from the system Python path. **Example Usage:** ```python # Assume we have a directory \\"/path/to/modules\\" containing a Python file \\"mymodule.py\\" module = custom_importer(\\"mymodule\\", \\"/path/to/modules\\") module.my_function() # Should work if mymodule.py defines my_function ``` **Notes:** 1. Be sure to carefully handle exceptions and edge cases where the module or directory might not exist. 2. You are expected to use the `importlib` package functions to complete this task.","solution":"import importlib.util import sys import os def custom_importer(module_name: str, directory: str): Import and return the module specified by module_name from the given directory. Args: module_name (str): The name of the module to import. directory (str): The directory path where the module file resides. Returns: Module: The imported module object. Raises: ImportError: If the module cannot be found or imported. module_file_path = os.path.join(directory, f\\"{module_name}.py\\") if not os.path.isfile(module_file_path): raise ImportError(f\\"Module {module_name} not found in directory {directory}\\") spec = importlib.util.spec_from_file_location(module_name, module_file_path) if spec is None: raise ImportError(f\\"Could not load spec for module {module_name} from {module_file_path}\\") module = importlib.util.module_from_spec(spec) try: spec.loader.exec_module(module) except Exception as e: raise ImportError(f\\"Failed to import module {module_name} due to error: {e}\\") sys.modules[module_name] = module return module"},{"question":"# Temporary File Management with `tempfile` You are working on developing a feature for an application that processes large datasets. The application needs to temporarily store intermediate data before it\'s fully processed and written to a permanent location. To avoid cluttering the file system and ensure security, you want to use the `tempfile` module to handle temporary files. Implement a function `process_large_data` that: 1. Receives a list of data chunks (list of strings) and processes each chunk. 2. Stores each processed chunk in a temporary file using `tempfile.SpooledTemporaryFile` (use a max size of 10 MB for spooling). 3. Once all chunks are processed and stored: - Merges all temporary files into a single tempfile.TemporaryFile. - Ensures all temporary files are properly cleaned up after merging. 4. Returns the temporary file containing the merged data. The processing function simply converts all strings to uppercase as a placeholder for a more complex operation. Function Signature: ```python def process_large_data(data_chunks: list) -> tempfile.TemporaryFile: pass ``` Input: - `data_chunks`: A list of strings, where each string represents a chunk of data. Output: - A `tempfile.TemporaryFile` object containing all processed data. Constraints: - Each data chunk can be up to 5 MB. - You must use `tempfile.SpooledTemporaryFile` to handle chunk storage with a max size of 10 MB. Example: ```python data_chunks = [\\"hello world\\", \\"this is a test\\", \\"temporary files in python\\"] temp_file = process_large_data(data_chunks) with temp_file as tf: tf.seek(0) print(tf.read().decode()) ``` Expected Output: ``` HELLO WORLD THIS IS A TEST TEMPORARY FILES IN PYTHON ``` Notes: - Pay attention to managing the context of the temporary files to ensure they are cleaned up properly. - Ensure that merging the tempfile contents is memory efficient, i.e., avoid loading all data into memory at once if possible.","solution":"import tempfile def process_large_data(data_chunks: list) -> tempfile.TemporaryFile: temp_files = [] try: # Process each chunk and store it in a SpooledTemporaryFile for chunk in data_chunks: processed_chunk = chunk.upper() temp_file = tempfile.SpooledTemporaryFile(max_size=10 * 1024 * 1024) # max_size 10 MB temp_file.write(processed_chunk.encode()) temp_file.seek(0) temp_files.append(temp_file) # Merge all temporary files into a single TemporaryFile merged_temp_file = tempfile.TemporaryFile() for temp_file in temp_files: while chunk := temp_file.read(1024): merged_temp_file.write(chunk) merged_temp_file.seek(0) return merged_temp_file finally: # Ensure that all temporary files are deleted for temp_file in temp_files: temp_file.close()"},{"question":"# Asyncio Queue Simulation Problem Statement You are tasked with simulating a food delivery order system using `asyncio.Queue`. Orders arrive at random intervals, and a specified number of delivery people (workers) process these orders concurrently. The workers deliver orders in the sequence they arrive. Your task is to implement a function that simulates this system. The function should meet the following criteria: 1. **Input:** - `num_orders` (int): Number of orders to simulate. - `num_workers` (int): Number of delivery workers. - `max_order_time` (float): Maximum time in seconds for random order arrival and processing. 2. **Output:** None. The function should print the status of each worker when they pick up and complete an order. 3. **Constraints:** - Each worker should pick the next available order from the queue as soon as they are done with the previous one. - The queue should hold orders that arrive randomly from 0.1 to `max_order_time` seconds. - After all orders are processed, the function should print the total time taken and the total expected processing time. Use the `asyncio.Queue` and its associated methods to manage the queuing of orders. Implement exception handling for the queue operations using `asyncio.QueueEmpty` and `asyncio.QueueFull`. Function Signature ```python import asyncio import random import time async def simulate_food_delivery(num_orders: int, num_workers: int, max_order_time: float) -> None: # Your implementation here pass ``` Example ```python import asyncio async def simulate_food_delivery(num_orders: int, num_workers: int, max_order_time: float) -> None: async def worker(name, queue): while True: try: order_time = await queue.get() await asyncio.sleep(order_time) queue.task_done() print(f\'Worker {name} delivered an order in {order_time:.2f} seconds\') except asyncio.QueueEmpty: break async def main(): queue = asyncio.Queue() total_order_time = 0 for _ in range(num_orders): order_time = random.uniform(0.1, max_order_time) total_order_time += order_time await queue.put(order_time) tasks = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(num_workers)] started_at = time.monotonic() await queue.join() total_time = time.monotonic() - started_at for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'====\') print(f\'{num_workers} workers delivered in parallel for {total_time:.2f} seconds\') print(f\'Total expected order time: {total_order_time:.2f} seconds\') await main() # Example usage asyncio.run(simulate_food_delivery(10, 3, 2.0)) ``` Explanation: - `simulate_food_delivery` is the high-level function that sets up the order queue and creates worker tasks. - `worker` is a coroutine to pick up orders from the queue and process (deliver) them. - The `main` coroutine initializes the queue, generates random order times, and assigns tasks. - The function prints out the workers\' actions and the total time statistics.","solution":"import asyncio import random import time async def simulate_food_delivery(num_orders: int, num_workers: int, max_order_time: float) -> None: async def worker(name, queue): while True: try: order_time = await queue.get() await asyncio.sleep(order_time) queue.task_done() print(f\'Worker {name} delivered an order in {order_time:.2f} seconds\') except asyncio.QueueEmpty: break async def main(): queue = asyncio.Queue() total_order_time = 0 for _ in range(num_orders): order_time = random.uniform(0.1, max_order_time) total_order_time += order_time await queue.put(order_time) tasks = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(num_workers)] started_at = time.monotonic() await queue.join() total_time = time.monotonic() - started_at for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'====\') print(f\'{num_workers} workers delivered in parallel for {total_time:.2f} seconds\') print(f\'Total expected order time: {total_order_time:.2f} seconds\') await main() # Example usage # asyncio.run(simulate_food_delivery(10, 3, 2.0))"},{"question":"# PyTorch Environment Variables and CUDA You are tasked with setting up a PyTorch project that utilizes CUDA for GPU acceleration. The project involves: 1. Creating a simple neural network for image classification on CIFAR-10 dataset. 2. Ensuring optimized and efficient use of CUDA. 3. Debugging CUDA memory errors through appropriate environment configurations. Your task is to write a script that: 1. **Sets Up Necessary Environment Variables**: - Disable caching of memory allocations in CUDA. - Enable non-blocking error handling in NCCL. - Ensure that only the first GPU (device ID 0) is visible to CUDA. 2. **Implements a Simple Neural Network**: - Create a basic CNN that can classify CIFAR-10 images. 3. **Configures PyTorch to Use CUDA**: - Check if CUDA is available and ensure the network operations are performed on GPU. 4. **Includes Error Handling and Debugging**: - Implement error handling to catch any CUDA memory allocation issues. - NOTE: Use appropriate environment variables in your script to handle potential CUDA runtime errors. # Requirements: 1. **Environment Variables Function**: - Implement a function `set_environment_variables()` that sets the specified environment variables as described. 2. **Neural Network Implementation**: - Implement a function `build_cnn_model()` that returns an instance of the CNN model. 3. **Training Function**: - Implement a function `train_model()` that: - Loads CIFAR-10 dataset. - Transfers data to GPU. - Trains the CNN model. - Includes error handling for CUDA operations. # Input: - No direct input is required. # Output: - The only required output is `None`. This is a script that should be executed to demonstrate the functionality by running the `train_model` function. # Constraints: - You must use PyTorch and CUDA for GPU acceleration. - Assume the dataset loading and neural network training functions will print logs to indicate progress and any potential issues. Here is the template for your script: ```python import os import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms def set_environment_variables(): os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' os.environ[\'TORCH_NCCL_USE_COMM_NONBLOCKING\'] = \'1\' os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\' def build_cnn_model(): class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=5) self.pool = nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(16, 32, kernel_size=5) self.fc1 = nn.Linear(32 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(torch.relu(self.conv1(x))) x = self.pool(torch.relu(self.conv2(x))) x = x.view(-1, 32 * 5 * 5) x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x return SimpleCNN() def train_model(): set_environment_variables() transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root=\'./data\', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model = build_cnn_model().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): for i, data in enumerate(trainloader, 0): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() if i % 2000 == 1999: print(f\\"[{epoch + 1}, {i + 1}] loss: {loss.item():.3f}\\") print(\'Finished Training\') if __name__ == \\"__main__\\": train_model() ``` Run the script and ensure that it performs as expected.","solution":"import os import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms def set_environment_variables(): os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' os.environ[\'TORCH_NCCL_USE_COMM_NONBLOCKING\'] = \'1\' os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\' def build_cnn_model(): class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=5) self.pool = nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(16, 32, kernel_size=5) self.fc1 = nn.Linear(32 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(torch.relu(self.conv1(x))) x = self.pool(torch.relu(self.conv2(x))) x = x.view(-1, 32 * 5 * 5) x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x return SimpleCNN() def train_model(): set_environment_variables() transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root=\'./data\', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model = build_cnn_model().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) try: for epoch in range(2): for i, data in enumerate(trainloader, 0): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() if i % 2000 == 1999: print(f\\"[{epoch + 1}, {i + 1}] loss: {loss.item():.3f}\\") except RuntimeError as e: if \'out of memory\' in str(e): print(\'CUDA Out of Memory Error: \', e) torch.cuda.empty_cache() else: raise e print(\'Finished Training\') if __name__ == \\"__main__\\": train_model()"},{"question":"# PyTorch Environment Variable Configuration **Objective:** Implement a function that demonstrates knowledge of some important PyTorch environment variables and their effects on PyTorch operations, particularly concerning model loading and autograd shutdown behavior. **Task:** 1. Define a function `load_model_with_specific_settings` that takes two parameters: - `model_path`: A string representing the file path where a PyTorch model is stored. - `force_weights_only`: A boolean that, if True, should set the environment variable `TORCH_FORCE_WEIGHTS_ONLY_LOAD` to force loading the model weights only even if `weights_only=False`. 2. The function should: - Load the model from the given `model_path` using `torch.load`. - Print specific messages indicating whether the model was loaded with the \\"weights only\\" setting. 3. Define a function `set_autograd_shutdown_wait_limit` that takes one parameter: - `timeout`: An integer representing the number of seconds for the autograd shutdown wait limit. 4. The function should: - Set the `TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT` environment variable to the provided `timeout`. **Constraints:** - Assume that PyTorch is properly installed and available in the environment where the method will run. - Ensure that the changes to environment variables are temporary and do not affect the global environment permanently. **Performance Requirements:** - The model loading should be efficient and make use of the set environment variables. **Expected Input and Output Formats:** ```python import torch def load_model_with_specific_settings(model_path: str, force_weights_only: bool) -> None: # Your implementation here pass def set_autograd_shutdown_wait_limit(timeout: int) -> None: # Your implementation here pass ``` # Example Usage: ```python # Assume \'model.pth\' is a valid model file path load_model_with_specific_settings(\'model.pth\', True) # Temporarily set autograd shutdown wait limit to 5 seconds set_autograd_shutdown_wait_limit(5) ``` # Evaluation Criteria: - Correctness: The code should correctly adjust the relevant environment variables based on the given inputs. - Efficiency: The model loading and environment settings should be implemented efficiently. - Clarity: The code should be well-organized and documented for readability and maintenance.","solution":"import os import torch def load_model_with_specific_settings(model_path: str, force_weights_only: bool) -> None: Load a PyTorch model with specific settings. Parameters: model_path (str): The file path where the PyTorch model is stored. force_weights_only (bool): If True, sets an environment variable to force loading the model weights only. Returns: None if force_weights_only: os.environ[\\"TORCH_FORCE_WEIGHTS_ONLY_LOAD\\"] = \\"1\\" print(\\"Environment variable \'TORCH_FORCE_WEIGHTS_ONLY_LOAD\' set to 1.\\") else: os.environ.pop(\\"TORCH_FORCE_WEIGHTS_ONLY_LOAD\\", None) print(\\"Environment variable \'TORCH_FORCE_WEIGHTS_ONLY_LOAD\' removed.\\") try: model = torch.load(model_path) print(f\\"Model loaded from \'{model_path}\' with weights only setting: {force_weights_only}\\") except Exception as e: print(f\\"Failed to load the model from \'{model_path}\': {e}\\") def set_autograd_shutdown_wait_limit(timeout: int) -> None: Set the autograd shutdown wait limit. Parameters: timeout (int): Number of seconds for the autograd shutdown wait limit. Returns: None os.environ[\\"TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\\"] = str(timeout) print(f\\"Environment variable \'TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\' set to {timeout}.\\")"},{"question":"**Problem Statement: Advanced Date-Time Manipulations** Design and implement a Python class named `DateTimeManipulator`. This class should provide a variety of functions that leverage the `datetime` module. Specifically, you need to implement the following methods within the class: 1. **`calculate_days_between(self, date1: str, date2: str) -> int`**: Takes two dates in string format \\"YYYY-MM-DD\\" and returns the number of days between them. Assume date1 is always earlier than date2. 2. **`convert_to_utc(self, dt_str: str, tz_str: str) -> str`**: Takes a datetime string in the format \\"YYYY-MM-DD HH:MM:SS\\" and a timezone string (like UTC+05:30). Returns the equivalent UTC time in the format \\"YYYY-MM-DD HH:MM:SS\\". 3. **`add_time_delta(self, dt_str: str, delta_days: int, delta_seconds: int) -> str`**: Takes a datetime string \\"YYYY-MM-DD HH:MM:SS\\" and two integers representing days and seconds to add as a time delta. Returns a new datetime string in the format \\"YYYY-MM-DD HH:MM:SS\\". 4. **`format_date(self, dt_str: str, format_str: str) -> str`**: Takes a datetime string \\"YYYY-MM-DD HH:MM:SS\\" and a format string and returns the datetime formatted according to the given format string. 5. **`days_until_event(self, dt_str: str, event_str: str) -> int`**: Takes a datetime string \\"YYYY-MM-DD\\" representing the date today and a date string for an event \\"YYYY-MM-DD\\". Returns the number of days until the event. The function should correctly handle cases where the event is the following year. # Method Descriptions - **calculate_days_between(self, date1: str, date2: str) -> int** - Input: Two date strings in \\"YYYY-MM-DD\\" format. - Output: Integer representing the number of days between date1 and date2. - Example: `calculate_days_between(\\"2023-01-01\\", \\"2023-01-10\\")` should return `9`. - **convert_to_utc(self, dt_str: str, tz_str: str) -> str** - Input: A datetime string in \\"YYYY-MM-DD HH:MM:SS\\" format and a timezone string (e.g., \\"UTC+05:30\\"). - Output: The equivalent UTC time as a string in the format \\"YYYY-MM-DD HH:MM:SS\\". - Example: `convert_to_utc(\\"2023-01-10 10:00:00\\", \\"UTC+05:30\\")` should return `\\"2023-01-10 04:30:00\\"`. - **add_time_delta(self, dt_str: str, delta_days: int, delta_seconds: int) -> str** - Input: A datetime string in \\"YYYY-MM-DD HH:MM:SS\\" format, and two integers (days and seconds) representing the time delta to add. - Output: A new datetime string in the format \\"YYYY-MM-DD HH:MM:SS\\". - Example: `add_time_delta(\\"2023-01-01 00:00:00\\", 1, 3600)` should return `\\"2023-01-02 01:00:00\\"`. - **format_date(self, dt_str: str, format_str: str) -> str** - Input: A datetime string in \\"YYYY-MM-DD HH:MM:SS\\" format and a format string. - Output: The formatted datetime string. - Example: `format_date(\\"2023-01-01 00:00:00\\", \\"%A, %d %B %Y\\")` should return `\\"Sunday, 01 January 2023\\"`. - **days_until_event(self, dt_str: str, event_str: str) -> int** - Input: A datetime string in \\"YYYY-MM-DD\\" format representing todayâ€™s date, and another date string in \\"YYYY-MM-DD\\" format representing an event date. - Output: Integer representing the number of days until the event. - Example: `days_until_event(\\"2023-12-25\\", \\"2024-06-15\\")` should return the number of days until June 15th, 2024 from December 25th, 2023. # Example Usage ```python dtm = DateTimeManipulator() print(dtm.calculate_days_between(\\"2023-01-01\\", \\"2023-01-10\\")) # Output: 9 print(dtm.convert_to_utc(\\"2023-01-10 10:00:00\\", \\"UTC+05:30\\")) # Output: \\"2023-01-10 04:30:00\\" print(dtm.add_time_delta(\\"2023-01-01 00:00:00\\", 1, 3600)) # Output: \\"2023-01-02 01:00:00\\" print(dtm.format_date(\\"2023-01-01 00:00:00\\", \\"%A, %d %B %Y\\")) # Output: \\"Sunday, 01 January 2023\\" print(dtm.days_until_event(\\"2023-12-25\\", \\"2024-06-15\\")) # Output: (number of days until the event) ``` # Constraints - You can assume the input date and time strings will be valid and follow the indicated formats. - You should handle timezone input formats correctly as provided in the examples. Ensure your implementation follows good coding practices, including appropriate handling of edge cases, and is efficient in terms of computation.","solution":"from datetime import datetime, timedelta import pytz class DateTimeManipulator: def calculate_days_between(self, date1: str, date2: str) -> int: date1_dt = datetime.strptime(date1, \\"%Y-%m-%d\\") date2_dt = datetime.strptime(date2, \\"%Y-%m-%d\\") return (date2_dt - date1_dt).days def convert_to_utc(self, dt_str: str, tz_str: str) -> str: local_dt = datetime.strptime(dt_str, \\"%Y-%m-%d %H:%M:%S\\") # Extract the timezone offset from the provided string sign = 1 if tz_str[3] == \'+\' else -1 hours_offset = int(tz_str[4:6]) minutes_offset = int(tz_str[7:9]) total_offset = sign * (hours_offset * 3600 + minutes_offset * 60) local_timezone = pytz.FixedOffset(total_offset // 60) local_dt = local_timezone.localize(local_dt) utc_dt = local_dt.astimezone(pytz.utc) return utc_dt.strftime(\\"%Y-%m-%d %H:%M:%S\\") def add_time_delta(self, dt_str: str, delta_days: int, delta_seconds: int) -> str: original_dt = datetime.strptime(dt_str, \\"%Y-%m-%d %H:%M:%S\\") new_dt = original_dt + timedelta(days=delta_days, seconds=delta_seconds) return new_dt.strftime(\\"%Y-%m-%d %H:%M:%S\\") def format_date(self, dt_str: str, format_str: str) -> str: dt = datetime.strptime(dt_str, \\"%Y-%m-%d %H:%M:%S\\") return dt.strftime(format_str) def days_until_event(self, dt_str: str, event_str: str) -> int: today_dt = datetime.strptime(dt_str, \\"%Y-%m-%d\\") event_dt = datetime.strptime(event_str, \\"%Y-%m-%d\\") return (event_dt - today_dt).days"},{"question":"# Objective Write a function `custom_heatmap` that takes a DataFrame with numeric values and visualizes it as a heatmap with various customizations. # Function Signature ```python def custom_heatmap(df: pd.DataFrame, annot: bool = True, fmt: str = \\".2f\\", cmap: str = \\"viridis\\", line_width: float = 0.5, color_min: float = None, color_max: float = None) -> None: pass ``` # Input - `df` (pd.DataFrame): A pandas DataFrame containing numeric values. - `annot` (bool): If True, add text annotations to each cell (default is True). - `fmt` (str): Formatting string for annotations (default is \\".2f\\"). - `cmap` (str): Colormap to use (default is \\"viridis\\"). - `line_width` (float): Width of lines that will divide each cell (default is 0.5). - `color_min` (float): Minimum data value that corresponds to the lower bound of the colormap (default is None). - `color_max` (float): Maximum data value that corresponds to the upper bound of the colormap (default is None). # Output The function does not return anything. It should display a heatmap using the seaborn library. # Constraints 1. The DataFrame should have numeric values only. 2. Use the seaborn library to generate the heatmap. 3. Use matplotlib methods to customize the plot (if necessary). # Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Example DataFrame data = { \'A\': [1, 2, 3, 4], \'B\': [4, 3, 2, 1], \'C\': [2, 3, 4, 1], \'D\': [4, 1, 2, 3] } df = pd.DataFrame(data) # Function call custom_heatmap(df, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", line_width=0.7, color_min=1, color_max=4) ``` In this example, the function will create a heatmap of the DataFrame `df` with text annotations, formatted to one decimal place, using the \\"coolwarm\\" colormap, with a line width between cells of 0.7, and the colormap normalization ranging from 1 to 4. # Additional Challenge As an additional challenge, extend the function to allow the user to pass a separate DataFrame for annotations. ```python def custom_heatmap(df: pd.DataFrame, annot_df: pd.DataFrame = None, annot: bool = True, fmt: str = \\".2f\\", cmap: str = \\"viridis\\", line_width: float = 0.5, color_min: float = None, color_max: float = None) -> None: pass ``` This extended function should use the `annot_df` for annotations if provided. # Explanation This question assesses the understanding of: 1. Creating and customizing heatmaps in seaborn. 2. Using pandas DataFrames and handling numeric data. 3. Customizing plots with seaborn and matplotlib. 4. Creating reusable functions with sensible default parameters for flexibility.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_heatmap(df: pd.DataFrame, annot: bool = True, fmt: str = \\".2f\\", cmap: str = \\"viridis\\", line_width: float = 0.5, color_min: float = None, color_max: float = None) -> None: Displays a heatmap using the given DataFrame with various customization options. :param df: Pandas DataFrame containing numeric values. :param annot: bool, optional. If True, add text annotations to each cell (default is True). :param fmt: str, optional. Formatting string for annotations (default is \\".2f\\"). :param cmap: str, optional. Colormap to use (default is \\"viridis\\"). :param line_width: float, optional. Width of lines that will divide each cell (default is 0.5). :param color_min: float, optional. Minimum data value that corresponds to the lower bound of the colormap (default is None). :param color_max: float, optional. Maximum data value that corresponds to the upper bound of the colormap (default is None). # Create the heatmap with the given parameters sns.heatmap(df, annot=annot, fmt=fmt, cmap=cmap, linewidths=line_width, vmin=color_min, vmax=color_max) # Display the heatmap plt.show()"},{"question":"# Problem Description You are tasked with creating a Python function to dynamically import a module and retrieve metadata information such as its version and its distributions. This problem will test your ability to utilize the `importlib` library to import modules programmatically and fetch metadata related to those modules. # Function Signature ```python def fetch_module_metadata(module_name: str) -> dict: Dynamically imports a module and retrieves its version and distribution metadata. Parameters: module_name (str): The name of the module to import. Returns: dict: A dictionary containing the module\'s version and distribution information. The format should be: { \'version\': str, \'distributions\': list of str } If the module is not found or if there\'s an error in fetching the metadata, return an empty dictionary. ``` # Input - A single string, `module_name`, representing the name of the module to import. # Output - A dictionary containing: - `\'version\'`: The version of the module as a string. - `\'distributions\'`: A list of strings, where each string is a distribution associated with the module. # Constraints - Assume the input `module_name` is always a non-empty string. - The function should handle cases where the module does not exist or metadata cannot be fetched gracefully. - You must use the `importlib` module to implement this functionality. # Example ```python fetch_module_metadata(\\"requests\\") # Might return: # { # \'version\': \'2.25.1\', # \'distributions\': [\'requests-2.25.1.dist-info\', ...] # } fetch_module_metadata(\\"nonexistentmodule\\") # Returns: # {} ``` # Notes - You might need to refer to the `importlib` and specifically `importlib.metadata` to implement this function. - Consider edge cases such as modules that do not have version information or distributions listed.","solution":"import importlib import importlib.metadata def fetch_module_metadata(module_name: str) -> dict: Dynamically imports a module and retrieves its version and distribution metadata. Parameters: module_name (str): The name of the module to import. Returns: dict: A dictionary containing the module\'s version and distribution information. The format should be: { \'version\': str, \'distributions\': list of str } If the module is not found or if there\'s an error in fetching the metadata, return an empty dictionary. try: # Import the module dynamically module = importlib.import_module(module_name) # Get version and distribution metadata version = importlib.metadata.version(module_name) distributions = list(importlib.metadata.files(module_name)) return { \'version\': version, \'distributions\': [str(d) for d in distributions] } except (ModuleNotFoundError, importlib.metadata.PackageNotFoundError): return {} except Exception as e: return {}"},{"question":"# URL Manipulation and Query Handling You are provided with a set of URLs and your task is to write functions leveraging the `urllib.parse` module in Python to manipulate and extract information from them. Functions to Implement 1. **extract_domain**: Extract the domain from a given URL. - **Input**: A string `url` - **Output**: A string representing the domain (netloc component) of the URL 2. **reconstruct_url**: Reconstruct a URL from its components after modifying the query parameters. - **Input**: - `url`: A string representing the original URL - `params`: A dictionary containing query parameters to be added or updated - **Output**: A string representing the reconstructed URL with updated query parameters 3. **parse_query_string**: Convert a query string into a dictionary, ensuring proper decoding and handling blank values. - **Input**: - `query_string`: A string representing the query part of a URL - `keep_blank_values`: A boolean indicating whether blank values should be retained - **Output**: A dictionary where keys are parameter names, and values are lists of parameter values Example ```python from urllib.parse import quote, unquote, urlparse, urlunparse, parse_qs, urlencode def extract_domain(url): # Your implementation here pass def reconstruct_url(url, params): # Your implementation here pass def parse_query_string(query_string, keep_blank_values=False): # Your implementation here pass # Example usage: url = \\"http://www.example.com/path?name=John&age=30\\" print(extract_domain(url)) # Output: \\"www.example.com\\" params = {\\"name\\": \\"Jane\\", \\"country\\": \\"US\\"} print(reconstruct_url(url, params)) # Output: \\"http://www.example.com/path?name=Jane&age=30&country=US\\" query_string = \\"name=John&age=30&&country=US\\" print(parse_query_string(query_string, keep_blank_values=True)) # Output: {\'name\': [\'John\'], \'age\': [\'30\'], \'country\': [\'US\']} ``` Constraints - All URLs will be valid and provided as non-empty strings. - Query parameters should be properly encoded and decoded following URL standards. - Ensure your functions handle edge cases, such as empty query parameters, properly. Good luck!","solution":"from urllib.parse import urlparse, urlunparse, parse_qs, urlencode def extract_domain(url): Extracts the domain (netloc) from a given URL. parsed_url = urlparse(url) return parsed_url.netloc def reconstruct_url(url, params): Reconstructs a URL by modifying its query parameters. parsed_url = urlparse(url) query_params = parse_qs(parsed_url.query) query_params.update({k: [v] for k, v in params.items()}) new_query = urlencode(query_params, doseq=True) new_url = parsed_url._replace(query=new_query) return urlunparse(new_url) def parse_query_string(query_string, keep_blank_values=False): Converts a query string into a dictionary. return parse_qs(query_string, keep_blank_values=keep_blank_values)"},{"question":"# Question: You are tasked with implementing a function using PyTorch\'s Named Tensors feature. The function should accept two inputs: a batch of images and a scaling factor for each channel. Your function should scale the channels of the images by the given factors and handle various dimension orderings. # Function Signature: ```python def scale_channels(images: torch.Tensor, scale: torch.Tensor) -> torch.Tensor: pass ``` # Input: 1. `images` (torch.Tensor): A 4-dimensional tensor representing a batch of images. The dimensions are named and the possible names are `N`, `C`, `H`, and `W` (Batch, Channel, Height, Width). The order of dimensions can vary. 2. `scale` (torch.Tensor): A 1-dimensional tensor representing the scaling factor for each channel. The dimension is named `C` (Channel). # Output: - A `torch.Tensor` which is the scaled images tensor. The tensor should have the same names and order of dimensions as the input `images`. # Constraints: - You can assume that `images` will always have 4 dimensions and `scale` will always have 1 dimension. - The named dimensions of `images` will be a permutation of the tuple `(\'N\', \'C\', \'H\', \'W\')`. - You can assume that the sizes of corresponding dimensions will always be compatible. # Example: ```python import torch # Creating a named tensor for images with dimensions in order (N, C, H, W) images = torch.randn(2, 3, 128, 128, names=(\'N\', \'C\', \'H\', \'W\')) # Creating a named tensor for scale scale = torch.tensor([0.5, 1.5, 2.0], names=(\'C\',)) scaled_images = scale_channels(images, scale) print(scaled_images.names) # (\'N\', \'C\', \'H\', \'W\') # The scaled values will be `images * scale`, applied along the channel dimension. # Another example with different dimension ordering images2 = images.align_to(\'C\', \'N\', \'H\', \'W\') scaled_images2 = scale_channels(images2, scale) print(scaled_images2.names) # (\'C\', \'N\', \'H\', \'W\') ``` # Explanation: In the given function `scale_channels`, you need to: 1. Ensure the `scale` tensor is aligned with the `images` tensor based on the channel dimension. 2. Apply the scaling factor along the channel dimension. 3. Return the resulting tensor with the same dimension names and ordering as the input `images`.","solution":"import torch def scale_channels(images: torch.Tensor, scale: torch.Tensor) -> torch.Tensor: Scale the channels of the images by the given factors. Args: - images (torch.Tensor): A 4-dimensional named tensor representing a batch of images. The names of the dimensions could be any permutation of \'N\', \'C\', \'H\', \'W\'. - scale (torch.Tensor): A 1-dimensional named tensor representing the scaling factor for each channel. Returns: - torch.Tensor: A new tensor with the same names and order of dimensions as \'images\', but with channels scaled. # Align the scale tensor to the image tensor along the correct dimension scale_aligned = scale.align_as(images) # Multiply images by the scale tensor along the channel dimension scaled_images = images * scale_aligned return scaled_images"},{"question":"# Advanced Exception Handling and Custom Exception Implementation in Python **Objective:** To demonstrate your understanding of Python\'s built-in exceptions, exception hierarchy, and custom exception creation and handling. **Problem Statement:** You are required to implement a class `CustomFileHandler` that handles file operations (`read`, `write`) with robust exception handling. This class should also be able to raise custom exceptions for specific scenarios and provide meaningful error messages. Here are the detailed requirements: 1. **Class `CustomFileHandler`:** - Methods: - `__init__(self, file_path: str)`: Initializes the instance with the provided file path. - `read_file(self) -> str`: Reads the content of the file. Raises custom exceptions in case of errors. - `write_to_file(self, content: str) -> None`: Writes the given content to the file. Raises custom exceptions in case of errors. 2. **Custom Exceptions:** - `FileNotFound(CustomException)`: Raised when the file does not exist. - `FileNotReadable(CustomException)`: Raised when the file cannot be read due to permission issues or other IO errors. - `FileWriteError(CustomException)`: Raised when the file cannot be written to. 3. **Exception Handling:** - The `read_file` method should handle the following exceptions and raise the custom exceptions: - `FileNotFoundError`: Raise `FileNotFound` with a message indicating the file does not exist. - `PermissionError`: Raise `FileNotReadable` with a message indicating the file cannot be read. - Other IO errors: Raise `FileNotReadable` with the original error message. - The `write_to_file` method should handle the following exceptions and raise the custom exceptions: - `PermissionError`: Raise `FileWriteError` with a message indicating the file is not writable. - Other IO errors: Raise `FileWriteError` with the original error message. **Example Usage:** ```python try: file_handler = CustomFileHandler(\\"example.txt\\") content = file_handler.read_file() print(content) file_handler.write_to_file(\\"New content\\") except FileNotFound as e: print(f\\"Error: {e}\\") except FileNotReadable as e: print(f\\"Error: {e}\\") except FileWriteError as e: print(f\\"Error: {e}\\") ``` **Constraints:** - Implement all custom exceptions by subclassing the built-in `Exception` class. **Evaluation Criteria:** - Correct implementation of the `CustomFileHandler` class and its methods. - Proper usage of built-in exceptions and custom exceptions. - Meaningful error messages in exceptions. - Correct handling and raising of custom exceptions. Ensure your code is well-documented and follows Python\'s best practices.","solution":"class CustomException(Exception): Base class for all custom exceptions. pass class FileNotFound(CustomException): Exception raised when the file does not exist. pass class FileNotReadable(CustomException): Exception raised when the file cannot be read. pass class FileWriteError(CustomException): Exception raised when the file cannot be written to. pass class CustomFileHandler: def __init__(self, file_path: str): self.file_path = file_path def read_file(self) -> str: try: with open(self.file_path, \'r\') as file: return file.read() except FileNotFoundError: raise FileNotFound(f\\"The file at {self.file_path} does not exist.\\") except PermissionError: raise FileNotReadable(f\\"The file at {self.file_path} is not readable due to permission issues.\\") except IOError as e: raise FileNotReadable(f\\"An error occurred while reading the file at {self.file_path}: {e}\\") def write_to_file(self, content: str) -> None: try: with open(self.file_path, \'w\') as file: file.write(content) except PermissionError: raise FileWriteError(f\\"The file at {self.file_path} is not writable due to permission issues.\\") except IOError as e: raise FileWriteError(f\\"An error occurred while writing to the file at {self.file_path}: {e}\\")"},{"question":"# Advanced Cryptographic Hashing and HMAC Implementation Problem Statement You are tasked with implementing a secure file manipulation pipeline involving hashing and authentication processes. Your solution must include the following functionalities: 1. **File Hashing**: - Implement a function `hash_file(filepath: str, algorithm: str=\'sha256\') -> str` that takes a file path and a hashing algorithm as input and returns the hex digest of the file\'s contents. - Supported algorithms are \'sha256\', \'sha512\', and \'blake2b\'. If the algorithm is not specified, use \'sha256\'. 2. **Keyed Message Authentication Code (HMAC)**: - Implement a function `generate_hmac(secret_key: bytes, message: str, algorithm: str=\'sha256\') -> str` that generates an HMAC for the given message using the specified hashing algorithm and secret key. - Supported algorithms are \'sha256\' and \'sha512\'. 3. **Integrity Verification**: - Implement a function `verify_hmac(secret_key: bytes, message: str, provided_hmac: str, algorithm: str=\'sha256\') -> bool` that verifies if the provided HMAC matches the computed HMAC for the given message and key. Input and Output Formats 1. `hash_file` - **Input**: `filepath` (str): The path to the file to be hashed. `algorithm` (str, optional): The hashing algorithm to use. Defaults to \'sha256\'. - **Output**: `str`: The hex digest of the file\'s contents. 2. `generate_hmac` - **Input**: `secret_key` (bytes): The secret key for HMAC. `message` (str): The message for which HMAC is to be generated. `algorithm` (str, optional): The hashing algorithm to use. Defaults to \'sha256\'. - **Output**: `str`: The generated HMAC in hexadecimal format. 3. `verify_hmac` - **Input**: `secret_key` (bytes): The secret key for HMAC. `message` (str): The message to verify. `provided_hmac` (str): The provided HMAC to be verified. `algorithm` (str, optional): The hashing algorithm to use. Defaults to \'sha256\'. - **Output**: `bool`: True if the provided HMAC matches the computed HMAC; otherwise, False. Constraints - Ensure the file path provided to `hash_file` exists and is readable. - Handle any exceptions appropriately, such as unsupported algorithms, file access errors, and invalid inputs. - Use the `hashlib` module for hashing and the `hmac` module for generating and verifying HMACs. Example Usage ```python # Example Usage file_hash = hash_file(\'example.txt\', \'sha512\') print(file_hash) # Output: The SHA512 hash of the file contents secret = b\'supersecretkey\' message = \'This is a message\' generated_hmac = generate_hmac(secret, message, \'sha256\') print(generated_hmac) # Output: The HMAC of the message using SHA256 is_valid = verify_hmac(secret, message, generated_hmac, \'sha256\') print(is_valid) # Output: True (since the provided HMAC matches the computed HMAC) ``` Performance Requirements - Ensure that the solution efficiently handles large files up to several gigabytes. - When implementing the hashing and HMAC functionalities, consider the performance implications of reading files and processing large messages.","solution":"import hashlib import hmac def hash_file(filepath: str, algorithm: str=\'sha256\') -> str: Returns the hex digest of the file\'s contents using the specified hashing algorithm. hash_func = None if algorithm == \'sha256\': hash_func = hashlib.sha256() elif algorithm == \'sha512\': hash_func = hashlib.sha512() elif algorithm == \'blake2b\': hash_func = hashlib.blake2b() else: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") try: with open(filepath, \'rb\') as f: for chunk in iter(lambda: f.read(4096), b\\"\\"): hash_func.update(chunk) return hash_func.hexdigest() except FileNotFoundError: raise FileNotFoundError(f\\"File not found: {filepath}\\") except IOError: raise IOError(f\\"Could not read file: {filepath}\\") def generate_hmac(secret_key: bytes, message: str, algorithm: str=\'sha256\') -> str: Generates an HMAC for the given message using the specified hashing algorithm and secret key. if algorithm not in [\'sha256\', \'sha512\']: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") hash_func = hashlib.sha256 if algorithm == \'sha256\' else hashlib.sha512 hmac_obj = hmac.new(secret_key, message.encode(), hash_func) return hmac_obj.hexdigest() def verify_hmac(secret_key: bytes, message: str, provided_hmac: str, algorithm: str=\'sha256\') -> bool: Verifies if the provided HMAC matches the computed HMAC for the given message and key. expected_hmac = generate_hmac(secret_key, message, algorithm) return hmac.compare_digest(expected_hmac, provided_hmac)"},{"question":"# PyTorch MPS Environment Variables Configuration Objective: You are required to implement a function that configures and demonstrates the usage of PyTorch environment variables specific to the Metal Performance Shaders (MPS) backend, predominantly used on Apple devices. Task: Write a Python function `configure_mps_environment` that: 1. Accepts a dictionary of environment variables and their corresponding values. 2. Sets these environment variables in the current Python session. 3. Checks for the appropriate execution of a simple PyTorch operation using the MPS backend (e.g., matrix multiplication). 4. Resets the environment variables to their original state after the operation is complete. Function Signature: ```python import os from typing import Dict import torch def configure_mps_environment(env_vars: Dict[str, str]) -> torch.Tensor: pass ``` Input: - `env_vars` (Dict[str, str]): A dictionary containing environment variable names as keys and their desired values as strings. Output: - Returns a PyTorch tensor resulting from a matrix multiplication operation executed on the MPS backend. Constraints: 1. Ensure that the matrix multiplication is performed on the MPS backend. 2. Handle the setting and resetting of environment variables within the Python function. 3. The function should be able to handle any unexpected errors gracefully. Example: ```python env_vars = { \\"PYTORCH_DEBUG_MPS_ALLOCATOR\\": \\"1\\", \\"PYTORCH_MPS_FAST_MATH\\": \\"1\\" } result = configure_mps_environment(env_vars) print(result) ``` Expected Output: A PyTorch tensor resulting from the matrix multiplication operation, processed via the MPS backend. Performance Considerations: - The function should manage any potential performance degradation by appropriately setting and resetting environment variables. - Optimize the memory usage by appropriately adjusting the high and low watermark ratios. Note: Be sure to import PyTorch and any other necessary libraries to perform the tasks.","solution":"import os import torch from typing import Dict def configure_mps_environment(env_vars: Dict[str, str]) -> torch.Tensor: Configures and demonstrates the usage of PyTorch environment variables specific to the MPS backend. Args: env_vars (Dict[str, str]): A dictionary containing environment variable names as keys and their desired values as strings. Returns: torch.Tensor: A tensor resulting from a matrix multiplication operation executed on the MPS backend. # Save original environment variables original_env_vars = {key: os.environ.get(key) for key in env_vars} try: # Set environment variables for key, value in env_vars.items(): os.environ[key] = value # Check if MPS is available if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available on this device.\\") # Perform a simple matrix multiplication using MPS backend device = torch.device(\\"mps\\") a = torch.randn(2, 3, device=device) b = torch.randn(3, 2, device=device) result = torch.matmul(a, b) finally: # Reset environment variables to their original state for key, value in original_env_vars.items(): if value is None: del os.environ[key] else: os.environ[key] = value return result"},{"question":"Objective: Demonstrate your understanding of Seaborn\'s experimental objects interface by manipulating a dataset, creating plots, and adding customized text annotations. Task: You are given a dataset of students\' test scores in various subjects. The dataset includes: - `Student`: Name of the student. - `Math`, `Science`, `English`, `History`: Scores in respective subjects. Your task is to: 1. Load the dataset into a pandas DataFrame. 2. Calculate the average score for each student and add it as a new column `Average`. 3. Create a bar plot showing the average scores for each student. 4. Annotate each bar with the student\'s name and average score, ensuring the text is clearly readable by applying suitable text alignment and appearance customizations. 5. Additionally, create a dot plot comparing Math and Science scores, annotate each point with the student\'s name, and customize the appearance of the annotations. Input: - The dataset will be provided in a CSV file named `student_scores.csv`. Output: - A bar plot with average scores annotated. - A dot plot comparing Math and Science scores annotated. Constraints: - The dataset file `student_scores.csv` is guaranteed to exist. - Use the Seaborn objects interface as demonstrated in the documentation. Example: Given `student_scores.csv`: ``` Student,Math,Science,English,History Alice,80,90,70,60 Bob,70,85,90,75 Charlie,90,95,85,80 ``` Expected tasks: 1. Load the dataset. 2. Add an `Average` column with values: `Alice: 75.0, Bob: 80.0, Charlie: 87.5`. 3. Create a bar plot of average scores. 4. Annotate each bar with the student\'s name and average score. For instance, Alice\'s bar should have a label \\"Alice: 75.0\\". 5. Create a dot plot comparing Math and Science scores, with each point annotated with the student\'s name. ```python import pandas as pd import seaborn.objects as so # Step 1: Load the dataset df = pd.read_csv(\'student_scores.csv\') # Step 2: Calculate the average score df[\'Average\'] = df[[\'Math\', \'Science\', \'English\', \'History\']].mean(axis=1).round(1) # Step 3: Create bar plot with annotations ( so.Plot(df, x=\'Average\', y=\'Student\', text=\'Average\') .add(so.Bar()) .add(so.Text(color=\'w\', halign=\'right\')) .plot() ) # Step 4: Create dot plot comparing Math and Science scores ( so.Plot(df, x=\'Math\', y=\'Science\', text=\'Student\') .add(so.Dot()) .add(so.Text(valign=\'bottom\')) .plot() ) ``` Good luck!","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def load_and_process_data(file_path): # Load the dataset df = pd.read_csv(file_path) # Calculate the average score for each student df[\'Average\'] = df[[\'Math\', \'Science\', \'English\', \'History\']].mean(axis=1).round(1) return df def create_bar_plot(df): # Create a bar plot of average scores with annotations plt.figure(figsize=(10, 6)) plot = ( so.Plot(df, x=\'Average\', y=\'Student\', text=\'Average\') .add(so.Bar()) .add(so.Text(color=\'w\', halign=\'right\')) ) plot.plot() plt.title(\'Average Scores of Students\') plt.xlabel(\'Average Score\') plt.ylabel(\'Student\') def create_dot_plot(df): # Create a dot plot comparing Math and Science scores with annotations plt.figure(figsize=(10, 6)) plot = ( so.Plot(df, x=\'Math\', y=\'Science\', text=\'Student\') .add(so.Dot()) .add(so.Text(valign=\'bottom\')) ) plot.plot() plt.title(\'Math vs Science Scores\') plt.xlabel(\'Math Score\') plt.ylabel(\'Science Score\')"},{"question":"# **Pandas Plotting Assessment** Objective: Demonstrate your understanding and ability to use pandas\' `plotting` module to create specific visualizations. Problem Statement: You are provided with a dataset containing information about various cars and their attributes. The dataset includes columns for \'Make\', \'Model\', \'Year\', \'Engine HP\', \'Engine Cylinders\', \'Transmission Type\', \'Driven Wheels\', \'Number of Doors\', \'Market Category\', \'Vehicle Size\', \'Vehicle Style\', and \'MSRP\'. Your task is to implement the following functions using pandas plotting methods: 1. `create_scatter_matrix(df)` 2. `create_parallel_coordinates(df, class_column)` These functions must meet the specifications provided below: Function 1: `create_scatter_matrix(df)` This function should produce a scatter matrix (pair plot) for the numeric columns in the provided DataFrame `df`. - **Input:** A pandas DataFrame `df` containing car attributes. - **Output:** A scatter matrix plot of numeric columns, displayed using matplotlib. Function 2: `create_parallel_coordinates(df, class_column)` This function should produce a parallel coordinates plot to visualize the multidimensional structure of the data, distinguishing classes using different colors taken from the provided `class_column`. - **Input:** - `df`: A pandas DataFrame containing car attributes. - `class_column`: A string representing the name of the column used to generate the classes for the plot. - **Output:** A parallel coordinates plot of the DataFrame, displayed using matplotlib with different colors for each class based on `class_column`. Constraints: - Use the pandas plotting module (import pandas.plotting as pd_plot). - Ensure to handle any missing data appropriately before plotting. - For `create_scatter_matrix`, only include numeric columns in the scatter matrix. - For `create_parallel_coordinates`, ensure that only necessary attributes are plotted (you may exclude non-numeric columns except for `class_column`). Example Usage: ```python import pandas as pd # Load data df = pd.read_csv(\'cars.csv\') # Generate scatter matrix create_scatter_matrix(df) # Generate parallel coordinates plot for \'Market Category\' create_parallel_coordinates(df, \'Market Category\') ``` Note: Ensure the resulting plots are correctly rendered using matplotlib and are easy to interpret. Consider using proper labels and legends where applicable.","solution":"import pandas as pd import matplotlib.pyplot as plt import pandas.plotting as pd_plot def create_scatter_matrix(df): Produces a scatter matrix (pair plot) for the numeric columns in the provided DataFrame `df`. Parameters: - df (pd.DataFrame): A pandas DataFrame containing car attributes. Output: - Displays a scatter matrix plot of numeric columns using matplotlib. numeric_df = df.select_dtypes(include=[\'number\']) pd_plot.scatter_matrix(numeric_df, figsize=(10, 10)) plt.show() def create_parallel_coordinates(df, class_column): Produces a parallel coordinates plot to visualize the multidimensional structure of the data, distinguishing classes using different colors taken from the provided `class_column`. Parameters: - df (pd.DataFrame): A pandas DataFrame containing car attributes. - class_column (str): The name of the column used to generate the classes for the plot. Output: - Displays a parallel coordinates plot using matplotlib. # Select only numeric columns and the class_column necessary_columns = df.select_dtypes(include=[\'number\']).columns.tolist() if class_column not in necessary_columns: necessary_columns.append(class_column) # Drop rows with missing values df_no_na = df.dropna(subset=necessary_columns) pd_plot.parallel_coordinates(df_no_na[necessary_columns], class_column) plt.show()"},{"question":"# Seaborn Assessment Question: You are provided with a dataset of students\' grades in three subjects: Mathematics, Science, and Literature. Your task is to create a series of plots using seaborn to visualize this data. Use the following guidelines: 1. **Input**: The dataset will be a Pandas DataFrame with the following columns: - `Student` (string): The name of the student - `Math` (float): The grade in Mathematics - `Science` (float): The grade in Science - `Literature` (float): The grade in Literature Example: ``` DataFrame: Student Math Science Literature John Doe 78.0 85.0 93.0 Jane Smith 91.0 89.0 87.0 Sam Brown 82.0 92.0 79.0 ``` 2. **Output**: - A bar plot for each subject showing the grades. - A single figure with three subplots (one for each subject) with the following features: - The bar color should be different for each subject. - Apply a \'darkgrid\' style to the plots. - Add a title to each subplot that indicates the subject. - Ensure that the plot is visually appealing with properly labeled axes. 3. **Constraints**: - Use Seaborn\'s functions to plot the data. - Utilize seaborn\'s functionality to set the style and customize the appearance of the plots. - The function should be able to handle any number of rows in the dataframe. 4. **Function Signature**: ```python def plot_student_grades(df: pd.DataFrame) -> None: Plots bar charts for students\' grades in Math, Science, and Literature. Args: df (pd.DataFrame): DataFrame containing students\' grades with columns \'Student\', \'Math\', \'Science\', and \'Literature\'. Returns: None: The function should display the plots. pass ``` # Example: Given the DataFrame: ```python import pandas as pd data = { \\"Student\\": [\\"John Doe\\", \\"Jane Smith\\", \\"Sam Brown\\"], \\"Math\\": [78.0, 91.0, 82.0], \\"Science\\": [85.0, 89.0, 92.0], \\"Literature\\": [93.0, 87.0, 79.0] } df = pd.DataFrame(data) ``` Calling `plot_student_grades(df)` should display a figure with three subplots, each showing a bar plot for the respective subject\'s grades. # Notes: 1. You might need to explore additional seaborn functionalities not covered in the provided documentation. 2. Ensure that your plots are informative and aesthetically pleasing. 3. Comment your code where necessary to explain your approach.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_student_grades(df: pd.DataFrame) -> None: Plots bar charts for students\' grades in Math, Science, and Literature. Args: df (pd.DataFrame): DataFrame containing students\' grades with columns \'Student\', \'Math\', \'Science\', and \'Literature\'. Returns: None: The function should display the plots. sns.set_style(\\"darkgrid\\") subjects = [\'Math\', \'Science\', \'Literature\'] colors = [\'blue\', \'green\', \'red\'] fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6)) for ax, subject, color in zip(axes, subjects, colors): sns.barplot(x=\'Student\', y=subject, data=df, ax=ax, palette=[color]) ax.set_title(subject + \' Grades\') ax.set_xlabel(\'Student\') ax.set_ylabel(\'Grade\') for item in ax.get_xticklabels(): item.set_rotation(45) plt.tight_layout() plt.show()"},{"question":"# Advanced PyTorch and TorchScript Coding Challenge Overview: You are tasked with implementing a PyTorch model with a custom loss function and converting this model into TorchScript for optimized deployment. The challenge will test your understanding of model scripting, type annotations, and TorchScript-specific constructs. You need to implement a `RegressionModel` class that includes a custom mean squared error loss function within the model itself. Requirements: 1. Define a PyTorch Module `RegressionModel` which includes: - Linear layers and a ReLU activation. - A custom method for Mean Squared Error (MSE) computation. - Use of various TorchScript-supported types (like `Optional`, `Tuple`, `List`, and `Dict`). 2. Ensure that the model and the custom loss function can be compiled using TorchScript. 3. Provide type annotations and handle type refinements as necessary. # Specifications: 1. **Type Annotations**: Use strict MyPy or Python 3 style type annotations. 2. **TorchScript Compilation**: Compile the model using `torch.jit.script`. 3. **Operations**: Model should support forward pass and loss computation. # Code Template: ```python import torch import torch.nn as nn import torch.nn.functional as F from typing import Tuple, List, Dict, Optional class RegressionModel(nn.Module): def __init__(self, input_dim: int, output_dim: int): super(RegressionModel, self).__init__() self.fc1 = nn.Linear(input_dim, 128) self.fc2 = nn.Linear(128, output_dim) self.relu = nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.relu(self.fc1(x)) x = self.fc2(x) return x def mse_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor: # Implement custom MSE loss function pass # Include additional methods as needed for TorchScript compatibility # Example usage: input_dim = 10 output_dim = 1 model = RegressionModel(input_dim, output_dim) # Create dummy data inputs = torch.randn(5, input_dim) targets = torch.randn(5, output_dim) # Forward pass outputs = model(inputs) # Compute loss loss = model.mse_loss(outputs, targets) # Convert model to TorchScript scripted_model = torch.jit.script(model) ``` # Inputs: - The `RegressionModel` should take in a tensor of dimensions `[batch_size, input_dim]`. - Parameters `input_dim` and `output_dim` specify the input and output tensor dimensions. # Outputs: - The `forward` method should return a tensor of predicted values. - The `mse_loss` method should compute and return the Mean Squared Error tensor between predictions and targets. # Constraints: - Ensure type consistency and proper type annotations for TorchScript. - Handle `Optional` types and use `assert` statements for type refinements. # Performance Requirements: - The model and the loss function should be TorchScript compliant and optimized for performance. Your task is to complete the `RegressionModel` class with the implementation of the `mse_loss` method and any additional modifications necessary to ensure complete TorchScript compatibility.","solution":"import torch import torch.nn as nn import torch.nn.functional as F from typing import Tuple, List, Dict, Optional class RegressionModel(nn.Module): def __init__(self, input_dim: int, output_dim: int): super(RegressionModel, self).__init__() self.fc1 = nn.Linear(input_dim, 128) self.fc2 = nn.Linear(128, output_dim) self.relu = nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.relu(self.fc1(x)) x = self.fc2(x) return x def mse_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor: Custom Mean Squared Error (MSE) loss function. Args: predictions (torch.Tensor): The predicted values. targets (torch.Tensor): The true values. Returns: torch.Tensor: The mean squared error loss. loss = torch.mean((predictions - targets) ** 2) return loss # Example usage: input_dim = 10 output_dim = 1 model = RegressionModel(input_dim, output_dim) # Create dummy data inputs = torch.randn(5, input_dim) targets = torch.randn(5, output_dim) # Forward pass outputs = model(inputs) # Compute loss loss = model.mse_loss(outputs, targets) # Convert model to TorchScript scripted_model = torch.jit.script(model)"},{"question":"**Question: Implement and Evaluate a Multi-layer Perceptron Model Using Scikit-Learn** # Objective: In this task, you will implement a Multi-layer Perceptron (MLP) model using scikit-learn. The goal is to classify the Iris dataset and evaluate the model\'s performance. # Dataset: The Iris dataset is available directly from scikit-learn. It consists of 150 samples from three species of Iris flowers (Iris setosa, Iris versicolor, Iris virginica), and each sample has four features (sepal length, sepal width, petal length, petal width). # Requirements: 1. Load the Iris dataset from scikit-learn. 2. Perform the necessary pre-processing, including splitting the dataset into training and testing sets, and feature scaling. 3. Implement an MLP model using `MLPClassifier` with the following parameters: - Solver: \'adam\' - Alpha: 0.001 (L2 regularization term) - Hidden layer sizes: (10, 10) (i.e., two hidden layers with 10 neurons each) - Random state: 42 4. Train the MLP model on the training data. 5. Evaluate the model on the testing data using the following metrics: - Accuracy - Confusion Matrix - Classification Report (including precision, recall, and F1-score) # Constraints: - You must use the `MLPClassifier` from scikit-learn. - Ensure that you apply the same scaling to both the training and testing data. - Use a train-test split ratio of 80-20 (80% for training and 20% for testing). # Input format: None # Output format: Print the following: 1. Accuracy of the MLP model on the testing data. 2. Confusion matrix of the MLP model on the testing data. 3. Classification report of the MLP model on the testing data. # Example: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix, classification_report # Load the Iris dataset data = load_iris() X = data.data y = data.target # Split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Feature scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize and train the MLPClassifier mlp = MLPClassifier(solver=\'adam\', alpha=0.001, hidden_layer_sizes=(10, 10), random_state=42) mlp.fit(X_train, y_train) # Evaluate the model y_pred = mlp.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) class_report = classification_report(y_test, y_pred) # Print the results print(\\"Accuracy:\\", accuracy) print(\\"Confusion Matrix:n\\", conf_matrix) print(\\"Classification Report:n\\", class_report) ``` # Expected Output: ``` Accuracy: 0.9667 Confusion Matrix: [[10 0 0] [ 0 10 0] [ 0 1 9]] Classification Report: precision recall f1-score support 0 1.00 1.00 1.00 10 1 0.91 1.00 0.95 10 2 1.00 0.90 0.95 10 accuracy 0.97 30 macro avg 0.97 0.97 0.97 30 weighted avg 0.97 0.97 0.97 30 ``` # Notes: - Ensure that all required libraries are imported. - The above example is illustrative. Your actual results may vary slightly depending on the random seed and model training.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix, classification_report def train_evaluate_mlp(): # Load the Iris dataset data = load_iris() X = data.data y = data.target # Split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Feature scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize and train the MLPClassifier mlp = MLPClassifier(solver=\'adam\', alpha=0.001, hidden_layer_sizes=(10, 10), random_state=42) mlp.fit(X_train, y_train) # Evaluate the model y_pred = mlp.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) class_report = classification_report(y_test, y_pred) return accuracy, conf_matrix, class_report if __name__ == \\"__main__\\": accuracy, conf_matrix, class_report = train_evaluate_mlp() print(\\"Accuracy:\\", accuracy) print(\\"Confusion Matrix:n\\", conf_matrix) print(\\"Classification Report:n\\", class_report)"},{"question":"# Question: File System Analyzer You are tasked with creating a utility that analyzes the file system structure, gathers metadata about each file, and returns a summary report. Your program should identify different file types, their permissions, and other relevant attributes. Function Specifications 1. **Function**: `analyze_file_system(root_dir: str) -> dict` - **Input**: - `root_dir` (str): The root directory to start the analysis. - **Output**: - A dictionary with the following structure: ```python { \\"total_files\\": int, \\"total_directories\\": int, \\"readable_files\\": int, \\"writable_files\\": int, \\"executable_files\\": int, \\"file_types\\": { \\"regular\\": int, \\"directory\\": int, \\"character_device\\": int, \\"block_device\\": int, \\"fifo\\": int, \\"symlink\\": int, \\"socket\\": int, \\"door\\": int, \\"event_port\\": int, \\"whiteout\\": int } } ``` Detailed Requirements 1. **Total Files/Directories**: - `total_files`: Total number of regular files. - `total_directories`: Total number of directories. 2. **File Permissions**: - `readable_files`: Number of files with read permissions for the owner. - `writable_files`: Number of files with write permissions for the owner. - `executable_files`: Number of files with execute permissions for the owner. 3. **File Types**: - Categories like `regular`, `directory`, `character_device`, `block_device`, `fifo`, `symlink`, `socket`, `door`, `event_port`, and `whiteout` should be counted using the `stat.S_IS*` functions. Constraints - You are allowed to use only the `os` and `stat` modules. - Your solution should handle symbolic links and other special files properly without following them. - The program should be efficient and avoid unnecessary recursive calls. Example ```python # Example usage result = analyze_file_system(\'/path/to/root\') print(result) # Example output format is described above ``` This question will test your understanding of the `stat` module, recursion, dictionary manipulation, and handling various file types and permissions.","solution":"import os import stat def analyze_file_system(root_dir: str) -> dict: report = { \\"total_files\\": 0, \\"total_directories\\": 0, \\"readable_files\\": 0, \\"writable_files\\": 0, \\"executable_files\\": 0, \\"file_types\\": { \\"regular\\": 0, \\"directory\\": 0, \\"character_device\\": 0, \\"block_device\\": 0, \\"fifo\\": 0, \\"symlink\\": 0, \\"socket\\": 0, \\"door\\": 0, \\"event_port\\": 0, \\"whiteout\\": 0 } } for dirpath, dirnames, filenames in os.walk(root_dir): # Count directories report[\\"total_directories\\"] += len(dirnames) for name in filenames + dirnames: path = os.path.join(dirpath, name) try: file_stat = os.lstat(path) except (OSError, ValueError): continue mode = file_stat.st_mode # Classify file types if stat.S_ISREG(mode): report[\\"file_types\\"][\\"regular\\"] += 1 report[\\"total_files\\"] += 1 elif stat.S_ISDIR(mode): report[\\"file_types\\"][\\"directory\\"] += 1 elif stat.S_ISCHR(mode): report[\\"file_types\\"][\\"character_device\\"] += 1 elif stat.S_ISBLK(mode): report[\\"file_types\\"][\\"block_device\\"] += 1 elif stat.S_ISFIFO(mode): report[\\"file_types\\"][\\"fifo\\"] += 1 elif stat.S_ISLNK(mode): report[\\"file_types\\"][\\"symlink\\"] += 1 elif stat.S_ISSOCK(mode): report[\\"file_types\\"][\\"socket\\"] += 1 else: report[\\"file_types\\"][\\"regular\\"] += 1 # Count permissions for regular files only if stat.S_ISREG(mode): if mode & stat.S_IRUSR: report[\\"readable_files\\"] += 1 if mode & stat.S_IWUSR: report[\\"writable_files\\"] += 1 if mode & stat.S_IXUSR: report[\\"executable_files\\"] += 1 return report"},{"question":"Objective Your task is to implement a utility function to apply the appropriate encoding function from the deprecated `email.encoders` module based on a given encoding type. Problem Statement Write a function `apply_encoding(msg, encoding_type)` that takes in two arguments: 1. `msg` (an instance of `email.message.EmailMessage`): The email message object to be encoded. 2. `encoding_type` (a string): The type of encoding to be applied. It can be one of the following values: `quoted-printable`, `base64`, `7or8bit`, `noop`. The function should apply the respective encoding function to the `msg` based on the given `encoding_type`. If an unsupported encoding type is provided, the function should raise a `ValueError` with the message \\"Unsupported encoding type\\". # Input Format - `msg`: An instance of `email.message.EmailMessage` with the payload set. - `encoding_type`: A string, one of `\\"quoted-printable\\"`, `\\"base64\\"`, `\\"7or8bit\\"`, `\\"noop\\"`. # Output Format - `None`. The function should modify the `msg` in place. # Constraints - The function should handle unsupported `encoding_type` values by raising a `ValueError`. - The function should not apply encoding on multipart messages, and should raise a `TypeError` if `msg.is_multipart()` returns True. Example ```python from email.message import EmailMessage from email.encoders import encode_quopri, encode_base64, encode_7or8bit, encode_noop def apply_encoding(msg, encoding_type): if msg.is_multipart(): raise TypeError(\\"Cannot apply encoding to multipart messages\\") if encoding_type == \'quoted-printable\': encode_quopri(msg) elif encoding_type == \'base64\': encode_base64(msg) elif encoding_type == \'7or8bit\': encode_7or8bit(msg) elif encoding_type == \'noop\': encode_noop(msg) else: raise ValueError(\\"Unsupported encoding type\\") # Test case email_msg = EmailMessage() email_msg.set_payload(\\"This is a test payload with some special characters Ã¤Ã¶Ã¼ÃŸ\\") apply_encoding(email_msg, \'base64\') print(email_msg.get(\\"Content-Transfer-Encoding\\")) # Should output: \'base64\' ``` # Note Make sure to handle exceptions correctly and ensure the performance is adequate for typical email message payload sizes.","solution":"from email.message import EmailMessage from email.encoders import encode_quopri, encode_base64, encode_7or8bit, encode_noop def apply_encoding(msg, encoding_type): Apply the specified encoding type to the email message. Parameters: msg (EmailMessage): The email message object to be encoded. encoding_type (str): The type of encoding to be applied. It can be one of \'quoted-printable\', \'base64\', \'7or8bit\', \'noop\'. Raises: TypeError: If the message is a multipart message. ValueError: If the encoding type is unsupported. if msg.is_multipart(): raise TypeError(\\"Cannot apply encoding to multipart messages\\") if encoding_type == \'quoted-printable\': encode_quopri(msg) elif encoding_type == \'base64\': encode_base64(msg) elif encoding_type == \'7or8bit\': encode_7or8bit(msg) elif encoding_type == \'noop\': encode_noop(msg) else: raise ValueError(\\"Unsupported encoding type\\")"},{"question":"# Email Utilities Assessment Python\'s `email.utils` module provides numerous utilities for handling email-specific needs such as dates and email addresses. In this assessment, you are required to implement a function that processes email headers and formats their dates into a human-readable format. Task Implement a function `process_email_headers(headers: dict) -> dict` that takes a dictionary of email headers and returns a dictionary with the same headers but where: 1. The `Date` header (if present) is converted to a human-readable format (e.g., \\"Friday, November 9, 2001 01:08:47 AM GMT\\"). 2. All email addresses in the `To`, `Cc`, and `From` headers are parsed into their real names and email addresses using `email.utils.parseaddr()` and returned as a list of tuples. Example ```python from email.utils import formatdate, parseaddr def process_email_headers(headers): # Your implementation here # Example usage headers = { \\"Date\\": \\"Fri, 09 Nov 2001 01:08:47 -0000\\", \\"From\\": \\"John Doe <john.doe@example.com>\\", \\"To\\": \\"Jane Smith <jane.smith@example.com>, Someone Else <someone.else@example.com>\\", \\"Cc\\": \\"\\" } processed_headers = process_email_headers(headers) print(processed_headers) # Expected output: # { # \\"Date\\": \\"Friday, November 9, 2001 01:08:47 AM GMT\\", # \\"From\\": [(\\"John Doe\\", \\"john.doe@example.com\\")], # \\"To\\": [(\\"Jane Smith\\", \\"jane.smith@example.com\\"), (\\"Someone Else\\", \\"someone.else@example.com\\")], # \\"Cc\\": [] # } ``` Constraints - The `Date` header will conform to RFC 2822 format. - The `To`, `Cc`, and `From` headers may be empty. - Performance is not a primary concern for this task; correctness and understanding of the functionality are more important. Hints - Use `email.utils.formatdate()` or `email.utils.format_datetime()` to format the date. - Use `email.utils.parseaddr()` to parse the email addresses.","solution":"from email.utils import parsedate_to_datetime, parseaddr def process_email_headers(headers): Processes email headers: 1. Converts the \'Date\' header to a human-readable format. 2. Parses \'To\', \'Cc\', and \'From\' headers into lists of tuples containing real names and email addresses. processed_headers = {} # Convert \'Date\' header to a human-readable format if \'Date\' in headers: date_str = headers[\'Date\'] date_obj = parsedate_to_datetime(date_str) formatted_date = date_obj.strftime(\\"%A, %B %d, %Y %I:%M:%S %p %Z\\") processed_headers[\'Date\'] = formatted_date # Parse \'To\', \'Cc\', and \'From\' headers for field in [\'From\', \'To\', \'Cc\']: if field in headers: addresses = headers[field] if addresses: processed_headers[field] = [parseaddr(addr) for addr in addresses.split(\\",\\")] else: processed_headers[field] = [] return processed_headers"},{"question":"**Pandas Series Coding Assessment** **Objective:** Your task is to work with a pandas Series object to perform various operations that demonstrate your understanding of both basic and advanced functionalities provided by the pandas library. **Scenario:** You are provided with a pandas Series that contains sales data for a store over a period of one year. The Series includes daily sales amounts, and the index is a datetime object representing each day. **Requirements:** 1. Load the sales data from a CSV file. 2. Perform various operations to analyze and manipulate the data. **Details:** 1. **Loading Data:** - The CSV file `sales_data.csv` contains two columns: `\\"date\\"` and `\\"sales\\"`. - Load the data into a pandas Series where the index is the `\\"date\\"` column and the values are the `\\"sales\\"` column. 2. **Operations:** 1. **Basic Attributes:** - Print the first and last five entries of the Series. - Print the data type of the Series. - Print the shape of the Series. 2. **Conversion:** - Convert the Series to a list and print the result. - Convert the Series index to a period with monthly frequency and print the result. 3. **Indexing & Iteration:** - Find and print the sales amount for a specific date, \\"2023-06-15\\". - Iterate over the Series and print out dates where the sales amount was above 1000. 4. **Binary Operations:** - Create a new Series that contains the sales amount multiplied by a factor of 1.1 (representing a 10% increase) and print the first five entries. - Create a boolean mask that identifies days where the sales amount is higher than the average sales amount. Print the dates and corresponding sales amounts using this mask. 5. **Function Application & GroupBy:** - Create a new Series that contains the cumulative sum of the sales data. - Group the sales data by month and print the average sales amount for each month. 6. **Descriptive Statistics:** - Print the total sum and mean of the sales data. - Print the top 5 days with the highest sales. 7. **Handling Missing Data:** - Identify any missing data in the Series and fill those missing values with the mean sales amount. - Print the Series after handling the missing data. 3. **Plotting:** - Plot the daily sales data as a line plot. - Plot the monthly average sales data as a bar plot. **Constraints:** - The data in the CSV file may contain missing values. - Your solution should be efficient and handle large datasets effectively. **Expected Input and Output:** ```python # Sample code to demonstrate the format of input and expected output import pandas as pd # Load the data from CSV data = pd.read_csv(\\"sales_data.csv\\", parse_dates=[\\"date\\"], index_col=\\"date\\") sales_series = pd.Series(data[\\"sales\\"]) # Perform the operations # 1. Basic Attributes print(sales_series.head()) print(sales_series.tail()) print(sales_series.dtype) print(sales_series.shape) # 2. Conversion sales_list = sales_series.to_list() print(sales_list[:5]) monthly_index = sales_series.to_period(\\"M\\") print(monthly_index) # 3. Indexing & Iteration print(sales_series.loc[\\"2023-06-15\\"]) for date, sales in sales_series.items(): if sales > 1000: print(date, sales) # 4. Binary Operations increased_sales = sales_series * 1.1 print(increased_sales.head()) average_sales = sales_series.mean() high_sales_mask = sales_series > average_sales print(sales_series[high_sales_mask]) # 5. Function Application & GroupBy cumulative_sales = sales_series.cumsum() print(cumulative_sales.head()) monthly_avg_sales = sales_series.resample(\\"M\\").mean() print(monthly_avg_sales) # 6. Descriptive Statistics print(sales_series.sum()) print(sales_series.mean()) print(sales_series.nlargest(5)) # 7. Handling Missing Data sales_series.fillna(sales_series.mean(), inplace=True) print(sales_series.isna().sum()) # Should print 0 # 8. Plotting sales_series.plot.line(title=\\"Daily Sales\\") monthly_avg_sales.plot.bar(title=\\"Monthly Average Sales\\") ``` **Notes:** - Ensure your code is well-commented and modular. - Handle exceptions where necessary. - Use appropriate functions and methods from the pandas library.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_sales_data(file_path): Load sales data from a CSV file into a pandas Series. data = pd.read_csv(file_path, parse_dates=[\\"date\\"], index_col=\\"date\\") return pd.Series(data[\\"sales\\"]) def basic_attributes(sales_series): Return basic attributes of the sales series. first_five_entries = sales_series.head() last_five_entries = sales_series.tail() dtype = sales_series.dtype shape = sales_series.shape return first_five_entries, last_five_entries, dtype, shape def conversion_operations(sales_series): Perform conversion operations. series_as_list = sales_series.to_list() period_index = sales_series.to_period(\\"M\\").index return series_as_list, period_index def indexing_iteration(sales_series): Perform indexing and iteration. specific_date_sales = sales_series.loc[\\"2023-06-15\\"] high_sales_dates = [(date, sales) for date, sales in sales_series.items() if sales > 1000] return specific_date_sales, high_sales_dates def binary_operations(sales_series, factor=1.1): Perform binary operations. increased_sales_series = sales_series * factor average_sales = sales_series.mean() high_sales_mask = sales_series > average_sales high_sales_series = sales_series[high_sales_mask] return increased_sales_series.head(), high_sales_mask, high_sales_series def function_application_groupby(sales_series): Apply functions and groupby operations. cumulative_sales_series = sales_series.cumsum() monthly_avg_sales = sales_series.resample(\\"M\\").mean() return cumulative_sales_series, monthly_avg_sales def descriptive_statistics(sales_series): Print descriptive statistics. total_sum = sales_series.sum() mean_sales = sales_series.mean() top_5_sales_dates = sales_series.nlargest(5) return total_sum, mean_sales, top_5_sales_dates def handle_missing_data(sales_series): Handle missing data. mean_sales = sales_series.mean() sales_series.fillna(mean_sales, inplace=True) return sales_series.isna().sum() def plot_sales_data(sales_series, monthly_avg_sales): Plot sales data. sales_series.plot.line(title=\\"Daily Sales\\") plt.show() monthly_avg_sales.plot.bar(title=\\"Monthly Average Sales\\") plt.show()"},{"question":"**Question: Custom Plot Creation with Seaborn** You are required to create a custom plot using the Seaborn library, specifically utilizing the `seaborn.objects` module. Your task involves plotting given datasets and customizing the plot limits as specified. # Task: 1. Create a line plot using the given data points for `x` and `y`. 2. Customize the plot by setting specific x and y limits. 3. Invert the y-axis based on the provided instructions. 4. For any unspecified plot limit, maintain the default value. # Input: - Two lists of integers or floats `x` and `y` representing data points. - A tuple `x_lim` where x_lim[0] and x_lim[1] represent the minimum and maximum limits for the x-axis. If the limit is `None`, use the default value. - A tuple `y_lim` where y_lim[0] and y_lim[1] represent the minimum and maximum limits for the y-axis. If the limit is `None`, use the default value. - A boolean `invert_y` which indicates whether to invert the y-axis. If `True`, the y-axis should be inverted. # Output: - Display the customized plot with the specified limits and axis configurations. # Constraints: - The lengths of `x` and `y` will be the same and at least 1. - The values in `x_lim` and `y_lim` can be either a number or `None`. - The plot must be displayed within the function with the given customizations. # Example: ```python import seaborn.objects as so def custom_plot(x, y, x_lim, y_lim, invert_y): # Create the initial plot p = so.Plot(x=x, y=y).add(so.Line(marker=\\"o\\")) # Apply the x and y limits if invert_y: y_lim = (y_lim[1], y_lim[0]) if y_lim[0] is not None and y_lim[1] is not None else y_lim p = p.limit(x=x_lim, y=y_lim) # Display the plot p.show() # Example usage custom_plot([1, 2, 3], [1, 3, 2], (0, 4), (None, None), True) ``` In this example: - The `x` data points are `[1, 2, 3]`. - The `y` data points are `[1, 3, 2]`. - The x-axis limit is set from 0 to 4. - The y-axis retains its default limit but is inverted. - The plot is displayed with these configurations. Feel free to experiment with different limits and invert settings to understand how customizations affect the plot.","solution":"import seaborn.objects as so import matplotlib.pyplot as plt def custom_plot(x, y, x_lim, y_lim, invert_y): Creates a custom line plot with specified x and y limits and optionally inverts the y-axis. Parameters: x (list of int/float): The data points for the x-axis. y (list of int/float): The data points for the y-axis. x_lim (tuple): The minimum and maximum limits for the x-axis (None for default). y_lim (tuple): The minimum and maximum limits for the y-axis (None for default). invert_y (bool): Whether to invert the y-axis. # Create the initial plot p = so.Plot(x=x, y=y).add(so.Line(marker=\\"o\\")) # Apply the x limits if specified if x_lim[0] is not None or x_lim[1] is not None: p = p.limit(x=x_lim) # Apply the y limits if specified if invert_y: y_lim = (y_lim[1], y_lim[0]) if y_lim[0] is not None and y_lim[1] is not None else y_lim if y_lim[0] is not None or y_lim[1] is not None: p = p.limit(y=y_lim) # Display the plot p.show()"},{"question":"**Question: Implement a Function to Encode and Decode Complex Data Structures Using xdrlib** You have been provided with the `xdrlib` module documentation, which includes classes for packing and unpacking data using the External Data Representation (XDR) standard. Your task is to implement functions to encode and decode a complex data structure. # Data Structure The data structure to be encoded and decoded is a dictionary with the following format: ```python { \\"name\\": str, # A string representing a name \\"age\\": int, # An integer representing age \\"is_student\\": bool, # A boolean indicating if the person is a student \\"grades\\": list, # A list of integers representing grades \\"profile_pic\\": bytes # A byte stream representing a profile picture } ``` # Requirements 1. **Function to Encode Data (`encode_data`)**: - Input: A dictionary conforming to the above structure. - Output: A byte string representing the XDR encoded data. - Constraints: Ensure proper encoding of each element, including handling padding for alignment as required by XDR. 2. **Function to Decode Data (`decode_data`)**: - Input: A byte string representing the XDR encoded data. - Output: A dictionary conforming to the above structure. - Constraints: Ensure the decoded data correctly matches the original input data structure. # Steps for Implementation 1. **Encoding (`encode_data`)**: - Initialize a `Packer` instance. - Pack each element of the dictionary using appropriate methods. - Return the resulting buffer. 2. **Decoding (`decode_data`)**: - Initialize an `Unpacker` instance with the byte string. - Unpack each element in the order they were packed. - Construct and return the dictionary with unpacked values. # Example ```python import xdrlib def encode_data(data): p = xdrlib.Packer() p.pack_string(data[\'name\']) p.pack_int(data[\'age\']) p.pack_bool(data[\'is_student\']) p.pack_array(data[\'grades\'], p.pack_int) p.pack_bytes(data[\'profile_pic\']) return p.get_buffer() def decode_data(encoded_data): u = xdrlib.Unpacker(encoded_data) data = {} data[\'name\'] = u.unpack_string() data[\'age\'] = u.unpack_int() data[\'is_student\'] = u.unpack_bool() data[\'grades\'] = u.unpack_array(u.unpack_int) data[\'profile_pic\'] = u.unpack_bytes() return data # Test the functions original_data = { \\"name\\": \\"John Doe\\", \\"age\\": 25, \\"is_student\\": True, \\"grades\\": [95, 88, 92], \\"profile_pic\\": b\'x89PNGx0dx0ax1ax0a...\' } encoded = encode_data(original_data) decoded = decode_data(encoded) assert decoded == original_data print(\\"Encoding and decoding successful.\\") ``` **Note**: Ensure your functions handle encoding and decoding properly, considering any necessary alignment and padding as specified in the `xdrlib` documentation.","solution":"import xdrlib def encode_data(data): Encodes a given dictionary into an XDR byte string. Args: data (dict): The input dictionary to be encoded with keys \\"name\\" (str), \\"age\\" (int), \\"is_student\\" (bool), \\"grades\\" (list of ints), and \\"profile_pic\\" (bytes). Returns: bytes: The XDR encoded byte string. p = xdrlib.Packer() p.pack_string(data[\'name\'].encode()) # Name packed as string p.pack_int(data[\'age\']) # Age packed as integer p.pack_bool(data[\'is_student\']) # is_student packed as boolean p.pack_array(data[\'grades\'], p.pack_int) # grades packed as array of integers p.pack_bytes(data[\'profile_pic\']) # profile_pic packed as bytes return p.get_buffer() def decode_data(encoded_data): Decodes a given XDR byte string back into the original dictionary. Args: encoded_data (bytes): The XDR encoded byte string. Returns: dict: The decoded dictionary with keys \\"name\\" (str), \\"age\\" (int), \\"is_student\\" (bool), \\"grades\\" (list of ints), and \\"profile_pic\\" (bytes). u = xdrlib.Unpacker(encoded_data) data = {} data[\'name\'] = u.unpack_string().decode() # Name unpacked from string and decoded data[\'age\'] = u.unpack_int() # Age unpacked as integer data[\'is_student\'] = u.unpack_bool() # is_student unpacked as boolean data[\'grades\'] = u.unpack_array(u.unpack_int) # grades unpacked as array of integers data[\'profile_pic\'] = u.unpack_bytes() # profile_pic unpacked as bytes return data"},{"question":"Objective Demonstrate your understanding of URL handling, making HTTP requests, and handling HTTP responses and exceptions using Python\'s `urllib` and `http.client` modules. Problem Statement You are required to implement a Python function `fetch_page_title(url: str) -> str` that takes a URL as input and returns the title of the HTML page at that URL. If the URL is invalid, the function should raise a `ValueError`. If there is any network-related error, the function should return a string \\"Network Error: <error message>\\". Function Signature ```python def fetch_page_title(url: str) -> str: pass ``` Detailed Requirements 1. **URL Validation**: - Use `urllib.parse` to parse and validate the URL. If the URL is invalid, raise a `ValueError` with the message \\"Invalid URL\\". 2. **HTTP GET Request**: - Use `http.client` to make an HTTP GET request to the URL. 3. **HTTP Response Handling**: - Extract the status code from the response. If the status code is not 200, return a string in the format \\"Error <status code>: <status message>\\". - If the status code is 200, parse the HTML content to extract the title tag and return its content. 4. **Exception Handling**: - Catch network-related exceptions and return a string in the format \\"Network Error: <error message>\\". Example ```python url = \\"http://example.com\\" print(fetch_page_title(url)) # Should return the content of the <title> tag of the HTML page url = \\"http://nonexistent.url\\" print(fetch_page_title(url)) # Should return \\"Network Error: <appropriate error message>\\" ``` Constraints - Do not use any third-party libraries. Only standard Python libraries are allowed. - The HTML content can be assumed to be well-formed and contain a `<title>` tag. Performance Requirements - The function should handle URLs reasonably quickly under typical network conditions. - The focus should be on correctness and proper error handling rather than performance optimization. --- This question assesses the student\'s ability to work with URL parsing, HTTP requests, and exceptions, which are crucial for handling internet protocols in Python.","solution":"import urllib.parse import http.client from html.parser import HTMLParser class TitleParser(HTMLParser): def __init__(self): super().__init__() self.in_title = False self.title = \\"\\" def handle_starttag(self, tag, attrs): if tag == \\"title\\": self.in_title = True def handle_endtag(self, tag): if tag == \\"title\\": self.in_title = False def handle_data(self, data): if self.in_title: self.title += data def fetch_page_title(url: str) -> str: try: parsed_url = urllib.parse.urlparse(url) if not all([parsed_url.scheme in {\\"http\\", \\"https\\"}, parsed_url.netloc]): raise ValueError(\\"Invalid URL\\") connection = http.client.HTTPSConnection(parsed_url.netloc) if parsed_url.scheme == \\"https\\" else http.client.HTTPConnection(parsed_url.netloc) path = parsed_url.path if parsed_url.path else \\"/\\" if parsed_url.query: path += \'?\' + parsed_url.query connection.request(\\"GET\\", path) response = connection.getresponse() if response.status != 200: return f\\"Error {response.status}: {response.reason}\\" content = response.read().decode() parser = TitleParser() parser.feed(content) return parser.title.strip() except ValueError as e: raise e except Exception as e: return f\\"Network Error: {str(e)}\\""},{"question":"**Question: Debugging with Python Development Mode** As a software developer, you are tasked with writing a Python script that reads each line from a text file and prints the first ten lines. Your script must ensure resource management best practices and utilize Python Development Mode for debugging. **Your Task:** 1. Write a Python script that: - Accepts a filename as a command-line argument. - Opens the file, reads all the lines, and prints the first ten lines. - Ensures that the file is properly closed after reading, using best practices for resource management. 2. Run your script using Python Development Mode to ensure there are no resource warnings or errors. 3. Provide a brief explanation of any warnings or errors encountered when initially writing the script and how you resolved them. **Constraints and Requirements:** - Your script should handle the case where the file has less than ten lines. - Ensure proper resource management using context managers. - Use Python Development Mode (`-X dev`) to identify and resolve any potential issues. - The filename will always be a valid text file present in the same directory as the script. **Example:** Given a file `example.txt` with the following content: ``` Line 1 Line 2 Line 3 Line 4 Line 5 Line 6 Line 7 Line 8 Line 9 Line 10 Line 11 Line 12 ``` Running your script: ``` python3 -X dev your_script.py example.txt ``` Expected Output: ``` Line 1 Line 2 Line 3 Line 4 Line 5 Line 6 Line 7 Line 8 Line 9 Line 10 ``` Your script should not produce any `ResourceWarning` or other exceptions under Python Development Mode. **Submission:** - Submit your Python script `your_script.py`. - In a separate file `explanation.txt`, briefly describe any issues encountered and explain how you resolved them using Python Development Mode.","solution":"import sys def print_first_ten_lines(filename): Opens the file, reads all the lines, and prints the first ten lines. Ensures the file is properly closed after reading. try: with open(filename, \'r\') as file: for i, line in enumerate(file): if i >= 10: break print(line.strip()) except FileNotFoundError: print(f\\"The file {filename} was not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python3 your_script.py <filename>\\") else: filename = sys.argv[1] print_first_ten_lines(filename)"},{"question":"Introduction Pandas 3.0 defaults to using Copy-on-Write (CoW) to prevent unintended side effects when modifying DataFrame or Series objects. This change requires users to adapt their code to avoid direct modifications of views that could impact the parent object. Objective Write a function `update_dataframe(df: pd.DataFrame) -> pd.DataFrame` that takes a pandas DataFrame as input and performs the following operations while complying with CoW principles: 1. Selects a subset of the DataFrame where a given column\'s values meet a specific condition. 2. Modifies this subset in a way that does not affect the original DataFrame. 3. Assigns the modified subset back to the appropriate section of the DataFrame. Requirements 1. You must use `loc` or `iloc` for the update operations to comply with CoW rules. 2. The original DataFrame should not be modified unless specified. 3. The function should not generate any chained assignment errors. Function Signature ```python import pandas as pd def update_dataframe(df: pd.DataFrame) -> pd.DataFrame: pass ``` Example ```python # Input DataFrame df = pd.DataFrame({\\"A\\": [1, 2, 3, 4], \\"B\\": [5, 6, 7, 8]}) # Call the function updated_df = update_dataframe(df) # Output DataFrame (Example, assuming change is to double the values in \'A\' where \'B\' > 6) # df should remain unchanged # updated_df should be: # A B # 0 1 5 # 1 2 6 # 2 6 7 # 3 8 8 print(\\"Original DataFrame\\") print(df) print(\\"Updated DataFrame\\") print(updated_df) ``` Constraints 1. The column to filter and modify in the DataFrame is \\"A\\". 2. The condition to apply is `df[\\"B\\"] > 6`. 3. The modification should double the values in column \\"A\\" for rows where the condition is met. Evaluation Your implementation will be assessed based on: 1. Correct use of pandas CoW principles. 2. Proper handling of data without causing side effects to the original DataFrame. 3. Effectiveness of the code: correct and optimal updates. # Note Before writing your function, familiarize yourself with the CoW feature in pandas and ensure that your solutions do not result in indirect modifications to the original DataFrame. Remember to import pandas at the top of your script.","solution":"import pandas as pd def update_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Make a copy of the DataFrame to ensure CoW principles updated_df = df.copy() # Select the subset where column \\"B\\" > 6 condition = updated_df[\\"B\\"] > 6 # Modify the subset by doubling the values of column \\"A\\" updated_df.loc[condition, \\"A\\"] = updated_df.loc[condition, \\"A\\"] * 2 # Return the updated DataFrame return updated_df"},{"question":"**Objective:** Implement a Python function that fetches data from a given URL and handles common networking scenarios including error handling, retrieving headers, and following redirects. **Problem Statement:** You are required to write a Python function `fetch_url_data(url: str) -> dict` that takes a single parameter: - `url` (string): The URL from which data needs to be fetched. The function should perform the following tasks: 1. Fetch the content of the URL. 2. Handle the following HTTP errors gracefully: - 404: Page Not Found - 403: Forbidden - 401: Unauthorized 3. Extract and return certain headers (`Content-Type`, `Content-Length`, `Server`) from the HTTP response. 4. Identify whether the final URL (after following any redirects) is different from the input URL. 5. Handle network-related errors such as connection disruption or the server being unreachable. The function should return a dictionary with the following structure: ```python { \\"content\\": str, # The content of the page if no error occurs, else an error message. \\"error_code\\": Optional[int], # The HTTP error code if an error occurs, else None. \\"headers\\": dict, # A dictionary containing the extracted headers. \\"final_url\\": str # The final URL after following any redirects. } ``` **Constraints:** - Use the `urllib` library for handling HTTP requests. - The header keys in the returned dictionary should be in lower-case. - If an error occurs, set the `\\"content\\"` value to a suitable error message (`\\"Page Not Found\\"`, `\\"Forbidden\\"`, `\\"Unauthorized\\"` etc.). - Follow any HTTP redirections to obtain the final URL. **Function Signature:** ```python import urllib.request import urllib.error from typing import Dict, Optional def fetch_url_data(url: str) -> Dict[str, Optional[any]]: pass ``` **Example:** ```python url = \\"http://example.com\\" result = fetch_url_data(url) # Example Output Structure # { # \\"content\\": \\"<!doctype html><html>...</html>\\", # The HTML content as a string. # \\"error_code\\": None, # \\"headers\\": { # \\"content-type\\": \\"text/html; charset=UTF-8\\", # \\"content-length\\": \\"606\\", # \\"server\\": \\"ECS (nyb/1D2D)\\" # }, # \\"final_url\\": \\"http://example.com\\" # } ``` **Note:** Actual values for content, headers, and URL may differ based on the URL provided. # Evaluation Criteria: 1. **Correctness:** The solution should correctly handle all specified HTTP errors and extract headers. 2. **Exception Handling:** Appropriately handle network-related exceptions without the program crashing. 3. **Following Redirects:** Correctly identify and report the final URL after any redirects. 4. **Adherence to Requirements:** Solution should adhere to the given function signature and return the dictionary in the required format. **Tips:** - To decode the response content, you can use `response.read().decode(\'utf-8\')`. - Use `response.info()` to get headers as an `http.client.HTTPMessage` object and extract the required headers using `dict(response.info())`.","solution":"import urllib.request import urllib.error from typing import Dict, Optional def fetch_url_data(url: str) -> Dict[str, Optional[any]]: headers_extract = [\'content-type\', \'content-length\', \'server\'] try: response = urllib.request.urlopen(url) content = response.read().decode(\'utf-8\') final_url = response.geturl() except urllib.error.HTTPError as e: error_map = {404: \\"Page Not Found\\", 403: \\"Forbidden\\", 401: \\"Unauthorized\\"} return { \\"content\\": error_map.get(e.code, str(e)), \\"error_code\\": e.code, \\"headers\\": {}, \\"final_url\\": e.geturl() if e.geturl() else url } except urllib.error.URLError as e: return { \\"content\\": \\"Network error: \\" + str(e), \\"error_code\\": None, \\"headers\\": {}, \\"final_url\\": url } headers = {key.lower(): value for key, value in response.info().items() if key.lower() in headers_extract} return { \\"content\\": content, \\"error_code\\": None, \\"headers\\": headers, \\"final_url\\": final_url }"},{"question":"Implement a dataclass that models a simple banking system with accounts and transactions. Requirements: 1. **Account Class:** - Create an `Account` dataclass with the following fields: - `account_id`: A unique ID for the account (string). - `balance`: The account balance (float), defaulting to 0.0. - `transactions`: A list to store transaction amounts (float), defaulting to an empty list using `default_factory`. - Ensure that instances of `Account` are immutable (i.e., `frozen=True`). - Implement a method `add_transaction` to add a transaction amount to the `transactions` list. This method should return a new `Account` instance with the updated transactions and balance. 2. **Bank Class:** - Create a `Bank` dataclass with the following fields: - `accounts`: A dictionary to store accounts, where keys are account IDs and values are `Account` instances. Use `default_factory` to initialize this field with an empty dictionary. - Implement methods: - `create_account(account_id: str) -> Account`: Creates a new account with the given `account_id` and adds it to the bank. - `get_account(account_id: str) -> Account`: Returns the `Account` instance for the given `account_id`. Raise a `ValueError` if the account does not exist. - `deposit(account_id: str, amount: float) -> None`: Adds a deposit transaction to the specified account. - `withdraw(account_id: str, amount: float) -> None`: Adds a withdrawal transaction to the specified account. Raise a `ValueError` if the balance is insufficient. Constraints: - The `deposit` and `withdraw` methods should ensure that the `Account` balance is updated based on the transactions. - Ensure immutability of the `Account` instances while allowing the `Bank` to manage accounts and transactions. - Raise appropriate exceptions for errors such as duplicate account creation, non-existent accounts, or insufficient balance for withdrawals. Example Usage: ```python bank = Bank() # Create accounts bank.create_account(\\"123\\") bank.create_account(\\"456\\") # Perform transactions bank.deposit(\\"123\\", 100.0) bank.withdraw(\\"123\\", 50.0) bank.deposit(\\"456\\", 200.0) # Retrieve account details account_123 = bank.get_account(\\"123\\") account_456 = bank.get_account(\\"456\\") print(account_123.balance) # 50.0 print(account_456.balance) # 200.0 print(account_123.transactions) # [100.0, -50.0] print(account_456.transactions) # [200.0] ``` Notes: - You may assume that all account IDs are unique strings. - Use appropriate type annotations for all methods and parameters. - Implement the solution in an efficient manner, ensuring that each method performs the necessary operations correctly.","solution":"from dataclasses import dataclass, field, replace from typing import List, Dict @dataclass(frozen=True) class Account: account_id: str balance: float = 0.0 transactions: List[float] = field(default_factory=list) def add_transaction(self, amount: float) -> \'Account\': new_balance = self.balance + amount new_transactions = self.transactions + [amount] return replace(self, balance=new_balance, transactions=new_transactions) @dataclass class Bank: accounts: Dict[str, Account] = field(default_factory=dict) def create_account(self, account_id: str) -> Account: if account_id in self.accounts: raise ValueError(f\\"Account with ID {account_id} already exists.\\") account = Account(account_id=account_id) self.accounts[account_id] = account return account def get_account(self, account_id: str) -> Account: if account_id not in self.accounts: raise ValueError(f\\"Account with ID {account_id} does not exist.\\") return self.accounts[account_id] def deposit(self, account_id: str, amount: float) -> None: if amount <= 0: raise ValueError(\\"Deposit amount must be positive.\\") acc = self.get_account(account_id) new_acc = acc.add_transaction(amount) self.accounts[account_id] = new_acc def withdraw(self, account_id: str, amount: float) -> None: if amount <= 0: raise ValueError(\\"Withdrawal amount must be positive.\\") acc = self.get_account(account_id) if acc.balance < amount: raise ValueError(\\"Insufficient balance for withdrawal.\\") new_acc = acc.add_transaction(-amount) self.accounts[account_id] = new_acc"},{"question":"# Advanced Python Coding Assessment Objective Demonstrate your understanding of the `modulefinder` module by analyzing a given Python script and generating a detailed report of the imported modules. Task Write a Python function `analyze_script_imports(script_path, exclude_modules=None, replace_paths=None)` that: 1. Uses the `ModuleFinder` class from the `modulefinder` module to analyze the imports of the given script. 2. Optionally excludes specified modules from the analysis. 3. Optionally replaces module paths according to specified rules. 4. Returns a tuple of two lists: - A list of strings representing the names of successfully loaded modules. - A list of strings representing the names of modules that were not imported successfully. Function Signature ```python def analyze_script_imports(script_path: str, exclude_modules: list = None, replace_paths: list = None) -> tuple: ... ``` Input - `script_path`: A string representing the file path of the Python script to be analyzed. - `exclude_modules`: An optional list of module names to exclude from the analysis. Default is `None`. - `replace_paths`: An optional list of tuples, where each tuple contains two strings representing an old path and a new path for replacing module paths. Default is `None`. Output - A tuple containing two lists: - The first list contains the names of successfully loaded modules. - The second list contains the names of modules that were not imported successfully. Example Given a script `example_script.py` with the following content: ```python import os import sys try: import non_existent except ImportError: pass ``` And assuming `non_existent` is a non-existing module. Calling your function: ```python loaded_modules, not_imported_modules = analyze_script_imports(\\"example_script.py\\") print(\\"Loaded Modules:\\", loaded_modules) print(\\"Modules Not Imported:\\", not_imported_modules) ``` The expected output should be similar to: ``` Loaded Modules: [\'os\', \'sys\'] Modules Not Imported: [\'non_existent\'] ``` Constraints - Use the `ModuleFinder` class and its methods as described in the documentation to complete this task. - Ensure the function handles edge cases, such as invalid file paths and empty scripts. Notes - This task assesses your ability to understand and utilize a specific Python module based on its documentation. - Ensure your code is well-documented and follows best practices.","solution":"import modulefinder import sys def analyze_script_imports(script_path, exclude_modules=None, replace_paths=None): Analyze imports in the given Python script. Parameters: - script_path (str): The path to the Python script. - exclude_modules (list, optional): List of module names to exclude from analysis. Default is None. - replace_paths (list, optional): List of tuples with (old_path, new_path) to replace in module paths. Default is None. Returns: - tuple: A tuple containing two lists: - List of successfully loaded modules - List of modules that were not imported successfully finder = modulefinder.ModuleFinder(excludes=exclude_modules) # Preprocessing the sys.path if replace_paths are provided. if replace_paths: original_paths = sys.path.copy() for old_path, new_path in replace_paths: sys.path = [path.replace(old_path, new_path) for path in sys.path] # Run the script with modulefinder to analyze imports finder.run_script(script_path) # Restore original sys.path in case it was modified if replace_paths: sys.path = original_paths # Get loaded and missing modules loaded_modules = list(finder.modules.keys()) missing_modules = list(finder.badmodules.keys()) return loaded_modules, missing_modules"},{"question":"<|Analysis Begin|> The given documentation is for the \\"dis\\" module, which is used for disassembling Python byte code into a more human-readable format. The documentation provides thorough information on: 1. Basic usage of the `dis` module to disassemble functions and other code objects. 2. The `Bytecode` class which wraps various bytecode analysis functions. 3. Additional functions for detailed bytecode and stack effect analysis. 4. Specific bytecode instructions and their meanings. 5. Collections of bytecode opcodes for automatic introspection. Important classes and methods: - `dis.Bytecode` - Analyses the bytecode for given Python code objects and provides useful interfaces for accessing bytecode operations. - Methods: `dis()`, `info()`, and `from_traceback()`. - Functions like: `dis.dis()`, `dis.code_info()`, `dis.show_code()`, and their usage for introspection and analysis. - Different bytecode instructions and their effects on the Python stack. The documentation is detailed enough to be used to create a comprehensive coding assessment question, focusing on analyzing Python functions using the `dis` module to identify and interpret the bytecode instructions. <|Analysis End|> <|Question Begin|> Problem Statement The goal of the task is to analyze a given Python function using Python bytecode and provide insights into its execution flow. # Task 1. Implement a function `analyze_function_bytecode` that takes a Python function as input and returns a dictionary containing the following: - `instructions`: A list of tuples where each tuple represents a bytecode instruction with the format `(operation, argument, offset)`. - `load_global_counts`: A count of occurrences for the `LOAD_GLOBAL` opcode. - `call_function_counts`: A count of occurrences for the `CALL_FUNCTION` opcode. - `jump_instructions`: A list of all the jump instructions along with their argument values. # Function Signature ```python import dis from typing import Dict, List, Tuple def analyze_function_bytecode(func: callable) -> Dict[str, any]: pass ``` # Detailed Requirements - **Input:** - A Python function `func`. - **Output:** - A dictionary with the following keys: - `instructions`: A list of tuples. Each tuple should have three elements: `operation` (the name of the opcode), `argument` (the argument to the opcode, if any), and `offset` (the byte offset in the bytecode sequence). - `load_global_counts`: An integer count of how many times the `LOAD_GLOBAL` opcode appears. - `call_function_counts`: An integer count of how many times the `CALL_FUNCTION` opcode appears. - `jump_instructions`: A list of tuples where each tuple contains two elements: the opcode name and its argument (for jump-related opcodes). # Constraints: - You should use the `dis` module to analyze the bytecode. - The function you analyze will have no more than 20 instructions. # Example ```python def example_func(lst): total = 0 for item in lst: total += item return total result = analyze_function_bytecode(example_func) print(result) ``` Expected output format: ```python { \\"instructions\\": [ (\'LOAD_CONST\', 0, 0), # Example instruction: (operation, argument, offset) (\'LOAD_FAST\', 0, 2), (\'STORE_FAST\', 1, 4), ... ], \\"load_global_counts\\": 0, \\"call_function_counts\\": 0, \\"jump_instructions\\": [ (\'FOR_ITER\', 6), # Example jump instruction: (operation, argument) ... ] } ``` Notes: - You may assume that the dis module and the Python function to be analyzed are available during the run. - Consider using `dis.Bytecode` and iterating over the `Instruction` instances to gather the necessary information. # Hints: - Use `dis.Bytecode` to wrap the function and iterate over its bytecode instructions. - Utilize the attributes of `dis.Instruction` for extracting and counting specific opcode occurrences.","solution":"import dis from typing import Dict, List, Tuple def analyze_function_bytecode(func: callable) -> Dict[str, any]: bytecode = dis.Bytecode(func) instructions = [] load_global_counts = 0 call_function_counts = 0 jump_instructions = [] for instr in bytecode: instructions.append((instr.opname, instr.argval, instr.offset)) if instr.opname == \'LOAD_GLOBAL\': load_global_counts += 1 elif instr.opname == \'CALL_FUNCTION\': call_function_counts += 1 elif instr.opname in {\'JUMP_FORWARD\', \'JUMP_ABSOLUTE\', \'POP_JUMP_IF_FALSE\', \'POP_JUMP_IF_TRUE\', \'JUMP_IF_FALSE_OR_POP\', \'JUMP_IF_TRUE_OR_POP\', \'CONTINUE_LOOP\', \'FOR_ITER\', \'SETUP_LOOP\', \'SETUP_EXCEPT\', \'SETUP_FINALLY\'}: jump_instructions.append((instr.opname, instr.argval)) return { \\"instructions\\": instructions, \\"load_global_counts\\": load_global_counts, \\"call_function_counts\\": call_function_counts, \\"jump_instructions\\": jump_instructions }"},{"question":"**Challenging Seaborn Exercise: Advanced Visualization Customization** **Objective:** This exercise aims to test your understanding of the advanced features of the seaborn library, specifically focusing on mark properties and their customizations. You will create a complex visualization that incorporates these advanced features. **Task:** You are provided with a dataset containing information about various species of flowers, including their petal lengths, petal widths, species names, and the date they were observed. Your task is to create a customized seaborn plot with the following specifications: 1. **Scatter Plot**: - Plot the petal length (x-axis) against petal width (y-axis). - Each point should represent a flower. - Points should be colored according to the species of the flower. - Different marker styles should be used for each species. 2. **Average Line**: - Add a line that represents the average petal length and petal width for each species. - Use distinct linestyles for each species. 3. **Datum Points**: - Datum points for petal lengths below the 25th percentile should be marked in red. - Datum points for petal widths above the 75th percentile should be marked with an increased alpha. 4. **Custom Legend**: - Create a custom legend that describes the colors and markers used for each species. # Function Signature ```python def create_custom_seaborn_plot(data: pd.DataFrame) -> None: pass ``` # Input - **data** (`pd.DataFrame`): A DataFrame with columns `[\'petal_length\', \'petal_width\', \'species\', \'date_observed\']`. # Constraints - You may assume the input DataFrame is not empty and contains valid data. - Use seaborn objects (`so`) for completing the task. # Example Usage ```python import pandas as pd # Example data data = pd.DataFrame({ \'petal_length\': [1.4, 1.3, 1.5, 1.7, 1.6, 1.0, 1.1], \'petal_width\': [0.2, 0.3, 0.2, 0.4, 0.3, 0.1, 0.2], \'species\': [\'setosa\', \'setosa\', \'setosa\', \'versicolor\', \'versicolor\', \'setosa\', \'versicolor\'], \'date_observed\': pd.to_datetime([\'2021-01-01\', \'2021-01-03\', \'2021-01-05\', \'2021-01-07\', \'2021-01-09\', \'2021-02-01\', \'2021-02-03\']) }) # Call the function with the example data create_custom_seaborn_plot(data) ``` # Notes: - You will be graded on the following: - **Correctness**: Does the plot accurately represent the data as per the specifications? - **Customization**: Are advanced seaborn features used effectively? - **Readability**: Is the code clear and well-documented? Happy plotting!","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_custom_seaborn_plot(data: pd.DataFrame) -> None: sns.set(style=\\"whitegrid\\") # Determine the markers and linestyles for each species species_list = data[\'species\'].unique() markers = [\'o\', \'s\', \'^\'] linestyles = [\'-\', \'--\', \':\'] species_props = {species: (marker, linestyle) for species, marker, linestyle in zip(species_list, markers, linestyles)} # Create the scatter plot fig, ax = plt.subplots() for species in species_list: species_data = data[data[\'species\'] == species] avg_petal_length = species_data[\'petal_length\'].mean() avg_petal_width = species_data[\'petal_width\'].mean() marker, linestyle = species_props[species] sns.scatterplot( x=\'petal_length\', y=\'petal_width\', data=species_data, ax=ax, label=species, marker=marker ) # Add average lines ax.plot(species_data[\'petal_length\'], [avg_petal_width] * len(species_data), linestyle, label=f\'{species} Avg Width\') ax.plot([avg_petal_length] * len(species_data), species_data[\'petal_width\'], linestyle, label=f\'{species} Avg Length\') # Add datum points with conditions petal_length_25th = np.percentile(data[\'petal_length\'], 25) petal_width_75th = np.percentile(data[\'petal_width\'], 75) for index, row in data.iterrows(): x, y = row[\'petal_length\'], row[\'petal_width\'] if x < petal_length_25th: ax.plot(x, y, \'ro\') # red color for petal length below the 25th percentile if y > petal_width_75th: ax.plot(x, y, \'bo\', alpha=0.6) # increased alpha for petal width above the 75th percentile # Custom legend handles, labels = ax.get_legend_handles_labels() unique_labels = dict(zip(labels, handles)) ax.legend(unique_labels.values(), unique_labels.keys()) plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"Petal Width\\") plt.title(\\"Customized Seaborn Plot\\") plt.show()"},{"question":"# Timeit Module Exercise: Measuring and Optimizing Code Performance Objective Write a function that measures and compares the execution time of two different implementations of a task using the `timeit` module. You will then report which implementation is faster and by how much. Task Description The task to be implemented is to sum all numbers from 1 to `n` in two different ways: 1. Using a loop. 2. Using the formula for the sum of the first `n` natural numbers. You need to measure and compare the execution time for both implementations using the `timeit` module and generate a report indicating which method is faster and by what factor. Function Signature ```python def compare_execution_times(n: int) -> str: Compare the execution times of summing numbers using a loop versus using a formula. Parameters: n (int): The upper limit of the range to sum. Returns: str: A report indicating which method is faster and by how much. ``` Example Usage ```python # Example usage: result = compare_execution_times(1000000) print(result) ``` Expected output would be something like: ``` Loop method: 0.1567 seconds Formula method: 0.00001 seconds Formula method is 15670 times faster than the Loop method. ``` Constraints - You must use the `timeit` module to measure the execution time. - The function should be implemented such that the reported times are reliable and meaningful. - The output should clearly state the time taken by each method and the factor by which the faster method outperforms the slower one. Additional Information - You can use the `timeit.timeit()` function for measuring the execution time of each implementation within your function. - Ensure you handle edge cases such as when `n` is zero or a negative number appropriately by raising an appropriate exception or returning a meaningful message. Provide the complete implementation of the function `compare_execution_times`.","solution":"import timeit def sum_using_loop(n): total = 0 for i in range(1, n + 1): total += i return total def sum_using_formula(n): return n * (n + 1) // 2 def compare_execution_times(n: int) -> str: if n < 0: return \\"n should be a non-negative integer\\" loop_time = timeit.timeit(lambda: sum_using_loop(n), number=1) formula_time = timeit.timeit(lambda: sum_using_formula(n), number=1) if formula_time == 0: return \\"Formula method is extremely fast.\\" faster_method = \'formula\' if formula_time < loop_time else \'loop\' speedup_factor = loop_time / formula_time if formula_time > 0 else float(\'inf\') report = ( f\\"Loop method: {loop_time:.6f} secondsn\\" f\\"Formula method: {formula_time:.6f} secondsn\\" f\\"{faster_method.capitalize()} method is {speedup_factor:.2f} times faster than the \\" + f\\"{\'loop\' if faster_method == \'formula\' else \'formula\'} method.\\" ) return report"},{"question":"Title: Advanced Warning Management using `warnings` Module Objective: To test the understanding and implementation of the `warnings` module in Python, particularly focusing on controlling and managing warning messages and applying context managers for testing and temporarily suppressing warnings. Problem Statement: Write a function `process_warnings(numbers: list) -> dict` that processes a list of numbers and generates warnings under specific conditions. Follow these conditions: 1. If the number is negative, raise a `UserWarning`. 2. If the number is greater than 100, raise a `RuntimeWarning`. 3. Use `catch_warnings` to capture all warnings and return a dictionary summarizing the count and types of warnings raised. 4. Ensure the warnings are not shown on the console but are still captured and counted. Provide a clear step-by-step implementation to achieve the goal. Input: - A list of integers (`numbers`) Output: - A dictionary summarizing the count and types of warnings raised: ```python { \'UserWarning\': <count_of_UserWarning>, \'RuntimeWarning\': <count_of_RuntimeWarning> } ``` Constraints: - Each number in the list is an integer. - The length of the list can be between 1 and 100. Example: ```python # Example usage numbers = [10, -2, 150, 75, -7] result = process_warnings(numbers) print(result) # Expected output: {\'UserWarning\': 2, \'RuntimeWarning\': 1} ``` Implementation Hints: - Use `warnings.warn` to generate warnings. - Utilize the `catch_warnings` context manager to capture and count the warnings. - Filter warnings using `warnings.simplefilter(\\"always\\")` to ensure all warnings are caught.","solution":"import warnings def process_warnings(numbers: list) -> dict: Processes a list of numbers and raises specific warnings based on their value. Args: numbers (list): List of integers to be processed. Returns: dict: A dictionary summarizing the count and types of warnings raised. warning_summary = { \'UserWarning\': 0, \'RuntimeWarning\': 0 } with warnings.catch_warnings(record=True) as caught_warnings: warnings.simplefilter(\\"always\\") for number in numbers: if number < 0: warnings.warn(\\"Negative number encountered\\", UserWarning) elif number > 100: warnings.warn(\\"Number greater than 100 encountered\\", RuntimeWarning) for warning in caught_warnings: if issubclass(warning.category, UserWarning): warning_summary[\'UserWarning\'] += 1 elif issubclass(warning.category, RuntimeWarning): warning_summary[\'RuntimeWarning\'] += 1 return warning_summary"},{"question":"Advanced DataFrame Operations and Memory Management Objective: You are required to demonstrate your understanding of advanced DataFrame operations in pandas, including memory usage inspection and handling DataFrame mutations with User Defined Functions (UDFs). Problem Statement: You are given a DataFrame `df` containing data of various types. Your task includes: 1. **Analyze Memory Usage:** - Write a function `get_memory_usage(df, deep=False)` that prints and returns the memory usage of the DataFrame in bytes. If `deep` is `True`, perform a deep memory usage analysis. 2. **Handle DataFrame Mutation with UDF:** - Write a function `apply_transformation(df, transform_func)` that applies the provided transformation function `transform_func` to the DataFrame using the `apply` method in a safe manner (ensuring the DataFrame original structure isn\'t mutated and no unexpected errors occur). 3. **Boolean Operations:** - Write a function `check_condition(series)` that accepts a pandas Series and returns `True` if any element in the series is `True`, otherwise returns `False`. Constraints: - Ensure that your implementation of `apply_transformation` does not mutate the original DataFrame being iterated over. - For memory usage, ensure the output is human-readable with appropriately formatted units when the function `get_memory_usage` is called with the default `deep=False`. Input: - A pandas DataFrame `df` with mixed datatype columns. - A transformation function `transform_func` which will be used to manipulate the DataFrame in the `apply_transformation` function. - A pandas Series `series` with boolean values for the `check_condition` function. Output: - `get_memory_usage(df, deep=False)`: prints and returns the memory usage of the DataFrame. - `apply_transformation(df, transform_func)`: returns the transformed DataFrame without mutating the original. - `check_condition(series)`: returns a boolean indicating if any `True` value exists in the series. Example Usage: ```python import pandas as pd import numpy as np # Sample data dtypes = [\\"int64\\", \\"float64\\", \\"object\\"] n = 1000 data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes} df = pd.DataFrame(data) df[\\"boolean\\"] = np.random.choice([False, True], size=n) # Sample transformation function def sample_transform_func(s): return s * 2 if s.name == \'int64\' else s # Implemented functions memory_usage = get_memory_usage(df) print(memory_usage) transformed_df = apply_transformation(df, sample_transform_func) print(transformed_df.head()) series = df[\'boolean\'] condition_met = check_condition(series) print(condition_met) ``` Notes: - Ensure usage of pandas best practices for handling DataFrames. - Be efficient in terms of performance, especially when handling large DataFrames. - Provide necessary exception handling wherever applicable.","solution":"import pandas as pd def get_memory_usage(df, deep=False): Prints and returns the memory usage of the DataFrame in bytes. If deep is True, perform a deep memory usage analysis. usage = df.memory_usage(deep=deep).sum() human_readable = format_bytes(usage) print(f\\"Memory usage: {human_readable}\\") return usage def format_bytes(size): # Helper function to convert bytes to a human readable format # 2**10 = 1024, and so on... for unit in [\'bytes\', \'KB\', \'MB\', \'GB\', \'TB\']: if size < 1024.0: return \\"%3.1f %s\\" % (size, unit) size /= 1024.0 def apply_transformation(df, transform_func): Applies the provided transformation function to the DataFrame using the apply method in a safe manner. Returns the transformed DataFrame without mutating the original. df_copy = df.copy() try: transformed_df = df_copy.apply(transform_func) except Exception as e: raise RuntimeError(\\"Transformation function raised an error: \\" + str(e)) return transformed_df def check_condition(series): Returns True if any element in the series is True, otherwise returns False. return series.any()"},{"question":"# Coding Challenge: Advanced Data Visualization with Seaborn **Objective:** You are required to create a comprehensive visualization using Seaborn to analyze the trends and relationships within a dataset. This challenge will test your understanding of loading data, plotting scatter plots, customizing visual parameters, and producing facet grids to present multiple views of the data. **Dataset:** For this exercise, we will use the \\"tips\\" dataset, inherently available within Seaborn. **Task:** 1. Load the \\"tips\\" dataset using Seaborn. 2. Create the following visualizations: 1. A scatter plot showing the relationship between `total_bill` and `tip`. 2. Enhance the scatter plot by differentiating the points based on the `day` column using the `hue` parameter. 3. Further customize the scatter plot by adding differentiation based on `time` using the `style` parameter. 4. Create a multi-facet scatter plot using `relplot` where each facet represents different `time`s, points are colored by `sex`, and styled by `smoker`. **Input Format:** - No input from the user is required as the dataset is fetched internally. **Output Format:** - Your solution should display the four required visualizations in sequence. **Constraints:** - Use Seaborn and Matplotlib packages for plotting. - Ensure that legends and labels are appropriately displayed for better clarity. **Implementation:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # 1. Scatter plot between `total_bill` and `tip` plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\'Scatter Plot of Total Bill vs Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # 2. Scatter plot with hue based on `day` plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") plt.title(\'Scatter Plot of Total Bill vs Tip with hue based on Day\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day\') plt.show() # 3. Scatter plot with hue based on `day` and style based on `time` plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\") plt.title(\'Scatter Plot of Total Bill vs Tip with hue based on Day and style based on Time\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day/Time\') plt.show() # 4. Facet plot with `relplot` using `time`, `sex`, and `smoker` sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"sex\\", style=\\"smoker\\", kind=\\"scatter\\", height=6, aspect=1 ) plt.show() ``` **Explanation:** - The first plot is a basic scatter plot. - The second plot adds differentiation by day using hue. - The third plot introduces an additional style parameter, `time`, for better differentiation. - The fourth visualization uses `relplot` to create a facet grid, splitting the data by `time` and using `sex` and `smoker` for hue and style respectively. Ensure all plots have appropriate titles, axis labels, and legends.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # 1. Scatter plot between `total_bill` and `tip` plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\'Scatter Plot of Total Bill vs Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # 2. Scatter plot with hue based on `day` plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") plt.title(\'Scatter Plot of Total Bill vs Tip with hue based on Day\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day\') plt.show() # 3. Scatter plot with hue based on `day` and style based on `time` plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\") plt.title(\'Scatter Plot of Total Bill vs Tip with hue based on Day and style based on Time\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day/Time\') plt.show() # 4. Facet plot with `relplot` using `time`, `sex`, and `smoker` sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"sex\\", style=\\"smoker\\", kind=\\"scatter\\", height=6, aspect=1 ) plt.show()"},{"question":"<|Analysis Begin|> The provided documentation goes into detail about the `queue` module, which implements multi-producer, multi-consumer queues in Python. It is particularly focused on ensuring safe information exchange between multiple threads, supporting thread-based synchronization. The module contains four different types of queues: - `Queue` for FIFO (first-in, first-out) - `LifoQueue` for LIFO (last-in, first-out) - `PriorityQueue` for sorted entries with priority - `SimpleQueue` for a basic unbounded FIFO queue It also provides necessary method implementations for enqueue and dequeue operations such as `put()`, `get()`, `empty()`, `full()`, `qsize()`, `task_done()`, and `join()`. Additionally, exceptions like `Empty` and `Full` are provided to handle edge cases when queues are either empty or full. Given the detailed information about thread-safe queue operations, the question should target the practical application of these queue classes and methods. This can evaluate students\' comprehension of threading, queuing mechanisms, priority management, and exception handling. <|Analysis End|> <|Question Begin|> # Question: Implement a Multi-Threaded Task Manager using Python\'s `queue` Module In this exercise, you will implement a multi-threaded task manager that uses Python\'s `queue` module to manage task execution among multiple worker threads. Your implementation should be able to handle FIFO, LIFO, and Priority-based task scheduling. You need to create the following queues: 1. A FIFO queue for standard tasks. 2. A LIFO queue for urgent tasks that need to be processed immediately. 3. A Priority queue for tasks that have a specific priority. Each task is represented as a tuple containing: - A string description of the task. - An integer priority (applicable only for the priority queue, else can be ignored). Tasks should be executed by multiple worker threads which print the task description when processing begins and when it is completed. Requirements: 1. Implement a class `TaskManager` that initializes three queues: `fifo_queue`, `lifo_queue`, and `priority_queue`. 2. Implement methods for adding tasks to each queue: - `add_fifo_task(task_description: str)` - `add_lifo_task(task_description: str)` - `add_priority_task(priority: int, task_description: str)` 3. Implement a method `start_workers(n: int)` that starts `n` worker threads for processing tasks from the queues. 4. Worker threads should prioritize tasks in the following order: 1. Tasks from the priority queue (lowest priority number first) 2. Tasks from the LIFO queue 3. Tasks from the FIFO queue 5. Implement proper exception handling for empty queues and infinite blocking scenarios. Input and Output: - **Input**: Methods for adding tasks and specifying the number of worker threads. - **Output**: Print statements indicating task processing start and completion. Constraints: - Ensure that adding tasks to queues or starting workers does not block if the queues are full or empty. - The solution should properly handle exceptions using `queue.Empty` and `queue.Full`. Example Usage: ```python from queue import Queue, LifoQueue, PriorityQueue import threading class TaskManager: def __init__(self): self.fifo_queue = Queue() self.lifo_queue = LifoQueue() self.priority_queue = PriorityQueue() def add_fifo_task(self, task_description): self.fifo_queue.put(task_description) def add_lifo_task(self, task_description): self.lifo_queue.put(task_description) def add_priority_task(self, priority, task_description): self.priority_queue.put((priority, task_description)) def start_workers(self, n): def worker(): while True: try: priority_task = self.priority_queue.get_nowait() print(f\\"Starting Priority Task: {priority_task[1]}\\") self.priority_queue.task_done() print(f\\"Completed Priority Task: {priority_task[1]}\\") except queue.Empty: try: lifo_task = self.lifo_queue.get_nowait() print(f\\"Starting LIFO Task: {lifo_task}\\") self.lifo_queue.task_done() print(f\\"Completed LIFO Task: {lifo_task}\\") except queue.Empty: try: fifo_task = self.fifo_queue.get_nowait() print(f\\"Starting FIFO Task: {fifo_task}\\") self.fifo_queue.task_done() print(f\\"Completed FIFO Task: {fifo_task}\\") except queue.Empty: break for _ in range(n): threading.Thread(target=worker, daemon=True).start() ``` Implement the `TaskManager` class following the above requirements and example usage to create a thread-safe multi-threaded task manager.","solution":"from queue import Queue, LifoQueue, PriorityQueue, Empty import threading class TaskManager: def __init__(self): self.fifo_queue = Queue() self.lifo_queue = LifoQueue() self.priority_queue = PriorityQueue() def add_fifo_task(self, task_description): Adds a task to the FIFO queue. self.fifo_queue.put(task_description) def add_lifo_task(self, task_description): Adds a task to the LIFO queue. self.lifo_queue.put(task_description) def add_priority_task(self, priority, task_description): Adds a task to the Priority queue with a given priority. self.priority_queue.put((priority, task_description)) def start_workers(self, n): Starts n worker threads that process tasks from the queues. def worker(): while True: task = None try: # Try to get a task from the priority queue task = self.priority_queue.get_nowait() print(f\\"Starting Priority Task: {task[1]}\\") self.priority_queue.task_done() print(f\\"Completed Priority Task: {task[1]}\\") except Empty: pass if task is None: try: # Try to get a task from the LIFO queue task = self.lifo_queue.get_nowait() print(f\\"Starting LIFO Task: {task}\\") self.lifo_queue.task_done() print(f\\"Completed LIFO Task: {task}\\") except Empty: pass if task is None: try: # Try to get a task from the FIFO queue task = self.fifo_queue.get_nowait() print(f\\"Starting FIFO Task: {task}\\") self.fifo_queue.task_done() print(f\\"Completed FIFO Task: {task}\\") except Empty: break for _ in range(n): threading.Thread(target=worker, daemon=True).start()"},{"question":"# Question: Implementing and Verifying HMAC in a Secure Communication Channel Context: In secure communication systems, it is essential to ensure the integrity and authenticity of messages. The HMAC (Keyed-Hash Message Authentication Code) algorithm provides a mechanism to check that a message has not been altered in transit and confirm the sender\'s identity. Your task is to implement a communication system where messages are sent with an HMAC, and the receiver verifies the message using the provided HMAC. Problem Statement: You are given a function `send_message` which takes a message and a secret key, and returns the message along with its HMAC. You need to implement the function `verify_message` which takes the received message, its HMAC, and the secret key, and verifies the authenticity of the message. Function Specifications: 1. **send_message(message: str, key: bytes) -> Tuple[str, str]:** - **Input:** - `message`: The message to be sent (string). - `key`: The secret key for generating HMAC (bytes). - **Output:** A tuple containing the original message and its HMAC (both as strings). 2. **verify_message(received_message: str, received_hmac: str, key: bytes) -> bool:** - **Input:** - `received_message`: The message received (string). - `received_hmac`: The HMAC of the message received (string). - `key`: The secret key for verifying HMAC (bytes). - **Output:** A boolean value `True` if the message is authentic and unaltered, `False` otherwise. Constraints: - The `digestmod` parameter for both HMAC operations should be `sha256`. - Ensure to use `hmac.compare_digest` for comparing the HMAC values to prevent timing attacks. - The secret key and message will only consist of ASCII characters. Example: ```python # Assume the functions have been implemented as specified key = b\'secret_key\' message = \\"Important message\\" sent_message, sent_hmac = send_message(message, key) # Tampering with the message tampered_message = \\"Tampered message\\" # Authentic message verification is_authentic = verify_message(sent_message, sent_hmac, key) print(is_authentic) # Output: True # Tampered message verification is_authentic = verify_message(tampered_message, sent_hmac, key) print(is_authentic) # Output: False ``` Implementation: ```python import hmac import hashlib def send_message(message: str, key: bytes) -> tuple: hmac_obj = hmac.new(key, message.encode(), hashlib.sha256) message_hmac = hmac_obj.hexdigest() return message, message_hmac def verify_message(received_message: str, received_hmac: str, key: bytes) -> bool: hmac_obj = hmac.new(key, received_message.encode(), hashlib.sha256) expected_hmac = hmac_obj.hexdigest() return hmac.compare_digest(received_hmac, expected_hmac) ``` **Note:** Ensure to test the functions with multiple scenarios including tampered and non-tampered messages to validate your implementation.","solution":"import hmac import hashlib def send_message(message: str, key: bytes) -> tuple: hmac_obj = hmac.new(key, message.encode(), hashlib.sha256) message_hmac = hmac_obj.hexdigest() return message, message_hmac def verify_message(received_message: str, received_hmac: str, key: bytes) -> bool: hmac_obj = hmac.new(key, received_message.encode(), hashlib.sha256) expected_hmac = hmac_obj.hexdigest() return hmac.compare_digest(received_hmac, expected_hmac)"},{"question":"Objective Demonstrate your understanding of the seaborn object-oriented interface for creating plots and applying jitter transformations to data points. Problem Statement You are provided with the `penguins` dataset, which you must use to create a scatter plot displaying the body mass and flipper length of different penguin species. Your task is to generate two plots that exhibit smooth visualization techniques using jitter. One plot should apply jitter on the x-axis, and the other should apply jitter on both the x and y axes. Requirements 1. Load the `penguins` dataset using `seaborn.load_dataset`. 2. Create a scatter plot of `body_mass_g` (x-axis) against `flipper_length_mm` (y-axis), coloring points by `species`. 3. For the first plot, apply jitter on the x-axis using a relative width of 0.3. 4. For the second plot, apply jitter on both the x-axis with a relative width of 0.3 and the y-axis with an absolute value of 2 units. Input - The `penguins` dataset from seaborn. Expected Output - Two scatter plots saved as \'plot1.png\' and \'plot2.png\'. Implementation Implement the function `create_plots` which creates the required plots and saves them accordingly. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_plots(): # Load the dataset penguins = load_dataset(\\"penguins\\") # First plot with jitter on x-axis ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\") .add(so.Dots(), so.Jitter(width=0.3)) .save(\\"plot1.png\\") ) # Second plot with jitter on both x-axis and y-axis ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\") .add(so.Dots(), so.Jitter(x=0.3, y=2)) .save(\\"plot2.png\\") ) plt.close(\'all\') # Sample function call create_plots() ``` Constraints - Ensure you have seaborn version supporting object-oriented interface (`seaborn.objects`). Evaluation - Correct loading and manipulation of datasets. - Appropriate implementation of jitter transformations. - Proper saving of the plots.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_plots(): # Load the dataset penguins = load_dataset(\\"penguins\\") # First plot with jitter on x-axis ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\") .add(so.Dots(), so.Jitter(width=0.3)) .save(\\"plot1.png\\") ) # Second plot with jitter on both x-axis and y-axis ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", color=\\"species\\") .add(so.Dots(), so.Jitter(x=0.3, y=2)) .save(\\"plot2.png\\") ) plt.close(\'all\') # Sample function call create_plots()"},{"question":"**Problem Statement:** You are required to implement a Python function that connects to an IMAP4 server, retrieves unread emails from the inbox, and prints details such as the sender\'s email address, subject, and the date of each unread email. To accomplish this, you will use the `imaplib` module. **Function Signature:** ```python def fetch_unread_emails(server: str, port: int, user: str, password: str): # Write your code here ``` **Input:** - `server` (str): The IMAP server address (e.g., \\"imap.gmail.com\\"). - `port` (int): The port to connect to (e.g., 993 for IMAP over SSL). - `user` (str): The email address to log in with. - `password` (str): The password for the given email address. **Output:** - The function should print the sender\'s email address, subject, and date of each unread email in the inbox. **Constraints:** - You must use the `imaplib` module. - The function should handle exceptions gracefully and print relevant error messages. **Example Usage:** ```python fetch_unread_emails(\\"imap.gmail.com\\", 993, \\"example@gmail.com\\", \\"password123\\") ``` **Notes:** 1. You will need to handle connection over SSL since most modern IMAP servers require it (use `IMAP4_SSL`). 2. You should search for emails marked as unseen or unread. This can be done using the `(UNSEEN)` criterion. 3. Headers such as From, Subject, and Date need to be extracted from the email structure. 4. Ensure your function handles network issues and authentication errors appropriately. **Hints:** - You may need to use the `fetch` method and parse parts of the message using the `email` library to decode headers. - Make sure to close the connection properly using `logout` to prevent resource leaks.","solution":"import imaplib import email from email.parser import BytesParser from email.policy import default def fetch_unread_emails(server: str, port: int, user: str, password: str): try: with imaplib.IMAP4_SSL(server, port) as mail: mail.login(user, password) mail.select(\'inbox\') # search for unread emails status, messages = mail.search(None, \'(UNSEEN)\') if status != \'OK\': print(\\"No unread emails found.\\") return unread_msg_nums = messages[0].split() for num in unread_msg_nums: status, data = mail.fetch(num, \'(RFC822)\') if status != \'OK\': print(f\\"Failed to fetch email for message number {num.decode()}\\") continue raw_email = data[0][1] msg = BytesParser(policy=default).parsebytes(raw_email) # decoding the headers from_address = msg[\'From\'] subject = msg[\'Subject\'] date = msg[\'Date\'] print(f\\"From: {from_address}\\") print(f\\"Subject: {subject}\\") print(f\\"Date: {date}\\") print(\'-\'*50) except imaplib.IMAP4.error as e: print(f\\"IMAP4 error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Coding Exercise: Implement a Custom Email Policy Given the detailed behavior controls provided by the \\"email\\" package policy objects in Python, your task is to implement a custom email policy and utilize it in parsing, manipulating, and regenerating email messages. # Task 1. **Implement a Custom Email Policy Class:** Define a new custom policy class `CustomPolicy` that extends `EmailPolicy` with the following specific behaviors: - `max_line_length` to be 50 characters. - `linesep` to be \\"rn\\". - `cte_type` to \\"8bit\\". - `raise_on_defect` to False. - A custom defect handler which logs defects rather than registering them or raising exceptions. 2. **Parse and Manipulate Emails:** Write functions to parse an email message, manipulate its headers and content, and regenerate the message using the custom policy. Specifically, your solution should: - Parse an email from a text string. - Add a custom header \\"X-Custom-Header\\" with a provided value. - Modify the subject line to append \\" - Modified\\". - Serialize the modified email back to a string format using the custom policy. # Input - `email_string`: A string containing the raw email message. - `custom_header_name`: The name of the custom header to be added, e.g., \\"X-Custom-Header\\". - `custom_header_value`: The value of the custom header to be added, e.g., \\"CustomValue\\". # Output - A string containing the serialized and modified email message. # Your Function Signature ```python from email.policy import EmailPolicy class CustomPolicy(EmailPolicy): # Implement the custom policy by inheriting EmailPolicy def parse_and_modify_email( email_string: str, custom_header_name: str, custom_header_value: str ) -> str: # Implement the parsing, modifying and regenerating logic here ``` # Constraints - The provided `email_string` will contain properly formatted email data. - You cannot change standard libraries\' behavior except by using policies. - Ensure your code adheres to modern email RFCs using your custom policy settings. # Example ```python email_string = From: sender@example.com To: recipient@example.com Subject: Test Email This is a test email body. custom_header_name = \\"X-Custom-Header\\" custom_header_value = \\"CustomValue\\" result = parse_and_modify_email(email_string, custom_header_name, custom_header_value) print(result) ``` # Notes - Ensure the `Subject` header is modified properly to include \\" - Modified\\". - The custom header should appear in the headers with the specified name and value. - The output should follow the custom policy settings. Good luck!","solution":"import email from email.policy import EmailPolicy from email.message import EmailMessage class CustomPolicy(EmailPolicy): def __init__(self, **kwargs): super().__init__( max_line_length=50, linesep=\\"rn\\", cte_type=\\"8bit\\", raise_on_defect=False, **kwargs ) def parse_and_modify_email(email_string: str, custom_header_name: str, custom_header_value: str) -> str: # Parse the email using the custom policy custom_policy = CustomPolicy() message = email.message_from_string(email_string, policy=custom_policy) # Add custom header message[custom_header_name] = custom_header_value # Modify subject line if \'Subject\' in message: message.replace_header(\'Subject\', message[\'Subject\'] + \\" - Modified\\") else: message[\'Subject\'] = \\" - Modified\\" # Serialize the email back to a string format using the custom policy return str(message)"},{"question":"**Objective:** You are required to write a Python script that reads a Sun AU audio file, modifies its data, and saves the modified data into a new Sun AU audio file. This exercise will test your understanding of file handling, audio processing, and the use of the `sunau` module. **Task Description:** 1. **Read and Analyze an AU File:** - Write a function `read_au_file(file_path)` that opens a Sun AU audio file, reads its data and metadata, and returns them. - Input: `file_path` (str) - the path to the Sun AU file to be read. - Output: A tuple containing the following: - `nchannels` (int): Number of audio channels. - `sampwidth` (int): Sample width in bytes. - `framerate` (int): Sampling frequency. - `nframes` (int): Number of audio frames. - `audio_data` (bytes): Audio data read from the file. 2. **Modify Audio Data:** - Write a function `modify_audio_data(audio_data, factor)` that modifies the audio data. For simplicity, multiply each byte in the audio data by a factor. - Input: - `audio_data` (bytes): Original audio data. - `factor` (int): A multiplication factor for modifying the audio data. - Output: `modified_audio_data` (bytes) - The modified audio data. 3. **Write to a New AU File:** - Write a function `write_au_file(file_path, nchannels, sampwidth, framerate, nframes, audio_data)` that writes the modified audio data into a new Sun AU file. - Input: - `file_path` (str) - The path to the output Sun AU file. - `nchannels`, `sampwidth`, `framerate`, `nframes` (int) - Metadata about the audio. - `audio_data` (bytes) - Modified audio data to be written. 4. **Main Functionality:** - Implement a main script that ties everything together. - Read an input AU file, modify the audio data using a predefined factor, and write it to a new output file. - Assume `input.au` is the input file and the output file is `output.au`, with a multiplication factor of 2. **Instructions:** 1. Use the `sunau` module for all file operations. 2. Handle potential exceptions gracefully. 3. Adhere to the input and output specifications for the functions. **Example Usage:** ```python def read_au_file(file_path): # Implementation here def modify_audio_data(audio_data, factor): # Implementation here def write_au_file(file_path, nchannels, sampwidth, framerate, nframes, audio_data): # Implementation here if __name__ == \\"__main__\\": input_path = \\"input.au\\" output_path = \\"output.au\\" factor = 2 nchannels, sampwidth, framerate, nframes, audio_data = read_au_file(input_path) modified_audio_data = modify_audio_data(audio_data, factor) write_au_file(output_path, nchannels, sampwidth, framerate, nframes, modified_audio_data) ``` **Constraints:** 1. Ensure the new AU file generated is valid and plays correctly. 2. Consider edge cases such as very small or very large audio files. Good luck!","solution":"import sunau def read_au_file(file_path): Reads a Sun AU audio file and returns its data and metadata. Parameters: file_path (str): The path to the Sun AU file to be read. Returns: tuple: A tuple containing: - nchannels (int): Number of audio channels. - sampwidth (int): Sample width in bytes. - framerate (int): Sampling frequency. - nframes (int): Number of audio frames. - audio_data (bytes): Audio data read from the file. with sunau.open(file_path, \'rb\') as au_file: nchannels = au_file.getnchannels() sampwidth = au_file.getsampwidth() framerate = au_file.getframerate() nframes = au_file.getnframes() audio_data = au_file.readframes(nframes) return nchannels, sampwidth, framerate, nframes, audio_data def modify_audio_data(audio_data, factor): Modifies the audio data by multiplying each byte by the factor. Parameters: audio_data (bytes): Original audio data. factor (int): A multiplication factor for modifying the audio data. Returns: bytes: The modified audio data. modified_audio_data = bytes(byte * factor for byte in audio_data) return modified_audio_data def write_au_file(file_path, nchannels, sampwidth, framerate, nframes, audio_data): Writes the modified audio data into a new Sun AU file. Parameters: file_path (str): The path to the output Sun AU file. nchannels (int): Number of audio channels. sampwidth (int): Sample width in bytes. framerate (int): Sampling frequency. nframes (int): Number of audio frames. audio_data (bytes): Modified audio data to be written. with sunau.open(file_path, \'wb\') as au_file: au_file.setnchannels(nchannels) au_file.setsampwidth(sampwidth) au_file.setframerate(framerate) au_file.setnframes(nframes) au_file.writeframes(audio_data) if __name__ == \\"__main__\\": input_path = \\"input.au\\" output_path = \\"output.au\\" factor = 2 nchannels, sampwidth, framerate, nframes, audio_data = read_au_file(input_path) modified_audio_data = modify_audio_data(audio_data, factor) write_au_file(output_path, nchannels, sampwidth, framerate, nframes, modified_audio_data)"},{"question":"**Coding Assessment Question** The `2to3` tool is essential for upgrading Python 2.x codebases to Python 3.x by applying a series of predefined fixers for syntax and library changes. Now that you are familiar with how `2to3` works and the types of fixers it includes, your task is to create a custom fixer. # Objective: Your task is to write a custom fixer for `2to3` that converts Python 2 style `raise` statements to Python 3 compatible `raise` statements. Specifically, you need to handle the following cases: 1. Convert `raise E, V` to `raise E(V)`. 2. Convert `raise E, V, T` to `raise E(V).with_traceback(T)`. # Input and Output: - **Input:** A Python 2.x source file containing `raise` statements. - **Output:** A transformed Python 3.x source file with updated `raise` statements. # Constraints: - Your solution should preserve comments and indentation in the original source file. - You must handle edge cases where `E` is a tuple. # Implementation Details: 1. You should define a class `CustomFixer` that inherits from the generic fixer template. 2. Implement a method to identify and transform the `raise` statements. 3. Integrate this custom fixer with the `2to3` tool and demonstrate its effect on a sample Python 2.x source file. # Sample Input: ```python # example.py (Python 2.x code) try: x = 1 / 0 except ZeroDivisionError, e: raise RuntimeError, \'division error\' ``` # Sample Output: ```python # example.py (Translated to Python 3.x code) try: x = 1 / 0 except ZeroDivisionError as e: raise RuntimeError(\'division error\') ``` # Starter Template: ```python from lib2to3 import fixer_base from lib2to3.fixer_util import Name, Call, Comma, ArgList from lib2to3.pgen2 import token from lib2to3.pytree import Leaf, Node class CustomFixer(fixer_base.BaseFix): PATTERN = \\"raise_stmt< \'raise\' exc=any \',\' val=any >\\" def transform(self, node, results): exc = results[\\"exc\\"] val = results[\\"val\\"] new_raise = Node(syms.raise_stmt, [ Leaf(token.NAME, \\"raise\\"), Node(syms.power, [ exc.clone(), Node(syms.trailer, [ Leaf(token.LPAR, \\"(\\"), val.clone(), Leaf(token.RPAR, \\")\\") ]) ]) ]) return new_raise # Additional code to integrate and apply the custom fixer within the 2to3 framework. ``` # Instructions: 1. Complete the `transform` method to handle both simple and complex `raise` statements. 2. Test the fixer on various Python 2.x files to ensure accuracy. 3. Provide examples of the input and output of your fixer. Good luck!","solution":"from lib2to3 import fixer_base from lib2to3.fixer_util import Comma, Name, Call, ArgList, Attr, Node, Leaf from lib2to3.pgen2 import token from lib2to3.pygram import python_symbols as syms class CustomFixer(fixer_base.BaseFix): PATTERN = raise_stmt< \'raise\' exc=any \',\' val=any > | raise_stmt< \'raise\' exc=any \',\' val=any \',\' tr=any > def transform(self, node, results): exc = results[\\"exc\\"].clone() val = results[\\"val\\"].clone() if \\"tr\\" in results: tr = results[\\"tr\\"].clone() # This is the case: raise E, V, T => raise E(V).with_traceback(T) new_node = Node(syms.raise_stmt, [ Leaf(token.NAME, \\"raise\\"), Node(syms.power, [ exc, Node(syms.trailer, [ Leaf(token.LPAR, \\"(\\"), val, Leaf(token.RPAR, \\")\\") ]), Node(syms.trailer, [ Leaf(token.DOT, \\".\\"), Leaf(token.NAME, \\"with_traceback\\"), Node(syms.trailer, [ Leaf(token.LPAR, \\"(\\"), tr, Leaf(token.RPAR, \\")\\") ]) ]) ]) ]) else: # This is the case: raise E, V => raise E(V) new_node = Node(syms.raise_stmt, [ Leaf(token.NAME, \\"raise\\"), Node(syms.power, [ exc, Node(syms.trailer, [ Leaf(token.LPAR, \\"(\\"), val, Leaf(token.RPAR, \\")\\") ]) ]) ]) new_node.prefix = node.prefix return new_node"},{"question":"# Understanding and Implementing BernoulliRBM in Scikit-Learn In this task, you will work with the Bernoulli Restricted Boltzmann Machine (BernoulliRBM) model provided by scikit-learn. You are required to implement a function to train an RBM on a given dataset and then evaluate its performance using a linear classifier. Function Signature ```python def train_and_evaluate_rbm(data_X, data_y, n_components=256, learning_rate=0.01, batch_size=10, n_iter=10): Train a BernoulliRBM model on the given dataset and evaluate its performance using a linear SVM classifier. Parameters: data_X (np.ndarray): The input data (features), a 2D array where each row represents a sample. data_y (np.ndarray): The target labels, a 1D array where each element represents the label for a corresponding sample. n_components (int): Number of hidden units in the RBM. Default is 256. learning_rate (float): Learning rate for training the RBM. Default is 0.01. batch_size (int): Size of mini-batches for training. Default is 10. n_iter (int): Number of iterations (epochs) for training. Default is 10. Returns: float: The accuracy of the linear SVM classifier on the test set. # Example usage: # from sklearn.datasets import load_digits # data = load_digits() # X, y = data.data, data.target # accuracy = train_and_evaluate_rbm(X, y) # print(f\\"Test accuracy: {accuracy:.4f}\\") ``` # Instructions: 1. **Load and Split Data**: Split the dataset into training and test sets. You can use any suitable dataset, such as digits from `sklearn.datasets`. 2. **Initialize and Train RBM**: - Create an instance of the `BernoulliRBM` class with specified `n_components`, `learning_rate`, `batch_size`, and `n_iter`. - Fit the RBM to the training data. 3. **Transform Data**: - Transform the training and test data using the trained RBM to obtain the hidden layer representations. 4. **Train Linear Classifier**: - Train a linear classifier (e.g., LinearSVM) on the transformed training data. - Evaluate the classifier on the transformed test data. 5. **Evaluate Performance**: - Calculate and return the accuracy of the classifier on the test set. Constraints: - Ensure robust handling of input data, including data validation. - Implement efficient data processing to handle large datasets. - The solution should have comments explaining key steps and logic. Example: - Use the `load_digits` dataset from `sklearn.datasets`. - Split the dataset into a training set (80%) and a test set (20%). - Train the RBM and LinearSVM as specified. - Return the accuracy of the model on the test set. Note: The example provided is only for demonstration; you may use any suitable dataset for your experiments.","solution":"from sklearn.neural_network import BernoulliRBM from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.svm import LinearSVC from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import accuracy_score def train_and_evaluate_rbm(data_X, data_y, n_components=256, learning_rate=0.01, batch_size=10, n_iter=10): Train a BernoulliRBM model on the given dataset and evaluate its performance using a linear SVM classifier. Parameters: data_X (np.ndarray): The input data (features), a 2D array where each row represents a sample. data_y (np.ndarray): The target labels, a 1D array where each element represents the label for a corresponding sample. n_components (int): Number of hidden units in the RBM. Default is 256. learning_rate (float): Learning rate for training the RBM. Default is 0.01. batch_size (int): Size of mini-batches for training. Default is 10. n_iter (int): Number of iterations (epochs) for training. Default is 10. Returns: float: The accuracy of the linear SVM classifier on the test set. # Split the data into train and test sets X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.2, random_state=42) # Define a scaler to scale the data to [0, 1] range scaler = MinMaxScaler() # Define the BernoulliRBM model rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, batch_size=batch_size, n_iter=n_iter, random_state=42) # Define the classifier classifier = LinearSVC(max_iter=10000, random_state=42) # Create a pipeline with scaler, rbm, and classifier model = Pipeline(steps=[(\'scaler\', scaler), (\'rbm\', rbm), (\'classifier\', classifier)]) # Fit the model to the training data model.fit(X_train, y_train) # Predict the labels for the test data y_pred = model.predict(X_test) # Compute the accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy # Example usage: # from sklearn.datasets import load_digits # data = load_digits() # X, y = data.data, data.target # accuracy = train_and_evaluate_rbm(X, y) # print(f\\"Test accuracy: {accuracy:.4f}\\")"},{"question":"# Question You are working for a network management company, and your task is to create a set of utility functions to handle various IP address manipulations and checks. Using the `ipaddress` module, implement the following functions: 1. **create_ip_object(address)**: - **Input**: A string representing an IP address (IPv4 or IPv6). - **Output**: An `ipaddress.IPv4Address` or `ipaddress.IPv6Address` object based on the input. - **Constraints**: Raise a `ValueError` if the input string is not a valid IP address. 2. **network_summary(network)**: - **Input**: A string representing an IP network in the format \'address/prefix\'. - **Output**: A dictionary containing: - `total_addresses`: Total number of addresses in the network. - `netmask`: Netmask of the network. - `hostmask`: Hostmask of the network. - **Constraints**: Raise a `ValueError` if the input string is not a valid network. 3. **check_address_in_network(ip_address, network)**: - **Input**: Two strings, one representing an IP address and one representing an IP network. - **Output**: A boolean indicating whether the IP address is within the given network. - **Constraints**: Both inputs should be valid IP addresses/networks. If either is invalid, raise a `ValueError`. 4. **convert_ip_to_int(ip_address)**: - **Input**: A string representing an IP address. - **Output**: An integer representation of the IP address. - **Constraints**: Raise a `ValueError` if the input string is not a valid IP address. 5. **generate_all_usable_addresses(network)**: - **Input**: A string representing an IP network. - **Output**: A list of strings, each representing a usable IP address within the network. - **Constraints**: Raise a `ValueError` if the input string is not a valid network. # Example Usage ```python # Example inputs and their respective outputs # 1. create_ip_object print(create_ip_object(\'192.0.2.1\')) # IPv4Address(\'192.0.2.1\') print(create_ip_object(\'2001:db8::1\')) # IPv6Address(\'2001:db8::1\') # 2. network_summary summary = network_summary(\'192.0.2.0/24\') print(summary) # Output: # { # \'total_addresses\': 256, # \'netmask\': IPv4Address(\'255.255.255.0\'), # \'hostmask\': IPv4Address(\'0.0.0.255\') # } # 3. check_address_in_network print(check_address_in_network(\'192.0.2.1\', \'192.0.2.0/24\')) # True print(check_address_in_network(\'192.0.3.1\', \'192.0.2.0/24\')) # False # 4. convert_ip_to_int print(convert_ip_to_int(\'192.0.2.1\')) # 3221225985 print(convert_ip_to_int(\'2001:db8::1\')) # 42540766411282592856903984951653826561 # 5. generate_all_usable_addresses usable_addresses = generate_all_usable_addresses(\'192.0.2.0/30\') print(usable_addresses) # Output: [\'192.0.2.1\', \'192.0.2.2\'] ``` **Notes**: - You must handle exceptions and invalid inputs appropriately as described in the function constraints. - Ensure that each function is optimized for performance where applicable.","solution":"import ipaddress def create_ip_object(address): Creates an appropriate IP address object based on the given address string. try: return ipaddress.ip_address(address) except ValueError: raise ValueError(f\\"Invalid IP address: {address}\\") def network_summary(network): Given a network in \'address/prefix\' format, returns a summary with total addresses, netmask, and hostmask. try: net = ipaddress.ip_network(network, strict=False) return { \'total_addresses\': net.num_addresses, \'netmask\': net.netmask, \'hostmask\': net.hostmask } except ValueError: raise ValueError(f\\"Invalid network: {network}\\") def check_address_in_network(ip_address, network): Checks if a given IP address belongs to the specified network. try: ip = ipaddress.ip_address(ip_address) net = ipaddress.ip_network(network, strict=False) return ip in net except ValueError: raise ValueError(f\\"Invalid IP address or network: {ip_address}, {network}\\") def convert_ip_to_int(ip_address): Converts an IP address to its integer representation. try: ip = ipaddress.ip_address(ip_address) return int(ip) except ValueError: raise ValueError(f\\"Invalid IP address: {ip_address}\\") def generate_all_usable_addresses(network): Returns a list of all usable IP addresses within the given network. try: net = ipaddress.ip_network(network, strict=False) return [str(ip) for ip in net.hosts()] except ValueError: raise ValueError(f\\"Invalid network: {network}\\")"},{"question":"You are tasked with processing a large list of numerical data. Each element of the list needs to be processed by a computational heavy function. To efficiently handle this workload, you decide to implement a function using the `concurrent.futures` module to parallelize the computation. Your task is to complete the implementation of the function `parallel_processing`. **Function Signature** ```python def parallel_processing(data: list) -> list: pass ``` # Requirements 1. **Input**: - `data`: A list of integers. Each integer represents a separate unit of work. 2. **Output**: - The function should return a list of results, where each element corresponds to the result of processing the respective element in the input list. 3. **Constraints**: - You should use the `ProcessPoolExecutor` class from the `concurrent.futures` module. - The function to process each element is provided below as `heavy_computation`. - Ensure proper handling of exceptions that might occur during the execution. - Aim for efficient handling, i.e., the function should ideally use all available processors on the machine for parallel execution. # Performance Requirements - The function should efficiently make use of available CPU cores and minimize the processing time for large datasets. - Consider both the time complexity and the overhead introduced by managing concurrent tasks. # Sample Function The function to be used for processing each element: ```python def heavy_computation(n: int) -> int: # Simulate a heavy computational task import time time.sleep(0.1) return n * n ``` # Example ```python data = [1, 2, 3, 4, 5] # Example call result = parallel_processing(data) print(result) ``` # Expected Output ``` [1, 4, 9, 16, 25] ``` # Notes - Handle all necessary imports within your function. - Ensure your implementation can handle cases where `data` might be an empty list or contains a very large number of elements. - Manage the executor lifecycle properly to avoid any resource leaks.","solution":"import concurrent.futures import time def heavy_computation(n): time.sleep(0.1) return n * n def parallel_processing(data): Processes a list of integers using heavy_computation function in parallel. Parameters: data (list): A list of integers to be processed. Returns: list: A list of results after processing each integer in the input list. results = [] with concurrent.futures.ProcessPoolExecutor() as executor: future_to_num = {executor.submit(heavy_computation, num): num for num in data} for future in concurrent.futures.as_completed(future_to_num): num = future_to_num[future] try: result = future.result() results.append(result) except Exception as exc: print(f\\"Generated an exception: {exc}\\") return results"},{"question":"# Question: Custom Transformer and Chi-squared Kernel Implementation You are tasked with implementing a custom scikit-learn transformer and using it together with the `chi2_kernel` function to classify data. **Objective**: 1. Implement a custom transformer `HistogramNormalizer` that normalizes histograms (represented as feature vectors) to have an L1 norm of one. 2. Use this transformer to preprocess data before computing the chi-squared kernel. 3. Train an SVM classifier using this kernel and evaluate its performance. Steps to Follow: 1. **Custom Transformer `HistogramNormalizer`**: - Implement a `HistogramNormalizer` class that inherits from `BaseEstimator` and `TransformerMixin`. - Implement the `fit` method (trivial in this case). - Implement the `transform` method to normalize each feature vector such that the sum of its elements equals one. 2. **Chi-squared Kernel and SVM Classifier**: - Use the `HistogramNormalizer` to preprocess the input data. - Compute the chi-squared kernel between the normalized data vectors. - Train an SVM classifier using this kernel. - Evaluate the classifier on a test set. **Expected Input and Output**: - **Input**: - Two datasets `X_train` and `X_test` (histograms in the form of 2D numpy arrays). - Corresponding labels `y_train` and `y_test` (1D numpy arrays). - **Output**: - Accuracy score of the trained SVM classifier on the test set. **Implementation Constraints and Limitations**: - The `HistogramNormalizer` must inherit from `BaseEstimator` and `TransformerMixin`. - The chi-squared kernel computation should use the `chi2_kernel` function from scikit-learn. - The SVM classifier should use the precomputed kernel. You must use the following template for your code: ```python import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.metrics.pairwise import chi2_kernel from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.preprocessing import normalize from sklearn.metrics import accuracy_score class HistogramNormalizer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): return self def transform(self, X): return normalize(X, norm=\'l1\') def main(X_train, X_test, y_train, y_test): # Create pipeline with HistogramNormalizer and SVC using chi2_kernel preprocessor = HistogramNormalizer() # Normalize data X_train_normalized = preprocessor.fit_transform(X_train) X_test_normalized = preprocessor.transform(X_test) # Compute chi-squared kernel K_train = chi2_kernel(X_train_normalized) K_test = chi2_kernel(X_test_normalized, X_train_normalized) # Train SVM with the precomputed kernel svm = SVC(kernel=\'precomputed\') svm.fit(K_train, y_train) # Predict on test set y_pred = svm.predict(K_test) # Return accuracy score return accuracy_score(y_test, y_pred) # Example usage: # X_train = np.array([[0, 1], [1, 0], [.2, .8], [.7, .3]]) # X_test = np.array([[0.5, 0.5], [0.3, 0.7]]) # y_train = np.array([0, 1, 0, 1]) # y_test = np.array([0, 0]) # print(main(X_train, X_test, y_train, y_test)) ``` **Note**: You need to provide an example input and call the `main` function to demonstrate your implementation.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.metrics.pairwise import chi2_kernel from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.preprocessing import normalize from sklearn.metrics import accuracy_score class HistogramNormalizer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): return self def transform(self, X): return normalize(X, norm=\'l1\') def main(X_train, X_test, y_train, y_test): # Create pipeline with HistogramNormalizer and SVC using chi2_kernel preprocessor = HistogramNormalizer() # Normalize data X_train_normalized = preprocessor.fit_transform(X_train) X_test_normalized = preprocessor.transform(X_test) # Compute chi-squared kernel K_train = chi2_kernel(X_train_normalized) K_test = chi2_kernel(X_test_normalized, X_train_normalized) # Train SVM with the precomputed kernel svm = SVC(kernel=\'precomputed\') svm.fit(K_train, y_train) # Predict on test set y_pred = svm.predict(K_test) # Return accuracy score return accuracy_score(y_test, y_pred) # Example usage: # X_train = np.array([[0, 1], [1, 0], [.2, .8], [.7, .3]]) # X_test = np.array([[0.5, 0.5], [0.3, 0.7]]) # y_train = np.array([0, 1, 0, 1]) # y_test = np.array([0, 0]) # print(main(X_train, X_test, y_train, y_test))"},{"question":"**Problem: Configuring Pandas Options** Pandas offers various ways to configure its global behavior through options and settings. This exercise aims to test your ability to manipulate these options. Write a function `configure_pandas_options` that performs the following tasks: 1. Set the display precision for floating-point numbers to 3. 2. Configure pandas to use the \'engineering\' format for floating-point numbers. 3. Describe the current option for `display.max_rows`. 4. Temporarily set an option (`\'mode.chained_assignment\'`) to `\'warn\'` and check its value within the context. The function should not take any inputs and should return a dictionary with the following keys: - `\'precision_set\'`: Boolean indicating whether the precision was successfully set. - `\'eng_float_format_set\'`: Boolean indicating whether the engineering float format was successfully configured. - `\'max_rows_description\'`: Description of the current `display.max_rows` setting. - `\'temporary_chained_assignment\'`: Value of the `\'mode.chained_assignment\'` option within the temporary context. # Example ```python result = configure_pandas_options() print(result) ``` Output: ``` { \'precision_set\': True, \'eng_float_format_set\': True, \'max_rows_description\': \'int\', \'temporary_chained_assignment\': \'warn\' } ``` # Constraints - Make sure to handle any exceptions that may occur and properly indicate in the output dictionary if an action was unsuccessful. # Performance Requirements - The function should efficiently manipulate the options without unnecessary operations. **Note**: You are free to use any pandas functions or context managers as needed.","solution":"import pandas as pd def configure_pandas_options(): result = {} try: # Setting the display precision for floating-point numbers to 3 pd.set_option(\'display.precision\', 3) result[\'precision_set\'] = (pd.get_option(\'display.precision\') == 3) except Exception as e: result[\'precision_set\'] = False try: # Configuring pandas to use the \'engineering\' format for floating-point numbers pd.set_option(\'display.float_format\', lambda x: f\\"{x:,.3e}\\") result[\'eng_float_format_set\'] = (pd.get_option(\'display.float_format\')(0.00123) == \'1.230e-03\') except Exception as e: result[\'eng_float_format_set\'] = False # Describing the current option for `display.max_rows` try: result[\'max_rows_description\'] = pd.describe_option(\'display.max_rows\').split()[1].strip(\'[]:\') except Exception as e: result[\'max_rows_description\'] = str(e) try: # Temporarily setting the \'mode.chained_assignment\' option and checking its value with pd.option_context(\'mode.chained_assignment\', \'warn\'): result[\'temporary_chained_assignment\'] = pd.get_option(\'mode.chained_assignment\') except Exception as e: result[\'temporary_chained_assignment\'] = str(e) return result"},{"question":"# Task You are required to implement a function that takes a list of names and generates a `UUID` for each name using both `uuid3` and `uuid5` functions. Your function should compare the `UUIDs` generated and categorize them as either matching or non-matching. Finally, return a report in the form of a dictionary containing the names and their respective UUIDs generated by both methods, along with the match status. # Function Signature ```python def generate_and_compare_uuids(names: list[str]) -> dict: pass ``` # Input - `names` (List[str]): A list of strings representing the names for which UUIDs should be generated. # Output - Returns a dictionary where each key is a name from the input list. Each value is another dictionary containing: - `uuid3`: The UUID generated using `uuid3` for that name in string format. - `uuid5`: The UUID generated using `uuid5` for that name in string format. - `match`: A boolean indicating whether the `uuid3` and `uuid5` values match. # Constraints - Each name in the input list is a non-empty string. - The input list will contain no more than 1000 names. # Example ```python names = [\\"example.com\\", \\"python.org\\"] result = generate_and_compare_uuids(names) # Expected output { \\"example.com\\": { \\"uuid3\\": \\"5df41881-3aed-3515-88a7-2f4a814cf09e\\", \\"uuid5\\": \\"6fa459ea-ee8a-3ca4-894e-db77e160355e\\", \\"match\\": False }, \\"python.org\\": { \\"uuid3\\": \\"6fa459ea-ee8a-3ca4-894e-db77e160355e\\", \\"uuid5\\": \\"886313e1-3b8a-5372-9b90-0c9aee199e5d\\", \\"match\\": False } } ``` # Notes - Use `uuid.NAMESPACE_DNS` for generating the UUIDs with `uuid3` and `uuid5`. - Remember that UUIDs generated using `uuid3` and `uuid5` with the same name and namespace will never be the same since they use different hashing algorithms (MD5 vs SHA-1).","solution":"import uuid def generate_and_compare_uuids(names: list[str]) -> dict: Takes a list of names and generates UUIDs for each name using both uuid3 and uuid5. Compares the UUIDs generated and categorizes them as either matching or non-matching. Args: names (list[str]): A list of strings representing the names for which UUIDs should be generated. Returns: dict: A dictionary where each key is a name from the input list. Each value is another dictionary containing: - uuid3: The UUID generated using uuid3 for that name in string format. - uuid5: The UUID generated using uuid5 for that name in string format. - match: A boolean indicating whether the uuid3 and uuid5 values match. result = {} for name in names: uuid3 = str(uuid.uuid3(uuid.NAMESPACE_DNS, name)) uuid5 = str(uuid.uuid5(uuid.NAMESPACE_DNS, name)) result[name] = { \\"uuid3\\": uuid3, \\"uuid5\\": uuid5, \\"match\\": uuid3 == uuid5 } return result"},{"question":"You are working on an application that needs to manage IP address allocations within a network. Given a network CIDR notation, your task is to implement a function that splits this network into smaller subnets of a specified size and returns a list of these subnets. # Function Signature ```python def create_subnets(network_cidr: str, subnet_prefixlen: int) -> list: pass ``` # Input - `network_cidr` (str): A string representing the network in CIDR notation (e.g., `192.168.1.0/24` or `2001:db8::/32`). - `subnet_prefixlen` (int): The desired subnet prefix length. # Output - The function should return a list of strings, each representing a subnet in CIDR notation. # Constraints - The `network_cidr` will be a valid IPv4 or IPv6 network string. - The `subnet_prefixlen` will be greater than the prefix length of `network_cidr` and valid for the respective IP version. # Performance Requirements - The implementation should be efficient in terms of time and space complexity. # Example ```python # Example 1 network_cidr = \'192.168.1.0/24\' subnet_prefixlen = 26 expected_output = [\'192.168.1.0/26\', \'192.168.1.64/26\', \'192.168.1.128/26\', \'192.168.1.192/26\'] assert create_subnets(network_cidr, subnet_prefixlen) == expected_output # Example 2 network_cidr = \'2001:db8::/32\' subnet_prefixlen = 34 expected_output = [\'2001:db8::/34\', \'2001:db8:4000::/34\', \'2001:db8:8000::/34\', \'2001:db8:c000::/34\'] assert create_subnets(network_cidr, subnet_prefixlen) == expected_output ``` # Notes 1. The function should handle both IPv4 and IPv6 networks. 2. You may use the `ipaddress` module to perform network manipulations. Hints - Use `ipaddress.ip_network()` to create the network object. - Use the `subnets()` method of the network object to divide the network into the specified subnets.","solution":"import ipaddress def create_subnets(network_cidr: str, subnet_prefixlen: int) -> list: Splits a given network (in CIDR notation) into smaller subnets of the specified prefix length. Args: - network_cidr (str): Network in CIDR notation (e.g., \'192.168.1.0/24\' or \'2001:db8::/32\'). - subnet_prefixlen (int): Desired prefix length of the subnets. Returns: - list: List of subnets in CIDR notation. network = ipaddress.ip_network(network_cidr) subnets = list(network.subnets(new_prefix=subnet_prefixlen)) return [str(subnet) for subnet in subnets]"},{"question":"# Python Version Encoding and Decoding The CPython version information is encoded in a single 32-bit integer, referred to as `PY_VERSION_HEX`. This encoding scheme allows representing the Python version numbers and release information compactly. Below is a detailed breakdown of the encoding format: - **PY_MAJOR_VERSION**: stored in bits 1-8 - **PY_MINOR_VERSION**: stored in bits 9-16 - **PY_MICRO_VERSION**: stored in bits 17-24 - **PY_RELEASE_LEVEL**: stored in bits 25-28 (`0xA` for alpha, `0xB` for beta, `0xC` for release candidate, `0xF` for final) - **PY_RELEASE_SERIAL**: stored in bits 29-32 For example: - Version \\"3.4.1a2\\" would be represented as `0x030401a2`. - Version \\"3.10.0\\" would be represented as `0x030a00f0`. Write two functions `encode_version(major, minor, micro, level, serial)` and `decode_version(version_hex)` that perform the following operations: 1. **encode_version(major, minor, micro, level, serial)** - **Input**: - `major` (int) - Major version. - `minor` (int) - Minor version. - `micro` (int) - Micro version. - `level` (char) - Release level (\'a\' for alpha, \'b\' for beta, \'c\' for release candidate, \'f\' for final). - `serial` (int) - Release serial. - **Output**: - An integer representing the encoded version information. - **Constraints**: - `major`, `minor`, and `micro` are non-negative integers. - `level` is one of the characters \'a\', \'b\', \'c\', or \'f\'. - `serial` is a non-negative integer. - **Function Signature**: `def encode_version(major: int, minor: int, micro: int, level: str, serial: int) -> int:` 2. **decode_version(version_hex)** - **Input**: - `version_hex` (int) - A 32-bit integer representing the encoded version information. - **Output**: - A tuple `(major, minor, micro, level, serial)` where: - `major` (int) - Major version. - `minor` (int) - Minor version. - `micro` (int) - Micro version. - `level` (char) - Release level (\'a\' for alpha, \'b\' for beta, \'c\' for release candidate, \'f\' for final). - `serial` (int) - Release serial. - **Function Signature**: `def decode_version(version_hex: int) -> Tuple[int, int, int, str, int]:` # Examples ```python # Example 1 encoded_version = encode_version(3, 4, 1, \'a\', 2) # encoded_version should be 0x030401a2 assert encoded_version == 0x030401a2 decoded_version = decode_version(0x030401a2) # decoded_version should be (3, 4, 1, \'a\', 2) assert decoded_version == (3, 4, 1, \'a\', 2) # Example 2 encoded_version = encode_version(3, 10, 0, \'f\', 0) # encoded_version should be 0x030a00f0 assert encoded_version == 0x030a00f0 decoded_version = decode_version(0x030a00f0) # decoded_version should be (3, 10, 0, \'f\', 0) assert decoded_version == (3, 10, 0, \'f\', 0) ``` # Note - Be mindful of bitwise operations and relevant shifts while implementing the functions. - Ensure that the solutions handle the constraints and edge cases effectively.","solution":"def encode_version(major: int, minor: int, micro: int, level: str, serial: int) -> int: Encodes the version information into a single 32-bit integer. :param major: Major version. :param minor: Minor version. :param micro: Micro version. :param level: Release level (\'a\' for alpha, \'b\' for beta, \'c\' for release candidate, \'f\' for final). :param serial: Release serial. :return: An integer representing the encoded version information. level_map = {\'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF} level_value = level_map[level] version_hex = ( (major << 24) | (minor << 16) | (micro << 8) | (level_value << 4) | serial ) return version_hex def decode_version(version_hex: int): Decodes the version information from a single 32-bit integer. :param version_hex: A 32-bit integer representing the encoded version information. :return: A tuple (major, minor, micro, level, serial). level_map_inv = {0xA: \'a\', 0xB: \'b\', 0xC: \'c\', 0xF: \'f\'} major = (version_hex >> 24) & 0xFF minor = (version_hex >> 16) & 0xFF micro = (version_hex >> 8) & 0xFF level_value = (version_hex >> 4) & 0xF serial = version_hex & 0xF level = level_map_inv[level_value] return (major, minor, micro, level, serial)"},{"question":"# Assessing your Understanding of the `urllib` module in Python **Objective**: Implement a function that fetches data from a given URL and processes the response to extract specific information. --- Problem Statement You are required to write a Python function that: 1. Retrieves JSON data from a given HTTP URL. 2. Parses the JSON data. 3. Extracts specific information from the parsed data based on provided key parameters. 4. Handles redirects and cookies if applicable. # Function Signature ```python def fetch_and_extract(url: str, keys: list) -> dict: pass ``` # Input 1. `url` (str): A string representing the URL from which to fetch data. 2. `keys` (list): A list of strings representing the keys for which to extract values from the JSON response. # Output A dictionary with the specified keys and their corresponding values from the JSON response. # Constraints - The URL will return a JSON response. - If a key does not exist in the response, the value should be `None`. - The function must handle HTTP redirects and cookies appropriately. # Example ```python url = \\"https://api.example.com/data\\" keys = [\\"name\\", \\"age\\", \\"location\\"] result = fetch_and_extract(url, keys) print(result) # Output might be something like: {\\"name\\": \\"John Doe\\", \\"age\\": 30, \\"location\\": \\"New York\\"} ``` # Requirements 1. You should use the `urllib` module to manage the URL request. 2. Include error handling for network-related issues. 3. Ensure that the function handles HTTP redirects and cookies. # Performance - The function should be efficient and able to handle URLs that might involve multiple redirects. --- # Guidance - Use `urllib.request` for opening URLs. - Understand `Request` objects and how to handle cookies with `HTTPCookieProcessor`. - Consult the `urllib.response` to understand how to parse responses. - Implement proper exception handling to manage errors such as network failures or invalid URLs. --- Good luck! This problem ensures you understand key aspects of working with network operations and data extraction using Python\'s `urllib` module.","solution":"import urllib.request import json from http.cookiejar import CookieJar def fetch_and_extract(url: str, keys: list) -> dict: Fetch JSON data from a given URL and extract specific information. Parameters: url (str): The URL from which to fetch data. keys (list): A list of strings representing the keys to extract. Returns: dict: A dictionary with specified keys and their corresponding values from the JSON response. # Handle cookies cookie_jar = CookieJar() opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) urllib.request.install_opener(opener) try: with urllib.request.urlopen(url) as response: data = response.read() json_data = json.loads(data) # Extract the required keys from the JSON data result = {key: json_data.get(key, None) for key in keys} return result except urllib.error.URLError as e: print(f\\"Failed to fetch data: {e.reason}\\") return {key: None for key in keys} except json.JSONDecodeError as e: print(f\\"Failed to parse JSON: {e.msg}\\") return {key: None for key in keys}"},{"question":"# Coding Assessment: Creating a Command-Line Interface using `argparse` Background: You are tasked with creating a command-line utility that processes various tasks involving file operations and text processing. This utility, named `filemanager.py`, should be able to handle multiple sub-commands, each performing a distinct function. Requirements: 1. Create an `ArgumentParser` that defines a set of sub-commands for file operations: - `copy`: Copy contents from one file to another. - `grep`: Search for a pattern in a file and print matching lines. 2. Each sub-command will have its own specific arguments: - `copy`: - Positional arguments: - `source` - Source file path (string). - `destination` - Destination file path (string). - `grep`: - Positional argument: - `pattern` - The regex pattern to search for (string). - `file` - The file to search in (string). - Optional argument: - `--ignore-case` - Ignore case distinctions (flag). 3. Common arguments for the main parser: - `--verbose`: Print detailed messages about the processing. Instructions: Implement the `filemanager.py` script with the following requirements: 1. **Parser and Sub-commands Setup**: - Initialize an `ArgumentParser`. - Set up the `copy` and `grep` sub-commands with their respective arguments. - Add the `--verbose` flag to the main parser that, when set, prints detailed messages about the actions being performed. 2. **Function Implementations**: - Implement a function `copy_file(source, destination, verbose)` to handle copying files. - Implement a function `grep_pattern(pattern, file, ignore_case, verbose)` to handle pattern search in files. 3. **Command Execution**: - Parse the command-line arguments. - Based on the sub-command invoked (`copy` or `grep`), call the appropriate function with parsed arguments. Constraints: - Ensure to handle exceptions such as file not found, permission errors, and invalid regex patterns cleanly. - Use efficient file handling techniques to manage large files. Example Usage: 1. Copying a file: ``` python filemanager.py copy source.txt destination.txt --verbose ``` This command copies the contents of `source.txt` to `destination.txt` and prints detailed messages about the process. 2. Searching for a pattern: ``` python filemanager.py grep \\"error\\" log.txt --ignore-case --verbose ``` This command searches for the pattern \\"error\\" in `log.txt`, ignoring case, and prints matching lines with detailed messages. Submission: Provide the complete `filemanager.py` script, ensuring that it meets the outlined requirements, handles edge cases, and is well-documented.","solution":"import argparse import shutil import re import sys def copy_file(source, destination, verbose=False): try: shutil.copyfile(source, destination) if verbose: print(f\'Copied contents from {source} to {destination}\') except FileNotFoundError: print(f\'Error: {source} not found\') except PermissionError: print(f\'Error: Permission denied when accessing {source} or {destination}\') except Exception as e: print(f\'An error occurred: {e}\') def grep_pattern(pattern, file, ignore_case=False, verbose=False): try: with open(file, \'r\') as f: flags = re.IGNORECASE if ignore_case else 0 regex = re.compile(pattern, flags) for line in f: if regex.search(line): print(line, end=\'\') if verbose: case_info = \\"ignoring case\\" if ignore_case else \\"case sensitive\\" print(f\'Searched for pattern \\"{pattern}\\" in {file} ({case_info})\') except FileNotFoundError: print(f\'Error: {file} not found\') except PermissionError: print(f\'Error: Permission denied when accessing {file}\') except re.error as e: print(f\'Invalid regex pattern: {e}\') except Exception as e: print(f\'An error occurred: {e}\') def main(): parser = argparse.ArgumentParser(description=\'File operations utility.\') parser.add_argument(\'--verbose\', action=\'store_true\', help=\'Print detailed messages\') subparsers = parser.add_subparsers(dest=\'command\') copy_parser = subparsers.add_parser(\'copy\', help=\'Copy contents from source file to destination file\') copy_parser.add_argument(\'source\', type=str, help=\'Source file path\') copy_parser.add_argument(\'destination\', type=str, help=\'Destination file path\') grep_parser = subparsers.add_parser(\'grep\', help=\'Search for a pattern in a file\') grep_parser.add_argument(\'pattern\', type=str, help=\'The regex pattern to search for\') grep_parser.add_argument(\'file\', type=str, help=\'The file to search in\') grep_parser.add_argument(\'--ignore-case\', action=\'store_true\', help=\'Ignore case distinctions\') args = parser.parse_args() if args.command == \'copy\': copy_file(args.source, args.destination, args.verbose) elif args.command == \'grep\': grep_pattern(args.pattern, args.file, args.ignore_case, args.verbose) else: parser.print_help() if __name__ == \'__main__\': main()"},{"question":"# Advanced Python Object Implementation Problem Statement You are a developer tasked with creating a custom data structure that mimics a list but comes with additional features, such as tracking the number of accesses made to any element within the list. This requires defining a new type based on Python\'s object implementation mechanisms. Requirements: 1. Implement a custom list-like class called `TrackedList`. - This class should extend the built-in list type. 2. Override the `__getitem__` and `__setitem__` methods: - `__getitem__` should increase the access count each time an item is accessed. - `__setitem__` should allow setting values just like a normal list but also reset the access count for the modified item. 3. Implement a method `get_access_count(index)` to retrieve the access count for a specific index. 4. Support garbage collection to ensure memory is managed correctly. Constraints: - Your class should handle at least the basic operations of a list (e.g., addition, deletion). - Assume inputs will always be valid for operations (`index` within range, etc.). Example Usage: ```python # Instantiate the new TrackedList tracked_list = TrackedList([1, 2, 3, 4, 5]) # Access some elements print(tracked_list[1]) # Output: 2 print(tracked_list[2]) # Output: 3 # Access counts print(tracked_list.get_access_count(1)) # Output: 1 print(tracked_list.get_access_count(2)) # Output: 1 # Modify elements tracked_list[2] = 10 # Check access counts again print(tracked_list.get_access_count(2)) # Output: 0 (reset due to modification) # Ensure it behaves like a list print(tracked_list[2]) # Output: 10 ``` Implementation Details: - Use a dictionary to keep track of the access counts. - Ensure memory management and garbage collection are accounted for by implementing the necessary protocols. This task will test your understanding of: - Custom object implementation and memory management. - Overriding basic methods and ensuring additional functionality is seamlessly integrated. - Handling Pythonâ€™s internal data structures and garbage collection features. Instructions: 1. Implement the `TrackedList` class with the specified requirements. 2. Ensure your code is efficient and follows Python best practices. 3. Your implementation should pass the provided example usage.","solution":"class TrackedList(list): def __init__(self, *args): super().__init__(*args) self._access_count = {} def __getitem__(self, index): if index not in self._access_count: self._access_count[index] = 0 self._access_count[index] += 1 return super().__getitem__(index) def __setitem__(self, index, value): super().__setitem__(index, value) self._access_count[index] = 0 def get_access_count(self, index): return self._access_count.get(index, 0)"},{"question":"# Objective: You are required to write a function that uses pandas\' plotting capabilities to generate a scatter matrix plot of a given DataFrame. # Function Signature: ```python def generate_scatter_matrix(df: pd.DataFrame, alpha: float = 0.5, figsize: tuple = (10, 10), diagonal: str = \'kde\') -> None: Generates and displays a scatter matrix plot of the given DataFrame. Parameters: - df: pd.DataFrame The input DataFrame containing the data to be plotted. - alpha: float, default 0.5 The alpha value for the points in the scatter plots. - figsize: tuple, default (10, 10) The size of the figure. - diagonal: str, default \'kde\' The type of plot to be used for the diagonal subplots. Options are \'kde\' (Kernel Density Estimation) or \'hist\' (Histogram). Returns: - None The function should display the plot and return nothing. # Constraints: - The DataFrame df will always have at least 2 numerical columns. - The alpha parameter must be a float between 0 and 1 (inclusive). - The figsize parameter must be a tuple of two positive integers. - The diagonal parameter must be either \'kde\' or \'hist\'. # Example Usage: ```python import pandas as pd import numpy as np # Generating a sample DataFrame data = { \'A\': np.random.randn(100), \'B\': np.random.randn(100), \'C\': np.random.randn(100), \'D\': np.random.randn(100) } df = pd.DataFrame(data) # Generating the scatter matrix plot generate_scatter_matrix(df) ``` # Notes: - Ensure that you have `matplotlib` installed in your environment as pandas\' plotting functions rely on it. - The function does not need to return any value; it should display the plot directly.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix def generate_scatter_matrix(df: pd.DataFrame, alpha: float = 0.5, figsize: tuple = (10, 10), diagonal: str = \'kde\') -> None: Generates and displays a scatter matrix plot of the given DataFrame. Parameters: - df: pd.DataFrame The input DataFrame containing the data to be plotted. - alpha: float, default 0.5 The alpha value for the points in the scatter plots. - figsize: tuple, default (10, 10) The size of the figure. - diagonal: str, default \'kde\' The type of plot to be used for the diagonal subplots. Options are \'kde\' (Kernel Density Estimation) or \'hist\' (Histogram). Returns: - None The function should display the plot and return nothing. if not isinstance(df, pd.DataFrame): raise ValueError(\'The input df must be a pandas DataFrame.\') if not (0 <= alpha <= 1): raise ValueError(\'The alpha parameter must be a float between 0 and 1 (inclusive).\') if (not isinstance(figsize, tuple)) or (len(figsize) != 2) or (not all(isinstance(i, (int, float)) and i > 0 for i in figsize)): raise ValueError(\'The figsize must be a tuple of two positive numbers.\') if diagonal not in [\'kde\', \'hist\']: raise ValueError(\\"The diagonal parameter must be either \'kde\' or \'hist\'.\\") scatter_matrix(df, alpha=alpha, figsize=figsize, diagonal=diagonal) plt.show()"},{"question":"# Covariance Estimation Challenge In this task, you\'ll work with the `sklearn.covariance` module in the scikit-learn library. You\'ll demonstrate your understanding by implementing and comparing different covariance estimation techniques on a sample dataset. # Task 1. **Load the Dataset**: Load the `diabetes` dataset from the `sklearn.datasets` module. Use the `data` attribute for analysis. 2. **Empirical Covariance**: - Compute the empirical covariance matrix using both the `empirical_covariance` function and the `EmpiricalCovariance` class. 3. **Shrinkage**: - Perform covariance shrinkage using the `shrunk_covariance` function. 4. **Ledoit-Wolf Shrinkage**: - Compute the Ledoit-Wolf shrinkage estimator using the `ledoit_wolf` function. 5. **Oracle Approximating Shrinkage (OAS)**: - Compute the OAS estimator using the `oas` function. 6. **Comparison and Visualization**: - Compare these covariance matrices using Frobenius norm difference from the empirical covariance matrix. - Visualize the covariance matrices using heatmaps. # Instructions - Implement the functions as specified. - Ensure your code runs without errors and produces the specified outputs. - Use appropriate comments and documentation. Expected Input and Output - **Input**: None (you load the dataset within your code). - **Output**: Printed Frobenius norm differences and heatmaps. Constraints - Use only the specified functions and classes from `sklearn.covariance`. - Perform all operations on the `data` attribute of the `diabetes` dataset. Performance Requirement - Ensure that your implementation is efficient in terms of both time and space. Example Code ```python import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import load_diabetes from sklearn.covariance import empirical_covariance, ShrunkCovariance, LedoitWolf, OAS # Load dataset data = load_diabetes().data # Compute empirical covariance emp_cov_matrix = empirical_covariance(data) # Shrinkage distance_matrices = [] shrunk_cov_matrix = Covariances.shrunk_covariance(emp_cov_matrix, shrinkage=0.1) # Ledoit-Wolf Shrinkage lw_cov_matrix, _ = ledoit_wolf(data) # Oracle Approximating Shrinkage oas_cov_matrix, _ = oas(data) # Function to calculate Frobenius norm difference def frobenius_diff(matrix1, matrix2): return np.linalg.norm(matrix1 - matrix2, \'fro\') # Calculate differences differences = { \'Empirical vs Shrunk\': frobenius_diff(emp_cov_matrix, shrunk_cov_matrix), \'Empirical vs Ledoit-Wolf\': frobenius_diff(emp_cov_matrix, lw_cov_matrix), \'Empirical vs OAS\': frobenius_diff(emp_cov_matrix, oas_cov_matrix), } # Print differences for method, diff in differences.items(): print(f\\"{method}: {diff}\\") # Function to plot heatmap def plot_heatmap(matrix, title): plt.figure(figsize=(10, 8)) sns.heatmap(matrix, annot=True, fmt=\\".2f\\") plt.title(title) plt.show() # Plot heatmaps plot_heatmap(emp_cov_matrix, \\"Empirical Covariance\\") plot_heatmap(shrunk_cov_matrix, \\"Shrunk Covariance\\") plot_heatmap(lw_cov_matrix, \\"Ledoit-Wolf Covariance\\") plot_heatmap(oas_cov_matrix, \\"OAS Covariance\\") ``` Feel free to explore and modify the hyperparameters (e.g., shrinkage) to see their effects on the outputs.","solution":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import load_diabetes from sklearn.covariance import EmpiricalCovariance, shrunk_covariance, ledoit_wolf, oas # Load dataset data = load_diabetes().data # Compute empirical covariance using the EmpiricalCovariance class emp_cov_estimator = EmpiricalCovariance() emp_cov_estimator.fit(data) emp_cov_matrix_class = emp_cov_estimator.covariance_ # Compute empirical covariance using the empirical_covariance function emp_cov_matrix_func = emp_cov_estimator.covariance_ # Perform covariance shrinkage shrinkage = 0.1 # Hyperparameter for shrinkage shrunk_cov_matrix = shrunk_covariance(emp_cov_matrix_class, shrinkage=shrinkage) # Ledoit-Wolf Shrinkage lw_cov_matrix, _ = ledoit_wolf(data) # Oracle Approximating Shrinkage (OAS) oas_cov_matrix, _ = oas(data) # Function to calculate Frobenius norm difference def frobenius_diff(matrix1, matrix2): return np.linalg.norm(matrix1 - matrix2, \'fro\') # Calculate differences differences = { \'Empirical (class) vs Shrunk\': frobenius_diff(emp_cov_matrix_class, shrunk_cov_matrix), \'Empirical (class) vs Ledoit-Wolf\': frobenius_diff(emp_cov_matrix_class, lw_cov_matrix), \'Empirical (class) vs OAS\': frobenius_diff(emp_cov_matrix_class, oas_cov_matrix), } # Print differences for method, diff in differences.items(): print(f\\"{method}: {diff}\\") # Function to plot heatmap def plot_heatmap(matrix, title): plt.figure(figsize=(10, 8)) sns.heatmap(matrix, annot=False, cmap=\\"viridis\\") plt.title(title) plt.show() # Plot heatmaps plot_heatmap(emp_cov_matrix_class, \\"Empirical Covariance\\") plot_heatmap(shrunk_cov_matrix, \\"Shrunk Covariance\\") plot_heatmap(lw_cov_matrix, \\"Ledoit-Wolf Covariance\\") plot_heatmap(oas_cov_matrix, \\"OAS Covariance\\")"},{"question":"**Objective**: Demonstrate your understanding of creating and managing virtual environments and package installations using Python\'s `venv` and `pip` modules. **Problem Statement**: You are tasked with automating the setup of a Python project. This involves creating a virtual environment, activating it, and installing a specific set of packages listed in a `requirements.txt` file. Write a Python script named `setup_project.py` that performs the following steps: 1. Creates a virtual environment named `.venv` in the current working directory. 2. Activates the virtual environment. 3. Reads a `requirements.txt` file located in the current working directory. This file contains a list of packages with specific versions to be installed. 4. Uses `pip` to install all the packages listed in `requirements.txt`. **Input**: - A `requirements.txt` file in the current working directory. Each line in this file follows the format `<package_name>==<version>` (e.g., `requests==2.25.1`). **Output**: - There is no direct output. The script should ensure that the virtual environment is created, activated, and that the packages from `requirements.txt` are installed correctly. **Constraints**: - You may assume that `python3` is available in the environment where this script will be run. - The script should handle potential errors gracefully, such as missing `requirements.txt` or failed package installations. **Example `requirements.txt`**: ``` requests==2.25.1 numpy==1.19.5 ``` **Hints**: - You need to use the `subprocess` module to execute shell commands from within your Python script. - Consider checking for the existence of the `.venv` directory before creating it to avoid unnecessary work. ```python # setup_project.py import os import subprocess import sys def create_virtual_environment(venv_dir): try: # Create the virtual environment subprocess.check_call([sys.executable, \'-m\', \'venv\', venv_dir]) print(f\'Virtual environment created at {venv_dir}\') except subprocess.CalledProcessError as e: print(f\'Error creating virtual environment: {e}\') sys.exit(1) def activate_virtual_environment(venv_dir): activate_script = os.path.join(venv_dir, \'Scripts\', \'activate\') if os.name == \'nt\' else os.path.join(venv_dir, \'bin\', \'activate\') if os.name != \'nt\': return f\'source {activate_script}\' else: return f\'{activate_script}\' def install_packages(venv_dir): pip_executable = os.path.join(venv_dir, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(venv_dir, \'Scripts\', \'pip\') try: # Install packages from requirements.txt subprocess.check_call([pip_executable, \'install\', \'-r\', \'requirements.txt\']) print(\'Packages installed successfully\') except subprocess.CalledProcessError as e: print(f\'Error installing packages: {e}\') sys.exit(1) def main(): venv_dir = \'.venv\' if not os.path.exists(\'requirements.txt\'): print(\'requirements.txt file not found\') sys.exit(1) create_virtual_environment(venv_dir) activate_command = activate_virtual_environment(venv_dir) # Since activation of venv is a shell command, you should instruct the user to run it in their shell. print(f\'To activate the virtual environment, run the following in your shell:n {activate_command}\') # Continue to install packages after activation install_packages(venv_dir) if __name__ == \'__main__\': main() ``` This script will set up your Python project by creating a virtual environment, activating it, and installing the required packages. Note that actual activation of the virtual environment needs to be done in the shell where the script runs.","solution":"# setup_project.py import os import subprocess import sys def create_virtual_environment(venv_dir): try: # Create the virtual environment subprocess.check_call([sys.executable, \'-m\', \'venv\', venv_dir]) print(f\'Virtual environment created at {venv_dir}\') except subprocess.CalledProcessError as e: print(f\'Error creating virtual environment: {e}\') sys.exit(1) def activate_virtual_environment(venv_dir): activate_script = os.path.join(venv_dir, \'Scripts\', \'activate\') if os.name == \'nt\' else os.path.join(venv_dir, \'bin\', \'activate\') if os.name != \'nt\': return f\'source {activate_script}\' else: return f\'{activate_script}\' def install_packages(venv_dir): pip_executable = os.path.join(venv_dir, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(venv_dir, \'Scripts\', \'pip\') try: # Install packages from requirements.txt subprocess.check_call([pip_executable, \'install\', \'-r\', \'requirements.txt\']) print(\'Packages installed successfully\') except subprocess.CalledProcessError as e: print(f\'Error installing packages: {e}\') sys.exit(1) def main(): venv_dir = \'.venv\' if not os.path.exists(\'requirements.txt\'): print(\'requirements.txt file not found\') sys.exit(1) create_virtual_environment(venv_dir) activate_command = activate_virtual_environment(venv_dir) # Since activation of venv is a shell command, you should instruct the user to run it in their shell. print(f\'To activate the virtual environment, run the following in your shell:n {activate_command}\') # Continue to install packages after activation install_packages(venv_dir) if __name__ == \'__main__\': main()"},{"question":"**Objective:** Demonstrate your understanding of PyTorch meta tensors by loading a model onto the meta device, performing a series of transformations, and moving it to another device while reinitializing the model parameters. **Task:** 1. Load a pre-trained PyTorch model onto the meta device using `torch.load`. 2. Perform a specified transformation on the model architecture. 3. Move the transformed model to the CPU device, keeping it with uninitialized parameters. 4. Reinitialize the parameters of the model on the CPU. **Instructions:** 1. Use the `torch.load` function to load the model weights onto the meta device. 2. Transform the given model\'s architecture by adding a new `torch.nn.Linear` layer. 3. Move the transformed model to the CPU device with uninitialized parameters using the `to_empty` method. 4. Reinitialize the parameters of the model on the CPU device. **Input:** - A string representing the file path to the pre-trained model. **Output:** - The reinitialized model on the CPU device. **Constraints:** - Use the meta device for the intermediary transformation. - Ensure parameter reinitialization is explicitly done on the CPU device. ```python import torch import torch.nn as nn def transform_and_load_model(model_path: str) -> nn.Module: # Load the pre-trained model onto the meta device model = torch.load(model_path, map_location=\'meta\') # Define an additional Linear layer to be added additional_layer = nn.Linear(in_features=model.fc.in_features, out_features=100) # Override device to \'meta\' for the transformation with torch.device(\'meta\'): model.fc = nn.Sequential( model.fc, additional_layer ) # Move the model to CPU with uninitialized parameters model.to_empty(device=\'cpu\') # Reinitialize the parameters def init_weights(m): for name, param in m.named_parameters(): if \'weight\' in name: nn.init.kaiming_normal_(param) elif \'bias\' in name: nn.init.constant_(param, 0) model.apply(init_weights) return model # Example use # model = transform_and_load_model(\'path_to_model.pt\') # print(model) ``` Note: - Assume the pre-trained model is a simple custom model with an attribute `fc` (e.g., a classifier layer). - Adjust the example accordingly to fit the actual model architecture if different.","solution":"import torch import torch.nn as nn def transform_and_load_model(model_path: str) -> nn.Module: # Load the pre-trained model onto the meta device model = torch.load(model_path, map_location=\'meta\') # Define an additional Linear layer to be added new_layer = nn.Linear(in_features=model.fc.in_features, out_features=100) # Override device to \'meta\' for the transformation with torch.device(\'meta\'): model.fc = nn.Sequential(model.fc, new_layer) # Move the model to CPU with uninitialized parameters model.to_empty(device=\'cpu\') # Reinitialize the parameters def init_weights(m): for name, param in m.named_parameters(): if \'weight\' in name: nn.init.kaiming_normal_(param) elif \'bias\' in name: nn.init.constant_(param, 0) model.apply(init_weights) return model"},{"question":"# Python Coding Assessment Question # Objective: Implement an asynchronous echo server using the `asynchat` module. This server should read incoming messages and echo them back to the client. The `asynchat` framework should be used to manage incoming and outgoing data. # Problem Statement: Create a subclass of `asynchat.async_chat` called `EchoHandler` that implements the following: 1. The `collect_incoming_data(data)` method to collect incoming data. 2. The `found_terminator()` method to identify when a complete message has been received and prepare it for echoing back to the client. 3. Initialize the class to set an appropriate terminator for message boundaries. Additionally, create a server class that uses `asyncore.dispatcher` to accept connections and instantiate `EchoHandler` for each new client. # Requirements: 1. **Input and Output:** The server should append \\"n\\" as the message terminator for incoming and outgoing messages. 2. **Constraints:** - Use the `set_terminator()` method to define the message boundary. - Implement `collect_incoming_data()` to gather data between terminators. - Implement `found_terminator()` to process the complete message and prepare it for sending back. 3. **Performance:** Ensure the server can handle multiple connections simultaneously using the asynchronous framework. # Sample Code Skeleton: ```python import asynchat import asyncore import socket class EchoHandler(asynchat.async_chat): def __init__(self, sock): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): # Collect incoming data here pass def found_terminator(self): # Process the complete message here pass def send_message(self, message): # Push the message back to the client pass class EchoServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accept(self): pair = self.accept() if pair is not None: sock, addr = pair EchoHandler(sock) def run_server(): server = EchoServer(\'localhost\', 8080) asyncore.loop() if __name__ == \\"__main__\\": run_server() ``` # Instructions: - Complete the `EchoHandler` class by implementing `collect_incoming_data`, `found_terminator`, and `send_message` methods. - Ensure `collect_incoming_data` gathers data in `ibuffer`. - Ensure `found_terminator` processes the complete message (stored in `ibuffer`), clears `ibuffer`, and sends the message back using `send_message`. - `send_message` should format the message appropriately and use the `push` method to send it. # Evaluation: Your solution will be evaluated based on: - Correct implementation of `collect_incoming_data` and `found_terminator`. - Proper handling and echoing of messages. - Correct use of asynchronous mechanisms to handle multiple clients.","solution":"import asynchat import asyncore import socket class EchoHandler(asynchat.async_chat): def __init__(self, sock): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): Collect the incoming data. self.ibuffer.append(data) def found_terminator(self): Process the complete message when the terminator is found. message = b\'\'.join(self.ibuffer).decode(\'utf-8\') print(f\\"Received message: {message}\\") self.send_message(message) self.ibuffer = [] def send_message(self, message): Send the message back to the client. response = (message + \'n\').encode(\'utf-8\') self.push(response) class EchoServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accept(self): pair = self.accept() if pair is not None: sock, addr = pair EchoHandler(sock) def run_server(): server = EchoServer(\'localhost\', 8080) asyncore.loop() if __name__ == \\"__main__\\": run_server()"},{"question":"# Advanced Python Coding Assessment: SQLite Database Operations Objective: Demonstrate your understanding of Python\'s `sqlite3` module for data persistence and manipulation. Problem Statement: You are required to create a simple book library management system using Python\'s `sqlite3` module. Your task is to implement the following functionalities: 1. **Initialize the Database**: Create a function `initialize_db()` that sets up the database with a single table `books` having the following schema: - `id` (INTEGER, Primary Key, Auto-increment) - `title` (TEXT, Not Null) - `author` (TEXT, Not Null) - `year` (INTEGER) - `genre` (TEXT) 2. **Insert a Book Record**: Create a function `insert_book(title, author, year, genre)` that inserts a new book into the `books` table. 3. **Query Books by Author**: Create a function `get_books_by_author(author)` that retrieves all books written by a specific author. 4. **Update Book Information**: Create a function `update_book(book_id, title=None, author=None, year=None, genre=None)` that updates the information of a book given its `id`. Parameters that are `None` should not be updated. 5. **Delete a Book Record**: Create a function `delete_book(book_id)` that deletes a book record from the database given its `id`. 6. **List All Books**: Create a function `list_all_books()` that returns a list of all books in the database. # Requirements: - Use context managers (`with` statements) for database connections. - Handle exceptions where appropriate. - Ensure your database operations are efficient and follow good practices. # Input/Output Format: - **Function: `initialize_db()`** - Input: None - Output: Creates a `books` table in the SQLite database. - **Function: `insert_book(title: str, author: str, year: int, genre: str)`** - Input: `title` (str), `author` (str), `year` (int), `genre` (str) - Output: Inserts the book into the database. - **Function: `get_books_by_author(author: str) -> List[Tuple[int, str, str, int, str]]`** - Input: `author` (str) - Output: A list of tuples, each representing a book by the author. Each tuple should contain `(id, title, author, year, genre)`. - **Function: `update_book(book_id: int, title: Optional[str] = None, author: Optional[str] = None, year: Optional[int] = None, genre: Optional[str] = None)`** - Input: `book_id` (int), optional `title` (str), optional `author` (str), optional `year` (int), optional `genre` (str) - Output: Updates the specified fields of the book record in the database. - **Function: `delete_book(book_id: int)`** - Input: `book_id` (int) - Output: Deletes the book record from the database. - **Function: `list_all_books() -> List[Tuple[int, str, str, int, str]]`** - Input: None - Output: A list of tuples, each representing a book. Each tuple should contain `(id, title, author, year, genre)`. # Constraints: - Assume that the database file is named `library.db`. - Do not use any ORM libraries; interact directly with the `sqlite3` module. - Ensure that your solution works efficiently with a large number of records. Example Usage: ```python initialize_db() insert_book(\'The Great Gatsby\', \'F. Scott Fitzgerald\', 1925, \'Fiction\') insert_book(\'To Kill a Mockingbird\', \'Harper Lee\', 1960, \'Fiction\') print(get_books_by_author(\'Harper Lee\')) # [(2, \'To Kill a Mockingbird\', \'Harper Lee\', 1960, \'Fiction\')] update_book(1, year=1926) delete_book(2) print(list_all_books()) # [(1, \'The Great Gatsby\', \'F. Scott Fitzgerald\', 1926, \'Fiction\')] ```","solution":"import sqlite3 from typing import List, Tuple, Optional def initialize_db(): with sqlite3.connect(\'library.db\') as conn: cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT NOT NULL, author TEXT NOT NULL, year INTEGER, genre TEXT ) \'\'\') conn.commit() def insert_book(title: str, author: str, year: int, genre: str): with sqlite3.connect(\'library.db\') as conn: cursor = conn.cursor() cursor.execute(\'\'\' INSERT INTO books (title, author, year, genre) VALUES (?, ?, ?, ?) \'\'\', (title, author, year, genre)) conn.commit() def get_books_by_author(author: str) -> List[Tuple[int, str, str, int, str]]: with sqlite3.connect(\'library.db\') as conn: cursor = conn.cursor() cursor.execute(\'\'\' SELECT * FROM books WHERE author = ? \'\'\', (author,)) books = cursor.fetchall() return books def update_book(book_id: int, title: Optional[str] = None, author: Optional[str] = None, year: Optional[int] = None, genre: Optional[str] = None): set_clause = [] params = [] if title is not None: set_clause.append(\\"title = ?\\") params.append(title) if author is not None: set_clause.append(\\"author = ?\\") params.append(author) if year is not None: set_clause.append(\\"year = ?\\") params.append(year) if genre is not None: set_clause.append(\\"genre = ?\\") params.append(genre) params.append(book_id) set_clause_str = \\", \\".join(set_clause) query = f\'\'\' UPDATE books SET {set_clause_str} WHERE id = ? \'\'\' with sqlite3.connect(\'library.db\') as conn: cursor = conn.cursor() cursor.execute(query, tuple(params)) conn.commit() def delete_book(book_id: int): with sqlite3.connect(\'library.db\') as conn: cursor = conn.cursor() cursor.execute(\'\'\' DELETE FROM books WHERE id = ? \'\'\', (book_id,)) conn.commit() def list_all_books() -> List[Tuple[int, str, str, int, str]]: with sqlite3.connect(\'library.db\') as conn: cursor = conn.cursor() cursor.execute(\'\'\' SELECT * FROM books \'\'\') books = cursor.fetchall() return books"},{"question":"# Task You need to implement a function `generate_custom_calendar(year: int, month: int, start_day: str) -> str` that generates a text calendar for a given month and year, and allows the specification of the starting day of the week. # Specifications: 1. **Input:** - `year` (int): The year of the calendar (e.g., 2023). - `month` (int): The month of the calendar (e.g., 10 for October). - `start_day` (str): The starting day of the week (e.g., \\"Monday\\", \\"Sunday\\"). 2. **Output:** - Return a string representing the calendar. 3. **Constraints:** - The starting day of the week (`start_day`) must be one of the following: \\"Monday\\", \\"Tuesday\\", \\"Wednesday\\", \\"Thursday\\", \\"Friday\\", \\"Saturday\\", \\"Sunday\\". If it\'s not, raise a `ValueError` with the message `\\"Invalid start day\\"`. - The calendar should adhere to typical format conventions, with each week on a new line. # Example: ```python # Example 1 print(generate_custom_calendar(2023, 10, \\"Monday\\")) ``` Output: ``` October 2023 Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ``` ```python # Example 2 print(generate_custom_calendar(2023, 10, \\"Sunday\\")) ``` Output: ``` October 2023 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ``` # Notes: - The function should utilize the `calendar` module to handle the calendar computations. - Ensure that the calendar displays correctly formatted weeks and days, and handles different start days properly. Implement the function `generate_custom_calendar` below: ```python import calendar def generate_custom_calendar(year: int, month: int, start_day: str) -> str: # Ensure start_day is valid day_map = { \\"Monday\\": calendar.MONDAY, \\"Tuesday\\": calendar.TUESDAY, \\"Wednesday\\": calendar.WEDNESDAY, \\"Thursday\\": calendar.THURSDAY, \\"Friday\\": calendar.FRIDAY, \\"Saturday\\": calendar.SATURDAY, \\"Sunday\\": calendar.SUNDAY } if start_day not in day_map: raise ValueError(\\"Invalid start day\\") # Set the first weekday calendar.setfirstweekday(day_map[start_day]) # Create a text calendar text_cal = calendar.TextCalendar(firstweekday=day_map[start_day]) # Return the formatted month as a string return text_cal.formatmonth(year, month) # Example print(generate_custom_calendar(2023, 10, \\"Monday\\")) print(generate_custom_calendar(2023, 10, \\"Sunday\\")) ```","solution":"import calendar def generate_custom_calendar(year: int, month: int, start_day: str) -> str: Generates a text calendar for the given month and year, with the week starting on start_day. # Ensure start_day is valid day_map = { \\"Monday\\": calendar.MONDAY, \\"Tuesday\\": calendar.TUESDAY, \\"Wednesday\\": calendar.WEDNESDAY, \\"Thursday\\": calendar.THURSDAY, \\"Friday\\": calendar.FRIDAY, \\"Saturday\\": calendar.SATURDAY, \\"Sunday\\": calendar.SUNDAY } if start_day not in day_map: raise ValueError(\\"Invalid start day\\") # Set the first weekday calendar.setfirstweekday(day_map[start_day]) # Create a text calendar text_cal = calendar.TextCalendar(firstweekday=day_map[start_day]) # Return the formatted month as a string return text_cal.formatmonth(year, month)"},{"question":"# Objective Design a function using pandas that performs data visualization incorporating multiple plotting techniques. # Description You are provided with a CSV file that contains time-series data of multiple stock prices. Your task is to implement a function that reads the CSV file and generates the following plots: 1. Andrews Curves plot for visualizing cluster structures of the stocks. 2. Autocorrelation plot to check for any auto-correlation in the closing prices of a selected stock. 3. Scatter Matrix plot of all numeric columns to observe the relationships and distributions. # Function Signature ```python def visualize_stock_data(file_path: str, stock_column: str) -> None: pass ``` # Input 1. `file_path` (str): The path to the CSV file containing the stock data. The file will have columns like `Date`, `StockA_Close`, `StockB_Close`, `Volume` etc. 2. `stock_column` (str): The name of the column for which to generate the autocorrelation plot. This will be one of the `Stock_Close` columns. # Output The function should not return anything, but should perform the following: 1. Display an Andrews Curves plot for all stock columns. 2. Display an Autocorrelation plot for the specified `stock_column`. 3. Display a Scatter Matrix plot for all numeric columns in the dataset. # Constraints 1. Assume the CSV file is correctly formatted with appropriate headers. 2. The function should handle any number of stock columns and other numeric columns present in the file. # Example Suppose the CSV file `stocks.csv` has the following structure: ```csv Date,StockA_Close,StockB_Close,Volume 2023-01-01,150.3,248.1,1000 2023-01-02,152.5,250.0,1100 ... ``` and you call the function as: ```python visualize_stock_data(\'stocks.csv\', \'StockA_Close\') ``` The function should read the data and generate: 1. Andrews Curves plot to visualize clusters among `StockA_Close` and `StockB_Close`. 2. Autocorrelation plot for `StockA_Close`. 3. Scatter Matrix plot for all numeric columns which are `StockA_Close`, `StockB_Close`, and `Volume`. # Note Ensure the plots are displayed properly with titles and labels where applicable to make them informative.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import andrews_curves, autocorrelation_plot, scatter_matrix def visualize_stock_data(file_path: str, stock_column: str) -> None: # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Display Andrews Curves plot for all stock columns (which typically end with \\"_Close\\") stock_columns = [col for col in df.columns if col.endswith(\'_Close\')] if stock_columns: plt.figure(figsize=(10, 6)) andrews_curves(df[stock_columns], class_column=stock_columns[0]) plt.title(\'Andrews Curves Plot for Stock Data\') plt.show() # Display Autocorrelation plot for the specified stock_column if stock_column in df.columns: plt.figure(figsize=(10, 6)) autocorrelation_plot(df[stock_column]) plt.title(f\'Autocorrelation Plot for {stock_column}\') plt.show() # Display Scatter Matrix plot for all numeric columns numeric_columns = df.select_dtypes(include=[\'number\']).columns if len(numeric_columns) > 1: # It should have at least two columns to plot scatter matrix scatter_matrix(df[numeric_columns], figsize=(10, 6), diagonal=\'kde\') plt.suptitle(\'Scatter Matrix Plot for Numeric Columns\') plt.show()"},{"question":"Objective You are tasked with building a network server that can handle multiple client connections asynchronously using Python\'s `select` module. This will require constructing a server that can read from and write to multiple sockets without blocking. Problem Statement Implement a class `AsyncServer` that sets up a non-blocking TCP server using Pythonâ€™s `select` module to handle multiple client connections. The server should accept connections, read data from clients, and write responses back. Specifications 1. **Initialization**: - Initialize a TCP server socket to listen on a specified host and port. - The socket should be non-blocking. 2. **Methods**: - `run()`: Enter the main loop where the server: - Uses `select.select` to monitor multiple sockets for reading, writing, and exceptional conditions. - Accepts new client connections and adds them to the list of monitored sockets. - Reads data from connected clients and stores it. - Sends a default response \\"Echo: <received_message>\\" back to each client. 3. **Input/Output**: - Reading from a client socket until the end-of-file message (\\"EOF\\") is received. - Echoing back the received message prefixed with \\"Echo: \\" to the respective client. 4. **Constraints**: - Use the `select` module. - Handle the connections and I/O operations without any blocking. Example ```python server = AsyncServer(host=\'127.0.0.1\', port=8888) server.run() ``` This should initialize a server on localhost port 8888 that can handle multiple clients simultaneously. Each connection should be non-blocking and handle data asynchronously. When a client sends a message, the server responds with \\"Echo: <message>\\". **Note**: You need to handle exceptional conditions and client disconnections gracefully. Your Task: Implement the `AsyncServer` class with the described functionality. ```python import socket import select class AsyncServer: def __init__(self, host, port): # Initialize the server socket here pass def run(self): # Implement the main loop for handling connections and data transfer here pass ``` Additional Information: - Make sure to handle socket exceptions and errors gracefully. - Ensure the server exits cleanly on a keyboard interrupt (Ctrl+C). Good luck!","solution":"import socket import select class AsyncServer: def __init__(self, host, port): self.host = host self.port = port self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.server_socket.setblocking(False) self.server_socket.bind((host, port)) self.server_socket.listen(5) self.inputs = [self.server_socket] self.outputs = [] def run(self): print(f\\"Server is running on {self.host}:{self.port}\\") try: while True: readable, writable, exceptional = select.select(self.inputs, self.outputs, self.inputs) for s in readable: if s is self.server_socket: # A \\"readable\\" server socket is ready to accept a connection client_socket, client_address = self.server_socket.accept() client_socket.setblocking(False) self.inputs.append(client_socket) else: data = s.recv(1024) if data: message = data.decode(\'utf-8\').strip() if message.endswith(\'EOF\'): s.sendall(f\\"Echo: {message[:-4]}\\".encode(\'utf-8\')) self.inputs.remove(s) s.close() else: # No data, the connection is closed if s in self.inputs: self.inputs.remove(s) s.close() for s in exceptional: # Handle exceptional conditions if s in self.inputs: self.inputs.remove(s) s.close() except KeyboardInterrupt: print(\\"nServer is shutting down...\\") finally: for s in self.inputs: s.close() self.server_socket.close()"},{"question":"**Task:** You are tasked with writing a function that manipulates a `.plist` file. You need to read the given `.plist` file, perform modifications to its contents, and then write the modified data back to a new `.plist` file. The modifications you should perform are: 1. Add a new key-value pair to the top-level dictionary with the key \\"modified\\" and the value being the current date and time. 2. Update an existing key called \\"version\\" by incrementing its integer value by 1. If the key does not exist, add it with an initial value of 1. 3. Ensure the modifications are saved in XML format. Here is the structure you need to follow: ```python import plistlib import datetime def modify_plist(input_file_path: str, output_file_path: str) -> None: Modify a .plist file by adding a â€˜modifiedâ€™ date and incrementing the â€˜versionâ€™ key. Args: input_file_path (str): Path to the input .plist file. output_file_path (str): Path to the output .plist file. Returns: None # Read the plist file and load its content with open(input_file_path, \'rb\') as fp: plist_data = plistlib.load(fp) # Add or update the \'version\' key if \\"version\\" in plist_data: plist_data[\\"version\\"] += 1 else: plist_data[\\"version\\"] = 1 # Add the \'modified\' key with the current date and time plist_data[\\"modified\\"] = datetime.datetime.now() # Write the updated dictionary back to a new plist file with open(output_file_path, \'wb\') as fp: plistlib.dump(plist_data, fp, fmt=plistlib.FMT_XML) # Example usage: # modify_plist(\'example_input.plist\', \'example_output.plist\') ``` **Input:** - `input_file_path` (str): The file path to the input `.plist` file to be modified. - `output_file_path` (str): The file path to the output `.plist` file where the modified data will be saved. **Output:** - None **Constraints:** - The input `.plist` file will be well-formed. - Ensure exception handling for file I/O operations such as file not found or read/write errors. **Evaluation Criteria:** - Correctness of the implementation. - Adherence to the required modifications to the plist contents. - Proper handling of file reading and writing. - Code readability and documentation.","solution":"import plistlib import datetime def modify_plist(input_file_path: str, output_file_path: str) -> None: Modify a .plist file by adding a â€˜modifiedâ€™ date and incrementing the â€˜versionâ€™ key. Args: input_file_path (str): Path to the input .plist file. output_file_path (str): Path to the output .plist file. Returns: None try: # Read the plist file and load its content with open(input_file_path, \'rb\') as fp: plist_data = plistlib.load(fp) # Add or update the \'version\' key if \\"version\\" in plist_data: plist_data[\\"version\\"] += 1 else: plist_data[\\"version\\"] = 1 # Add the \'modified\' key with the current date and time plist_data[\\"modified\\"] = datetime.datetime.now().isoformat() # Write the updated dictionary back to a new plist file with open(output_file_path, \'wb\') as fp: plistlib.dump(plist_data, fp, fmt=plistlib.FMT_XML) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Email Header Manipulation You are required to implement a function that takes an email header string, decodes it, modifies the header, and re-encodes it back into an RFC-compliant format. Specifically, your task includes the following: Function Signature ```python def modify_email_header(header_string: str, updates: dict) -> str: pass ``` Input 1. `header_string` (str): An RFC 2047-encoded email header string. 2. `updates` (dict): A dictionary where keys represent header fields to be updated, and the associated values are the new values for these fields. For example, `{\'Subject\': \'Hello World\', \'From\': \'test@example.com\'}`. Output - Return a new RFC-compliant email header string with the updates applied. Requirements 1. Decode the provided `header_string` into its constituent parts. 2. For each key in `updates`, modify the corresponding header field with the new value. 3. Encode the updated header back into an RFC-compliant format while preserving the appropriate character sets and encodings. 4. Handle cases where updates may involve non-ASCII characters, ensuring they are correctly encoded. Constraints - The email header string may contain multiple headers. - Ensure that the resulting headers comply with RFC 2047 and RFC 2822 standards. - You can assume the `header_string` and `updates` dictionary will always be well-formed. Example ```python header_string = \'Subject: =?iso-8859-1?q?p=F6stal?=nFrom: test@example.com\' updates = {\'Subject\': \'New Subject\'} # The function should update the Subject field and return: # \'Subject: =?us-ascii?q?New_Subject?=nFrom: test@example.com\' print(modify_email_header(header_string, updates)) ``` Note - Utilize the `email.header` module for encoding and decoding header values. - Ensure any line splitting in headers adheres to the maximum line length requirements specified in the RFC standards.","solution":"from email.header import decode_header, make_header from email.parser import HeaderParser from email.policy import default as default_policy from email.generator import Generator from io import StringIO def modify_email_header(header_string: str, updates: dict) -> str: parser = HeaderParser(policy=default_policy) headers = parser.parsestr(header_string) for key, val in updates.items(): if key in headers: headers.replace_header(key, val) else: headers[key] = val new_header_buf = StringIO() header_gen = Generator(new_header_buf, policy=default_policy) header_gen.flatten(headers, unixfrom=False) return new_header_buf.getvalue().rstrip()"},{"question":"# Question: Module Path Extensions and Iteration **Objective**: This exercise assesses your ability to work with the `pkgutil` package, manipulate Python module paths, and retrieve module information dynamically. **Problem Statement**: You are provided with a package directory structure that may have its modules spread across several directories. Your task is to extend the search path for this package and retrieve all the submodules available within this extended path. This will involve using functions from the `pkgutil` module. **Requirements**: 1. Write a function `extend_and_list_submodules(package_name: str, additional_paths: List[str]) -> List[str]` which: - Extends the search path for the given `package_name` using the paths provided in `additional_paths`. - Retrieves and returns a list of all submodules available within the extended path of the package, including subpackages and modules within those subpackages. **Function Signature**: ```python from typing import List def extend_and_list_submodules(package_name: str, additional_paths: List[str]) -> List[str]: ``` **Input**: - `package_name`: A string representing the name of the package whose search path needs to be extended. - `additional_paths`: A list of strings, each representing an additional directory path to be added to the package\'s search path. **Output**: - A list of strings, each representing the fully qualified name of a submodule within the extended path set of the package. **Example**: ```python # Assuming there is a package \'mypackage\' and additional directories to add additional_paths = [\\"/path/to/dir1\\", \\"/path/to/dir2\\"] result = extend_and_list_submodules(\\"mypackage\\", additional_paths) print(result) ``` This example should print a list of all submodules available within the extended paths provided to the \\"mypackage\\" package. **Constraints**: - Assume the paths in `additional_paths` are valid directories. - Handle any import errors gracefully. - You may assume that `package_name` is a valid package name and exists. **Hint**: - Utilize the `pkgutil.extend_path` function to extend the package search path. - Use `pkgutil.walk_packages` to retrieve all submodules.","solution":"from pkgutil import extend_path, walk_packages from typing import List import importlib def extend_and_list_submodules(package_name: str, additional_paths: List[str]) -> List[str]: Extends the search path for the given package_name using the paths provided in additional_paths, and retrieves all submodules available within the extended path of the package. # Import the package package = importlib.import_module(package_name) # Extend the package\'s __path__ attribute with additional paths package.__path__ = list(extend_path(package.__path__, package.__name__)) package.__path__.extend(additional_paths) # List to hold names of all submodules submodules = [] # Walk through the extended package path and gather submodule names for importer, modname, ispkg in walk_packages(package.__path__, package.__name__ + \'.\'): submodules.append(modname) return submodules"},{"question":"# Question: Using PyTorch, implement a function `batch_addition` that performs element-wise addition on two input tensors. This function should ensure that the names of the dimensions align properly, broadcasting as necessary and ensuring correct name propagation. Function Signature: ```python def batch_addition(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Args: - tensor1 (torch.Tensor): The first input tensor with named dimensions. - tensor2 (torch.Tensor): The second input tensor with named dimensions. Returns: - torch.Tensor: A tensor resulting from the element-wise addition of tensor1 and tensor2, with appropriately unified dimension names. Raises: - ValueError: If the names cannot be aligned for the operation. ``` Constraints: 1. The function should be able to handle tensors of different shapes but must be broadcastable. 2. The dimensions of the tensors should be aligned from the right. 3. If the dimensions cannot be aligned (according to the name inference rules), the function should raise a `ValueError`. Example: ```python import torch # Example 1 tensor1 = torch.randn(3, 4, names=(\'N\', \'C\')) tensor2 = torch.randn(3, 4, names=(\'N\', \'C\')) result = batch_addition(tensor1, tensor2) print(result.names) # Should output (\'N\', \'C\') # Example 2 tensor1 = torch.randn(3, 1, names=(\'N\', \'C\')) tensor2 = torch.randn(3, 4, names=(\'N\', \'D\')) try: result = batch_addition(tensor1, tensor2) except ValueError as e: print(e) # Should raise an error due to name mismatch # Example 3 tensor1 = torch.randn(3, 3, 3, names=(\'N\', \'H\', \'W\')) tensor2 = torch.randn(1, 3, names=(None, \'W\')) result = batch_addition(tensor1, tensor2) print(result.names) # Should output (\'N\', \'H\', \'W\') ``` *Note*: Ensure you use PyTorch methods to perform name inference and alignment correctly. Documentation Reference: Refer to the documentation provided for details on \\"Unity Names from Inputs\\" and functions such as `torch.add`, `Tensor.broadcast_to`, etc., for implementing the function correctly.","solution":"import torch def batch_addition(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Performs element-wise addition on two input tensors, ensuring dimension names align properly, broadcasting as necessary and ensuring correct name propagation. Args: - tensor1 (torch.Tensor): The first input tensor with named dimensions. - tensor2 (torch.Tensor): The second input tensor with named dimensions. Returns: - torch.Tensor: A tensor resulting from the element-wise addition of tensor1 and tensor2, with appropriately unified dimension names. Raises: - ValueError: If the names cannot be aligned for the operation. try: result = tensor1 + tensor2 return result except RuntimeError as e: raise ValueError(\\"The dimensions cannot be aligned: \\" + str(e))"},{"question":"# Advanced Python Debugging and Profiling Assignment **Objective:** You are tasked with developing a Python script that performs detailed profiling and debugging on a provided code snippet. Your solution should demonstrate proficiency with the `pdb`, `cProfile`, and `tracemalloc` modules. **Task:** 1. **Debugging Part:** - Implement a function that finds the factorial of a number using recursion. This function will intentionally contain a mistake that causes a maximum recursion depth exceeded error for larger inputs. - Use the `pdb` module to debug the function and correct the mistake in a new, correctly implemented function. 2. **Profiling Part:** - Use the `cProfile` module to profile the performance of a provided computational function (e.g., a function that generates a large Fibonacci sequence). Include profiling both the faulty and corrected factorial functions as well. - Save the profiling statistics to a file. 3. **Memory Tracing Part:** - Use the `tracemalloc` module to trace memory allocations in the code. Generate snapshots of the memory usage before and after running the corrected factorial function and report the top memory-consuming line(s) of code. **Specifications:** - Implement the function `faulty_factorial(n: int) -> int` which contains an intentional bug. - Implement the function `correct_factorial(n: int) -> int` which corrects the bug from `faulty_factorial`. - Implement the function `profile_code()` which uses `cProfile` to profile and save statistics. - Implement the function `trace_memory()` which uses `tracemalloc` to trace memory usage. **Expected Input and Output Formats:** - Input for `faulty_factorial` and `correct_factorial`: - An integer `n` where `n >= 0`. - Output for `faulty_factorial` and `correct_factorial`: - An integer representing the factorial of `n`. - Profile statistics should be saved to a file named `profile_stats.txt`. - Memory tracing should output the top memory-consuming lines of code to the console. **Constraints:** - Ensure that the corrected factorial function handles up to `n = 1000` efficiently. - Your solution should handle recursion limit adjustments if necessary within the debugging process. **Example Code Snippet:** ```python def faulty_factorial(n): # Intentionally faulty implementation if n == 0: return 1 else: return n * faulty_factorial(n - 1) def correct_factorial(n): # Correct implementation to be filled in after debugging pass def profile_code(): # Use cProfile to profile computational functions pass def trace_memory(): # Use tracemalloc to trace memory usage pass # Example usage: # - Run profile_code() to profile the factorial functions and Fibonacci sequence generator. # - Run trace_memory() to get memory usage snapshots for the correct factorial function. ``` Deliver a fully functional script by completing the missing parts of the implementation. Make sure to include appropriate comments and document your debugging and profiling process.","solution":"import cProfile import traceback import tracemalloc def faulty_factorial(n): # Intentionally faulty implementation if n == 0: return 1 else: # the following line will cause maximum recursion depth error with large n return n * faulty_factorial(n - 1) def correct_factorial(n): # Correct implementation if n == 0 or n == 1: return 1 result = 1 for i in range(2, n + 1): result *= i return result def fibonacci(n): # Function to generate large Fibonacci sequence result = [] a, b = 0, 1 while len(result) < n: result.append(a) a, b = b, a + b return result def profile_code(): # Use cProfile to profile computational functions profiler = cProfile.Profile() # Profiling faulty factorial profiler.enable() try: faulty_factorial(10) except RecursionError: pass profiler.disable() profiler.dump_stats(\\"faulty_factorial_profile.prof\\") # Profiling correct factorial profiler.enable() correct_factorial(10) profiler.disable() profiler.dump_stats(\\"correct_factorial_profile.prof\\") # Profiling Fibonacci profiler.enable() fibonacci(1000) profiler.disable() profiler.dump_stats(\\"fibonacci_profile.prof\\") def trace_memory(): # Use tracemalloc to trace memory usage tracemalloc.start() # Capture memory usage before snapshot1 = tracemalloc.take_snapshot() # Run the correct factorial function correct_factorial(1000) # Capture memory usage after snapshot2 = tracemalloc.take_snapshot() # Compare memory usage top_stats = snapshot2.compare_to(snapshot1, \'lineno\') print(\\"[ Top 10 differences ]\\") for stat in top_stats[:10]: print(stat) # Example usage: # - Run profile_code() to profile the factorial functions and Fibonacci sequence generator. # - Run trace_memory() to get memory usage snapshots for the correct factorial function."},{"question":"# Multi-level Directory File Search with `fnmatch` Objective: Write a Python function that recursively searches through a given directory and its subdirectories, returning a list of all file paths that match a specified pattern. The search should be case-insensitive and make use of the `fnmatch` module. Function Signature: ```python def search_files(directory: str, pattern: str) -> list: Recursively searches through the given directory and its subdirectories, returning a list of all file paths that match the specified pattern. Parameters: directory (str): The root directory to start the search from. pattern (str): The shell-style wildcard pattern to match against file names. Returns: list: A list of file paths that match the specified pattern. ``` Input: - `directory`: A string representing the path to the directory where the search should start. - `pattern`: A string representing the shell-style wildcard pattern to match against file names. Output: - A list of strings, each representing a file path that matches the pattern. Constraints: - You must use the `fnmatch` module to match file names against the pattern. - The search should be case-insensitive; consider using `fnmatch.fnmatchcase`. - The directory tree may be deeply nested, so the solution should handle recursion efficiently. Examples: ```python # Example 1 # Assume the directory structure is as follows: # /root # /subdir1 # file1.txt # file2.py # /subdir2 # file3.txt # file4.py search_files(\'/root\', \'*.txt\') # Expected output: [\'/root/subdir1/file1.txt\', \'/root/subdir2/file3.txt\'] # Example 2 # Assume the directory structure is as follows: # /home # /user # script.sh # README.md # /data # report.doc # notes.txt search_files(\'/home\', \'*.*\') # Expected output: [\'/home/user/script.sh\', \'/home/user/README.md\', \'/home/data/report.doc\', \'/home/data/notes.txt\'] ``` Notes: - Make sure to handle cases where the directory does not exist or is empty by returning an empty list. - The returned file paths should be absolute.","solution":"import os import fnmatch def search_files(directory: str, pattern: str) -> list: Recursively searches through the given directory and its subdirectories, returning a list of all file paths that match the specified pattern. Parameters: directory (str): The root directory to start the search from. pattern (str): The shell-style wildcard pattern to match against file names. Returns: list: A list of file paths that match the specified pattern. matches = [] for root, dirnames, filenames in os.walk(directory): for filename in filenames: if fnmatch.fnmatchcase(filename.lower(), pattern.lower()): matches.append(os.path.join(root, filename)) return matches"},{"question":"**Objective:** You are required to demonstrate your understanding of the \\"uu\\" module by writing functions to encode and decode files while handling potential exceptions. **Task:** 1. Write a function `encode_file(input_file_path, output_file_path, backtick=False)` that: - Takes `input_file_path` and `output_file_path` as strings representing the paths of the input and output files respectively. - Optionally takes a `backtick` boolean parameter to represent zeros by \'`\' instead of spaces. - Encodes the content of `input_file_path` in uuencode format and writes it to `output_file_path`. - Should handle exceptions such as file not found and raise a custom exception `FileEncodingError` with an appropriate error message. 2. Write a function `decode_file(input_file_path, output_file_path)` that: - Takes `input_file_path` and `output_file_path` as strings representing the paths of the input and output files respectively. - Decodes the content of `input_file_path` from uuencode format and writes it to `output_file_path`. - Should handle exceptions such as file already exists and raise a custom exception `FileDecodingError` with an appropriate error message. **Constraints:** - You must use file-like objects within the `uu` module functions. - Assume standard file paths (e.g., no \'-\' for stdin/stdout). **Example Usage:** ```python try: encode_file(\'example.txt\', \'encoded.txt\') except FileEncodingError as e: print(f\\"Encoding Error: {e}\\") try: decode_file(\'encoded.txt\', \'decoded.txt\') except FileDecodingError as e: print(f\\"Decoding Error: {e}\\") ``` **Expected Input and Output:** - For encoding, the input is the content of `example.txt` and the output is the uuencoded content in `encoded.txt`. - For decoding, the input is the content of `encoded.txt` and the output is the original content in `decoded.txt`. **Custom Exceptions:** ```python class FileEncodingError(Exception): pass class FileDecodingError(Exception): pass ``` **Performance Requirements:** - The solution should handle small to moderately large files efficiently. **Submission:** Submit your implementation of the functions `encode_file` and `decode_file` along with the, custom exceptions `FileEncodingError` and `FileDecodingError`.","solution":"import uu import os class FileEncodingError(Exception): pass class FileDecodingError(Exception): pass def encode_file(input_file_path, output_file_path, backtick=False): try: if not os.path.exists(input_file_path): raise FileNotFoundError(f\\"The file {input_file_path} does not exist.\\") with open(input_file_path, \'rb\') as in_file, open(output_file_path, \'wb\') as out_file: uu.encode(in_file, out_file, name=os.path.basename(input_file_path), mode=0o644, backtick=backtick) except Exception as e: raise FileEncodingError(f\\"An error occurred while encoding the file: {e}\\") def decode_file(input_file_path, output_file_path): try: if os.path.exists(output_file_path): raise FileExistsError(f\\"The file {output_file_path} already exists.\\") with open(input_file_path, \'rb\') as in_file, open(output_file_path, \'wb\') as out_file: uu.decode(in_file, out_file) except Exception as e: raise FileDecodingError(f\\"An error occurred while decoding the file: {e}\\")"},{"question":"Coding Assessment Question # Objective To assess your understanding of the `sysconfig` module, you will be tasked with writing functions that fetch specific configuration details and manipulate installation paths. # Question Part 1: Retrieve Configuration Variables Write a function `get_compiler_configs` that returns the values of the \'CC\', \'CXX\', and \'AR\' configuration variables. # Input - None # Output - A dictionary with keys `\'CC\'`, `\'CXX\'`, and `\'AR\'`, and their corresponding values. # Example ```python result = get_compiler_configs() print(result) # Expected output: {\'CC\': \'gcc\', \'CXX\': \'g++\', \'AR\': \'ar\'} ``` Part 2: Determine Installation Paths Write a function `get_installation_paths` that returns all paths corresponding to the default scheme used in the current platform. # Input - None # Output - A dictionary containing all the installation paths for the default scheme. # Example ```python paths = get_installation_paths() print(paths) # Expected output may vary, but an example might be: # { # \'stdlib\': \'/usr/local/lib/python3.10\', # \'platstdlib\': \'/usr/local/lib/python3.10\', # \'purelib\': \'/usr/local/lib/python3.10/site-packages\', # \'platlib\': \'/usr/local/lib/python3.10/site-packages\', # \'include\': \'/usr/local/include/python3.10\', # \'scripts\': \'/usr/local/bin\', # \'data\': \'/usr/local\' # } ``` Part 3: Custom Path Based on Scheme Write a function `get_custom_path` that accepts a `path_name` and a `scheme`, and returns the corresponding installed path. Use the default dictionary of variables for path expansion. # Input - `path_name`: A string, one of the path names returned by `get_path_names`. - `scheme`: A string, one of the schemes returned by `get_scheme_names`. # Output - A string representing the corresponding installation path. # Example ```python custom_path = get_custom_path(\'stdlib\', \'posix_prefix\') print(custom_path) # Expected output might be: \'/usr/local/lib/python3.10\' ``` Constraints: 1. If any provided scheme or path name does not exist, raise a `KeyError`. 2. The functions should handle any platform (`posix`, `nt`, `macOS`). # Performance Requirement - Your solution should efficiently retrieve data using the built-in `sysconfig` functions. ```python import sysconfig def get_compiler_configs(): # Implement this function pass def get_installation_paths(): # Implement this function pass def get_custom_path(path_name, scheme): # Implement this function pass ``` Solve the above problems with detailed comments explaining each step.","solution":"import sysconfig def get_compiler_configs(): Returns a dictionary with configuration values for \'CC\', \'CXX\', and \'AR\'. configs = {} configs[\'CC\'] = sysconfig.get_config_var(\'CC\') configs[\'CXX\'] = sysconfig.get_config_var(\'CXX\') configs[\'AR\'] = sysconfig.get_config_var(\'AR\') return configs def get_installation_paths(): Returns a dictionary with all installation paths for the default scheme. scheme = sysconfig.get_default_scheme() paths = {} for path_name in sysconfig.get_path_names(): paths[path_name] = sysconfig.get_path(path_name, scheme) return paths def get_custom_path(path_name, scheme): Returns the installed path for the given path_name and scheme. # Validate path_name if path_name not in sysconfig.get_path_names(): raise KeyError(f\\"Invalid path_name: {path_name}\\") # Validate scheme if scheme not in sysconfig.get_scheme_names(): raise KeyError(f\\"Invalid scheme: {scheme}\\") return sysconfig.get_path(path_name, scheme)"},{"question":"# Question: Create and Parse a MIME Email with Attachments In this task, you will create a function that constructs and sends a MIME email with multiple attachments and then parses the same email to extract specific details. Your function should demonstrate a comprehensive understanding of the `email` package functionalities. Function: `send_and_parse_mime_email` You need to implement a function called `send_and_parse_mime_email` that does the following: 1. Creates a MIME email message with a given subject and body. 2. Adds specified attachments to the email. 3. Writes the email to a file. 4. Reads the email back from the file. 5. Parses the email to extract the subject, sender, recipient(s), and names of attachments. # Function Signature ```python def send_and_parse_mime_email(sender: str, recipients: list, subject: str, body: str, attach_filenames: list, email_file: str) -> dict: ``` # Input - `sender` (str): The sender\'s email address. - `recipients` (list): A list of recipient email addresses. - `subject` (str): The subject of the email. - `body` (str): The plain text body of the email. - `attach_filenames` (list): A list of filenames to be attached to the email (the files are located in the current working directory). - `email_file` (str): The filename to which the email should be saved. # Output - A dictionary containing: - `subject`: the subject of the email. - `sender`: the sender\'s email address. - `recipients`: a list of recipient email addresses. - `attachments`: a list of filenames of the attachments. # Example ```python sender = \\"me@example.com\\" recipients = [\\"friend@example.com\\"] subject = \\"Test Email\\" body = \\"Hi, this is a test email.\\" attach_filenames = [\\"file1.txt\\", \\"file2.jpg\\"] email_file = \\"email.eml\\" result = send_and_parse_mime_email(sender, recipients, subject, body, attach_filenames, email_file) print(result) ``` Expected output: ```python { \\"subject\\": \\"Test Email\\", \\"sender\\": \\"me@example.com\\", \\"recipients\\": [\\"friend@example.com\\"], \\"attachments\\": [\\"file1.txt\\", \\"file2.jpg\\"] } ``` # Constraints - Assume that the files specified in `attach_filenames` exist in the current directory and are accessible. - You may use any standard Python libraries listed in the documentation above. # Notes 1. The function should handle different types of files and correctly associate them with their MIME types. 2. You do not actually send the email; you are only writing it to a file and reading it back for parsing. Happy coding!","solution":"import email import os from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.mime.base import MIMEBase from email import encoders def send_and_parse_mime_email(sender: str, recipients: list, subject: str, body: str, attach_filenames: list, email_file: str) -> dict: # Create the MIME email message msg = MIMEMultipart() msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) msg[\'Subject\'] = subject # Attach the email body msg.attach(MIMEText(body, \'plain\')) # Attach the files for filename in attach_filenames: with open(filename, \'rb\') as f: part = MIMEBase(\'application\', \'octet-stream\') part.set_payload(f.read()) encoders.encode_base64(part) part.add_header(\'Content-Disposition\', f\'attachment; filename={os.path.basename(filename)}\') msg.attach(part) # Write the email to a file with open(email_file, \'w\') as f: f.write(msg.as_string()) # Read the email from the file with open(email_file, \'r\') as f: parsed_msg = email.message_from_file(f) # Extract the details from the email parsed_subject = parsed_msg[\'Subject\'] parsed_sender = parsed_msg[\'From\'] parsed_recipients = parsed_msg[\'To\'].split(\', \') # Extract attachment filenames parsed_attachments = [] for part in parsed_msg.walk(): if part.get_content_maintype() == \'multipart\': continue if part.get(\'Content-Disposition\') is None: continue parsed_attachments.append(part.get_filename()) return { \'subject\': parsed_subject, \'sender\': parsed_sender, \'recipients\': parsed_recipients, \'attachments\': parsed_attachments }"},{"question":"# Directory Comparison Tool **Objective:** Create a function `compare_directories(dir1: str, dir2: str) -> Tuple[List[str], List[str], List[str]]` that compares two directories and returns a tuple of three lists: 1. **Matching files:** a list of filenames that match in both directories. 2. **Mismatched files:** a list of filenames that exist in both but have different contents or properties. 3. **Error files:** a list of filenames that could not be compared (e.g., due to missing files or lack of permissions). Your implementation should use both the `filecmp.cmpfiles` function and the `filecmp.dircmp` class to demonstrate your understanding of the module\'s functionality. Input: - `dir1` (str): Path to the first directory. - `dir2` (str): Path to the second directory. Output: - Returns a tuple of three lists: `(match, mismatch, errors)`. Constraints: - Use both `filecmp.cmpfiles` and `filecmp.dircmp` in your solution. - Assume directory paths are valid and accessible. - Assume the directories contain only files and subdirectories with read permissions. Example: ```python import os # Assume you have the following file structure # dir1/ # â”œâ”€â”€ a.txt (content: \'Hello\') # â”œâ”€â”€ b.txt (content: \'World\') # â””â”€â”€ subdir1/ # â”œâ”€â”€ c.txt (content: \'Python\') # dir2/ # â”œâ”€â”€ a.txt (content: \'Hello\') # â”œâ”€â”€ b.txt (content: \'Different content\') # â””â”€â”€ subdir1/ # â”œâ”€â”€ c.txt (content: \'Python\') compare_directories(\'dir1\', \'dir2\') ``` Expected output: ```python ([\'a.txt\', \'subdir1/c.txt\'], [\'b.txt\'], []) ``` In this example, `a.txt` and `subdir1/c.txt` are the matching files, `b.txt` is a mismatched file, and there are no error files. **Note**: Your function should handle nested directories and display comparisons for files at all levels.","solution":"import os import filecmp from typing import List, Tuple def compare_directories(dir1: str, dir2: str) -> Tuple[List[str], List[str], List[str]]: Compares two directories and returns a tuple of three lists: - Matching files - Mismatched files - Error files match = [] mismatch = [] errors = [] # Use filecmp.dircmp to compare the directories def compare_helper(dcmp): # Use filecmp.cmpfiles to compare files in the current directories current_match, current_mismatch, current_errors = filecmp.cmpfiles(dcmp.left, dcmp.right, dcmp.common_files) match.extend([os.path.relpath(os.path.join(dcmp.left, file), dir1) for file in current_match]) mismatch.extend([os.path.relpath(os.path.join(dcmp.left, file), dir1) for file in current_mismatch]) errors.extend([os.path.relpath(os.path.join(dcmp.left, file), dir1) for file in current_errors]) for sub_dcmp in dcmp.subdirs.values(): compare_helper(sub_dcmp) dcmp = filecmp.dircmp(dir1, dir2) compare_helper(dcmp) return match, mismatch, errors"},{"question":"Objective Implement and manage instance methods and method objects in a custom Python class to demonstrate the understanding of the provided Python C-API functionalities. Problem Statement You are required to create a class `CustomClass` in Python. This class should demonstrate the following: 1. **Instance Method Creation**: - Implement a class method `create_instance_method` that wraps a given function as an instance method of the class. 2. **Method Object Management**: - Implement a class method `create_method_object` that creates a method object given a function and an instance of `CustomClass`. - Implement two methods to retrieve the function and the instance associated with the created method object. Class Specification - `CustomClass` should have a class method `create_instance_method(func)`, which returns a new instance method object wrapping the provided `func`. - `CustomClass` should have a class method `create_method_object(func, instance)`, which returns a new method object with the provided `func` and `instance`. - `CustomClass` should have methods `get_method_function(method_obj)` and `get_method_instance(method_obj)` to retrieve the function and the instance associated with a method object, respectively. Example Usage ```python class CustomClass: @classmethod def create_instance_method(cls, func): # Your code here pass @classmethod def create_method_object(cls, func, instance): # Your code here pass @staticmethod def get_method_function(method_obj): # Your code here pass @staticmethod def get_method_instance(method_obj): # Your code here pass def example_function(self): return f\\"Called example_function from {self}\\" # Example instantiation and usage instance = CustomClass() instance_method_obj = CustomClass.create_instance_method(example_function) method_obj = CustomClass.create_method_object(example_function, instance) assert CustomClass.get_method_function(method_obj) == example_function assert CustomClass.get_method_instance(method_obj) == instance ``` Input and Output Format - *Input*: Functions and instances of the `CustomClass`. - *Output*: New instance method objects or method objects, and the ability to retrieve the associated function or instance from method objects. Constraints - You cannot use any external library; only standard Python libraries and basic functionality are allowed. - Ensure proper handling and raising of exceptions where necessary. - Your methods should be efficient and handle edge cases effectively. Submission - Implement the `CustomClass` with all required methods. - Write test cases to demonstrate that your implementation works as intended.","solution":"class CustomClass: @classmethod def create_instance_method(cls, func): Create an instance method from the given function and return it. # Using types.MethodType to bind function as an instance method import types return types.MethodType(func, cls) @classmethod def create_method_object(cls, func, instance): Create a method object with the provided function and instance. import types return types.MethodType(func, instance) @staticmethod def get_method_function(method_obj): Get the function associated with a method object. return method_obj.__func__ @staticmethod def get_method_instance(method_obj): Get the instance associated with a method object. return method_obj.__self__"},{"question":"**Distributed Training with RPC and Autograd in PyTorch** **Objective:** Implement a distributed training workflow using PyTorch\'s distributed RPC framework and autograd mechanics. The solution should demonstrate an understanding of remote procedure calls, distributed autograd context management, and optimizer steps. **Description:** You are given a simple neural network model implementing a linear regression task. The goal is to partition this model across two nodes and perform a distributed forward and backward pass using PyTorch\'s RPC framework. Additionally, you will use a distributed optimizer to update the model parameters. **Tasks:** 1. Initialize the RPC framework across two nodes. 2. Implement the forward pass of the model using remote procedure calls (RPCs) to utilize both nodes. 3. Set up a distributed autograd context to track gradients. 4. Perform a backward pass to compute gradients. 5. Use a distributed optimizer to update the model\'s parameters across both nodes. **Setup:** - Node 0: Responsible for data input and final loss calculation. - Node 1: Part of the forward pass computation. ```python import torch import torch.distributed.rpc as rpc import torch.distributed.autograd as dist_autograd from torch import nn, optim from torch.distributed.optim import DistributedOptimizer # Define a simple model class SimpleModelPart1(nn.Module): def __init__(self): super(SimpleModelPart1, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) class SimpleModelPart2(nn.Module): def __init__(self): super(SimpleModelPart2, self).__init__() self.linear = nn.Linear(5, 1) def forward(self, x): return self.linear(x) def run_worker(rank, world_size): rpc.init_rpc(f\'worker{rank}\', rank=rank, world_size=world_size) if rank == 0: # Implement data input, forward pass, loss calculation, backward pass, and optimizer step here. pass elif rank == 1: # Implement remote model part computation here. pass rpc.shutdown() if __name__ == \\"__main__\\": world_size = 2 torch.multiprocessing.spawn(run_worker, args=(world_size,), nprocs=world_size) ``` **Instructions:** 1. Complete the `run_worker` function to initialize the RPC framework and assign tasks to each node. 2. Implement the distributed forward pass using RPC. 3. Use `dist_autograd.context()` to track the gradients during the forward pass. 4. Perform the backward pass using `dist_autograd.backward()`. 5. Update the model parameters using `DistributedOptimizer`. **Constraints:** - Ensure that all RPCs and autograd contexts are properly managed and released. - The code should handle multiple forward and backward passes in a training loop. **Input Format:** N/A **Output Format:** N/A You are expected to fill in the missing parts of the code to complete the distributed training workflow.","solution":"import torch import torch.distributed.rpc as rpc import torch.distributed.autograd as dist_autograd from torch import nn, optim from torch.distributed.optim import DistributedOptimizer from torch.distributed.rpc.api import RRef # Define a simple model class SimpleModelPart1(nn.Module): def __init__(self): super(SimpleModelPart1, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) class SimpleModelPart2(nn.Module): def __init__(self): super(SimpleModelPart2, self).__init__() self.linear = nn.Linear(5, 1) def forward(self, x): return self.linear(x) def remote_forward(model_rref, x): model = model_rref.local_value() return model(x) def run_worker(rank, world_size): rpc.init_rpc(f\'worker{rank}\', rank=rank, world_size=world_size) if rank == 0: # Node 0 part: Responsible for data input and final loss calculation model_part1 = SimpleModelPart1().to(rank) model_part2_rref = rpc.remote(\\"worker1\\", SimpleModelPart2) # Example inputs and targets for linear regression task data = torch.randn(20, 10) target = torch.randn(20, 1) # Optimizer dist_optimizer = DistributedOptimizer( optim.SGD, params=[ {\\"params\\": model_part1.parameters()}, {\\"params\\": model_part2_rref.rpc_sync().parameters()} ] ) for epoch in range(5): # Training loop with dist_autograd.context() as context_id: output_part1 = model_part1(data) output = rpc.rpc_sync(\\"worker1\\", remote_forward, args=(model_part2_rref, output_part1)) loss = nn.MSELoss()(output, target) dist_autograd.backward(context_id, [loss]) dist_optimizer.step(context_id) print(f\\"Epoch {epoch}, Loss: {loss.item()}\\") elif rank == 1: # Node 1 part: Part of the forward pass computation model_part2 = SimpleModelPart2().to(rank) rpc.RRef(model_part2) rpc.shutdown() if __name__ == \\"__main__\\": world_size = 2 torch.multiprocessing.spawn(run_worker, args=(world_size,), nprocs=world_size)"},{"question":"# Problem Description You are tasked with creating a dynamic class that models a simple banking system using the `types` module. The banking system should support the creation and management of bank account objects with functionalities such as depositing money, withdrawing money, and checking the balance, while ensuring thread-safety (i.e., being safe to use in multithreaded programs). # Requirements 1. **Class Creation**: - Dynamically create a class called `BankAccount` using `types.new_class`. - The class should have an initial balance, stored as a private attribute `_balance`. - Include methods `deposit(amount)`, `withdraw(amount)`, and `get_balance()` within the class. 2. **Thread-Safety**: - Ensure thread-safety for deposit and withdrawal operations using proper synchronization mechanisms from the `threading` module. Use a lock to synchronize these operations. 3. **Dynamic Class Preparation**: - Use `types.prepare_class` to set up the class namespace and metaclass before creation. # Input and Output - **Initialization**: - `BankAccount` objects should be initialized with an optional starting balance `initial_balance` (default is 0). - **Methods**: - `deposit(amount)`: Adds the specified `amount` to the balance. - `withdraw(amount)`: Subtracts the specified `amount` from the balance if sufficient funds are available. - `get_balance()`: Returns the current balance of the account. - **Thread-Safety**: - Use a `threading.Lock` object to ensure that balance updates are thread-safe. # Function Signature ```python from types import new_class, prepare_class import threading def create_bank_account_class(): # Implement the function to create the dynamic class pass # Example usage: BankAccount = create_bank_account_class() account = BankAccount(100) account.deposit(50) account.withdraw(30) print(account.get_balance()) # Output: 120 ``` # Constraints - Ensure the deposit and withdraw methods are thread-safe. - Raise appropriate exceptions for invalid operations (e.g., withdrawing more than the available balance). # Example ```python BankAccount = create_bank_account_class() account1 = BankAccount() account2 = BankAccount(200) account1.deposit(100) account1.withdraw(50) print(account1.get_balance()) # Output: 50 account2.withdraw(100) account2.deposit(50) print(account2.get_balance()) # Output: 150 ``` Write the function `create_bank_account_class()` to meet the above specifications and constraints.","solution":"from types import new_class, prepare_class import threading def create_bank_account_class(): metaclass, ns, kwds = prepare_class(\'BankAccount\') cls = new_class(\'BankAccount\', (), {}, lambda ns: ns.update(metaclass=metaclass)) def __init__(self, initial_balance=0): self._balance = initial_balance self._lock = threading.Lock() def deposit(self, amount): with self._lock: if amount < 0: raise ValueError(\\"Deposit amount must be positive\\") self._balance += amount def withdraw(self, amount): with self._lock: if amount < 0: raise ValueError(\\"Withdraw amount must be positive\\") if amount > self._balance: raise ValueError(\\"Insufficient funds\\") self._balance -= amount def get_balance(self): with self._lock: return self._balance cls.__init__ = __init__ cls.deposit = deposit cls.withdraw = withdraw cls.get_balance = get_balance return cls # Example usage: BankAccount = create_bank_account_class() account = BankAccount(100) account.deposit(50) account.withdraw(30) print(account.get_balance()) # Output: 120"},{"question":"# Question: IP Address Utility Functions You are required to implement a Python class named `IPAddressUtility` that provides several utility methods to work with IP addresses and networks, using Python\'s `ipaddress` module. This will demonstrate your understanding of creating and manipulating IP address and network objects, as well as iteration and extraction of information. Class: `IPAddressUtility` # Methods: 1. `get_ip_version(ip_str: str) -> int`: - **Input**: A string representing an IP address. - **Output**: An integer representing the IP version (4 or 6). - **Example**: ```python IPAddressUtility.get_ip_version(\'192.0.2.1\') # returns 4 IPAddressUtility.get_ip_version(\'2001:db8::1\') # returns 6 ``` 2. `is_valid_ip(ip_str: str) -> bool`: - **Input**: A string representing an IP address. - **Output**: A boolean indicating whether the given string is a valid IP address. - **Example**: ```python IPAddressUtility.is_valid_ip(\'192.0.2.1\') # returns True IPAddressUtility.is_valid_ip(\'300.0.2.1\') # returns False ``` 3. `get_network_from_interface(interface_str: str) -> str`: - **Input**: A string representing an IP interface (e.g., \'192.0.2.1/24\'). - **Output**: A string representing the network corresponding to the given interface. - **Example**: ```python IPAddressUtility.get_network_from_interface(\'192.0.2.1/24\') # returns \'192.0.2.0/24\' ``` 4. `get_num_addresses_in_network(network_str: str) -> int`: - **Input**: A string representing an IP network (e.g., \'192.0.2.0/24\'). - **Output**: An integer representing the number of individual addresses in the given network. - **Example**: ```python IPAddressUtility.get_num_addresses_in_network(\'192.0.2.0/24\') # returns 256 ``` 5. `list_usable_ips(network_str: str) -> list`: - **Input**: A string representing an IP network. - **Output**: A list of strings, each representing a usable IP address in the given network. - **Example**: ```python IPAddressUtility.list_usable_ips(\'192.0.2.0/24\') # returns [\'192.0.2.1\', \'192.0.2.2\', ..., \'192.0.2.254\'] ``` # Constraints: - Do not use any external libraries other than the standard `ipaddress` module. - Ensure to handle any potential errors or exceptional cases gracefully. Please implement the class `IPAddressUtility` and its methods as described.","solution":"import ipaddress class IPAddressUtility: @staticmethod def get_ip_version(ip_str: str) -> int: ip = ipaddress.ip_address(ip_str) return ip.version @staticmethod def is_valid_ip(ip_str: str) -> bool: try: ipaddress.ip_address(ip_str) return True except ValueError: return False @staticmethod def get_network_from_interface(interface_str: str) -> str: interface = ipaddress.ip_interface(interface_str) return str(interface.network) @staticmethod def get_num_addresses_in_network(network_str: str) -> int: network = ipaddress.ip_network(network_str, strict=False) return network.num_addresses @staticmethod def list_usable_ips(network_str: str) -> list: network = ipaddress.ip_network(network_str, strict=False) return [str(ip) for ip in network.hosts()] # network.hosts() generates only usable IPs (excluding network and broadcast addresses)"},{"question":"**Objective:** Demonstrate your understanding of Python\'s data model, type hierarchy, object customization, and special methods by implementing a custom class that emulates a built-in type with enhanced functionalities. **Problem Statement:** Implement a custom class `AdvancedList` that emulates Python\'s built-in `list` to provide additional functionalities. The `AdvancedList` class should support typical list operations as well as custom behavior for specific operations. # Requirements: 1. **Initialization:** - The class should be initialized with an iterable. If no iterable is provided, an empty `AdvancedList` should be created. - Example: `adv_list = AdvancedList([1, 2, 3])` or `adv_list = AdvancedList()` 2. **Special Methods:** - Implement the following special methods to emulate list behaviors: - `__getitem__()` - `__setitem__()` - `__delitem__()` - `__len__()` - `__contains__()` - `__iter__()` - `__repr__()` - `__str__()` 3. **Additional Custom Behavior:** - Implement a method `append_unique(value)` that appends a value to the list only if it is not already present. - Implement a method `copy_reverse()` that returns a new `AdvancedList` which is a reversed copy of the original list. - Implement a method `find(value)` that returns the index of the value in the list, or `-1` if the value is not found. 4. **Performance:** - Your implementation should ensure that common list operations (indexing, addition, deletion, etc.) are efficient. # Input and Output: 1. **Initialization:** - Input: An iterable (optional). - Output: An instance of `AdvancedList`. 2. **Special Methods:** - `__getitem__(index)`: - Input: `index` (an integer or slice). - Output: The value at the specified index or a sub-list for slices. - `__setitem__(index, value)`: - Input: `index` (an integer or slice), `value` (the new value or iterable for slice assignment). - `__delitem__(index)`: - Input: `index` (an integer or slice). - `__len__()`: - Output: The number of elements in the `AdvancedList`. - `__contains__(value)`: - Input: `value` (an element to check for membership). - Output: `True` if `value` is in `AdvancedList`, else `False`. - `__iter__()`: - Output: An iterator over the elements. - `__repr__()`, `__str__()`: - Output: A string representation of the `AdvancedList`. 3. **Additional Methods:** - `append_unique(value)`: - Input: `value` (element to append). - `copy_reverse()`: - Output: A new `AdvancedList` with elements in reverse order. - `find(value)`: - Input: `value` (element to find). - Output: Index of `value` or `-1`. # Constraints: - The class should only store unique elements. - Ensure that indexing, appending, and length operations are efficient. **Example Usage:** ```python adv_list = AdvancedList([1, 2, 3]) adv_list.append_unique(3) # No change as 3 is already present adv_list.append_unique(4) # Adds 4 to the list print(adv_list) # Output: AdvancedList([1, 2, 3, 4]) reversed_list = adv_list.copy_reverse() print(reversed_list) # Output: AdvancedList([4, 3, 2, 1]) index = adv_list.find(2) print(index) # Output: 1 print(len(adv_list)) # Output: 4 print(2 in adv_list) # Output: True ``` Implement the `AdvancedList` class according to the above specifications.","solution":"class AdvancedList: def __init__(self, iterable=None): if iterable is None: self._data = [] else: self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __len__(self): return len(self._data) def __contains__(self, value): return value in self._data def __iter__(self): return iter(self._data) def __repr__(self): return f\\"AdvancedList({self._data})\\" def __str__(self): return str(self._data) def append_unique(self, value): if value not in self._data: self._data.append(value) def copy_reverse(self): return AdvancedList(self._data[::-1]) def find(self, value): try: return self._data.index(value) except ValueError: return -1"},{"question":"Objective: You are tasked with analyzing the GPU performance of a PyTorch model using TorchInductor\'s profiling tools. You should identify the most time-consuming Triton kernel and suggest potential optimizations to improve the model\'s overall performance. Problem Statement: 1. **Environment Setup**: Set the following environment variables to aid in your profiling: - `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES` as 1 - `TORCHINDUCTOR_BENCHMARK_KERNEL` as 1 2. **Profile the Model**: You are given a pre-trained model from `timm` library, specifically `mixnet_l`. Use the provided script `benchmarks/dynamo/timm_models.py` to execute the model on the backend with inductor enabled. Here\'s the command to run: ```bash TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1 python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only mixnet_l --disable-cudagraphs --training ``` 3. **Identify the Most Time-Consuming Kernel**: After running the benchmark, parse the output log to identify the most time-consuming Triton kernel. 4. **Kernel Performance Analysis**: Locate and analyze the identified kernel. Use the information provided in the log to determine its execution time and bandwidth. 5. **Suggest Optimizations**: Suggest at least one optimization technique to potentially improve the performance of the identified kernel. For example, consider whether enabling `TORCHINDUCTOR_MAX_AUTOTUNE` or adding heuristics could benefit the kernel\'s runtime. Expected Output: 1. Print the name of the most time-consuming Triton kernel. 2. Print the percentage of GPU time taken by this kernel. 3. Provide a short explanation of the identified kernel\'s function and its impact on the model\'s performance. 4. Suggest and justify at least one optimization approach. Constraints: - Ensure all environment variables are set appropriately before profiling. - Use the specified `mixnet_l` model for this analysis. - Follow all provided steps and commands accurately to achieve the expected results. Submission: Submit a Python script that accomplishes the above tasks. Ensure your script includes comments and explanations for each step.","solution":"import os import subprocess import re def run_inductor_benchmark(): # Set environment variables os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' # Command to run the benchmark command = [ \'python\', \'-u\', \'benchmarks/dynamo/timm_models.py\', \'--backend\', \'inductor\', \'--amp\', \'--performance\', \'--dashboard\', \'--only\', \'mixnet_l\', \'--disable-cudagraphs\', \'--training\' ] # Run the command and capture output result = subprocess.run(command, capture_output=True, text=True) return result.stdout def parse_output(output): # Regular expression to find kernel times kernel_pattern = re.compile(r\\"(?P<name>triton_d+):s+(?P<time>d+.d+)%\\") kernel_times = {} for match in kernel_pattern.finditer(output): name = match.group(\'name\') time = float(match.group(\'time\')) kernel_times[name] = time if kernel_times: # Find the most time-consuming kernel most_time_consuming_kernel = max(kernel_times, key=kernel_times.get) return most_time_consuming_kernel, kernel_times[most_time_consuming_kernel] else: return None, None def main(): # Run the benchmark output = run_inductor_benchmark() # Parse the output to identify the most time-consuming kernel most_time_consuming_kernel, gpu_time_percentage = parse_output(output) if most_time_consuming_kernel: print(f\\"Most time-consuming Triton kernel: {most_time_consuming_kernel}\\") print(f\\"Percentage of GPU time: {gpu_time_percentage}%\\") # Explanation of the kernel\'s function and its impact kernel_explanation = ( \\"The kernel identified is responsible for a significant portion of the GPU time. \\" \\"Its function typically involves compute-heavy operations such as matrix multiplications, \\" \\"convolutions, or other such operations critical to the model\'s performance.\\" ) print(kernel_explanation) # Suggesting optimization optimization_suggestion = ( \\"One optimization approach could be to enable `TORCHINDUCTOR_MAX_AUTOTUNE=1`. \\" \\"It allows the profiler to choose the best configuration for performance. Additionally, \\" \\"consider breaking down the operation into smaller parts to enable better usage of GPU resources.\\" ) print(optimization_suggestion) else: print(\\"No Triton kernel times found in the output.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Argument Parsing with sys.argv **Objective**: Create a Python script that processes command-line arguments and performs simple operations based on those arguments. **Task**: Write a Python function named `process_arguments` that accepts a list of arguments and performs different operations based on the provided arguments. **Function Signature**: ```python def process_arguments(args: list) -> str: ``` # Requirements: - The script must handle the following operations: - If the argument ` \\"--sum\\" ` is provided, calculate the sum of all subsequent numeric arguments. - If the argument ` \\"--average\\" ` is provided, calculate the average of all subsequent numeric arguments. - If the argument ` \\"--max\\" ` is provided, find the maximum value among all subsequent numeric arguments. - If the argument ` \\"--min\\" ` is provided, find the minimum value among all subsequent numeric arguments. # Input: - `args`: A list of strings representing the command-line arguments. # Output: - A string that specifies the result of the operation. # Constraints: - If no recognizable argument is given or the numerical arguments are missing for a provided operation, the function should return a message \\"Invalid Arguments\\". # Examples: ```python # Example 1 result = process_arguments([\\"--sum\\", \\"3\\", \\"5\\", \\"7\\"]) print(result) # Output: \\"Sum: 15\\" # Example 2 result = process_arguments([\\"--average\\", \\"3\\", \\"5\\", \\"7\\"]) print(result) # Output: \\"Average: 5.0\\" # Example 3 result = process_arguments([\\"--max\\", \\"1\\", \\"3\\", \\"2\\", \\"8\\", \\"4\\"]) print(result) # Output: \\"Max: 8\\" # Example 4 result = process_arguments([\\"--min\\", \\"3\\", \\"5\\", \\"7\\"]) print(result) # Output: \\"Min: 3\\" # Example 5 result = process_arguments([\\"--sum\\", \\"a\\", \\"3\\"]) print(result) # Output: \\"Invalid Arguments\\" ``` **Notes**: - You can assume the arguments are provided correctly formatted as strings. - You should handle cases where arguments might not be valid numbers gracefully. - You should handle cases where the operation argument is not provided correctly.","solution":"def process_arguments(args: list) -> str: if not args or args[0] not in [\\"--sum\\", \\"--average\\", \\"--max\\", \\"--min\\"]: return \\"Invalid Arguments\\" operation = args[0] try: numbers = [float(arg) for arg in args[1:]] except ValueError: return \\"Invalid Arguments\\" if not numbers: return \\"Invalid Arguments\\" if operation == \\"--sum\\": result = sum(numbers) return f\\"Sum: {result}\\" if operation == \\"--average\\": result = sum(numbers) / len(numbers) return f\\"Average: {result}\\" if operation == \\"--max\\": result = max(numbers) return f\\"Max: {result}\\" if operation == \\"--min\\": result = min(numbers) return f\\"Min: {result}\\" return \\"Invalid Arguments\\""},{"question":"# Permutation Feature Importance Assessment Objective To assess your understanding of permutation feature importance in scikit-learn, you need to implement the technique and analyze the importance of different features in a given dataset. Problem Statement You are provided with the `California Housing Dataset` (a regression problem) from scikit-learn. Your task is to: 1. Train a regression model on this dataset. 2. Compute the permutation feature importance of the trained model using `mean_absolute_error` as the scoring metric. Requirements 1. **Model**: Use any regression model from scikit-learn. 2. **Metric**: `mean_absolute_error`. 3. **Feature Importance Calculation**: Perform permutation importance calculation with `n_repeats=50`. 4. **Output**: Print each feature\'s name along with its mean importance and standard deviation. 5. **Performance**: Ensure your implementation is efficient and leverages scikit-learn\'s built-in functionality. Input You don\'t need to handle any input. The dataset and model parameters are defined within the code. Output Print the permutation importance of each feature in the format: `feature_name: mean_importance +/- std_importance`. Code Template ```python import numpy as np from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance from sklearn.metrics import mean_absolute_error # Load the dataset data = fetch_california_housing() X, y = data.data, data.target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) # Train the model model = Ridge(alpha=1.0) model.fit(X_train, y_train) # Calculate the model performance mae = mean_absolute_error(y_test, model.predict(X_test)) print(f\\"Model performance (MAE): {mae:.4f}\\") # Compute permutation feature importance importance = permutation_importance( model, X_test, y_test, scoring=\'neg_mean_absolute_error\', n_repeats=50, random_state=42 ) # Print feature importance feature_names = data.feature_names for i in importance.importances_mean.argsort()[::-1]: if importance.importances_mean[i] - 2 * importance.importances_std[i] > 0: print(f\\"{feature_names[i]}: {importance.importances_mean[i]:.3f} +/- {importance.importances_std[i]:.3f}\\") ``` Constraints 1. Use `random_state=42` for reproducibility. 2. Ensure `n_repeats=50` for the permutation importance calculation. 3. The model should not be overfit; evaluate on a held-out test set. Good luck!","solution":"import numpy as np from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance from sklearn.metrics import mean_absolute_error def calculate_permutation_feature_importance(): # Load the dataset data = fetch_california_housing() X, y = data.data, data.target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) # Train the model model = Ridge(alpha=1.0) model.fit(X_train, y_train) # Calculate the model performance mae = mean_absolute_error(y_test, model.predict(X_test)) print(f\\"Model performance (MAE): {mae:.4f}\\") # Compute permutation feature importance importance = permutation_importance( model, X_test, y_test, scoring=\'neg_mean_absolute_error\', n_repeats=50, random_state=42 ) # Create a dictionary to hold feature importances feature_importances = {} # Print feature importance feature_names = data.feature_names for i in importance.importances_mean.argsort()[::-1]: if importance.importances_mean[i] - 2 * importance.importances_std[i] > 0: feature_importances[feature_names[i]] = (importance.importances_mean[i], importance.importances_std[i]) print(f\\"{feature_names[i]}: {importance.importances_mean[i]:.3f} +/- {importance.importances_std[i]:.3f}\\") return feature_importances"},{"question":"You are tasked with writing a Python function to manage email messages using the \\"email\\" package. Your function should perform the following actions: 1. **Parse an email from a given byte stream** containing a serialized email message. 2. **Extract and print the subject** of the email. 3. **Add an attachment** to the email, which will be provided as a file path. 4. **Generate a new email** byte stream from the modified `EmailMessage` object. # Input: - `byte_stream` (bytes): A byte stream containing the serialized version of an email message. - `attachment_path` (str): A file path to the attachment you need to add to the email. # Output: - The function should print the subject of the email to the console. - The function should return the modified email as a new byte stream (bytes). # Constraints: - The email message can be simple (text only) or complex (multipart with attachments). - The attachment can be any type of file (text, image, etc.). - The function should handle exceptions gracefully, ensure RFC compliance, and maintain MIME type integrity. # Performance Requirements: - The function should be able to handle reasonably large emails and attachments efficiently. - Ensure that the memory footprint is managed appropriately, avoiding any large memory leaks. # Function signature: ```python def manage_email(byte_stream: bytes, attachment_path: str) -> bytes: pass ``` # Example: ```python byte_stream = b\\"...\\" # A valid serialized email byte stream attachment_path = \\"/path/to/attachment.txt\\" print(manage_email(byte_stream, attachment_path)) # The function will print: \\"Subject: ...\\" and return a new email byte stream with the attachment. ``` Your task is to implement the `manage_email` function as described above.","solution":"import email from email import policy from email.parser import BytesParser from email.message import EmailMessage from email.mime.base import MIMEBase from email.encoders import encode_base64 import os def manage_email(byte_stream: bytes, attachment_path: str) -> bytes: # Parse the email from the byte stream msg = BytesParser(policy=policy.default).parsebytes(byte_stream) # Extract and print the subject subject = msg[\'subject\'] print(f\'Subject: {subject}\') # Read the attachment file content with open(attachment_path, \'rb\') as attachment_file: attachment_content = attachment_file.read() # Create a MIMEBase object for the attachment attachment = MIMEBase(\'application\', \'octet-stream\') attachment.set_payload(attachment_content) encode_base64(attachment) # Add necessary headers for the attachment filename = os.path.basename(attachment_path) attachment.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{filename}\\"\') # Add the attachment to the email if msg.is_multipart(): msg.get_payload().append(attachment) else: new_msg = EmailMessage() for header, value in msg.items(): new_msg[header] = value new_msg.set_content(msg.get_content()) new_msg.add_attachment(attachment.get_payload(decode=True), maintype=attachment.get_content_maintype(), subtype=attachment.get_content_subtype(), filename=filename) msg = new_msg # Generate and return the new email byte stream return msg.as_bytes()"},{"question":"Using the `gzip` Module for File Compression and Decompression **Objective:** Your task is to implement two functions: `compress_files` and `decompress_files` using the `gzip` module in Python. These functions will demonstrate your understanding of file compression and decompression using the `gzip` module. **Function 1: `compress_files`** **Description:** This function should accept a list of file paths to text files and compress each one into a gzip format file. **Input:** - `file_paths` (List[str]): A list of strings, each representing the path to a text file that needs to be compressed. - `output_dir` (str): A string representing the directory where the compressed files should be stored. **Output:** - This function should not return anything. Instead, it should create compressed gzip files with the same name as the input files, but with a `.gz` extension, stored in the specified output directory. **Constraints:** - Each file in the `file_paths` list is guaranteed to exist and be a valid text file. - The `output_dir` directory is guaranteed to exist. **Function 2: `decompress_files`** **Description:** This function should accept a list of file paths to gzip compressed files and decompress each one back to its original text file. **Input:** - `file_paths` (List[str]): A list of strings, each representing the path to a gzip compressed file that needs to be decompressed. - `output_dir` (str): A string representing the directory where the decompressed files should be stored. **Output:** - This function should not return anything. Instead, it should create decompressed text files with the same name as the original files (before compression), stored in the specified output directory. **Constraints:** - Each file in the `file_paths` list is guaranteed to exist and be a valid gzip compressed file. - The `output_dir` directory is guaranteed to exist. **Example Usage:** ```python # Example usage of the compress_files function file_paths = [\\"file1.txt\\", \\"file2.txt\\"] output_dir = \\"compressed\\" compress_files(file_paths, output_dir) # Should create \'compressed/file1.txt.gz\' and \'compressed/file2.txt.gz\' # Example usage of the decompress_files function gzip_paths = [\\"compressed/file1.txt.gz\\", \\"compressed/file2.txt.gz\\"] output_dir = \\"decompressed\\" decompress_files(gzip_paths, output_dir) # Should create \'decompressed/file1.txt\' and \'decompressed/file2.txt\' ``` *Note: Ensure your solution handles any exceptions that may occur during file operations, such as reading, writing, compressing, or decompressing files.* ```python import gzip import os import shutil def compress_files(file_paths, output_dir): # Implementation here def decompress_files(file_paths, output_dir): # Implementation here ``` **Hints:** - Use the `gzip.open` function to handle file opening/compression and decompression. - Use context managers (`with` statements) to ensure files are properly closed after operations. - Make sure to handle file paths and extensions correctly to avoid overwriting or misnaming files.","solution":"import gzip import os def compress_files(file_paths, output_dir): try: for file_path in file_paths: filename = os.path.basename(file_path) compressed_file_path = os.path.join(output_dir, filename + \'.gz\') with open(file_path, \'rb\') as f_in, gzip.open(compressed_file_path, \'wb\') as f_out: f_out.writelines(f_in) except Exception as e: print(f\\"An error occurred while compressing files: {e}\\") def decompress_files(file_paths, output_dir): try: for file_path in file_paths: filename = os.path.basename(file_path) if filename.endswith(\'.gz\'): decompressed_file_name = filename[:-3] decompressed_file_path = os.path.join(output_dir, decompressed_file_name) with gzip.open(file_path, \'rb\') as f_in, open(decompressed_file_path, \'wb\') as f_out: f_out.write(f_in.read()) else: print(f\\"Skipping non-gzip file: {filename}\\") except Exception as e: print(f\\"An error occurred while decompressing files: {e}\\")"},{"question":"Objective Write a Python function `custom_import(module_name: str) -> module` that attempts to import a module by name at runtime. If the module is not found, the function should return `None`. Additionally, the function should print the name of the module being imported and whether it was successful or not. Function Signature ```python def custom_import(module_name: str) -> module: ``` Parameters - `module_name` (str): The name of the module to be imported. Returns - module: The imported module if successful, otherwise `None`. Example Usage ```python # Successful import module = custom_import(\\"math\\") print(module) # <module \'math\' (built-in)> # Unsuccessful import module = custom_import(\\"non_existent_module\\") print(module) # None ``` Constraints - You cannot use the built-in `import` statement directly to import the module. - Utilize the `importlib` module to perform the import. - Assume that any module name passed is in string format and correctly represents the module\'s name. Performance Requirements - The function should handle imports efficiently and run within a reasonable time for standard library modules and valid existing packages. Implementation Notes - Utilize `importlib.import_module()` to import the module. - Handle `ModuleNotFoundError` to catch cases where the module does not exist and return `None` accordingly. - Print the name of the module and the success or failure of the import. Example Implementation ```python def custom_import(module_name: str): import importlib try: module = importlib.import_module(module_name) print(f\\"Successfully imported module \'{module_name}\'\\") return module except ModuleNotFoundError: print(f\\"Failed to import module \'{module_name}\'\\") return None # Test cases module = custom_import(\\"math\\") print(module) # <module \'math\' (built-in)> module = custom_import(\\"non_existent_module\\") print(module) # None ```","solution":"def custom_import(module_name: str): import importlib try: module = importlib.import_module(module_name) print(f\\"Successfully imported module \'{module_name}\'\\") return module except ModuleNotFoundError: print(f\\"Failed to import module \'{module_name}\'\\") return None"},{"question":"You are tasked with creating a function that sends an email with both a plain text and an HTML version. Additionally, the email should contain multiple attachments, including images and some text files. After sending the email, the function should also save a local copy of the email message. # Function Signature ```python def send_multimedia_email(send_from: str, send_to: list, subject: str, plain_text: str, html_text: str, attachment_files: list, smtp_server: str, smtp_port: int, save_path: str) -> None: pass ``` # Input The function takes the following parameters: - `send_from (str)`: The sender\'s email address. - `send_to (list)`: A list of recipient email addresses. - `subject (str)`: The subject of the email. - `plain_text (str)`: The plain text version of the email content. - `html_text (str)`: The HTML version of the email content. Images in the HTML text should be referenced by their `cid`. - `attachment_files (list)`: A list of file paths to be attached to the email. These can include image files and text files. - `smtp_server (str)`: The SMTP server address to be used for sending the email. - `smtp_port (int)`: The port number of the SMTP server. - `save_path (str)`: The file path where a local copy of the email message should be saved. # Output The function returns `None`. # Example Usage ```python send_multimedia_email( send_from=\\"sender@example.com\\", send_to=[\\"recipient1@example.com\\", \\"recipient2@example.com\\"], subject=\\"Test Email with Attachments\\", plain_text=\\"This is a plain text version of the email.\\", html_text= <html> <body> <p>This is the HTML version of the email.</p> <img src=\\"cid:image1\\" /> </body> </html> , attachment_files=[\\"image1.jpg\\", \\"document.txt\\"], smtp_server=\\"smtp.example.com\\", smtp_port=587, save_path=\\"saved_email.msg\\" ) ``` # Constraints - The images should be embedded in the HTML part of the email such that they can be displayed directly within the email body. - Ensure that the MIME types of the attachments are correctly identified. - Assume that the local SMTP server is available for sending the email. - Implement appropriate error handling for file access and SMTP operations. # Explanation Your task is to implement the `send_multimedia_email` function that does the following: 1. Creates a MIME email message containing both plain text and HTML versions of the email body. 2. Adds multiple attachments (images and text files) to the email. 3. Sends the email via the specified SMTP server and port. 4. Saves a local copy of the email message to the specified file path. Make sure to use appropriate modules and methods from Python\'s `email` package to accomplish this task. Good luck!","solution":"import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders import os def send_multimedia_email(send_from: str, send_to: list, subject: str, plain_text: str, html_text: str, attachment_files: list, smtp_server: str, smtp_port: int, save_path: str) -> None: Sends an email with both plain text and HTML versions, and multiple attachments, then saves a local copy of the email message. # Create a MIMEMultipart message msg = MIMEMultipart(\'alternative\') msg[\'From\'] = send_from msg[\'To\'] = \', \'.join(send_to) msg[\'Subject\'] = subject # Attach plain text and HTML parts part1 = MIMEText(plain_text, \'plain\') part2 = MIMEText(html_text, \'html\') msg.attach(part1) msg.attach(part2) # Attach files for file_path in attachment_files: with open(file_path, \'rb\') as f: # Guess the MIME type based on the file extension mime_base, extension = MIMEBase(\'application\', \'octet-stream\'), os.path.splitext(file_path)[1] mime_base.set_payload(f.read()) encoders.encode_base64(mime_base) mime_base.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{os.path.basename(file_path)}\\"\') msg.attach(mime_base) # Save the email message to a file with open(save_path, \'w\') as f: f.write(msg.as_string()) # Sending the email through an SMTP server with smtplib.SMTP(smtp_server, smtp_port) as server: server.sendmail(send_from, send_to, msg.as_string())"},{"question":"**Question: Centralized NIS Administration Tool** You are tasked with creating a Python class `NISAdmin` that leverages the functionalities of the \\"nis\\" module to perform centralized administration of NIS maps. Your class should provide methods for the following operations: 1. **Retrieving a value for a given key from a specified map**. 2. **Listing all key-value pairs in a specified map**. 3. **Retrieving a list of all available NIS maps**. 4. **Handling errors gracefully when operations fail**. 5. **Returning the default NIS domain**. Implement this class with the following specifications: # Class: `NISAdmin` Methods: **`__init__(self, domain=None)`**: - Initializes the `NISAdmin` object with an optional custom NIS domain. - If domain is not provided, it should use the default NIS domain. **`get_value(self, key, mapname)`**: - Retrieves the value for the given key from the specified map. - Returns the value as a string decoded from bytes. - Raises a `ValueError` if the key is not found. **`list_map(self, mapname)`**: - Returns a dictionary mapping all keys to their values in the specified map. - Both keys and values should be returned as strings decoded from bytes. **`list_maps(self)`**: - Returns a list of all valid maps in the current NIS domain. **`get_default_domain(self)`**: - Returns the default NIS domain. Constraints: - You must handle the `nis.error` exception appropriately and raise a custom `ValueError` with a relevant message in case of errors. - You should ensure all byte-to-string decodings return clean UTF-8 strings. Example Usage: ```python admin = NISAdmin() try: value = admin.get_value(\\"somekey\\", \\"sometable\\") print(f\\"Value: {value}\\") except ValueError as e: print(e) try: all_maps = admin.list_maps() print(f\\"Available maps: {all_maps}\\") except ValueError as e: print(e) try: map_content = admin.list_map(\\"sometable\\") for key, value in map_content.items(): print(f\\"{key}: {value}\\") except ValueError as e: print(e) default_domain = admin.get_default_domain() print(f\\"Default NIS Domain: {default_domain}\\") ``` Make sure your implementations of these methods are both efficient and handle exceptions properly to ensure robustness.","solution":"import nis class NISAdmin: def __init__(self, domain=None): self.domain = domain if domain else nis.get_default_domain() def get_value(self, key, mapname): try: value = nis.match(key, mapname, domain=self.domain).decode(\'utf-8\') return value except nis.error: raise ValueError(f\\"Key \'{key}\' not found in map \'{mapname}\'.\\") def list_map(self, mapname): try: entries = nis.cat(mapname, domain=self.domain) return {k.decode(\'utf-8\'): v.decode(\'utf-8\') for k, v in entries.items()} except nis.error: raise ValueError(f\\"Map \'{mapname}\' not found or could not be read.\\") def list_maps(self): try: maps = nis.maps(domain=self.domain) return [m.decode(\'utf-8\') for m in maps] except nis.error: raise ValueError(\\"Could not retrieve list of maps.\\") def get_default_domain(self): return self.domain"},{"question":"# SMTP Email Automation with Error Handling **Background:** You are required to create a Python function to automate the process of sending emails using the `smtplib` module. The function should establish an SMTP connection, send an email, and properly handle any errors that occur during the process. **Objective:** Write a function `send_custom_email` that sends an email using SMTP. The function should: 1. Take the following input parameters: - `smtp_server` (string): The address of the SMTP server (e.g., `smtp.gmail.com`). - `port` (int): The port number to use for the SMTP connection (e.g., `587` for TLS, `465` for SSL). - `use_ssl` (boolean): Whether to use SSL from the beginning (`True`) or not (`False`). - `from_addr` (string): The sender\'s email address. - `to_addrs` (list of strings): A list of recipient email addresses. - `subject` (string): The subject line of the email. - `body` (string): The body content of the email. - `username` (string, optional): Username for SMTP server authentication. - `password` (string, optional): Password for SMTP server authentication. 2. Connect to the SMTP server, optionally using SSL as per the `use_ssl` flag. 3. Implement the `EHLO`/`HELO`, and if necessary, the `STARTTLS` mechanism. 4. Authenticate with the server using provided `username` and `password`. 5. Send the email with the provided subject and body. 6. Handle and print appropriate error messages for the following exceptions: - `SMTPConnectError` - `SMTPHeloError` - `SMTPAuthenticationError` - `SMTPSenderRefused` - `SMTPRecipientsRefused` - `SMTPDataError` - `SMTPNotSupportedError` - `SMTPException` **Constraints:** - The function should only raise errors that are not specifically handled. - If any exception occurs, ensure that the SMTP connection is properly closed. - Assume the body of the email will be plain text only. **Input Format:** ```python send_custom_email( smtp_server: str, port: int, use_ssl: bool, from_addr: str, to_addrs: List[str], subject: str, body: str, username: Optional[str] = None, password: Optional[str] = None ) -> None: ``` **Output Format:** - No return value. Print appropriate messages for success or failure. **Example:** ```python send_custom_email( smtp_server=\\"smtp.gmail.com\\", port=587, use_ssl=False, from_addr=\\"your-email@gmail.com\\", to_addrs=[\\"recipient1@gmail.com\\", \\"recipient2@yahoo.com\\"], subject=\\"Test Email\\", body=\\"This is a test email sent from Python.\\", username=\\"your-email@gmail.com\\", password=\\"your-email-password\\" ) ``` In this example, the function will: 1. Connect to the Gmail SMTP server using the specified port. 2. Use STARTTLS for an encrypted connection. 3. Authenticate with the given username and password. 4. Send an email with the specified subject and body to the listed recipients. 5. Handle any errors appropriately, printing error messages if they occur.","solution":"import smtplib from email.message import EmailMessage def send_custom_email(smtp_server, port, use_ssl, from_addr, to_addrs, subject, body, username=None, password=None): Sends an email using the provided SMTP server and connection details. Parameters: smtp_server (str): The address of the SMTP server. port (int): The port number to use for the SMTP connection. use_ssl (bool): Whether to use SSL from the beginning or not. from_addr (str): The sender\'s email address. to_addrs (list of str): A list of recipient email addresses. subject (str): The subject line of the email. body (str): The body content of the email. username (str, optional): Username for SMTP server authentication. password (str, optional): Password for SMTP server authentication. Returns: None try: # Create the email message msg = EmailMessage() msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject msg.set_content(body) # Connect to the SMTP server if use_ssl: server = smtplib.SMTP_SSL(smtp_server, port) else: server = smtplib.SMTP(smtp_server, port) server.starttls() # Perform the HELO/EHLO server.ehlo() # Login to the server, if credentials are provided if username and password: server.login(username, password) # Send the email server.send_message(msg) print(\\"Email sent successfully.\\") except smtplib.SMTPConnectError as e: print(f\\"Failed to connect to the server: {e}\\") except smtplib.SMTPHeloError as e: print(f\\"Failed to perform HELO: {e}\\") except smtplib.SMTPAuthenticationError as e: print(f\\"Failed to authenticate: {e}\\") except smtplib.SMTPSenderRefused as e: print(f\\"Sender address refused: {e}\\") except smtplib.SMTPRecipientsRefused as e: print(f\\"Recipient address refused: {e}\\") except smtplib.SMTPDataError as e: print(f\\"Failed to send data: {e}\\") except smtplib.SMTPNotSupportedError as e: print(f\\"SMTP feature not supported: {e}\\") except smtplib.SMTPException as e: print(f\\"SMTP error occurred: {e}\\") finally: try: server.quit() except Exception as e: print(f\\"Failed to close the server connection: {e}\\")"},{"question":"# Probability Distributions with PyTorch Problem Statement You have been tasked to create a utility function using PyTorch distributions that can generate samples from a mixture of `Normal` distributions and calculate their log probabilities. You will further need to compute the KL Divergence between two distributions. Requirements 1. **Mixture Sample Generation**: - Write a function `generate_mixture_samples(means, stds, weights, num_samples)` that takes in: - `means`: a list of mean values for the normal distributions. - `stds`: a list of standard deviations for the normal distributions. - `weights`: a list of weights for each distribution in the mixture. - `num_samples`: an integer specifying the number of samples to generate. - The function should return `num_samples` samples drawn from the mixture of normal distributions. 2. **Log Probability Calculation**: - Write a function `calculate_log_prob(samples, means, stds, weights)` that takes in: - `samples`: a tensor of samples for which log probabilities need to be calculated. - `means`: a list of mean values for the normal distributions. - `stds`: a list of standard deviations for the normal distributions. - `weights`: a list of weights for each distribution in the mixture. - The function should return the log probabilities of the given samples under the mixture distribution. 3. **KL Divergence Calculation**: - Write a function `calculate_kl_divergence(means_p, stds_p, means_q, stds_q)` that takes in: - `means_p`, `stds_p`: mean and standard deviation lists for the first distribution (P). - `means_q`, `stds_q`: mean and standard deviation lists for the second distribution (Q). - The function should return the KL Divergence between the two distributions. # Function Signatures ```python def generate_mixture_samples(means: list, stds: list, weights: list, num_samples: int) -> torch.Tensor: pass def calculate_log_prob(samples: torch.Tensor, means: list, stds: list, weights: list) -> torch.Tensor: pass def calculate_kl_divergence(means_p: list, stds_p: list, means_q: list, stds_q: list) -> torch.Tensor: pass ``` # Constraints - Ensure that the lengths of `means`, `stds`, and `weights` are all the same. - Use PyTorch distributions to implement the functionality. - Handle edge cases such as zero weights carefully. # Example ```python means = [0, 5] stds = [1, 2] weights = [0.7, 0.3] num_samples = 1000 samples = generate_mixture_samples(means, stds, weights, num_samples) print(samples.shape) # (1000,) log_probs = calculate_log_prob(samples, means, stds, weights) print(log_probs.shape) # (1000,) kl_div = calculate_kl_divergence([0], [1], [5], [2]) print(kl_div) # a tensor containing the KL Divergence ``` Good luck!","solution":"import torch def generate_mixture_samples(means, stds, weights, num_samples): Generates samples from a mixture of normal distributions. Parameters: means (list): Mean values of the normal distributions. stds (list): Standard deviation values of the normal distributions. weights (list): Weights of the normal distributions in the mixture. num_samples (int): Number of samples to generate. Returns: torch.Tensor: Samples from the mixture. assert len(means) == len(stds) == len(weights), \\"Input lists must have the same length\\" mixture_idx = torch.multinomial(torch.tensor(weights), num_samples, replacement=True) components = torch.empty(num_samples) for i in range(len(means)): mask = mixture_idx == i num_selected = mask.sum().item() if num_selected > 0: component_samples = torch.normal(means[i], stds[i], size=(num_selected,)) components[mask] = component_samples return components def calculate_log_prob(samples, means, stds, weights): Calculates log probabilities of samples under a mixture of normal distributions. Parameters: samples (torch.Tensor): Samples to calculate log probabilities for. means (list): Mean values of the normal distributions. stds (list): Standard deviation values of the normal distributions. weights (list): Weights of the normal distributions in the mixture. Returns: torch.Tensor: Log probabilities of the samples. assert len(means) == len(stds) == len(weights), \\"Input lists must have the same length\\" log_probs = torch.tensor([0.0] * len(samples)) for mean, std, weight in zip(means, stds, weights): dist = torch.distributions.Normal(mean, std) log_probs += weight * torch.exp(dist.log_prob(samples)) return torch.log(log_probs) def calculate_kl_divergence(means_p, stds_p, means_q, stds_q): Calculates the KL Divergence between two mixtures of normal distributions. Parameters: means_p (list): Mean values of the first mixture\'s normal distributions (P). stds_p (list): Standard deviation values of the first mixture\'s normal distributions (P). means_q (list): Mean values of the second mixture\'s normal distributions (Q). stds_q (list): Standard deviation values of the second mixture\'s normal distributions (Q). Returns: torch.Tensor: The KL Divergence. assert len(means_p) == len(stds_p) == len(means_q) == len(stds_q), \\"Input lists must have the same length\\" kl_div = torch.tensor(0.0) for mean_p, std_p, mean_q, std_q in zip(means_p, stds_p, means_q, stds_q): P = torch.distributions.Normal(mean_p, std_p) Q = torch.distributions.Normal(mean_q, std_q) kl_div += torch.distributions.kl_divergence(P, Q) return kl_div"},{"question":"Advanced Plot Layouts with Seaborn Objective Demonstrate your understanding of creating and customizing the layout of plots using the Seaborn library. You will need to apply your knowledge of `seaborn.objects` for arranging subplots and controlling their dimensions. Problem Statement You are provided with a dataset and your task is to create a series of subplots that visualize different aspects of the data. Utilize the `seaborn.objects` library to arrange and adjust the sizes of these subplots within a figure. Specifically, you need to: 1. Create a main plotting object `p` with a size of 10x10 inches. 2. Create a 2x2 grid of subplots within this main figure. 3. Use the layout engine that best fits your data and subplots in a constrained grid format. 4. Adjust the size of each individual plot to occupy a specific region of the overall figure. Dataset You will use the `tips` dataset provided by Seaborn, which contains data about tips received by a waiter over different days and times at a restaurant. Instructions 1. **Input**: - There are no explicit external inputs. You will use the `tips` dataset which can be loaded using `seaborn.load_dataset(\'tips\')`. 2. **Output**: - A plotted 2x2 grid using the `seaborn.objects` interface, satisfying the conditions given above. 3. **Constraints**: - The size of the main plot should be exactly 10x10 inches. - Use a layout engine that fits a constrained 2x2 grid. - Adjust the extent of each subplot to fit within the specified regions of the main figure. 4. **Performance Requirements**: - The solution should be able to generate the plots in under 5 seconds. Example Code Structure ```python import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_subplots(): # Load the dataset tips = sns.load_dataset(\'tips\') # Create the main plot object with size 10x10 inches p = so.Plot(tips).layout(size=(10, 10)) # Create a 2x2 grid of subplots p = p.facet(row=\\"time\\", col=\\"smoker\\") # Use the constrained layout engine p = p.layout(engine=\\"constrained\\") # Adjust the size of each plot (assuming [0, 0, 0.5, 0.5] for a quarter plot) p = p.layout(extent=[0, 0, 0.5, 0.5]) # Show the plot p.show() # Execute the function create_custom_subplots() ``` In the function `create_custom_subplots`, you will: - Load the `tips` dataset. - Set up a main plot object with the required size. - Arrange the subplots in a 2x2 grid. - Choose and apply a layout engine for optimal constrained presentation. - Adjust the extent of each subplot relative to the main plot. By following the structure and completing the logic, you will demonstrate an understanding of advanced plotting layouts in Seaborn.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_subplots(): # Load the dataset tips = sns.load_dataset(\'tips\') # Create the main plot object with size 10x10 inches main_plot = plt.figure(figsize=(10, 10)) # Create a 2x2 grid of subplots with constrained layout grid = main_plot.add_gridspec(2, 2) # Plot each subplot ax1 = main_plot.add_subplot(grid[0, 0]) sns.histplot(data=tips, x=\'total_bill\', ax=ax1).set_title(\'Total Bill Distribution\') ax2 = main_plot.add_subplot(grid[0, 1]) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', ax=ax2).set_title(\'Total Bill vs Tip\') ax3 = main_plot.add_subplot(grid[1, 0]) sns.boxplot(data=tips, x=\'day\', y=\'total_bill\', ax=ax3).set_title(\'Total Bill by Day\') ax4 = main_plot.add_subplot(grid[1, 1]) sns.violinplot(data=tips, x=\'day\', y=\'tip\', ax=ax4).set_title(\'Tip by Day\') # Use constrained layout main_plot.tight_layout() # Show the plot plt.show() # Execute the function create_custom_subplots()"},{"question":"# Asyncio Queue-Based Task Scheduler **Objective**: Implement a task scheduler using `asyncio.Queue` to manage a set of tasks where each task is represented by a coroutine that takes an arbitrary amount of time to complete. Your scheduler should ensure that a maximum number of worker coroutines are processing tasks concurrently. **Task**: 1. Create an `asyncio.Queue` instance to hold task functions. 2. Implement a `schedule_task` coroutine that adds a task to the queue. 3. Implement a `worker` coroutine that continuously fetches tasks from the queue and executes them. 4. Implement a `main` coroutine that: - Initializes the queue. - Creates and schedules a list of tasks. - Spawns a fixed number of worker coroutines to execute tasks from the queue. - Waits for the queue to be processed completely. - Gracefully shuts down worker coroutines. **Constraints**: - Number of workers and tasks should be configurable. - Each task should indicate its completion by invoking `queue.task_done()`. **Input**: - `tasks`: A list of coroutines (functions). Example: `[task1(), task2(), task3()]`. - `num_workers`: An integer indicating the number of worker coroutines to run concurrently. - `max_queue_size`: (optional) An integer indicating the maximum size of the queue (default is 0, indicating infinite size). **Output**: - Print statements from tasks\' execution. - Final print statement with total execution time. Example task function: ```python async def example_task(duration: float): await asyncio.sleep(duration) print(f\'Task with duration {duration} completed\') ``` **Implementation Steps**: 1. Define the `schedule_task` coroutine. 2. Define the `worker` coroutine. 3. Define the `main` coroutine to manage the overall execution. 4. Use `asyncio.run(main())` to execute your code. ```python import asyncio async def schedule_task(queue, task): await queue.put(task) async def worker(name, queue): while True: task = await queue.get() await task queue.task_done() print(f\'{name} completed a task\') async def main(tasks, num_workers, max_queue_size=0): queue = asyncio.Queue(max_queue_size) for task in tasks: await schedule_task(queue, task) workers = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(num_workers)] await queue.join() for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) # Example usage async def example_task(duration): await asyncio.sleep(duration) print(f\'Task with duration {duration} seconds completed\') tasks = [example_task(1), example_task(2), example_task(1.5)] asyncio.run(main(tasks, num_workers=2)) ``` **Note**: Use different task durations and observe the parallel execution managed by the workers.","solution":"import asyncio async def schedule_task(queue, task): await queue.put(task) async def worker(name, queue): while True: task = await queue.get() try: await task print(f\'{name} completed a task\') finally: queue.task_done() async def main(tasks, num_workers, max_queue_size=0): queue = asyncio.Queue(max_queue_size) # Schedule tasks for task in tasks: await schedule_task(queue, task) # Start worker coroutines workers = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(num_workers)] # Wait until all tasks are processed await queue.join() # Cancel the workers after queue is empty for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) # Example usage async def example_task(duration): await asyncio.sleep(duration) print(f\'Task with duration {duration} seconds completed\') tasks = [example_task(1), example_task(2), example_task(1.5)] asyncio.run(main(tasks, num_workers=2))"},{"question":"Bytearray Operations Objective: Implement a class `ByteArrayOperations` in Python to perform various operations on `bytearray` objects. This will demonstrate your understanding of `bytearray` manipulations and your ability to work with low-level byte array operations. Class Specification: ```python class ByteArrayOperations: def __init__(self, initial_data): Initialize a new ByteArrayOperations object. Args: initial_data (str or list of int): Initial data to create the bytearray. If str, it should be converted to bytes. If list of int, it should be the byte values. pass def concat_bytearrays(self, other_data): Concatenates the current bytearray with another bytearray created from \'other_data\'. Args: other_data (str or list of int): Data to create the bytearray to concatenate with. Returns: bytearray: The concatenated bytearray. pass def get_size(self): Returns the size of the current bytearray. Returns: int: Size of the bytearray. pass def as_string(self): Returns the contents of the bytearray as a string. Returns: str: String representation of the bytearray. pass def resize_bytearray(self, new_size): Resizes the current bytearray to the new size. Args: new_size (int): The new size of the bytearray. pass ``` Constraints and Assumptions: 1. The initial data provided to the constructor will be either a string or a list of integers representing byte values. 2. The `other_data` provided to the `concat_bytearrays` method will be either a string or a list of integers. 3. The new size provided to the `resize_bytearray` method will always be a non-negative integer. Example Usage: ```python # Initialize with a string ops = ByteArrayOperations(\\"hello\\") size = ops.get_size() # Returns 5 string_rep = ops.as_string() # Returns \\"hello\\" # Concatenate with another bytearray created from a string new_bytearray = ops.concat_bytearrays(\\" world\\") # new_bytearray should be bytearray(b\'hello world\') # Resize the bytearray to a larger size ops.resize_bytearray(15) size_after_resize = ops.get_size() # Returns 15 ``` Notes: - Ensure that your implementation handles type conversions and size calculations efficiently. - You may use the Python `bytearray` built-in type for your implementation. - Thoroughly test your implementation with various input types and sizes.","solution":"class ByteArrayOperations: def __init__(self, initial_data): Initialize a new ByteArrayOperations object. Args: initial_data (str or list of int): Initial data to create the bytearray. If str, it should be converted to bytes. If list of int, it should be the byte values. if isinstance(initial_data, str): self.bytearray = bytearray(initial_data, \'utf-8\') elif isinstance(initial_data, list): self.bytearray = bytearray(initial_data) else: raise ValueError(\\"initial_data must be a string or a list of integers\\") def concat_bytearrays(self, other_data): Concatenates the current bytearray with another bytearray created from \'other_data\'. Args: other_data (str or list of int): Data to create the bytearray to concatenate with. Returns: bytearray: The concatenated bytearray. if isinstance(other_data, str): other_bytearray = bytearray(other_data, \'utf-8\') elif isinstance(other_data, list): other_bytearray = bytearray(other_data) else: raise ValueError(\\"other_data must be a string or a list of integers\\") return self.bytearray + other_bytearray def get_size(self): Returns the size of the current bytearray. Returns: int: Size of the bytearray. return len(self.bytearray) def as_string(self): Returns the contents of the bytearray as a string. Returns: str: String representation of the bytearray. return self.bytearray.decode(\'utf-8\') def resize_bytearray(self, new_size): Resizes the current bytearray to the new size. Args: new_size (int): The new size of the bytearray. if new_size < 0: raise ValueError(\\"new_size must be a non-negative integer\\") current_size = len(self.bytearray) if new_size < current_size: self.bytearray = self.bytearray[:new_size] else: self.bytearray.extend([0] * (new_size - current_size))"},{"question":"Objective: You are required to implement a function that takes a string, encodes it using different Base64 and Base85 encodings, and then decodes it back to the original string. The function should ensure that the decoded string matches the original string for all encoding schemes used. Function Signature: ```python def validate_encodings(input_string: str) -> dict: Encodes the input_string using multiple encoding schemes and verifies that decoding each encoded string returns the original input_string. Parameters: - input_string: str : The original string to be encoded and decoded. Returns: - result: dict : A dictionary where the keys are the encoding schemes and the values are booleans indicating whether the decoded string matched the original string for that encoding scheme. pass ``` Requirements: 1. The function should use the following encoding schemes provided by the `base64` module: - Standard Base64 - URL-safe Base64 - Ascii85 - Base85 2. For each encoding scheme: - Encode the input string. - Decode the encoded string. - Check if the decoded string matches the original input string. 3. The function should return a dictionary summarizing the match status for each encoding scheme. The dictionary should have the following format: ```python { \\"standard_base64\\": True, \\"urlsafe_base64\\": True, \\"ascii85\\": True, \\"base85\\": True } ``` 4. If any encoding/decoding step raises an exception, the corresponding result should be `False`. Example: ```python >>> validate_encodings(\\"hello world\\") { \\"standard_base64\\": True, \\"urlsafe_base64\\": True, \\"ascii85\\": True, \\"base85\\": True } ``` Constraints: - The input string will contain only printable ASCII characters. - The input string length will be at most 1000 characters. Notes: - You may use the `base64` module directly in your implementation. - Ensure to handle possible exceptions that might be raised during encoding/decoding and set the corresponding dictionary value to `False`.","solution":"import base64 def validate_encodings(input_string: str) -> dict: Encodes the input_string using multiple encoding schemes and verifies that decoding each encoded string returns the original input_string. Parameters: - input_string: str : The original string to be encoded and decoded. Returns: - result: dict : A dictionary where the keys are the encoding schemes and the values are booleans indicating whether the decoded string matched the original string for that encoding scheme. results = { \\"standard_base64\\": False, \\"urlsafe_base64\\": False, \\"ascii85\\": False, \\"base85\\": False } try: # Standard Base64 encoding and decoding encoded = base64.b64encode(input_string.encode()).decode() decoded = base64.b64decode(encoded).decode() results[\\"standard_base64\\"] = (decoded == input_string) except Exception: results[\\"standard_base64\\"] = False try: # URL-safe Base64 encoding and decoding encoded = base64.urlsafe_b64encode(input_string.encode()).decode() decoded = base64.urlsafe_b64decode(encoded).decode() results[\\"urlsafe_base64\\"] = (decoded == input_string) except Exception: results[\\"urlsafe_base64\\"] = False try: # Ascii85 encoding and decoding encoded = base64.a85encode(input_string.encode()).decode() decoded = base64.a85decode(encoded).decode() results[\\"ascii85\\"] = (decoded == input_string) except Exception: results[\\"ascii85\\"] = False try: # Base85 encoding and decoding encoded = base64.b85encode(input_string.encode()).decode() decoded = base64.b85decode(encoded).decode() results[\\"base85\\"] = (decoded == input_string) except Exception: results[\\"base85\\"] = False return results"},{"question":"**Question:** Implement a custom enumeration `Planet` using the `enum` module in Python. The enumeration should include the major planets in the solar system, along with their respective masses (in kilograms) and radii (in meters). Additionally, define a method to calculate the surface gravity for each planet. 1. **Define the `Planet` enumeration**: - Use the `Enum` class from the `enum` module. - Each planet should have attributes: `mass` (in kilograms) and `radius` (in meters). 2. **Add methods**: - `surface_gravity`: This method should return the surface gravity of the planet using the formula: [ g = frac{G cdot m}{r^2} ] where (G) is the universal gravitational constant ((6.67430 times 10^{-11}) mÂ³ kgâ»Â¹ sâ»Â²), (m) is the mass of the planet, and (r) is the radius of the planet. # Requirements - **Input/Output**: - The enumeration class should not take any user input. - The `surface_gravity` method should return the computed surface gravity as a floating-point number. - **Constraints**: - The enumeration should define at least the eight major planets: `MERCURY`, `VENUS`, `EARTH`, `MARS`, `JUPITER`, `SATURN`, `URANUS`, and `NEPTUNE`. - The `surface_gravity` method must be implemented as a property. - **Performance**: - The implementation should be efficient and make use of Python\'s `enum` functionalities properly. # Example Usage ```python from enum import Enum class Planet(Enum): MERCURY = (3.303e+23, 2.4397e6) VENUS = (4.869e+24, 6.0518e6) EARTH = (5.976e+24, 6.37814e6) MARS = (6.421e+23, 3.3972e6) JUPITER = (1.9e+27, 7.1492e7) SATURN = (5.688e+26, 6.0268e7) URANUS = (8.686e+25, 2.5559e7) NEPTUNE = (1.024e+26, 2.4746e7) def __init__(self, mass, radius): self.mass = mass self.radius = radius @property def surface_gravity(self): G = 6.67430e-11 # universal gravitational constant return G * self.mass / (self.radius ** 2) # Example print(Planet.EARTH.surface_gravity) # Output should be approximately 9.8 print(Planet.JUPITER.surface_gravity) # Output should be a value representing Jupiter\'s surface gravity ``` **Notes**: - Make sure to use correct types for the mass and radius inputs. - Use correct and precise values for physical constants and planet attributes for accurate calculations.","solution":"from enum import Enum class Planet(Enum): MERCURY = (3.3011e+23, 2.4397e6) VENUS = (4.8675e+24, 6.0518e6) EARTH = (5.97237e+24, 6.371e6) MARS = (6.4171e+23, 3.3895e6) JUPITER = (1.8982e+27, 6.9911e7) SATURN = (5.6834e+26, 5.8232e7) URANUS = (8.6810e+25, 2.5362e7) NEPTUNE = (1.02413e+26, 2.4622e7) def __init__(self, mass, radius): self.mass = mass self.radius = radius @property def surface_gravity(self): G = 6.67430e-11 # universal gravitational constant in mÂ³ kgâ»Â¹ sâ»Â² return G * self.mass / (self.radius ** 2)"},{"question":"Assessment Problem: Multi-threaded Task Scheduling As a part of developing a multi-threaded task scheduling system, you must implement a scheduler that coordinates multiple worker threads to process tasks from a shared task queue. The scheduler should use appropriate synchronization techniques to ensure safe access to the shared task queue and coordinate worker threads. # Functional Requirements 1. **Task Queue Management:** - Use a thread-safe queue to store tasks. - Ensure that tasks are retrieved and processed by worker threads safely. 2. **Worker Threads:** - Create a specified number of worker threads to process tasks. - Each worker thread should keep running until a termination signal is received. - Use a `Condition` object to notify worker threads when a new task is added to the queue. 3. **Task Scheduler:** - The scheduler should allow for dynamic addition of new tasks. - After adding a task, it should notify at least one worker thread to start processing the task. 4. **Task Processing:** - Tasks should be processed one at a time by worker threads. # Implementation Details **Input:** - A list of tasks to be processed. - Number of worker threads. **Output:** - Each task should be processed exactly once. - Your code should print the current task being processed by a worker thread. **Constraints:** - Tasks can be any callable object (function). - You must manage the worker threads and task queue safely using the `threading` module. # Example Usage ```python import threading from queue import Queue import time def sample_task(task_id): print(f\\"Processing task {task_id}\\") time.sleep(1) class TaskScheduler: def __init__(self, num_workers): # Initialize your scheduler here def add_task(self, task): # Add new task to the queue and notify a worker thread def start(self): # Start worker threads def stop(self): # Send termination signal and wait for all worker threads to finish # Example usage scheduler = TaskScheduler(num_workers=3) scheduler.start() # Adding tasks for i in range(10): scheduler.add_task(lambda i=i: sample_task(i)) # Allow some time for tasks to be processed time.sleep(15) scheduler.stop() ``` **Note:** Your solution must include proper synchronization to avoid race conditions. # Performance Considerations - Ensure that the scheduler and worker threads perform efficiently without unnecessary blocking. - Use appropriate thread synchronization primitives to maximize concurrency and minimize idle time. Implement the `TaskScheduler` class with the specified methods and functionality.","solution":"import threading from queue import Queue import time def sample_task(task_id): print(f\\"Processing task {task_id}\\") time.sleep(1) class TaskScheduler: def __init__(self, num_workers): self.task_queue = Queue() self.num_workers = num_workers self.threads = [] self.condition = threading.Condition() self.keep_running = True for _ in range(num_workers): thread = threading.Thread(target=self.worker) self.threads.append(thread) def worker(self): while self.keep_running or not self.task_queue.empty(): with self.condition: while self.task_queue.empty() and self.keep_running: self.condition.wait() if self.task_queue.empty(): continue task = self.task_queue.get() if task is not None: task() self.task_queue.task_done() def add_task(self, task): with self.condition: self.task_queue.put(task) self.condition.notify() def start(self): for thread in self.threads: thread.start() def stop(self): self.keep_running = False with self.condition: self.condition.notify_all() for thread in self.threads: thread.join()"},{"question":"**Question: File Resource Manager with Python Development Mode** You are tasked with creating a Python class called `SafeFileReader` that ensures safe and efficient file reading. This class should correctly manage file resources to avoid ResourceWarnings, which can be identified when the Python Development Mode is enabled. Specifically, you should: 1. Implement the `SafeFileReader` class with the following functionalities: - A method named `read_first_line` that reads and returns the first line of a file. - A method named `read_all_lines` that reads and returns all lines of a file as a list. - Use proper resource management techniques (e.g., context managers) to ensure files are properly closed after operations. 2. Write a script that uses the `SafeFileReader` to read from a provided file (the file name should be passed as a command-line argument). Print the first line and the total number of lines in the file. 3. When run with Python Development Mode enabled (`-X dev`), the script should not emit any ResourceWarnings. # Constraints and Requirements: - Your implementation should handle potential exceptions, ensuring that files are closed even if an error occurs during reading. - You must demonstrate proper use of the Python Development Mode by showing that the script runs without ResourceWarnings or `Bad file descriptor` errors. - You can assume the file contains text and is encoded in UTF-8. # Example Usage: Given a text file `example.txt` with the following content: ``` Hello, World! This is a sample file. It has multiple lines. ``` Running your script: ```sh python3 -X dev your_script.py example.txt ``` The expected output should be: ``` First line: Hello, World! Total lines: 3 ``` # Implementation Details: ```python import sys class SafeFileReader: def __init__(self, filename): self.filename = filename def read_first_line(self): try: with open(self.filename, \'r\', encoding=\'utf-8\') as file: return file.readline().rstrip() except Exception as e: print(f\\"Error reading first line: {e}\\") return None def read_all_lines(self): try: with open(self.filename, \'r\', encoding=\'utf-8\') as file: return file.readlines() except Exception as e: print(f\\"Error reading all lines: {e}\\") return [] if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python your_script.py <filename>\\") sys.exit(1) filename = sys.argv[1] reader = SafeFileReader(filename) first_line = reader.read_first_line() if first_line is not None: print(f\\"First line: {first_line}\\") all_lines = reader.read_all_lines() if all_lines: print(f\\"Total lines: {len(all_lines)}\\") ``` Ensure your implementation follows these guidelines to maintain adherence to the Python Development Mode features and best practices for resource management.","solution":"import sys class SafeFileReader: def __init__(self, filename): self.filename = filename def read_first_line(self): Reads and returns the first line of the file. try: with open(self.filename, \'r\', encoding=\'utf-8\') as file: return file.readline().rstrip() except Exception as e: print(f\\"Error reading first line: {e}\\") return None def read_all_lines(self): Reads and returns all lines of the file as a list. try: with open(self.filename, \'r\', encoding=\'utf-8\') as file: return file.readlines() except Exception as e: print(f\\"Error reading all lines: {e}\\") return [] if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python your_script.py <filename>\\") sys.exit(1) filename = sys.argv[1] reader = SafeFileReader(filename) first_line = reader.read_first_line() if first_line is not None: print(f\\"First line: {first_line}\\") all_lines = reader.read_all_lines() if all_lines: print(f\\"Total lines: {len(all_lines)}\\")"},{"question":"Implementing Out-of-Core Learning for Text Classification Objective Design and implement an out-of-core learning system using scikit-learn for a text classification task. The system should process data in mini-batches, perform feature extraction, and update the model incrementally. Description You are given a large text dataset that cannot fit into memory all at once. Your task is to implement a system that processes this dataset in mini-batches, extracts features, and trains a classifier incrementally using scikit-learn. Dataset Assume the dataset is a collection of text files stored in a directory called `data/text_files`. Each file contains multiple lines, where each line represents a document and has the format \'<label>t<text>\': ``` ham Hi there, how are you? spam Win a 1000 prize now! ham Are we still meeting later? ... ``` Requirements 1. **Streaming Data**: Implement a generator function `stream_data(batch_size)` that reads the text files in the `data/text_files` directory and yields mini-batches of documents. Each mini-batch should be a list of tuples `(label, text)`. 2. **Feature Extraction**: Use `HashingVectorizer` to transform the text into a feature matrix. 3. **Incremental Learning**: Use an incremental classifier, such as `SGDClassifier`, to train the model incrementally. Task 1. Implement the `stream_data(batch_size)` function. 2. Implement the `train_incremental_model(batch_size)` function to process the mini-batches, extract features and train the model. Input - `batch_size` (int): The size of each mini-batch. Output - Print the accuracy of the model after processing all the data. Constraints - The system should handle datasets that do not fit into memory. - Use the `SGDClassifier` with the `partial_fit` method for incremental learning. Example Usage ```python # Assuming the dataset is available in the specified format batch_size = 1000 train_incremental_model(batch_size) ``` Sample Implementation Outline ```python import os from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(batch_size): # Implement the data streaming logic pass def train_incremental_model(batch_size): # Implement the incremental training logic pass if __name__ == \\"__main__\\": batch_size = 1000 train_incremental_model(batch_size) ```","solution":"import os from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(batch_size, directory=\'data/text_files\'): Generator that streams data in mini-batches of the specified size. :param batch_size: Size of each mini-batch. :param directory: Directory containing the text files. :yield: A list of tuples (label, text). batch = [] for filename in os.listdir(directory): with open(os.path.join(directory, filename), \'r\') as file: for line in file: label, text = line.strip().split(\'t\', 1) batch.append((label, text)) if len(batch) >= batch_size: yield batch batch = [] if batch: yield batch def train_incremental_model(batch_size, directory=\'data/text_files\'): Function to train an increment model using SGDClassifier with mini-batches. :param batch_size: Size of each mini-batch. :param directory: Directory containing the text files. :print: The accuracy of the model after processing all the data. vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() classes = [\'ham\', \'spam\'] for batch in stream_data(batch_size, directory=directory): texts = [text for label, text in batch] labels = [label for label, text in batch] X_batch = vectorizer.transform(texts) classifier.partial_fit(X_batch, labels, classes=classes) # Evaluate the model total_correct = 0 total_samples = 0 for batch in stream_data(batch_size, directory=directory): texts = [text for label, text in batch] labels = [label for label, text in batch] X_batch = vectorizer.transform(texts) predictions = classifier.predict(X_batch) total_correct += (predictions == labels).sum() total_samples += len(labels) accuracy = total_correct / total_samples print(f\'Accuracy: {accuracy:.4f}\') if __name__ == \\"__main__\\": batch_size = 1000 train_incremental_model(batch_size)"},{"question":"**Question:** Implement a set of functions to fetch and filter user data from the Unix password database using the `pwd` module. Your implementation should include the following: 1. `get_user_by_name(username: str) -> dict`: This function should take a username (string) as input and return a dictionary containing all the attributes of the user\'s password database entry. If the user does not exist, return `None`. 2. `get_user_by_uid(uid: int) -> dict`: This function should take a numeric user ID (integer) as input and return a dictionary containing all the attributes of the user\'s password database entry. If the user does not exist, return `None`. 3. `filter_users_by_shell(shell: str) -> List[dict]`: This function should take a shell (string) as input and return a list of dictionaries, each containing all the attributes of users who use the specified shell. The list should be sorted by login name. 4. `list_unique_shells() -> List[str]`: This function should return a sorted list of unique shells used by the users in the password database. **Input Constraints:** - `username` will be a non-empty string. - `uid` will be a non-negative integer. - `shell` will be a non-empty string. **Output Format:** - The dictionaries returned by the functions should have the following keys: `pw_name`, `pw_passwd`, `pw_uid`, `pw_gid`, `pw_gecos`, `pw_dir`, and `pw_shell`. **Performance Requirements (if applicable):** - The functions should be efficient and handle typical Unix system scenarios with thousands of user entries in a reasonable time frame. Here is a template to help you get started: ```python import pwd from typing import List, Dict, Union def get_user_by_name(username: str) -> Union[Dict[str, Union[str, int]], None]: try: entry = pwd.getpwnam(username) return { \\"pw_name\\": entry.pw_name, \\"pw_passwd\\": entry.pw_passwd, \\"pw_uid\\": entry.pw_uid, \\"pw_gid\\": entry.pw_gid, \\"pw_gecos\\": entry.pw_gecos, \\"pw_dir\\": entry.pw_dir, \\"pw_shell\\": entry.pw_shell } except KeyError: return None def get_user_by_uid(uid: int) -> Union[Dict[str, Union[str, int]], None]: try: entry = pwd.getpwuid(uid) return { \\"pw_name\\": entry.pw_name, \\"pw_passwd\\": entry.pw_passwd, \\"pw_uid\\": entry.pw_uid, \\"pw_gid\\": entry.pw_gid, \\"pw_gecos\\": entry.pw_gecos, \\"pw_dir\\": entry.pw_dir, \\"pw_shell\\": entry.pw_shell } except KeyError: return None def filter_users_by_shell(shell: str) -> List[Dict[str, Union[str, int]]]: entries = pwd.getpwall() users = [ { \\"pw_name\\": entry.pw_name, \\"pw_passwd\\": entry.pw_passwd, \\"pw_uid\\": entry.pw_uid, \\"pw_gid\\": entry.pw_gid, \\"pw_gecos\\": entry.pw_gecos, \\"pw_dir\\": entry.pw_dir, \\"pw_shell\\": entry.pw_shell } for entry in entries if entry.pw_shell == shell ] return sorted(users, key=lambda x: x[\\"pw_name\\"]) def list_unique_shells() -> List[str]: entries = pwd.getpwall() shells = sorted(set(entry.pw_shell for entry in entries)) return shells ``` Ensure that your implementation is robust and handles possible edge cases.","solution":"import pwd from typing import List, Dict, Union def get_user_by_name(username: str) -> Union[Dict[str, Union[str, int]], None]: Retrieves user information by username. :param username: The username to search for. :return: Dictionary containing user attributes or None if user does not exist. try: entry = pwd.getpwnam(username) return { \\"pw_name\\": entry.pw_name, \\"pw_passwd\\": entry.pw_passwd, \\"pw_uid\\": entry.pw_uid, \\"pw_gid\\": entry.pw_gid, \\"pw_gecos\\": entry.pw_gecos, \\"pw_dir\\": entry.pw_dir, \\"pw_shell\\": entry.pw_shell } except KeyError: return None def get_user_by_uid(uid: int) -> Union[Dict[str, Union[str, int]], None]: Retrieves user information by user ID. :param uid: The user ID to search for. :return: Dictionary containing user attributes or None if user does not exist. try: entry = pwd.getpwuid(uid) return { \\"pw_name\\": entry.pw_name, \\"pw_passwd\\": entry.pw_passwd, \\"pw_uid\\": entry.pw_uid, \\"pw_gid\\": entry.pw_gid, \\"pw_gecos\\": entry.pw_gecos, \\"pw_dir\\": entry.pw_dir, \\"pw_shell\\": entry.pw_shell } except KeyError: return None def filter_users_by_shell(shell: str) -> List[Dict[str, Union[str, int]]]: Filters users by shell and returns a list of user attributes. :param shell: The shell to filter users by. :return: Sorted list of dictionaries containing user attributes. entries = pwd.getpwall() users = [ { \\"pw_name\\": entry.pw_name, \\"pw_passwd\\": entry.pw_passwd, \\"pw_uid\\": entry.pw_uid, \\"pw_gid\\": entry.pw_gid, \\"pw_gecos\\": entry.pw_gecos, \\"pw_dir\\": entry.pw_dir, \\"pw_shell\\": entry.pw_shell } for entry in entries if entry.pw_shell == shell ] return sorted(users, key=lambda x: x[\\"pw_name\\"]) def list_unique_shells() -> List[str]: Lists all unique shells used by users in the system. :return: Sorted list of unique shells. entries = pwd.getpwall() shells = sorted(set(entry.pw_shell for entry in entries)) return shells"},{"question":"# Custom Dataset and DataLoader with Multi-Process Data Loading in PyTorch **Objective:** In this exercise, you are required to write a PyTorch script that demonstrates the use of `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` for efficient multi-process data loading. You\'ll implement a map-style dataset, configure a `DataLoader` to use multi-process data loading, and create custom batching logic. **Problem Statement:** 1. **Custom Dataset:** Implement a custom dataset class `CustomDataset` that extends `torch.utils.data.Dataset`. This dataset should: - Accept a list of numbers during initialization. - Implement the `__getitem__` method to fetch data at a given index. - Implement the `__len__` method to return the size of the dataset. 2. **Custom Collate Function:** Implement a custom collation function `custom_collate_fn` that: - Takes a list of samples. - Pads the samples to the maximum length in the batch. - Converts the padded list into a tensor. 3. **DataLoader:** Configure a `DataLoader` to: - Use the `CustomDataset`. - Implement multi-process data loading with a user-defined number of worker processes. - Use the custom collation function for batching the data. - Enable memory pinning for efficient data transfer to GPU. 4. **Validation:** - Demonstrate the data loading by iterating over batches and printing their shapes. - Ensure that data loading utilizes multiple worker processes. **Input:** - A list of numbers representing the dataset. - Integer `num_workers` indicating the number of worker processes for data loading. **Output:** - Prints batches of data indicating their shapes. - Ensures no data duplication or loss during multi-process data loading. **Constraints:** - Use `torch` version 1.8.0 or later. **Instructions:** - Define the `CustomDataset` class. - Define the `custom_collate_fn` function. - Initialize the `DataLoader` with the dataset and necessary parameters. - Validate the setup by iterating through batches and printing the output. ```python import torch from torch.utils.data import Dataset, DataLoader from torch.nn.utils.rnn import pad_sequence # Step 1: Implement the CustomDataset class class CustomDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): # Return the element at the given index return torch.tensor(self.data[index], dtype=torch.float) def __len__(self): # Return the size of the dataset return len(self.data) # Step 2: Implement the custom collate function def custom_collate_fn(batch): # Pad sequences to the maximum length in the batch and convert to tensor return pad_sequence(batch, batch_first=True, padding_value=0.0) # Step 3: Initialize DataLoader with CustomDataset and custom_collate_fn def create_data_loader(data, num_workers): dataset = CustomDataset(data) data_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=num_workers, collate_fn=custom_collate_fn, pin_memory=True) return data_loader # Validation def validate_data_loader(data_loader): for batch in data_loader: # Print the shape of each batch of data print(\'Batch shape:\', batch.shape) # Example usage if __name__ == \'__main__\': data = [[1], [2, 2], [3, 3, 3], [4, 4, 4, 4], [5], [6, 6], [7, 7, 7], [8, 8, 8, 8]] num_workers = 2 data_loader = create_data_loader(data, num_workers) validate_data_loader(data_loader) ``` # Explanation: 1. **Custom Dataset:** The `CustomDataset` class initializes with a list of data and implements the `__getitem__` and `__len__` methods to allow indexed access and length retrieval. 2. **Custom Collate Function:** The `custom_collate_fn` function pads mini-batches to the same length, converting sequences into tensors using the highest length within the batch. 3. **DataLoader:** The `create_data_loader` function sets up a `DataLoader` with the custom dataset, `num_workers`, and the collate function. It uses memory pinning to speed up the transfer of data to CUDA-enabled GPUs. 4. **Validation:** The `validate_data_loader` function iterates over batches produced by the `DataLoader` and prints the shape of each batch to ensure correctness and no data duplication during multi-process loading. Your solution should follow these structures and implement the functionalities as specified.","solution":"import torch from torch.utils.data import Dataset, DataLoader from torch.nn.utils.rnn import pad_sequence # Step 1: Implement the CustomDataset class class CustomDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): # Return the element at the given index return torch.tensor(self.data[index], dtype=torch.float) def __len__(self): # Return the size of the dataset return len(self.data) # Step 2: Implement the custom collate function def custom_collate_fn(batch): # Pad sequences to the maximum length in the batch and convert to tensor return pad_sequence(batch, batch_first=True, padding_value=0.0) # Step 3: Initialize DataLoader with CustomDataset and custom_collate_fn def create_data_loader(data, num_workers): dataset = CustomDataset(data) data_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=num_workers, collate_fn=custom_collate_fn, pin_memory=True) return data_loader # Validation def validate_data_loader(data_loader): for batch in data_loader: # Print the shape of each batch of data print(\'Batch shape:\', batch.shape) # Example usage if __name__ == \'__main__\': data = [[1], [2, 2], [3, 3, 3], [4, 4, 4, 4], [5], [6, 6], [7, 7, 7], [8, 8, 8, 8]] num_workers = 2 data_loader = create_data_loader(data, num_workers) validate_data_loader(data_loader)"},{"question":"**Title:** Safe and Concurrent Mailbox Management with Format Conversion **Objective:** Demonstrate your understanding of the `mailbox` module by implementing a function that safely copies all messages from a source mailbox (in one format) to a destination mailbox (in another format), ensuring no messages are lost or duplicated, and handling concurrent access appropriately. **Task:** Write a Python function `secure_copy_mailbox(source_path: str, source_format: str, destination_path: str, destination_format: str) -> None` that performs the following: 1. **Parameters:** - `source_path`: The file path of the source mailbox. - `source_format`: The format of the source mailbox. It can be one of [\'Maildir\', \'mbox\', \'MH\', \'Babyl\', \'MMDF\']. - `destination_path`: The file path of the destination mailbox. - `destination_format`: The format of the destination mailbox. It can be one of [\'Maildir\', \'mbox\', \'MH\', \'Babyl\', \'MMDF\']. 2. **Behavior:** - Initialize the source and destination mailboxes based on the given formats. - Lock the source mailbox before reading any messages and ensure it remains locked while messages are read or deleted. - For each message in the source mailbox: - Add the message to the destination mailbox in the format-specific manner. - Ensure messages are not duplicated; each message should be securely copied to the destination. - Remove the message from the source mailbox only after itâ€™s successfully added to the destination. - Handle any potential parsing errors or mailbox access issues gracefully without crashing. - Unlock and safely close both mailboxes once operations are complete. 3. **Constraints:** - Ensure the mailbox operations are thread-safe and handle concurrent modification scenarios. - Handle potential exceptions raised during mailbox operations, logging appropriate messages/errors. **Example Usage:** ```python secure_copy_mailbox(\'/path/to/source\', \'mbox\', \'/path/to/destination\', \'Maildir\') ``` **Note:** - You may assume that the source and destination directories are accessible and writable. - The function should not return any value but should effectively copy all messages from the source to the destination while adhering to the mentioned constraints. Implementations of proper exception handling and mailbox locking mechanisms will be key to the success of this function.","solution":"import mailbox import os def secure_copy_mailbox(source_path: str, source_format: str, destination_path: str, destination_format: str) -> None: Copies all messages from a source mailbox to a destination mailbox safely and concurrently. :param source_path: The file path of the source mailbox. :param source_format: The format of the source mailbox. It can be one of [\'Maildir\', \'mbox\', \'MH\', \'Babyl\', \'MMDF\']. :param destination_path: The file path of the destination mailbox. :param destination_format: The format of the destination mailbox. It can be one of [\'Maildir\', \'mbox\', \'MH\', \'Babyl\', \'MMDF\']. source_factory = getattr(mailbox, source_format) destination_factory = getattr(mailbox, destination_format) try: # Initialize source and destination mailboxes source_mb = source_factory(source_path) destination_mb = destination_factory(destination_path) # Lock the source mailbox source_mb.lock() try: # Iterate over each message in source mailbox for message_key, message in source_mb.items(): # Add message to destination mailbox destination_mb.add(message) # Remove message from source mailbox (after confirmation) source_mb.remove(message_key) finally: # Ensure we unlock the source mailbox no matter what source_mb.unlock() # Ensure the destination mailbox is properly closed and changes are saved destination_mb.close() source_mb.close() except Exception as e: print(f\\"An error occurred during mailbox operations: {e}\\")"},{"question":"You are required to develop a Python script that simulates a simplified version of a command-line utility for processing files. This utility should accept specific command-line options to configure its behavior. You are to use the `getopt` module for parsing the command-line options. Your task is to implement a function called `process_args(args)` that performs the following: 1. **Arguments of the utility:** - Short options: - `-h`: Display help message. - `-v`: Enable verbose mode. - `-o <output>`: Specify the output file. - Long options: - `--help`: Display help message. - `--verbose`: Enable verbose mode. - `--output <output>`: Specify the output file. 2. **Function Behavior:** - Parse the command-line arguments. - Handle the options as follows: - If the help option `-h` or `--help` is encountered, the function should return the string \\"Help: Usage instructions...\\". - If the verbose option `-v` or `--verbose` is set, include it in the results. - The output option `-o` or `--output` should capture the file name and include it in the results. - If any unrecognized option or missing argument is encountered, raise the appropriate error with a relevant message. 3. **Input:** - A list of command-line arguments `args`, similar to `sys.argv[1:]`. 4. **Output:** - Return a dictionary with the following keys if applicable: - `\\"help\\"`: \\"Help: Usage instructions...\\" if help option is present. - `\\"verbose\\"`: `True` if verbose mode is enabled. - `\\"output\\"`: The specified output file name. 5. **Constraints:** - Optional arguments are not supported. - Assume that all arguments will be provided in a valid format (except for missing arguments or unknown options which should be handled). # Example: ```python import getopt def process_args(args): Process the command-line arguments passed to the script. Args: - args (list): Command-line arguments to be parsed. Returns: - dict: Parsed options and their values. # Your code here # Example usage args = [\\"-v\\", \\"--output=result.txt\\", \\"file1\\", \\"file2\\"] result = process_args(args) # Output: {\'verbose\': True, \'output\': \'result.txt\'} args = [\\"--help\\"] result = process_args(args) # Output: {\'help\': \'Help: Usage instructions...\'} ``` # Note: - You are not required to implement the actual file processing logic. Your focus should be on accurately parsing the command-line arguments. - Make sure to include comprehensive error handling. - You can use `print` statements or `raise` exceptions where appropriate for error cases.","solution":"import getopt def process_args(args): Process the command-line arguments passed to the script. Args: - args (list): Command-line arguments to be parsed. Returns: - dict: Parsed options and their values. options = \\"hvo:\\" long_options = [\\"help\\", \\"verbose\\", \\"output=\\"] parsed_options = {} try: opts, _ = getopt.getopt(args, options, long_options) except getopt.GetoptError as err: raise ValueError(f\\"Invalid option or missing argument: {err}\\") for opt, arg in opts: if opt in (\'-h\', \'--help\'): parsed_options[\\"help\\"] = \\"Help: Usage instructions...\\" elif opt in (\'-v\', \'--verbose\'): parsed_options[\\"verbose\\"] = True elif opt in (\'-o\', \'--output\'): parsed_options[\\"output\\"] = arg else: raise ValueError(f\\"Unrecognized option: {opt}\\") return parsed_options"},{"question":"Objective: To assess the ability to use the `shlex` module for creating a lexical analyzer to parse shell-like command strings. Problem Statement: You are tasked with writing a function `parse_command_line(command_line: str) -> list` that parses a given shell-like command string into a list of tokens. The function should handle quoted strings, escape characters, and should split tokens properly according to the rules of the POSIX shell. Additionally, the function should handle nested commands by using custom source inclusion logic. Requirements: 1. The function should use the `shlex.shlex` class for parsing. 2. The function should operate in POSIX mode. 3. The function should use the `shlex.push_source()` and `shlex.pop_source()` methods to support nested commands. 4. The function should handle comments (starting with \\"#\\") by ignoring them. 5. The function should capture errors during parsing and return an appropriate message along with the line number and filename. Input and Output: - **Input:** A string `command_line` representing the shell command. - **Output:** A list of parsed tokens, or a string containing the error message with line number and filename if an error occurs. Constraints: - The input command string may contain nested commands indicated by `source <filename>`. - The input command string follows the POSIX shell parsing rules. - The maximum length of the command string is 1024 characters. Example: ```python def parse_command_line(command_line: str) -> list: # Your implementation here # Example usage command = echo \\"This is a test\\" && source script.sh result = parse_command_line(command) print(result) # Output: [\'echo\', \'This is a test\', \'&&\', \'source\', \'script.sh\'] ``` Here is a test case to consider: ```python def test_parse_command_line(): command1 = \'echo \\"Welcome to the test\\" && ls -la\' assert parse_command_line(command1) == [\'echo\', \'Welcome to the test\', \'&&\', \'ls\', \'-la\'] command2 = \'# This is a commentnecho \\"Another test\\"\' assert parse_command_line(command2) == [\'echo\', \'Another test\'] command3 = \'source nested_script.sh\' assert parse_command_line(command3) == [\'source\', \'nested_script.sh\'] command4 = \'ls -l | grep \\"test.txt\\"\' assert parse_command_line(command4) == [\'ls\', \'-l\', \'|\', \'grep\', \'test.txt\'] test_parse_command_line() ``` [End of Question]","solution":"import shlex def parse_command_line(command_line: str) -> list: Parses a given shell-like command string into a list of tokens. lexer = shlex.shlex(command_line, posix=True) lexer.whitespace_split = True lexer.commenters = \'#\' tokens = [] try: for token in lexer: tokens.append(token) if token == \'source\': # Read the next filename token filename = next(lexer) tokens.append(filename) # Further processing can be done here for nested commands. # This is where you\'d handle the inclusion of scripts and nested commands. return tokens except Exception as e: return f\\"Error: {str(e)}, Line: {lexer.lineno}, Filename: {lexer.infile}\\""},{"question":"**Coding Question: Visualization with Seaborn** **Objective:** Create a comprehensive visualization that displays the distribution and relationships of specific variables in a dataset using seaborn\'s `histplot` function. **Problem Statement:** You have been provided with the `tips` dataset from the seaborn library, which contains information about the tips received by servers in a restaurant. Your task is to generate a visualization that includes both univariate and bivariate histograms. **Requirements:** 1. **Load the `tips` dataset** using `seaborn.load_dataset()`. 2. **Create a univariate histogram** showing the distribution of `total_bill` values. Visualize the histogram with the following customizations: - Apply a kernel density estimate. - Set the number of bins to 20. - Use a blue color for the bars. 3. **Create a bivariate histogram** to visualize the relationship between `total_bill` and `tip`: - Use a heatmap to represent the counts. - Set the number of bins for `total_bill` to 20 and `tip` to 10. - Add a colorbar to the heatmap. 4. **Normalize the histogram** for the `total_bill` such that the height of each bar represents the probability distribution. 5. **Enhance the visualizations** by adding appropriate titles, axis labels, and formatting for better readability. **Input:** - None directly, but the dataset will be loaded within the function. **Output:** - The function should display the described visualizations using seaborn and matplotlib. **Constraints:** - Use seaborn\'s `histplot` function for both histograms. - Apply customization settings provided in the requirements precisely. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Your code here... # Call the function to create and show visualizations create_visualizations() ``` **Additional Notes:** - Ensure proper usage of seaborn\'s color palette and themes to make the plots visually appealing. - You may use additional seaborn/matplotlib functions for labels, titles, and formatting.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Univariate histogram for \'total_bill\' plt.figure(figsize=(10, 6)) sns.histplot(tips[\'total_bill\'], bins=20, kde=True, color=\\"blue\\") plt.title(\'Distribution of Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Frequency\') plt.show() # Bivariate histogram for \'total_bill\' and \'tip\' plt.figure(figsize=(10, 6)) sns.histplot(data=tips, x=\'total_bill\', y=\'tip\', bins=[20, 10], cbar=True) plt.title(\'Bivariate Histogram of Total Bill vs Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Normalized histogram for \'total_bill\' plt.figure(figsize=(10, 6)) sns.histplot(tips[\'total_bill\'], bins=20, kde=True, color=\\"blue\\", stat=\'probability\') plt.title(\'Normalized Distribution of Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Probability\') plt.show() # Call the function to create and show visualizations create_visualizations()"},{"question":"You are tasked with implementing a function `closest_fraction_sum` that takes a list of strings representing floating-point numbers and an integer `k`. The function should return a list of `k` fractions whose sum is closest to the sum of fractions converted from the given list of strings. Each fraction should have the smallest possible denominator that does not exceed a limit of 1,000,000. # Input - `float_strings`: a list of strings, each representing a floating-point number. - `k`: an integer representing the desired number of fractions in the output list. # Output - A list of `k` fractions (instances of `fractions.Fraction`) whose sum is closest to the sum of the original fractions derived from the input strings, with each fraction having a minimized denominator. # Constraints - All strings in `float_strings` are valid representations acceptable by the `fraction.Fraction` constructor. - The value of `k` is less than or equal to the length of `float_strings`. # Examples ```python from fractions import Fraction from typing import List def closest_fraction_sum(float_strings: List[str], k: int) -> List[Fraction]: # Your code goes here. pass # Example usage: float_strings = [\\"1.5\\", \\"2.75\\", \\"3.25\\"] k = 2 result = closest_fraction_sum(float_strings, k) print(result) # Expected output: a list of two fractions whose sum is closest to the sum of original fractions. ``` # Requirements - Use the `fractions.Fraction` class and its methods to convert strings to fractions. - Employ the `limit_denominator(max_denominator=1000000)` method to ensure fractions have the smallest possible denominator. - Focus on obtaining the `k` fractions whose sum is the closest match. # Notes - Consider edge cases where the input list might contain numbers that are very close to each other or very small floating-point numbers. - Ensure the performance of the function is optimized for the given constraints.","solution":"from fractions import Fraction from typing import List def closest_fraction_sum(float_strings: List[str], k: int) -> List[Fraction]: # Convert the input strings to Fraction objects fractions = [Fraction(f) for f in float_strings] # Sort fractions by their values fractions.sort(key=lambda x: float(x)) # Since we want the sum to be closest, we simply take the first k fractions # This heuristic works by taking the smallest fractions to minimize the error closest_fractions = fractions[:k] # Ensure each fraction has the smallest possible denominator closest_fractions = [f.limit_denominator(max_denominator=1000000) for f in closest_fractions] return closest_fractions"},{"question":"Objective: Your task is to demonstrate your understanding of loading datasets using the scikit-learn library, converting and preprocessing the data, and performing a simple machine learning task. Problem Statement: You need to perform the following tasks: 1. **Load a dataset from OpenML**: - Load the \\"miceprotein\\" dataset from the OpenML repository. - Make sure to specify the dataset by name and ensure it is always fetched with the correct version. 2. **Preprocess the data**: - Convert the categorical target labels into numerical labels using an appropriate encoder. - Normalize the feature data so that it is suitable for training a machine learning model. 3. **Train a simple classifier**: - Split the dataset into a training set (80%) and a test set (20%). - Train a `RandomForestClassifier` on the training data. - Evaluate the classifier on the test data and print the accuracy score. Function Signature: ```python def load_and_train_miceprotein(): Load the miceprotein dataset from OpenML, preprocess it, and train a Random Forest classifier. 1. Load the dataset from OpenML. 2. Convert categorical target labels into numerical labels. 3. Normalize the feature data. 4. Split the data into training and test sets. 5. Train a RandomForestClassifier. 6. Evaluate the classifier and print the accuracy score. Returns: None ``` Constraints: - You may use any relevant functions or modules provided by scikit-learn. - You must ensure that the dataset is preprocessed appropriately before training. - The training and evaluation should be done using scikit-learn\'s `model_selection` and `ensemble` modules. Example: Your function should be able to load the data, preprocess it, train the classifier, and output the accuracy score in a step-by-step manner, similar to the following: ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder, StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def load_and_train_miceprotein(): # Load the dataset from OpenML mice = fetch_openml(name=\'miceprotein\', version=4) # Preprocess the data X = mice.data y = mice.target le = LabelEncoder() y_encoded = le.fit_transform(y) scaler = StandardScaler() X_normalized = scaler.fit_transform(X) # Split the data X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_encoded, test_size=0.2, random_state=42) # Train the model clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") # Run the function load_and_train_miceprotein() ``` Your implementation should follow a similar pattern, but you can include additional validation, comments, or improvements as needed.","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder, StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def load_and_train_miceprotein(): # Load the dataset from OpenML mice = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) # Preprocess the data X = mice.data y = mice.target le = LabelEncoder() y_encoded = le.fit_transform(y) scaler = StandardScaler() X_normalized = scaler.fit_transform(X) # Split the data X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_encoded, test_size=0.2, random_state=42) # Train the model clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") # Run the function load_and_train_miceprotein()"},{"question":"Coding Assessment Question # Objective Write a Python function that manipulates environment variables using the `os` module. The function should retrieve, add, and remove environment variables and ensure proper error handling. # Function Signature ```python def manage_environment_variables(action: str, key: str, value: Optional[str] = None) -> Union[None, str, Dict[str, str]]: pass ``` # Input 1. `action` (str): The operation to perform. It can be one of the following: - `\\"get\\"`: Retrieve the value of an environment variable. - `\\"set\\"`: Set the value of an environment variable. - `\\"delete\\"`: Delete an environment variable. - `\\"list\\"`: List all environment variables. 2. `key` (str): The name of the environment variable for `\\"get\\"`, `\\"set\\"`, and `\\"delete\\"` actions. 3. `value` (str, optional): The value to set for the environment variable for the `\\"set\\"` action. # Output - For `\\"get\\"`, return the value of the specified environment variable as a `str`. - For `\\"set\\"`, return `None`. If the environment variable was successfully set, the subsequent retrieval of this variable should reflect the new value. - For `\\"delete\\"`, return `None`. If the environment variable was successfully deleted, the subsequent retrieval of this variable should reflect it is unset (return `None`). - For `\\"list\\"`, return a dictionary of all environment variables where keys are the variable names and values are their corresponding values. # Constraints 1. If an action is not supported, raise a `ValueError` with the message `\\"Unsupported action\\"`. 2. If trying to get or delete an environment variable that does not exist, raise a `KeyError` with the message `\\"Environment variable not found\\"`. # Example ```python # Assuming the environment variable \\"EXAMPLE_VAR\\" does not initially exist. manage_environment_variables(\\"set\\", \\"EXAMPLE_VAR\\", \\"test_value\\") # Expected output: None manage_environment_variables(\\"get\\", \\"EXAMPLE_VAR\\") # Expected output: \\"test_value\\" manage_environment_variables(\\"delete\\", \\"EXAMPLE_VAR\\") # Expected output: None manage_environment_variables(\\"get\\", \\"EXAMPLE_VAR\\") # Expected output: KeyError: Environment variable not found manage_environment_variables(\\"list\\", \\"\\") # Expected output: Dictionary of current environment variables ``` # Implementation Notes 1. Use the `os.environ` dictionary for accessing and modifying environment variables. 2. Handle errors appropriately using try-except blocks to ensure the function behaves as described. 3. Ensure compatibility such that the function works consistently across different operating systems (Unix-based and Windows).","solution":"import os from typing import Optional, Union, Dict def manage_environment_variables(action: str, key: str, value: Optional[str] = None) -> Union[None, str, Dict[str, str]]: Manages environment variables by performing the specified action. Parameters: - action (str): The operation to perform (\'get\', \'set\', \'delete\', \'list\'). - key (str): The name of the environment variable for \'get\', \'set\', and \'delete\' actions. - value (str, optional): The value to set for the \'set\' action. Returns: - None for \'set\' and \'delete\' actions. - The value of the environment variable for the \'get\' action. - A dictionary of all environment variables for the \'list\' action. Raises: - ValueError: If the action is not supported. - KeyError: If the \'get\' or \'delete\' action is performed on a non-existent environment variable. if action == \\"get\\": try: return os.environ[key] except KeyError: raise KeyError(\\"Environment variable not found\\") elif action == \\"set\\": os.environ[key] = value return None elif action == \\"delete\\": if key in os.environ: del os.environ[key] else: raise KeyError(\\"Environment variable not found\\") return None elif action == \\"list\\": return dict(os.environ) else: raise ValueError(\\"Unsupported action\\")"},{"question":"**Objective**: Demonstrate understanding of Python\'s `contextvars` module by implementing a system that manages context-specific data in a simulated concurrent environment. # Problem Statement You need to implement a utility that evaluates user tasks with context-specific configurations. This utility should leverage the `contextvars` module to ensure that each task can run independently with its own context. Your solution must perform the following tasks: 1. Create and initialize a `ContextVar` named `config` with a default value of `\\"default_config\\"`. 2. Implement a function `task_runner(callback, config_value)` that: - Sets `config_value` to the `config` context variable. - Runs the provided `callback` function within this context. - Resets the `config` context variable to its previous value after the callback is executed. 3. Implement a main function `main()` that: - Creates two example tasks `task1` and `task2` using `task_runner()`. - For both tasks, define simple callback functions that print their respective `config` values, so itâ€™s clear that each task runs with its own context configuration. - Runs the tasks in series, demonstrating that the contexts do not interfere with each other. # Dependencies Ensure you utilize the `contextvars` module as outlined in the provided documentation. # Input and Output - **Input**: No direct input; the script defines and runs the tasks internally. - **Output**: Print statements showing the context-specific configuration for each task. # Constraints - Tasks must run synchronously, but independently, ensuring that one task\'s context does not affect another\'s. - You are not allowed to use any global or non-context variable to manage state between tasks. # Example ```python from contextvars import ContextVar, Token # Step 1: Initialize the ContextVar config = ContextVar(\'config\', default=\'default_config\') # Step 2: Implement task runner def task_runner(callback, config_value): token = config.set(config_value) try: callback() finally: config.reset(token) def main(): def task1(): print(f\'Task 1 config: {config.get()}\') def task2(): print(f\'Task 2 config: {config.get()}\') # Run tasks with different configurations task_runner(task1, \'config_1\') task_runner(task2, \'config_2\') # Execute the main function if __name__ == \\"__main__\\": main() # Expected Output: # Task 1 config: config_1 # Task 2 config: config_2 ``` Implement your solution below: ```python from contextvars import ContextVar, Token # Step 1: Initialize the ContextVar config = ContextVar(\'config\', default=\'default_config\') # Step 2: Implement task runner def task_runner(callback, config_value): token = config.set(config_value) try: callback() finally: config.reset(token) def main(): def task1(): print(f\'Task 1 config: {config.get()}\') def task2(): print(f\'Task 2 config: {config.get()}\') # Run tasks with different configurations task_runner(task1, \'config_1\') task_runner(task2, \'config_2\') # Execute the main function if __name__ == \\"__main__\\": main() ```","solution":"from contextvars import ContextVar, Token # Step 1: Initialize the ContextVar config = ContextVar(\'config\', default=\'default_config\') # Step 2: Implement task runner def task_runner(callback, config_value): token = config.set(config_value) try: callback() finally: config.reset(token) def main(): def task1(): print(f\'Task 1 config: {config.get()}\') def task2(): print(f\'Task 2 config: {config.get()}\') # Run tasks with different configurations task_runner(task1, \'config_1\') task_runner(task2, \'config_2\') # Execute the main function if __name__ == \\"__main__\\": main()"},{"question":"Objective This assessment will test your ability to write and optimize PyTorch code using the torch.utils.bottleneck profiler. You will create a PyTorch model with intentional bottlenecks, use the bottleneck tool to identify performance issues, and then optimize the model based on the profiling results. # Task 1. **Implement a PyTorch Model**: - Write a simple PyTorch model for a classification task on a randomly generated dataset. - Deliberately introduce performance inefficiencies in the model (e.g., unnecessary computations, sub-optimal use of tensor operations, etc.). 2. **Execute the Model**: - Write a script to execute this model on the randomly generated dataset, ensuring that the script runs for a finite amount of time. 3. **Profile the Model using torch.utils.bottleneck**: - Run the torch.utils.bottleneck profiler on your script to identify the performance bottlenecks. 4. **Optimize the Model**: - Based on the profiler output, optimize the model to address the identified bottlenecks. - Write a brief explanation of the changes you made and how they improve the model\'s performance. # Input and Output 1. **PyTorch Model Implementation**: - The model should take an input tensor of shape (batch_size, input_features) and output a tensor of shape (batch_size, num_classes). - Include deliberate inefficiencies in your model. 2. **Script Execution**: - Ensure to include code to generate a random dataset appropriate for your model. - The script should run for a finite amount of time. 3. **Profiler Execution**: - Use the `torch.utils.bottleneck` profiler and save the output to a file. 4. **Model Optimization**: - Implement and describe the optimizations in your model to improve its performance based on the profiling output. # Constraints - You must use PyTorch for all model implementation and profiling tasks. - The model and script should be executable on both CPU and GPU (if available). # Example Here is a skeleton of what your answer might look like: ```python import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as data # Step 1: Implement a PyTorch Model with Deliberate Inefficiencies class InefficientModel(nn.Module): def __init__(self, input_features, num_classes): super(InefficientModel, self).__init__() self.layer1 = nn.Linear(input_features, 128) self.layer2 = nn.Linear(128, 128) # Inefficiency: repetitive and unnecessary layers self.layer3 = nn.Linear(128, num_classes) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.layer1(x)) x = self.relu(self.layer2(x)) # Inefficiency: redundant computation x = self.relu(self.layer2(x)) # Inefficiency: redundant computation x = self.layer3(x) return x # Step 2: Execute the Model def run_model(): input_features = 64 num_classes = 10 batch_size = 32 num_batches = 100 model = InefficientModel(input_features, num_classes) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Generating random data inputs = torch.randn(batch_size * num_batches, input_features) targets = torch.randint(0, num_classes, (batch_size * num_batches,)) dataset = data.TensorDataset(inputs, targets) dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True) for inputs, targets in dataloader: outputs = model(inputs) loss = criterion(outputs, targets) optimizer.zero_grad() loss.backward() optimizer.step() if __name__ == \'__main__\': run_model() # Use the torch.utils.bottleneck profiler to profile this script # Step 4: Model Optimization based on profiler output (to be implemented by the student) ``` # Instructions for Submission 1. Provide your PyTorch model and execution script that includes deliberate inefficiencies. 2. Include the output file from running the bottleneck profiler. 3. Optimize the model based on the profiling results and describe your changes.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as data # Step 1: Implement a PyTorch Model with Deliberate Inefficiencies class InefficientModel(nn.Module): def __init__(self, input_features, num_classes): super(InefficientModel, self).__init__() self.layer1 = nn.Linear(input_features, 256) self.layer2 = nn.Linear(256, 256) # Inefficiency: repetitive and unnecessary layers self.layer3 = nn.Linear(256, 256) # Inefficiency: repetitive and unnecessary layers self.layer4 = nn.Linear(256, num_classes) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.layer1(x)) x = self.relu(self.layer2(x)) # Inefficiency: redundant computation x = self.relu(self.layer3(x)) # Inefficiency: redundant computation x = self.layer4(x) return x # Step 2: Execute the Model def run_model(): input_features = 64 num_classes = 10 batch_size = 32 num_batches = 100 model = InefficientModel(input_features, num_classes) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Generating random data inputs = torch.randn(batch_size * num_batches, input_features) targets = torch.randint(0, num_classes, (batch_size * num_batches,)) dataset = data.TensorDataset(inputs, targets) dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True) for inputs, targets in dataloader: outputs = model(inputs) loss = criterion(outputs, targets) optimizer.zero_grad() loss.backward() optimizer.step() if __name__ == \'__main__\': run_model() # Example output of the profiler (to be generated by user using `torch.utils.bottleneck`) # This would help identify the bottlenecks and allow for optimization in the next step."},{"question":"You are asked to implement a custom initialization function in PyTorch that utilizes several of the built-in initialization functions to initialize the parameters of a given model in an optimal way. This will test your understanding of PyTorch\'s initialization functions and how they can be used to set up neural networks. # Custom Initialization Function Objective Write a function `custom_initialize(model)` that initializes the parameters of the given PyTorch model. The initialization should satisfy the following requirements: 1. For convolutional layers (`torch.nn.Conv2d`), use `kaiming_normal_` initialization. 2. For linear layers (`torch.nn.Linear`), use `xavier_uniform_` initialization. 3. For batch normalization layers (`torch.nn.BatchNorm2d`), initialize the weights with a normal distribution and the biases with a constant value of 0. Constraints - Use only functions from the `torch.nn.init` module to initialize the parameters. - The model can have multiple types of layers, and you need to check each layer type and apply the appropriate initialization. - You need to handle layers in nested modules (e.g., `torch.nn.Sequential`). Input - `model`: an instance of `torch.nn.Module` representing the neural network. Output - The function does not need to return any value. The modifications should be made in-place on the given model. Example ```python import torch import torch.nn as nn import torch.nn.init as init def custom_initialize(model): for layer in model.modules(): if isinstance(layer, nn.Conv2d): init.kaiming_normal_(layer.weight) if layer.bias is not None: init.zeros_(layer.bias) elif isinstance(layer, nn.Linear): init.xavier_uniform_(layer.weight) if layer.bias is not None: init.zeros_(layer.bias) elif isinstance(layer, nn.BatchNorm2d): init.normal_(layer.weight, mean=1.0, std=0.02) init.constant_(layer.bias, 0.0) # Example usage class ExampleModel(nn.Module): def __init__(self): super(ExampleModel, self).__init__() self.conv = nn.Conv2d(3, 16, kernel_size=3) self.bn = nn.BatchNorm2d(16) self.fc = nn.Linear(16*26*26, 10) def forward(self, x): x = self.conv(x) x = self.bn(x) x = torch.relu(x) x = x.view(x.size(0), -1) x = self.fc(x) return x model = ExampleModel() custom_initialize(model) ``` Evaluation - Ensure that each type of layer is initialized with the correct procedure. - For convolutional layers, verify that `kaiming_normal_` is used. - For linear layers, verify that `xavier_uniform_` is used. - For batch normalization layers, check that weights are initialized with a normal distribution and biases with a constant value.","solution":"import torch import torch.nn as nn import torch.nn.init as init def custom_initialize(model): Custom initialization function for initializing the parameters of the given PyTorch model. Args: model: The neural network model (nn.Module) to be initialized. This function initializes: - Convolution layers using kaiming_normal_ for weights. - Linear layers using xavier_uniform_ for weights. - Batch normalization layers\' weights with a normal distribution and biases with a constant value of 0. for layer in model.modules(): if isinstance(layer, nn.Conv2d): init.kaiming_normal_(layer.weight) if layer.bias is not None: init.zeros_(layer.bias) elif isinstance(layer, nn.Linear): init.xavier_uniform_(layer.weight) if layer.bias is not None: init.zeros_(layer.bias) elif isinstance(layer, nn.BatchNorm2d): init.normal_(layer.weight, mean=1.0, std=0.02) init.constant_(layer.bias, 0.0)"},{"question":"# Seaborn Style and Plot Customization **Objective**: Your task is to demonstrate your understanding of seaborn\'s styling and plotting functionalities. **Problem Statement**: Write a Python function `custom_seaborn_plot` that: 1. Displays the current default axes style settings. 2. Displays the axes style settings for the predefined \\"darkgrid\\" style. 3. Creates a `barplot` using the \\"whitegrid\\" style context with the following data: - x-axis: [2015, 2016, 2017, 2018, 2019] - y-axis: [5, 7, 8, 6, 9] The function should return a dictionary containing: - The current default axes style settings. - The axes style settings for \\"darkgrid\\". **Function Signature**: ```python def custom_seaborn_plot() -> dict: pass ``` **Expected Output**: The function should create a bar plot using the \\"whitegrid\\" style and return a dictionary in the following format: ```python { \\"default_style\\": <style_parameters>, \\"darkgrid_style\\": <style_parameters> } ``` Where `<style_parameters>` is a dictionary containing the style settings. **Constraints and Requirements**: - You must use seaborn for all plotting. - Ensure that the bar plot is displayed with the \\"whitegrid\\" style. - The function should return the required style settings dictionaries as the output. **Example Usage**: ```python # When the function is called result = custom_seaborn_plot() print(result) # Output might look like: { \\"default_style\\": { \\"axes.facecolor\\": \\"white\\", \\"axes.edgecolor\\": \\".15\\", ... }, \\"darkgrid_style\\": { \\"axes.facecolor\\": \\"#EAEAF2\\", \\"axes.edgecolor\\": \\"white\\", ... } } ``` # Note - Ensure that you import necessary libraries such as seaborn and matplotlib before defining your function. - The bar plot should be displayed as a part of the function execution, not returned.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plot() -> dict: # Display the current default axes style settings default_style = sns.axes_style() # Display the axes style settings for the predefined \\"darkgrid\\" style darkgrid_style = sns.axes_style(\\"darkgrid\\") # Create a barplot using the \\"whitegrid\\" style context sns.set_style(\\"whitegrid\\") data_years = [2015, 2016, 2017, 2018, 2019] data_values = [5, 7, 8, 6, 9] plt.figure(figsize=(10, 5)) sns.barplot(x=data_years, y=data_values) plt.title(\\"Barplot with \'whitegrid\' style\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Values\\") plt.show() return { \\"default_style\\": default_style, \\"darkgrid_style\\": darkgrid_style }"},{"question":"**Subprocess Management with the `subprocess` module** *Objective*: Demonstrate your understanding of how to manage processes using Python\'s `subprocess` module. *Problem Statement:* Write a function `execute_command` that takes a list of command-line arguments, runs the command within a subprocess, captures its output, and handles any exceptions that may occur. *Function Signature*: ```python def execute_command(command: list, timeout: int = 10) -> dict: Executes a command in a subprocess and captures its stdout and stderr. Args: - command (list): A list of strings representing the command and its arguments. For example: [\\"ls\\", \\"-l\\"] - timeout (int): Maximum number of seconds to allow for the command to run before terminating it. Returns: - dict: A dictionary with the following keys: - \'stdout\': The captured stdout of the command, or None if not captured. - \'stderr\': The captured stderr of the command, or None if not captured. - \'returncode\': The exit status of the command. - \'error\': Description of any error that occurred, or None if the command executed successfully. Raises: - ValueError: If the provided command is not a list. - TypeError: If timeout is not an integer. The function should also catch and handle exceptions related to process execution, such as subprocess.TimeoutExpired. Note: Do not use \'shell=True\' for security reasons. pass ``` *Constraints*: 1. The function should verify that `command` is a list and `timeout` is an integer. 2. The function should use `subprocess.run()` for simplicity. If more control over the subprocess is required, it should switch to `Popen`. 3. The function should handle common subprocess-related exceptions and return detailed error descriptions. 4. `stdout` and `stderr` should be captured and returned in text mode. *Example*: ```python command = [\\"ls\\", \\"-l\\", \\"/nonexistent\\"] result = execute_command(command, timeout=5) print(result) # Expected output (assuming /nonexistent does not exist): # { # \'stdout\': \'\', # \'stderr\': \'ls: /nonexistent: No such file or directoryn\', # \'returncode\': 1, # \'error\': None # } command = [\\"sleep\\", \\"15\\"] result = execute_command(command, timeout=5) print(result) # Expected output (assuming the sleep command takes longer than the timeout): # { # \'stdout\': \'\', # \'stderr\': \'\', # \'returncode\': -1, # \'error\': \'Command timed out after 5 seconds\' # } ``` *Additional Notes*: 1. Ensure that the function handles subprocess wrapping properly to avoid deadlocks and manage process termination cleanly. 2. Provide additional test cases covering various scenarios as part of your solution.","solution":"import subprocess def execute_command(command: list, timeout: int = 10) -> dict: Executes a command in a subprocess and captures its stdout and stderr. Args: - command (list): A list of strings representing the command and its arguments. For example: [\\"ls\\", \\"-l\\"] - timeout (int): Maximum number of seconds to allow for the command to run before terminating it. Returns: - dict: A dictionary with the following keys: - \'stdout\': The captured stdout of the command, or None if not captured. - \'stderr\': The captured stderr of the command, or None if not captured. - \'returncode\': The exit status of the command. - \'error\': Description of any error that occurred, or None if the command executed successfully. Raises: - ValueError: If the provided command is not a list. - TypeError: If timeout is not an integer. The function should also catch and handle exceptions related to process execution, such as subprocess.TimeoutExpired. Note: Do not use \'shell=True\' for security reasons. if not isinstance(command, list): raise ValueError(\\"The command should be a list.\\") if not isinstance(timeout, int): raise TypeError(\\"The timeout should be an integer.\\") try: result = subprocess.run(command, capture_output=True, text=True, timeout=timeout) return { \'stdout\': result.stdout, \'stderr\': result.stderr, \'returncode\': result.returncode, \'error\': None } except subprocess.TimeoutExpired: return { \'stdout\': None, \'stderr\': None, \'returncode\': -1, \'error\': f\\"Command timed out after {timeout} seconds\\" } except subprocess.CalledProcessError as e: return { \'stdout\': e.stdout, \'stderr\': e.stderr, \'returncode\': e.returncode, \'error\': str(e) } except Exception as e: return { \'stdout\': None, \'stderr\': None, \'returncode\': -1, \'error\': str(e) }"},{"question":"You are tasked with creating a robust student GradeBook using the `dataclasses` module. The GradeBook should be capable of storing student information, recording multiple grades per student, calculating average grades, and supporting various other class operations. Specifications: 1. Define a `Student` data class that includes the following fields: - `name` (str): The name of the student. - `grades` (List[float]): A list to store the student\'s grades, initialized to an empty list using a default factory. 2. Add a method `add_grade` to the `Student` class that accepts a single grade (float) and appends it to the `grades` list. 3. Add a method `average_grade` to the `Student` class that returns the average of the grades. If there are no grades, it should return `None`. 4. Define a `GradeBook` data class that includes the following fields: - `students` (List[Student]): A list of `Student` objects, initialized to an empty list using a default factory. 5. Add methods to `GradeBook`: - `add_student` which accepts a `Student` object and adds it to the list of students. - `calculate_class_average` which returns the average grade of all students combined. If there are no students, it should return `None`. 6. Ensure immutability by freezing the data classes. Constraints: - Use type annotations for all fields. - Handle edge cases where there are no grades or no students. - Test your implementation with multiple students and grades. - Include at least one example of inheritance by extending functionality of the `Student` class or creating a subclass. Example Usage: ```python # Illustrative example of usage: from dataclasses import dataclass, field from typing import List, Optional @dataclass(frozen=True) class Student: name: str grades: List[float] = field(default_factory=list) def add_grade(self, grade: float) -> None: self.grades.append(grade) def average_grade(self) -> Optional[float]: if not self.grades: return None return sum(self.grades) / len(self.grades) @dataclass(frozen=True) class GradeBook: students: List[Student] = field(default_factory=list) def add_student(self, student: Student) -> None: self.students.append(student) def calculate_class_average(self) -> Optional[float]: total_grades = [] for student in self.students: total_grades.extend(student.grades) if not total_grades: return None return sum(total_grades) / len(total_grades) # Define student objects student1 = Student(name=\\"Alice\\") student1.add_grade(90.0) student1.add_grade(85.0) student2 = Student(name=\\"Bob\\") student2.add_grade(75.0) student2.add_grade(80.0) # Define gradebook object and add students gradebook = GradeBook() gradebook.add_student(student1) gradebook.add_student(student2) # Calculate class average print(gradebook.calculate_class_average()) # Expected output: 82.5 ``` Implement the `Student` and `GradeBook` classes based on the specifications above. Ensure all methods and functionalities work as expected.","solution":"from dataclasses import dataclass, field from typing import List, Optional @dataclass(frozen=True) class Student: name: str grades: List[float] = field(default_factory=list) def add_grade(self, grade: float) -> None: self.grades.append(grade) def average_grade(self) -> Optional[float]: if not self.grades: return None return sum(self.grades) / len(self.grades) @dataclass(frozen=True) class GradeBook: students: List[Student] = field(default_factory=list) def add_student(self, student: Student) -> None: self.students.append(student) def calculate_class_average(self) -> Optional[float]: total_grades = [] for student in self.students: total_grades.extend(student.grades) if not total_grades: return None return sum(total_grades) / len(total_grades)"},{"question":"# Python Coding Assessment Question You are required to write a Python function that demonstrates your understanding of the \\"posix\\" module, large file support, and environment variables. Task Description 1. **Function Implementation**: Implement a function `configure_environment_and_handle_large_file()` that: - Reads an environment variable `TARGET_DIR` to find the target directory. - If the `TARGET_DIR` environment variable does not exist, defaults to the current user\'s home directory. - Creates a large file (larger than 2 GiB) named `large_file.txt` in the target directory. - Reads the first 1024 bytes of this file and returns them as a bytes object. 2. **Requirements**: - Utilize both the `os` and `posix` modules as appropriate. - Ensure the code handles scenarios where the environment variable is not set and defaults correctly. - Demonstrate the handling of large files specifically adhering to POSIX standards. Constraints - You may assume the availability of sufficient storage space. - The function should be compatible with both Unix and Windows systems, but you may utilize `posix`-specific features wherever relevant. Input and Output - **Input**: No input parameters. - **Output**: A bytes object containing the first 1024 bytes of the large file. ```python import os import posix def configure_environment_and_handle_large_file(): # Your implementation here pass ``` Tips - To create a large file, you can write a small amount of data and then use `os.truncate()` or `posix.ftruncate()` to set the file size. - Be mindful of ensuring cross-platform compatibility when dealing with environment variables and file paths.","solution":"import os import posix def configure_environment_and_handle_large_file(): # Get the target directory from the environment variable or default to the user\'s home directory target_dir = os.getenv(\'TARGET_DIR\', os.path.expanduser(\'~\')) # Ensure the target directory exists if not os.path.exists(target_dir): os.makedirs(target_dir) # Path to the large file large_file_path = os.path.join(target_dir, \'large_file.txt\') # Create the large file and set its size to be larger than 2 GiB (2 * 1024 * 1024 * 1024 + 1) with open(large_file_path, \'wb\') as f: f.write(b\'start\' * 205) posix.ftruncate(f.fileno(), 2 * 1024 * 1024 * 1024 + 1) # Set size to just over 2 GiB # Read the first 1024 bytes of the file with open(large_file_path, \'rb\') as f: first_1024_bytes = f.read(1024) return first_1024_bytes"},{"question":"# Abstract Base Class Implementation â€“ Custom Collection **Objective**: Implement a custom collection class that adheres to the behavior of a `collections.abc.Sequence`. Your class should directly inherit from `collections.abc.Sequence` and implement the necessary abstract methods. **Requirements**: 1. Create a class named `CustomSequence` that inherits from `collections.abc.Sequence`. 2. Implement the following required abstract methods: - `__getitem__(self, index)`: Method to retrieve an item at a given index. - `__len__(self)`: Method to return the length of the sequence. 3. The `CustomSequence` class should initialize with an iterable (list) and store it internally. 4. Utilize the mixin methods provided by `collections.abc.Sequence` for operations like `__contains__`. 5. Ensure your class is properly working with `issubclass()` and `isinstance()` checks for `collections.abc.Sequence`. **Constraints**: - The `__getitem__` method should raise an `IndexError` if the index is out of bounds. - Custom implementation of `__getitem__` should have a time complexity of O(1). - Use only standard libraries. **Input**: - Initialization with a list of elements. **Output**: - The methods should behave as expected for a sequence type. **Example**: ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = list(data) # Ensures the data is stored as a list def __getitem__(self, index): if index < 0 or index >= len(self._data): raise IndexError(\\"Index out of bounds\\") return self._data[index] def __len__(self): return len(self._data) # Example usage: my_seq = CustomSequence([1, 2, 3, 4, 5]) print(issubclass(CustomSequence, Sequence)) # Output: True print(isinstance(my_seq, Sequence)) # Output: True print(my_seq[2]) # Output: 3 print(len(my_seq)) # Output: 5 print(3 in my_seq) # Output: True try: print(my_seq[10]) # Should raise IndexError except IndexError as e: print(e) # Output: Index out of bounds ``` This question tests the ability to implement abstract methods, leverage mixin methods, and understand the behavior of abstract base classes.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = list(data) # Ensures the data is stored as a list def __getitem__(self, index): if index < 0 or index >= len(self._data): raise IndexError(\\"Index out of bounds\\") return self._data[index] def __len__(self): return len(self._data) # Example usage: # my_seq = CustomSequence([1, 2, 3, 4, 5]) # print(issubclass(CustomSequence, Sequence)) # Output: True # print(isinstance(my_seq, Sequence)) # Output: True # print(my_seq[2]) # Output: 3 # print(len(my_seq)) # Output: 5 # print(3 in my_seq) # Output: True # try: # print(my_seq[10]) # Should raise IndexError # except IndexError as e: # print(e) # Output: Index out of bounds"},{"question":"# Question: Plotting with Seaborn\'s FacetGrid In this assessment, you will demonstrate your understanding of Seaborn\'s `FacetGrid` by creating a complex multi-plot grid from a dataset. You will showcase your ability to customize the plots using different seaborn functionalities. **Instructions:** 1. Load the `tips` dataset using `sns.load_dataset(\\"tips\\")`. 2. Create a `FacetGrid` object with: - Columns defined by the `day` variable. - Rows defined by the `time` variable. - Color (`hue`) mapped to the `smoker` variable. 3. Map a `scatterplot` to each grid cell, plotting `total_bill` on the x-axis and `tip` on the y-axis. 4. Add a regression line to each scatterplot using `sns.regplot`. 5. Add a median reference line across all subplots for the `tip` value. 6. Adjust the grid to have `height` of 4 and an `aspect` ratio of 1. 7. Set axis labels as \\"Total Bill ()\\" and \\"Tip ()\\". 8. Add titles to each subplot showing the `time` and `day`. 9. Save the final plot as `facet_grid_plot.png`. **Expected Input and Output:** - **Input:** The `tips` DataFrame loaded from seaborn. - **Output:** A file named `facet_grid_plot.png` containing the multi-plot grid. **Requirements:** - Your code should be written in Python. - Utilize seaborn and matplotlib for plotting. - Ensure your plots are correctly labeled and visually clear. **Example Code Structure:** ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Load the dataset tips = sns.load_dataset(\\"tips\\") # 2. Create the FacetGrid g = sns.FacetGrid(tips, col=\\"day\\", row=\\"time\\", hue=\\"smoker\\", height=4, aspect=1) # 3. Map scatterplot and regplot g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") g.map_dataframe(sns.regplot, x=\\"total_bill\\", y=\\"tip\\", scatter=False, truncate=True) # 4. Add a median reference line g.refline(y=tips[\\"tip\\"].median()) # 5. Customize the grid g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") # 6. Save the plot g.tight_layout() g.savefig(\\"facet_grid_plot.png\\") ``` Make sure your implementation satisfies all the instructions and requirements for full credit.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_facet_grid_plot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the FacetGrid with specified parameters g = sns.FacetGrid(tips, col=\\"day\\", row=\\"time\\", hue=\\"smoker\\", height=4, aspect=1) # Map scatterplot and regplot to each grid cell g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") g.map_dataframe(sns.regplot, x=\\"total_bill\\", y=\\"tip\\", scatter=False, truncate=True) # Add a median reference line for the \'tip\' value g.refline(y=tips[\\"tip\\"].median()) # Customize the axis labels and titles g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") # Adjust the layout g.tight_layout() # Save the plot as facet_grid_plot.png g.savefig(\\"facet_grid_plot.png\\") # Call the function to create the plot create_facet_grid_plot()"},{"question":"Your task is to implement a custom PyTorch module called `ConditionalActivation` that utilizes `torch.cond` to decide between different activation functions based on the sum of the input tensor elements. Specifically, if the sum of all elements in the input tensor is greater than `threshold`, apply the ReLU activation function; otherwise, apply the Tanh activation function. # Requirements: 1. Implement a class `ConditionalActivation` which inherits from `torch.nn.Module`. 2. The class should initialize with a `threshold` parameter in its constructor. 3. Implement the `forward` method to use `torch.cond` for branching based on the sum of input tensor elements. 4. The `true_fn` within `forward` should apply `torch.relu` to the input tensor. 5. The `false_fn` within `forward` should apply `torch.tanh` to the input tensor. # Additional Requirements: - Input: A single tensor `x` (torch.Tensor) with any shape. - Output: The result of applying the relevant activation function as a tensor. - Constraints: Ensure your implementation handles tensors of different shapes and values. - Performance: Your implementation should not significantly degrade in performance as input size increases. # Example ```python import torch class ConditionalActivation(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return torch.relu(x) def false_fn(x: torch.Tensor): return torch.tanh(x) return torch.cond(x.sum() > self.threshold, true_fn, false_fn, (x,)) # Example usage model = ConditionalActivation(threshold=0.0) input_tensor1 = torch.tensor([[-1.0, 2.0], [-3.0, 4.0]]) input_tensor2 = torch.tensor([[-5.0, -6.0], [-7.0, -8.0]]) output1 = model(input_tensor1) # The sum is 2.0, which is greater than 0.0, so ReLU is applied output2 = model(input_tensor2) # The sum is -26.0, which is not greater than 0.0, so Tanh is applied ``` Write your solution in the space provided below: ```python # Your implementation ```","solution":"import torch import torch.nn as nn class ConditionalActivation(nn.Module): def __init__(self, threshold: float): super(ConditionalActivation, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: if x.sum() > self.threshold: return torch.relu(x) else: return torch.tanh(x)"},{"question":"You are required to implement a Python test suite for a Bank Account system using the `unittest` module. Your task involves creating and running test cases to validate the behavior of a simple `BankAccount` class. Here is the `BankAccount` class which has been implemented: ```python class BankAccount: def __init__(self, initial_balance=0): self.balance = initial_balance def deposit(self, amount): if amount <= 0: raise ValueError(\\"Deposit amount must be positive\\") self.balance += amount def withdraw(self, amount): if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): return self.balance ``` Your task is to: 1. Create a set of unit tests for the `BankAccount` class to ensure its methods work correctly under different conditions. 2. Use the various features of the `unittest` module: - Define a test case class inheriting from `unittest.TestCase`. - Implement `setUp` and `tearDown` methods for setting up a bank account before each test and cleaning up after each test. - Use multiple assertion methods to verify the correctness of the methods. - Ensure the tests handle exceptions correctly using `assertRaises`. - Include a test suite that aggregates all test cases. - Demonstrate the use of skipping tests and expected failures. # Instructions 1. Create a test case class `TestBankAccount`: - In the `setUp` method, create a `BankAccount` instance with an initial balance of 100. - In the `tearDown` method, set the `BankAccount` instance to `None`. 2. Write the following test methods: - `test_deposit`: Ensure depositing a positive amount updates the balance correctly. - `test_withdraw`: Ensure withdrawing an amount updates the balance correctly. - `test_withdraw_insufficient_funds`: Ensure withdrawing more than the balance raises a `ValueError`. - `test_deposit_zero_or_negative`: Ensure depositing zero or a negative amount raises a `ValueError`. 3. Create a test suite `suite` which includes the above test cases. 4. Implement the `main` block to run the tests with verbosity. 5. Use decorators to: - Skip a test method (e.g., `test_skip`) with a reason. - Mark a test method as an expected failure (e.g., `test_expected_failure`) that intentionally fails. # Example Output Your tests should be executed by running the script, resulting in an output similar to: ```plaintext test_deposit (__main__.TestBankAccount) ... ok test_deposit_zero_or_negative (__main__.TestBankAccount) ... ok test_expected_failure (__main__.TestBankAccount) ... expected failure test_skip (__main__.TestBankAccount) ... skipped \'Not implemented yet\' test_withdraw (__main__.TestBankAccount) ... ok test_withdraw_insufficient_funds (__main__.TestBankAccount) ... ok ---------------------------------------------------------------------- Ran 6 tests in 0.001s OK (expected failures=1, skipped=1) ``` # Submission Submit the complete Python script containing the `BankAccount` class and the `unittest` test suite.","solution":"import unittest class BankAccount: def __init__(self, initial_balance=0): self.balance = initial_balance def deposit(self, amount): if amount <= 0: raise ValueError(\\"Deposit amount must be positive\\") self.balance += amount def withdraw(self, amount): if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): return self.balance"},{"question":"**Problem Statement: Comparing Linear Regression, Ridge Regression, and Lasso Regression using Scikit-Learn** # Objective Write a Python function that: 1. Implements Linear Regression, Ridge Regression, and Lasso Regression from the scikit-learn linear models. 2. Fits these models to a given dataset. 3. Returns the coefficients and mean squared error of each model. # Function Signature ```python def compare_regressions(X_train, y_train, X_test, y_test, ridge_alpha=1.0, lasso_alpha=0.1): Compare Linear, Ridge, and Lasso Regression models. Parameters: - X_train: np.array, Training data (features). - y_train: np.array, Training data (target). - X_test: np.array, Test data (features). - y_test: np.array, Test data (target). - ridge_alpha: float, Regularization strength for Ridge regression. - lasso_alpha: float, Regularization strength for Lasso regression. Returns: - dict: A dictionary with model names as keys and a tuple of (coefficients, mean_squared_error) as values. pass ``` # Constraints - The dataset will contain no missing values. - Feature matrix `X_train`, `X_test` are 2-dimensional numpy arrays, and target vectors `y_train`, `y_test` are 1-dimensional numpy arrays. - Use mean squared error (MSE) as the metric for evaluating model performance. - Outputs should include the coefficients of the linear models and their corresponding MSE on the test data. # Input - `X_train`: A numpy array of shape (n_samples_train, n_features). - `y_train`: A numpy array of shape (n_samples_train,). - `X_test`: A numpy array of shape (n_samples_test, n_features). - `y_test`: A numpy array of shape (n_samples_test,). - `ridge_alpha`: (Optional) A float specifying regularization strength for Ridge Regression. Default is 1.0. - `lasso_alpha`: (Optional) A float specifying regularization strength for Lasso Regression. Default is 0.1. # Output - A dictionary with keys \'Linear Regression\', \'Ridge Regression\', and \'Lasso Regression\'. Each key maps to a tuple of the form (coefficients, mean_squared_error), where: - `coefficients` is a numpy array of shape (n_features,). - `mean_squared_error` is a float. # Example ```python from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split # Sample data generation X, y = make_regression(n_samples=100, n_features=5, noise=0.1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) result = compare_regressions(X_train, y_train, X_test, y_test) print(result) ``` # Note - This task assesses the ability to implement and apply different linear regression models using scikit-learn. - Consider using appropriate scikit-learn functions such as `LinearRegression`, `Ridge`, and `Lasso`. - Ensure the returning dictionary is in the required format with correct labels and values.","solution":"import numpy as np from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_squared_error def compare_regressions(X_train, y_train, X_test, y_test, ridge_alpha=1.0, lasso_alpha=0.1): Compare Linear, Ridge, and Lasso Regression models. Parameters: - X_train: np.array, Training data (features). - y_train: np.array, Training data (target). - X_test: np.array, Test data (features). - y_test: np.array, Test data (target). - ridge_alpha: float, Regularization strength for Ridge regression. - lasso_alpha: float, Regularization strength for Lasso regression. Returns: - dict: A dictionary with model names as keys and a tuple of (coefficients, mean_squared_error) as values. # Initialize the models linear_reg = LinearRegression() ridge_reg = Ridge(alpha=ridge_alpha) lasso_reg = Lasso(alpha=lasso_alpha) # Fit the models linear_reg.fit(X_train, y_train) ridge_reg.fit(X_train, y_train) lasso_reg.fit(X_train, y_train) # Make predictions y_pred_linear = linear_reg.predict(X_test) y_pred_ridge = ridge_reg.predict(X_test) y_pred_lasso = lasso_reg.predict(X_test) # Calculate mean squared errors mse_linear = mean_squared_error(y_test, y_pred_linear) mse_ridge = mean_squared_error(y_test, y_pred_ridge) mse_lasso = mean_squared_error(y_test, y_pred_lasso) # Get coefficients coef_linear = linear_reg.coef_ coef_ridge = ridge_reg.coef_ coef_lasso = lasso_reg.coef_ # Create the results dictionary results = { \'Linear Regression\': (coef_linear, mse_linear), \'Ridge Regression\': (coef_ridge, mse_ridge), \'Lasso Regression\': (coef_lasso, mse_lasso) } return results"},{"question":"Coding Assessment Question # Secure Data Handling and File Operations This task involves creating a secure data handling component that can serialize and deserialize objects to and from a file. You are to implement two key functions securely: `save_data` and `load_data`. 1. `save_data` should serialize a given dictionary to a file located at a given filepath using JSON format. 2. `load_data` should deserialize data from a given filepath back into a dictionary. Both functions need to incorporate security practices to avoid common vulnerabilities such as: - Injection attacks from malformed serialized content. - Handling files securely to prevent issues like race conditions or tampered data files. # Function Signatures ```python def save_data(data: dict, filepath: str) -> None: Serialize the provided dictionary to a JSON file securely. Parameters: - data (dict): The dictionary to serialize. - filepath (str): The path to the file where data will be saved. Returns: - None Raises: - ValueError: If the provided data is not a dictionary. - IOError: For issues related to file writing. pass def load_data(filepath: str) -> dict: Deserialize JSON data from a file safely into a dictionary. Parameters: - filepath (str): The path to the file to load data from. Returns: - dict: The deserialized dictionary. Raises: - ValueError: If the deserialized data is not a dictionary. - IOError: For issues related to file reading. pass ``` # Constraints and Requirements: 1. The `save_data` function must ensure that the data is indeed a dictionary before attempting serialization. 2. Both functions should handle file access in a way that is safe from race conditions and improper file handling. 3. Ensure proper error handling to catch and raise appropriate exceptions. 4. Use JSON for serialization as it is human-readable and less vulnerable than `pickle`. # Example Usage ```python data = { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"is_student\\": False } filepath = \\"data.json\\" # Save the dictionary to a file save_data(data, filepath) # Load the data from the file loaded_data = load_data(filepath) assert data == loaded_data ``` Ensure your implementation is secure, efficient, and correctly handles the serialization/deserialization process.","solution":"import json import os def save_data(data: dict, filepath: str) -> None: Serialize the provided dictionary to a JSON file securely. Parameters: - data (dict): The dictionary to serialize. - filepath (str): The path to the file where data will be saved. Returns: - None Raises: - ValueError: If the provided data is not a dictionary. - IOError: For issues related to file writing. if not isinstance(data, dict): raise ValueError(\\"Data must be a dictionary.\\") try: # Using os.open with appropriate flags for secure file handling flags = os.O_WRONLY | os.O_CREAT | os.O_TRUNC with os.fdopen(os.open(filepath, flags, 0o600), \'w\') as f: json.dump(data, f) except IOError as e: raise IOError(f\\"Could not write to file: {e}\\") def load_data(filepath: str) -> dict: Deserialize JSON data from a file safely into a dictionary. Parameters: - filepath (str): The path to the file to load data from. Returns: - dict: The deserialized dictionary. Raises: - ValueError: If the deserialized data is not a dictionary. - IOError: For issues related to file reading. try: with open(filepath, \'r\') as f: data = json.load(f) if not isinstance(data, dict): raise ValueError(\\"Deserialized data is not a dictionary.\\") return data except (IOError, json.JSONDecodeError) as e: raise IOError(f\\"Could not read from file: {e}\\")"},{"question":"# Incremental Compression and Decompression with bzip2 **Objective**: Implement a function that reads a large text file, compresses it incrementally using `bz2.BZ2Compressor`, writes the compressed data to an output file, then reads the compressed file and decompresses it using `bz2.BZ2Decompressor` to verify the integrity of the compression process. **Function Signature**: ```python def incremental_compression_decompression(input_file: str, output_file: str) -> bool: Reads data from input_file, compresses it incrementally and writes to output_file. Then reads from output_file, decompresses it incrementally and verifies integrity by comparing to original input data. Parameters: - input_file (str): Path to the input file with plain text data. - output_file (str): Path to the output bzip2-compressed file. Returns: - bool: True if the decompressed data matches the original input data, False otherwise. pass ``` **Input/Output Format**: - `input_file`: a path (string) to a plain text file. - `output_file`: a path (string) to be used for writing the compressed data. **Steps**: 1. Read data from `input_file` in chunks. 2. Incrementally compress the data using `bz2.BZ2Compressor` and write to `output_file`. 3. After writing all compressed data, flush the compressor to finish the compression process. 4. Read the compressed data from `output_file`. 5. Incrementally decompress the data using `bz2.BZ2Decompressor`. 6. Verify the integrity of the decompressed data by comparing it to the original data. **Constraints**: - The function should handle the file operations efficiently, reading and writing in chunks to manage memory usage. - Your implementation should ensure that the compressed and decompressed data are handled correctly to prevent data corruption. **Example**: ```python input_file = \\"example.txt\\" output_file = \\"compressed_example.bz2\\" # Assuming \'example.txt\' contains some large text data: result = incremental_compression_decompression(input_file, output_file) print(result) # Should print True if the decompressed data matches the original ``` **Performance Requirements**: - The solution should efficiently handle large files by processing data in chunks. - Proper error handling should be implemented for file access and processing operations.","solution":"import bz2 def incremental_compression_decompression(input_file: str, output_file: str) -> bool: Reads data from input_file, compresses it incrementally and writes to output_file. Then reads from output_file, decompresses it incrementally and verifies integrity by comparing to original input data. Parameters: - input_file (str): Path to the input file with plain text data. - output_file (str): Path to the output bzip2-compressed file. Returns: - bool: True if the decompressed data matches the original input data, False otherwise. chunk_size = 1024 # Define chunk size for incremental processing # Step 1: Read and compress the data incrementally compressor = bz2.BZ2Compressor() with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: while chunk := infile.read(chunk_size): compressed_data = compressor.compress(chunk) outfile.write(compressed_data) # Flush remaining data in compressor outfile.write(compressor.flush()) # Step 2: Read and decompress the data incrementally decompressor = bz2.BZ2Decompressor() with open(output_file, \'rb\') as compressed_file: decompressed_data = b\\"\\" while chunk := compressed_file.read(chunk_size): decompressed_data += decompressor.decompress(chunk) # Verify the integrity of decompressed data by comparing to original data with open(input_file, \'rb\') as infile: original_data = infile.read() return original_data == decompressed_data"},{"question":"# Problem Description You are tasked with analyzing the relationship between the tips and total bill from the famous `tips` dataset using seaborn. To accomplish this, you need to follow these steps: 1. **Load the `tips` dataset**. 2. **Create a custom sequential color palette**: Generate a sequential color palette from light gray to a specified color using hex color code `#1f77b4`. 3. **Visualize the data**: - Create a scatter plot of `total_bill` versus `tip` using the created color palette. The size of the points should correspond to the `size` column in the dataset. - Add a linear regression line with confidence intervals to the scatter plot to show the relationship between `total_bill` and `tip`. # Input No input from the user. Load the `tips` dataset using seaborn. # Output A scatter plot visualizing the relationship between `total_bill` and `tip`, with point sizes corresponding to the `size` column and a linear regression line showing the trend. # Function Signature ```python def visualize_tips_relationship(): import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = sns.load_dataset(\'tips\') # Load the tips dataset # Step 2: Create a custom sequential color palette palette = sns.light_palette(\\"#1f77b4\\", as_cmap=True) # Create a color palette # Step 3: Visualize the data plt.figure(figsize=(10, 6)) # Set the size of the plot scatter_plot = sns.scatterplot(data=data, x=\'total_bill\', y=\'tip\', size=\'size\', hue=\'size\', palette=palette, legend=False) sns.regplot(data=data, x=\'total_bill\', y=\'tip\', scatter=False, ax=scatter_plot) # Display the plot plt.title(\'Relationship between Total Bill and Tip\') plt.xlabel(\'Total Bill ()\') plt.ylabel(\'Tip ()\') plt.show() ``` # Constraints and Requirements 1. Make sure that the `seaborn` and `matplotlib` packages are properly imported and used. 2. The scatter plot must use the custom palette created in step 2, with point size representing the `size` column. 3. The regression line must have confidence intervals to show the trend effectively. 4. Ensure the plot is labeled correctly with title, X-axis label, and Y-axis label. # Example Output The generated plot should display a scatter plot of the `total_bill` against `tip` with point sizes corresponding to the `size` column in the dataset, using the sequential color palette. The linear regression line should appropriately fit the data points with confidence intervals.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_relationship(): Load the tips dataset and visualize the relationship between total bill and tips, with point sizes corresponding to the size column, using a custom color palette. # Step 1: Load the dataset data = sns.load_dataset(\'tips\') # Step 2: Create a custom sequential color palette palette = sns.light_palette(\\"#1f77b4\\", as_cmap=True) # Step 3: Visualize the data plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=data, x=\'total_bill\', y=\'tip\', size=\'size\', hue=\'size\', palette=palette, legend=False) sns.regplot(data=data, x=\'total_bill\', y=\'tip\', scatter=False, ax=scatter_plot) # Display the plot plt.title(\'Relationship between Total Bill and Tip\') plt.xlabel(\'Total Bill ()\') plt.ylabel(\'Tip ()\') plt.show()"},{"question":"**Problem: Multi-process Data Sharing with Shared Memory** You are tasked with creating a utility for sharing data between multiple processes using the `multiprocessing.shared_memory` module in Python. Your goal is to implement a function that creates a shared memory block, writes an integer array to it, and then reads back the data from multiple processes to validate the shared memory access. This will demonstrate the effective use of the `SharedMemory` class. Implement the following functions: 1. `create_shared_memory(size)`: - **Input**: `size` (int) - The number of integers to store in the shared memory. - **Output**: A tuple `(shm_name, shm_size)` where `shm_name` is the name of the shared memory block and `shm_size` is the size of the shared memory block in bytes. - **Description**: This function should create a shared memory block of appropriate size to store `size` integers and initialize it with a sequence of integers from `0` to `size-1`. The function should return the name and size of the shared memory. 2. `read_shared_memory(shm_name, shm_size)`: - **Input**: `shm_name` (str) - The name of the shared memory block. `shm_size` (int) - The size of the shared memory block in bytes. - **Output**: A list of integers read from the shared memory. - **Description**: This function should attach to an existing shared memory block specified by `shm_name` and read the integer array stored in it, returning the array as a list of integers. 3. `cleanup_shared_memory(shm_name)`: - **Input**: `shm_name` (str) - The name of the shared memory block. - **Output**: None - **Description**: This function should attach to the shared memory block and perform necessary cleanup by closing and unlinking the shared memory block. # Constraints: - You may assume the input size `size` for `create_shared_memory` is a positive integer less than 1,000,000. - You must use the `multiprocessing.shared_memory` module. - Ensure to handle exceptions appropriately, especially when reading from or cleaning up shared memory. # Example Usage: ```python # Creating shared memory and initializing data shm_name, shm_size = create_shared_memory(10) # Reading data from shared memory in a different process data = read_shared_memory(shm_name, shm_size) print(data) # Output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # Cleaning up shared memory cleanup_shared_memory(shm_name) ``` Implement the functions and ensure they work correctly when executed in both the same process and across multiple processes.","solution":"import multiprocessing.shared_memory as shm import array def create_shared_memory(size): Creates a shared memory block and initializes it with a sequence of integers. Args: size (int): Number of integers to store in shared memory. Returns: (tuple): A tuple containing (shm_name, shm_size) # Create an array of integers from 0 to size-1 data = array.array(\'i\', range(size)) # Create a shared memory block shm_size = data.buffer_info()[1] * data.itemsize shared_mem = shm.SharedMemory(create=True, size=shm_size) # Write data to shared memory shared_mem_buf = shared_mem.buf shared_mem_buf[:shm_size] = data.tobytes() return shared_mem.name, shm_size def read_shared_memory(shm_name, shm_size): Reads data from an existing shared memory block. Args: shm_name (str): Name of the shared memory block. shm_size (int): Size in bytes of the shared memory block. Returns: list: Data read from shared memory as a list of integers. # Connect to the existing shared memory block existing_shm = shm.SharedMemory(name=shm_name) # Read and parse the data from shared memory shared_mem_buf = existing_shm.buf data = array.array(\'i\') data.frombytes(shared_mem_buf[:shm_size]) # Cleanup the shared memory reference existing_shm.close() return data.tolist() def cleanup_shared_memory(shm_name): Cleans up the shared memory block. Args: shm_name (str): Name of the shared memory block. Returns: None # Connect to the existing shared memory block existing_shm = shm.SharedMemory(name=shm_name) # Close and unlink the shared memory block existing_shm.close() existing_shm.unlink()"},{"question":"Objective Assess a student\'s ability to utilize seaborn\'s `residplot` function. This includes loading a dataset, creating a residual plot of a regression model, and modifying the plot with higher-order terms and LOWESS smoothing. Problem Statement You are provided with a dataset `mpg` which contains various attributes of cars such as weight, horsepower, and mpg (miles per gallon). Using `seaborn`, you need to create residual plots that evaluate the fit of linear and non-linear models. Tasks 1. **Load Dataset:** - Load the `mpg` dataset using seaborn\'s `load_dataset` function. 2. **Create Residual Plots:** - Create a residual plot of mpg against weight. - Create a residual plot of mpg against horsepower. - Create a higher-order residual plot (order=2) of mpg against horsepower. - Create a LOWESS-smoothed residual plot of mpg against horsepower with a red curve. 3. **Custom Function:** - Write a function `create_residual_plots` that takes `dataset_name: str` as an input and produces the required residual plots. # Function Signature ```python def create_residual_plots(dataset_name: str) -> None: pass ``` # Constraints - Use seaborn to load the dataset and create plots. - The function should not return anything but should display all the plots. - Assume `seaborn` and necessary libraries are already installed. # Example When the function `create_residual_plots(\\"mpg\\")` is called, it should: - Load the `mpg` dataset. - Create and display the four specified residual plots. Performance requirements The plots should be generated efficiently without unnecessary computations or memory usage.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_residual_plots(dataset_name: str) -> None: # Load the dataset df = sns.load_dataset(dataset_name) # Residual plot of mpg against weight plt.figure(figsize=(10, 6)) sns.residplot(x=\'weight\', y=\'mpg\', data=df) plt.title(\'Residual Plot of MPG vs Weight\') plt.show() # Residual plot of mpg against horsepower plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=df) plt.title(\'Residual Plot of MPG vs Horsepower\') plt.show() # Higher-order residual plot (order=2) of mpg against horsepower plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=df, order=2) plt.title(\'Higher-Order Residual Plot (order=2) of MPG vs Horsepower\') plt.show() # LOWESS-smoothed residual plot of mpg against horsepower with a red curve plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=df, lowess=True, color=\'red\') plt.title(\'LOWESS-smoothed Residual Plot of MPG vs Horsepower\') plt.show()"},{"question":"# Advanced File & Directory Manipulation with Python **Problem Statement:** You are tasked with designing a function that synchronizes files from a source directory to a target directory. This synchronization should include the following features: 1. **Copy New Files**: Any file that exists in the source directory but not in the target directory should be copied over to the target directory. 2. **Update Modified Files**: If a file exists in both directories but the source file is newer (based on the last modified time), the file in the target directory should be replaced with the newer file from the source directory. 3. **Delete Removed Files**: Any file that exists in the target directory but not in the source directory should be deleted from the target directory. To achieve this, you should use the appropriate modules and functions from the provided documentation. **Function Signature:** ```python def synchronize_directories(source_dir: str, target_dir: str) -> None: ``` **Input:** - `source_dir` (str): The path to the source directory. - `target_dir` (str): The path to the target directory. **Output:** - The function should not return anything but should perform the synchronization as described above. **Constraints:** - You can assume that `source_dir` and `target_dir` are valid directories that exist on the filesystem. - You may not use third-party libraries for this task; only standard libraries described in the provided documentation are allowed. **Example:** Assume the following directory structures at `source_dir` and `target_dir` before synchronization: `source_dir`: ``` /source_dir file1.txt (last modified: 2023-10-01) file2.txt (last modified: 2023-10-03) file3.txt (last modified: 2023-10-05) ``` `target_dir`: ``` /target_dir file1.txt (last modified: 2023-09-30) file2.txt (last modified: 2023-10-03) file4.txt (last modified: 2023-10-02) ``` After running `synchronize_directories(source_dir, target_dir)`, the `target_dir` should look like this: ``` /target_dir file1.txt (last modified: 2023-10-01) file2.txt (last modified: 2023-10-03) file3.txt (last modified: 2023-10-05) ``` **Implementation Notes:** - Use the `pathlib` module for handling paths. - Use the `shutil` and `os` modules for file operations. - Ensure your function handles large directories efficiently in terms of both time and space.","solution":"from pathlib import Path import shutil from os import stat def synchronize_directories(source_dir: str, target_dir: str) -> None: source_path = Path(source_dir) target_path = Path(target_dir) # Copy new and update modified files for source_file in source_path.glob(\'**/*\'): if source_file.is_file(): target_file = target_path / source_file.relative_to(source_path) if not target_file.exists() or source_file.stat().st_mtime > target_file.stat().st_mtime: target_file.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(source_file, target_file) # Delete removed files for target_file in target_path.glob(\'**/*\'): source_file = source_path / target_file.relative_to(target_path) if target_file.is_file() and not source_file.exists(): target_file.unlink()"},{"question":"# Seaborn JointPlot Advanced Usage You are given a dataset of penguin measurements from the `seaborn` library. Your task is to create a comprehensive visualization that includes several features of the `sns.jointplot` function. **Requirements:** 1. Load the `penguins` dataset from `seaborn`. 2. Create a scatterplot with marginal histograms using `sns.jointplot`. 3. Color the points based on the species of the penguins (`hue=\'species\'`). 4. Use `kind=\'kde\'` to draw bivariate and univariate KDEs. 5. Customize the scatterplot markers to be bigger and have a plus sign (`marker=\\"+\\"`, `s=100`). 6. Set the size of the plot to have a height of 7 and a ratio of 2. 7. Add additional layers using the `JointGrid` object: - Add a red KDE plot to the joint area. - Add red rug plots to the marginal areas. # Input There are no specific inputs; the dataset will be loaded within your code. # Output The output should be a matplotlib figure containing the joint plot with the described customizations. # Constraints - Use the seaborn functions demonstrated in the provided documentation. - Ensure the final plot is clear and all customizations are applied correctly. # Example ```python import seaborn as sns # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the jointplot with specified customizations g = sns.jointplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", marker=\\"+\\", s=100, height=7, ratio=2 ) # Add additional layers g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) g.plot_marginals(sns.rugplot, color=\\"r\\", height=-.15, clip_on=False) ``` Ensure your final plot matches the example provided, displaying a scatter plot with KDEs, colored by species, and enhanced with additional layers.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_jointplot(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values penguins = penguins.dropna(subset=[\'bill_length_mm\', \'bill_depth_mm\', \'species\']) # Create the jointplot with specified customizations g = sns.jointplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"scatter\\", marker=\\"+\\", s=100, height=7, ratio=2 ) # Add KDE plots g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) g.plot_marginals(sns.rugplot, color=\\"r\\", height=-.15, clip_on=False) # Show the plot plt.show() return g"},{"question":"**Objective:** Design a simple file explorer application using the `tkinter.tix` module that showcases the usage of multiple widgets provided by `tkinter.tix`. **Task:** Implement a Python function `create_file_explorer()` that creates and runs a file explorer application using the `tkinter.tix` module. The application should have the following features: 1. A `DirTree` widget to allow users to navigate the directory structure. 2. A `FileSelectBox` widget to allow users to select files within the chosen directory. 3. A `Meter` widget that gives users an indication of the progress of file loading (simulate a delay of 2 seconds for each file as it is \\"loaded\\" to demonstrate this functionality). **Function Signature:** ```python def create_file_explorer(): pass ``` **Requirements:** 1. Use the `tkinter.tix.Tk` class to create the main window. 2. Implement a `DirTree` widget for directory selection. 3. Implement a `FileSelectBox` widget for file selection. 4. Implement a `Meter` widget to show progress. 5. Simulate file loading with the Meter widget by adding a delay of 2 seconds per file using `time.sleep(2)`. 6. Ensure the UI is appropriately laid out and user-friendly. **Expected Input and Output:** - This function does not take any parameters or return any values. It should simply create and run the file explorer application. **Constraints:** - You can assume that necessary modules (`tkinter.tix`, `time`, etc.) are installed and available. **Performance Requirements:** - **UI Responsiveness:** The UI should remain responsive during the simulated file loading with the progress indicated by the Meter widget. **Example Implementation:** ```python from tkinter import tix, Tk, constants as c import time import threading def simulate_file_loading(meter, max_value, num_files): for i in range(num_files): time.sleep(2) # Simulating delay meter.set(i + 1, max_value) def create_file_explorer(): root = tix.Tk() root.title(\\"File Explorer\\") # Create DirTree widget dir_tree = tix.DirTree(root, browsecmd=lambda i: print(f\\"Selected directory: {dir_tree.cget(\'value\')}\\")) dir_tree.pack(side=c.LEFT, fill=c.BOTH, expand=True) # Create FileSelectBox widget file_select_box = tix.FileSelectBox(root) file_select_box.pack(side=c.TOP, fill=c.BOTH, expand=True) # Create Meter widget meter = tix.Meter(root, value=0, max=100) meter.pack(side=c.BOTTOM, fill=c.X, expand=False) # Simulate file loading on a separate thread to keep the UI responsive num_files = 5 # Simulating the loading of 5 files. threading.Thread(target=simulate_file_loading, args=(meter, 100, num_files)).start() root.mainloop() ```","solution":"from tkinter import tix, Tk, constants as c import time import threading def simulate_file_loading(meter, max_value, num_files): for i in range(num_files): time.sleep(2) # Simulating delay meter.set(i + 1, max_value) def create_file_explorer(): root = tix.Tk() root.title(\\"File Explorer\\") # Create DirTree widget dir_tree = tix.DirTree(root, browsecmd=lambda i: print(f\\"Selected directory: {dir_tree.cget(\'value\')}\\")) dir_tree.pack(side=c.LEFT, fill=c.BOTH, expand=True) # Create FileSelectBox widget file_select_box = tix.FileSelectBox(root) file_select_box.pack(side=c.TOP, fill=c.BOTH, expand=True) # Create Meter widget meter = tix.Meter(root, value=0, max=100) meter.pack(side=c.BOTTOM, fill=c.X, expand=False) # Simulate file loading on a separate thread to keep the UI responsive num_files = 5 # Simulating the loading of 5 files. threading.Thread(target=simulate_file_loading, args=(meter, 100, num_files)).start() root.mainloop()"},{"question":"You are given a dataset `penguins` consisting of various measurements (numerical and categorical) on penguins. Your task is to create a comprehensive visualization that analyzes the relationships between these numerical variables, using the seaborn library\'s PairGrid class. Write a Python function `create_pairgrid` that takes the following parameters: 1. `hue_variable` (str): The name of the categorical variable to be used for color encoding. 2. `vars_list` (list of str): List of numerical variables to be included in the grid. The function should: - Load the `penguins` dataset using `sns.load_dataset(\\"penguins\\")`. - Create a PairGrid with the specified numerical variables and color encoding based on the `hue_variable`. - Use `histplot` for plotting the diagonal subplots showing the distribution of each numerical variable. - Use `scatterplot` for the off-diagonal subplots showing relationships between pairs of variables. - Add a legend to the plot. - Display the plot using `plt.show()`. **Function Signature:** ```python def create_pairgrid(hue_variable: str, vars_list: list) -> None: pass ``` # Example: ```python hue_variable = \\"species\\" vars_list = [\\"body_mass_g\\", \\"bill_length_mm\\", \\"flipper_length_mm\\"] create_pairgrid(hue_variable, vars_list) ``` # Constraints: - The `hue_variable` must be either \\"species\\", \\"island\\", or \\"sex\\". - The `vars_list` must be a subset of [\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"]. - If invalid inputs are provided, the function should raise a `ValueError` with an appropriate message. This problem requires you to demonstrate a comprehensive understanding of seaborn\'s PairGrid class and effectively use it to create insightful visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_pairgrid(hue_variable: str, vars_list: list) -> None: valid_hue_variables = [\\"species\\", \\"island\\", \\"sex\\"] valid_vars = [\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"] if hue_variable not in valid_hue_variables: raise ValueError(f\\"Invalid hue variable: {hue_variable}. Must be one of {valid_hue_variables}.\\") if not set(vars_list).issubset(valid_vars): raise ValueError(f\\"Invalid vars_list: {vars_list}. Must be a subset of {valid_vars}.\\") penguins = sns.load_dataset(\\"penguins\\") g = sns.PairGrid(penguins, hue=hue_variable, vars=vars_list) g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot) g.add_legend() plt.show()"},{"question":"**Problem Statement** You are given a dataset containing information about various restaurants\' tipping behavior. Your task is to create a series of visualizations using the `seaborn` library to explore and analyze this data. **Dataset** The dataset `tips` includes the following columns: - `total_bill`: Total bill amount - `tip`: Tip given - `sex`: Gender of the person paying the bill - `smoker`: Whether the person is a smoker or not - `day`: Day of the week - `time`: Time of day (Lunch/Dinner) - `size`: Size of the party **Requirements** 1. Load the `tips` dataset using `seaborn`. 2. Create a swarm plot to visualize the distribution of the `total_bill` amounts for each day of the week. 3. Enhance the swarm plot by using the `hue` parameter to differentiate the points based on the `sex` column. 4. Create another swarm plot to compare the `total_bill` amounts across different times of the day (`Lunch` vs. `Dinner`), facetted by day of the week. Use different colors to differentiate between smokers and non-smokers. 5. Customize the plot further by adjusting the size of the points to avoid overlap and modifying the marker style and linewidth. **Function Signature** ```python def plot_tips_data(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Swarm plot for total_bill by day plot1 = sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\") # 2. Swarm plot for total_bill by day with hue for sex plot2 = sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\") # 3. Facetted swarm plot for total_bill by time and day, with hue for smoker plot3 = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"smoker\\", col=\\"day\\", aspect=.5 ) # 4. Customized swarm plot plot4 = sns.swarmplot( data=tips, x=\\"day\\", y=\\"total_bill\\", marker=\\"o\\", linewidth=1, size=5 ) # Display the plots sns.plt.show(plot1) sns.plt.show(plot2) sns.plt.show(plot3) sns.plt.show(plot4) ``` **Constraints** - Ensure that the plots are clear and readable. - Avoid overlapping of points where possible. - Apply appropriate aesthetics to make the plots visually appealing. - Use `seaborn` version 0.12 or later to ensure compatibility with newer features. **Example Output** The function should generate and display the specified visualizations based on the `tips` dataset. Each plot should effectively convey the data trends and comparisons detailed above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_tips_data(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Swarm plot for total_bill by day plt.figure(figsize=(10, 7)) plot1 = sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\") plot1.set_title(\'Swarm plot of Total Bill by Day\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Total Bill\') plt.show() # 2. Swarm plot for total_bill by day with hue for sex plt.figure(figsize=(10, 7)) plot2 = sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", dodge=True) plot2.set_title(\'Swarm plot of Total Bill by Day with Hue for Sex\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Total Bill\') plt.show() # 3. Facetted swarm plot for total_bill by time and day, with hue for smoker plot3 = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"smoker\\", col=\\"day\\", aspect=0.6, height=5 ) plot3.set_titles(\'Total Bill by Time of Day and Day of Week\') plot3.set_axis_labels(\'Time of Day\', \'Total Bill\') plot3.add_legend(title=\'Smoker\') plt.show() # 4. Customized swarm plot plt.figure(figsize=(10, 7)) plot4 = sns.swarmplot( data=tips, x=\\"day\\", y=\\"total_bill\\", marker=\\"o\\", linewidth=1, size=5 ) plot4.set_title(\'Customized Swarm plot of Total Bill by Day\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Total Bill\') plt.show()"},{"question":"# Multiprocessing with Shared Tensors in PyTorch Objective: Implement a multiprocessing solution using PyTorch that demonstrates the efficient sharing and updating of tensor data between multiple processes, while adhering to best practices for avoiding deadlocks and ensuring efficient CPU utilization. Description: You are provided with a model and a dataset. Your task is to write a multiprocessing program to train the model asynchronously using shared tensors. Requirements: 1. **Initialize**: - Create a simple neural network model. - Set up a shared training dataset and corresponding DataLoader. 2. **Multiprocessing**: - Use `torch.multiprocessing` to create multiple worker processes for training the model. - Ensure that model parameters are shared between all processes. - Update the model parameters asynchronously in each process. 3. **Avoid Oversubscription**: - Prevent CPU oversubscription by setting the appropriate number of threads per process. 4. **Best Practices**: - Explicitly handle CUDA tensors if you\'re using a GPU. - Ensure no global statements are unguarded by `if __name__ == \'__main__\'`. Input: - No direct inputs; the model and dataset should be defined within your program. Output: - Print the training progress and loss for each epoch and process. Constraints: - Use a maximum of 4 processes for the training. - Ensure that memory is managed efficiently to avoid unnecessary copies of tensors. Example: ```python import torch import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.utils.data import DataLoader, TensorDataset import math class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def train(rank, model, data_loader): torch.set_num_threads(math.floor(torch.get_num_threads() / 4)) # Avoid oversubscription criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(10): for inputs, targets in data_loader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\'Process {rank}, Epoch {epoch}, Loss: {loss.item()}\') if __name__ == \'__main__\': model = SimpleModel() model.share_memory() # Share model parameters between processes # Generate dummy dataset inputs = torch.randn(100, 10) targets = torch.randn(100, 1) dataset = TensorDataset(inputs, targets) data_loader = DataLoader(dataset, batch_size=10) processes = [] for rank in range(4): p = mp.Process(target=train, args=(rank, model, data_loader)) p.start() processes.append(p) for p in processes: p.join() ``` Implement the above solution in a Python script, ensuring that the described requirements and best practices are followed.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.utils.data import DataLoader, TensorDataset import math class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def train(rank, model, data_loader): torch.set_num_threads(math.floor(torch.get_num_threads() / 4)) # Avoid oversubscription criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(10): epoch_loss = 0.0 for inputs, targets in data_loader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() epoch_loss += loss.item() print(f\'Process {rank}, Epoch {epoch}, Loss: {epoch_loss/len(data_loader)}\') if __name__ == \'__main__\': model = SimpleModel() model.share_memory() # Share model parameters between processes # Generate dummy dataset inputs = torch.randn(100, 10) targets = torch.randn(100, 1) dataset = TensorDataset(inputs, targets) data_loader = DataLoader(dataset, batch_size=10) processes = [] for rank in range(4): p = mp.Process(target=train, args=(rank, model, data_loader)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"# Custom Testing Utility with PyTorch Objective Create a custom testing utility in PyTorch that includes functions to: 1. Generate random tensors with specified properties. 2. Compare the closeness of two tensors within a specified tolerance. 3. Incorporate an additional function to calculate the Mean Squared Error (MSE) between two tensors. Requirements 1. **Function `generate_random_tensor(shape, min_value, max_value, dtype=torch.float)`**: - **Input**: - `shape` (tuple of ints): The shape of the tensor to generate. - `min_value` (float): Minimum value for tensor elements. - `max_value` (float): Maximum value for tensor elements. - `dtype` (torch.dtype): Data type of the tensor (default: `torch.float`). - **Output**: - `torch.Tensor`: A tensor with random values within the specified range and shape. 2. **Function `check_tensors_close(tensor_a, tensor_b, rtol=1e-5, atol=1e-8)`**: - **Input**: - `tensor_a` (torch.Tensor): First tensor to compare. - `tensor_b` (torch.Tensor): Second tensor to compare. - `rtol` (float): Relative tolerance parameter (default: `1e-5`). - `atol` (float): Absolute tolerance parameter (default: `1e-8`). - **Output**: - `bool`: `True` if tensors are close within specified tolerance, otherwise `False`. 3. **Function `calculate_mse(tensor_a, tensor_b)`**: - **Input**: - `tensor_a` (torch.Tensor): First tensor. - `tensor_b` (torch.Tensor): Second tensor. - **Output**: - `torch.Tensor`: A tensor containing the Mean Squared Error between `tensor_a` and `tensor_b`. Constraints - Assume the input tensors for `check_tensors_close` and `calculate_mse` functions will always have the same shape. - Use the `torch.testing` module functions where suitable. - Ensure the code is optimized for performance. Example ```python import torch # Example Usage tensor_a = generate_random_tensor((3, 3), 0, 1) tensor_b = generate_random_tensor((3, 3), 0, 1) print(check_tensors_close(tensor_a, tensor_b)) print(calculate_mse(tensor_a, tensor_b)) ``` Implement the functions according to the specifications above.","solution":"import torch def generate_random_tensor(shape, min_value, max_value, dtype=torch.float): Generates a random tensor with specified properties. Parameters: shape (tuple of ints): The shape of the tensor to generate. min_value (float): Minimum value for tensor elements. max_value (float): Maximum value for tensor elements. dtype (torch.dtype): Data type of the tensor (default: torch.float). Returns: torch.Tensor: A tensor with random values within the specified range and shape. return (max_value - min_value) * torch.rand(size=shape, dtype=dtype) + min_value def check_tensors_close(tensor_a, tensor_b, rtol=1e-5, atol=1e-8): Checks if two tensors are close within a specified tolerance. Parameters: tensor_a (torch.Tensor): First tensor to compare. tensor_b (torch.Tensor): Second tensor to compare. rtol (float): Relative tolerance parameter (default: 1e-5). atol (float): Absolute tolerance parameter (default: 1e-8). Returns: bool: True if tensors are close within specified tolerance, otherwise False. return torch.allclose(tensor_a, tensor_b, rtol=rtol, atol=atol) def calculate_mse(tensor_a, tensor_b): Calculates the Mean Squared Error (MSE) between two tensors. Parameters: tensor_a (torch.Tensor): First tensor. tensor_b (torch.Tensor): Second tensor. Returns: torch.Tensor: A tensor containing the Mean Squared Error between tensor_a and tensor_b. return torch.mean((tensor_a - tensor_b) ** 2)"},{"question":"**Problem Statement:** Your task is to implement a function that interacts with Python\'s floating-point objects using the provided C API functions. Specifically, you will create a floating-point number from a string, verify its type, convert it to a double in C, and return information about the floating-point precision and limits. # Requirements Implement a function `float_operations(float_str: str)` that performs the following tasks: 1. Converts the given string `float_str` to a floating-point object using `PyFloat_FromString`. 2. Checks if the created object is a `PyFloatObject` using `PyFloat_Check` and `PyFloat_CheckExact`. 3. Converts the `PyFloatObject` to a C double using `PyFloat_AsDouble`. 4. Retrieves and returns information about the floating-point object precision and limits using `PyFloat_GetInfo`, `PyFloat_GetMax`, and `PyFloat_GetMin`. # Function Signature ```python def float_operations(float_str: str) -> dict: pass ``` # Input - `float_str` (str): A string representing a floating-point number. # Output - A dictionary with the following keys and corresponding values: - `\'is_float_object\'`: Boolean indicating if the created object is a `PyFloatObject`. - `\'is_exact_float\'`: Boolean indicating if the created object is exactly a `PyFloatObject`. - `\'c_double\'`: The C double representation of the created `PyFloatObject`. - `\'float_info\'`: The struct containing information about the precision, minimum, and maximum values. - `\'max_float\'`: The maximum representable finite float. - `\'min_positive_float\'`: The minimum normalized positive float. # Constraints - You can assume that `float_str` will always be a valid string representation of a floating-point number. - Performance requirements: Your method should be efficient and handle typical input sizes quickly. # Example ```python result = float_operations(\\"123.456\\") print(result) ``` Output: ```json { \\"is_float_object\\": True, \\"is_exact_float\\": True, \\"c_double\\": 123.456, \\"float_info\\": {...}, # Struct containing relevant information \\"max_float\\": 1.7976931348623157e+308, \\"min_positive_float\\": 2.2250738585072014e-308 } ``` You may write your function in Python, but it should be designed to conceptually demonstrate the use of the provided C functions for floating-point objects.","solution":"import sys import ctypes import struct def float_operations(float_str: str) -> dict: Performs various operations on a floating-point number represented as a string. # Step 1: Convert the string to a floating-point object try: float_obj = float(float_str) except ValueError: raise ValueError(f\\"Invalid float string: {float_str}\\") # Step 2 and 3: Check if the object is a float and if it is exactly a float is_float_object = isinstance(float_obj, float) is_exact_float = isinstance(float_obj, float) # Step 4: Convert the float object to a C double c_double = float_obj # Step 5: Retrieve floating-point precision and limits using sys.float_info float_info = sys.float_info result = { \'is_float_object\': is_float_object, \'is_exact_float\': is_exact_float, \'c_double\': c_double, \'float_info\': { \'max\': float_info.max, \'min\': float_info.min, \'epsilon\': float_info.epsilon, \'dig\': float_info.dig, \'mant_dig\': float_info.mant_dig, \'max_exp\': float_info.max_exp, \'min_exp\': float_info.min_exp, }, \'max_float\': float_info.max, \'min_positive_float\': float_info.min } return result"},{"question":"You are provided with a synthetic dataset containing features and a target variable. Your task is to implement a function that performs the following operations using scikit-learn: 1. **Load and prepare the dataset**: - Use the `make_classification` function to generate a classification dataset with `n_samples=1000`, `n_features=20`, `n_informative=10`, `n_redundant=5`, and `random_state=42`. 2. **Preprocessing**: - Split the dataset into training (70%) and testing (30%) sets using `train_test_split`. - Standardize the features using `StandardScaler`. 3. **Model Training**: - Train a `GradientBoostingClassifier` with a specific `n_iter_no_change` parameter. - Your function should handle any warnings that arise when a specific parameter, `n_iter_no_change`, is changed. 4. **Evaluation**: - Return the classification report on the test set as a dictionary. # Function Signature: ```python def scikit_learn_workflow(n_iter_no_change: int) -> dict: pass ``` # Constraints: - You must use `train_test_split` from `sklearn.model_selection`. - You must use `StandardScaler` from `sklearn.preprocessing`. - You must handle the warning mentioned in the provided documentation and ensure the code runs without raising this warning when the `n_iter_no_change` parameter is set. # Notes: - The function should be self-contained and should use synthetic data generated within the function. - When handling warnings, ensure that your approach considers scikit-learn\'s best practices. - The classification report should include precision, recall, and F1-score for each class. # Example Usage: ```python report = scikit_learn_workflow(n_iter_no_change=5) print(report) # Expected Output (Sample output, exact numbers may vary): # { # \'0\': {\'precision\': 0.92, \'recall\': 0.91, \'f1-score\': 0.91, \'support\': 150}, # \'1\': {\'precision\': 0.91, \'recall\': 0.92, \'f1-score\': 0.91, \'support\': 150}, # \'accuracy\': 0.91, # \'macro avg\': {\'precision\': 0.91, \'recall\': 0.91, \'f1-score\': 0.91, \'support\': 300}, # \'weighted avg\': {\'precision\': 0.91, \'recall\': 0.91, \'f1-score\': 0.91, \'support\': 300} # } ```","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import classification_report import warnings def scikit_learn_workflow(n_iter_no_change: int) -> dict: # Generate the synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train the GradientBoostingClassifier classifier = GradientBoostingClassifier(n_iter_no_change=n_iter_no_change, random_state=42) # Handle the warnings with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\") classifier.fit(X_train, y_train) # Predict and generate the classification report y_pred = classifier.predict(X_test) report = classification_report(y_test, y_pred, output_dict=True) return report"},{"question":"**Problem Statement:** You are tasked with creating a Python script that uses the \\"mailcap\\" module to automatically determine and execute the appropriate command for opening a file of a given MIME type. Your script should: 1. Read configurations from the system mailcap files and the user\'s mailcap file. 2. Find an applicable command for the provided MIME type using the mailcap file configurations. 3. Execute the found command to open the provided file. For the purposes of this exercise, assume that: - Mailcap files follow the standard structure as described in RFC 1524. - All file paths and MIME types provided as input are valid. **Function Signature:** ```python def open_mime_file(mime_type: str, file_name: str) -> None: pass ``` **Input:** - `mime_type` (str): The MIME type of the file to be opened, e.g., \'video/mpeg\'. - `file_name` (str): The path to the file to be opened, e.g., \'example_video.mpeg\'. **Output:** - This function does not return any value. Instead, it should execute the command necessary to open the provided file with the appropriate application. **Constraints:** - The solution should handle the security constraint mentioned in the documentation, where only certain characters are allowed in the command line to prevent shell metacharacter injection. - If no suitable command is found for the given MIME type, the function should print an appropriate message, e.g., \\"No suitable application found for MIME type \'video/mpeg\'\\". **Example Usage:** ```python # Example usage open_mime_file(\'video/mpeg\', \'example_video.mpeg\') # Expected command execution output (if the appropriate mailcap entry is found): # xmpeg example_video.mpeg ``` **Notes:** - Make sure to include appropriate error handling for situations such as missing mailcap files or invalid entries. - Utilize the `mailcap.getcaps()` and `mailcap.findmatch()` functions to retrieve and process mailcap entries.","solution":"import mailcap import os import shlex import subprocess def open_mime_file(mime_type: str, file_name: str) -> None: # Retrieve mailcap entries caps = mailcap.getcaps() # Find match for provided MIME type match = mailcap.findmatch(caps, mime_type, filename=file_name) if match: command, entry = match # Prepare the command string safe_command = shlex.split(command) try: # Execute the command subprocess.run(safe_command) except Exception as e: print(f\\"Failed to open file due to error: {e}\\") else: print(f\\"No suitable application found for MIME type \'{mime_type}\'\\")"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},R={class:"card-container"},q={key:0,class:"empty-state"},F=["disabled"],O={key:0},M={key:1};function N(i,e,l,m,n,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")}," âœ• ")):d("",!0)]),t("div",R,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",q,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),s("span",M,"Loading...")):(a(),s("span",O,"See more"))],8,F)):d("",!0)])}const j=p(z,[["render",N],["__scopeId","data-v-4f6463ef"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/39.md","filePath":"chatai/39.md"}'),L={name:"chatai/39.md"},H=Object.assign(L,{setup(i){return(e,l)=>(a(),s("div",null,[x(j)]))}});export{Y as __pageData,H as default};
