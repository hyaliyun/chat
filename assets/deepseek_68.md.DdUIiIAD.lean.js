import{_ as p,o as a,c as n,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(s,e,l,m,i,o){return a(),n("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-2d089d0f"]]),I=JSON.parse('[{"question":"# PyTorch Numerical Accuracy and Batched Computations Objective: The goal of this task is to assess your understanding of numerical stability, accuracy, and batched computations in PyTorch. You will implement a function that performs batched matrix multiplications and handles potential numerical issues that may arise. Task: You need to implement a function `batched_matrix_product` in PyTorch that: 1. Takes in two 3D tensors `A` and `B` suitable for batched matrix multiplication. 2. Computes the batched matrix products using single precision (FP32) and double precision (FP64), and compares the results. 3. Computes the element-wise difference between the results of FP32 and FP64 computations. Input: * `A`: A 3D tensor of shape `(batch_size, m, n)` where `batch_size` is the number of batches, and `m, n` are the matrix dimensions. * `B`: A 3D tensor of shape `(batch_size, n, p)` where `batch_size` is the number of batches, and `n, p` are the matrix dimensions. Output: * A dictionary with the following keys: * `fp32_result`: The resulting tensor from batched matrix multiplication using FP32 precision. * `fp64_result`: The resulting tensor from batched matrix multiplication using FP64 precision. * `difference`: The element-wise absolute difference between `fp32_result` and `fp64_result`. Constraints: 1. Ensure that the computation handles potential overflow and underflow when dealing with large values. 2. You must not use loops for the matrix multiplications; leverage PyTorch\'s built-in batched computation capabilities. 3. Assume that the input tensors are compatible for matrix multiplication. Example: ```python import torch def batched_matrix_product(A, B): # Your implementation here pass # Example inputs A = torch.randn(10, 5, 5) B = torch.randn(10, 5, 5) result = batched_matrix_product(A, B) print(result[\'fp32_result\']) print(result[\'fp64_result\']) print(result[\'difference\']) ``` Notes: 1. Ensure that the tensors `A` and `B` are converted to the appropriate precision types before performing the multiplications. 2. The function should handle different device types (CPU and GPU) seamlessly. # Evaluation Criteria: 1. Correctness of the implementation. 2. Handling of numerical stability and precision. 3. Efficiency and use of PyTorch\'s batched computation capabilities. 4. Code readability and comments explaining key steps. Good luck!","solution":"import torch def batched_matrix_product(A, B): Computes batched matrix multiplication using FP32 and FP64 precision. Parameters: A (torch.Tensor): 3D tensor of shape (batch_size, m, n) B (torch.Tensor): 3D tensor of shape (batch_size, n, p) Returns: dict: Dictionary containing the fp32_result, fp64_result and the difference. # Ensure the tensors are in the correct device and dtype A_fp32 = A.to(dtype=torch.float32) B_fp32 = B.to(dtype=torch.float32) A_fp64 = A.to(dtype=torch.float64) B_fp64 = B.to(dtype=torch.float64) # Perform batched matrix multiplication fp32_result = torch.bmm(A_fp32, B_fp32) fp64_result = torch.bmm(A_fp64, B_fp64) # Compute the element-wise absolute difference difference = torch.abs(fp32_result.to(dtype=torch.float64) - fp64_result) return { \'fp32_result\': fp32_result, \'fp64_result\': fp64_result, \'difference\': difference }"},{"question":"Coding Assessment Question # Objective Create a Python function to dynamically configure logging using a dictionary. This function should handle both standard and custom logging components, demonstrate proper handling of different logging configurations, and ensure correct application considering potential security risks. # Problem Statement Write a function `configure_logging(config_dict: dict) -> None` that uses the given dictionary `config_dict` to configure Python’s logging module. This function should: 1. Validate the dictionary structure to ensure it is correctly formatted according to the schema described in the documentation. 2. Create and configure custom handlers, formatters, and filters if specified in the dictionary. 3. Log a test message using the newly configured logging settings and output the result to demonstrate that the configuration was successful. 4. Ensure security considerations are handled appropriately when using user-defined objects. # Input - `config_dict` (dict): A dictionary representing the logging configuration. The dictionary schema will be as described in the logging documentation. # Output - None. However, a test log message should be printed using the newly configured logging settings. # Constraints - The function should raise appropriate exceptions if there are any issues with the configuration dictionary, such as invalid formats or missing keys. - When using custom formatters, handlers, or filters, ensure they are securely instantiated and avoid evaluating arbitrary code. # Example ```python config_dict_example = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\' } }, \'loggers\': { \'my_logger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\'], \'propagate\': False } }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'] } } # Expected Usage configure_logging(config_dict_example) ``` # Notes - You may use the built-in `logging.config` module to assist with configuration. - For the purpose of this question, a simple custom formatter example is embedded in the provided dictionary. You may create more complex custom classes and ensure proper integration. - Ensure that validation and security considerations are adequately addressed. # Hint Use Python’s `logging.config.dictConfig` function along with custom class instantiation logic to handle user-defined objects.","solution":"import logging import logging.config def configure_logging(config_dict: dict) -> None: Configures logging based on the provided configuration dictionary. Parameters: config_dict (dict): A dictionary containing the logging configuration. Raises: ValueError: If there are any issues with the configuration dictionary. # Validate the structure of config_dict if not isinstance(config_dict, dict): raise ValueError(\\"The logging configuration must be provided as a dictionary.\\") required_keys = {\'version\'} if not required_keys.issubset(config_dict.keys()): raise ValueError(f\\"The configuration dictionary must contain the keys: {required_keys}\\") # Try to apply the logger configuration try: logging.config.dictConfig(config_dict) # Log a test message logger = logging.getLogger(\\"my_logger\\") logger.debug(\\"Logging configuration was applied successfully.\\") except Exception as e: raise ValueError(f\\"An error occurred while configuring logging: {e}\\")"},{"question":"Overview In this exercise, you are tasked with implementing a data preprocessing pipeline using scikit-learn. This pipeline will involve scaling features and reducing the dimensionality of the dataset using Principal Component Analysis (PCA). Task Given a dataset, you are required to: 1. Scale the features using StandardScaler. 2. Reduce the dimensionality of the dataset to a specified number of components using PCA. 3. Implement this preprocessing in a single, seamless pipeline. Input - A dataset `X` with shape (n_samples, n_features), where `n_samples` is the number of samples and `n_features` is the number of features. - An integer `n_components` specifying the number of principal components to retain in the PCA step. Output - Transformed dataset `X_transformed` with shape (n_samples, n_components). Requirements 1. Use the `StandardScaler` from `sklearn.preprocessing` to scale the features. 2. Use the `PCA` from `sklearn.decomposition` to perform dimensionality reduction. 3. Use a pipeline from `sklearn.pipeline`. Constraints - The dataset `X` will have at least 5 samples and 5 features. - The value of `n_components` will be less than or equal to the number of features in `X`. Example ```python from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline def preprocess_data(X, n_components): Preprocess the dataset by applying scaling and PCA. Parameters: X: numpy.ndarray of shape (n_samples, n_features) The input dataset. n_components: int The number of principal components to retain. Returns: X_transformed: numpy.ndarray of shape (n_samples, n_components) The transformed dataset. # Create a pipeline with a StandardScaler and PCA pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=n_components)) ]) # Fit and transform the data X_transformed = pipeline.fit_transform(X) return X_transformed # Example usage: import numpy as np X = np.array([[0.8, 1.0, 2.5], [2.4, 1.5, 3.8], [1.2, 0.7, 0.9], [0.5, 2.0, 1.3], [2.2, 2.5, 3.1]]) n_components = 2 X_transformed = preprocess_data(X, n_components) print(X_transformed) ``` Explanation In the provided example, the input dataset `X` with shape (5, 3) is first scaled using `StandardScaler`. Then, PCA is applied to reduce the dataset to 2 principal components. The resulting `X_transformed` has the shape (5, 2).","solution":"from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline def preprocess_data(X, n_components): Preprocess the dataset by applying scaling and PCA. Parameters: X: numpy.ndarray of shape (n_samples, n_features) The input dataset. n_components: int The number of principal components to retain. Returns: X_transformed: numpy.ndarray of shape (n_samples, n_components) The transformed dataset. # Create a pipeline with a StandardScaler and PCA pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=n_components)) ]) # Fit and transform the data X_transformed = pipeline.fit_transform(X) return X_transformed"},{"question":"Problem You are required to create a custom class and define a special pickling function for this class using the `copyreg` module. The tasks for this exercise are as follows: 1. Implement a class `Person` which has the attributes: - `name` (string) - `age` (integer) - `email` (string) 2. Implement a function `pickle_person(person)` that will be used to pickle instances of `Person`. This function should: - Return a tuple where the first item is the `Person` class and the second item is a tuple containing the attributes of the `Person` instance (`name`, `age`, `email`). 3. Register the `pickle_person` function for the `Person` class using `copyreg.pickle`. 4. Demonstrate that your pickling function works by: - Creating an instance of the `Person` class. - Using the `copy.copy` function to create a shallow copy of this instance. - Using the `pickle.dumps` function to serialize the instance. # Input You do not take any input from the user for this task. # Output Your code should print the following: 1. A message stating \\"pickling a Person instance...\\" every time the `pickle_person` function is called. 2. The shallow copy of the `Person` instance. 3. The pickled (serialized) version of the `Person` instance. # Example ```python import copyreg, copy, pickle # Implement the Person class # Implement the pickle_person function # Register the pickle function using copyreg.pickle # Create an instance of Person person = Person(\\"John Doe\\", 30, \\"johndoe@example.com\\") # Shallow copy the person instance person_copy = copy.copy(person) # Serialize the person instance person_pickled = pickle.dumps(person) # Output # pickling a Person instance... # <Person object with name=\'John Doe\', age=30, email=\'johndoe@example.com\'> # b\'...serialized data...\' ``` # Constraints - Make sure the `name` is a non-empty string. - Make sure the `age` is a non-negative integer. - Make sure the `email` is a valid email string. # Performance - The implementation should handle serialization of a moderate number of `Person` instances efficiently.","solution":"import copyreg, copy, pickle class Person: def __init__(self, name, age, email): if not isinstance(name, str) or not name: raise ValueError(\\"Name must be a non-empty string\\") if not isinstance(age, int) or age < 0: raise ValueError(\\"Age must be a non-negative integer\\") if not isinstance(email, str) or \\"@\\" not in email: raise ValueError(\\"Email must be a valid email string\\") self.name = name self.age = age self.email = email def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, email={self.email})\\" def pickle_person(person): print(\\"pickling a Person instance...\\") return (Person, (person.name, person.age, person.email)) # Register the pickle function using copyreg.pickle copyreg.pickle(Person, pickle_person) # Create an instance of Person person = Person(\\"John Doe\\", 30, \\"johndoe@example.com\\") # Shallow copy the person instance person_copy = copy.copy(person) # Serialize the person instance person_pickled = pickle.dumps(person) # Output print(person_copy) print(person_pickled)"},{"question":"**Problem Statement:** You are tasked with creating a custom email encoder function using Python 3\'s email package. This function should dynamically choose the appropriate encoding based on the content of the message and set the necessary *Content-Transfer-Encoding* header. Write a function `custom_encode(msg)` that takes a `Message` object as input, examines its payload, and applies the following logic: - If the payload contains more than 50% non-printable characters (ascertainable by `str.isprintable()`), use base64 encoding. - If the payload is mostly printable but contains any unprintable characters, use quoted-printable encoding. - If the payload is fully printable, use 7bit or 8bit encoding (you can decide the exact encoding based on the payload). You may assume the payload is text and can be obtained using `msg.get_payload()`. Your function should not modify multipart messages and should raise a `TypeError` if a multipart message is passed. # Function Signature ```python def custom_encode(msg) -> None: pass ``` # Input - **msg**: A single `Message` object from Python\'s email package. # Output None (the function should modify the `msg` object in-place). # Constraints - Do not use any deprecated facilities from Python\'s email package. - Do not modify multipart messages; raise `TypeError` if needed. # Example ```python from email.message import Message from email.encoders import encode_quopri, encode_base64, encode_7or8bit msg = Message() msg.set_payload(\\"This is a test message with some non-printable characters x00x01.\\") custom_encode(msg) assert msg[\'Content-Transfer-Encoding\'] == \\"base64\\" msg.set_payload(\\"This is a fully printable message.\\") custom_encode(msg) assert msg[\'Content-Transfer-Encoding\'] in [\\"7bit\\", \\"8bit\\"] ``` # Notes - Use the available encoding functions (`encode_quopri`, `encode_base64`, `encode_7or8bit`) from the email.encoders module to set the payload and headers accordingly. - The function should modify the `msg` object directly without returning anything.","solution":"from email.message import Message from email.encoders import encode_quopri, encode_base64, encode_7or8bit def custom_encode(msg) -> None: if msg.is_multipart(): raise TypeError(\\"Multipart messages are not supported\\") payload = msg.get_payload() total_chars = len(payload) non_printable_chars = sum(1 for c in payload if not c.isprintable()) if non_printable_chars / total_chars > 0.5: encode_base64(msg) msg[\'Content-Transfer-Encoding\'] = \'base64\' elif non_printable_chars > 0: encode_quopri(msg) msg[\'Content-Transfer-Encoding\'] = \'quoted-printable\' else: encode_7or8bit(msg) msg[\'Content-Transfer-Encoding\'] = \'7bit\' # or \'8bit\' based on a decision, here choosing \'7bit\'"},{"question":"# Socket Programming Task: Implementing an Echo Server and Client Objective Design and implement a simple echo server and client using the Python `socket` module. The server will listen for incoming connections, receive messages from clients, and send back the same messages. The client will connect to the server, send a message, and print the echoed message received from the server. Requirements 1. **Echo Server:** - The server should use the IPv4 address family (`AF_INET`) and use a TCP socket (`SOCK_STREAM`). - The server should bind to `localhost` (127.0.0.1) and port `65432`. - The server should handle multiple client connections using one accept loop (i.e., it should stay running to handle multiple clients one after another). - For each connected client, the server should: - Receive a message. - Echo the received message back to the client. - Close the connection with the client when the message \\"bye\\" is received from that client. 2. **Client:** - The client should use the IPv4 address family (`AF_INET`) and use a TCP socket (`SOCK_STREAM`). - The client should connect to the server at `localhost` (127.0.0.1) and port `65432`. - The client should: - Send a user-input message to the server. - Print the response received from the server. - Close the connection when the message \\"bye\\" is sent. Input/Output Formats - Both server and client should handle string messages encoded in UTF-8. - The messages may contain any printable characters. - The server prints a message on the console whenever a client connects or disconnects. Constraints - Ensure the server is robust against improperly closed connections. - Use blocking sockets for simplicity. Performance Requirements - Handle multiple client connections without server crashes. - Clients should connect and receive responses in real-time (minimal latency). # Implementation Setup 1. **Server Implementation:** - Implement the server in a function `start_echo_server()`. 2. **Client Implementation:** - Implement the client in a function `start_echo_client(message)` where `message` is a string to be sent to the server. # Example Execution ```python # Server: Run this in one script or interactive session. start_echo_server() # Client: Run this in another script or interactive session. start_echo_client(\\"Hello, Server!\\") start_echo_client(\\"How are you?\\") start_echo_client(\\"bye\\") ``` Server Console Output Example ``` Server started on localhost:65432 Connected by (\'127.0.0.1\', 12345) Received: Hello, Server! Sent: Hello, Server! Connection closed. Connected by (\'127.0.0.1\', 12346) Received: How are you? Sent: How are you? Connection closed. Connected by (\'127.0.0.1\', 12347) Received: bye Sent: bye Connection closed. ``` Client Console Output Example ``` Sent: Hello, Server! Received: Hello, Server! Sent: How are you? Received: How are you? Sent: bye Received: bye ``` # Submission Submit your implementation of: - `start_echo_server()` - `start_echo_client(message)` The functions should be fully functional, correctly handling socket operations, client connections, and message echoes.","solution":"import socket def start_echo_server(): host = \'127.0.0.1\' port = 65432 with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.bind((host, port)) s.listen() print(f\\"Server started on {host}:{port}\\") while True: conn, addr = s.accept() with conn: print(f\\"Connected by {addr}\\") while True: data = conn.recv(1024) if not data: break message = data.decode(\'utf-8\') print(f\\"Received: {message}\\") conn.sendall(data) print(f\\"Sent: {message}\\") if message.lower() == \\"bye\\": break print(\\"Connection closed.\\") def start_echo_client(message): host = \'127.0.0.1\' port = 65432 with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((host, port)) print(f\\"Connected to server at {host}:{port}\\") print(f\\"Sent: {message}\\") s.sendall(message.encode(\'utf-8\')) data = s.recv(1024) print(f\\"Received: {data.decode(\'utf-8\')}\\") if message.lower() == \\"bye\\": print(\\"Connection closed by client.\\")"},{"question":"Objective This task will assess your ability to define and use Python enums, including creating advanced enums with custom behaviors and ensuring unique values for enum members. Problem Statement You are tasked with implementing an enum class to represent various user permissions within a software system using the `enum` module in Python. This system includes basic user permissions and roles. Each role can comprise multiple permissions using bitwise operations. Requirements 1. **Permission Enum:** - Define an `IntFlag` enum called `Permission` that includes the following values: - `READ` with a value of `1` - `WRITE` with a value of `2` - `EXECUTE` with a value of `4` - Allow combinations of these permissions using bitwise operations. 2. **Role Enum:** - Define a `Flag` enum called `Role` that includes the following roles: - `USER` having `Permission.READ` - `DEVELOPER` having `Permission.READ | Permission.WRITE` - `ADMIN` having `Permission.READ | Permission.WRITE | Permission.EXECUTE` - Ensure that no roles can have the same exact combination of permissions using the `unique` decorator. 3. **Methods:** - Implement a method in the `Role` enum called `has_permission` which takes a `Permission` value as an argument and returns `True` if the role includes that permission, and `False` otherwise. - Implement a method in the `Permission` enum called `describe` which returns a string description of the permissions. Function Signatures ```python from enum import IntFlag, Flag, auto, unique class Permission(IntFlag): # Define permissions here class Role(Flag): # Define roles here # Testing the methods assert Role.ADMIN.has_permission(Permission.WRITE) == True assert Role.USER.has_permission(Permission.EXECUTE) == False assert Permission.READ.describe() == \\"READ\\" assert Permission.WRITE.describe() == \\"WRITE\\" ``` Constraints - Enums should be defined using the class-based syntax. - Use the `unique` decorator to ensure each role has a unique permission combination. # Example Usage ```python # Example Usage admin_permissions = Role.ADMIN print(admin_permissions) # Output: Role.ADMIN print(admin_permissions.has_permission(Permission.WRITE)) # Output: True developer_permissions = Role.DEVELOPER print(developer_permissions) # Output: Role.DEVELOPER print(developer_permissions.has_permission(Permission.EXECUTE)) # Output: False print(Permission.READ.describe()) # Output: READ print(Permission.WRITE.describe()) # Output: WRITE ``` # Note This assessment will evaluate your understanding of: - Basic and advanced enum usage. - Enum creation with `IntFlag`, bitwise operations, and ensuring unique values. - Writing custom methods within enums.","solution":"from enum import IntFlag, Flag, unique class Permission(IntFlag): READ = 1 WRITE = 2 EXECUTE = 4 def describe(self): return self.name @unique class Role(Flag): USER = Permission.READ DEVELOPER = Permission.READ | Permission.WRITE ADMIN = Permission.READ | Permission.WRITE | Permission.EXECUTE def has_permission(self, permission): return (self.value & permission.value) == permission.value"},{"question":"# Python Built-in Types: Coding Assessment Problem Statement: You are tasked with implementing a custom data structure based on the built-in `list` type. This data structure, `CustomList`, should allow only positive integers to be added to the list. You must implement the following features: 1. **Initialization**: Initialize the `CustomList` with an optional iterable containing positive integers. If any of the elements are not positive integers, raise a `ValueError` with the message \\"All elements must be positive integers\\". 2. **Add Method**: Implement a method `add_element` that adds a single positive integer to the `CustomList`. If the element is not a positive integer, raise a `ValueError` with the message \\"Element must be a positive integer\\". 3. **Remove Method**: Implement a method `remove_element` that removes a specified element if it exists in the `CustomList`. If the element is not in the list, it should raise a `ValueError` with the message \\"Element not found\\". 4. **Sum of Elements**: Implement a method `sum_elements` that returns the sum of all elements in the `CustomList`. 5. **Size Method**: Implement a method `size` that returns the number of elements in the `CustomList`. 6. **String Representation**: Override the `__str__` method to return a string representation of the `CustomList` in the format: \\"CustomList: [element1, element2, ...]\\" Input and Output: ```python class CustomList: def __init__(self, iterable=None): pass def add_element(self, element): pass def remove_element(self, element): pass def sum_elements(self): pass def size(self): pass def __str__(self): pass ``` Example: ```python cl = CustomList([1, 2, 3]) print(cl) # CustomList: [1, 2, 3] cl.add_element(4) print(cl) # CustomList: [1, 2, 3, 4] cl.remove_element(2) print(cl) # CustomList: [1, 3, 4] print(cl.size()) # 3 print(cl.sum_elements()) # 8 try: cl.add_element(-1) except ValueError as e: print(e) # Element must be a positive integer try: cl = CustomList([1, -2, 3]) except ValueError as e: print(e) # All elements must be positive integers try: cl.remove_element(10) except ValueError as e: print(e) # Element not found ``` # Constraints: - Elements of `CustomList` must always be positive integers. - Methods should handle errors gracefully by raising `ValueError` with appropriate messages as specified. Evaluation Criteria: - Correctness and completeness of the `CustomList` class implementation. - Appropriate use of built-in types and methods. - Adherence to the provided interface and error handling. Implement the `CustomList` class by adhering to the specifications provided.","solution":"class CustomList: def __init__(self, iterable=None): Initialize the CustomList with an optional iterable containing positive integers. If any of the elements are not positive integers, raise a ValueError. self.elements = [] if iterable is not None: for elem in iterable: self._validate_positive_integer(elem) self.elements.append(elem) def add_element(self, element): Add a single positive integer to the CustomList. Raise a ValueError if the element is not a positive integer. self._validate_positive_integer(element) self.elements.append(element) def remove_element(self, element): Remove a specified element if it exists in the CustomList. Raise a ValueError if the element is not found. if element not in self.elements: raise ValueError(\\"Element not found\\") self.elements.remove(element) def sum_elements(self): Return the sum of all elements in the CustomList. return sum(self.elements) def size(self): Return the number of elements in the CustomList. return len(self.elements) def __str__(self): Return a string representation of the CustomList. return f\\"CustomList: {self.elements}\\" def _validate_positive_integer(self, element): Validate that the element is a positive integer. Raise a ValueError if it is not. if not isinstance(element, int) or element <= 0: raise ValueError(\\"Element must be a positive integer\\")"},{"question":"# **Coding Assessment Question** **Objective:** Design a function that securely generates different types of random tokens for varied security applications. **Problem Statement:** You have been tasked with creating a secure token generator for a web application. The generator must be able to produce three types of tokens: 1. **Byte Token**: A random byte string of a specified length. 2. **Hexadecimal Token**: A random hexadecimal string of a specified byte length. 3. **URL-Safe Token**: A random URL-safe string of a specified byte length. The function should also ensure the use of secure random number generation and validate the length of the tokens as per the given constraints. **Function Signature:** ```python def generate_secure_token(token_type: str, nbytes: int) -> str: Generates a secure token based on the type and specified byte length. Parameters: - token_type (str): The type of token to generate. It can be \'bytes\', \'hex\', or \'urlsafe\'. - nbytes (int): The number of bytes of randomness to use for generating the token. Returns: - str: The generated token string. Raises: - ValueError: If the token_type is invalid. pass ``` **Expected Input and Output:** 1. **Input**: - `token_type` (str): The type of the token (\'bytes\', \'hex\', or \'urlsafe\'). - `nbytes` (int): The number of bytes for the token\'s randomness. 2. **Output**: - A string representing the generated token. **Constraints:** - The `nbytes` argument must be a positive integer (1 ≤ nbytes ≤ 64). - If the token type is not one of \'bytes\', \'hex\', or \'urlsafe\', the function must raise a `ValueError`. **Performance Requirements:** - The function should leverage the `secrets` module for all random number generations to ensure cryptographic strength. **Example Usage:** ```python # Example 1: Generate a 16-byte token in hexadecimal token1 = generate_secure_token(\'hex\', 16) print(token1) # Output: A 32-character hexadecimal string. # Example 2: Generate a 32-byte URL-safe token token2 = generate_secure_token(\'urlsafe\', 32) print(token2) # Output: A URL-safe string with 32 bytes of randomness. # Example 3: Generate an 8-byte random byte token token3 = generate_secure_token(\'bytes\', 8) print(token3) # Output: An 8-byte long random byte string. # Example 4: Invalid token type should raise a ValueError try: token4 = generate_secure_token(\'invalid_type\', 10) except ValueError as e: print(e) # Output: \\"Invalid token type. Must be \'bytes\', \'hex\', or \'urlsafe\'.\\" ``` **Note:** Students should ensure their function is secure and correctly handles input validation.","solution":"import secrets def generate_secure_token(token_type: str, nbytes: int) -> str: Generates a secure token based on the type and specified byte length. Parameters: - token_type (str): The type of token to generate. It can be \'bytes\', \'hex\', or \'urlsafe\'. - nbytes (int): The number of bytes of randomness to use for generating the token. Returns: - str: The generated token string. Raises: - ValueError: If the token_type is invalid. if not (1 <= nbytes <= 64): raise ValueError(\\"nbytes must be between 1 and 64.\\") if token_type == \'bytes\': return secrets.token_bytes(nbytes).decode(\'latin1\') elif token_type == \'hex\': return secrets.token_hex(nbytes) elif token_type == \'urlsafe\': return secrets.token_urlsafe(nbytes) else: raise ValueError(\\"Invalid token type. Must be \'bytes\', \'hex\', or \'urlsafe\'.\\")"},{"question":"# Custom Sequence Type Implementation You are required to implement a custom sequence type in Python, which demonstrates your understanding of the Python object structures and sequence protocols. Objectives: - Create a new class `CustomList` that mimics the behavior of Python\'s built-in list type. - Implement the following methods in `CustomList`: - `__init__(self, iterable)`: Constructor that initializes the object with items from the iterable. - `__len__(self)`: Return the length of the custom list. - `__getitem__(self, index)`: Retrieve an item by index (support both positive and negative indices). - `__setitem__(self, index, value)`: Set an item at a specific index. - `__delitem__(self, index)`: Delete an item by index. - `__iter__(self)`: Return an iterator over the custom list items. - `__contains__(self, item)`: Check if an item exists in the custom list. - `__repr__(self)`: Return a string representation of the custom list. Example: ```python clist = CustomList([1, 2, 3, 4]) print(len(clist)) # Output: 4 print(clist[2]) # Output: 3 clist[2] = 10 print(clist[2]) # Output: 10 del clist[2] print(clist) # Output: CustomList([1, 2, 4]) print(3 in clist) # Output: False print([i for i in clist]) # Output: [1, 2, 4] print(clist) # Output: CustomList([1, 2, 4]) ``` Constraints: - The `CustomList` should support positive and negative indexing. - Ensure that the methods handle edge cases such as out-of-bounds indices appropriately by throwing `IndexError`. Performance Requirements: - The implementation should conform to the time complexity requirements similar to Python\'s built-in list operations.","solution":"class CustomList: def __init__(self, iterable): self._data = list(iterable) def __len__(self): return len(self._data) def __getitem__(self, index): if index >= len(self._data) or index < -len(self._data): raise IndexError(\\"list index out of range\\") return self._data[index] def __setitem__(self, index, value): if index >= len(self._data) or index < -len(self._data): raise IndexError(\\"list assignment index out of range\\") self._data[index] = value def __delitem__(self, index): if index >= len(self._data) or index < -len(self._data): raise IndexError(\\"list deletion index out of range\\") del self._data[index] def __iter__(self): return iter(self._data) def __contains__(self, item): return item in self._data def __repr__(self): return f\'CustomList({self._data})\'"},{"question":"**Question: Implementing Complex Attribute Handlers** In this exercise, you will create a Python class that simulates some of the low-level operations detailed in the provided documentation. Your task is to implement a class named `CustomClass` that allows attribute manipulation and item access in a sophisticated manner. **Requirements:** 1. **Class Implementation:** - Create a `CustomClass` that can store and retrieve attributes and items. 2. **Methods to Implement:** - `has_attribute(self, attr_name: str) -> bool`: Returns `True` if the attribute `attr_name` exists in the instance, `False` otherwise. - `get_attribute(self, attr_name: str) -> Any`: Returns the value of the attribute `attr_name`. If the attribute does not exist, raise an `AttributeError`. - `set_attribute(self, attr_name: str, value: Any) -> None`: Sets the attribute `attr_name` to the provided `value`. - `del_attribute(self, attr_name: str) -> None`: Deletes the attribute `attr_name`. If the attribute does not exist, raise an `AttributeError`. - `compare(self, other: \'CustomClass\', opid: str) -> bool`: Compares the instance with another `CustomClass` instance using the specified operation—valid operations are `\\"==\\"`, `\\"!=\\"`, `\\"<\\"`, `\\"<=\\"`, `\\">\\"`, `\\">=\\"`. 3. **Constraints:** - You cannot use any built-in functions like `hasattr`, `getattr`, `setattr`, or `delattr` directly in the implementation of the methods. - You must simulate the behavior as if they were implemented in lower-level code. **Sample Usage:** ```python obj1 = CustomClass() obj2 = CustomClass() # Setting attributes obj1.set_attribute(\'name\', \'Alice\') obj1.set_attribute(\'age\', 30) obj2.set_attribute(\'name\', \'Bob\') obj2.set_attribute(\'age\', 25) # Getting attributes assert obj1.get_attribute(\'name\') == \'Alice\' assert obj2.get_attribute(\'name\') == \'Bob\' # Checking attributes assert obj1.has_attribute(\'name\') is True assert obj2.has_attribute(\'address\') is False # Deleting attributes obj1.del_attribute(\'age\') assert obj1.has_attribute(\'age\') is False # Comparing objects assert obj1.compare(obj2, \\"==\\") is False assert obj1.compare(obj2, \\"!=\\") is True ``` **Notes:** - Think carefully about how to implement the internal storage of attributes. - Handle exceptions properly to match the behavior described by the requirements. **Performance Requirements:** - Implement the methods efficiently considering potential large number of attributes. Good luck!","solution":"class CustomClass: def __init__(self): self.__dict__[\'_attributes\'] = {} def has_attribute(self, attr_name: str) -> bool: return attr_name in self._attributes def get_attribute(self, attr_name: str): if attr_name in self._attributes: return self._attributes[attr_name] else: raise AttributeError(f\\"\'CustomClass\' object has no attribute \'{attr_name}\'\\") def set_attribute(self, attr_name: str, value): self._attributes[attr_name] = value def del_attribute(self, attr_name: str): if attr_name in self._attributes: del self._attributes[attr_name] else: raise AttributeError(f\\"\'CustomClass\' object has no attribute \'{attr_name}\'\\") def compare(self, other: \'CustomClass\', opid: str) -> bool: if not isinstance(other, CustomClass): raise TypeError(\\"Comparisons can only be made between CustomClass instances.\\") valid_ops = {\\"==\\", \\"!=\\", \\"<\\", \\"<=\\", \\">\\", \\">=\\"} if opid not in valid_ops: raise ValueError(f\\"Invalid operation \'{opid}\'. Supported operations are {valid_ops}.\\") if opid == \\"==\\": return self._attributes == other._attributes elif opid == \\"!=\\": return self._attributes != other._attributes elif opid == \\"<\\": return list(self._attributes.items()) < list(other._attributes.items()) elif opid == \\"<=\\": return list(self._attributes.items()) <= list(other._attributes.items()) elif opid == \\">\\": return list(self._attributes.items()) > list(other._attributes.items()) elif opid == \\">=\\": return list(self._attributes.items()) >= list(other._attributes.items())"},{"question":"**Title:** Implement a Performance-analytical Solution for a Fibonacci Sequence Generator Objective: Write a Python function to generate the nth Fibonacci number. Using the `timeit` and `cProfile` modules, you must provide detailed performance analysis to identify and optimize any performance bottlenecks in your solution. Description: 1. **Function Implementation:** Implement a function `fibonacci(n: int) -> int` that returns the nth Fibonacci number. - The Fibonacci sequence is defined as: - `F(0) = 0` - `F(1) = 1` - `F(n) = F(n-1) + F(n-2) for n >= 2` 2. **Performance Analysis:** - Use the `timeit` module to measure the average time taken to compute the 30th Fibonacci number over 10000 executions. - Use the `cProfile` module to profile the function and identify the main performance bottlenecks. 3. **Optimization:** - Optimize the `fibonacci` function to reduce the computation time. You can choose any algorithmic optimizations (e.g., memoization, iterative implementation). - Provide the new performance metrics using `timeit` and `cProfile` for the optimized solution. Input: - An integer `n` representing the position in the Fibonacci sequence. Output: - An integer representing the nth Fibonacci number. Constraints: - `0 <= n <= 50` Example: ```python def fibonacci(n: int) -> int: # Your implementation here pass # Example Usage n = 30 print(fibonacci(n)) # Expected output for n=30: 832040 ``` Additional Requirements: 1. Provide a detailed summary of your performance analysis and optimizations. 2. Include the `timeit` and `cProfile` data before and after optimization in your final submission. Submission: Submit your Python script with the function implementation, analysis, and optimizations.","solution":"def fibonacci_recursive(n: int) -> int: Returns the nth Fibonacci number using a naive recursive approach. if n <= 1: return n return fibonacci_recursive(n-1) + fibonacci_recursive(n-2) def fibonacci_optimized(n: int) -> int: Returns the nth Fibonacci number using an optimized iterative approach. if n <= 1: return n fib = [0, 1] for i in range(2, n + 1): fib.append(fib[-1] + fib[-2]) return fib[n] # Performance analysis using timeit if __name__ == \\"__main__\\": import timeit import cProfile import pstats import io n = 30 # Timing the recursive implementation (Expected to be slow) recursive_time = timeit.timeit(lambda: fibonacci_recursive(n), number=10) print(f\\"Average time for recursive implementation: {recursive_time / 10:.8f} seconds\\") # Timing the optimized implementation optimized_time = timeit.timeit(lambda: fibonacci_optimized(n), number=10000) print(f\\"Average time for optimized implementation: {optimized_time / 10000:.8f} seconds\\") # Profiling the recursive implementation pr = cProfile.Profile() pr.enable() fibonacci_recursive(n) pr.disable() s = io.StringIO() sortby = \'cumulative\' ps = pstats.Stats(pr, stream=s).sort_stats(sortby) ps.print_stats() print(\\"Profile data for recursive implementation:\\") print(s.getvalue()) # Profiling the optimized implementation pr = cProfile.Profile() pr.enable() fibonacci_optimized(n) pr.disable() s = io.StringIO() sortby = \'cumulative\' ps = pstats.Stats(pr, stream=s).sort_stats(sortby) ps.print_stats() print(\\"Profile data for optimized implementation:\\") print(s.getvalue())"},{"question":"# Timedelta Data Manipulation You are given a dataset that tracks events at specific timestamps over a week. Each event has a start and end time, and you need to compute the duration of each event, the average duration per day, and resample the events to group them into hourly intervals. **Input**: A CSV file (`events.csv`) with the following columns: - `event_id` (integer): The unique identifier for each event. - `start_time` (string): The start time of the event in the format `YYYY-MM-DD HH:MM:SS`. - `end_time` (string): The end time of the event in the format `YYYY-MM-DD HH:MM:SS`. Example CSV content: ```csv event_id,start_time,end_time 1,2023-01-01 08:00:00,2023-01-01 09:30:00 2,2023-01-01 10:15:00,2023-01-01 12:45:00 3,2023-01-02 14:00:00,2023-01-02 16:00:00 ... ``` **Output**: 1. A DataFrame containing the `event_id`, `duration` (as a Timedelta object), and the start day of the event. 2. The average duration of events per day. 3. A DataFrame resampled to hourly intervals, showing the count of events that started in each hour. # Function Definition Implement the following function: ```python import pandas as pd def analyze_events(file_path: str): Analyzes events from a CSV file and performs several Timedelta operations. Parameters: file_path (str): The path to the CSV file containing the events. Returns: DataFrame: A DataFrame containing event_id, duration, and start day. Series: A Series containing the average duration of events per day. DataFrame: A DataFrame resampled to hourly intervals, showing the count of events. # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Convert start_time and end_time to datetime df[\'start_time\'] = pd.to_datetime(df[\'start_time\']) df[\'end_time\'] = pd.to_datetime(df[\'end_time\']) # Compute duration as a Timedelta df[\'duration\'] = df[\'end_time\'] - df[\'start_time\'] # Extract the start day of the event df[\'start_day\'] = df[\'start_time\'].dt.date # Compute average duration per day avg_duration_per_day = df.groupby(\'start_day\')[\'duration\'].mean() # Resample events to hourly intervals df.set_index(\'start_time\', inplace=True) hourly_resampled = df.resample(\'H\')[\'event_id\'].count() return df[[\'event_id\', \'duration\', \'start_day\']], avg_duration_per_day, hourly_resampled ``` **Constraints**: 1. You should use pandas Timedelta functionalities to compute durations. 2. The function should handle missing values gracefully. **Performance Requirements**: 1. Your implementation should be efficient with handling large datasets (up to 100,000 events). Use this function to analyze the events data and return the results as specified.","solution":"import pandas as pd def analyze_events(file_path: str): Analyzes events from a CSV file and performs several Timedelta operations. Parameters: file_path (str): The path to the CSV file containing the events. Returns: DataFrame: A DataFrame containing event_id, duration, and start day. Series: A Series containing the average duration of events per day. DataFrame: A DataFrame resampled to hourly intervals, showing the count of events. # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Convert start_time and end_time to datetime df[\'start_time\'] = pd.to_datetime(df[\'start_time\']) df[\'end_time\'] = pd.to_datetime(df[\'end_time\']) # Compute duration as a Timedelta df[\'duration\'] = df[\'end_time\'] - df[\'start_time\'] # Extract the start day of the event df[\'start_day\'] = df[\'start_time\'].dt.date # Compute average duration per day avg_duration_per_day = df.groupby(\'start_day\')[\'duration\'].mean() # Resample events to hourly intervals df.set_index(\'start_time\', inplace=True) hourly_resampled = df.resample(\'H\')[\'event_id\'].count() return df[[\'event_id\', \'duration\', \'start_day\']].reset_index(drop=True), avg_duration_per_day, hourly_resampled"},{"question":"**Question: Implement a Custom Completer Using `rlcompleter`** You are required to implement a custom completer function using the `rlcompleter` module. This function should be capable of providing auto-completions for a given text based on different namespaces. **Part 1: Basic Completer** Create a `CustomCompleter` class that: 1. Is initialized with a custom namespace (a dictionary of possible identifier names). 2. Has a method `complete(text: str, state: int) -> str` that: - Completes from the custom namespace. - Returns the `state`th completion for the given `text`. **Part 2: Advanced Completer** Extend `CustomCompleter` to: 1. Handle dotted names and return completions based on attributes of objects in the custom namespace. 2. Catch any exceptions raised during evaluation and return `None` if the completion cannot be performed. **Specifications:** - Your `complete` method should not evaluate functions during the completion. - If `text` includes a period, use the `dir()` function to find matches. - Silently catch and handle any exceptions that occur during the evaluation. **Example:** ```python custom_ns = { \'math\': __import__(\'math\'), \'sys\': __import__(\'sys\'), \'variable\': \'example\' } completer = CustomCompleter(custom_ns) print(completer.complete(\\"ma\\", 0)) # Should return \\"math\\" print(completer.complete(\\"math.si\\", 1)) # Should return \\"sin\\" print(completer.complete(\\"variable.foo\\", 0)) # Should return None, as \'foo\' is not an attribute of \'str\' ``` **Constraints:** - The custom namespace will only contain valid Python identifiers and their corresponding objects. - The `text` provided will always be a string and can be empty. - The `state` will always be a non-negative integer. Your implementation should demonstrate an understanding of: - Namespace handling. - Safe attribute access and evaluation. - Proper use of the `rlcompleter` module and Readline library. **Performance Requirements:** - The `complete` method should return results efficiently for up to 1000 possible completions in the namespace. Good luck!","solution":"import rlcompleter import re class CustomCompleter: def __init__(self, namespace): self.namespace = namespace def complete(self, text, state): if \'.\' in text: try: obj, _, attr = text.rpartition(\'.\') obj = eval(obj, self.namespace) matches = [f\\"{text.rpartition(\'.\')[0]}.{a}\\" for a in dir(obj) if a.startswith(attr)] except Exception: return None else: matches = [k for k in self.namespace.keys() if k.startswith(text)] try: return matches[state] except IndexError: return None"},{"question":"Mathematical Expression Calculator You are required to implement a Python function that evaluates a mathematical expression based on specified rules. The expression will only involve non-negative integers and a subset of the mathematical functions provided by the Python math module. Function Signature ```python def evaluate_expression(operators, constants, expression): # Implementation here ``` Input 1. **operators**: A list of strings, each representing a valid math function (like \'sqrt\', \'pow\', etc.). You can choose any functions as required from the math module provided in the documentation. 2. **constants**: A dictionary where keys are constant names (like \'pi\', \'e\', etc.) and values are the corresponding float values. 3. **expression**: A string representing a mathematical expression using numbers, constants, and functions provided in the input lists. The expression will always be valid and properly formatted. Output - The function should return the calculated result of the expression as a float. Constraints - The expression will be a valid Python expression. - It will not contain any variables other than the constants provided in the input. - All operators/functions provided in the operators list should be supported. - You need to handle operations like division by zero or taking the log of non-positive numbers gracefully, by raising appropriate errors. Example ```python from math import sqrt, pi, exp, log # Sample Calculation operators = [\'sqrt\', \'pow\', \'log\'] constants = {\'pi\': pi, \'e\': 2.718281828459045} expression = \\"sqrt(16) + log(e, 10) + pi\\" result = evaluate_expression(operators, constants, expression) print(result) ``` **Expected Output:** ``` 8.144729885849392 ``` **Explanation:** - `sqrt(16)` returns `4.0`. - `log(e, 10)` computes the base-10 logarithm of `e`, which is ≈ `0.4342944819032518`. - `pi` is approximately `3.141592653589793`. - The expression evaluates to `4.0 + 0.4342944819032518 + 3.141592653589793 = 7.575887135382344`. Note: Ensure that the implementation is robust and includes all necessary error handling as described.","solution":"import math def evaluate_expression(operators, constants, expression): Evaluates a mathematical expression based on specified functions and constants. Parameters: operators (list): A list of strings, each representing valid math functions. constants (dict): A dictionary mapping constant names to their values. expression (str): A string representing the mathematical expression to evaluate. Returns: float: The result of the evaluated expression. # Create a local namespace dictionary with allowed math functions and constants local_namespace = {name: getattr(math, name) for name in operators} local_namespace.update(constants) try: # Evaluate the expression within the context of the local namespace result = eval(expression, {\\"__builtins__\\": None}, local_namespace) except Exception as e: # If an error occurs, raise a ValueError raise ValueError(f\\"Error evaluating expression: {e}\\") return float(result)"},{"question":"Objective: Write a function `serialize_and_deserialize_records` that takes a list of dictionaries representing student records, serializes them using the `struct` module, and then deserializes them back into a list of dictionaries. Each student record contains the following fields: - `name`: a string of up to 10 characters. - `age`: an integer. - `grade`: an integer (1-12). Function Signature: ```python def serialize_and_deserialize_records(records: list) -> list: pass ``` Input: - `records`: a list of dictionaries where each dictionary represents a student record with the keys `name`, `age`, and `grade`. Output: - A list of dictionaries representing the student records after serialization and deserialization. Constraints: 1. The `name` field in each record should be a string of up to 10 characters. If the name is shorter than 10 characters, it should be padded with null bytes (`\'x00\'`). 2. The `age` field should be an integer between 0 and 120. 3. The `grade` field should be an integer between 1 and 12. Example: ```python records = [ {\\"name\\": \\"Alice\\", \\"age\\": 14, \\"grade\\": 9}, {\\"name\\": \\"Bob\\", \\"age\\": 12, \\"grade\\": 7} ] output = serialize_and_deserialize_records(records) # Expected output: # [ # {\\"name\\": \\"Alice\\", \\"age\\": 14, \\"grade\\": 9}, # {\\"name\\": \\"Bob\\", \\"age\\": 12, \\"grade\\": 7} # ] ``` # Requirements: 1. Use the `struct` module to serialize and deserialize the records. 2. The format string should ensure that each record is packed efficiently with proper alignment and padding. Performance: - Make sure the function can handle a list of up to 1000 student records efficiently. Notes: - Pay attention to the alignment and padding while packing the records. - You may assume that the input records are well-formed and adhere to the specified constraints.","solution":"import struct def serialize_and_deserialize_records(records: list) -> list: serialized_data = b\'\' for record in records: name = record[\'name\'].ljust(10, \'0\') age = record[\'age\'] grade = record[\'grade\'] serialized_data += struct.pack(\'10sii\', name.encode(\'utf-8\'), age, grade) deserialized_records = [] record_size = struct.calcsize(\'10sii\') for i in range(0, len(serialized_data), record_size): name, age, grade = struct.unpack(\'10sii\', serialized_data[i:i+record_size]) deserialized_records.append({ \'name\': name.decode(\'utf-8\').rstrip(\'0\'), \'age\': age, \'grade\': grade }) return deserialized_records"},{"question":"# Python Exception Handling and Subclassing Objective: You are required to demonstrate your understanding of Python\'s exception handling mechanisms and your ability to extend existing exceptions by creating subclasses. Problem Statement: You are tasked with implementing a function that processes a list of integers and performs certain arithmetic operations. You should handle specific exceptions that might occur during the process, and create custom exceptions where necessary. 1. Implement a function `process_numbers(numbers: List[int]) -> float` that takes a list of integers and performs the following: - It computes the sum of the first element (numerator) and the second element (denominator), then performs the division to get the result. - It should handle the following exceptions: - If the list is empty or has less than two elements, raise a custom exception `InsufficientElementsError`. - If the second element (the denominator) is zero, raise a `ZeroDivisionError`. - If any other unexpected exception occurs, it should be logged with a custom message which includes the type of exception and the details. 2. Create a custom subclass `InsufficientElementsError` that inherits from `Exception`. This exception should take an additional custom attribute `elements_provided` which indicates how many elements were provided in the list. 3. Include appropriate unit tests that cover each of the following scenarios: - List with zero elements. - List with one element. - List where the second element is zero. - List that successfully returns a result. Function Signature: ```python from typing import List class InsufficientElementsError(Exception): def __init__(self, elements_provided: int): self.elements_provided = elements_provided self.message = f\\"Insufficient elements provided: {elements_provided}. At least 2 elements are required.\\" super().__init__(self.message) def process_numbers(numbers: List[int]) -> float: pass # Include this if you want to check your implementation def test_process_numbers(): pass if __name__ == \\"__main__\\": test_process_numbers() ``` Constraints: - You can assume that the input list, if not empty, contains integers. - You are only allowed to use built-in exceptions and the custom exceptions you create. - You should not use any external libraries except for `typing` for type hints. Example Usage: ```python try: result = process_numbers([10, 0]) except InsufficientElementsError as e: print(e) except ZeroDivisionError as e: print(\\"Cannot divide by zero.\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") try: result = process_numbers([10, 5]) print(result) # Expected output: 2.0 except Exception as e: print(f\\"An error occurred: {e}\\") ```","solution":"from typing import List class InsufficientElementsError(Exception): def __init__(self, elements_provided: int): self.elements_provided = elements_provided self.message = f\\"Insufficient elements provided: {elements_provided}. At least 2 elements are required.\\" super().__init__(self.message) def process_numbers(numbers: List[int]) -> float: try: if len(numbers) < 2: raise InsufficientElementsError(len(numbers)) numerator = numbers[0] denominator = numbers[1] return numerator / denominator except ZeroDivisionError as e: print(\\"Cannot divide by zero.\\") raise e except InsufficientElementsError: raise except Exception as e: print(f\\"An unexpected error occurred: {type(e).__name__}: {e}\\") raise e"},{"question":"# Question: Implement a Custom Readline-Based History Management You are required to implement a custom history management system using the `readline` module in Python. Your implementation should handle the following tasks: 1. **Initialize history**: On startup, load the history from a specified history file. 2. **Save history**: Before the program exits, save the current in-memory history to the specified history file. 3. **Append to history**: Provide a function to add a new line to the history. 4. **Retrieve history**: Provide a function to retrieve an entry from the history by its index. Requirements 1. You must use the `readline` module\'s provided functions to manage the history. 2. You should use a custom filename for the history file, not the default `~/.python_history`. 3. You should handle cases where the history file does not exist gracefully. Implementation Details 1. The history file should be called `custom_python_history`. 2. The maximum history length should be set to 1000 lines. 3. Four main functions should be implemented: - `initialize_history(histfile)`: This function should load the history from the specified file. - `save_history(histfile)`: This function should save the current history to the specified file. - `add_to_history(line)`: This function should add a new line to the history. - `get_history_entry(index)`: This function should return the history entry at the given index. Example ```python import atexit import os import readline histfile = os.path.join(os.path.expanduser(\\"~\\"), \\"custom_python_history\\") def initialize_history(histfile): try: readline.read_history_file(histfile) readline.set_history_length(1000) except FileNotFoundError: open(histfile, \'wb\').close() def save_history(histfile): readline.set_history_length(1000) readline.write_history_file(histfile) def add_to_history(line): readline.add_history(line) def get_history_entry(index): return readline.get_history_item(index) if __name__ == \\"__main__\\": initialize_history(histfile) atexit.register(save_history, histfile) # Example usage: add_to_history(\\"First line\\") add_to_history(\\"Second line\\") print(get_history_entry(1)) # Should print: \\"First line\\" print(get_history_entry(2)) # Should print: \\"Second line\\" ``` Constraints - Only use the `readline` module to implement the required functionalities. - You should ensure that the history management is robust and handles edge cases (e.g., no history file found, out-of-bound history index).","solution":"import os import readline import atexit histfile = os.path.join(os.path.expanduser(\\"~\\"), \\"custom_python_history\\") def initialize_history(histfile): Initialize the history by loading it from the specified file. If the file does not exist, it creates an empty history file. try: readline.read_history_file(histfile) readline.set_history_length(1000) except FileNotFoundError: open(histfile, \'wb\').close() def save_history(histfile): Save the current history to the specified file. readline.set_history_length(1000) readline.write_history_file(histfile) def add_to_history(line): Add a new line to the history. readline.add_history(line) def get_history_entry(index): Return the history entry at the given index, if it exists. return readline.get_history_item(index) if __name__ == \\"__main__\\": initialize_history(histfile) atexit.register(save_history, histfile) # Example usage: add_to_history(\\"First line\\") add_to_history(\\"Second line\\") print(get_history_entry(1)) # Should print: \\"First line\\" print(get_history_entry(2)) # Should print: \\"Second line\\""},{"question":"# Advanced Coding Assessment: Data Persistence with `pickle` and `sqlite3` Problem Statement You are required to write a Python script that demonstrates the use of the `pickle` module for serializing and deserializing Python objects and the `sqlite3` module for storing and retrieving the serialized data. Specifically, you need to implement two functions: 1. `save_data(data: dict, db_filename: str, table_name: str) -> None` 2. `load_data(db_filename: str, table_name: str) -> dict` Detailed Requirements 1. **Function: `save_data`** - **Input**: - `data` (dict): A dictionary of data to be serialized and stored in the database. - `db_filename` (str): The name of the SQLite database file. - `table_name` (str): The name of the table where the data will be stored. - **Process**: - Serialize the dictionary using the `pickle` module. - Store the serialized data into a specified table within the SQLite database using the `sqlite3` module. - **Output**: - None 2. **Function: `load_data`** - **Input**: - `db_filename` (str): The name of the SQLite database file. - `table_name` (str): The name of the table from which the data will be retrieved. - **Process**: - Retrieve the serialized data from the specified table within the SQLite database using the `sqlite3` module. - Deserialize the data using the `pickle` module to obtain the original dictionary. - **Output**: - Returns the deserialized dictionary. Constraints - The `table_name` should exist in the database, and it should have a column named `data` to store the serialized data. - Handle any exceptions that might occur during the database operations or serialization/deserialization process, and ensure the integrity of the data and database. Example Usage ```python data = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"city\\": \\"Wonderland\\" } db_filename = \\"data.db\\" table_name = \\"user_data\\" # Save the data to the SQLite database save_data(data, db_filename, table_name) # Load the data from the SQLite database loaded_data = load_data(db_filename, table_name) assert loaded_data == data print(\\"Data loaded successfully:\\", loaded_data) ``` Performance Requirements - The implementation should be efficient in terms of time and space complexity. - The `sqlite3` operations should be optimized for minimum overhead. # Notes - Consider edge cases such as empty dictionaries, large data, and database connectivity issues. - Use Python 3.10+ for implementation.","solution":"import sqlite3 import pickle def save_data(data: dict, db_filename: str, table_name: str) -> None: Serializes the dictionary using pickle and stores it into a specified table within the SQLite database. :param data: A dictionary of data to be serialized and stored. :param db_filename: The name of the SQLite database file. :param table_name: The name of the table where the data will be stored. # Serialize the data serialized_data = pickle.dumps(data) # Connect to the database conn = sqlite3.connect(db_filename) cursor = conn.cursor() # Create table if it doesn\'t exist cursor.execute(f CREATE TABLE IF NOT EXISTS {table_name} ( id INTEGER PRIMARY KEY AUTOINCREMENT, data BLOB NOT NULL ) ) # Insert the serialized data into the table cursor.execute(f\\"INSERT INTO {table_name} (data) VALUES (?)\\", (serialized_data,)) # Commit the transaction and close the connection conn.commit() conn.close() def load_data(db_filename: str, table_name: str) -> dict: Retrieves the serialized data from the specified table in the SQLite database and deserializes it. :param db_filename: The name of the SQLite database file. :param table_name: The name of the table from which the data will be retrieved. :return: The deserialized dictionary. # Connect to the database conn = sqlite3.connect(db_filename) cursor = conn.cursor() # Retrieve the serialized data from the table cursor.execute(f\\"SELECT data FROM {table_name} ORDER BY id DESC LIMIT 1\\") row = cursor.fetchone() # Deserialize the data if it exists if row: serialized_data = row[0] data = pickle.loads(serialized_data) else: data = {} # Close the connection conn.close() return data"},{"question":"# Sparse Data Structures in pandas Objective Design a function `process_sparse_data` that demonstrates competence in handling sparse data structures in pandas. The function should effectively convert dense matrices to sparse formats, perform operations, and efficiently convert back to dense format while reducing memory usage. Function Signature ```python def process_sparse_data(dense_matrix: np.ndarray, fill_value: float) -> Tuple[pd.DataFrame, pd.DataFrame, float, float]: pass ``` Input - `dense_matrix` (np.ndarray): An N x M numpy array with some `NaN` values. - `fill_value` (float): A value to be used as the fill value for the sparse array conversion. Output - Returns a tuple containing: - `sparse_df`: A pandas DataFrame in sparse format from the given dense matrix. - `restored_df`: The dense pandas DataFrame converted back from the sparse DataFrame. - `original_memory_usage`: Memory usage of the original dense DataFrame (in bytes). - `sparse_memory_usage`: Memory usage of the sparse DataFrame (in bytes). Constraints - Usage of pandas SparseArray, SparseDtype, and their related functionality is required. - Ensure efficiency in memory usage for sparse structures compared to dense structures. - Handle different shapes and sizes of the input dense matrix. Example ```python import numpy as np import pandas as pd dense_matrix = np.random.randn(10, 5) dense_matrix[2:8, 1:4] = np.nan sparse_df, restored_df, original_memory_usage, sparse_memory_usage = process_sparse_data(dense_matrix, np.nan) print(sparse_df) print(restored_df) print(f\\"Original Memory Usage: {original_memory_usage} bytes\\") print(f\\"Sparse Memory Usage: {sparse_memory_usage} bytes\\") ``` # Notes - The memory usage outputs should clearly demonstrate the difference in memory consumption between sparse and dense data structures. - Ensure all necessary imports, including `numpy` and `pandas`, are made within the function or in the example provided.","solution":"import numpy as np import pandas as pd from typing import Tuple def process_sparse_data(dense_matrix: np.ndarray, fill_value: float) -> Tuple[pd.DataFrame, pd.DataFrame, float, float]: Converts a dense numpy matrix with NaNs to a sparse pandas DataFrame, then converts it back to dense, and calculates memory usage for both representations. Parameters: dense_matrix (np.ndarray): An N x M numpy array with some NaN values. fill_value (float): A value to be used as the fill value for the sparse array conversion. Returns: tuple: containing - sparse_df: The sparse representation of the input dense matrix. - restored_df: The dense DataFrame converted back from the sparse representation. - original_memory_usage: Memory usage of the original dense DataFrame (in bytes). - sparse_memory_usage: Memory usage of the sparse DataFrame (in bytes). # Convert the dense matrix to a pandas DataFrame dense_df = pd.DataFrame(dense_matrix) # Calculate memory usage of the original dense DataFrame original_memory_usage = dense_df.memory_usage(deep=True).sum() # Convert the dense DataFrame to a sparse DataFrame sparse_df = dense_df.astype(pd.SparseDtype(\\"float\\", fill_value)) # Calculate memory usage of the sparse DataFrame sparse_memory_usage = sparse_df.memory_usage(deep=True).sum() # Convert the sparse DataFrame back to dense format restored_df = sparse_df.sparse.to_dense() return sparse_df, restored_df, original_memory_usage, sparse_memory_usage"},{"question":"Coding Assessment Question # Objective Implement a function using PyTorch named tensors that involves creating, manipulating, and aligning tensors, and performing a complex set of operations. # Function Signature ```python def tensor_manipulation_operations(*inputs) -> dict: ``` # Description 1. This function takes several tensor inputs (minimum 3) and performs the following operations: 2. **Input Constraints**: - Each input tensor will have named dimensions. - Tensors can vary in size and named dimensions but will share at least one common named dimension across all inputs. 3. **Steps**: - **Identify Common Dimensions**: Find out the shared named dimension(s) across all input tensors. - **Reorder Dimensions by Name**: Use `align_to()` or similar methods to reorder each tensor so that the common dimension(s) are first. - **Flatten Dimensions**: Flatten all non-common dimensions of each tensor into a single dimension named \'flattened\'. - **Combine Tensors**: Combine the tensors by concatenating them along the common dimension. - **Sum Along Specific Axis**: Sum the resulting tensor along one of the specified named dimensions. - **Mean of Sums**: Calculate the mean of the sum for each tensor and store the results in a dictionary mapping from tensor name to their mean sum. # Returns - A dictionary where: - The keys are the tensor indices (index-based or a unique identifier if provided in the input). - The values are the mean of the sum of the combined tensor operations described above. # Example ```python import torch # Example input tensors tensor1 = torch.randn(2, 3, 4, names=(\'A\', \'B\', \'C\')) tensor2 = torch.randn(2, 5, 4, names=(\'A\', \'D\', \'C\')) tensor3 = torch.randn(2, 5, names=(\'A\', \'E\')) # Sample function call result = tensor_manipulation_operations(tensor1, tensor2, tensor3) # The result will look like: # { # 0: mean_value_1, # 1: mean_value_2, # 2: mean_value_3 # } ``` # Notes - Ensure to handle potential errors or mismatches in tensor dimensions correctly by raising appropriate exceptions. - The solution should demonstrate a thorough understanding of named tensor operations, including alignment, flattening, and reducing dimensions while maintaining named dimension semantics.","solution":"import torch def tensor_manipulation_operations(*inputs) -> dict: Perform specified tensor operations on the provided input tensors. Parameters: inputs (torch.Tensor): The input tensors with named dimensions. Returns: dict: Contains the mean of the sum of the tensors along their common dimension. # Find common dimensions common_dims = set(inputs[0].names) for tensor in inputs[1:]: common_dims.intersection_update(set(tensor.names)) if not common_dims: raise ValueError(\\"No common dimensions found among input tensors.\\") # Select the first common dimension to operate on common_dim = next(iter(common_dims)) results = {} for idx, tensor in enumerate(inputs): # Reorder dimensions to move the common dimension to the front tensor = tensor.align_to(common_dim, ...) # Flatten all non-common dimensions non_common_dims = [name for name in tensor.names if name != common_dim] if non_common_dims: tensor = tensor.flatten(non_common_dims, \'flattened\') # Sum the tensor along the common dimension tensor_sum = tensor.sum(dim=tensor.names.index(common_dim)) # Calculate mean of the sum mean_sum = tensor_sum.mean().item() results[idx] = mean_sum return results"},{"question":"Coding Assessment Question: Using `torch.cond` for Dynamic Control Flow in Neural Networks # Objective Demonstrate your understanding and capability to implement and use the `torch.cond` function in PyTorch for dynamic control flow based on the shape and value of input tensors. # Problem Statement You are tasked with implementing a neural network module that dynamically selects between two different operations based on the shape and sum of input tensors. The module should behave as follows: - If the shape of the input tensor along the first dimension is greater than 5, then: - If the sum of the input tensor is greater than 10, apply the cosine of the tensor. - Otherwise, apply the sine of the tensor. - Otherwise, apply the hyperbolic tangent of the tensor. # Function Signature ```python import torch class DynamicCondNet(torch.nn.Module): def __init__(self): super(DynamicCondNet, self).__init__() def true_fn_1(self, x: torch.Tensor) -> torch.Tensor: return x.cos() def false_fn_1(self, x: torch.Tensor) -> torch.Tensor: return x.sin() def true_fn_2(self, x: torch.Tensor) -> torch.Tensor: return torch.tanh(x) def forward(self, x: torch.Tensor) -> torch.Tensor: def inner_cond(x: torch.Tensor) -> torch.Tensor: return torch.cond(x.sum() > 10, self.true_fn_1, self.false_fn_1, (x,)) return torch.cond(x.shape[0] > 5, inner_cond, self.true_fn_2, (x,)) ``` # Constraints - You may assume `x` is always a one-dimensional tensor. - The implementation should correctly utilize `torch.cond` to maintain dynamic control flow based on both shape and sum of the input tensor. - The module should be able to handle tensors of varying shapes and values as inputs. # Example ```python model = DynamicCondNet() # Test with shape less than or equal to 5 inp1 = torch.tensor([1, 2, 3, 4]) assert torch.allclose(model(inp1), torch.tanh(inp1)) # Test with shape greater than 5 and sum greater than 10 inp2 = torch.tensor([2, 2, 2, 2, 2, 2]) assert torch.allclose(model(inp2), inp2.cos()) # Test with shape greater than 5 and sum less than or equal to 10 inp3 = torch.tensor([1, 1, 1, 1, 1, 1]) assert torch.allclose(model(inp3), inp3.sin()) ``` # Evaluation Criteria - Correct usage of `torch.cond` for dynamic control flow. - Proper implementation of true and false branches based on shape and sum conditions. - Validity of the module outputs based on different input scenarios.","solution":"import torch class DynamicCondNet(torch.nn.Module): def __init__(self): super(DynamicCondNet, self).__init__() def true_fn_1(self, x: torch.Tensor) -> torch.Tensor: return x.cos() def false_fn_1(self, x: torch.Tensor) -> torch.Tensor: return x.sin() def true_fn_2(self, x: torch.Tensor) -> torch.Tensor: return torch.tanh(x) def forward(self, x: torch.Tensor) -> torch.Tensor: if x.shape[0] > 5: if x.sum().item() > 10: return self.true_fn_1(x) else: return self.false_fn_1(x) else: return self.true_fn_2(x)"},{"question":"**Coding Assessment Question: Formatted Output and File Handling** # Objective Write a Python function that reads data from a text file, processes this data by performing some calculations, formats the processed data neatly, and writes the formatted output to a new text file. # Problem Statement You are given a text file named \\"input.txt\\". Each line in the file contains a student\'s name followed by their test scores in different subjects, separated by a space. The first score represents Math, the second represents Science, and the third represents English. Example of file content: ``` Alice 89 92 85 Bob 78 81 79 Charlie 90 88 91 ``` You need to perform the following tasks: 1. Write a function `process_scores(input_file: str, output_file: str) -> None` that: - Reads the data from the given `input_file`. - Calculates the average score for each student. - Formats each student\'s data into a neatly aligned string with their name, scores, and average score. - Writes the formatted output to the given `output_file`. # Input - `input_file`: A string representing the path to the input text file. # Output - `output_file`: A string representing the path to the output text file to which the formatted data should be written. # Constraints - Each line in the input file will have exactly one student\'s name followed by three integers representing their test scores. - The names and scores will be properly formatted and will need no validation. # Example Suppose the input file `input.txt` contains the following lines: ``` Alice 89 92 85 Bob 78 81 79 Charlie 90 88 91 ``` After processing, the output file `output.txt` should contain: ``` Name Math Science English Average Alice 89 92 85 88.67 Bob 78 81 79 79.33 Charlie 90 88 91 89.67 ``` # Function Signature ```python def process_scores(input_file: str, output_file: str) -> None: pass ``` # Additional Notes - Use formatted string literals (f-strings) or the `str.format()` method for output formatting. - Ensure the columns are properly aligned. - Use the `with` statement for reading and writing files. # Hints - The average score should be displayed up to two decimal places. - Consider using `str.ljust()` or other string manipulation methods to achieve alignment.","solution":"def process_scores(input_file: str, output_file: str) -> None: with open(input_file, \'r\') as infile: lines = infile.readlines() formatted_lines = [\\"Name Math Science English Average\\"] for line in lines: parts = line.split() name = parts[0] scores = list(map(int, parts[1:])) average = sum(scores) / len(scores) formatted_line = f\\"{name.ljust(10)}{scores[0]:<6}{scores[1]:<8}{scores[2]:<8}{average:.2f}\\" formatted_lines.append(formatted_line) with open(output_file, \'w\') as outfile: for formatted_line in formatted_lines: outfile.write(formatted_line + \'n\')"},{"question":"# Custom Transformer Implementation **Objective**: Create a custom transformer that normalizes the data such that each feature column has a mean of zero and a standard deviation of one, making it compatible with scikit-learn’s estimator API. This transformer should be capable of working within scikit-learn pipelines and follow all the requirements outlined in the provided documentation. **Task**: You need to implement a custom transformer named `ZScoreNormalizer` which performs Z-score normalization. This transformer should work with scikit-learn’s model selection tools and pipelines. Specifically, it should: 1. Inherit from `BaseEstimator` and `TransformerMixin`. 2. Implement the `fit` and `transform` methods. 3. Store the mean and standard deviation of the training data during the `fit` method using attributes ending with an underscore. 4. Handle the case where `fit` is called multiple times, resetting its parameters each time. 5. Pass all of scikit-learn’s standard checks for estimators. **Specifications**: 1. **Input**: - `X`: array-like of shape (n_samples, n_features) - `y`: Ignored (only included for compatibility with pipelines) 2. **Output**: - The transformed array where each feature column has zero mean and unit variance. **Hints**: - Use `sklearn.utils.validation.check_array` to validate incoming arrays. - Use `np.mean` and `np.std` for calculating the mean and standard deviation. - Ensure attributes `mean_` and `std_` are created during fitting and used during transformation. - Use `check_is_fitted` in the transform method to validate that the transformer was fitted. # Example ```python import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted class ZScoreNormalizer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): X = check_array(X) self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) return self def transform(self, X): check_is_fitted(self, [\'mean_\', \'std_\']) X = check_array(X) return (X - self.mean_) / self.std_ # Example Usage normalizer = ZScoreNormalizer() X = np.array([[1, 2], [3, 4], [5, 6]]) normalizer.fit(X) X_transformed = normalizer.transform(X) print(X_transformed) ``` **Testing**: Verify your implementation using the `check_estimator` function from `sklearn.utils.estimator_checks` to ensure it passes all standard estimator checks. ```python from sklearn.utils.estimator_checks import check_estimator check_estimator(ZScoreNormalizer()) ```","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted class ZScoreNormalizer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): X = check_array(X) self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) # To avoid division by zero during transformation self.std_[self.std_ == 0] = 1 return self def transform(self, X): check_is_fitted(self, [\'mean_\', \'std_\']) X = check_array(X) return (X - self.mean_) / self.std_"},{"question":"**Challenge Level:** Advanced # Problem Statement You are to demonstrate your understanding of the `copyreg` module by performing the following tasks: 1. Define a class `Rectangle` that has attributes `width` and `height`. 2. Implement a method within this class called `area` that calculates and returns the area of the rectangle. 3. Define a \\"reduction\\" function `pickle_rectangle` that will be used to pickle instances of `Rectangle`. 4. Register the `pickle_rectangle` function using `copyreg.pickle`. 5. Write additional code to: - Create an instance of `Rectangle`. - Copy the instance using the `copy` module. - Pickle and unpickle the instance using the `pickle` module. 6. Display the outputs during each step to show that the pickling and copying processes are working properly. # Requirements - Your `Rectangle` class must include an appropriate `__init__` method to initialize its attributes. - The `area` method must calculate and return the area of the rectangle. - The `pickle_rectangle` function must manage the pickling of the `Rectangle` instances properly. - Use the `copyreg.pickle` function to register `pickle_rectangle` for the `Rectangle` class. - Ensure the whole process (copying and pickling/unpickling) maintains the integrity of the `Rectangle` object. # Input and Output - There is no specific input. The task is more about verifying the functionality of the pickle and copy processes. - Output should include: - The width, height, and area of the original, copied, and unpickled `Rectangle` instances. - Any print statements within the `pickle_rectangle` function to show it has been called. # Constraints - Ensure proper error handling where necessary, especially for invalid inputs or operations. - Maintain the order of operations as specified to ensure clarity of the demonstration process. # Example ```python import copyreg, copy, pickle class Rectangle: def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def pickle_rectangle(rectangle): print(\\"pickling a Rectangle instance...\\") return Rectangle, (rectangle.width, rectangle.height) # Register the Rectangle class with the pickle_rectangle function copyreg.pickle(Rectangle, pickle_rectangle) # Create an instance of Rectangle rect = Rectangle(3, 4) print(f\\"Original Rectangle: width={rect.width}, height={rect.height}, area={rect.area()}\\") # Copy the instance rect_copy = copy.copy(rect) print(f\\"Copied Rectangle: width={rect_copy.width}, height={rect_copy.height}, area={rect_copy.area()}\\") # Pickle the instance rect_pickled = pickle.dumps(rect) # Unpickle the instance rect_unpickled = pickle.loads(rect_pickled) print(f\\"Unpickled Rectangle: width={rect_unpickled.width}, height={rect_unpickled.height}, area={rect_unpickled.area()}\\") ``` The output should show the dimensions and area of the original, copied, and unpickled `Rectangle` instances, as well as any messages indicating the pickling process.","solution":"import copyreg, copy, pickle class Rectangle: def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def pickle_rectangle(rectangle): print(\\"pickling a Rectangle instance...\\") return Rectangle, (rectangle.width, rectangle.height) # Register the Rectangle class with the pickle_rectangle function copyreg.pickle(Rectangle, pickle_rectangle) # Create an instance of Rectangle rect = Rectangle(3, 4) print(f\\"Original Rectangle: width={rect.width}, height={rect.height}, area={rect.area()}\\") # Copy the instance rect_copy = copy.copy(rect) print(f\\"Copied Rectangle: width={rect_copy.width}, height={rect_copy.height}, area={rect_copy.area()}\\") # Pickle the instance rect_pickled = pickle.dumps(rect) # Unpickle the instance rect_unpickled = pickle.loads(rect_pickled) print(f\\"Unpickled Rectangle: width={rect_unpickled.width}, height={rect_unpickled.height}, area={rect_unpickled.area()}\\")"},{"question":"**Title: Implementing a Path Converter** You are tasked with implementing a function in Python that mimics the behavior of the `PyOS_FSPath` function provided in the Python 3.10 C API. This function should return the filesystem representation for a given path object. # Requirements 1. The function should be named `get_fs_path`. 2. The function should accept a single parameter: - `path`: an object that could be a string, bytes, or an object implementing the `os.PathLike` interface. 3. Return a string if the input is a path-like object, string, or bytes object. 4. Raise a `TypeError` if the object does not match the expectations. 5. For objects implementing the `os.PathLike` interface, use its `__fspath__` method to retrieve the filesystem representation. # Input and Output Formats Input - An object `path` that can be a string, bytes, or an object implementing the `os.PathLike` interface. Output - A string representing the filesystem path. Constraints - The `path` parameter can be of type string, bytes, or an object with the `__fspath__` method. - The function must raise a `TypeError` if the input type does not meet the criteria. - Your implementation should handle typical usage within reasonable resource limits. # Function Signature ```python def get_fs_path(path: any) -> str: pass ``` # Example ```python class CustomPath: def __fspath__(self): return \\"/custom/path\\" # Example 1 path = \\"/home/user/documents\\" print(get_fs_path(path)) # Output: \\"/home/user/documents\\" # Example 2 path = b\\"/home/user/documents\\" print(get_fs_path(path)) # Output: \\"/home/user/documents\\" # Example 3 path = CustomPath() print(get_fs_path(path)) # Output: \\"/custom/path\\" # Example 4 path = 12345 try: print(get_fs_path(path)) except TypeError as e: print(e) # Output: \\"TypeError: Expected str, bytes, or os.PathLike object, not int\\" ``` # Notes - Ensure proper error handling and raise `TypeError` with appropriate messages for invalid input types. - Utilize the `os` module to check for objects implementing the `os.PathLike` interface.","solution":"import os def get_fs_path(path): Returns the filesystem path representation of path. if isinstance(path, (str, bytes)): # If path is of type string or bytes, convert bytes to string and return return path.decode() if isinstance(path, bytes) else path elif isinstance(path, os.PathLike): # If path is an object implementing os.PathLike, use its __fspath__ method return path.__fspath__() else: # Raise TypeError if path is of an unexpected type raise TypeError(f\\"Expected str, bytes, or os.PathLike object, not {type(path).__name__}\\")"},{"question":"<|Analysis Begin|> The provided documentation thoroughly explains the `email.message.EmailMessage` class from Python 3.6 onwards. This class is essential for working with email messages in Python and offers various functionalities such as setting/querying header fields, accessing message bodies, and managing structured sequences of sub-messages. Key points and functionalities that stand out include: 1. **Headers and Payload**: Headers store field names and values in RFC 5322 or RFC 6532 format, whereas payloads can be simple text, binary objects, or a structured sequence of sub-messages. 2. **Dictionary-like Interface**: The `EmailMessage` object behaves like an ordered dictionary for headers, with additional functionality to handle MIME types and manage payloads. 3. **Serialization**: Methods like `as_string()` and `as_bytes()` are provided to serialize the message for transmission or storage. 4. **MIME Handling**: Functions such as `is_multipart()`, `set_boundary()`, and `get_body()` allow for intricate management of MIME message parts and content types. 5. **Header and Payload Management**: Methods to add, replace, and delete headers (`add_header()`, `replace_header()`, `del_param()`) along with payload manipulation methods (`walk()`, `iter_attachments()`) offer extensive control over the message structure. Given these features, the questions should focus on: - Creating and manipulating email messages. - Handling headers and payloads (including MIME parts). - Serialization and deserialization of messages. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Create and manipulate an email message using Python\'s `email.message.EmailMessage` class. Demonstrate your understanding of managing headers, handling different payload types, and serializing the message. Problem Statement You are to implement a function `create_email_message(headers, body_content, attachments)` that constructs an email message. The function must: 1. Add specified headers to the email. 2. Set the email body with the given content. 3. Attach any provided attachments to the email. 4. Serialize the email message to a string with valid format. Function Signature ```python def create_email_message(headers: dict, body_content: str, attachments: list) -> str: pass ``` Input 1. `headers`: A dictionary where keys are header names and values are header values (e.g., `{\'Subject\': \'Test\', \'From\': \'test@example.com\'}`). 2. `body_content`: A string representing the main body content of the email. 3. `attachments`: A list of tuples, where each tuple contains: - filename (str): Name of the file to be attached. - file_content (str): The actual content of the file as a string. Output - A string representation of the complete email message, including headers, body, and attachments (if any). Constraints - The `headers` dictionary will contain at least the `Subject` and `From` fields. - The `body_content` will be a non-empty string. - The `attachments` list can be empty. Example ```python headers = { \'Subject\': \'Meeting Invitation\', \'From\': \'organizer@example.com\', \'To\': \'attendee@example.com\' } body_content = \\"Please join the meeting tomorrow at 10 AM.\\" attachments = [ (\'agenda.txt\', \'10:00 AM - Introductionn10:15 AM - Project Updatesn\'), (\'location.txt\', \'Conference room 1Bn\') ] result = create_email_message(headers, body_content, attachments) print(result) ``` # Expected Output (formatted for clarity): ```plain Subject: Meeting Invitation From: organizer@example.com To: attendee@example.com MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"==boundary==\\" --==boundary== Content-Type: text/plain; charset=\\"utf-8\\" Please join the meeting tomorrow at 10 AM. --==boundary== Content-Disposition: attachment; filename=\\"agenda.txt\\" Content-Type: text/plain; charset=\\"utf-8\\" 10:00 AM - Introduction 10:15 AM - Project Updates --==boundary== Content-Disposition: attachment; filename=\\"location.txt\\" Content-Type: text/plain; charset=\\"utf-8\\" Conference room 1B --==boundary==-- ``` # Note Ensure that the email message adheres to MIME standards, with appropriate headers for different parts, and the boundary identifiers are correctly set and used.","solution":"from email.message import EmailMessage from email.policy import default def create_email_message(headers: dict, body_content: str, attachments: list) -> str: # Create the email message object msg = EmailMessage() # Set the headers for key, value in headers.items(): msg[key] = value # Set the body content msg.set_content(body_content) # Add attachments if any for filename, file_content in attachments: msg.add_attachment(file_content, filename=filename) # Serialize the message to string return msg.as_string(policy=default)"},{"question":"**Question:** You are given access to a Unix system\'s password database through the `pwd` module in Python. Your task is to write functions that perform specific queries and manipulations on the password database. **Function 1: `get_usernames_by_uid_range(uid_start, uid_end)`** This function should return a list of user names whose user IDs (UIDs) fall within the given range `[uid_start, uid_end]`, inclusive. The returned list should be sorted in ascending order by UID. **Input:** - `uid_start` (int): The starting UID of the range. - `uid_end` (int): The ending UID of the range. **Output:** - List of strings: User names whose UIDs fall within the specified range, sorted in ascending order by UID. **Function 2: `filter_users_by_shell(shell)`** This function should return a list of user names who use a specific shell. The returned list should be sorted alphabetically by user name. **Input:** - `shell` (str): The desired command interpreter (shell). **Output:** - List of strings: User names who use the specified shell, sorted alphabetically. **Function 3: `get_home_directories_containing_keyword(keyword)`** This function should return a list of user home directories that contain a specific keyword in the directory path. The returned list should be sorted lexicographically. **Input:** - `keyword` (str): The keyword to search for within home directories. **Output:** - List of strings: User home directories that contain the keyword, sorted lexicographically. **Constraints:** - You may assume that the Unix system has a valid password database, and the `pwd` module is available. - You should handle cases where no users match the criteria by returning an empty list. **Performance Requirements:** - Your solution should be efficient and make use of the functionalities provided by the `pwd` module effectively. Here is the sample implementation template to get you started: ```python import pwd def get_usernames_by_uid_range(uid_start, uid_end): # Implement this function pass def filter_users_by_shell(shell): # Implement this function pass def get_home_directories_containing_keyword(keyword): # Implement this function pass ``` **Example Usage:** ```python # Example 1: # Assume the system has users with UIDs: 1001, 1002, 1003, 1004 print(get_usernames_by_uid_range(1001, 1003)) # Might output: [\'user1\', \'user2\', \'user3\'] # Example 2: # Assume \'bin/bash\' is used by \'alice\' and \'bob\' print(filter_users_by_shell(\'/bin/bash\')) # Might output: [\'alice\', \'bob\'] # Example 3: # Assume home directories: \'/home/alice\', \'/home/bob\', \'/mnt/alice\' print(get_home_directories_containing_keyword(\'alice\')) # Might output: [\'/home/alice\', \'/mnt/alice\'] ``` Make sure to test your functions thoroughly. Good luck!","solution":"import pwd def get_usernames_by_uid_range(uid_start, uid_end): Returns a list of user names whose UIDs fall within the range [uid_start, uid_end], inclusive, sorted by UID. all_users = pwd.getpwall() filtered_users = [user.pw_name for user in all_users if uid_start <= user.pw_uid <= uid_end] return sorted(filtered_users, key=lambda username: pwd.getpwnam(username).pw_uid) def filter_users_by_shell(shell): Returns a list of user names who use the specified shell, sorted alphabetically by user name. all_users = pwd.getpwall() filtered_users = [user.pw_name for user in all_users if user.pw_shell == shell] return sorted(filtered_users) def get_home_directories_containing_keyword(keyword): Returns a list of user home directories that contain the specified keyword, sorted lexicographically. all_users = pwd.getpwall() filtered_home_dirs = [user.pw_dir for user in all_users if keyword in user.pw_dir] return sorted(filtered_home_dirs)"},{"question":"# **Coding Assessment Question** **Problem Statement: Mixed Data Processor** You are required to implement a function called `process_mixed_data(data)`. This function takes a list of mixed data types and performs the following tasks: 1. Iterates through each element of the list. 2. If the element is an integer or float, it adds it to a running sum. 3. If the element is a string, it converts the string to an integer and adds it to the running sum. 4. If the element is a list, it attempts to add the sum of the list to the running sum. 5. If any other data type is encountered, raise a `TypeError`. 6. Properly handle any exceptions that may arise during these operations Additionally, after processing each element, the function should append the current running sum to a results list. The function must return this results list. In case of an exception: 1. Catch and handle each specific exception type (e.g., `ValueError`, `TypeError`, etc.), printing an appropriate message. 2. Continue to the next element after handling an exception. 3. Ensure that any necessary clean-up operations are performed at the end of the function execution. **Function Signature** ```python def process_mixed_data(data: list) -> list: pass ``` **Input** - A list `data` containing integers, floats, strings, lists, and potentially other types. **Output** - A list containing the running sum after processing each element of the input list. **Constraints** - Each element in the input list can be of any data type. - The function should handle exceptions gracefully and continue processing the remaining elements. **Example** ```python print(process_mixed_data([1, \\"2\\", [3, 4], \\"a\\", 5.5, {}, 7])) # Expected output: [1, 3, 10, 10, 15.5, 15.5, 22.5] ``` For the input above, the function processes each element as follows: 1. 1 -> Add to sum, running sum is 1 2. \\"2\\" -> Convert to integer and add to sum, running sum is 3 3. [3, 4] -> Sum list elements and add to sum, running sum is 10 4. \\"a\\" -> Cannot convert to integer, handle `ValueError` and continue, running sum is 10 5. 5.5 -> Add to sum, running sum is 15.5 6. {} -> Raise `TypeError`, handle it, and continue, running sum is 15.5 7. 7 -> Add to sum, running sum is 22.5 **Notes** - Use `try`, `except`, `else`, and `finally` blocks appropriately. - Demonstrate proper exception handling and clean-up actions.","solution":"def process_mixed_data(data): results = [] running_sum = 0 for item in data: try: if isinstance(item, (int, float)): running_sum += item elif isinstance(item, str): try: running_sum += int(item) except ValueError: print(f\\"ValueError: Cannot convert string \'{item}\' to integer\\") elif isinstance(item, list): running_sum += sum(item) else: raise TypeError(f\\"Unsupported data type found: {type(item).__name__}\\") except (TypeError, ValueError) as e: print(f\\"Exception encountered: {e}\\") finally: results.append(running_sum) return results"},{"question":"**Question: Working with the \\"marshal\\" Module for Serialization and Deserialization** You are required to implement two functions, `serialize_data` and `deserialize_data`, that respectively serialize Python objects to binary format and deserialize from binary format using the \\"marshal\\" module. # Function 1: `serialize_data` This function should accept a single parameter `data` and serialize it using the `marshal.dumps` function. It should return the serialized bytes object. **Input:** - `data` (a Python object): The data to be serialized. It can be of a supported type (integer, float, list, tuple, dictionary, etc.). **Output:** - Returns a bytes object representing the serialized data. **Constraints:** - Raise a `ValueError` if the `data` contains any unsupported type. # Function 2: `deserialize_data` This function should accept a single parameter `serialized_data` (bytes object) and deserialize it using the `marshal.loads` function. It should return the original Python object. **Input:** - `serialized_data` (bytes): A bytes object containing the serialized data. **Output:** - Returns the original Python object represented by the `serialized_data`. **Constraints:** - If the `serialized_data` is not in a valid format, the function should handle and raise an appropriate exception like `EOFError`, `ValueError`, or `TypeError`. # Example Usage: ```python import marshal def serialize_data(data): try: serialized_data = marshal.dumps(data) return serialized_data except ValueError as err: raise ValueError(f\\"Unsupported data type: {err}\\") def deserialize_data(serialized_data): try: data = marshal.loads(serialized_data) return data except (EOFError, ValueError, TypeError) as err: raise err # Example data = {\'a\': [1, 2, 3], \'b\': 4.5} serialized_data = serialize_data(data) print(serialized_data) # Outputs the serialized bytes deserialized_data = deserialize_data(serialized_data) print(deserialized_data) # Outputs: {\'a\': [1, 2, 3], \'b\': 4.5} ``` # Notes: 1. Ensure your code handles all types mentioned in the documentation. 2. The functions should handle errors gracefully and provide meaningful error messages for unsupported data types or invalid serialized data. 3. Make sure to test the functions with various data types including nested structures.","solution":"import marshal def serialize_data(data): Serialize the data using marshal.dumps. Parameters: data (any supported Python object): The data to be serialized. Returns: bytes: The serialized bytes object. Raises: ValueError: If the data contains any unsupported type. try: serialized_data = marshal.dumps(data) return serialized_data except ValueError as err: raise ValueError(f\\"Unsupported data type: {err}\\") def deserialize_data(serialized_data): Deserialize the data from the provided bytes object using marshal.loads. Parameters: serialized_data (bytes): A bytes object containing the serialized data. Returns: any: The original Python object represented by the serialized_data. Raises: EOFError, ValueError, TypeError: If the serialized_data is not in valid format. try: data = marshal.loads(serialized_data) return data except (EOFError, ValueError, TypeError) as err: raise err"},{"question":"Objective Design a function that loads a specified dataset from the openml.org repository, preprocesses the data, trains a logistic regression model, and evaluates its performance. Task Write a function: ```python def load_and_evaluate_openml_dataset(data_id: int) -> float: Loads a dataset from OpenML by data_id, preprocesses it, trains a logistic regression model, and evaluates its accuracy. Parameters: data_id (int): The dataset ID from OpenML. Returns: float: The accuracy of the trained logistic regression model on the test set. ``` Steps 1. **Load the Dataset**: - Use `fetch_openml` to load the dataset from openml.org using the specified `data_id`. 2. **Preprocess the Data**: - If the dataset features contain any non-numerical columns, convert them to numerical form using appropriate encoding (e.g., `OneHotEncoder` or `OrdinalEncoder`). - Split the data into training and testing sets (80% train, 20% test). 3. **Train the Model**: - Train a logistic regression model on the training set. 4. **Evaluate the Model**: - Evaluate the model on the test set and return the accuracy. Constraints: - You may assume that the dataset has at least one categorical feature and one numerical feature. - You may assume that the target column is called `class`. - You should use scikit-learn\'s `LogisticRegression` for training the model. - Implement any necessary preprocessing steps to prepare the data for training. - Ensure the accuracy is computed correctly to evaluate model performance. Example ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer import numpy as np # Example function implementation def load_and_evaluate_openml_dataset(data_id: int) -> float: # Load the dataset dataset = fetch_openml(data_id=data_id) X, y = dataset.data, dataset.target # Identify categorical and numerical columns categorical_columns = X.select_dtypes(include=[\'category\', \'object\']).columns numerical_columns = X.select_dtypes(include=[\'number\']).columns # Preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numerical_columns), (\'cat\', OneHotEncoder(handle_unknown=\'ignore\'), categorical_columns) ]) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create pipeline model = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression(max_iter=1000))]) # Train the model model.fit(X_train, y_train) # Evaluate the model accuracy = model.score(X_test, y_test) return accuracy # Example usage of the function accuracy = load_and_evaluate_openml_dataset(31) # Known data_id in OpenML print(f\\"Model Accuracy: {accuracy}\\") ``` In the example, replace `31` with any valid `data_id` to test the function.","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer import numpy as np def load_and_evaluate_openml_dataset(data_id: int) -> float: Loads a dataset from OpenML by data_id, preprocesses it, trains a logistic regression model, and evaluates its accuracy. Parameters: data_id (int): The dataset ID from OpenML. Returns: float: The accuracy of the trained logistic regression model on the test set. # Load the dataset dataset = fetch_openml(data_id=data_id) X, y = dataset.data, dataset.target # Identify categorical and numerical columns categorical_columns = X.select_dtypes(include=[\'category\', \'object\']).columns numerical_columns = X.select_dtypes(include=[\'number\']).columns # Preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numerical_columns), (\'cat\', OneHotEncoder(handle_unknown=\'ignore\'), categorical_columns) ]) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create pipeline model = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression(max_iter=1000))]) # Train the model model.fit(X_train, y_train) # Evaluate the model accuracy = model.score(X_test, y_test) return accuracy"},{"question":"# Command-Line Interface with `argparse` Objective Create a command-line tool using Python, which performs different operations based on sub-commands provided. The tool should be capable of managing a hypothetical file system, allowing users to create directories, list contents, and remove directories. Requirements 1. **Sub-commands**: - `mkdir` : Create a new directory. - `ls` : List the contents of a directory. - `rmdir` : Remove an existing directory. 2. **Command-Line Arguments**: - For `mkdir`: Takes one required argument, `directory_name`, which is the name of the directory to be created. - For `ls`: Takes one optional argument, `directory_name`. If no directory name is provided, it lists the contents of the current directory. - For `rmdir`: Takes one required argument, `directory_name`, which is the name of the directory to be removed. 3. **Constraints**: - The tool should handle any exceptions that arise (e.g., trying to create a directory that already exists, removing a non-existent directory, etc.). - Use the `argparse` module to handle the command-line arguments and sub-commands. - Directory operations (mkdir, ls, rmdir) should mimic actual file system operations; however, you don’t need to maintain a real file system – you can use an in-memory representation for simplicity. Implementation Details - Define a Python script that uses the `argparse` module. - Implement functions to handle each sub-command. - Ensure proper error handling and validation of inputs. - Simulate a file system using an in-memory dictionary or similar data structure. Example Usage ```sh # Creating a directory named \'docs\' python cli_tool.py mkdir docs # Listing contents of the current directory python cli_tool.py ls # Listing contents of the \'docs\' directory python cli_tool.py ls docs # Removing the \'docs\' directory python cli_tool.py rmdir docs ``` Expected Input and Output - **mkdir**: - Input: `python cli_tool.py mkdir docs` - Output: `Directory \'docs\' created.` - **ls**: - Input: `python cli_tool.py ls` - Output: `[List of directories/files in the current directory]` - Input: `python cli_tool.py ls docs` - Output: `[List of directories/files in \'docs\']` - **rmdir**: - Input: `python cli_tool.py rmdir docs` - Output: `Directory \'docs\' removed.` # Assessment Criteria - Correctly using the `argparse` module to handle sub-commands and arguments. - Proper implementation of directory operations in-memory. - Effective error handling and user feedback. - Code readability and adherence to Pythonic conventions. Good luck!","solution":"import argparse # Simulated file system using a dictionary file_system = {} def mkdir(directory_name): if directory_name in file_system: return f\\"Error: Directory \'{directory_name}\' already exists.\\" file_system[directory_name] = {} return f\\"Directory \'{directory_name}\' created.\\" def ls(directory_name=None): if directory_name: if directory_name not in file_system: return f\\"Error: Directory \'{directory_name}\' does not exist.\\" return list(file_system[directory_name].keys()) return list(file_system.keys()) def rmdir(directory_name): if directory_name not in file_system: return f\\"Error: Directory \'{directory_name}\' does not exist.\\" del file_system[directory_name] return f\\"Directory \'{directory_name}\' removed.\\" def main(): parser = argparse.ArgumentParser(description=\\"Manage a hypothetical file system.\\") subparsers = parser.add_subparsers(dest=\'command\') mkdir_parser = subparsers.add_parser(\'mkdir\', help=\'Create a new directory\') mkdir_parser.add_argument(\'directory_name\', type=str, help=\'Name of the directory to create\') ls_parser = subparsers.add_parser(\'ls\', help=\'List contents of a directory\') ls_parser.add_argument(\'directory_name\', type=str, nargs=\'?\', default=None, help=\'Name of the directory to list contents of (default: current directory)\') rmdir_parser = subparsers.add_parser(\'rmdir\', help=\'Remove an existing directory\') rmdir_parser.add_argument(\'directory_name\', type=str, help=\'Name of the directory to remove\') args = parser.parse_args() if args.command == \'mkdir\': print(mkdir(args.directory_name)) elif args.command == \'ls\': contents = ls(args.directory_name) if isinstance(contents, str): # error message print(contents) else: for item in contents: print(item) elif args.command == \'rmdir\': print(rmdir(args.directory_name)) if __name__ == \\"__main__\\": main()"},{"question":"# Seaborn Visualization Task You\'re given the Titanic dataset and are required to create various visualizations using seaborn to analyze the data. The dataset can be loaded using `sns.load_dataset(\\"titanic\\")`. Implement the following functions to create visualizations: 1. **Function Name:** `plot_class_distribution` - **Input:** None - **Output:** A seaborn count plot showing the distribution of passengers across different classes. - **Constraints:** Use the column `class` from the Titanic dataset. ```python def plot_class_distribution(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") plot = sns.countplot(titanic, x=\\"class\\") plt.show() return plot ``` 2. **Function Name:** `plot_class_survival_distribution` - **Input:** None - **Output:** A seaborn count plot showing the distribution of passengers across different classes, grouped by their survival status. - **Constraints:** Use the columns `class` and `survived` from the Titanic dataset. ```python def plot_class_survival_distribution(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") plot = sns.countplot(titanic, x=\\"class\\", hue=\\"survived\\") plt.show() return plot ``` 3. **Function Name:** `plot_class_survival_normalized` - **Input:** None - **Output:** A seaborn count plot showing the percentage distribution of passengers across different classes, grouped by their survival status. - **Constraints:** Use the columns `class` and `survived` from the Titanic dataset. Normalize the counts to percentages. ```python def plot_class_survival_normalized(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") plot = sns.countplot(titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\") plt.show() return plot ``` # Instructions - Implement the functions detailed above. Each function should perform the indicated visualization and return the seaborn plot object. - Ensure proper import statements are included within the functions. - Use appropriate seaborn methods as indicated in the function descriptions. - The functions should be standalone and should handle dataset loading and theme setting internally. - Test your functions by calling them to ensure they produce the correct visualizations.","solution":"def plot_class_distribution(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") plot = sns.countplot(x=\\"class\\", data=titanic) plt.show() return plot def plot_class_survival_distribution(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") plot = sns.countplot(x=\\"class\\", hue=\\"survived\\", data=titanic) plt.show() return plot def plot_class_survival_normalized(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") # Create a normalized count plot plot = sns.histplot(titanic, x=\\"class\\", hue=\\"survived\\", multiple=\\"fill\\", stat=\\"probability\\", shrink=.8) plt.show() return plot"},{"question":"**Objective:** Design a Python function that uses the `subprocess` module to execute a series of shell commands and capture their output and errors. The function should handle various scenarios such as command failures, timeouts, and output redirection. **Function Specification:** 1. **Function Name:** `execute_commands` 2. **Inputs:** - `commands` (List of strings): A list of shell commands to execute. - `timeout` (int): A timeout in seconds for each command execution. 3. **Outputs:** - Returns a list of dictionaries, each containing: - `command`: The executed command (string). - `returncode`: The return code of the executed command (int). - `stdout`: The captured standard output (string or `None` if not captured). - `stderr`: The captured standard error (string or `None` if not captured). - `error`: Any exception raised during command execution (string or `None` if no exception). 4. **Constraints:** - The function should execute each command sequentially. - Capture both standard output and standard error. - Handle command timeouts gracefully using `subprocess.TimeoutExpired`. - Handle command failures gracefully using `subprocess.CalledProcessError`. - Use the `subprocess.run()` function for simplicity. **Example:** ```python def execute_commands(commands, timeout): # Your code here # Example usage commands = [\\"ls -l\\", \\"echo Hello\\", \\"cat non_existent_file\\", \\"sleep 2\\"] timeout = 1 result = execute_commands(commands, timeout) for res in result: print(res) ``` The expected output for this example might look like: ```python [ {\'command\': \'ls -l\', \'returncode\': 0, \'stdout\': \'...\', \'stderr\': \'\', \'error\': None}, {\'command\': \'echo Hello\', \'returncode\': 0, \'stdout\': \'Hellon\', \'stderr\': \'\', \'error\': None}, {\'command\': \'cat non_existent_file\', \'returncode\': 1, \'stdout\': \'\', \'stderr\': \'cat: non_existent_file: No such file or directoryn\', \'error\': None}, {\'command\': \'sleep 2\', \'returncode\': -1, \'stdout\': None, \'stderr\': None, \'error\': \'Command timed out\'} ] ``` **Notes:** - Ensure the function handles different types of output encoding appropriately. - Consider edge cases such as empty command lists. - Ensure the function has proper error handling and logging to make debugging easy. **Tips:** - Use `subprocess.run()` with appropriate arguments to capture output and error. - Use try-except blocks to handle exceptions. - Test the function with various commands and scenarios to ensure robustness.","solution":"import subprocess from typing import List, Dict def execute_commands(commands: List[str], timeout: int) -> List[Dict[str, any]]: results = [] for command in commands: result = {\'command\': command, \'returncode\': None, \'stdout\': None, \'stderr\': None, \'error\': None} try: completed_process = subprocess.run( command, shell=True, capture_output=True, text=True, timeout=timeout ) result[\'returncode\'] = completed_process.returncode result[\'stdout\'] = completed_process.stdout result[\'stderr\'] = completed_process.stderr except subprocess.TimeoutExpired as e: result[\'returncode\'] = -1 result[\'error\'] = f\\"Command timed out: {str(e)}\\" except subprocess.CalledProcessError as e: result[\'returncode\'] = e.returncode result[\'error\'] = f\\"Command failed: {str(e)}\\" result[\'stdout\'] = e.stdout result[\'stderr\'] = e.stderr except Exception as e: result[\'error\'] = str(e) results.append(result) return results"},{"question":"# Coding Assessment: Dynamic Package Resource Retrieval **Objective:** Demonstrate your understanding of Python\'s `pkgutil` module by implementing a function that retrieves resource data from specified modules. **Problem Statement:** Write a Python function `find_package_resources` which does the following: 1. Takes as input a list of package names to search for. 2. For each specified package: - Extends its search path using `pkgutil.extend_path()`. - Iterates over all submodules and subpackages. - Attempts to locate a resource file named `data.txt` within each found package or subpackage using `pkgutil.get_data()`. 3. Returns a dictionary where the keys are the package/subpackage names and the values are the contents of the `data.txt` files, or `None` if the resource file is not found. **Function Signature:** ```python def find_package_resources(packages: list) -> dict: pass ``` **Input:** - `packages`: A list of strings, where each string is the name of a package. **Output:** - A dictionary where keys are package/subpackage names and values are the contents (binary string) of their `data.txt` file or `None`. **Constraints:** - You must use the `pkgutil` module\'s functionalities (`extend_path`, `iter_modules`, `get_data`). - Assume all package names provided are valid and can be imported. - Handle any exceptions that occur during importing or accessing data gracefully. **Example:** ```python packages = [\'mypackage\', \'anotherpackage\'] result = find_package_resources(packages) ``` Here, `result` might be: ```python { \'mypackage\': b\'Content of mypackage data.txt\', \'mypackage.submodule\': b\'Content of mypackage.submodule data.txt\', \'anotherpackage\': None, ... } ``` **Note:** - If `data.txt` is not found in a package or subpackage, the corresponding dictionary value should be `None`. - You do not need to implement actual modules or files; assume their structure and contents for the context of this question. **Hint:** - Leverage `pkgutil.iter_modules` to find submodules and subpackages. - Utilize `pkgutil.get_data` to fetch the contents of `data.txt`. **Evaluation Criteria:** - Correctness: The function works as expected and retrieves package data appropriately. - Usage of `pkgutil`: Proper use of specified `pkgutil` functions. - Robustness: Handles exceptions and edge cases correctly. - Code clarity and comments: Code is readable with appropriate comments explaining the logic.","solution":"import pkgutil import importlib import sys def find_package_resources(packages: list) -> dict: resources = {} for package in packages: try: pkg = importlib.import_module(package) pkg.__path__ = pkgutil.extend_path(pkg.__path__, package) modules = pkgutil.walk_packages(pkg.__path__, package + \'.\') except ImportError: resources[package] = None continue all_modules = [package] for finder, name, ispkg in modules: all_modules.append(name) for mod in all_modules: try: data = pkgutil.get_data(mod, \'data.txt\') resources[mod] = data except Exception: resources[mod] = None return resources"},{"question":"# Type-safe Linked List Implementation **Objective:** Implement a type-safe linked list using the `typing` module in Python. The linked list should support generic types, ensuring that all elements are of the same type. **Instructions:** 1. Define a class `Node` that represents a node in the linked list. Each node should store a value of a generic type and a reference to the next node in the list. 2. Define a class `LinkedList` that represents the linked list. The linked list class should be able to: - Append an element to the end of the list. - Find an element in the list. - Delete an element from the list. - Display the entire list. 3. Use the `typing` module to ensure that the linked list is type-safe. Specifically, use `TypeVar` and `Generic` to enforce that all elements in the linked list are of the same type. 4. Handle edge cases such as attempting to delete or find an element in an empty list gracefully. **Constraints:** - The implementation should use type hints to specify the types of function arguments and return types. - You are not allowed to use lists or other built-in data structures to store the elements of the linked list. - The implementation should be efficient in terms of both time and space complexity. **Example Usage:** ```python from typing import TypeVar, Generic, Optional T = TypeVar(\'T\') class Node(Generic[T]): def __init__(self, value: T): self.value = value self.next: Optional[Node[T]] = None class LinkedList(Generic[T]): def __init__(self): self.head: Optional[Node[T]] = None def append(self, value: T) -> None: pass # Your implementation here def find(self, value: T) -> Optional[Node[T]]: pass # Your implementation here def delete(self, value: T) -> bool: pass # Your implementation here def display(self) -> None: pass # Your implementation here # Example usage: ll = LinkedList[int]() ll.append(1) ll.append(2) ll.append(3) ll.display() # Output: 1 -> 2 -> 3 ll.delete(2) ll.display() # Output: 1 -> 3 ``` **Note:** You need to fill in the implementation details for the `append`, `find`, `delete`, and `display` methods.","solution":"from typing import TypeVar, Generic, Optional T = TypeVar(\'T\') class Node(Generic[T]): def __init__(self, value: T): self.value = value self.next: Optional[Node[T]] = None class LinkedList(Generic[T]): def __init__(self): self.head: Optional[Node[T]] = None def append(self, value: T) -> None: new_node = Node(value) if not self.head: self.head = new_node else: current = self.head while current.next: current = current.next current.next = new_node def find(self, value: T) -> Optional[Node[T]]: current = self.head while current: if current.value == value: return current current = current.next return None def delete(self, value: T) -> bool: current = self.head prev = None while current: if current.value == value: if prev: prev.next = current.next else: self.head = current.next return True prev = current current = current.next return False def display(self) -> None: current = self.head elements = [] while current: elements.append(str(current.value)) current = current.next print(\\" -> \\".join(elements)) # Example usage: # ll = LinkedList[int]() # ll.append(1) # ll.append(2) # ll.append(3) # ll.display() # Output: 1 -> 2 -> 3 # ll.delete(2) # ll.display() # Output: 1 -> 3"},{"question":"# Event Scheduler with Task Dependencies You are required to implement a task scheduler using the `sched` module. The task scheduler should be capable of scheduling multiple tasks with dependencies, such that a task can only run after its dependencies (other tasks) have run. Requirements 1. **Function Name**: `schedule_tasks` 2. **Inputs**: - A list of tasks, where each task is represented as a tuple `(name, time, priority, dependencies)` - `name` (str): The unique identifier for the task. - `time` (float): The scheduled time for the task. - `priority` (int): The priority of the task. Lower numbers indicate higher priority. - `dependencies` (list): A list of names of tasks that this task depends on. 3. **Output**: - A list of task names in the order they are executed. Constraints - Task names will be unique. - Time values are positive floating-point numbers. - Priorities are non-negative integers. - Dependencies, if any, are always valid and do not form cycles. Performance Requirements - The function should handle up to 10,000 tasks efficiently. - The scheduling should respect task dependencies and execute tasks accordingly. # Function Signature ```python def schedule_tasks(tasks: List[Tuple[str, float, int, List[str]]]) -> List[str]: pass ``` # Example ```python tasks = [ (\\"task1\\", 10.0, 1, []), (\\"task2\\", 15.0, 2, [\\"task1\\"]), (\\"task3\\", 20.0, 1, [\\"task1\\", \\"task2\\"]), (\\"task4\\", 5.0, 3, []), ] print(schedule_tasks(tasks)) # Output could be: [\\"task4\\", \\"task1\\", \\"task2\\", \\"task3\\"] # Explanation: # - \\"task1\\" and \\"task4\\" can run at any time based on their priority. # - \\"task2\\" depends on \\"task1\\" and cannot run until \\"task1\\" has been executed. # - \\"task3\\" depends on both \\"task1\\" and \\"task2\\" and will run last. ``` Use the `sched` module to help achieve the scheduling and ensure that the task execution respects the dependencies specified.","solution":"import sched import time from typing import List, Tuple def schedule_tasks(tasks: List[Tuple[str, float, int, List[str]]]) -> List[str]: task_graph = {task[0]: task for task in tasks} indegree = {task[0]: 0 for task in tasks} for task in tasks: for dep in task[3]: indegree[task[0]] += 1 zero_indegree_tasks = [task for task in indegree if indegree[task] == 0] executed_tasks = [] scheduler = sched.scheduler(time.time, time.sleep) while zero_indegree_tasks: zero_indegree_tasks.sort(key=lambda x: (task_graph[x][1], task_graph[x][2])) current_task = zero_indegree_tasks.pop(0) executed_tasks.append(current_task) for task in tasks: if current_task in task[3]: indegree[task[0]] -= 1 if indegree[task[0]] == 0: zero_indegree_tasks.append(task[0]) return executed_tasks"},{"question":"# Question: Advanced Function Manipulation with functools You are required to create a solution for a fictional data processing application. The problem involves implementing a class and a standalone function using the `functools` module to enhance performance and maintainability. Here are the specifics: Part 1: Class with Cached Properties Create a class `DataProcessor` that takes a list of numeric data as its argument. Implement the following: 1. **A method `mean(self)`**: Calculates and returns the mean of the data. This method should utilize `@functools.cached_property` to cache the result. 2. **A method `stdev(self)`**: Calculates and returns the standard deviation of the data using the mean calculated previously. Use `@functools.cached_property` for this as well. Part 2: Partial Functions and Memoization Create a standalone function: 1. **A function `expensive_computation(x)`**: Takes an integer x and returns x^2 after simulating a time-consuming computation (hint: use `time.sleep(2)` to simulate delay). 2. **A function `quick_computation(x, cache={})`**: Implements the `expensive_computation` function using `@functools.lru_cache` to cache previously computed results for quick lookup. Also, create a variant of this function using `functools.partial`. Part 3: Single Dispatch Implement a function to demonstrate single dispatch: 1. **A function `process(value)`**: Uses `functools.singledispatch` to handle different data types: - **For integers**: Returns the square of the value. - **For strings**: Returns the string reversed. - **For lists**: Returns the list with all elements doubled. # Implementation Requirements - **DataProcessor class**: - Should use `@functools.cached_property` decorators. - Should properly import and handle necessary libraries. - **expensive_computation and quick_computation functions**: - Utilize `time.sleep(2)` to simulate delay in `expensive_computation`. - Use `@functools.lru_cache` in `quick_computation`. - Create a partial function variant. - **process function**: - Implement as a single dispatch generic function. # Input and Output DataProcessor Class - Input: A list of numbers. - Output: The mean and standard deviation of the list. expensive_computation - Input: An integer. - Output: The square of the integer (simulated delay). quick_computation - Input: An integer. - Output: The square of the integer (cached results). process function - Input: An integer, a string, or a list. - Output: Processed result as per the instructions above. # Constraints 1. Implement proper exception handling. 2. Use the specified decorators from functools to handle cache and partial function implementation. 3. Ensure the solution is efficient and maintains code readability. **Note**: The use of mathematical and statistical functions such as `mean` and `stdev` should be correctly implemented or imported. Example ```python from functools import cached_property, lru_cache, partial, singledispatch import time class DataProcessor: def __init__(self, data): self._data = data @cached_property def mean(self): return sum(self._data) / len(self._data) @cached_property def stdev(self): mean = self.mean return (sum((x - mean) ** 2 for x in self._data) / len(self._data)) ** 0.5 @lru_cache(maxsize=None) def expensive_computation(x): time.sleep(2) return x ** 2 def quick_computation(x): return expensive_computation(x) partial_computation = partial(expensive_computation) @singledispatch def process(value): raise NotImplementedError(\\"Unsupported type\\") @process.register def _(value: int): return value ** 2 @process.register def _(value: str): return value[::-1] @process.register def _(value: list): return [2 * x for x in value] # Test cases dp = DataProcessor([1, 2, 3, 4, 5]) print(dp.mean) # 3.0 print(dp.stdev) # Approx. 1.414 print(expensive_computation(5)) # Delayed result: 25 print(quick_computation(5)) # Cached result: 25 print(partial_computation(10)) # Delayed result: 100 print(process(5)) # 25 print(process(\\"abc\\")) # \\"cba\\" print(process([1, 2, 3])) # [2, 4, 6] ``` This question covers the usage of functools to solve real-world problems making use of various utilities provided by the module.","solution":"from functools import cached_property, lru_cache, singledispatch, partial import time import math class DataProcessor: def __init__(self, data): self._data = data @cached_property def mean(self): return sum(self._data) / len(self._data) @cached_property def stdev(self): mean = self.mean return (sum((x - mean) ** 2 for x in self._data) / len(self._data)) ** 0.5 @lru_cache(maxsize=None) def expensive_computation(x): time.sleep(2) return x ** 2 def quick_computation(x): return expensive_computation(x) partial_computation = partial(expensive_computation) @singledispatch def process(value): raise NotImplementedError(\\"Unsupported type\\") @process.register def _(value: int): return value ** 2 @process.register def _(value: str): return value[::-1] @process.register def _(value: list): return [2 * x for x in value]"},{"question":"Using the `pickle` module, create a system that can serialize and deserialize custom class instances, including handling persistent IDs and custom reduction methods. Objective Implement a Python class `TaskManager` that allows creating, storing, saving to a file, and loading from a file a collection of tasks. Each task should be an instance of a custom class `Task`, which has the following attributes: - `id`: A unique identifier for the task (integer). - `description`: A description of the task (string). - `completed`: A boolean indicating whether the task is completed or not. The `TaskManager` class should support the following methods: - `add_task(task: Task)`: Adds a new task to the manager. - `complete_task(task_id: int)`: Marks the task with the given ID as completed. - `save_to_file(filename: str)`: Serializes the entire `TaskManager` to a file. - `load_from_file(filename: str)`: Deserializes the `TaskManager` from a file. - `get_tasks() -> List[Task]`: Returns a list of all tasks currently in the manager. Requirements 1. Use `pickle` for serialization and deserialization of the `TaskManager` and its `Task` instances. 2. Implement custom reduction methods (`__reduce__` or `__reduce_ex__`) for the `Task` class to ensure it is pickled and unpickled correctly. 3. Handle any potential issues with recursive references or shared objects within the `TaskManager`. 4. Ensure that completing a task only affects the specific instance of the task and not other shared references. Constraints - The `id` of each `Task` instance must be unique within a `TaskManager`. - The solution should work with Python 3.8 or higher. Example Usage ```python # Creating Task instances task1 = Task(1, \'Write code\', False) task2 = Task(2, \'Review code\', False) # Creating TaskManager and adding tasks manager = TaskManager() manager.add_task(task1) manager.add_task(task2) # Completing a task manager.complete_task(1) # Saving to file manager.save_to_file(\'tasks.pkl\') # Loading from file new_manager = TaskManager.load_from_file(\'tasks.pkl\') print(new_manager.get_tasks()) # Outputs list of tasks with task1 marked as completed ``` Implement the `Task` and `TaskManager` classes with the required methods and functionalities. Note Consider edge cases and ensure your code is well-documented and follows best practices for serialization using the `pickle` module.","solution":"import pickle class Task: def __init__(self, id, description, completed=False): self.id = id self.description = description self.completed = completed def __repr__(self): return f\\"Task(id={self.id}, description={self.description}, completed={self.completed})\\" def complete(self): self.completed = True def __reduce__(self): return (self.__class__, (self.id, self.description, self.completed)) class TaskManager: def __init__(self): self.tasks = {} def add_task(self, task: Task): if task.id in self.tasks: raise ValueError(f\\"Task with id {task.id} already exists.\\") self.tasks[task.id] = task def complete_task(self, task_id: int): if task_id not in self.tasks: raise ValueError(f\\"Task with id {task_id} does not exist.\\") self.tasks[task_id].complete() def save_to_file(self, filename: str): with open(filename, \'wb\') as file: pickle.dump(self, file) @classmethod def load_from_file(cls, filename: str): with open(filename, \'rb\') as file: return pickle.load(file) def get_tasks(self): return list(self.tasks.values())"},{"question":"# Coding Challenge: Filter and Modify a List of Numbers You are given a list of integers. Your task is to write a function that filters out the even numbers and then applies a transformation to each of the remaining odd numbers. Specifically: 1. **Filter out all the even numbers** from the list. 2. **Square each** of the remaining odd numbers. Implement the function `filter_and_square(numbers: list) -> list` that takes a list of integers as input and returns a new list containing the squared values of the odd numbers. **Examples:** ```python filter_and_square([1, 2, 3, 4, 5]) # Returns: [1, 9, 25] filter_and_square([10, 15, 20, 25]) # Returns: [225, 625] filter_and_square([2, 4, 6, 8]) # Returns: [] filter_and_square([1, 3, 5, 7, 9]) # Returns: [1, 9, 25, 49, 81] ``` **Constraints:** - The `numbers` list will have at most 10^6 elements. - Each integer in the list will be in the range `-10^6` to `10^6`. **Requirements:** - Use the `filter` and `map` built-in functions to solve this problem. - Consider the efficiency of your solution given the constraints. **Function Signature:** ```python def filter_and_square(numbers: list) -> list: pass ```","solution":"def filter_and_square(numbers: list) -> list: Filters out even numbers and squares the remaining odd numbers. :param numbers: List of integers. :return: List of squared values of the filtered odd numbers. return list(map(lambda x: x ** 2, filter(lambda x: x % 2 != 0, numbers)))"},{"question":"Semi-Supervised Learning using Scikit-Learn Objective: Implement and evaluate a semi-supervised learning technique using the `SelfTrainingClassifier` and compare it with a purely supervised approach using a given dataset with a mix of labeled and unlabeled data. Instructions: 1. **Data Preparation**: Download the `digits` dataset available in scikit-learn. Split the data into a training set and a test set. Use the training set for semi-supervised learning. 2. **Label Manipulation**: Randomly select a portion of labels from the training set and set them to `-1` to simulate unlabeled data. Ensure that at least 20% of the training data remains labeled. 3. **Model Implementation**: - Implement a `SelfTrainingClassifier` with a basic supervised classifier such as `LogisticRegression`. - Train the classifier with the manipulated training data. 4. **Evaluation**: - Evaluate the trained `SelfTrainingClassifier` on the test set and print the accuracy. - Train a purely supervised classifier (`LogisticRegression`) using only the labeled training data and evaluate its accuracy on the test set. - Compare the accuracy results of the semi-supervised and supervised approaches. 5. **Advanced Task (Optional)**: Experiment with `LabelPropagation` or `LabelSpreading` on the same data and compare their performances with the `SelfTrainingClassifier`. Expected Input and Output: Input: - `digits` dataset from scikit-learn. Output: - Accuracy of `SelfTrainingClassifier` on the test set. - Accuracy of purely supervised `LogisticRegression` on the test set. - (Optional) Accuracy of `LabelPropagation` or `LabelSpreading` on the test set. Constraints: - Use `scikit-learn` for the implementation. - The seed for randomness should be set for reproducibility. - Ensure that at least 20% of the training data is labeled after manipulation. Performance Requirements: - Efficient handling of the digits dataset. - Proper evaluation and comparison of accuracy between different models. ```python # Starter Code: import numpy as np from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.metrics import accuracy_score # Seed for reproducibility np.random.seed(42) # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y) # Randomly change 40% of the labels in y_train to -1 to simulate unlabeled data n_labeled_points = int(len(y_train) * 0.6) indices = np.random.choice(len(y_train), n_labeled_points, replace=False) y_train_unlabeled = np.copy(y_train) y_train_unlabeled[indices] = -1 # Implement SelfTrainingClassifier with LogisticRegression base_clf = LogisticRegression(max_iter=1000) self_training_clf = SelfTrainingClassifier(base_clf) self_training_clf.fit(X_train, y_train_unlabeled) # Evaluate SelfTrainingClassifier y_pred_self_training = self_training_clf.predict(X_test) accuracy_self_training = accuracy_score(y_test, y_pred_self_training) # Train a purely supervised LogisticRegression supervised_clf = LogisticRegression(max_iter=1000) supervised_clf.fit(X_train[indices], y_train[indices]) # Evaluate the supervised classifier y_pred_supervised = supervised_clf.predict(X_test) accuracy_supervised = accuracy_score(y_test, y_pred_supervised) # Print the results print(\\"Accuracy of SelfTrainingClassifier: {:.2f}%\\".format(accuracy_self_training * 100)) print(\\"Accuracy of purely supervised LogisticRegression: {:.2f}%\\".format(accuracy_supervised * 100)) # Optional: Experiment with LabelPropagation or LabelSpreading and compare the results ``` Notes: - Ensure that the accuracy comparison is fair by using the same test set. - Optional task is not mandatory but aims to provide additional insights into semi-supervised learning capabilities.","solution":"import numpy as np from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score # Seed for reproducibility np.random.seed(42) # Load and split the digits dataset digits = load_digits() X, y = digits.data, digits.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y) # Randomly change 40% of the labels in y_train to -1 to simulate unlabeled data n_labeled_points = int(len(y_train) * 0.6) indices = np.random.choice(len(y_train), n_labeled_points, replace=False) y_train_unlabeled = np.copy(y_train) y_train_unlabeled[indices] = -1 # Implement SelfTrainingClassifier with LogisticRegression base_clf = LogisticRegression(max_iter=1000, solver=\'liblinear\') self_training_clf = SelfTrainingClassifier(base_clf) self_training_clf.fit(X_train, y_train_unlabeled) # Evaluate SelfTrainingClassifier y_pred_self_training = self_training_clf.predict(X_test) accuracy_self_training = accuracy_score(y_test, y_pred_self_training) # Train a purely supervised LogisticRegression labeled_indices = np.where(y_train_unlabeled != -1)[0] supervised_clf = LogisticRegression(max_iter=1000, solver=\'liblinear\') supervised_clf.fit(X_train[labeled_indices], y_train[labeled_indices]) # Evaluate the supervised classifier y_pred_supervised = supervised_clf.predict(X_test) accuracy_supervised = accuracy_score(y_test, y_pred_supervised) # Optional: Experiment with LabelPropagation label_propagation = LabelPropagation() label_propagation.fit(X_train, y_train_unlabeled) # Evaluate LabelPropagation y_pred_label_propagation = label_propagation.predict(X_test) accuracy_label_propagation = accuracy_score(y_test, y_pred_label_propagation) # Print the results print(f\\"Accuracy of SelfTrainingClassifier: {accuracy_self_training * 100:.2f}%\\") print(f\\"Accuracy of purely supervised LogisticRegression: {accuracy_supervised * 100:.2f}%\\") print(f\\"Accuracy of LabelPropagation: {accuracy_label_propagation * 100:.2f}%\\")"},{"question":"# Assessing Your Understanding of `functools` You are working on optimizing a data processing application. One of the tasks involves frequently computing the standard deviation of subsets of data from a large dataset. To make this process more efficient, you decide to use caching techniques provided by the `functools` module. Task: 1. **Implement a class `DataProcessor` with the following requirements:** - The class should be initialized with a list of numeric data. - Include a method `get_subset` to return a subset of the data given a start and end index. - Include a method `calculate_stdev` that calculates the standard deviation of a subset of the data between given start and end indices. Use `statistics.stdev` for this calculation. - Cache the result of `calculate_stdev` to optimize repeated calculations. Use an appropriate caching decorator from `functools`. - Include a method `clear_cache` that clears the cache associated with `calculate_stdev`. 2. **Implement the `calculate_stdev_cached_property` as a cached property to return the standard deviation of the entire dataset.** Constraints: - The cache for `calculate_stdev` should be able to handle at least 128 different subsets. - Each cached method should be clearly annotated with the appropriate decorator. - Ensure that the methods handle invalid indices by raising an `IndexError`. # Example Usage ```python from statistics import stdev from functools import lru_cache, cached_property class DataProcessor: def __init__(self, data): self.data = data def get_subset(self, start, end): if start < 0 or end > len(self.data) or start > end: raise IndexError(\\"Invalid indices\\") return self.data[start:end] @lru_cache(maxsize=128) def calculate_stdev(self, start, end): subset = self.get_subset(start, end) return stdev(subset) def clear_cache(self): self.calculate_stdev.cache_clear() @cached_property def calculate_stdev_cached_property(self): return stdev(self.data) ``` # Notes: - Ensure your class handles edge cases such as empty data lists or invalid indices. - Write test cases to demonstrate the caching functionality and edge case handling.","solution":"from statistics import stdev from functools import lru_cache, cached_property class DataProcessor: def __init__(self, data): self.data = data def get_subset(self, start, end): if not (0 <= start < len(self.data)) or not (0 < end <= len(self.data)) or start >= end: raise IndexError(\\"Invalid indices\\") return self.data[start:end] @lru_cache(maxsize=128) def calculate_stdev(self, start, end): subset = self.get_subset(start, end) if len(subset) < 2: raise ValueError(\\"At least two data points are required to calculate standard deviation\\") return stdev(subset) @cached_property def calculate_stdev_cached_property(self): if len(self.data) < 2: raise ValueError(\\"At least two data points are required to calculate standard deviation\\") return stdev(self.data) def clear_cache(self): self.calculate_stdev.cache_clear()"},{"question":"Objective: Create a new Python type that acts as a custom container with certain advanced behaviors using the PyTypeObject structure. Description: You are required to implement a custom `MyContainer` type using the PyTypeObject structure in a C extension for Python. This container will have features like: - Allowing both integer and string keys. - Supporting basic operations: item addition, deletion, and retrieval. - Providing iteration over key-value pairs. - Supporting basic comparison operations. Implement the following methods for the `MyContainer` type: - `__init__`: Initialize the container with an optional dictionary of items. - `__getitem__`: Retrieve a value based on a key. - `__setitem__`: Set a value for a specific key. - `__delitem__`: Remove an item based on a key. - `__iter__`: Return an iterator object for the container. - `__next__`: Support iteration. - `__eq__`, `__lt__`, `__le__`, `__gt__`, `__ge__`: Comparisons by the sum of numeric values in the container. Requirements: 1. Define the `MyContainer_Type` structure with appropriate fields. 2. Initialize it correctly using the `PyTypeObject` fields. 3. Ensure that type supports GC if necessary by implementing `tp_traverse` and `tp_clear`. Constraints: - The container should handle integer and string keys seamlessly. - It should be able to handle dynamic additions and deletions of items. - Custom comparison methods should compare based on the summed value of numeric items. Input and Output: - The container should be initialized optionally with a dictionary. - Operations like item addition, deletion, and retrieval should raise appropriate Python exceptions if the keys do not exist or are invalid. - Iteration over the container will yield key-value pairs. - Comparison should follow standard Python practices using `==`, `<`, `<=`, `>`, `>=`. Performance: - Your implementation should efficiently manage memory and performance, adhering to Python\'s garbage collection practices where applicable. Example: ```python # Python Usage Example container = MyContainer({1: 10, \\"two\\": 20}) container[3] = 30 print(container[1]) # Output: 10 del container[3] for key, value in container: print(key, value) # Comparisons container1 = MyContainer({1: 10}) container2 = MyContainer({1: 20}) print(container1 < container2) # Output: True ```","solution":"class MyContainer: def __init__(self, initial_data=None): if initial_data is None: self._data = {} else: self._data = dict(initial_data) def __getitem__(self, key): return self._data[key] def __setitem__(self, key, value): self._data[key] = value def __delitem__(self, key): del self._data[key] def __iter__(self): self._iter = iter(self._data.items()) return self def __next__(self): return next(self._iter) def __eq__(self, other): return self._sum_values() == other._sum_values() def __lt__(self, other): return self._sum_values() < other._sum_values() def __le__(self, other): return self._sum_values() <= other._sum_values() def __gt__(self, other): return self._sum_values() > other._sum_values() def __ge__(self, other): return self._sum_values() >= other._sum_values() def _sum_values(self): return sum(value for value in self._data.values() if isinstance(value, (int, float)))"},{"question":"# Warning Control Assessment Python\'s `warnings` module provides functionalities to issue warnings and control how they are handled. Your task is to implement functions that demonstrate the use of warning issuing, filtering, and context management. Function 1: issue_warning Implement a function `issue_warning` that takes two arguments: `message` (a string) and `category` (a subclass of `Warning`), and issues a warning with the specified message and category. ```python def issue_warning(message: str, category: Warning) -> None: Issues a warning with the given message and category. Args: - message (str): the warning message to be issued. - category (Warning): the warning category (subclass of Warning). Returns: None ``` Function 2: filter_warnings Implement a function `filter_warnings` that takes a list of filter configurations and applies them using `warnings.filterwarnings`. Each filter configuration should be a dictionary with the keys: `action`, `message`, `category`, `module`, and `lineno`. ```python def filter_warnings(filters: list) -> None: Applies a list of warning filters. Args: - filters (list): A list of filter configurations. Each configuration is a dictionary with the following keys: - action (str): the filter action. - message (str): a regular expression to match the warning message. - category (Warning): the warning category (subclass of Warning). - module (str): a regular expression to match the module name. - lineno (int): the line number where the warning occurred. Returns: None ``` Function 3: capture_warnings Implement a function `capture_warnings` that takes a callable and its arguments, runs the callable within the context of `warnings.catch_warnings(record=True)`, and returns the list of captured warning messages. ```python def capture_warnings(callable_func: callable, *args, **kwargs) -> list: Captures warnings generated by the given callable. Args: - callable_func (callable): the function that is expected to issue warnings. - args, kwargs: arguments to be passed to the callable. Returns: list: A list of captured warning messages. ``` # Constraints and Requirements: - Use the appropriate classes and functions from the `warnings` module. - Ensure that `issue_warning` can handle any valid subclass of `Warning`. - The `filter_warnings` function should properly set up warning filters based on the provided configurations. - The `capture_warnings` function should correctly capture and return warnings triggered during the execution of the given function. # Example Usage: ```python import warnings # Example function to issue a DeprecationWarning def deprecated_function(): warnings.warn(\\"This function is deprecated\\", DeprecationWarning) # Issue a warning issue_warning(\\"This is a user warning\\", UserWarning) # Filter configuration to always show DeprecationWarnings filters = [ {\\"action\\": \\"always\\", \\"message\\": \\".*deprecated.*\\", \\"category\\": DeprecationWarning, \\"module\\": \\".*\\", \\"lineno\\": 0} ] # Apply filters filter_warnings(filters) # Capture warnings from calling the deprecated function captured_warnings = capture_warnings(deprecated_function) print(captured_warnings) # Output should include the deprecation warning message ```","solution":"import warnings def issue_warning(message: str, category: Warning) -> None: Issues a warning with the given message and category. Args: - message (str): the warning message to be issued. - category (Warning): the warning category (subclass of Warning). Returns: None warnings.warn(message, category) def filter_warnings(filters: list) -> None: Applies a list of warning filters. Args: - filters (list): A list of filter configurations. Each configuration is a dictionary with the following keys: - action (str): the filter action. - message (str): a regular expression to match the warning message. - category (Warning): the warning category (subclass of Warning). - module (str): a regular expression to match the module name. - lineno (int): the line number where the warning occurred. Returns: None for f in filters: warnings.filterwarnings(f[\'action\'], f[\'message\'], f[\'category\'], f[\'module\'], f[\'lineno\']) def capture_warnings(callable_func: callable, *args, **kwargs) -> list: Captures warnings generated by the given callable. Args: - callable_func (callable): the function that is expected to issue warnings. - args, kwargs: arguments to be passed to the callable. Returns: list: A list of captured warning messages. with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") callable_func(*args, **kwargs) return [str(warning.message) for warning in w]"},{"question":"You are tasked with developing a function to analyze a 3-dimensional signal using PyTorch\'s Fourier Transform capabilities. # Problem Statement: Write a function `analyze_signal` in PyTorch, which performs the following operations on a given 3-dimensional signal dataset: 1. Compute the N-dimensional discrete Fourier transform (DFT) using `torch.fft.fftn`. 2. Compute the frequency bins for each dimension using `torch.fft.fftfreq`. 3. Shift the zero-frequency component to the center of the spectrum using `torch.fft.fftshift`. 4. Compute the inverse N-dimensional DFT to approximate the original signal using `torch.fft.ifftn`. 5. Shift back the zero-frequency component to the original positions using `torch.fft.ifftshift`. The function should return a dictionary with the following keys: - `\\"frequency_domain\\"`: The transformed signal in the frequency domain. - `\\"frequency_bins\\"`: A list of tensors representing frequency bins for each dimension. - `\\"approx_time_domain\\"`: The approximated original signal (inverse transform result). # Function Signature: ```python def analyze_signal(signal: torch.Tensor) -> dict: # Your code here ``` # Input: - `signal`: A 3-dimensional PyTorch tensor of shape `(D1, D2, D3)` representing the input signal. # Output: - A dictionary with three keys: - `\\"frequency_domain\\"`: A PyTorch tensor of the same shape as the input, representing the frequency domain signal. - `\\"frequency_bins\\"`: A list of three PyTorch tensors, each representing the frequency bins for the respective dimensions. - `\\"approx_time_domain\\"`: A PyTorch tensor of the same shape as the input, representing the approximated original signal in the time domain. # Constraints: - The input signal can have arbitrary dimensions but will always be 3-dimensional. - Utilize the `torch.fft` module functionalities as specified. - Ensure that the original signal is well-approximated after the inverse transform (within a reasonable numerical threshold). # Example Usage: ```python import torch # Example input signal signal = torch.randn(8, 8, 8) # Analyze the signal result = analyze_signal(signal) print(\\"Frequency Domain:\\", result[\\"frequency_domain\\"]) print(\\"Frequency Bins:\\", result[\\"frequency_bins\\"]) print(\\"Approx Time Domain:\\", result[\\"approx_time_domain\\"]) ``` Ensure your solution is efficient and leverages the provided PyTorch FFT module functions appropriately.","solution":"import torch def analyze_signal(signal: torch.Tensor) -> dict: Analyzes a given 3-dimensional signal using PyTorch\'s Fourier Transform capabilities. Parameters: signal (torch.Tensor): A 3-dimensional tensor representing the input signal. Returns: dict: A dictionary containing: - \\"frequency_domain\\": The signal in the frequency domain. - \\"frequency_bins\\": The frequency bins for each dimension. - \\"approx_time_domain\\": The approximated original signal from the inverse transform. # Compute the N-dimensional discrete Fourier transform (DFT) frequency_domain = torch.fft.fftn(signal) # Compute the frequency bins for each dimension shape = signal.shape frequency_bins = [torch.fft.fftfreq(n) for n in shape] # Shift the zero-frequency component to the center of the spectrum freq_domain_shifted = torch.fft.fftshift(frequency_domain) # Compute the inverse N-dimensional DFT to approximate the original signal inv_freq_domain_shifted = torch.fft.ifftshift(freq_domain_shifted) approx_time_domain = torch.fft.ifftn(inv_freq_domain_shifted).real return { \\"frequency_domain\\": freq_domain_shifted, \\"frequency_bins\\": frequency_bins, \\"approx_time_domain\\": approx_time_domain }"},{"question":"You have been given the task to visualize the performance of a machine learning model. To make the visualization more insightful and aesthetically pleasing, you need to create a custom color palette using seaborn\'s `cubehelix_palette` function. Implement a function `custom_cubehelix_palette` that meets the following specifications: # Function Signature ```python def custom_cubehelix_palette(n_colors: int, start: float, rot: float, gamma: float, hue: float, dark: float, light: float, reverse: bool, as_cmap: bool) -> sns.palettes._ColorPalettes: pass ``` # Parameters - `n_colors` (int): The number of colors in the palette. Must be a positive integer. - `start` (float): The starting point of the cubehelix. Usually between 0 and 3. - `rot` (float): The amount of rotation in the helix. Can be positive or negative to reverse the direction. - `gamma` (float): The gamma correction factor. Affects the luminance ramp. - `hue` (float): The saturation of the colors in the palette. Must be between 0 and 1. - `dark` (float): The luminance of the darkest color. Usually between 0 and 1. - `light` (float): The luminance of the lightest color. Usually between 0 and 1. - `reverse` (bool): Whether to reverse the direction of the luminance ramp. - `as_cmap` (bool): Whether to return a colormap object. # Returns - `palette` (sns.palettes._ColorPalettes): Returns a seaborn color palette or colormap object based on the `as_cmap` flag. # Constraints 1. The parameters must adhere to the acceptable ranges and types as described. 2. The function should return a valid seaborn color palette or colormap object. 3. Ensure all edge cases are handled, such as invalid parameter values (e.g., negative `n_colors`). # Example Usage ```python import seaborn as sns import matplotlib.pyplot as plt palette = custom_cubehelix_palette(n_colors=8, start=0, rot=0.4, gamma=1.0, hue=0.8, dark=0.2, light=0.8, reverse=False, as_cmap=False) sns.palplot(palette) cmap = custom_cubehelix_palette(n_colors=8, start=0, rot=0.4, gamma=1.0, hue=0.8, dark=0.2, light=0.8, reverse=False, as_cmap=True) sns.heatmap(data=[[1,2],[3,4]], cmap=cmap) plt.show() ``` Your task is to implement the `custom_cubehelix_palette` function that generates and returns the desired color palette or colormap, as demonstrated in the examples. Ensure to thoroughly test your function with various inputs to validate its correctness.","solution":"import seaborn as sns def custom_cubehelix_palette(n_colors: int, start: float, rot: float, gamma: float, hue: float, dark: float, light: float, reverse: bool, as_cmap: bool): Create a custom cubehelix palette using seaborn\'s cubehelix_palette function. Parameters: - n_colors (int): The number of colors in the palette. Must be a positive integer. - start (float): The starting point of the cubehelix. Usually between 0 and 3. - rot (float): The amount of rotation in the helix. Can be positive or negative to reverse the direction. - gamma (float): The gamma correction factor. Affects the luminance ramp. - hue (float): The saturation of the colors in the palette. Must be between 0 and 1. - dark (float): The luminance of the darkest color. Usually between 0 and 1. - light (float): The luminance of the lightest color. Usually between 0 and 1. - reverse (bool): Whether to reverse the direction of the luminance ramp. - as_cmap (bool): Whether to return a colormap object. Returns: - palette (sns.palettes._ColorPalettes): Returns a seaborn color palette or colormap object based on the as_cmap flag. if not isinstance(n_colors, int) or n_colors <= 0: raise ValueError(\\"n_colors must be a positive integer.\\") if not (0 <= hue <= 1): raise ValueError(\\"hue must be between 0 and 1.\\") if not (0 <= dark <= 1): raise ValueError(\\"dark must be between 0 and 1.\\") if not (0 <= light <= 1): raise ValueError(\\"light must be between 0 and 1.\\") palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse, as_cmap=as_cmap) return palette"},{"question":"**Title:** Analyzing and Visualizing Sales Data using Seaborn You have been provided with a dataset that contains information regarding a company\'s sales performance over multiple years across different regions. Your task is to analyze this dataset and create visualizations that help in understanding the sales trends and relationships between the various variables using the seaborn library. **Dataset Description:** - `sales_data.csv`: This file contains the following columns: - `date`: The date of the sales record. - `sales`: The sales amount. - `region`: The region where the sale was made. - `product_type`: The type of product sold. - `units_sold`: The number of units sold. **Tasks:** 1. **Data Preparation:** - Load the `sales_data.csv` dataset into a pandas DataFrame. - Convert the `date` column to a datetime data type. - Extract the year from the `date` column and create a new column `year`. 2. **Sales Relationship Visualization:** - Create a scatter plot to analyze the relationship between the number of units sold (`units_sold`) and the sales amount (`sales`). Use different colors to distinguish between product types (`product_type`). - Modify the scatter plot to additionally use different markers to distinguish between regions (`region`). 3. **Sales Trend Visualization:** - Create a line plot to visualize the trend of total sales over the years in each region. - Customize the plot to show different lines for each region (`region`) with different line styles and colors. 4. **Sales and Units Sold Analysis:** - Create a faceted scatter plot to visualize the relationship between `units_sold` and `sales`, this time faceting the plots by `region` and `product_type`. 5. **Summary:** - Provide a brief summary (as a comment in the code) highlighting any patterns, trends, or insights you observed from the visualizations. **Constraints:** - Use suitable seaborn functions (`relplot`, `scatterplot`, `lineplot`) to achieve the task. - Ensure the visualizations are clear and informative, following best practices for data visualization. **Expected Output:** - A Python script (.py) or Jupyter notebook (.ipynb) containing the code for the above tasks, along with the visualizations and a brief summary of insights. **Example Input:** ```python import pandas as pd data = pd.read_csv(\'sales_data.csv\') print(data.head()) ``` ```plaintext date sales region product_type units_sold 0 2021-01-01 10000 North Gadget 50 1 2021-01-02 15000 South Device 30 2 2021-01-03 7000 East Gadget 20 3 2021-01-04 20000 West Appliance 80 4 2021-01-05 12000 North Device 40 ``` Please ensure your code is well-commented and follows standard Python best practices.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_prepare_data(filepath): Load and prepare the sales data for analysis and visualization. Parameters: filepath (str): The path to the sales data CSV file. Returns: pd.DataFrame: A pandas DataFrame with the prepared data. # Load the dataset data = pd.read_csv(filepath) # Convert \'date\' column to datetime type data[\'date\'] = pd.to_datetime(data[\'date\']) # Extract the year from \'date\' and create a new \'year\' column data[\'year\'] = data[\'date\'].dt.year return data def create_scatter_plot_units_sold_vs_sales(data): Create a scatter plot to analyze the relationship between units sold and sales amount. Parameters: data (pd.DataFrame): The prepared sales data. plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'units_sold\', y=\'sales\', hue=\'product_type\', style=\'region\', data=data) plt.title(\'Scatter Plot of Sales vs. Units Sold\') plt.xlabel(\'Units Sold\') plt.ylabel(\'Sales Amount\') plt.legend(title=\'Product Type and Region\') plt.show() def create_line_plot_sales_trend(data): Create a line plot to visualize the trend of total sales over the years in each region. Parameters: data (pd.DataFrame): The prepared sales data. data[\'total_sales\'] = data.groupby([\'year\', \'region\'])[\'sales\'].transform(\'sum\') plt.figure(figsize=(10, 6)) sns.lineplot(x=\'year\', y=\'total_sales\', hue=\'region\', data=data, marker=\'o\') plt.title(\'Sales Trend Over the Years by Region\') plt.xlabel(\'Year\') plt.ylabel(\'Total Sales\') plt.legend(title=\'Region\') plt.show() def create_facet_scatter_plot(data): Create a faceted scatter plot to analyze the relationship between units sold and sales by region and product type. Parameters: data (pd.DataFrame): The prepared sales data. g = sns.FacetGrid(data, col=\'region\', row=\'product_type\', margin_titles=True) g.map_dataframe(sns.scatterplot, x=\'units_sold\', y=\'sales\') g.set_axis_labels(\'Units Sold\', \'Sales Amount\') g.fig.suptitle(\'Scatter Plot of Units Sold vs. Sales by Region and Product Type\', y=1.05) plt.show() # Summary (as a comment): # The scatter plot revealed a positive correlation between the number of units sold and the sales amount. # Different regions and product types show varying intensities of this relationship. # The line plot highlighted noticeable year-to-year trends in sales among different regions. # Faceted scatter plots allowed detailed examination of how the relationship between units sold and sales varied across regions and product types."},{"question":"# Asynchronous Task Management Across Platforms Due to the differences in how different platforms handle asynchronous I/O, particularly with Python\'s `asyncio` module, it is essential to write code that adapts to these variations. This task requires you to implement an asynchronous function that works correctly on both Windows and macOS, taking into account the constraints provided in the documentation. Task: Write an asynchronous Python function to perform a network I/O operation that adapts to the constraints of both `SelectorEventLoop` and `ProactorEventLoop`. Function Signature: ```python import asyncio async def platform_aware_network_operation(): ``` Requirements: 1. **Windows Specifics**: - Use `ProactorEventLoop` by default, which supports subprocesses. - Ensure that methods unsupported by the event loop (e.g., `loop.add_writer()`) are not used. - Demonstrate how you would handle platform checks and fallbacks. 2. **macOS Specifics**: - For macOS versions ≤ 10.8, manually configure the event loop to use `SelectSelector` or `PollSelector` to support character devices. - Ensure your solution is compatible with modern macOS versions without additional configuration. 3. **General Requirements**: - Implement a simple TCP echo client as the network operation, which connects to a given server, sends a message, and receives a response. - Ensure the function gracefully handles and logs exceptions. - Your solution should be explicitly aware of the platform it is running on and adapt the event loop configuration accordingly. Input and Output: - **Input**: None. - **Output**: None. The function should print the received response from the server. Hint: Use the `platform` module to detect the operating system and conditionally set the event loop. Below is a skeleton to get you started: ```python import asyncio import platform import selectors async def echo_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() async def platform_aware_network_operation(): os_name = platform.system() if os_name == \'Windows\': # On Windows, use ProactorEventLoop if hasattr(asyncio, \'ProactorEventLoop\'): loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) elif os_name == \'Darwin\': # For older versions of macOS, configure selector if platform.mac_ver()[0] in [\'10.6\', \'10.7\', \'10.8\']: selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) try: await echo_client(\\"Hello, World!\\") except Exception as e: print(f\'An error occurred: {e}\') if __name__ == \'__main__\': asyncio.run(platform_aware_network_operation()) ``` **Note**: The provided skeleton sets up the basic structure for the platform-specific event loop management. Your implementation should finalize the logic to ensure it functions correctly under each specified platform\'s constraints.","solution":"import asyncio import platform import selectors async def echo_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() async def platform_aware_network_operation(): os_name = platform.system() if os_name == \'Windows\': # On Windows, use ProactorEventLoop if hasattr(asyncio, \'ProactorEventLoop\'): loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) elif os_name == \'Darwin\': # For older versions of macOS, configure selector if platform.mac_ver()[0] in [\'10.6\', \'10.7\', \'10.8\']: selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) try: await echo_client(\\"Hello, World!\\") except Exception as e: print(f\'An error occurred: {e}\') if __name__ == \'__main__\': asyncio.run(platform_aware_network_operation())"},{"question":"You have been provided with documentation on how to use the `asyncio` package\'s streaming capabilities to handle network connections in Python. Using this knowledge, you are tasked with implementing a mini chat server and client using asyncio streams. # Part 1: Chat Server Your first task is to implement the chat server. The server should accept multiple clients, broadcast messages received from any client to all connected clients, and handle client disconnections gracefully. **Requirements**: 1. Implement a function `start_chat_server(host, port)` that starts the chat server. 2. Use `asyncio.start_server` to accept connections. 3. For each connection, create a task to handle communication with that client. 4. When a message is received from a client, broadcast it to all connected clients. 5. Ensure the server handles client disconnections and removes clients from the broadcast list when they disconnect. ```python import asyncio async def start_chat_server(host: str, port: int): # Implement the function to start the chat server here pass # You can run your server locally for testing # asyncio.run(start_chat_server(\'127.0.0.1\', 8888)) ``` # Part 2: Chat Client Your second task is to implement the chat client. The client should connect to the chat server, send messages typed by the user to the server, and display messages broadcasted by the server. **Requirements**: 1. Implement a function `start_chat_client(host, port)` that starts the chat client. 2. Use `asyncio.open_connection` to connect to the server. 3. Create tasks to handle sending messages from the user to the server and receiving messages from the server. 4. Use `asyncio.create_task` or similar to manage these concurrent tasks. ```python import asyncio async def start_chat_client(host: str, port: int): # Implement the function to start the chat client here pass # You can run your client to connect to the server for testing # asyncio.run(start_chat_client(\'127.0.0.1\', 8888)) ``` # Example Usage 1. Run the server: ```bash python -c \'import asyncio; from your_module import start_chat_server; asyncio.run(start_chat_server(\\"127.0.0.1\\", 8888))\' ``` 2. Run multiple clients: ```bash python -c \'import asyncio; from your_module import start_chat_client; asyncio.run(start_chat_client(\\"127.0.0.1\\", 8888))\' ``` # Evaluation Criteria 1. **Correctness**: The server must correctly accept multiple clients, broadcast messages, and handle disconnections. The client must connect to the server, send, and receive messages appropriately. 2. **Concurrency**: Proper use of `asyncio` to manage asynchronous operations without blocking. 3. **Error Handling**: Graceful handling of common errors like network interruptions and client disconnections. 4. **Code Quality**: Code should be clean, well-documented, and follow Python best practices.","solution":"import asyncio active_clients = [] async def broadcast_message(message, writer): for client in active_clients: if client != writer: client.write(message) await client.drain() async def handle_client(reader, writer): try: active_clients.append(writer) while True: data = await reader.read(100) if not data: break message = data.decode() await broadcast_message(data, writer) except asyncio.CancelledError: pass finally: active_clients.remove(writer) writer.close() await writer.wait_closed() async def start_chat_server(host: str, port: int): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() async def send_message(writer, username): loop = asyncio.get_event_loop() while True: message = await loop.run_in_executor(None, input) message = f\\"{username}: {message}n\\" writer.write(message.encode()) await writer.drain() async def receive_message(reader): while True: data = await reader.read(100) if not data: break message = data.decode() print(message) async def start_chat_client(host: str, port: int): reader, writer = await asyncio.open_connection(host, port) username = input(\\"Enter your username: \\") send_task = asyncio.create_task(send_message(writer, username)) recv_task = asyncio.create_task(receive_message(reader)) try: await asyncio.gather(send_task, recv_task) except asyncio.CancelledError: pass finally: writer.close() await writer.wait_closed()"},{"question":"**Objective:** Demonstrate your understanding of the `fileinput` module by implementing a function that reads multiple files, processes their lines, and handles different encodings and file modes. **Problem Statement:** You are given a list of files containing text data. Implement a function `process_files` that reads these files line by line using the `fileinput` module. The function should process each line by converting it to uppercase and writing it back to the same file. Additionally, it should handle files with different encodings. **Function Signature:** ```python def process_files(file_list: list[str], encoding: str = \\"utf-8\\") -> None: # Your code here ``` **Input:** - `file_list`: A list of strings where each string is a file path. - `encoding`: An optional string specifying the encoding to be used (default is `\\"utf-8\\"`). **Output:** - The function does not return anything. Instead, it rewrites each file in `file_list` such that all lines in the files are converted to uppercase. **Constraints:** - Each file in `file_list` contains valid text data encoded in the specified encoding or the default encoding (`utf-8`). - The function should handle potential I/O errors gracefully. - The function should use the `fileinput` module for reading the lines from the files. **Example Usage:** ```python file_list = [\\"file1.txt\\", \\"file2.txt\\"] process_files(file_list, encoding=\\"utf-8\\") ``` After execution, all lines in `file1.txt` and `file2.txt` should be converted to uppercase. **Guidelines:** 1. Use the `fileinput.input` with the context manager for reading lines. 2. Ensure the function works with multiple files, processing each line and writing it back to the same file. 3. Use appropriate error handling for file operations and encoding issues. 4. Test the function with various encodings and file contents to ensure correctness. **Additional Notes:** - You may assume all files in `file_list` are accessible and writable. - Consider the performance of your implementation, especially with large files.","solution":"import fileinput import os def process_files(file_list: list[str], encoding: str = \\"utf-8\\") -> None: Reads a list of files, processes their lines to convert them to uppercase, and writes the changes back to the same files. :param file_list: List of file paths to process :param encoding: Encoding to use for reading and writing files (default is \\"utf-8\\") for file_path in file_list: try: # Read and process lines with fileinput.input(files=[file_path], inplace=True, encoding=encoding) as file: for line in file: print(line.upper(), end=\'\') except (OSError, IOError, UnicodeError) as e: print(f\\"An error occurred while processing {file_path}: {e}\\")"},{"question":"**Coding Assessment Question:** You are given a sequence of filenames and a list of patterns. Your task is to implement a function `match_filenames(filenames: List[str], patterns: List[str]) -> Dict[str, List[str]]` that groups the filenames according to the specified patterns using Unix shell-style wildcards. # Function Signature: ```python def match_filenames(filenames: List[str], patterns: List[str]) -> Dict[str, List[str]]: ``` # Input: - `filenames`: A list of strings where each string is a filename. - `patterns`: A list of strings where each string is a pattern with Unix shell-style wildcards. # Output: - Returns a dictionary where each key is a pattern and the value is a list of filenames matching that pattern. # Constraints: - Each filename and pattern consists of uppercase/lowercase English letters, digits, and characters like `-`, `_`, `.`, `/`. - The number of filenames will not exceed 10^4. - The number of patterns will not exceed 50. - Each filename\'s length will not exceed 100 characters. - Each pattern\'s length will not exceed 100 characters. # Examples: ```python filenames = [\'example.txt\', \'sample.txt\', \'testfile.py\', \'script.sh\', \'data.csv\'] patterns = [\'*.txt\', \'*.py\', \'*.sh\', \'*data*\'] result = match_filenames(filenames, patterns) # Expected Output: # { # \'*.txt\': [\'example.txt\', \'sample.txt\'], # \'*.py\': [\'testfile.py\'], # \'*.sh\': [\'script.sh\'], # \'*data*\': [\'data.csv\'] # } ``` # Note: - Use the `fnmatch.filter` function to perform the pattern matching efficiently as discussed in the documentation. - Make sure your implementation handles both case-sensitive and case-insensitive patterns appropriately. # Additional Performance Requirements: - The implementation should be optimized to handle the constraints efficiently and reduce unnecessary computations.","solution":"import fnmatch from typing import List, Dict def match_filenames(filenames: List[str], patterns: List[str]) -> Dict[str, List[str]]: Groups filenames according to the specified patterns using Unix shell-style wildcards. Args: filenames (List[str]): A list of strings where each string is a filename. patterns (List[str]): A list of strings where each string is a pattern with Unix shell-style wildcards. Returns: Dict[str, List[str]]: A dictionary where each key is a pattern and the value is a list of filenames matching that pattern. result = {} for pattern in patterns: matched_files = fnmatch.filter(filenames, pattern) result[pattern] = matched_files return result"},{"question":"In this task, you will implement a simple distributed training scenario using PyTorch where you need to handle uneven inputs across multiple nodes. This scenario will involve the classes `Join`, `Joinable`, and `JoinHook`. # Problem Statement You are given a PyTorch model and a dataset that is distributed unevenly across multiple nodes. Write a function `perform_distributed_training` that uses a generic join context manager to handle the distributed training with uneven inputs. Function Signature ```python def perform_distributed_training(model: torch.nn.Module, datasets: List[torch.utils.data.Dataset], rank: int, world_size: int) -> None: pass ``` # Input - `model` (torch.nn.Module): The PyTorch model to be trained. - `datasets` (List[torch.utils.data.Dataset]): A list of datasets, where each dataset corresponds to the data available on a specific node. - `rank` (int): The rank of the current process. This is an integer from 0 to `world_size - 1` that uniquely identifies each process. - `world_size` (int): The total number of processes involved in the training. # Output The function should not return anything but should perform training on the provided model using the given datasets, considering the uneven split of datasets across nodes. # Constraints - You must use the `Join`, `Joinable`, and `JoinHook` classes to handle the unevenness in input data across nodes. - Ensure proper synchronization and handling of termination conditions when some nodes might finish processing their data earlier than others. # Example ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, Dataset from typing import List class SimpleDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index] class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def perform_distributed_training(model: torch.nn.Module, datasets: List[torch.utils.data.Dataset], rank: int, world_size: int) -> None: # Implement the function here pass # Example usage datasets = [SimpleDataset([torch.randn(10) for _ in range(size)]) for size in [100, 200, 150]] model = SimpleModel() rank = 0 # Example rank world_size = 3 # Total number of nodes perform_distributed_training(model, datasets, rank, world_size) ``` Note: You can simulate the distributed environment, but the focus should be on correctly using the `Join`, `Joinable`, and `JoinHook` classes.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.utils.data import DataLoader, Dataset from typing import List class SimpleDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index] class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def perform_distributed_training(model: torch.nn.Module, datasets: List[torch.utils.data.Dataset], rank: int, world_size: int) -> None: # Initialize the process group dist.init_process_group(backend=\'gloo\', rank=rank, world_size=world_size) # Prepare the data loader for the current process dataset = datasets[rank] dataloader = DataLoader(dataset, batch_size=32, shuffle=True) # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Create a Joinable context for the training process class TrainingLoopHook(dist.JoinHook): def main_hook(self, *args, **kwargs): for i, data in enumerate(dataloader): optimizer.zero_grad() outputs = model(data) targets = torch.zeros_like(outputs) # For simplicity, assuming targets are zero loss = criterion(outputs, targets) loss.backward() optimizer.step() # Wrap training loop in dist.Join context with dist.Join([TrainingLoopHook(rank)]): pass # Cleanup the process group dist.destroy_process_group()"},{"question":"Python Sequence Operations Objective: Implement a class `CustomSequence` that illustrates various sequence operations. This class should demonstrate a deep understanding of sequence protocols, akin to those described in the documentation. Requirements: 1. **Constructor:** - Initialize the sequence with an iterable of elements. 2. **Methods:** - `__len__(self)`: - Return the length of the sequence (`len()` equivalent). - `__getitem__(self, index)`: - Return the item at the given index. Support both positive and negative indexing. - `__setitem__(self, index, value)`: - Set the item at the given index to a new value. - `__delitem__(self, index)`: - Delete the item at the given index. - `__contains__(self, value)`: - Check if the sequence contains a given value (`in` operator equivalent). - `__add__(self, other)`: - Return the concatenation of the sequence with another sequence (`+` operator equivalent). - `__mul__(self, count)`: - Return the sequence repeated `count` times (`*` operator equivalent). - `__slice__(self, start, end)`: - Return a new sequence that is a slice from `start` to `end` (`o[start:end]` equivalent). - `count(self, value)`: - Return the count of occurrences of the given value in the sequence. - `index(self, value)`: - Return the index of the first occurrence of the given value in the sequence. 3. **Constraints:** - The sequence will only contain elements that are integers. - Raises `IndexError` for invalid indices. - Raises `ValueError` for invalid operations that require a correct value presence (like `index()`). 4. **Performance Considerations:** - Ensure that all operations are efficient with respect to their expected behavior. Avoid redundant operations on the sequence. Example: ```python class CustomSequence: def __init__(self, iterable): self.sequence = list(iterable) def __len__(self): return len(self.sequence) def __getitem__(self, index): return self.sequence[index] def __setitem__(self, index, value): self.sequence[index] = value def __delitem__(self, index): del self.sequence[index] def __contains__(self, value): return value in self.sequence def __add__(self, other): return CustomSequence(self.sequence + other.sequence) def __mul__(self, count): return CustomSequence(self.sequence * count) def __slice__(self, start, end): return CustomSequence(self.sequence[start:end]) def count(self, value): return self.sequence.count(value) def index(self, value): return self.sequence.index(value) # Usage example seq = CustomSequence([1, 2, 3, 4]) print(len(seq)) # Output: 4 print(seq[2]) # Output: 3 seq[2] = 10 print(seq[2]) # Output: 10 del seq[2] print(3 in seq) # Output: False new_seq = seq + CustomSequence([5, 6]) print(new_seq * 2) # Output: CustomSequence([1, 2, 4, 5, 6, 1, 2, 4, 5, 6]) print(seq.__slice__(1, 3)) # Output: CustomSequence([2, 4]) print(seq.count(2)) # Output: 1 print(seq.index(2)) # Output: 1 ``` Constraints: - Do not use in-built or external sequence manipulation libraries except basic list operations. - Ensure comprehensive test cases to cover all methods.","solution":"class CustomSequence: def __init__(self, iterable): self.sequence = list(iterable) def __len__(self): return len(self.sequence) def __getitem__(self, index): if index < 0: index = len(self.sequence) + index if index < 0 or index >= len(self.sequence): raise IndexError(\\"Index out of range\\") return self.sequence[index] def __setitem__(self, index, value): if index < 0: index = len(self.sequence) + index if index < 0 or index >= len(self.sequence): raise IndexError(\\"Index out of range\\") self.sequence[index] = value def __delitem__(self, index): if index < 0: index = len(self.sequence) + index if index < 0 or index >= len(self.sequence): raise IndexError(\\"Index out of range\\") del self.sequence[index] def __contains__(self, value): return value in self.sequence def __add__(self, other): if not isinstance(other, CustomSequence): raise TypeError(\\"Can only concatenate with another CustomSequence\\") return CustomSequence(self.sequence + other.sequence) def __mul__(self, count): return CustomSequence(self.sequence * count) def __slice__(self, start, end): return CustomSequence(self.sequence[start:end]) def count(self, value): return self.sequence.count(value) def index(self, value): if value not in self.sequence: raise ValueError(f\\"{value} is not in sequence\\") return self.sequence.index(value)"},{"question":"# Advanced Python Coding Assessment: Struct Module Objective: Demonstrate your understanding of the `struct` module by implementing functions to handle packed binary data, managing both packing and unpacking processes with considerations for byte order, size, and alignment. Problem Statement: 1. **Packing Data:** Implement a function `pack_data` that takes a format string and a list of values to pack into a binary format. The function should handle standard size and byte orders based on the format string provided, and return the packed binary data. ```python def pack_data(format_string: str, values: list) -> bytes: Pack the list of values according to the format string into a binary format. Parameters: - format_string (str): The format string specifying the data layout. - values (list): A list of values to pack into binary data. Returns: - bytes: The packed binary data. pass ``` 2. **Unpacking Data:** Implement a function `unpack_data` that takes a format string and a binary buffer, and unpacks the data into its original values based on the format string. ```python def unpack_data(format_string: str, buffer: bytes) -> tuple: Unpack the binary buffer according to the format string into the corresponding values. Parameters: - format_string (str): The format string specifying the data layout. - buffer (bytes): The binary data to unpack. Returns: - tuple: The unpacked values. pass ``` 3. **Handling Native Structures:** Implement a class `NativeDataHandler` that uses the `Struct` class for packing and unpacking binary data. The class should be initialized with a format string and provide methods to pack and unpack data using the compiled struct for efficiency. ```python class NativeDataHandler: def __init__(self, format_string: str): Initialize the NativeDataHandler with a format string. Parameters: - format_string (str): The format string for the binary data. pass def pack(self, *values) -> bytes: Pack the values into binary data using the compiled format. Parameters: - values: The values to pack. Returns: - bytes: The packed binary data. pass def unpack(self, buffer: bytes) -> tuple: Unpack the binary data using the compiled format. Parameters: - buffer (bytes): The binary data to unpack. Returns: - tuple: The unpacked values. pass ``` Input and Output Formats 1. **pack_data Function:** - **Input:** - `format_string`: \\"hhb\\" - `values`: [1, 2, 3] - **Output:** b\'x01x00x02x00x03\' 2. **unpack_data Function:** - **Input:** - `format_string`: \\"hhb\\" - `buffer`: b\'x01x00x02x00x03\' - **Output:** (1, 2, 3) 3. **NativeDataHandler Class:** - **Pack Method Example:** ```python handler = NativeDataHandler(\'hhb\') packed_data = handler.pack(1, 2, 3) # packed_data should be b\'x01x00x02x00x03\' ``` - **Unpack Method Example:** ```python handler = NativeDataHandler(\'hhb\') unpacked_data = handler.unpack(b\'x01x00x02x00x03\') # unpacked_data should be (1, 2, 3) ``` Constraints: - The byte order can be either native, little-endian, or big-endian, as specified by the format string. - Input values should match the format string provided. - Raise a suitable exception if the data size mismatches or if the values are out of the valid range. Performance Requirements: - The implementation should handle typical pack/unpack operations efficiently. - Utilization of the `Struct` class for managing repetitive packing/unpacking operations is encouraged for performance optimization.","solution":"import struct def pack_data(format_string: str, values: list) -> bytes: Pack the list of values according to the format string into a binary format. Parameters: - format_string (str): The format string specifying the data layout. - values (list): A list of values to pack into binary data. Returns: - bytes: The packed binary data. return struct.pack(format_string, *values) def unpack_data(format_string: str, buffer: bytes) -> tuple: Unpack the binary buffer according to the format string into the corresponding values. Parameters: - format_string (str): The format string specifying the data layout. - buffer (bytes): The binary data to unpack. Returns: - tuple: The unpacked values. return struct.unpack(format_string, buffer) class NativeDataHandler: def __init__(self, format_string: str): Initialize the NativeDataHandler with a format string. Parameters: - format_string (str): The format string for the binary data. self.struct = struct.Struct(format_string) def pack(self, *values) -> bytes: Pack the values into binary data using the compiled format. Parameters: - values: The values to pack. Returns: - bytes: The packed binary data. return self.struct.pack(*values) def unpack(self, buffer: bytes) -> tuple: Unpack the binary data using the compiled format. Parameters: - buffer (bytes): The binary data to unpack. Returns: - tuple: The unpacked values. return self.struct.unpack(buffer)"},{"question":"**Advanced Python Debugging Tool Implementation** # Background: You are required to implement a function that extracts and presents detailed information about the current execution frame\'s environment. This includes built-in functions, local variables, global variables, and the current line number of the executed frame. # Task: Implement a Python function `debug_current_frame()` that: 1. Retrieves and prints the list of names of all built-in functions available in the current execution frame. 2. Retrieves and prints all local variables present in the current execution frame. 3. Retrieves and prints all global variables present in the current execution frame. 4. Retrieves and prints the line number currently being executed in the current frame. # Function Signature: ```python def debug_current_frame() -> None: pass ``` # Expected Output: The function should print: 1. A header `Built-in Functions:` followed by a list of all built-in function names. 2. A header `Local Variables:` followed by a dictionary of all local variable names and their values. 3. A header `Global Variables:` followed by a dictionary of all global variable names and their values. 4. A header `Current Line Number:` followed by the current line number being executed. # Constraints: - Your implementation must properly handle cases where there is no current executing frame. - Use the provided functions to retrieve the necessary information about the current frame. # Example Usage: ```python def sample_code(): x = 10 y = 20 debug_current_frame() sample_code() ``` Expected output (format may vary): ``` Built-in Functions: [\'abs\', \'all\', \'any\', ..., \'zip\'] Local Variables: {\'x\': 10, \'y\': 20} Global Variables: {\'__name__\': \'__main__\', ...} Current Line Number: 5 ``` Implement the `debug_current_frame()` function to fulfill the requirements specified above. Ensure to handle potential edge cases and format the output clearly for ease of understanding.","solution":"import sys def debug_current_frame(): Prints detailed information about the current execution frame: built-in functions, local variables, global variables, and the current line number. frame = sys._getframe(1) # Retrieve and print built-in functions builtins = dir(__builtins__) print(\\"Built-in Functions:\\") print(builtins) # Retrieve and print local variables local_vars = frame.f_locals print(\\"nLocal Variables:\\") print(local_vars) # Retrieve and print global variables global_vars = frame.f_globals print(\\"nGlobal Variables:\\") print(global_vars) # Retrieve and print current line number current_line_no = frame.f_lineno print(\\"nCurrent Line Number:\\") print(current_line_no)"},{"question":"# Comprehensive Pathname Manipulation Challenge You are given a task to process a collection of file paths spread across various directories. Your goal is to filter, manipulate, and derive information from these paths using the **os.path** module. Implement the following functions: 1. **normalize_paths(paths: list, base_directory: str) -> list** - **Input:** - `paths`: List of path strings (both absolute and relative). - `base_directory`: A string representing the base directory. - **Output:** - Returns a list of normalized absolute paths relative to the `base_directory`. - **Constraints:** - If a path is relative, join it with the `base_directory`. - Ensure all paths are normalized (redundant separators and up-level references are collapsed). 2. **filter_existing_paths(normalized_paths: list) -> list** - **Input:** - `normalized_paths`: List of normalized absolute path strings. - **Output:** - Returns a list of paths that exist on the file system. - **Constraints:** - Utilize appropriate file existence checks to filter paths. 3. **common_path(filtered_paths: list) -> str** - **Input:** - `filtered_paths`: List of filtered existing path strings. - **Output:** - Returns the longest common sub-path among the given paths. - **Constraints:** - Handle errors where paths might contain both absolute and relative paths. # Example ```python base_directory = \\"/project\\" paths = [\\"./src/file.py\\", \\"/usr/local/bin/script.sh\\", \\"../temp/data.txt\\"] normalized_paths = normalize_paths(paths, base_directory) # Output example: [\'/project/src/file.py\', \'/usr/local/bin/script.sh\', \'/temp/data.txt\'] filtered_paths = filter_existing_paths(normalized_paths) # Output example (depending on actual file system): [\'/project/src/file.py\', \'/usr/local/bin/script.sh\'] common_subpath = common_path(filtered_paths) # Output example: \'/\' or \'/usr\' ``` # Implementation Tips - **normalize_paths**: Use `os.path.abspath()` and `os.path.normpath()`. - **filter_existing_paths**: Use `os.path.exists()`. - **common_path**: Use `os.path.commonpath()`. # Requirements - Code should be clear, efficient, and handle edge cases. - Follow Pythonic conventions for importing modules. # Evaluation Criteria - Correctness: Output must meet the described functionality. - Code Quality: Readability, use of proper naming conventions, and comments where necessary. - Efficiency: Solutions should handle large lists of paths efficiently. Good luck!","solution":"import os def normalize_paths(paths, base_directory): Normalize paths to absolute paths relative to the base directory. :param paths: List of path strings (both absolute and relative). :param base_directory: A string representing the base directory. :return: List of normalized absolute paths. normalized_paths = [] for path in paths: if not os.path.isabs(path): path = os.path.join(base_directory, path) normalized_paths.append(os.path.abspath(os.path.normpath(path))) return normalized_paths def filter_existing_paths(normalized_paths): Filter a list of paths to only include those that exist on the file system. :param normalized_paths: List of normalized absolute path strings. :return: List of existing paths. return [path for path in normalized_paths if os.path.exists(path)] def common_path(filtered_paths): Find the longest common sub-path among the given paths. :param filtered_paths: List of filtered existing path strings. :return: The longest common sub-path. if not filtered_paths: return \\"\\" return os.path.commonpath(filtered_paths)"},{"question":"You are tasked with writing a function that organizes files into a structured directory based on their file extensions. The function should scan a provided directory, create subdirectories for each file type, and move the files into the corresponding subdirectories. # Task Implement a function `organize_files_by_extension(directory_path: str) -> None` that takes a path to a directory and organizes the files in that directory by their extensions. # Function Signature ```python def organize_files_by_extension(directory_path: str) -> None: ``` # Input - `directory_path` (str): A path to the directory to organize. # Output - None. (The function should modify the filesystem directly by creating directories and moving files.) # Example Suppose the directory contains the following files: ``` example_dir/ file1.txt file2.jpg file3.txt file4.png file5.jpg ``` After calling `organize_files_by_extension(\\"example_dir\\")`, the directory should be organized as follows: ``` example_dir/ txt/ file1.txt file3.txt jpg/ file2.jpg file5.jpg png/ file4.png ``` # Requirements 1. Create a subdirectory for each file extension found in the directory. 2. Move each file into the corresponding subdirectory. 3. Skip directories and only process files. 4. Handle any errors that might occur during the process gracefully, such as permission errors or files that cannot be moved. # Constraints - You may assume that the input directory path is valid and accessible. - You may use any function or class from the `pathlib` module. # Hints - Use `Path.iterdir()` to list directory contents. - Use `Path.suffix` to get the file extension. - Use methods such as `Path.mkdir()`, `Path.rename()`, and others to manage the filesystem. Good luck, and make sure to handle edge cases and validate your solution against different types of file structures!","solution":"import os from pathlib import Path def organize_files_by_extension(directory_path: str) -> None: Organizes files in the given directory by their file extensions. Creates subdirectories for each file type and moves files into them. Parameters: - directory_path (str): The path to the directory to organize. dir_path = Path(directory_path) if not dir_path.exists() or not dir_path.is_dir(): raise ValueError(f\\"The provided path {directory_path} is not a valid directory\\") for entry in dir_path.iterdir(): if entry.is_file(): file_extension = entry.suffix.lstrip(\\".\\").lower() if file_extension: target_dir = dir_path / file_extension if not target_dir.exists(): target_dir.mkdir() destination = target_dir / entry.name entry.rename(destination)"},{"question":"Objective: The objective of this task is to demonstrate your understanding of the `inspect` module in Python by creating a function that retrieves detailed information about another function\'s signature and its components, including the return annotation and default values for parameters. Problem Statement: You are to implement the function `detailed_inspect(func: Callable) -> str`. This function receives a callable object (typically a function) and returns a string with detailed information about its signature. The information should include: 1. The names and kinds of all parameters. 2. The default values for any parameters that have them (use a special marker if they don\'t). 3. The annotations for the parameters. 4. The return annotation of the function. **Function Signature:** ```python def detailed_inspect(func: Callable) -> str: pass ``` Detailed Requirements: 1. Use the `inspect.signature()` function to retrieve the function signature. 2. Iterate through the parameters of the function and gather: - Parameter name. - Parameter kind (e.g., POSITIONAL_ONLY, POSITIONAL_OR_KEYWORD, VAR_POSITIONAL, KEYWORD_ONLY, VAR_KEYWORD). - Default value or a placeholder if not set. - Annotation or a placeholder if not set. 3. Gather the return annotation of the function. 4. Format the information into a human-readable string as specified in the output format below. Expected Input: - `func`: A Python callable (e.g., function). Expected Output: A string containing detailed information about the function\'s signature. Example: ```python def example_function(a: int, b: \'float\' = 5.0, *args: \'varargs\', **kwargs: \'varkw\') -> bool: pass print(detailed_inspect(example_function)) ``` **Expected Output:** ``` Function: example_function a: POSITIONAL_OR_KEYWORD, No default, Annotation: int b: POSITIONAL_OR_KEYWORD, Default: 5.0, Annotation: float args: VAR_POSITIONAL, No default, Annotation: varargs kwargs: VAR_KEYWORD, No default, Annotation: varkw Return annotation: bool ``` Constraints: 1. Only use functionalities from the `inspect` module and standard Python libraries. 2. Handle cases where the function may not have annotations or default values. **Note**: This problem tests your ability to navigate and use the `inspect` module, particularly with function signatures and annotations. Good luck!","solution":"import inspect from typing import Callable def detailed_inspect(func: Callable) -> str: sig = inspect.signature(func) details = [f\\"Function: {func.__name__}\\"] for name, param in sig.parameters.items(): kind = param.kind default = param.default annotation = param.annotation if default is inspect.Parameter.empty: default_str = \\"No default\\" else: default_str = f\\"Default: {default}\\" if annotation is inspect.Parameter.empty: annotation_str = \\"No annotation\\" else: annotation_str = f\\"Annotation: {annotation}\\" details.append(f\\" {name}: {kind}, {default_str}, {annotation_str}\\") if sig.return_annotation is inspect.Signature.empty: return_annot_str = \\"No return annotation\\" else: return_annot_str = f\\"Return annotation: {sig.return_annotation}\\" details.append(return_annot_str) return \\"n\\".join(details)"},{"question":"# WAV File Manipulation Objective Write a Python function that reads a given WAV file, manipulates its audio data, and writes the modified data to a new WAV file. Specifically, your function should halve the amplitude of the audio samples, effectively reducing the volume by 50%. Function Signature ```python def reduce_volume(input_file: str, output_file: str) -> None: pass ``` Input - `input_file` (str): The path to the input WAV file in read mode `rb`. - `output_file` (str): The path to the output WAV file in write mode `wb`. Output - The function should write a new WAV file with the same parameters as the input file but with the audio data\'s amplitude reduced by half. Constraints - Assume input WAV files are in \\"WAVE_FORMAT_PCM\\". - Handle files with 1 (mono) or 2 (stereo) channels. - Sample widths can be 1, 2, or 4 bytes. Example Let\'s say `input_file` is a WAV file with audio data. The function should read the file, reduce the volume of the audio data, and save the modified data to `output_file`. Notes - You can utilize methods such as `wave.open()`, `Wave_read.getnchannels()`, `Wave_read.getsampwidth()`, `Wave_read.getframerate()`, `Wave_read.getnframes()`, `Wave_read.readframes()`, `Wave_write.setnchannels()`, `Wave_write.setsampwidth()`, `Wave_write.setframerate()`, and `Wave_write.writeframesraw()`. - Don\'t forget to handle the conversion of bytes to integer types appropriately based on sample width when reducing the volume, and revert it back to bytes before writing to the output file. - Ensure that the output file maintains the same sampling rate, number of channels, and other parameters as the input file. Implementation Notes - Open the input file in read mode `rb` to create a `Wave_read` object. - Read the necessary parameters using the appropriate methods. - Read the frames data and process the samples to reduce the amplitude. - Open the output file in write mode `wb`, set the parameters, and write the modified frames data. - Close both the input and output files properly. Example Implementation (Outline) ```python import wave def reduce_volume(input_file: str, output_file: str) -> None: with wave.open(input_file, \'rb\') as infile: params = infile.getparams() nchannels = infile.getnchannels() sampwidth = infile.getsampwidth() framerate = infile.getframerate() nframes = infile.getnframes() audio_data = infile.readframes(nframes) # Process audio_data to reduce volume by half # This will vary based on the sample width with wave.open(output_file, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(modified_audio_data) ``` Complete the function to handle the audio data processing for different `sampwidth`.","solution":"import wave import struct def reduce_volume(input_file: str, output_file: str) -> None: with wave.open(input_file, \'rb\') as infile: params = infile.getparams() nchannels = infile.getnchannels() sampwidth = infile.getsampwidth() framerate = infile.getframerate() nframes = infile.getnframes() audio_data = infile.readframes(nframes) # Process audio_data to reduce volume by half if sampwidth == 1: fmt = \\"{}B\\".format(nframes * nchannels) samples = struct.unpack(fmt, audio_data) new_samples = bytes(int(sample * 0.5) for sample in samples) elif sampwidth == 2: fmt = \\"{}h\\".format(nframes * nchannels) samples = struct.unpack(fmt, audio_data) new_samples = struct.pack(fmt, *(int(sample * 0.5) for sample in samples)) elif sampwidth == 4: fmt = \\"{}i\\".format(nframes * nchannels) samples = struct.unpack(fmt, audio_data) new_samples = struct.pack(fmt, *(int(sample * 0.5) for sample in samples)) else: raise ValueError(f\\"Unsupported sample width: {sampwidth}\\") with wave.open(output_file, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(new_samples)"},{"question":"You are provided with the `diamonds` dataset available from the seaborn library. Your task is to create a plot using `seaborn.objects` that visualizes the interquartile range (IQR) of the `carat` variable for each `clarity` group, differentiating the bars by the `cut` variable. Requirements: 1. **Dataset Load**: Load the `diamonds` dataset using `seaborn`. 2. **Plot Creation**: - Use `so.Plot` to create a bar plot. - Aggregate the `carat` variable by computing the interquartile range (IQR) for each `clarity` group. - Use different colors for different `cut` values, and dodge the bars accordingly. 3. **Visualization**: Your plot should clearly distinguish between different `clarity` groups and `cut` values. Input - There is no input for this specific task. You can directly load the `diamonds` dataset using seaborn. Output - Display the plot as specified. Constraints - You should use the seaborn `objects` module for this task. - Assume that seaborn and other necessary libraries are pre-installed. Example Here\'s an example of how the plot can be structured. Don\'t use the default settings blindly; ensure that all the details as per the requirements are met. ```python import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot p = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add( so.Bar(), so.Agg(lambda x: x.quantile(.75) - x.quantile(.25)), so.Dodge(), color=\\"cut\\" ) # Display the plot p.show() ``` Ensure that your plot accurately represents the IQR of `carat` for each `clarity` value with the `cut` variable differentiated by color.","solution":"import seaborn as sns import seaborn.objects as so # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the plot p = so.Plot(diamonds, x=\'clarity\', y=\'carat\').add( so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), so.Dodge(), color=\'cut\' ) # Display the plot p.show()"},{"question":"You are required to implement and test a function that processes sales data. The function should handle various error conditions gracefully and perform appropriate data manipulations. Function Implementation **Function Name:** `process_sales_data` **Parameters:** - `data` (pd.DataFrame): A DataFrame containing sales data with at least the following columns: \'Date\', \'Product\', \'Quantity\', \'Price\'. The \'Date\' column should be of datetime type, \'Product\' should be string, \'Quantity\' and \'Price\' should be numeric. **Returns:** - A processed DataFrame with a new column \'Total\' which is calculated as `Quantity * Price`. The DataFrame should be sorted by \'Date\' in ascending order. **Exceptions to Handle:** - Raise `errors.EmptyDataError` if the input DataFrame is empty. - Raise `errors.InvalidColumnName` if any of the required columns are missing. - Raise `errors.DataError` if \'Quantity\' or \'Price\' columns contain non-numeric values. - Raise `errors.OutOfBoundsDatetime` if the \'Date\' column contains out-of-bounds datetime values. Testing You also need to write a test suite for this function, covering all the specified conditions and using the pandas testing tools for assertions. **Test Suite Functions:** - `test_valid_input()`: Asserts that the function works correctly for valid input. - `test_empty_data()`: Asserts that the function raises `errors.EmptyDataError` for an empty DataFrame. - `test_missing_columns()`: Asserts that the function raises `errors.InvalidColumnName` for DataFrames missing one or more required columns. - `test_non_numeric_values()`: Asserts that the function raises `errors.DataError` for non-numeric values in \'Quantity\' or \'Price\'. - `test_out_of_bounds_datetime()`: Asserts that the function raises `errors.OutOfBoundsDatetime` for invalid \'Date\' values. You are encouraged to use `pandas.testing.assert_frame_equal` for comparing DataFrames in your tests. Here is a skeleton to get you started: ```python import pandas as pd from pandas import errors, testing def process_sales_data(data): # Implement the function here. pass # Test functions def test_valid_input(): # Write the test for valid input. pass def test_empty_data(): # Write the test for empty data. pass def test_missing_columns(): # Write the test for missing columns. pass def test_non_numeric_values(): # Write the test for non-numeric values. pass def test_out_of_bounds_datetime(): # Write the test for out-of-bounds datetime values. pass # You can run the tests to ensure your function works correctly. if __name__ == \\"__main__\\": test_valid_input() test_empty_data() test_missing_columns() test_non_numeric_values() test_out_of_bounds_datetime() print(\\"All tests passed.\\") ```","solution":"import pandas as pd from pandas import errors class CustomErrors: class EmptyDataError(Exception): pass class InvalidColumnName(Exception): pass class DataError(Exception): pass class OutOfBoundsDatetime(Exception): pass def process_sales_data(data): if data.empty: raise CustomErrors.EmptyDataError(\\"The input DataFrame is empty.\\") required_columns = [\'Date\', \'Product\', \'Quantity\', \'Price\'] if not all(column in data.columns for column in required_columns): raise CustomErrors.InvalidColumnName(\\"One or more required columns are missing.\\") try: data[\'Date\'] = pd.to_datetime(data[\'Date\']) except pd.errors.OutOfBoundsDatetime: raise CustomErrors.OutOfBoundsDatetime(\\"The \'Date\' column contains out-of-bounds datetime values.\\") if not pd.api.types.is_numeric_dtype(data[\'Quantity\']) or not pd.api.types.is_numeric_dtype(data[\'Price\']): raise CustomErrors.DataError(\\"\'Quantity\' or \'Price\' columns contain non-numeric values.\\") data[\'Total\'] = data[\'Quantity\'] * data[\'Price\'] data = data.sort_values(by=\'Date\').reset_index(drop=True) return data"},{"question":"Objective Create a custom command-line application using Python\'s `cmd` module that manages a simple inventory system for a store. Requirements 1. Create a subclass of `cmd.Cmd` named `InventoryShell`. 2. Implement the following `do_*` methods to handle inventory commands: - `do_add(name quantity)`: Adds a specified quantity of an item to the inventory. - `do_remove(name quantity)`: Removes a specified quantity of an item from the inventory. - `do_list()`: Lists all items in the inventory with their quantities. - `do_clear()`: Clears the entire inventory. - `do_exit()`: Exits the command interpreter. 3. The inventory should be stored as a dictionary where keys are item names and values are their quantities. 4. Implement the `do_help` method to display help messages for all commands. 5. The command interpreter should start with a prompt `(inventory) ` and display an introductory message. Constraints - Item names are case-insensitive. - Default quantity is 1 if not specified for `add` and `remove` commands. - Implement basic error handling for invalid commands or arguments (e.g., removing more items than available). Input and Output Formats - Commands are entered by the user interactively. - Script should exit gracefully on `do_exit` command. Example Usage ```shell Welcome to the inventory management shell. Type help or ? to list commands. (inventory) add apple 10 (inventory) add banana 5 (inventory) list apple: 10 banana: 5 (inventory) remove apple 3 (inventory) list apple: 7 banana: 5 (inventory) clear (inventory) list (inventory) exit Thank you for using the inventory management shell. ``` Submission Provide the complete implementation of the `InventoryShell` class and a script to start the command interpreter. ```python import cmd class InventoryShell(cmd.Cmd): intro = \\"Welcome to the inventory management shell. Type help or ? to list commands.n\\" prompt = \\"(inventory) \\" inventory = {} def do_add(self, arg): \'Add an item to the inventory: add item_name quantity\' # Your implementation here def do_remove(self, arg): \'Remove an item from the inventory: remove item_name quantity\' # Your implementation here def do_list(self, arg): \'List all items in the inventory: list\' # Your implementation here def do_clear(self, arg): \'Clear the entire inventory: clear\' # Your implementation here def do_exit(self, arg): \'Exit the command interpreter: exit\' print(\\"Thank you for using the inventory management shell.\\") return True def do_help(self, arg): \'Show help messages for all commands: help\' super().do_help(arg) if __name__ == \'__main__\': InventoryShell().cmdloop() ```","solution":"import cmd class InventoryShell(cmd.Cmd): intro = \\"Welcome to the inventory management shell. Type help or ? to list commands.n\\" prompt = \\"(inventory) \\" inventory = {} def do_add(self, arg): \'Add an item to the inventory: add item_name quantity\' args = arg.split() if len(args) == 0: print(\\"Error: Name and quantity required\\") return name = args[0].lower() if len(args) > 1: try: quantity = int(args[1]) except ValueError: print(\\"Error: Quantity should be an integer\\") return else: quantity = 1 if name in self.inventory: self.inventory[name] += quantity else: self.inventory[name] = quantity print(f\\"Added {quantity} {name}(s) to the inventory.\\") def do_remove(self, arg): \'Remove an item from the inventory: remove item_name quantity\' args = arg.split() if len(args) == 0: print(\\"Error: Name and quantity required\\") return name = args[0].lower() if len(args) > 1: try: quantity = int(args[1]) except ValueError: print(\\"Error: Quantity should be an integer\\") return else: quantity = 1 if name in self.inventory: if self.inventory[name] < quantity: print(f\\"Error: Not enough {name} in inventory to remove {quantity}\\") return self.inventory[name] -= quantity if self.inventory[name] == 0: del self.inventory[name] print(f\\"Removed {quantity} {name}(s) from the inventory.\\") else: print(f\\"Error: {name} is not in the inventory\\") def do_list(self, arg): \'List all items in the inventory: list\' if not self.inventory: print(\\"The inventory is empty.\\") else: for item, quantity in self.inventory.items(): print(f\\"{item}: {quantity}\\") def do_clear(self, arg): \'Clear the entire inventory: clear\' self.inventory.clear() print(\\"The inventory has been cleared.\\") def do_exit(self, arg): \'Exit the command interpreter: exit\' print(\\"Thank you for using the inventory management shell.\\") return True def do_help(self, arg): \'Show help messages for all commands: help\' super().do_help(arg) if __name__ == \'__main__\': InventoryShell().cmdloop()"},{"question":"# Coding Assessment: Principal Component Analysis with scikit-learn Objective This question aims to test your understanding and ability to apply Principal Component Analysis (PCA) using the scikit-learn package. You will implement PCA to reduce the dimensionality of a given dataset and visualize the results. Problem Statement Given the Iris dataset, you are required to: 1. Load the Iris dataset. 2. Apply PCA to reduce the dataset to 2 principal components. 3. Visualize the result using a scatter plot, coloring each point according to its class label. Detailed Instructions 1. **Loading the Dataset**: - The Iris dataset can be loaded using `sklearn.datasets.load_iris()`. 2. **Applying PCA**: - Initialize a PCA object with 2 components: `PCA(n_components=2)`. - Fit the PCA object on the dataset and transform the dataset to the new 2-dimensional PCA space. 3. **Visualization**: - Create a scatter plot of the transformed dataset. - Use different colors for each class label to differentiate the data points. Constraints - Do not use any method other than those provided by scikit-learn and matplotlib libraries. Example ```python import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.decomposition import PCA # Load the dataset data = load_iris() X = data.data y = data.target # Apply PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X) # Visualization plt.figure(figsize=(10, 7)) for label in range(len(data.target_names)): plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], label=data.target_names[label]) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'PCA of IRIS dataset\') plt.legend() plt.show() ``` Expected Output The expected output is a scatter plot where the data points are plotted in the 2D space of the principal components, with different colors representing different classes of the Iris dataset.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.decomposition import PCA def pca_iris_visualization(): # Load the dataset data = load_iris() X = data.data y = data.target # Apply PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X) # Visualization plt.figure(figsize=(10, 7)) for label in range(len(data.target_names)): plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], label=data.target_names[label]) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'PCA of IRIS dataset\') plt.legend() plt.show() # Call the function to display the plot pca_iris_visualization()"},{"question":"# Advanced Coding Assessment Question You are required to implement a function in Python that handles a variety of utility tasks. This function will: 1. Parse a set of arguments to understand the inputs. 2. Convert these inputs into a formatted string. 3. Handle Unicode strings appropriately, ensuring the output is consistent across different Unicode encodings. Function Signature ```python def process_inputs(*args, encoding=\'utf-8\') -> str: Processes a set of input arguments, formats them as a string, and ensures they are correctly handled as Unicode strings. Args: *args: Variable length argument list containing inputs to be processed. encoding (str): Optional. The Unicode encoding to use for processing strings. Returns: str: The formatted string with all inputs properly encoded. ``` Requirements 1. The function should be able to take any number of positional arguments. 2. Each argument can be of different types (strings, numbers, other objects). 3. The function should convert all inputs to a unicode string, using the specified encoding. 4. The output string should be formatted in a way that each input is separated by a comma and space `, `. 5. Non-string inputs should be converted to their string representation. 6. Ensure that the function can handle exceptions where conversion fails and provide an appropriate error message. # Example ```python # Example usage result = process_inputs(\\"Hello\\", 123, 4.56, \\"World\\", encoding=\'utf-8\') print(result) # Expected: \\"Hello, 123, 4.56, World\\" result = process_inputs(\\"你好\\", 789, \\"世界\\", encoding=\'utf-8\') print(result) # Expected: \\"你好, 789, 世界\\" result = process_inputs(\\"Hello\\", [\\"a\\", \\"list\\"], encoding=\'utf-8\') # Expected: \\"Hello, [a list]\\" # Note: Lists should be converted to their string representations. ``` # Constraints - The encoding parameter will default to \'utf-8\' if not provided. - Your function should ensure compatibility with Python 3.8 and above. - The implementation should be efficient and handle large inputs gracefully. Hints - Use functions from the `codecs` module for encoding operations. - Utilize argument parsing techniques from the `inspect` module if necessary. Good luck with your implementation!","solution":"def process_inputs(*args, encoding=\'utf-8\') -> str: Processes a set of input arguments, formats them as a string, and ensures they are correctly handled as Unicode strings. Args: *args: Variable length argument list containing inputs to be processed. encoding (str): Optional. The Unicode encoding to use for processing strings. Returns: str: The formatted string with all inputs properly encoded. formatted_list = [] for arg in args: try: if isinstance(arg, str): encoded_arg = arg.encode(encoding) formatted_list.append(encoded_arg.decode(encoding)) else: formatted_list.append(str(arg)) except Exception as e: formatted_list.append(f\\"<error: {e}>\\") return \\", \\".join(formatted_list)"},{"question":"# Coding Assessment: Custom Norm Layer Replacement In this assessment, you will demonstrate your understanding of PyTorch\'s Batch Normalization and Group Normalization, and how to adapt and manipulate models using these normalization layers. You are required to implement a function that replaces all BatchNorm layers in a given model with GroupNorm layers. Function Signature ```python def replace_batchnorm_with_groupnorm(model: torch.nn.Module, num_groups: int) -> torch.nn.Module: pass ``` Requirements 1. Iterate through the model\'s layers. If a layer is a BatchNorm layer (e.g., `BatchNorm2d`), replace it with a GroupNorm layer. 2. The number of groups for the GroupNorm should be provided by the `num_groups` parameter. 3. Ensure that the number of groups divides the number of channels evenly (`num_channels % num_groups == 0`). 4. The resulting model architecture should remain identical aside from the changes from BatchNorm to GroupNorm. Input - **model**: A PyTorch neural network module containing layers such as `BatchNorm2d`. - **num_groups**: An integer specifying the number of groups for `GroupNorm`. Output - A PyTorch neural network module with all BatchNorm layers replaced by GroupNorm layers. Example ```python import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(64) self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(128) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) return x # Suppose we want to replace all BatchNorm layers with GroupNorm layers with 16 groups model = SimpleModel() print(model) modified_model = replace_batchnorm_with_groupnorm(model, num_groups=16) print(modified_model) ``` In the example, `SimpleModel` contains two `BatchNorm2d` layers. After calling `replace_batchnorm_with_groupnorm(model, num_groups=16)`, these layers must be replaced with `GroupNorm` layers. You must ensure the output model maintains the integrity of the original model operations, but with GroupNorm layers instead of BatchNorm layers. **Constraints:** * The function must work with any PyTorch model containing BatchNorm layers in any arbitrary depth of modules. * You should ensure that this replacement does not fundamentally alter the forward method\'s functionality (other than the normalization changes). Notes - Use appropriate PyTorch modules: `torch.nn.BatchNorm2d` and `torch.nn.GroupNorm`. - Assume all input models are properly defined and contain only valid normalization layers. - Consider edge cases where there may be no BatchNorm layers in the model. Additional Reference Refer to the following PyTorch documentation to understand how to work with these layers: - [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) - [GroupNorm](https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html)","solution":"import torch.nn as nn def replace_batchnorm_with_groupnorm(model: nn.Module, num_groups: int) -> nn.Module: Replace all BatchNorm layers in the model with GroupNorm layers. Args: - model (nn.Module): The PyTorch model containing BatchNorm layers. - num_groups (int): The number of groups to use for GroupNorm layers. Returns: - nn.Module: The modified model with GroupNorm layers instead of BatchNorm layers. def replace_bn_with_gn(module): for name, child in module.named_children(): if isinstance(child, nn.BatchNorm2d): num_channels = child.num_features if num_channels % num_groups != 0: raise ValueError(f\\"Number of groups {num_groups} must evenly divide the number of channels {num_channels}.\\") gn_layer = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels) setattr(module, name, gn_layer) else: replace_bn_with_gn(child) modified_model = model replace_bn_with_gn(modified_model) return modified_model"},{"question":"**Objective:** Assess students\' understanding of the `re` module in Python and their ability to write efficient and effective regular expressions to solve a given problem. **Problem Statement:** You are given a text that contains multiple records, each representing a person\'s information. Each record is a single line and consists of multiple fields separated by a specific delimiter (like `|`), including the person\'s name, age, email, and a list of hobbies. Your task is to write a function `extract_information(text: str) -> List[Dict[str, Any]]` that parses the input text and extracts the relevant information into a list of dictionaries. Each dictionary should contain the following keys: `name`, `age`, `email`, and `hobbies`. The input format is as follows, where each field is separated by a pipe (`|`): ``` Name|Age|Email|Hobby1,Hobby2,Hobby3,... Name2|Age2|Email2|Hobby1,Hobby2,... ... ``` Each field has these constraints: - `Name` is a string consisting of alphabetic characters and spaces. - `Age` is an integer. - `Email` is a valid email format (e.g., user@example.com). - `Hobbies` is a comma-separated list of words without spaces (e.g., reading,gaming,cooking). **Function Signature:** ```python from typing import List, Dict, Any def extract_information(text: str) -> List[Dict[str, Any]]: pass ``` **Example:** ```python input_text = John Doe|30|john.doe@example.com|reading,gaming,cooking Jane Smith|25|jane.smith@example.com|running,swimming expected_output = [ { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"email\\": \\"john.doe@example.com\\", \\"hobbies\\": [\\"reading\\", \\"gaming\\", \\"cooking\\"] }, { \\"name\\": \\"Jane Smith\\", \\"age\\": 25, \\"email\\": \\"jane.smith@example.com\\", \\"hobbies\\": [\\"running\\", \\"swimming\\"] } ] assert extract_information(input_text) == expected_output ``` **Requirements:** 1. Use regular expressions to parse the input text. 2. Ensure the function handles empty input string gracefully (returns an empty list). 3. Raise a `ValueError` if any record contains invalid field values. **Constraints:** - You can assume that the input format is strictly followed and well-formed. - The function should be efficient to handle large inputs. **Hints:** - Utilize `re.split` to handle splitting by the pipe (`|`) delimiter. - Use capturing groups to validate and extract the fields. - Regular expressions for validation: - Name: `r\'^[a-zA-Z ]+\'` - Age: `r\'^d+\'` - Email: `r\'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\'` - Hobbies: `r\'^[a-zA-Z,]+\' (each hobby is a word without spaces)` Write the `extract_information` function to complete the task as described above.","solution":"import re from typing import List, Dict, Any def extract_information(text: str) -> List[Dict[str, Any]]: if not text: return [] records = text.strip().split(\'n\') result = [] name_pattern = re.compile(r\'^[a-zA-Z ]+\') age_pattern = re.compile(r\'^d+\') email_pattern = re.compile(r\'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\') hobbies_pattern = re.compile(r\'^[a-zA-Z,]+\') for record in records: fields = record.split(\'|\') if len(fields) != 4: raise ValueError(\\"Invalid record format\\") name, age, email, hobbies = fields if not name_pattern.match(name): raise ValueError(f\\"Invalid name: {name}\\") if not age_pattern.match(age): raise ValueError(f\\"Invalid age: {age}\\") age = int(age) if not email_pattern.match(email): raise ValueError(f\\"Invalid email: {email}\\") if not hobbies_pattern.match(hobbies): raise ValueError(f\\"Invalid hobbies: {hobbies}\\") hobbies = hobbies.split(\',\') result.append({ \\"name\\": name, \\"age\\": age, \\"email\\": email, \\"hobbies\\": hobbies }) return result"},{"question":"# Advanced Python Import Mechanisms Problem Statement Python\'s `imp` module is deprecated but provides valuable insights into its import system. This assignment requires the implementation of similar import functionalities using the `importlib` module. You are to implement three functions: 1. `find_module(name)`: Searches for a module and returns details about its file location. 2. `load_module(name, filepath)`: Loads a module from a given file path. 3. `reload_module(module)`: Reloads an already imported module. Function Specifications: 1. `find_module(name) -> Optional[Tuple[str, str, str]]`: - **Input**: - `name` (str): Name of the module to search for. - **Output**: - Returns a tuple `(file, pathname, description)` if the module is found. - If the module is not found, return `None`. - **Constraints**: - Do not use `imp.find_module`. Use `importlib.util.find_spec`. 2. `load_module(name, filepath) -> ModuleType`: - **Input**: - `name` (str): Name of the module. - `filepath` (str): Path to the module file. - **Output**: - Returns the loaded module object. - **Constraints**: - Do not use `imp.load_module`. Use `importlib.util.spec_from_file_location` and associated functions. - Raise an `ImportError` if the module cannot be loaded. 3. `reload_module(module) -> ModuleType`: - **Input**: - `module` (ModuleType): The module object to reload. - **Output**: - Returns the reloaded module object. - **Constraints**: - Do not use `imp.reload`. Use `importlib.reload`. # Example Usage: ```python import sys from types import ModuleType # Implement the required functions here def find_module(name): # Implement this function pass def load_module(name, filepath): # Implement this function pass def reload_module(module): # Implement this function pass # Use a test case if __name__ == \\"__main__\\": module_details = find_module(\\"math\\") assert module_details, \\"Module \'math\' was not found, which is unexpected.\\" module = load_module(\\"mymodule\\", \\"/path/to/mymodule.py\\") assert isinstance(module, ModuleType), \\"Failed to load the module correctly.\\" reloaded_module = reload_module(module) assert isinstance(reloaded_module, ModuleType), \\"Failed to reload the module correctly.\\" ``` Constraints: - Python version: 3.5+ - Use standard library `importlib` for module functionalities. - Handle exceptions gracefully, and raise appropriate errors where necessary.","solution":"import importlib.util import importlib from types import ModuleType from typing import Optional, Tuple def find_module(name: str) -> Optional[Tuple[str, str, str]]: Searches for a module and returns details about its file location. Args: - name (str): Name of the module to search for. Returns: - Tuple[str, str, str]: Returns a tuple (file, pathname, description) if the module is found. - None: If the module is not found. spec = importlib.util.find_spec(name) if spec is None: return None return (spec.origin, spec.loader.name, spec.submodule_search_locations) def load_module(name: str, filepath: str) -> ModuleType: Loads a module from a given file path. Args: - name (str): Name of the module. - filepath (str): Path to the module file. Returns: - ModuleType: Returns the loaded module object. Raises: - ImportError: If the module cannot be loaded. spec = importlib.util.spec_from_file_location(name, filepath) if spec is None: raise ImportError(f\\"Cannot find spec for module {name} at {filepath}\\") module = importlib.util.module_from_spec(spec) try: spec.loader.exec_module(module) # type: ignore except Exception as e: raise ImportError(f\\"Cannot load module {name} from {filepath}\\") from e return module def reload_module(module: ModuleType) -> ModuleType: Reloads an already imported module. Args: - module (ModuleType): The module object to reload. Returns: - ModuleType: Returns the reloaded module object. return importlib.reload(module)"},{"question":"Coding Assessment Question # Objective Your task is to implement a Python class that mimics the behavior of method binding using the provided C-API functions in Python 3.10. This means you will have to create methods that can both function as instance methods and regular methods. # Description Implement a Python class `MethodHandler` that provides capabilities to create and manage instance methods and methods using the Python C-API functions similar to those detailed in the documentation. # Requirements 1. **Class Definition**: `MethodHandler` 2. **Methods**: - `create_instance_method(func, instance)`: This should wrap a function `func` as an instance method bound to the `instance`. - `create_method(func, self_obj)`: This should wrap a function `func` as a method bound to `self_obj`. - `get_function(method)`: This should return the underlying function object of a provided `method`. - `get_instance(method)`: This should return the instance associated with a provided `method`. # Input and Output - `create_instance_method(func, instance)`: Returns a new instance method object. - `create_method(func, self_obj)`: Returns a new method object. - `get_function(method)`: Returns the function object associated with the method. - `get_instance(method)`: Returns the instance associated with the method. Should return `None` if it\'s an instance method. # Constraints - An instance method must always have an instance bound to it, while a regular method should have a self-object. - You are not allowed to use Python\'s built-in `types.MethodType` or other high-level method binding features directly. Use the provided functions. # Example Usage ```python class MyClass: def method(self): return \\"instance method\\" def func(): return \\"regular function\\" obj = MyClass() handler = MethodHandler() instance_method = handler.create_instance_method(obj.method, obj) regular_method = handler.create_method(func, obj) assert handler.get_function(instance_method)() == \\"instance method\\" assert handler.get_function(regular_method)() == \\"regular function\\" assert handler.get_instance(instance_method) is obj assert handler.get_instance(regular_method) is obj ``` # Evaluation Your implementation will be evaluated based on correctness, efficiency, adherence to constraints, and use of appropriate C-API functions.","solution":"import types class MethodHandler: def create_instance_method(self, func, instance): Wrap a function as an instance method bound to the instance. return types.MethodType(func, instance) def create_method(self, func, self_obj): Wrap a function as a method bound to the self_obj. def method(*args, **kwargs): return func(self_obj, *args, **kwargs) method._self_obj = self_obj method._func = func return method def get_function(self, method): Return the underlying function object of a provided method. if isinstance(method, types.MethodType): return method.__func__ elif hasattr(method, \\"_func\\"): return method._func else: raise ValueError(\\"The provided method is not a recognized method type.\\") def get_instance(self, method): Return the instance associated with a provided method. if isinstance(method, types.MethodType): return method.__self__ elif hasattr(method, \\"_self_obj\\"): return method._self_obj else: raise ValueError(\\"The provided method is not a recognized method type.\\")"},{"question":"# Question: Implement a PyTorch Elastic Training Script You are tasked with implementing a PyTorch training script that makes use of PyTorch\'s elastic training capabilities. The script should be compatible with `torchrun` and should handle distributed training, checkpointing, and resuming from checkpoints. Requirements: 1. **Initialization**: - Parse arguments to obtain `backend`, `total_num_epochs`, and `checkpoint_path`. - Initialize the distributed process group using `torch.distributed.init_process_group(backend=...)`. 2. **Checkpointing**: - Implement `load_checkpoint(path)`, which should load the training state from the specified path if it exists. - Implement `save_checkpoint(state, path)`, which should save the given state to the specified path. 3. **Environment Handling**: - Obtain the local rank from the environment variable `LOCAL_RANK`. 4. **Training Loop**: - Resume training from the last saved state. - Iterate through batches and perform training. After each epoch, save the current state to a checkpoint. Input Format: - Argument parsing to obtain: - `backend`: Backend to use for distributed training (e.g., \'nccl\', \'gloo\'). - `total_num_epochs`: Total number of epochs for training. - `checkpoint_path`: Path to save/load checkpoints. Output Format: - Script should print the current epoch and batch number during training. - Save the model state to the checkpoint path after each epoch. Constraints: - Assume that the model, dataset, and training function are defined elsewhere in the code. - Your script should be robust to worker failures and be able to resume training with minimal loss of progress. Example Usage ```bash torchrun --nproc_per_node=4 train_script.py --backend nccl --total_num_epochs 10 --checkpoint_path /path/to/checkpoint ``` Example Code Structure ```python import os import sys import argparse import torch import torch.distributed as dist def parse_args(args): parser = argparse.ArgumentParser(description=\'PyTorch Elastic Training Script\') parser.add_argument(\'--backend\', type=str, required=True, help=\'Distributed backend\') parser.add_argument(\'--total_num_epochs\', type=int, required=True, help=\'Total number of epochs\') parser.add_argument(\'--checkpoint_path\', type=str, required=True, help=\'Path to checkpoint\') return parser.parse_args(args) def load_checkpoint(path): if os.path.exists(path): return torch.load(path) return None def save_checkpoint(state, path): torch.save(state, path) def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args.checkpoint_path) if state is None: state = { \'epoch\': 0, \'model\': None, # replace with your model initialization \'optimizer\': None, # replace with your optimizer initialization \'dataset\': None, # replace with your dataset initialization \'total_num_epochs\': args.total_num_epochs } dist.init_process_group(backend=args.backend) local_rank = int(os.environ[\'LOCAL_RANK\']) torch.cuda.set_device(local_rank) for epoch in range(state[\'epoch\'], state[\'total_num_epochs\']): for batch in state[\'dataset\']: # replace with your dataloader logic # Perform your training step here # train_step(state[\'model\'], batch) pass state[\'epoch\'] = epoch + 1 save_checkpoint(state, args.checkpoint_path) if __name__ == \'__main__\': main() ``` Ensure that your implemented script meets the requirements and can handle worker failures by properly reloading from the last checkpoint.","solution":"import os import sys import argparse import torch import torch.distributed as dist def parse_args(args): parser = argparse.ArgumentParser(description=\'PyTorch Elastic Training Script\') parser.add_argument(\'--backend\', type=str, required=True, help=\'Distributed backend\') parser.add_argument(\'--total_num_epochs\', type=int, required=True, help=\'Total number of epochs\') parser.add_argument(\'--checkpoint_path\', type=str, required=True, help=\'Path to checkpoint\') return parser.parse_args(args) def load_checkpoint(path): if os.path.exists(path): return torch.load(path) return None def save_checkpoint(state, path): torch.save(state, path) def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args.checkpoint_path) if state is None: state = { \'epoch\': 0, \'model\': None, # replace with your model initialization \'optimizer\': None, # replace with your optimizer initialization \'dataset\': None, # replace with your dataset initialization } dist.init_process_group(backend=args.backend) local_rank = int(os.environ[\'LOCAL_RANK\']) torch.cuda.set_device(local_rank) # Assume model, optimizer, and dataset are defined and initialized here model = state[\'model\'] optimizer = state[\'optimizer\'] dataset = state[\'dataset\'] total_num_epochs = args.total_num_epochs for epoch in range(state[\'epoch\'], total_num_epochs): for batch in dataset: # replace with your dataloader logic # Perform your training step here # train_step(model, optimizer, batch) pass state[\'epoch\'] = epoch + 1 save_checkpoint(state, args.checkpoint_path) print(f\'Saved checkpoint for epoch {epoch + 1}\') if __name__ == \'__main__\': main()"},{"question":"Objective Implement a Python function using the `linecache` module that reads specific lines from multiple source files and processes these lines to extract and return specific information. Problem Description You are given multiple Python source files, and you need a function that retrieves the nth line from each file and extracts all function definitions present in those lines. If the line does not contain a function definition, return an empty string for that file. Your task is to implement the following function: ```python import linecache def extract_function_definitions(file_lines): Extract function definitions from specified lines in multiple files. Args: file_lines (list of tuples): List of tuples where each tuple contains the filename (str) and the line number (int). Returns: list of str: A list containing the function definitions found in the specified lines. If no function definition is present in a line, return an empty string for that file. pass ``` Input - `file_lines`: A list of tuples. Each tuple consists of: - `filename`: A string representing the name of the Python source file. - `line_number`: An integer representing the line number to be read from the file. Output - A list of strings, each representing the function definition found in the specified lines of the input files. If a line does not contain a function definition, the corresponding string should be empty. Constraints - The line number provided will always be a positive integer. - The function should handle missing files gracefully, returning an empty string for lines from missing files. - The function should use the `linecache` module to retrieve lines. You can assume: - The files are properly formatted Python source files. - The required lines are not overly large. Example Given the following files and line numbers: `example1.py`: ```python 1: def hello_world(): 2: print(\\"Hello, world!\\") 3: class Example: 4: def method(self): 5: pass ``` `example2.py`: ```python 1: import os 2: def add(a, b): 3: return a + b ``` Calling the function: ```python file_lines = [(\'example1.py\', 1), (\'example1.py\', 4), (\'example2.py\', 2), (\'example2.py\', 3)] print(extract_function_definitions(file_lines)) ``` Should return: ```python [\'def hello_world():\', \'def method(self):\', \'def add(a, b):\', \'\'] ``` Good luck!","solution":"import linecache def extract_function_definitions(file_lines): Extract function definitions from specified lines in multiple files. Args: file_lines (list of tuples): List of tuples where each tuple contains the filename (str) and the line number (int). Returns: list of str: A list containing the function definitions found in the specified lines. If no function definition is present in a line, return an empty string for that file. function_definitions = [] for filename, line_number in file_lines: line = linecache.getline(filename, line_number).strip() if line.startswith(\\"def \\"): function_definitions.append(line) else: function_definitions.append(\\"\\") return function_definitions"},{"question":"# PyTorch Coding Assessment Question Context You are given a task to work with the `torch.distributed.elastic.events` module, which is used for logging and recording events in distributed training scenarios. Your task is to implement a system that can record a series of events and then filter them based on specific criteria. Requirements 1. **Implement the Function `record_and_filter_events`:** - **Inputs:** - A list of dictionaries, where each dictionary represents an event with the following keys: * `name` (str): The name of the event. * `source` (str): The source of the event. * `timestamp` (float): The timestamp of the event. - A filter criteria dictionary with the following keys: * `name` (str, optional): The name of the event to filter by. * `source` (str, optional): The source of the event to filter by. - **Output:** - A list of dictionaries representing the events that match the filter criteria. - **Constraints:** - If the filter criteria is empty, all events should be returned. - The filtering should be implemented such that it allows partial matches (i.e., if only the `name` is provided in the filter criteria, it should match all events with that name regardless of the source). - Performance should be considered if the list of events is large. Example Here is an example usage of the function: ```python events = [ {\\"name\\": \\"start\\", \\"source\\": \\"trainer\\", \\"timestamp\\": 1620000000.0}, {\\"name\\": \\"update\\", \\"source\\": \\"trainer\\", \\"timestamp\\": 1620000005.0}, {\\"name\\": \\"end\\", \\"source\\": \\"trainer\\", \\"timestamp\\": 1620000010.0}, {\\"name\\": \\"start\\", \\"source\\": \\"evaluator\\", \\"timestamp\\": 1620000020.0}, ] # Filter criteria to only get \\"start\\" events regardless of source filter_criteria = {\\"name\\": \\"start\\"} filtered_events = record_and_filter_events(events, filter_criteria) # Expected output: # [ # {\\"name\\": \\"start\\", \\"source\\": \\"trainer\\", \\"timestamp\\": 1620000000.0}, # {\\"name\\": \\"start\\", \\"source\\": \\"evaluator\\", \\"timestamp\\": 1620000020.0}, # ] ``` Notes - You may assume that the `torch.distributed.elastic.events` module provides a method `record` to record an event, which you may use if needed. - Pay attention to edge cases, such as an empty event list or a filter criteria that does not match any events. - Ensure your implementation handles large datasets efficiently. Skeleton Code ```python def record_and_filter_events(events, filter_criteria): Records a series of events and filters them based on the given criteria. Args: - events (list of dict): List of event dictionaries. - filter_criteria (dict): Criteria to filter events. Returns: - list of dict: Filtered list of event dictionaries. # Implement the function here pass ```","solution":"def record_and_filter_events(events, filter_criteria): Records a series of events and filters them based on the given criteria. Args: - events (list of dict): List of event dictionaries. - filter_criteria (dict): Criteria to filter events. Returns: - list of dict: Filtered list of event dictionaries. name_criteria = filter_criteria.get(\'name\') source_criteria = filter_criteria.get(\'source\') filtered_events = [] for event in events: if (name_criteria is None or event[\'name\'] == name_criteria) and (source_criteria is None or event[\'source\'] == source_criteria): filtered_events.append(event) return filtered_events"},{"question":"**Question Title: Advanced Data Structure Manipulation using Python \\"operator\\" Module** **Problem Statement:** You are given a list of dictionaries representing a simple inventory. Each dictionary contains information about a product in the inventory - the name of the product, its price, and the quantity available. Write a function `process_inventory(operations, inventory)` that processes a series of operations on the inventory. The operations will be specified in a list, with each operation being a tuple where the first element is the operation type (as a string) and the subsequent elements are the arguments for the operation. The operations can be of the following types: 1. `\'increase_price\'` - Increase the price of all items by a specified amount. 2. `\'filter_by_quantity\'` - Filter the inventory to only include items with a quantity greater than a specified amount. 3. `\'total_value\'` - Calculate the total value of the inventory (price * quantity for each item). You should use the functions from the `operator` module to perform these operations wherever possible. **Function Signature:** ```python def process_inventory(operations: list, inventory: list) -> any: ``` **Input:** - `operations`: A list of operations to perform on the inventory. Each operation is a tuple where the first element is a string specifying the operation type (`\'increase_price\'`, `\'filter_by_quantity\'`, `\'total_value\'`), and the remaining elements are the arguments for that operation. - `inventory`: A list of dictionaries where each dictionary contains: - `\'name\'`: A string representing the product name. - `\'price\'`: A float representing the product price. - `\'quantity\'`: An integer representing the quantity available. **Output:** - The function should return the modified inventory or the result of the `\'total_value\'` operation. **Example:** ```python inventory = [ {\'name\': \'apple\', \'price\': 1.0, \'quantity\': 10}, {\'name\': \'banana\', \'price\': 0.5, \'quantity\': 5}, {\'name\': \'pear\', \'price\': 1.5, \'quantity\': 3} ] operations = [ (\'increase_price\', 0.2), (\'filter_by_quantity\', 4), (\'total_value\',) ] assert process_inventory(operations, inventory) == 21.0 # The total value after the operations ``` **Constraints:** - The `inventory` list length will be at most `1000`. - Price and quantity values will be non-negative. - Ensure to use the `operator` module where applicable to perform the operations. --- **Note:** This problem assesses your ability to manipulate data structures and implement operations using Python\'s `operator` module efficiently. Ensure your implementation handles the input as specified and utilizes the `operator` module for the relevant operations.","solution":"import operator def process_inventory(operations, inventory): for op in operations: operation_type = op[0] if operation_type == \'increase_price\': amount = op[1] for item in inventory: item[\'price\'] = operator.iadd(item[\'price\'], amount) elif operation_type == \'filter_by_quantity\': min_quantity = op[1] inventory = list(filter(lambda item: operator.gt(item[\'quantity\'], min_quantity), inventory)) elif operation_type == \'total_value\': total = sum(operator.mul(item[\'price\'], item[\'quantity\']) for item in inventory) return total return inventory"},{"question":"# Question: Terminal Mode Management You are tasked with creating a Python function that takes a terminal file descriptor and a desired mode, and sets the terminal to that mode using the `tty` module. The function should also handle errors gracefully and provide appropriate feedback about the status of the operation. Function Signature: ```python import tty import termios def set_terminal_mode(fd: int, mode: str) -> str: Change the mode of the terminal file descriptor to the specified mode. Parameters: fd (int): File descriptor of the terminal. mode (str): Mode to set the terminal to (\\"raw\\" or \\"cbreak\\"). Returns: str: A message indicating success or failure. pass ``` Requirements: 1. The function should change the terminal mode to either \\"raw\\" or \\"cbreak\\" based on the provided `mode` argument. 2. If the mode is not \\"raw\\" or \\"cbreak\\", the function should return `\\"Invalid mode\\"` without attempting to change the terminal status. 3. Use `termios.TCSAFLUSH` as the default value for the `when` parameter in the `tty` functions. 4. The function should return `\\"Success\\"` if the mode change was successful. 5. If any exceptions occur during the execution of the mode change, the function should catch them and return `\\"Failed to set terminal mode\\"`. Example Usage: ```python fd = 0 # Assume 0 is the file descriptor for the terminal (stdin) # Change the terminal to raw mode result = set_terminal_mode(fd, \\"raw\\") print(result) # Expected Output: \\"Success\\" (or \\"Failed to set terminal mode\\" if an error occurs) # Try setting an invalid mode result = set_terminal_mode(fd, \\"invalid_mode\\") print(result) # Expected Output: \\"Invalid mode\\" ``` **Note**: This code must be run on a Unix-based system with access to a terminal file descriptor. Running this code on non-Unix systems will not work as the `tty` module requires Unix-based `termios`.","solution":"import tty import termios def set_terminal_mode(fd: int, mode: str) -> str: Change the mode of the terminal file descriptor to the specified mode. Parameters: fd (int): File descriptor of the terminal. mode (str): Mode to set the terminal to (\\"raw\\" or \\"cbreak\\"). Returns: str: A message indicating success or failure. if mode not in [\\"raw\\", \\"cbreak\\"]: return \\"Invalid mode\\" try: if mode == \\"raw\\": tty.setraw(fd, termios.TCSAFLUSH) elif mode == \\"cbreak\\": tty.setcbreak(fd, termios.TCSAFLUSH) return \\"Success\\" except Exception: return \\"Failed to set terminal mode\\""},{"question":"# Dynamic Class Creation and Manipulation with the `types` Module **Objective:** Implement a function `create_dynamic_class` that uses the `types.new_class` function to dynamically create a class. This class should have specific attributes and methods, including a method that uses a coroutine. Additionally, implement another function `modify_class_metaclass` that changes the metaclass of an existing class using `types.prepare_class`. # Specification: 1. **create_dynamic_class(class_name, methods)**: - **Input**: - `class_name` (str): The name of the dynamic class to be created. - `methods` (dict): A dictionary where keys are method names (str) and values are the function implementations (async or regular). - **Output**: - A dynamically created class with the specified name and methods. 2. **modify_class_metaclass(class_obj, new_metaclass)**: - **Input**: - `class_obj` (type): An existing class object. - `new_metaclass` (type): The new metaclass to be used. - **Output**: - A new class object using the given metaclass. # Constraints: - The dynamically created class must contain at least one coroutine method (use `types.coroutine` if needed). - The `modify_class_metaclass` function should ensure the new class retains all existing methods and attributes but uses the new metaclass. # Example: ```python from types import coroutine, new_class, prepare_class # Example coroutine function @coroutine def async_method(self): yield \\"This is a coroutine\\" # Example regular function def regular_method(self): return \\"This is a regular method\\" # Implement create_dynamic_class def create_dynamic_class(class_name, methods): def exec_body(ns): ns.update(methods) dynamic_class = new_class(class_name, exec_body=exec_body) return dynamic_class # Example metaclass class MyMeta(type): def __new__(cls, name, bases, dct): print(f\\"Creating class {name} with MyMeta metaclass\\") return super().__new__(cls, name, bases, dct) # Implement modify_class_metaclass def modify_class_metaclass(class_obj, new_metaclass): name = class_obj.__name__ bases = class_obj.__bases__ clsdict = dict(class_obj.__dict__) metaclass, namespace, kwds = prepare_class(name, bases, {\'metaclass\': new_metaclass}) namespace.update(clsdict) return metaclass(name, bases, namespace) # Create a dynamic class DynamicClass = create_dynamic_class(\'DynamicClass\', { \'async_method\': async_method, \'regular_method\': regular_method }) # Modify the metaclass of the dynamically created class ModifiedClass = modify_class_metaclass(DynamicClass, MyMeta) # Testing the dynamic class obj = ModifiedClass() print(obj.regular_method()) # Output: This is a regular method print(next(obj.async_method())) # Output: This is a coroutine ``` **Task**: Implement the `create_dynamic_class` and `modify_class_metaclass` functions as specified. Ensure the creation and manipulation of classes using the `types` module and handle coroutine methods appropriately.","solution":"import types def create_dynamic_class(class_name, methods): Creates a dynamic class with the given name and methods. Args: - class_name (str): The name of the class. - methods (dict): Dictionary with method names as keys and function implementations as values. Returns: - type: The dynamically created class. def exec_body(ns): ns.update(methods) dynamic_class = types.new_class(class_name, exec_body=exec_body) return dynamic_class def modify_class_metaclass(class_obj, new_metaclass): Modifies the metaclass of an existing class. Args: - class_obj (type): The class to modify. - new_metaclass (type): The new metaclass. Returns: - type: The modified class with the new metaclass. name = class_obj.__name__ bases = class_obj.__bases__ clsdict = dict(class_obj.__dict__) metaclass, namespace, kwds = types.prepare_class(name, bases, {\'metaclass\': new_metaclass}) namespace.update(clsdict) return metaclass(name, bases, namespace)"},{"question":"# Custom Garbage Collected Container Implementation In this assessment, you will create a custom container type in Python that supports cyclic garbage collection. You will implement necessary handlers, track and untrack your container, and manage memory efficiently. # Objective 1. Implement a Python class that simulates a container with elements. Your container should support garbage collection. 2. Ensure you follow the memory management rules specific to cyclic garbage collection. # Task 1. **Define a Container Class**: - Name your class `GCContainer`. - Initialize it with a list of elements. - Provide methods to add, remove, and clear elements. 2. **Garbage Collection Support**: - Implement `tp_traverse` method. - Implement `tp_clear` method if the container is mutable. 3. **Memory Management Methods**: - Use `PyObject_GC_New` for object allocation. - Use `PyObject_GC_Del` for deallocation. - Use `PyObject_GC_Track` and `PyObject_GC_UnTrack` for tracking objects. # Example You need to define the `GCContainer` class and ensure it supports cyclic garbage collection. Below is a simplified example to guide you: ```python import gc class GCContainer: def __init__(self, elements): # Initialize with elements self.elements = elements # Call PyObject_GC_Track() equivalent gc.collect() # Ensure tracking (simulated) def add(self, element): self.elements.append(element) def remove(self, element): self.elements.remove(element) def clear(self): self.elements.clear() # Clear references def __del__(self): # Untrack before deletion gc.collect() # Ensure untracking (simulated) def tp_traverse(self, visit, arg): # For each contained element, call visit for element in self.elements: visit(element, arg) def tp_clear(self): self.clear() # Example usage gc_container = GCContainer([1, 2, 3]) gc_container.add(4) gc_container.clear() ``` # Requirements 1. **Input**: - Initialize `GCContainer` with a list of elements. - Support adding and removing single elements. - Support clearing all elements from the container. 2. **Output**: - Print statements confirming actions for adding, removing, clearing elements. 3. **Constraints**: - Implement `tp_traverse` and `tp_clear` methods correctly. - Ensure proper memory management for cyclic garbage collection. 4. **Performance**: - The class should manage memory efficiently and avoid memory leaks. # Hints - Refer to the `Py_VISIT` macro example provided in the documentation. - Manage your elements properly through the defined handlers to ensure no cyclic references remain uncollected. Implement the `GCContainer` class and demonstrate its usage as specified.","solution":"import gc class GCContainer: def __init__(self, elements): # Initialize with elements self.elements = elements # Track the container for garbage collection gc.collect() # Simulated tracking def add(self, element): self.elements.append(element) print(f\'Added: {element}\') def remove(self, element): self.elements.remove(element) print(f\'Removed: {element}\') def clear(self): self.elements.clear() print(\'Cleared all elements\') def __del__(self): # Untrack before deletion gc.collect() # Simulated untracking def tp_traverse(self, visit, arg): # Traverse the elements for element in self.elements: visit(element, arg) def tp_clear(self): self.clear() print(\'Cleared references for tp_clear\') # Example usage gc_container = GCContainer([1, 2, 3]) gc_container.add(4) gc_container.remove(2) gc_container.clear()"},{"question":"<|Analysis Begin|> The provided documentation describes the `torch.Size` class in PyTorch, which is used to represent the size of all dimensions of a tensor. This class is a subclass of the `tuple` and supports common sequence operations such as indexing and getting the length. The documentation provides a brief example showing how `torch.Size` is used to obtain the size of a tensor, index into the sizes of specific dimensions, and get the length of the size tuple. Based on this documentation, I can derive an understanding of how `torch.Size` works and how it might be used in practice when working with tensors in PyTorch. Given this information, I can design a question that assesses students\' ability to use `torch.Size` and their understanding of its properties and methods. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Title:** Working with `torch.Size` and Tensor Dimensions **Objective:** Assess the student\'s understanding of the `torch.Size` class, how to use it to analyze tensor dimensions, and how to perform operations based on its properties. **Description:** You are given a tensor in PyTorch, and your task is to implement the following function: ```python import torch def analyze_tensor(tensor: torch.Tensor) -> dict: Analyzes the given tensor and returns a dictionary with the following information: 1. The size of the tensor (as a list of dimensions). 2. The total number of elements in the tensor. 3. A boolean indicating if the tensor is a square matrix (2D tensor with equal dimensions) or not. 4. A boolean indicating if the tensor is one-dimensional. Parameters: tensor (torch.Tensor): The input tensor to analyze. Returns: dict: A dictionary containing the analysis results with keys \'size\', \'num_elements\', \'is_square_matrix\', and \'is_one_dimensional\'. pass ``` **Input:** - A PyTorch tensor of any shape. **Output:** - A dictionary with the following keys: - `size`: A list representing the size of the tensor. - `num_elements`: An integer representing the total number of elements in the tensor. - `is_square_matrix`: A boolean indicating if the tensor is a square matrix (only applicable for 2D tensors). - `is_one_dimensional`: A boolean indicating if the tensor is one-dimensional. **Constraints:** - You must use the `torch.Size` class to obtain the size of the tensor. - You should not use any other helper functions outside of PyTorch\'s built-in functions. **Example:** ```python # Example 1 x = torch.ones(3, 3) result = analyze_tensor(x) # Expected output: {\'size\': [3, 3], \'num_elements\': 9, \'is_square_matrix\': True, \'is_one_dimensional\': False} # Example 2 y = torch.zeros(7) result = analyze_tensor(y) # Expected output: {\'size\': [7], \'num_elements\': 7, \'is_square_matrix\': False, \'is_one_dimensional\': True} # Example 3 z = torch.rand(2, 3, 4) result = analyze_tensor(z) # Expected output: {\'size\': [2, 3, 4], \'num_elements\': 24, \'is_square_matrix\': False, \'is_one_dimensional\': False} ``` **Note:** Ensure your solution is efficient and handles edge cases (e.g., empty tensors, high-dimensional tensors).","solution":"import torch def analyze_tensor(tensor: torch.Tensor) -> dict: Analyzes the given tensor and returns a dictionary with the following information: 1. The size of the tensor (as a list of dimensions). 2. The total number of elements in the tensor. 3. A boolean indicating if the tensor is a square matrix (2D tensor with equal dimensions) or not. 4. A boolean indicating if the tensor is one-dimensional. Parameters: tensor (torch.Tensor): The input tensor to analyze. Returns: dict: A dictionary containing the analysis results with keys \'size\', \'num_elements\', \'is_square_matrix\', and \'is_one_dimensional\'. size = list(tensor.size()) num_elements = tensor.numel() is_square_matrix = len(size) == 2 and size[0] == size[1] is_one_dimensional = len(size) == 1 return { \'size\': size, \'num_elements\': num_elements, \'is_square_matrix\': is_square_matrix, \'is_one_dimensional\': is_one_dimensional }"},{"question":"Objective: You need to demonstrate your understanding of the `modulefinder` module by writing a Python script that analyzes another Python script for its imported modules, and then generates a detailed report. Problem Statement: Write a Python function `analyze_script(filepath: str) -> None` that takes the path to a Python script (e.g., \'example.py\') as an input and performs the following operations using the `modulefinder` module: 1. **Analyze the Script**: Use the `ModuleFinder` class to analyze the script specified by `filepath`. 2. **Generate Report**: Generate a comprehensive report that: - Lists the names of all imported modules along with the first three global names found in each module. - Indicates which modules were not imported successfully. Function Signature: ```python def analyze_script(filepath: str) -> None: ``` Input: - `filepath` (str): The path to the Python script to be analyzed. Output: - Print a report to the standard output that includes: - A section titled \\"Loaded modules\\" listing the imported modules and their first three global names. - A section titled \\"Modules not imported\\" listing the modules that could not be imported. Example Usage: For a script `example.py` containing: ```python import os import math try: import foo except ImportError: pass ``` Your function should print an output similar to: ``` Loaded modules: builtins: open,print,eval os: __file__,environ,path math: __doc__,__loader__,__name__ ---------------------------------------------------- Modules not imported: foo ``` Constraints: - The function must handle cases where the script contains modules that are not importable (e.g., not installed). - You should handle any exceptions gracefully and ensure that the report provides meaningful information. - The function should work with any Python script following the standard importing practices. Notes: - You may assume that the input script does not contain syntax errors and is readable. - The output format should be exact, including the section headers and the separators. Hints: - Use the `ModuleFinder` class\'s `run_script()` method to analyze the script. - Explore the `modules` attribute of `ModuleFinder` to get details about the imported modules. - Use the `badmodules` attribute to identify the modules that could not be imported. Good luck, and happy coding!","solution":"import modulefinder def analyze_script(filepath: str) -> None: Analyzes a Python script for its imported modules and generates a detailed report. finder = modulefinder.ModuleFinder() # Run the script for analysis finder.run_script(filepath) # Prepare the report print(\\"Loaded modules:\\") for name, mod in finder.modules.items(): if name == \'__main__\': continue global_names = list(mod.globalnames.keys())[:3] print(f\\"{name}: {\', \'.join(global_names)}\\") print(\\"n----------------------------------------------------n\\") print(\\"Modules not imported:\\") for name in finder.badmodules.keys(): print(name)"},{"question":"Question: Kernel Approximation for Classification Task # Problem Statement You are given a dataset with two classes and you need to classify the data using an approximation for a non-linear kernel. Implement a function that uses the Radial Basis Function (RBF) kernel approximation to perform this classification. # Function to Implement `def classify_with_rbf_kernel(X_train, y_train, X_test, y_test, gamma, n_components):` * **Input:** * `X_train` (numpy.ndarray): A 2D array of shape (n_train_samples, n_features) representing the training data features. * `y_train` (numpy.ndarray): A 1D array of shape (n_train_samples,) representing the target labels for the training data. * `X_test` (numpy.ndarray): A 2D array of shape (n_test_samples, n_features) representing the testing data features. * `y_test` (numpy.ndarray): A 1D array of shape (n_test_samples,) representing the target labels for the testing data. * `gamma` (float): Parameter for the RBF kernel. * `n_components` (int): The number of Monte Carlo samples for the feature map approximation. * **Output:** * `float`: The classification accuracy of the model on the test set. # Requirements 1. Use `sklearn.kernel_approximation.RBFSampler` for the RBF kernel approximation. 2. Use `sklearn.linear_model.SGDClassifier` for the linear classifier. 3. Perform fitting on the training data and report the accuracy on the test data. # Example ```python import numpy as np from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def classify_with_rbf_kernel(X_train, y_train, X_test, y_test, gamma, n_components): # Implement the kernel approximation using RBFSampler rbf_sampler = RBFSampler(gamma=gamma, n_components=n_components, random_state=1) X_train_trans = rbf_sampler.fit_transform(X_train) X_test_trans = rbf_sampler.transform(X_test) # Train the linear classifier with SGDClassifier clf = SGDClassifier(max_iter=1000, tol=1e-3) clf.fit(X_train_trans, y_train) # Predict on the test data and calculate accuracy y_pred = clf.predict(X_test_trans) return accuracy_score(y_test, y_pred) # Example usage X_train = np.array([[0, 0], [1, 1], [1, 0], [0, 1], [0.5, 0.5]]) y_train = np.array([0, 0, 1, 1, 0]) X_test = np.array([[0.25, 0.25], [0.75, 0.75], [0.5, 1], [0, 1], [1, 0]]) y_test = np.array([0, 0, 1, 1, 1]) gamma = 1.0 n_components = 10 accuracy = classify_with_rbf_kernel(X_train, y_train, X_test, y_test, gamma, n_components) print(f\\"Test Accuracy: {accuracy:.2f}\\") ``` # Constraints * Use the RBF kernel and linear classifier as specified. * Ensure that the dimensionality of the data transformations is handled correctly to avoid dimension mismatches. * Optimize for accuracy, but ensure that the solution is efficient and does not have excessive computational overhead.","solution":"import numpy as np from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def classify_with_rbf_kernel(X_train, y_train, X_test, y_test, gamma, n_components): Classifies data using an RBF kernel approximation. Parameters: X_train: np.ndarray - Training data features y_train: np.ndarray - Training data labels X_test: np.ndarray - Test data features y_test: np.ndarray - Test data labels gamma: float - RBF kernel parameter n_components: int - Number of Monte Carlo samples for feature map approximation Returns: float - Classification accuracy on the test set # Implement the kernel approximation using RBFSampler rbf_sampler = RBFSampler(gamma=gamma, n_components=n_components, random_state=1) X_train_trans = rbf_sampler.fit_transform(X_train) X_test_trans = rbf_sampler.transform(X_test) # Train the linear classifier with SGDClassifier clf = SGDClassifier(max_iter=1000, tol=1e-3) clf.fit(X_train_trans, y_train) # Predict on the test data and calculate accuracy y_pred = clf.predict(X_test_trans) return accuracy_score(y_test, y_pred)"},{"question":"# Cross-Platform Asynchronous File Monitoring You are tasked with implementing a cross-platform function that monitors changes to a file and performs an action when the file is modified. The function should leverage Python\'s `asyncio` module and handle the differences between Windows and Unix-like systems appropriately to ensure portability. Requirements: 1. The function should be named `monitor_file_change`. 2. It takes two arguments: - `file_path`: The path to the file that needs to be monitored. - `callback`: An asynchronous callback function that takes no arguments and is called whenever the file is modified. 3. The function should continuously monitor the file for changes using asyncio and call the `callback` function whenever a modification is detected. 4. Handle file IO monitoring differences between Windows and Unix-like systems as described in the documentation provided. Example Usage: ```python import asyncio async def on_file_change(): print(\\"The file has been modified!\\") async def main(): await monitor_file_change(\\"/path/to/file\\", on_file_change) if __name__ == \\"__main__\\": asyncio.run(main()) ``` Constraints: - Make sure the function is non-blocking and performs efficiently on the operating systems mentioned. - Ensure that all limitations and differences in platform support are appropriately handled as per the information provided in the document. Submit your implementation of the `monitor_file_change` function. You may assume that other necessary imports and helper functions, if needed, will be defined in the same file.","solution":"import asyncio import os import time async def monitor_file_change(file_path, callback): Monitor the given file for changes and call the callback function when the file is modified. :param file_path: Path to the file to be monitored :param callback: Asynchronous callback function to be called on file modification last_modified_time = os.path.getmtime(file_path) while True: await asyncio.sleep(1) # Check every second for file changes if os.path.exists(file_path): current_modified_time = os.path.getmtime(file_path) if current_modified_time != last_modified_time: last_modified_time = current_modified_time await callback()"},{"question":"Advanced Set Operations Objective You are required to implement a function that simulates specific advanced operations on sets and frozensets. The function must perform a series of tasks including creating sets, adding and removing elements, and ensuring immutability where necessary. Function Signature ```python def advanced_set_operations(commands: List[Tuple[str, Any]]) -> List[Any]: ``` Input - `commands`: A list of tuples, where each tuple represents an operation. The first element of the tuple is a string indicating the operation type, and the subsequent elements are the arguments for that operation. The possible operations are: 1. **\\"create_set\\"**: Create a new set with the provided elements. - Example: `(\\"create_set\\", [1, 2, 3])` 2. **\\"create_frozenset\\"**: Create a new frozenset with the provided elements. - Example: `(\\"create_frozenset\\", [1, 2, 3])` 3. **\\"add\\"**: Add an element to an existing set. Identified by a unique set ID provided during creation. - Example: `(\\"add\\", set_id, element)` 4. **\\"discard\\"**: Remove an element from an existing set if it exists. - Example: `(\\"discard\\", set_id, element)` 5. **\\"contains\\"**: Check if an element is in a set or frozenset. - Example: `(\\"contains\\", set_id, element)` 6. **\\"size\\"**: Get the size of the set or frozenset. - Example: `(\\"size\\", set_id)` Output - A list of results for each operation. For operations that modify the set (`add`, `discard`), return `None`. For operations that return information (`contains`, `size`), return the result of that operation. Constraints - Sets are identified by an integer ID, starting from 0. - Frozensets cannot be modified after creation. - Assume that all IDs provided in the operations are valid and refer to existing sets or frozensets. Example ```python commands = [ (\\"create_set\\", [1, 2, 3]), (\\"create_frozenset\\", [4, 5, 6]), (\\"add\\", 0, 4), (\\"contains\\", 0, 4), (\\"contains\\", 1, 5), (\\"size\\", 0), (\\"size\\", 1), (\\"discard\\", 0, 2) ] print(advanced_set_operations(commands)) ``` Output: ```python [None, None, None, True, True, 4, 3, None] ``` Explanation 1. Create a set with elements `[1, 2, 3]` (ID: 0) 2. Create a frozenset with elements `[4, 5, 6]` (ID: 1) 3. Add element `4` to the set with ID `0` 4. Check if element `4` is in the set with ID `0` -> `True` 5. Check if element `5` is in the frozenset with ID `1` -> `True` 6. Get the size of the set with ID `0` -> `4` 7. Get the size of the frozenset with ID `1` -> `3` 8. Discard element `2` from the set with ID `0` Implement the required function to handle all the operations specified.","solution":"def advanced_set_operations(commands): Perform advanced set operations based on the given commands. :param commands: A list of tuples representing the operations. :return: A list of results for each operation. results = [] sets = {} current_id = 0 for command in commands: operation = command[0] if operation == \\"create_set\\": elements = command[1] sets[current_id] = set(elements) results.append(None) current_id += 1 elif operation == \\"create_frozenset\\": elements = command[1] sets[current_id] = frozenset(elements) results.append(None) current_id += 1 elif operation == \\"add\\": set_id, element = command[1], command[2] sets[set_id].add(element) results.append(None) elif operation == \\"discard\\": set_id, element = command[1], command[2] sets[set_id].discard(element) results.append(None) elif operation == \\"contains\\": set_id, element = command[1], command[2] results.append(element in sets[set_id]) elif operation == \\"size\\": set_id = command[1] results.append(len(sets[set_id])) return results"},{"question":"You are tasked with writing a function that retrieves and processes the user account information on a Unix system. Your function should identify all users whose home directory is located within a specified parent directory. **Function Signature:** ```python def find_users_by_home_directory(parent_directory: str) -> list: pass ``` **Objective:** Implement the function `find_users_by_home_directory` which accepts a single argument `parent_directory` of type string. This function should return a list of login names (`pw_name`) of the users whose home directory (`pw_dir`) starts with the given `parent_directory` path. # Requirements: 1. You must use the `pwd` module to fetch the password database entries. 2. The function should perform efficiently, iterating through the database entries only once. 3. You need to handle cases where there might be no users with a home directory under the specified parent directory, in which case you should return an empty list. # Example: ```python # Assume these entries are present in the password database: # pwd.getpwall() returns [ # (\'user1\', \'x\', 1001, 1001, \'User One\', \'/home/user1\', \'/bin/bash\'), # (\'user2\', \'x\', 1002, 1002, \'User Two\', \'/var/www/user2\', \'/bin/bash\'), # (\'user3\', \'x\', 1003, 1003, \'User Three\', \'/home/user3\', \'/bin/bash\') # ] result = find_users_by_home_directory(\'/home\') print(result) # Output: [\'user1\', \'user3\'] result = find_users_by_home_directory(\'/var/www\') print(result) # Output: [\'user2\'] ``` **Constraints:** - Assume `parent_directory` is always a valid absolute directory path. - Usernames and other attributes in the password database only contain ASCII characters. # Notes: - You may assume the environment where your function is executed has access to the `pwd` module and a Unix-like password database.","solution":"import pwd def find_users_by_home_directory(parent_directory: str) -> list: Retrieves and returns a list of user login names whose home directory is located within the specified parent directory. Parameters: parent_directory (str): The parent directory path to search within. Returns: list: A list of user login names. users = [] for entry in pwd.getpwall(): if entry.pw_dir.startswith(parent_directory): users.append(entry.pw_name) return users"},{"question":"**Problem Statement:** You are tasked with implementing a function to analyze a dataset. This analysis involves performing various numerical computations including basic statistical measures, precise arithmetic operations, and generating random samples for simulation. Implement a function `analyze_data(data: List[float]) -> Dict[str, Any]` that performs the following: 1. **Basic Statistics**: - Calculate the mean, median, variance, and standard deviation of the input data using the `statistics` module. 2. **Precision Arithmetic**: - Calculate the sum of all elements using the `decimal` module to ensure precision. - Perform various operations (addition, subtraction, multiplication, division) on the first two elements of the dataset using `decimal` arithmetic. 3. **Random Sampling**: - Generate 100 random samples from the dataset using the `random` module and return these samples in the result. 4. **Results Format**: - Return a dictionary containing the following keys and their corresponding calculated values: ```json { \\"mean\\": ..., \\"median\\": ..., \\"variance\\": ..., \\"standard_deviation\\": ..., \\"sum\\": ..., \\"decimal_operations\\": { \\"addition\\": ..., \\"subtraction\\": ..., \\"multiplication\\": ..., \\"division\\": ... }, \\"random_samples\\": [...] } ``` # Input Format: - A list of floating-point numbers `data` (0 < len(data) <= 10^6). Assume the list will have at least two elements for arithmetic operations. # Output Format: - A dictionary with statistical measures, results of precise arithmetic operations, and a list of random samples from the given data. # Constraints: - Ensure that calculations involving precision use the `decimal` module. - Use appropriate functions from the `statistics` module for statistical measures. - Use the `random` module to generate random samples. # Example ```python data = [23.54, 12.32, 45.67, 89.23, 56.89, 34.12, 49.50, 72.18] result = analyze_data(data) print(result) ``` Expected output: ```json { \\"mean\\": 47.18125, \\"median\\": 47.935, \\"variance\\": 791.6360696428571, \\"standard_deviation\\": 28.136819249773705, \\"sum\\": Decimal(\'383.45\'), \\"decimal_operations\\": { \\"addition\\": Decimal(\'35.86\'), \\"subtraction\\": Decimal(\'11.22\'), \\"multiplication\\": Decimal(\'289.7428\'), \\"division\\": Decimal(\'1.9098039215686274\') }, \\"random_samples\\": [a list of 100 random samples from the input data] } ``` **Note:** - The specific values for addition, subtraction, multiplication, and division in the example output are based on the first two elements of the dataset. - Random samples may vary; hence actual values for `random_samples` in the example won\'t be fixed.","solution":"from typing import List, Dict, Any import statistics import decimal import random def analyze_data(data: List[float]) -> Dict[str, Any]: result = {} # Basic Statistics result[\\"mean\\"] = statistics.mean(data) result[\\"median\\"] = statistics.median(data) result[\\"variance\\"] = statistics.variance(data) result[\\"standard_deviation\\"] = statistics.stdev(data) # Precision Arithmetic decimal.getcontext().prec = 28 # Set precision for decimal arithmetic sum_decimal = sum(decimal.Decimal(str(x)) for x in data) result[\\"sum\\"] = sum_decimal first_elem = decimal.Decimal(str(data[0])) second_elem = decimal.Decimal(str(data[1])) decimal_operations = { \\"addition\\": first_elem + second_elem, \\"subtraction\\": first_elem - second_elem, \\"multiplication\\": first_elem * second_elem, \\"division\\": first_elem / second_elem } result[\\"decimal_operations\\"] = decimal_operations # Random Sampling random_samples = random.choices(data, k=100) result[\\"random_samples\\"] = random_samples return result"},{"question":"# Python Coding Assessment **Objective**: This question aims to evaluate your understanding of advanced function manipulation using the `functools` module in Python. You need to implement and work with cached properties and functools-based decorators to optimize and simplify function-related tasks. # Problem Statement You are required to create a class that represents a mathematical sequence generator with optimized property access and function invocation using the `functools` module. Implement the following features: 1. A class `SequenceGenerator` that initializes with a sequence of numbers. 2. Define a property named `average` that calculates and returns the average of the sequence. Use the `@cached_property` decorator to cache the result. 3. Implement a method `add_number` that adds a new number to the sequence and ensures that the cached `average` value is cleared when the sequence is modified. 4. Define a function `is_prime` that checks whether a given number is a prime number. Use the `@lru_cache` decorator to cache the result of prime checks. 5. Implement a method `nth_fibonacci` in the class to calculate the nth Fibonacci number, utilizing caching for optimization. Use the `@lru_cache` decorator. # Input/Output - **Class Initialization**: - Input: List of integers (e.g., `[1, 2, 3, 4, 5]`) - Output: None - **`average` Property** (cached): - Input: None - Output: Float representing the average of the sequence (e.g., `3.0`) - **`add_number` Method**: - Input: Integer (e.g., `6`) - Output: None - **`is_prime` Function** (cached): - Input: Integer (e.g., `7`) - Output: Boolean indicating if the number is prime (e.g., `True`) - **`nth_fibonacci` Method** (cached): - Input: Integer (n) indicating the nth position in the Fibonacci sequence (e.g., `10`) - Output: Integer representing the nth Fibonacci number (e.g., `55`) # Constraints - The sequence will contain at least one integer. - Elements added to the sequence will be non-negative integers. - The `nth_fibonacci` method will be called with values of n ranging between 0 and 100. # Implementation Requirements 1. You must use `functools.cached_property` for the `average` property. 2. You must use `functools.lru_cache` for the `is_prime` function and the `nth_fibonacci` method. 3. Your class should handle sequence additions gracefully by clearing the cached properties accordingly. # Performance Requirements - Your implementation should efficiently handle sequences with up to 10,000 numbers. - Caching mechanisms must be in place to optimize repeated calculations. Here is the skeleton code to get you started: ```python from functools import cached_property, lru_cache from math import sqrt class SequenceGenerator: def __init__(self, numbers): self._numbers = list(numbers) @cached_property def average(self): return sum(self._numbers) / len(self._numbers) def add_number(self, number): self._numbers.append(number) if \'average\' in self.__dict__: del self.__dict__[\'average\'] @staticmethod @lru_cache(maxsize=None) def is_prime(num): if num < 2: return False for n in range(2, int(sqrt(num)) + 1): if num % n == 0: return False return True @lru_cache(maxsize=None) def nth_fibonacci(self, n): if n == 0: return 0 elif n == 1: return 1 else: return self.nth_fibonacci(n-1) + self.nth_fibonacci(n-2) ``` # Example Usage ```python seq = SequenceGenerator([1, 2, 3, 4, 5]) print(seq.average) # Output: 3.0 seq.add_number(6) print(seq.average) # Output: 3.5 print(SequenceGenerator.is_prime(11)) # Output: True print(SequenceGenerator.is_prime(12)) # Output: False print(seq.nth_fibonacci(10)) # Output: 55 ``` Implement the class and its methods to pass the provided example usage.","solution":"from functools import cached_property, lru_cache from math import sqrt class SequenceGenerator: def __init__(self, numbers): self._numbers = list(numbers) @cached_property def average(self): return sum(self._numbers) / len(self._numbers) def add_number(self, number): self._numbers.append(number) if \'average\' in self.__dict__: del self.__dict__[\'average\'] @staticmethod @lru_cache(maxsize=None) def is_prime(num): if num < 2: return False for n in range(2, int(sqrt(num)) + 1): if num % n == 0: return False return True @lru_cache(maxsize=None) def nth_fibonacci(self, n): if n == 0: return 0 elif n == 1: return 1 else: return self.nth_fibonacci(n-1) + self.nth_fibonacci(n-2)"},{"question":"Objective: Demonstrate comprehension of Python\'s `fnmatch` module for Unix shell-style wildcard pattern matching and perform file and list operations using these patterns. Problem: You are working on a program that needs to perform file management tasks, including identifying and processing files based on patterns. 1. **Function 1: `match_filenames`** - Write a function named `match_filenames` that takes a list of filenames and a pattern string. The function should return a list of filenames that match the given pattern. Use `fnmatch.filter` for this task. - **Input:** - `filenames` (List of strings): A list of filenames to check. - `pattern` (String): The pattern to match the filenames against. - **Output:** - A list of filenames that match the input pattern. - **Example:** ```python filenames = [\'data1.txt\', \'data2.csv\', \'notes.txt\', \'image.png\'] pattern = \'*.txt\' match_filenames(filenames, pattern) # Output: [\'data1.txt\', \'notes.txt\'] ``` 2. **Function 2: `case_sensitive_match`** - Write a function named `case_sensitive_match` that performs a case-sensitive match, taking a filename and a pattern string as input, and returns `True` if they match, otherwise `False`. Use `fnmatch.fnmatchcase` for this task. - **Input:** - `filename` (String): The filename to check. - `pattern` (String): The pattern to match the filename against. - **Output:** - Boolean value indicating if the filename matches the pattern case-sensitively. - **Example:** ```python case_sensitive_match(\'DataReport.TXT\', \'*.TXT\') # Output: False ``` 3. **Function 3: `convert_pattern_to_regex`** - Write a function named `convert_pattern_to_regex` that takes a pattern and returns its equivalent regular expression string. Use `fnmatch.translate` for this task. - **Input:** - `pattern` (String): The pattern to convert. - **Output:** - The equivalent regular expression string. - **Example:** ```python convert_pattern_to_regex(\'*.txt\') # Output: \'(?s:.*.txt)Z\' ``` Constraints: - The functions should account for typical filename characters and patterns, as commonly used in Unix-like systems. - Ensure efficient handling of list operations with potentially large lists of filenames. Performance Requirements: - Solutions should handle lists with up to 10,000 filenames efficiently. - Assume typical filename lengths with moderate complexity patterns. Implement the three functions as specified above and provide test cases to demonstrate their correctness.","solution":"import fnmatch def match_filenames(filenames, pattern): Returns a list of filenames that match the given pattern. :param filenames: A list of filenames to check. :param pattern: The pattern to match the filenames against. :return: A list of filenames that match the input pattern. return fnmatch.filter(filenames, pattern) def case_sensitive_match(filename, pattern): Returns True if the filename matches the given pattern case-sensitively, otherwise False. :param filename: The filename to check. :param pattern: The pattern to match the filename against. :return: Boolean value indicating if the filename matches the pattern case-sensitively. return fnmatch.fnmatchcase(filename, pattern) def convert_pattern_to_regex(pattern): Converts a Unix shell-style wildcard pattern to a regular expression string. :param pattern: The pattern to convert. :return: The equivalent regular expression string. return fnmatch.translate(pattern)"},{"question":"**Objective:** Implement a Python function to execute a sequence of shell commands, capture their outputs, handle errors, and ensure efficient resource management using the `subprocess` module. **Function Signature:** ```python def run_commands(commands: list, timeout: int, combine_output: bool = False) -> list: pass ``` **Description:** Your task is to implement the function `run_commands(commands: list, timeout: int, combine_output: bool = False) -> list`. - **Input:** - `commands` (list): A list of command strings to be executed sequentially. Each command is a string. - `timeout` (int): The timeout value in seconds for each individual command. - `combine_output` (bool): If `True`, stderr should be combined with stdout; if `False`, keep them separate. - **Output:** - A list of dictionaries where each dictionary represents the result of a command execution. Each dictionary should have the following keys: - `\'command\'`: The command that was executed. - `\'returncode\'`: The return code of the command. - `\'stdout\'`: The standard output of the command (or combined output, if `combine_output` is `True`). - `\'stderr\'`: The standard error output of the command (only if `combine_output` is `False`). - `\'error\'`: Any exception message if an error occurred or if the command timed out. - **Constraints:** - Each command must be executed sequentially; the next command should not start until the previous one finishes. - Properly manage resources to prevent deadlocks. - Use `subprocess.run()` for simple cases and `subprocess.Popen` for advanced usage when needed. - Ensure to handle `TimeoutExpired` and other relevant exceptions gracefully. - **Performance Requirements:** - The solution should be efficient in terms of managing subprocesses and should not wait indefinitely on any one command. **Example:** ```python commands = [ \\"echo Hello, World!\\", \\"ls -l\\", \\"sleep 5\\" # for testing the timeout ] timeout = 2 combine_output = True results = run_commands(commands, timeout, combine_output) for res in results: print(res) ``` **Expected Output:** ```python [ { \'command\': \'echo Hello, World!\', \'returncode\': 0, \'stdout\': \'Hello, World!n\', \'stderr\': None, \'error\': None }, { \'command\': \'ls -l\', \'returncode\': 0, \'stdout\': \'contents of the directoryn\', \'stderr\': None, \'error\': None }, { \'command\': \'sleep 5\', \'returncode\': None, \'stdout\': \'\', \'stderr\': None, \'error\': \'Command timed out\' } ] ``` **Note:** - Make sure your function adheres to the description and handles edge cases, such as invalid commands and excessive output size. Good luck!","solution":"import subprocess def run_commands(commands: list, timeout: int, combine_output: bool = False) -> list: results = [] for cmd in commands: result = { \'command\': cmd, \'returncode\': None, \'stdout\': None, \'stderr\': None, \'error\': None } try: if combine_output: proc = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout) result[\'stdout\'] = proc.stdout result[\'returncode\'] = proc.returncode else: proc = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=timeout) result[\'stdout\'] = proc.stdout result[\'stderr\'] = proc.stderr result[\'returncode\'] = proc.returncode except subprocess.TimeoutExpired: result[\'error\'] = \'Command timed out\' except Exception as e: result[\'error\'] = str(e) results.append(result) return results"},{"question":"**Objective:** Implement a Python function that reads lines from a given file descriptor and writes these lines to a new file, but only if they contain a specified substring. Your function should leverage both low-level file descriptor operations and high-level Python file handling. **Function Signature:** ```python def filter_and_write_lines(fd: int, substring: str, output_file_path: str) -> None: Reads lines from a file descriptor, filters by substring, and writes to a new file. Parameters: fd (int): The file descriptor of the opened input file. substring (str): The substring to search for within each line of the input file. output_file_path (str): The path of the output file where matching lines are written. Returns: None pass ``` **Requirements:** 1. **Input/Output**: - The input `fd` is an integer file descriptor of an already opened file. - The input `substring` is a string, and only lines containing this substring should be written to the output file. - The `output_file_path` is a string specifying where to write the filtered lines. 2. **Constraints**: - Ensure that file descriptors and file objects are used appropriately. - Perform appropriate error handling, raising exceptions where necessary. - Do not assume the input file is small enough to fit into memory; handle it line by line. 3. **Example**: ```python # Assuming `fd` is a file descriptor for a file containing: # \\"Hello WorldnThis is a testnHello againn\\", and we want to filter lines containing \\"Hello\\". filter_and_write_lines(fd, \\"Hello\\", \\"output.txt\\") # After running the function, \\"output.txt\\" should contain: # \\"Hello WorldnHello againn\\". ``` **Performance Requirements**: - The function should handle large files efficiently, processing them line by line to avoid excessive memory consumption. **Hints**: - Use `PyObject_AsFileDescriptor`, `PyFile_GetLine`, and Python’s file handling capabilities to implement the function. - Remember to handle closing of files and file descriptors properly to avoid resource leaks. **Note**: - Assume all necessary imports and low-level setup have been made. Focus on the logic of the function itself.","solution":"import os def filter_and_write_lines(fd: int, substring: str, output_file_path: str) -> None: Reads lines from a file descriptor, filters by substring, and writes to a new file. Parameters: fd (int): The file descriptor of the opened input file. substring (str): The substring to search for within each line of the input file. output_file_path (str): The path of the output file where matching lines are written. Returns: None # Open the output file for writing with open(output_file_path, \'w\') as output_file: # Open the input file from the file descriptor with os.fdopen(fd, \'r\') as input_file: for line in input_file: if substring in line: output_file.write(line)"},{"question":"# Seaborn Advanced Plotting with Seaborn Objects You are tasked with writing a function using the seaborn library to visualize data from a provided dataset. The function should generate a plot with specific features, ensuring a comprehensive assessment of your understanding of seaborn’s capabilities. Function Signature ```python def visualize_seaborn_plot(dataset_name: str, output_filename: str) -> None: pass ``` Input - `dataset_name` (str): The name of the dataset to be loaded using seaborn\'s `load_dataset` function. Possible values are \\"dowjones\\" or \\"fmri\\". - `output_filename` (str): The filename (with path) where the resultant plot image should be saved. Expected Output The function should not return anything, but it should: 1. Load the specified dataset using seaborn’s `load_dataset`. 2. If the dataset is \\"dowjones\\": - Create a line plot of \'Date\' on the x-axis and \'Price\' on the y-axis. - Add a marker to indicate sampled data points. - Save the plot to the `output_filename`. 3. If the dataset is \\"fmri\\": - Query the dataset to keep only rows where \'region\' is \'parietal\' and \'event\' is \'stim\'. - Create line plots of \'timepoint\' on the x-axis and \'signal\' on the y-axis, using different lines for each \'subject\'. - Add error bands to the plot, grouped by \'event\'. - Save the plot to the `output_filename`. Example ```python # For the \\"dowjones\\" dataset visualize_seaborn_plot(\\"dowjones\\", \\"dowjones_plot.png\\") # For the \\"fmri\\" dataset visualize_seaborn_plot(\\"fmri\\", \\"fmri_plot.png\\") ``` Constraints - You must use seaborn\'s `objects` interface and the provided methods and examples. - Your solution should handle potential errors, such as invalid dataset names. - Ensure that the plot is saved with high resolution suitable for publication. Guidance - Refer to seaborn documentation for additional properties and methods if needed. - Make sure to write clean and readable code, organizing imports and function definitions correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_seaborn_plot(dataset_name, output_filename): Load a dataset using seaborn and create a plot as follows: - For the \\"dowjones\\" dataset: - Create a line plot of \'Date\' on the x-axis and \'Price\' on the y-axis. - Add a marker to indicate sampled data points. - Save the plot to the `output_filename`. - For the \\"fmri\\" dataset: - Query the dataset to keep only rows where \'region\' is \'parietal\' and \'event\' is \'stim\'. - Create line plots of \'timepoint\' on the x-axis and \'signal\' on the y-axis, using different lines for each \'subject\'. - Add error bands to the plot, grouped by \'event\'. - Save the plot to the `output_filename`. try: if dataset_name not in [\\"dowjones\\", \\"fmri\\"]: raise ValueError(\\"Invalid dataset name. Choose either \'dowjones\' or \'fmri\'.\\") data = sns.load_dataset(dataset_name) if dataset_name == \\"dowjones\\": plt.figure(figsize=(10, 6)) ax = sns.lineplot(data=data, x=\'Date\', y=\'Price\', marker=\'o\') ax.set_title(\\"Dow Jones Prices Over Time\\") plt.savefig(output_filename, dpi=300) elif dataset_name == \\"fmri\\": data_filtered = data[(data[\'region\'] == \'parietal\') & (data[\'event\'] == \'stim\')] plt.figure(figsize=(10, 6)) ax = sns.lineplot(data=data_filtered, x=\'timepoint\', y=\'signal\', hue=\'subject\', style=\'event\', marker=\'o\') ax.set_title(\\"FMRI Signal Over Time by Subject\\") plt.savefig(output_filename, dpi=300) plt.close() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Python Profiling Assignment** **Objective**: Write code to profile a given function, save the profiling results to a file, and display sorted profiling data based on cumulative time spent in the functions. # Problem Statement You are provided with a function `target_function(data)` that processes a list of integers. Your tasks are to: 1. Profile the `target_function` using `cProfile`. 2. Save the profiling results to a file named `\'profile_results.out\'`. 3. Read the profiling results using the `pstats` module. 4. Print the profiling statistics sorted by cumulative time. # Instructions 1. **Profiling Code**: - Import the required modules: `cProfile` and `pstats`. - Define your `target_function` as follows: ```python def target_function(data): result = 0 for num in data: if num % 2 == 0: result += num ** 2 else: result += num ** 3 return result ``` - Create a list of 10,000 integers and profile the `target_function` on it. 2. **Save the Profiling Results**: - Use `cProfile.run()` to profile `target_function` with the generated list and save the profiling results to `\'profile_results.out\'`. 3. **Load and Display Profiling Results**: - Use the `pstats` module to load the profiling data from `\'profile_results.out\'`. - Sort the profiling data by cumulative time. - Print the sorted profiling statistics to the console. # Expected Input and Output Format - **Input**: None (the required data will be generated within the script). - **Output**: Profile statistics printed to the console, sorted by cumulative time. # Constraints - Use the `cProfile` and `pstats` libraries as described. - The list of integers should contain exactly 10,000 elements. - The output file must be named `\'profile_results.out\'`. # Example Implementation Here is a skeleton of the solution to get you started: ```python import cProfile import pstats def target_function(data): result = 0 for num in data: if num % 2 == 0: result += num ** 2 else: result += num ** 3 return result def main(): data = list(range(1, 10001)) # Step 1: Profile the function and save the results cProfile.run(\'target_function(data)\', \'profile_results.out\') # Step 2: Load and display the profiling results with open(\'profile_results.out\', \'r\') as f: stats = pstats.Stats(\'profile_results.out\') stats.strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE).print_stats() if __name__ == \\"__main__\\": main() ``` # Submission Submit the final script containing the `target_function`, profiling code, and results printing. Ensure your solution prints the profiling results sorted by cumulative time.","solution":"import cProfile import pstats def target_function(data): result = 0 for num in data: if num % 2 == 0: result += num ** 2 else: result += num ** 3 return result def main(): data = list(range(1, 10001)) # Step 1: Profile the function and save the results cProfile.run(\'target_function(data)\', \'profile_results.out\') # Step 2: Load and display the profiling results stats = pstats.Stats(\'profile_results.out\') stats.strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE).print_stats() if __name__ == \\"__main__\\": main()"},{"question":"Objective The goal of this assignment is to assess your understanding of the MPS backend in PyTorch and your ability to leverage the Metal programming framework for efficient GPU computations on MacOS devices. Task You are required to implement a function named `prepare_and_train_model` that performs the following operations: 1. **Checks if the MPS backend is available**. If not, the function should return an appropriate message. 2. **Creates a tensor of shape (10, 10) filled with ones directly on the MPS device**. 3. **Performs a simple operation on the tensor (e.g., multiplying the tensor by a scalar value of 3)**. 4. **Defines a simple neural network model**. 5. **Moves the model to the MPS device**. 6. **Performs a forward pass of the tensor through the model to get a prediction**. 7. **Returns the final tensor and prediction as output**. Expected Function Signature ```python import torch import torch.nn as nn def prepare_and_train_model(): # Your implementation here return result_tensor, prediction ``` Constraints - Ensure that you handle the case when the MPS device is not available. - The simple neural network model can be anything (for example, a single linear layer). Example ```python # Assuming MPS device is available and YourFavoriteNet is defined: result_tensor, prediction = prepare_and_train_model() print(result_tensor) # Expected: Tensor of shape (10, 10) with each element being 3.0 on the MPS device print(prediction) # Expected: Output of the forward pass of the tensor through the model ``` Note: Do not forget to check the availability of the MPS backend as the first step in your implementation.","solution":"import torch import torch.nn as nn def prepare_and_train_model(): # Check if MPS is available if not torch.backends.mps.is_available(): return \\"MPS device is not available\\", None # Create a tensor of shape (10, 10) filled with ones directly on the MPS device device = torch.device(\\"mps\\") tensor = torch.ones((10, 10), device=device) # Perform a simple operation on the tensor (multiply by 3) result_tensor = tensor * 3 # Define a simple neural network model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = nn.Linear(10, 10) def forward(self, x): return self.linear(x) # Create model and move it to the MPS device model = SimpleNet().to(device) # Perform a forward pass of the tensor through the model to get a prediction prediction = model(result_tensor) return result_tensor, prediction"},{"question":"# PyTorch Meta Device Challenge Objective: Implement the `initialize_meta_tensor` function and demonstrate the use of the meta device by performing the following operations. Problem Statement: Write a function `initialize_meta_tensor` that accepts a shape tuple and initializes a tensor on the meta device with the given shape. Then, load this tensor on the `cpu` device with random values. Also, implement a function `initialize_empty_model` that initializes a given neural network model on the meta device, transfers it to the `cpu` device without initializing its parameters, and then manually initializes its parameters using a specified initialization function. Function Signature: ```python import torch import torch.nn as nn def initialize_meta_tensor(shape): Initialize a tensor on the meta device with the given shape. Params: - shape (tuple): Shape of the tensor to be created. Returns: - cpu_tensor (torch.Tensor): Tensor initialized with random values and transferred to the cpu device. pass def initialize_empty_model(model_cls, *args, **kwargs): Initialize a given neural network model on the meta device and transfer it to the cpu device without parameters. Then, manually initialize its parameters using the specified method. Params: - model_cls (class): A class of the model to be initialized. - *args: Arguments required to initialize the model class. - **kwargs: Keyword arguments required to initialize the model class. Returns: - model (torch.nn.Module): An instance of the model transferred to cpu device with parameters manually initialized. pass ``` Expected Example Implementations: ```python import torch import torch.nn as nn def initialize_meta_tensor(shape): with torch.device(\'meta\'): meta_tensor = torch.empty(shape, device=\'meta\') cpu_tensor = torch.randn_like(meta_tensor, device=\'cpu\') return cpu_tensor def initialize_empty_model(model_cls, *args, **kwargs): with torch.device(\'meta\'): model = model_cls(*args, **kwargs) model = model.to_empty(device=\\"cpu\\") def weight_init(m): if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) if m.bias is not None: nn.init.zeros_(m.bias) model.apply(weight_init) return model ``` Constraints: - Ensure that the meta tensor initialization does not involve any actual data computation. - Use `torch.empty_like` or similar functions to initialize the tensor on `cpu`. - You cannot initialize models with data on the meta device; must transfer them properly to the `cpu` and then initialize. Performance Requirements: - The functions should handle tensor shapes up to (1000, 1000) efficiently. - The initialization of model parameters should be done correctly to avoid uninitialized parameters. Testing: - Test `initialize_meta_tensor` with multiple shapes to ensure it works as expected. - Test `initialize_empty_model` with different neural network models, like `nn.Linear`, `nn.Conv2d`, etc. Good luck!","solution":"import torch import torch.nn as nn def initialize_meta_tensor(shape): Initialize a tensor on the meta device with the given shape. Params: - shape (tuple): Shape of the tensor to be created. Returns: - cpu_tensor (torch.Tensor): Tensor initialized with random values and transferred to the cpu device. meta_tensor = torch.empty(shape, device=\'meta\') cpu_tensor = torch.empty_like(meta_tensor, device=\'cpu\').normal_() return cpu_tensor def initialize_empty_model(model_cls, *args, **kwargs): Initialize a given neural network model on the meta device and transfer it to the cpu device without parameters. Then, manually initialize its parameters using the specified method. Params: - model_cls (class): A class of the model to be initialized. - *args: Arguments required to initialize the model class. - **kwargs: Keyword arguments required to initialize the model class. Returns: - model (torch.nn.Module): An instance of the model transferred to cpu device with parameters manually initialized. model = model_cls(*args, **kwargs).to(device=\'meta\') model = model.to_empty(device=\\"cpu\\") def weight_init(m): if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d): nn.init.xavier_uniform_(m.weight) if m.bias is not None: nn.init.zeros_(m.bias) model.apply(weight_init) return model"},{"question":"**Advanced Python Logger Implementation** **Objective:** To assess your understanding of the `logging.handlers` module and its advanced configuration features in Python. You will implement a sophisticated logger that utilizes multiple handlers to achieve comprehensive logging capabilities. **Problem Statement:** You are required to implement a custom logger class in Python that should use the following handlers from the `logging.handlers` module: 1. **FileHandler**: To log messages to a file named `app.log`. 2. **RotatingFileHandler**: To rotate this log file after it reaches a specified size limit. Maintain a backup count of 5. 3. **SMTPHandler**: To send an email alert for `CRITICAL` log messages to specified email recipients via SMTP. 4. **StreamHandler**: To print log messages to the console. **Function Specifications:** 1. **Class Name:** `AdvancedLogger` 2. **Method:** `__init__(self, log_filename: str, size_limit: int, email_config: dict)` - **Parameters:** - `log_filename` (string): The name of the log file. - `size_limit` (int): The maximum size of the log file in bytes before it gets rotated. - `email_config` (dict): A dictionary containing SMTP configuration with the following keys: - `\'mailhost\'`: (string or tuple) SMTP server address. Can be a string like \'localhost\' or a tuple like (\'smtp.example.com\', 587). - `\'fromaddr\'`: (string) Sender\'s email address. - `\'toaddrs\'`: (list of strings) List of recipient email addresses. - `\'subject\'`: (string) Subject line for the email. - `\'credentials\'`: (tuple) Credentials for the SMTP server in the form (`username`, `password`). Optional. - `\'secure\'`: (tuple) A tuple for using a secure protocol (TLS or SSL). Optional. - **Returns:** None **Constraints:** - The logger should write messages to a file and rotate the log file based on the given size limit. - The logger should send an email to the given addresses if a `CRITICAL` log message is logged. **Example Usage:** ```python email_config = { \'mailhost\': (\'smtp.example.com\', 587), \'fromaddr\': \'alert@example.com\', \'toaddrs\': [\'admin@example.com\'], \'subject\': \'Critical Error Alert\', \'credentials\': (\'user\', \'password\'), \'secure\': () } logger = AdvancedLogger(\'app.log\', 10485760, email_config) # 10 MB limit for file rotation logger.logger.info(\'This is an info message\') logger.logger.critical(\'This is a critical message\') ``` **Output:** - Write the log messages to the `app.log` file. - Rotate the log file once it reaches the size limit, keeping the last 5 backup files. - Send an email for the `CRITICAL` log message. **Hint:** You can refer to the `logging` module\'s `FileHandler`, `RotatingFileHandler`, `StreamHandler`, and `SMTPHandler` classes for implementation details.","solution":"import logging from logging.handlers import RotatingFileHandler, SMTPHandler class AdvancedLogger: def __init__(self, log_filename: str, size_limit: int, email_config: dict): # Create a logger self.logger = logging.getLogger(\'AdvancedLogger\') self.logger.setLevel(logging.DEBUG) # Create file handler file_handler = RotatingFileHandler( log_filename, maxBytes=size_limit, backupCount=5) file_handler.setLevel(logging.DEBUG) # Create console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # Add all handlers to the logger self.logger.addHandler(file_handler) self.logger.addHandler(console_handler) # If email configuration is provided, create and add SMTPHandler if email_config: mailhost = email_config[\'mailhost\'] fromaddr = email_config[\'fromaddr\'] toaddrs = email_config[\'toaddrs\'] subject = email_config[\'subject\'] credentials = email_config.get(\'credentials\', None) secure = email_config.get(\'secure\', None) smtp_handler = SMTPHandler(mailhost, fromaddr, toaddrs, subject, credentials, secure) smtp_handler.setLevel(logging.CRITICAL) self.logger.addHandler(smtp_handler) # Define a formatter and set it for all handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_handler.setFormatter(formatter) console_handler.setFormatter(formatter) if email_config: smtp_handler.setFormatter(formatter)"},{"question":"# Advanced Python Boolean Handling You are to implement a Python function that interacts with boolean values using concepts similar to those provided in the documentation. The goal is to assess your understanding of boolean operations and memory handling. Function Signature: ```python def custom_boolean_from_long(value: int) -> int: pass ``` Description: - Implement a function `custom_boolean_from_long(value: int) -> int` that takes a single integer as input and simulates the behavior of `PyBool_FromLong(long v)` as described in the documentation. - If `value` is non-zero, return `1` (to represent `Py_True`). - If `value` is zero, return `0` (to represent `Py_False`). Constraints: - The input integer will be in the range of -10^6 to 10^6. Example: ```python print(custom_boolean_from_long(0)) # Output: 0 print(custom_boolean_from_long(10)) # Output: 1 print(custom_boolean_from_long(-5)) # Output: 1 ``` Note: - Ensure the function works efficiently for the provided input range. - Pay attention to edge cases, such as the smallest and largest possible values within the provided range.","solution":"def custom_boolean_from_long(value: int) -> int: Returns 1 if value is non-zero, otherwise returns 0. Emulates the behavior of `PyBool_FromLong(long v)` in Python. return 1 if value != 0 else 0"},{"question":"**Question: DataFrame Memory Usage and Safe Mutations** You are given a pandas DataFrame `df` with unknown data and structure. You are required to implement the following two functions that explore memory usage and safely transform the DataFrame: 1. **memory_usage_report(df: pd.DataFrame) -> pd.Series:** This function should return a pandas Series summarizing the memory usage of each column in the DataFrame, including the DataFrame\'s index memory usage. The result should be more accurate and include the memory usage of objects inside any columns. 2. **safe_transform_df(df: pd.DataFrame, transform_func: Callable[[pd.Series], pd.Series]) -> pd.DataFrame:** This function accepts a DataFrame and a user-defined function (UDF) as inputs, applies the UDF to each column or row of the DataFrame, and returns a new transformed DataFrame. The transformation should be performed in a thread-safe manner, ensuring that the original DataFrame is not mutated during the process. Assume the UDF will be applied column-wise (`axis=0`). **Function Signatures:** ```python def memory_usage_report(df: pd.DataFrame) -> pd.Series: pass def safe_transform_df(df: pd.DataFrame, transform_func: Callable[[pd.Series], pd.Series]) -> pd.DataFrame: pass ``` **Input Format:** 1. `df`: A pandas DataFrame with an arbitrary structure and data. 2. `transform_func`: A user-defined function that takes a pandas Series (a DataFrame column) and returns a pandas Series. **Output Format:** 1. `memory_usage_report`: A pandas Series where the index represents the column names (including the index), and the values represent their respective memory usages in bytes. 2. `safe_transform_df`: A DataFrame that is the result of applying `transform_func` to each column of the original DataFrame in a thread-safe manner. **Constraints:** 1. You should not modify the original DataFrame `df` inside `safe_transform_df`. 2. Ensure the memory usage calculation in `memory_usage_report` includes an accurate deep introspection of objects in the DataFrame. **Example Usage:** ```python import pandas as pd import numpy as np # Example DataFrame data = { \'A\': [1, 2, 3], \'B\': [\'a\', \'b\', \'c\'], \'C\': [1.1, 2.2, 3.3] } df = pd.DataFrame(data) # Memory Usage Report mem_usage = memory_usage_report(df) print(mem_usage) # User-defined transformation function def add_one(series: pd.Series) -> pd.Series: return series.apply(lambda x: x + 1 if pd.api.types.is_numeric_dtype(series) else x) # Transform DataFrame transformed_df = safe_transform_df(df, add_one) print(transformed_df) # Original DataFrame should remain unchanged print(df) ``` In this example: 1. The `memory_usage_report` function would return a Series showing the memory usage of each column and the index. 2. The `safe_transform_df` would return a new DataFrame with numeric columns incremented by one, without altering the original DataFrame. This question assesses the understanding of memory management, safe data manipulation, and working with DataFrames in pandas.","solution":"import pandas as pd from typing import Callable def memory_usage_report(df: pd.DataFrame) -> pd.Series: Returns a pandas Series summarizing the memory usage of each column including the DataFrame\'s index memory usage. memory_usage = df.memory_usage(deep=True) memory_usage.index = [\\"Index\\"] + list(df.columns) return memory_usage def safe_transform_df(df: pd.DataFrame, transform_func: Callable[[pd.Series], pd.Series]) -> pd.DataFrame: Applies a user-defined function to each column of the DataFrame in a thread-safe manner and returns a new transformed DataFrame. # Create a copy of the DataFrame to ensure the original DataFrame is not mutated df_copy = df.copy() for column in df_copy.columns: df_copy[column] = transform_func(df_copy[column]) return df_copy"},{"question":"# Advanced Task: Custom TCP Logging Server Using asyncio Transports and Protocols Objective: You are required to create a custom TCP logging server using asyncio\'s transport and protocol abstractions. This server should accept incoming connections, log the received messages, and maintain the connections until explicitly closed. Requirements: 1. **Server Protocol (`LoggingServerProtocol`)**: - On successful connection (`connection_made()`), log the peer address. - When data is received (`data_received()`), log the message and prepend a timestamp. - On connection loss (`connection_lost()`), log a disconnection message including any exception details if applicable. 2. **Client Protocol (`LoggingClientProtocol`)**: - On successful connection (`connection_made()`), send a welcome message. - On data received (`data_received()`), log the received data. - On connection loss (`connection_lost()`), log the closure. 3. **Main Function**: - Create and run the server using low-level asyncio APIs (`loop.create_server`). - Accept multiple concurrent client connections. - Run the server on `127.0.0.1` and port `12345`. Expected Input and Output Formats: - **Input**: Messages sent by the client to the server. - **Output**: Each message received by the server should be logged with a timestamp, client address, and message content. Constraints: - Ensure the server can handle multiple concurrent client connections. - Implement all necessary error handling. Performance Requirements: - The server should handle and log messages in real-time without blocking on any single connection. Sample Log Output: ``` [2023-10-12 10:20:30] Connection from (\'127.0.0.1\', 56789) [2023-10-12 10:20:32] Received from (\'127.0.0.1\', 56789): \'Hello Server\' [2023-10-12 10:20:35] Connection lost from (\'127.0.0.1\', 56789) ``` Implementation: ```python import asyncio import datetime class LoggingServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') print(\'[{}] Connection from {}\'.format(datetime.datetime.now(), self.peername)) def data_received(self, data): message = data.decode() print(\'[{}] Received from {}: {!r}\'.format(datetime.datetime.now(), self.peername, message)) self.transport.write(b\'Message receivedn\') def connection_lost(self, exc): if exc: print(\'[{}] Connection lost from {} with error: {}\'.format(datetime.datetime.now(), self.peername, exc)) else: print(\'[{}] Connection lost from {}\'.format(datetime.datetime.now(), self.peername)) class LoggingClientProtocol(asyncio.Protocol): def __init__(self, message): self.message = message def connection_made(self, transport): self.transport = transport print(\'Sending message: {}\'.format(self.message)) self.transport.write(self.message.encode()) def data_received(self, data): print(\'Received: {}\'.format(data.decode())) def connection_lost(self, exc): print(\'The server closed the connection\') self.transport.close() async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: LoggingServerProtocol(), \'127.0.0.1\', 12345) print(\'Server started\') async with server: await server.serve_forever() # Run the server asyncio.run(main()) ``` **Note**: Ensure to test the implementation with multiple clients sending messages concurrently to the server.","solution":"import asyncio import datetime class LoggingServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') print(f\'[{datetime.datetime.now()}] Connection from {self.peername}\') def data_received(self, data): message = data.decode() timestamp = datetime.datetime.now() print(f\'[{timestamp}] Received from {self.peername}: {message!r}\') log_message = f\'[{timestamp}] {self.peername}: {message}n\' # Save to log file (optional but useful for actual logging) with open(\'server.log\', \'a\') as f: f.write(log_message) self.transport.write(b\'Message receivedn\') def connection_lost(self, exc): if exc: print(f\'[{datetime.datetime.now()}] Connection lost from {self.peername} with error: {exc}\') else: print(f\'[{datetime.datetime.now()}] Connection lost from {self.peername}\') class LoggingClientProtocol(asyncio.Protocol): def __init__(self, message): self.message = message def connection_made(self, transport): self.transport = transport print(f\'Sending message: {self.message}\') self.transport.write(self.message.encode()) def data_received(self, data): print(f\'Received: {data.decode()}\') def connection_lost(self, exc): print(\'The server closed the connection\') self.transport.close() async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: LoggingServerProtocol(), \'127.0.0.1\', 12345) print(\'Server started\') async with server: await server.serve_forever() # Run the server if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Pandas DataFrame Manipulation and Analysis Background You are given a dataset of a hypothetical company\'s employee records stored in a CSV file named `employees.csv`. The dataset contains the following columns: - `EmployeeID`: Unique identifier for each employee. - `Name`: Name of the employee. - `Age`: Age of the employee. - `Department`: Department where the employee works. - `Salary`: Salary of the employee. - `JoiningDate`: Date when the employee joined the company. Task Your task is to perform a series of operations to analyze and manipulate this data using pandas. Implement the function `employee_analysis(filepath: str) -> pd.DataFrame` that will read the data from the CSV file and perform the following steps: 1. **Reading Data**: Load the data from the CSV file into a pandas DataFrame. 2. **Handling Missing Values**: - Remove any rows where the `Salary` or `Department` is missing. - Replace missing values in the `Age` column with the mean age of the remaining employees. 3. **Data Transformation**: - Convert the `JoiningDate` column to datetime format. - Create a new column `YearsInCompany` that calculates the number of years each employee has been with the company as of the current date. 4. **Aggregation**: - Compute the mean salary for each department. - Identify the department with the highest mean salary. 5. **Filtering**: - Filter the data to include only employees who are older than 30 years and have a salary greater than the mean salary of their department. 6. **Sorting and Exporting**: - Sort the filtered data by `YearsInCompany` in descending order. - Export the resulting DataFrame to a new CSV file named `filtered_employees.csv`. Constraints - Assume the `JoiningDate` column is in the format `YYYY-MM-DD`. - Use `pandas` for all data manipulation. - Ensure your code handles possible cases where the dataset may have missing values or requires type conversion. Performance Requirements - The function should efficiently handle datasets up to 1 million records without significant performance degradation. Expected Function Signature ```python import pandas as pd def employee_analysis(filepath: str) -> pd.DataFrame: pass ``` Your implementation should be tested and validated to ensure it meets all the outlined steps correctly.","solution":"import pandas as pd from datetime import datetime def employee_analysis(filepath: str) -> pd.DataFrame: # Step 1: Reading Data df = pd.read_csv(filepath) # Step 2: Handling Missing Values df.dropna(subset=[\'Salary\', \'Department\'], inplace=True) df[\'Age\'].fillna(df[\'Age\'].mean(), inplace=True) # Step 3: Data Transformation df[\'JoiningDate\'] = pd.to_datetime(df[\'JoiningDate\']) current_date = datetime.now() df[\'YearsInCompany\'] = df[\'JoiningDate\'].apply(lambda x: (current_date - x).days // 365) # Step 4: Aggregation mean_salaries = df.groupby(\'Department\')[\'Salary\'].mean() highest_mean_salary_dept = mean_salaries.idxmax() # Step 5: Filtering df = df.merge(mean_salaries.rename(\'MeanDepartmentSalary\'), on=\'Department\') filtered_df = df[(df[\'Age\'] > 30) & (df[\'Salary\'] > df[\'MeanDepartmentSalary\'])] # Step 6: Sorting and Exporting filtered_df = filtered_df.sort_values(by=\'YearsInCompany\', ascending=False) filtered_df.to_csv(\'filtered_employees.csv\', index=False) return filtered_df"},{"question":"**Objective**: Implement and optimize a matrix multiplication on XPU with memory management and correct usage of streams for parallel execution. **Problem Statement**: You are required to implement a function `xpu_matrix_multiplication` that performs matrix multiplication on Intel\'s XPU using PyTorch\'s XPU functionalities. This function should: 1. Initialize the XPU. 2. Set the random seed for reproducibility. 3. Allocate matrices A and B of given sizes on the XPU, with their elements initialized to random values. 4. Use a Stream to perform matrix multiplication in parallel. 5. Return the resulting matrix. Additionally, your implementation should manage memory efficiently by: - Checking the available memory before allocation. - Clearing the cache after the computation. - Releasing the memory as soon as it is no longer needed. **Function Signature**: ```python def xpu_matrix_multiplication(m: int, n: int, p: int) -> torch.Tensor: pass ``` **Parameters**: - `m` (int): Number of rows of matrix `A`. - `n` (int): Number of columns of matrix `B` and rows of matrix `B`. - `p` (int): Number of columns of matrix `A` and rows of matrix `B`. **Returns**: - `torch.Tensor`: The resulting matrix from the multiplication. **Constraints**: - Implement the function utilizing PyTorch\'s XPU module functionalities. - Ensure that the operations are performed on XPU. - Enforce memory management best practices as outlined. **Example**: ```python result = xpu_matrix_multiplication(1000, 2000, 1500) print(result.shape) # Should print torch.Size([1000, 2000]) ``` **Additional Requirements**: - Ensure that the computation is reproducible by setting a random seed. - Handle any potential exceptions related to memory allocation or device availability. **Hints**: - Use `torch.xpu.init()` to initialize the XPU device. - Use `torch.xpu.manual_seed(seed_value)` to set the random seed. - Allocate matrices using `torch.rand((size), device=torch.xpu.current_device())`. - Use `torch.xpu.stream()` to create and manage streams. - Check memory usage with functions like `torch.xpu.memory_allocated()`. - Clear memory cache using `torch.xpu.empty_cache()`.","solution":"import torch def xpu_matrix_multiplication(m: int, n: int, p: int) -> torch.Tensor: Performs matrix multiplication on Intel\'s XPU using PyTorch\'s XPU functionalities. Args: m (int): Number of rows of matrix A. n (int): Number of columns of matrix B and rows of matrix A. p (int): Number of columns of matrix B. Returns: torch.Tensor: The resulting matrix from the multiplication. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available\\") # Initialize the XPU torch.xpu.init() # Set random seed for reproducibility seed = 42 torch.manual_seed(seed) torch.xpu.manual_seed(seed) # Allocate matrices A and B on the XPU try: A = torch.rand((m, p), device=torch.xpu.current_device()) B = torch.rand((p, n), device=torch.xpu.current_device()) except RuntimeError as e: raise RuntimeError(\\"Failed to allocate memory on XPU: \\" + str(e)) # Create a stream for parallel execution stream = torch.xpu.stream() with torch.xpu.stream(stream): # Perform matrix multiplication result = torch.mm(A, B) # Ensure completion of the stream stream.synchronize() # Clear the cache to manage memory torch.xpu.empty_cache() return result"},{"question":"You are required to implement a function that utilizes Python\'s underlying C API for bytearray objects (as described in the documentation). The aim is to create, manipulate, and return a specific bytearray object following a series of operations. # Function Specification **Function Name**: `process_bytearray` **Input Parameters**: 1. `input_string` (str): A string of characters to be converted into a bytearray. 2. `additional_string` (str): A string of characters to be concatenated to the bytearray created from `input_string`. 3. `resize_length` (int): An integer specifying the new length to which the final bytearray should be resized. **Returns**: - A bytearray object that has gone through the specified operations. # Instructions 1. Convert `input_string` into a bytearray. 2. Convert `additional_string` into a bytearray. 3. Concatenate the two bytearrays. 4. Resize the resulting concatenated bytearray to `resize_length`. # Example ```python def process_bytearray(input_string: str, additional_string: str, resize_length: int) -> bytearray: # Your implementation here # Example usage result = process_bytearray(\\"hello\\", \\"world\\", 8) print(result) # Output: bytearray(b\'hellowor\') ``` # Constraints - Both `input_string` and `additional_string` will only contain ASCII characters. - `resize_length` will always be a non-negative integer no greater than the total length of the concatenated bytearray. - You are encouraged to use the provided C API functions in a conceptual manner since this question assumes we\'re working in Python code but understanding the underlying C operations. # Notes - The focus here is to understand the sequence of operations on bytearrays. - Efficiency and correct use of bytearray manipulation functions are essential. - Students should assume the direct equivalent methods and operations in Python if an actual C API overlay isn’t directly callable within their environment.","solution":"def process_bytearray(input_string: str, additional_string: str, resize_length: int) -> bytearray: # Convert input_string and additional_string into bytearrays bytearray1 = bytearray(input_string, \'ascii\') bytearray2 = bytearray(additional_string, \'ascii\') # Concatenate the two bytearrays concatenated_bytearray = bytearray1 + bytearray2 # Resize the concatenated bytearray to the specified length if resize_length < len(concatenated_bytearray): resized_bytearray = concatenated_bytearray[:resize_length] else: resized_bytearray = concatenated_bytearray.ljust(resize_length, b\'x00\') return resized_bytearray # Example usage result = process_bytearray(\\"hello\\", \\"world\\", 8) print(result) # Output: bytearray(b\'hellowor\')"},{"question":"# Task: Objective: Demonstrate your understanding of sparse tensors in PyTorch by implementing functions for constructing sparse tensors and performing basic operations on them. Problem Statement: 1. Write a function `create_sparse_tensor(data, indices, size, layout)` that takes the following inputs: - `data` (list of values): The non-zero values to be stored in the sparse tensor. - `indices` (list of index tuples): The indices where the non-zero values are located in the sparse tensor. - `size` (tuple): The size of the sparse tensor. - `layout` (str): The data layout format of the sparse tensor, one of `\\"coo\\"`, `\\"csr\\"`, or `\\"csc\\"`. The function should return a sparse tensor created using the provided data, indices, size, and layout. 2. Write a function `sparse_tensor_addition(tensor1, tensor2)` that takes two sparse tensors of the same layout and size, and returns their sum as a dense tensor. 3. Write a function `sparse_matrix_vector_multiplication(sparse_matrix, dense_vector)` that takes: - `sparse_matrix`: A 2D sparse tensor (can be of any supported sparse format). - `dense_vector`: A dense vector (1D tensor). The function should return the result of the sparse matrix-vector multiplication as a dense vector. 4. Write a function `convert_to_sparse_and_back(tensor, layout)` that: - Converts the given dense tensor to a sparse tensor of the specified layout (`\\"coo\\"`, `\\"csr\\"`, or `\\"csc\\"`). - Converts the sparse tensor back to a dense tensor. - Returns the resulting dense tensor. # Constraints: - You may assume the input tensors are always valid. - Use the appropriate PyTorch functions and methods for conversions and operations. # Example: ```python import torch def create_sparse_tensor(data, indices, size, layout): # implementation here def sparse_tensor_addition(tensor1, tensor2): # implementation here def sparse_matrix_vector_multiplication(sparse_matrix, dense_vector): # implementation here def convert_to_sparse_and_back(tensor, layout): # implementation here # Example usage: data = [3, 4, 5] indices = [(0, 2), (1, 0), (1, 2)] size = (2, 3) layout = \\"coo\\" sparse_tensor = create_sparse_tensor(data, indices, size, layout) print(sparse_tensor) # tensor(indices=tensor([[0, 1, 1], # [2, 0, 2]]), # values=tensor([3, 4, 5]), # size=(2, 3), nnz=3, layout=torch.sparse_coo) dense_tensor = torch.tensor([[0, 0, 3], [4, 0, 5]], dtype=torch.float32) result_tensor = convert_to_sparse_and_back(dense_tensor, \\"csr\\") print(result_tensor) # tensor([[0., 0., 3.], # [4., 0., 5.]]) ``` # Note: - Ensure your function handles errors and invalid inputs gracefully. - Pay attention to the performance for large sparse tensors.","solution":"import torch def create_sparse_tensor(data, indices, size, layout): Create a sparse tensor with given data, indices, size and layout. values = torch.tensor(data, dtype=torch.float32) indices = torch.tensor(list(zip(*indices)), dtype=torch.int64) if layout == \\"coo\\": return torch.sparse_coo_tensor(indices, values, size) elif layout == \\"csr\\": return torch.sparse_csr_tensor(indices[0], indices[1], values, size) elif layout == \\"csc\\": return torch.sparse_csc_tensor(indices[0], indices[1], values, size) else: raise ValueError(\\"Unsupported sparse layout: {}\\".format(layout)) def sparse_tensor_addition(tensor1, tensor2): Add two sparse tensors and return the result as a dense tensor. if tensor1.layout != tensor2.layout: raise ValueError(\\"Tensor layouts must match.\\") if tensor1.size() != tensor2.size(): raise ValueError(\\"Tensor sizes must match.\\") return (tensor1.to_dense() + tensor2.to_dense()) def sparse_matrix_vector_multiplication(sparse_matrix, dense_vector): Perform sparse matrix-vector multiplication and return the result as a dense vector. if len(dense_vector.size()) != 1: raise ValueError(\\"dense_vector must be a 1D tensor.\\") return torch.mv(sparse_matrix.to_dense(), dense_vector) def convert_to_sparse_and_back(tensor, layout): Convert a dense tensor to a sparse tensor and then back to a dense tensor. if layout == \\"coo\\": sparse_tensor = tensor.to_sparse_coo() elif layout == \\"csr\\": sparse_tensor = tensor.to_sparse_csr() elif layout == \\"csc\\": sparse_tensor = tensor.to_sparse_csc() else: raise ValueError(\\"Unsupported sparse layout: {}\\".format(layout)) return sparse_tensor.to_dense()"},{"question":"# Python Coding Assessment Question **Objective:** Demonstrate your understanding of Unicode manipulation and conversion by working with different character encodings. Implement a function in Python that takes a Unicode string, performs various transformations, and returns the results in specific formats. **Problem Statement:** Write a Python function `transform_unicode_string(input_str)` that performs the following tasks: 1. **Validation**: Ensure `input_str` is a valid Unicode string. If it is not, raise a `TypeError` with an appropriate message. 2. **Uppercase Conversion**: Convert and return the Unicode string to uppercase using Unicode properties. 3. **UTF-8 Encoding**: Encode the uppercase string to a UTF-8 encoded bytes object. 4. **Latin-1 Encoding**: Encode the uppercase string to a Latin-1 encoded bytes object, ensuring all characters are within the Latin-1 range. If a character is out of range, return `None`. 5. **Substring Extraction**: Return a substring of the original Unicode string starting from the 5th character to the 10th character (1-based index). **Input Format:** - `input_str` (string): A Unicode string to be processed. **Output Format:** A tuple containing: 1. Uppercase Unicode string 2. UTF-8 encoded bytes object 3. Latin-1 encoded bytes object (or `None` if out of range) 4. Substring from the 5th to the 10th character **Function Signature:** ```python def transform_unicode_string(input_str: str) -> tuple: pass ``` **Constraints:** - The input string will have a minimum length of 10 characters. - You can use standard Python libraries and methods for string manipulation. **Example:** ```python input_str = \\"Hello, Unicode! 😊\\" result = transform_unicode_string(input_str) print(result) ``` Expected Output: ```plaintext (\'HELLO, UNICODE! 😊\', b\'HELLO, UNICODE! xf0x9fx98x8a\', b\'HELLO, UNICODE! ?\', \'o, Uni\') ``` **Explanation:** 1. Validation: Checks that `input_str` is a valid Unicode string. 2. Uppercase Conversion: Converts \\"Hello, Unicode! 😊\\" to \\"HELLO, UNICODE! 😊\\". 3. UTF-8 Encoding: Encodes the uppercase string to UTF-8. 4. Latin-1 Encoding: Converts the uppercase to Latin-1, replacing any out-of-range characters with a placeholder `?`. 5. Substring Extraction: Extracts characters from indices 5 to 10 from the original string. **Notes:** - Make sure to handle any potential exceptions that might arise during encoding. - Use Python\'s built-in capabilities for string manipulations and encoding conversions.","solution":"def transform_unicode_string(input_str: str) -> tuple: Transforms a Unicode string by performing various tasks: 1. Validation: Ensure input_str is a valid Unicode string. 2. Uppercase Conversion: Converts the Unicode string to uppercase. 3. UTF-8 Encoding: Encodes the uppercase string to UTF-8. 4. Latin-1 Encoding: Encodes the uppercase string to Latin-1. 5. Substring Extraction: Extracts a substring from the 5th to the 10th character. # Validation if not isinstance(input_str, str): raise TypeError(\\"Input must be a valid Unicode string\\") # Uppercase Conversion upper_str = input_str.upper() # UTF-8 Encoding utf8_encoded = upper_str.encode(\'utf-8\') # Latin-1 Encoding try: latin1_encoded = upper_str.encode(\'latin-1\') except UnicodeEncodeError: latin1_encoded = None # Substring Extraction (1-based index, so start from index 4 and end at index 10) substring = input_str[4:10] return (upper_str, utf8_encoded, latin1_encoded, substring)"},{"question":"**Question: Comprehensive Class Implementation with Inheritance and Iterators** Write a Python program that defines a set of classes to represent a library system. The system should include classes to represent books, members, and the library itself. Your implementation should demonstrate comprehension of class construction, inheritance, scopes, namespaces, iterators, and generators. # Specifications 1. **Class `Book`**: - Attributes: - `title` (string): title of the book. - `author` (string): author of the book. - Methods: - `__init__(self, title, author)`: Initializes `title` and `author`. 2. **Class `Member`**: - Attributes: - `name` (string): name of the member. - `borrowed_books` (list): List of borrowed `Book` objects. - Methods: - `__init__(self, name)`: Initializes `name` and an empty list `borrowed_books`. - `borrow_book(self, book: Book)`: Appends a given `book` to `borrowed_books`. - `return_book(self, book: Book)`: Removes a given `book` from `borrowed_books`. - `list_books(self)`: Returns an iterator over `borrowed_books`. 3. **Class `Library`**: - Attributes: - `books` (list): List of `Book` objects in the library. - `members` (list): List of `Member` objects. - Methods: - `__init__(self)`: Initializes an empty list `books` and `members`. - `add_book(self, book: Book)`: Adds a given `book` to `books`. - `register_member(self, member: Member)`: Adds a given `member` to `members`. - `borrow_book(self, member_name: str, book_title: str)`: Allows a `member` to borrow a `book` by providing their `name` and `book`\'s `title`. Removes the book from library\'s `books` and adds it to member\'s `borrowed_books`. - `return_book(self, member_name: str, book_title: str)`: Allows a `member` to return a `book` by providing their `name` and `book`\'s `title`. Removes the book from member\'s `borrowed_books` and adds it back to library\'s `books`. # Constraints - A member can only borrow books that are currently available in the library. - A member can only return books that they have borrowed from the library. - Use proper error handling for cases like trying to borrow a book that is not available or trying to return a book that was not borrowed. # Example Usage ```python # Create a library library = Library() # Create some books book1 = Book(\'To Kill a Mockingbird\', \'Harper Lee\') book2 = Book(\'1984\', \'George Orwell\') # Add books to the library library.add_book(book1) library.add_book(book2) # Create a member member = Member(\'Alice\') # Register the member library.register_member(member) # Member borrows a book library.borrow_book(\'Alice\', \'1984\') # List borrowed books borrowed_books = member.list_books() for book in borrowed_books: print(book.title) # Member returns a book library.return_book(\'Alice\', \'1984\') ``` Write the classes `Book`, `Member`, and `Library` in Python following the provided specifications. Validate your code with the provided example usage and ensure it handles edge cases gracefully.","solution":"class Book: def __init__(self, title, author): self.title = title self.author = author class Member: def __init__(self, name): self.name = name self.borrowed_books = [] def borrow_book(self, book): self.borrowed_books.append(book) def return_book(self, book): self.borrowed_books.remove(book) def list_books(self): return iter(self.borrowed_books) class Library: def __init__(self): self.books = [] self.members = [] def add_book(self, book): self.books.append(book) def register_member(self, member): self.members.append(member) def borrow_book(self, member_name, book_title): member = next((m for m in self.members if m.name == member_name), None) book = next((b for b in self.books if b.title == book_title), None) if not member: raise ValueError(\\"Member not found\\") if not book: raise ValueError(\\"Book not available in the library\\") self.books.remove(book) member.borrow_book(book) def return_book(self, member_name, book_title): member = next((m for m in self.members if m.name == member_name), None) book = next((b for b in member.borrowed_books if b.title == book_title), None) if not member: raise ValueError(\\"Member not found\\") if not book: raise ValueError(\\"Book not borrowed by the member\\") member.return_book(book) self.books.append(book)"},{"question":"**Advanced Seaborn KDE Plotting:** You are provided with three datasets: `tips`, `iris`, and `diamonds`, available directly from seaborn. You are required to analyze these datasets and create different KDE plots as specified below. # Task 1. **Univariate KDE Plot:** - Load the `tips` dataset. - Plot a KDE of the `total_bill` column. - Adjust the bandwidth to `0.3` for a smoother appearance. - Set the plot to display along the y-axis. - Use `hue` parameter to visualize the KDE for different `time` (`lunch` vs `dinner`). 2. **Conditional KDE with Stacking and Normalizing:** - Use the `tips` dataset again. - Create a stacked KDE plot of `total_bill` differentiated by `time`. - Normalize the stacked distribution so that each KDE sums to 1 at every point on the x-axis. 3. **Bivariate KDE Plot:** - Load the `geyser` dataset. - Plot a bivariate KDE of `waiting` and `duration`. - Use `hue` to differentiate between `kind`. - Modify the plot to show filled contours with the `mako` colormap. 4. **Customized KDE Plot:** - Load the `diamonds` dataset. - Plot a KDE of the `price` column. - Apply log scale to the KDE plot. - Modify the appearance by filling the area under the curve and using a `crest` palette. - Ensure that the lines are not visible by setting `linewidth=0`. # Constraints: - The plots must be clearly labeled with appropriate titles and axis labels. - Each plot should use the seaborn library’s functions and options to fulfill the given specifications. # Input: No explicit input. Use the datasets provided within seaborn. # Output: 4 separate KDE plots as specified in the tasks above. # Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Univariate KDE Plot tips = sns.load_dataset(\\"tips\\") sns.kdeplot(data=tips, y=\\"total_bill\\", hue=\\"time\\", bw_adjust=0.3) plt.title(\\"Smoothed KDE of Total Bill by Time\\") plt.show() # Task 2: Conditional KDE with Stacking and Normalizing sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.title(\\"Normalized Stacked KDE of Total Bill by Time\\") plt.show() # Task 3: Bivariate KDE Plot geyser = sns.load_dataset(\\"geyser\\") sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, cmap=\\"mako\\") plt.title(\\"Bivariate KDE of Waiting vs Duration by Kind\\") plt.show() # Task 4: Customized KDE Plot diamonds = sns.load_dataset(\\"diamonds\\") sns.kdeplot(data=diamonds, x=\\"price\\", log_scale=True, fill=True, palette=\\"crest\\", linewidth=0) plt.title(\\"Log-Scaled KDE of Diamond Prices\\") plt.show() ``` Ensure you adhere to the guidance and specifications for each task to successfully complete the assessment.","solution":"import seaborn as sns import matplotlib.pyplot as plt def univariate_kde_plot(): tips = sns.load_dataset(\\"tips\\") sns.kdeplot(data=tips, y=\\"total_bill\\", hue=\\"time\\", bw_adjust=0.3) plt.title(\\"Smoothed KDE of Total Bill by Time\\") plt.ylabel(\\"Total Bill\\") plt.xlabel(\\"Density\\") plt.show() def conditional_kde_with_stacking(): tips = sns.load_dataset(\\"tips\\") sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.title(\\"Normalized Stacked KDE of Total Bill by Time\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Proportion\\") plt.show() def bivariate_kde_plot(): geyser = sns.load_dataset(\\"geyser\\") sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, cmap=\\"mako\\") plt.title(\\"Bivariate KDE of Waiting vs Duration by Kind\\") plt.xlabel(\\"Waiting\\") plt.ylabel(\\"Duration\\") plt.show() def customized_kde_plot(): diamonds = sns.load_dataset(\\"diamonds\\") sns.kdeplot(data=diamonds, x=\\"price\\", log_scale=True, fill=True, palette=\\"crest\\", linewidth=0) plt.title(\\"Log-Scaled KDE of Diamond Prices\\") plt.xlabel(\\"Price (log scale)\\") plt.ylabel(\\"Density\\") plt.show()"},{"question":"Question # Sorting and Custom Key Function Implementation You are given a list of employees where each employee is represented as a dictionary with the following keys: `name` (string), `department` (string), and `salary` (integer). **Examples:** ```python employees = [ {\'name\': \'Alice\', \'department\': \'HR\', \'salary\': 50000}, {\'name\': \'Bob\', \'department\': \'Engineering\', \'salary\': 80000}, {\'name\': \'Charlie\', \'department\': \'Marketing\', \'salary\': 60000}, {\'name\': \'David\', \'department\': \'Engineering\', \'salary\': 70000} ] ``` # Task Write a function `sort_employees(employees: List[Dict[str, Any]], sort_by: str, reverse: bool = False) -> List[Dict[str, Any]]` that sorts the list of employees according to the specified key (`sort_by`) in either ascending (default) or descending order (specified by the `reverse` parameter). # Input - `employees` (List[Dict[str, Any]]): A list of dictionaries representing employees. - `sort_by` (str): The key to sort by, which can be either `\\"name\\"`, `\\"department\\"`, or `\\"salary\\"`. - `reverse` (bool): A boolean parameter indicating whether to sort in descending order. # Output - A list of sorted employee dictionaries. # Constraints - The `sort_by` parameter will always be one of `\\"name\\"`, `\\"department\\"`, or `\\"salary\\"`. - The `employees` list will contain at least one employee dictionary. - Employee names, departments, and salaries are guaranteed to be non-null. # Example ```python employees = [ {\'name\': \'Alice\', \'department\': \'HR\', \'salary\': 50000}, {\'name\': \'Bob\', \'department\': \'Engineering\', \'salary\': 80000}, {\'name\': \'Charlie\', \'department\': \'Marketing\', \'salary\': 60000}, {\'name\': \'David\', \'department\': \'Engineering\', \'salary\': 70000} ] result = sort_employees(employees, sort_by=\'department\', reverse=False) # Output: # [ # {\'name\': \'Bob\', \'department\': \'Engineering\', \'salary\': 80000}, # {\'name\': \'David\', \'department\': \'Engineering\', \'salary\': 70000}, # {\'name\': \'Alice\', \'department\': \'HR\', \'salary\': 50000}, # {\'name\': \'Charlie\', \'department\': \'Marketing\', \'salary\': 60000} # ] ``` # Note: - Make sure that the function handles both ascending and descending order. - Use Python\'s built-in sorting functions effectively, possibly leveraging key functions. # Implementation ```python from typing import List, Dict, Any def sort_employees(employees: List[Dict[str, Any]], sort_by: str, reverse: bool = False) -> List[Dict[str, Any]]: # Your implementation here pass # Example usage employees = [ {\'name\': \'Alice\', \'department\': \'HR\', \'salary\': 50000}, {\'name\': \'Bob\', \'department\': \'Engineering\', \'salary\': 80000}, {\'name\': \'Charlie\', \'department\': \'Marketing\', \'salary\': 60000}, {\'name\': \'David\', \'department\': \'Engineering\', \'salary\': 70000} ] print(sort_employees(employees, sort_by=\'department\', reverse=False)) ```","solution":"from typing import List, Dict, Any def sort_employees(employees: List[Dict[str, Any]], sort_by: str, reverse: bool = False) -> List[Dict[str, Any]]: Sort the list of employee dictionaries by the specified key in either ascending or descending order. Args: - employees (List[Dict[str, Any]]): A list of dictionaries representing employees. - sort_by (str): The key to sort by, which can be either \\"name\\", \\"department\\", or \\"salary\\". - reverse (bool): A boolean parameter indicating whether to sort in descending order. Returns: - List[Dict[str, Any]]: A list of sorted employee dictionaries. return sorted(employees, key=lambda x: x[sort_by], reverse=reverse)"},{"question":"**Pandas Coding Assessment Question** # Objective This coding assessment will test your ability to work with different types of Index objects in pandas, including their creation, manipulation, and querying. # Question You are given a DataFrame containing sales data with the following structure: - `Date`: Timestamps of each sale. - `Region`: Region where the sale was made. - `Product`: Product category sold. - `Quantity`: Number of units sold. - `Price`: Price per unit. Your task is to perform the following operations: 1. **Create a MultiIndex DataFrame** - Convert the given DataFrame into a MultiIndex DataFrame using `Region` and `Date` as the hierarchical index. 2. **Index Manipulations** - Reset the index to its default integer index. - Create a `DatetimeIndex` using the \\"Date\\" column, and set this as the new index. 3. **Filter and Analyze Data** - Filter the data to include only the sales that occurred in the year 2021. - Calculate and return the total quantity and average price of products sold in each region during 2021. Input - A pandas DataFrame `df` with the following columns: `Date`, `Region`, `Product`, `Quantity`, and `Price`. Output - A DataFrame with columns `Region`, `Total_Quantity`, and `Average_Price` for sales occurred in 2021. Constraints - You can assume that the `Date` column is already in datetime format. - The dataframe may contain duplicate dates for the same region. Performance Requirements - The solution should efficiently handle DataFrames containing up to 1 million rows. Example Suppose the input DataFrame `df` looks like this: ```python import pandas as pd data = { \\"Date\\": [\\"2021-01-01\\", \\"2021-02-01\\", \\"2021-03-01\\", \\"2020-05-01\\", \\"2021-06-01\\"], \\"Region\\": [\\"North\\", \\"North\\", \\"South\\", \\"North\\", \\"South\\"], \\"Product\\": [\\"A\\", \\"B\\", \\"A\\", \\"A\\", \\"B\\"], \\"Quantity\\": [10, 20, 30, 40, 50], \\"Price\\": [100, 200, 300, 400, 500] } df = pd.DataFrame(data) df[\\"Date\\"] = pd.to_datetime(df[\\"Date\\"]) ``` Your function should return a DataFrame like the following if the `df` above were passed as input: ```python result = pd.DataFrame({ \\"Region\\": [\\"North\\", \\"South\\"], \\"Total_Quantity\\": [30, 80], \\"Average_Price\\": [150.0, 400.0] }) ``` # Implementation Below is the function signature you should implement: ```python import pandas as pd def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: # Your implementation goes here pass ``` Good luck, and make sure to test your function with various inputs to ensure it works as expected.","solution":"import pandas as pd def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Create a MultiIndex DataFrame df_multiindex = df.set_index([\'Region\', \'Date\']) # Step 2: Reset the index to default integer index df_reset_index = df_multiindex.reset_index() # Step 3: Create a DatetimeIndex using the \\"Date\\" column and set it as the new index df_reset_index.set_index(pd.DatetimeIndex(df_reset_index[\'Date\']), inplace=True) # Step 4: Filter the data to include only the sales that occurred in the year 2021 df_2021 = df_reset_index[df_reset_index.index.year == 2021] # Step 5: Calculate total quantity and average price of products sold in each region during 2021 result = df_2021.groupby(\'Region\').agg( Total_Quantity=pd.NamedAgg(column=\'Quantity\', aggfunc=\'sum\'), Average_Price=pd.NamedAgg(column=\'Price\', aggfunc=\'mean\') ).reset_index() return result"},{"question":"# XML Parsing with SAX You are required to implement a custom SAX parser using the `xml.sax.xmlreader` package. Your task is to parse a given XML string and extract information about certain tags and their attributes. Specifically, you need to implement a function that: 1. Parses the XML string incrementally. 2. Collects all the tags and their attributes occurring in the XML. 3. Returns a structured dictionary with the tag names as keys and a list of their attributes as values. Function Signature ```python def parse_xml_incrementally(xml_string: str) -> dict: pass ``` Expected Input - `xml_string` (str): A well-formed XML string. Expected Output - `result` (dict): A dictionary where the keys are tag names (str) and the values are lists of attribute dictionaries (list of dict). Each attribute dictionary contains attribute names as keys and their corresponding values. Constraints - You must use the `xml.sax.xmlreader.IncrementalParser` class for parsing the XML string. - The XML string can be large, so you need to handle it incrementally to avoid loading the entire document into memory at once. - Ensure your implementation handles namespaces properly. Example ```python xml_string = <library> <book id=\\"b1\\" author=\\"Author 1\\" title=\\"Title 1\\"/> <book id=\\"b2\\" author=\\"Author 2\\" title=\\"Title 2\\"/> <magazine id=\\"m1\\" editor=\\"Editor 1\\" title=\\"Magazine Title 1\\"/> </library> result = parse_xml_incrementally(xml_string) print(result) ``` Expected Output ```python { \'library\': [], \'book\': [ {\'id\': \'b1\', \'author\': \'Author 1\', \'title\': \'Title 1\'}, {\'id\': \'b2\', \'author\': \'Author 2\', \'title\': \'Title 2\'} ], \'magazine\': [ {\'id\': \'m1\', \'editor\': \'Editor 1\', \'title\': \'Magazine Title 1\'} ] } ``` Performance Requirements - Your solution should efficiently handle large XML documents. - Ensure the parsing is done in a memory-efficient manner using the incremental parsing approach. You are required to use the relevant classes and methods discussed in the provided documentation to accomplish this task.","solution":"from xml.sax import handler, make_parser from xml.sax.xmlreader import IncrementalParser class CustomSAXHandler(handler.ContentHandler): def __init__(self): self.result = {} def startElement(self, name, attrs): attr_dict = {k: v for k, v in attrs.items()} if name in self.result: self.result[name].append(attr_dict) else: self.result[name] = [attr_dict] def parse_xml_incrementally(xml_string: str) -> dict: parser = make_parser() handler = CustomSAXHandler() parser.setContentHandler(handler) index = 0 chunk_size = 1024 while index < len(xml_string): chunk = xml_string[index:index + chunk_size] parser.feed(chunk) index += chunk_size parser.close() return handler.result"},{"question":"# Seaborn Coding Assessment Objective Demonstrate your understanding of `seaborn` and the `FacetGrid` class by creating a comprehensive plot based on a dataset. Instructions 1. **Data Loading**: - Load the \\"tips\\" dataset from seaborn. 2. **FacetGrid Creation**: - Using the `FacetGrid` class, create a grid of plots that: - Assigns `time` to columns. - Assigns `day` to rows. 3. **Plotting**: - On this grid, plot histograms of the `total_bill` variable, making sure to: - Use `binwidth` of 3. - Set the range for the bins from 0 to 60. 4. **Customization**: - Set the overall size of each subplot to have a `height` of 4 and an `aspect` ratio of 0.75. 5. **Advanced Customization**: - Add a horizontal reference line at the median of the `total_bill`. - Customize each subplot to have: - X-axis labelled as \\"Total Bill\\". - Y-axis labelled as \\"Frequency\\". - Titles displaying the specific `time` and `day` combination. 6. **Legend and Annotations**: - Add a legend to the plot. - Annotate each subplot with the count of observations (samples) present in that subplot. Constraints - Ensure that each subplot is properly adjusted and no overlapping occurs. - The final grid should be saved as an image file named \\"tips_facet_grid.png\\". Example Input and Output While no explicit input is provided by the user beyond the necessary dataset, the function call should look like: ```python create_faceted_plot() ``` And the output should be an image file \\"tips_facet_grid.png\\" saved in the current directory. Skeleton Code ```python import seaborn as sns import matplotlib.pyplot as plt def create_faceted_plot(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Create a FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"day\\", height=4, aspect=0.75) # Map the histogram plot on the grid g.map_dataframe(sns.histplot, x=\\"total_bill\\", binwidth=3, binrange=(0, 60)) # Add a median reference line g.refline(y=tips[\\"total_bill\\"].median(), color=\'red\') # Set axis labels g.set_axis_labels(\\"Total Bill\\", \\"Frequency\\") # Set titles g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") # Add legend g.add_legend() # Annotate each facet with the count of observations def annotate(data, **kws): n = len(data) ax = plt.gca() ax.text(.1, .6, f\\"N = {n}\\", transform=ax.transAxes) g.map_dataframe(annotate) # Adjust layout and save the plot g.tight_layout() g.savefig(\\"tips_facet_grid.png\\") # Call the function to create the plot create_faceted_plot() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_faceted_plot(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Create a FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"day\\", height=4, aspect=0.75) # Define a function to add reference line and annotations def plot_histplot_with_refline(data, **kws): ax = plt.gca() sns.histplot(data, x=\\"total_bill\\", binwidth=3, binrange=(0, 60), ax=ax, **kws) median = np.median(data[\'total_bill\']) ax.axhline(median, color=\'red\', lw=2, ls=\'--\') ax.text(1, 0.9, f\\"Median = {median:.2f}\\", transform=ax.transAxes) sample_size = len(data) ax.text(.01, .9, f\\"N = {sample_size}\\", transform=ax.transAxes) # Map the custom plotting function on the grid g.map_dataframe(plot_histplot_with_refline) # Set axis labels g.set_axis_labels(\\"Total Bill\\", \\"Frequency\\") # Set titles g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") # Adjust layout and save the plot g.tight_layout() g.savefig(\\"tips_facet_grid.png\\") # Call the function to create the plot create_faceted_plot()"},{"question":"# Matching and Filtering System Using Python 3.10 **Objective:** Create a function named `filter_and_process_items` that: - Accepts a list of tuples, where each tuple holds information about an item in the format `(id, category, value)`. - Filters these items based on the category and processes them differently based on the value. **Requirements:** - Use `match` statements to differentiate between actions based on the category. - Implement default and keyword arguments to manage the filtering behavior. - The function should print results based on specified logic. # Function Signature: ```python def filter_and_process_items( items: list[tuple[int, str, float]], category_filter: str = \\"all\\", *, threshold: float = 10.0 ) -> None: Filters and processes a list of items based on category and value. Parameters: items (list of tuples): Each tuple contains (id, category, value). category_filter (str): The category to filter by (default is \\"all\\"). threshold (float): The value threshold that triggers a special process (default is 10.0). Returns: None: Just prints the processed results. ``` # Inputs: - `items`: A list of tuples. Each tuple is of the format `(id, category, value)`, e.g., `[(1, \'A\', 9.5), (2, \'B\', 15.0), ...]`. - `category_filter`: A string indicating the category to filter. Default is \\"all\\" which includes all categories. - `threshold`: A float indicating the threshold value. # Outputs: - This function does not return any values. It prints the results of the processed items based on their categories and values. # Logic: 1. **Filtering:** - If `category_filter` is \\"all\\", include all items. - Otherwise, only include items matching the given category. 2. **Processing:** - Use `match` statements to handle different categories: - If the category is \'A\' and the value is greater than the threshold, mark it as \\"High A-Value\\". - If the category is \'B\', add 5.0 to its value. - If the category is different, just print the item as it is. 3. **Example Usage:** ```python items = [ (1, \'A\', 9.5), (2, \'A\', 15.0), (3, \'B\', 7.0), (4, \'C\', 12.5) ] filter_and_process_items(items, \'A\', threshold=10.0) ``` # Expected Outputs: - For the example above, the expected output when `filter_and_process_items` is invoked: ``` Item 1 is below the threshold. Item 2 is a High A-Value. (3, \'B\', 7.0) (4, \'C\', 12.5) # Since category filter is \'A\', this would not be included unless filter is \'all\'. ``` # Constraints: - Utilize the `match` statement for categorical processing. - The function should be efficient in handling up to 1000 items. # Note: - Pay attention to PEP8 coding style guidelines while implementing the function.","solution":"def filter_and_process_items( items: list[tuple[int, str, float]], category_filter: str = \\"all\\", *, threshold: float = 10.0 ) -> None: Filters and processes a list of items based on category and value. Parameters: items (list of tuples): Each tuple contains (id, category, value). category_filter (str): The category to filter by (default is \\"all\\"). threshold (float): The value threshold that triggers a special process (default is 10.0). Returns: None: Just prints the processed results. filtered_items = [item for item in items if category_filter == \\"all\\" or item[1] == category_filter] for item in filtered_items: id, category, value = item match category: case \'A\' if value > threshold: print(f\\"Item {id} is a High A-Value.\\") case \'A\': print(f\\"Item {id} is below the threshold.\\") case \'B\': new_value = value + 5.0 print((id, category, new_value)) case _: print(item)"},{"question":"Objective The goal of this exercise is to assess your understanding of the `asyncio` event loop and related asynchronous programming concepts in Python. You will create an asynchronous TCP server that can handle multiple client connections, echo back messages sent by clients, and handle proper connection shutdowns. Problem Statement Implement a class `EchoServer` that does the following: 1. Opens a TCP server on a given host and port. 2. Accepts multiple client connections. 3. For each connected client: - Reads messages sent by clients asynchronously. - Echoes each message back to the respective client. - Logs each message to the console. 4. Handles SIGINT (Ctrl+C) and SIGTERM signals to gracefully shut down the server. 5. Ensures that all clients are properly disconnected before the server stops. Detailed Requirements: 1. **Class Definition:** - Define a class `EchoServer` with a constructor that accepts `host` and `port`. - Include a method `start()` to run the server. 2. **Server Setup:** - Using `asyncio.start_server()`, start a TCP server on the provided host and port. - Ensure that the server can handle multiple simultaneous client connections. 3. **Client Handling:** - For each connected client, read lines using `StreamReader` and echo them back using `StreamWriter`. - Log each received message to the console in the format: `Received from <client_address>: <message>`. 4. **Signal Handling:** - Implement handlers for `SIGINT` and `SIGTERM` to gracefully shut down the server. - Ensure the server logs `Received signal <signame>: shutting down` before the shutdown. - All connections should be properly closed before the server stops. 5. **Cleanup:** - Ensure that any resources such as sockets are released properly upon shutdown. Example Usage ```python # main.py import asyncio from echo_server import EchoServer async def main(): server = EchoServer(\'127.0.0.1\', 8888) await server.start() if __name__ == \'__main__\': asyncio.run(main()) ``` When the server is running, clients can connect to it using `telnet` or other TCP clients to test the functionality. Constraints - Use of high-level asyncio APIs is encouraged. - Handle potential exceptions gracefully. - Make use of asyncio event loop methods such as `add_signal_handler()`. Input/Output - **Input:** Host and Port for the server. - **Output:** Console logs of received messages and signal interruptions. Your solution should demonstrate a deep understanding of asynchronous programming and event-driven architectures using Python\'s `asyncio` package.","solution":"import asyncio import signal class EchoServer: def __init__(self, host, port): self.host = host self.port = port self.server = None async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received from {addr}: {message}\\") writer.write(data) await writer.drain() except asyncio.CancelledError: pass finally: print(f\\"Connection closed {addr}\\") writer.close() await writer.wait_closed() async def start_server(self): self.server = await asyncio.start_server(self.handle_client, self.host, self.port) addr = self.server.sockets[0].getsockname() print(f\'Serving on {addr}\') asyncio.get_event_loop().add_signal_handler(signal.SIGINT, self.shutdown) asyncio.get_event_loop().add_signal_handler(signal.SIGTERM, self.shutdown) def shutdown(self): print(\\"Received signal: shutting down\\") self.server.close() async def start(self): await self.start_server() async with self.server: await self.server.serve_forever()"},{"question":"**Assessment Question: Complex Shape Drawing with Python Turtle Graphics** **Objective:** Create an advanced function that makes use of Python\'s turtle graphics to draw a complex pattern combining multiple shapes, colors, and movements. This exercise will assess your understanding of turtle movements, drawing capabilities, event bindings, and color customizations. **Problem Statement:** Design and implement a function, `draw_complex_pattern()`, which utilizes the turtle graphics library to draw a complex pattern on the screen. The pattern should include: 1. Multiple shapes (such as circles, squares, and triangles) drawn in different colors. 2. The ability to change the pen size dynamically within the function based on specific rules. 3. Filling shapes with different colors. 4. Moving the turtle to different positions without drawing (using penup and pendown). 5. Event handling capability to change color or shape dynamically upon clicking on the screen. **Function Signature:** ```python def draw_complex_pattern(): pass ``` **Requirements:** 1. **Shapes and Colors:** - Draw at least three different shapes (circle, square, triangle) with different colors. - The shapes should be filled with colors using `begin_fill()` and `end_fill()`. 2. **Dynamic Pen Size:** - Change the pen size based on certain conditions, for example, increase the pen size after drawing a shape. 3. **Movement:** - Use `penup()` and `pendown()` to position the turtle without drawing lines. 4. **Events Handling:** - Implement an `onclick` event to change the drawing color or shape dynamically when the screen is clicked. **Example Output:** When `draw_complex_pattern()` is invoked, it should render a graphical pattern on the screen which satisfies all the above requirements. The turtle should demonstrate movement by drawing various shapes filled with colors at different locations, with responsiveness to click events. **Notes:** 1. Ensure the turtle graphics screen remains open to display the drawing result until clicked. 2. Utilize the methods like `penup()`, `pendown()`, `color()`, `begin_fill()`, `end_fill()`, `forward()`, `left()`, `right()`, etc., effectively to achieve the desired outcome. 3. Manage the screen setup and events handling inside the `draw_complex_pattern()`. **Constraints:** 1. The function should be self-contained and not require any additional libraries apart from the standard turtle module. 2. Ensure the function behaves correctly irrespective of the screen size. **Test Case:** ```python draw_complex_pattern() ``` Expected behavior: - The screen should display a pattern with at least three different shapes, each filled with different colors. - The pen size changes dynamically during drawing. - Clicking on the screen leads to color or shape change for subsequent drawings.","solution":"import turtle def draw_complex_pattern(): Draw a complex pattern using turtle graphics with multiple shapes, colors, dynamic pen sizes, and handling click events to change behavior. screen = turtle.Screen() screen.title(\\"Complex Pattern Drawing with Turtle Graphics\\") screen.setup(width=800, height=600) t = turtle.Turtle() # Function to draw a square def draw_square(size): t.begin_fill() for _ in range(4): t.forward(size) t.right(90) t.end_fill() # Function to draw a triangle def draw_triangle(size): t.begin_fill() for _ in range(3): t.forward(size) t.left(120) t.end_fill() # Function to draw a circle def draw_circle(radius): t.begin_fill() t.circle(radius) t.end_fill() # Function to handle drawings with dynamic pen size def draw_shapes(): t.penup() t.goto(-200, 0) t.pendown() t.color(\\"red\\", \\"yellow\\") t.pensize(5) draw_square(100) t.penup() t.goto(0, 0) t.pendown() t.color(\\"blue\\", \\"green\\") t.pensize(7) draw_triangle(100) t.penup() t.goto(200, 0) t.pendown() t.color(\\"purple\\", \\"pink\\") t.pensize(10) draw_circle(50) # Event handler to change color on click def change_color(x, y): colors = [\\"red\\", \\"blue\\", \\"green\\", \\"purple\\", \\"orange\\"] current_color = t.color()[0] new_color = colors[(colors.index(current_color) + 1) % len(colors)] t.color(new_color) # Binding the change_color function to mouse click screen.onclick(change_color) # Execute the drawing draw_shapes() # Keeping the window open until user closes it screen.mainloop()"},{"question":"# WAV File Analysis and Conversion You are required to write a Python function that reads a WAV file, extracts its audio data, performs some analysis, and writes the results to a new WAV file. Specifically, your function should: 1. **Read the Input WAV File**: Read a given WAV file and retrieve its properties such as the number of channels, sample width, frame rate, and number of frames. 2. **Analyze the Audio Data**: - Compute and print the duration of the audio in seconds. - Compute and print the average amplitude of the audio. 3. **Convert and Write to a New WAV File**: - Reduce the frame rate of the audio by half. - Save the modified audio data to a new WAV file. # Function Signature ```python def analyze_and_convert_wav(input_file: str, output_file: str) -> None: pass ``` # Parameters - `input_file` (str): The path to the input WAV file. - `output_file` (str): The path where the output WAV file will be saved. # Constraints - You can assume that the input WAV file is in the PCM format and is seekable. - Make sure the resulting output file is a valid WAV file and maintains the original number of channels and sample width. # Example ```python # Assuming \'input.wav\' is a 10 seconds mono WAV file with a frame rate of 44100 Hz # This should create \'output.wav\' with a frame rate of 22050 Hz and half the duration. analyze_and_convert_wav(\'input.wav\', \'output.wav\') ``` # Notes - You should handle any necessary exception to ensure the program does not crash during file operations. - You may use the Python standard library only. - When reading frames, consider the appropriate size to avoid excessive memory use for large files. # Your Task Implement the `analyze_and_convert_wav` function to achieve the specified functionality, ensuring it adheres to the described parameters and constraints.","solution":"import wave import numpy as np def analyze_and_convert_wav(input_file: str, output_file: str) -> None: try: with wave.open(input_file, \'rb\') as wav_in: params = wav_in.getparams() n_frames = params.nframes frame_rate = params.framerate n_channels = params.nchannels sample_width = params.sampwidth audio_data = wav_in.readframes(n_frames) audio_array = np.frombuffer(audio_data, dtype=np.int16) duration = n_frames / frame_rate average_amplitude = np.mean(np.abs(audio_array)) print(f\'Duration: {duration:.2f} seconds\') print(f\'Average Amplitude: {average_amplitude:.2f}\') new_frame_rate = frame_rate // 2 with wave.open(output_file, \'wb\') as wav_out: wav_out.setnchannels(n_channels) wav_out.setsampwidth(sample_width) wav_out.setframerate(new_frame_rate) new_audio_data = audio_array[::2].tobytes() wav_out.writeframes(new_audio_data) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# **Coding Assessment Question** You are provided with a CSV file containing transaction data. The CSV file has the following columns: `TransactionID`, `CustomerID`, `TransactionDate`, `ProductID`, `Amount`, `Quantity`, and `ProductCategory`. Your task is to: 1. Read the CSV file into a pandas DataFrame. The CSV file has the following characteristics: - The delimiter is a comma (`,`). - The first row contains the header information. - Some missing values in the `ProductCategory` column are represented as `\\"NA\\"`. - The `TransactionDate` column should be parsed as dates. 2. Perform the following data cleaning and transformation: - Replace missing values in the `ProductCategory` column with the value `\\"Unknown\\"`. - Add a new column `TotalAmount` which is the product of `Amount` and `Quantity`. 3. Generate a summary DataFrame showing the total number of transactions and the total amount spent for each `ProductCategory`. 4. Save the summary DataFrame to a JSON file named `transaction_summary.json`. # **Function Signature** Your code should be contained in a function with the following signature: ```python import pandas as pd def process_transactions(csv_file: str) -> None: pass ``` # **Input and Output** - **Input**: - `csv_file`: A string representing the file path of the CSV file to be processed. - **Output**: - The function does not return any values. Instead, it should save the output JSON file. # **Example** If the input CSV file (`transactions.csv`) looks like this: ```csv TransactionID,CustomerID,TransactionDate,ProductID,Amount,Quantity,ProductCategory 1,1001,2023-01-12,2001,50,2,Electronics 2,1002,2023-01-13,2002,30,1,NA 3,1001,2023-01-14,2003,20,3,Home 4,1003,2023-01-15,2002,30,2,Electronics 5,1002,2023-01-16,2004,15,1,Toys ``` After executing the function, the output JSON file (`transaction_summary.json`) should contain: ```json [ {\\"ProductCategory\\": \\"Electronics\\", \\"TotalTransactions\\": 3, \\"TotalAmount\\": 140.0}, {\\"ProductCategory\\": \\"Home\\", \\"TotalTransactions\\": 1, \\"TotalAmount\\": 60.0}, {\\"ProductCategory\\": \\"Toys\\", \\"TotalTransactions\\": 1, \\"TotalAmount\\": 15.0}, {\\"ProductCategory\\": \\"Unknown\\", \\"TotalTransactions\\": 1, \\"TotalAmount\\": 30.0} ] ``` Note: Ensure the summary data is sorted by `ProductCategory`.","solution":"import pandas as pd def process_transactions(csv_file: str) -> None: # Read the CSV file into a pandas DataFrame with specified options df = pd.read_csv(csv_file, delimiter=\',\', parse_dates=[\'TransactionDate\'], na_values=[\'NA\']) # Replace missing values in the ProductCategory column with \\"Unknown\\" df[\'ProductCategory\'].fillna(\'Unknown\', inplace=True) # Add a new column \'TotalAmount\' which is the product of \'Amount\' and \'Quantity\' df[\'TotalAmount\'] = df[\'Amount\'] * df[\'Quantity\'] # Generate the summary DataFrame summary_df = df.groupby(\'ProductCategory\').agg( TotalTransactions=pd.NamedAgg(column=\'TransactionID\', aggfunc=\'count\'), TotalAmount=pd.NamedAgg(column=\'TotalAmount\', aggfunc=\'sum\') ).reset_index() # Save the summary DataFrame to a JSON file summary_df.to_json(\'transaction_summary.json\', orient=\'records\')"},{"question":"# Question You are given a dataset `mpg` which contains data on various car models. Here is a brief description of some relevant columns of the dataset: - `mpg`: miles per gallon (continuous variable). - `weight`: weight of the car (continuous variable). - `acceleration`: acceleration of the car (continuous variable). - `horsepower`: horsepower of the car (continuous variable). - `cylinders`: number of cylinders in the car (discrete variable). - `displacement`: displacement of the car (continuous variable). - `origin`: origin of the car (ordinal categorical variable with values \\"usa\\", \\"europe\\", \\"japan\\"). Your task is to write a function `create_regression_plot` that takes in two parameters: `x` and `y`, representing the column names of the variables to be plotted on the x-axis and y-axis, respectively. The function should generate a seaborn regression plot with the following requirements: 1. If the `x` or `y` variables are \'displacement\' and \'mpg\' respectively, fit a log-linear regression (`logx=True`). 2. If `x` is \'cylinders\', jitter the x data points by 0.15 (`x_jitter=.15`). 3. For any other continuous variables, fit a second-order polynomial regression (`order=2`). 4. Customize the plot to display a 99% confidence interval, change the marker to \'x\', set the line color to red, and set the marker color to grey (`color=\\".3\\"`). # Example Usage ```python # Example 1: create_regression_plot(\\"weight\\", \\"acceleration\\") # This would generate a plot with a second-order polynomial regression between weight and acceleration. # Example 2: create_regression_plot(\\"displacement\\", \\"mpg\\") # This would generate a plot with a log-linear regression between displacement and mpg. # Example 3: create_regression_plot(\\"cylinders\\", \\"weight\\") # This would generate a plot with jittered x data points between cylinders and weight. ``` # Function Signature ```python def create_regression_plot(x: str, y: str) -> None: pass # Replace this line with your implementation. ``` # Constraints - The function should use the `seaborn` library for plotting. - Ensure that the dataset `mpg` is loaded within the function. - The function should not return anything but must display the plot. Good luck!","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_regression_plot(x: str, y: str) -> None: # Load the dataset mpg = sns.load_dataset(\'mpg\') # Determine the type of regression to use. if x == \'displacement\' and y == \'mpg\': sns.lmplot(data=mpg, x=x, y=y, logx=True, ci=99, line_kws={\'color\': \'red\'}, scatter_kws={\'s\': 50, \'color\': \'.3\', \'marker\': \'x\'}) elif x == \'cylinders\': sns.lmplot(data=mpg, x=x, y=y, x_jitter=.15, ci=99, line_kws={\'color\': \'red\'}, scatter_kws={\'s\': 50, \'color\': \'.3\', \'marker\': \'x\'}) else: sns.lmplot(data=mpg, x=x, y=y, order=2, ci=99, line_kws={\'color\': \'red\'}, scatter_kws={\'s\': 50, \'color\': \'.3\', \'marker\': \'x\'}) # Display the plot plt.show()"},{"question":"# Question: You are tasked to write a Python function that interacts with binary data. Specifically, you need to utilize Python\'s C API for bytes objects to create, append, resize, and retrieve contents of bytes objects. Your task is to implement the following functionality: 1. **Create** a bytes object from a given C string. 2. **Append** two bytes objects. 3. **Resize** a bytes object to a specific length. 4. **Retrieve** and display the size and contents of a bytes object. # Requirements: 1. Implement the `create_bytes_object` function that takes a C string and returns a new bytes object. 2. Implement the `append_bytes_objects` function that takes two bytes objects and returns a new bytes object which is the concatenation of the first and second bytes objects. 3. Implement the `resize_bytes_object` function that takes a bytes object and a new size, resizes the original bytes object and returns it. 4. Implement the `get_bytes_content` function that retrieves and returns the size and contents of the given bytes object. Function Signature: ```python def create_bytes_object(c_string: str) -> bytes: pass def append_bytes_objects(bytes1: bytes, bytes2: bytes) -> bytes: pass def resize_bytes_object(bytes_obj: bytes, new_size: int) -> bytes: pass def get_bytes_content(bytes_obj: bytes) -> (int, str): pass ``` Input and Output: - `create_bytes_object(c_string: str) -> bytes`: - **Input**: A C string (Python `str`). - **Output**: A new Python bytes object. - `append_bytes_objects(bytes1: bytes, bytes2: bytes) -> bytes`: - **Input**: Two Python bytes objects. - **Output**: A new bytes object which is the concatenation of the first and second bytes objects. - `resize_bytes_object(bytes_obj: bytes, new_size: int) -> bytes`: - **Input**: A bytes object and an integer indicating the new size. - **Output**: The resized bytes object. - `get_bytes_content(bytes_obj: bytes) -> (int, str)`: - **Input**: A bytes object. - **Output**: A tuple containing the size of the bytes object and its content as a string. Constraints: - The C string provided to `create_bytes_object` will not be `NULL`. - The new size provided to `resize_bytes_object` will be a non-negative integer. - Proper error handling and type checking should be implemented as per the provided API documentation. Example Usage: ```python # Example usage of the implemented functions # Step 1: Create bytes object from C string bytes_obj1 = create_bytes_object(\\"hello\\") # Step 2: Append two bytes objects bytes_obj2 = create_bytes_object(\\"world\\") appended_bytes = append_bytes_objects(bytes_obj1, bytes_obj2) # Step 3: Resize the appended bytes object resized_bytes = resize_bytes_object(appended_bytes, 15) # Step 4: Retrieve contents and size of the resized bytes object size, content = get_bytes_content(resized_bytes) print(size, content) # Expected output: 15, \'helloworldx00x00x00x00\' ``` Please write and test these functions in Python using appropriate C API calls, documenting any assumptions and constraints.","solution":"def create_bytes_object(c_string: str) -> bytes: Creates a bytes object from a given C string. return bytes(c_string, \'utf-8\') def append_bytes_objects(bytes1: bytes, bytes2: bytes) -> bytes: Appends two bytes objects and returns a new bytes object. return bytes1 + bytes2 def resize_bytes_object(bytes_obj: bytes, new_size: int) -> bytes: Resizes a bytes object to a specific length. If the new length is longer, pad with null bytes. If the new length is shorter, truncate the bytes object. if new_size < len(bytes_obj): return bytes_obj[:new_size] else: return bytes_obj + b\'x00\' * (new_size - len(bytes_obj)) def get_bytes_content(bytes_obj: bytes) -> (int, str): Retrieves and returns the size and contents of a bytes object. return (len(bytes_obj), bytes_obj.decode(\'utf-8\', errors=\'ignore\'))"},{"question":"**Persistent Dictionary with Shelve** As a software developer, you have been tasked with creating a simple configuration store for a new application. The configuration needs to persist between program executions and should support storing different types of configuration values. You will use the `shelve` module to implement a `ConfigStore` class that will manage this persistent storage. Your task is to implement the `ConfigStore` class with the following functionalities: 1. **Initialization**: The class should be initialized with a filename for the storage. 2. **Set Configuration**: A method to set a configuration with a key-value pair. 3. **Get Configuration**: A method to retrieve the value for a given key. 4. **Delete Configuration**: A method to delete a configuration by key. 5. **List Keys**: A method to list all keys in the store. 6. **Context Manager Support**: The class should support context manager protocol to ensure the shelf is closed properly. # Requirements: 1. You must use the `shelve` module for persistence. 2. Your class should ensure that all configurations are stored and retrieved correctly. 3. Ensure the class handles the closing of the shelf properly to prevent data loss. 4. Implement the `__enter__` and `__exit__` methods for context manager support. 5. Include necessary error handling, for example, when attempting to retrieve or delete a non-existing key. # Example Usage: ```python class ConfigStore: def __init__(self, filename): # Implement initialization def set_config(self, key, value): # Implement set configuration def get_config(self, key): # Implement get configuration def delete_config(self, key): # Implement delete configuration def list_keys(self): # Implement listing keys def __enter__(self): # Implement enter for context manager def __exit__(self, exc_type, exc_value, traceback): # Implement exit for context manager # Example usage with ConfigStore(\'config.db\') as store: store.set_config(\'username\', \'admin\') print(store.get_config(\'username\')) # Output: admin store.delete_config(\'username\') print(store.list_keys()) # Output: [] ``` # Constraints: 1. The `filename` parameter should be a string. 2. The `key` parameter should be a string. 3. The `value` can be any Python object that can be pickled. Ensure your implementation adheres to the above requirements and behaves correctly for all edge cases.","solution":"import shelve class ConfigStore: def __init__(self, filename): self.filename = filename self.store = None def __enter__(self): self.store = shelve.open(self.filename, writeback=True) return self def __exit__(self, exc_type, exc_value, traceback): if self.store is not None: self.store.close() def set_config(self, key, value): self.store[key] = value self.store.sync() # Ensure changes are written to disk def get_config(self, key): if key in self.store: return self.store[key] else: raise KeyError(f\\"Key \'{key}\' not found in the configuration store.\\") def delete_config(self, key): if key in self.store: del self.store[key] self.store.sync() # Ensure changes are written to disk else: raise KeyError(f\\"Key \'{key}\' not found in the configuration store.\\") def list_keys(self): return list(self.store.keys())"},{"question":"# Advanced Data Class Implementation You are asked to implement a data class that models a library\'s catalog system. The catalog should store books, each of which has several attributes. Additionally, you must incorporate various features of the `dataclasses` module as described in the documentation. Specifications 1. **Book Data Class**: - Fields: - `title` (str): The title of the book. - `author` (str): The author of the book. - `published_year` (int): The year the book was published. - `pages` (int): The number of pages the book contains. - `quantity_in_stock` (int, default=0): The number of copies available in the library. - Methods: - `__post_init__(self) -> None`: Should ensure `published_year` is not in the future. If it is, raise a `ValueError`. 2. **Library Data Class**: - Fields: - `name` (str): The name of the library. - `books` (list[Book]): A list of books in the library. - Methods: - `add_book(self, book: Book) -> None`: Adds a new book to the catalog. - `total_books(self) -> int`: Returns the total number of books in the catalog. - `find_by_author(self, author: str) -> list[Book]`: Returns a list of books by a given author. 3. **Performance Constraints**: - Use appropriate data structures to ensure that `total_books` and `find_by_author` operations are efficient. - Assume no more than 10,000 books in the catalog. 4. **Input & Output**: - The methods as described should behave according to the specifications. 5. **Constraints**: - Ensure `pages` and `quantity_in_stock` are non-negative integers. - Apply `kw_only` to only allow keyword-only arguments for the `Library` fields. - Use `slots` for the `Library` and `Book` classes to optimize memory usage. Implementation Write the implementations for the `Book` and `Library` data classes, following the provided specifications. ```python from dataclasses import dataclass, field from typing import List, Any # Your implementation here ```","solution":"from dataclasses import dataclass, field, KW_ONLY from typing import List from datetime import datetime @dataclass(slots=True) class Book: title: str author: str published_year: int pages: int quantity_in_stock: int = 0 def __post_init__(self) -> None: current_year = datetime.now().year if self.published_year > current_year: raise ValueError(\\"Published year cannot be in the future\\") if self.pages < 0: raise ValueError(\\"Number of pages cannot be negative\\") if self.quantity_in_stock < 0: raise ValueError(\\"Quantity in stock cannot be negative\\") @dataclass(slots=True, kw_only=True) class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book) -> None: self.books.append(book) def total_books(self) -> int: return len(self.books) def find_by_author(self, author: str) -> List[Book]: return [book for book in self.books if book.author == author]"},{"question":"Objective Your task is to implement a type-safe mini-database using user-defined generic types and `TypedDict` from the `typing` module that adheres to specified type constraints. Requirements 1. **Define Type Variables**: Create type variables `K` (keys) and `V` (values) that are later used to parameterize the database class. The key must be a string and the value type is generic. 2. **TypedDict for Record**: Define a `Record` `TypedDict` that enforces specific structure for each database record. Each record must have a `name` and `data`. The `name` is a `str`, and `data` is of generic type `V`. 3. **Class Definitions**: Implement a `Database` class which is a generic class managing a collection of records. The `Database` class should: - Use `K` as key type (must be a `str`) and `V` as value type. - Ensure type constraints using `Generic[K, V]`. - Allow adding records with a method `add_record` that takes a keyof type `K` and a `Record` containing the data. - Allow retrieving records with a method `get_record` that takes a key of type `K` and returns a `Record` containing the appropriate data type `V`. 4. **Validation and Consistency**: Utilize the type checking capabilities provided by the `typing` module to ensure type safety. Implementation Complete the following implementations: ```python from typing import TypeVar, Generic, TypedDict, Dict # Define type variables K = TypeVar(\'K\', bound=str) V = TypeVar(\'V\') # Define a TypedDict for records class Record(TypedDict): name: str data: V # Define the Database class class Database(Generic[K, V]): def __init__(self): self.records: Dict[K, Record] = {} def add_record(self, key: K, record: Record) -> None: Add a record to the database self.records[key] = record def get_record(self, key: K) -> Record: Retrieve a record from the database by key return self.records[key] # Example usage if __name__ == \\"__main__\\": db = Database[str, int]() db.add_record(\\"user1\\", {\\"name\\": \\"Alice\\", \\"data\\": 42}) db.add_record(\\"user2\\", {\\"name\\": \\"Bob\\", \\"data\\": 84}) user1 = db.get_record(\\"user1\\") print(user1) # Output: {\'name\': \'Alice\', \'data\': 42} user2 = db.get_record(\\"user2\\") print(user2) # Output: {\'name\': \'Bob\', \'data\': 84} ``` Constraints - The `Database` class should enforce type constraints at both compile and runtime. - Use `TypedDict` to ensure the structure of records. - Follow the type hinting rules to ensure type safety. Additional Notes - This task will help assess your understanding of the `typing` module in Python, specifically working with generics, `TypedDict`, and type variables. - Ensure your solution passes type checking with static type checkers such as `mypy`. Good Luck!","solution":"from typing import TypeVar, Generic, TypedDict, Dict # Define type variables K = TypeVar(\'K\', bound=str) V = TypeVar(\'V\') # Define a TypedDict for records class Record(TypedDict): name: str data: V # Define the Database class class Database(Generic[K, V]): def __init__(self): self.records: Dict[K, Record] = {} def add_record(self, key: K, record: Record) -> None: Add a record to the database self.records[key] = record def get_record(self, key: K) -> Record: Retrieve a record from the database by key return self.records[key]"},{"question":"Objective You are tasked with analyzing the Palmer Penguins dataset and visualizing the distribution of penguin body mass and flipper length using seaborn\'s `objects` module. The goal is to create dot plots with jitter for better visualization of data points. Requirements 1. **Load the `penguins` dataset** from seaborn. 2. **Create two different dot plots**: - Plot 1: Display `species` on the x-axis and `body_mass_g` on the y-axis with default jitter settings. - Plot 2: Display `body_mass_g` on the x-axis and `flipper_length_mm` on the y-axis with custom jitter settings: - Apply a width jitter of 0.5 on the x-axis. - Apply jitter of 100 units on the x-axis and 5 units on the y-axis. Expected Input and Output - **Input**: There is no function input as the dataset is loaded internally and plots are generated within the function. - **Output**: The function should display the two generated plots. Constraints - Ensure the use of seaborn\'s `objects` module for creating the plots. - The function should handle any missing data appropriately (e.g., dropping missing values). Function Signature ```python def visualize_penguin_data(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Drop rows with missing values penguins = penguins.dropna() # Create Plot 1 plot1 = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) plot1.show() # Create Plot 2 plot2 = ( so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=100, y=5)) ) plot2.show() ``` Example Usage ```python visualize_penguin_data() ``` This function should load the Palmer Penguins dataset, create the specified plots, and display them.","solution":"def visualize_penguin_data(): import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values penguins = penguins.dropna() # Create Plot 1: Species vs Body Mass with jitter plot1 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) # Create Plot 2: Body Mass vs Flipper Length with custom jitter settings plot2 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(width=0.5, x=100, y=5)) ) # Display the plots plot1.show() plot2.show()"},{"question":"You are provided with a dataset named `titanic` which contains passenger information from the Titanic dataset. Your task is to perform the following: 1. Load the `titanic` dataset using seaborn. 2. Create a point plot to visualize the average age of passengers with different `Pclass` and `Sex`. The `Pclass` should be on the x-axis, and the average `age` should be on the y-axis. 3. Customize the plot to include the following: - Different markers for each `Sex`. - Error bars representing the standard deviation of ages. - Use distinct colors for male and female. - Use appropriate axis labels and plot title to make the plot self-explanatory. - Adjust the plot appearance to make it visually appealing and accessible. The final output should be a well-customized seaborn point plot. Input No specific input from the user is required. You will load the dataset directly within the code. Output The output should be a seaborn point plot visualizing the average age with the specified customizations. Constraints - Ensure that the error bars are clearly visible. - Use seaborn and matplotlib libraries only for plotting. - Do not use additional datasets other than the provided `titanic` dataset from seaborn. Performance Requirements - The code should handle missing data points appropriately. - The plotting should be efficient and should not hang on a standard dataset size. # Example Code ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the point plot sns.pointplot(data=titanic, x=\\"Pclass\\", y=\\"age\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], dodge=True, capsize=.2, palette=\\"colorblind\\") # Customize the appearance plt.title(\\"Average Age of Titanic Passengers by Class and Sex\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Average Age\\") plt.show() ``` Explanation: This example demonstrates loading the Titanic dataset, creating a point plot, and customizing it using seaborn. The resulting plot should give insights into passenger age distributions across different classes and sexes on the Titanic.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_average_age_by_class_and_sex(): Creates a point plot to visualize the average age of Titanic passengers by different Pclass and Sex with customization. # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the point plot sns.pointplot(data=titanic, x=\\"pclass\\", y=\\"age\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], dodge=True, capsize=.2, palette=\\"husl\\", ci=\\"sd\\") # Customize the appearance plt.title(\\"Average Age of Titanic Passengers by Class and Sex\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Average Age\\") plt.legend(title=\'Sex\') plt.grid(linestyle=\'--\', linewidth=0.5) plt.show()"},{"question":"# Advanced Argument Parsing and String Formatting in Python **Objective** Implement a function in Python that demonstrates advanced argument parsing and value construction. The goal is to create a versatile formatter utility that takes various types of inputs, processes them, and returns a formatted string output. **Function Signature** ```python def advanced_formatter(*args, **kwargs) -> str: pass ``` **Function Description** This function should: 1. Accept a variable number of positional (`*args`) and keyword arguments (`**kwargs`). 2. Parse the arguments and perform the following: - If an argument is a string, it should be converted to uppercase. - If an argument is an integer, it should be squared. - If an argument is a float, it should be formatted to two decimal places. - If a keyword argument `reverse_strings` is `True`, all string arguments should be reversed before being converted to uppercase. - If a keyword argument `delimiter` is provided, use it to join the results; otherwise, join results with a space. 3. Return a single formatted string that incorporates all processed inputs. **Input and Output Examples** Example 1: ```python # Given arguments: \\"hello\\", 3, 2.333, reverse_strings=True, delimiter=\\"|\\" # Process: \\"hello\\" -> \\"OLLEH\\", 3 -> 9, 2.333 -> \\"2.33\\" # Result: \\"OLLEH|9|2.33\\" result = advanced_formatter(\\"hello\\", 3, 2.333, reverse_strings=True, delimiter=\\"|\\") print(result) # Output: \\"OLLEH|9|2.33\\" ``` Example 2: ```python # Given arguments: \\"python\\", \\"rocks\\", 10, reverse_strings=False # Process: \\"python\\" -> \\"PYTHON\\", \\"rocks\\" -> \\"ROCKS\\", 10 -> 100 # Result: \\"PYTHON ROCKS 100\\" result = advanced_formatter(\\"python\\", \\"rocks\\", 10, reverse_strings=False) print(result) # Output: \\"PYTHON ROCKS 100\\" ``` **Constraints** - The function must handle up to 100 arguments. - The function should be efficient with a time complexity of O(n), where n is the number of arguments. - Handle edge cases gracefully such as: - No arguments. - All arguments of the same or different types. - Absence of optional keyword arguments. Feel free to use any part of the Python standard library to implement this function.","solution":"def advanced_formatter(*args, **kwargs) -> str: results = [] reverse_strings = kwargs.get(\\"reverse_strings\\", False) delimiter = kwargs.get(\\"delimiter\\", \\" \\") for arg in args: if isinstance(arg, str): if reverse_strings: processed_arg = arg[::-1].upper() else: processed_arg = arg.upper() elif isinstance(arg, int): processed_arg = str(arg ** 2) elif isinstance(arg, float): processed_arg = f\\"{arg:.2f}\\" else: processed_arg = str(arg) results.append(processed_arg) return delimiter.join(results)"},{"question":"Objective: Write a Python script that connects to a Telnet server, performs some interactions, and handles any potential errors gracefully. Requirements: 1. Your script should define a function `interact_telnet_server(host, port, user, password, commands)` that performs the following steps: - Connects to a Telnet server using the provided `host` and `port`. - Logs in with the provided `user` and `password`. - Executes a list of Telnet commands provided in the `commands` list. - Reads the server response after each command and displays it. - Handles exceptions such as EOF and timeout gracefully. - Closes the connection properly before the script exits. 2. The function should adhere to the following input and output formats: - **Input:** - `host`: A string representing the hostname of the Telnet server. - `port`: An integer representing the port number of the Telnet server. - `user`: A string representing the username for login. - `password`: A string representing the password for login. - `commands`: A list of strings where each string is a Telnet command to be executed. - **Output:** - The function should print the server\'s response after each command. 3. **Constraints:** - `host` should be a valid hostname. - `port` should be a valid port number (between 1 and 65535). - You must handle raised exceptions like `EOFError` and `TimeoutError` effectively to ensure the script does not crash abruptly. - Use appropriate Telnet methods for reading and writing, as described in the documentation. Example Usage: ```python def interact_telnet_server(host, port, user, password, commands): # Your implementation here pass if __name__ == \\"__main__\\": HOST = \\"localhost\\" PORT = 23 USER = \\"username\\" PASSWORD = \\"password\\" COMMANDS = [\\"ls\\", \\"pwd\\", \\"whoami\\"] interact_telnet_server(HOST, PORT, USER, PASSWORD, COMMANDS) ``` Expected Output: ``` Output after \'ls\': file1.txt file2.txt Output after \'pwd\': /home/username Output after \'whoami\': username ``` **Note:** This is just an example illustrating desired output format. The actual output will depend on the server and commands executed. Hints: - Make use of `with` statement for context management to automatically handle connection closure. - Use appropriate read methods like `read_until`, `read_all`, etc., depending on the condition. - Handle exceptions to ensure the script exits cleanly and displays relevant error messages.","solution":"import telnetlib def interact_telnet_server(host, port, user, password, commands): try: with telnetlib.Telnet(host, port) as tn: tn.read_until(b\\"login: \\") tn.write(user.encode(\'ascii\') + b\\"n\\") tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") # Read the welcome message, usually displayed after successful login welcome_msg = tn.read_until(b\\" \\") print(welcome_msg.decode(\'ascii\')) for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") response = tn.read_until(b\\" \\") print(f\\"Output after \'{command}\':\\") print(response.decode(\'ascii\')) except EOFError: print(\\"EOFError: Connection closed unexpectedly\\") except TimeoutError: print(\\"TimeoutError: Connection timed out\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": HOST = \\"localhost\\" PORT = 23 USER = \\"username\\" PASSWORD = \\"password\\" COMMANDS = [\\"ls\\", \\"pwd\\", \\"whoami\\"] interact_telnet_server(HOST, PORT, USER, PASSWORD, COMMANDS)"},{"question":"# Assessment Question: Event Scheduler with Recurring Events You are tasked with creating an event scheduler that can manage events, including their occurrence times and types. Your implementation should support recurring events, handle different time zones, and use efficient data structures for storing and retrieving the events. Requirements: 1. **Event Management:** - Define an Event class with the following properties: - `event_id` (int): Unique identifier for the event. - `start_time` (datetime): Timestamp for when the event starts. - `end_time` (datetime): Timestamp for when the event ends. - `title` (str): Title of the event. - `description` (str): Description of the event. - `timezone` (str): Timezone of the event\'s start and end times. - `recurrence` (Enum): Defines how often the event recurs (None, Daily, Weekly, Monthly). 2. **Time Zone Handling:** - Use the `datetime` and `zoneinfo` modules to correctly handle event times in their respective time zones. - Implement a method to convert event times to a specified time zone. 3. **Recurring Events:** - Implement a method to calculate the next occurrence of a recurring event based on its recurrence type and last occurrence. 4. **Efficient Storage and Retrieval:** - Use suitable data structures from the `collections` module to efficiently store and retrieve events. - Implement methods to: - Add a new event. - Retrieve all events happening on a specific date. - Retrieve all events for a given date range. 5. **Handling Resource Conflicts:** - Ensure that no two events overlap within the same timezone. - If an event conflicts with another, raise an appropriate exception. Constraints: - Events are assumed not to span multiple time zones. - Performance is crucial when retrieving events for a date range (assume millions of events). Example Usage: ```python from datetime import datetime from zoneinfo import ZoneInfo from enum import Enum # Define Recurrence Enum class Recurrence(Enum): NONE = \\"None\\" DAILY = \\"Daily\\" WEEKLY = \\"Weekly\\" MONTHLY = \\"Monthly\\" # Define Event Class class Event: def __init__(self, event_id, start_time, end_time, title, description, timezone, recurrence=Recurrence.NONE): pass # Implement the constructor def convert_timezone(self, new_timezone: str): pass # Implement the method to convert event times to new timezone def next_occurrence(self): pass # Implement the method to calculate the next occurrence # Define Event Scheduler class EventScheduler: def __init__(self): pass # Initialize data structures def add_event(self, event: Event): pass # Implement add event def get_events_for_date(self, date: datetime): pass # Implement retrieval for specific date def get_events_for_date_range(self, start_date: datetime, end_date: datetime): pass # Implement retrieval for date range def check_conflicts(self, new_event: Event): pass # Implement conflict checking # Usage Example if __name__ == \\"__main__\\": scheduler = EventScheduler() event1 = Event(1, datetime(2023, 10, 15, 10, 0, tzinfo=ZoneInfo(\'UTC\')), datetime(2023, 10, 15, 12, 0, tzinfo=ZoneInfo(\'UTC\')), \\"Meeting\\", \\"Team meeting\\", \\"UTC\\", Recurrence.WEEKLY) scheduler.add_event(event1) events_today = scheduler.get_events_for_date(datetime(2023, 10, 15, tzinfo=ZoneInfo(\'UTC\'))) print(events_today) ``` In your implementation, make sure you handle all the edge cases and test with examples provided.","solution":"from datetime import datetime, timedelta from zoneinfo import ZoneInfo from enum import Enum from collections import defaultdict, OrderedDict class Recurrence(Enum): NONE = \\"None\\" DAILY = \\"Daily\\" WEEKLY = \\"Weekly\\" MONTHLY = \\"Monthly\\" class Event: def __init__(self, event_id, start_time, end_time, title, description, timezone, recurrence=Recurrence.NONE): self.event_id = event_id self.start_time = start_time.astimezone(ZoneInfo(timezone)) self.end_time = end_time.astimezone(ZoneInfo(timezone)) self.title = title self.description = description self.timezone = timezone self.recurrence = recurrence def convert_timezone(self, new_timezone: str): self.start_time = self.start_time.astimezone(ZoneInfo(new_timezone)) self.end_time = self.end_time.astimezone(ZoneInfo(new_timezone)) self.timezone = new_timezone def next_occurrence(self): if self.recurrence == Recurrence.NONE: return None elif self.recurrence == Recurrence.DAILY: next_start = self.start_time + timedelta(days=1) next_end = self.end_time + timedelta(days=1) elif self.recurrence == Recurrence.WEEKLY: next_start = self.start_time + timedelta(weeks=1) next_end = self.end_time + timedelta(weeks=1) elif self.recurrence == Recurrence.MONTHLY: next_start = self._add_months(self.start_time, 1) next_end = self._add_months(self.end_time, 1) return Event(self.event_id, next_start, next_end, self.title, self.description, self.timezone, self.recurrence) def _add_months(self, dt, months): new_month = dt.month + months year_increment = (new_month - 1) // 12 new_month = new_month % 12 or 12 new_year = dt.year + year_increment return dt.replace(year=new_year, month=new_month) class EventScheduler: def __init__(self): self.events = OrderedDict() def add_event(self, event: Event): # Check for conflicts before adding if self.check_conflicts(event): raise Exception(\\"Event time conflicts with an existing event.\\") self.events[event.event_id] = event def get_events_for_date(self, date: datetime): target_date = date.date() result = [] for event_id, event in self.events.items(): if event.start_time.date() <= target_date <= event.end_time.date(): result.append(event) return result def get_events_for_date_range(self, start_date: datetime, end_date: datetime): result = [] for event_id, event in self.events.items(): if event.start_time <= end_date and event.end_time >= start_date: result.append(event) return result def check_conflicts(self, new_event: Event): for event_id, event in self.events.items(): if event.timezone == new_event.timezone and not (new_event.end_time <= event.start_time or new_event.start_time >= event.end_time): return True return False"},{"question":"# Sparse Matrix Operations with PyTorch Objective: This question is designed to test your understanding of sparse tensor functionalities in PyTorch, including converting dense tensors to sparse formats, and performing operations on sparse tensors. Problem Statement: You are provided with a dense matrix `A` and a vector `v`. Your task is to: 1. Convert the dense matrix `A` to a sparse COO (Coordinate) format. 2. Convert the same dense matrix `A` to a sparse CSR (Compressed Sparse Row) format. 3. Perform matrix-vector multiplication using the COO sparse matrix and vector `v`. 4. Perform matrix-vector multiplication using the CSR sparse matrix and vector `v`. 5. Return the results of both multiplications and confirm if they are the same. Note: The conversions to sparse formats should only include non-zero elements for efficiency. Input: - A (2D Tensor): A dense matrix of shape `(m, n)`. - v (1D Tensor): A dense vector of shape `(n)`. Output: - result_coo (1D Tensor): Result of COO matrix-vector multiplication. - result_csr (1D Tensor): Result of CSR matrix-vector multiplication. - is_equal (Boolean): `True` if both results are the same, else `False`. Constraints: - The matrix `A` will have dimensions `3 <= m, n <= 1000`. - The vector `v` will have a length equal to the number of columns in `A`. Example: ```python import torch # Example Inputs A = torch.tensor([[0.0, 1.0, 0.0], [4.0, 0.0, 0.0], [0.0, 0.0, 5.0]]) v = torch.tensor([1.0, 2.0, 3.0]) # Expected Outputs result_coo = torch.tensor([2.0, 4.0, 15.0]) result_csr = torch.tensor([2.0, 4.0, 15.0]) is_equal = True ``` Function Signature: ```python def sparse_operations(A: torch.Tensor, v: torch.Tensor): pass ```","solution":"import torch def sparse_operations(A: torch.Tensor, v: torch.Tensor): # Convert the dense matrix A to sparse COO format A_coo = A.to_sparse() # Convert the dense matrix A to sparse CSR format A_csr = A.to_sparse_csr() # Perform matrix-vector multiplication using the COO sparse matrix result_coo = torch.matmul(A_coo, v) # Perform matrix-vector multiplication using the CSR sparse matrix result_csr = torch.matmul(A_csr, v) # Check if the results are the same is_equal = torch.equal(result_coo, result_csr) return result_coo, result_csr, is_equal"},{"question":"# Custom Transformer Implementation in scikit-learn You are tasked with implementing a custom transformer in scikit-learn that scales features but only for columns with numerical data. Your transformer should be compatible with scikit-learn pipelines and model selection tools. Specifically, you need to implement the `NumScaler` transformer which scales only the numerical features of the dataset using Standard Scaling (mean=0, variance=1). # Objective Implement a scikit-learn compatible transformer `NumScaler` which scales numerical features of the dataset. # Requirements: 1. **Initialization** (`__init__`): - Accept a parameter `strategy` with a default value `\'standard\'`. This parameter decides the scaling strategy and supports only `\'standard\'` (mean=0, variance=1) for now. 2. **Fitting** (`fit`): - Accept `X` (array-like of shape `(n_samples, n_features)`) and `y=None`. - Identify and store indices of numeric columns. - Compute and store the mean and standard deviation for numeric columns. 3. **Transformation** (`transform`): - Accept `X` and apply the scaling only to numeric columns using stored mean and standard deviation. - Return the transformed dataset. 4. **Add `fit_transform` method for efficiency**: - Implement `fit_transform` method for performance optimization, should not separately call `fit` and `transform`. # Input: - Data in the form of a Pandas DataFrame containing mixed types (numeric and non-numeric). # Output: - Transformed DataFrame with scaled numeric columns. # Constraints: - Use standard scaling only for numeric columns. - Non-numeric columns should remain unchanged. - Ensure compatibility with scikit-learn\'s `Pipeline` and `GridSearchCV`. # Example: ```python import pandas as pd from sklearn.pipeline import Pipeline from sklearn.utils import check_array class NumScaler(BaseEstimator, TransformerMixin): def __init__(self, strategy=\'standard\'): self.strategy = strategy def fit(self, X, y=None): X = check_array(X, dtype=None, ensure_2d=True, force_all_finite=\'allow-nan\') self.numeric_columns_ = X.select_dtypes(include=[pd.np.number]).columns self.means_ = X[self.numeric_columns_].mean() self.stds_ = X[self.numeric_columns_].std() return self def transform(self, X): X = check_array(X, dtype=None, ensure_2d=True, force_all_finite=\'allow-nan\') X_copy = X.copy() X_copy[self.numeric_columns_] = (X[self.numeric_columns_] - self.means_) / self.stds_ return X_copy def fit_transform(self, X, y=None): self.fit(X, y) return self.transform(X) # Example usage data = { \'age\': [25, 30, 35], \'salary\': [50000, 60000, 70000], \'department\': [\'HR\', \'Engineer\', \'Manager\'] } df = pd.DataFrame(data) scaler = NumScaler() scaled_df = scaler.fit_transform(df) print(scaled_df) ``` # Your Task: Using the provided class scaffold, complete the `NumScaler` transformer to meet all requirements specified. Ensure it properly scales only numeric columns and leaves non-numeric columns unchanged.","solution":"import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils import check_array class NumScaler(BaseEstimator, TransformerMixin): def __init__(self, strategy=\'standard\'): self.strategy = strategy def fit(self, X, y=None): X = pd.DataFrame(X) # Ensure that input is a DataFrame self.numeric_columns_ = X.select_dtypes(include=[\'number\']).columns self.means_ = X[self.numeric_columns_].mean() self.stds_ = X[self.numeric_columns_].std() return self def transform(self, X): X = pd.DataFrame(X) # Ensure that input is a DataFrame X_copy = X.copy() X_copy[self.numeric_columns_] = (X_copy[self.numeric_columns_] - self.means_) / self.stds_ return X_copy def fit_transform(self, X, y=None): return self.fit(X, y).transform(X)"},{"question":"You are required to implement a feature for processing student grades stored in a CSV file. The CSV file contains rows of student information with the following columns: `student_id`, `name`, and up to 5 columns representing scores in various subjects. The number of score columns may vary between files. You need to write a script that reads the CSV file, computes the average score for each student, and writes the result to a new CSV file. The new CSV file should contain `student_id`, `name`, and `average_score` columns. # Requirements: 1. Implement the function `compute_average_scores(input_filepath: str, output_filepath: str) -> None`. 2. The function should read the CSV file from `input_filepath` and compute the average score for each student. 3. The average score should be rounded to two decimal places. 4. Write the results to the CSV file specified by `output_filepath` with columns `student_id`, `name`, and `average_score`. 5. Handle files with varying numbers of score columns. # Input: - `input_filepath`: A string representing the path to the input CSV file. - `output_filepath`: A string representing the path to the output CSV file. # Output: - The function should write the results to the CSV file specified by `output_filepath` and return `None`. # Constraints: - The input CSV file does not have a header row. - All score columns contain numerical values. # Example: Suppose we have an input CSV file `grades.csv` with the following content: ``` 1,John Doe,85,78,92 2,Jane Smith,88,90,79,91 3,Bob Johnson,65,75 ``` After processing, the output CSV file should look like `averages.csv`: ``` 1,John Doe,85.00 2,Jane Smith,87.00 3,Bob Johnson,70.00 ``` # Note: - Use the `csv.reader` for reading the input CSV file. - Use the `csv.writer` for writing the output CSV file. # Sample function signature: ```python import csv def compute_average_scores(input_filepath: str, output_filepath: str) -> None: pass ``` Write the implementation of the function `compute_average_scores` based on the above requirements.","solution":"import csv def compute_average_scores(input_filepath: str, output_filepath: str) -> None: # Open the input CSV file for reading with open(input_filepath, mode=\'r\', newline=\'\') as infile: infile_reader = csv.reader(infile) # Prepare the list to hold output rows output_data = [] for row in infile_reader: # Extract student_id and name student_id = row[0] name = row[1] # Extract scores and compute the average score rounded to 2 decimal places scores = [float(score) for score in row[2:]] average_score = round(sum(scores) / len(scores), 2) # Append the computed data to the output list output_data.append([student_id, name, f\\"{average_score:.2f}\\"]) # Open the output CSV file for writing with open(output_filepath, mode=\'w\', newline=\'\') as outfile: outfile_writer = csv.writer(outfile) # Write rows to the output file outfile_writer.writerows(output_data)"},{"question":"# Dynamic Class Creation and Manipulation Problem Statement You are tasked with creating a custom class dynamically and performing various operations using the `types` module. Your goal is to: 1. Dynamically create a class named `DynamicClass` with the following properties: - It has a single base class `BaseClass` (you should define this base class too). - It has one class-level attribute `class_attr` with the value `\'class level\'`. - It has an instance method `method_foo` that returns the string `\\"method foo\\"`. 2. Instantiate this dynamically created class and: - Add an instance attribute `instance_attr` with the value `\'instance level\'`. - Call the `method_foo` method and save its result. 3. Create another class dynamically, named `AnotherDynamicClass` with the following properties: - It has `DynamicClass` as one of its base classes. - It has a class method `class_method_bar` using `types.ClassMethodDescriptorType`, which returns the string `\\"class method bar\\"`. 4. Implement the function `create_dynamic_classes()` which performs all of the above steps and returns a dictionary with: - The dynamically created instance of `DynamicClass`. - The value from calling `method_foo` on the instance of `DynamicClass`. - The dynamically created class `AnotherDynamicClass` itself. - The value from calling `class_method_bar` on `AnotherDynamicClass` itself (not an instance). Function Signature ```python def create_dynamic_classes() -> dict: pass ``` Expected Output The returned dictionary from `create_dynamic_classes` should have the following structure: ```python { \\"dynamic_instance\\": <instance of DynamicClass>, \\"foo_result\\": \\"method foo\\", \\"another_dynamic_class\\": <class type of AnotherDynamicClass>, \\"bar_result\\": \\"class method bar\\" } ``` Constraints - Do not use the `type()` function for dynamic class creation. - Use the `types.new_class()` method to create the classes dynamically. - Ensure that methods are correctly bound to the classes and instances where applicable. - Use `types.ClassMethodDescriptorType` for defining the class method in `AnotherDynamicClass`. Example Usage ```python result = create_dynamic_classes() print(result[\\"foo_result\\"]) # Should output: \\"method foo\\" print(result[\\"bar_result\\"]) # Should output: \\"class method bar\\" ``` Notes - Ensure that the created class and its methods function correctly as per the specifications. - Use appropriate techniques from the `types` module to achieve the solution.","solution":"import types # Define the base class class BaseClass: pass def create_dynamic_classes(): # Define the method for DynamicClass def method_foo(self): return \\"method foo\\" # Create the class using types.new_class DynamicClass = types.new_class(\'DynamicClass\', (BaseClass,), {}, lambda ns: ns.update({\'class_attr\': \'class level\', \'method_foo\': method_foo})) # Create an instance of DynamicClass and set an instance attribute dynamic_instance = DynamicClass() dynamic_instance.instance_attr = \'instance level\' # Call method_foo on the instance and get the result foo_result = dynamic_instance.method_foo() # Define the class method for AnotherDynamicClass @classmethod def class_method_bar(cls): return \\"class method bar\\" # Create the AnotherDynamicClass using types.new_class AnotherDynamicClass = types.new_class(\'AnotherDynamicClass\', (DynamicClass,), {}, lambda ns: ns.update({\'class_method_bar\': class_method_bar})) # Call class_method_bar on AnotherDynamicClass (not an instance) bar_result = AnotherDynamicClass.class_method_bar() return { \\"dynamic_instance\\": dynamic_instance, \\"foo_result\\": foo_result, \\"another_dynamic_class\\": AnotherDynamicClass, \\"bar_result\\": bar_result }"},{"question":"Dynamic Sorted Log Analyzer You are provided with the functionality to maintain logs in a sorted order using the `bisect` module. Your task is to implement a log analyzer that can efficiently manage insertion and search operations on log entries. Each log entry consists of a timestamp (in the format \\"YYYY-MM-DD HH:MM:SS\\") and a message. The log analyzer should allow: 1. Inserting new log entries while maintaining the list sorted by timestamp. 2. Finding the most recent log entry before a given timestamp. 3. Finding the earliest log entry after a given timestamp. To achieve this, you will implement the following functions: Function 1: `insert_log_entry(logs, entry)` - **Input:** - `logs`: a list of tuples representing log entries, where each tuple contains a string for the timestamp and a string for the message. The list is already sorted by timestamp. - `entry`: a tuple representing the new log entry to be inserted, containing a string for the timestamp and a string for the message. - **Output:** Modify the `logs` list in-place to include the new `entry` while maintaining the sorted order. Function 2: `find_recent_log(logs, timestamp)` - **Input:** - `logs`: a list of tuples representing log entries, where each tuple contains a string for the timestamp and a string for the message. The list is already sorted by timestamp. - `timestamp`: a string representing the timestamp to search for. - **Output:** A tuple representing the most recent log entry before the given `timestamp`. If no such entry exists, raise a `ValueError` with an appropriate message. Function 3: `find_earliest_log(logs, timestamp)` - **Input:** - `logs`: a list of tuples representing log entries, where each tuple contains a string for the timestamp and a string for the message. The list is already sorted by timestamp. - `timestamp`: a string representing the timestamp to search for. - **Output:** A tuple representing the earliest log entry after the given `timestamp`. If no such entry exists, raise a `ValueError` with an appropriate message. Use the `bisect` module functions to implement the insertion and search functionalities described above. # Constraints: - The timestamp format will always be \\"YYYY-MM-DD HH:MM:SS\\". - Log entries in the `logs` list will always be sorted by timestamp. - The maximum number of log entries `N` will be `10^5`. # Example: ```python logs = [(\\"2023-01-01 10:00:00\\", \\"System started\\"), (\\"2023-01-01 12:00:00\\", \\"User logged in\\"), (\\"2023-01-01 12:30:00\\", \\"User performed action\\")] new_entry = (\\"2023-01-01 11:00:00\\", \\"Network initialized\\") insert_log_entry(logs, new_entry) print(logs) # Output: [(\'2023-01-01 10:00:00\', \'System started\'), (\'2023-01-01 11:00:00\', \'Network initialized\'), (\'2023-01-01 12:00:00\', \'User logged in\'), (\'2023-01-01 12:30:00\', \'User performed action\')] recent_log = find_recent_log(logs, \\"2023-01-01 12:15:00\\") print(recent_log) # Output: (\'2023-01-01 12:00:00\', \'User logged in\') earliest_log = find_earliest_log(logs, \\"2023-01-01 11:00:00\\") print(earliest_log) # Output: (\'2023-01-01 12:00:00\', \'User logged in\') ``` # Note: You may assume the input to the functions will always adhere to the specified format, and you do not need to handle invalid input.","solution":"import bisect def insert_log_entry(logs, entry): Inserts a new log entry into the logs while maintaining the sorted order by timestamp. bisect.insort(logs, entry) def find_recent_log(logs, timestamp): Finds the most recent log entry before the given timestamp. index = bisect.bisect_left(logs, (timestamp,)) if index == 0: raise ValueError(\\"No log entry found before the given timestamp.\\") return logs[index - 1] def find_earliest_log(logs, timestamp): Finds the earliest log entry after the given timestamp. index = bisect.bisect_right(logs, (timestamp,)) if index == len(logs): raise ValueError(\\"No log entry found after the given timestamp.\\") return logs[index]"},{"question":"# Memory Management with MemoryView Objects You are required to write a Python function that uses memoryview objects to interact with buffer protocols. Specifically, your task is to create a read-write memory view of a bytes-like object, manipulate it by modifying certain elements, and then return both the modified object and its memory view to verify the changes. Function Signature ```python def manipulate_memory_view(byte_obj: bytes, indices: list, new_values: list) -> tuple: Manipulates a memoryview of a byte object and returns the modified object and its memory view. Parameters: byte_obj (bytes): A bytes-like object to be used as the buffer. indices (list): A list of indices in the byte object where modifications will occur. new_values (list): A list of new values to write into the byte object at specified indices. Returns: tuple: A tuple containing the modified byte object and its memoryview object. ``` Input - `byte_obj`: A bytes-like object (e.g., `b\'abcdef\'`). - `indices`: A list of integers representing the indices in the byte object that should be modified. - `new_values`: A list of integers representing the new values to write at specified indices. This list should be of the same length as `indices`. Constraints - The `indices` must be valid indices within the length of the `byte_obj`. - The `new_values` must be valid bytes (integers between 0 and 255). - Both `indices` and `new_values` must have the same length. Example ```python byte_obj = b\'abcdef\' indices = [1, 3] new_values = [120, 100] modified_obj, memory_view = manipulate_memory_view(byte_obj, indices, new_values) ``` Output: ```python (modified_obj == b\'axcdef\', memory_view.tolist() == [97, 120, 99, 100, 101, 102]) ``` In the above example, the byte object `b\'abcdef\'` is modified so that at index 1, the value `\'b\'` (98) is replaced by `\'x\'` (120), and at index 3, the value `\'d\'` (100) is replaced by `\'d\'` (100). Steps 1. Create a read-write memory view of the `byte_obj`. 2. Modify the contents of the memory view according to the provided `indices` and `new_values`. 3. Return the modified bytes object and the memory view. Notes - Ensure that your function handles edge cases, such as invalid indices, gracefully. - Remember to maintain the immutability of the original `byte_obj` by creating a copy if necessary.","solution":"def manipulate_memory_view(byte_obj, indices, new_values): Manipulates a memoryview of a byte object and returns the modified object and its memory view. Parameters: byte_obj (bytes): A bytes-like object to be used as the buffer. indices (list): A list of indices in the byte object where modifications will occur. new_values (list): A list of new values to write into the byte object at specified indices. Returns: tuple: A tuple containing the modified byte object and its memoryview object. # Convert byte object to a mutable byte array byte_array = bytearray(byte_obj) # Create a memoryview of the byte array mem_view = memoryview(byte_array) # Modify the memory view according to the indices and new values for idx, val in zip(indices, new_values): mem_view[idx] = val # Return the modified byte array (converted back to bytes) and the memory view return bytes(byte_array), mem_view"},{"question":"In this assessment, you are required to write a function `extract_annotations` which takes an object (which can be a function, class, or module) and an optional boolean parameter `eval_strings` (default value is `False`). The function should return the annotations dictionary of the provided object. If the `eval_strings` parameter is `True`, any stringized annotations should be evaluated to their corresponding Python values. Requirements: 1. **Python Version Compatibility**: The function should correctly handle objects in both Python 3.10+ and older versions. 2. **Stringized Annotations**: The function should optionally evaluate stringized annotations to actual Python values if `eval_strings` is `True`. 3. **Type Checks**: Ensure the function can handle functions, classes (including handling inherited annotations correctly), and modules. Input: - `obj`: A function, class, or module from which annotations are to be extracted. - `eval_strings` (optional, default `False`): A boolean flag indicating whether stringized annotations should be evaluated. Output: - A dictionary containing the annotations of the provided object. If there are no annotations, it should return an empty dictionary. Function Signature: ```python def extract_annotations(obj: object, eval_strings: bool = False) -> dict: pass ``` Constraints: - The solution should be compatible with Python 3.6+. Example Usage: ```python # For Python 3.10+ def foo(x: \'int\') -> \'str\': pass class Bar: y: \'float\' z: \'List[int]\' print(extract_annotations(foo)) # Output: {\'x\': \'int\', \'return\': \'str\'} print(extract_annotations(foo, eval_strings=True)) # Output: {\'x\': <class \'int\'>, \'return\': <class \'str\'>} bar_instance = Bar() print(extract_annotations(bar_instance)) # Output: {\'y\': \'float\', \'z\': \'List[int]\'} print(extract_annotations(bar_instance, eval_strings=True)) # Output: {\'y\': <class \'float\'>, \'z\': typing.List[int]} ``` Evaluation Criteria: - Correct extraction of annotations for different object types. - Proper handling of stringized annotations based on the `eval_strings` parameter. - Maintainability and readability of the code. - Handling edge cases and potential errors gracefully.","solution":"import typing def extract_annotations(obj: object, eval_strings: bool = False) -> dict: Extracts the annotations from the provided object. Parameters: obj (object): A function, class, or module to extract annotations from. eval_strings (bool): If True, evaluates stringized annotations to Python values. Default is False. Returns: dict: A dictionary of annotations. if not hasattr(obj, \'__annotations__\'): return {} annotations = obj.__annotations__ if eval_strings: evaluated_annotations = {} for key, value in annotations.items(): if isinstance(value, str): evaluated_annotations[key] = eval(value, vars(typing)) else: evaluated_annotations[key] = value return evaluated_annotations return annotations"},{"question":"# Python Group Database Query and Analysis You are tasked with writing a function that processes Unix group database information to extract and analyze group memberships. Your function should satisfy the following requirements: 1. **Function Definition**: ```python def analyze_group_members(group_name: str) -> dict: Analyze the group members and return relevant statistics. Parameters: group_name (str): The name of the group to analyze. Returns: dict: A dictionary containing: - \\"group_name\\": The name of the group. - \\"group_id\\": The numerical group ID. - \\"member_count\\": The number of members in the group. - \\"members\\": A list of member names sorted in alphabetical order. ``` 2. **Function Behavior**: - The function should use `grp.getgrnam()` to retrieve the group entry for the specified `group_name`. - Collect the group ID (`gr_gid`), the list of members (`gr_mem`), and the number of members in the group. - Return a dictionary containing: - `\\"group_name\\"`: The name of the group. - `\\"group_id\\"`: The numerical group ID. - `\\"member_count\\"`: The count of members in the group. - `\\"members\\"`: A list of all members, sorted alphabetically. 3. **Error Handling**: - If the group with the specified name does not exist, the function should raise a `KeyError` with an appropriate message. 4. **Example**: ```python # Assuming there\'s a group \'staff\' with ID 50 and members [\'alice\', \'bob\', \'charlie\'] result = analyze_group_members(\'staff\') print(result) # Output: # { # \\"group_name\\": \\"staff\\", # \\"group_id\\": 50, # \\"member_count\\": 3, # \\"members\\": [\\"alice\\", \\"bob\\", \\"charlie\\"] # } ``` # Constraints - The input `group_name` will always be a non-empty string. - Ensure that your function efficiently handles the retrieval and processing of group data. - Consider edge cases such as groups with no members. Use the provided documentation and the `grp` module functions to implement this solution.","solution":"import grp def analyze_group_members(group_name: str) -> dict: Analyze the group members and return relevant statistics. Parameters: group_name (str): The name of the group to analyze. Returns: dict: A dictionary containing: - \\"group_name\\": The name of the group. - \\"group_id\\": The numerical group ID. - \\"member_count\\": The number of members in the group. - \\"members\\": A list of member names sorted in alphabetical order. try: group_entry = grp.getgrnam(group_name) group_id = group_entry.gr_gid members = sorted(group_entry.gr_mem) member_count = len(members) return { \\"group_name\\": group_name, \\"group_id\\": group_id, \\"member_count\\": member_count, \\"members\\": members } except KeyError: raise KeyError(f\\"Group \'{group_name}\' not found\\")"},{"question":"<|Analysis Begin|> The provided documentation contains information about C API macros and functions in the Python \\"datetime\\" module used for creating, checking, and extracting information from various date and time objects such as `date`, `datetime`, `time`, and `timedelta`. It includes functions for: - Type-checking objects to see if they are instances or subtypes of specific `datetime` components. - Creating date, time, datetime, timedelta, and timezone objects. - Extracting specific fields from these objects. Given that this involves understanding how to manipulate date and time objects using specific macros and functions, an appropriate question could focus on the detailed use of these functions, which requires creating or manipulating date and time objects and potentially checking their types and extracting fields from them. <|Analysis End|> <|Question Begin|> # Problem Statement You have been tasked with creating a small library that will help in managing and checking various date and time objects using the Python datetime C API. Specifically, you need to implement functionality to create, validate, and extract information from these objects. Write a Python program to achieve the following: 1. **Creating Date and Time Objects**: Implement a function `create_date_time_objects` that: - Takes the parameters year, month, day, hour, minute, second, microsecond and returns a `datetime.datetime` object. - Takes the parameters hour, minute, second, microsecond and returns a `datetime.time` object. - Takes the parameters days, seconds, microseconds and returns a `datetime.timedelta` object. 2. **Type Checking**: Implement a function `check_object_type` that: - Takes a Python object and checks if it\'s of type `date`, `datetime`, `time`, `timedelta`, or `timezone`. The function should return a string indicating the type of the object. 3. **Extracting DateTime Fields**: Implement a function `extract_datetime_fields` that: - Takes a `datetime.datetime` object and returns a dictionary with the keys `\'year\'`, `\'month\'`, `\'day\'`, `\'hour\'`, `\'minute\'`, `\'second\'`, `\'microsecond\'`, and `\'fold\'`. - Takes a `datetime.timedelta` object and returns a dictionary with the keys `\'days\'`, `\'seconds\'`, and `\'microseconds\'`. # Constraints - All input values for creating objects will lie within the valid ranges for years, months, days, hours, minutes, seconds, and microseconds as historically valid for `datetime` module. - The functions should not use try-except blocks to handle null inputs - assume all inputs will be non-null and valid. # Example Usage ```python # Creating datetime, time, and timedelta objects dt = create_date_time_objects(year=2023, month=10, day=22, hour=14, minute=30, second=00, microsecond=0) tm = create_date_time_objects(hour=14, minute=30, second=0, microsecond=0) td = create_date_time_objects(days=1, seconds=3600, microseconds=1000) # Checking object types print(check_object_type(dt)) # Output: \\"datetime\\" print(check_object_type(tm)) # Output: \\"time\\" print(check_object_type(td)) # Output: \\"timedelta\\" # Extract fields from datetime object fields = extract_datetime_fields(dt) print(fields) # Output: {\'year\': 2023, \'month\': 10, \'day\': 22, \'hour\': 14, \'minute\': 30, \'second\': 0, \'microsecond\': 0, \'fold\': 0} # Extract fields from timedelta object delta_fields = extract_datetime_fields(td) print(delta_fields) # Output: {\'days\': 1, \'seconds\': 3600, \'microseconds\': 1000} ``` # Implementation Note Use the methods and macros provided in the documentation to implement the functionality. Your solution should leverage these low-level operations to demonstrate your understanding of the datetime C API.","solution":"from datetime import datetime, time, timedelta def create_date_time_objects(year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, days=None, seconds=None, microseconds=None): Create datetime, time, or timedelta objects based on provided arguments. If year, month, and day are provided, returns a datetime.datetime object. If hour and minute are provided, returns a datetime.time object. If days, seconds, microseconds are provided, returns a datetime.timedelta object. if year is not None and month is not None and day is not None: return datetime(year, month, day, hour, minute, second, microsecond) if hour is not None and minute is not None: return time(hour, minute, second, microsecond) if days is not None and seconds is not None and microseconds is not None: return timedelta(days=days, seconds=seconds, microseconds=microseconds) raise ValueError(\\"Insufficient parameters provided to create an object.\\") def check_object_type(obj): Check the type of a datetime object and return its type as a string. if isinstance(obj, datetime): return \\"datetime\\" if isinstance(obj, time): return \\"time\\" if isinstance(obj, timedelta): return \\"timedelta\\" raise ValueError(\\"Unknown object type.\\") def extract_datetime_fields(obj): Extract fields from a datetime or timedelta object and return them in a dictionary. if isinstance(obj, datetime): return { \'year\': obj.year, \'month\': obj.month, \'day\': obj.day, \'hour\': obj.hour, \'minute\': obj.minute, \'second\': obj.second, \'microsecond\': obj.microsecond, \'fold\': obj.fold, } if isinstance(obj, timedelta): return { \'days\': obj.days, \'seconds\': obj.seconds, \'microseconds\': obj.microseconds, } raise ValueError(\\"Object must be a datetime or timedelta instance.\\")"},{"question":"**Title**: Measuring and Optimizing Function Performance using `timeit` **Objective**: To assess students\' proficiency in using the `timeit` module to measure execution time, analyze performance, and optimize code snippets. **Problem Statement**: You are given three different implementations of a function that processes a list of integers. Your task is to use the `timeit` module to measure and compare the performance of these implementations. After identifying the fastest implementation, you are required to explain why it is faster and suggest any further optimizations. # Implementations: 1. **Implementation A**: Using a for-loop to iterate over the list and create a new list with squares of the elements. ```python def process_list_a(numbers): result = [] for number in numbers: result.append(number * number) return result ``` 2. **Implementation B**: Using a list comprehension to create a new list with squares of the elements. ```python def process_list_b(numbers): return [number * number for number in numbers] ``` 3. **Implementation C**: Using the `map` function with a lambda to create a new list with squares of the elements. ```python def process_list_c(numbers): return list(map(lambda number: number * number, numbers)) ``` # Task: 1. Write a Python script that uses the `timeit` module to measure the execution time of each implementation when processing a list of 1,000,000 integers from 0 to 999,999. 2. Compare the execution times and identify the fastest implementation. 3. Provide an explanation of why the fastest implementation performs better than the others. 4. Suggest any further optimizations that could be applied to the fastest implementation. # Constraints: - You should use the `timeit` module\'s callable interface to measure the execution times. - Perform at least 5 repetitions of the timing for each implementation to ensure reliable results. - Ensure that the garbage collection is disabled during the timing using `gc.disable()` in the setup. # Expected Output: 1. A summary of the execution times for each implementation. 2. Identification of the fastest implementation. 3. A detailed explanation of why the fastest implementation is more efficient. 4. Suggestions for further optimizations if applicable. **Performance Requirements**: - Aim to perform the timing accurately, ensuring minimal interference from other processes. - Use proper setup code to initialize the list of integers before timing the function calls. **Example Output**: ``` Execution times (in seconds): Implementation A: [time1, time2, ...] Implementation B: [time1, time2, ...] Implementation C: [time1, time2, ...] The fastest implementation is: Implementation X Explanation: [Provide a detailed explanation based on the timing results and the code structure] Further optimizations: [Provide any suggestions if applicable] ``` # Notes: - Remember to import necessary modules such as `timeit` and `gc`. - You can define a utility function to run the timing tests and gather the results for each implementation. **Bonus**: - Implement your own version of `autorange()` to determine the optimal number of iterations for timing each implementation dynamically.","solution":"import timeit import gc def process_list_a(numbers): result = [] for number in numbers: result.append(number * number) return result def process_list_b(numbers): return [number * number for number in numbers] def process_list_c(numbers): return list(map(lambda number: number * number, numbers)) def measure_time(func, numbers, repetitions=5): gc.disable() timer = timeit.Timer(lambda: func(numbers)) times = timer.repeat(repeat=repetitions, number=1) gc.enable() return times if __name__ == \\"__main__\\": numbers = list(range(1000000)) repetitions = 5 time_a = measure_time(process_list_a, numbers, repetitions) time_b = measure_time(process_list_b, numbers, repetitions) time_c = measure_time(process_list_c, numbers, repetitions) print(f\\"Execution times (in seconds):\\") print(f\\"Implementation A: {time_a}\\") print(f\\"Implementation B: {time_b}\\") print(f\\"Implementation C: {time_c}\\") min_time_a = min(time_a) min_time_b = min(time_b) min_time_c = min(time_c) if min_time_a < min_time_b and min_time_a < min_time_c: fastest_implementation = \\"Implementation A\\" elif min_time_b < min_time_a and min_time_b < min_time_c: fastest_implementation = \\"Implementation B\\" else: fastest_implementation = \\"Implementation C\\" print(f\\"The fastest implementation is: {fastest_implementation}\\") # Explanation: # List comprehension (Implementation B) is generally the fastest because: # 1. It is directly optimized in C-Python. # 2. Less overhead than for-loops or lambda functions. # 3. Memory allocation is handled more efficiently. # Further optimizations: # Using NumPy for numerical operations can further optimize performance due to # its highly optimized C and Fortran routines."},{"question":"# PyTorch Distributed Multiprocessing Task You are tasked with developing a solution that demonstrates the use of PyTorch\'s `torch.distributed.elastic.multiprocessing` module to start multiple worker processes and handle their execution context, logging, and results. Objective Implement a function `run_distributed_processes(num_processes: int, script_path: str) -> dict` that: 1. Starts `num_processes` worker processes to execute a script located at `script_path`. 2. Collects and returns the results of the executed processes in a dictionary format. 3. Logs the activity of each process using the default logging specifications. Specifications - **Input:** - `num_processes` (int): Number of worker processes to start. - `script_path` (str): Path to the script that each worker process will execute. - **Output:** - A dictionary where the keys are process IDs and the values are their respective results. - **Constraints:** - Make sure the script executed by each process can run concurrently with others. - Properly handle any exceptions that might occur during process execution. - Use the default logging specifications provided by `torch.distributed.elastic.multiprocessing`. - **Performance:** - Ensure that the processes are started and managed efficiently, with proper synchronization and minimal overhead. Example ```python results = run_distributed_processes(4, \'worker_script.py\') print(results) ``` Expected output format (example): ```python { \'process_1\': \'result_1\', \'process_2\': \'result_2\', \'process_3\': \'result_3\', \'process_4\': \'result_4\' } ``` Additional Information You may refer to the PyTorch documentation on `torch.distributed.elastic.multiprocessing` for further details on the classes and functions you might need. **Notes:** - Ensure your function handles synchronization between processes properly. - Take into account edge cases, such as invalid script paths or failure of individual processes.","solution":"import logging import os import torch.multiprocessing as mp def worker_process_func(script_path, return_dict, process_id): try: # Assuming script_path is a Python script that we can run with exec globals_dict = {} with open(script_path, \'r\') as script_file: exec(script_file.read(), globals_dict) result = globals_dict.get(\'result\', \'No result\') return_dict[process_id] = result logging.info(f\\"Process {process_id} completed successfully.\\") except Exception as e: return_dict[process_id] = f\\"Failed with error: {e}\\" logging.error(f\\"Process {process_id} failed with error: {e}\\") def run_distributed_processes(num_processes: int, script_path: str) -> dict: if not os.path.isfile(script_path): raise ValueError(f\\"Script path {script_path} does not exist or is not a file.\\") manager = mp.Manager() return_dict = manager.dict() processes = [] for i in range(num_processes): p = mp.Process(target=worker_process_func, args=(script_path, return_dict, f\'process_{i+1}\')) processes.append(p) p.start() for p in processes: p.join() return dict(return_dict)"},{"question":"Objective The goal of this assignment is to assess your understanding of Support Vector Machines (SVMs) and your ability to implement a classification task using scikit-learn\'s `SVC`. You will also need to demonstrate your ability to perform hyperparameter tuning and use a custom kernel function. Description You are given a dataset with features and labels. Your task is to: 1. Implement an SVM classifier using scikit-learn\'s `SVC`. 2. Perform hyperparameter tuning to find the best values for the parameters `C` and `gamma`. 3. Use a custom kernel function for classification and evaluate its performance. Dataset You can generate a synthetic dataset for this task using scikit-learn\'s `make_classification` function. You may use the following code snippet to generate the dataset: ```python from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_clusters_per_class=2, random_state=42) ``` Steps 1. **Data Preparation**: Split the dataset into training and testing sets. 2. **SVM Implementation**: Implement the SVM classifier using the `SVC` class. 3. **Hyperparameter Tuning**: Use GridSearchCV to find the best values for `C` and `gamma`. 4. **Custom Kernel Function**: Implement a custom kernel function and use it in the `SVC` class. 5. **Evaluation**: Evaluate the performance of your classifier using accuracy, precision, recall, and F1-score. Task Details 1. **Data Preparation**: - Split the dataset into 80% training and 20% testing sets. 2. **SVM Implementation**: - Initialize the `SVC` classifier with `kernel=\'rbf\'`. - Fit the classifier on the training data. - Predict the labels for the testing data. 3. **Hyperparameter Tuning**: - Use `GridSearchCV` to perform hyperparameter tuning. - Search for the best values for `C` in `[0.1, 1, 10, 100]` and `gamma` in `[0.001, 0.01, 0.1, 1]`. - Use cross-validation with 5 folds. 4. **Custom Kernel Function**: - Implement a custom kernel function `(X, Y) -> np.dot(X, Y.T) ** 2`. - Use this kernel function in the `SVC` class. - Fit the classifier using this custom kernel on the training data. - Predict the labels for the testing data using the custom kernel. 5. **Evaluation**: - Evaluate the performance of the classifier using the following metrics: accuracy, precision, recall, and F1-score. Expected Input and Output - **Input**: The synthetic dataset generated using the provided code snippet. - **Output**: The classifier\'s performance metrics for both the default RBF kernel and the custom kernel. Constraints and Limitations - Use scikit-learn version 0.24.0 or later. - Ensure that the training and testing splits are reproducible by setting a random seed. - The custom kernel function must be a valid kernel function that can be used in `SVC`. Performance Requirements - Hyperparameter tuning should be done efficiently to minimize computational time. - The custom kernel implementation should be correct and result in a meaningful classification performance. Example ```python from sklearn.svm import SVC from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Step 1: Data Preparation X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_clusters_per_class=2, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: SVM Implementation clf = SVC(kernel=\'rbf\') clf.fit(X_train, y_train) y_pred = clf.predict(X_test) # Step 3: Hyperparameter Tuning param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [0.001, 0.01, 0.1, 1]} grid = GridSearchCV(SVC(kernel=\'rbf\'), param_grid, cv=5) grid.fit(X_train, y_train) best_params = grid.best_params_ best_clf = grid.best_estimator_ # Step 4: Custom Kernel Function def custom_kernel(X, Y): return np.dot(X, Y.T) ** 2 custom_clf = SVC(kernel=custom_kernel) custom_clf.fit(X_train, y_train) custom_y_pred = custom_clf.predict(X_test) # Step 5: Evaluation metrics = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred), \'recall\': recall_score(y_test, y_pred), \'f1_score\': f1_score(y_test, y_pred) } custom_metrics = { \'accuracy\': accuracy_score(y_test, custom_y_pred), \'precision\': precision_score(y_test, custom_y_pred), \'recall\': recall_score(y_test, custom_y_pred), \'f1_score\': f1_score(y_test, custom_y_pred) } print(\\"Default RBF Kernel Metrics:\\", metrics) print(\\"Custom Kernel Metrics:\\", custom_metrics) ```","solution":"from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score import numpy as np def create_dataset(): Generates a synthetic dataset using sklearn\'s make_classification function. X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_clusters_per_class=2, random_state=42) return X, y def split_dataset(X, y): Splits the dataset into training and testing sets. return train_test_split(X, y, test_size=0.2, random_state=42) def svm_rbf(X_train, y_train, X_test, y_test): Trains and evaluates an SVM with RBF kernel. clf = SVC(kernel=\'rbf\') clf.fit(X_train, y_train) y_pred = clf.predict(X_test) return { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred), \'recall\': recall_score(y_test, y_pred), \'f1_score\': f1_score(y_test, y_pred) } def hyperparameter_tuning(X_train, y_train): Performs hyperparameter tuning using GridSearchCV. param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [0.001, 0.01, 0.1, 1]} grid = GridSearchCV(SVC(kernel=\'rbf\'), param_grid, cv=5) grid.fit(X_train, y_train) return grid.best_params_, grid.best_estimator_ def custom_kernel(X, Y): Custom kernel function. return np.dot(X, Y.T) ** 2 def svm_custom_kernel(X_train, y_train, X_test, y_test): Trains and evaluates an SVM with the custom kernel. custom_clf = SVC(kernel=custom_kernel) custom_clf.fit(X_train, y_train) custom_y_pred = custom_clf.predict(X_test) return { \'accuracy\': accuracy_score(y_test, custom_y_pred), \'precision\': precision_score(y_test, custom_y_pred), \'recall\': recall_score(y_test, custom_y_pred), \'f1_score\': f1_score(y_test, custom_y_pred) }"},{"question":"# Question You are provided with two datasets: \\"penguins\\" and \\"flights\\", which can be loaded directly from seaborn. Your task is to create a function that visualizes various relationships within the \'penguins\' dataset and demonstrates your comprehension of seaborn’s plotting capabilities. Function Signature ```python def visualize_penguin_data(): pass ``` Instructions 1. **Load the data**: Use seaborn to load the \\"penguins\\" dataset and name it `penguins`. 2. **Create Visualizations**: - Generate a point plot displaying `body_mass_g` (y-axis) against `island` (x-axis) where data points are grouped by `sex` denoted by different colors. - Enhance the above plot by coding the `hue` variable using the markers and linestyles. Use markers `[\\"o\\", \\"s\\"]` and linestyles `[\\"-\\", \\"--\\"]`. - Include error bars that represent the standard deviation for each grouping. - Customize the appearance by setting the point plot\'s line color to \\".2\\", marker to \\"D\\", and no linestyle. - Create a second plot to handle overplotting: Display `bill_depth_mm` against `species`, grouped by `sex` and dodging the points. 3. **Compile and display**: - Combine all plots into a single visualization using subplot strategies. - Display all plots using matplotlib’s `plt.show()`. Outputs - The function should generate and display the described plots. - Ensure the plots are clear, well-labeled, and aesthetically pleasing. You can refer to the seaborn documentation and examples provided above to help you implement this function. Additional Constraints - Ensure that the function does not return any value, only displays the plots. - Use seaborn and matplotlib for plotting. - Python\'s version should be 3.6 or above. Your implementation should demonstrate good coding practices, including appropriate comments and organization. **Example Usage** ```python visualize_penguin_data() ``` This call should display the visualizations described above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a point plot displaying `body_mass_g` against `island`, grouped by `sex` plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) sns.pointplot( data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", dodge=True, markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errwidth=1, ci=\\"sd\\" ).set_title(\\"Point Plot of Body Mass by Island and Sex\\") # Customize appearance: line color to \\".2\\", marker to \\"D\\", no linestyle plt.subplot(1, 2, 2) sns.pointplot( data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", dodge=True, errwidth=1, ci=\\"sd\\", palette=\\"dark\\", markers=\\"D\\", linestyles=\\"\\", ).set_title(\\"Customized Point Plot of Body Mass by Island and Sex\\") # Another plot to handle overplotting: `bill_depth_mm` against `species`, grouped by `sex` and dodging points plt.figure(figsize=(10, 5)) sns.catplot( data=penguins, x=\\"species\\", y=\\"bill_depth_mm\\", hue=\\"sex\\", kind=\\"point\\", dodge=True, errwidth=1, aspect=2, markers=[\\"o\\", \\"s\\"], ).set_axis_labels(\\"Species\\", \\"Bill Depth mm\\").set_titles(\\"Bill Depth by Species and Sex\\") plt.show()"},{"question":"**Objective:** Implement Principal Component Analysis (PCA) on the Iris dataset and visualize the results. Evaluate the variance explained by the principal components and discuss the findings. # Problem Statement: You are required to perform Principal Component Analysis (PCA) on the Iris dataset to reduce its dimensionality and visualize the results. The Iris dataset consists of 150 samples with four features: sepal length, sepal width, petal length, and petal width. The dataset is labeled with three classes indicating the species of the Iris flower. You should use PCA to project the data onto a two-dimensional space and explain the variance captured by the principal components. # Requirements: 1. Load the Iris dataset from scikit-learn. 2. Standardize the features (mean=0 and variance=1). 3. Apply PCA to reduce the dimensionality of the dataset to two principal components. 4. Visualize the transformed dataset using a scatter plot, where the points are colored according to their class labels. 5. Compute and display the amount of variance explained by the two principal components. 6. Discuss the results, including insights gained from the visualization and the importance of each principal component. # Input Format: - None (dataset is loaded from `sklearn.datasets`) # Output Format: - A scatter plot of the transformed data. - A printout of the variance explained by the principal components. - A text discussion of your interpretation of the results. # Constraints: - You must use scikit-learn\'s PCA class. - The scatter plot should distinguish different classes with different colors. - Your code should be organized and well-documented. # Performance Requirements: - The entire processing including PCA should complete within a reasonable time frame on standard computing resources. # Sample Solution Structure: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Plot the transformed data plt.figure(figsize=(8, 6)) for target in np.unique(y): plt.scatter(X_pca[y == target, 0], X_pca[y == target, 1], label=iris.target_names[target]) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'PCA of Iris Dataset\') plt.legend() plt.show() # Print the variance explained by the principal components explained_variance = pca.explained_variance_ratio_ print(f\'Variance explained by Principal Component 1: {explained_variance[0]:.2f}\') print(f\'Variance explained by Principal Component 2: {explained_variance[1]:.2f}\') # Discuss the results discussion = The scatter plot demonstrates the distribution of the Iris dataset in the reduced two-dimensional space. The variance explained by the first and second principal components indicates how much information is captured by these components. Discuss which features might be contributing most to these principal components. print(discussion) ``` # Evaluation Criteria: - Correct implementation of feature standardization and PCA. - Accurate calculation and display of explained variance. - Clarity and informativeness of the scatter plot. - Insightfulness of the discussion on results.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA def perform_pca_and_plot(): # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Plot the transformed data plt.figure(figsize=(8, 6)) for target in np.unique(y): plt.scatter(X_pca[y == target, 0], X_pca[y == target, 1], label=iris.target_names[target]) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'PCA of Iris Dataset\') plt.legend() plt.show() # Print the variance explained by the principal components explained_variance = pca.explained_variance_ratio_ print(f\'Variance explained by Principal Component 1: {explained_variance[0]:.2f}\') print(f\'Variance explained by Principal Component 2: {explained_variance[1]:.2f}\') # Interpret the results discussion = The scatter plot demonstrates the distribution of the Iris dataset in the reduced two-dimensional space. The first principal component captures the largest variance in the data, followed by the second principal component. This reduction from four dimensions to two dimensions retains most of the important information for classification. The variance explained by the principal components shows that approximately xx% of the variance is explained by the first two components, indicating their importance. print(discussion) return X_scaled, X_pca, explained_variance"},{"question":"# Question: Implementing and Managing Asynchronous Tasks with asyncio Description: You are tasked with implementing an asynchronous function using the asyncio module to simulate a multi-stage data processing pipeline. The pipeline involves fetching data, processing it, and then saving the results. Each stage should run asynchronously, and certain debugging practices should be applied. Requirements: 1. **Fetching Stage** - Implement an asynchronous function `fetch_data` that simulates fetching data from a remote source. This function should: - Take no arguments. - Use `asyncio.sleep` to simulate a network delay of 2 seconds. - Return a list of numbers from 1 to 10. 2. **Processing Stage** - Implement an asynchronous function `process_data` that processes the fetched data. This function should: - Accept a list of numbers as an argument. - Use `asyncio.sleep` to simulate a processing delay of 1 second for each number. - Return the square of each number in the list in an asynchronous manner, ensuring that each number is processed independently. 3. **Saving Stage** - Implement an asynchronous function `save_results` that simulates saving the processed data. This function should: - Accept a list of squared numbers as an argument. - Use `asyncio.sleep` to simulate a saving delay of 1 second. - Print the list of squared numbers as the final output. 4. **Debugging Configuration** - In your main function, ensure that `asyncio` debug mode is enabled. - Utilize appropriate logging practices to track the asyncio operations. 5. **Main Function** - Implement the main function `main`: - It should fetch the data using `fetch_data`. - Process the fetched data using `process_data`. - Save the results using `save_results`. - Ensure that all stages run asynchronously and efficiently. Constraints: - Do not use any external libraries other than asyncio and logging. - Ensure the solution follows asyncio\'s best practices to avoid common pitfalls such as \\"forgotten await\\" and unhandled exceptions. Example Output: ```python DEBUG:asyncio:Using selector: EpollSelector DEBUG:root:Fetched data: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] DEBUG:root:Processed data: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] DEBUG:root:Saved results: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] ``` Implementation Template: ```python import asyncio import logging # Configure logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger() logger.setLevel(logging.DEBUG) async def fetch_data(): Simulates fetching data from a remote source. await asyncio.sleep(2) data = list(range(1, 11)) logger.debug(f\\"Fetched data: {data}\\") return data async def process_data(data): Processes fetched data asynchronously. processed_data = [] for number in data: await asyncio.sleep(1) # Simulate processing delay processed_data.append(number ** 2) logger.debug(f\\"Processed data: {processed_data}\\") return processed_data async def save_results(results): Simulates saving the processed data. await asyncio.sleep(1) logger.debug(f\\"Saved results: {results}\\") async def main(): # Enable asyncio debug mode asyncio.get_running_loop().set_debug(True) # Fetch data data = await fetch_data() # Process data processed_data = await process_data(data) # Save results await save_results(processed_data) if __name__ == \\"__main__\\": asyncio.run(main()) ``` Notes: - Ensure that the script runs without any warnings or errors and that all asynchronous operations are properly awaited. - The implementation should demonstrate a clear understanding of asyncio\'s principles and logging/debugging practices.","solution":"import asyncio import logging # Configure logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger() logger.setLevel(logging.DEBUG) async def fetch_data(): Simulates fetching data from a remote source. await asyncio.sleep(2) data = list(range(1, 11)) logger.debug(f\\"Fetched data: {data}\\") return data async def process_data(data): Processes fetched data asynchronously. async def process_number(number): await asyncio.sleep(1) # Simulate processing delay return number ** 2 tasks = [process_number(number) for number in data] processed_data = await asyncio.gather(*tasks) logger.debug(f\\"Processed data: {processed_data}\\") return processed_data async def save_results(results): Simulates saving the processed data. await asyncio.sleep(1) logger.debug(f\\"Saved results: {results}\\") async def main(): # Enable asyncio debug mode asyncio.get_running_loop().set_debug(True) # Fetch data data = await fetch_data() # Process data processed_data = await process_data(data) # Save results await save_results(processed_data) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"In this assessment, you will create a custom extension array for pandas that provides specialized behavior for handling a hypothetical new data type: `ComplexNumberArray`, which represents arrays of complex numbers. Requirements: 1. **Custom Extension Array**: - Create a class `ComplexNumber` that represents a complex number with `real` and `imag` attributes. - Implement the class `ComplexNumberArray` that inherits from `pandas.api.extensions.ExtensionArray` and holds an array of `ComplexNumber` objects. 2. **Methods to Implement**: - `_from_sequence(cls, scalars, dtype=None, copy=False)`: Class method to create an `ComplexNumberArray` from a sequence of scalars. - `_from_factorized(cls, values, original)`: Class method to create an array from factorization. - `astype(self, dtype, copy=True)`: Method to cast to different dtype. - `isna(self)`: Method to identify null complex numbers (both `real` and `imag` are zero). - `take(self, indices, allow_fill=False, fill_value=None)`: Method to take elements from the array. 3. **Custom Accessor**: - Register a custom accessor `@pd.api.extensions.register_series_accessor(\\"complex\\")` that adds a method `magnitude` to compute the magnitudes of the complex numbers in the series. Constraints: - Only use standard library functions and the pandas package for implementation. - Ensure the custom array behaves correctly with pandas\' DataFrame operations. Input: 1. A sequence of tuples representing complex numbers. Example: `[(1, 2), (3, 4), (0, 0), (5, 6)]`. 2. Indices to take from the array: `[0, 2, 3]`. Output: - The created `ComplexNumberArray`. - Result of the `take` method. - Series with custom accessor and magnitudes of complex numbers. Example: ```python import pandas as pd from pandas.api.extensions import ExtensionArray class ComplexNumber: # Implementation of the ComplexNumber class class ComplexNumberArray(ExtensionArray): # Implementation of the ComplexNumberArray class # Register the custom accessor @pd.api.extensions.register_series_accessor(\\"complex\\") class ComplexAccessor: def __init__(self, pandas_series): self._series = pandas_series def magnitude(self): # Calculate magnitudes # Example usage data = [(1, 2), (3, 4), (0, 0), (5, 6)] indices = [0, 2, 3] complex_arr = ComplexNumberArray._from_sequence(data) result_take = complex_arr.take(indices) # Creating a pandas Series with ComplexNumberArray s = pd.Series(complex_arr) magnitudes = s.complex.magnitude() print(complex_arr) print(result_take) print(magnitudes) ``` Complete the implementation for the `ComplexNumber`, `ComplexNumberArray`, and `ComplexAccessor` classes as described.","solution":"import pandas as pd import numpy as np from pandas.api.extensions import ExtensionArray, ExtensionDtype class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag def __repr__(self): return f\\"ComplexNumber(real={self.real}, imag={self.imag})\\" class ComplexNumberArray(ExtensionArray): def __init__(self, values): self.data = np.array(values, dtype=object) @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): complex_numbers = [ComplexNumber(real, imag) for real, imag in scalars] return cls(complex_numbers) @classmethod def _from_factorized(cls, values, original): return cls(values) def astype(self, dtype, copy=True): dtype = pd.api.types.pandas_dtype(dtype) if isinstance(dtype, ComplexNumberDtype): if copy: return self.copy() return self return np.array([complex(item.real, item.imag) for item in self.data]).astype(dtype, copy=copy) def isna(self): return np.array([item.real == 0 and item.imag == 0 for item in self.data]) def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: result = [self.data[i] if i >= 0 else fill_value for i in indices] else: result = [self.data[i] for i in indices] return ComplexNumberArray(result) def __getitem__(self, item): if isinstance(item, int): return self.data[item] else: return ComplexNumberArray(self.data[item]) def __len__(self): return len(self.data) @property def dtype(self): return ComplexNumberDtype() def __eq__(self, other): if isinstance(other, ComplexNumberArray): return np.array([self_item.real == other_item.real and self_item.imag == other_item.imag for self_item, other_item in zip(self.data, other.data)]) return False class ComplexNumberDtype(ExtensionDtype): name = \'complexnumber\' type = ComplexNumber kind = \'O\' na_value = ComplexNumber(0, 0) @classmethod def construct_array_type(cls): return ComplexNumberArray # Register the custom accessor @pd.api.extensions.register_series_accessor(\\"complex\\") class ComplexAccessor: def __init__(self, pandas_series): self._series = pandas_series def magnitude(self): return np.sqrt([item.real ** 2 + item.imag ** 2 for item in self._series.values.data]) # Example usage data = [(1, 2), (3, 4), (0, 0), (5, 6)] indices = [0, 2, 3] complex_arr = ComplexNumberArray._from_sequence(data) result_take = complex_arr.take(indices) # Creating a pandas Series with ComplexNumberArray s = pd.Series(complex_arr) magnitudes = s.complex.magnitude() print(complex_arr) print(result_take) print(magnitudes)"},{"question":"**Objective**: Your task is to implement a function that receives a pandas DataFrame and returns a detailed summary of its memory usage, data type, and count of missing values in each column. This exercise will test your understanding of advanced pandas functionalities, memory management, and handling missing data with appropriate data types. **Function Specification**: ```python def dataframe_summary(df: pd.DataFrame) -> pd.DataFrame: Given a DataFrame, returns a summary DataFrame with memory usage, data types, and the count of missing values in each column. Parameters: df (pd.DataFrame): The input DataFrame to summarize. Returns: pd.DataFrame: A DataFrame summarizing memory usage (in bytes), data types, and the count of missing values in each column. pass ``` **Input**: - `df` (pd.DataFrame): The input DataFrame which may contain various data types, including integers, floats, objects, categories, and complex numbers. **Output**: - A pandas DataFrame with the following columns: 1. **Column Name** (index): The name of the column from the original DataFrame. 2. **Memory Usage (bytes)**: The memory usage of each column in bytes. 3. **Data Type**: The dtype of each column. 4. **Missing Values**: Count of missing values in each column. **Constraints**: - For integer columns with missing values, ensure that the missing values are handled correctly with nullable integer extension types (e.g., `pd.Int64Dtype`). - The function should use `memory_usage=\'deep\'` to get accurate memory usage statistics. - The solution should not rely on external libraries other than pandas and numpy. **Example**: ```python import pandas as pd import numpy as np data = { \'A\': np.random.randint(0, 100, size=1000), \'B\': np.random.rand(1000), \'C\': np.random.choice([\'X\', \'Y\', \'Z\'], size=1000), \'D\': [pd.Timestamp(\'20210101\')] * 1000, \'E\': pd.Series(np.random.choice([1, None], size=1000), dtype=pd.Int64Dtype()), } df = pd.DataFrame(data) summary_df = dataframe_summary(df) print(summary_df) ``` Expected output: ``` | Column Name | Memory Usage (bytes) | Data Type | Missing Values | |-------------|----------------------|-------------|----------------| | A | 8000 | int64 | 0 | | B | 8000 | float64 | 0 | | C | 5142 | object | 0 | | D | 16000 | datetime64[ns] | 0 | | E | 8096 | Int64 | 495 | ``` Ensure your solution is efficient and leverages pandas capabilities thoroughly.","solution":"import pandas as pd def dataframe_summary(df: pd.DataFrame) -> pd.DataFrame: Given a DataFrame, returns a summary DataFrame with memory usage, data types, and the count of missing values in each column. Parameters: df (pd.DataFrame): The input DataFrame to summarize. Returns: pd.DataFrame: A DataFrame summarizing memory usage (in bytes), data types, and the count of missing values in each column. summary_data = { \'Column Name\': df.columns, \'Memory Usage (bytes)\': [df[col].memory_usage(deep=True) for col in df.columns], \'Data Type\': [df[col].dtype for col in df.columns], \'Missing Values\': [df[col].isnull().sum() for col in df.columns] } summary_df = pd.DataFrame(summary_data) return summary_df.set_index(\'Column Name\')"},{"question":"<|Analysis Begin|> The provided documentation discusses various functions related to the PySequence protocol in Python 3.10. These functions perform different operations on Python sequences, such as checking if an object is a sequence, getting the size of a sequence, concatenating sequences, repeating sequences, accessing and modifying sequence items and slices, counting occurrences of an item in a sequence, checking for membership, and converting sequences to lists or tuples. The functions significantly overlap with standard Python list operations but with an emphasis on low-level, C API usage. To design an effective coding question, we can focus on ensuring understanding and implementation of fundamental sequence manipulations in a Pythonic manner. The question should require students to implement functionality similar to the documented sequence operations using Python constructs. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Implement a set of Python functions that mimic the behavior of the sequence protocol functions provided in the documentation. Your task is to write these functions from scratch using Python constructs without directly using the functions specified in the documentation. **Functions to Implement:** 1. `is_sequence(obj) -> bool` Checks if the given object is a sequence. A sequence in this context is any object that implements the `__getitem__` method and is not a dictionary. **Input:** Any Python object. **Output:** `True` if the object is a sequence else `False`. 2. `sequence_size(seq) -> int` Returns the number of items in the sequence. **Input:** A sequence object. **Output:** The number of items in the sequence. 3. `sequence_concat(seq1, seq2) -> List` Returns the concatenation of two sequences. **Input:** Two sequence objects. **Output:** A new sequence containing the elements of both input sequences. 4. `sequence_repeat(seq, count) -> List` Repeats the sequence `count` times. **Input:** A sequence object and an integer count. **Output:** A new sequence that is the input sequence repeated `count` times. 5. `sequence_getitem(seq, i)` Returns the `i`th element of the sequence. **Input:** A sequence object and an integer index. **Output:** The `i`th element of the sequence. 6. `sequence_getslice(seq, i1, i2) -> List` Returns a slice of the sequence from index `i1` to index `i2`. **Input:** A sequence object and two integer indices. **Output:** A new sequence containing the slice from `i1` to `i2`. 7. `sequence_count(seq, value) -> int` Returns the number of occurrences of `value` in the sequence. **Input:** A sequence object and a value. **Output:** The count of occurrences of the value in the sequence. 8. `sequence_contains(seq, value) -> bool` Checks if `value` is present in the sequence. **Input:** A sequence object and a value. **Output:** `True` if the value is in the sequence else `False`. **Constraints:** - Do not use any built-in function that directly performs the specified task (e.g., `list.count` for `sequence_count`). - Use only standard Python libraries. - Ensure your functions handle edge cases appropriately. - Aim for efficient implementations, especially for larger sequences. **Example Usage:** ```python # Example sequence seq = [1, 2, 3, 4, 5] # 1. Checking if seq is a sequence print(is_sequence(seq)) # Output: True # 2. Getting the size of the sequence print(sequence_size(seq)) # Output: 5 # 3. Concatenating sequences print(sequence_concat(seq, [6, 7, 8])) # Output: [1, 2, 3, 4, 5, 6, 7, 8] # 4. Repeating sequences print(sequence_repeat(seq, 2)) # Output: [1, 2, 3, 4, 5, 1, 2, 3, 4, 5] # 5. Getting an item print(sequence_getitem(seq, 2)) # Output: 3 # 6. Getting a slice print(sequence_getslice(seq, 1, 3)) # Output: [2, 3] # 7. Counting occurrences print(sequence_count(seq, 3)) # Output: 1 # 8. Checking if a sequence contains a value print(sequence_contains(seq, 4)) # Output: True ``` Please implement these functions in a clear, optimized, and pythonic manner to demonstrate your understanding of sequence manipulation.","solution":"def is_sequence(obj): Returns True if the object is a sequence, False otherwise. return hasattr(obj, \'__getitem__\') and not isinstance(obj, dict) def sequence_size(seq): Returns the number of items in the sequence. count = 0 for _ in seq: count += 1 return count def sequence_concat(seq1, seq2): Returns the concatenation of two sequences. return [item for item in seq1] + [item for item in seq2] def sequence_repeat(seq, count): Repeats the sequence count times. return [item for item in seq for _ in range(count)] def sequence_getitem(seq, i): Returns the ith element of the sequence. return seq[i] def sequence_getslice(seq, i1, i2): Returns a slice of the sequence from index i1 to index i2. return seq[i1:i2] def sequence_count(seq, value): Returns the number of occurrences of value in the sequence. count = 0 for item in seq: if item == value: count += 1 return count def sequence_contains(seq, value): Returns True if the sequence contains the value, False otherwise. for item in seq: if item == value: return True return False"},{"question":"# **Sparse DataFrame Manipulation** Objective: You are required to demonstrate your understanding of pandas\' sparse data structures. You need to perform the following tasks on a given dense DataFrame: 1. Convert the dense DataFrame to a sparse DataFrame using `SparseDtype`. 2. Calculate the density of the sparse DataFrame. 3. Apply a specified NumPy ufunc to the entire sparse DataFrame. 4. Convert the resulting sparse DataFrame back to a dense DataFrame and verify a specific cell value. Input Format: * A dense pandas DataFrame `df` with shape `(1000, 10)` filled with random values and some `np.nan` values. * A string representing a NumPy ufunc to apply (e.g., `\'np.abs\'`). Output Format: * A tuple containing: 1. The density of the sparse DataFrame (float). 2. The resulting dense DataFrame after applying the ufunc (pandas DataFrame). 3. Value at cell (0, 0) from the resulting dense DataFrame (float). Constraints: * Ensure that the conversion to sparse and back to dense is handled efficiently in terms of memory. * Handle the ufunc string safely, assuming common mathematical functions (e.g., `np.abs`, `np.sqrt`, etc.). Example: ```python # Sample Input import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(1000, 10)) df.iloc[100:200, 3:7] = np.nan ufunc = \'np.sqrt\' # Sample Output (0.90, <resulting_dense_dataframe>, 0.123) # Example values ``` Implementation: ```python def sparse_dataframe_operations(df, ufunc): import pandas as pd import numpy as np # Convert dense DataFrame to Sparse DataFrame sparse_df = df.astype(pd.SparseDtype(float, np.nan)) # Calculate the density of the sparse DataFrame density = sparse_df.sparse.density # Apply the specified ufunc to each element in the Sparse DataFrame func = eval(ufunc) modified_sparse_df = sparse_df.applymap(func) # Convert the resulting Sparse DataFrame back to a dense DataFrame resulting_dense_df = modified_sparse_df.sparse.to_dense() # Get the value at cell (0, 0) from the resulting dense DataFrame cell_value = resulting_dense_df.iloc[0, 0] return density, resulting_dense_df, cell_value # Sample Function Call # density, resulting_df, cell_val = sparse_dataframe_operations(df, \'np.sqrt\') ``` Demonstrate your function with the provided `df` and `ufunc` string, ensuring that it handles the memory-efficient sparse conversion and correct application of the ufunc.","solution":"import pandas as pd import numpy as np def sparse_dataframe_operations(df, ufunc): Perform operations on a dense dataframe including converting to sparse dataframe, calculating density, applying a ufunc, and converting back to dense dataframe. # Convert dense DataFrame to Sparse DataFrame sparse_df = df.astype(pd.SparseDtype(float, np.nan)) # Calculate the density of the sparse DataFrame density = sparse_df.sparse.density # Apply the specified ufunc to each element in the Sparse DataFrame func = eval(ufunc) modified_sparse_df = sparse_df.applymap(func) # Convert the resulting Sparse DataFrame back to a dense DataFrame resulting_dense_df = modified_sparse_df.sparse.to_dense() # Get the value at cell (0, 0) from the resulting dense DataFrame cell_value = resulting_dense_df.iloc[0, 0] return density, resulting_dense_df, cell_value"},{"question":"**Custom Sequence Type Implementation** **Objective**: Implement a custom sequence type in Python that integrates properly with Python\'s memory management and garbage collection. **Task**: You will implement a custom sequence-like object named `CustomSequence`. This object should: 1. Support standard sequence operations such as getting an item by index, setting an item, and calculating its length. 2. Be dynamically resizable (similar to lists). 3. Include basic garbage collection support to manage memory efficiently. **Requirements**: 1. Implement a class `CustomSequence` with the following methods: - `__init__(self, initial_data=None)`: Initializes the custom sequence. If `initial_data` is provided, it should be an iterable that initializes the sequence. - `__len__(self)`: Returns the number of elements in the sequence. - `__getitem__(self, index)`: Returns the element at the given index. - `__setitem__(self, index, value)`: Sets the element at the given index to `value`. - `append(self, value)`: Adds `value` to the end of the sequence. 2. Implement integration with Python\'s garbage collection: - Use appropriate Python object structures for memory management. - Ensure cyclic references can be detected and handled by Python\'s garbage collector. **Constraints**: - The sequence should handle up to 100,000 elements efficiently. - The `__getitem__` and `__setitem__` operations should be O(1) on average. **Hints**: - Consider using Python\'s `collections.abc.Sequence` as a base for your class. - Pay close attention to memory handling to avoid memory leaks. - Utilize the `gc` module to assist with garbage collection management. **Example**: ```python # Example usage of CustomSequence seq = CustomSequence([1, 2, 3]) print(len(seq)) # Output: 3 print(seq[0]) # Output: 1 seq[1] = 10 print(seq[1]) # Output: 10 seq.append(20) print(len(seq)) # Output: 4 print(seq[3]) # Output: 20 ``` Submit your implementation of the `CustomSequence` class below.","solution":"import gc class CustomSequence: def __init__(self, initial_data=None): if initial_data is not None: self._data = list(initial_data) else: self._data = [] gc.collect() def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value gc.collect() def append(self, value): self._data.append(value) gc.collect()"},{"question":"You are required to implement a function named `create_built_distribution` in Python. This function should automate the creation of built distributions using the Distutils `setup.py` script. The function should accept the path to the setup script and a list of formats as input and output a list of paths to the created distribution files. # Function Signature: ```python def create_built_distribution(setup_script_path: str, formats: List[str]) -> List[str]: pass ``` # Input: - `setup_script_path` (str): The file path to the `setup.py` script. - `formats` (List[str]): A list of string formats for the distribution (e.g., `[\'gztar\', \'zip\', \'rpm\']`). # Output: - List[str]: A list of the file paths to the created distribution files. # Constraints: - Ensure that the provided formats are valid (`gztar`, `bztar`, `xztar`, `ztar`, `tar`, `zip`, `rpm`, `pkgtool`, `sdux`, `msi`). - Handle the execution of the `setup.py` script using the `subprocess` module. - Capture and log any errors encountered during the execution. - Assume that the setup script is located in a directory with appropriate read/write permissions. - Make sure that Python and necessary tools for creating the distributions are pre-installed on the system. # Example: ```python # Example usage dist_files = create_built_distribution(\\"path/to/setup.py\\", [\'gztar\', \'zip\']) print(dist_files) # Output could be something like: [ # \\"path/to/dist/your-package-1.0.gztar\\", # \\"path/to/dist/your-package-1.0.zip\\" # ] ``` # Note: - Using the given setup script path and the list of formats, this function should run the necessary `python setup.py` commands to produce the specified built distributions. - Validate that the provided formats are among the supported formats. If an invalid format is detected, raise a `ValueError` with an appropriate message. - Capture any command execution errors and return an empty list if an error occurs.","solution":"import subprocess from typing import List, Optional import os def create_built_distribution(setup_script_path: str, formats: List[str]) -> List[str]: Automate the creation of built distributions using the Distutils setup.py script. Args: setup_script_path (str): The file path to the setup.py script. formats (List[str]): A list of string formats for the distribution (e.g., [\'gztar\', \'zip\', \'rpm\']). Returns: List[str]: A list of the file paths to the created distribution files. valid_formats = {\'gztar\', \'bztar\', \'xztar\', \'ztar\', \'tar\', \'zip\', \'rpm\', \'pkgtool\', \'sdux\', \'msi\'} # Validate formats for f in formats: if f not in valid_formats: raise ValueError(f\\"Invalid format: {f}. Supported formats are: {valid_formats}\\") # Run the setup script with the appropriate command distribution_paths = [] for f in formats: try: # Run the command to create the distribution subprocess.run([\'python\', setup_script_path, \'sdist\', \'--formats\', f], check=True) # Assuming the distributions are created in a \'dist\' directory dist_dir = os.path.join(os.path.dirname(setup_script_path), \'dist\') files_created = os.listdir(dist_dir) files_of_format = [os.path.join(dist_dir, file) for file in files_created if file.endswith(f)] distribution_paths.extend(files_of_format) except subprocess.CalledProcessError as e: print(f\\"An error occurred while running the setup script: {e}\\") return [] return distribution_paths"},{"question":"# FTP Directory Synchronization Tool Objective Create a Python function that synchronizes a local directory with a remote directory on an FTP server using the `ftplib` module. The function should ensure that all files present in the local directory are also present on the remote directory, and only those files. Function Signature ```python import os from ftplib import FTP, error_perm def sync_local_to_ftp(local_dir: str, ftp_host: str, ftp_user: str = \'anonymous\', ftp_passwd: str = \'\', ftp_dir: str = \'\') -> None: Synchronize the contents of a local directory with a remote directory on an FTP server. Parameters: - local_dir (str): Path to the local directory. - ftp_host (str): Hostname of the FTP server. - ftp_user (str): Username to login to the FTP server. Default is \'anonymous\'. - ftp_passwd (str): Password to login to the FTP server. Default is an empty string. - ftp_dir (str): Path to the remote directory on the FTP server. Default is the root directory. Returns: - None Raises: - OSError: If there are issues accessing the local directory or file paths. - ftplib.all_errors: If there are issues with the FTP connection or commands. pass ``` Expected Behavior - Connects to the FTP server using the provided credentials and navigates to the specified remote directory. - Lists all files in the local directory and the remote FTP directory. - Uploads files from the local directory to the remote directory if they are not already present. - Deletes files from the remote directory that are not present in the local directory, ensuring both directories contain the same files. - Uses binary mode for file transfers to handle all file types correctly. - Properly handles and logs any exceptions or errors encountered during the FTP operations. Constraints - Assume that the FTP server supports basic commands such as `LIST`, `NLST`, `STOR`, and `DELE`. - Assume sufficient permissions for reading the local directory and modifying the remote directory. - Ensure efficient operation considering network and I/O performance where possible. Example ```python # Local directory contains: [\'file1.txt\', \'file2.txt\'] # Remote FTP directory contains: [\'file2.txt\', \'file3.txt\'] # After running: sync_local_to_ftp(\'/path/to/local_dir\', \'ftp.example.com\', \'user\', \'password\', \'/path/to/remote_dir\') # Remote FTP directory should contain: [\'file1.txt\', \'file2.txt\'] ``` Hints - Use the `os` module to list files in the local directory. - Utilize `FTP.nlst()` to list files in the remote directory. - Employ `FTP.storbinary()` to upload files. - Use `FTP.delete()` to remove files from the remote directory. - Implement proper exception handling to catch and log potential issues like connection errors or permission issues.","solution":"import os from ftplib import FTP, error_perm def sync_local_to_ftp(local_dir: str, ftp_host: str, ftp_user: str = \'anonymous\', ftp_passwd: str = \'\', ftp_dir: str = \'\') -> None: try: # Connect to the FTP server with FTP(ftp_host) as ftp: ftp.login(user=ftp_user, passwd=ftp_passwd) # Change to the target directory on the FTP server if specified if ftp_dir: ftp.cwd(ftp_dir) # List files in the remote FTP directory remote_files = ftp.nlst() # List files in the local directory local_files = os.listdir(local_dir) # Upload files from local directory to remote directory for file in local_files: local_file_path = os.path.join(local_dir, file) if os.path.isfile(local_file_path) and file not in remote_files: with open(local_file_path, \'rb\') as f: ftp.storbinary(f\'STOR {file}\', f) # Delete files from remote directory that are not in local directory for file in remote_files: if file not in local_files: try: ftp.delete(file) except error_perm: pass # Keep going even if there\'s an error deleting a file except OSError as e: print(f\\"OS error: {e}\\") except error_perm as e: print(f\\"FTP error: {e}\\") except Exception as e: print(f\\"Unexpected error: {e}\\")"},{"question":"Event Priority Scheduler in Multi-threaded Environment You are tasked with implementing a custom task scheduler using Python’s `sched` module. You have to extend this functionality to support dynamic priority adjustment and robust handling of events in a multi-threaded environment. **Objective**: 1. Implement a class `PriorityScheduler` that schedules tasks using both absolute and relative times with priority. 2. Create a method to dynamically change the priority of a scheduled event. 3. Implement thread-safe mechanisms to handle scheduling and execution. # Requirements: 1. **Class Construction**: Define a class `PriorityScheduler` that uses: ```python class PriorityScheduler: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) self.lock = threading.Lock() ``` 2. **Adding Events**: Implement methods `schedule_event_abs` and `schedule_event` to add events at an absolute time and after a delay respectively. ```python def schedule_event_abs(self, time, priority, action, argument=(), kwargs={}): with self.lock: event = self.scheduler.enterabs(time, priority, action, argument, kwargs) return event def schedule_event(self, delay, priority, action, argument=(), kwargs={}): with self.lock: event = self.scheduler.enter(delay, priority, action, argument, kwargs) return event ``` 3. **Running Scheduler**: Implement a method `run_scheduler` to execute scheduled events. This should be thread-safe. ```python def run_scheduler(self, blocking=True): with self.lock: self.scheduler.run(blocking) ``` 4. **Changing Event Priority**: Implement a method `change_priority` to dynamically change the priority of an existing event. ```python def change_priority(self, event, new_priority): with self.lock: self.scheduler.cancel(event) event.priority = new_priority self.scheduler.enterabs(event.time, new_priority, event.action, event.argument, event.kwargs) ``` # Input: - Times and delays use the standard library’s `time` module format. - `action` is a callable function. - `argument` is a tuple of positional arguments. - `kwargs` is a dictionary of keyword arguments. # Output: - Methods should properly schedule events and handle the execution as specified. - `change_priority` should effectively update the event\'s priority without re-scheduling the entire queue. # Constraints: - Ensure that the scheduler can handle at least 100 events efficiently. - The priority change should maintain the event’s original scheduling time. - Implement and showcase thread-safe scheduling in a multi-threaded manner. # Example Usage: ```python import time import threading scheduler = PriorityScheduler() def print_message(message): print(f\\"{time.time()}: {message}\\") # Schedule events event1 = scheduler.schedule_event_abs(time.time() + 10, 1, print_message, argument=(\\"First Event\\",)) event2 = scheduler.schedule_event(5, 2, print_message, argument=(\\"Second Event\\",)) event3 = scheduler.schedule_event(5, 1, print_message, argument=(\\"Third Event\\",)) # Change priority scheduler.change_priority(event3, 3) # Run Scheduler scheduler.run_scheduler() ``` In this example: - Events are scheduled with different times and priorities. - The priority of `event3` is changed before execution. - The scheduler runs and executes events while maintaining thread safety. **Note**: Include appropriate imports such as `sched`, `time`, and `threading` at the beginning of your script.","solution":"import sched import threading import time class PriorityScheduler: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) self.lock = threading.Lock() def schedule_event_abs(self, abs_time, priority, action, argument=(), kwargs={}): with self.lock: event = self.scheduler.enterabs(abs_time, priority, action, argument, kwargs) return event def schedule_event(self, delay, priority, action, argument=(), kwargs={}): with self.lock: event = self.scheduler.enter(delay, priority, action, argument, kwargs) return event def run_scheduler(self, blocking=True): with self.lock: self.scheduler.run(blocking) def change_priority(self, event, new_priority): with self.lock: self.scheduler.cancel(event) new_event = self.scheduler.enterabs(event.time, new_priority, event.action, event.argument, event.kwargs) return new_event"},{"question":"<|Analysis Begin|> The provided documentation is an overview of various modules and utilities available in Python 3.10 related to importing modules. It briefly mentions specific modules and their functionalities without going into detailed explanations or examples. These modules offer advanced import mechanisms and customization of the import process, which includes: - Importing modules from Zip archives (`zipimport`). - Utilities for working with packages (`pkgutil`). - Finding modules used by a script (`modulefinder`). - Locating and executing Python modules (`runpy`). - Implementation details of the import system (`importlib` and its submodules). - Metadata management for Python packages (`importlib.metadata`). The information provided here is more of a summary or index of the modules and their purposes rather than a complete, detailed guide. However, it does give us enough context to create a challenging and contextually appropriate question. <|Analysis End|> <|Question Begin|> # Code Assessment Question: Implementing Custom Module Importer Objective: Demonstrate your understanding of the advanced import mechanism in Python 3.10 by creating a custom importer. # Problem Statement In this task, you are required to implement a custom module importer that can import Python modules from a specified directory or ZIP archive. You need to define a class `CustomImporter` that integrates with Python\'s import system. This importer should be able to: 1. Import a module from a specified directory. 2. Import a module from a specified ZIP archive. 3. Handle cases where the module or package is not present and raise appropriate errors. # Instructions 1. **Implement the `CustomImporter` class:** - **`__init__(self, source_path: str)`**: Initialize with the source path, which can be either a directory path or a ZIP file path. - **`find_module(self, fullname: str, path: Optional[str] = None)`**: Locate the module specified by `fullname`. If the module is found in the source path or ZIP archive, return `self`; otherwise, return `None`. - **`load_module(self, fullname: str)`**: Load and return the module specified by `fullname`. If the module cannot be loaded, raise `ImportError`. 2. **Register the `CustomImporter` with Python\'s import system** using `sys.meta_path`. Ensure this importer takes precedence over the default import mechanisms. 3. **Handle import errors gracefully** by providing meaningful error messages when an import fails. # Example Usage ```python # Assuming we have a directory named `custom_modules` with a module `example.py` # or a zip archive `modules.zip` containing `example.py` import sys # Initialize the importer with the directory path importer = CustomImporter(\'/path/to/custom_modules\') sys.meta_path.insert(0, importer) # Now we can import the module using the custom importer import example # Initialize the importer with the zip archive path importer = CustomImporter(\'/path/to/modules.zip\') sys.meta_path.insert(0, importer) # Now we can import the module using the custom importer import example ``` # Constraints - Ensure that your implementation is compatible with Python 3.10. - The solution should handle both plain directories and ZIP archives. - Only implement the necessary importing mechanisms; do not use any external libraries that simplify this process (e.g., `zipimporter`). # Hints - Refer to the `importlib` module and its submodules for useful utilities and abstract base classes that can assist in implementing custom importers. - Use the `zipfile` module to interact with ZIP archives if necessary. - Remember to handle edge cases such as the module not existing or the specified path being invalid. Good luck!","solution":"import sys import os import zipfile import importlib.util import importlib.machinery from types import ModuleType from typing import Optional class CustomImporter: def __init__(self, source_path: str): self.source_path = source_path self.is_zip = zipfile.is_zipfile(source_path) def find_module(self, fullname: str, path: Optional[str] = None): if self.is_zip: with zipfile.ZipFile(self.source_path) as z: if self._get_module_path(fullname) in z.namelist(): return self else: module_path = os.path.join(self.source_path, self._get_module_path(fullname)) if os.path.exists(module_path): return self return None def load_module(self, fullname: str) -> ModuleType: if self.is_zip: return self._load_module_from_zip(fullname) else: return self._load_module_from_dir(fullname) def _get_module_path(self, fullname: str) -> str: module_name = fullname.split(\'.\')[-1] return module_name + \'.py\' def _load_module_from_zip(self, fullname: str) -> ModuleType: module_path = self._get_module_path(fullname) with zipfile.ZipFile(self.source_path) as z: try: module_code = z.read(module_path) except KeyError: raise ImportError(f\\"Module {fullname} not found in {self.source_path}\\") spec = importlib.util.spec_from_loader(fullname, loader=None) module = importlib.util.module_from_spec(spec) exec(module_code, module.__dict__) sys.modules[fullname] = module return module def _load_module_from_dir(self, fullname: str) -> ModuleType: module_path = os.path.join(self.source_path, self._get_module_path(fullname)) if not os.path.exists(module_path): raise ImportError(f\\"Module {fullname} not found in directory {self.source_path}\\") spec = importlib.util.spec_from_file_location(fullname, module_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) sys.modules[fullname] = module return module # Register the CustomImporter in sys.meta_path def register_custom_importer(source_path: str): importer = CustomImporter(source_path) sys.meta_path.insert(0, importer)"},{"question":"# Text Editor Using `curses` You are required to implement a basic text editor in Python using the `curses` module. The text editor should support the following functionalities: 1. **Create a text window** that users can type into. 2. **Handle basic cursor movements** using arrow keys. 3. **Support text insertion and deletion**. 4. **Persist the text entered and display it back** correctly upon re-running the editor. Requirements: 1. Initialize the curses application screen. 2. Create a window for text editing. 3. Handle user inputs including alphabets, digits, special characters, and control keys for cursor movement and text manipulation: - Arrow keys for navigating the text (up, down, left, right). - Backspace key for deleting the character before the cursor. - Enter key for inserting a newline character. 4. Write the entire content of the text editor to a file named \\"text_content.txt\\" upon exiting. # Details: `initialize_screen` function: 1. **Input**: None 2. **Output**: Initialize and return the main window (stdscr). `edit_text` function: 1. **Input**: stdscr (window object) 2. **Output**: None 3. **Behavior**: Lets the user edit text and handle inputs as described. `save_to_file` function: 1. **Input**: stdscr (window object) 2. **Output**: None 3. **Behavior**: Save the content of the window to \\"text_content.txt\\". # Constraints: 1. The maximum size of the text window should be 20 rows by 50 columns. 2. Support up to a maximum of 1000 characters. # Example: When the user edits text and presses keys, the text should be updated in real-time in the terminal window. Upon exiting the editor, the written content should be saved into the file. Here is a template of the code structure you should follow: ```python import curses def initialize_screen(): This function initializes the curses screen and returns the stdscr object. stdscr = curses.initscr() curses.noecho() curses.cbreak() stdscr.keypad(True) return stdscr def edit_text(stdscr): This function allows text editing inside the curses window. max_y, max_x = 20, 50 win = curses.newwin(max_y, max_x, 0, 0) cursor_y, cursor_x = 0, 0 while True: key = win.getch() if key == curses.KEY_UP: # Move cursor up pass elif key == curses.KEY_DOWN: # Move cursor down pass elif key == curses.KEY_LEFT: # Move cursor left pass elif key == curses.KEY_RIGHT: # Move cursor right pass elif key == 127: # Handling backspace in some terminals # Handle backspace pass elif key == ord(\'n\'): # Handle enter key pass elif key == ord(\'q\'): # Exit on pressing \'q\' break else: # Handle character input win.addch(cursor_y, cursor_x, key) # Calling save_to_file function after exiting the loop save_to_file(win) def save_to_file(win): This function saves the window content to a file. with open(\\"text_content.txt\\", \\"w\\") as f: content = win.instr().decode(\'utf-8\') f.write(content) def main(): stdscr = initialize_screen() try: edit_text(stdscr) finally: curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": main() ``` *Note*: Make sure `curses` applications should be run in a proper terminal environment. Running from IDE might not handle curses applications properly.","solution":"import curses def initialize_screen(): This function initializes the curses screen and returns the stdscr object. stdscr = curses.initscr() curses.noecho() curses.cbreak() stdscr.keypad(True) return stdscr def edit_text(stdscr): This function allows text editing inside the curses window, handling cursor movements and text insertion/deletion. max_y, max_x = 20, 50 win = curses.newwin(max_y, max_x, 0, 0) contents = [\\"\\"] # list of strings to hold each line of text cursor_y, cursor_x = 0, 0 while True: win.clear() for i, line in enumerate(contents): win.addstr(i, 0, line) win.move(cursor_y, cursor_x) win.refresh() key = win.getch() if key == curses.KEY_UP and cursor_y > 0: cursor_y -= 1 cursor_x = min(cursor_x, len(contents[cursor_y])) elif key == curses.KEY_DOWN and cursor_y < len(contents) - 1: cursor_y += 1 cursor_x = min(cursor_x, len(contents[cursor_y])) elif key == curses.KEY_LEFT and cursor_x > 0: cursor_x -= 1 elif key == curses.KEY_RIGHT and cursor_x < len(contents[cursor_y]): cursor_x += 1 elif key == 127: # Handle backspace if cursor_x > 0: contents[cursor_y] = contents[cursor_y][:cursor_x - 1] + contents[cursor_y][cursor_x:] cursor_x -= 1 elif cursor_y > 0: cursor_x = len(contents[cursor_y - 1]) contents[cursor_y - 1] += contents[cursor_y] del contents[cursor_y] cursor_y -= 1 elif key == ord(\'n\'): # Handle enter key contents.insert(cursor_y + 1, contents[cursor_y][cursor_x:]) contents[cursor_y] = contents[cursor_y][:cursor_x] cursor_y += 1 cursor_x = 0 elif key == ord(\'q\'): # Exit on pressing \'q\' break else: # Add character to current position contents[cursor_y] = contents[cursor_y][:cursor_x] + chr(key) + contents[cursor_y][cursor_x:] cursor_x += 1 if len(contents) >= max_y and contents[-1] == \\"\\": contents.pop() save_to_file(contents) def save_to_file(contents): This function saves the window content to a file. with open(\\"text_content.txt\\", \\"w\\") as f: f.write(\\"n\\".join(contents)) def main(): stdscr = initialize_screen() try: edit_text(stdscr) finally: curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": main()"},{"question":"**Principal Component Analysis (PCA) Implementation and Analysis using scikit-learn** # Objective: Implement PCA using scikit-learn, project a dataset onto a reduced number of components, and analyze the explained variance and the principal components. # Problem Statement: You are given the Iris dataset, a classic dataset in machine learning. Your task is to: 1. Load the dataset. 2. Apply PCA to reduce its dimensions to 2. 3. Project the dataset into this new 2D space. 4. Plot the transformed data. 5. Calculate and print the explained variance ratios. # Requirements: 1. Use PCA from scikit-learn. 2. The transformed dataset should be plotted using matplotlib. 3. Include axis labels and a title in the plot. 4. Print the explained variance ratios of the 2 principal components. # Input Format: None (the loading of the Iris dataset should be handled within the function). # Output Format: - A plot showing the transformed dataset in the 2D space of the first two principal components. - Printed explained variance ratios of the first two principal components. # Function Signature: ```python def perform_pca_and_plot(): pass ``` # Example: Here is an example of what the output should look like in terms of the plot and printed results: ```plaintext Explained variance ratio of component 1: 0.92461872 Explained variance ratio of component 2: 0.05306648 ``` ![Sample PCA Plot](https://your/output/plot/path.png) # Constraints: - Use only the scikit-learn package for PCA. - Ensure code cleanliness and readability. # Hints: - You can use `sklearn.datasets.load_iris` to load the dataset. - Use `sklearn.decomposition.PCA` for applying PCA. - For plotting, you might want to use matplotlib\'s `scatter` function. # Note: - Ensure the plot is correctly labeled for clarity. - Make sure to handle any potential edge cases, such as verifying the dataset\'s loading correctly. ```python # Sample code outline for guidance import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.datasets import load_iris def perform_pca_and_plot(): # Load the iris dataset iris = load_iris() X = iris.data y = iris.target # Perform PCA reduction to 2 components pca = PCA(n_components=2) X_r = pca.fit_transform(X) # Plotting the results plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] target_names = iris.target_names for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, lw=2, label=target_name) plt.legend(loc=\\"best\\", shadow=False, scatterpoints=1) plt.title(\'PCA of IRIS dataset\') # Print the explained variance ratios print(f\'Explained variance ratio of component 1: {pca.explained_variance_ratio_[0]:.8f}\') print(f\'Explained variance ratio of component 2: {pca.explained_variance_ratio_[1]:.8f}\') plt.show() ```","solution":"import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.datasets import load_iris def perform_pca_and_plot(): # Load the iris dataset iris = load_iris() X = iris.data y = iris.target # Perform PCA reduction to 2 components pca = PCA(n_components=2) X_r = pca.fit_transform(X) # Plotting the results plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] target_names = iris.target_names for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, lw=2, label=target_name) plt.legend(loc=\\"best\\", shadow=False, scatterpoints=1) plt.title(\'PCA of IRIS dataset\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') # Print the explained variance ratios print(f\'Explained variance ratio of component 1: {pca.explained_variance_ratio_[0]:.8f}\') print(f\'Explained variance ratio of component 2: {pca.explained_variance_ratio_[1]:.8f}\') plt.show()"},{"question":"# Question: Implement and Evaluate LDA and QDA Classifiers Using `scikit-learn` You are provided with a dataset containing labeled instances of various classes. Your task is to implement Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) classifiers using the `scikit-learn` library and evaluate their performance. Dataset - The dataset is a CSV file named `classification_data.csv`, which contains the following columns: - `feature1`, `feature2`, ..., `featureN` (numerical features) - `label` (class labels) Tasks 1. **Load and Prepare Data**: - Load the dataset from the CSV file. - Split the dataset into training and testing sets (80% training, 20% testing). 2. **Implement LDA and QDA**: - Implement LDA using `LinearDiscriminantAnalysis` from `scikit-learn`. - Implement QDA using `QuadraticDiscriminantAnalysis` from `scikit-learn`. 3. **Training and Prediction**: - Train both classifiers on the training set. - Predict the labels for the testing set using both classifiers. 4. **Evaluation**: - Evaluate the performance of both classifiers using the accuracy score and confusion matrix. - Print and compare the results for LDA and QDA. 5. **Dimensionality Reduction with LDA**: - Perform dimensionality reduction on the training set using LDA (set `n_components` to 2). - Plot the reduced data on a 2D plot with different colors for different classes. Constraints - Use the `scikit-learn` library for your implementation. - Your implementation should handle cases where the dataset has a varying number of features and classes. Expected Input and Output ```python def lda_and_qda_classification(file_path: str): # Expected Input: file_path (string) - Path to the CSV file # Expected Output: Prints accuracy and confusion matrix for both LDA and QDA # Plots the 2D reduced data using LDA # Example usage lda_and_qda_classification(\\"classification_data.csv\\") ``` Submission - Submit your code in a Python file named `lda_qda_classification.py`. - Ensure your code is well-documented and follows PEP 8 guidelines. **Note**: Make sure to install the necessary packages before running your code: ```bash pip install scikit-learn matplotlib ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score, confusion_matrix import matplotlib.pyplot as plt def lda_and_qda_classification(file_path: str): # Load the dataset data = pd.read_csv(file_path) # Separate features and labels X = data.drop(columns=[\'label\']) y = data[\'label\'] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement LDA lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) y_pred_lda = lda.predict(X_test) # Implement QDA qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred_qda = qda.predict(X_test) # Evaluation print(\\"LDA Accuracy:\\", accuracy_score(y_test, y_pred_lda)) print(\\"LDA Confusion Matrix:n\\", confusion_matrix(y_test, y_pred_lda)) print(\\"QDA Accuracy:\\", accuracy_score(y_test, y_pred_qda)) print(\\"QDA Confusion Matrix:n\\", confusion_matrix(y_test, y_pred_qda)) # Dimensionality reduction with LDA lda_reduction = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda_reduction.fit_transform(X_train, y_train) plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] target_names = y_train.unique() for color, i, target_name in zip(colors, target_names, target_names): plt.scatter(X_r2[y_train == i, 0], X_r2[y_train == i, 1], alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of training dataset\') plt.show()"},{"question":"# Complex Number Operations with `cmath` Problem Statement: You are tasked with implementing a function that takes in a complex number in rectangular coordinates (real and imaginary parts) and performs the following operations sequentially: 1. Converts the complex number from rectangular to polar coordinates. 2. Computes the square root of the complex number. 3. Calculates the exponential of the complex number. 4. Converts the resulting complex number back to rectangular coordinates. The function must handle complex numbers appropriately and should consider edge cases where branch cuts might influence the behavior of the mathematical functions used. Function Signature: ```python import cmath def complex_operations(real: float, imag: float) -> (complex, float, complex): pass ``` Input: - `real` (float): The real part of the complex number. - `imag` (float): The imaginary part of the complex number. Output: - A tuple containing three elements: 1. A complex number representing the initial complex number in polar coordinates as returned by `cmath.polar()`. 2. A float representing the modulus of the initial complex number. 3. A complex number representing the result after applying the described operations and converting back to rectangular coordinates. Example: ```python # Example 1 real = -1.0 imag = 0.0 result = complex_operations(real, imag) # Should return ((1.0, 3.141592653589793), 1.0, -0.36787944117144233 + 0j) # Example 2 real = 3.0 imag = 4.0 result = complex_operations(real, imag) # Should return ((5.0, 0.9272952180016122), 5.0, (1.5337120583626565+0j)) ``` Constraints: - The real and imaginary parts should be finite numbers. - Consider the branch cuts and edge cases as described in the `cmath` module documentation. - Use the `cmath` module functions to perform the operations. Notes: - Ensure that the outputs conform with the behavior and characteristics described in the `cmath` documentation. - Implement error handling for invalid inputs as appropriate based on the behavior of the `cmath` module functions.","solution":"import cmath def complex_operations(real: float, imag: float): Performs a series of operations on a complex number: 1. Converts from rectangular to polar coordinates. 2. Computes the square root of the complex number. 3. Calculates the exponential of the complex number. 4. Converts the resulting complex number back to rectangular coordinates. Args: real (float): The real part of the complex number. imag (float): The imaginary part of the complex number. Returns: tuple: A tuple containing: - Complex number in polar coordinates (r, theta). - Modulus of the initial complex number. - Complex number result after operations in rectangular coordinates. c = complex(real, imag) # 1. Convert to polar coordinates polar_coords = cmath.polar(c) # 2. Compute the square root of the complex number sqrt_c = cmath.sqrt(c) # 3. Calculate the exponential of the complex number exp_c = cmath.exp(c) return (polar_coords, polar_coords[0], exp_c)"},{"question":"# Objective: You are tasked with analyzing a dataset on the survival of passengers on the Titanic. Your goal is to use Seaborn to create several insightful visualizations that will help understand the data better. # Dataset: You\'ll be working with the Titanic dataset (`titanic`). You can load it using: ```python import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") ``` # Instructions: 1. **Scatter Plot**: Plot a scatter plot to analyze the relationship between age and fare paid by the passengers. Distinguish the data points by the ‘sex’ of the passengers using different colors. Ensure the plot is well-labeled. 2. **Boxen Plot**: Create a boxen plot to show the distribution of the fare paid by passengers for each class (`class`), with different colors representing whether or not the passenger survived (`survived`). 3. **Facet Grid**: Use a facet grid to create multiple scatter plots segmented by passenger class (`class`). Each plot should display the relationship between age and fare, with points colored by survival status (`survived`). 4. **Heatmap**: Create a heatmap to visualize any correlations among the following columns: `survived`, `age`, `fare`, `pclass`. 5. **Customization**: Customize the heatmap by adding titles, labels, and a color palette of your choice. # Constraints: - The dataset should be loaded from Seaborn’s built-in datasets. - Limit your code to standard Python libraries and Seaborn. - The scatter plot, boxen plot, facet grid, and heatmap should be clearly labeled with appropriate titles and axis labels. - Ensure the plots are readable with distinct colors and legends where applicable. # Example Output: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Scatter Plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=titanic, x=\'age\', y=\'fare\', hue=\'sex\') plt.title(\'Age vs. Fare paid by Titanic Passengers\') plt.xlabel(\'Age\') plt.ylabel(\'Fare\') plt.show() # Boxen Plot plt.figure(figsize=(10, 6)) sns.boxenplot(data=titanic, x=\'class\', y=\'fare\', hue=\'survived\') plt.title(\'Fare distribution by Class and Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Fare\') plt.show() # Facet Grid g = sns.FacetGrid(titanic, col=\\"class\\", hue=\'survived\') g.map(sns.scatterplot, \\"age\\", \\"fare\\").add_legend() g.fig.suptitle(\'Age vs. Fare segmented by Class and Survival Status\', y=1.05) plt.show() # Heatmap corr_matrix = titanic[[\'survived\', \'age\', \'fare\', \'pclass\']].corr() plt.figure(figsize=(8, 6)) sns.heatmap(corr_matrix, annot=True, cmap=\'coolwarm\') plt.title(\'Correlation Heatmap of Titanic Dataset\') plt.show() ``` **Expected Output:** - A scatter plot of Age vs. Fare colored by Sex. - A boxen plot showing fare distribution by class and survival status. - A facet grid of scatter plots segmented by passenger class and survival status. - A correlation heatmap of selected features from the Titanic dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Scatter Plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=titanic, x=\'age\', y=\'fare\', hue=\'sex\') plt.title(\'Age vs. Fare paid by Titanic Passengers\') plt.xlabel(\'Age\') plt.ylabel(\'Fare\') plt.show() # Boxen Plot plt.figure(figsize=(10, 6)) sns.boxenplot(data=titanic, x=\'class\', y=\'fare\', hue=\'survived\') plt.title(\'Fare distribution by Class and Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Fare\') plt.show() # Facet Grid g = sns.FacetGrid(titanic, col=\\"class\\", hue=\'survived\') g.map(sns.scatterplot, \\"age\\", \\"fare\\").add_legend() g.fig.suptitle(\'Age vs. Fare segmented by Class and Survival Status\', y=1.05) plt.show() # Heatmap corr_matrix = titanic[[\'survived\', \'age\', \'fare\', \'pclass\']].corr() plt.figure(figsize=(8, 6)) sns.heatmap(corr_matrix, annot=True, cmap=\'coolwarm\') plt.title(\'Correlation Heatmap of Titanic Dataset\') plt.show()"},{"question":"**Command-Line Option Parsing with getopt Module** As a Python developer, you are tasked with creating a utility script that processes command-line options to control its behavior. Your task involves implementing this script using the `getopt` module. # Problem Statement Write a script that accepts the following command-line options: - `-i <inputfile>` or `--input <inputfile>`: Specifies the input file. - `-o <outputfile>` or `--output <outputfile>`: Specifies the output file. - `-v` or `--verbose`: Enables verbose mode. - `-h` or `--help`: Displays help message and exits. The script should parse and validate these command-line options, handle possible errors, and print a summary of the options provided. # Function Signature ```python def parse_options(argv: list) -> dict: Parses command-line options and returns a dictionary of the parsed options. Args: argv (list): List of command-line arguments excluding the script name. Returns: dict: Dictionary with keys \'input\', \'output\', \'verbose\', \'help\' and their corresponding values. ``` # Input - `argv` is a list of strings representing the command-line arguments (excluding the script name). # Output - Returns a dictionary containing parsed options: - `input`: Path to the input file (string) or `None`. - `output`: Path to the output file (string) or `None`. - `verbose`: Boolean indicating whether verbose mode is enabled. - `help`: Boolean indicating whether help was requested. # Constraints - If both `-i` and `--input` options are provided, the latter should override the former. - The same applies to `-o` and `--output`. - Use the `getopt.getopt` function for parsing options. - If an unrecognized option or a missing argument is encountered, the function should raise `getopt.GetoptError`. # Example ```python # Example usage # Command: --input data.txt --output result.txt -v options = parse_options([\'--input\', \'data.txt\', \'--output\', \'result.txt\', \'-v\']) print(options) # Output: {\'input\': \'data.txt\', \'output\': \'result.txt\', \'verbose\': True, \'help\': False} # Command: -i data.txt -v options = parse_options([\'-i\', \'data.txt\', \'-v\']) print(options) # Output: {\'input\': \'data.txt\', \'output\': None, \'verbose\': True, \'help\': False} # Command: -h options = parse_options([\'-h\']) print(options) # Output: {\'input\': None, \'output\': None, \'verbose\': False, \'help\': True} ``` # Implementation Notes - Ensure to handle errors using `getopt.GetoptError` and print an appropriate error message. - Implement reasonable defaults for cases where some options are not provided. # Additional Information Feel free to use any necessary imports like `sys` and `getopt`. **Good luck!**","solution":"import getopt def parse_options(argv: list) -> dict: Parses command-line options and returns a dictionary of the parsed options. Args: argv (list): List of command-line arguments excluding the script name. Returns: dict: Dictionary with keys \'input\', \'output\', \'verbose\', \'help\' and their corresponding values. opts, _ = getopt.getopt(argv, \\"hi:o:v\\", [\\"help\\", \\"input=\\", \\"output=\\", \\"verbose\\"]) options = { \'input\': None, \'output\': None, \'verbose\': False, \'help\': False } for opt, arg in opts: if opt in (\\"-i\\", \\"--input\\"): options[\'input\'] = arg elif opt in (\\"-o\\", \\"--output\\"): options[\'output\'] = arg elif opt in (\\"-v\\", \\"--verbose\\"): options[\'verbose\'] = True elif opt in (\\"-h\\", \\"--help\\"): options[\'help\'] = True return options"},{"question":"**Question: Dimensionality Reduction and Classification using PCA** You are provided with a dataset containing features of different scales. Your task is to create a machine learning pipeline that performs the following steps: 1. Standardizes the features to have zero mean and unit variance. 2. Reduces the dimensionality of the data using Principal Component Analysis (PCA), keeping 95% of the variance. 3. Trains a Logistic Regression model on the reduced dataset. 4. Evaluates the performance of the model using cross-validation. # Input: - A CSV file `data.csv` containing the dataset with `n` samples and `m` features. The last column in the CSV file is the target variable. # Output: - A float representing the mean accuracy of the Logistic Regression model from cross-validation. # Constraints: - You must use scikit-learn\'s PCA for dimensionality reduction. - The Logistic Regression should use default parameters. - Perform standard 5-fold cross-validation. # Example: ```python import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score def load_data(file_path): return pd.read_csv(file_path) def dimensionality_reduction_pca(X): scaler = StandardScaler() X_scaled = scaler.fit_transform(X) pca = PCA(n_components=0.95) X_reduced = pca.fit_transform(X_scaled) return X_reduced def train_and_evaluate(X, y): model = LogisticRegression() scores = cross_val_score(model, X, y, cv=5) return scores.mean() def main(file_path): data = load_data(file_path) X = data.iloc[:, :-1] y = data.iloc[:, -1] X_reduced = dimensionality_reduction_pca(X) mean_accuracy = train_and_evaluate(X_reduced, y) return mean_accuracy # Example usage file_path = \'data.csv\' print(main(file_path)) ``` # Note: - In the example, the function `load_data` loads the dataset from the provided file path. - The function `dimensionality_reduction_pca` standardizes the features and applies PCA to reduce dimensionality while retaining 95% of the variance. - The function `train_and_evaluate` trains a Logistic Regression model using the reduced dataset and evaluates it using 5-fold cross-validation, returning the mean accuracy. - The `main` function ties all the steps together, returning the mean accuracy of the model. Ensure that your implementation follows this structure and correctly processes the provided dataset.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score def load_data(file_path): Load the CSV file into a pandas DataFrame. return pd.read_csv(file_path) def dimensionality_reduction_pca(X): Standardize the features and apply PCA to reduce dimensionality while retaining 95% of the variance. scaler = StandardScaler() X_scaled = scaler.fit_transform(X) pca = PCA(n_components=0.95) X_reduced = pca.fit_transform(X_scaled) return X_reduced def train_and_evaluate(X, y): Train a Logistic Regression model using the reduced dataset and evaluate it using 5-fold cross-validation. Return the mean accuracy. model = LogisticRegression() scores = cross_val_score(model, X, y, cv=5) return scores.mean() def main(file_path): Main function to load data, perform PCA, train and evaluate the model, and return the mean accuracy. data = load_data(file_path) X = data.iloc[:, :-1] y = data.iloc[:, -1] X_reduced = dimensionality_reduction_pca(X) mean_accuracy = train_and_evaluate(X_reduced, y) return mean_accuracy"},{"question":"# Advanced Dictionary Manipulation with Python C API Concepts Objective You are tasked with creating a Python class `AdvancedDict` that enhances the functionality of a standard dictionary with additional features inspired by C API functions described in the provided documentation. Requirements 1. **Initialization**: The class should be initialized with an optional dictionary. If no dictionary is provided, it should start empty. 2. **Methods to Implement**: - `check_key_existence(self, key)`: - **Input**: `key` (any hashable type). - **Output**: Boolean indicating whether the key exists in the dictionary. - `add_key_value_pair(self, key, value)`: - **Input**: `key` (any hashable type), `value` (any type). - **Output**: None. - **Constraint**: Raise a `TypeError` if the key is unhashable. - `delete_key(self, key)`: - **Input**: `key` (any hashable type). - **Output**: None. - **Constraint**: If key does not exist, do nothing. Raise `TypeError` if the key is unhashable. - `clear_dict(self)`: - **Output**: None. Empties the dictionary. - `get_keys(self)`: - **Output**: List of all keys in the dictionary. - `get_values(self)`: - **Output**: List of all values in the dictionary. - `get_items(self)`: - **Output**: List of tuples where each tuple is a key-value pair. - `merge_with(self, other_dict, override=True)`: - **Input**: `other_dict` (a dictionary), `override` (boolean). - **Output**: None. - **Constraint**: Merge with `other_dict`. If `override` is False, existing keys should not be overwritten. 3. **Performance Requirements**: - Methods should aim to have an average time complexity of O(1) where applicable (e.g., adding, deleting, checking existence). # Example Usage ```python # Initialize with a dictionary my_dict = AdvancedDict({\'a\': 1, \'b\': 2}) # Check key existence print(my_dict.check_key_existence(\'a\')) # True print(my_dict.check_key_existence(\'c\')) # False # Add key-value pairs my_dict.add_key_value_pair(\'c\', 3) # Get keys, values, and items print(my_dict.get_keys()) # [\'a\', \'b\', \'c\'] print(my_dict.get_values()) # [1, 2, 3] print(my_dict.get_items()) # [(\'a\', 1), (\'b\', 2), (\'c\', 3)] # Delete a key my_dict.delete_key(\'b\') # Clear dictionary my_dict.clear_dict() # Merge dictionaries dict1 = AdvancedDict({\'x\': 10, \'y\': 20}) dict2 = {\'y\': 200, \'z\': 30} dict1.merge_with(dict2, override=False) print(dict1.get_items()) # [(\'x\', 10), (\'y\', 20), (\'z\', 30)] ``` Constraints - You should not use built-in dictionary methods directly for functionalities you are supposed to implement (e.g., do not use `dict.update` for `merge_with`). - Raise appropriate Python exceptions as specified in the requirements.","solution":"class AdvancedDict: def __init__(self, initial_dict=None): self._dict = initial_dict if initial_dict is not None else {} def check_key_existence(self, key): return key in self._dict def add_key_value_pair(self, key, value): try: self._dict[key] = value except TypeError: raise TypeError(\\"Key must be hashable\\") def delete_key(self, key): try: self._dict.pop(key, None) except TypeError: raise TypeError(\\"Key must be hashable\\") def clear_dict(self): self._dict.clear() def get_keys(self): return list(self._dict.keys()) def get_values(self): return list(self._dict.values()) def get_items(self): return list(self._dict.items()) def merge_with(self, other_dict, override=True): for key, value in other_dict.items(): if key not in self._dict or override: self._dict[key] = value"},{"question":"# Objective Implement a function that preprocesses an image using FFT transformations and applies a simple filter in the frequency domain. The function should perform the following tasks: 1. Compute the 2-dimensional Fast Fourier Transform (FFT) of the input image. 2. Apply a low-pass filter in the frequency domain by zeroing out all frequency components outside a specified radius. 3. Transform the filtered frequency image back to the spatial domain using the inverse FFT. 4. Return the real part of the processed image. # Function Signature ```python import torch def apply_low_pass_fft(image: torch.Tensor, radius: float) -> torch.Tensor: Applies a low-pass filter to the input image in the frequency domain. Parameters: image (torch.Tensor): A 2D tensor representing a grayscale image. radius (float): Radius of the low-pass filter in the frequency domain. Returns: torch.Tensor: The processed image after the inverse FFT. pass ``` # Input - `image`: A 2-dimensional tensor of shape `(H, W)` representing a grayscale image. - `radius`: A float that represents the cutoff radius for the low-pass filter in the frequency domain. # Output - A 2-dimensional tensor of the same shape as the input image, representing the processed image after applying the low-pass filter. # Constraints and Assumptions - The input image tensor will have dimensions `(H, W)` where `H` and `W` are powers of 2 to simplify the use of FFT. - You may assume `radius` is a positive float and does not exceed half of the minimum dimension of the image. - The computations should be efficient, leveraging GPU acceleration if available. # Example ```python import torch # Sample 2D image tensor image = torch.rand((256, 256)) # Apply low-pass filter with radius 30 processed_image = apply_low_pass_fft(image, 30.0) # \'processed_image\' contains the filtered image in the spatial domain ``` # Instructions - Use `torch.fft.fft2` and `torch.fft.ifft2` to compute the FFT and its inverse. - Apply the low-pass filter in the frequency domain by zeroing out the frequency components outside the specified radius. - Use `torch.fft.fftfreq` to generate frequency bins. - Ensure that the returned image is real by extracting the real part of the complex result from `ifft2`.","solution":"import torch def apply_low_pass_fft(image: torch.Tensor, radius: float) -> torch.Tensor: Applies a low-pass filter to the input image in the frequency domain. Parameters: image (torch.Tensor): A 2D tensor representing a grayscale image. radius (float): Radius of the low-pass filter in the frequency domain. Returns: torch.Tensor: The processed image after the inverse FFT. # Compute the 2D FFT of the input image fft_image = torch.fft.fft2(image) # Shift the zero frequency component to the center fft_image = torch.fft.fftshift(fft_image) # Prepare the mask for the low-pass filter H, W = image.shape y, x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing=\'ij\') center_y, center_x = H // 2, W // 2 mask = ((x - center_x)**2 + (y - center_y)**2)**0.5 <= radius # Apply the mask to zero out frequencies outside the radius fft_image_filtered = fft_image * mask # Shift the zero frequency component back to the original position fft_image_filtered = torch.fft.ifftshift(fft_image_filtered) # Transform the filtered image back to the spatial domain using inverse FFT processed_image = torch.fft.ifft2(fft_image_filtered) # Return the real part of the processed image return torch.real(processed_image)"},{"question":"Problem Statement You are building a utility function to analyze HTTP responses. Given a list of HTTP status codes, you need to output a summary of counts for each category of response status. HTTP status codes are divided into different categories based on the leading digit: - **1xx**: Informational responses - **2xx**: Successful responses - **3xx**: Redirection messages - **4xx**: Client error responses - **5xx**: Server error responses Your task is to implement the function `analyze_http_statuses(status_codes: List[int]) -> Dict[str, int]` which takes a list of HTTP status codes and returns a dictionary summarizing the count of each category of response status. Input - `status_codes`: A list of integers, where each integer represents an HTTP status code. Output - A dictionary with the following keys: - `\'1xx\'`: Count of informational responses. - `\'2xx\'`: Count of successful responses. - `\'3xx\'`: Count of redirection messages. - `\'4xx\'`: Count of client error responses. - `\'5xx\'`: Count of server error responses. Constraints - Each status code in the input list will be a valid HTTP status code as defined in the `http.HTTPStatus` enum. Example ```python from typing import List, Dict def analyze_http_statuses(status_codes: List[int]) -> Dict[str, int]: # Your implementation here # Example usage: status_codes = [200, 201, 404, 500, 301, 204, 400, 503, 100] print(analyze_http_statuses(status_codes)) # Output: {\'1xx\': 1, \'2xx\': 3, \'3xx\': 1, \'4xx\': 2, \'5xx\': 2} ``` Notes - You should leverage the `http.HTTPStatus` enum provided by the `http` package to validate and categorize the status codes. - Ensure your solution handles both common and uncommon status codes gracefully.","solution":"from typing import List, Dict def analyze_http_statuses(status_codes: List[int]) -> Dict[str, int]: summary = {\'1xx\': 0, \'2xx\': 0, \'3xx\': 0, \'4xx\': 0, \'5xx\': 0} for code in status_codes: if 100 <= code < 200: summary[\'1xx\'] += 1 elif 200 <= code < 300: summary[\'2xx\'] += 1 elif 300 <= code < 400: summary[\'3xx\'] += 1 elif 400 <= code < 500: summary[\'4xx\'] += 1 elif 500 <= code < 600: summary[\'5xx\'] += 1 return summary"},{"question":"You are tasked with simulating a restaurant order processing system using `asyncio` queues. The system should handle orders with different priorities. Task: 1. Implement an asynchronous function `process_order` that simulates processing an order. 2. Implement an asynchronous function `restaurant_system` that handles orders using an `asyncio.PriorityQueue`. Details: - Orders are represented as tuples in the form `(priority, order_id)`, where `priority` is an integer (lower number means higher priority) and `order_id` is a unique identifier for the order. - The `process_order` function should take an order, simulate processing it with `asyncio.sleep` for a time derived from the `priority` (e.g., `priority * 0.1` seconds), and print a message indicating the order was processed. - The `restaurant_system` function should: - Accept a list of orders and process them using three concurrent worker tasks. - Ensure that orders are processed in priority order. - Use `PriorityQueue` to manage the orders. - Each worker should retrieve, process an order, and call `task_done` when the order is complete. Function Signatures: ```python import asyncio from typing import List, Tuple async def process_order(order: Tuple[int, int]) -> None: pass async def restaurant_system(orders: List[Tuple[int, int]]) -> None: pass ``` Example Usage: ```python orders = [(1, \'order1\'), (3, \'order3\'), (2, \'order2\'), (1, \'order4\')] asyncio.run(restaurant_system(orders)) ``` Expected Output: ``` Order order1 with priority 1 is being processed. Order order4 with priority 1 is being processed. Order order2 with priority 2 is being processed. Order order3 with priority 3 is being processed. ``` Constraints: 1. Ensure the `restaurant_system` processes orders concurrently but respects the priority. 2. Handle all orders until the queue is empty and ensure all workers are properly cancelled after processing. Notes: - Use `asyncio.PriorityQueue` for managing the orders. - Feel free to add helper functions if needed. - Ensure your solution handles concurrent tasks correctly and avoids race conditions.","solution":"import asyncio from typing import List, Tuple async def process_order(order: Tuple[int, int]) -> None: priority, order_id = order await asyncio.sleep(priority * 0.1) print(f\\"Order {order_id} with priority {priority} is being processed.\\") async def worker(name: str, queue: asyncio.PriorityQueue) -> None: while True: order = await queue.get() await process_order(order) queue.task_done() async def restaurant_system(orders: List[Tuple[int, int]]) -> None: queue = asyncio.PriorityQueue() for order in orders: queue.put_nowait(order) workers = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(3)] await queue.join() # Wait until all orders are processed for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True)"},{"question":"# Question: Optimizing Decision Threshold for Binary Classification You are given a dataset for binary classification and need to write a function that: 1. Trains a classification model on the dataset. 2. Tunes the decision threshold to optimize a specific metric. 3. Evaluates its performance using cross-validation. **Function Signature:** ```python def optimized_classifier(X_train, y_train, metric=\'f1\', cv_splits=5): Trains and optimizes a classification model using a tuned decision threshold. Parameters: - X_train (pd.DataFrame or np.ndarray): Input feature matrix for training. - y_train (pd.Series or np.ndarray): Class labels for training. - metric (str): The scoring metric to optimize (\'f1\', \'precision\', \'recall\', etc.) - cv_splits (int): Number of splits for cross-validation (default is 5). Returns: - best_score (float): Best cross-validated score for the chosen metric. - best_threshold (float): Best decision threshold according to cross-validation. - fitted_model (TunedThresholdClassifierCV or FixedThresholdClassifier): The optimized classifier. ``` **Requirements and Constraints:** 1. **Input Format**: - `X_train`: A Pandas DataFrame or Numpy array of shape `(n_samples, n_features)`. - `y_train`: A Pandas Series or Numpy array of shape `(n_samples,)`. - `metric`: A string indicating the scoring metric to optimize (default is \'f1\'). - `cv_splits`: An integer specifying the number of cross-validation folds (default is 5). 2. **Output Format**: - `best_score`: A float representing the best score obtained through cross-validation. - `best_threshold`: A float indicating the best decision threshold identified. - `fitted_model`: A `TunedThresholdClassifierCV` (or `FixedThresholdClassifier`) instance fitted to the data. 3. **Constraints**: - Implement the function using scikit-learn’s `TunedThresholdClassifierCV` for threshold tuning. - The `scoring` parameter in `TunedThresholdClassifierCV` should use scikit-learn metrics only. - If using `FixedThresholdClassifier`, make sure the model does not refit during instantiation. **Additional Information**: - Ensure proper handling of metrics using `make_scorer` to customize the `pos_label` if needed. - For cross-validation, use StratifiedKFold from scikit-learn to maintain class distribution across folds. **Example Usage:** ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression # Generate synthetic data for demonstration X, y = make_classification(n_samples=1000, n_features=20, weights=[0.7, 0.3], random_state=42) # Train and optimize the classifier best_score, best_threshold, model = optimized_classifier(X, y, metric=\'recall\', cv_splits=5) print(f\\"Best Score: {best_score}\\") print(f\\"Best Threshold: {best_threshold}\\") ```","solution":"import numpy as np import pandas as pd from sklearn.metrics import make_scorer, f1_score from sklearn.model_selection import StratifiedKFold, cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.base import BaseEstimator, ClassifierMixin class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, base_model, scoring=\'f1\', cv_splits=5): self.base_model = base_model self.scoring = scoring self.cv_splits = cv_splits self.best_threshold = 0.5 def fit(self, X, y): best_score = -np.inf # We want to maximize the score thresholds = np.linspace(0.1, 0.9, 9) for threshold in thresholds: cv = StratifiedKFold(n_splits=self.cv_splits) scores = [] for train_idx, val_idx in cv.split(X, y): self.base_model.fit(X[train_idx], y[train_idx]) y_prob = self.base_model.predict_proba(X[val_idx])[:, 1] y_pred = (y_prob >= threshold).astype(int) score = f1_score(y[val_idx], y_pred) # Adjust this to use self.scoring scores.append(score) mean_score = np.mean(scores) if mean_score > best_score: best_score = mean_score self.best_threshold = threshold # Refit on entire dataset with best threshold self.base_model.fit(X, y) return self def predict(self, X): y_prob = self.base_model.predict_proba(X)[:, 1] return (y_prob >= self.best_threshold).astype(int) def predict_proba(self, X): return self.base_model.predict_proba(X) def optimized_classifier(X_train, y_train, metric=\'f1\', cv_splits=5): base_model = LogisticRegression(solver=\'liblinear\') model = TunedThresholdClassifierCV(base_model, scoring=metric, cv_splits=cv_splits) model.fit(X_train, y_train) best_score = max(cross_val_score(model, X_train, y_train, cv=StratifiedKFold(n_splits=cv_splits), scoring=make_scorer(f1_score))) return best_score, model.best_threshold, model"},{"question":"**Coding Assessment Question: Python Built-in Types and Operations** **Objective:** Assume you have a list of various data elements representing a mixture of types: integers, floats, strings, and nested lists. Your task is to perform a series of operations, ensuring you test your understanding of various built-in types and their methods as per Python 3.x specifications. **Problem Statement:** 1. **Input:** - A list of mixed elements `mixed_list` which might include integers, floats, strings, nested lists, empty strings, empty lists, and `None` values. 2. **Output:** - A dictionary containing: * **\\"count_per_type\\"**: A dictionary with counts of each type of element in `mixed_list`. * **\\"truthy_elements\\"**: A list containing all the truthy elements from `mixed_list`. * **\\"flattened_list\\"**: A list where all nested lists within `mixed_list` are flattened. * **\\"frequency_numeric\\"**: The frequency of each numeric element (integers and floats) in `mixed_list` as a dictionary. 3. **Example:** - Input: `mixed_list = [1, 2.5, \\"hello\\", [3, 4], \\"\\", None, [5, [6, 7]], 1]` - Output: ```python { \\"count_per_type\\": {\\"int\\": 2, \\"float\\": 1, \\"str\\": 2, \\"list\\": 2, \\"NoneType\\": 1}, \\"truthy_elements\\": [1, 2.5, \\"hello\\", [3, 4], [5, [6, 7]], 1], \\"flattened_list\\": [1, 2.5, \\"hello\\", 3, 4, \\"\\", None, 5, 6, 7, 1], \\"frequency_numeric\\": {1: 2, 2.5: 1} } ``` 4. **Constraints:** - Each element in the `mixed_list` will be of types: `int`, `float`, `str`, `list`, or `NoneType`. - Nested lists will only contain types: `int` or `float`. **Function Signature:** ```python def analyze_mixed_list(mixed_list: list) -> dict: # Your implementation here ``` **Requirements:** 1. Your function should accurately count the occurrences of each type. 2. Properly filter out and return all elements that evaluate to `True` when tested for their truth value. 3. Flatten all nested lists, preserve order, and ensure no nested lists remain in the flattened result. 4. Calculate and return the frequency of each numeric element (aggregate counts for integers and floats) in `mixed_list`. **Instructions:** - Ensure to handle nested lists properly, regardless of their depth. - Use built-in functions and methods where applicable. - Do not use any external libraries. Utilize Python\'s native capabilities for manipulation. - Test your function thoroughly with various cases to ensure accuracy. **Note:** You are encouraged to write additional helper functions if necessary to break down the problem into manageable parts.","solution":"def analyze_mixed_list(mixed_list: list) -> dict: from collections import defaultdict def flatten(lst): Helper function to flatten a nested list. flattened = [] for item in lst: if isinstance(item, list): flattened.extend(flatten(item)) else: flattened.append(item) return flattened count_types = defaultdict(int) truthy_elements = [] numeric_freq = defaultdict(int) # Flattening the mixed list flattened_list = flatten(mixed_list) for element in mixed_list: count_types[type(element).__name__] += 1 if element: truthy_elements.append(element) for item in flattened_list: if isinstance(item, (int, float)): numeric_freq[item] += 1 return { \\"count_per_type\\": dict(count_types), \\"truthy_elements\\": truthy_elements, \\"flattened_list\\": flattened_list, \\"frequency_numeric\\": dict(numeric_freq) }"},{"question":"# Custom Heap-Allocated Type Implementation Using the provided documentation on creating and manipulating Python types, implement a custom heap-allocated type in Python using the C API. The type should support basic operations such as instantiation, attribute setting/getting, and a custom method. Requirements: 1. **Type Definition and Creation**: - Create a new type named `CustomType` with attributes `name` (string) and `value` (integer). - Ensure the type inherits from `object`. 2. **Attribute Management**: - Implement getters and setters for `name` and `value`. - Ensure that any manual modifications to these attributes are handled properly. 3. **Custom Method**: - Add a method named `multiply_value` which multiplies the `value` attribute by a given integer parameter and returns the result. Constraints: - You must use the functions and structures described in the provided documentation. - The solution should handle memory allocation and garbage collection appropriately. - Proper error handling and input validation should be included. Expected Implementation Outline: - Define the `PyTypeObject` for `CustomType`. - Implement functions for attribute management and the custom method. - Use `PyType_FromSpecWithBases` or `PyType_FromModuleAndSpec` to create and initialize the type. - Ensure the type is registered and available for use in Python. Example Usage: ```python # Assuming the extension module is compiled and imported as `custom_module` from custom_module import CustomType # Create an instance of CustomType obj = CustomType(name=\\"example\\", value=10) # Access and modify attributes print(obj.name) # Output: example print(obj.value) # Output: 10 obj.name = \\"test\\" obj.value = 20 print(obj.name) # Output: test print(obj.value) # Output: 20 # Use the custom method result = obj.multiply_value(3) print(result) # Output: 60 ``` Note: Provide the necessary C code snippets and details on how to compile and test the module, emphasizing the use of functions mentioned in the documentation.","solution":"class CustomType: CustomType that supports \'name\' (string) and \'value\' (integer) attributes, with a custom method \'multiply_value\'. def __init__(self, name, value): self.name = name self.value = value @property def name(self): return self._name @name.setter def name(self, value): if not isinstance(value, str): raise TypeError(\\"name must be a string\\") self._name = value @property def value(self): return self._value @value.setter def value(self, value): if not isinstance(value, int): raise TypeError(\\"value must be an integer\\") self._value = value def multiply_value(self, multiplier): if not isinstance(multiplier, int): raise TypeError(\\"multiplier must be an integer\\") return self.value * multiplier"},{"question":"**Covariance Estimation with Different Methods** You are given a dataset and your task is to implement functions to estimate its covariance matrix using various methods provided by the `sklearn.covariance` module. You will need to assess the dataset using empirical covariance, shrunk covariance, Ledoit-Wolf shrinkage, Oracle Approximating Shrinkage, and robust covariance estimation methods. # Dataset Assume the dataset is a two-dimensional NumPy array (n_samples x n_features) provided as input to your functions. # Tasks 1. **Empirical Covariance** 2. **Shrunk Covariance**: Use a shrinkage coefficient of 0.1. 3. **Ledoit-Wolf Shrinkage** 4. **Oracle Approximating Shrinkage (OAS)** 5. **Robust Covariance**: Using the Minimum Covariance Determinant (MCD) method. # Function Definitions ```python def empirical_covariance(data: np.ndarray) -> np.ndarray: Compute the empirical covariance matrix of the given dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). Returns: np.ndarray: The empirical covariance matrix of shape (n_features, n_features). pass def shrunk_covariance(data: np.ndarray, shrinkage: float = 0.1) -> np.ndarray: Compute the shrunk covariance matrix of the given dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). shrinkage (float): Shrinkage coefficient. Returns: np.ndarray: The shrunk covariance matrix of shape (n_features, n_features). pass def ledoit_wolf_covariance(data: np.ndarray) -> np.ndarray: Compute the Ledoit-Wolf shrunk covariance matrix of the given dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). Returns: np.ndarray: The Ledoit-Wolf shrunk covariance matrix of shape (n_features, n_features). pass def oas_covariance(data: np.ndarray) -> np.ndarray: Compute the Oracle Approximating Shrinkage (OAS) covariance matrix of the given dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). Returns: np.ndarray: The OAS covariance matrix of shape (n_features, n_features). pass def robust_covariance(data: np.ndarray) -> np.ndarray: Compute the robust covariance matrix of the given dataset using the Minimum Covariance Determinant (MCD) method. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). Returns: np.ndarray: The robust covariance matrix of shape (n_features, n_features). pass ``` # Constraints - You must handle the data centering appropriately. - You should use the `sklearn.covariance` module for all implementations. - Assume the data is always provided as a 2D numpy array with dimensions n_samples x n_features (n_samples ≥ n_features). # Evaluation Your implementation should be correct and efficient. You should also ensure to properly document your code. The accuracy of the covariance matrix will be compared against the expected values derived from sklearn’s built-in methods.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet def empirical_covariance(data: np.ndarray) -> np.ndarray: Compute the empirical covariance matrix of the given dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). Returns: np.ndarray: The empirical covariance matrix of shape (n_features, n_features). emp_cov = EmpiricalCovariance().fit(data) return emp_cov.covariance_ def shrunk_covariance(data: np.ndarray, shrinkage: float = 0.1) -> np.ndarray: Compute the shrunk covariance matrix of the given dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). shrinkage (float): Shrinkage coefficient. Returns: np.ndarray: The shrunk covariance matrix of shape (n_features, n_features). shrunk_cov = ShrunkCovariance(shrinkage=shrinkage).fit(data) return shrunk_cov.covariance_ def ledoit_wolf_covariance(data: np.ndarray) -> np.ndarray: Compute the Ledoit-Wolf shrunk covariance matrix of the given dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). Returns: np.ndarray: The Ledoit-Wolf shrunk covariance matrix of shape (n_features, n_features). lw_cov = LedoitWolf().fit(data) return lw_cov.covariance_ def oas_covariance(data: np.ndarray) -> np.ndarray: Compute the Oracle Approximating Shrinkage (OAS) covariance matrix of the given dataset. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). Returns: np.ndarray: The OAS covariance matrix of shape (n_features, n_features). oas_cov = OAS().fit(data) return oas_cov.covariance_ def robust_covariance(data: np.ndarray) -> np.ndarray: Compute the robust covariance matrix of the given dataset using the Minimum Covariance Determinant (MCD) method. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). Returns: np.ndarray: The robust covariance matrix of shape (n_features, n_features). mcd_cov = MinCovDet().fit(data) return mcd_cov.covariance_"},{"question":"# Task You are given two datasets: `fmri` and `seaice`, which you need to load using Seaborn. Your task is to perform specific data manipulations and create a composite plot that demonstrates the use of advanced features in Seaborn. # Datasets Description - **fmri**: Contains data on brain activity. - **seaice**: Tracks the extent of sea ice over time. # Steps 1. **Load and Prepare Data:** - Load the `fmri` dataset and filter it to contain only records where `region` is `\'parietal\'`. - Load the `seaice` dataset. Transform it to include columns for the day of the year (`Day`) and year (`Year`). Filter out the data before the year 1980. Convert the `Year` column to string format, pivot the table such that `Day` is the index, `Year` is the column, and `Extent` is the value. Filter this table further to include only the years 1980 and 2019, then drop any missing values. 2. **Create Visualizations:** - Using the `seaice` dataset, create a `seaborn.objects.Plot` that visualizes the interval between the sea ice extents for the years 1980 and 2019 using a Band. Ensure the intervals are visually distinguishable by adding appropriate customization (e.g., alpha blending, edge width). - Using the `fmri` dataset, create a `seaborn.objects.Plot` that shows the average brain signal over time, with error bands representing the intervals. Color code the plot based on the `event` column. # Expected Input and Output Formats Input: No input data is required as the datasets will be loaded from Seaborn\'s built-in datasets. Output: Two plots: 1. A plot showing the sea ice extents for the years 1980 and 2019 with bands representing the intervals. 2. A plot showing the average brain signal over time from the `fmri` dataset, with error bands representing the intervals, colored by `event`. # Constraints - Use Seaborn\'s `objects` interface for plotting. - Ensure that the plots are clear, well-labeled, and informative. ```python # Your implementation here import seaborn.objects as so from seaborn import load_dataset # Load and prepare the fmri dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and prepare the seaice dataset seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Create the sea ice plot plot_seaice = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") plot_seaice.add(so.Band(alpha=0.5, edgewidth=2)) # Create the fmri plot plot_fmri = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) # Ensure both plots are displayed plot_seaice.show() plot_fmri.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def prepare_fmri_data(): Loads the fmri dataset and filters it to include only records where the region is \'parietal\'. fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") return fmri def prepare_seaice_data(): Loads the seaice dataset, transforms and filters the data to include columns for the day of the year and year, filters out records before the year 1980, and pivots the table to include only the years 1980 and 2019. seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.dayofyear, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) return seaice def create_seaice_plot(seaice): Creates a seaborn.objects.Plot visualizing the sea ice extents for the years 1980 and 2019 with bands representing the intervals. plot_seaice = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") plot_seaice.add(so.Band(alpha=0.5, edgewidth=2)) return plot_seaice def create_fmri_plot(fmri): Creates a seaborn.objects.Plot showing the average brain signal over time from the fmri dataset, with error bands representing the intervals, colored by event. plot_fmri = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) return plot_fmri"},{"question":"# Question You are provided with a PyTorch tensor of arbitrary dimensions. Write a function `reshape_tensor` which takes this tensor and an integer `new_size`. The function should resize the provided tensor such that all elements are preserved, but the tensor\'s shape has exactly `new_size` number of elements in the last dimension while keeping the other dimensions as consistent as possible. Specifically, the reshaped tensor should have one or two additional dimensions as required to accommodate the new size, while not changing the total number of elements. If the tensor cannot be reshaped into the exact new size without losing data, raise a `ValueError`. Inputs: - `tensor`: A `torch.Tensor` of arbitrary shape. - `new_size`: An integer representing the desired size of the last dimension of the tensor after reshaping. Outputs: - `torch.Tensor`: The reshaped tensor. Constraints: - The function should raise a `ValueError` if the tensor cannot be reshaped into the required format without losing data. Example: ```python import torch def reshape_tensor(tensor, new_size): # Your implementation here pass # Example usage: tensor = torch.ones(4, 5) new_size = 2 reshaped_tensor = reshape_tensor(tensor, new_size) print(reshaped_tensor.shape) # Expected Output: torch.Size([2, 10]) ``` Your task is to implement the `reshape_tensor` function.","solution":"import torch def reshape_tensor(tensor, new_size): Reshape the given tensor such that the last dimension has exactly new_size elements, while preserving all the tensor elements. Args: tensor (torch.Tensor): The input tensor. new_size (int): The desired size of the last dimension of the tensor after reshaping. Returns: torch.Tensor: The reshaped tensor. Raises: ValueError: If the tensor cannot be reshaped to the required format without losing data. total_elements = tensor.numel() if total_elements % new_size != 0: raise ValueError(\\"The tensor cannot be reshaped to the desired size without losing data.\\") reshaped_shape = (total_elements // new_size, new_size) return tensor.view(reshaped_shape)"},{"question":"**Objective:** Optimize the performance of a simple machine learning task using Python, NumPy/SciPy, and Cython. **Task:** 1. Implement a K-means clustering algorithm in pure Python using NumPy and SciPy. 2. Profile the algorithm to identify performance bottlenecks. 3. Optimize the identified bottlenecks by rewriting those parts of the code in Cython. 4. Compare the performance before and after optimization. **Instructions:** 1. **K-means Algorithm in Python:** - Implement the K-means clustering algorithm. - The function should take as input: - `data`: A 2D NumPy array of shape `(n_samples, n_features)` representing the data points. - `k`: An integer representing the number of clusters. - `max_iter`: An optional integer representing the maximum number of iterations (default=300). - `tol`: An optional float representing the tolerance to declare convergence (default=1e-4). - The function should return: - `centroids`: A 2D NumPy array of shape `(k, n_features)` representing the final cluster centers. - `labels`: A 1D NumPy array of length `n_samples` representing the cluster assignments of each data point. 2. **Profiling the Algorithm:** - Use the IPython `%timeit` and `%prun` magics to measure the overall execution time and identify the main bottlenecks in your implementation. 3. **Optimizing with Cython:** - Rewrite the identified bottlenecks using Cython. - Ensure your Cython implementation is integrated and called within the main Python function. 4. **Performance Comparison:** - Measure and compare the execution time of the algorithm before and after optimization. **Constraints:** - Use only Python, NumPy, SciPy, and Cython for the implementation and optimization parts. - The K-means algorithm should be implemented from scratch; use of scikit-learn\'s KMeans is prohibited. **Expected Input and Output Format:** ```python def kmeans(data: np.ndarray, k: int, max_iter: int = 300, tol: float = 1e-4) -> Tuple[np.ndarray, np.ndarray]: # Your implementation here pass # Example usage: data = np.random.rand(100, 2) # 100 samples, 2 features k = 3 centroids, labels = kmeans(data, k) ``` **Example Input:** ```python data = np.array([[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0]]) k = 2 centroids, labels = kmeans(data, k, max_iter=100, tol=1e-4) ``` **Example Output:** ```python centroids: array([[1.16666667, 1.46666667], [7.33333333, 9.0 ]]) labels: array([0, 0, 1, 1, 0, 1]) ``` **Submission Requirements:** - Submit the Python implementation of the K-means algorithm. - Submit the profiling results identifying the bottlenecks. - Submit the optimized Cython code. - A brief report comparing the performance before and after optimization. **Grading Criteria:** - Correctness of the K-means clustering implementation. - Proper profiling and identification of bottlenecks. - Effectiveness of the Cython optimization. - Clarity and conciseness of the performance comparison report.","solution":"import numpy as np from scipy.spatial.distance import cdist def kmeans(data, k, max_iter=300, tol=1e-4): K-means clustering algorithm. Parameters: data (numpy.ndarray): A 2D array of shape (n_samples, n_features). k (int): Number of clusters. max_iter (int, optional): Maximum number of iterations. Default is 300. tol (float, optional): Tolerance to declare convergence. Default is 1e-4. Returns: centroids (numpy.ndarray): A 2D array of shape (k, n_features) representing the final cluster centers. labels (numpy.ndarray): A 1D array of length n_samples representing the cluster assignments. n_samples, n_features = data.shape # Step 1: Initialize centroids randomly np.random.seed(42) centroids = data[np.random.choice(n_samples, k, replace=False)] for _ in range(max_iter): # Step 2: Assign clusters distances = cdist(data, centroids, \'euclidean\') labels = np.argmin(distances, axis=1) # Step 3: Recompute centroids new_centroids = np.array([data[labels == j].mean(axis=0) for j in range(k)]) # Step 4: Check for convergence if np.all(np.linalg.norm(new_centroids - centroids, axis=1) < tol): break centroids = new_centroids return centroids, labels"},{"question":"# Question: Python Version Parser and Formatter Python defines its version using several components: major version, minor version, micro version, release level, and release serial. These components can be combined into a single integer value called `PY_VERSION_HEX`. You are required to implement two functions: `parse_version(hex_version)` and `format_version(major, minor, micro, release_level, release_serial)`. 1. **parse_version(hex_version)**: - **Input**: An integer `hex_version` representing the encoded version. - **Output**: A tuple `(major, minor, micro, release_level, release_serial)` representing the individual version components. 2. **format_version(major, minor, micro, release_level, release_serial)**: - **Input**: Five integers representing the major version, minor version, micro version, release level, and release serial. - **Output**: An integer `hex_version` representing the encoded version. Release levels are represented as follows: - Alpha: 0xA - Beta: 0xB - Release Candidate: 0xC - Final: 0xF **Examples**: ```python # Example 1 hex_version = 0x030401a2 assert parse_version(hex_version) == (3, 4, 1, 0xA, 2) assert format_version(3, 4, 1, 0xA, 2) == hex_version # Example 2 hex_version = 0x030a00f0 assert parse_version(hex_version) == (3, 10, 0, 0xF, 0) assert format_version(3, 10, 0, 0xF, 0) == hex_version ``` **Constraints**: - 0 <= major <= 255 - 0 <= minor <= 255 - 0 <= micro <= 255 - release_level must be one of {0xA, 0xB, 0xC, 0xF} - 0 <= release_serial <= 15 **Note**: The functions must handle invalid inputs gracefully, raising appropriate exceptions where necessary. Write your implementations of `parse_version` and `format_version` below: ```python def parse_version(hex_version): # Your code here def format_version(major, minor, micro, release_level, release_serial): # Your code here ```","solution":"def parse_version(hex_version): Parses the encoded version integer into its individual components. Args: hex_version (int): The encoded version integer. Returns: tuple: A tuple containing major, minor, micro, release_level, release_serial. major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF release_level = (hex_version >> 4) & 0xF release_serial = hex_version & 0xF return major, minor, micro, release_level, release_serial def format_version(major, minor, micro, release_level, release_serial): Combines the individual version components into a single encoded version integer. Args: major (int): The major version component. minor (int): The minor version component. micro (int): The micro version component. release_level (int): The release level component. release_serial (int): The release serial component. Returns: int: The encoded version integer. if not (0 <= major <= 255): raise ValueError(\\"Major version must be between 0 and 255.\\") if not (0 <= minor <= 255): raise ValueError(\\"Minor version must be between 0 and 255.\\") if not (0 <= micro <= 255): raise ValueError(\\"Micro version must be between 0 and 255.\\") if release_level not in {0xA, 0xB, 0xC, 0xF}: raise ValueError(\\"Release level must be one of {0xA, 0xB, 0xC, 0xF}.\\") if not (0 <= release_serial <= 15): raise ValueError(\\"Release serial must be between 0 and 15.\\") return (major << 24) | (minor << 16) | (micro << 8) | (release_level << 4) | release_serial"},{"question":"# Assessing Multiprocessing in Python **Objective:** Design a function that utilizes the `multiprocessing` package to calculate the factorial of multiple numbers concurrently. You need to demonstrate your understanding of creating and managing processes, handling shared resources, and collecting results from multiple processes. # Problem Statement: **Task:** Write a Python function `calculate_factorials` that: 1. Takes a list of non-negative integers as input. 2. Uses a pool of worker processes to compute the factorial of each integer concurrently. 3. Returns a dictionary where the keys are the input integers and the values are their respective factorials. # Constraints: - You should use the `Pool` class from the `multiprocessing` module. - Ensure that the main process waits for all worker processes to complete. - Handle any exceptions that may be raised during the computation of factorials. - The function should handle lists of integers of reasonable length (up to 1000 integers). # Input: - `nums`: A list of non-negative integers. # Output: - A dictionary mapping each integer in the input list to its factorial. # Example: ```python def calculate_factorials(nums): # Your implementation here. # Example usage: nums = [0, 1, 5, 7] result = calculate_factorials(nums) print(result) # Output: {0: 1, 1: 1, 5: 120, 7: 5040} ``` # Additional Notes: - The factorial of a non-negative integer `n` is the product of all positive integers less than or equal to `n`. - For your implementation, ensure to use proper exception handling to manage any issues that arise during the computation process. - Your solution should efficiently utilize the multiprocessor environment. # Hints: - Consider using the `map` or `apply_async` methods of the `Pool` class to distribute the factorial computations across multiple processes. - You may write a helper function to compute the factorial of a single number, which will be called by the worker processes. **Good Luck!**","solution":"from multiprocessing import Pool from math import factorial def factorial_worker(n): return n, factorial(n) def calculate_factorials(nums): with Pool() as pool: results = pool.map(factorial_worker, nums) return dict(results)"},{"question":"Context: In this assessment, you will demonstrate your understanding of Python exception handling by implementing a function that processes a list of mathematical operations provided by the user. This function should handle various types of exceptions, raise custom exceptions when necessary, and ensure proper clean-up actions using the `with` statement. Problem Statement: You need to write a function `process_operations(operations, log_file)` that takes two arguments: 1. `operations` (list): A list of tuples where each tuple contains two elements - a string representing a mathematical operation (e.g., `\'3 + 5\'`) and a string representing the operation type (`\'addition\'`, `\'subtraction\'`, `\'multiplication\'`, `\'division\'`). 2. `log_file` (str): A string representing the name of a file where the results and errors of the operations should be logged. # Expected Input and Output: - The function should process each operation and write the result or an appropriate error message into the `log_file`. - The function should handle division by zero, invalid mathematical expressions, and invalid operation types. - If an invalid operation type is encountered, raise a custom exception called `InvalidOperationTypeError`. This exception should inherit from the base `Exception` class. - Ensure the file is closed properly after all operations, using the `with` statement. - Each log entry should be on a new line in the format: `Operation: <operation_string>, Result: <result_or_error_message>`. # Constraints and Limitations: - Do not use `eval` for evaluating the mathematical expressions. - Handle exceptions using `try`, `except`, `else`, and `finally` blocks appropriately. - If the `operations` list is empty, write \\"No operations provided\\" to the log file. # Performance Requirements: - Ensure the function performs efficiently with up to 1000 operations. # Example Usage: ```python def process_operations(operations, log_file): class InvalidOperationTypeError(Exception): pass with open(log_file, \'w\') as file: if not operations: file.write(\\"No operations providedn\\") for operation, operation_type in operations: try: if operation_type == \'addition\': result = eval(operation) elif operation_type == \'subtraction\': result = eval(operation) elif operation_type == \'multiplication\': result = eval(operation) elif operation_type == \'division\': result = eval(operation) else: raise InvalidOperationTypeError(f\\"Invalid operation type: {operation_type}\\") file.write(f\\"Operation: {operation}, Result: {result}n\\") except ZeroDivisionError as e: file.write(f\\"Operation: {operation}, Result: division by zero errorn\\") except (ValueError, SyntaxError) as e: file.write(f\\"Operation: {operation}, Result: invalid mathematical expressionn\\") except InvalidOperationTypeError as e: file.write(f\\"Operation: {operation}, Result: {str(e)}n\\") except Exception as e: file.write(f\\"Operation: {operation}, Result: unexpected error: {str(e)}n\\") finally: pass # `with` statement ensures the file is closed after all operations. # Example call: operations = [ (\'3 + 5\', \'addition\'), (\'10 / 0\', \'division\'), (\'4 - 2\', \'subtraction\'), (\'2 * 3\', \'multiplication\'), (\'4 / 2\', \'division\'), (\'4 ** 2\', \'power\') # InvalidOperationTypeError should be raised for this ] process_operations(operations, \'operations_log.txt\') ```","solution":"def process_operations(operations, log_file): class InvalidOperationTypeError(Exception): pass def evaluate_operation(operation, operation_type): try: parts = operation.split(\' \') if len(parts) != 3: raise ValueError(\\"Invalid format\\") left_operand, operator, right_operand = parts left_operand = float(left_operand) right_operand = float(right_operand) if operation_type == \'addition\' and operator == \'+\': return left_operand + right_operand elif operation_type == \'subtraction\' and operator == \'-\': return left_operand - right_operand elif operation_type == \'multiplication\' and operator == \'*\': return left_operand * right_operand elif operation_type == \'division\' and operator == \'/\': if right_operand == 0: raise ZeroDivisionError return left_operand / right_operand else: raise InvalidOperationTypeError(f\\"Invalid operation type: {operation_type}\\") except ValueError: raise ValueError(\\"Invalid mathematical expression\\") with open(log_file, \'w\') as file: if not operations: file.write(\\"No operations providedn\\") return for operation, operation_type in operations: try: result = evaluate_operation(operation, operation_type) file.write(f\\"Operation: {operation}, Result: {result}n\\") except ZeroDivisionError: file.write(f\\"Operation: {operation}, Result: division by zero errorn\\") except InvalidOperationTypeError as e: file.write(f\\"Operation: {operation}, Result: {str(e)}n\\") except ValueError as e: file.write(f\\"Operation: {operation}, Result: invalid mathematical expressionn\\") except Exception as e: file.write(f\\"Operation: {operation}, Result: unexpected error: {str(e)}n\\")"},{"question":"# Seaborn Line Plot Assessment Objective: To assess your understanding of seaborn\'s `objects` module and your ability to manipulate and visualize data using this module. Task: Using the seaborn package, complete the following tasks: 1. **Load the Dow Jones and fMRI datasets** using seaborn\'s `load_dataset` function. 2. **Create a line plot for the Dow Jones dataset**: - The x-axis should represent the dates. - The y-axis should represent the prices. - Add a line to connect sorted observations. 3. **Create a grouped line plot for the fMRI dataset**: - Focus only on data where the region is `parietal` and the event is `stim`. - The x-axis should represent the time points. - The y-axis should represent the signal. - Group the data by `subject` to create multiple lines. 4. **Enhance the grouped line plot**: - Add an error band to show the variability of the data for each `event`. - Use different colors for each `region`. - Add markers to the line plot to indicate sampled data values. Input: - There are no external inputs. Use the datasets loaded from seaborn\'s `load_dataset` function. Output: - Save the plots as PNG files: 1. `dowjones_line_plot.png` 2. `fmri_grouped_line_plot.png` 3. `fmri_enhanced_grouped_line_plot.png` Constraints: - Use seaborn\'s `objects` module for creating the plots. - Ensure the plots are labeled appropriately for clarity. Example Code Structure: ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Create and save Dow Jones line plot dowjones_plot = so.Plot(dowjones, \\"Date\\", \\"Price\\").add(so.Line()) dowjones_plot.save(\\"dowjones_line_plot.png\\") # Create and save fMRI grouped line plot fmri_subset = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") fmri_grouped_plot = ( fmri_subset .pipe(so.Plot, \\"timepoint\\", \\"signal\\") .add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") ) fmri_grouped_plot.save(\\"fmri_grouped_line_plot.png\\") # Enhance and save fMRI grouped line plot fmri_enhanced_plot = so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") fmri_enhanced_result = ( fmri_enhanced_plot .add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) ) fmri_enhanced_result.save(\\"fmri_enhanced_grouped_line_plot.png\\") ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Create and save Dow Jones line plot dowjones_plot = so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\").add(so.Line()) dowjones_plot.save(\\"dowjones_line_plot.png\\") # Create and save fMRI grouped line plot fmri_subset = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") fmri_grouped_plot = ( fmri_subset .pipe(so.Plot, x=\\"timepoint\\", y=\\"signal\\") .add(so.Line(), group=\\"subject\\") ) fmri_grouped_plot.save(\\"fmri_grouped_line_plot.png\\") # Enhance and save fMRI grouped line plot fmri_enhanced_plot = ( so.Plot(fmri.query(\\"region == \'parietal\' and event == \'stim\'\\"), x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\") .add(so.Line(), group=\\"subject\\") .add(so.Band(), group=\\"event\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), group=\\"subject\\") ) fmri_enhanced_plot.save(\\"fmri_enhanced_grouped_line_plot.png\\")"},{"question":"**PyTorch Coding Challenge** You are given a PyTorch model and two sets of tensors: the original set and a quantized set. Your task is to implement a series of functions that will help compare these two sets of tensors using the utility functions provided in `torch.ao.ns.fx.utils`. Specifically, you will compute the Signal-to-Quantization-Noise Ratio (SQNR), the normalized L2 error, and the cosine similarity between the tensors from the original and the quantized sets. # Function Requirements 1. **compute_sqnr_set**: - **Input**: Two lists of PyTorch tensors (`original_tensors` and `quantized_tensors`). Each list contains tensors of the same shapes, and both lists have the same number of elements. - **Output**: A list of SQNR values, one for each corresponding pair of tensors. - **Constraints**: Use the `torch.ao.ns.fx.utils.compute_sqnr` function to compute the SQNR for each corresponding pair of tensors. 2. **compute_normalized_l2_error_set**: - **Input**: Two lists of PyTorch tensors (`original_tensors` and `quantized_tensors`). Each list contains tensors of the same shapes, and both lists have the same number of elements. - **Output**: A list of normalized L2 error values, one for each corresponding pair of tensors. - **Constraints**: Use the `torch.ao.ns.fx.utils.compute_normalized_l2_error` function to compute the normalized L2 error for each corresponding pair of tensors. 3. **compute_cosine_similarity_set**: - **Input**: Two lists of PyTorch tensors (`original_tensors` and `quantized_tensors`). Each list contains tensors of the same shapes, and both lists have the same number of elements. - **Output**: A list of cosine similarity values, one for each corresponding pair of tensors. - **Constraints**: Use the `torch.ao.ns.fx.utils.compute_cosine_similarity` function to compute the cosine similarity for each corresponding pair of tensors. # Expected Input and Output Formats ```python from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity import torch def compute_sqnr_set(original_tensors, quantized_tensors): Parameters: original_tensors (list of torch.Tensor): List of original tensors quantized_tensors (list of torch.Tensor): List of quantized tensors Returns: list of float: List of SQNR values for each pair of corresponding tensors pass def compute_normalized_l2_error_set(original_tensors, quantized_tensors): Parameters: original_tensors (list of torch.Tensor): List of original tensors quantized_tensors (list of torch.Tensor): List of quantized tensors Returns: list of float: List of normalized L2 error values for each pair of corresponding tensors pass def compute_cosine_similarity_set(original_tensors, quantized_tensors): Parameters: original_tensors (list of torch.Tensor): List of original tensors quantized_tensors (list of torch.Tensor): List of quantized tensors Returns: list of float: List of cosine similarity values for each pair of corresponding tensors pass ``` # Constraints and Performance Requirements: 1. Assume all tensors are of the same shape within each list. 2. Both input lists will have the same length. 3. The function implementations should be efficient in terms of time complexity, but there are no strict performance constraints. Your task is to implement the `compute_sqnr_set`, `compute_normalized_l2_error_set`, and `compute_cosine_similarity_set` functions using the appropriate utility functions from `torch.ao.ns.fx.utils`.","solution":"from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def compute_sqnr_set(original_tensors, quantized_tensors): Parameters: original_tensors (list of torch.Tensor): List of original tensors quantized_tensors (list of torch.Tensor): List of quantized tensors Returns: list of float: List of SQNR values for each pair of corresponding tensors sqnr_values = [] for original_tensor, quantized_tensor in zip(original_tensors, quantized_tensors): sqnr = compute_sqnr(original_tensor, quantized_tensor) sqnr_values.append(sqnr) return sqnr_values def compute_normalized_l2_error_set(original_tensors, quantized_tensors): Parameters: original_tensors (list of torch.Tensor): List of original tensors quantized_tensors (list of torch.Tensor): List of quantized tensors Returns: list of float: List of normalized L2 error values for each pair of corresponding tensors l2_error_values = [] for original_tensor, quantized_tensor in zip(original_tensors, quantized_tensors): l2_error = compute_normalized_l2_error(original_tensor, quantized_tensor) l2_error_values.append(l2_error) return l2_error_values def compute_cosine_similarity_set(original_tensors, quantized_tensors): Parameters: original_tensors (list of torch.Tensor): List of original tensors quantized_tensors (list of torch.Tensor): List of quantized tensors Returns: list of float: List of cosine similarity values for each pair of corresponding tensors cosine_similarity_values = [] for original_tensor, quantized_tensor in zip(original_tensors, quantized_tensors): cosine_similarity = compute_cosine_similarity(original_tensor, quantized_tensor) cosine_similarity_values.append(cosine_similarity) return cosine_similarity_values"},{"question":"Objective To evaluate your understanding of the PyTorch \\"meta\\" device and its practical applications in model loading and tensor operations. Question You are required to implement a function named `meta_tensor_operations` that performs the following tasks: 1. Saves a randomly initialized tensor of shape `(10, 10)` to a file. 2. Loads the saved tensor onto the \\"meta\\" device. 3. Creates a simple neural network model (`torch.nn.Linear`) on the \\"meta\\" device with specified input and output dimensions. 4. Moves the model to an empty state on the CPU. Function Signature ```python def meta_tensor_operations(file_path: str, input_dim: int, output_dim: int) -> torch.nn.Module: Parameters: - file_path: str, the path where the tensor will be saved and loaded from. - input_dim: int, the input dimension for the Linear model. - output_dim: int, the output dimension for the Linear model. Returns: - A torch.nn.Module object representing the Linear model moved to the CPU with uninitialized parameters. pass ``` Constraints 1. You must use the \\"meta\\" device for appropriate tensor and model operations as described. 2. The function should properly handle file read/write operations and device context management. 3. Ensure that the resulting model contains the correct metadata (input and output dimensions) after moving to the CPU. Example ```python model = meta_tensor_operations(\\"tensor.pt\\", 10, 5) print(model) # Expected output: # Linear(in_features=10, out_features=5, bias=True) ``` Ensure that your implementation adheres to the function signature and the expected behavior as described.","solution":"import torch def meta_tensor_operations(file_path: str, input_dim: int, output_dim: int) -> torch.nn.Module: Parameters: - file_path: str, the path where the tensor will be saved and loaded from. - input_dim: int, the input dimension for the Linear model. - output_dim: int, the output dimension for the Linear model. Returns: - A torch.nn.Module object representing the Linear model moved to the CPU with uninitialized parameters. # 1. Create a random tensor and save it to a file tensor = torch.randn(10, 10) torch.save(tensor, file_path) # 2. Load the tensor onto the \\"meta\\" device loaded_tensor = torch.load(file_path, map_location=\\"meta\\") # 3. Create a Linear model on the \\"meta\\" device model_meta = torch.nn.Linear(input_dim, output_dim, device=\\"meta\\") # 4. Move the model to the CPU model = torch.nn.Linear(input_dim, output_dim).to_empty(device=\\"cpu\\") return model"},{"question":"Objective: You are given a task to visualize the relationship between different attributes of the penguins dataset using seaborn. Specifically, you need to create a histogram, include additional data mappings such as color for different categories, and handle potential overlapping issues using transformations. Requirements: 1. **Load** the \'penguins\' dataset from seaborn. 2. **Create** a plot that: - Uses a bar plot to show the count of different penguin species. - Uses color to differentiate between male and female penguins. - Applies a transformation to handle overlapping bars (use `Dodge`). - Sets custom properties such as edge width and transparency for better visualization. 3. **Add** error bars to the bar plot representing the standard deviation of the body mass for each species. # Instructions: 1. Implement a function `plot_penguins_data()` that performs the tasks outlined above. 2. Your plot should be clear and make good use of seaborn functionalities demonstrated in the documentation. 3. The function should handle any additional necessary data preprocessing. # Input: - No input parameters. # Output: - A visualization plot created by seaborn will be displayed as a result of the function execution. # Code Template: ```python import seaborn.objects as so from seaborn import load_dataset def plot_penguins_data(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create a plot using seaborn objects plot = (so.Plot(penguins, x=\\"species\\", color=\\"sex\\") .add(so.Bar(edgewidth=2, alpha=0.6), so.Hist(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge())) # Display the plot plot.show() # Call the function to display the plot plot_penguins_data() ``` # Constraints: - Ensure that the function gracefully handles missing data within the dataset. - The plot should clearly display the separation between different categories and should be aesthetically pleasing.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_data(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Data preprocessing to handle missing data penguins = penguins.dropna().reset_index(drop=True) # Create a plot using seaborn plot = sns.catplot( data=penguins, x=\\"species\\", hue=\\"sex\\", kind=\\"count\\", palette=\\"deep\\", edgecolor=\\".6\\", alpha=0.7, dodge=True ) # Adding error bars for the standard deviation of the body mass error_dict = penguins.groupby([\'species\', \'sex\'])[\'body_mass_g\'].std().reset_index() for index, row in error_dict.iterrows(): plt.errorbar(row[\'species\'], penguins[penguins[\'species\']==row[\'species\']][\'body_mass_g\'].mean(), yerr=row[\'body_mass_g\'], fmt=\'none\', c=\'black\', capsize=5) # Set plot properties for better visualization plot.set_axis_labels(\\"Species\\", \\"Count\\") plot.set_titles(\\"Penguin Species Count by Sex\\") plot.despine(left=True) plt.show() # Call the function to display the plot plot_penguins_data()"},{"question":"Problem Statement You are provided with a dataset consisting of multiple features. Your task is to implement a scikit-learn pipeline that performs the following steps: 1. Standardizes the data (features) using `StandardScaler`. 2. Reduces the dimensionality of the dataset using Principal Component Analysis (PCA) to retain 95% of the variance. 3. Outputs the shape of the original dataset and the transformed dataset after PCA. # Input and Output Specifications **Input:** - A 2D numpy array `data` of shape (n_samples, n_features), where `n_samples` is the number of samples and `n_features` is the number of features. **Output:** - A tuple containing two elements: 1. An integer tuple representing the shape of the original dataset. 2. An integer tuple representing the shape of the transformed dataset (i.e., after dimensionality reduction using PCA). # Constraints - The pipeline should be implemented using scikit-learn. - The implementation should properly handle the standardization and PCA steps within the pipeline. # Example ```python import numpy as np # Example Input data = np.array([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0], [9.0, 10.0, 11.0]]) # Expected Output ((3, 3), (3, 2)) # The numbers are illustrative; the actual retained components will vary. ``` # Instructions 1. Implement the function `reduce_dimensionality(data: np.ndarray) -> tuple`. 2. Make sure to import the necessary modules from scikit-learn. 3. The function should follow the scikit-learn pipeline best practices. ```python def reduce_dimensionality(data: np.ndarray) -> tuple: from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline # Create a pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=0.95)) ]) # Fit and transform the data using the pipeline transformed_data = pipeline.fit_transform(data) return data.shape, transformed_data.shape ``` # Note - Your solution should be robust and handle different sizes of input data. - Ensure that your solution is efficient and leverages the scikit-learn pipeline appropriately.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline def reduce_dimensionality(data: np.ndarray) -> tuple: Performs standardization and PCA on the dataset to retain 95% of variance. Args: data (np.ndarray): The input dataset with shape (n_samples, n_features). Returns: tuple: A tuple containing the shape of the original and the transformed dataset. # Create a pipeline for standardizing the data and applying PCA pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=0.95)) ]) # Fit the pipeline and transform the data transformed_data = pipeline.fit_transform(data) # Return the shape of the original and the transformed data return data.shape, transformed_data.shape"},{"question":"**Question: Pandas Series Data Analysis** You are given a CSV file containing daily sales data of a retail store. The CSV file has two columns: `date` and `sales`. **Objective** Write a function `analyze_sales_data` that reads the CSV into a pandas Series, performs several analyses and transformations, and returns a dictionary with specific results. **Function Signature** ```python def analyze_sales_data(file_path: str) -> dict: pass ``` **Input** - `file_path` (string): The path to the CSV file containing the sales data. **Output** - A dictionary with the following keys and corresponding values: - `\'total_sales\'`: Total sales over the period. - `\'mean_sales\'`: Mean daily sales. - `\'median_sales\'`: Median daily sales. - `\'max_sales\'`: Maximum sales in a day. - `\'min_sales\'`: Minimum sales in a day. - `\'num_days_missing_data\'`: Number of days with missing sales data. - `\'sales_on_max_day\'`: Sales on the day with the highest sales. - `\'sales_on_min_day\'`: Sales on the day with the lowest sales. - `\'monthly_total_sales\'`: A pandas Series with total sales for each month. **Constraints** - Handle any missing values in the sales data by filling them with the median sales value of the given data. - The \'date\' column follows the format `YYYY-MM-DD`. **Example Usage** ```python # Assume the file \'daily_sales.csv\' is structured as follows: # date,sales # 2022-01-01,100 # 2022-01-02,200 # ... result = analyze_sales_data(\'daily_sales.csv\') print(result) # Output might look like: # { # \'total_sales\': 10000, # \'mean_sales\': 150.0, # \'median_sales\': 140.0, # \'max_sales\': 300, # \'min_sales\': 50, # \'num_days_missing_data\': 2, # \'sales_on_max_day\': 300, # \'sales_on_min_day\': 50, # \'monthly_total_sales\': pd.Series; 2022-01: 3000, 2022-02: 2500, ... # } ``` **Additional Information** - You may assume the CSV file is well-formed and contains no corrupted data. - Utilize pandas\' `to_datetime` function to parse dates and `resample` function for monthly aggregation.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> dict: # Read CSV into a pandas DataFrame df = pd.read_csv(file_path) df[\'date\'] = pd.to_datetime(df[\'date\']) df.set_index(\'date\', inplace=True) # Handle missing values by filling them with median sales value median_sales = df[\'sales\'].median() missing_data_count = df[\'sales\'].isna().sum() df[\'sales\'].fillna(median_sales, inplace=True) total_sales = df[\'sales\'].sum() mean_sales = df[\'sales\'].mean() median_sales_filled = df[\'sales\'].median() max_sales = df[\'sales\'].max() min_sales = df[\'sales\'].min() sales_on_max_day = df[df[\'sales\'] == max_sales].index[0] sales_on_min_day = df[df[\'sales\'] == min_sales].index[0] # Calculate monthly total sales monthly_total_sales = df[\'sales\'].resample(\'M\').sum() result = { \'total_sales\': total_sales, \'mean_sales\': mean_sales, \'median_sales\': median_sales_filled, \'max_sales\': max_sales, \'min_sales\': min_sales, \'num_days_missing_data\': missing_data_count, \'sales_on_max_day\': sales_on_max_day, \'sales_on_min_day\': sales_on_min_day, \'monthly_total_sales\': monthly_total_sales } return result"},{"question":"# Advanced Coding Assessment: Simulation of Customer Service at a Bank You are tasked to simulate a bank with multiple service counters, where customers arrive according to a random time schedule, and each customer\'s service takes a random amount of time. The bank has `n` service counters, and the service times and inter-arrival times of customers are drawn from exponential distributions. The goal is to find out the average waiting time for customers and the maximum waiting time over a simulation of `m` customers. Problem Statement Write a function `simulate_bank_service(n: int, m: int, arrival_rate: float, service_rate: float) -> (float, float)` that simulates the customer service process at a bank with `n` service counters for `m` customers. It returns a tuple of two values: - The average waiting time of customers. - The maximum waiting time observed during the simulation. Use the following parameters: - `n` (int): Number of service counters. - `m` (int): Number of customers. - `arrival_rate` (float): The average rate (λ) of customer arrivals (customers per unit time). Use this to compute the inter-arrival time. - `service_rate` (float): The average rate (μ) at which services are completed (services per unit time). Use this to compute the service time. The inter-arrival time of customers should be generated using an exponential distribution with a mean of `1/arrival_rate`. Similarly, the service time should be generated using an exponential distribution with a mean of `1/service_rate`. You may use the functions `expovariate` and `heapreplace` from the `random` and `heapq` modules, respectively. Example Calculation ```python def simulate_bank_service(n, m, arrival_rate, service_rate): import heapq from random import expovariate waits = [] arrival_time = 0.0 servers = [0.0] * n # Time when each server becomes available heapq.heapify(servers) for _ in range(m): arrival_time += expovariate(arrival_rate) next_server_available = servers[0] wait = max(0.0, next_server_available - arrival_time) waits.append(wait) service_duration = expovariate(service_rate) service_completed = arrival_time + wait + service_duration heapq.heapreplace(servers, service_completed) average_wait = sum(waits) / len(waits) max_wait = max(waits) return average_wait, max_wait # Example usage avg_wait, max_wait = simulate_bank_service(n=3, m=1000, arrival_rate=0.1, service_rate=0.2) print(f\\"Average Wait: {avg_wait:.2f}, Max Wait: {max_wait:.2f}\\") ``` # Constraints: - The number of counters `n` is between 1 and 100. - The number of customers `m` is between 1 and 10^6. - The arrival rate and service rate are positive floating-point numbers. # Performance Requirements: - The function should complete in a reasonable amount of time for the largest inputs. - Proper use of efficient algorithms and data structures is expected to handle up to 10^6 customers effectively. Consider edge cases, such as when all customers arrive at nearly the same time or when service times are extremely short/long.","solution":"def simulate_bank_service(n: int, m: int, arrival_rate: float, service_rate: float) -> (float, float): import heapq from random import expovariate waits = [] arrival_time = 0.0 servers = [0.0] * n # Time when each server becomes available heapq.heapify(servers) for _ in range(m): arrival_time += expovariate(arrival_rate) next_server_available = servers[0] wait = max(0.0, next_server_available - arrival_time) waits.append(wait) service_duration = expovariate(service_rate) service_completed = arrival_time + wait + service_duration heapq.heapreplace(servers, service_completed) average_wait = sum(waits) / len(waits) max_wait = max(waits) return average_wait, max_wait"},{"question":"Coding Assessment Question # Objective To assess your understanding of PyTorch distributions, you will implement a function that uses multiple types of distributions from the `torch.distributions` module. You need to demonstrate your knowledge in initializing distributions, sampling from them, and computing probabilities. # Task Implement a function `sample_and_compute_probabilities` that does the following: 1. Initializes three distributions: - A Normal distribution with given mean and standard deviation. - A Bernoulli distribution with a given probability of success. - A Categorical distribution with given probabilities for each category. 2. Samples a given number of samples from each of these distributions. 3. Computes and returns the log-probabilities of these samples. # Function Signature ```python def sample_and_compute_probabilities(normal_params, bernoulli_prob, categorical_probs, num_samples): Args: - normal_params (tuple): A tuple (mean, std) where mean is the mean of the Normal distribution, and std is the standard deviation. - bernoulli_prob (float): Probability of success for the Bernoulli distribution. - categorical_probs (list): Probabilities for each category in the Categorical distribution. - num_samples (int): Number of samples to draw from each distribution. Returns: - dict: A dictionary with three keys: \\"normal\\", \\"bernoulli\\", and \\"categorical\\". Each key maps to another dictionary with two keys: \\"samples\\": A tensor of samples drawn from the distribution. \\"log_probs\\": A tensor of log-probabilities of the samples. pass ``` # Example Usage ```python normal_params = (0, 1) bernoulli_prob = 0.5 categorical_probs = [0.1, 0.3, 0.6] num_samples = 5 result = sample_and_compute_probabilities(normal_params, bernoulli_prob, categorical_probs, num_samples) # The output format will be: # { # \\"normal\\": { # \\"samples\\": tensor([...]), # \\"log_probs\\": tensor([...]) # }, # \\"bernoulli\\": { # \\"samples\\": tensor([...]), # \\"log_probs\\": tensor([...]) # }, # \\"categorical\\": { # \\"samples\\": tensor([...]), # \\"log_probs\\": tensor([...]) # } # } ``` # Constraints - Use the PyTorch `torch.distributions` module to create and manage the distributions. - Handle edge cases where inputs might not form valid distributions (e.g., invalid probabilities). - Ensure that the samples and log-probabilities are computed correctly and efficiently. # Tips - Refer to the `torch.distributions` documentation for the proper initialization and methods to use distributions. - Utilize PyTorch functions to ensure compatibility and performance. # Evaluation Criteria - Correct implementation and usage of PyTorch distributions. - Accurate sampling and computation of log-probabilities. - Code clarity and adherence to the function signature.","solution":"import torch from torch.distributions import Normal, Bernoulli, Categorical def sample_and_compute_probabilities(normal_params, bernoulli_prob, categorical_probs, num_samples): Args: - normal_params (tuple): A tuple (mean, std) where mean is the mean of the Normal distribution, and std is the standard deviation. - bernoulli_prob (float): Probability of success for the Bernoulli distribution. - categorical_probs (list): Probabilities for each category in the Categorical distribution. - num_samples (int): Number of samples to draw from each distribution. Returns: - dict: A dictionary with three keys: \\"normal\\", \\"bernoulli\\", and \\"categorical\\". Each key maps to another dictionary with two keys: \\"samples\\": A tensor of samples drawn from the distribution. \\"log_probs\\": A tensor of log-probabilities of the samples. normal_dist = Normal(normal_params[0], normal_params[1]) bernoulli_dist = Bernoulli(bernoulli_prob) categorical_dist = Categorical(torch.tensor(categorical_probs)) normal_samples = normal_dist.sample((num_samples,)) bernoulli_samples = bernoulli_dist.sample((num_samples,)) categorical_samples = categorical_dist.sample((num_samples,)) normal_log_probs = normal_dist.log_prob(normal_samples) bernoulli_log_probs = bernoulli_dist.log_prob(bernoulli_samples) categorical_log_probs = categorical_dist.log_prob(categorical_samples) return { \\"normal\\": { \\"samples\\": normal_samples, \\"log_probs\\": normal_log_probs }, \\"bernoulli\\": { \\"samples\\": bernoulli_samples, \\"log_probs\\": bernoulli_log_probs }, \\"categorical\\": { \\"samples\\": categorical_samples, \\"log_probs\\": categorical_log_probs } }"},{"question":"Custom Tensor-like Object Implementation Objective: Create a custom tensor-like object that correctly integrates with PyTorch\'s `__torch_function__` protocol. Your implementation should demonstrate an understanding of overriding and extending PyTorch functions. Problem Statement: You are required to implement a custom tensor-like class called `MyTensor` that uses the `__torch_function__` protocol to override PyTorch\'s `torch.add` operation. Your `MyTensor` class should: 1. Behave similarly to a `torch.Tensor` object. 2. Override the `torch.add` operation to scale the result by a factor of 2. Requirements: 1. Implement a class called `MyTensor`. 2. Integrate the `__torch_function__` protocol to override the behavior of `torch.add` for instances of `MyTensor`. 3. Ensure that the overridden `torch.add` operation scales the result by a factor of 2. 4. Implement a helper function `is_tensor_like` to determine if an object is instance of your `MyTensor` class. Input: - Two `MyTensor` objects or one `MyTensor` object and one `torch.Tensor` object. Output: - A `MyTensor` object with the result scaled by a factor of 2. Example: ```python import torch a = MyTensor(torch.tensor([1, 2, 3])) b = MyTensor(torch.tensor([4, 5, 6])) result = torch.add(a, b) print(result) # MyTensor(tensor([10, 14, 18])) # Adding standard tensor and MyTensor c = torch.tensor([7, 8, 9]) result = torch.add(a, c) print(result) # MyTensor(tensor([16, 20, 24])) ``` Constraints: 1. You can only use PyTorch and Python standard library. 2. Performance should be considered - avoid unnecessary computations. Implementation: ```python import torch from torch.overrides import has_torch_function, handle_torch_function class MyTensor: def __init__(self, data): if not isinstance(data, torch.Tensor): raise TypeError(\\"Data must be a torch.Tensor\\") self.data = data def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} # Handle only torch.add, let other functions run normally if func == torch.add: raw_result = func(*args, **kwargs) return MyTensor(raw_result * 2) return NotImplemented @property def shape(self): return self.data.shape @property def dtype(self): return self.data.dtype def __repr__(self): return f\\"MyTensor({self.data})\\" def is_tensor_like(obj): return isinstance(obj, MyTensor) ```","solution":"import torch from torch.overrides import has_torch_function, handle_torch_function class MyTensor: def __init__(self, data): if not isinstance(data, torch.Tensor): raise TypeError(\\"Data must be a torch.Tensor\\") self.data = data def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func == torch.add: args = [arg.data if isinstance(arg, MyTensor) else arg for arg in args] result = func(*args, **kwargs) return MyTensor(result * 2) return NotImplemented @property def shape(self): return self.data.shape @property def dtype(self): return self.data.dtype def __repr__(self): return f\\"MyTensor({self.data})\\" def is_tensor_like(obj): return isinstance(obj, MyTensor)"},{"question":"# Assessing Comprehension of Python Control Flow and Function Definitions Objective: Write a Python function to analyze a list of dictionaries representing books in a library and return statistics on books based on specific criteria. Function Definition: Define a function named `analyze_books` that takes a list of dictionaries where each dictionary contains the keys: `\'title\'`, `\'author\'`, `\'pages\'`, `\'genre\'`, and a variable number of additional keyword arguments that specify specific criteria. # Parameters: 1. `books` (list of dictionaries): A list containing dictionaries with each book\'s information. 2. **kwargs: Additional keyword arguments to filter and analyze the books. The allowed keyword arguments are: - `min_pages` (int): Minimum number of pages a book must have to be included in the analysis. - `max_pages` (int): Maximum number of pages a book can have to be included in the analysis. - `genre` (str): The genre of books to be included in the analysis. # Returns: A dictionary with the following keys and associated values: - `\'total_books\'` (int): Total number of books that meet the criteria. - `\'average_pages\'` (float): The average number of pages in the books that meet the criteria. - `\'books\'` (list of str): A sorted list (alphabetically) of titles of the books that meet the criteria. # Constraints: - The list of books is guaranteed to be non-empty. - Each book dictionary will always contain the keys: `\'title\'`, `\'author\'`, `\'pages\'`, and `\'genre\'`. Example Input: ```python books = [ {\'title\': \'Book A\', \'author\': \'Author 1\', \'pages\': 150, \'genre\': \'Fiction\'}, {\'title\': \'Book B\', \'author\': \'Author 2\', \'pages\': 200, \'genre\': \'Science\'}, {\'title\': \'Book C\', \'author\': \'Author 3\', \'pages\': 300, \'genre\': \'Fiction\'}, {\'title\': \'Book D\', \'author\': \'Author 4\', \'pages\': 180, \'genre\': \'Science\'} ] ``` Example Usage: ```python results = analyze_books(books, min_pages=150, genre=\'Fiction\') print(results) ``` Example Output: ```python { \'total_books\': 2, \'average_pages\': 225, \'books\': [\'Book A\', \'Book C\'] } ``` Note: Ensure your function handles different combinations of keyword arguments appropriately and efficiently. The function should be both functional and capable of handling edge cases like no books matching the criteria.","solution":"def analyze_books(books, **kwargs): Analyze a list of dictionaries representing books and return statistics based on specified criteria. Parameters: books (list of dict): List of dictionaries each containing \'title\', \'author\', \'pages\', and \'genre\'. **kwargs: Additional keyword arguments to filter and analyze the books. - min_pages (int): Minimum number of pages a book must have to be included. - max_pages (int): Maximum number of pages a book can have to be included. - genre (str): The genre of books to be included. Returns: dict: A dictionary with the keys \'total_books\', \'average_pages\', \'books\' containing the statistics. min_pages = kwargs.get(\'min_pages\', 0) max_pages = kwargs.get(\'max_pages\', float(\'inf\')) genre = kwargs.get(\'genre\', None) filtered_books = [ book for book in books if book[\'pages\'] >= min_pages and book[\'pages\'] <= max_pages and (genre is None or book[\'genre\'] == genre) ] total_books = len(filtered_books) average_pages = sum(book[\'pages\'] for book in filtered_books) / total_books if total_books > 0 else 0 book_titles = sorted(book[\'title\'] for book in filtered_books) return { \'total_books\': total_books, \'average_pages\': average_pages, \'books\': book_titles, }"},{"question":"# Advanced Coding Assessment Problem Statement You are working on a project that involves reading and writing binary data files which follow a specific structure. Your task is to implement functions to handle this data format using Python\'s `struct` module. The binary data represents records of students with the following structure: - An 8-byte floating point number (`double`) representing the student\'s score. - A 4-byte integer (`int`) representing the student\'s ID. - A variable-length string (up to 50 characters) for the student\'s name. The length of the string is specified by a 1-byte unsigned integer that precedes the actual string data. You need to implement two functions: 1. `pack_student_records(records)`: - **Input**: A list of tuples, where each tuple contains: - A floating point number (student\'s score). - An integer (student\'s ID). - A string (student\'s name). - **Output**: A bytes object representing the packed binary data. - **Constraints**: - The student\'s name should be truncated to 50 characters if it exceeds this length. - The total length of the packed data should be as compact as possible without unnecessary padding. 2. `unpack_student_records(data)`: - **Input**: A bytes object containing the packed binary data. - **Output**: A list of tuples, where each tuple contains: - A floating point number (student\'s score). - An integer (student\'s ID). - A string (student\'s name). Your implementation should ensure that the data can be correctly packed and unpacked while preserving the original information. Example ```python records = [ (95.5, 12345, \'Alice\'), (88.0, 67890, \'Bob the Builder\'), (72.5, 11121, \'Charlie Chaplin\') ] # Packing the records packed_data = pack_student_records(records) # Unpacking the records unpacked_records = unpack_student_records(packed_data) assert records == unpacked_records ``` **Notes**: - Use the `struct` module and ensure the binary data format adheres to the structure described above. - Pay particular attention to byte order, size, and alignment to ensure cross-platform compatibility. - The length of the packed data should be minimal and avoid unnecessary padding. **Performance requirements**: - The solution should handle a list of up to 10,000 student records efficiently.","solution":"import struct def pack_student_records(records): Packs a list of student records into a binary format. Parameters: records (list of tuples): List of tuples, each containing (float, int, string). Returns: bytes: Packed binary data. packed_data = [] for score, student_id, name in records: # Truncate name to 50 characters name = name[:50] name_bytes = name.encode(\'utf-8\') name_length = len(name_bytes) format_string = f\\"=diB{name_length}s\\" packed_record = struct.pack(format_string, score, student_id, name_length, name_bytes) packed_data.append(packed_record) return b\'\'.join(packed_data) def unpack_student_records(data): Unpacks binary data into a list of student records. Parameters: data (bytes): Packed binary data. Returns: list of tuples: List of tuples, each containing (float, int, string). unpacked_records = [] offset = 0 while offset < len(data): score, student_id, name_length = struct.unpack_from(\\"=diB\\", data, offset) offset += struct.calcsize(\\"=diB\\") name_bytes = struct.unpack_from(f\\"{name_length}s\\", data, offset)[0] offset += struct.calcsize(f\\"{name_length}s\\") name = name_bytes.decode(\'utf-8\') unpacked_records.append((score, student_id, name)) return unpacked_records"},{"question":"**Objective:** Demonstrate understanding of Python\'s module execution and namespace handling using the `runpy` module. **Instructions:** Using the `runpy` module, implement a function named `execute_module_code` that dynamically executes code in a specified Python module by its name. Your function should: - Accept an absolute module name as input. - Optionally accept a dictionary of global variables to pre-populate in the module\'s globals dictionary. - Ensure that changes to the `sys` module are reverted after execution. - Return the resulting module globals dictionary. **Function Signature:** ```python def execute_module_code(module_name: str, globals_dict: dict = None) -> dict: Executes the Python module specified by module_name without importing it. Parameters: - module_name (str): The absolute name of the module to be executed. - globals_dict (dict, optional): A dictionary of global variables to pre-populate. Returns: - dict: The resulting module globals dictionary after execution. ``` **Constraints:** - You may assume that the provided module name is always a valid absolute module name. - Ensure that your function is thread-safe where possible. - Use `runpy.run_module` from the `runpy` module for executing the module code. **Example Usage:** Assume we have a module named `example_module.py` with the following content: ```python # example_module.py variables = [\'a\', \'b\', \'c\'] def example_function(): return \\"Hello, World!\\" ``` Running your function: ```python result = execute_module_code(\'example_module\') print(result[\'variables\']) # Output should be [\'a\', \'b\', \'c\'] print(result[\'example_function\']()) # Output should be \\"Hello, World!\\" ``` In this example, result should contain all global variables and functions defined within `example_module.py`. **Notes:** - Make sure to handle any potential exceptions that might occur during the execution process. - Pay attention to the manipulation of the `sys` module as detailed in the provided documentation. Good luck!","solution":"import runpy import sys def execute_module_code(module_name: str, globals_dict: dict = None) -> dict: Executes the Python module specified by module_name without importing it. Parameters: - module_name (str): The absolute name of the module to be executed. - globals_dict (dict, optional): A dictionary of global variables to pre-populate. Returns: - dict: The resulting module globals dictionary after execution. original_sys_modules = dict(sys.modules) if globals_dict is None: globals_dict = {} try: result = runpy.run_module(module_name, init_globals=globals_dict, run_name=\'__main__\') return result finally: # Ensure original sys.modules state is restored sys.modules.clear() sys.modules.update(original_sys_modules)"},{"question":"**Objective**: To assess the understanding of the `turtle` module in Python, particularly the ability to use basic turtle movement commands, control the turtle\'s drawing pen, handle user input, and interact with screen events. **Question**: You are tasked with creating a Turtle Graphics-based drawing application. The application should: 1. Draw a regular polygon based on user input. 2. Allow the user to change the color of the polygon dynamically. 3. Implement key bindings to control the turtle\'s visibility and clear the screen. 4. Allow the user to save the current screen drawing as an image file. **Requirements**: - Implement a function `draw_polygon(sides, length)` that draws a regular polygon with the specified number of sides and side length. - Implement key bindings: - Pressing \\"C\\" clears the screen. - Pressing \\"H\\" hides the turtle. - Pressing \\"S\\" shows the turtle. - Pressing \\"Q\\" quits the application. - Use `textinput()` to prompt the user to change the pen color. - The application should handle mouse clicks to set the turtle\'s position to the clicked location. - Use `numinput()` to prompt the user for the number of sides and side length. - The user should be able to save the drawing by pressing \\"P\\" which will prompt for a file name and save the current screen. **Input/Output Specifications**: - The `draw_polygon()` function takes two arguments: - `sides` (int): Number of sides of the polygon. - `length` (int): Length of each side of the polygon. - The application should display prompts and handle user input gracefully. **Constraints**: - The number of sides of the polygon should be an integer between 3 and 10. - The length of each side should be an integer between 10 and 200. - Use the `turtle` module to handle all graphical operations. **Performance Requirement**: - Ensure the turtle graphics operations execute smoothly without noticeable delay. **Example**: ```python import turtle def draw_polygon(sides, length): angle = 360 / sides for _ in range(sides): turtle.forward(length) turtle.right(angle) def change_color(): color = turtle.textinput(\\"Color Change\\", \\"Enter a color name or hex code:\\") turtle.pencolor(color) def clear_screen(): turtle.clearscreen() def hide_turtle(): turtle.hideturtle() def show_turtle(): turtle.showturtle() def quit_app(): turtle.bye() def save_drawing(): filename = turtle.textinput(\\"Save Drawing\\", \\"Enter file name (without extension):\\") canvas = turtle.getcanvas() canvas.postscript(file=filename + \\".eps\\") def move_turtle(x, y): turtle.penup() turtle.goto(x, y) turtle.pendown() def main(): screen = turtle.Screen() screen.title(\\"Turtle Drawing Application\\") sides = turtle.numinput(\\"Polygon\\", \\"Enter number of sides (3-10):\\", minval=3, maxval=10) length = turtle.numinput(\\"Polygon\\", \\"Enter length of sides (10-200):\\", minval=10, maxval=200) screen.listen() screen.onkey(clear_screen, \\"c\\") screen.onkey(hide_turtle, \\"h\\") screen.onkey(show_turtle, \\"s\\") screen.onkey(quit_app, \\"q\\") screen.onkey(save_drawing, \\"p\\") screen.onclick(move_turtle) turtle.speed(1) change_color() draw_polygon(int(sides), int(length)) turtle.done() if __name__ == \\"__main__\\": main() ``` **Notes**: - Ensure all methods work correctly and handle invalid inputs gracefully. - Provide clear comments in the code to explain functionality. - Test the application with different inputs to ensure reliability.","solution":"import turtle def draw_polygon(sides, length): angle = 360 / sides for _ in range(sides): turtle.forward(length) turtle.right(angle) def change_color(): color = turtle.textinput(\\"Color Change\\", \\"Enter a color name or hex code:\\") if color: turtle.pencolor(color) def clear_screen(): turtle.clearscreen() def hide_turtle(): turtle.hideturtle() def show_turtle(): turtle.showturtle() def quit_app(): turtle.bye() def save_drawing(): filename = turtle.textinput(\\"Save Drawing\\", \\"Enter file name (without extension):\\") if filename: canvas = turtle.getcanvas() canvas.postscript(file=filename + \\".eps\\") def move_turtle(x, y): turtle.penup() turtle.goto(x, y) turtle.pendown() def main(): screen = turtle.Screen() screen.title(\\"Turtle Drawing Application\\") sides = turtle.numinput(\\"Polygon\\", \\"Enter number of sides (3-10):\\", minval=3, maxval=10) length = turtle.numinput(\\"Polygon\\", \\"Enter length of sides (10-200):\\", minval=10, maxval=200) screen.listen() screen.onkey(clear_screen, \\"c\\") screen.onkey(hide_turtle, \\"h\\") screen.onkey(show_turtle, \\"s\\") screen.onkey(quit_app, \\"q\\") screen.onkey(save_drawing, \\"p\\") screen.onclick(move_turtle) turtle.speed(1) change_color() draw_polygon(int(sides), int(length)) turtle.done() if __name__ == \\"__main__\\": main()"},{"question":"**Email and JSON Data Handling Challenge** **Objective:** This question assesses your ability to work with both the `email` and `json` modules to manipulate email data and JSON objects. **Problem Statement:** You are given a JSON string representing a collection of emails. Each email includes fields such as `From`, `To`, `Subject`, and `Body`. Your task is to write a function that: 1. Parses the JSON string to extract the emails. 2. For each email, creates a proper email message using the `email` module. 3. Encodes these email messages into MIME format. 4. Returns a JSON string that maps email subjects to their corresponding MIME-encoded email representations. Implement the following function: ```python import json import email def process_emails(json_string): Parses a JSON string representing a collection of emails, creates and encodes email messages in MIME format, and returns a JSON string mapping email subjects to their MIME representations. Parameters: json_string (str): A JSON string containing emails information. Returns: str: A JSON string that maps email subjects to their MIME-encoded email representations. # Your code here ``` **Input:** - A JSON string where each email is an object with the following structure: ```json [ { \\"From\\": \\"sender@example.com\\", \\"To\\": \\"receiver@example.com\\", \\"Subject\\": \\"Test Email\\", \\"Body\\": \\"This is a test email.\\" }, ... ] ``` **Output:** - A JSON string where keys are email subjects and values are MIME-encoded email representations. **Example:** ```python json_emails = \'\'\' [ { \\"From\\": \\"sender@example.com\\", \\"To\\": \\"receiver@example.com\\", \\"Subject\\": \\"Test Email\\", \\"Body\\": \\"This is a test email.\\" }, { \\"From\\": \\"another@example.com\\", \\"To\\": \\"other@example.com\\", \\"Subject\\": \\"Another Test\\", \\"Body\\": \\"This is another test email.\\" } ] \'\'\' result = process_emails(json_emails) # \'result\' should be a JSON string similar to: # { # \\"Test Email\\": \\"MIME-encoded-string-1\\", # \\"Another Test\\": \\"MIME-encoded-string-2\\" # } ``` **Constraints:** - Ensure proper handling of special characters in email fields. - Validate the input to handle cases where required fields might be missing. - Assume input string is well-formed JSON. **Notes:** - Use the `email.message.EmailMessage` class to create email objects. - Use the `email.generator` module to generate MIME strings. You can refer to Python\'s official documentation for `email` and `json` modules if needed.","solution":"import json from email.message import EmailMessage from email.generator import BytesGenerator from io import BytesIO def process_emails(json_string): Parses a JSON string representing a collection of emails, creates and encodes email messages in MIME format, and returns a JSON string mapping email subjects to their MIME representations. Parameters: json_string (str): A JSON string containing emails information. Returns: str: A JSON string that maps email subjects to their MIME-encoded email representations. # Parse the JSON string emails = json.loads(json_string) # Dictionary to map subjects to MIME representations subject_to_mime = {} for email_data in emails: # Create email message msg = EmailMessage() msg[\'From\'] = email_data[\'From\'] msg[\'To\'] = email_data[\'To\'] msg[\'Subject\'] = email_data[\'Subject\'] msg.set_content(email_data[\'Body\']) # Encode email to MIME format buffer = BytesIO() generator = BytesGenerator(buffer) generator.flatten(msg) mime_string = buffer.getvalue().decode(\'utf-8\') # Map subject to MIME string subject_to_mime[email_data[\'Subject\']] = mime_string # Convert the result to JSON string return json.dumps(subject_to_mime, ensure_ascii=False)"},{"question":"**Pandas Plotting Challenge** **Objective:** Demonstrate your understanding of advanced plotting functionalities in Pandas by creating a customized visualization. **Problem Statement:** You are provided with a dataset containing sales data for different products over a period of 12 months. You need to: 1. Generate a DataFrame with sales data. 2. Create a line plot for the monthly sales of all products. 3. Customize the plot as follows: - Add a secondary y-axis for total monthly sales. - Use a colormap for the lines representing different products. - Include error bars representing the standard deviation of sales for each product. 4. Save the plot as an image file. **Dataset Setup:** Generate a DataFrame `df_sales` with the following conditions: 1. Index: Date range from `2023-01-01` to `2023-12-31` with monthly frequency. 2. Columns: `\'Product_A\'`, `\'Product_B\'`, `\'Product_C\'` - each column containing random sales values. 3. Add two columns: `\'Total_Sales\'` (sum of all product sales for each month) and `\'Standard_Deviation\'` (standard deviation of product sales for each month). **Constraints:** 1. Use `numpy` for generating random sales data. 2. Seed the random number generator with `42` for consistency. 3. The plot should have a title, x and y labels, and a legend. 4. The secondary y-axis should be labeled as \\"Total Sales\\". 5. Use appropriate error bars for each product line. **Expected Input:** None (data is generated within the script). **Expected Output:** - A line plot of monthly sales with the specified customizations. - Save the plot as an image file named `monthly_sales_plot.png`. **Sample Output:** A file `monthly_sales_plot.png` saved to your working directory. ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt # Step 1: Generate DataFrame np.random.seed(42) dates = pd.date_range(start=\'2023-01-01\', end=\'2023-12-31\', freq=\'M\') data = np.random.randint(100, 400, size=(12, 3)) df_sales = pd.DataFrame(data, columns=[\'Product_A\', \'Product_B\', \'Product_C\'], index=dates) # Step 2: Calculate Total Sales and Standard Deviation df_sales[\'Total_Sales\'] = df_sales.sum(axis=1) df_sales[\'Standard_Deviation\'] = df_sales[[\'Product_A\', \'Product_B\', \'Product_C\']].std(axis=1) # Step 3: Create Line Plot fig, ax = plt.subplots() df_sales[[\'Product_A\', \'Product_B\', \'Product_C\']].plot(ax=ax, colormap=\'plasma\', yerr=df_sales[\'Standard_Deviation\']) ax.set_ylabel(\'Sales\') ax.set_title(\'Monthly Sales of Products\') # Secondary y-axis ax2 = ax.twinx() df_sales[\'Total_Sales\'].plot(ax=ax2, color=\'grey\', linestyle=\'--\', linewidth=2, label=\'Total Sales\') ax2.set_ylabel(\'Total Sales\') # Format Plot fig.legend(loc=\'upper left\', bbox_to_anchor=(0.1, 0.9)) plt.savefig(\'monthly_sales_plot.png\') plt.show() ``` Ensure your code meets the problem statement requirements and consider edge cases (e.g., NaN values, plotting issues).","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt # Step 1: Generate DataFrame np.random.seed(42) dates = pd.date_range(start=\'2023-01-01\', end=\'2023-12-31\', freq=\'M\') data = np.random.randint(100, 400, size=(12, 3)) df_sales = pd.DataFrame(data, columns=[\'Product_A\', \'Product_B\', \'Product_C\'], index=dates) # Step 2: Calculate Total Sales and Standard Deviation df_sales[\'Total_Sales\'] = df_sales.sum(axis=1) df_sales[\'Standard_Deviation\'] = df_sales[[\'Product_A\', \'Product_B\', \'Product_C\']].std(axis=1) # Step 3: Create Line Plot fig, ax = plt.subplots() df_sales[[\'Product_A\', \'Product_B\', \'Product_C\']].plot(ax=ax, colormap=\'plasma\', yerr=df_sales[\'Standard_Deviation\'], marker=\'o\') ax.set_ylabel(\'Sales\') ax.set_title(\'Monthly Sales of Products\') # Secondary y-axis ax2 = ax.twinx() df_sales[\'Total_Sales\'].plot(ax=ax2, color=\'grey\', linestyle=\'--\', linewidth=2, label=\'Total Sales\') ax2.set_ylabel(\'Total Sales\') # Format Plot ax.legend(loc=\'upper left\', bbox_to_anchor=(0.1, 0.9)) # Save plot as an image file plt.savefig(\'monthly_sales_plot.png\') plt.show()"},{"question":"**Problem Statement: Task Manager Simulator** You are required to implement a task manager simulator using Python\'s `asyncio` library. The task manager should be able to schedule multiple asynchronous tasks, compute results concurrently, manage timeouts, and ensure the proper handling of cancellations. The purpose of this exercise is to assess your comprehension and application of various `asyncio` concepts. **Requirements:** 1. **Function Signature:** Implement a function `async def task_manager(task_specs: List[Tuple[int, str]]) -> Dict[str, Any]:` 2. **Input:** * `task_specs` (List of Tuples): Each tuple consists of two elements: - An integer `delay` representing the time in seconds the task should take to complete. - A string `task_name` which is the name of the task. 3. **Output:** * A dictionary containing the following keys: - `\\"results\\"`: A list of results of completed tasks in the order they complete. - `\\"exceptions\\"`: Metadata about tasks that threw exceptions (including timeouts and cancellations). - `\\"order_of_completion\\"`: The order in which tasks completed or were cancelled. 4. **Constraints and Rules:** * Each task should \\"fake\\" doing some work by sleeping for the given `delay`. * Run all tasks concurrently. * Any task that takes longer than 5 seconds to complete should be automatically cancelled. * Tasks should be scheduled in such a way that cancellations (including timeouts) do not block the completion of other tasks. * Handle potential asyncio.TimeoutError and asyncio.CancelledError exceptions. 5. **Performance:** * Ensure that the solution uses the asyncio event loop efficiently and does not block it unnecessarily. **Example:** ```python import asyncio from typing import List, Tuple, Dict, Any async def task_manager(task_specs: List[Tuple[int, str]]) -> Dict[str, Any]: # Your implementation here # Example of expected structure of results: return { \\"results\\": [\\"result_of_task_1\\", \\"result_of_task_2\\"], \\"exceptions\\": [{\\"task_name\\": \\"task_3\\", \\"exception_type\\": \\"TimeoutError\\"}], \\"order_of_completion\\": [\\"task_1\\", \\"task_2\\", \\"task_3\\"] } # Running the function with asyncio async def run_example(): tasks = [(3, \\"task_1\\"), (6, \\"task_2\\"), (1, \\"task_3\\")] result = await task_manager(tasks) print(result) asyncio.run(run_example()) ``` **Expected Output:** ```json { \\"results\\": [\\"task_3\\", \\"task_1\\"], \\"exceptions\\": [{\\"task_name\\": \\"task_2\\", \\"exception_type\\": \\"TimeoutError\\"}], \\"order_of_completion\\": [\\"task_3\\", \\"task_1\\", \\"task_2\\"] } ``` **Notes:** - Ensure to catch exceptions and handle them properly as described in the requirements. - Use `asyncio.gather()` to run given tasks concurrently while handling exceptions. - Favor structured concurrency and use the provided asyncio methods effectively.","solution":"import asyncio from typing import List, Tuple, Dict, Any async def simulated_task(delay: int, task_name: str) -> str: await asyncio.sleep(delay) return task_name async def task_wrapper(delay: int, task_name: str): try: result = await asyncio.wait_for(simulated_task(delay, task_name), timeout=5) return (result, None) except asyncio.TimeoutError: return (None, {\\"task_name\\": task_name, \\"exception_type\\": \\"TimeoutError\\"}) except asyncio.CancelledError: return (None, {\\"task_name\\": task_name, \\"exception_type\\": \\"CancelledError\\"}) async def task_manager(task_specs: List[Tuple[int, str]]) -> Dict[str, Any]: tasks = [task_wrapper(delay, task_name) for delay, task_name in task_specs] completed, pending = await asyncio.wait(tasks) results = [] exceptions = [] order_of_completion = [] for task in completed: result, exception = await task if result: results.append(result) if exception: exceptions.append(exception) order_of_completion.append(result if result else exception[\\"task_name\\"]) return { \\"results\\": results, \\"exceptions\\": exceptions, \\"order_of_completion\\": order_of_completion } # Example to test the module with asyncio # async def run_example(): # tasks = [(3, \\"task_1\\"), (6, \\"task_2\\"), (1, \\"task_3\\")] # result = await task_manager(tasks) # print(result) # asyncio.run(run_example())"},{"question":"You are tasked with building a utility script for inspecting Python packages installed in the current environment. This utility should provide a summary of a package’s metadata, entry points, and file listing. Specifically, you need to implement a function `inspect_package(package_name: str) -> dict` which takes the name of a package as input and returns a dictionary with the following information: - The version of the package. - Metadata of the package. - Entry points for the `console_scripts` group. - Files installed by the package. Your function should utilize the `importlib.metadata` API to gather this information. # Function Signature ```python def inspect_package(package_name: str) -> dict: ``` # Input - `package_name` (str): The name of the package to inspect. # Output - A dictionary with keys: `version`, `metadata`, `console_scripts`, `files`, each containing relevant details about the package. - `version` (str): The version of the package. - `metadata` (dict): Metadata of the package as a dictionary. - `console_scripts` (dict): Dictionary of entry points in the `console_scripts` group. - `files` (list): List of file paths installed by the package. # Example ```python result = inspect_package(\'wheel\') print(result) # Output might be: # { # \'version\': \'0.32.3\', # \'metadata\': {\'Metadata-Version\': \'2.1\', \'Name\': \'wheel\', ... }, # \'console_scripts\': {\'wheel\': \'<function main at 0x...>\'}, # \'files\': [\'wheel/__init__.py\', \'wheel/bdist_wheel.py\', ... ] # } ``` # Constraints - Assume the package is already installed in the Python environment. - Handle any potential exceptions that may arise if metadata or files are not found. - Metadata keys and file paths should be precisely as available within the installed package context. # Note - Use the `importlib.metadata` API methods such as `version`, `metadata`, `entry_points`, and `files` to retrieve the necessary information. # Performance Considerations - The function should efficiently handle the extraction of data without significant latency, suitable for interactive use cases or automated scripts relying on quick inspection of multiple packages.","solution":"import importlib.metadata def inspect_package(package_name: str) -> dict: Inspects a Python package and returns a dictionary with its version, metadata, console scripts, and installed files. Args: package_name (str): Name of the package to inspect. Returns: dict: Dictionary containing the package\'s version, metadata, console scripts, and files. try: # Get version of the package version = importlib.metadata.version(package_name) # Get metadata of the package metadata = dict(importlib.metadata.metadata(package_name)) # Get entry points for the console_scripts group entry_points = importlib.metadata.entry_points() console_scripts = { ep.name: ep.value for ep in entry_points.get(\'console_scripts\', []) } # Get files installed by the package distribution = importlib.metadata.distribution(package_name) files = [str(file) for file in distribution.files] return { \'version\': version, \'metadata\': metadata, \'console_scripts\': console_scripts, \'files\': files } except importlib.metadata.PackageNotFoundError: return { \'version\': None, \'metadata\': {}, \'console_scripts\': {}, \'files\': [] }"},{"question":"# Question: Title: Implementing a Custom Set Class Description: In this exercise, you are required to implement a custom set class that inherits from the `collections.abc.MutableSet` abstract base class. This custom set should provide all functionalities of a mutable set, such as adding, removing, and checking elements. Additionally, it should efficiently handle the uniqueness of elements while maintaining the insertion order. Requirements: 1. **Class Name**: `OrderedSet` 2. **Inheritance**: The class should inherit from `collections.abc.MutableSet`. 3. **Methods to Implement**: - `__init__(self, iterable=None)`: Initializes the set. It should accept an optional iterable to initialize the set. - `__contains__(self, value)`: Checks if the value is in the set. - `__iter__(self)`: Returns an iterator over the set. - `__len__(self)`: Returns the number of elements in the set. - `add(self, value)`: Adds a value to the set. - `discard(self, value)`: Removes a value from the set if it is present. - `__repr__(self)`: Returns the string representation of the set. Constraints: - The elements in the set should maintain their insertion order. Input and Output Examples: Examples: ```python # Example usage of the OrderedSet class # Creating an OrderedSet oset = OrderedSet([1, 2, 3]) print(oset) # Output: OrderedSet([1, 2, 3]) # Adding elements oset.add(4) print(oset) # Output: OrderedSet([1, 2, 3, 4]) # Checking membership print(2 in oset) # Output: True print(5 in oset) # Output: False # Iterating over the set for element in oset: print(element) # Output: 1 2 3 4 # Getting the length of the set print(len(oset)) # Output: 4 # Removing elements oset.discard(3) print(oset) # Output: OrderedSet([1, 2, 4]) oset.discard(5) # Should not raise an error print(oset) # Output: OrderedSet([1, 2, 4]) ``` Note: - You are not allowed to use Python’s built-in `set` directly within the `OrderedSet` class. - Your implementation should strive for efficiency, maintaining the properties of a set while keeping the insertion order of elements. **Good luck!**","solution":"from collections.abc import MutableSet class OrderedSet(MutableSet): def __init__(self, iterable=None): self._items = [] self._set = set() if iterable: for item in iterable: self.add(item) def __contains__(self, value): return value in self._set def __iter__(self): return iter(self._items) def __len__(self): return len(self._set) def add(self, value): if value not in self._set: self._set.add(value) self._items.append(value) def discard(self, value): if value in self._set: self._set.remove(value) self._items.remove(value) def __repr__(self): return f\\"OrderedSet({self._items})\\""},{"question":"# Garbage Collection Management and Debugging You are given a complex program that occasionally runs out of memory and exhibits a high number of uncollectable objects. Your task is to implement a function that can help manage garbage collection more effectively and analyze the uncollectable objects. Implement the function `manage_gc(debug=True)` that: 1. **Disables** automatic garbage collection to prevent it from running during critical program operations. 2. Sets custom threshold values to control the frequency of garbage collection. 3. **Manually triggers** garbage collection to clean up as much memory as possible. 4. If the `debug` parameter is `True`, enables debugging flags to print detailed information about the collected, uncollectable objects and the garbage list. 5. Gathers and **returns statistics** about the garbage collection process in the following format: ```python { \\"collections\\": [int, int, int], \\"collected\\": [int, int, int], \\"uncollectable\\": [int, int, int], \\"garbage\\": list, } ``` Where: - `\\"collections\\"`: Number of times each generation was collected. - `\\"collected\\"`: Number of objects collected in each generation. - `\\"uncollectable\\"`: Number of objects found to be uncollectable in each generation. - `\\"garbage\\"`: List of uncollectable objects currently in the `gc.garbage`. Expected Input: - `debug`: A boolean value to enable/disable debugging mode. Default is `True`. Expected Output: - A dictionary containing collection statistics and the list of uncollectable objects. Example: ```python def manage_gc(debug=True): # Your implementation here # Example usage stats = manage_gc() print(stats) ``` Constraints: - Ensure that your implementation does not significantly interfere with normal program operations. Notes: - Use the functions provided in the `gc` module to implement the required functionality. - Refer to the `gc` documentation provided for detailed information on each function and constant required.","solution":"import gc def manage_gc(debug=True): Manages garbage collection and returns statistics. Param: debug -- If True, enables debugging output. Returns: Dictionary containing statistics about collections, collected objects, uncollectable objects and the current garbage list. # Disable automatic garbage collection gc.disable() # Set custom threshold for garbage collection (modifying these based on heuristics/environment might be needed) gc.set_threshold(700, 10, 10) # Collect garbage manually collected_stats = gc.collect() if debug: # Enable debugging gc.set_debug(gc.DEBUG_STATS | gc.DEBUG_COLLECTABLE | gc.DEBUG_UNCOLLECTABLE) # Gathering garbage collection statistics collections = gc.get_count() uncollectable = len(gc.garbage) # Disable debugging after use to avoid performance impact on the rest of the program gc.set_debug(0) else: collections = gc.get_count() uncollectable = len(gc.garbage) # Compile statistics stats = { \\"collections\\": collections, \\"collected\\": gc.get_stats(), \\"uncollectable\\": [uncollectable], \\"garbage\\": gc.garbage, } # Re-enable automatic garbage collection gc.enable() # Return statistics return stats"},{"question":"You have been tasked with analyzing user password security and providing insights about users\' password change policies within a Unix system using the \\"spwd\\" module. Implement a function `analyze_password_policies` that performs the following tasks: 1. Retrieve all shadow password entries using `spwd.getspall()`. 2. Create a dictionary where each key is a user\'s login name (`sp_namp`) and the value is another dictionary containing: - `last_change`: The number of days since the last password change (`sp_lstchg`). - `min_days`: The minimum number of days required between password changes (`sp_min`). - `max_days`: The maximum number of days allowed between password changes (`sp_max`). - `warn_days`: The number of days before expiration to warn the user to change their password (`sp_warn`). - `inactive_days`: The number of days after password expiration when the account becomes inactive (`sp_inact`). - `expire_days`: The expiration date for the account (`sp_expire`). 3. Create another dictionary that summarizes the common password policies: - `avg_min_days`: The average of the minimum days between password changes. - `avg_max_days`: The average of the maximum days between password changes. - `avg_warn_days`: The average number of days users are warned before password expiration. - `accounts_with_strong_policy`: The count of accounts where the maximum days between password changes (`sp_max`) is less than a specified threshold (e.g., 60 days). Your function should return a tuple of these two dictionaries. # Function Signature ```python def analyze_password_policies(threshold: int) -> (dict, dict): pass ``` # Example ```python result = analyze_password_policies(60) print(result) ``` # Constraints - You must have sufficient privileges to access the shadow password database. - Handle potential exceptions where the `spwd` module functions might fail due to insufficient permissions or missing entries. # Notes - Make sure to handle potential `PermissionError` and `KeyError` exceptions. - Assume that some values like `sp_min`, `sp_max`, etc., could be negative, which represents the absence of a particular policy.","solution":"import spwd from statistics import mean def analyze_password_policies(threshold: int) -> (dict, dict): try: shadow_entries = spwd.getspall() except PermissionError: raise PermissionError(\\"Insufficient permissions to access the shadow password database.\\") user_policies = {} min_days_list = [] max_days_list = [] warn_days_list = [] accounts_with_strong_policy = 0 for entry in shadow_entries: user_policies[entry.sp_nam] = { \'last_change\': entry.sp_lstchg, \'min_days\': entry.sp_min, \'max_days\': entry.sp_max, \'warn_days\': entry.sp_warn, \'inactive_days\': entry.sp_inact, \'expire_days\': entry.sp_expire } if entry.sp_min >= 0: min_days_list.append(entry.sp_min) if entry.sp_max >= 0: max_days_list.append(entry.sp_max) if entry.sp_max < threshold: accounts_with_strong_policy += 1 if entry.sp_warn >= 0: warn_days_list.append(entry.sp_warn) summary = { \'avg_min_days\': mean(min_days_list) if min_days_list else 0, \'avg_max_days\': mean(max_days_list) if max_days_list else 0, \'avg_warn_days\': mean(warn_days_list) if warn_days_list else 0, \'accounts_with_strong_policy\': accounts_with_strong_policy } return user_policies, summary"},{"question":"Objective You are required to demonstrate your understanding of managing and utilizing color palettes using the Seaborn package in Python. Task Write a function `generate_colormap_samples` that generates and returns multiple colormap samples based on the given input parameters. Function Signature ```python def generate_colormap_samples(palettes: list[str], samples: int, as_cmap: bool) -> dict: pass ``` Parameters - `palettes` (list of str): A list of colormap names (e.g., [\\"viridis\\", \\"Set2\\", ...]). - `samples` (int): The number of color samples to generate from each colormap. If `as_cmap` is True, this parameter will be ignored. - `as_cmap` (bool): If True, return the continuous colormap object instead of sampled colors. Returns - dict: A dictionary where the keys are the colormap names and the values are either the lists of color samples or the continuous colormap objects, based on `as_cmap`. Example ```python # When as_cmap is False result = generate_colormap_samples([\\"viridis\\", \\"Set2\\"], 4, False) assert result == { \'viridis\': [(0.267004, 0.004874, 0.329415), (0.190631, 0.407061, 0.556089), (0.20803, 0.718701, 0.472873), (0.993248, 0.906157, 0.143936)], \'Set2\': [(0.4, 0.760784, 0.6470588), (0.9882353, 0.5529412, 0.38431373), (0.5529412, 0.627451, 0.79607844), (0.90588236, 0.5411765, 0.7647059)] } # When as_cmap is True result = generate_colormap_samples([\\"viridis\\"], 4, True) assert isinstance(result[\\"viridis\\"], matplotlib.colors.ListedColormap) ``` Constraints 1. You must use the `seaborn.mpl_palette` function to generate the colormap samples. 2. If the palette name is not valid, it should be skipped from the result. 3. If `samples` is <= 0 and `as_cmap` is False, return an empty dictionary. Note: You may need to import required packages to appear in compatibility with the codes. Additional Information Please ensure your implementation handles edge cases and invalid inputs gracefully.","solution":"import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap def generate_colormap_samples(palettes: list[str], samples: int, as_cmap: bool) -> dict: Generates and returns multiple colormap samples or continuous colormaps. Parameters: - palettes (list of str): A list of colormap names (e.g., [\\"viridis\\", \\"Set2\\", ...]). - samples (int): The number of color samples to generate from each colormap if as_cmap is False. - as_cmap (bool): If True, return the continuous colormap object instead of sampled colors. Returns: - dict: A dictionary where keys are colormap names and values are lists of color samples or continuous colormap objects. result = {} for palette in palettes: try: if as_cmap: result[palette] = sns.color_palette(palette, as_cmap=True) else: if samples > 0: result[palette] = sns.color_palette(palette, samples) except ValueError: # Skip invalid palettes continue return result"},{"question":"# Advanced Context Manager with Contextlib Objective Design a context manager that ensures resources are acquired and released correctly, even when exceptions occur, demonstrating your understanding of the `contextlib` module. Requirements 1. **Function Implementation**: - Create a context manager using the `contextlib.contextmanager` decorator named `manage_database_connection`. - This context manager should: - Acquire a hypothetical database connection (e.g., using a mock function `open_database_connection()`). - Yield the connection object for use within the `with` block. - Ensure that the connection is gracefully closed (e.g., using a mock function `close_database_connection()`) when the `with` block is exited, even if an exception occurs. 2. **Exception Handling**: - If an unhandled exception occurs within the `with` block, it should be logged using a mock logging function `log_exception()` but should not suppress the exception. Input and Output Format: - **Input**: This will be determined by the specific resource or database connection mechanics, but assume basic mock functions provided as part of the question. - **Output**: Proper resource management and exception handling as described. Constraints: - Use only `contextlib.contextmanager` for this implementation. - Mock the resource acquisition and release functions within the provided code to simulate real-world behavior without needing actual database operations. # Example Provide an example of usage and expected behavior: ```python from contextlib import contextmanager # Mock functions for demonstration def open_database_connection(): print(\\"Database connection opened\\") return \\"db_connection\\" def close_database_connection(conn): print(f\\"Database connection {conn} closed\\") def log_exception(exc): print(f\\"Exception Logged: {exc}\\") @contextmanager def manage_database_connection(): # Acquire resource (mock implementation) conn = open_database_connection() try: # Yield resource to be used in \'with\' block yield conn except Exception as exc: # Log exception log_exception(exc) # Re-raise exception to ensure it is not suppressed raise finally: # Ensure resource is released close_database_connection(conn) # Example Usage try: with manage_database_connection() as conn: print(f\\"Using {conn}\\") raise ValueError(\\"An error occurred while using the connection\\") except ValueError: print(\\"Caught ValueError after logging\\") # Expected Output: # Database connection opened # Using db_connection # Exception Logged: An error occurred while using the connection # Database connection db_connection closed # Caught ValueError after logging ``` # Evaluation Criteria: - Correct implementation of `contextlib.contextmanager` for managing a resource. - Proper yield and cleanup logic. - Appropriate exception handling and logging without suppressing the exception. - The code should demonstrate the expected behavior as given in the example usage.","solution":"from contextlib import contextmanager # Mock functions for demonstration def open_database_connection(): print(\\"Database connection opened\\") return \\"db_connection\\" def close_database_connection(conn): print(f\\"Database connection {conn} closed\\") def log_exception(exc): print(f\\"Exception Logged: {exc}\\") @contextmanager def manage_database_connection(): # Acquire resource (mock implementation) conn = open_database_connection() try: # Yield resource to be used in \'with\' block yield conn except Exception as exc: # Log exception log_exception(exc) # Re-raise exception to ensure it is not suppressed raise finally: # Ensure resource is released close_database_connection(conn)"},{"question":"You are tasked with implementing a custom graph transformation using PyTorch\'s FX framework. Your goal is to transform a given computational graph such that specific arithmetic operations are either modified or replaced to optimize the performance of the model. Task: 1. **Transformation Overview**: - Identify and replace all occurrences of `torch.ops.aten.sub.Tensor` with two operations: first, negate the second operand using `torch.ops.aten.neg.default`, and then add the result to the first operand using `torch.ops.aten.add.Tensor`. 2. **Requirements**: - Implement a transformer class `ReplaceSubWithNegAdd` that extends `torch.fx.Transformer`. - In the `call_function` method, detect calls to `torch.ops.aten.sub.Tensor` and replace them with the proposed transformation. - Return the modified `GraphModule`. 3. **Input**: - A PyTorch `GraphModule` object which includes the FX graph to be transformed. 4. **Output**: - A transformed `GraphModule` object where all `torch.ops.aten.sub.Tensor` operations have been replaced as specified. 5. **Constraints**: - Your implementation must correctly handle various input graphs, including those with multiple subtractions, nested operations, and varying operand types. - Maintain the integrity of the original graph to ensure no other nodes are inadvertently modified. Example: ```python import torch import torch.fx class ReplaceSubWithNegAdd(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.ops.aten.sub.Tensor: x, y = args neg_y = super().call_function(torch.ops.aten.neg.default, (y,), {}) return super().call_function(torch.ops.aten.add.Tensor, (x, neg_y), {}) return super().call_function(target, args, kwargs) # Example usage class MyModule(torch.nn.Module): def forward(self, x, y): return x - y # Trace the module to create a GraphModule traced_module = torch.fx.symbolic_trace(MyModule()) # Transform the graph transformer = ReplaceSubWithNegAdd(traced_module) transformed_gm = transformer.transform() # Verify transformation print(transformed_gm.graph) transformed_gm.graph.print_tabular() ``` Notes: - The provided `MyModule` is a simple example to illustrate the transformation. Your solution should handle graphs of similar or higher complexity. - Ensure to test your implementation with diverse GraphModule inputs to confirm its robustness.","solution":"import torch import torch.fx class ReplaceSubWithNegAdd(torch.fx.Transformer): def call_function(self, target, args, kwargs): if target == torch.ops.aten.sub.Tensor: x, y = args neg_y = super().call_function(torch.ops.aten.neg.default, (y,), {}) return super().call_function(torch.ops.aten.add.Tensor, (x, neg_y), {}) return super().call_function(target, args, kwargs) def transform_module(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: transformer = ReplaceSubWithNegAdd(gm) return transformer.transform()"},{"question":"# XML Analysis and Manipulation Objective: Write a function `modify_xml` that takes an XML string and performs the following operations: 1. Parses the XML string into an ElementTree. 2. Adds a new attribute `status=\\"processed\\"` to every `<country>` element. 3. Removes all `<neighbor>` elements where the `direction` attribute is \\"E\\". 4. Finds the `<country>` element with the name \\"Panama\\" and updates its `rank` element to \\"70\\". 5. Generates a string representation of the modified XML and returns it. Input: * An XML string that represents countries and their neighbors. Output: * A string representing the modified XML document. Example: ``` Input: <?xml version=\\"1.0\\"?> <data> <country name=\\"Liechtenstein\\"> <rank>1</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\\"Austria\\" direction=\\"E\\"/> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\"> <rank>4</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> <country name=\\"Panama\\"> <rank>68</rank> <year>2011</year> <gdppc>13600</gdppc> <neighbor name=\\"Costa Rica\\" direction=\\"W\\"/> <neighbor name=\\"Colombia\\" direction=\\"E\\"/> </country> </data> Output: <data> <country name=\\"Liechtenstein\\" status=\\"processed\\"> <rank>1</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\" status=\\"processed\\"> <rank>4</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> <country name=\\"Panama\\" status=\\"processed\\"> <rank>70</rank> <year>2011</year> <gdppc>13600</gdppc> <neighbor name=\\"Costa Rica\\" direction=\\"W\\"/> </country> </data> ``` Constraints: * The XML string is guaranteed to be well-formed. * The XML structure is similar to the provided sample. # Function Signature: ```python def modify_xml(xml_string: str) -> str: pass ``` # Notes: * Use the `xml.etree.ElementTree` module for XML parsing and manipulation. * Make sure to preserve the order and structure of the original XML while modifying it as specified above. * Implement the function and test it with the provided example to ensure correctness.","solution":"import xml.etree.ElementTree as ET def modify_xml(xml_string: str) -> str: # Parse the XML string into an ElementTree root = ET.fromstring(xml_string) for country in root.findall(\'country\'): # Add a new attribute status=\\"processed\\" to every <country> element country.set(\'status\', \'processed\') # Remove all <neighbor> elements where the direction attribute is \\"E\\" neighbors = country.findall(\'neighbor\') for neighbor in neighbors: if neighbor.get(\'direction\') == \'E\': country.remove(neighbor) # Find the <country> element with the name \\"Panama\\" if country.get(\'name\') == \'Panama\': rank = country.find(\'rank\') # Update its rank element to \\"70\\" if rank is not None: rank.text = \'70\' # Generate a string representation of the modified XML and return it return ET.tostring(root, encoding=\'unicode\')"},{"question":"You are required to implement a Python function using the `telnetlib` module to interact with a Telnet server. This function will connect to a server, perform specific read and write operations, handle connection status, and manage option negotiations. # Function Signature ```python def telnet_interaction(host: str, port: int, user: str, password: str) -> str: Connects to a Telnet server, logs in with provided credentials, navigates to a specified directory, lists its contents, and handles option negotiation. Parameters: - host (str): The hostname or IP address of the Telnet server. - port (int): The port number of the Telnet server. - user (str): The username for logging into the Telnet server. - password (str): The password for logging into the Telnet server. Returns: - str: The list of files and directories in the specified directory as a string. # Your implementation here ``` # Requirements 1. **Connection:** - Connect to the provided `host` and `port` using the `telnetlib.Telnet` class. - Ensure proper handling of connection timeouts and errors. 2. **Login:** - Read until the login prompt (`\\"login: \\"`), send the `user` value. - Read until the password prompt (`\\"Password: \\"`), send the `password` value. 3. **Commands:** - Navigate to a specified directory (you can choose any directory for the purpose of this assignment, e.g., `\\"/etc\\"`). - List the contents of the directory (e.g., using the `ls` command). 4. **Read Response:** - Read the entire response from the server until EOF or until a specified termination string. - Ensure to handle partial reads and timeouts correctly. 5. **Cleanup:** - Close the connection properly ensuring all resources are freed. 6. **Option Negotiation:** - Implement a callback function to handle any option negotiations received from the server. # Example Usage ```python result = telnet_interaction(\\"127.0.0.1\\", 23, \\"user123\\", \\"pass123\\") print(result) # Should print the list of files and directories in the specified directory ``` # Constraints - Only use the methods and classes provided by `telnetlib`. - Handle corner cases such as incorrect login credentials, connection errors, etc. - Ensure all bytes are handled and converted correctly between bytes and strings. # Notes Ensure your code is clean, handles exceptions properly, and follows good programming practices. # Performance - The function should be capable of handling moderate delays in network communication. - It should efficiently manage the connection to ensure minimal overhead.","solution":"import telnetlib def telnet_interaction(host: str, port: int, user: str, password: str) -> str: Connects to a Telnet server, logs in with provided credentials, navigates to a specified directory, lists its contents, and handles option negotiation. Parameters: - host (str): The hostname or IP address of the Telnet server. - port (int): The port number of the Telnet server. - user (str): The username for logging into the Telnet server. - password (str): The password for logging into the Telnet server. Returns: - str: The list of files and directories in the specified directory as a string. try: telnet = telnetlib.Telnet(host, port, timeout=10) telnet.read_until(b\\"login: \\") telnet.write(user.encode(\'ascii\') + b\\"n\\") telnet.read_until(b\\"Password: \\") telnet.write(password.encode(\'ascii\') + b\\"n\\") telnet.write(b\\"cd /etcn\\") telnet.write(b\\"lsn\\") output = telnet.read_all().decode(\'ascii\') telnet.close() return output except Exception as e: return str(e)"},{"question":"You have been provided with the `tkinter.colorchooser` module, which allows users to pick colors via a native color picker dialog. Your task is to implement a Python function that creates a simple GUI application where the user can click a button to choose a color, and the selected color will be displayed in a `Label` widget as a solid background color. # Function Signature ```python def create_color_picker_app(): pass ``` # Expected Behavior - The function `create_color_picker_app` should create a `tkinter` window with the following components: - A single button labeled \\"Choose Color\\". - A label that shows the selected color\'s hexadecimal value and whose background color changes to the selected color. - When the user clicks the \\"Choose Color\\" button, the `tkinter.colorchooser.askcolor` function should open the color picker dialog. - After the user selects a color and confirms their choice, the label should update to display the selected color\'s hexadecimal value and change its background to that color. - If the user cancels the color picker dialog, no changes should be made to the label. # Input - No direct input is required since this function sets up a GUI window. # Output - This function does not return any value. It just sets up a GUI application. # Constraints - You should handle the Tkinter main loop within the function. - Use meaningful variable names and add necessary comments for readability. # Example ```python def create_color_picker_app(): import tkinter as tk from tkinter import colorchooser def choose_color(): color_code = colorchooser.askcolor(title=\\"Choose a color\\") if color_code[1]: label.config(text=color_code[1], bg=color_code[1]) root = tk.Tk() root.title(\\"Color Picker App\\") button = tk.Button(root, text=\\"Choose Color\\", command=choose_color) button.pack(pady=20) label = tk.Label(root, text=\\"No color selected\\", width=30, height=10) label.pack(pady=20) root.mainloop() # The function can be invoked to start the application like so: # create_color_picker_app() ``` Implement the `create_color_picker_app` function based on the example provided and ensure it meets the specified requirements.","solution":"def create_color_picker_app(): import tkinter as tk from tkinter import colorchooser def choose_color(): color_code = colorchooser.askcolor(title=\\"Choose a color\\") if color_code[1]: label.config(text=color_code[1], bg=color_code[1]) root = tk.Tk() root.title(\\"Color Picker App\\") button = tk.Button(root, text=\\"Choose Color\\", command=choose_color) button.pack(pady=20) label = tk.Label(root, text=\\"No color selected\\", width=30, height=10) label.pack(pady=20) root.mainloop() # To start the application, you would call the function: # create_color_picker_app()"},{"question":"You are tasked with automating the creation of a `setup.cfg` file for a Python project. The `setup.cfg` file should be customizable based on user input, allowing them to specify different options for various Distutils commands. Requirements: 1. Implement a function `create_setup_cfg(configurations: dict) -> None` that generates a `setup.cfg` file. - The `configurations` dictionary will have command names as keys and their respective options as value dictionaries. - Each option dictionary contains option names as keys and their corresponding values as values. Input Format: - A single dictionary `configurations` where: ```python { \\"command_1\\": { \\"option_1\\": \\"value_1\\", \\"option_2\\": \\"value_2\\" }, \\"command_2\\": { \\"option_1\\": \\"value_1\\" }, ... } ``` Constraints: - Assume the options and values are all strings. - Handle multiple commands and multiple options for each command. - Generate a properly formatted `setup.cfg` file based on the input dictionary. Output: - There is no return value. The function should create a file named `setup.cfg` in the current directory with the following format: ``` [command_1] option_1=value_1 option_2=value_2 [command_2] option_1=value_1 ``` # Example: Input: ```python configurations = { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": \\"/path/to/headers\\" }, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"John Doe <john.doe@example.com>\\" } } ``` Function Call: ```python create_setup_cfg(configurations) ``` Generated `setup.cfg` file: ``` [build_ext] inplace=1 include_dirs=/path/to/headers [bdist_rpm] release=1 packager=John Doe <john.doe@example.com> ``` # Additional Task: - Ensure your function handles indentation for multi-line values properly if any option\'s value spans multiple lines. Hints: - Use the `open()` function with appropriate flags to create and write to the file. - Iterate through the dictionary items to generate the required sections and options.","solution":"def create_setup_cfg(configurations): Generates a setup.cfg file based on the provided configurations dictionary. :param configurations: A dictionary where keys are command names and values are dictionaries of options for those commands. with open(\'setup.cfg\', \'w\') as file: for command, options in configurations.items(): file.write(f\'[{command}]n\') for option, value in options.items(): file.write(f\'{option}={value}n\') file.write(\'n\')"},{"question":"You are required to implement a simple asynchronous chat server and client using the `asyncio` and `socket` modules. The communication between the server and clients should be secured using the `ssl` module. Server Requirements: 1. The server should listen on a specified IP address and port. 2. The server should use SSL for secure communication. 3. The server should handle multiple clients asynchronously using `asyncio`. 4. The server should broadcast any message received from a client to all connected clients. Client Requirements: 1. The client should connect to the server using SSL. 2. The client should send user input to the server. 3. The client should display any messages received from the server. # Input and Output Format: - The server should not take any input during runtime. - Clients should take user input as messages and send them to the server. - The server should broadcast received messages to all connected clients. # Function Implementations: You need to implement the following functions: Server Functions: 1. `start_server(ip: str, port: int, certfile: str, keyfile: str) -> None` - Starts the chat server on the given IP address and port with the specified SSL certificate and key files. 2. `handle_client(reader, writer) -> None` - Handles communication with a connected client. Client Functions: 1. `start_client(ip: str, port: int, certfile: str) -> None` - Connects to the chat server on the given IP address and port with the specified SSL certificate. # Constraints: - Use `asyncio` for handling asynchronous I/O operations. - Use `ssl` module to wrap the socket for secure communication. - You can assume that valid IP addresses, ports, and file paths for SSL certificates and keys will be provided. # Sample Usage: ```python # Starting the server # This should be run in a separate terminal or process start_server(\'127.0.0.1\', 8888, \'server.crt\', \'server.key\') # Starting the client # This should also be run in separate terminal(s) or process(es) start_client(\'127.0.0.1\', 8888, \'server.crt\') ``` # Additional Notes: - Ensure that your implementation handles common errors such as connection drops gracefully. - Make use of Python\'s `asyncio.run()` function to run the main server and client coroutines. - You can generate a self-signed SSL certificate and key for testing purposes using the `openssl` command.","solution":"import asyncio import ssl clients = [] async def handle_client(reader, writer): global clients addr = writer.get_extra_info(\'peername\') clients.append(writer) print(f\\"New connection from {addr}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received message from {addr}: {message}\\") for client in clients: if client != writer: client.write(data) await client.drain() except Exception as e: print(f\\"Error handling client {addr}: {e}\\") finally: print(f\\"Connection closed from {addr}\\") clients.remove(writer) writer.close() await writer.wait_closed() async def start_server(ip: str, port: int, certfile: str, keyfile: str): ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) ssl_context.load_cert_chain(certfile, keyfile) server = await asyncio.start_server( handle_client, ip, port, ssl=ssl_context, ) async with server: print(f\\"Server started on {ip}:{port}\\") await server.serve_forever() async def start_client(ip: str, port: int, certfile: str): ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT) ssl_context.check_hostname = False ssl_context.verify_mode = ssl.CERT_NONE ssl_context.load_verify_locations(certfile) reader, writer = await asyncio.open_connection( ip, port, ssl=ssl_context, ) async def send_message(): loop = asyncio.get_event_loop() while True: message = await loop.run_in_executor(None, input, \\"\\") writer.write(message.encode()) await writer.drain() async def receive_message(): while True: data = await reader.read(100) if not data: break print(data.decode()) await asyncio.gather(send_message(), receive_message()) writer.close() await writer.wait_closed() # To run the server, use the following: # asyncio.run(start_server(\'127.0.0.1\', 8888, \'server.crt\', \'server.key\')) # To run a client, use the following: # asyncio.run(start_client(\'127.0.0.1\', 8888, \'server.crt\'))"},{"question":"You are required to develop a small HTTP client utility in Python using the `http.client` module. Your utility should interact with a web server to perform the following tasks: 1. **Send a GET request** to a specified URL. 2. **Send a POST request** with a JSON payload to another specified URL. 3. Extract and print specific information from the responses of both requests. Requirements: 1. **GET Request**: - The function `perform_get_request(url)` performs a GET request to the specified `url`. - Print the HTTP status code and reason phrase from the response. - If the status code is 200, read and print the first 500 characters of the response body. 2. **POST Request**: - The function `perform_post_request(url, data)` performs a POST request to the specified `url` with `data` as the request body. - `data` is a dictionary that should be converted to a JSON string before being sent. - Print the HTTP status code and reason phrase from the response. - If the status code indicates success (200-299), read and print the response body. 3. Create `main()` function to demonstrate the usage of both `perform_get_request` and `perform_post_request` functions with example URLs: - `GET` from: `https://jsonplaceholder.typicode.com/posts/1` - `POST` to: `https://jsonplaceholder.typicode.com/posts` - `data` to send with the POST request: ```python { \\"title\\": \\"foo\\", \\"body\\": \\"bar\\", \\"userId\\": 1 } ``` Function Signatures: ```python import http.client import urllib.parse import json def perform_get_request(url: str) -> None: # Your code here def perform_post_request(url: str, data: dict) -> None: # Your code here def main() -> None: example_get_url = \\"https://jsonplaceholder.typicode.com/posts/1\\" example_post_url = \\"https://jsonplaceholder.typicode.com/posts\\" example_post_data = { \\"title\\": \\"foo\\", \\"body\\": \\"bar\\", \\"userId\\": 1 } print(\\"Performing GET request\\") perform_get_request(example_get_url) print(\\"nPerforming POST request\\") perform_post_request(example_post_url, example_post_data) if __name__ == \\"__main__\\": main() ``` Constraints: - You must use the `http.client` module for sending HTTP requests. - Handle any network errors gracefully by printing an appropriate message. - Ensure the final response reading is done only after checking the successful status code. Example Output: ``` Performing GET request Status: 200 OK Body: { \\"userId\\": 1, \\"id\\": 1, \\"title\\": \\"sunt aut facere repellat provident occaecati excepturi optio reprehenderit\\", \\"body\\": \\"quia et suscipitnsuscipit r... ... Performing POST request Status: 201 Created Response Body: { \\"title\\": \\"foo\\", \\"body\\": \\"bar\\", \\"userId\\": 1, \\"id\\": 101 } ``` **Note**: The exact output will depend on the current implementation of the example URLs used.","solution":"import http.client import urllib.parse import json def perform_get_request(url: str) -> None: parsed_url = urllib.parse.urlparse(url) conn = http.client.HTTPSConnection(parsed_url.netloc) try: conn.request(\\"GET\\", parsed_url.path) response = conn.getresponse() print(f\\"Status: {response.status} {response.reason}\\") if response.status == 200: body = response.read(500).decode() print(f\\"Body: {body}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: conn.close() def perform_post_request(url: str, data: dict) -> None: parsed_url = urllib.parse.urlparse(url) conn = http.client.HTTPSConnection(parsed_url.netloc) headers = {\'Content-type\': \'application/json\'} json_data = json.dumps(data) try: conn.request(\\"POST\\", parsed_url.path, body=json_data, headers=headers) response = conn.getresponse() print(f\\"Status: {response.status} {response.reason}\\") if 200 <= response.status < 300: body = response.read().decode() print(f\\"Response Body: {body}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: conn.close() def main() -> None: example_get_url = \\"https://jsonplaceholder.typicode.com/posts/1\\" example_post_url = \\"https://jsonplaceholder.typicode.com/posts\\" example_post_data = { \\"title\\": \\"foo\\", \\"body\\": \\"bar\\", \\"userId\\": 1 } print(\\"Performing GET request\\") perform_get_request(example_get_url) print(\\"nPerforming POST request\\") perform_post_request(example_post_url, example_post_data) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Coding Challenge Problem Statement You are tasked with creating a custom data container that behaves like a set but with additional constraints and functionalities. This custom container should inherit from `collections.abc.Set` and provide the following functionalities: 1. **No Duplicates:** Automatically reject any duplicate values upon insertion. 2. **Max Size:** Limit the container to hold a maximum of 10 unique elements. Adding additional elements should raise a `ValueError`. 3. **Even Numbers Only:** Accept only even numbers. Any attempt to add an odd number should raise a `ValueError`. Your task is to implement the class `EvenLimitedSet` that satisfies the given constraints and behaves like a set. Class and Methods Implement the following class and methods: ```python import collections.abc class EvenLimitedSet(collections.abc.Set): def __init__(self, iterable=None): \'\'\'Initializes the set with the given iterable.\'\'\' # Your code here def __contains__(self, value): \'\'\'Checks if the value is in the set.\'\'\' # Your code here def __iter__(self): \'\'\'Returns an iterator over the set elements.\'\'\' # Your code here def __len__(self): \'\'\'Returns the number of elements in the set.\'\'\' # Your code here def add(self, value): \'\'\'Adds a new even number to the set. If the set exceeds 10 elements or the value is not even, raises ValueError.\'\'\' # Your code here def discard(self, value): \'\'\'Removes the value from the set if it exists.\'\'\' # Your code here ``` Additional Requirements 1. **Method Detail:** - `__init__(self, iterable)` should initialize the set with the given iterable, ensuring the constraints are met. - `__contains__(self, value)` should return `True` if the set contains the value, `False` otherwise. - `__iter__(self)` should return an iterator over the values in the set. - `__len__(self)` should return the number of values in the set. - `add(self, value)` should add `value` to the set if it is an even number and the set has less than 10 elements. Raise `ValueError` otherwise. - `discard(self, value)` should remove `value` from the set if it exists. 2. **Input Constraints:** - For `__init__`, the input `iterable` is optional. If provided, it can be any iterable containing even numbers. 3. **Output:** - Your class methods should not print anything. They should only return appropriate values or raise exceptions if needed. 4. **Performance Requirements:** - Your implementation should ensure that set operations (`add`, `discard`, `__contains__`, etc.) are efficient and ideally have O(1) complexity for insertion and membership checks. Example Usage ```python # Creating an instance s = EvenLimitedSet([2, 4, 6, 8]) print(len(s)) # Output: 4 # Adding an element s.add(10) print(len(s)) # Output: 5 # Attempt to add duplicate element s.add(4) # No effect, 4 is already in the set # Attempt to add more than 10 elements for i in range(12, 34, 2): s.add(i) # Raises ValueError when adding the 11th element # Removing an element s.discard(4) print(4 in s) # Output: False ``` Implement the class `EvenLimitedSet` meeting the above specifications.","solution":"import collections.abc class EvenLimitedSet(collections.abc.Set): def __init__(self, iterable=None): self._data = set() if iterable is not None: for item in iterable: self.add(item) def __contains__(self, value): return value in self._data def __iter__(self): return iter(self._data) def __len__(self): return len(self._data) def add(self, value): if value % 2 != 0: raise ValueError(\\"Only even numbers are allowed.\\") if value in self._data: return if len(self._data) >= 10: raise ValueError(\\"Cannot exceed 10 elements in the set.\\") self._data.add(value) def discard(self, value): self._data.discard(value)"},{"question":"# Seaborn Coding Challenge You are provided with a dataset containing information about tips received in a restaurant. Your task is to create various insightful visualizations using the `stripplot` function from the Seaborn library to interpret the data better. Follow the instructions below to accomplish the task. Dataset The dataset `tips` can be loaded using: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` Instructions 1. **Basic Strip Plot**: Create a basic strip plot to show the distribution of the total bill amounts (`total_bill`). 2. **Categorical Split Strip Plot**: Create a strip plot to compare the distribution of the total bill amounts (`total_bill`) for different days (`day`). 3. **Hue-Enhanced Strip Plot**: Create a strip plot with `total_bill` on the x-axis, `day` on the y-axis, and color the data points by gender (`sex`) using the `hue` parameter. 4. **Hue with Numeric Variable**: Create a strip plot with `total_bill` on the x-axis, `day` on the y-axis, and color the data points by the size of the group (`size`) using the `hue` parameter. 5. **Multiple Customizations**: Create a strip plot with the following customizations: - `total_bill` on the x-axis. - `day` on the y-axis. - Points colored by `time` (lunch/dinner) using the `hue` parameter. - Disable jitter for the data points. - Use a diamond marker (`D`). - Set the marker size to 20. - Set the marker transparency to 0.1. - Set the marker edge width to 1. 6. **Multi-Facet Strip Plot**: Create a faceted strip plot using `catplot` that splits the data based on: - Columns by `day`. - `total_bill` on the y-axis. - Time (`time`) on the x-axis. - Points colored by `sex`, ensuring facets are synchronized properly. Expected Outputs For each plot, ensure you use appropriate Seaborn and Matplotlib functions to display the plot within a Jupyter Notebook. Input Format There are no direct inputs for this task as you will be loading and working with the `tips` dataset using Seaborn. Constraints - Use only Seaborn functionalities discussed in the provided documentation. - Ensure the visualizations are clear and follow best practices for data visualization. Performance Requirements - Your code should execute without errors. - The plots should be correctly displayed within a reasonable time frame (less than 30 seconds for all plots together). Provide the Python code implementation for this challenge.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") def create_strip_plots(): # Basic Strip Plot plt.figure(figsize=(8, 6)) sns.stripplot(data=tips, x=\'total_bill\') plt.title(\'Basic Strip Plot of Total Bill\') plt.show() # Categorical Split Strip Plot plt.figure(figsize=(8, 6)) sns.stripplot(data=tips, x=\'total_bill\', y=\'day\') plt.title(\'Categorical Split Strip Plot of Total Bill by Day\') plt.show() # Hue-Enhanced Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\'total_bill\', y=\'day\', hue=\'sex\') plt.title(\'Hue-Enhanced Strip Plot of Total Bill by Day and Colored by Gender\') plt.legend(title=\'Gender\') plt.show() # Hue with Numeric Variable plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\'total_bill\', y=\'day\', hue=\'size\') plt.title(\'Strip Plot of Total Bill by Day Colored by Size of the Group\') plt.legend(title=\'Group Size\') plt.show() # Multiple Customizations plt.figure(figsize=(12, 8)) sns.stripplot( data=tips, x=\'total_bill\', y=\'day\', hue=\'time\', jitter=False, marker=\'D\', size=20, alpha=0.1, linewidth=1) plt.title(\'Customized Strip Plot of Total Bill by Day with Time Hue\') plt.legend(title=\'Time\') plt.show() # Multi-Facet Strip Plot g = sns.catplot( data=tips, kind=\'strip\', x=\'time\', y=\'total_bill\', row=\'day\', hue=\'sex\', height=4, aspect=1.2) g.fig.suptitle(\\"Faceted Strip Plot of Total Bill by Day, Split by Time with Gender Hue\\", y=1.02) plt.show()"},{"question":"**Model Publishing and Loading with PyTorch Hub** In this coding assessment, you will demonstrate your understanding of `torch.hub` by publishing a new model and ensuring it can be loaded correctly. You are required to implement an entry point for a custom model, add it to a mock PyTorch Hub repository, and provide sample code to load this model. # Task 1: Define the Custom Model 1. Implement a simple neural network model using PyTorch. 2. Save the model\'s state dictionary to a local file. # Task 2: Create `hubconf.py` 1. Create a `hubconf.py` file that defines an entry point for your custom model. 2. Ensure the entry point can load the model\'s pre-trained weights from the saved state dictionary file. # Task 3: Load the Model using `torch.hub` 1. Provide a sample script to load the model from the `hubconf.py` entry point using `torch.hub.load()`. # Constraints and Requirements: 1. The custom model should be a simple feed-forward neural network. 2. The saved model weights should be less than 2GB. 3. The `hubconf.py` entry point should correctly load and return the model. 4. The sample script should demonstrate the use of `torch.hub.list()` to verify the availability of the model, `torch.hub.help()` to display its docstring, and `torch.hub.load()` to load the model. # Sample Solution Structure Define the Custom Model ```python import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # Create and save model weights model = SimpleNN(10, 5, 2) torch.save(model.state_dict(), \'simplenn_weights.pth\') ``` Create `hubconf.py` ```python dependencies = [\'torch\'] import torch from my_model import SimpleNN def simplenn(pretrained=False, **kwargs): model = SimpleNN(10, 5, 2) if pretrained: model.load_state_dict(torch.load(\'simplenn_weights.pth\')) return model ``` Load the Model using `torch.hub` ```python import torch.hub # List available models print(torch.hub.list(\'/path/to/hubconf/directory\')) # Display help for the simplenn model print(torch.hub.help(\'/path/to/hubconf/directory\', \'simplenn\')) # Load the pre-trained simplenn model model = torch.hub.load(\'/path/to/hubconf/directory\', \'simplenn\', pretrained=True) print(model) ``` # Submission Submit your implementation of the custom model definition, `hubconf.py`, and the sample script demonstrating model loading.","solution":"import torch import torch.nn as nn # Define the custom neural network model class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # Instantiate and save model weights model = SimpleNN(10, 5, 2) torch.save(model.state_dict(), \'simplenn_weights.pth\')"},{"question":"# Advanced Coding Assessment: Custom Distribution and Sampling Objective Demonstrate your understanding of the PyTorch `torch.distributions` module by creating a custom mixture distribution and performing various operations on it. Problem You need to create a custom mixture distribution from a weighted combination of Normal distributions using the `torch.distributions` module in PyTorch. Implement and test the following: 1. **CustomMixtureDistribution Class**: - Create a class `CustomMixtureDistribution` that takes a list of Normal distributions and their corresponding weights. - Implement methods to: - Sample from the mixture distribution. - Compute the log probability of a sample. - Compute the entropy of the mixture distribution. 2. **Implementation Details**: - The constructor should initialize the Normal distributions and the weights. - Ensure that weights provided are normalized to sum up to 1. Function Signatures ```python import torch class CustomMixtureDistribution: def __init__(self, means: torch.Tensor, stds: torch.Tensor, weights: torch.Tensor): Initialize the CustomMixtureDistribution. :param means: A 1D tensor representing the means of the Normal distributions. :param stds: A 1D tensor representing the standard deviations of the Normal distributions. :param weights: A 1D tensor representing the weights of each component in the mixture distribution. pass def sample(self, sample_shape: torch.Size = torch.Size()): Generate a sample from the mixture distribution. :param sample_shape: The shape of the sample to generate. :return: A sample from the mixture distribution. pass def log_prob(self, value: torch.Tensor): Compute the log probability of a sample. :param value: A sample for which to compute the log probability. :return: The log probability of the sample. pass def entropy(self): Compute the entropy of the mixture distribution. :return: The entropy of the mixture distribution. pass # Example of using the CustomMixtureDistribution means = torch.tensor([0.0, 2.0, 5.0]) stds = torch.tensor([1.0, 0.5, 1.5]) weights = torch.tensor([0.2, 0.5, 0.3]) mixture = CustomMixtureDistribution(means, stds, weights) # Generate a sample sample = mixture.sample() # Compute log probability of a sample log_prob = mixture.log_prob(sample) # Compute the entropy of the mixture distribution entropy_value = mixture.entropy() ``` Constraints - You must use PyTorch\'s `torch.distributions` module. - Ensure that the distribution weights are normalized inside the constructor. - The methods should be efficient, taking advantage of PyTorch\'s vectorized operations where possible. Performance Requirements - The solution should handle large tensors efficiently without significant slowdowns. Testing and Validation - Ensure to test your implementation with different sets of means, standard deviations, and weights. - Compare the entropy computation with manually computed entropies for validation.","solution":"import torch from torch.distributions import Normal, Categorical class CustomMixtureDistribution: def __init__(self, means: torch.Tensor, stds: torch.Tensor, weights: torch.Tensor): Initialize the CustomMixtureDistribution. :param means: A 1D tensor representing the means of the Normal distributions. :param stds: A 1D tensor representing the standard deviations of the Normal distributions. :param weights: A 1D tensor representing the weights of each component in the mixture distribution. self.means = means self.stds = stds self.weights = weights / weights.sum() self.components = [Normal(mean, std) for mean, std in zip(means, stds)] def sample(self, sample_shape: torch.Size = torch.Size()): Generate a sample from the mixture distribution. :param sample_shape: The shape of the sample to generate. :return: A sample from the mixture distribution. categorical = Categorical(self.weights) component_idx = categorical.sample(sample_shape) samples = torch.stack([comp.sample(sample_shape) for comp in self.components], dim=-1) return samples.gather(-1, component_idx.unsqueeze(-1)).squeeze(-1) def log_prob(self, value: torch.Tensor): Compute the log probability of a sample. :param value: A sample for which to compute the log probability. :return: The log probability of the sample. log_probs = torch.stack([comp.log_prob(value) for comp in self.components], dim=-1) weighted_log_probs = log_probs + torch.log(self.weights) return torch.logsumexp(weighted_log_probs, dim=-1) def entropy(self): Compute the entropy of the mixture distribution. :return: The entropy of the mixture distribution. component_entropies = torch.tensor([comp.entropy().mean().item() for comp in self.components]) H = -torch.sum(self.weights * torch.log(self.weights)) weighted_entropies = self.weights * (component_entropies + H) return weighted_entropies.sum() means = torch.tensor([0.0, 2.0, 5.0]) stds = torch.tensor([1.0, 0.5, 1.5]) weights = torch.tensor([0.2, 0.5, 0.3]) mixture = CustomMixtureDistribution(means, stds, weights) # Generate a sample sample = mixture.sample() # Compute log probability of a sample log_prob = mixture.log_prob(sample) # Compute the entropy of the mixture distribution entropy_value = mixture.entropy()"},{"question":"# Seaborn Boxplot Assessment **Objective**: Demonstrate your understanding of Seaborn\'s boxplot functionality to analyze the relationship between different variables in a dataset. **Dataset**: Use the Titanic dataset provided by Seaborn. **Tasks**: 1. **Load the Dataset**: Load the Titanic dataset using Seaborn. This dataset contains information about the Titanic passengers, including details like age, class, fare, and survival status. 2. **Basic Boxplot**: Create a single horizontal boxplot showing the distribution of ages of Titanic passengers. Ensure to handle any missing values in the `age` column. 3. **Grouped Boxplot**: Create a boxplot displaying the distribution of ages grouped by the passenger class (`class`). The boxplot should be vertical. 4. **Nested Grouped Boxplot**: Create a nested grouped boxplot displaying the distribution of ages grouped by the passenger class (`class`) and further divided by their survival status (`alive`). 5. **Customized Boxplot**: Customize the appearance of the previous nested grouped boxplot by: - Drawing the boxes as line art and adding a small gap between them. - Modifying the color and width of all the lines in the plot. - Customizing the appearance of outliers using different markers. **Requirements**: - Use Seaborn\'s boxplot function to create the visualizations. - Ensure that the Seaborn theme is set to `whitegrid`. - Handle any missing values appropriately. - Ensure the plots are labeled clearly and appropriately, including titles and axis labels. - Include code comments to explain your approach. **Constraints**: - The resulting boxplots should be clear and easy to interpret. - Ensure you handle edge cases, such as missing values in the dataset. **Input**: No specific input by the user. **Output**: Visualization plots generated using Seaborn. **Performance**: The code should run efficiently and handle the Titanic dataset without performance issues. Here is a skeleton code to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set the Seaborn theme sns.set_theme(style=\\"whitegrid\\") # Task 1: Basic Boxplot plt.figure(figsize=(10, 6)) # Your code here # Task 2: Grouped Boxplot plt.figure(figsize=(10, 6)) # Your code here # Task 3: Nested Grouped Boxplot plt.figure(figsize=(10, 6)) # Your code here # Task 4: Customized Boxplot plt.figure(figsize=(14, 8)) # Your code here plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set the Seaborn theme sns.set_theme(style=\\"whitegrid\\") # Task 1: Basic Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"age\\", data=titanic) plt.title(\'Distribution of Ages of Titanic Passengers\') plt.xlabel(\'Age\') plt.show() # Task 2: Grouped Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"class\\", y=\\"age\\", data=titanic) plt.title(\'Distribution of Ages Grouped by Passenger Class\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show() # Task 3: Nested Grouped Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"class\\", y=\\"age\\", hue=\\"alive\\", data=titanic) plt.title(\'Distribution of Ages Grouped by Class and Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show() # Task 4: Customized Boxplot plt.figure(figsize=(14, 8)) sns.boxplot(x=\\"class\\", y=\\"age\\", hue=\\"alive\\", data=titanic, linewidth=2.5, dodge=True, boxprops=dict(edgecolor=\\"k\\"), flierprops=dict(marker=\'o\', color=\'r\', alpha=0.5)) plt.title(\'Customized Boxplot: Distribution of Ages Grouped by Class and Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.legend(title=\'Survival Status\') plt.show()"},{"question":"**Objective:** Demonstrate your understanding of the `io` module, focusing on handling both text and binary streams, seeking within files, and managing different types of file operations efficiently. Problem Statement: You are required to implement a function `process_mixed_streams()` that processes both text and binary data from multiple sources. Specifically, the function will: 1. Read text data from a file, convert it to uppercase, and write it to an in-memory text stream. 2. Read binary data from another file, transform it by reversing the bytes, and write it to an in-memory binary stream. 3. Return a dictionary containing the following: - Key `\'text\'` with the content from the in-memory text stream as a string. - Key `\'binary\'` with the content from the in-memory binary stream as bytes. Function Signature: ```python def process_mixed_streams(text_file_path: str, binary_file_path: str) -> dict: pass ``` Input: - `text_file_path`: A string representing the file path to the input text file. - `binary_file_path`: A string representing the file path to the input binary file. The input files are guaranteed to exist and be readable. Output: - A dictionary with the following format: ```python { \'text\': \'<uppercase_text_data>\', \'binary\': b\'<reversed_binary_data>\' } ``` Requirements: 1. Use `StringIO` and `BytesIO` for in-memory operations. 2. Properly handle different file modes (`r`, `rb`, etc.) for text and binary files. 3. Ensure that the function efficiently handles large files by utilizing buffer reading methods where applicable. 4. Implement error handling where suitable, especially related to file operations. Example: Assume `text_file.txt` contains: ``` Hello, World! python310 IO module ``` And `binary_file.bin` contains: ``` 01 23 45 67 89 ab cd ef ``` ```python result = process_mixed_streams(\'text_file.txt\', \'binary_file.bin\') print(result) ``` Output (assuming the content of text and binary files are as described): ```python { \'text\': \'HELLO, WORLD!nPYTHON310 IO MODULEn\', \'binary\': b\'xefxcdxabx89x67x45x23x01\' } ``` Constraints: - The text file will not exceed 10 MB. - The binary file will not exceed 10 MB. Performance: - Aim for optimal performance for operations involving large files, leveraging Python\'s efficient I/O mechanisms. Implement the function `process_mixed_streams()` to meet the specified requirements.","solution":"import io def process_mixed_streams(text_file_path: str, binary_file_path: str) -> dict: # Initialize in-memory text and binary streams text_stream = io.StringIO() binary_stream = io.BytesIO() # Read, process, and write the text data with open(text_file_path, \'r\', encoding=\'utf-8\') as text_file: text_data = text_file.read() text_stream.write(text_data.upper()) # Read, process, and write the binary data with open(binary_file_path, \'rb\') as binary_file: binary_data = binary_file.read() binary_stream.write(binary_data[::-1]) # Seek to the beginning of the streams to read their contents text_stream.seek(0) binary_stream.seek(0) # Prepare the result dictionary result = { \'text\': text_stream.read(), \'binary\': binary_stream.read() } # Close the in-memory streams text_stream.close() binary_stream.close() # Return the resulting dictionary return result"},{"question":"Problem Statement You are given a PyTorch tensor of random values on the CUDA device. You need to perform a series of operations on this tensor using multiple CUDA streams to improve computational efficiency. However, you must ensure that there are no data races and that the operations are properly synchronized. Your task is to: 1. Create a tensor `a` of shape `(100, 1000)` filled with random values. 2. Perform the following operations on the tensor using different CUDA streams: - Multiply the tensor `a` by 5 and store the result in a new tensor `b`. - Add tensor `b` to the original tensor `a` and store the result back in tensor `a`. Ensure proper synchronization between these operations to avoid data races, so that the final tensor `a` is accurate. Requirements - Use the CUDA Stream Sanitizer to detect any synchronization issues. - Resolve any issues by properly synchronizing the streams. Constraints - You must use the CUDA Stream Sanitizer as demonstrated in the provided documentation. - Assume that the CUDA device is correctly set up and PyTorch is installed. - You must ensure that the streams do not cause any data races. Input No input is required as the tensor is created directly within the code. Output Print the final tensor `a` after all operations have been completed and properly synchronized. Example ```python import torch def synchronize_streams(): # Create tensor `a` on the CUDA device with random values. a = torch.rand(100, 1000, device=\\"cuda\\") stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() # Perform multiplication on a new stream and store the result in tensor `b` with torch.cuda.stream(stream1): b = torch.mul(a, 5) # Ensure synchronization before adding the results back to tensor `a` with torch.cuda.stream(stream2): stream2.wait_stream(stream1) a.add_(b) # Print the final tensor `a` print(a) # Enable CUDA Stream Sanitizer torch.cuda._sanitizer.enable_cuda_sanitizer() # Run the synchronization function synchronize_streams() ``` 1. Copy the code and save it in a file named `example_sync.py`. 2. Run the file using the following command to check for synchronization issues: ```sh TORCH_CUDA_SANITIZER=1 python example_sync.py ``` 3. Ensure that the script runs without any reported errors, indicating that there are no data races.","solution":"import torch def synchronize_streams(): # Create tensor `a` on the CUDA device with random values. a = torch.rand(100, 1000, device=\\"cuda\\") stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() # Perform multiplication on a new stream and store the result in tensor `b` with torch.cuda.stream(stream1): b = torch.mul(a, 5) # Ensure synchronization before adding the results back to tensor `a` with torch.cuda.stream(stream2): stream2.wait_stream(stream1) a.add_(b) # Synchronize streams to ensure all operations are completed torch.cuda.synchronize() # Return the final tensor `a` return a"},{"question":"**Question: Customizing Seaborn Plot Themes and Display Settings** You are tasked with creating a customized seaborn plot, configuring its theme, and adjusting display settings as specified. Follow the steps below to complete the task: 1. Create a simple line plot using seaborn and matplotlib with the following data: ```python import pandas as pd data = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5], \'y\': [2, 3, 5, 7, 11] }) ``` 2. Use the seaborn `Plot` class from `seaborn.objects` to create the plot. 3. Customize the theme of this plot to: - Set the axes background color to white. - Use the \\"whitegrid\\" style for the overall plot. 4. Adjust the display settings so that: - The plot is rendered in SVG format. - HiDPI scaling is disabled. - The embedded image scaling is set to 0.7. 5. Ensure that your plot is displayed correctly in a Jupyter notebook. **Constraints:** - You should use the seaborn `Plot` class from `seaborn.objects`. - Ensure that the theme and display settings are applied properly as specified. **Expected Output:** - A line plot displayed with the specified theme and display settings. **Example Code Template:** ```python import matplotlib.pyplot as plt import seaborn as sns import seaborn.objects as so import pandas as pd # Step 1: Prepare the data data = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5], \'y\': [2, 3, 5, 7, 11] }) # Step 2: Create the plot using seaborn\'s Plot class p = so.Plot(data, x=\'x\', y=\'y\').add(so.Line()) # Step 3: Customize the theme so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" # Set axes background to white from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Use \\"whitegrid\\" style # Step 4: Adjust display settings so.Plot.config.display[\\"format\\"] = \\"svg\\" # Render plot in SVG format so.Plot.config.display[\\"hidpi\\"] = False # Disable HiDPI scaling so.Plot.config.display[\\"scaling\\"] = 0.7 # Set image scaling to 0.7 # Display the plot p ``` Provide the full implementation to achieve the specified result.","solution":"import matplotlib.pyplot as plt import seaborn as sns import seaborn.objects as so import pandas as pd # Step 1: Prepare the data data = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5], \'y\': [2, 3, 5, 7, 11] }) # Step 2: Create the plot using seaborn\'s Plot class p = so.Plot(data, x=\'x\', y=\'y\').add(so.Line()) # Step 3: Customize the theme so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" # Set axes background to white from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Use \\"whitegrid\\" style # Step 4: Adjust display settings so.Plot.config.display[\\"format\\"] = \\"svg\\" # Render plot in SVG format so.Plot.config.display[\\"hidpi\\"] = False # Disable HiDPI scaling so.Plot.config.display[\\"scaling\\"] = 0.7 # Set image scaling to 0.7 # Display the plot p"},{"question":"Objective To assess the student\'s understanding and ability to use the `torch.fft` module to perform and manipulate Fourier Transforms. Problem Statement You are given a one-dimensional signal array. Your task is to write a function that performs the following steps using PyTorch: 1. Compute the Fast Fourier Transform (FFT) of the signal. 2. Shift the zero frequency component to the center of the spectrum. 3. Compute the inverse FFT of the shifted spectrum. 4. Compare the original signal with the inverse FFT result and return the maximum absolute difference between them. Function Signature ```python import torch def fft_signal_processing(signal: torch.Tensor) -> float: pass ``` Input - `signal` (torch.Tensor): A one-dimensional tensor representing the input signal. The tensor will have a length between 64 to 4096. Elements of the signal are floating point numbers. Output - `max_diff` (float): The maximum absolute difference between the original signal and the result of the inverse FFT. Constraints - To use the functions from `torch.fft` module suitably within the above steps. - Use `torch.fft.fft` to compute the FFT. - Use `torch.fft.fftshift` to shift the zero frequency component. - Use `torch.fft.ifft` to compute the inverse FFT. Example ```python import torch # Example signal signal = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]) # Call the function max_difference = fft_signal_processing(signal) print(max_difference) # Output: 0.0 (or a very small value close to zero) ``` Notes - You may assume that the input signal is always valid as per the constraints. - The maximum absolute difference between the original signal and the inverse FFT should ideally be very small (on the order of numerical precision errors). Implementation Hints - You can make use of `torch.abs()` and `torch.max()` to find the maximum absolute difference. Good luck!","solution":"import torch def fft_signal_processing(signal: torch.Tensor) -> float: # Step 1: Compute the Fast Fourier Transform (FFT) of the signal fft_result = torch.fft.fft(signal) # Step 2: Shift the zero frequency component to the center of the spectrum shifted_fft_result = torch.fft.fftshift(fft_result) # Step 3: Compute the inverse FFT of the shifted spectrum inv_fft_result = torch.fft.ifft(torch.fft.ifftshift(shifted_fft_result)) # Step 4: Compare the original signal with the inverse FFT result max_diff = torch.max(torch.abs(signal - inv_fft_result)).item() return max_diff"},{"question":"# Pathlib Module Coding Assessment Objective Implement a function to identify and count Python files contained within a specified directory (including its subdirectories) and compute the total size of these files. This task will demonstrate your ability to utilize both basic and advanced features of the `pathlib` module. Problem Statement Write a function `count_python_files(path: str) -> dict` that takes a directory path as input and returns a dictionary with the following keys: - `\'count\'`: The total number of Python files (`.py` extension) found within the input directory and its subdirectories. - `\'total_size\'`: The cumulative size (in bytes) of all these Python files. Input - `path` (str): The path to the directory you want to search. Output - A dictionary with keys `\'count\'` and `\'total_size\'` denoting the total number of `.py` files and their total size in bytes, respectively. Constraints - You may assume the input directory always exists and you have read permissions for all directories and files within. - Only consider files with the `.py` extension (case insensitive). Example Usage ```python import os from pathlib import Path def count_python_files(path: str) -> dict: # Your implementation here # Example directory structure: # test_dir/ # ├── file1.py # ├── file2.txt # └── sub_dir # ├── file3.py # └── file4.py # Example function call result = count_python_files(\'test_dir\') print(result) # Output: {\'count\': 3, \'total_size\': total_size_in_bytes} ``` Additional Considerations - Make sure to handle large directory structures efficiently. - You may use any helper functions as needed for clarity and modularity.","solution":"from pathlib import Path def count_python_files(path: str) -> dict: Count the number of Python files and compute the total size of these files in a specified directory. Parameters: path (str): Path to the directory to search. Returns: dict: A dictionary with the total count of Python files and their cumulative size in bytes. directory = Path(path) python_files = list(directory.rglob(\'*.py\')) file_count = len(python_files) total_size = sum(f.stat().st_size for f in python_files) return {\'count\': file_count, \'total_size\': total_size}"},{"question":"# PyTorch Data Type Characteristics You are asked to implement a function that retrieves and displays comprehensive characteristics of a given PyTorch data type `dtype`. The function should accept both floating-point and integer data types and output their numerical properties as a dictionary. Function Signature ```python def get_dtype_properties(dtype: torch.dtype) -> dict: pass ``` Input - `dtype` (torch.dtype): The data type for which to retrieve numerical properties. This can be either floating-point or integer type. Output - A dictionary containing the numerical properties of the given data type. For floating-point types, this should include `bits`, `eps`, `max`, `min`, `tiny`, and `resolution`. For integer types, this should include `bits`, `max`, and `min`. Example ```python from torch import float32, int32 props_float32 = get_dtype_properties(float32) print(props_float32) # Output should be something like: # { # \'bits\': 32, # \'eps\': 1.1920928955078125e-07, # \'max\': 3.4028234663852886e+38, # \'min\': -3.4028234663852886e+38, # \'tiny\': 1.1754943508222875e-38, # \'resolution\': 1e-07 # } props_int32 = get_dtype_properties(int32) print(props_int32) # Output should be something like: # { # \'bits\': 32, # \'max\': 2147483647, # \'min\': -2147483648 # } ``` Constraints - Assume the `dtype` provided is valid and supported by PyTorch `torch.finfo` or `torch.iinfo`. - Handle both floating-point and integer data types appropriately, choosing the correct class (`torch.finfo` or `torch.iinfo`) based on `dtype`. Notes: - You are not required to handle any invalid inputs (i.e., inputs that are not supported by `torch.finfo` or `torch.iinfo`). - Ensure performance is efficient even if you need to make nested calls or handle multiple attributes within the function.","solution":"import torch def get_dtype_properties(dtype: torch.dtype) -> dict: Retrieves and displays comprehensive characteristics of a given PyTorch data type. Parameters: dtype (torch.dtype): The data type for which to retrieve numerical properties (either floating-point or integer). Returns: dict: A dictionary containing the numerical properties of the given data type. if dtype.is_floating_point: info = torch.finfo(dtype) return { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny, \'resolution\': info.resolution } else: info = torch.iinfo(dtype) return { \'bits\': info.bits, \'max\': info.max, \'min\': info.min }"},{"question":"Multi-producer Multi-consumer via PriorityQueue **Objective**: Implement a multi-threaded producer-consumer system using the `queue.PriorityQueue` class. Producers will generate tasks with varying priorities, and consumers will process them according to the priority order. **Requirements**: 1. **Producer Threads**: Multiple producer threads should generate tasks with a priority and place them into the `PriorityQueue`. 2. **Consumer Threads**: Multiple consumer threads should consume tasks from the queue and process them based on their priority. 3. **Task Details**: Each task should be a tuple in the form `(priority_number, data)`, where `priority_number` determines the task\'s priority, and `data` can be any information related to the task. 4. **Concurrency Control**: Ensure that concurrent access to the queue is properly managed. 5. **Task Completion**: Use `task_done` and `join` methods to ensure that all tasks are processed correctly before ending the main program. **Input Format**: - Number of producer threads, number of consumer threads. - Each producer will generate a list of tasks. **Output Format**: - Print the task as it is being processed by the consumer. **Performance Requirements**: - The system should efficiently manage tasks even with a large number of tasks and threads. - Handle threading and synchronization issues effectively. **Example**: ```python import threading import queue import time # Define the function for producer threads def producer(q, id, tasks): for task in tasks: print(f\\"Producer {id} putting task {task}\\") q.put(task) time.sleep(0.1) print(f\\"Producer {id} finished\\") # Define the function for consumer threads def consumer(q, id): while True: try: priority, task = q.get(timeout=1) # Timeout to exit the loop print(f\\"Consumer {id} processing task {task} with priority {priority}\\") q.task_done() except queue.Empty: break print(f\\"Consumer {id} finished\\") # Main function to setup and run the threads if __name__ == \'__main__\': q = queue.PriorityQueue() producer_threads = [] consumer_threads = [] # Define tasks with varying priorities tasks1 = [(2, \'task1\'), (1, \'task2\'), (3, \'task3\')] tasks2 = [(1, \'task4\'), (3, \'task5\'), (2, \'task6\')] # Create and start producer threads for i, tasks in enumerate([tasks1, tasks2], 1): t = threading.Thread(target=producer, args=(q, i, tasks)) t.start() producer_threads.append(t) # Create and start consumer threads for i in range(1, 4): t = threading.Thread(target=consumer, args=(q, i)) t.start() consumer_threads.append(t) # Join producer threads for t in producer_threads: t.join() # Wait until all tasks have been processed q.join() # Join consumer threads for t in consumer_threads: t.join() print(\\"All work completed\\") ``` Constraints: - Ensure thread safety and proper synchronization. The example provided above illustrates a basic setup. Students need to expand and adapt it as needed. - Use appropriate exception handling for managing queue operations. **Note**: This example needs to be expanded with more comprehension checks and performance constraints for the assessment.","solution":"import threading import queue import time def producer(q, id, tasks): for task in tasks: print(f\\"Producer {id} putting task {task}\\") q.put(task) time.sleep(0.1) print(f\\"Producer {id} finished\\") def consumer(q, id): while True: try: priority, task = q.get(timeout=1) # Timeout to exit the loop print(f\\"Consumer {id} processing task {task} with priority {priority}\\") q.task_done() except queue.Empty: break print(f\\"Consumer {id} finished\\") def main(num_producers, num_consumers, producer_tasks): q = queue.PriorityQueue() producer_threads = [] consumer_threads = [] # Create and start producer threads for i, tasks in enumerate(producer_tasks, 1): t = threading.Thread(target=producer, args=(q, i, tasks)) t.start() producer_threads.append(t) # Create and start consumer threads for i in range(1, num_consumers + 1): t = threading.Thread(target=consumer, args=(q, i)) t.start() consumer_threads.append(t) # Join producer threads for t in producer_threads: t.join() # Wait until all tasks have been processed q.join() # Join consumer threads for t in consumer_threads: t.join() print(\\"All work completed\\")"},{"question":"You are tasked with building a Decision Tree model to classify the Iris dataset. However, the challenge extends beyond just building a classifier. You will also explore the model\'s performance under different parameter settings, visualize the decision tree, and perform pruning to avoid overfitting. Additionally, you will handle missing values appropriately and ensure your model is robust in dealing with them. # Requirements and Constraints - Use the `DecisionTreeClassifier` from `sklearn.tree`. - Load the Iris dataset using `sklearn.datasets.load_iris`. - Visualize the tree using `sklearn.tree.plot_tree`. - Split the dataset into training and test sets using `sklearn.model_selection.train_test_split`. - Evaluate your model\'s performance using `accuracy_score` from `sklearn.metrics`. # Steps 1. **Load the Data** - Load the Iris dataset and inspect it. Use `sklearn.datasets.load_iris`. 2. **Data Preprocessing** - Introduce some random missing values in the dataset. - Handle these missing values appropriately using imputation methods from `sklearn.impute`. 3. **Model Training** - Split the dataset into training and test sets (70-30 split). - Train a `DecisionTreeClassifier` with default settings. - Visualize the decision tree using `plot_tree`. 4. **Model Evaluation** - Evaluate the model\'s performance using `accuracy_score`. 5. **Hyperparameter Tuning** - Experiment with different settings for `max_depth`, `min_samples_split`, and `min_samples_leaf`. - Compare the performance of models with different parameter settings. 6. **Pruning** - Implement minimal cost-complexity pruning (`ccp_alpha` parameter) to avoid overfitting. - Evaluate the pruned model and compare its performance with the unpruned model. 7. **Handling Missing Values** - Train a model on the data with missing values and handle them appropriately as described in the documentation. - Evaluate this model\'s performance and compare it with the performance on the dataset without missing values. # Submission - Python function definition implementing each of the above steps. - Comments and explanations for each major step. - Output plots of the decision tree before and after pruning. - Accuracy scores for all models evaluated. - Discussion on the impact of different parameter settings and handling of missing values. # Example Function Signature ```python def extensive_decision_tree_classification(): import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.metrics import accuracy_score from sklearn.impute import SimpleImputer # Step 1: Load Data iris = load_iris() X, y = iris.data, iris.target # Optional: Introduce random missing values for step 2 rng = np.random.default_rng(seed=42) missing_indices = rng.choice(X.size, size=int(X.size * 0.1), replace=False) X.ravel()[missing_indices] = np.nan # Step 2: Handle Missing Values imputer = SimpleImputer(strategy=\'mean\') X = imputer.fit_transform(X) # Step 3: Split Data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Train Model clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Step 4: Evaluate Model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Visualize Tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Hyperparameter Tuning and Pruning (additional steps to be implemented) # Return the result summary return accuracy result = extensive_decision_tree_classification() print(\\"Accuracy of the Decision Tree Classifier:\\", result) ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.metrics import accuracy_score from sklearn.impute import SimpleImputer def extensive_decision_tree_classification(): # Step 1: Load Data iris = load_iris() X, y = iris.data, iris.target # Introduce random missing values rng = np.random.default_rng(seed=42) missing_indices = rng.choice(X.size, size=int(X.size * 0.1), replace=False) X.ravel()[missing_indices] = np.nan # Step 2: Handle Missing Values imputer = SimpleImputer(strategy=\'mean\') X = imputer.fit_transform(X) # Step 3: Split Data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 4: Train Model clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate Model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Visualize Tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Step 5: Hyperparameter Tuning tuned_accuracies = [] for max_depth in [2, 3, 4, 5]: clf_tuned = DecisionTreeClassifier(max_depth=max_depth, random_state=42) clf_tuned.fit(X_train, y_train) y_pred_tuned = clf_tuned.predict(X_test) acc_tuned = accuracy_score(y_test, y_pred_tuned) tuned_accuracies.append((max_depth, acc_tuned)) # Step 6: Pruning path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas clfs = [] for ccp_alpha in ccp_alphas: clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf_pruned.fit(X_train, y_train) clfs.append(clf_pruned) pruned_accuracies = [(clf.ccp_alpha, accuracy_score(y_test, clf.predict(X_test))) for clf in clfs] best_pruned_clf = max(clfs, key=lambda clf: accuracy_score(y_test, clf.predict(X_test))) # Visualize Pruned Tree plt.figure(figsize=(20,10)) plot_tree(best_pruned_clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Return the result summary return { \'initial_accuracy\': accuracy, \'tuned_accuracies\': tuned_accuracies, \'pruned_accuracies\': pruned_accuracies, \'best_pruned_accuracy\': accuracy_score(y_test, best_pruned_clf.predict(X_test)) }"},{"question":"**Question: Implement a Python Script for Batch Compilation Using `py_compile`** You are tasked with creating a function that compiles a list of Python source files into bytecode files. Your function should leverage the `py_compile.compile` function and support several advanced features to demonstrate an in-depth understanding of the `py_compile` module. # Function Signature ```python def batch_compile(source_files: list, optimize: int = -1, invalidation_mode=\\"TIMESTAMP\\", raise_errors=False, quiet_level=0) -> dict: pass ``` # Parameters - **source_files** (list): A list of file paths (as strings) for the Python source files you need to compile. - **optimize** (int, optional): Optimization level to be passed to the `py_compile.compile` function. Default is -1. - **invalidation_mode** (str, optional): The invalidation mode for the bytecode, either \\"TIMESTAMP\\" or \\"CHECKED_HASH\\". Default is \\"TIMESTAMP\\". - **raise_errors** (bool, optional): Whether to raise a `PyCompileError` when a compilation error occurs. Default is False. - **quiet_level** (int, optional): Level of quietness for error handling. Can be 0, 1, or 2. Default is 0. # Returns - **dict**: A dictionary where keys are the source file paths and values are the paths to the compiled bytecode files or error messages. # Constraints - If the source file is not found, add an error message to the corresponding key in the return dictionary. - Handle all potential exceptions that `py_compile.compile` might raise and log appropriate error messages in the return dictionary. - Use the correct invalidation mode from `py_compile.PycInvalidationMode`. # Example ```python source_files = [\\"script1.py\\", \\"script2.py\\"] result = batch_compile(source_files, optimize=2, invalidation_mode=\\"CHECKED_HASH\\", raise_errors=True, quiet_level=1) # Result: { # \'script1.py\': \'path_to_compiled_bytecode/script1.cpython-310.pyc\', # \'script2.py\': \'Error: Description of the compilation error\' # } ``` # Notes - Ensure that your implementation respects the specified invalidation modes by correctly interfacing with `py_compile.PycInvalidationMode`. - Simulate conditions where the source files may not exist and handle these gracefully in your implementation. - Optimize for readability and maintainability in your code.","solution":"import py_compile import os def batch_compile(source_files: list, optimize: int = -1, invalidation_mode=\\"TIMESTAMP\\", raise_errors=False, quiet_level=0) -> dict: result = {} # Determine the invalidation mode if invalidation_mode == \\"TIMESTAMP\\": invalidation_mode = py_compile.PycInvalidationMode.TIMESTAMP elif invalidation_mode == \\"CHECKED_HASH\\": invalidation_mode = py_compile.PycInvalidationMode.CHECKED_HASH else: raise ValueError(\\"Invalid invalidation mode\\") for source_file in source_files: try: if not os.path.isfile(source_file): result[source_file] = \\"Error: Source file not found\\" continue compiled_file = py_compile.compile( source_file, cfile=None, dfile=None, doraise=raise_errors, optimize=optimize, invalidation_mode=invalidation_mode, quiet=quiet_level ) result[source_file] = compiled_file except Exception as e: result[source_file] = f\\"Error: {str(e)}\\" return result"},{"question":"**Linear Discriminant Analysis (LDA) Implementation from Scratch** **Objective:** Your task is to implement the Linear Discriminant Analysis (LDA) classifier from scratch using Python, without using any built-in classification libraries. You will then apply your implementation on a dataset to classify the data points into different categories. **Problem Statement:** Linear Discriminant Analysis (LDA) is a linear classifier that aims to separate two or more classes by learning the feature subspace that best separates the classes. Write a Python function `my_lda(X_train, y_train, X_test)` that performs the following steps: 1. **Fit the model using the training data:** - Compute the mean vectors for each class. - Compute the within-class scatter matrix. - Compute the between-class scatter matrix. - Compute the eigenvalues and corresponding eigenvectors for the scatter matrices. - Select the largest eigenvectors to form a transformation matrix. - Project the training data onto the new feature subspace. 2. **Predict the classes for the test data:** - Project the test data onto the learned subspace. - Compute the probabilities of each class. - Assign the class with the highest probability. **Function Signature:** ```python def my_lda(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: # Your code here ``` **Input:** - `X_train` - A numpy array of shape (m, n) representing the training data features where `m` is the number of training samples and `n` is the number of features. - `y_train` - A numpy array of shape (m,) representing the class labels for the training data. - `X_test` - A numpy array of shape (p, n) representing the test data features where `p` is the number of test samples. **Output:** - Returns a numpy array of shape (p,) containing the predicted class labels for the test data. **Constraints:** - You may assume that there are at least two classes and that the classes are linearly separable. - The training data will not have any missing values. **Example:** ```python import numpy as np # Sample data X_train = np.array([[1.0, 2.5], [2.0, 3.5], [3.0, 4.0], [5.0, 6.5], [3.5, 2.0], [4.5, 3.0]]) y_train = np.array([0, 0, 0, 1, 1, 1]) X_test = np.array([[2.0, 3.0], [4.0, 5.0]]) # Call the function predictions = my_lda(X_train, y_train, X_test) print(predictions) # Expected output: [0 1] ``` **Notes:** - You are not allowed to use `LinearDiscriminantAnalysis` or any other classification library from scikit-learn directly in your implementation. - You can use libraries like numpy for matrix operations. Good luck!","solution":"import numpy as np def my_lda(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: # Step 1: Compute the mean vectors for each class class_labels = np.unique(y_train) mean_vectors = [] for cls in class_labels: mean_vectors.append(np.mean(X_train[y_train == cls], axis=0)) # Step 2: Compute the within-class scatter matrix n_features = X_train.shape[1] Sw = np.zeros((n_features, n_features)) for cls, mean_vec in zip(class_labels, mean_vectors): class_scatter = np.zeros((n_features, n_features)) for row in X_train[y_train == cls]: row, mean_vec = row.reshape(n_features, 1), mean_vec.reshape(n_features, 1) class_scatter += (row - mean_vec).dot((row - mean_vec).T) Sw += class_scatter # Step 3: Compute the between-class scatter matrix overall_mean = np.mean(X_train, axis=0).reshape(n_features, 1) Sb = np.zeros((n_features, n_features)) for i, mean_vec in enumerate(mean_vectors): n = X_train[y_train == class_labels[i]].shape[0] mean_vec = mean_vec.reshape(n_features, 1) Sb += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T) # Step 4: Compute the eigenvalues and eigenvectors for the scatter matrices eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb)) # Step 5: Select the largest eigenvectors to form a transformation matrix eig_pairs = [(eig_vals[i], eig_vecs[:, i]) for i in range(len(eig_vals))] eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True) W = np.hstack((eig_pairs[0][1].reshape(n_features, 1), eig_pairs[1][1].reshape(n_features, 1))) # Step 6: Project the test data onto the new feature subspace X_train_lda = X_train.dot(W) X_test_lda = X_test.dot(W) # Predict the class labels for the test set from scipy.spatial.distance import cdist means_lda = np.array([np.mean(X_train_lda[y_train == cls], axis=0) for cls in class_labels]) dist = cdist(X_test_lda, means_lda) predictions = class_labels[np.argmin(dist, axis=1)] return predictions"},{"question":"Implement a Custom Attention Layer in PyTorch Attention mechanisms have become a critical component in many neural network architectures, allowing models to focus on specific parts of the input for better performance. In this task, you will implement a custom attention layer in PyTorch. Task: Implement a class `CustomAttention` which forms an attention layer. This class should be a subclass of `torch.nn.Module` and should include the following functionalities: 1. **Initialization**: In the constructor (`__init__`), initialize any trainable parameters, e.g., weight matrices for queries, keys, and values, biases, etc. 2. **Forward Pass**: * Expected Input: a tuple of two tensors `(queries, keys)` and an optional tensor `values`. If `values` is not provided, use `keys` as the `values`. * `queries`, `keys`, and `values` will be of shape `(batch_size, seq_len, feature_dim)`. * Compute the attention scores using the dot-product attention mechanism. * Apply the softmax function to the attention scores to obtain the attention weights. * Compute the output as a weighted sum of the `values` tensor. 3. **Output**: the resulting tensor of shape `(batch_size, seq_len, feature_dim)` and the attention weights tensor of shape `(batch_size, seq_len, seq_len)`. Detailed Steps: 1. Define a class `CustomAttention` that inherits from `torch.nn.Module`. 2. In the constructor (`__init__`), define weight matrices for transforming `queries`, `keys`, and `values`. 3. Implement the `forward` method to compute attention scores, apply softmax, and compute the attention output. ```python import torch import torch.nn as nn class CustomAttention(nn.Module): def __init__(self, feature_dim): super(CustomAttention, self).__init__() self.query_layer = nn.Linear(feature_dim, feature_dim) self.key_layer = nn.Linear(feature_dim, feature_dim) self.value_layer = nn.Linear(feature_dim, feature_dim) def forward(self, queries, keys, values=None): if values is None: values = keys # Transform queries, keys, values queries = self.query_layer(queries) keys = self.key_layer(keys) values = self.value_layer(values) # Compute attention scores attention_scores = torch.bmm(queries, keys.transpose(1, 2)) # (batch_size, seq_len, seq_len) # Apply softmax to get attention weights attention_weights = torch.softmax(attention_scores, dim=-1) # Compute attention output attention_output = torch.bmm(attention_weights, values) # (batch_size, seq_len, feature_dim) return attention_output, attention_weights ``` Constraints: 1. Assume `feature_dim` is always a positive integer. 2. Assume `batch_size` and `seq_len` are always positive integers and consistent across queries, keys, and values. Performance Requirements: The solution should be efficient, ideally with a complexity of (O(batch_size times seq_len^2 times feature_dim)). Evaluation: Your implementation will be tested against multiple sets of inputs to ensure correctness, efficiency, and scalability.","solution":"import torch import torch.nn as nn class CustomAttention(nn.Module): def __init__(self, feature_dim): super(CustomAttention, self).__init__() self.query_layer = nn.Linear(feature_dim, feature_dim) self.key_layer = nn.Linear(feature_dim, feature_dim) self.value_layer = nn.Linear(feature_dim, feature_dim) def forward(self, queries, keys, values=None): if values is None: values = keys # Transform queries, keys, values queries_transformed = self.query_layer(queries) keys_transformed = self.key_layer(keys) values_transformed = self.value_layer(values) # Compute attention scores attention_scores = torch.bmm(queries_transformed, keys_transformed.transpose(1, 2)) # Apply softmax to get attention weights attention_weights = torch.softmax(attention_scores, dim=-1) # Compute attention output attention_output = torch.bmm(attention_weights, values_transformed) return attention_output, attention_weights"},{"question":"# Capsule Interface Simulation in Python Your task is to simulate a simplified version of `PyCapsule` in Python. Create a class `PyCapsule` that will encapsulate a Python object and allow retrieval and modification of its attributes in a controlled manner, similar to the provided C API. Class: `PyCapsule` Implement the following methods for the `PyCapsule` class: 1. **`__init__(self, pointer, name=None, destructor=None)`** - Initialize a capsule with a given pointer (represented as any Python object), an optional name (a string), and an optional destructor (a callable). - Raise a `ValueError` if the pointer is `None`. 2. **`get_pointer(self)`** - Return the encapsulated pointer. - Raise a `RuntimeError` if the pointer cannot be retrieved. 3. **`get_name(self)`** - Return the name of the capsule. - Return `None` if the capsule has no name. 4. **`set_name(self, name)`** - Set or update the name of the capsule. - Raise a `ValueError` if the name is not a string. 5. **`is_valid(self)`** - Return `True` if the capsule is valid (i.e., has a non-`None` pointer). - Return `False` otherwise. 6. **`__del__(self)`** - If a destructor is provided during initialization, call the destructor with the capsule as an argument when the capsule is destroyed. Constraints - The destructor, if provided, must be a callable. - Ensure thread safety for get and set operations. Examples ```python # Example usage of PyCapsule class def destructor(capsule): print(f\\"Capsule {capsule.get_name()} is being destroyed.\\") capsule1 = PyCapsule(pointer=42, name=\\"test.capsule\\", destructor=destructor) print(capsule1.get_pointer()) # Output: 42 print(capsule1.get_name()) # Output: test.capsule print(capsule1.is_valid()) # Output: True capsule1.set_name(\\"updated.capsule\\") print(capsule1.get_name()) # Output: updated.capsule # Deleting the capsule should trigger the destructor del capsule1 # Output: Capsule updated.capsule is being destroyed. ``` Note: You are encouraged to write comprehensive test cases to ensure the robustness of your implementation.","solution":"import threading class PyCapsule: def __init__(self, pointer, name=None, destructor=None): if pointer is None: raise ValueError(\\"Pointer cannot be None\\") self._pointer = pointer self._name = name self._destructor = destructor self._lock = threading.Lock() def get_pointer(self): with self._lock: if self._pointer is None: raise RuntimeError(\\"Pointer cannot be retrieved\\") return self._pointer def get_name(self): with self._lock: return self._name def set_name(self, name): if not isinstance(name, str): raise ValueError(\\"Name must be a string\\") with self._lock: self._name = name def is_valid(self): with self._lock: return self._pointer is not None def __del__(self): if self._destructor is not None: self._destructor(self)"},{"question":"# Custom Descriptor Implementation in Python **Objective**: Implement a custom descriptor class in Python and demonstrate its usage in a sample class. Your implementation should showcase understanding of creating and using descriptors, including data descriptors and method descriptors. Task 1. **Create a Custom Descriptor**: - Implement a custom descriptor class `DataDescriptor` that will handle attribute access for a specific attribute, ensuring that the value is always an integer. 2. **Descriptor Usage**: - Implement a sample class `SampleClass` that uses `DataDescriptor` for one of its attributes, say `value`. 3. **Validation**: - Add validation within the descriptor to ensure that the value set is always an integer. If a non-integer value is assigned, raise a `TypeError`. 4. **Check Descriptor Type**: - Implement a function `is_data_descriptor` to check if a given descriptor object is a data descriptor. Implementation Details 1. **DataDescriptor Class**: - `__get__(self, instance, owner)`: Retrieve the value of the attribute. - `__set__(self, instance, value)`: Set the value of the attribute with validation. - `__delete__(self, instance)`: Delete the attribute (optional). 2. **SampleClass**: - Define an attribute `value` using the `DataDescriptor`. 3. **is_data_descriptor Function**: - This function should use the `PyDescr_IsData` functionality. Input and Output - **Input**: Various operations on `SampleClass` instances, including setting and getting the attribute `value`. - **Output**: Proper behavior as defined above and validation messages. Example Usage ```python class DataDescriptor: # Implement the custom descriptor here class SampleClass: value = DataDescriptor() def is_data_descriptor(descriptor): # Implement the function to check if it\'s a data descriptor instance = SampleClass() instance.value = 42 print(instance.value) # Output: 42 instance.value = \'a string\' # Should raise TypeError print(is_data_descriptor(instance.__class__.__dict__[\'value\'])) # Should print True or 1 ``` Ensure your solution includes comprehensive error handling and validation, demonstrating a deep understanding of descriptor behavior and usage in Python.","solution":"class DataDescriptor: def __init__(self): self._value = None def __get__(self, instance, owner): return self._value def __set__(self, instance, value): if not isinstance(value, int): raise TypeError(\\"Value must be an integer\\") self._value = value def __delete__(self, instance): del self._value class SampleClass: value = DataDescriptor() def is_data_descriptor(descriptor): return all(hasattr(descriptor, method) for method in (\'__get__\', \'__set__\'))"},{"question":"# GroupBy Pandas Assessment You are given a dataset containing sales information for a retail store. The dataset includes the following columns: - `Date`: The date of the sale. - `Store_ID`: The unique identifier for the store. - `Product_ID`: The unique identifier for the product. - `Sales`: The number of units sold. - `Revenue`: The total revenue from the sales. Your task is to implement a function `analyze_sales(df: pd.DataFrame) -> pd.DataFrame:` that takes a DataFrame as input and returns a modified DataFrame that includes aggregated metrics for each store. The metrics should include: 1. The total number of units sold (`Total_Sales`). 2. The total revenue generated (`Total_Revenue`). 3. The average number of units sold per sale (`Average_Sales_Per_Sale`). 4. The date of the day with the highest revenue (`Date_Highest_Revenue`). Additionally, your function should return only the top 5 stores based on the `Total_Revenue`. Please ensure that your solution meets the following criteria: - The input DataFrame will contain at least 1000 rows. - The solution should be optimized to handle large datasets efficiently. - The output DataFrame should be sorted by `Total_Revenue` in descending order. The format of the input DataFrame is shown below: ```python import pandas as pd data = { \'Date\': [\'2021-01-01\', \'2021-01-02\', \'2021-01-03\', ...], \'Store_ID\': [1, 2, 1, ...], \'Product_ID\': [101, 102, 103, ...], \'Sales\': [10, 20, 30, ...], \'Revenue\': [100, 200, 300, ...] } df = pd.DataFrame(data) ``` Example of the output DataFrame: ```python Store_ID Total_Sales Total_Revenue Average_Sales_Per_Sale Date_Highest_Revenue 0 2 ... ... ... ... 1 1 ... ... ... ... 2 3 ... ... ... ... 3 4 ... ... ... ... 4 5 ... ... ... ... ``` Please write your implementation for the function `analyze_sales(df: pd.DataFrame) -> pd.DataFrame:` below.","solution":"import pandas as pd def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: # Group by Store_ID and aggregate necessary metrics aggregated_df = df.groupby(\'Store_ID\').agg( Total_Sales=pd.NamedAgg(column=\'Sales\', aggfunc=\'sum\'), Total_Revenue=pd.NamedAgg(column=\'Revenue\', aggfunc=\'sum\'), Average_Sales_Per_Sale=pd.NamedAgg(column=\'Sales\', aggfunc=\'mean\') ).reset_index() # Find the date with the highest revenue for each store highest_revenue_df = df.groupby([\'Store_ID\', \'Date\'])[\'Revenue\'].sum().reset_index() idx = highest_revenue_df.groupby([\'Store_ID\'])[\'Revenue\'].idxmax() date_highest_revenue = highest_revenue_df.loc[idx, [\'Store_ID\', \'Date\']] date_highest_revenue = date_highest_revenue.rename(columns={\'Date\': \'Date_Highest_Revenue\'}) # Merge the dataframes to include the Date_Highest_Revenue in the aggregated dataframe final_df = aggregated_df.merge(date_highest_revenue, on=\'Store_ID\', how=\'left\') # Sort by Total_Revenue in descending order and return the top 5 stores final_df = final_df.sort_values(by=\'Total_Revenue\', ascending=False) return final_df.head(5)"},{"question":"**Objective**: Demonstrate your understanding of the `pkgutil` module and its utilities related to the import system and package support in Python. **Problem Statement**: You are tasked with creating a utility function to list all modules and submodules within a given package, retrieve their absolute file paths, and extend the search path of a package. The function should take one or two arguments: the name of the package and optionally, a path to extend the search path. If the second argument is provided, the search path for the modules should be extended using `pkgutil.extend_path`. **Function Signature**: ```python def package_inspector(package_name: str, extend_path_with: Optional[str] = None) -> List[Tuple[str, str]]: pass ``` **Input**: - `package_name` (str): The name of the package to inspect. - `extend_path_with` (Optional[str]): An optional path to extend the search path of the package if necessary. **Output**: - List of tuples: Each tuple should contain the name of the module/submodule and its absolute file path. **Constraints**: - The package specified must exist and be importable. - The function should handle errors gracefully if a module or submodule cannot be imported or if the path extension fails. - Only Python modules and submodules should be considered. **Example**: ```python # Example usage of package_inspector function result = package_inspector(\'ctypes\') print(result) # Example usage with extending the path result = package_inspector(\'my_package\', \'/path/to/dir\') print(result) ``` **Hints**: - Use `pkgutil.iter_modules` or `pkgutil.walk_packages` to find all modules and submodules within a package. - Use `pkgutil.extend_path` to extend the search path if the second argument is provided. - Use `inspect` or `os.path` to retrieve absolute file paths of modules. Your implementation should clearly demonstrate your understanding of the `pkgutil` module and its application in a real-world scenario.","solution":"import pkgutil import importlib import os from typing import List, Tuple, Optional def package_inspector(package_name: str, extend_path_with: Optional[str] = None) -> List[Tuple[str, str]]: Inspects the package to list all modules and submodules with their absolute file paths. Optionally extends the search path of the package. Parameters: - package_name (str): The name of the package to inspect. - extend_path_with (Optional[str]): An optional path to extend the search path of the package. Returns: - List[Tuple[str, str]]: A list of tuples where each tuple contains the name and the absolute file path of the module. results = [] try: # Import the package package = importlib.import_module(package_name) # If extend_path_with is provided, extend the package\'s search path if extend_path_with: package.__path__ = pkgutil.extend_path(package.__path__, extend_path_with) # Iterate through the modules in the package for importer, mod_name, ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \'.\'): try: # Import each module mod = importlib.import_module(mod_name) # Get the file path of the module file_path = os.path.abspath(mod.__file__) results.append((mod_name, file_path)) except Exception as e: # Handle the error gracefully by logging the error and continuing print(f\\"Could not import module {mod_name}: {e}\\") except Exception as e: print(f\\"Could not import package {package_name}: {e}\\") return results"},{"question":"**Coding Assessment Question** # TorchScript Compatibility Conversion In this question, you are tasked with converting a given PyTorch code to be compatible with TorchScript. Specifically, you need to handle unsupported tensor operations, initialization methods, and adapt to the required schema of certain functions. Input You will be provided with the following PyTorch model definition that will utilize operations unsupported directly in TorchScript. Your task is to convert this model so that it can be compiled using TorchScript\'s `torch.jit.script`. ```python import torch import torch.nn as nn import torch.nn.functional as F class MyModel(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(MyModel, self).__init__() self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) self.fc = nn.Linear(hidden_size, output_size) def forward(self, x): h0 = torch.zeros(1, x.size(0), self.rnn.hidden_size).requires_grad_() out, _ = self.rnn(x, h0) out = self.fc(out[:, -1, :]) return out input_size = 10 hidden_size = 20 output_size = 5 model = MyModel(input_size, hidden_size, output_size) ``` Expected Output 1. Modify the `MyModel` class such that it can be compiled with `torch.jit.script`. 2. Make sure any unsupported operations or divergences are correctly handled. 3. Provide a script demonstrating the updated model and JIT compilation. Constraints - You cannot use unsupported constructs like `nn.RNN`. - Ensure that the initialization and tensor operations adhere to the constraints of TorchScript, such as requiring `dtype`, `device`, etc., appropriately. - You must handle the conversion process while maintaining equivalent functionality. # Example ```python class MyModelTransformed(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(MyModelTransformed, self).__init__() self.fc = nn.Linear(hidden_size, output_size) self.hidden_size = hidden_size def forward(self, x): h0 = torch.zeros(1, x.size(0), self.hidden_size, dtype=torch.float, device=x.device) # Manually implement RNN behavior to an extent that is compatible with TorchScript out = self.custom_rnn(x, h0) out = self.fc(out[:, -1, :]) return out def custom_rnn(self, x, h0): # Simple RNN implementation - replace with detailed implementation suitable for TorchScript. for t in range(x.size(1)): h0 = torch.tanh(torch.add(torch.matmul(x[:, t, :], self.fc.weight.t()), self.fc.bias) + h0) return h0 input_size = 10 hidden_size = 20 output_size = 5 model_transformed = MyModelTransformed(input_size, hidden_size, output_size) scripted_model = torch.jit.script(model_transformed) torch.jit.save(scripted_model, \\"model_scripted.pt\\") ``` Your task is to provide your solution in a similar way that includes the transformed model, ensuring compatibility with TorchScript.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class MyModelTransformed(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(MyModelTransformed, self).__init__() self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size self.fc = nn.Linear(hidden_size, output_size) self.rnn_weight_ih = nn.Parameter(torch.randn(hidden_size, input_size)) self.rnn_weight_hh = nn.Parameter(torch.randn(hidden_size, hidden_size)) self.rnn_bias = nn.Parameter(torch.randn(hidden_size)) def forward(self, x): h0 = torch.zeros(1, x.size(0), self.hidden_size, dtype=torch.float, device=x.device) out = self.custom_rnn(x, h0) out = self.fc(out[:, -1, :]) return out def custom_rnn(self, x, h0): outputs = [] h_t = h0 for t in range(x.size(1)): x_t = x[:, t, :] h_t = torch.tanh(torch.addmm(self.rnn_bias, x_t, self.rnn_weight_ih.t()) + torch.mm(h_t.squeeze(0), self.rnn_weight_hh.t())) outputs.append(h_t.unsqueeze(1)) return torch.cat(outputs, dim=1) # Demonstration of converting and saving the model input_size = 10 hidden_size = 20 output_size = 5 model_transformed = MyModelTransformed(input_size, hidden_size, output_size) scripted_model = torch.jit.script(model_transformed) torch.jit.save(scripted_model, \\"model_scripted.pt\\")"},{"question":"Objective This task will assess your understanding of PyTorch\'s `torch.fx` module, particularly in graph manipulation and transformation of neural network models. Task You are given a PyTorch module, `SimpleModule`, which consists of several operations including addition. Your task is to: 1. Use the `torch.fx` module to perform symbolic tracing to get a graph representation of `SimpleModule`. 2. Perform a transformation on this graph to replace all `torch.add` operations with `torch.sub`. 3. Recompile the transformed graph into a new `torch.nn.Module`. 4. Verify that the new module produces the correct outputs. Detailed Steps 1. **Symbolic Tracing:** Create a function `trace_module` that takes a module as input and returns its graph representation. 2. **Graph Transformation:** Create a function `replace_add_with_sub` that takes a traced graph and returns a new graph where all `torch.add` operations are replaced with `torch.sub`. 3. **Recompilation:** Create a function `recompile_module` that takes the original module and the transformed graph and returns a new `torch.nn.Module`. 4. **Verification:** Create a function `verify_transformation` that takes the original and transformed modules and a test input, and verifies that the output of the transformed module matches the expected result of replacing addition with subtraction. Example Module The module you will start with: ```python import torch class SimpleModule(torch.nn.Module): def __init__(self): super(SimpleModule, self).__init__() self.linear = torch.nn.Linear(3, 3) def forward(self, x): return torch.add(self.linear(x), self.linear.weight) ``` Expected Functions 1. **trace_module:** ```python def trace_module(module: torch.nn.Module) -> torch.fx.Graph: Trace the given module to obtain its graph representation. Args: module (torch.nn.Module): The PyTorch module to trace. Returns: torch.fx.Graph: The graph representation of the module. pass ``` 2. **replace_add_with_sub:** ```python def replace_add_with_sub(graph: torch.fx.Graph) -> torch.fx.Graph: Transform the given graph to replace all torch.add calls with torch.sub. Args: graph (torch.fx.Graph): The original graph. Returns: torch.fx.Graph: The transformed graph. pass ``` 3. **recompile_module:** ```python def recompile_module(original_module: torch.nn.Module, transformed_graph: torch.fx.Graph) -> torch.nn.Module: Recompile the given graph into a new torch.nn.Module. Args: original_module (torch.nn.Module): The original module. transformed_graph (torch.fx.Graph): The transformed graph. Returns: torch.nn.Module: The newly compiled module with the transformed graph. pass ``` 4. **verify_transformation:** ```python def verify_transformation(original_module: torch.nn.Module, transformed_module: torch.nn.Module, test_input: torch.Tensor) -> bool: Verify that the transformed module produces the expected output. Args: original_module (torch.nn.Module): The original module. transformed_module (torch.nn.Module): The transformed module with replaced operations. test_input (torch.Tensor): The test input tensor to pass through the modules. Returns: bool: True if the transformation is correct, False otherwise. pass ``` Example Usage ```python # Define the original module original_module = SimpleModule() # Trace the module to get its graph graph = trace_module(original_module) # Transform the graph to replace additions with subtractions transformed_graph = replace_add_with_sub(graph) # Recompile the transformed graph into a new module transformed_module = recompile_module(original_module, transformed_graph) # Verify the transformation test_input = torch.randn(1, 3) assert verify_transformation(original_module, transformed_module, test_input), \\"Transformation verification failed!\\" ``` Submit the code for the functions `trace_module`, `replace_add_with_sub`, `recompile_module`, and `verify_transformation`.","solution":"import torch import torch.fx class SimpleModule(torch.nn.Module): def __init__(self): super(SimpleModule, self).__init__() self.linear = torch.nn.Linear(3, 3) def forward(self, x): return torch.add(self.linear(x), self.linear.weight) def trace_module(module: torch.nn.Module) -> torch.fx.GraphModule: Trace the given module to obtain its graph representation. Args: module (torch.nn.Module): The PyTorch module to trace. Returns: torch.fx.GraphModule: The graph representation of the module. tracer = torch.fx.Tracer() graph = tracer.trace(module) return torch.fx.GraphModule(module, graph) def replace_add_with_sub(graph_module: torch.fx.GraphModule) -> torch.fx.GraphModule: Transform the given graph to replace all torch.add calls with torch.sub. Args: graph_module (torch.fx.GraphModule): The original graph module. Returns: torch.fx.GraphModule: The transformed graph module. for node in graph_module.graph.nodes: if node.op == \'call_function\' and node.target == torch.add: with graph_module.graph.inserting_after(node): new_node = graph_module.graph.call_function(torch.sub, node.args, node.kwargs) node.replace_all_uses_with(new_node) graph_module.graph.erase_node(node) graph_module.recompile() return graph_module def recompile_module(original_module: torch.nn.Module, transformed_graph_module: torch.fx.GraphModule) -> torch.nn.Module: Recompile the given graph module into a new torch.nn.Module. Args: original_module (torch.nn.Module): The original module. transformed_graph_module (torch.fx.GraphModule): The transformed graph module. Returns: torch.nn.Module: The newly compiled module with the transformed graph. return transformed_graph_module def verify_transformation(original_module: torch.nn.Module, transformed_module: torch.nn.Module, test_input: torch.Tensor) -> bool: Verify that the transformed module produces the expected output. Args: original_module (torch.nn.Module): The original module. transformed_module (torch.nn.Module): The transformed module with replaced operations. test_input (torch.Tensor): The test input tensor to pass through the modules. Returns: bool: True if the transformation is correct, False otherwise. original_output = original_module(test_input) expected_output = original_module.linear(test_input) - original_module.linear.weight transformed_output = transformed_module(test_input) return torch.allclose(transformed_output, expected_output)"},{"question":"# URL Handling and Parsing Challenge Objective Write a function that fetches data from a given URL, processes the content to extract all the unique URLs within the page, and returns these URLs in a structured format. Function Signature ```python def fetch_and_extract_urls(url: str, base_url: str) -> list: pass ``` Input - `url` (str): The URL of the page to fetch and read. - `base_url` (str): The base URL for relative links encountered on the page. Output - Returns a list of unique URLs found in the page. The URLs should be fully qualified (i.e., including the protocol and domain). Constraints - The function should handle different types of URLs including absolute and relative links. - The function should ensure that each URL is properly formatted and unique. - The function should raise an appropriate exception if the URL cannot be accessed or any other error occurs during fetching or parsing. Example ```python url = \\"http://example.com\\" base_url = \\"http://example.com\\" result = fetch_and_extract_urls(url, base_url) # Example output might be: # [ # \\"http://example.com/page1\\", # \\"http://example.com/page2\\", # \\"http://anotherdomain.com\\" # ] ``` Constraints - The function must use `urllib.request` to fetch the data. - The function must use `urllib.parse` to handle URL parsing and construction. - Do not use any third-party libraries. Notes - While extracting URLs, consider both `<a>` tags with the `href` attribute as well as any URLs present in the `src` attributes of other tags (e.g., `<img>`, `<script>`). - Make sure to handle and convert relative URLs to absolute URLs using the base URL provided.","solution":"import urllib.request from urllib.parse import urljoin, urlparse from html.parser import HTMLParser class LinkExtractor(HTMLParser): def __init__(self, base_url): super().__init__() self.base_url = base_url self.links = set() def handle_starttag(self, tag, attrs): if tag in [\\"a\\", \\"img\\", \\"script\\"]: for attr in attrs: if tag == \\"a\\" and attr[0] == \\"href\\": self.links.add(urljoin(self.base_url, attr[1])) elif tag in [\\"img\\", \\"script\\"] and attr[0] == \\"src\\": self.links.add(urljoin(self.base_url, attr[1])) def fetch_and_extract_urls(url: str, base_url: str) -> list: try: response = urllib.request.urlopen(url) content_type = response.headers.get(\'Content-Type\') if \\"text/html\\" not in content_type: raise ValueError(\\"The content is not an HTML page\\") html_content = response.read().decode(\'utf-8\') parser = LinkExtractor(base_url) parser.feed(html_content) return list(parser.links) except Exception as e: raise e"},{"question":"# Event Scheduler Implementation You are tasked with implementing a custom event scheduler using the `sched` module. Your scheduler should be able to: 1. Schedule new events with both absolute and relative times. 2. Cancel events if needed. 3. Execute events at the correct time respecting their priority. 4. Provide a summary of scheduled events at any time. # Task Function 1: `schedule_event(sched_obj, is_absolute, time_value, priority, action, args=(), kwargs={})` This function schedules a new event using the provided `sched_obj`. - If `is_absolute` is `True`, the event should be scheduled with absolute time (`enterabs`). - If `is_absolute` is `False`, the event should be scheduled with relative time (`enter`). **Parameters:** - `sched_obj`: An instance of `sched.scheduler`. - `is_absolute`: A boolean indicating whether the time is absolute or relative. - `time_value`: The time value for the event (either absolute or relative). - `priority`: The priority of the event (integer, lower value means higher priority). - `action`: The function to be called when the event fires. - `args`: (optional) A tuple of positional arguments for the action. - `kwargs`: (optional) A dictionary of keyword arguments for the action. Function 2: `cancel_event(sched_obj, event)` This function cancels a scheduled event. **Parameters:** - `sched_obj`: An instance of `sched.scheduler`. - `event`: The event to be canceled (returned by `enter` or `enterabs`). Function 3: `run_scheduler(sched_obj, blocking=True)` This function runs all scheduled events. **Parameters:** - `sched_obj`: An instance of `sched.scheduler`. - `blocking`: If `True`, the scheduler runs until no more events are scheduled. If `False`, it executes events due to expire soonest and returns. Function 4: `get_scheduled_events(sched_obj)` This function returns a summary of all scheduled events in the format: ```python [(time, priority, action.__name__, args, kwargs), ...] ``` **Parameters:** - `sched_obj`: An instance of `sched.scheduler`. # Example Usage ```python import sched, time def print_message(msg=\\"default\\"): print(f\\"{time.time()}: {msg}\\") s = sched.scheduler(time.time, time.sleep) # Schedule events event1 = schedule_event(s, False, 5, 1, print_message, args=(\\"First message\\",)) event2 = schedule_event(s, True, time.time() + 10, 2, print_message, kwargs={\\"msg\\": \\"Second message\\"}) # Get and print scheduled events print(get_scheduled_events(s)) # Run the scheduler run_scheduler(s) # Cancel an event (if still pending) cancel_event(s, event1) ``` # Constraints - Ensure the code is thread-safe if executed in a multi-threaded environment. - Handle any exceptions that may arise during event execution without halting the scheduler. **Good luck!**","solution":"import sched import time def schedule_event(sched_obj, is_absolute, time_value, priority, action, args=(), kwargs={}): Schedules a new event using the provided sched_obj. Parameters: - sched_obj: An instance of sched.scheduler. - is_absolute: A boolean indicating whether the time is absolute or relative. - time_value: The time value for the event (either absolute or relative). - priority: The priority of the event (integer, lower value means higher priority). - action: The function to be called when the event fires. - args: A tuple of positional arguments for the action. - kwargs: A dictionary of keyword arguments for the action. Returns: - The event scheduled. if is_absolute: event = sched_obj.enterabs(time_value, priority, action, argument=args, kwargs=kwargs) else: event = sched_obj.enter(time_value, priority, action, argument=args, kwargs=kwargs) return event def cancel_event(sched_obj, event): Cancels a scheduled event. Parameters: - sched_obj: An instance of sched.scheduler. - event: The event to be canceled (returned by enter or enterabs). sched_obj.cancel(event) def run_scheduler(sched_obj, blocking=True): Runs all scheduled events. Parameters: - sched_obj: An instance of sched.scheduler. - blocking: If True, the scheduler runs until no more events are scheduled. If False, it executes events due to expire soonest and returns. sched_obj.run(blocking) def get_scheduled_events(sched_obj): Returns a summary of all scheduled events. Parameters: - sched_obj: An instance of sched.scheduler. Returns: - A list of tuples: (time, priority, action.__name__, args, kwargs) return [(e.time, e.priority, e.action.__name__, e.argument, e.kwargs) for e in sched_obj.queue]"},{"question":"Objective: To assess your understanding and ability to effectively use seaborn\'s `so.Plot` object for statistical data visualization. Task: Using the seaborn library, you will create a line plot with the given dataset and implement the following requirements: 1. **Load the Dataset:** - Load the \'fmri\' dataset using `seaborn.load_dataset(\\"fmri\\")`. 2. **Basic Line Plot:** - Create a line plot of the \'signal\' over \'timepoint\' using the `so.Plot` object. - Use the `region` variable to color the lines. 3. **Orientation:** - Create another plot similar to the one above but orient the line horizontally (i.e., the \'timepoint\' on y-axis and \'signal\' on x-axis). 4. **Grouped Lines:** - Filter the dataset to include only data where `region == \'frontal\'` and `event == \'stim\'`. - Create a plot that shows multiple lines for this filtered data, grouped by \'subject\'. Ensure each subject\'s line is plotted uniquely. 5. **Lines with Bands:** - Show error bands around your lines by using the `so.Band` class along with the lines. 6. **Markers:** - Add circular markers to the line to indicate data points where sampling occurred. Input Format: - No specific input requirements. The data should be loaded within the script. Output Format: - The script should generate the required plots and display them. Constraints: - Ensure that your plots are clearly labeled and colored for easier interpretation. - Use appropriate plot titles and axis labels to make the visualization self-explanatory. Implement the function `create_seaborn_plots` that performs the above tasks and displays the resulting plots. ```python import seaborn.objects as so from seaborn import load_dataset def create_seaborn_plots(): # Load the dataset fmri = load_dataset(\\"fmri\\") # Basic Line Plot plot1 = so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\").add(so.Line()) plot1.show() # Orientation plot2 = so.PPlot(fmri, x=\\"signal\\", y=\\"timepoint\\", color=\\"region\\").add(so.Line()) plot2.show() # Grouped Lines filtered_fmri = fmri.query(\\"region == \'frontal\' and event == \'stim\'\\") plot3 = so.Plot(filtered_fmri, \\"timepoint\\", \\"signal\\").add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") plot3.show() # Lines with Bands plot4 = ( so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") ) plot4.show() # Markers plot5 = plot4.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) plot5.show() # Call the function to execute it create_seaborn_plots() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_seaborn_plots(): # Load the dataset fmri = load_dataset(\\"fmri\\") # Basic Line Plot plot1 = so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\").add(so.Line()) plot1.show() plt.title(\'Signal over Time by Region\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') # Orientation plot2 = so.Plot(fmri, x=\\"signal\\", y=\\"timepoint\\", color=\\"region\\").add(so.Line()) plot2.show() plt.title(\'Signal over Time by Region (Horizontal)\') plt.xlabel(\'Signal\') plt.ylabel(\'Timepoint\') # Grouped Lines filtered_fmri = fmri.query(\\"region == \'frontal\' and event == \'stim\'\\") plot3 = so.Plot(filtered_fmri, \\"timepoint\\", \\"signal\\").add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") plot3.show() plt.title(\'Signal over Time for Frontal Region during Stim Event\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') # Lines with Bands plot4 = ( so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line()) .add(so.Band(), group=\\"event\\") ) plot4.show() plt.title(\'Signal over Time with Error Bands\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') # Markers plot5 = plot4.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) plot5.show() plt.title(\'Signal over Time with Error Bands and Markers\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') # Call the function to execute it create_seaborn_plots()"},{"question":"# Parsing and Analyzing Email Messages You are provided with a collection of email messages that you need to process. Each email message can be in the form of a bytes-like object, a string, or a file. The messages can be simple, non-MIME or MIME multipart messages. Your task is to implement a function `analyze_email_messages` that processes a list of email messages, parses them using the `email` package, and returns a summary of each message. The summary should include: 1. The subject of the email. 2. Whether the email is multipart or not. 3. The number of parts if the email is multipart. 4. Any defects found during parsing. Function Signature ```python from typing import List, Dict, Union def analyze_email_messages(messages: List[Union[str, bytes]]) -> List[Dict[str, Union[str, bool, int, List[str]]]]: ``` Input - `messages`: A list of email messages. Each message is either a string or bytes-like object. Output - A list of dictionaries, with each dictionary containing the summary of one email message. Each dictionary has the following keys: - `subject`: (str) The subject of the email. - `is_multipart`: (bool) Whether the email is multipart or not. - `part_count`: (int) The number of parts if the email is multipart, otherwise 0. - `defects`: (List[str]) A list of defect descriptions found during parsing, if any. Example ```python messages = [ b\\"Subject: Test emailnnThis is a test email.\\", \\"Subject: Multipart emailnMIME-Version: 1.0nContent-Type: multipart/mixed; boundary=\\"===============7330845974216740156==\\"nn--===============7330845974216740156==nContent-Type: text/plainnnThis is the body.n--===============7330845974216740156==nContent-Type: text/htmlnn<html><body>This is the HTML body.</body></html>n--===============7330845974216740156==--\\" ] assert analyze_email_messages(messages) == [ { \\"subject\\": \\"Test email\\", \\"is_multipart\\": False, \\"part_count\\": 0, \\"defects\\": [] }, { \\"subject\\": \\"Multipart email\\", \\"is_multipart\\": True, \\"part_count\\": 2, \\"defects\\": [] } ] ``` Constraints 1. You may assume that the messages are properly structured according to RFC standards. 2. You should handle both string and bytes-like message formats. 3. Utilize the appropriate parsing classes and methods from the `email` package for your implementation. 4. Optimize for readability and clarity of the code.","solution":"from typing import List, Dict, Union from email import message_from_bytes, message_from_string from email.message import EmailMessage from email.policy import default def analyze_email_messages(messages: List[Union[str, bytes]]) -> List[Dict[str, Union[str, bool, int, List[str]]]]: result = [] for message in messages: if isinstance(message, bytes): msg = message_from_bytes(message, policy=default) else: msg = message_from_string(message, policy=default) summary = { \\"subject\\": msg.get(\'subject\', \'\'), \\"is_multipart\\": msg.is_multipart(), \\"part_count\\": 0, \\"defects\\": [str(defect) for defect in msg.defects] } if msg.is_multipart(): summary[\\"part_count\\"] = len(msg.get_payload()) result.append(summary) return result"},{"question":"**Objective**: Demonstrate understanding of the `netrc` module for parsing `.netrc` files, handling exceptions, and fetching authentication details. **Problem Statement**: You are required to implement a function `get_authentication_details(file_path: str, host: str) -> tuple` that reads a `.netrc` file to extract authentication details for a specified host. # Function Specifications - **Function Name**: `get_authentication_details` - **Input Parameters**: - `file_path` (str): The path to the `.netrc` file. - `host` (str): The hostname for which the authentication details are to be fetched. - **Returns**: A tuple `(login, account, password)` containing authentication details for the specified host. If the host or default entry is not found, return `None`. - **Raises**: - `FileNotFoundError`: If the specified `.netrc` file does not exist. - `netrc.NetrcParseError`: If there are parse errors in the `.netrc` file. # Constraints and Notes - The function should handle the `netrc` module\'s `FileNotFoundError` and `NetrcParseError` appropriately by propagating these exceptions. - Ensure that the file is read using UTF-8 encoding first, as per the latest changes in version 3.10, before falling back to the locale-specific encoding. - Assume that the file permissions are correct and do not implement the POSIX permission checks. - You are not required to handle macros within the `.netrc` file. # Example ```python # Example .netrc file contents: # machine example.com # login testuser # password testpassword file_path = \'/path/to/.netrc\' host = \'example.com\' details = get_authentication_details(file_path, host) print(details) # Output should be (\'testuser\', None, \'testpassword\') ``` **Hints**: 1. Use `netrc.netrc(file)` to parse the `.netrc` file. 2. Use `authenticators(host)` method to get the authentication tuple for the specified host. ```python def get_authentication_details(file_path: str, host: str) -> tuple: import netrc try: nrc = netrc.netrc(file_path) return nrc.authenticators(host) except FileNotFoundError as fnf: raise fnf except netrc.NetrcParseError as npe: raise npe ```","solution":"def get_authentication_details(file_path: str, host: str) -> tuple: import netrc try: nrc = netrc.netrc(file_path) auth_details = nrc.authenticators(host) if auth_details is None: auth_details = nrc.authenticators(\'default\') return auth_details except FileNotFoundError as fnf: raise fnf except netrc.NetrcParseError as npe: raise npe"},{"question":"Advanced Line Plot Visualization Objective: To assess your understanding and ability to use seaborn for creating and customizing advanced line plots. Problem Statement: Given a dataset containing 10 years of monthly airline passenger data, perform the following tasks using seaborn: 1. **Load Data**: - Load the `flights` dataset using seaborn. 2. **Visualize Progress Over Time**: - Create a line plot that shows the total number of passengers per year. Ensure that the plot aggregates data across all months. 3. **Monthly Trends**: - Create a wide-form DataFrame where each month\'s passenger data is a column, and the index is the year. - Produce a line plot of this wide-form data to plot a separate line for each month. 4. **Grouped by Month**: - Create a long-form line plot showing the number of passengers for each month over time. - Use the `hue` parameter to differentiate between months and the `style` parameter to apply unique styles for each month. 5. **Customized Plot with Error Bars**: - Use the `fmri` dataset (also available in seaborn) to create a line plot showing the `signal` against the `timepoint`. - Use `hue` to differentiate between different `events`, and apply error bars that extend two standard errors wide. 6. **Combining Variables with Facets**: - Use the `relplot` function to create a facet grid line plot for the `fmri` dataset. The plot should: - Display separate subplots for different regions (`col=\'region\'`). - Use `hue` and `style` to differentiate between events. Input Format: - You do not need to handle input file reading; all required datasets are available through seaborn. Output: - For each task, display the relevant plots showing correct usage of seaborn functionalities. Constraints: - You must use seaborn and its functionalities for the plots. - Ensure your plots are clear and appropriately labeled. Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Task 2: Line plot of total passengers per year # (Your code for Task 2 goes here) # Task 3: Wide-form DataFrame and line plot for each month # (Your code for Task 3 goes here) # Task 4: Long-form line plot grouped by month # (Your code for Task 4 goes here) # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Task 5: Customized plot with error bars # (Your code for Task 5 goes here) # Task 6: Facet grid line plot for fmri dataset # (Your code for Task 6 goes here) plt.show() ``` Ensure your code and plots are readable and well-documented.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def load_flights_data(): return sns.load_dataset(\\"flights\\") def plot_total_passengers_per_year(flights): # Aggregating total passengers per year yearly_passengers = flights.groupby(\\"year\\").agg({\\"passengers\\": \\"sum\\"}).reset_index() plt.figure(figsize=(10, 6)) sns.lineplot(data=yearly_passengers, x=\\"year\\", y=\\"passengers\\") plt.title(\'Total Number of Passengers per Year\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.show() def plot_monthly_trends(flights): # Pivot table to create wide-form dataframe wide_form = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") plt.figure(figsize=(12, 8)) sns.lineplot(data=wide_form) plt.title(\'Monthly Trends in Number of Passengers\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() def plot_trends_grouped_by_month(flights): plt.figure(figsize=(14, 8)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", style=\\"month\\", markers=True, dashes=False) plt.title(\'Number of Passengers Over Time Grouped by Month\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() def plot_customized_error_bar(): fmri = sns.load_dataset(\\"fmri\\") plt.figure(figsize=(10, 6)) sns.lineplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", ci=\\"sd\\") plt.title(\'FMRI Signal Over Time with Error Bars\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.legend(title=\'Event\') plt.show() def facet_grid_line_plot(): fmri = sns.load_dataset(\\"fmri\\") g = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"event\\", col=\\"region\\", kind=\\"line\\", ci=\\"sd\\" ) g.set_titles(\\"Region: {col_name}\\") g.fig.suptitle(\'FMRI Signal Over Time by Region with Facets\', y=1.02) plt.show() def main(): # Task 1: Load Data flights = load_flights_data() # Task 2: Total passengers per year plot_total_passengers_per_year(flights) # Task 3: Monthly trends plot_monthly_trends(flights) # Task 4: Grouped by month plot_trends_grouped_by_month(flights) # Task 5: Customized plot with error bars plot_customized_error_bar() # Task 6: Facet plot facet_grid_line_plot() if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with implementing a function that computes the difference between two datetime objects and returns this difference in a human-readable string format. Function Signature ```python def datetime_difference(dt1, dt2): Compute and return the difference between two datetime objects. Parameters: dt1 (datetime.datetime): The first datetime object. dt2 (datetime.datetime): The second datetime object. Returns: str: A string describing the difference in the format: \\"X days, Y hours, Z minutes, W seconds\\" ``` Requirements 1. **Input:** - `dt1` and `dt2` are valid `datetime.datetime` objects. 2. **Output:** - The function should return a string describing the difference in the format of \\"X days, Y hours, Z minutes, W seconds\\". - If `dt1` is earlier than `dt2`, the difference should be positive. 3. **Constraints:** - You may not use any additional libraries beyond Python’s standard library. - You must check if the provided objects are valid `datetime.datetime` instances using the provided type-check macros. 4. **Performance:** - The function should run efficiently for any valid datetime objects. Example ```python from datetime import datetime dt1 = datetime(2021, 10, 25, 10, 00, 00) dt2 = datetime(2021, 10, 25, 12, 30, 45) difference = datetime_difference(dt1, dt2) print(difference) # Output: \\"0 days, 2 hours, 30 minutes, 45 seconds\\" ``` Hints - Utilize the `datetime.timedelta` class for computing differences between datetime objects. - Use the provided macros from the `datetime` library to validate and extract necessary fields from the datetime objects.","solution":"from datetime import datetime def datetime_difference(dt1, dt2): Compute and return the difference between two datetime objects. Parameters: dt1 (datetime.datetime): The first datetime object. dt2 (datetime.datetime): The second datetime object. Returns: str: A string describing the difference in the format: \\"X days, Y hours, Z minutes, W seconds\\" if not isinstance(dt1, datetime) or not isinstance(dt2, datetime): raise ValueError(\\"Both arguments must be datetime objects\\") difference = abs(dt2 - dt1) days = difference.days hours, remainder = divmod(difference.seconds, 3600) minutes, seconds = divmod(remainder, 60) return f\\"{days} days, {hours} hours, {minutes} minutes, {seconds} seconds\\""},{"question":"You are tasked with creating a utility that benchmarks the performance of a given list of Python functions. Specifically, you need to implement a function named `benchmark_functions` that: 1. Accepts a list of function names as strings. 2. Accepts a setup string to import or define the functions in the list. 3. Accepts the number of iterations for time measurements. 4. Times the execution of these functions using the `timeit` module. 5. Compares their execution times and returns a dictionary where the keys are function names and values are their respective execution times. # Constraints: - Each function will take exactly one argument, which will be an integer. - The function must handle exceptions and print relevant tracebacks using `print_exc` if a function raises an error during execution. - The setup code will include the imports or function definitions necessary for the functions in the list. # Function Signature: ```python def benchmark_functions(function_names: List[str], setup: str, iterations: int) -> Dict[str, float]: pass ``` # Input: - `function_names`: List (e.g., `[\\"func1\\", \\"func2\\", \\"func3\\"]`) - `setup`: String (e.g., `\\"from my_module import func1, func2, func3\\"`) - `iterations`: Integer (e.g., `100000`) # Output: - Dictionary with function names as keys and their execution times (in seconds) as values (e.g., `{\\"func1\\": 0.034, \\"func2\\": 0.045, \\"func3\\": 0.029}`) # Example: ```python def benchmark_functions(function_names, setup, iterations): import timeit results = {} for func in function_names: try: timer = timeit.Timer(f\\"{func}(42)\\", setup=setup) results[func] = timer.timeit(number=iterations) except Exception as e: timer.print_exc() results[func] = None return results # Example usage setup_code = def func1(n): return n**2 def func2(n): return n**3 def func3(n): return n**4 function_list = [\\"func1\\", \\"func2\\", \\"func3\\"] iterations = 100000 print(benchmark_functions(function_list, setup_code, iterations)) ``` The student is expected to use the `timeit` module\'s functionalities correctly, ensure the setup code is executed within the `Timer` instance, and handle any exceptions that occur during the timing process.","solution":"import timeit import traceback from typing import List, Dict def benchmark_functions(function_names: List[str], setup: str, iterations: int) -> Dict[str, float]: Benchmarks the execution time of specified functions. :param function_names: List of function names as strings. :param setup: String of setup code to import or define the functions. :param iterations: Number of iterations to run for each function. :return: Dictionary where keys are function names and values are their execution times. results = {} for func in function_names: try: timer = timeit.Timer(f\\"{func}(42)\\", setup=setup) results[func] = timer.timeit(number=iterations) except Exception as e: traceback.print_exc() results[func] = None return results"},{"question":"# Problem: Tumor Detection using Tuned Decision Threshold Your task is to implement a function that trains a binary classifier for tumor detection and tunes its decision threshold to prioritize high recall using cross-validation. In this context, recall is of utmost importance as we aim to minimize false negatives, even if it comes at the cost of increased false-positive rates. Requirements: 1. **Function Name**: `train_tuned_model` 2. **Inputs**: - `X_train`: A 2D list or numpy array of shape (n_samples, n_features) representing the training data. - `y_train`: A list or numpy array of length n_samples containing binary class labels (0 or 1) for the training data. - `estimator`: A scikit-learn classifier instance to be used as the base model. - `pos_label`: An integer (0 or 1) indicating the label of interest for which recall needs to be maximized. - `cv`: An integer indicating the number of cross-validation folds. 3. **Output**: - The function should return the trained `TunedThresholdClassifierCV` instance. Constraints: - Ensure that the function uses `TunedThresholdClassifierCV` with recall as the scoring metric. - Use `make_scorer` to properly set `pos_label` for the recall score. - Train the model while optimizing the decision threshold using the provided cross-validation splits. Example Usage: ```python from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification # Generate a synthetic dataset with 10% positive class X_train, y_train = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=0) # Instantiate a Logistic Regression model base_model = LogisticRegression() # Call the function to train and tune the model tuned_model = train_tuned_model(X_train, y_train, base_model, pos_label=1, cv=5) # Use the tuned model to make predictions predictions = tuned_model.predict(X_train) ``` Note: - The solution must accurately account for the provided `pos_label` when creating the recall scorer. - Make sure to handle any imports required within the function. Good luck and happy coding!","solution":"from sklearn.metrics import make_scorer, recall_score from sklearn.model_selection import cross_val_score, StratifiedKFold from sklearn.base import BaseEstimator, ClassifierMixin, clone import numpy as np class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, base_estimator, pos_label=1, cv=5): self.base_estimator = base_estimator self.pos_label = pos_label self.cv = cv self.best_threshold_ = 0.5 def fit(self, X, y): skf = StratifiedKFold(n_splits=self.cv) thresholds = np.arange(0.1, 1.0, 0.1) best_recall = 0 for train_idx, test_idx in skf.split(X, y): X_train, X_test = X[train_idx], X[test_idx] y_train, y_test = y[train_idx], y[test_idx] estimator = clone(self.base_estimator) estimator.fit(X_train, y_train) probs = estimator.predict_proba(X_test)[:, 1] for threshold in thresholds: y_pred = (probs >= threshold).astype(int) recall = recall_score(y_test, y_pred, pos_label=self.pos_label) if recall > best_recall: best_recall = recall self.best_threshold_ = threshold self.base_estimator.fit(X, y) return self def predict(self, X): probs = self.base_estimator.predict_proba(X)[:, 1] return (probs >= self.best_threshold_).astype(int) def train_tuned_model(X_train, y_train, estimator, pos_label, cv): Trains a binary classifier and tunes its decision threshold to prioritize high recall using cross-validation. Parameters: - X_train: 2D list or numpy array of shape (n_samples, n_features) - y_train: List or numpy array of length n_samples containing binary class labels - estimator: A scikit-learn classifier instance to be used as the base model - pos_label: An integer indicating the label of interest for which recall needs to be maximized - cv: An integer indicating the number of cross-validation folds Returns: - The trained TunedThresholdClassifierCV instance. tuned_model = TunedThresholdClassifierCV(base_estimator=estimator, pos_label=pos_label, cv=cv) tuned_model.fit(X_train, y_train) return tuned_model"},{"question":"Objective Demonstrate your understanding of the `shlex` module by implementing a function that securely constructs and executes a shell command using user inputs. Problem Statement You are tasked with creating a function that compiles a secure shell command based on a list of received filenames and a command template. The command will be executed using the `subprocess` module, ensuring that filenames are appropriately quoted for shell safety. Function Signature ```python import subprocess from shlex import quote, split def secure_shell_command(command_template: str, filenames: list): Compiles a secure shell command from a list of filenames and executes it. Args: - command_template (str): The shell command template as a string. It should include a placeholder \'{}\' for filenames. - filenames (list): A list of filenames to be plugged into the command template. Returns: - str: The standard output of the executed command. Raises: - ValueError: if `command_template` doesn’t contain \'{}\' as placeholder. - subprocess.CalledProcessError: If the command returns a non-zero exit status. ``` Constraints 1. The `command_template` must include `{}` as a placeholder for filenames. 2. The function should safely handle any special characters in filenames. 3. The function should return the standard output of the executed command. Instructions 1. Use the `quote` function from the `shlex` module to safely quote each filename. 2. Substitute the quoted filenames into the `command_template`. 3. Use `subprocess.run` to execute the constructed command. 4. Ensure that execution errors are appropriately handled and returned. Example ```python # Example usage command_template = \\"ls -l {}\\" filenames = [\\"somefile.txt\\", \\"another file with spaces.txt\\", \\"unsafe&file;name.txt\\"] output = secure_shell_command(command_template, filenames) print(output) ``` Notes - The example should be used for testing the correctness of your function. - Ensure the function is tested with various filename inputs that include spaces and special characters to verify the security and correctness.","solution":"import subprocess from shlex import quote def secure_shell_command(command_template: str, filenames: list): Compiles a secure shell command from a list of filenames and executes it. Args: - command_template (str): The shell command template as a string. It should include a placeholder \'{}\' for filenames. - filenames (list): A list of filenames to be plugged into the command template. Returns: - str: The standard output of the executed command. Raises: - ValueError: if `command_template` doesn’t contain \'{}\' as placeholder. - subprocess.CalledProcessError: If the command returns a non-zero exit status. if \'{}\' not in command_template: raise ValueError(\\"The command template must contain \'{}\' as a placeholder.\\") quoted_filenames = \' \'.join([quote(filename) for filename in filenames]) command = command_template.replace(\'{}\', quoted_filenames) result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) return result.stdout"},{"question":"Problem Statement You are tasked with writing a function that processes a list of dictionaries representing students\' scores in different subjects. Your goal is to compute the average score of each student and determine the grade based on the average score. Grades are assigned as follows: - A if the average score is 90 or above. - B if the average score is between 80 and 89 (inclusive). - C if the average score is between 70 and 79 (inclusive). - D if the average score is between 60 and 69 (inclusive). - F if the average score is below 60. Additionally, your function should accept additional keyword arguments that can override the default grading cutoff values. Input and Output Formats - **Input:** - A list of dictionaries, where each dictionary contains a student\'s \\"name\\" (a string) and subject scores (key-value pairs where keys are subject names and values are scores). - Optional keyword arguments to specify custom grading cutoffs. - **Output:** - A list of dictionaries, where each dictionary contains a student\'s \\"name\\", their \\"average\\" score, and their \\"grade\\". Function Signature ```python def calculate_grades(students: list, **grade_cutoffs) -> list: pass ``` Example ```python students = [ {\\"name\\": \\"Alice\\", \\"math\\": 95, \\"science\\": 85, \\"history\\": 80}, {\\"name\\": \\"Bob\\", \\"math\\": 70, \\"science\\": 75, \\"history\\": 60}, {\\"name\\": \\"Charlie\\", \\"math\\": 50, \\"science\\": 45, \\"history\\": 55} ] # Default grading cutoffs grades = calculate_grades(students) # Output: [{\'name\': \'Alice\', \'average\': 86.67, \'grade\': \'B\'}, {\'name\': \'Bob\', \'average\': 68.33, \'grade\': \'D\'}, {\'name\': \'Charlie\', \'average\': 50.0, \'grade\': \'F\'}] # Custom grading cutoffs grades = calculate_grades(students, A=85, B=75, C=65, D=55) # Output: [{\'name\': \'Alice\', \'average\': 86.67, \'grade\': \'A\'}, {\'name\': \'Bob\', \'average\': 68.33, \'grade\': \'C\'}, {\'name\': \'Charlie\', \'average\': 50.0, \'grade\': \'F\'}] ``` Constraints - Each student will have scores in at least one subject. - Scores are integer values between 0 and 100. - You can assume that the input list of students and their scores are valid (i.e., no need to handle invalid data types or formats). Your implementation should demonstrate a clear understanding of: - Iterating through lists and dictionaries. - Calculating averages. - Using `if`, `elif`, and `else` statements for control flow. - Handling optional keyword arguments in functions.","solution":"def calculate_grades(students: list, **grade_cutoffs) -> list: Calculate the average scores and grades for a given list of students based on their scores in various subjects. Keyword Arguments: grade_cutoffs -- custom grading cutoff values (A, B, C, D). Returns: A list of dictionaries containing each student\'s name, their average score, and their grade. # Default grading cutoffs grade_boundaries = { \'A\': 90, \'B\': 80, \'C\': 70, \'D\': 60 } # Override default cutoffs with provided keyword arguments if any grade_boundaries.update(grade_cutoffs) def compute_grade(average): if average >= grade_boundaries[\'A\']: return \'A\' elif average >= grade_boundaries[\'B\']: return \'B\' elif average >= grade_boundaries[\'C\']: return \'C\' elif average >= grade_boundaries[\'D\']: return \'D\' else: return \'F\' result = [] for student in students: name = student[\'name\'] scores = [v for k, v in student.items() if k != \'name\'] if scores: average = sum(scores) / len(scores) grade = compute_grade(average) result.append({ \'name\': name, \'average\': round(average, 2), \'grade\': grade }) return result"},{"question":"Given is a brief documentation on weak references in Python, here is a coding assessment question: # Objective: Demonstrate your understanding of weak references by implementing a caching mechanism for a class `ExpensiveObject`. This cache should hold weak references to ensure that the objects can still be garbage collected when no longer in use elsewhere in the program. # Requirements: 1. Implement a class `ExpensiveObject` that takes a single argument during initialization. 2. Implement a class `Cache` which keeps weak references to `ExpensiveObject` instances. 3. The `Cache` class should: - Provide a method `get_or_create` which either retrieves a cached instance of `ExpensiveObject` with a specified value if it exists, or creates and caches a new instance of the object if it does not. - Ensure that only `ExpensiveObject` instances with a specified value (provided during initialization) can be retrieved or created. - Handle the cleanup of weak references and remove them from the cache when the objects are collected. # Constraints: - You must use Python\'s `weakref` module. - You may assume that all `ExpensiveObject` values are unique. # Example: ```python class ExpensiveObject: pass class Cache: def get_or_create(self, value): pass # Usage cache = Cache() obj1 = cache.get_or_create(\'value1\') obj2 = cache.get_or_create(\'value2\') obj1_again = cache.get_or_create(\'value1\') assert obj1 is obj1_again # Should be True assert obj1 is not obj2 # Should be True ```","solution":"import weakref class ExpensiveObject: def __init__(self, value): self.value = value class Cache: def __init__(self): self._cache = weakref.WeakValueDictionary() def get_or_create(self, value): if value in self._cache: return self._cache[value] else: obj = ExpensiveObject(value) self._cache[value] = obj return obj"},{"question":"You are required to write a function that takes the name of a seaborn dataset and creates a series of visualizations to analyze the distribution of a specified continuous variable and its relationship with a categorical variable. Implement a function called `create_custom_plots` that accomplishes the following tasks: 1. Loads a specified seaborn dataset. 2. Creates a histogram to show the distribution of a specified continuous variable with the default number of bins. 3. Creates a normalized histogram to show proportions. 4. Creates faceted histograms for each category within a specified categorical variable, normalized within each category. 5. Creates an overlaid density plot of the continuous variable, grouped by a specified categorical variable. 6. Creates a stacked bar chart showing the proportions of the continuous variable, grouped by the specified categorical variable. Function Signature ```python def create_custom_plots(dataset_name: str, continuous_var: str, categorical_var: str) -> None: pass ``` Input - `dataset_name` (str): The name of the dataset to be loaded (e.g., `\\"penguins\\"`). - `continuous_var` (str): The name of the continuous variable to be analyzed (e.g., `\\"flipper_length_mm\\"`). - `categorical_var` (str): The name of the categorical variable to be used for grouping and faceting (e.g., `\\"island\\"`). Output - The function does not return any value but should display the following plots. Each plot should be properly titled and labeled: 1. A histogram of the continuous variable. 2. A normalized histogram of the continuous variable. 3. Faceted histograms for each category of the categorical variable, normalized within each facet. 4. An overlaid density plot of the continuous variable by the categorical variable. 5. A stacked bar chart showing the proportions of the continuous variable, grouped by the categorical variable. Constraints - Assume the dataset is available in seaborn\'s repository. - Make sure to handle cases where the specified columns do not exist in the dataset by printing a meaningful error message. # Example Usage ```python create_custom_plots(\'penguins\', \'flipper_length_mm\', \'island\') ``` This should create the specified plots for the `penguins` dataset using `flipper_length_mm` as the continuous variable and `island` as the categorical variable.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_custom_plots(dataset_name: str, continuous_var: str, categorical_var: str) -> None: Creates a series of visualizations to analyze the distribution of a specified continuous variable and its relationship with a categorical variable. try: data = sns.load_dataset(dataset_name) except: print(f\\"Dataset \'{dataset_name}\' not found in seaborn\'s repository.\\") return if continuous_var not in data.columns: print(f\\"Continuous variable \'{continuous_var}\' does not exist in the dataset.\\") return if categorical_var not in data.columns: print(f\\"Categorical variable \'{categorical_var}\' does not exist in the dataset.\\") return # 1. Histogram of the continuous variable plt.figure(figsize=(10, 6)) sns.histplot(data[continuous_var], kde=False) plt.title(f\'Histogram of {continuous_var}\') plt.xlabel(continuous_var) plt.ylabel(\'Frequency\') plt.show() # 2. Normalized histogram of the continuous variable plt.figure(figsize=(10, 6)) sns.histplot(data[continuous_var], kde=False, stat=\\"probability\\") plt.title(f\'Normalized Histogram of {continuous_var}\') plt.xlabel(continuous_var) plt.ylabel(\'Proportion\') plt.show() # 3. Faceted histograms for each category of the categorical variable g = sns.displot(data, x=continuous_var, col=categorical_var, stat=\\"probability\\", col_wrap=3) g.set_titles(\\"{col_name}\\") g.set_axis_labels(continuous_var, \\"Proportion\\") plt.subplots_adjust(top=0.9) g.fig.suptitle(f\'Faceted Normalized Histograms of {continuous_var} by {categorical_var}\') plt.show() # 4. Overlaid density plot of the continuous variable by the categorical variable plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=continuous_var, hue=categorical_var, common_norm=False) plt.title(f\'Overlaid Density Plot of {continuous_var} by {categorical_var}\') plt.xlabel(continuous_var) plt.ylabel(\'Density\') plt.show() # 5. Stacked bar chart (requires binning the continuous variable) data[\'binned\'] = pd.cut(data[continuous_var], bins=10) proportions = data.groupby([categorical_var, \'binned\']).size().groupby(level=0).apply(lambda x: x / x.sum()).unstack() proportions.plot(kind=\'bar\', stacked=True) plt.title(f\'Stacked Bar Chart of Binned {continuous_var} by {categorical_var}\') plt.xlabel(categorical_var) plt.ylabel(\'Proportion\') plt.legend(title=f\'Binned {continuous_var}\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show()"},{"question":"Coding Assessment Question # Title: Unicode Normalization and File Encoding Conversion # Objective: To assess students\' understanding of Unicode handling, normalization, and file operations in Python. The task requires implementing a function to normalize Unicode strings and save them to a file in a specified encoding. # Problem Statement: You are provided with a list of Unicode strings, some of which may contain characters represented by different sequences of code points (e.g., \'ê\' could be U+00EA or U+0065 U+0302). Your task is to normalize all strings to the NFC form (Canonical Composition) and write them to a specified file in UTF-8 encoding. Implement the function `normalize_and_write_to_file(unicode_strings: List[str], file_path: str):` Function Details - **Input:** - `unicode_strings` (List[str]): A list of Unicode strings to be normalized and written to the file. - `file_path` (str): Path to the file where the normalized strings should be saved. - **Output:** - None. The function should write the normalized strings to the specified file, each string on a new line. - **Constraints:** - Ensure the output file is encoded in UTF-8. - Assume the input list will contain at least one string. - The function should handle file operations securely and must close the file after writing. - **Requirements:** - Use `unicodedata.normalize()` to normalize the Unicode strings. - Handle file operations using `with open` statement. # Example: ```python from typing import List import unicodedata def normalize_and_write_to_file(unicode_strings: List[str], file_path: str): with open(file_path, \'w\', encoding=\'utf-8\') as file: for u_str in unicode_strings: normalized_str = unicodedata.normalize(\'NFC\', u_str) file.write(normalized_str + \'n\') # Example usage unicode_strings = [ \\"ê\\", # Single character U+00EA \\"u0065u0302\\", # Combining sequence for \'ê\' \\"éclair\\", # Contains \'é\' U+00E9 \\"u0065u0301clair\\" # Combining sequence for \'éclair\' ] file_path = \'output.txt\' normalize_and_write_to_file(unicode_strings, file_path) ``` After executing the above example, the `output.txt` file should contain: ``` ê ê éclair éclair ``` # Notes: - The `normalize_and_write_to_file` function should strictly comply with the UTF-8 encoding requirement and handle any possible exceptions for robustness. - Ensure that each Unicode string is written on a new line in the output file.","solution":"from typing import List import unicodedata def normalize_and_write_to_file(unicode_strings: List[str], file_path: str): Normalizes Unicode strings to NFC form and writes them to a specified file in UTF-8 encoding. :param unicode_strings: List[str]: A list of Unicode strings to be normalized and written to the file. :param file_path: str: Path to the file where the normalized strings should be saved. # Open the file in write mode with utf-8 encoding with open(file_path, \'w\', encoding=\'utf-8\') as file: # Normalize each string and write to file for u_str in unicode_strings: normalized_str = unicodedata.normalize(\'NFC\', u_str) file.write(normalized_str + \'n\')"},{"question":"You are required to implement a command-line utility that calculates basic statistics (mean, median, standard deviation) for a list of numbers provided by the user. The utility should accept options for each of these statistics and output the corresponding values. Use the `optparse` module to handle the command-line options. # Input Format: - The utility must accept the following command-line options: - `-m` or `--mean`: Calculate the mean of the numbers. - `-d` or `--median`: Calculate the median of the numbers. - `-s` or `--std`: Calculate the standard deviation of the numbers. - `-f FILE` or `--file=FILE`: Read the list of numbers from a specified file. If this option is skipped, the numbers should be read from standard input. # Output Format: - Based on the options provided, the utility should print the calculated statistics to the console, each on a new line. # Constraints: - The list of numbers may contain up to 100,000 entries. - Each number is a valid floating-point number. # Performance Requirements: - Your implementation should efficiently handle the input size and perform calculations in a reasonable time. # Example Usage: ```bash # Command to calculate mean and median from a file python statistics_util.py --file=numbers.txt -m -d # Command to calculate all statistics from standard input python statistics_util.py -m -d -s ``` # Example Output: ``` Mean: 50.5 Median: 50.0 Standard Deviation: 29.01149 ``` # Implementation Notes: - Use `optparse` to handle the command-line options. - You may use the `statistics` module in Python for calculating mean, median, and standard deviation. ```python from optparse import OptionParser import statistics def main(): usage = \\"usage: %prog [options] arg\\" parser = OptionParser(usage=usage) parser.add_option(\\"-m\\", \\"--mean\\", action=\\"store_true\\", dest=\\"mean\\", help=\\"Calculate mean\\") parser.add_option(\\"-d\\", \\"--median\\", action=\\"store_true\\", dest=\\"median\\", help=\\"Calculate median\\") parser.add_option(\\"-s\\", \\"--std\\", action=\\"store_true\\", dest=\\"std\\", help=\\"Calculate standard deviation\\") parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"Read numbers from file\\", metavar=\\"FILE\\") (options, args) = parser.parse_args() if options.filename: with open(options.filename, \'r\') as file: numbers = [float(line.strip()) for line in file if line.strip()] else: import sys numbers = [float(line.strip()) for line in sys.stdin if line.strip()] if options.mean: print(f\\"Mean: {statistics.mean(numbers)}\\") if options.median: print(f\\"Median: {statistics.median(numbers)}\\") if options.std: print(f\\"Standard Deviation: {statistics.stdev(numbers)}\\") if __name__ == \\"__main__\\": main() ``` Test your implementation thoroughly to ensure it handles all edge cases and meets the performance requirements.","solution":"from optparse import OptionParser import statistics import sys def calculate_statistics(numbers, calculate_mean, calculate_median, calculate_std): results = {} if calculate_mean: results[\'mean\'] = statistics.mean(numbers) if calculate_median: results[\'median\'] = statistics.median(numbers) if calculate_std: results[\'std\'] = statistics.stdev(numbers) return results def main(): usage = \\"usage: %prog [options] arg\\" parser = OptionParser(usage=usage) parser.add_option(\\"-m\\", \\"--mean\\", action=\\"store_true\\", dest=\\"mean\\", help=\\"Calculate mean\\") parser.add_option(\\"-d\\", \\"--median\\", action=\\"store_true\\", dest=\\"median\\", help=\\"Calculate median\\") parser.add_option(\\"-s\\", \\"--std\\", action=\\"store_true\\", dest=\\"std\\", help=\\"Calculate standard deviation\\") parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"Read numbers from file\\", metavar=\\"FILE\\") (options, args) = parser.parse_args() numbers = [] if options.filename: with open(options.filename, \'r\') as file: numbers = [float(line.strip()) for line in file if line.strip()] else: numbers = [float(line.strip()) for line in sys.stdin if line.strip()] results = calculate_statistics(numbers, options.mean, options.median, options.std) for key, value in results.items(): print(f\\"{key.capitalize()}: {value}\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective Write a Python function that uses the `sysconfig` module to generate a summary of installation schemes and their associated paths for the current platform. Task Implement a function `generate_installation_summary` that: 1. Retrieves all supported installation schemes. 2. For each scheme, gathers all the associated installation paths. 3. Outputs a summary dictionary where each key is a scheme name and each value is another dictionary mapping path names to their corresponding paths. Input This function does not take any inputs. Output The function should return a dictionary structured as follows: ```python { \\"scheme_name1\\": { \\"path_name1\\": \\"path1\\", \\"path_name2\\": \\"path2\\", ... }, \\"scheme_name2\\": { \\"path_name1\\": \\"path1\\", \\"path_name2\\": \\"path2\\", ... }, ... } ``` Constraints - Use the `sysconfig` module to retrieve and expand the required values. - Handle any platform-related peculiarities that might affect the availability of certain schemes or paths. Performance Requirements The solution should be efficient and not unnecessarily recompute values. The entire process should complete within a reasonable time frame even for systems with many installation schemes and paths. Example ```python import sysconfig def generate_installation_summary(): # Your implementation here # Example usage: summary = generate_installation_summary() print(summary) ``` The output dictionary will vary based on the platform and Python installation, but it should follow the described format.","solution":"import sysconfig def generate_installation_summary(): Generates a summary of installation schemes and their associated paths. :return: Dictionary containing scheme names mapped to their respective paths # Retrieve all the supported schemes schemes = sysconfig.get_scheme_names() # Create a dictionary to store the summary summary = {} # Iterate through each scheme for scheme in schemes: # Retrieve all paths for the current scheme paths = sysconfig.get_paths(scheme) # Add scheme and its paths to the summary dictionary summary[scheme] = paths return summary"},{"question":"Objective Demonstrate your understanding of Python\'s Abstract Base Classes (ABCs) in the `collections.abc` module by implementing a custom iterable container using one or more of these ABCs. Task Implement a custom class `CustomList` that behaves like a sequence but with additional constraints: 1. It should inherit from the Abstract Base Class `collections.abc.Sequence`. 2. It should store even numbers only and ignore any odd numbers added to it. 3. Implement the following methods: - `__init__(self, iterable)`: Initialize the container with an iterable of numbers. - `__getitem__(self, index)`: Get the item at the specified index. - `__len__(self)`: Return the length of the container. - `append(self, value)`: Add a value to the container (ignore if odd). - `__contains__(self, value)`: Check if a value is in the container. Specifications - **Input**: You will initialize your `CustomList` with an iterable (e.g., list or tuple) of integers. - **Output**: The class should allow access to its elements using indexing, len(), and in checks. It should also allow appending elements conditionally. Constraints - Your implementation should strictly follow the interface of `collections.abc.Sequence`. - Ensure the `append` method only accepts even numbers. - Do not use any libraries outside of Python\'s standard library. Examples ```python from collections.abc import Sequence class CustomList(Sequence): def __init__(self, iterable): self.data = [item for item in iterable if item % 2 == 0] def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) def append(self, value): if value % 2 == 0: self.data.append(value) def __contains__(self, value): return value in self.data # Example usage clist = CustomList([1, 2, 3, 4, 5, 6]) print(clist[1]) # Output should be 2 print(len(clist)) # Output should be 3 clist.append(8) print(clist) # Output should show the internal even number list: [2, 4, 6, 8] print(3 in clist) # Output should be False print(4 in clist) # Output should be True ``` Write your implementation of the `CustomList` class below: ```python from collections.abc import Sequence class CustomList(Sequence): def __init__(self, iterable): self.data = [item for item in iterable if item % 2 == 0] def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) def append(self, value): if value % 2 == 0: self.data.append(value) def __contains__(self, value): return value in self.data ```","solution":"from collections.abc import Sequence class CustomList(Sequence): def __init__(self, iterable): Initialize the container with an iterable of numbers, storing only even numbers. self.data = [item for item in iterable if item % 2 == 0] def __getitem__(self, index): Get the item at the specified index. return self.data[index] def __len__(self): Return the length of the container. return len(self.data) def append(self, value): Add a value to the container if it\'s even, otherwise ignore it. if value % 2 == 0: self.data.append(value) def __contains__(self, value): Check if a value is in the container. return value in self.data"},{"question":"Objective The goal is to implement a solution that uses both `ThreadPoolExecutor` and `ProcessPoolExecutor` to manage heterogeneous tasks concurrently. You will need to ensure that the code handles exceptions gracefully and efficiently manages resources. Description You are given two types of tasks: I/O-bound tasks that involve fetching URLs and CPU-bound tasks that involve computing prime numbers. 1. **I/O-Bound Tasks**: These tasks fetch the content from a list of URLs and return the content length. 2. **CPU-Bound Tasks**: These tasks compute whether a given number is prime. Your task is to fetch URLs using `ThreadPoolExecutor` and check for prime numbers using `ProcessPoolExecutor`. Implement the following function: ```python import concurrent.futures import urllib.request import math def fetch_url(url, timeout=60): with urllib.request.urlopen(url, timeout=timeout) as conn: return len(conn.read()) def is_prime(n): if n < 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.sqrt(n)) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True def execute_tasks(urls, numbers, io_max_workers=5, cpu_max_workers=2): results = {} # Fetch URLs using ThreadPoolExecutor with concurrent.futures.ThreadPoolExecutor(max_workers=io_max_workers) as thread_executor: future_to_url = {thread_executor.submit(fetch_url, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: results[url] = future.result() except Exception as exc: results[url] = f\\"Generated an exception: {exc}\\" # Calculate primes using ProcessPoolExecutor with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_max_workers) as process_executor: future_to_number = {process_executor.submit(is_prime, num): num for num in numbers} for future in concurrent.futures.as_completed(future_to_number): num = future_to_number[future] try: results[num] = future.result() except Exception as exc: results[num] = f\\"Generated an exception: {exc}\\" return results ``` Parameters - `urls`: List of URLs to fetch. - `numbers`: List of integers to check for primality. - `io_max_workers`: Maximum number of threads for handling URL fetching (default is 5). - `cpu_max_workers`: Maximum number of processes for handling prime number computation (default is 2). Constraints 1. Handle possible exceptions during URL fetching and prime number calculation. 2. Ensure that resources are efficiently managed using context managers. 3. Assume that the URLs and numbers provided are valid and the length of lists will be at most 100. Example ```python urls = [\'http://www.example.com\', \'http://www.python.org\'] numbers = [29, 15, 23, 77] print(execute_tasks(urls, numbers)) ``` Expected Output (example): ```python { \'http://www.example.com\': 1256, \'http://www.python.org\': 5890, 29: True, 15: False, 23: True, 77: False } ``` The function should complete all tasks concurrently and handle any exceptions that arise, returning results as a dictionary where keys are URLs and integers and values are content lengths and boolean primality results respectively.","solution":"import concurrent.futures import urllib.request import math def fetch_url(url, timeout=60): Fetches the content from the given URL and returns its length. Parameters: - url (str): The URL to fetch. - timeout (int): The timeout for the URL fetch operation. Returns: - int: The length of the content fetched from the URL. with urllib.request.urlopen(url, timeout=timeout) as conn: return len(conn.read()) def is_prime(n): Determines if the given number is prime. Parameters: - n (int): The number to check for primality. Returns: - bool: True if the number is prime, False otherwise. if n < 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.sqrt(n)) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True def execute_tasks(urls, numbers, io_max_workers=5, cpu_max_workers=2): Executes URL fetching tasks with a ThreadPoolExecutor and prime number checking tasks with a ProcessPoolExecutor. Parameters: - urls (list): A list of URLs to fetch. - numbers (list): A list of numbers to check for primality. - io_max_workers (int): Maximum number of threads for URL fetching. - cpu_max_workers (int): Maximum number of processes for prime number checking. Returns: - dict: A dictionary containing the results of the tasks. results = {} # Fetch URLs using ThreadPoolExecutor with concurrent.futures.ThreadPoolExecutor(max_workers=io_max_workers) as thread_executor: future_to_url = {thread_executor.submit(fetch_url, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: results[url] = future.result() except Exception as exc: results[url] = f\\"Generated an exception: {exc}\\" # Calculate primes using ProcessPoolExecutor with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_max_workers) as process_executor: future_to_number = {process_executor.submit(is_prime, num): num for num in numbers} for future in concurrent.futures.as_completed(future_to_number): num = future_to_number[future] try: results[num] = future.result() except Exception as exc: results[num] = f\\"Generated an exception: {exc}\\" return results"},{"question":"**Objective:** Assess your ability to utilize seaborn for data visualization, particularly using the `seaborn.objects` module. This exercise will also test your skills in data manipulation and handling missing values. **Problem Statement:** You are provided with a dataset `seaice` that contains columns `Date` and `Extent`. The goal is to create a visualization that demonstrates the change in sea ice extent over time with some added complexities: 1. **Clean the Data:** The dataset may contain missing values in the `Extent` column. You need to handle them by interpolating the missing values. 2. **Plot Customization:** Create a plot that shows the sea ice extent over the year. Ensure to display data points with different colors corresponding to different decades. **Requirements:** 1. **Load Dataset:** Use `seaborn` to load the `seaice` dataset. 2. **Handle Missing Values:** Interpolate missing values in the `Extent` column. 3. **Generate Plot:** - X-axis should represent the day of the year obtained from the `Date` column. - Y-axis should represent the `Extent`. - Differentiate data points by color based on the decade they belong to. 4. **Visual Customization:** - Use a suitable color palette that makes distinctions clear. - Facet the plot by decade for better visualization and presentation. - Customize the plot\'s appearance to make it visually appealing. **Input:** - None, directly load the dataset using seaborn\'s `load_dataset`. **Output:** - A seaborn plot with the described specifications. **Example:** ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Step 1: Load the dataset seaice = load_dataset(\\"seaice\\") # Step 2: Handle missing values seaice[\'Extent\'] = seaice[\'Extent\'].interpolate() # Step 3: Create the plot ( so.Plot( x=seaice[\\"Date\\"].dt.day_of_year, y=seaice[\\"Extent\\"], color=seaice[\\"Date\\"].dt.year // 10 * 10 # Group by decade ) .facet(seaice[\\"Date\\"].dt.year // 10 * 10) .add(so.Lines(linewidth=.5, color=\\"#bbca\\"), col=None) .add(so.Lines(linewidth=1)) .scale(color=\\"ch:rot=-.2,light=.7\\") .layout(size=(8, 4)) .label(title=\\"Sea Ice Extent over Decades\\") .show() ) ``` **Constraints:** - Ensure to work with missing data appropriately. - Use appropriate formatting and visualization techniques for clear, insightful representation of the data. **Performance Requirements:** - The code should efficiently handle the dataset and produce the plot without significant delays.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def create_seaice_plot(): # Step 1: Load the dataset seaice = load_dataset(\\"seaice\\") # Step 2: Handle missing values seaice[\'Extent\'] = seaice[\'Extent\'].interpolate() # Step 3: Add Day of Year and Decade columns seaice[\'DayOfYear\'] = seaice[\'Date\'].dt.day_of_year seaice[\'Decade\'] = (seaice[\'Date\'].dt.year // 10) * 10 # Step 4: Create the plot plot = ( so.Plot( data=seaice, x=\\"DayOfYear\\", y=\\"Extent\\", color=\\"Decade\\" ) .facet(\\"Decade\\") .add(so.Line(linewidth=.5, color=\\"#bbca\\"), col=None) .add(so.Line(linewidth=1)) .scale(color=\\"ch:rot=-.2,light=.7\\") .layout(size=(8, 4)) .label(title=\\"Sea Ice Extent over Decades\\") ) return plot.show()"},{"question":"# Custom Dataset and DataLoader Implementation in PyTorch **Objective:** Create a custom dataset using `torch.utils.data.Dataset` and utilize `torch.utils.data.DataLoader` to iterate through the data. **Problem Statement:** You are given a dataset containing images and their corresponding labels. Your task is to create a custom dataset class `CustomImageDataset` that inherits from `torch.utils.data.Dataset` and implement the necessary methods to load the images and labels. Furthermore, you need to use `torch.utils.data.DataLoader` to create an iterable data loader for batch processing. **Requirements:** 1. **Custom Dataset Class:** - Implement the `__init__`, `__len__`, and `__getitem__` methods for the `CustomImageDataset` class. - The `__init__` method should take two parameters: a list of image file paths and a list of labels. - The `__getitem__` method should return a tuple `(image, label)` where `image` is a loaded image tensor, and `label` is the corresponding label. - Use `PIL` (Python Imaging Library) for loading images and `torchvision.transforms` to convert images to tensors. 2. **DataLoader:** - Create a DataLoader instance using your custom dataset. - The DataLoader should shuffle the data and support batching of size 32. **Input:** - A Python list of image file paths: `image_paths` - A Python list of integer labels: `labels` **Output:** - A DataLoader instance that can be used to iterate through the dataset in batches. **Constraints:** - Assume the images are in a format supported by PIL (such as PNG or JPEG). - Ensure that the DataLoader returns batches of size 32. - Ensure the DataLoader shuffles the dataset at the end of each epoch. **Example:** ```python from typing import List from torch.utils.data import Dataset, DataLoader from PIL import Image import torchvision.transforms as transforms import torch class CustomImageDataset(Dataset): def __init__(self, image_paths: List[str], labels: List[int]): self.image_paths = image_paths self.labels = labels self.transform = transforms.ToTensor() def __len__(self): return len(self.image_paths) def __getitem__(self, idx): image_path = self.image_paths[idx] label = self.labels[idx] image = Image.open(image_path) image = self.transform(image) return image, label # Example usage image_paths = [\'./image1.png\', \'./image2.png\', ...] labels = [0, 1, ...] dataset = CustomImageDataset(image_paths, labels) dataloader = DataLoader(dataset, batch_size=32, shuffle=True) # Iterate through the dataloader for images, labels in dataloader: print(images.size(), labels.size()) ``` **Note:** You do not need to implement the example usage part, but make sure your custom dataset and DataLoader meet the specified criteria.","solution":"from typing import List from torch.utils.data import Dataset, DataLoader from PIL import Image import torchvision.transforms as transforms import torch class CustomImageDataset(Dataset): def __init__(self, image_paths: List[str], labels: List[int]): self.image_paths = image_paths self.labels = labels self.transform = transforms.ToTensor() def __len__(self): return len(self.image_paths) def __getitem__(self, idx): image_path = self.image_paths[idx] label = self.labels[idx] image = Image.open(image_path) image = self.transform(image) return image, label def create_dataloader(image_paths: List[str], labels: List[int], batch_size=32): dataset = CustomImageDataset(image_paths, labels) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) return dataloader"},{"question":"Coding Assessment Question # Objective Design a program using the PyTorch `torch.distributions` module that demonstrates your understanding of probability distributions. The program should create a few distributions, sample from them, and perform some transformations or calculations on the samples. # Task 1. **Create Distributions**: - Define a normal distribution with a mean of `0` and a standard deviation of `1`. - Define a Bernoulli distribution with a probability of success `0.3`. - Define a Beta distribution with shape parameters `2.0` and `5.0`. 2. **Sample and Manipulate**: - Draw `1000` samples from each of the above-defined distributions. - Calculate the mean and variance of the samples from each distribution. - Calculate the log probability of the first 5 samples from each distribution. 3. **Conditional Transformation**: - Assume the Beta-distributed samples represent probabilities of success. Use these probabilities to sample from a Bernoulli distribution for each of the 1000 samples. - Calculate the mean of the new Bernoulli samples obtained this way. # Input and Output Format - **Input**: None - **Output**: - Print the means and variances of the samples from each distribution. - Print the log probabilities of the first 5 samples from each distribution. - Print the mean of the newly generated Bernoulli samples obtained using the Beta-distributed probabilities. # Performance Requirements - The entire program should run efficiently without significant delay. - Your solution should demonstrate familiarity with the PyTorch `torch.distributions` package and effective usage of its methods. # Constraints - You must use the `torch.distributions` module for defining distributions, sampling, and calculating log probabilities. - You are not allowed to use other libraries (like NumPy or SciPy) to perform statistical calculations directly related to the task. # Example The following is a brief example illustrating part of the task: ```python import torch from torch.distributions.normal import Normal from torch.distributions.bernoulli import Bernoulli from torch.distributions.beta import Beta # Define a normal distribution normal_dist = Normal(0, 1) samples = normal_dist.sample((1000,)) mean = samples.mean() variance = samples.var() log_probs = normal_dist.log_prob(samples[:5]) print(f\\"Normal Distribution Mean: {mean}\\") print(f\\"Normal Distribution Variance: {variance}\\") print(f\\"Log Probabilities of first 5 samples: {log_probs}\\") # Define other distributions and follow similar steps as above ... ``` # Evaluation Criteria - Correctness: The program should output correct means, variances, and log probabilities. - Code Quality: Code should be well-organized, properly commented, and follow good coding practices. - Efficiency: The program should handle the sample size efficiently without unnecessary computations.","solution":"import torch from torch.distributions.normal import Normal from torch.distributions.bernoulli import Bernoulli from torch.distributions.beta import Beta def distribution_analysis(): # Create distributions normal_dist = Normal(0, 1) bernoulli_dist = Bernoulli(0.3) beta_dist = Beta(2.0, 5.0) # Sampling normal_samples = normal_dist.sample((1000,)) bernoulli_samples = bernoulli_dist.sample((1000,)) beta_samples = beta_dist.sample((1000,)) # Calculate mean and variance normal_mean = normal_samples.mean().item() normal_variance = normal_samples.var().item() bernoulli_mean = bernoulli_samples.mean().item() bernoulli_variance = bernoulli_samples.var().item() beta_mean = beta_samples.mean().item() beta_variance = beta_samples.var().item() # Calculate log probabilities of the first 5 samples normal_log_probs = normal_dist.log_prob(normal_samples[:5]) bernoulli_log_probs = bernoulli_dist.log_prob(bernoulli_samples[:5]) beta_log_probs = beta_dist.log_prob(beta_samples[:5]) # Use Beta-distributed samples as probabilities in a Bernoulli distribution bernoulli_from_beta = Bernoulli(beta_samples) new_bernoulli_samples = bernoulli_from_beta.sample() new_bernoulli_mean = new_bernoulli_samples.mean().item() # Print results print(f\\"Normal Distribution Mean: {normal_mean}\\") print(f\\"Normal Distribution Variance: {normal_variance}\\") print(f\\"Normal Distribution Log Probabilities: {normal_log_probs}\\") print(f\\"Bernoulli Distribution Mean: {bernoulli_mean}\\") print(f\\"Bernoulli Distribution Variance: {bernoulli_variance}\\") print(f\\"Bernoulli Distribution Log Probabilities: {bernoulli_log_probs}\\") print(f\\"Beta Distribution Mean: {beta_mean}\\") print(f\\"Beta Distribution Variance: {beta_variance}\\") print(f\\"Beta Distribution Log Probabilities: {beta_log_probs}\\") print(f\\"New Bernoulli from Beta Mean: {new_bernoulli_mean}\\")"},{"question":"Advanced GroupBy Operations with Pandas Objective: Demonstrate your understanding of the pandas `GroupBy` functionality by performing advanced data operations and extracting key metrics from a dataset. Problem Statement: You are given a dataset containing sales information for various products across different regions and time periods. Your task is to write a function that performs the following operations: 1. Group the data by product and region. 2. Compute the total sales and the average sales per month for each product in each region. 3. Determine the month with the highest sales for each product in each region. 4. For each product in each region, identify the first month that registered sales above a given threshold. Your function should return a DataFrame with the following structure: - `Product`: The name of the product. - `Region`: The region where the sales were recorded. - `TotalSales`: The total sales for the product in the given region. - `AverageMonthlySales`: The average sales per month for the product in the given region. - `MonthMaxSales`: The month with the highest sales for the product in the given region. - `FirstMonthAboveThreshold`: The first month where sales exceeded the given threshold. Input Format: Your function should accept the following parameters: 1. `df` (pd.DataFrame): A DataFrame with the following columns: - `Month`: The month when the sales were recorded (string in the format \'YYYY-MM\'). - `Product`: The name of the product (string). - `Region`: The sales region (string). - `Sales`: The sales amount (float). 2. `threshold` (float): The sales threshold for identifying the first month with sales above this value. Output Format: - A pandas DataFrame with the specified structure containing the computed metrics. Constraints: - The input DataFrame will contain at least one row per product and region. - The \'Month\' column will be in the format \'YYYY-MM\' and must be converted to datetime. Performance Requirements: - Your function should make efficient use of the pandas `groupby` operations. - Aim for clear and concise code that leverages pandas built-in methods effectively. Example: ```python import pandas as pd data = { \'Month\': [\'2021-01\', \'2021-02\', \'2021-03\', \'2021-01\', \'2021-02\', \'2021-03\'], \'Product\': [\'A\', \'A\', \'A\', \'B\', \'B\', \'B\'], \'Region\': [\'North\', \'North\', \'North\', \'South\', \'South\', \'South\'], \'Sales\': [100, 150, 100, 200, 250, 100] } df = pd.DataFrame(data) threshold = 120 # Define the function according to the problem statement result = advanced_groupby_operations(df, threshold) print(result) ``` Expected Output: ``` Product Region TotalSales AverageMonthlySales MonthMaxSales FirstMonthAboveThreshold 0 A North 350 116.67 2021-02 2021-02 1 B South 550 183.33 2021-02 2021-01 ``` Your Task: Implement the function `advanced_groupby_operations` that meets the specified requirements. ```python def advanced_groupby_operations(df, threshold): # Your implementation here pass ```","solution":"import pandas as pd def advanced_groupby_operations(df, threshold): # Convert the \'Month\' column to datetime df[\'Month\'] = pd.to_datetime(df[\'Month\'], format=\'%Y-%m\') # Group by Product and Region grouped = df.groupby([\'Product\', \'Region\']) # Calculate Total Sales and Average Monthly Sales metrics = grouped[\'Sales\'].agg(TotalSales=\'sum\', AverageMonthlySales=\'mean\').reset_index() # Find the Month with Maximum Sales max_sales = grouped.apply(lambda x: x.loc[x[\'Sales\'].idxmax()][\'Month\']).reset_index() max_sales.columns = [\'Product\', \'Region\', \'MonthMaxSales\'] # Find the first month where sales exceeded the threshold above_threshold = grouped.apply(lambda x: x[x[\'Sales\'] > threshold].iloc[0][\'Month\'] if any(x[\'Sales\'] > threshold) else pd.NaT).reset_index() above_threshold.columns = [\'Product\', \'Region\', \'FirstMonthAboveThreshold\'] # Merge all metrics into one DataFrame results = pd.merge(metrics, max_sales, on=[\'Product\', \'Region\']) results = pd.merge(results, above_threshold, on=[\'Product\', \'Region\']) # Format columns results[\'AverageMonthlySales\'] = results[\'AverageMonthlySales\'].round(2) results[\'MonthMaxSales\'] = results[\'MonthMaxSales\'].dt.strftime(\'%Y-%m\') results[\'FirstMonthAboveThreshold\'] = results[\'FirstMonthAboveThreshold\'].dt.strftime(\'%Y-%m\') return results"},{"question":"**Question: Configuration File Management with Python** You are tasked with writing a Python utility to manage the `setup.cfg` file used in Python package distribution. This utility should allow users to: 1. Parse the existing `setup.cfg` file. 2. Add or update options for specified commands. 3. Save the modified configuration back to the `setup.cfg` file. **Requirements:** 1. **Parsing the File:** - Create a function `parse_setup_cfg(filepath: str) -> dict` that reads a `setup.cfg` file and returns a dictionary representation of its contents. The dictionary keys should be the command names and the values should be dictionaries of option-value pairs. 2. **Updating the File:** - Create a function `update_setup_cfg(config: dict, command: str, option: str, value: str) -> None` that takes a configuration dictionary (as output by the previous function), a command name, option name, and value, and updates the configuration dictionary appropriately. 3. **Saving the File:** - Create a function `save_setup_cfg(filepath: str, config: dict) -> None` that writes the modified configuration dictionary back to the `setup.cfg` file in the correct format. **Constraints:** - The configuration dictionary should maintain the structure: `{command: {option: value, ...}, ...}`. - Comments and blank lines in the original `setup.cfg` file do not need to be preserved. - Assume that the configuration file is small enough to be read and written in one go. - Raise appropriate exceptions if the provided `setup.cfg` file path does not exist or is unreadable. **Example:** Given the following `setup.cfg` file: ```ini [build_ext] inplace=1 [bdist_rpm] release=1 packager=Greg Ward <gward@python.net> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Your program should be able to parse this file into: ```python { \'build_ext\': { \'inplace\': \'1\' }, \'bdist_rpm\': { \'release\': \'1\', \'packager\': \'Greg Ward <gward@python.net>\', \'doc_files\': \'CHANGES.txt README.txt USAGE.txt doc/ examples/\' } } ``` After executing `update_setup_cfg(config, \'build_ext\', \'inplace\', \'0\')` the dictionary should be: ```python { \'build_ext\': { \'inplace\': \'0\' }, \'bdist_rpm\': { \'release\': \'1\', \'packager\': \'Greg Ward <gward@python.net>\', \'doc_files\': \'CHANGES.txt README.txt USAGE.txt doc/ examples/\' } } ``` The `save_setup_cfg` function should then write the updated configuration back to the file: ```ini [build_ext] inplace=0 [bdist_rpm] release=1 packager=Greg Ward <gward@python.net> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` **Note:** Do not use any third-party packages for parsing or writing configuration files.","solution":"def parse_setup_cfg(filepath: str) -> dict: from configparser import ConfigParser import os if not os.path.exists(filepath): raise FileNotFoundError(f\\"The file at path {filepath} does not exist.\\") config = ConfigParser() config.read(filepath) config_dict = {} for section in config.sections(): config_dict[section] = dict(config.items(section)) return config_dict def update_setup_cfg(config: dict, command: str, option: str, value: str) -> None: if command not in config: config[command] = {} config[command][option] = value def save_setup_cfg(filepath: str, config: dict) -> None: from configparser import ConfigParser config_parser = ConfigParser() for command, options in config.items(): config_parser[command] = options with open(filepath, \'w\') as configfile: config_parser.write(configfile)"},{"question":"Objective To assess your understanding of pandas\' `Resampling` functionality, you need to write code that resamples a time series dataset and computes various statistics. Question Given a DataFrame containing time series data, implement a function `analyze_time_series` that takes in the following parameters: - `df` (pd.DataFrame): A DataFrame with a DateTimeIndex and a column `value` representing measurements. - `rule` (str): A string representing the offset alias for resampling (e.g., \'D\' for daily, \'M\' for monthly). Your task is to: 1. Downsample the DataFrame to the specified frequency using the given rule. 2. Using the resampled data, compute the following statistics for each period: - Mean (`mean`) - Standard Deviation (`std`) - Maximum (`max`) - Minimum (`min`) 3. Upsample the DataFrame back to the original frequency, using forward fill (`ffill`) for missing values. The function should return a DataFrame with the upsampled data. Input - `df`: pd.DataFrame with DateTimeIndex and one column `value`. Example: ``` value 2023-01-01 00:00:00 1 2023-01-01 01:00:00 2 2023-01-01 02:00:00 3 ... ``` - `rule`: str, e.g., \'D\', \'M\', \'H\', etc. Output - A DataFrame with the original DateTimeIndex, resampled to the frequency specified by `rule`, and includes the columns `mean`, `std`, `max`, and `min` upsampled to the original frequency. Constraints - The DateTimeIndex of `df` is guaranteed to be continuous and properly sorted. - Ensure that your solution efficiently handles large datasets. Performance Requirements - The solution should be optimized for performance, as time series data can be quite large. Example ```python import pandas as pd def analyze_time_series(df, rule): # Downsample resampled = df.resample(rule).agg({ \'value\': [\'mean\', \'std\', \'max\', \'min\'] }) # Flatten the column index resampled.columns = [\'mean\', \'std\', \'max\', \'min\'] # Upsample back to the original frequency upsampled = resampled.resample(df.index.freq).ffill() return upsampled # Sample usage date_range = pd.date_range(start=\'2023-01-01\', end=\'2023-01-10\', freq=\'H\') data = pd.Series(range(len(date_range)), index=date_range) df = pd.DataFrame({\'value\': data}) result = analyze_time_series(df, \'D\') print(result) ``` Note: This is an example structure; the actual values may vary depending on the input dataset. Testing Create a few test cases with different frequencies (\'D\', \'M\', \'H\', etc.) and verify the correctness of your solution.","solution":"import pandas as pd def analyze_time_series(df, rule): Downsample a DataFrame to the specified frequency, compute statistics, and upsample back. Parameters ---------- df : pd.DataFrame A DataFrame with a DateTimeIndex and a column \'value\'. rule : str A string representing the offset alias for resampling (e.g., \'D\' for daily, \'M\' for monthly). Returns ------- pd.DataFrame A DataFrame with columns \'mean\', \'std\', \'max\', \'min\' upsampled to the original frequency. # Downsample resampled = df.resample(rule).agg({ \'value\': [\'mean\', \'std\', \'max\', \'min\'] }) # Flatten the column multi-index resampled.columns = [\'mean\', \'std\', \'max\', \'min\'] # Upsample back to the original frequency, forward filling missing values upsampled = resampled.resample(df.index.freq).ffill() return upsampled"},{"question":"# Question: Composite Data Structure Serialization Using `xdrlib` **Problem Statement:** You are required to implement two functions, `serialize_data(data: dict) -> bytes` and `deserialize_data(xdr_data: bytes) -> dict`, that make use of the `xdrlib` module to serialize and deserialize a complex composite data structure. The composite data structure to be serialized is a dictionary with the following structure: ```python { \\"name\\": str, \\"age\\": int, \\"height\\": float, \\"scores\\": List[int], \\"active\\": bool } ``` **Function 1: `serialize_data(data: dict) -> bytes`** 1. **Input:** - `data` (dict): A dictionary containing the specified structure. 2. **Output:** - Returns a bytes object that represents the XDR encoded data. 3. **Constraints:** - `name` is a non-empty string. - `age` is a non-negative integer. - `height` is a non-negative float. - `scores` is a list containing at least one integer. - `active` is a boolean. **Function 2: `deserialize_data(xdr_data: bytes) -> dict`** 1. **Input:** - `xdr_data` (bytes): A bytes object containing the XDR encoded data. 2. **Output:** - Returns a dictionary conforming to the specified structure. 3. **Constraints:** - The input `xdr_data` must be valid XDR encoded bytes as produced by the `serialize_data` function. **Requirements:** - You must use the `xdrlib` module\'s `Packer` class to pack the data and the `Unpacker` class to unpack the data. - Ensure alignment and correctness in the encoded and decoded forms. **Example:** ```python input_data = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"height\\": 5.5, \\"scores\\": [85, 90, 95], \\"active\\": True } xdr_encoded = serialize_data(input_data) print(xdr_encoded) # The serialized XDR bytes output decoded_data = deserialize_data(xdr_encoded) print(decoded_data) # {\'name\': \'Alice\', \'age\': 30, \'height\': 5.5, \'scores\': [85, 90, 95], \'active\': True} ``` **Guidelines:** - You can assume that the input data will always be correctly formatted according to the specified structure. - Raise appropriate exceptions if any deviations are encountered during serialization or deserialization. - Be mindful of XDR data alignment and padding requirements. Good luck and happy coding!","solution":"import xdrlib def serialize_data(data: dict) -> bytes: packer = xdrlib.Packer() # Pack \\"name\\" (string) name = data[\'name\'] packer.pack_string(name.encode(\'utf-8\')) # Pack \\"age\\" (int) age = data[\'age\'] packer.pack_int(age) # Pack \\"height\\" (float) height = data[\'height\'] packer.pack_double(height) # Pack \\"scores\\" (List of int) scores = data[\'scores\'] packer.pack_int(len(scores)) # First pack the length of the list for score in scores: packer.pack_int(score) # Pack \\"active\\" (bool) active = data[\'active\'] packer.pack_bool(active) return packer.get_buffer() def deserialize_data(xdr_data: bytes) -> dict: unpacker = xdrlib.Unpacker(xdr_data) # Unpack \\"name\\" (string) name = unpacker.unpack_string().decode(\'utf-8\') # Unpack \\"age\\" (int) age = unpacker.unpack_int() # Unpack \\"height\\" (float) height = unpacker.unpack_double() # Unpack \\"scores\\" (List of int) length_of_scores = unpacker.unpack_int() scores = [] for _ in range(length_of_scores): scores.append(unpacker.unpack_int()) # Unpack \\"active\\" (bool) active = unpacker.unpack_bool() return { \\"name\\": name, \\"age\\": age, \\"height\\": height, \\"scores\\": scores, \\"active\\": active }"},{"question":"# PyTorch Coding Assessment Question Background You are provided with a neural network model that uses Batch Normalization. Due to the constraints of using `functorch` with vectorized maps (vmap), in-place updates to running statistics of Batch Normalization cannot be performed. To solve this, you need to modify the neural network model to replace Batch Norm layers with Group Norm layers, as discussed in the provided documentation. Objective 1. Implement a function `convert_batchnorm_to_groupnorm` that takes a neural network model and replaces all Batch Norm layers with Group Norm layers. The number of groups should be set to the number of channels divided by 4 (i.e., `G = C // 4`). 2. Implement a simple neural network model that includes Batch Norm layers. 3. Convert the Batch Norm layers in the model to Group Norm layers using the function you implemented. 4. Verify the conversion by printing the modified model and ensuring Group Norm layers are in place of Batch Norm layers. Function Signature ```python import torch import torch.nn as nn def convert_batchnorm_to_groupnorm(model: nn.Module) -> nn.Module: Convert all BatchNorm layers in the given model to GroupNorm layers. Args: - model (nn.Module): The neural network model with BatchNorm layers. Returns: - nn.Module: The model with BatchNorm layers replaced by GroupNorm layers. pass class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(64) self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(128) self.fc1 = nn.Linear(128 * 32 * 32, 256) self.bn3 = nn.BatchNorm1d(256) self.fc2 = nn.Linear(256, 10) def forward(self, x): x = self.bn1(self.conv1(x)) x = self.bn2(self.conv2(x)) x = x.view(x.size(0), -1) # flatten the tensor x = self.bn3(self.fc1(x)) x = self.fc2(x) return x # Test the function model = SimpleNet() print(\\"Original Model:\\") print(model) modified_model = convert_batchnorm_to_groupnorm(model) print(\\"Modified Model:\\") print(modified_model) ``` Constraints - The number of channels `C` must be divisible by 4 for the Group Norm conversion. - Ensure that the replacement maintains the same behavior where possible (i.e., the number of groups should be such that there are `C // G` channels per group). Example When applied to the `SimpleNet` model: - `BatchNorm2d(64)` should be replaced with `GroupNorm(16, 64)`. - `BatchNorm2d(128)` should be replaced with `GroupNorm(32, 128)`. - `BatchNorm1d(256)` should be replaced with `GroupNorm(64, 256)`. Note that you may need to handle BatchNorm1d and BatchNorm2d separately or with conditional logic in your function. **Hint:** You might find it useful to use the `torch.nn.Module.apply` method, which applies a function to every submodule of the model.","solution":"import torch import torch.nn as nn def convert_batchnorm_to_groupnorm(model: nn.Module) -> nn.Module: Convert all BatchNorm layers in the given model to GroupNorm layers. Args: - model (nn.Module): The neural network model with BatchNorm layers. Returns: - nn.Module: The model with BatchNorm layers replaced by GroupNorm layers. for name, module in model.named_modules(): if isinstance(module, nn.BatchNorm2d): num_features = module.num_features model._modules[name] = nn.GroupNorm(num_features // 4, num_features) elif isinstance(module, nn.BatchNorm1d): num_features = module.num_features model._modules[name] = nn.GroupNorm(num_features // 4, num_features) return model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(64) self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(128) self.fc1 = nn.Linear(128 * 32 * 32, 256) self.bn3 = nn.BatchNorm1d(256) self.fc2 = nn.Linear(256, 10) def forward(self, x): x = self.bn1(self.conv1(x)) x = self.bn2(self.conv2(x)) x = x.view(x.size(0), -1) # flatten the tensor x = self.bn3(self.fc1(x)) x = self.fc2(x) return x # Example usage of the function model = SimpleNet() print(\\"Original Model:\\") print(model) modified_model = convert_batchnorm_to_groupnorm(model) print(\\"Modified Model:\\") print(modified_model)"},{"question":"You are tasked with implementing a demonstration comparing Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) using scikit-learn, including both classification and dimensionality reduction aspects. Objectives: 1. Train LDA and QDA classifiers on a synthetic dataset. 2. Perform dimensionality reduction using LDA and visualize the transformed data. 3. Evaluate the classification performance of both LDA and QDA. Requirements: - Input: A synthetic dataset `make_classification(n_samples=300, n_features=10, n_classes=3, n_informative=8, random_state=42)`. - Implement: - Data generation. - Training LDA and QDA classifiers. - Dimensionality reduction using LDA. - Visualization of decision boundaries for both LDA and QDA. - Performance evaluation using accuracy score. - Visualize the transformed data using LDA (2D plot if `n_components=2`). Steps: 1. Generate the synthetic dataset. 2. Split the dataset into training and testing sets (70-30 split). 3. Train LDA and QDA classifiers on the training set. 4. Use the trained LDA classifier to transform the test data (reduce to 2 components). 5. Plot the decision boundaries for both LDA and QDA. 6. Calculate and print the accuracy scores for both classifiers on the test set. 7. Plot the LDA-transformed data for visual inspection. Implementation: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from matplotlib.colors import ListedColormap # Generate synthetic dataset X, y = make_classification(n_samples=300, n_features=10, n_classes=3, n_informative=8, random_state=42) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the classifiers lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() # Train the classifiers lda.fit(X_train, y_train) qda.fit(X_train, y_train) # Predict and evaluate y_pred_lda = lda.predict(X_test) y_pred_qda = qda.predict(X_test) accuracy_lda = accuracy_score(y_test, y_pred_lda) accuracy_qda = accuracy_score(y_test, y_pred_qda) print(f\\"LDA Accuracy: {accuracy_lda:.2f}\\") print(f\\"QDA Accuracy: {accuracy_qda:.2f}\\") # Dimensionality reduction using LDA (n_components=2) X_lda = lda.transform(X_test) # Visualization of LDA-transformed data plt.figure(figsize=(8, 6)) colors = [\'r\', \'g\', \'b\'] for color, i in zip(colors, [0, 1, 2]): plt.scatter(X_lda[y_test == i, 0], X_lda[y_test == i, 1], alpha=0.8, color=color, label=f\\"Class {i}\\") plt.legend(loc=\'best\') plt.title(\'LDA-transformed data (2 components)\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.grid() plt.show() # Function to plot decision boundaries def plot_decision_boundary(model, X, y, title): cmap = ListedColormap([\'#FF0000\', \'#00FF00\', \'#0000FF\']) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8, cmap=cmap) plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\'k\') plt.title(title) plt.show() # Visualize decision boundaries (using first 2 features for 2D plot) plot_decision_boundary(lda, X_test[:, :2], y_test, \\"LDA Decision Boundary\\") plot_decision_boundary(qda, X_test[:, :2], y_test, \\"QDA Decision Boundary\\") ``` # Notes: - Use `matplotlib` for plotting. - Ensure that the dimensionality reduction step using `LDA.transform` results in a 2D plot if `n_components=2`. Constraints: - You must use scikit-learn and matplotlib libraries. - The dataset should be generated as specified and only the provided synthetic dataset should be used. Submission: - Submit your Python code in `.py` file format. - Ensure your code is well-commented and follows best practices for code readability and organization.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from matplotlib.colors import ListedColormap def train_and_evaluate_lda_qda(): # Generate synthetic dataset X, y = make_classification(n_samples=300, n_features=10, n_classes=3, n_informative=8, random_state=42) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the classifiers lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() # Train the classifiers lda.fit(X_train, y_train) qda.fit(X_train, y_train) # Predict and evaluate y_pred_lda = lda.predict(X_test) y_pred_qda = qda.predict(X_test) accuracy_lda = accuracy_score(y_test, y_pred_lda) accuracy_qda = accuracy_score(y_test, y_pred_qda) print(f\\"LDA Accuracy: {accuracy_lda:.2f}\\") print(f\\"QDA Accuracy: {accuracy_qda:.2f}\\") return lda, qda, X_test, y_test, accuracy_lda, accuracy_qda def plot_lda_transformed_data(lda, X_test, y_test): # Dimensionality reduction using LDA (n_components=2) X_lda = lda.transform(X_test) # Visualization of LDA-transformed data plt.figure(figsize=(8, 6)) colors = [\'r\', \'g\', \'b\'] for color, i in zip(colors, [0, 1, 2]): plt.scatter(X_lda[y_test == i, 0], X_lda[y_test == i, 1], alpha=0.8, color=color, label=f\\"Class {i}\\") plt.legend(loc=\'best\') plt.title(\'LDA-transformed data (2 components)\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.grid() plt.show() def plot_decision_boundary(model, X, y, title): cmap = ListedColormap([\'#FF0000\', \'#00FF00\', \'#0000FF\']) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, y[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8, cmap=cmap) plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\'k\') plt.title(title) plt.show() # Example usage: if __name__ == \\"__main__\\": lda, qda, X_test, y_test, accuracy_lda, accuracy_qda = train_and_evaluate_lda_qda() plot_lda_transformed_data(lda, X_test, y_test) plot_decision_boundary(lda, X_test[:, :2], y_test, \\"LDA Decision Boundary\\") plot_decision_boundary(qda, X_test[:, :2], y_test, \\"QDA Decision Boundary\\")"},{"question":"You are provided with a dataset on global temperatures across different countries over several years. You are required to create a visual representation of this data using seaborn. Task 1. **Load the Dataset** - Load the dataset from a CSV file, preprocess it as necessary to make it suitable for plotting. - Assume the dataset file is named `global_temps.csv` and has the following columns: `Year`, `Country`, `Temperature_C`. 2. **Data Preprocessing** - Make sure there are no missing values in the dataset. If there are missing values, handle them appropriately by interpolation. - Convert the dataset into a format suitable for plotting with seaborn. 3. **Plotting** - Create a facet grid plot where each subplot represents a different country. - In each subplot, produce an area plot that shows the temperature trends over the years. - Color each country\'s plot area in the same hue. 4. **Customization** - Ensure that the x-axis represents the years, and the y-axis represents the temperature. - Add appropriate axis labels and a title for the overall plot. - Differentiate the plots using edge colors or fill colors as demonstrated in the documentation. 5. **Stack Plot (Optional)** - Create a separate figure that shows a stacked area plot indicating the part-whole relationship of temperatures across countries for each year. Expected Input and Output Formats - **Input:** A CSV file named `global_temps.csv` containing columns `Year`, `Country`, `Temperature_C`. - **Output:** A facet grid area plot for each country and optionally a stacked area plot. The plots should be plotted within a Python script or a Jupyter Notebook. Ensure your solution demonstrates good practice in data handling and visualization, as well as familiarity with seaborn tools. # Example Here is an example of how your code might begin: ```python import seaborn.objects as so import pandas as pd # Load the dataset global_temps = pd.read_csv(\'global_temps.csv\') # Data preprocessing # Handle missing values, pivoting, and other transformations here # Create the facet grid plot # Facet grid with an area plot for each country # Create the stacked area plot (optional) ``` Remember to provide sufficient comments and explanations within your code to clarify the steps and logic used.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_preprocess_data(file_path): Loads and preprocesses the dataset. # Load the dataset df = pd.read_csv(file_path) # Handle missing values via interpolation df.interpolate(method=\'linear\', limit_direction=\'forward\', axis=0, inplace=True) return df def plot_facet_grid(df): Plot facet grid with an area plot for each country. # Create the facet grid g = sns.FacetGrid(df, col=\\"Country\\", col_wrap=4, height=4, sharey=False) g.map(plt.fill_between, \\"Year\\", \\"Temperature_C\\", alpha=0.7) g.set_titles(\\"{col_name}\\") g.set_axis_labels(\\"Year\\", \\"Temperature (°C)\\") plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Temperature Trends Across Different Countries\') plt.show() # Uncomment the following lines to run the code with your csv file # file_path = \'global_temps.csv\' # df = load_and_preprocess_data(file_path) # plot_facet_grid(df) # Optional: Function to create a stacked area plot def plot_stacked_area(df): Create a stacked area plot indicating the part-whole relationship of temperatures across countries for each year. df_pivot = df.pivot(index=\'Year\', columns=\'Country\', values=\'Temperature_C\').fillna(0) colors = sns.color_palette(\\"hsv\\", len(df[\'Country\'].unique())) df_pivot.plot(kind=\'area\', stacked=True, figsize=(12, 8), color=colors) plt.title(\'Stacked Area Plot of Global Temperatures Across Countries Over Years\') plt.xlabel(\'Year\') plt.ylabel(\'Temperature (°C)\') plt.legend(loc=\'center left\', bbox_to_anchor=(1, 0.5), title=\'Country\') plt.show() # Uncomment the following lines to run the code with your csv file # plot_stacked_area(df)"},{"question":"**Python Coding Assessment Question** **Objective**: Demonstrate your understanding of the `ensurepip` package and programmatic API usage in Python by creating a set of utility functions. **Problem Statement**: You are required to implement a Python module that simulates the functionalities provided by the `ensurepip` package. Your module should provide two functions: 1. `get_pip_version()` - This function should simulate the `ensurepip.version()` function and return the string specifying the version of \\"pip\\" that will be installed. 2. `bootstrap_pip(root=None, upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0)` - This function should simulate the `ensurepip.bootstrap(...)` function. **Function Specifications**: 1. `get_pip_version` - **Input**: No input parameters. - **Output**: A string specifying the version of \\"pip\\" (e.g., \\"pip 21.2.4\\"). - **Constraints**: The function should return a hardcoded version string (you can assume the \\"pip\\" version is \\"21.2.4\\"). 2. `bootstrap_pip` - **Input**: - `root` (default: `None`): A string specifying an alternative root directory. - `upgrade` (default: `False`): A boolean indicating whether to upgrade an existing \\"pip\\" installation. - `user` (default: `False`): A boolean indicating whether to use the user scheme rather than installing globally. - `altinstall` (default: `False`): A boolean deciding if the \\"pipX\\" script should be installed. - `default_pip` (default: `False`): A boolean deciding if the \\"pip\\" script should be installed in addition to the regular scripts. - `verbosity` (default: `0`): An integer controlling the level of output from the bootstrapping operation. - **Output**: A dictionary summarizing the provided options and indicating the success of the operation. Example Output: ```python { \\"root\\": \\"/example/root\\", \\"upgrade\\": True, \\"user\\": False, \\"altinstall\\": False, \\"default_pip\\": True, \\"verbosity\\": 1, \\"status\\": \\"success\\" } ``` - **Constraints**: - The function should raise a `ValueError` if both `altinstall` and `default_pip` are `True`. **Examples**: - Example for `get_pip_version`: ```python print(get_pip_version()) # Output: \\"pip 21.2.4\\" ``` - Example for `bootstrap_pip`: ```python result = bootstrap_pip(root=\\"/example/root\\", upgrade=True, user=False, altinstall=False, default_pip=True, verbosity=1) print(result) # Output: # { # \\"root\\": \\"/example/root\\", # \\"upgrade\\": True, # \\"user\\": False, # \\"altinstall\\": False, # \\"default_pip\\": True, # \\"verbosity\\": 1, # \\"status\\": \\"success\\" # } ``` Your implementation should validate the inputs and raise appropriate exceptions if constraints are violated. **Note**: - This question will assess your ability to understand and simulate package functionalities using programmatic API and handle inputs and outputs precisely. - Ensure your function signatures match the specifications exactly to pass the assessments.","solution":"def get_pip_version(): Returns the version of pip that would be installed. return \\"pip 21.2.4\\" def bootstrap_pip(root=None, upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0): Simulates the ensurepip.bootstrap(...) function and returns a summary of the provided options. Raises ValueError if both altinstall and default_pip are True. if altinstall and default_pip: raise ValueError(\\"altinstall and default_pip cannot both be True at the same time.\\") result = { \\"root\\": root, \\"upgrade\\": upgrade, \\"user\\": user, \\"altinstall\\": altinstall, \\"default_pip\\": default_pip, \\"verbosity\\": verbosity, \\"status\\": \\"success\\" } return result"},{"question":"Signal Handling in Python You are tasked with implementing a signal handling mechanism in Python which performs specific actions when different signals are received. You are to create a class `SignalHandler` that manages the registration of signal handlers for `SIGINT`, `SIGTERM`, and custom user signals. Additionally, you must implement a timer using `SIGALRM` which will trigger after a specified timeout period. # Requirements 1. **Class `SignalHandler`:** - **Methods:** - `__init__(self)`: Initializes the signal handler. - `register_signal(self, signum, handler)`: Registers a custom handler for a given signal number. - `start_timer(self, seconds)`: Starts a timer that sends `SIGALRM` after the specified number of seconds. - `default_handler(self, signum, frame)`: The default handler that processes received signals and performs a simple print operation. - **Attributes:** - `handlers`: A dictionary to store custom handlers for different signals. 2. **Functionality:** - Register a custom handler for `SIGINT` and `SIGTERM` signals. - Set up a timer that sends `SIGALRM` signal after 10 seconds. 3. **Custom Signal Handlers:** - For `SIGINT` (KeyboardInterrupt), print \\"SIGINT received.\\" - For `SIGTERM` (Termination signal), print \\"SIGTERM received.\\" - For `SIGALRM` (Alarm signal), print \\"Timer expired.\\" # Constraints - Only the main thread should be allowed to register signal handlers. - Use the `signal` module functions like `signal.signal`, `signal.alarm`, and other appropriate functions. # Input and Output - **No direct input/output**: Instead, you should manually test the class and its methods by triggering signals from the terminal or within the code. # Implementation ```python import signal import time class SignalHandler: def __init__(self): self.handlers = {} signal.signal(signal.SIGINT, self.default_handler) signal.signal(signal.SIGTERM, self.default_handler) def register_signal(self, signum, handler): self.handlers[signum] = handler signal.signal(signum, handler) def start_timer(self, seconds): signal.signal(signal.SIGALRM, self.default_handler) signal.alarm(seconds) def default_handler(self, signum, frame): if signum in self.handlers: print(f\\"Custom handler for signal: {signum}\\") self.handlers[signum](signum, frame) else: if signum == signal.SIGINT: print(\\"SIGINT received.\\") elif signum == signal.SIGTERM: print(\\"SIGTERM received.\\") elif signum == signal.SIGALRM: print(\\"Timer expired.\\") # Create an instance of the SignalHandler class handler = SignalHandler() # Register signals handler.register_signal(signal.SIGUSR1, lambda s, f: print(\\"Received custom user signal 1 (SIGUSR1).\\")) handler.register_signal(signal.SIGUSR2, lambda s, f: print(\\"Received custom user signal 2 (SIGUSR2).\\")) # Start a timer for 10 seconds handler.start_timer(10) # Simulate waiting so that signals can be received print(\\"Waiting for signals... Press Ctrl+C to send SIGINT or kill the process to send SIGTERM.\\") try: while True: time.sleep(1) except KeyboardInterrupt: print(\\"Manual interrupt (SIGINT).\\") ``` # Explanation - The `SignalHandler` class initializes with default handlers for `SIGINT` and `SIGTERM`. - The `register_signal` method takes a signal number and handler function, registering the handler for the given signal. - The `start_timer` method sets a timer that will send a `SIGALRM` signal after a specified period. - The `default_handler` method handles the signals either by calling custom handlers if available or performing default operations like printing messages. Implement and test the `SignalHandler` class to ensure it performs as expected under different signal conditions.","solution":"import signal import time import os class SignalHandler: def __init__(self): self.handlers = {} signal.signal(signal.SIGINT, self.default_handler) signal.signal(signal.SIGTERM, self.default_handler) def register_signal(self, signum, handler): self.handlers[signum] = handler signal.signal(signum, handler) def start_timer(self, seconds): signal.signal(signal.SIGALRM, self.default_handler) signal.alarm(seconds) def default_handler(self, signum, frame): if signum in self.handlers: print(f\\"Custom handler for signal: {signum}\\") self.handlers[signum](signum, frame) else: if signum == signal.SIGINT: print(\\"SIGINT received.\\") elif signum == signal.SIGTERM: print(\\"SIGTERM received.\\") elif signum == signal.SIGALRM: print(\\"Timer expired.\\") def custom_sigusr1_handler(signum, frame): print(\\"Received custom user signal 1 (SIGUSR1).\\") def custom_sigusr2_handler(signum, frame): print(\\"Received custom user signal 2 (SIGUSR2).\\") # Demonstrational code (would need to be run in a main script to see the effects) # handler = SignalHandler() # handler.register_signal(signal.SIGUSR1, custom_sigusr1_handler) # handler.register_signal(signal.SIGUSR2, custom_sigusr2_handler) # handler.start_timer(10) # # print(\\"Waiting for signals... Press Ctrl+C to send SIGINT or kill the process to send SIGTERM.\\") # try: # while True: # time.sleep(1) # except KeyboardInterrupt: # print(\\"Manual interrupt (SIGINT).\\")"},{"question":"# Pandas Coding Assessment Question Objective: To assess your understanding of pandas, you are required to write a function that performs the following tasks on a dataset representing sales data. Problem Statement: You are provided with two CSV files: `products.csv` and `sales.csv`. Your task is to write a function `analyze_sales_data(products_csv, sales_csv)` that reads these CSV files into pandas DataFrames and performs the following operations: 1. **Data Loading:** - Load the CSV files into pandas DataFrames. 2. **Data Merging:** - Merge the two DataFrames on the `product_id` column. 3. **Data Cleaning:** - Handle missing data by filling any NA values in the `sale_amount` column with 0. - Drop any rows where the `product_name` is missing. 4. **Data Analysis:** - Calculate the total sales amount for each product and return a DataFrame with columns `product_id`, `product_name`, and `total_sales`. - Sort the resulting DataFrame by `total_sales` in descending order. Input: - `products_csv`: String representing the path to the CSV file `products.csv`. - `sales_csv`: String representing the path to the CSV file `sales.csv`. Output: - A pandas DataFrame with the columns `product_id`, `product_name`, and `total_sales` sorted by `total_sales` in descending order. Example: Suppose `products.csv` contains: ```csv product_id,product_name 1,Product A 2,Product B 3,Product C ``` and `sales.csv` contains: ```csv sale_id,product_id,sale_amount 101,1,200.5 102,2,100.0 103,1,150.0 104,3,300.0 105,2,50.0 106,3, ``` The output DataFrame should be: ``` product_id product_name total_sales 0 1 Product A 350.5 1 3 Product C 300.0 2 2 Product B 150.0 ``` Constraints: - Assume the CSV files are well-formed and contain only the columns mentioned above. - Your solution should handle any reasonable size of datasets within typical memory limits. ```python def analyze_sales_data(products_csv, sales_csv): # Write your code here pass ```","solution":"import pandas as pd def analyze_sales_data(products_csv, sales_csv): # Load the CSV files into pandas DataFrames products_df = pd.read_csv(products_csv) sales_df = pd.read_csv(sales_csv) # Merge the two DataFrames on the `product_id` column merged_df = pd.merge(sales_df, products_df, on=\'product_id\') # Handle missing data by filling any NA values in the `sale_amount` column with 0 merged_df[\'sale_amount\'].fillna(0, inplace=True) # Drop any rows where the `product_name` is missing merged_df.dropna(subset=[\'product_name\'], inplace=True) # Calculate the total sales amount for each product total_sales_df = merged_df.groupby([\'product_id\', \'product_name\'], as_index=False)[\'sale_amount\'].sum() total_sales_df.rename(columns={\'sale_amount\': \'total_sales\'}, inplace=True) # Sort the resulting DataFrame by `total_sales` in descending order sorted_sales_df = total_sales_df.sort_values(by=\'total_sales\', ascending=False) return sorted_sales_df"},{"question":"**Coding Assessment Question: Unicode String Normalization and Comparison** # Objective: Implement functions to compare Unicode strings by normalizing them, and to extract specific Unicode character properties. # Problem Statement: You are tasked with implementing two main functions that work with Unicode strings. 1. **Function `compare_unicode_strings(s1: str, s2: str) -> bool`:** - This function takes two Unicode strings `s1` and `s2` as input. - It should normalize both strings to Normalization Form D (NFD) and then compare them for equality. - Return `True` if the normalized strings are equal, otherwise `False`. 2. **Function `unicode_character_info(ch: str) -> Dict[str, Any]`:** - This function takes a single Unicode character `ch` as input. - It should return a dictionary with the following keys: - `\\"code_point\\"`: The Unicode code point of the character in the format `U+XXXX`. - `\\"name\\"`: The name of the character. - `\\"category\\"`: The general category to which the character belongs. - `\\"numeric_value\\"`: The numeric value of the character, if it exists; otherwise `None`. # Input and Output Formats: - `compare_unicode_strings(s1: str, s2: str) -> bool` - **Input**: Two strings, `s1` and `s2`. - **Output**: A boolean value, `True` or `False`. - `unicode_character_info(ch: str) -> Dict[str, Any]` - **Input**: A single character string `ch`. - **Output**: A dictionary with keys `\\"code_point\\"`, `\\"name\\"`, `\\"category\\"`, and `\\"numeric_value\\"`. # Example: ```python # Example usage of compare_unicode_strings assert compare_unicode_strings(\\"café\\", \\"café\\") == True # Notice that \\"é\\" are in different forms # Example usage of unicode_character_info info = unicode_character_info(\'Ⅷ\') assert info == { \\"code_point\\": \\"U+2167\\", \\"name\\": \\"ROMAN NUMERAL EIGHT\\", \\"category\\": \\"Nl\\", \\"numeric_value\\": 8 } ``` # Constraints: - The input strings `s1` and `s2` can contain any Unicode characters. - The input character `ch` will be a valid Unicode character. - Keep in mind the performance implications when handling very large strings. # Performance Requirements: - Ensure the functions are efficient, especially in terms of normalizing and comparing strings. # Implementation Notes: - You will need to import the `unicodedata` module for normalization and extracting character properties. - Use `unicodedata.normalize()` to normalize strings. - Use `unicodedata.name()`, `unicodedata.category()`, and `unicodedata.numeric()` to get the character properties. Write your implementation below: ```python import unicodedata def compare_unicode_strings(s1: str, s2: str) -> bool: # Normalize both strings to NFD normalized_s1 = unicodedata.normalize(\'NFD\', s1) normalized_s2 = unicodedata.normalize(\'NFD\', s2) # Compare and return result return normalized_s1 == normalized_s2 def unicode_character_info(ch: str) -> Dict[str, Any]: info = { \\"code_point\\": f\\"U+{ord(ch):04X}\\", \\"name\\": unicodedata.name(ch, \\"UNKNOWN\\"), \\"category\\": unicodedata.category(ch), \\"numeric_value\\": unicodedata.numeric(ch, None) } return info # Test the example usage assert compare_unicode_strings(\\"café\\", \\"café\\") == True info = unicode_character_info(\'Ⅷ\') assert info == { \\"code_point\\": \\"U+2167\\", \\"name\\": \\"ROMAN NUMERAL EIGHT\\", \\"category\\": \\"Nl\\", \\"numeric_value\\": 8 } print(\\"All tests passed.\\") ```","solution":"import unicodedata from typing import Dict, Any def compare_unicode_strings(s1: str, s2: str) -> bool: # Normalize both strings to NFD normalized_s1 = unicodedata.normalize(\'NFD\', s1) normalized_s2 = unicodedata.normalize(\'NFD\', s2) # Compare and return result return normalized_s1 == normalized_s2 def unicode_character_info(ch: str) -> Dict[str, Any]: info = { \\"code_point\\": f\\"U+{ord(ch):04X}\\", \\"name\\": unicodedata.name(ch, \\"UNKNOWN\\"), \\"category\\": unicodedata.category(ch), \\"numeric_value\\": unicodedata.numeric(ch, None) } return info"},{"question":"# Bytearray Manipulation in Python C Extensions You have been provided with a Python C extension module documentation that explains functions to manipulate bytearray objects. For this exercise, you will implement a Python function that leverages these C functions by creating a Python C extension. Your task is to implement the following functionalities: 1. **Check if an object is a bytearray**: Write a function `is_bytearray(obj)` that returns `True` if the provided object is a bytearray or an instance of a subtype, and `False` otherwise. 2. **Create a bytearray from a string**: Write a function `bytearray_from_string(string)` that takes a Python string and converts it to a bytearray. 3. **Concatenate two bytearrays**: Write a function `concat_bytearrays(ba1, ba2)` that concatenates two bytearrays and returns the result. 4. **Resize a bytearray**: Write a function `resize_bytearray(ba, new_size)` that resizes a given bytearray to `new_size`. Expected Input and Output Formats - `is_bytearray(obj)` - Input: `obj` (any Python object) - Output: `True` or `False` (boolean) - `bytearray_from_string(string)` - Input: `string` (Python string) - Output: Bytearray object - `concat_bytearrays(ba1, ba2)` - Input: `ba1` and `ba2` (bytearray objects) - Output: Concatenated bytearray object - `resize_bytearray(ba, new_size)` - Input: `ba` (bytearray object), `new_size` (integer) - Output: Resized bytearray object Constraints - Ensure the inputs are valid as per their types. - Handle any necessary error checking and return appropriate Python exceptions. - Performance is crucial. Ensure that your implementation efficiently handles large bytearray operations. Code Implementation You need to write a Python C extension to achieve this. Provide a setup script to compile the extension and a main Python script to test your functions. Here is a brief template to get you started: ```python # bytearray_extension.c #include <Python.h> static PyObject* is_bytearray(PyObject* self, PyObject* args) { // Your implementation here using PyByteArray_Check } static PyObject* bytearray_from_string(PyObject* self, PyObject* args) { // Your implementation here using PyByteArray_FromStringAndSize } static PyObject* concat_bytearrays(PyObject* self, PyObject* args) { // Your implementation here using PyByteArray_Concat } static PyObject* resize_bytearray(PyObject* self, PyObject* args) { // Your implementation here using PyByteArray_Resize } static PyMethodDef BytearrayMethods[] = { {\\"is_bytearray\\", is_bytearray, METH_VARARGS, \\"Check if object is a bytearray.\\"}, {\\"bytearray_from_string\\", bytearray_from_string, METH_VARARGS, \\"Create bytearray from string.\\"}, {\\"concat_bytearrays\\", concat_bytearrays, METH_VARARGS, \\"Concatenate two bytearrays.\\"}, {\\"resize_bytearray\\", resize_bytearray, METH_VARARGS, \\"Resize a bytearray.\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef bytearraymodule = { PyModuleDef_HEAD_INIT, \\"bytearray_extension\\", NULL, -1, BytearrayMethods }; PyMODINIT_FUNC PyInit_bytearray_extension(void) { return PyModule_Create(&bytearraymodule); } ``` ```python # setup.py from distutils.core import setup, Extension module = Extension(\'bytearray_extension\', sources=[\'bytearray_extension.c\']) setup(name=\'BytearrayExtension\', version=\'1.0\', description=\'Python C extension for bytearray manipulation\', ext_modules=[module]) ``` ```python # test_bytearray_extension.py import bytearray_extension as be def test_functions(): print(be.is_bytearray(bytearray())) # Expected True print(be.is_bytearray(\\"not a bytearray\\")) # Expected False ba = be.bytearray_from_string(\\"Hello, World!\\") print(ba) # Expected bytearray(b\'Hello, World!\') ba1 = bytearray(b\'Hello, \') ba2 = bytearray(b\'World!\') ba_concat = be.concat_bytearrays(ba1, ba2) print(ba_concat) # Expected bytearray(b\'Hello, World!\') be.resize_bytearray(ba, 5) print(ba) # Expected bytearray(b\'Hello\') if __name__ == \\"__main__\\": test_functions() ``` Provide the implementation for the `is_bytearray`, `bytearray_from_string`, `concat_bytearrays`, and `resize_bytearray` functions in the C extension file to complete this assessment.","solution":"def is_bytearray(obj): Check if an object is a bytearray. Args: obj (any): The object to check. Returns: bool: True if the object is a bytearray or an instance of a subtype, False otherwise. return isinstance(obj, bytearray) def bytearray_from_string(string): Create a bytearray from a string. Args: string (str): The string to convert. Returns: bytearray: The converted bytearray. if not isinstance(string, str): raise TypeError(\\"Input must be a string\\") return bytearray(string, \'utf-8\') def concat_bytearrays(ba1, ba2): Concatenate two bytearrays. Args: ba1 (bytearray): The first bytearray. ba2 (bytearray): The second bytearray. Returns: bytearray: The concatenated bytearray. if not isinstance(ba1, bytearray) or not isinstance(ba2, bytearray): raise TypeError(\\"Both inputs must be bytearrays\\") return ba1 + ba2 def resize_bytearray(ba, new_size): Resize a bytearray to a new size. Args: ba (bytearray): The bytearray to resize. new_size (int): The new size. Returns: bytearray: The resized bytearray. if not isinstance(ba, bytearray): raise TypeError(\\"Input must be a bytearray\\") if not isinstance(new_size, int): raise TypeError(\\"New size must be an integer\\") if new_size < len(ba): return ba[:new_size] else: return ba + bytearray(new_size - len(ba))"},{"question":"- ExtensionArray Manipulation with Pandas Problem Statement You are required to create and manipulate a custom ExtensionArray in pandas. You will implement a custom array that extends `pandas.api.extensions.ExtensionArray`. You will then implement certain methods of this custom array and perform operations on it. Task 1. **CustomArray Class**: Implement a class `CustomArray` that extends `pandas.api.extensions.ExtensionArray`. Your class should: - Store data in a numpy array. - Implement the following methods: - `dtype` (attribute): Return the data type of the array. - `ndim` (attribute): Return the number of dimensions (always 1). - `shape` (attribute): Return the shape of the array. - `take(self, indices, allow_fill=False, fill_value=None)`: Take elements from the array. 2. **Operations on CustomArray**: - Create an instance of `CustomArray` with an example data array. - Perform the `take` method on the instance with a set of indices and return the result. Constraints - Depending on the method, ensure appropriate error handling, especially for index out of bounds. - Use numpy for array operations where applicable. - For `take`, if `allow_fill` is `True` and an index is beyond the bounds of the array, use `fill_value` for those positions. Expected Input and Output **Input:** - An integer list or numpy array to initialize `CustomArray`. - Indices (list of integers) to be used in the `take` method. - Boolean `allow_fill` to specify if out-of-bounds indices should be filled. - `fill_value` to use if `allow_fill` is True. **Output:** - A new array with elements taken from the original array using provided indices. # Example ```python import numpy as np import pandas as pd class CustomArray(pd.api.extensions.ExtensionArray): # Initialize with data def __init__(self, data): self._data = np.asarray(data) @property def dtype(self): return self._data.dtype @property def ndim(self): return 1 @property def shape(self): return self._data.shape def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: result = np.full(len(indices), fill_value, dtype=self.dtype) for i, idx in enumerate(indices): if 0 <= idx < len(self._data): result[i] = self._data[idx] else: result = self._data[indices] return result # Instantiate CustomArray with sample data array = CustomArray([1, 2, 3, 4, 5]) # Example operations output = array.take([1, 3, 5], allow_fill=True, fill_value=-1) print(output) # Output: array([ 2, 4, -1]) ``` Note Test your implementation thoroughly to handle edge cases.","solution":"import numpy as np import pandas as pd class CustomArray(pd.api.extensions.ExtensionArray): # Initialize with data def __init__(self, data): self._data = np.asarray(data) @property def dtype(self): return self._data.dtype @property def ndim(self): return 1 @property def shape(self): return self._data.shape def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: result = np.full(len(indices), fill_value, dtype=self.dtype) for i, idx in enumerate(indices): if 0 <= idx < len(self._data): result[i] = self._data[idx] else: result = self._data[indices] return result"},{"question":"# Asynchronous Non-Blocking Server You are tasked with creating a simple non-blocking echo server using Python\'s asynchronous IO modules, specifically `asyncio`. The server should handle multiple client connections asynchronously, read messages from clients, and echo the messages back to the respective clients. Requirements: 1. Use the `asyncio` module to implement asynchronous communication. 2. The server should handle multiple concurrent client connections. 3. Implement the server such that it echoes back any message received from a client to the client. 4. Ensure that the server does not block while waiting for client messages. Input: There is no direct input in the traditional sense. The server will continuously run, waiting for client connections and messages. Output: The server will send back to each client any messages it receives from that client. Constraints: 1. The server should gracefully handle and ignore malformed or incomplete messages. 2. The server should be able to handle at least 10 simultaneous client connections. 3. The communication should assume UTF-8 encoding for messages. Performance Requirements: 1. The server should efficiently scale with the number of clients within the handling limit. Example: Pseudo-code to illustrate expected behavior: ```python # Server starts and waits for client connections Client A connects Client B connects # Client A sends a message \\"Hello\\" # Server receives the message and echoes back to Client A: \\"Hello\\" # Client B sends a message \\"World\\" # Server receives the message and echoes back to Client B: \\"World\\" # Clients can continue sending messages and getting them echoed back ``` Implementation Details: - Use `asyncio.start_server` to create the server. - Define `asyncio` tasks to handle client connections and messages. - Read from and write to client sockets asynchronously. ```python import asyncio async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connected to {addr}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode(\'utf8\') print(f\\"Received {message!r} from {addr!r}\\") print(f\\"Echoing: {message!r}\\") writer.write(data) await writer.drain() except Exception as e: print(f\\"Exception: {e}\\") finally: print(f\\"Closing connection to {addr}\\") writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Entry point to start the server asyncio.run(main()) ``` Notes: - Make sure your implementation is robust to handle errors and edge cases. - Comment your code adequately to explain design choices.","solution":"import asyncio async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connected to {addr}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode(\'utf-8\') print(f\\"Received {message!r} from {addr!r}\\") print(f\\"Echoing: {message!r}\\") writer.write(data) await writer.drain() except Exception as e: print(f\\"Exception: {e}\\") finally: print(f\\"Closing connection to {addr}\\") writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"Task: Write a Python function `analyze_function_bytecode` that takes another function as an argument and returns a dictionary with detailed information about the bytecode of the given function. Specifically, the dictionary should include: 1. `opnames` - List of operation names (the name of each bytecode instruction). 2. `consts` - List of constants used in the function. 3. `names` - List of names (variables, functions, etc.) accessed by the function. 4. `jumps` - Dictionary where the keys are indices of bytecode instructions that are jump targets and the values are instruction offsets. Function Signature: ```python def analyze_function_bytecode(func: callable) -> dict: pass ``` Input: - `func` (callable): A function whose bytecode needs to be analyzed. Output: - A dictionary with the following structure: ```python { \'opnames\': List[str], \'consts\': List[Any], \'names\': List[str], \'jumps\': Dict[int, int] } ``` Example: ```python def sample_function(x): if x > 10: return x * 2 return x + 2 result = analyze_function_bytecode(sample_function) print(result) ``` Expected Output: ```python { \'opnames\': [\'LOAD_FAST\', \'LOAD_CONST\', \'COMPARE_OP\', \'POP_JUMP_IF_FALSE\', \'LOAD_FAST\', \'LOAD_CONST\', \'BINARY_MULTIPLY\', \'RETURN_VALUE\', \'LOAD_FAST\', \'LOAD_CONST\', \'BINARY_ADD\', \'RETURN_VALUE\'], \'consts\': [10, 2, 2], \'names\': [], \'jumps\': {6: 12, 16: 30} } ``` Constraints: - You must use the `dis` module to disassemble the bytecode. Instructions: 1. Use the `dis.Bytecode` class to iterate over the bytecode instructions of the given function. 2. Collect the operation names from the `opname` attribute of each instruction. 3. Extract and collect constants and names from the attributes `consts` and `names` in `func.__code__`. 4. Detect jump target instructions and store them in the `jumps` dictionary. This question will test your understanding of Python bytecode and your ability to use the `dis` module for code analysis.","solution":"import dis def analyze_function_bytecode(func: callable) -> dict: Analyzes the bytecode of a given function and returns details about it. Parameters: func (callable): The function to analyze. Returns: dict: A dictionary containing details about the function\'s bytecode. bytecode = dis.Bytecode(func) result = { \'opnames\': [], \'consts\': list(func.__code__.co_consts), \'names\': list(func.__code__.co_names), \'jumps\': {} } for instr in bytecode: result[\'opnames\'].append(instr.opname) if instr.is_jump_target: result[\'jumps\'][instr.offset] = instr.argval return result"},{"question":"You are required to implement an echo server using the asyncio low-level APIs. An echo server listens for incoming TCP connections, reads data sent by the clients, and echoes it back. The server should be able to handle multiple clients at the same time. # Requirements 1. **Create and manage the event loop**: - Use `asyncio.get_event_loop()` to obtain or create an event loop. - Ensure the loop runs until manually stopped. 2. **Create a TCP server**: - Use `loop.create_server()` to create a server that listens on a specified host and port. - Use an appropriate protocol class to handle incoming connections and data reception. 3. **Define the Protocol class**: - Implement the `connection_made`, `data_received`, and `connection_lost` callback methods to handle client connections and data. # Expected Input and Output - The server should start and listen on `127.0.0.1` and `8888`. - Each client that connects can send multiple messages. The server should echo each message back to the client. - Properly handle client disconnections. # Implementation Constraints - The solution must use asyncio\'s low-level APIs as specified in the documentation. - The server should be able to handle at least 50 concurrent client connections. - Ensure the implementation is efficient and handles I/O operations asynchronously. # Example Client Interaction ```python # Client 1 nc 127.0.0.1 8888 Hello, server! Hello, server! ``` ```python # Client 2 nc 127.0.0.1 8888 Another client here. Another client here. ``` # Additional Requirements Provide proper docstrings for your protocol class and server creation function. Ensure the code follows Python\'s PEP 8 style guide. # Code Template ```python import asyncio class EchoProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(f\\"Connection from {self.transport.get_extra_info(\'peername\')}\\") def data_received(self, data): message = data.decode() print(f\\"Received {message} from {self.transport.get_extra_info(\'peername\')}\\") self.transport.write(data) print(\\"Echoed back the message\\") def connection_lost(self, exc): print(\\"Connection closed\\") async def main(host, port): loop = asyncio.get_event_loop() server = await loop.create_server(EchoProtocol, host, port) print(f\\"Serving on {server.sockets[0].getsockname()}\\") try: await server.serve_forever() except KeyboardInterrupt: pass finally: server.close() await server.wait_closed() if __name__ == \'__main__\': asyncio.run(main(\'127.0.0.1\', 8888)) ```","solution":"import asyncio class EchoProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\\"Connection from {peername}\\") def data_received(self, data): message = data.decode() peername = self.transport.get_extra_info(\'peername\') print(f\\"Received {message} from {peername}\\") self.transport.write(data) print(f\\"Echoed back the message to {peername}\\") def connection_lost(self, exc): peername = self.transport.get_extra_info(\'peername\') print(f\\"Connection to {peername} closed\\") async def main(host, port): loop = asyncio.get_event_loop() server = await loop.create_server(EchoProtocol, host, port) print(f\\"Serving on {server.sockets[0].getsockname()}\\") try: await loop.run_forever() except KeyboardInterrupt: pass finally: server.close() await server.wait_closed() if __name__ == \'__main__\': asyncio.run(main(\'127.0.0.1\', 8888))"},{"question":"**Objective**: Demonstrate the ability to use `torch.cond` to implement conditional operations based on the shape and value of input tensors. **Task**: Write a PyTorch module `CustomCondModule` which uses `torch.cond` to apply different operations to the input tensor `x` depending on both its shape and sum of its elements. Specifically: 1. If the number of elements in `x` is greater than 6 and the sum of elements in `x` is greater than 5.0, calculate the cosine of `x`. 2. If the number of elements in `x` is greater than 6 but the sum of elements is not greater than 5.0, calculate the sine of `x`. 3. If the number of elements in `x` is not greater than 6 and the sum of elements is greater than 5.0, add 2 to all elements of `x`. 4. If none of the above conditions are met, subtract 1 from all elements of `x`. **Implementation Requirements**: - Implement the `CustomCondModule` class with a `forward` method to perform the operations described using `torch.cond`. - Ensure the `forward` method performs efficiently and adheres to the provided constraints. **Input Format**: - A single input tensor `x` (torch.Tensor) of any shape and real-valued elements. **Output Format**: - A tensor of the same shape as `x`, with values transformed according to the specified conditions. **Constraints**: - Use the `torch.cond` function to implement the conditional logic. - You must define the functions to be used for the true and false branches within your module. **Example**: ```python import torch class CustomCondModule(torch.nn.Module): def __init__(self): super(CustomCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def condition1_fn(x): return x.cos() def condition2_fn(x): return x.sin() def condition3_fn(x): return x + 2 def condition4_fn(x): return x - 1 def case1(x): return torch.cond(torch.sum(x) > 5.0, condition1_fn, condition2_fn, (x,)) def case2(x): return torch.cond(torch.sum(x) > 5.0, condition3_fn, condition4_fn, (x,)) return torch.cond(torch.numel(x) > 6, case1, case2, (x,)) # Test cases model = CustomCondModule() inp1 = torch.Tensor([1, 2, 0, 2, 1, 1]) inp2 = torch.Tensor([1, 2, 0, 2, 1, 1, 0, 1]) inp3 = torch.Tensor([6]) inp4 = torch.Tensor([1, 1]) print(model(inp1)) print(model(inp2)) print(model(inp3)) print(model(inp4)) ``` **Expected Outputs**: 1. For `inp1 = torch.Tensor([1, 2, 0, 2, 1, 1])`, the sum is `7` and `num_elements` is `6`: `inp1 + 2`. 2. For `inp2 = torch.Tensor([1, 2, 0, 2, 1, 1, 0, 1])`, the sum is `8` and `num_elements` is `8`: `cos(inp2)`. 3. For `inp3 = torch.Tensor([6])`, the sum is `6` and `num_elements` is `1`: `inp3 + 2`. 4. For `inp4 = torch.Tensor([1, 1])`, the sum is `2` and `num_elements` is `2`: `inp4 - 1`.","solution":"import torch # torch.cond functionality is simulated here as torch.cond is not a standard PyTorch functionality # Assuming a helper function that can simulate the conditional branch logic based on PyTorch operations class CustomCondModule(torch.nn.Module): def __init__(self): super(CustomCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def condition1_fn(x): return torch.cos(x) def condition2_fn(x): return torch.sin(x) def condition3_fn(x): return x + 2 def condition4_fn(x): return x - 1 num_elements = torch.numel(x) sum_elements = torch.sum(x) if num_elements > 6: if sum_elements > 5.0: return condition1_fn(x) else: return condition2_fn(x) else: if sum_elements > 5.0: return condition3_fn(x) else: return condition4_fn(x)"},{"question":"# Custom Python Interpreter Objective: You are required to create a custom Python interactive interpreter using the `code` and `codeop` modules. This custom interpreter should have some additional features beyond standard Python behavior. Task: 1. Implement a class `CustomInterpreter` which inherits from the `code.InteractiveInterpreter` base class. 2. Add a feature to the interpreter where every variable assignment is logged into a dictionary attribute called `variables`. 3. Implement a `print_log` method within `CustomInterpreter` that prints the logged variable assignments along with their values. 4. Implement an error counter that keeps track of how many errors have occurred during the execution of code. Constraints: - You should only use the `code` and `codeop` modules for handling the interaction and compilation of Python code. - The behavior for variable assignments should follow the Python conventions strictly. - The counter for errors should be updated every time an exception occurs during the execution of any code. Input: - You do not need to handle any specific input formats for instantiating the class. - The class should be able to execute multiple lines of code fed into it as a string. Output: - The `variables` dictionary should update dynamically as new variables are assigned. - The `print_log` method should output the variable names along with their assigned values. - An additional method `error_count` should return the total number of errors encountered during the usage of the interpreter. Example Usage: ```python # Example of how your class should work: interpreter = CustomInterpreter() # Execute some code interpreter.runcode(\\"x = 10\\") interpreter.runcode(\\"y = 20\\") interpreter.runcode(\\"def add(a, b): return a + b\\") interpreter.runcode(\\"z = add(x, y)\\") # Print variable assignments interpreter.print_log() # Introduce an error interpreter.runcode(\\"w = add(x, unknown_var)\\") # Check error count print(interpreter.error_count()) # Output should be 1 ``` The `print_log` output should be: ``` x: 10 y: 20 z: 30 ``` Note: Make sure to handle edge cases such as reassignment of variables and ensure that the interpreter does not crash on runtime errors but instead updates the error count.","solution":"import code class CustomInterpreter(code.InteractiveInterpreter): def __init__(self, locals=None): super().__init__(locals) self.variables = {} self._error_count = 0 def runcode(self, code): try: compiled_code = self.compile(code, symbol=\'exec\') if compiled_code is not None: exec(compiled_code, self.locals) self.log_variables() except Exception as err: self._error_count += 1 def log_variables(self): for key, value in self.locals.items(): if not key.startswith(\'__\') and not callable(value): self.variables[key] = value def print_log(self): for key, value in self.variables.items(): print(f\\"{key}: {value}\\") def error_count(self): return self._error_count"},{"question":"# Functional Programming in Python Objective: Assess students\' understanding of advanced functional programming concepts in Python. Question: You are required to implement a functional programming solution to a problem involving collections of data. Given a list of dictionaries representing students and their respective scores in various subjects, you need to perform transformations and aggregations using functional programming paradigms. Input: - A list of dictionaries, where each dictionary represents a student and contains the following keys: - `name`: a string representing the student\'s name. - `scores`: a dictionary where keys are subject names and values are the scores in those subjects (integers). For example: ```python students = [ {\'name\': \'Alice\', \'scores\': {\'math\': 90, \'science\': 100, \'literature\': 70}}, {\'name\': \'Bob\', \'scores\': {\'math\': 80, \'science\': 60, \'literature\': 85}}, {\'name\': \'Charlie\', \'scores\': {\'math\': 65, \'science\': 75, \'literature\': 80}} ] ``` Requirements: 1. **Filter** the students to include only those who have scored at least 75 in all subjects. 2. **Transform** the data to a format where each student\'s name and their average score are represented as a dictionary with keys `name` and `average_score`. 3. **Sort** the transformed data by `average_score` in descending order. Output: - A list of dictionaries, where each dictionary contains: - `name`: a string representing the student\'s name. - `average_score`: a float representing the student\'s average score across all subjects. For the given input, the output should be: ```python [ {\'name\': \'Alice\', \'average_score\': 86.67} ] ``` Constraints: - Use functional programming paradigms (i.e., `map`, `filter`, `reduce`, `lambda` functions, list comprehensions). - Avoid using explicit loops (`for`, `while`). Function Signature: ```python def process_student_scores(students: list) -> list: ``` Notes: - You can assume that the scores are always integers, and there is at least one subject per student. - The average score should be rounded to 2 decimal places. Implement the `process_student_scores` function based on the requirements above.","solution":"from functools import reduce def process_student_scores(students): # Step 1: Filter students who have scored at least 75 in all subjects filtered_students = filter(lambda student: all(score >= 75 for score in student[\'scores\'].values()), students) # Step 2: Transform the data to get the average score transformed_students = map(lambda student: { \'name\': student[\'name\'], \'average_score\': round(sum(student[\'scores\'].values()) / len(student[\'scores\']), 2) }, filtered_students) # Step 3: Sort the students by average_score in descending order sorted_students = sorted(transformed_students, key=lambda student: student[\'average_score\'], reverse=True) return sorted_students"},{"question":"**Coding Challenge: Handling File Encoding and Decoding using binhex** You are given a module `binhex` that can encode a binary file into a binhex file (an ASCII representation of Macintosh files) and decode a binhex file back into its binary form. Despite being deprecated, your task is to write two functions that use the `binhex` module to: 1. Convert a list of binary files to binhex files. 2. Convert a list of binhex files back to their original binary forms. You must also handle any potential errors that might occur during the encoding and decoding processes. # Function Specifications 1. **encode_files_to_binhex** - **Input**: - `file_list`: A list of tuples where each tuple contains two strings. - The first string is the input binary filename. - The second string is the output binhex filename. - **Output**: - A list of strings indicating successful encoding for each file in the format: `\\"Encoded <input_filename> to <output_filename>\\"`. 2. **decode_files_from_binhex** - **Input**: - `file_list`: A list of tuples where each tuple contains two strings. - The first string is the input binhex filename. - The second string is the output binary filename. - **Output**: - A list of strings indicating successful decoding for each file in the format: `\\"Decoded <input_filename> to <output_filename>\\"`. # Constraints - If an error occurs during encoding or decoding, handle it gracefully by appending an error message to the output list in the format: `\\"Error processing <filename>: <error_message>\\"`. - Assume that the input files and output directories are always valid and the files are accessible. # Example: ```python file_list_to_encode = [ (\'file1.bin\', \'file1.hqx\'), (\'file2.bin\', \'file2.hqx\') ] file_list_to_decode = [ (\'file1.hqx\', \'file1_decoded.bin\'), (\'file2.hqx\', \'file2_decoded.bin\') ] # Expected Output for encode_files_to_binhex # [ # \\"Encoded file1.bin to file1.hqx\\", # \\"Encoded file2.bin to file2.hqx\\" # ] # Expected Output for decode_files_from_binhex # [ # \\"Decoded file1.hqx to file1_decoded.bin\\", # \\"Decoded file2.hqx to file2_decoded.bin\\" # ] ``` Implement the two functions `encode_files_to_binhex` and `decode_files_from_binhex` using the `binhex` module according to the specifications.","solution":"import binhex def encode_files_to_binhex(file_list): results = [] for input_filename, output_filename in file_list: try: binhex.binhex(input_filename, output_filename) results.append(f\\"Encoded {input_filename} to {output_filename}\\") except Exception as e: results.append(f\\"Error processing {input_filename}: {str(e)}\\") return results def decode_files_from_binhex(file_list): results = [] for input_filename, output_filename in file_list: try: binhex.hexbin(input_filename, output_filename) results.append(f\\"Decoded {input_filename} to {output_filename}\\") except Exception as e: results.append(f\\"Error processing {input_filename}: {str(e)}\\") return results"},{"question":"Objective: Write a Python function that utilizes `asyncio` to run multiple shell commands concurrently and return the following details: 1. Command executed. 2. Return code of the command. 3. Standard output (stdout) of the command. 4. Standard error (stderr) of the command. Function Signature: ```python import asyncio async def run_commands_concurrently(commands: list) -> list: Execute multiple shell commands concurrently and capture their return code, stdout, and stderr. Args: commands (list): A list of shell commands to be executed concurrently. Each element in the list is a string representing a single shell command. Returns: list: A list of dictionaries, each containing the following keys: - \'command\': The command that was executed. - \'returncode\': The return code of the command. - \'stdout\': The standard output produced by the command. - \'stderr\': The standard error produced by the command. pass ``` Input: - `commands` (list): A list of shell commands to be executed concurrently. Each command is a string. Output: - (list): A list of dictionaries containing the following keys: - `command`: The command that was executed. - `returncode`: The return code of the command. - `stdout`: The standard output produced by the command. - `stderr`: The standard error produced by the command. Constraints: - Ensure all whitespace and special characters in the command are quoted appropriately. - The function should handle exceptions and return error details. - Use `asyncio.create_subprocess_shell` to execute the commands. Example: ```python import asyncio async def run_commands_concurrently(commands: list) -> list: results = [] async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() result = { \'command\': cmd, \'returncode\': proc.returncode, \'stdout\': stdout.decode(), \'stderr\': stderr.decode() } return result tasks = [run(cmd) for cmd in commands] results = await asyncio.gather(*tasks) return results # Example usage: commands = [\'echo Hello World\', \'ls /invalid_path\'] output = asyncio.run(run_commands_concurrently(commands)) print(output) ``` Expected output: ```python [ { \'command\': \'echo Hello World\', \'returncode\': 0, \'stdout\': \'Hello Worldn\', \'stderr\': \'\' }, { \'command\': \'ls /invalid_path\', \'returncode\': 1, \'stdout\': \'\', \'stderr\': \'ls: /invalid_path: No such file or directoryn\' } ] ``` This function should demonstrate your capability to: - Write asynchronous code using `asyncio`. - Manage and interact with subprocesses. - Handle and report errors effectively.","solution":"import asyncio async def run_commands_concurrently(commands: list) -> list: Execute multiple shell commands concurrently and capture their return code, stdout, and stderr. Args: commands (list): A list of shell commands to be executed concurrently. Each element in the list is a string representing a single shell command. Returns: list: A list of dictionaries, each containing the following keys: - \'command\': The command that was executed. - \'returncode\': The return code of the command. - \'stdout\': The standard output produced by the command. - \'stderr\': The standard error produced by the command. results = [] async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() result = { \'command\': cmd, \'returncode\': proc.returncode, \'stdout\': stdout.decode(), \'stderr\': stderr.decode() } return result tasks = [run(cmd) for cmd in commands] results = await asyncio.gather(*tasks) return results"},{"question":"# Abstract Base Classes and Sequence Implementation Problem Statement You are required to implement a custom sequence type that adheres to the `collections.abc.Sequence` abstract base class. This custom sequence should be indexed and should store elements in an underlying list. You need to ensure that all mandatory methods (`__len__`, `__getitem__`) are implemented as per the `Sequence` specification, and optionally override the mixin methods. Additionally, you should provide a method to reverse the elements of the sequence and a method to slice the sequence with a step value. Task 1. Create a class `CustomSequence` that inherits from `collections.abc.Sequence`. 2. Implement the required methods for the `Sequence` ABC: - `__len__(self) -> int`: Returns the number of elements in the sequence. - `__getitem__(self, index: int) -> Any`: Returns the element at the given index. 3. Implement the following additional methods: - `reverse(self) -> None`: Reverses the elements of the sequence in-place. - `slice(self, start: int, stop: int, step: int) -> \'CustomSequence\'`: Returns a new `CustomSequence` object with the elements sliced from the original sequence. Constraints - You must not use any libraries or frameworks other than `collections.abc`. - Your `CustomSequence` class should be able to handle both positive and negative indices. - The `reverse` method should modify the sequence in-place. Example ```python from collections.abc import Sequence class CustomSequence(Sequence): # Your implementation here # Example usage seq = CustomSequence([1, 2, 3, 4, 5]) print(len(seq)) # Output: 5 print(seq[2]) # Output: 3 seq.reverse() print(seq[0]) # Output: 5 sliced_seq = seq.slice(1, 4, 1) print(list(sliced_seq)) # Output: [4, 3, 2] ``` # Notes - Ensure that your implementation of `CustomSequence` passes standard checks of a sequence type, such as being iterable and supporting the `len` and index access functionalities. Submission Requirements - Implement the class `CustomSequence` with the required and additional methods. - Ensure your code passes the provided example to demonstrate functionality. - Submit the code with necessary comments and documentation.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = list(data) def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def reverse(self): self._data.reverse() def slice(self, start, stop, step): return CustomSequence(self._data[start:stop:step])"},{"question":"Fault Traceback Handling in Python **Objective:** Implement a Python script that simulates a long-running process and demonstrates the usage of the `faulthandler` module to handle faults and timeouts. **Scenario:** You are required to implement a Python function `long_running_process(timeout, filename)` that simulates a long-running process. If the process takes longer than a specified timeout, the Python tracebacks of all threads should be dumped into a log file. Additionally, if a specific user signal (SIGUSR1) is received, the program should immediately dump the thread tracebacks into the same log file. **Requirements:** 1. **Function Signature:** ```python def long_running_process(timeout: int, filename: str) -> None: ``` 2. **Parameters:** - `timeout` (int): The number of seconds before the tracebacks are dumped. - `filename` (str): The name of the file where the tracebacks will be logged. 3. **Behavior:** - Enable the `faulthandler` to handle faults and dump Python tracebacks into the provided file. - Use `faulthandler.dump_traceback_later()` to schedule the traceback dumping after the given timeout. - Register a user signal (SIGUSR1) to immediately dump the tracebacks into the log file when the signal is received. 4. **Constraints:** - Only use the methods and modules provided in the Python standard library. - The program should be robust and handle the opening and closing of the file correctly. 5. **Example:** ```python import signal import time import os def long_running_process(timeout, filename): import faulthandler with open(filename, \'w\') as f: faulthandler.enable(f) faulthandler.dump_traceback_later(timeout, file=f) faulthandler.register(signal.SIGUSR1, file=f) # Simulate a long-running process try: while True: print(\\"Running...\\") time.sleep(1) except KeyboardInterrupt: pass faulthandler.cancel_dump_traceback_later() faulthandler.disable() if __name__ == \\"__main__\\": log_file = \\"traceback.log\\" timeout_seconds = 10 long_running_process(timeout_seconds, log_file) ``` **Notes:** - You can test the function by running it in the terminal and sending the `SIGUSR1` signal using the `kill` command (e.g., `kill -SIGUSR1 <pid>`). - Ensure that the traceback log file contains the tracebacks after the timeout or upon receiving the signal.","solution":"import faulthandler import signal import time def long_running_process(timeout: int, filename: str) -> None: Simulates a long-running process and dumps the tracebacks of all threads into a log file if the process takes longer than the specified timeout or immediately upon receiving the SIGUSR1 signal. Parameters: timeout (int): The number of seconds before dumping the tracebacks. filename (str): The name of the file where the tracebacks will be logged. # Open the file to log tracebacks with open(filename, \'w\') as f: # Enable faulthandler to write tracebacks to the file faulthandler.enable(f) # Schedule a traceback dump after the specified timeout faulthandler.dump_traceback_later(timeout, file=f) # Register a handler to dump tracebacks on receiving SIGUSR1 signal faulthandler.register(signal.SIGUSR1, file=f) # Simulate a long-running process try: while True: print(\\"Running...\\") time.sleep(1) except KeyboardInterrupt: pass finally: # Ensure to cancel any scheduled traceback dumps and disable faulthandler faulthandler.cancel_dump_traceback_later() faulthandler.disable()"},{"question":"# Question: Implementation of a Simple Key-Value Storage System **Description:** You are tasked with implementing a simple key-value storage system using the `dbm` package in Python. The system should support basic operations like setting a key-value pair, retrieving a value for a given key, deleting a key, and listing all keys. Additionally, you should implement functionality to return the storage type used (dbm.gnu, dbm.ndbm, or dbm.dumb). **Requirements:** 1. Implement the `KeyValueStore` class with the following methods: - `__init__(self, filename: str, flag: str = \'c\', mode: int = 0o666)`: Initializes the database with the given filename, flag, and mode. - `set(self, key: str, value: str)`: Stores the key-value pair in the database. - `get(self, key: str, default: str = None) -> str`: Retrieves the value for the given key. Returns `default` if the key does not exist. - `delete(self, key: str)`: Deletes the key-value pair from the database. - `list_keys(self) -> list`: Lists all keys in the database. - `storage_type(self) -> str`: Returns the storage type used (`dbm.gnu`, `dbm.ndbm`, or `dbm.dumb`). **Constraints:** - Both keys and values must be strings. - Use appropriate exception handling to manage errors such as trying to delete a non-existing key. - The database should be closed properly after usage, preferably using the context management protocol. **Example Usage:** ```python store = KeyValueStore(\'mydb\') store.set(\'name\', \'Alice\') print(store.get(\'name\')) # Output: Alice store.set(\'age\', \'30\') print(store.list_keys()) # Output: [b\'name\', b\'age\'] store.delete(\'name\') print(store.get(\'name\', \'Unknown\')) # Output: Unknown print(store.storage_type()) # Output: dbm.gnu (or another dbm type based on availability) ``` **Note:** - Ensure the keys and values are converted to bytes before storing and retrieved as strings. **Testing:** You should write a set of tests that cover: - Initialization of the `KeyValueStore` class. - Adding, retrieving, listing, and deleting key-value pairs. - Retrieving the storage type. - Exception handling for operations like deleting a non-existing key and adding non-string data.","solution":"import dbm class KeyValueStore: def __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666): self.filename = filename self.flag = flag self.mode = mode self.db = dbm.open(self.filename, self.flag, self.mode) def set(self, key: str, value: str): self.db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') def get(self, key: str, default: str = None) -> str: try: return self.db[key.encode(\'utf-8\')].decode(\'utf-8\') except KeyError: return default def delete(self, key: str): try: del self.db[key.encode(\'utf-8\')] except KeyError: raise KeyError(\\"Given key does not exist in the database\\") def list_keys(self) -> list: return [key.decode(\'utf-8\') for key in self.db.keys()] def storage_type(self) -> str: return self.db.__module__ def close(self): self.db.close()"},{"question":"**Objective:** Implement a series of functions to process a text file containing information about various products. Each line in the file will represent a product in the format: `product_id,product_name,quantity,price`. The task will require reading from the file, performing some operations, and saving the results in JSON format. **Specifications:** 1. **Function Name:** `read_products` - **Input:** `filename` (str) - the name of the text file to read from. - **Output:** List of dictionaries with each dictionary representing a product. - **Description:** Reads the file with each line containing product details separated by commas. Each line should be parsed and converted to a dictionary with keys: `\'product_id\'`, `\'product_name\'`, `\'quantity\'`, and `\'price\'`. `\'quantity\'` and `\'price\'` should be converted to integers and float respectively, while `\'product_id\'` and `\'product_name\'` should remain strings. ```python def read_products(filename: str) -> list: pass ``` 2. **Function Name:** `calculate_total_value` - **Input:** `products` (list) - list of product dictionaries. - **Output:** Dictionary where keys are `product_id` and values are total value (quantity * price). - **Description:** For each product in the list, calculate the total value and store it in a dictionary. ```python def calculate_total_value(products: list) -> dict: pass ``` 3. **Function Name:** `save_to_json` - **Input:** - `data` (dict) - dictionary with product total values. - `filename` (str) - the name of the JSON file to write to. - **Output:** None - **Description:** Save the given dictionary to a JSON file with UTF-8 encoding. ```python def save_to_json(data: dict, filename: str) -> None: pass ``` # Constraints 1. You can assume the text file is well-formatted and does not contain any duplicate `product_id`s. 2. The filename strings (for both text and JSON files) will always be valid and the files will be in the local directory. # Example Given a text file `products.txt` with the following content: ``` 1001,Widget,4,19.99 1002,Gizmo,2,24.99 1003,Doodad,5,9.99 ``` The expected output for implementing these functionalities would be: 1. Calling `read_products(\'products.txt\')` will return: ```python [ {\'product_id\': \'1001\', \'product_name\': \'Widget\', \'quantity\': 4, \'price\': 19.99}, {\'product_id\': \'1002\', \'product_name\': \'Gizmo\', \'quantity\': 2, \'price\': 24.99}, {\'product_id\': \'1003\', \'product_name\': \'Doodad\', \'quantity\': 5, \'price\': 9.99} ] ``` 2. Calling `calculate_total_value` with the above output will return: ```python { \'1001\': 79.96, \'1002\': 49.98, \'1003\': 49.95 } ``` 3. Calling `save_to_json` with the above dictionary and `output.json` will save the data to `output.json` file. ```python import json def read_products(filename: str) -> list: products = [] with open(filename, \'r\', encoding=\'utf-8\') as f: for line in f: product_id, product_name, quantity, price = line.strip().split(\',\') products.append({ \'product_id\': product_id, \'product_name\': product_name, \'quantity\': int(quantity), \'price\': float(price) }) return products def calculate_total_value(products: list) -> dict: total_value = {} for product in products: total_value[product[\'product_id\']] = product[\'quantity\'] * product[\'price\'] return total_value def save_to_json(data: dict, filename: str) -> None: with open(filename, \'w\', encoding=\'utf-8\') as f: json.dump(data, f, ensure_ascii=False, indent=4) # Example usage: # products = read_products(\'products.txt\') # total_value = calculate_total_value(products) # save_to_json(total_value, \'output.json\') ```","solution":"import json def read_products(filename: str) -> list: products = [] with open(filename, \'r\', encoding=\'utf-8\') as f: for line in f: product_id, product_name, quantity, price = line.strip().split(\',\') products.append({ \'product_id\': product_id, \'product_name\': product_name, \'quantity\': int(quantity), \'price\': float(price) }) return products def calculate_total_value(products: list) -> dict: total_value = {} for product in products: total_value[product[\'product_id\']] = product[\'quantity\'] * product[\'price\'] return total_value def save_to_json(data: dict, filename: str) -> None: with open(filename, \'w\', encoding=\'utf-8\') as f: json.dump(data, f, ensure_ascii=False, indent=4)"},{"question":"**Title: Modeling and Analysis using PyTorch Probability Distributions** **Objective:** The purpose of this question is to assess your understanding of the `torch.distributions` module, specifically how to work with different probability distributions, perform sampling, compute probabilities, and apply transformations. You will be tasked with simulating data using specific distributions and conducting analysis on the simulated data. **Problem Statement:** You are provided with data that seems to follow a mixture of two normal distributions. Your task is to implement functions to model this scenario using `torch.distributions`, sample data from this model, and compute the log probability of given data points under your model. **Requirements:** 1. Implement a function `create_mixture_model` that returns a mixture of two normal distributions with specified means, standard deviations, and mixture weights. 2. Implement a function `sample_data` that uses the mixture model to generate a specified number of data points. 3. Implement a function `compute_log_prob` that computes the log probability of provided data points under the mixture model. **Function Signatures:** ```python def create_mixture_model(mean1: float, std1: float, mean2: float, std2: float, weight1: float, weight2: float) -> torch.distributions.Distribution: Create and return a mixture of two normal distributions. :param mean1: Mean of the first normal distribution. :param std1: Standard deviation of the first normal distribution. :param mean2: Mean of the second normal distribution. :param std2: Standard deviation of the second normal distribution. :param weight1: Weight of the first distribution in the mixture (should add up to 1 with weight2). :param weight2: Weight of the second distribution in the mixture (should add up to 1 with weight1). :return: A mixture distribution object. pass def sample_data(mixture_model: torch.distributions.Distribution, num_samples: int) -> torch.Tensor: Generate samples from the given mixture model. :param mixture_model: The mixture distribution created by `create_mixture_model`. :param num_samples: The number of samples to generate. :return: A tensor containing the generated samples. pass def compute_log_prob(mixture_model: torch.distributions.Distribution, data: torch.Tensor) -> torch.Tensor: Compute the log probability of the given data points under the mixture model. :param mixture_model: The mixture distribution created by `create_mixture_model`. :param data: A tensor containing the data points to evaluate. :return: A tensor containing the log probabilities of each data point. pass ``` **Example Usage:** ```python import torch from torch.distributions import MixtureSameFamily, Categorical, Normal # Example parameters for the mixture model mean1, std1 = 0.0, 1.0 mean2, std2 = 5.0, 1.0 weight1, weight2 = 0.3, 0.7 # Create the mixture model mixture_model = create_mixture_model(mean1, std1, mean2, std2, weight1, weight2) # Generate samples samples = sample_data(mixture_model, 1000) # Compute log probabilities for the generated samples log_probs = compute_log_prob(mixture_model, samples) print(log_probs) ``` In this example, you will specify the means, standard deviations, and mixture weights to create a mixture of two normal distributions, generate samples from this model, and compute the log probabilities for these samples. **Constraints:** - Ensure that weights `weight1` and `weight2` add up to 1. - The number of samples for `sample_data` should be a positive integer. - You may use any functionality available in the `torch.distributions` module but must implement the functions as specified. **Performance Requirements:** - Ensure that the implementation is efficient for generating and evaluating several thousand samples. Good luck!","solution":"import torch from torch.distributions import MixtureSameFamily, Categorical, Normal def create_mixture_model(mean1: float, std1: float, mean2: float, std2: float, weight1: float, weight2: float) -> torch.distributions.Distribution: Create and return a mixture of two normal distributions. :param mean1: Mean of the first normal distribution. :param std1: Standard deviation of the first normal distribution. :param mean2: Mean of the second normal distribution. :param std2: Standard deviation of the second normal distribution. :param weight1: Weight of the first distribution in the mixture (should add up to 1 with weight2). :param weight2: Weight of the second distribution in the mixture (should add up to 1 with weight1). :return: A mixture distribution object. # Ensure the weights sum to 1 assert weight1 + weight2 == 1.0, \\"Weights should sum to 1\\" # Create component distributions component_dist = Normal(torch.tensor([mean1, mean2]), torch.tensor([std1, std2])) # Create categorical distribution for mixture weights mixture_dist = Categorical(torch.tensor([weight1, weight2])) # Create the mixture model mixture_model = MixtureSameFamily(mixture_dist, component_dist) return mixture_model def sample_data(mixture_model: torch.distributions.Distribution, num_samples: int) -> torch.Tensor: Generate samples from the given mixture model. :param mixture_model: The mixture distribution created by `create_mixture_model`. :param num_samples: The number of samples to generate. :return: A tensor containing the generated samples. # Ensure the number of samples is positive assert num_samples > 0, \\"Number of samples should be positive\\" # Sample data from the mixture model samples = mixture_model.sample((num_samples,)) return samples def compute_log_prob(mixture_model: torch.distributions.Distribution, data: torch.Tensor) -> torch.Tensor: Compute the log probability of the given data points under the mixture model. :param mixture_model: The mixture distribution created by `create_mixture_model`. :param data: A tensor containing the data points to evaluate. :return: A tensor containing the log probabilities of each data point. # Compute and return log probabilities of the data points log_prob = mixture_model.log_prob(data) return log_prob"},{"question":"Objective: Demonstrate your ability to use seaborn objects to create and customize plots. Question: You are provided with the \'mpg\' dataset. Your task is to create a complex scatter plot using seaborn objects that satisfies the following requirements: 1. Plot \'horsepower\' on the x-axis and \'mpg\' on the y-axis. 2. Use different colors for different \'origin\' categories. 3. Use different markers for different ranges of \'weight\': - Light weight: `weight < 2000` (e.g., \'o\') - Medium weight: `2000 <= weight < 3000` (e.g., \'x\') - Heavy weight: `weight >= 3000` (e.g., asterisk *) 4. Add jitter to the points on the y-axis to visually separate overlapping points. 5. Points should have partial transparency to indicate density (e.g., `alpha=0.5`). Input: None (The \'mpg\' dataset should be loaded within the function). Output: A seaborn plot displayed using matplotlib\'s `plt.show()`. Constraints: - The weights of the cars are categorized as described (Light, Medium, Heavy). - The function should run efficiently and handle the entire dataset. Function Signature: ```python import seaborn.objects as so from seaborn import load_dataset def plot_mpg_data(): import matplotlib.pyplot as plt # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot object p = so.Plot(mpg, \\"horsepower\\", \\"mpg\\") # Define a function to categorize weights def categorize_weight(weight): if weight < 2000: return \'Light\' elif 2000 <= weight < 3000: return \'Medium\' else: return \'Heavy\' # Apply the function to the dataset to create the \'weight_category\' column mpg[\'weight_category\'] = mpg[\'weight\'].apply(categorize_weight) # Map the categories to specific markers weight_marker_mapping = { \'Light\': \'o\', \'Medium\': \'x\', \'Heavy\': \'*\' } # Create the plot with the specified properties p.add(so.Dots(), color=\\"origin\\", marker=\\"weight_category\\").scale(marker=weight_marker_mapping).add(so.Jitter(0.25), so.Dots(alpha=0.5)) # Show the plot plt.show() # Example usage: # plot_mpg_data() ``` **Note**: You don\'t need to return anything from the function. The goal is to generate and display the plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_mpg_data(): import matplotlib.pyplot as plt # Load the dataset mpg = load_dataset(\\"mpg\\") # Define a function to categorize weights def categorize_weight(weight): if weight < 2000: return \'Light\' elif 2000 <= weight < 3000: return \'Medium\' else: return \'Heavy\' # Apply the function to the dataset to create the \'weight_category\' column mpg[\'weight_category\'] = mpg[\'weight\'].apply(categorize_weight) # Map the categories to specific markers weight_marker_mapping = { \'Light\': \'o\', \'Medium\': \'x\', \'Heavy\': \'*\' } # Create the plot object p = so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\", color=\\"origin\\", marker=\\"weight_category\\").scale(marker=weight_marker_mapping) # Add jitter and transparency, create the scatter plot p.add(so.Dots(alpha=0.5), so.Jitter(y=0.25)) # Show the plot plt.show() # Example usage: # plot_mpg_data()"},{"question":"# Compilation Automation with `py_compile` Your task is to create a function that automates the compilation of multiple Python source files into their corresponding byte-code files. Additionally, the function should allow for different invalidation modes for the byte-code files and handle errors robustly. Function Signature ```python def compile_multiple_files(file_list, invalidation_mode, compile_path=None, raise_errors=False, quiet_level=1): Compiles multiple Python source files into byte-code. Parameters: - file_list (list of str): A list of file paths to the Python source files to be compiled. - invalidation_mode (str): The invalidation mode to use (\'timestamp\', \'checked_hash\', \'unchecked_hash\'). - compile_path (str, optional): The directory to store compiled byte-code files. If None, use default paths. - raise_errors (bool, optional): If True, raise exceptions on compile errors. Otherwise, write errors to stderr. - quiet_level (int, optional): Controls verbosity of error handling. 0: Write error string to stderr and return None. 1: Same as 0, but more quiet. 2: No message written, raise_errors has no effect. Returns: - A list of paths to the compiled byte-code files or None for files that failed to compile. pass ``` Requirements 1. **Input and Output**: - The function receives: - `file_list`: A list of file paths to the source files `[str]`. - `invalidation_mode`: A string representing the invalidation mode (`\'timestamp\'`, `\'checked_hash\'`, `\'unchecked_hash\'`). - `compile_path`: A directory where compiled files should be saved (`str`, optional). - `raise_errors`: A boolean to decide whether to raise exceptions on errors (`bool`, optional). - `quiet_level`: An integer to set the verbosity level (`int`, optional). - The function returns a list of paths to the compiled byte-code files, or `None` for files that failed to compile. 2. **Functionality**: - Compile each source file in `file_list` to its corresponding byte-code file. - Use the `py_compile` module\'s `compile` function. - Use the specified `invalidation_mode` for the compilation process. - Respect the `raise_errors` and `quiet_level` parameters to handle errors appropriately. - Save the compiled files in the specified `compile_path` if provided; otherwise, use the default path. 3. **Constraints**: - You should map the `invalidation_mode` string to the appropriate member of `py_compile.PycInvalidationMode`. - Ensure robust error handling based on the `raise_errors` and `quiet_level` parameters. You may assume the following about the input: - `file_list` contains valid paths to Python source files. - `invalidation_mode` is always one of `\'timestamp\'`, `\'checked_hash\'`, `\'unchecked_hash\'`. # Example Usage ```python file_list = [\\"script1.py\\", \\"script2.py\\"] invalidation_mode = \\"checked_hash\\" compile_path = \\"./compiled\\" raise_errors = False quiet_level = 1 compiled_files = compile_multiple_files(file_list, invalidation_mode, compile_path, raise_errors, quiet_level) print(compiled_files) # Expected output: [\'./compiled/script1.pyc\', \'./compiled/script2.pyc\'] ``` By solving this task, students will demonstrate their ability to use the `py_compile` module effectively, handle exceptions, and manage file paths flexibly.","solution":"import os import py_compile def compile_multiple_files(file_list, invalidation_mode, compile_path=None, raise_errors=False, quiet_level=1): Compiles multiple Python source files into byte-code. Parameters: - file_list (list of str): A list of file paths to the Python source files to be compiled. - invalidation_mode (str): The invalidation mode to use (\'timestamp\', \'checked_hash\', \'unchecked_hash\'). - compile_path (str, optional): The directory to store compiled byte-code files. If None, use default paths. - raise_errors (bool, optional): If True, raise exceptions on compile errors. Otherwise, write errors to stderr. - quiet_level (int, optional): Controls verbosity of error handling. 0: Write error string to stderr and return None. 1: Same as 0, but more quiet. 2: No message written, raise_errors has no effect. Returns: - A list of paths to the compiled byte-code files or None for files that failed to compile. invalidation_modes = { \'timestamp\': py_compile.PycInvalidationMode.TIMESTAMP, \'checked_hash\': py_compile.PycInvalidationMode.CHECKED_HASH, \'unchecked_hash\': py_compile.PycInvalidationMode.UNCHECKED_HASH } if invalidation_mode not in invalidation_modes: raise ValueError(f\\"Invalid invalidation mode: {invalidation_mode}\\") compiled_files = [] for file_path in file_list: try: compiled_file = py_compile.compile( file_path, cfile=os.path.join(compile_path, os.path.basename(file_path)+\'c\') if compile_path else None, doraise=raise_errors, invalidation_mode=invalidation_modes[invalidation_mode] ) compiled_files.append(compiled_file) except Exception as e: if quiet_level == 0: print(f\\"Error compiling {file_path}: {e}\\", file=sys.stderr) compiled_files.append(None) elif quiet_level == 1: # quieter output, less detailed error messages compiled_files.append(None) elif quiet_level == 2: compiled_files.append(None) return compiled_files"},{"question":"# Multi-Threaded Task Scheduler **Problem Statement**: You are required to implement a multi-threaded task scheduler in Python using the `threading` module. The scheduler should manage tasks submitted to it, execute them concurrently, and ensure proper synchronization to avoid race conditions. **Task Requirements**: 1. Implement a `TaskScheduler` class which: - Initializes with a specified number of worker threads. - Has methods to add tasks to the scheduler. - Ably manages and executes tasks while ensuring no data corruption or race conditions occur. - Uses `Condition` objects to signal and control the thread execution. 2. Implement a mechanism to demonstrate: - Adding tasks. - Starting and stopping the scheduler. - Proper handling of task completion. **Class Definition**: ```python import threading from queue import Queue class TaskScheduler: def __init__(self, num_workers: int): Initializes the TaskScheduler with the given number of worker threads. Args: num_workers (int): The number of worker threads. # Your implementation here def add_task(self, task: callable, *args, **kwargs): Adds a task to the scheduler. Args: task (callable): The task function to be executed. *args: The positional arguments for the task. **kwargs: The keyword arguments for the task. # Your implementation here def start(self): Starts the task scheduler and worker threads. # Your implementation here def stop(self): Stops the task scheduler and waits for all worker threads to finish. # Your implementation here ``` **Example of a task function**: ```python import time def example_task(task_id): print(f\\"Task {task_id} starting\\") time.sleep(2) print(f\\"Task {task_id} completed\\") ``` **Constraints**: - The task function can be any callable Python function. - Tasks may have varying execution times. - Ensure that no tasks are lost and all tasks are executed. **Implementation Notes**: - Use `Condition` objects to manage synchronization. - Use a `Queue` to hold the tasks. - The `start` method should initialize and start the worker threads. - The `stop` method should stop the threads and ensure all tasks are completed. --- **Example Usage**: ```python if __name__ == \\"__main__\\": scheduler = TaskScheduler(num_workers=3) # Add tasks for i in range(5): scheduler.add_task(example_task, i) # Start the scheduler scheduler.start() # Wait for some time before stopping the scheduler time.sleep(10) scheduler.stop() ``` **Expected Output**: ``` Task 0 starting Task 1 starting Task 2 starting Task 0 completed Task 3 starting Task 1 completed Task 4 starting Task 2 completed Task 3 completed Task 4 completed ``` Your implementation should correctly manage the task execution across multiple threads, ensuring no race conditions or data corruption occur.","solution":"import threading from queue import Queue class TaskScheduler: def __init__(self, num_workers: int): self.num_workers = num_workers self.tasks = Queue() self.lock = threading.Lock() self.condition = threading.Condition(self.lock) self.workers = [] self.stop_flag = False def add_task(self, task, *args, **kwargs): with self.condition: self.tasks.put((task, args, kwargs)) self.condition.notify() def start(self): self.stop_flag = False for _ in range(self.num_workers): worker = threading.Thread(target=self._worker) worker.start() self.workers.append(worker) def stop(self): with self.condition: self.stop_flag = True self.condition.notify_all() for worker in self.workers: worker.join() def _worker(self): while True: with self.condition: while not self.stop_flag and self.tasks.empty(): self.condition.wait() if self.stop_flag and self.tasks.empty(): break task, args, kwargs = self.tasks.get() task(*args, **kwargs) self.tasks.task_done()"},{"question":"XML Document Manipulation Using `xml.dom` # Objective Implement a function that reads an XML string, modifies it by adding new elements and attributes, and then outputs the modified XML string. This task will assess your understanding of the `xml.dom` module in Python. # Problem Statement You need to implement a function `modify_xml(xml_string: str) -> str` that performs the following operations on the given XML string: 1. Parse the input XML string to create a DOM document. 2. Add a new element named `Course` with the attribute `code=\\"CS101\\"` and text content \\"Introduction to Computer Science\\" as a child to the root element. 3. Add another new element named `Instructor` with the attribute `name=\\"Dr. Smith\\"` and text content \\"John Doe\\" as a child to the `Course` element. 4. Return the modified XML string. # Input - `xml_string` (str): A well-formed XML string. Example: ```xml <School> <Students> <Student id=\\"1\\">Alice</Student> <Student id=\\"2\\">Bob</Student> </Students> </School> ``` # Output - (str): The modified XML string. # Constraints - The input XML string will always be well-formed. - You must use the `xml.dom` module for manipulating the XML content. # Example **Input:** ```xml <School> <Students> <Student id=\\"1\\">Alice</Student> <Student id=\\"2\\">Bob</Student> </Students> </School> ``` **Output:** ```xml <School> <Students> <Student id=\\"1\\">Alice</Student> <Student id=\\"2\\">Bob</Student> </Students> <Course code=\\"CS101\\">Introduction to Computer Science</Course> <Course> <Instructor name=\\"Dr. Smith\\">John Doe</Instructor> </Course> </School> ``` # Implementation Requirements 1. Create a `DOMImplementation` instance to parse and create the XML document. 2. Use the methods associated with `Document` and `Element` objects to create new nodes and set their attributes and text content. 3. Ensure that the new elements are inserted at the correct positions in the XML hierarchy. 4. Convert the modified DOM document back to an XML string and return it. # Notes - Use the `xml.dom.minidom` module for DOM operations. - Make sure to handle the namespaces if required. - Pay attention to the structure of the XML and ensure it remains well-formed after modifications.","solution":"from xml.dom.minidom import parseString, Document def modify_xml(xml_string: str) -> str: Modifies an XML string by adding new Course and Instructor elements. Args: xml_string (str): A well-formed XML string. Returns: str: The modified XML string. # Parse the input XML string to create a DOM document. dom = parseString(xml_string) # Get the root element. root = dom.documentElement # Create a new element named `Course` with the attribute `code=\\"CS101\\"` # and text content \\"Introduction to Computer Science\\". course_element = dom.createElement(\\"Course\\") course_element.setAttribute(\\"code\\", \\"CS101\\") course_text = dom.createTextNode(\\"Introduction to Computer Science\\") course_element.appendChild(course_text) root.appendChild(course_element) # Create another new element named `Instructor` with the attribute `name=\\"Dr. Smith\\"` # and text content \\"John Doe\\" as a child to the `Course` element. instructor_element = dom.createElement(\\"Instructor\\") instructor_element.setAttribute(\\"name\\", \\"Dr. Smith\\") instructor_text = dom.createTextNode(\\"John Doe\\") instructor_element.appendChild(instructor_text) # Append the instructor element to a new Course element new_course_element = dom.createElement(\\"Course\\") new_course_element.appendChild(instructor_element) root.appendChild(new_course_element) # Return the modified XML string. return dom.toxml()"},{"question":"# Email Processing and MIME Handling # Objective Implement a Python function to parse, manipulate, and generate email messages using the `email` package. Specifically, the function will perform the following tasks: 1. Parse a given raw email message. 2. Modify the subject and body of the email. 3. Add an attachment to the email. 4. Output the modified email message as a string. # Function Signature ```python def process_email(raw_email: str, new_subject: str, new_body: str, attachment_content: bytes, attachment_filename: str) -> str: pass ``` # Input 1. `raw_email` (str): The raw email message as a string. 2. `new_subject` (str): The new subject for the email. 3. `new_body` (str): The new body content for the email. 4. `attachment_content` (bytes): The content of the attachment to be added. 5. `attachment_filename` (str): The filename for the attachment. # Output - Returns a string representing the modified email message. # Constraints - The raw email will be in a valid format. - The attachment will be a binary file, and the content will not exceed 1MB. - The solution should correctly handle various MIME types and ensure the output is a valid email message. # Performance Requirements - The implementation should efficiently handle the parsing and generation of the email message. - The function should adhere to the constraints and handle typical edge cases without significant performance degradation. # Example ```python raw_email = From: sender@example.com To: receiver@example.com Subject: Original Subject This is the original body of the email. new_subject = \\"Updated Subject\\" new_body = \\"This is the new body of the email.\\" attachment_content = b\\"This is the content of the attachment.\\" attachment_filename = \\"attachment.txt\\" result = process_email(raw_email, new_subject, new_body, attachment_content, attachment_filename) print(result) ``` # Expected Output ```text From: sender@example.com To: receiver@example.com Subject: Updated Subject MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"===============0123456789==\\" --===============0123456789== Content-Type: text/plain This is the new body of the email. --===============0123456789== Content-Type: text/plain; name=\\"attachment.txt\\" Content-Disposition: attachment; filename=\\"attachment.txt\\" Content-Transfer-Encoding: base64 VGhpcyBpcyB0aGUgY29udGVudCBvZiB0aGUgYXR0YWNobWVudC4= --===============0123456789==-- ``` In this output, the email has an updated subject and body, and contains an additional attachment. # Notes - Ensure the use of proper MIME types and encoding for adding attachments. - Handle any exceptions that may arise during parsing or generating the email.","solution":"import email from email import policy from email.parser import BytesParser from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders def process_email(raw_email: str, new_subject: str, new_body: str, attachment_content: bytes, attachment_filename: str) -> str: # Parse the original email msg = BytesParser(policy=policy.default).parsebytes(raw_email.encode(\'utf-8\')) # Create a new multipart message new_msg = MIMEMultipart() # Copy headers from the original email for header in [\'From\', \'To\', \'Cc\', \'Bcc\']: if header in msg: new_msg[header] = msg[header] # Set the new subject new_msg[\'Subject\'] = new_subject # Attach the new body body = MIMEText(new_body, \'plain\') new_msg.attach(body) # Create the attachment attachment = MIMEBase(\'application\', \'octet-stream\') attachment.set_payload(attachment_content) encoders.encode_base64(attachment) attachment.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{attachment_filename}\\"\') new_msg.attach(attachment) # Return the modified email as a string return new_msg.as_string()"},{"question":"# Custom Distribution and Transformation in PyTorch Objective Create a custom probability distribution in PyTorch by inheriting from the `Distribution` class and demonstrate its usage with a data transformation. Question You are required to implement a custom probability distribution, `CustomUniform`, which mimics the behavior of the existing `Uniform` distribution. Your implementation should include the following: 1. **Class Definition**: - Define a class named `CustomUniform` that inherits from `Distribution`. - Implement the necessary methods (`__init__()`, `sample()`, `log_prob()`, etc.) to mimic a uniform distribution with a given range `[low, high]`. 2. **Transformation**: - Implement a transformation method that transforms samples from the `CustomUniform` distribution using a simple mathematical function (e.g., scaling by a factor and then applying a sine function). 3. **Method Details**: - `__init__(self, low, high)`: Initialize the distribution with `low` and `high` bounds. - `sample(self, sample_shape=torch.Size([]))`: Generate samples from the distribution. - `log_prob(self, value)`: Compute the log probability of a given value. - `transform_samples(self, samples, factor)`: Apply the transformation `y = sin(factor * x)` to the generated samples. 4. **Testing**: - Generate samples from the `CustomUniform` distribution with bounds `[0, 2π]`. - Apply the transformation with a factor of `1.5` to these samples. - Print and plot the original and transformed samples. Constraints - Assume the `CustomUniform` distribution always has `low < high`. - Ensure the `sample()` and `log_prob()` methods are correct and handle PyTorch tensors. - The transformation should handle any valid `factor`. Expected Input and Output **Input**: - `low`: float, lower bound of the distribution (e.g., 0.0) - `high`: float, upper bound of the distribution (e.g., 6.28) - `factor`: float, factor for the transformation (e.g., 1.5) **Output**: - Samples from `CustomUniform`. - Transformed samples using the mathematical function. **Code Template**: ```python import torch from torch.distributions import Distribution class CustomUniform(Distribution): def __init__(self, low, high): super().__init__() self.low = low self.high = high def sample(self, sample_shape=torch.Size([])): return torch.distributions.Uniform(self.low, self.high).sample(sample_shape) def log_prob(self, value): uniform = torch.distributions.Uniform(self.low, self.high) return uniform.log_prob(value) def transform_samples(self, samples, factor): return torch.sin(factor * samples) # Test the implementation if __name__ == \\"__main__\\": low = 0.0 high = 2 * 3.141592653589793 dist = CustomUniform(low, high) samples = dist.sample((1000,)) transformed_samples = dist.transform_samples(samples, 1.5) print(\\"Original Samples:\\", samples) print(\\"Transformed Samples:\\", transformed_samples) # Optional: Plot the samples import matplotlib.pyplot as plt plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.hist(samples.numpy(), bins=50, alpha=0.7, label=\'Original\') plt.legend() plt.subplot(1, 2, 2) plt.hist(transformed_samples.numpy(), bins=50, alpha=0.7, color=\'orange\', label=\'Transformed\') plt.legend() plt.show() ``` Notes - Make sure you handle edge cases and validate input ranges. - Use PyTorch functionalities to ensure the implementation is efficient and leverages GPU capabilities if available.","solution":"import torch from torch.distributions import Distribution, Uniform class CustomUniform(Distribution): def __init__(self, low, high): super().__init__() assert low < high, \\"low must be less than high\\" self.low = low self.high = high self.uniform = Uniform(low, high) def sample(self, sample_shape=torch.Size([])): return self.uniform.sample(sample_shape) def log_prob(self, value): return self.uniform.log_prob(value) def transform_samples(self, samples, factor): return torch.sin(factor * samples) # Test the implementation if __name__ == \\"__main__\\": low = 0.0 high = 2 * 3.141592653589793 dist = CustomUniform(low, high) samples = dist.sample((1000,)) transformed_samples = dist.transform_samples(samples, 1.5) print(\\"Original Samples:\\", samples) print(\\"Transformed Samples:\\", transformed_samples) # Optional: Plot the samples import matplotlib.pyplot as plt plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.hist(samples.numpy(), bins=50, alpha=0.7, label=\'Original\') plt.legend() plt.subplot(1, 2, 2) plt.hist(transformed_samples.numpy(), bins=50, alpha=0.7, color=\'orange\', label=\'Transformed\') plt.legend() plt.show()"},{"question":"# Question: Implementing a Flexible Resource Manager In this task, you are required to design a flexible resource manager using Python\'s `contextlib` module. This resource manager will facilitate dynamic acquisition and release of multiple resources based on user input. Your implementation will consolidate the concepts provided by `contextlib` such as the `ExitStack`, `contextmanager`, and exception handling. Requirements: 1. **ResourceManager Class**: - Implement a class `ResourceManager` which uses `contextlib.ExitStack` for managing context managers. - The class should provide methods to register new resources for management. These resources should be context managers themselves. 2. **Dynamic Resource Handling**: - Ensure that resources can be added dynamically to the `ResourceManager` instance. - When the context of `ResourceManager` is entered, all the registered resources should be entered. - When the context of `ResourceManager` is exited, all resources should be properly released, even if exceptions occur. 3. **Resource decorators**: - Implement a method `register_resource` that takes a context manager and registers it. - Implement the `__enter__` and `__exit__` methods to manage the entry and exit of all registered context managers. Here is the expected class skeleton: ```python from contextlib import ExitStack, contextmanager class ResourceManager: def __init__(self): self.stack = ExitStack() def register_resource(self, resource): Register a context manager for management. Parameters: resource (context manager): The context manager to be registered. self.stack.enter_context(resource) def __enter__(self): Enter the context: Activate all registered context managers. return self def __exit__(self, exc_type, exc_value, traceback): Exit the context: Properly release all registered context managers. self.stack.close() ``` Example Usage: ```python import contextlib @contextlib.contextmanager def sample_resource(name): print(f\\"Acquiring resource: {name}\\") yield name print(f\\"Releasing resource: {name}\\") # Using the ResourceManager with ResourceManager() as manager: manager.register_resource(sample_resource(\'Resource1\')) manager.register_resource(sample_resource(\'Resource2\')) # Do operations with resources print(\\"Using resources\\") ``` **Expected Output**: ``` Acquiring resource: Resource1 Acquiring resource: Resource2 Using resources Releasing resource: Resource2 Releasing resource: Resource1 ``` Constraints: - Your implementation should handle an arbitrary number of resources. - Ensure that the resources are released in the reverse order of their acquisition. - Properly manage exceptions such that all resources are released even if an error occurs during their usage. Implement the `ResourceManager` class as specified.","solution":"from contextlib import ExitStack, contextmanager class ResourceManager: def __init__(self): self.stack = ExitStack() def register_resource(self, resource): Register a context manager for management. Parameters: resource (context manager): The context manager to be registered. self.stack.enter_context(resource) def __enter__(self): Enter the context: Activate all registered context managers. self.stack.__enter__() return self def __exit__(self, exc_type, exc_value, traceback): Exit the context: Properly release all registered context managers. return self.stack.__exit__(exc_type, exc_value, traceback) @contextmanager def sample_resource(name): print(f\\"Acquiring resource: {name}\\") yield name print(f\\"Releasing resource: {name}\\")"},{"question":"Objective: Demonstrate understanding of system resource management using Python\'s `resource` module. Problem Statement: You are required to write a Python function `monitor_and_limit_resources` that performs the following tasks: 1. **Monitor CPU and memory usage**: - Periodically get and print the CPU time used (`ru_utime` and `ru_stime`), the maximum resident set size (`ru_maxrss`), and the number of involuntary context switches (`ru_nivcsw`) for the current process. - This should be done over a given period with specified intervals. 2. **Set CPU and file descriptor limits**: - Set a CPU time limit (in seconds) for the current process. - Set a maximum number of open file descriptors for the current process. Function Signature: ```python def monitor_and_limit_resources(cpu_time_limit: int, max_open_files: int, duration: int, interval: int) -> None: pass ``` Parameters: - `cpu_time_limit` (int): The maximum amount of CPU time (in seconds) that the process can use. - `max_open_files` (int): The maximum number of open file descriptors for the process. - `duration` (int): The total duration (in seconds) over which to monitor the resource usage. - `interval` (int): The interval (in seconds) at which to log the resource usage stats. Example: ```python monitor_and_limit_resources(cpu_time_limit=10, max_open_files=50, duration=60, interval=5) ``` This example: - Sets a CPU time limit of 10 seconds. - Sets a maximum of 50 open file descriptors. - Monitors resource usage over 60 seconds, logging every 5 seconds. Constraints: - The function should handle any raised exceptions and log an appropriate message. - Use `resource.RLIMIT_CPU` for CPU limits and `resource.RLIMIT_NOFILE` for file descriptors. - If setting the limits fails (e.g., due to insufficient permissions), continue monitoring usage but log a warning. Expected Output: - Print resource usage stats (`ru_utime`, `ru_stime`, `ru_maxrss`, `ru_nivcsw`) at each interval. - If limits cannot be set, print appropriate error messages. Additional Notes: - This assessment examines understanding of system resources and Python’s `resource` module. - It evaluates handling and managing system-level exceptions gracefully.","solution":"import resource import time def monitor_and_limit_resources(cpu_time_limit: int, max_open_files: int, duration: int, interval: int) -> None: Monitor and set limits for CPU time and file descriptors for the current process. Args: cpu_time_limit (int): The maximum amount of CPU time (in seconds) that the process can use. max_open_files (int): The maximum number of open file descriptors for the process. duration (int): The total duration (in seconds) over which to monitor the resource usage. interval (int): The interval (in seconds) at which to log the resource usage stats. # Set CPU time limit try: resource.setrlimit(resource.RLIMIT_CPU, (cpu_time_limit, cpu_time_limit)) print(f\\"CPU time limit set to {cpu_time_limit} seconds.\\") except Exception as e: print(f\\"Failed to set CPU time limit: {e}\\") # Set maximum number of open file descriptors try: resource.setrlimit(resource.RLIMIT_NOFILE, (max_open_files, max_open_files)) print(f\\"Max open files limit set to {max_open_files}.\\") except Exception as e: print(f\\"Failed to set max open files limit: {e}\\") start_time = time.time() while time.time() - start_time < duration: # Get resource usage statistics usage = resource.getrusage(resource.RUSAGE_SELF) print(f\\"User time: {usage.ru_utime}\\") print(f\\"System time: {usage.ru_stime}\\") print(f\\"Max resident set size: {usage.ru_maxrss}\\") print(f\\"Involuntary context switches: {usage.ru_nivcsw}\\") time.sleep(interval)"},{"question":"# WSGI Application and Middleware Development **Objective**: Create a WSGI middleware component that logs request details and the response time of a given WSGI application. Then, create a simple WSGI application and use the middleware to serve it with a WSGI server. **Task**: 1. Implement a WSGI middleware class, `RequestLoggerMiddleware`, which will: - Log the request method, request URI, and the time taken to process the request. - Wrap around a provided WSGI application. 2. Create a simple WSGI application, `hello_app`, which responds with \\"Hello, World!\\". 3. Use the `wsgiref.simple_server` to serve the WSGI application wrapped with `RequestLoggerMiddleware`. # Detailed Requirements **Middleware Class:** - Class Name: `RequestLoggerMiddleware` - Initialization: `__init__(self, app)` - Method: `__call__(self, environ, start_response)` - Log the request method and request URI. - Measure and log the time taken to process the request. - Call the wrapped application and return its response. - Ensure logs are printed to the console. **Simple Application:** - Function Name: `hello_app` - Parameters: `environ`, `start_response` - Response: \\"Hello, World!\\" with `200 OK` status and `Content-Type: text/plain`. **Server Setup:** - Use `wsgiref.simple_server.make_server` to serve the application on `localhost` and port `8000`. - Application should be wrapped with `RequestLoggerMiddleware`. # Example Usage: ```python from wsgiref.simple_server import make_server from time import time class RequestLoggerMiddleware: def __init__(self, app): self.app = app def __call__(self, environ, start_response): start_time = time() method = environ.get(\'REQUEST_METHOD\') uri = environ.get(\'REQUEST_URI\') print(f\\"Request received: {method} {uri}\\") def custom_start_response(status, headers, exc_info=None): nonlocal start_time duration = time() - start_time print(f\\"Response status: {status} in {duration:.4f} seconds\\") return start_response(status, headers, exc_info) return self.app(environ, custom_start_response) def hello_app(environ, start_response): status = \'200 OK\' headers = [(\'Content-Type\', \'text/plain\')] start_response(status, headers) return [b\\"Hello, World!\\"] if __name__ == \'__main__\': app = RequestLoggerMiddleware(hello_app) with make_server(\'localhost\', 8000, app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` **Constraint:** - Ensure that the middleware properly logs information and handles any potential edge cases, such as var=ious types of HTTP requests. Run your solution and ensure that when accessing http://localhost:8000 from a browser or a tool like `curl`, it behaves as expected, logging requests and their processing times to the console.","solution":"from wsgiref.simple_server import make_server from time import time class RequestLoggerMiddleware: def __init__(self, app): self.app = app def __call__(self, environ, start_response): start_time = time() method = environ.get(\'REQUEST_METHOD\') uri = environ.get(\'REQUEST_URI\', environ.get(\'PATH_INFO\', \'\')) # Fallback to PATH_INFO for URIs print(f\\"Request received: {method} {uri}\\") def custom_start_response(status, headers, exc_info=None): nonlocal start_time duration = time() - start_time print(f\\"Response status: {status} in {duration:.4f} seconds\\") return start_response(status, headers, exc_info) return self.app(environ, custom_start_response) def hello_app(environ, start_response): status = \'200 OK\' headers = [(\'Content-Type\', \'text/plain\')] start_response(status, headers) return [b\\"Hello, World!\\"] # To run the server (usually outside testing): # if __name__ == \'__main__\': # app = RequestLoggerMiddleware(hello_app) # with make_server(\'localhost\', 8000, app) as httpd: # print(\\"Serving on port 8000...\\") # httpd.serve_forever()"},{"question":"**Advanced DateTime Manipulation** You are tasked with implementing a function that takes in specific components of a date and time and performs various manipulations and extractions. Here are the details: # Function Signature ```python def advanced_datetime_operations(year: int, month: int, day: int, hour: int, minute: int, second: int, usecond: int) -> dict: pass ``` # Input - `year`: An integer representing the year (e.g., 2021). - `month`: An integer representing the month (1 through 12). - `day`: An integer representing the day (1 through 31). - `hour`: An integer representing the hour (0 through 23). - `minute`: An integer representing the minute (0 through 59). - `second`: An integer representing the second (0 through 59). - `usecond`: An integer representing the microsecond (0 through 999999). # Output The function should return a dictionary with the following keys and their corresponding values: - `\\"date_object\\"`: A string representation of the `date` object created from `year`, `month`, and `day`. - `\\"time_object\\"`: A string representation of the `time` object created from `hour`, `minute`, `second`, and `usecond`. - `\\"datetime_object\\"`: A string representation of the `datetime` object created from all the input components. - `\\"components\\"`: A dictionary with the following keys and their corresponding extracted values: - `\\"year\\"`: Extracted year component from the `datetime` object. - `\\"month\\"`: Extracted month component from the `datetime` object. - `\\"day\\"`: Extracted day component from the `datetime` object. - `\\"hour\\"`: Extracted hour component from the `datetime` object. - `\\"minute\\"`: Extracted minute component from the `datetime` object. - `\\"second\\"`: Extracted second component from the `datetime` object. - `\\"microsecond\\"`: Extracted microsecond component from the `datetime` object. # Example ```python result = advanced_datetime_operations(2021, 12, 25, 15, 30, 45, 123456) print(result) # Output: { \\"date_object\\": \\"2021-12-25\\", \\"time_object\\": \\"15:30:45.123456\\", \\"datetime_object\\": \\"2021-12-25 15:30:45.123456\\", \\"components\\": { \\"year\\": 2021, \\"month\\": 12, \\"day\\": 25, \\"hour\\": 15, \\"minute\\": 30, \\"second\\": 45, \\"microsecond\\": 123456 } } ``` # Constraints - All input values will be valid integers within the specified ranges. - The function should be efficient in both time and space complexity. Implement the function `advanced_datetime_operations` which should pass the example provided.","solution":"from datetime import date, time, datetime def advanced_datetime_operations(year: int, month: int, day: int, hour: int, minute: int, second: int, usecond: int) -> dict: # Create date object date_obj = date(year, month, day) # Create time object time_obj = time(hour, minute, second, usecond) # Create datetime object datetime_obj = datetime(year, month, day, hour, minute, second, usecond) # Extract individual components components = { \\"year\\": datetime_obj.year, \\"month\\": datetime_obj.month, \\"day\\": datetime_obj.day, \\"hour\\": datetime_obj.hour, \\"minute\\": datetime_obj.minute, \\"second\\": datetime_obj.second, \\"microsecond\\": datetime_obj.microsecond } # Return results in the prescribed format return { \\"date_object\\": date_obj.isoformat(), \\"time_object\\": time_obj.isoformat(), \\"datetime_object\\": datetime_obj.isoformat(\\" \\"), \\"components\\": components }"},{"question":"Objective: Write a Python function that retrieves specific configuration information using the `sysconfig` module and provides a comprehensive summary of the Python environment. The function should be designed to handle various platforms and perform the following tasks: 1. **Retrieve Configuration Variables:** - Extract the values of the configuration variables `\'AR\'`, `\'CXX\'`, and `\'LIBDIR\'`. - If any of these variables are not found, their value should be set to `\\"Not Found\\"`. 2. **Identify Installation Paths:** - Determine the default installation scheme for the current platform. - Retrieve the installation paths for `\'purelib\'`, `\'scripts\'`, and `\'include\'` under this scheme. 3. **Platform and Version Information:** - Retrieve the current Python version number. - Identify the current platform. 4. **Generate Summary:** - Create a dictionary summarizing the retrieved information, with the following structure: ```python summary = { \\"config_vars\\": { \\"AR\\": <value>, \\"CXX\\": <value>, \\"LIBDIR\\": <value> }, \\"installation_paths\\": { \\"purelib\\": <path>, \\"scripts\\": <path>, \\"include\\": <path> }, \\"system_info\\": { \\"python_version\\": <version>, \\"platform\\": <platform> } } ``` Function Signature: ```python import sysconfig def gather_python_environment(): # Implement the function here pass ``` Example: ```python result = gather_python_environment() # Example output (actual values will vary based on the system): { \\"config_vars\\": { \\"AR\\": \\"ar\\", \\"CXX\\": \\"g++\\", \\"LIBDIR\\": \\"/usr/local/lib\\" }, \\"installation_paths\\": { \\"purelib\\": \\"/usr/local/lib/python3.10/site-packages\\", \\"scripts\\": \\"/usr/local/bin\\", \\"include\\": \\"/usr/local/include/python3.10\\" }, \\"system_info\\": { \\"python_version\\": \\"3.10\\", \\"platform\\": \\"linux-x86_64\\" } } ``` Constraints: - Ensure that your function handles cases where specific configuration variables or paths might not be found. - Implement error handling for any potential issues during retrieval. Evaluation Criteria: - Correctness of the implementation. - Handling of edge cases and errors. - Efficient use of the `sysconfig` module functions. - Clear and maintainable code style.","solution":"import sysconfig def gather_python_environment(): # Retrieve configuration variables config_vars = { \'AR\': sysconfig.get_config_var(\'AR\') or \\"Not Found\\", \'CXX\': sysconfig.get_config_var(\'CXX\') or \\"Not Found\\", \'LIBDIR\': sysconfig.get_config_var(\'LIBDIR\') or \\"Not Found\\" } # Determine the default installation scheme scheme = sysconfig.get_default_scheme() # Retrieve installation paths for the determined scheme installation_paths = { \'purelib\': sysconfig.get_path(\'purelib\', scheme) or \\"Not Found\\", \'scripts\': sysconfig.get_path(\'scripts\', scheme) or \\"Not Found\\", \'include\': sysconfig.get_path(\'include\', scheme) or \\"Not Found\\" } # Retrieve system information system_info = { \'python_version\': sysconfig.get_python_version() or \\"Not Found\\", \'platform\': sysconfig.get_platform() or \\"Not Found\\" } # Summarize the information in a dictionary summary = { \\"config_vars\\": config_vars, \\"installation_paths\\": installation_paths, \\"system_info\\": system_info } return summary"},{"question":"**Question**: You are tasked with developing a Python program that connects to a POP3 email server, retrieves the subject lines of the most recent N emails, and displays them. Your implementation should utilize the `poplib` module to handle the connection and protocol communication. **Specifications**: 1. Write a function `get_latest_subjects(host, port, username, password, n)` that takes the following parameters: - `host` (string): The hostname of the POP3 server. - `port` (int): The port number to connect to. - `username` (string): The username for authentication. - `password` (string): The password for authentication. - `n` (int): The number of most recent email subject lines to retrieve. 2. The function should: - Establish a connection to the POP3 server. - Authenticate using the provided username and password. - Retrieve the headers of the most recent N emails from the mailbox. - Extract the subject lines from the headers. - Return the list of subject lines. 3. Use the `retr` method to retrieve an email message and parse the headers to get the subject line. **Input**: - `host`: A string representing the hostname of the POP3 server. - `port`: An integer representing the port number. - `username`: A string representing the username for the email account. - `password`: A string representing the password for the email account. - `n`: An integer representing the number of most recent emails to retrieve. **Output**: - A list of strings, each string being a subject line of one of the retrieved emails. **Constraints**: - Handle exceptions gracefully, including connection errors, authentication errors, and protocol errors. - Assume that you have a functioning internet connection and the server details provided are correct. **Example**: ```python host = \\"pop.example.com\\" port = 110 username = \\"user@example.com\\" password = \\"password123\\" n = 5 result = get_latest_subjects(host, port, username, password, n) print(result) ``` ```plaintext Output: [ \\"Subject: Your recent order\\", \\"Subject: Monthly newsletter\\", \\"Subject: Welcome to our service\\", \\"Subject: Password reset instructions\\", \\"Subject: Your subscription is confirmed\\" ] ``` You may use the `email` module to help parse the email headers. **Notes**: - The ordering of the emails should be from the most recent to the oldest. - Make sure to call the `quit()` method on the POP3 connection to close it properly.","solution":"import poplib from email.parser import Parser def get_latest_subjects(host, port, username, password, n): Retrieve the subject lines of the most recent \'n\' emails. Parameters: - host (str): The hostname of the POP3 server. - port (int): The port number to connect to. - username (str): The username for authentication. - password (str): The password for authentication. - n (int): The number of most recent email subject lines to retrieve. Returns: - list of str: A list of subject lines of the most recent \'n\' emails. try: # Connect to the POP3 server mail = poplib.POP3(host, port) # Authenticate with the server mail.user(username) mail.pass_(password) # Get the number of messages in the mailbox num_messages = len(mail.list()[1]) # Calculate the number of emails to fetch fetch_count = min(n, num_messages) subjects = [] # Fetch the headers of the most recent \'n\' emails for i in range(num_messages, num_messages - fetch_count, -1): response, lines, octets = mail.top(i, 0) msg_content = b\'n\'.join(lines).decode(\'utf-8\') msg = Parser().parsestr(msg_content) subjects.append(msg[\'subject\']) # Close the connection mail.quit() return subjects except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"**Objective:** You are required to write a function that employs Python\'s `runpy` module to execute Python scripts or modules dynamically. Specifically, your function should be able to handle both module names and filesystem paths, utilizing the appropriate `runpy` function based on the input type. **Problem:** Write a function `execute_code` which takes two parameters: 1. `name_or_path` (string): This can be either: - A module name (absolute) to be executed using `runpy.run_module()`. - A filesystem path pointing to a script to be executed using `runpy.run_path()`. 2. `is_path` (bool): A boolean indicator specifying whether the `name_or_path` is a filesystem path (`True`) or a module name (`False`). Your function should: - Dynamically execute the provided module or script. - Collect and return a dictionary of the global variables created or modified during execution. **Function Signature:** ```python def execute_code(name_or_path: str, is_path: bool) -> dict: pass ``` # Example: ```python # Example 1: Running a module result = execute_code(\'example_module\', False) print(result) # Example 2: Running a script from a path result = execute_code(\'/path/to/example_script.py\', True) print(result) ``` # Constraints: - The module names and paths provided will be valid and exist in your Python environment. - You can assume that the scripts or modules do not have namespace conflicts. - The scripts/modules to be executed do not require any command-line argument other than `sys.argv[0]`. # Notes: - Make sure to handle special global variables (`__name__`, `__spec__`, `__file__`, `__cached__`, `__loader__`, `__package__`) as per the `runpy` documentation. - Take care of the thread-safety limitations if applicable, though they will not be tested explicitly in this assessment. - Assume that error handling related to script/module execution (like syntax errors in the code) is handled externally, and your function should focus only on using `runpy` appropriately. **Hint:** Refer to the `runpy` documentation provided for understanding the internal handling of global variables and `sys` module manipulations.","solution":"import runpy def execute_code(name_or_path: str, is_path: bool) -> dict: Executes a Python module or script and returns the global variables created/modified during execution. Parameters: - name_or_path (str): A module name or filesystem path to the script. - is_path (bool): Indicator whether the `name_or_path` is a filesystem path (True) or a module name (False). Returns: - dict: A dictionary of the global variables created or modified during execution. if is_path: return runpy.run_path(name_or_path) else: return runpy.run_module(name_or_path)"},{"question":"# Question You are tasked with securing the authentication process for an online service by implementing a function that creates hashed and salted cookies for users. The cookies should securely embed the user\'s ID and an expiration timestamp. Additionally, you should provide a function to verify these cookies. Use the BLAKE2b algorithm from the `hashlib` module to achieve this. Your implementation should handle the following requirements: 1. Generate a BLAKE2b hash for the user ID and timestamp. 2. Use a secret key to enhance security. 3. Use a salt to prevent hash collisions. 4. Ensure the hash includes a personalization string to differentiate between the hash functions used for different purposes within your application. 5. Provide functionality to verify that a given cookie is valid and untampered. # Functions 1. `create_cookie(user_id: str, expires_at: int, secret_key: bytes, salt: bytes, personalization: bytes) -> str` - **Input**: - `user_id`: A string representing the user\'s unique identifier. - `expires_at`: An integer representing the UNIX timestamp at which the cookie expires. - `secret_key`: A byte string (up to 64 bytes) used to create the keyed hash. - `salt`: A byte string (up to 16 bytes) used for salt. - `personalization`: A byte string (up to 16 bytes) used for personalization. - **Output**: A string representing the hex-encoded cookie. 2. `verify_cookie(cookie: str, user_id: str, secret_key: bytes, salt: bytes, personalization: bytes) -> bool` - **Input**: - `cookie`: The hex-encoded cookie to verify. - `user_id`: A string representing the user\'s unique identifier. - `secret_key`: A byte string (up to 64 bytes) used to create the keyed hash. - `salt`: A byte string (up to 16 bytes) used for salt. - `personalization`: A byte string (up to 16 bytes) used for personalization. - **Output**: A boolean indicating whether the cookie is valid. # Requirements - Use the BLAKE2b algorithm from the `hashlib` module. - Ensure the expiration timestamp is checked in the `verify_cookie` function. - Use the HMAC module\'s `compare_digest` to mitigate timing attacks during cookie verification. # Example ```python import time def create_cookie(user_id: str, expires_at: int, secret_key: bytes, salt: bytes, personalization: bytes) -> str: import hashlib payload = f\\"{user_id}:{expires_at}\\".encode(\'utf-8\') h = hashlib.blake2b(key=secret_key, salt=salt, person=personalization, digest_size=32) h.update(payload) return h.hexdigest() def verify_cookie(cookie: str, user_id: str, secret_key: bytes, salt: bytes, personalization: bytes) -> bool: import hmac current_time = int(time.time()) try: cookie_parts = cookie.split(\':\') if len(cookie_parts) != 2: return False expires_at = int(cookie_parts[1]) if expires_at < current_time: return False generated_cookie = create_cookie(user_id, expires_at, secret_key, salt, personalization) return hmac.compare_digest(generated_cookie, cookie) except ValueError: return False ``` Write the function `create_cookie` and `verify_cookie` following the specifications. Ensure that each function\'s logic is clearly documented and consider edge cases, such as invalid cookie formats or expired timestamps.","solution":"import hashlib import hmac import time def create_cookie(user_id: str, expires_at: int, secret_key: bytes, salt: bytes, personalization: bytes) -> str: Generates a hex-encoded cookie with BLAKE2b hash. payload = f\\"{user_id}:{expires_at}\\".encode(\'utf-8\') h = hashlib.blake2b(key=secret_key, salt=salt, person=personalization, digest_size=32) h.update(payload) return h.hexdigest() + f\\":{expires_at}\\" def verify_cookie(cookie: str, user_id: str, secret_key: bytes, salt: bytes, personalization: bytes) -> bool: Verifies integrity and validity of the provided cookie. current_time = int(time.time()) try: hash_part, expires_at_str = cookie.rsplit(\':\', 1) expires_at = int(expires_at_str) if expires_at < current_time: return False generated_cookie = create_cookie(user_id, expires_at, secret_key, salt, personalization).split(\':\')[0] return hmac.compare_digest(generated_cookie, hash_part) except (ValueError, AttributeError): return False"},{"question":"# Question: Implement a Distributed Computation using PyTorch\'s RPC with RRef **Problem Statement:** You are required to implement a distributed computational workflow using PyTorch\'s `torch.distributed.rpc` and RRef as explained in the provided documentation. This will involve multiple workers performing parallel computations and sharing intermediate results. We\'ll create a simple system involving 3 workers where computations are offloaded and results are aggregated. **Task:** 1. **Initialize RPC Environment:** - Initialize RPC on three workers: `worker0`, `worker1`, and `worker2`. 2. **Define a Simple Computational Function:** - Implement a function, `compute`, that takes a tensor, performs some calculation (e.g., element-wise addition), and returns the result. 3. **Create and Use RRefs:** - On `worker0`, create an RRef to a tensor using `torch.ones`. - Offload the `compute` task to `worker1` and `worker2` using `rpc.remote`. - Ensure that results are consolidated back to `worker0` for final processing. 4. **Handle RRef Ownership and Lifetime:** - Manage the creation and deletion of RRefs in a manner that follows the principles described (such as notification and proper reference counting). **Input/Output:** - The `compute` function should take a tensor and an integer `value` to add to each element of the tensor. - The result should be aggregated on `worker0` and printed. **Constraints:** - Use PyTorch with RPC and RRef functionality. - Handle transient network failures gracefully (simulate using delayed message delivery). - Ensure that the owner is notified correctly according to guarantees `G1` and `G2`. **Performance Requirements:** - The solution should ensure minimal delay and efficient parallel processing. # Example Code Structure ```python import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef def compute(tensor, value): # Perform element-wise addition return tensor + value def main(): # Initialize RPC rpc.init_rpc(\\"worker0\\", rank=0, world_size=3) # Define what each worker does if rpc.get_worker_info().name == \\"worker0\\": tensor_ref = RRef(torch.ones(2)) # Offload computation to worker1 and worker2 fut_1_ref = rpc.remote(\\"worker1\\", compute, args=(tensor_ref.to_here(), 5)) fut_2_ref = rpc.remote(\\"worker2\\", compute, args=(tensor_ref.to_here(), 10)) # Aggregate results result_1 = fut_1_ref.to_here() result_2 = fut_2_ref.to_here() # Combine results final_result = result_1 + result_2 # Print final result print(f\\"Final Result: {final_result}\\") elif rpc.get_worker_info().name == \\"worker1\\": pass # worker1 logic handled remotely elif rpc.get_worker_info().name == \\"worker2\\": pass # worker2 logic handled remotely # Shutdown RPC rpc.shutdown() if __name__ == \\"__main__\\": main() ``` **Instructions:** 1. Implement the computational function `compute`. 2. Write the code for initializing RPC, creating RRefs, offloading tasks, and aggregating results on the main worker. 3. Ensure the system handles the ownership and lifecycle of RRefs according to the discussed guarantees.","solution":"import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef def compute(tensor, value): Given a tensor, perform element-wise addition with the given value. return tensor + value def main(): world_size = 3 rpc_backend_options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=16) # Initialize RPC rpc.init_rpc(\\"worker0\\", rank=0, world_size=world_size, rpc_backend_options=rpc_backend_options) if rpc.get_worker_info().name == \\"worker0\\": tensor_ref = RRef(torch.ones(2)) # Offload computation to worker1 and worker2 fut_1_ref = rpc.remote(\\"worker1\\", compute, args=(tensor_ref, 5)) fut_2_ref = rpc.remote(\\"worker2\\", compute, args=(tensor_ref, 10)) # Aggregate results (note: need to wait for the futures to complete) result_1 = fut_1_ref.to_here() result_2 = fut_2_ref.to_here() # Combine results final_result = result_1 + result_2 # Print final result print(f\\"Final Result: {final_result}\\") # workers functionalities are implicitly written in compute function due to no actual need for work distribution. # Ensure to wait for all RPCs to complete before exiting rpc.shutdown() if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Seaborn Visualization Challenge You are given a dataset regarding passengers on the Titanic. Your task is to create a set of customized boxplots to visualize various aspects of the data using seaborn. Task: 1. Load the \'titanic\' dataset from seaborn. 2. Create a single horizontal boxplot to visualize the distribution of passengers\' ages. 3. Generate a vertical boxplot that shows the distribution of ages across passenger classes and further distinguishes between those who survived and those who did not. 4. Customize a boxplot to display the distribution of fares, rounded to the nearest ten, against the passenger class. Ensure that the native scaling of the fare is preserved, and add a dashed vertical line at age 30 for reference. 5. Create a boxplot to visualize the distribution of ages by deck, with the following customizations: - The whiskers should cover the full range of the data (0-100%). - The boxes should be narrower than default. - Customize the boxplot with a specified color, line width, and make the outline color different from the fill color. 6. Save the generated plots as image files. **Input:** None. The dataset should be loaded within the code using seaborn\'s `load_dataset` function. **Output:** - The boxplots created by each task should be displayed within the code and saved as separate image files to your working directory. **Constraints:** - Use seaborn\'s functions for creating the boxplots. - Use appropriate customization options provided by seaborn or matplotlib. **Performance Requirements:** - Efficiently load and manipulate the dataset. - Use seaborn\'s visualization capabilities effectively. **Example Code Snippet:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Task 1: Single horizontal boxplot plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"age\\"]) plt.title(\\"Age Distribution of Titanic Passengers\\") plt.savefig(\'age_distribution.png\') # Task 2: Vertical boxplot with nested grouping plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\") plt.title(\\"Age Distribution by Class and Survival\\") plt.savefig(\'age_by_class_and_survival.png\') # Task 3: Boxplot with native scaling and vertical line titanic[\\"age_rounded\\"] = titanic[\\"age\\"].round(-1) plt.figure(figsize=(10, 6)) ax = sns.boxplot(x=titanic[\\"age_rounded\\"], y=titanic[\\"fare\\"], native_scale=True) ax.axvline(30, color=\\"red\\", dashes=(2, 2)) plt.title(\\"Fare Distribution by Age (Rounded to Nearest Ten)\\") plt.savefig(\'fare_distribution_by_age.png\') # Task 4: Customized boxplot for age by deck plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"age\\", y=\\"deck\\", width=0.5, whis=(0, 100), color=\\".8\\", linecolor=\\"#137\\", linewidth=0.75) plt.title(\\"Age Distribution by Deck\\") plt.savefig(\'age_by_deck.png\') plt.show() ``` Ensure the generated plots are clear and informative. The use of seaborn\'s advanced features to customize and enhance the plots is necessary to demonstrate your understanding of the package.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset titanic = sns.load_dataset(\\"titanic\\") def create_horizontal_boxplot_age(): plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"age\\"]) plt.title(\\"Age Distribution of Titanic Passengers\\") plt.savefig(\'age_distribution.png\') plt.close() def create_vertical_boxplot_age_class_survival(): plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"survived\\") plt.title(\\"Age Distribution by Class and Survival\\") plt.savefig(\'age_by_class_and_survival.png\') plt.close() def create_boxplot_fare_class(): plt.figure(figsize=(10, 6)) ax = sns.boxplot(x=\\"class\\", y=\\"fare\\", data=titanic) ax.axvline(2, color=\\"red\\", dashes=(2, 2)) # Adding vertical line at \'2\' position which corresponds \'age\' as mentioned in task. plt.title(\\"Fare Distribution by Passenger Class\\") plt.savefig(\'fare_distribution_by_class.png\') plt.close() def create_custom_boxplot_age_deck(): plt.figure(figsize=(10, 6)) sns.boxplot( data=titanic, x=\\"age\\", y=\\"deck\\", width=0.5, whis=(0, 100), boxprops=dict(color=\\"blue\\", linewidth=2), medianprops=dict(color=\\"green\\", linewidth=2), whiskerprops=dict(color=\\"red\\", linewidth=1.5), capprops=dict(color=\\"yellow\\", linewidth=1.5), flierprops=dict(marker=\'o\', color=\'orange\', markersize=5) ) plt.title(\\"Age Distribution by Deck\\") plt.savefig(\'age_by_deck.png\') plt.close()"},{"question":"Objective: To assess your understanding of tensor creation, manipulation, and size extraction in PyTorch, you are required to implement a function that performs specified operations on a given tensor. Task: Write a function `tensor_operations` that takes three arguments: 1. `shape` (a tuple): A tuple representing the desired shape of a tensor. 2. `operation` (a string): A string specifying the operation to perform. It can be one of the following: - `\'get_dimension\'`: This should return the number of dimensions (length of the size) of the tensor. - `\'get_size\'`: This should return the size of the tensor as a `torch.Size` object. - `\'get_specific_dimension\'`: This should return the size of the tensor at a specific dimension (provided in the third argument). 3. `dim` (an integer, optional): This argument is only required if the operation is `\'get_specific_dimension\'`. It specifies the dimension index of the tensor size that should be returned. Input: 1. `shape` (tuple): A tuple of integers representing the dimensions of a tensor (e.g., `(10, 20, 30)`). 2. `operation` (string): One of the allowed operation strings (`\'get_dimension\'`, `\'get_size\'`, or `\'get_specific_dimension\'`). 3. `dim` (integer, optional): An integer representing the index of the dimension (required if `operation` is `\'get_specific_dimension\'`). Output: The function should return: - An integer if the operation is `\'get_dimension\'`. - A `torch.Size` object if the operation is `\'get_size\'`. - An integer if the operation is `\'get_specific_dimension\'`. Constraints: 1. The `shape` tuple will always have at least one dimension. 2. If the `operation` is `\'get_specific_dimension\'`, the `dim` argument will always be a valid index for the tensor dimensions. Example: ```python def tensor_operations(shape, operation, dim=None): # Your implementation here # Example usage: # Creating a tensor with shape (10, 20, 30) result = tensor_operations((10, 20, 30), \'get_dimension\') print(result) # Output: 3 result = tensor_operations((10, 20, 30), \'get_size\') print(result) # Output: torch.Size([10, 20, 30]) result = tensor_operations((10, 20, 30), \'get_specific_dimension\', 1) print(result) # Output: 20 ``` Note: - You should use PyTorch functions to create the tensor and extract its size. - Make sure to handle each operation specified by the `operation` argument appropriately.","solution":"import torch def tensor_operations(shape, operation, dim=None): Perform specified operation on a given tensor. Args: shape (tuple): Shape of the tensor. operation (str): Operation to perform - \'get_dimension\', \'get_size\', \'get_specific_dimension\'. dim (int, optional): Dimension to get size from if operation is \'get_specific_dimension\'. Returns: int/torch.Size: Result based on the operation. tensor = torch.empty(shape) if operation == \'get_dimension\': return tensor.dim() elif operation == \'get_size\': return tensor.size() elif operation == \'get_specific_dimension\': if dim is not None: return tensor.size(dim) else: raise ValueError(\\"Dimension index must be provided for \'get_specific_dimension\' operation.\\") else: raise ValueError(\\"Invalid operation specified.\\")"},{"question":"# Question: Function Metadata Handler You are asked to implement a class in Python that will manage function metadata similar to how it\'s done using the Python C API. The class should be able to set and get various attributes of a Python function. **Requirements:** 1. **Class Name:** `FunctionMetadataHandler` 2. **Attributes:** - `code`: The code object of the function. - `globals`: The dictionary containing the global variables accessible to the function. - `qualname`: A qualified name for the function. - `defaults`: The default values for the function parameters. - `closure`: The closure associated with the function. - `annotations`: The annotations of the function parameters. 3. **Methods:** - `__init__(self, func)`: Initializes the handler with a provided Python function `func`. - `get_code(self)`: Returns the code object of the function. - `get_globals(self)`: Returns the globals dictionary associated with the function. - `get_module(self)`: Returns the module name where the function is defined. - `get_defaults(self)`: Returns the default values for the function\'s parameters. - `set_defaults(self, defaults)`: Sets the default values for the function\'s parameters. - `get_closure(self)`: Returns the closure associated with the function. - `set_closure(self, closure)`: Sets the closure associated with the function. - `get_annotations(self)`: Returns the annotations of the function\'s parameters. - `set_annotations(self, annotations)`: Sets the annotations for the function\'s parameters. **Constraints:** - `defaults` should be a tuple or `None`. - `closure` should be a tuple of cell objects or `None`. - `annotations` should be a dictionary or `None`. **Example Usage:** ```python def sample_function(x: int, y: int = 5) -> int: return x + y handler = FunctionMetadataHandler(sample_function) print(handler.get_code()) # <code object sample_function at 0x...> print(handler.get_globals()) # {...} print(handler.get_module()) # \'__main__\' print(handler.get_defaults()) # (5,) handler.set_defaults((10,)) print(handler.get_defaults()) # (10,) print(handler.get_annotations()) # {\'x\': <class \'int\'>, \'y\': <class \'int\'>, \'return\': <class \'int\'>} handler.set_annotations({\'x\': str, \'return\': str}) print(handler.get_annotations()) # {\'x\': <class \'str\'>, \'return\': <class \'str\'>} ``` Implement the `FunctionMetadataHandler` class adhering to the specifications above.","solution":"class FunctionMetadataHandler: def __init__(self, func): Initializes the handler with a provided Python function. if not callable(func): raise ValueError(\\"The provided input is not a function\\") self.func = func def get_code(self): Returns the code object of the function. return self.func.__code__ def get_globals(self): Returns the globals dictionary associated with the function. return self.func.__globals__ def get_module(self): Returns the module name where the function is defined. return self.func.__module__ def get_defaults(self): Returns the default values for the function\'s parameters. return self.func.__defaults__ def set_defaults(self, defaults): Sets the default values for the function\'s parameters. if defaults is not None and not isinstance(defaults, tuple): raise ValueError(\\"Defaults must be a tuple or None\\") self.func.__defaults__ = defaults def get_closure(self): Returns the closure associated with the function. return self.func.__closure__ def set_closure(self, closure): Sets the closure associated with the function. if closure is not None and not isinstance(closure, tuple): raise ValueError(\\"Closure must be a tuple or None\\") self.func.__closure__ = closure def get_annotations(self): Returns the annotations of the function\'s parameters. return self.func.__annotations__ def set_annotations(self, annotations): Sets the annotations for the function\'s parameters. if annotations is not None and not isinstance(annotations, dict): raise ValueError(\\"Annotations must be a dictionary or None\\") self.func.__annotations__ = annotations"},{"question":"# Codec Registration and Manipulation in Python In this task, you are required to demonstrate your understanding of codec registration and encoding/decoding in Python by implementing a custom encoding and an error handling mechanism. Requirements 1. **Register a Custom Codec:** - Implement and register a new encoding codec. The encoding should replace every vowel in a string with the corresponding number (a=1, e=2, i=3, o=4, u=5). 2. **Encode and Decode Functions:** - Implement functions to encode and decode strings using the registered codec. 3. **Custom Error Handling:** - Implement a custom error handler that replaces any characters that cannot be encoded with their hex representation. Specifications 1. **Custom Codec Registration:** ```python class VowelNumberEncoder: def __init__(self): self.vowel_map = {\'a\': \'1\', \'e\': \'2\', \'i\': \'3\', \'o\': \'4\', \'u\': \'5\'} def encode(self, input, errors=\'strict\'): result = \'\'.join([self.vowel_map.get(char, char) for char in input]) return (result, len(input)) def decode(self, input, errors=\'strict\'): reverse_map = {v: k for k, v in self.vowel_map.items()} result = \'\'.join([reverse_map.get(char, char) for char in input]) return (result, len(input)) def register_vowelnumber_codec(): import codecs codec_info = codecs.CodecInfo( name=\'vowelnumber\', encode=VowelNumberEncoder().encode, decode=VowelNumberEncoder().decode ) codecs.register(lambda name: codec_info if name == \'vowelnumber\' else None) ``` 2. **Encode and Decode Functions:** ```python def encode_with_vowelnumber(text): return text.encode(\'vowelnumber\') def decode_with_vowelnumber(encoded_text): return encoded_text.decode(\'vowelnumber\') ``` 3. **Custom Error Handler:** ```python def hex_replacement_error_handler(exception): replacement = \'\'.join([f\'x{ord(char):02x}\' for char in exception.object[exception.start:exception.end]]) return (replacement, exception.end) def register_hex_replacement_error_handler(): import codecs codecs.register_error(\'hexreplace\', hex_replacement_error_handler) ``` Testing the Implementation ```python def test_vowelnumber_codec_and_error_handling(): # Register codec and error handler register_vowelnumber_codec() register_hex_replacement_error_handler() original_text = \\"hello world\\" encoded_text = encode_with_vowelnumber(original_text) decoded_text = decode_with_vowelnumber(encoded_text) assert encoded_text == b\'h2ll4 w4rld\', f\\"Expected b\'h2ll4 w4rld\', but got {encoded_text}\\" assert decoded_text == original_text, f\\"Expected \'hello world\', but got {decoded_text}\\" try: faulty_encoded_text = \\"§\\".encode(\'vowelnumber\', errors=\'hexreplace\') except UnicodeEncodeError as e: replaced_fallback = hex_replacement_error_handler(e)[0] assert replaced_fallback == \'xa7\', f\\"Expected \'xa7\', but got {replaced_fallback}\\" test_vowelnumber_codec_and_error_handling() ``` # Constraints - You must use the functions and mechanisms described to perform codec registration and error handling in Python. - Ensure compatibility with Python 3.10 or higher as it includes the required API functions for codec management. # Notes The question aims to assess your ability to use codec registration and manipulation APIs, understand and implement custom encoding/decoding logic, and manage encoding errors properly using custom handlers.","solution":"import codecs class VowelNumberEncoder: def __init__(self): self.vowel_map = {\'a\': \'1\', \'e\': \'2\', \'i\': \'3\', \'o\': \'4\', \'u\': \'5\'} def encode(self, input, errors=\'strict\'): result = \'\'.join([self.vowel_map.get(char, char) for char in input]) return (result.encode(), len(input)) def decode(self, input, errors=\'strict\'): input = input.decode() reverse_map = {v: k for k, v in self.vowel_map.items()} result = \'\'.join([reverse_map.get(char, char) for char in input]) return (result, len(input)) def register_vowelnumber_codec(): codec_info = codecs.CodecInfo( name=\'vowelnumber\', encode=VowelNumberEncoder().encode, decode=VowelNumberEncoder().decode ) codecs.register(lambda name: codec_info if name == \'vowelnumber\' else None) def encode_with_vowelnumber(text): return codecs.encode(text, \'vowelnumber\') def decode_with_vowelnumber(encoded_text): return codecs.decode(encoded_text, \'vowelnumber\') def hex_replacement_error_handler(exception): replacement = \'\'.join([f\'x{ord(char):02x}\' for char in exception.object[exception.start:exception.end]]) return (replacement, exception.end) def register_hex_replacement_error_handler(): codecs.register_error(\'hexreplace\', hex_replacement_error_handler)"},{"question":"**Title: Implement a CGI Script to Process User Data and File Uploads** Objective: You are tasked with creating a CGI script in Python that processes user-submitted form data and handles file uploads. Your script should validate the form data, ensure secure handling of file uploads, and return a formatted HTML response displaying the processed information. Requirements: 1. **Form Fields**: - Your HTML form contains the following fields: - `username` (Text) - `email` (Text, should be a valid email format) - `bio` (Textarea) - `profile_picture` (File upload) 2. **Input Validation**: - All fields are required. - `email` should be validated against a regex to ensure it is in the proper format. 3. **File Handling**: - Ensure the uploaded `profile_picture` is of MIME type `image/jpeg` or `image/png`. - Limit the size of `profile_picture` to 2MB. 4. **Output**: - Return an HTML response displaying the submitted data. - If any input is invalid, return an HTML response indicating the specific errors. Constraints: - Use the `cgi.FieldStorage` class to read and process form data. - Implement proper handling of file uploads using the `file` attribute of `FieldStorage`. - Your script should be secure against typical CGI-related vulnerabilities. Input: The inputs are provided through form submission (method POST), and you must read it using the `cgi.FieldStorage` class. Output: Generate an HTML response as a string. Template: ```python #!/usr/bin/env python3 import cgi import cgitb import re # Enable CGI traceback for debugging cgitb.enable() def validate_email(email): # Add a regex to validate email format return re.match(r\'^[a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,6}\', email) def handle_file_upload(fileitem): # Check if file: if fileitem.file and fileitem.filename: # Ensure it is a valid mime-type if fileitem.type not in (\'image/jpeg\', \'image/png\'): return False, \\"Invalid file type. Only JPEG or PNG allowed.\\" # Ensure file size (in memory) if len(fileitem.value) > 2 * 1024 * 1024: return False, \\"File size exceeds 2MB limit.\\" return True, \\"\\" return False, \\"Failed to upload file.\\" def application(): form = cgi.FieldStorage() # Initialize response response = <html> <head> <title>Form Submission</title> </head> <body> # Validating form fields errors = [] username = form.getfirst(\\"username\\", \\"\\").strip() email = form.getfirst(\\"email\\", \\"\\").strip() bio = form.getfirst(\\"bio\\", \\"\\").strip() profile_picture = form[\'profile_picture\'] if \'profile_picture\' in form else None if not username: errors.append(\\"Username is required.\\") if not email or not validate_email(email): errors.append(\\"Valid email is required.\\") if not bio: errors.append(\\"Bio is required.\\") if not profile_picture: errors.append(\\"Profile picture is required.\\") else: valid, msg = handle_file_upload(profile_picture) if not valid: errors.append(msg) if errors: response += \\"<h1>Errors:</h1>\\" response += \\"<ul>\\" for error in errors: response += f\\"<li>{error}</li>\\" response += \\"</ul>\\" else: response += f\\"<h1>Form Submission Successful</h1>\\" response += f\\"<p>Username: {username}</p>\\" response += f\\"<p>Email: {email}</p>\\" response += f\\"<p>Bio: {bio}</p>\\" response += f\\"<p>Profile picture uploaded successfully.</p>\\" response += \\"</body></html>\\" print(\\"Content-Type: text/html\\") print() print(response) if __name__ == \\"__main__\\": application() ``` Instructions: 1. Implement the `validate_email(email)` function to validate the email format. 2. Implement the `handle_file_upload(fileitem)` function to check file type and size. 3. Integrate the form validation and file handling in the `application` function. 4. Make sure that the outputted HTML is correctly formatted. **Note:** - Your CGI script should be tested in an environment that supports CGI, such as a local server setup to handle CGI scripts.","solution":"#!/usr/bin/env python3 import cgi import cgitb import re # Enable CGI traceback for debugging cgitb.enable() def validate_email(email): Validate the email format using regex. return re.match(r\'^[a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,6}\', email) def handle_file_upload(fileitem): Check the file type and size for the uploaded profile picture. if fileitem.file and fileitem.filename: fileitem.file.seek(0, 2) # Seek to the end of the file file_size = fileitem.file.tell() # Get the file size fileitem.file.seek(0) # Reset the file pointer to the start if fileitem.type not in (\'image/jpeg\', \'image/png\'): return False, \\"Invalid file type. Only JPEG or PNG allowed.\\" if file_size > 2 * 1024 * 1024: return False, \\"File size exceeds 2MB limit.\\" return True, \\"\\" return False, \\"Failed to upload file.\\" def application(): form = cgi.FieldStorage() # Initialize response response = <html> <head> <title>Form Submission</title> </head> <body> # Validating form fields errors = [] username = form.getfirst(\\"username\\", \\"\\").strip() email = form.getfirst(\\"email\\", \\"\\").strip() bio = form.getfirst(\\"bio\\", \\"\\").strip() profile_picture = form[\'profile_picture\'] if \'profile_picture\' in form else None if not username: errors.append(\\"Username is required.\\") if not email or not validate_email(email): errors.append(\\"Valid email is required.\\") if not bio: errors.append(\\"Bio is required.\\") if not profile_picture: errors.append(\\"Profile picture is required.\\") else: valid, msg = handle_file_upload(profile_picture) if not valid: errors.append(msg) if errors: response += \\"<h1>Errors:</h1>\\" response += \\"<ul>\\" for error in errors: response += f\\"<li>{error}</li>\\" response += \\"</ul>\\" else: response += f\\"<h1>Form Submission Successful</h1>\\" response += f\\"<p>Username: {username}</p>\\" response += f\\"<p>Email: {email}</p>\\" response += f\\"<p>Bio: {bio}</p>\\" response += f\\"<p>Profile picture uploaded successfully.</p>\\" response += \\"</body></html>\\" print(\\"Content-Type: text/html\\") print() print(response) if __name__ == \\"__main__\\": application()"},{"question":"# Binhex Encoding and Decoding with File Handling You are required to design two functions that replicate the encoding and decoding functionalities of the deprecated `binhex` module in Python. Instead of using the `binhex` module, leverage the `binascii` module to perform ASCII-to-binary and binary-to-ASCII conversions to develop equivalent functionality. Function 1: `custom_binhex(input_filename, output_filename)` This function should take a binary file and convert it to a custom binhex format. **Input:** - `input_filename`: A string representing the name of the binary file to be encoded. - `output_filename`: A string representing the name of the file where the encoded data will be stored. **Output:** - No return value. The function should create a file with the encoded content. Function 2: `custom_hexbin(input_filename, output_filename)` This function should decode a custom binhex formatted file back into its original binary format. **Input:** - `input_filename`: A string representing the name of the binhex file to be decoded. - `output_filename`: A string representing the name of the file to store the decoded binary content. **Output:** - No return value. The function should create a file with the decoded binary content. **Constraints:** - Ensure that the encoded files use only ASCII characters. - The encoded format should be reversible, allowing the original file to be fully recovered. - Handle errors appropriately by raising meaningful exceptions when encoding or decoding fails. For example: ```python import binascii def custom_binhex(input_filename, output_filename): # Your implementation goes here pass def custom_hexbin(input_filename, output_filename): # Your implementation goes here pass # Example usage: try: custom_binhex(\'example.binary\', \'example.binhex\') custom_hexbin(\'example.binhex\', \'example_recovered.binary\') except Exception as e: print(f\\"An error occurred: {e}\\") ``` Ensure you handle file operations and error conditions properly in your implementation.","solution":"import binascii def custom_binhex(input_filename, output_filename): Encode a binary file to a custom binhex format. :param input_filename: Name of the binary file to be encoded. :param output_filename: Name of the file where the encoded data will be stored. try: with open(input_filename, \'rb\') as f_in: binary_data = f_in.read() hex_data = binascii.hexlify(binary_data).decode(\'ascii\') with open(output_filename, \'w\') as f_out: f_out.write(hex_data) except Exception as e: raise RuntimeError(f\\"Encoding failed: {e}\\") def custom_hexbin(input_filename, output_filename): Decode a custom binhex formatted file back into its original binary format. :param input_filename: Name of the binhex file to be decoded. :param output_filename: Name of the file to store the decoded binary content. try: with open(input_filename, \'r\') as f_in: hex_data = f_in.read() binary_data = binascii.unhexlify(hex_data) with open(output_filename, \'wb\') as f_out: f_out.write(binary_data) except Exception as e: raise RuntimeError(f\\"Decoding failed: {e}\\")"},{"question":"You are given a directory with an unknown nested structure containing multiple types of files. Your task is to find all files with specific extensions and return the sorted list of their paths. # Requirements: 1. Implement a function `find_files(root_dir, extensions)`. 2. **Input:** - `root_dir` (str): The root directory from which to start the search. - `extensions` (List[str]): A list of file extensions (e.g., `[\'txt\', \'gif\']`) to search for. 3. **Output:** - Return a sorted list of paths to the files that match the given extensions. The paths should be relative to the `root_dir`. 4. **Constraints:** - Search should be case insensitive. - The search must be recursive, i.e., it should look into all subdirectories. - Do not include broken symlinks in the output. 5. **Performance Requirements:** The solution should handle large directory structures efficiently using `glob` capabilities. # Example: ```python import os # Example directory structure: # /root_dir # ├── file1.txt # ├── file2.gif # ├── file3.JPG # └── subdir # ├── file4.gif # └── file5.txt # root_dir = \'root_dir\' extensions = [\'txt\', \'gif\'] expected_result = [ \'file1.txt\', \'file2.gif\', \'subdir/file4.gif\', \'subdir/file5.txt\' ] # Call the function result = find_files(root_dir, extensions) assert result == expected_result ``` # Notes: - Ensure that the function makes use of `glob.glob` or `glob.iglob` for pattern matching. - Handle different operating systems\' path separators appropriately. - Be mindful of performance when dealing with large directory trees.","solution":"import os import glob def find_files(root_dir, extensions): Finds all files with specific extensions in a directory (and subdirectories). :param root_dir: The root directory where the search starts :param extensions: A list of file extensions to search for :return: A sorted list of relative file paths with the given extensions # Convert extensions to lowercase and create search patterns patterns = [f\\"**/*.{ext.lower()}\\" for ext in extensions] # Find all matching files matching_files = [] for pattern in patterns: matching_files.extend( glob.glob(os.path.join(root_dir, pattern), recursive=True)) # Filter out broken symlinks and normalize paths valid_files = [] for file in matching_files: if os.path.isfile(file) and not os.path.islink(file): # Get the relative path rel_path = os.path.relpath(file, root_dir) valid_files.append(rel_path) # Sort the results return sorted(valid_files)"},{"question":"Objective The purpose of this assessment is to evaluate your understanding of advanced string formatting, file I/O operations, and JSON serialization/deserialization in Python. Task You are required to develop a small Python program that processes employee records. The program should read a file containing employee details, process the data, and then write the processed data into two different files: one with formatted output and another with JSON-serialized data. Specifications 1. **Input File:** - The input file, `employees.txt`, contains multiple lines, each representing an employee\'s details. - Each line has the following format: `id,name,salary,department`. Example: ``` 1,John Doe,50000,Engineering 2,Jane Smith,60000,Marketing 3,Bob Brown,55000,Sales ``` 2. **Output Files:** - `formatted_employees.txt`: A text file with formatted employee details. Each line should be formatted as `ID: <id>, Name: <name>, Salary: <salary>, Department: <department>`. - `employees.json`: A JSON file containing a list of employee records, where each record is represented as a dictionary. 3. **Program Requirements:** - Read data from the `employees.txt` file. - Process the data to format it as specified. - Write the formatted data to `formatted_employees.txt`. - Serialize the data to JSON format and write it to `employees.json`. 4. **Constraints:** - Assume the input file `employees.txt` is correctly formatted. - The JSON file must be encoded in UTF-8. 5. **Performance Requirements:** - The program should be efficient and able to handle large files with thousands of employee records. Function Signature ```python def process_employee_records(input_filename: str, formatted_output_filename: str, json_output_filename: str) -> None: pass ``` Expected Behavior 1. Given the input file `employees.txt`: ``` 1,John Doe,50000,Engineering 2,Jane Smith,60000,Marketing 3,Bob Brown,55000,Sales ``` 2. The `formatted_employees.txt` file should contain: ``` ID: 1, Name: John Doe, Salary: 50000, Department: Engineering ID: 2, Name: Jane Smith, Salary: 60000, Department: Marketing ID: 3, Name: Bob Brown, Salary: 55000, Department: Sales ``` 3. The `employees.json` file should contain: ```json [ {\\"id\\": \\"1\\", \\"name\\": \\"John Doe\\", \\"salary\\": \\"50000\\", \\"department\\": \\"Engineering\\"}, {\\"id\\": \\"2\\", \\"name\\": \\"Jane Smith\\", \\"salary\\": \\"60000\\", \\"department\\": \\"Marketing\\"}, {\\"id\\": \\"3\\", \\"name\\": \\"Bob Brown\\", \\"salary\\": \\"55000\\", \\"department\\": \\"Sales\\"} ] ``` Submission Please submit the Python function `process_employee_records` implementing the described functionality.","solution":"import json def process_employee_records(input_filename: str, formatted_output_filename: str, json_output_filename: str) -> None: employees = [] # Read data from input file with open(input_filename, \'r\') as file: for line in file: id, name, salary, department = line.strip().split(\',\') employees.append({\\"id\\": id, \\"name\\": name, \\"salary\\": salary, \\"department\\": department}) # Write formatted data to output file with open(formatted_output_filename, \'w\') as file: for employee in employees: file.write(f\\"ID: {employee[\'id\']}, Name: {employee[\'name\']}, Salary: {employee[\'salary\']}, Department: {employee[\'department\']}n\\") # Write JSON data to output file with open(json_output_filename, \'w\', encoding=\'utf-8\') as file: json.dump(employees, file, ensure_ascii=False, indent=4)"},{"question":"# Advanced Coding Problem: High-Precision Financial Calculations In financial applications, precision is paramount. The `decimal` module in Python provides a way to perform exact arithmetic that avoids the rounding issues of binary floating-point numbers. **Problem Statement:** You are building a simple financial application. Implement a class `HighPrecisionAccount` that uses the `decimal` module to manage account balances with high precision. The account should support depositing, withdrawing, and transferring funds with exact decimal arithmetic. Additionally, the class should support calculating interest over a period with precise rounding methods. **Class Requirements:** 1. **Initialization**: - The account should be created with an initial balance (decimal). - The account should have a fixed interest rate (decimal) for calculating interest. 2. **Methods**: - `deposit(amount: Decimal)`: Adds the specified amount to the account balance. - `withdraw(amount: Decimal)`: Subtracts the specified amount from the account balance. - `transfer_to(amount: Decimal, other: \'HighPrecisionAccount\')`: Transfers the specified amount to another `HighPrecisionAccount`. - `calculate_interest(periods: int, rounding: str) -> Decimal`: Calculates the interest over a number of periods using a specified rounding method and returns the calculated interest. 3. **Constraints**: - Ensure that withdrawals and transfers do not result in a negative balance. - Handle exceptions for invalid operations gracefully. 4. **Rounding Methods**: - Implement support for multiple rounding methods as specified in the `decimal` module: `ROUND_UP`, `ROUND_DOWN`, `ROUND_HALF_UP`, `ROUND_HALF_DOWN`, `ROUND_HALF_EVEN`. **Expected Functionality and Examples:** ```python from decimal import Decimal, getcontext, setcontext, Context, ROUND_UP class HighPrecisionAccount: def __init__(self, initial_balance: Decimal, interest_rate: Decimal): self.balance = initial_balance self.interest_rate = interest_rate def deposit(self, amount: Decimal): self.balance += amount def withdraw(self, amount: Decimal): if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def transfer_to(self, amount: Decimal, other: \'HighPrecisionAccount\'): if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount other.deposit(amount) def calculate_interest(self, periods: int, rounding: str) -> Decimal: context = getcontext().copy() context.rounding = rounding setcontext(context) compounded_amount = self.balance * ((1 + self.interest_rate) ** periods) interest = compounded_amount - self.balance return interest # Example usage: acc1 = HighPrecisionAccount(Decimal(\'1000.00\'), Decimal(\'0.05\')) acc2 = HighPrecisionAccount(Decimal(\'500.00\'), Decimal(\'0.03\')) acc1.deposit(Decimal(\'200.00\')) acc1.withdraw(Decimal(\'50.00\')) acc1.transfer_to(Decimal(\'100.00\'), acc2) interest = acc1.calculate_interest(10, ROUND_UP) print(f\\"Acc1 Balance: {acc1.balance}, Interest: {interest}\\") ``` **Input Format:** - No direct input. Methods are called programmatically. **Output Format:** - No direct output. Methods return appropriate values or modify internal state. **Note:** - Use the `decimal` module to ensure that all operations are performed with high precision. - Validate inputs and handle exceptions appropriately. Your implementation should show a clear understanding of the `decimal` module and its context management for precise arithmetic operations.","solution":"from decimal import Decimal, getcontext, setcontext class HighPrecisionAccount: def __init__(self, initial_balance: Decimal, interest_rate: Decimal): self.balance = initial_balance self.interest_rate = interest_rate def deposit(self, amount: Decimal): self.balance += amount def withdraw(self, amount: Decimal): if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def transfer_to(self, amount: Decimal, other: \'HighPrecisionAccount\'): if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount other.deposit(amount) def calculate_interest(self, periods: int, rounding: str) -> Decimal: context = getcontext().copy() context.rounding = rounding setcontext(context) compounded_amount = self.balance * ((1 + self.interest_rate) ** periods) interest = compounded_amount - self.balance return interest"},{"question":"# Python Coding Assessment - Symbol Table Analysis Objective You are required to demonstrate your understanding of the `symtable` module in Python by analyzing the symbol table generated for a given source code. The analysis should include details about functions, classes, and symbols defined in the code. Task Write a Python function `analyze_symbol_table(code: str, filename: str, compile_type: str) -> dict` that performs the following: 1. Generates the symbol table for the given Python source code. 2. Extracts and returns a dictionary containing: - The type of the top-level symbol table (module). - A list of functions and their respective details (`name`, `parameters`, `locals`, `globals`, `nonlocals`, `frees`). - A list of classes and their respective details (`name`, `methods`). - A list of symbols and their properties (`name`, `is_referenced`, `is_imported`, `is_parameter`, `is_local`, `is_global`, `is_nonlocal`, `is_free`, `is_assigned`, `is_namespace`). Input - `code`: A string containing the Python source code. - `filename`: A string representing the name of the file containing the code. - `compile_type`: A string specifying the compile type (e.g., \\"exec\\", \\"eval\\", or \\"single\\"). Output - A dictionary with the following structure: ```python { \\"top_level_type\\": str, \\"functions\\": [ { \\"name\\": str, \\"parameters\\": list, \\"locals\\": list, \\"globals\\": list, \\"nonlocals\\": list, \\"frees\\": list } ], \\"classes\\": [ { \\"name\\": str, \\"methods\\": list } ], \\"symbols\\": [ { \\"name\\": str, \\"is_referenced\\": bool, \\"is_imported\\": bool, \\"is_parameter\\": bool, \\"is_local\\": bool, \\"is_global\\": bool, \\"is_nonlocal\\": bool, \\"is_free\\": bool, \\"is_assigned\\": bool, \\"is_namespace\\": bool } ] } ``` Constraints - You must utilize the `symtable` module to generate and analyze the symbol table. - Ensure that you handle any possible exceptions that might arise during symbol table generation or analysis. Example ```python code = import math def add(a, b): return a + b class Calculator: def multiply(self, a, b): return a * b result = add(3, 4) calc = Calculator() print(calc.multiply(5, 6)) filename = \\"example_code.py\\" compile_type = \\"exec\\" # Function Call analyze_symbol_table(code, filename, compile_type) # Expected Output { \\"top_level_type\\": \\"module\\", \\"functions\\": [ { \\"name\\": \\"add\\", \\"parameters\\": [\\"a\\", \\"b\\"], \\"locals\\": [\\"a\\", \\"b\\"], \\"globals\\": [], \\"nonlocals\\": [], \\"frees\\": [] }, { \\"name\\": \\"multiply\\", \\"parameters\\": [\\"self\\", \\"a\\", \\"b\\"], \\"locals\\": [\\"self\\", \\"a\\", \\"b\\"], \\"globals\\": [], \\"nonlocals\\": [], \\"frees\\": [] } ], \\"classes\\": [ { \\"name\\": \\"Calculator\\", \\"methods\\": [\\"multiply\\"] } ], \\"symbols\\": [ { \\"name\\": \\"math\\", \\"is_referenced\\": True, \\"is_imported\\": True, \\"is_parameter\\": False, \\"is_local\\": False, \\"is_global\\": True, \\"is_nonlocal\\": False, \\"is_free\\": False, \\"is_assigned\\": False, \\"is_namespace\\": False }, ... ] } ```","solution":"import symtable def analyze_symbol_table(code: str, filename: str, compile_type: str) -> dict: Analyzes the symbol table for a given Python source code. Parameters: - code (str): Python source code. - filename (str): Name of the file containing the code. - compile_type (str): The compile type (\\"exec\\", \\"eval\\", \\"single\\"). Returns: - dict: A dictionary containing details about functions, classes, and symbols. # Create the symbol table from the code symbol_table = symtable.symtable(code, filename, compile_type) result = { \\"top_level_type\\": symbol_table.get_type(), \\"functions\\": [], \\"classes\\": [], \\"symbols\\": [] } # Helper function to analyze a symbol table def analyze_sym_table(table): # Collect function details for child in table.get_children(): if child.get_type() == \'function\': func_details = { \\"name\\": child.get_name(), \\"parameters\\": list(child.get_parameters()), \\"locals\\": list(child.get_locals()), \\"globals\\": list(child.get_globals()), \\"nonlocals\\": list(child.get_nonlocals()), \\"frees\\": list(child.get_frees()) } result[\\"functions\\"].append(func_details) elif child.get_type() == \'class\': class_details = { \\"name\\": child.get_name(), \\"methods\\": [m.get_name() for m in child.get_children() if m.get_type() == \'function\'] } result[\\"classes\\"].append(class_details) # Recursively analyze child tables analyze_sym_table(child) # Collect symbol details for symbol in table.get_symbols(): symbol_details = { \\"name\\": symbol.get_name(), \\"is_referenced\\": symbol.is_referenced(), \\"is_imported\\": symbol.is_imported(), \\"is_parameter\\": symbol.is_parameter(), \\"is_local\\": symbol.is_local(), \\"is_global\\": symbol.is_global(), \\"is_nonlocal\\": symbol.is_nonlocal(), \\"is_free\\": symbol.is_free(), \\"is_assigned\\": symbol.is_assigned(), \\"is_namespace\\": symbol.is_namespace() } result[\\"symbols\\"].append(symbol_details) # Analyze the top-level symbol table analyze_sym_table(symbol_table) return result"},{"question":"**Question: Using the `codeop` module** The goal of this exercise is to create a small interactive Python console using the `codeop` module. This console should: 1. Accept and compile multiple lines of input. 2. Determine if the entered code constitutes complete statements and handle incomplete statements properly. 3. Remember and enforce any `__future__` statements for subsequent inputs. 4. Execute the code if the compiled code object is ready and output the result. # Task Specification 1. Implement a function `interactive_console()` which keeps accepting input from the user. 2. Use `codeop.CommandCompiler` to manage the compilation process. 3. The function should print `>>>` as the prompt for a new statement and `...` for continuation lines of an incomplete statement. 4. If an error occurs during compilation or execution, the function should catch and print the error message. 5. The function should stop accepting input when the user types `exit()`, which indicates the termination of the interactive console. # Expected Function Signature ```python def interactive_console(): Runs an interactive console using the codeop module. ``` # Example Usage ```python if __name__ == \\"__main__\\": interactive_console() ``` # Example Interaction ``` >>> print(\\"Hello, World!\\") Hello, World! >>> a = 10 >>> b = 20 >>> a + b 30 >>> def foo(): ... return \\"bar\\" ... >>> foo() \'bar\' >>> exit() ``` # Performance Considerations - The function should handle typical interactive console input smoothly. - Ensure that the interactive behavior is consistent and intuitive for users. # Constraints and Limitations - Python version should be 3.10 or above. - Do not use other external libraries other than the standard library. The assessment will evaluate both your understanding of the `codeop` module and your ability to create an interactive loop for code execution. Clear and concise implementation along with robust error handling will be considered during evaluation.","solution":"import codeop def interactive_console(): Runs an interactive console using the codeop module. compiler = codeop.CommandCompiler() ps1, ps2 = \\">>> \\", \\"... \\" more_input = False user_input = \\"\\" while True: try: if more_input: prompt = ps2 else: prompt = ps1 new_line = input(prompt) if new_line.strip() == \\"exit()\\": break user_input += new_line + \'n\' code_obj = compiler(user_input) if code_obj is None: more_input = True continue exec(code_obj, globals()) user_input = \\"\\" more_input = False except Exception as e: print(f\\"Error: {e}\\") user_input = \\"\\" more_input = False if __name__ == \\"__main__\\": interactive_console()"},{"question":"# PyTorch Benchmarking Challenge Objective Your task is to benchmark and compare the performance of different neural network layers using PyTorch\'s `torch.utils.benchmark` module. Task Implement a function `benchmark_conv_layers` that benchmarks the performance of different configurations of 2D convolutional layers on random input tensors. Function Signature ```python def benchmark_conv_layers(in_channels_list: list, out_channels_list: list, kernel_sizes: list, input_size: tuple, num_runs: int) -> dict: Benchmarks different configurations of 2D convolutional layers. Args: in_channels_list (list): List of input channel sizes to test. out_channels_list (list): List of output channel sizes to test. kernel_sizes (list): List of kernel sizes to test. input_size (tuple): Size of the input tensor (batch_size, in_channels, height, width). num_runs (int): Number of times to run each benchmark for averaging. Returns: dict: Dictionary where keys are tuples of (in_channels, out_channels, kernel_size) and values are average execution time in seconds. ``` Details 1. **Create different 2D convolution layers** using combinations of input channels (`in_channels_list`), output channels (`out_channels_list`), and kernel sizes (`kernel_sizes`). 2. **Generate random input tensors** of the specified `input_size`. 3. **Use PyTorch\'s `torch.utils.benchmark.Timer` to measure the execution time** of each convolution layer configuration over a specified number of runs (`num_runs`). 4. **Return the average execution time in a dictionary**, with keys representing the configuration `(in_channels, out_channels, kernel_size)`. Example Usage ```python in_channels_list = [1, 3, 5] out_channels_list = [16, 32] kernel_sizes = [3, 5] input_size = (8, 3, 64, 64) # batch_size, in_channels, height, width num_runs = 10 results = benchmark_conv_layers(in_channels_list, out_channels_list, kernel_sizes, input_size, num_runs) print(results) ``` Constraints - Use PyTorch to define and run the convolutional layers. - Ensure that your benchmarking code is efficient and does not include unnecessary overhead. - The function should properly handle cases where the input dimensions do not match the channel sizes in the configurations being tested. Performance Requirements - Implement the function to minimize any extra overhead in measuring the execution times. - Conduct multiple runs (`num_runs`) for each configuration to get a stable, average execution time. Good luck, and happy benchmarking!","solution":"import torch import torch.nn as nn import torch.utils.benchmark as benchmark def benchmark_conv_layers(in_channels_list: list, out_channels_list: list, kernel_sizes: list, input_size: tuple, num_runs: int) -> dict: results = {} for in_channels in in_channels_list: for out_channels in out_channels_list: for kernel_size in kernel_sizes: # Ensure the input and conv layer dimensions match if in_channels != input_size[1]: continue conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size) input_tensor = torch.randn(input_size) timer = benchmark.Timer( \\"conv_layer(input_tensor)\\", globals={\\"conv_layer\\": conv_layer, \\"input_tensor\\": input_tensor} ) time = timer.timeit(num_runs).mean results[(in_channels, out_channels, kernel_size)] = time return results"},{"question":"Objective You are tasked with managing configuration data for an application using Apple property list (plist) format. Your goal is to write a function that reads a plist file, updates a specific key-value pair, and writes the updated plist back to a file. You must handle both binary and XML formatted plist files. Function Details Implement the function `update_plist(file_path, key, value, fmt)`: - `file_path`: A string representing the path to the plist file. - `key`: A string representing the key in the plist whose value needs to be updated. - `value`: The new value to be assigned to the given key. - `fmt`: A string specifying the format of the plist file. It can be `\\"xml\\"` or `\\"binary\\"`. The function should perform the following steps: 1. Read the plist file specified by `file_path`. 2. Update the value associated with the given key. 3. Write the updated plist data back to the same file with the specified format. Constraints - The plist file must exist at the specified `file_path`. - The key to be updated must be a valid string key in the top-level dictionary of the plist. - The new value must be of a type supported by the plist format (e.g., strings, integers, floats, booleans, lists, dictionaries, `bytes`, `bytearray`, or `datetime.datetime` objects). Input and Output Formats **Input:** - The function takes four parameters: `file_path` (str), `key` (str), `value` (any valid plist type), `fmt` (str). **Output:** - The function does not return any value. Example Usage ```python import plistlib from datetime import datetime # Example plist content example_plist = { \\"name\\": \\"ExampleApp\\", \\"version\\": 1.0, \\"last_updated\\": datetime.now(), \\"settings\\": { \\"theme\\": \\"dark\\", \\"notifications\\": True } } # Write example plist to a file with open(\\"config.plist\\", \\"wb\\") as f: plistlib.dump(example_plist, f) # Function call to update plist update_plist(\\"config.plist\\", \\"version\\", 1.1, \\"xml\\") # Check updated plist with open(\\"config.plist\\", \\"rb\\") as f: updated_plist = plistlib.load(f) print(updated_plist[\\"version\\"]) # Output: 1.1 ``` Additional Information Make sure to handle the file I/O operations properly and use the appropriate `plistlib` functions (`load`, `loads`, `dump`, `dumps`) based on the read/write operations and formats.","solution":"import plistlib def update_plist(file_path, key, value, fmt): Updates a specific key-value pair in a plist file. Args: file_path (str): Path to the plist file. key (str): Key in the plist to update. value (any valid plist type): New value to assign to the key. fmt (str): Format of the plist file (\\"xml\\" or \\"binary\\"). # Read the plist file content with open(file_path, \'rb\') as f: plist_data = plistlib.load(f) # Update the value for the given key plist_data[key] = value # Write the updated plist content back to the file in the specified format if fmt == \\"xml\\": with open(file_path, \'wb\') as f: plistlib.dump(plist_data, f, fmt=plistlib.FMT_XML) elif fmt == \\"binary\\": with open(file_path, \'wb\') as f: plistlib.dump(plist_data, f, fmt=plistlib.FMT_BINARY) else: raise ValueError(\\"Unsupported format. Use \'xml\' or \'binary\'.\\")"},{"question":"Objective: To test the understanding of multi-threading and synchronization using the low-level threading API provided by the _thread module in Python. Problem Statement: Implement a multi-threaded program to simulate a simple producer-consumer problem using the `_thread` module. Description: The producer will generate items (integers) and place them into a shared buffer. The consumer will remove items from the buffer and process them. Ensure proper synchronization using locks to avoid race conditions. Function Definitions: 1. **producer(shared_buffer, lock, produced_count, max_items)** - **Input**: - `shared_buffer`: A list acting as the shared buffer. - `lock`: A lock object created using `_thread.allocate_lock()`. - `produced_count`: An integer indicating the number of items produced. - `max_items`: Maximum number of items to produce. - **Output**: None. The function should add integers to the `shared_buffer`. 2. **consumer(shared_buffer, lock, consumed_count, max_items)** - **Input**: - `shared_buffer`: A list acting as the shared buffer. - `lock`: A lock object created using `_thread.allocate_lock()`. - `consumed_count`: An integer indicating how many items the consumer has processed. - `max_items`: Maximum number of items to consume. - **Output**: None. The function should remove integers from the `shared_buffer` and simulate processing. 3. **main()** - **Input**: None. - **Output**: None. The function orchestrates the creation of producer and consumer threads and manages synchronization. Your implementation must create threads using `_thread.start_new_thread()`, ensuring synchronization using a lock object. The `main` function should start producer and consumer threads, and they should run until the specified number of items (`max_items`) is produced and consumed. Constraints: - Ensure that the buffer operates under thread safety with appropriate locking mechanisms. - The buffer size should be fixed to 10 items. - The producer should wait if the buffer is full, and the consumer should wait if the buffer is empty. Example Usage: ```python def producer(shared_buffer, lock, produced_count, max_items): # Implement the producer logic with synchronization def consumer(shared_buffer, lock, consumed_count, max_items): # Implement the consumer logic with synchronization def main(): # Implement the main function to start the threads and manage synchronization # Example execution if __name__ == \\"__main__\\": main() ``` **Note**: Ensure proper exception handling and resource cleanup to prevent deadlocks and race conditions.","solution":"import _thread import time def producer(shared_buffer, lock, produced_count, max_items): item = 0 while produced_count[0] < max_items: lock.acquire() if len(shared_buffer) < 10: shared_buffer.append(item) produced_count[0] += 1 print(f\'Produced: {item}\') item += 1 lock.release() time.sleep(0.1) # Simulate time taken to produce def consumer(shared_buffer, lock, consumed_count, max_items): while consumed_count[0] < max_items: lock.acquire() if shared_buffer: item = shared_buffer.pop(0) consumed_count[0] += 1 print(f\'Consumed: {item}\') lock.release() time.sleep(0.1) # Simulate time taken to consume def main(): shared_buffer = [] produced_count = [0] consumed_count = [0] max_items = 10 lock = _thread.allocate_lock() _thread.start_new_thread(producer, (shared_buffer, lock, produced_count, max_items)) _thread.start_new_thread(consumer, (shared_buffer, lock, consumed_count, max_items)) # Allow threads to run while produced_count[0] < max_items or consumed_count[0] < max_items: time.sleep(0.1) if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement:** Given a dataset, use the seaborn library to create a complex visualization to understand the relationship and distribution between multiple variables. Your task is to write a function `create_custom_plot` that takes a DataFrame with penguin data and produces a combined plot showing: 1. The average body mass per species with error bars. 2. The distribution of flipper lengths within each species. **Input:** - A pandas DataFrame `df` with the following columns: - `species` (string): Species of the penguin. - `body_mass_g` (float): Body mass of the penguin. - `flipper_length_mm` (float): Flipper length of the penguin. - `sex` (string): Sex of the penguin. **Output:** - The function should generate and display a seaborn plot with the specified requirements. **Requirements:** - The average body mass per species should be represented by dash lines with error bars. - The distribution of flipper lengths should be shown using jittered dots. **Constraints:** - You can assume the input DataFrame is correctly structured and cleaned. - Use the objects and methods specified in the seaborn library as demonstrated in the documentation. **Function Signature:** ```python import seaborn.objects as so import pandas as pd def create_custom_plot(df: pd.DataFrame): pass ``` **Example:** ```python import seaborn.objects as so import pandas as pd # Example DataFrame data = { \\"species\\": [\\"Adelie\\", \\"Adelie\\", \\"Chinstrap\\", \\"Chinstrap\\", \\"Gentoo\\", \\"Gentoo\\"], \\"body_mass_g\\": [3750, 3800, 3550, 3650, 4850, 5000], \\"flipper_length_mm\\": [181, 186, 194, 188, 210, 220], \\"sex\\": [\\"Male\\", \\"Female\\", \\"Male\\", \\"Female\\", \\"Male\\", \\"Female\\"] } df = pd.DataFrame(data) def create_custom_plot(df: pd.DataFrame): p = so.Plot(df, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") p.add(so.Dash(), so.Agg(), so.Dodge()) p.add(so.Dots(), so.Dodge(), so.Jitter()) p.show() ``` Your implementation should properly visualize the required information and demonstrate an understanding of the seaborn library\'s capabilities as outlined in the provided documentation.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_custom_plot(df: pd.DataFrame): Creates a custom seaborn plot for penguin dataset. Shows: 1. The average body mass per species with error bars. 2. The distribution of flipper lengths within each species represented by jittered dots. plt.figure(figsize=(12, 8)) # Create a subplot for average body mass per species with error bars plt.subplot(1, 2, 1) sns.pointplot(x=\\"species\\", y=\\"body_mass_g\\", data=df, ci=\\"sd\\", linestyles=\\"--\\", capsize=.1, markers=\\"D\\", join=False) plt.title(\'Average Body Mass per Species\') # Create a subplot for the distribution of flipper lengths within each species plt.subplot(1, 2, 2) sns.stripplot(x=\\"species\\", y=\\"flipper_length_mm\\", data=df, jitter=True, dodge=True, marker=\'o\', alpha=0.5) plt.title(\'Distribution of Flipper Lengths within Species\') plt.tight_layout() plt.show()"},{"question":"**Objective:** Demonstrate your understanding of the asyncio subprocess API in Python 3.10. --- **Question:** You are tasked with creating a command-line utility that launches multiple shell commands in parallel, captures their outputs, and aggregates the results. Each command should be executed as a subprocess using asyncio\'s subprocess API. Your function should handle any potential errors that may arise during command execution. Write a function `run_commands(commands: List[str]) -> Tuple[List[str], List[str]]` that takes a list of shell commands as input and returns a tuple containing two lists: 1. A list of the standard outputs (stdout) for each command. 2. A list of the standard errors (stderr) for each command. Your function should: - Execute all the commands in parallel. - Capture both stdout and stderr for each command. - Handle any errors that occur during subprocess creation or execution. - Return the results in the order corresponding to the input commands. **Constraints:** - Each command in the input list is a valid shell command. - The function should not assume any specific behavior or output from the commands. - The total number of commands will not exceed 10. - The size of the output from any command will not exceed 1 MB. **Performance Requirement:** - Your solution should efficiently handle the concurrent execution of the specified number of subprocesses. ```python from typing import List, Tuple import asyncio async def run_command(cmd: str) -> Tuple[str, str]: process = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return stdout.decode(), stderr.decode() async def run_commands(commands: List[str]) -> Tuple[List[str], List[str]]: tasks = [run_command(cmd) for cmd in commands] results = await asyncio.gather(*tasks) stdout_list, stderr_list = zip(*results) return list(stdout_list), list(stderr_list) # Example usage: commands = [\'echo Hello\', \'ls /nonexistent_directory\'] stdout_list, stderr_list = asyncio.run(run_commands(commands)) print(\'Standard Output:\', stdout_list) print(\'Standard Error:\', stderr_list) ``` **Note**: Be sure to test your function with a variety of commands to ensure it handles different scenarios and edge cases appropriately.","solution":"from typing import List, Tuple import asyncio async def run_command(cmd: str) -> Tuple[str, str]: Executes a single shell command and captures its stdout and stderr outputs. :param cmd: The shell command to execute. :return: A tuple containing the stdout and stderr outputs of the command. process = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return stdout.decode(), stderr.decode() async def run_commands(commands: List[str]) -> Tuple[List[str], List[str]]: Executes multiple shell commands in parallel and captures their outputs. :param commands: A list of shell commands to execute. :return: A tuple containing two lists: - A list of stdout outputs for each command. - A list of stderr outputs for each command. tasks = [run_command(cmd) for cmd in commands] results = await asyncio.gather(*tasks) stdout_list, stderr_list = zip(*results) return list(stdout_list), list(stderr_list) # Example usage: # commands = [\'echo Hello\', \'ls /nonexistent_directory\'] # stdout_list, stderr_list = asyncio.run(run_commands(commands)) # print(\'Standard Output:\', stdout_list) # print(\'Standard Error:\', stderr_list)"},{"question":"# Advanced Python Coding Assessment: Implementing Argument Parsing in Python using Argument Clinic Objective You are required to demonstrate your understanding and application of Argument Clinic by implementing a sample Python function that simulates the automatic generation of argument parsing code as described in the documentation. The function should handle the conversion of function arguments based on the given format units and generate corresponding Python-compatible parsing code. Problem Statement Write a Python function `generate_argument_parsing_code` that takes the following input: - `function_name`: A string representing the name of the function. - `parameters`: A list of dictionaries where each dictionary represents a parameter with keys `\\"name\\"`, `\\"type\\"`, and optional `\\"default\\"`. The function should output a string that contains the generated parsing code. The parsing code should: 1. Handle the conversion of function arguments based on their types. 2. Include default values for parameters if specified. 3. Generate appropriate handling for positional and keyword arguments. 4. Return an error message if the argument types are incorrect. 5. Example method definitions should be included to verify the functionality. Input Format - `function_name` (str): The name of the function. - `parameters` (list of dict): Each dictionary in the list contains: - `name` (str): The name of the parameter. - `type` (str): The type of the parameter (e.g., \'int\', \'str\', \'bool\', etc.). - `default` (optional, varies): The default value for the parameter, if applicable. Output Format - (str): A string representing the generated argument parsing code in Python. Example ```python def generate_argument_parsing_code(function_name, parameters): # Implement the function... function_name = \\"sample_function\\" parameters = [ {\\"name\\": \\"x\\", \\"type\\": \\"int\\"}, {\\"name\\": \\"y\\", \\"type\\": \\"str\\", \\"default\\": \\"default_value\\"}, {\\"name\\": \\"flag\\", \\"type\\": \\"bool\\", \\"default\\": False} ] print(generate_argument_parsing_code(function_name, parameters)) ``` Expected Output: ```python def sample_function(x, y=\\"default_value\\", flag=False): if not isinstance(x, int): raise TypeError(f\\"x must be of type int\\") if not isinstance(y, str): raise TypeError(f\\"y must be of type str\\") if not isinstance(flag, bool): raise TypeError(f\\"flag must be of type bool\\") # function implementation goes here return ``` Constraints - All parameter types will be one of the following: `int`, `str`, `bool`, `float`. - Default values, if any, will be consistent with their respective types. - If no default value is provided, the parameter is considered required. - The function should handle up to 10 parameters. Create the function and verify it with given test cases using the mentioned input and output formats. Ensure efficient and correct conversion and handling of argument types.","solution":"def generate_argument_parsing_code(function_name, parameters): Generates Python code for a function with proper argument parsing and validation. :param function_name: Name of the function :param parameters: List of dictionaries defining the parameters :return: String representing the generated argument parsing code # Creating the function signature sig_elements = [] for param in parameters: if \'default\' in param: sig_elements.append(f\\"{param[\'name\']}={repr(param[\'default\'])}\\") else: sig_elements.append(param[\'name\']) function_signature = f\\"def {function_name}({\', \'.join(sig_elements)}):\\" # Creating the type check code type_checks = [] for param in parameters: type_checks.append(f\\" if not isinstance({param[\'name\']}, {param[\'type\']}):\\") type_checks.append(f\\" raise TypeError(f\\"{param[\'name\']} must be of type {param[\'type\']}\\")\\") # Combine all parts to form final function final_code = function_signature + \\"n\\" final_code += \\"n\\".join(type_checks) final_code += \\"nn # function implementation goes heren\\" final_code += \\" returnn\\" return final_code"},{"question":"# Advanced Python310 Coding Assessment Objective Design a Python program that configures logging behavior using the `logging.config.dictConfig` function. Your task is to develop a dictionary that fully configures a logger, several handlers, formatters, and filters as specified. Problem Statement You are required to configure logging for a hypothetical application using the `logging.config.dictConfig` function. Your logging configuration must meet the following criteria: 1. Create a logger named `appLogger` that logs messages at the `DEBUG` level and above. 2. There should be a console handler that outputs logs to `sys.stdout` using the `INFO` level, formatted with timestamps and log levels. 3. Add a rotating file handler that outputs logs to a file named `app.log` using the `DEBUG` level, with a maximum file size of 1MB and keeping up to 5 backups. This handler should use a detailed format for logs. 4. The root logger should be configured to log all messages at `WARNING` level and above. 5. Utilize a custom filter to allow only messages containing the word \\"important\\" for the file handler. Requirements - **Usage of `logging.config.dictConfig` to apply the configuration.** - **Creation of appropriate log messages to demonstrate that the configuration works.** Input and Output Formats Although there is no explicit input required, your final implementation should create log messages from the `appLogger` to display the usage of each configured handler and filter. Constraints - **You must not write/read any configuration files; everything should be done in the code.** - **Ensure the loggers and handlers perform as described, including proper formatter usage.** Example ```python import logging import logging.config import sys def setup_logging(): config = { \'version\': 1, \'formatters\': { \'console\': { \'format\': \'%(asctime)s - %(levelname)s - %(message)s\', }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', }, }, \'filters\': { \'importantFilter\': { \'()\': CustomFilter } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'INFO\', \'formatter\': \'console\', \'stream\': sys.stdout, }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'maxBytes\': 1024 * 1024, \'backupCount\': 5, \'filters\': [\'importantFilter\'], }, }, \'loggers\': { \'appLogger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': False, }, }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'], }, } logging.config.dictConfig(config) class CustomFilter(logging.Filter): def filter(self, record): return \'important\' in record.getMessage() if __name__ == \\"__main__\\": setup_logging() # Example log messages logger = logging.getLogger(\'appLogger\') logger.debug(\'This is a debug message\') logger.info(\'This is an important info message\') logger.warning(\'This is a warning message\') root_logger = logging.getLogger() root_logger.warning(\'Root logger warning message\') ``` In this example, - The `appLogger` logs `DEBUG` level messages to both console and file handlers. - Console handlers only show `INFO` and above with a simple format. - File handlers show `DEBUG` with a detailed format, rotating logs. - Custom filter ensures only messages with \\"important\\" go to the file. Your task is to replicate and fully understand this configuration and behavior.","solution":"import logging import logging.config import sys class ImportantFilter(logging.Filter): def filter(self, record): return \'important\' in record.getMessage() def setup_logging(): config = { \'version\': 1, \'formatters\': { \'console\': { \'format\': \'%(asctime)s - %(levelname)s - %(message)s\', }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', }, }, \'filters\': { \'importantFilter\': { \'()\': ImportantFilter, } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'INFO\', \'formatter\': \'console\', \'stream\': sys.stdout, }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'maxBytes\': 1024 * 1024, \'backupCount\': 5, \'filters\': [\'importantFilter\'], }, }, \'loggers\': { \'appLogger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': False, }, }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'], }, } logging.config.dictConfig(config) if __name__ == \\"__main__\\": setup_logging() logger = logging.getLogger(\'appLogger\') logger.debug(\'This is a debug message\') logger.info(\'This is an important info message\') logger.warning(\'This is a warning message\') root_logger = logging.getLogger() root_logger.warning(\'Root logger warning message\')"},{"question":"**Objective:** You are tasked with implementing a function that utilizes several features of the `sys` module to dynamically collect and log system information while a script is running, and enforce certain settings. **Problem Statement:** Create a Python function `monitor_system_info` that performs the following tasks: 1. **Collect system information:** - Retrieve and store the initial command-line arguments used to start the script. - Get the executable path of the current Python interpreter. - Determine the platform on which the script is running. - Retrieve Python\'s version information. - Capture information about the current state of the interpreter, including garbage collection status and memory usage. Specifically, use `sys.getallocatedblocks()` to get the number of currently allocated memory blocks. 2. **Set up and enforce a memory usage limit:** - Limit the maximum depth of recursion in the script to a specified value. - Limit the integer string conversion length to a specified value. 3. **Enable dynamic tracing for debugging purposes:** - Set a trace function to log each function call made during the script\'s execution. The trace function should log the function name, line number, and event type (e.g., call, return). 4. **Log collected information and trace activities:** - All collected information should be printed or logged in a readable format. **Function Signature:** ```python def monitor_system_info(recursion_limit: int, max_str_digits: int): pass ``` **Example Usage:** ```python def example_function(): print(\\"Hello, world!\\") def main(): monitor_system_info(recursion_limit=100, max_str_digits=20) example_function() if __name__ == \\"__main__\\": main() ``` **Implementation Details:** - The `recursion_limit` parameter specifies the maximum depth of recursion to be set using `sys.setrecursionlimit()`. - The `max_str_digits` parameter specifies the maximum number of digits allowed in an integer string conversion using `sys.set_int_max_str_digits()`. - Use `sys.settrace()` to enable the trace function and log function calls, returns, and exceptions. - The trace function should format and log the output to the console or a log file. **Constraints:** 1. You should handle any potential errors gracefully and log them. 2. Ensure that the trace function does not significantly slow down the script execution. 3. The logging format should be human-readable. By completing this task, you will demonstrate an understanding of the `sys` module, handling of system-level configurations, and the ability to implement dynamic tracing and logging.","solution":"import sys import platform import logging def monitor_system_info(recursion_limit: int, max_str_digits: int): Function to monitor system information and set system constraints. Args: - recursion_limit (int): The maximum depth of recursion allowed. - max_str_digits (int): The maximum number of digits allowed in integer string conversion. # Configure logging logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(message)s\') logger = logging.getLogger() # Collect system information cmd_args = sys.argv executable_path = sys.executable platform_info = platform.platform() python_version = sys.version garbage_collector_status = sys.getallocatedblocks() # Retrieve initial system settings logger.info(f\\"Command-line arguments: {cmd_args}\\") logger.info(f\\"Python executable path: {executable_path}\\") logger.info(f\\"Platform: {platform_info}\\") logger.info(f\\"Python version: {python_version}\\") logger.info(f\\"Garbage collector allocated blocks: {garbage_collector_status}\\") # Enforce system constraints try: sys.setrecursionlimit(recursion_limit) sys.set_int_max_str_digits(max_str_digits) except ValueError as e: logger.error(f\\"Error setting system constraints: {e}\\") logger.info(f\\"Recursion limit set to: {recursion_limit}\\") logger.info(f\\"Max integer string conversion length set to: {max_str_digits}\\") # Define trace function def trace_function(frame, event, arg): code = frame.f_code function_name = code.co_name line_no = frame.f_lineno logger.info(f\\"Trace - Function: {function_name}, Line: {line_no}, Event: {event}\\") return trace_function # Set tracing sys.settrace(trace_function) logger.info(\\"System monitoring has been configured successfully.\\")"},{"question":"**Objective**: To assess your understanding of seaborn\'s theming capabilities and basic plotting functions. **Problem Statement**: You are provided with data representing the average monthly temperatures and rainfall for a year. Using the seaborn library, you need to create a visualization that effectively communicates this data. Your task includes setting a specific theme, creating a dual-axis plot with temperature and rainfall, and customizing the appearance. # Input: 1. A list of 12 strings representing the months of the year: ```python months = [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"] ``` 2. A list of 12 floats representing the average temperatures for each month: ```python temperatures = [6.1, 7.7, 10.5, 14.3, 18.7, 22.3, 24.6, 24.3, 20.1, 15.1, 10.0, 7.1] ``` 3. A list of 12 floats representing the average rainfall (in mm) for each month: ```python rainfall = [78.1, 55.7, 62.5, 45.3, 55.2, 70.1, 74.6, 64.3, 58.1, 72.6, 81.0, 80.4] ``` # Requirements: 1. **Theme Setup**: Set the theme to `whitegrid` and use the `pastel` color palette. 2. **Plot Type**: Create a dual-axis plot where: - One axis shows the line plot for temperatures. - The other axis shows the bar plot for rainfall. 3. **Customization**: - Remove the top and right spines. - Set the title of the plot to `\\"Average Monthly Weather\\"`. # Output: - A plot should be displayed meeting the requirements above. # Constraints: - Ensure that the plot is clearly labeled with appropriate axis labels and legends. - You can use any additional seaborn or matplotlib functions as needed to achieve the required output. # Implementation: You can start your implementation with the following template code: ```python import seaborn as sns import matplotlib.pyplot as plt # Provided data months = [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"] temperatures = [6.1, 7.7, 10.5, 14.3, 18.7, 22.3, 24.6, 24.3, 20.1, 15.1, 10.0, 7.1] rainfall = [78.1, 55.7, 62.5, 45.3, 55.2, 70.1, 74.6, 64.3, 58.1, 72.6, 81.0, 80.4] # Set theme sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") # Create dual-axis plot fig, ax1 = plt.subplots() # Plot temperature data ax1.plot(months, temperatures, color=\\"b\\", marker=\\"o\\", label=\\"Temperature\\") ax1.set_xlabel(\\"Month\\") ax1.set_ylabel(\\"Temperature (°C)\\", color=\\"b\\") # Create a second y-axis to plot rainfall data ax2 = ax1.twinx() ax2.bar(months, rainfall, color=\\"lightblue\\", alpha=0.6, label=\\"Rainfall\\") ax2.set_ylabel(\\"Rainfall (mm)\\", color=\\"lightblue\\") # Customize plot appearance ax1.spines[\\"top\\"].set_visible(False) ax1.spines[\\"right\\"].set_visible(False) plt.title(\\"Average Monthly Weather\\") fig.tight_layout() plt.legend(loc=\'upper left\') plt.show() ``` **Note**: This is a guided template, feel free to modify and extend it as per the requirements. Ensure each customization and theme setting is properly applied as per the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_avg_monthly_weather(months, temperatures, rainfall): Plots the average monthly temperature and rainfall on a dual-axis plot with specified theming. Parameters: - months (List[str]): List of month names. - temperatures (List[float]): List of average monthly temperatures. - rainfall (List[float]): List of average monthly rainfall. # Set theme sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") # Create dual-axis plot fig, ax1 = plt.subplots() # Plot temperature data ax1.plot(months, temperatures, color=\\"b\\", marker=\\"o\\", label=\\"Temperature\\") ax1.set_xlabel(\\"Month\\") ax1.set_ylabel(\\"Temperature (°C)\\", color=\\"b\\") # Create a second y-axis to plot rainfall data ax2 = ax1.twinx() ax2.bar(months, rainfall, color=\\"lightblue\\", alpha=0.6, label=\\"Rainfall\\") ax2.set_ylabel(\\"Rainfall (mm)\\", color=\\"lightblue\\") # Customize plot appearance ax1.spines[\\"top\\"].set_visible(False) ax1.spines[\\"right\\"].set_visible(False) plt.title(\\"Average Monthly Weather\\") fig.tight_layout() plt.legend(loc=\'upper left\') plt.show()"},{"question":"Objective Write a Python function that mimics the behavior of a few selected C API functions from the `PyMapping` family. You will implement a mapping class in Python that provides methods equivalent to the following C API functions: 1. `PyMapping_Check` 2. `PyMapping_Size` 3. `PyMapping_GetItemString` 4. `PyMapping_SetItemString` 5. `PyMapping_HasKeyString` 6. `PyMapping_Keys` 7. `PyMapping_Values` 8. `PyMapping_Items` Function Specifications You need to implement a class `CustomMapping` with the following methods: 1. **`check(self)`**: Checks if the object provides the mapping protocol by looking for `__getitem__`. Returns `True` or `False`. 2. **`size(self)`**: Returns the number of key-value pairs in the mapping. Equivalent to `len(self.data)`. 3. **`get_item(self, key: str)`**: Retrieves the value associated with `key` in the mapping. Returns the value if `key` is found or raises a `KeyError` if not. 4. **`set_item(self, key: str, value: any)`**: Sets the `value` associated with `key` in the mapping. If `key` already exists, the value is updated. If `key` does not exist, it is added. 5. **`has_key(self, key: str)`**: Checks if `key` exists in the mapping. Returns `True` or `False`. 6. **`keys(self)`**: Returns a list of all keys in the mapping. 7. **`values(self)`**: Returns a list of all values in the mapping. 8. **`items(self)`**: Returns a list of tuples, each containing a key-value pair. Constraints - You cannot use Python\'s built-in `dict` methods directly for the operations; implement the logic manually where necessary. - Your class should handle concurrency issues like race conditions gracefully. Example ```python class CustomMapping: def __init__(self): self.data = {} def check(self): return hasattr(self.data, \'__getitem__\') def size(self): return len(self.data) def get_item(self, key: str): if key in self.data: return self.data[key] else: raise KeyError(f\\"Key \'{key}\' not found\\") def set_item(self, key: str, value: any): self.data[key] = value def has_key(self, key: str): return key in self.data def keys(self): return list(self.data.keys()) def values(self): return list(self.data.values()) def items(self): return list(self.data.items()) # Usage Example mapping = CustomMapping() mapping.set_item(\\"a\\", 1) print(mapping.get_item(\\"a\\")) # Output: 1 print(mapping.size()) # Output: 1 print(mapping.has_key(\\"a\\")) # Output: True print(mapping.keys()) # Output: [\'a\'] print(mapping.values()) # Output: [1] print(mapping.items()) # Output: [(\'a\', 1)] ``` In your implementation, ensure that you write unit tests to validate each function to ensure correctness.","solution":"class CustomMapping: def __init__(self): self.data = {} def check(self): return hasattr(self.data, \'__getitem__\') def size(self): return len(self.data) def get_item(self, key: str): if key in self.data: return self.data[key] else: raise KeyError(f\\"Key \'{key}\' not found\\") def set_item(self, key: str, value: any): self.data[key] = value def has_key(self, key: str): return key in self.data def keys(self): return list(self.data.keys()) def values(self): return list(self.data.values()) def items(self): return list(self.data.items())"},{"question":"# Custom Command-Line Calculator You are tasked with implementing a custom command-line-based calculator using the `cmd` module in Python. Your calculator should support the following commands: 1. **add** - Adds two numbers. 2. **subtract** - Subtracts the second number from the first. 3. **multiply** - Multiplies two numbers. 4. **divide** - Divides the first number by the second. 5. **square** - Squares a number. 6. **exit** - Exits the calculator. Your implementation should include: - Appropriate handling of invalid inputs, displaying an error message. - Custom command autocompletion. - A `do_help` method that describes each command. - Proper input prompt and banners. - Example session as a reference. Example Session: ```python Welcome to the Custom Calculator! Type help or ? to list commands. (calc) add 10 20 Result: 30 (calc) subtract 50 15 Result: 35 (calc) multiply 6 7 Result: 42 (calc) divide 20 4 Result: 5.0 (calc) square 8 Result: 64 (calc) help add Add two numbers. Usage: add <num1> <num2> (calc) exit Thank you for using the Custom Calculator! ``` Constraints: - Ensure appropriate exception handling for invalid inputs (e.g., non-numeric values, division by zero). - The calculator should function seamlessly using the `cmd` module\'s features. Implementation Details: 1. Subclass the `cmd.Cmd` class. 2. Define methods prefixed with `do_` for each command. 3. Utilize `cmdloop()` method to start the command loop. 4. Use `precmd()` and `postcmd()` hooks if necessary. 5. Implement custom auto-completion for commands. **Input:** Command-line inputs as described in the commands. **Output:** Calculation results or appropriate error messages. Boilerplate Code: ```python import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the Custom Calculator! Type help or ? to list commands.n\' prompt = \'(calc) \' def do_add(self, arg): pass def do_subtract(self, arg): pass def do_multiply(self, arg): pass def do_divide(self, arg): pass def do_square(self, arg): pass def do_exit(self, arg): print(\'Thank you for using the Custom Calculator!\') return True def default(self, line): print(f\'Unknown command: {line}\') def emptyline(self): pass # Implement custom completion and help methods as needed if __name__ == \'__main__\': CalculatorShell().cmdloop() ``` Complete the implementation to create your custom command-line calculator.","solution":"import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the Custom Calculator! Type help or ? to list commands.n\' prompt = \'(calc) \' def do_add(self, arg): \'Add two numbers. Usage: add <num1> <num2>\' try: num1, num2 = map(float, arg.split()) print(f\'Result: {num1 + num2}\') except ValueError: print(\'Invalid input. Usage: add <num1> <num2>\') def do_subtract(self, arg): \'Subtract the second number from the first. Usage: subtract <num1> <num2>\' try: num1, num2 = map(float, arg.split()) print(f\'Result: {num1 - num2}\') except ValueError: print(\'Invalid input. Usage: subtract <num1> <num2>\') def do_multiply(self, arg): \'Multiply two numbers. Usage: multiply <num1> <num2>\' try: num1, num2 = map(float, arg.split()) print(f\'Result: {num1 * num2}\') except ValueError: print(\'Invalid input. Usage: multiply <num1> <num2>\') def do_divide(self, arg): \'Divide the first number by the second. Usage: divide <num1> <num2>\' try: num1, num2 = map(float, arg.split()) if num2 == 0: print(\'Error: Division by zero.\') else: print(f\'Result: {num1 / num2}\') except ValueError: print(\'Invalid input. Usage: divide <num1> <num2>\') def do_square(self, arg): \'Square a number. Usage: square <num>\' try: num = float(arg) print(f\'Result: {num * num}\') except ValueError: print(\'Invalid input. Usage: square <num>\') def do_exit(self, arg): \'Exit the calculator.\' print(\'Thank you for using the Custom Calculator!\') return True def default(self, line): print(f\'Unknown command: {line}\') def emptyline(self): pass if __name__ == \'__main__\': CalculatorShell().cmdloop()"},{"question":"**Coding Assessment Question: Parsing and Utilizing `robots.txt` with `RobotFileParser`** # Background: A `robots.txt` file is used by websites to manage and control the behavior of automated web crawlers and spiders. Specifically, it tells these crawlers which parts of the site should not be accessed or crawled. The Python module `urllib.robotparser` allows you to parse `robots.txt` files. In this task, you are required to use the `RobotFileParser` class from this module to answer some questions based on a given `robots.txt` file. # Task: Write a Python function `analyze_robots_txt(url, useragent, target_url)` that performs the following steps: 1. **Set the URL of the robots.txt**: - Initialize a `RobotFileParser` object and set the URL to the given `url`. 2. **Read and parse `robots.txt`**: - Read the `robots.txt` file from the URL. 3. **Fetch details**: - Check if the `useragent` is allowed to fetch the `target_url` using the `can_fetch` method. - Retrieve the crawl delay for the `useragent` using the `crawl_delay` method. - Get the request rate for the `useragent` using the `request_rate` method. - Extract any listed site maps using the `site_maps` method. 4. **Return the results**: - Return a dictionary with the following keys: - `\\"can_fetch\\"`: Boolean value indicating if the useragent can fetch the target URL. - `\\"crawl_delay\\"`: The crawl delay in seconds for the useragent (or None if not specified). - `\\"request_rate\\"`: A tuple `(requests, seconds)` indicating the request rate for the useragent (or None if not specified). - `\\"site_maps\\"`: List of sitemaps found in the `robots.txt` file (or None if not specified). # Function Signature: ```python def analyze_robots_txt(url: str, useragent: str, target_url: str) -> dict: pass ``` # Example Usage: ```python url = \\"http://www.example.com/robots.txt\\" useragent = \\"Googlebot\\" target_url = \\"http://www.example.com/search\\" result = analyze_robots_txt(url, useragent, target_url) # Example output (actual values may vary based on `robots.txt` file content): # { # \\"can_fetch\\": True, # \\"crawl_delay\\": 10, # \\"request_rate\\": (5, 20), # \\"site_maps\\": [\\"http://www.example.com/sitemap.xml\\"] # } ``` # Constraints and Assumptions: - Assume network connectivity is available and the given `url` points to a valid `robots.txt` file. - Handle any edge cases where specific parameters are not present in the `robots.txt`.","solution":"from urllib.robotparser import RobotFileParser def analyze_robots_txt(url: str, useragent: str, target_url: str) -> dict: rp = RobotFileParser() rp.set_url(url) rp.read() can_fetch = rp.can_fetch(useragent, target_url) crawl_delay = rp.crawl_delay(useragent) request_rate = rp.request_rate(useragent) site_maps = rp.site_maps() return { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": (request_rate.requests, request_rate.seconds) if request_rate else None, \\"site_maps\\": site_maps if site_maps else None }"},{"question":"**Question**: You are tasked with creating a Python script that performs two main functions using the `tarfile` module: 1. **Create an Archive**: - The script should create a tar archive from a list of files and directories specified by the user. - The archive should be created using gzip compression. - The user should have the option to reset the user and group information of each file in the archive to root (uid and gid set to 0, uname and gname set to \\"root\\"). 2. **Extract an Archive**: - The script should extract all files and directories from a given tar archive into a specified directory. - Implement a custom extraction filter that accepts only regular files and directories, mimicking the behavior of the `\'data\'` filter provided by the `tarfile` module. - The filter should ensure that no symbolic links, hard links, or special files (like device files) are extracted, enhancing the security of the extraction process. **Input/Output Formats**: *Input*: - A list of file and directory paths to be archived. - The name of the output tar archive. - The name of the tar file to be extracted. - The directory to which the tar archive should be extracted. *Output*: - A tar archive file with gzip compression containing the specified files and directories. - Extraction of the tar archive into the specified directory with security filters applied. **Constraints**: - Handle exceptions where necessary to ensure that invalid files or directories are not processed. - Ensure that the tar archive creation and extraction process do not overwrite existing files or directories unintentionally. Here is a template to help you get started: ```python import tarfile import os def create_archive(file_paths, output_archive): def reset(tarinfo): tarinfo.uid = tarinfo.gid = 0 tarinfo.uname = tarinfo.gname = \\"root\\" return tarinfo with tarfile.open(output_archive, \\"w:gz\\") as tar: for path in file_paths: tar.add(path, filter=reset) print(f\\"Archive {output_archive} created successfully.\\") def extract_archive(tar_path, extract_path): def custom_filter(member): if member.isreg() or member.isdir(): member.uid = member.gid = member.uname = member.gname = None return member else: return None with tarfile.open(tar_path, \\"r:gz\\") as tar: tar.extractall(path=extract_path, filter=custom_filter) print(f\\"Archive {tar_path} extracted successfully to {extract_path}.\\") # Example usage # To create the archive file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"directory1\\"] output_archive = \\"output_archive.tar.gz\\" create_archive(file_paths, output_archive) # To extract the archive tar_path = \\"output_archive.tar.gz\\" extract_path = \\"extracted_files\\" extract_archive(tar_path, extract_path) ``` Implement the functions `create_archive` and `extract_archive` as described above. Use the provided template to complete your task. Make sure your script handles various scenarios like non-existent files, incorrect paths, and potential security threats mentioned in the extraction filter guidelines.","solution":"import tarfile import os def create_archive(file_paths, output_archive): def reset(tarinfo): tarinfo.uid = tarinfo.gid = 0 tarinfo.uname = tarinfo.gname = \\"root\\" return tarinfo with tarfile.open(output_archive, \\"w:gz\\") as tar: for path in file_paths: if os.path.exists(path): tar.add(path, filter=reset) else: print(f\\"Warning: {path} does not exist and will not be added to the archive.\\") print(f\\"Archive {output_archive} created successfully.\\") def extract_archive(tar_path, extract_path): def custom_filter(member): if member.isreg() or member.isdir(): member.uid = member.gid = member.uname = member.gname = None return member else: return None os.makedirs(extract_path, exist_ok=True) with tarfile.open(tar_path, \\"r:gz\\") as tar: tar.extractall(path=extract_path, members=(m for m in tar.getmembers() if custom_filter(m))) print(f\\"Archive {tar_path} extracted successfully to {extract_path}.\\") # Example usage # To create the archive file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"directory1\\"] output_archive = \\"output_archive.tar.gz\\" create_archive(file_paths, output_archive) # To extract the archive tar_path = \\"output_archive.tar.gz\\" extract_path = \\"extracted_files\\" extract_archive(tar_path, extract_path)"},{"question":"# Question: Implementing a Custom Autograd Function and Utilizing Saved Tensors in PyTorch **Objective**: Demonstrate your understanding of PyTorch’s autograd system by implementing a custom autograd Function that computes the forward and backward pass for a specific operation while utilizing saved tensors. **Problem Statement**: Implement a custom autograd Function `ExpSumAndSquare` that: 1. Takes a tensor `x` as input and returns a tensor `y` where each element ( y_i = exp(x_i) + x_i^2 ). 2. Saves the necessary tensors during the forward pass to compute the gradient in the backward pass. **Requirements**: 1. Define the custom autograd Function `ExpSumAndSquare` by subclassing `torch.autograd.Function`. 2. Override the `forward` and `backward` static methods: - In the `forward` method, compute ( y = exp(x) + x^2 ) and save any tensors you will need in the `backward` pass using `ctx.save_for_backward`. - In the `backward` method, use the saved tensors to compute the gradient of the loss with respect to `x`. **Input Format**: - A tensor `x` of arbitrary shape with `requires_grad=True`. **Output Format**: - A tensor `y` of the same shape as `x`. **Constraints**: - Do not use in-place operations. - All operations and tensor manipulations should be compatible with PyTorch’s autograd system. **Performance Requirements**: - Ensure the implementation is efficient and leverages PyTorch’s tensor operations effectively. Here is a template to help you get started: ```python import torch class ExpSumAndSquare(torch.autograd.Function): @staticmethod def forward(ctx, x): # Compute forward pass (y = exp(x) + x^2) # Save necessary tensors for backward pass y = None # replace with the actual computation return y @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors # Compute gradients of the loss with respect to input grad_input = None # replace with the actual gradient computation return grad_input # Example usage: x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = ExpSumAndSquare.apply(x) y.backward(torch.ones_like(x)) print(x.grad) ``` **Guidelines**: - Carefully think about which tensors from the forward pass are required for the backward pass. - Ensure the forward and backward methods perform tensor operations that PyTorch’s autograd can track. **Evaluation Criteria**: - Correctness: The solution should correctly compute the forward and backward passes. - Efficiency: The implementation should avoid unnecessary computations and memory usage. - Usage of PyTorch’s autograd features: Demonstrating a solid understanding of saving and retrieving tensors for backward computation.","solution":"import torch class ExpSumAndSquare(torch.autograd.Function): @staticmethod def forward(ctx, x): # Compute forward pass (y = exp(x) + x^2) exp_x = torch.exp(x) x_square = x ** 2 y = exp_x + x_square # Save necessary tensors for backward pass ctx.save_for_backward(x, exp_x) return y @staticmethod def backward(ctx, grad_output): # Retrieve saved tensors x, exp_x = ctx.saved_tensors # Compute gradient of the loss with respect to input x grad_x = grad_output * (exp_x + 2 * x) return grad_x # Example usage: # x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # y = ExpSumAndSquare.apply(x) # y.backward(torch.ones_like(x)) # print(x.grad)"},{"question":"Objective: Implement a Python function that verifies and processes a given buffer object according to specified requirements. Problem Statement: Write a function `process_buffer(buffer_view: Py_buffer, indices: list) -> int` that takes two arguments: 1. `buffer_view`: a buffer view object with the following attributes: - `buf`: a pointer to the start of the logical structure described by the buffer fields. - `len`: the total length of the buffer. - `itemsize`: the size of each item in the buffer. - `ndim`: the number of dimensions. - `shape`: a list of sizes in each dimension. - `strides`: a list of byte strides. 2. `indices`: a list of indices representing the position of an element in the n-dimensional buffer. Your function should perform the following: 1. Verify that the provided indices are valid for the given buffer. The indices are valid if they are within the range specified by the shape of the buffer in each dimension. 2. Calculate the memory address of the element at the given indices using the strides. 3. Return the value at the calculated memory address assuming the buffer contains integers. Input: - `buffer_view` (of type `Py_buffer`): The buffer view object with attributes as described above. - `indices` (list of int): A list of indices for each dimension of the buffer. Output: - `int`: The integer value at the calculated memory address within the buffer. Constraints: - Assume buffer contains only integer values. - Indices list is guaranteed to have a length equal to `buffer_view.ndim`. - Perform necessary error handling for invalid indices. Example: ```python class Py_buffer: def __init__(self, buf, len, itemsize, ndim, shape, strides): self.buf = buf self.len = len self.itemsize = itemsize self.ndim = ndim self.shape = shape self.strides = strides def process_buffer(buffer_view: Py_buffer, indices: list) -> int: # Your implementation goes here pass # Example usage: buffer = Py_buffer(buf=bytearray([10, 20, 30, 40, 50, 60, 70, 80]), len=8, itemsize=1, ndim=1, shape=[8], strides=[1]) indices = [3] result = process_buffer(buffer, indices) print(result) # Output should be 40 ``` Your task is to implement the `process_buffer` function. Hint: Use the `strides` attribute to calculate the memory address of the desired element.","solution":"class Py_buffer: def __init__(self, buf, len, itemsize, ndim, shape, strides): self.buf = buf self.len = len self.itemsize = itemsize self.ndim = ndim self.shape = shape self.strides = strides def process_buffer(buffer_view: Py_buffer, indices: list) -> int: # Verify the indices are within the valid range for i in range(buffer_view.ndim): if not (0 <= indices[i] < buffer_view.shape[i]): raise IndexError(f\\"Index {indices[i]} out of bounds for dimension {i+1} with size {buffer_view.shape[i]}\\") # Calculate the memory address using the strides memory_address = 0 for dim in range(buffer_view.ndim): memory_address += indices[dim] * buffer_view.strides[dim] # Return the integer value at the calculated memory address return buffer_view.buf[memory_address]"},{"question":"Objective Create a function `analyze_signal` using PyTorch\'s FFT functionality. This function will take a 1D tensor representing a time-domain signal and perform several analyses, returning the signal\'s frequency domain representation, the magnitude spectrum, and the reconstructed time-domain signal. Function Signature ```python def analyze_signal(signal: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Analyzes a given time-domain signal using FFT. Args: signal (torch.Tensor): A 1D tensor of floats representing the input time-domain signal. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: - The frequency domain representation of the signal (complex tensor). - The magnitude spectrum of the signal (real tensor). - The reconstructed time-domain signal after inverse FFT (real tensor). ``` Requirements 1. **Input:** - `signal`: A 1D tensor of floats, with length ( n ), where ( n ) is a power of 2 (e.g., 256, 512). 2. **Output:** - A tuple containing three elements: 1. `freq_domain_signal`: A 1D tensor representing the frequency domain (complex values). 2. `magnitude_spectrum`: A 1D tensor representing the magnitude spectrum of the frequency domain signal. 3. `reconstructed_signal`: A 1D tensor representing the time-domain signal reconstructed using the inverse FFT. 3. **Constraints:** - You must use PyTorch\'s FFT functions (`torch.fft.fft` and `torch.fft.ifft`). 4. **Performance Requirements:** - The function should be optimized for performance given the constraints. - Assume the length of the signal (n) will be a power of two to ensure efficient computation. Example ```python import torch signal = torch.sin(2 * torch.pi * torch.arange(256.0) / 256) # Example signal freq_domain_signal, magnitude_spectrum, reconstructed_signal = analyze_signal(signal) print(\\"Frequency domain signal:\\", freq_domain_signal) print(\\"Magnitude spectrum:\\", magnitude_spectrum) print(\\"Reconstructed signal:\\", reconstructed_signal) ``` In this example, `analyze_signal` will provide the frequency domain representation of a sine wave, its magnitude spectrum, and the reconstructed signal from the inverse FFT. Additional Notes - Verify that the reconstructed signal closely matches the input signal to validate correctness.","solution":"import torch from typing import Tuple def analyze_signal(signal: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Analyzes a given time-domain signal using FFT. Args: signal (torch.Tensor): A 1D tensor of floats representing the input time-domain signal. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: - The frequency domain representation of the signal (complex tensor). - The magnitude spectrum of the signal (real tensor). - The reconstructed time-domain signal after inverse FFT (real tensor). # Calculate the frequency domain representation using FFT freq_domain_signal = torch.fft.fft(signal) # Calculate the magnitude spectrum magnitude_spectrum = torch.abs(freq_domain_signal) # Reconstruct the time-domain signal using inverse FFT reconstructed_signal = torch.real(torch.fft.ifft(freq_domain_signal)) return freq_domain_signal, magnitude_spectrum, reconstructed_signal"},{"question":"# Background: You are given two matrices of samples `X` and `Y`, where each row represents a sample and each column represents a feature. Your task is to compute various pairwise metrics and kernels between the rows of these matrices using the `sklearn.metrics.pairwise` module. # Task: Write a Python function `compute_metrics_and_kernels(X, Y)` that takes in two 2D numpy arrays, `X` and `Y`, and returns a dictionary containing the following pairwise computations: 1. **Euclidean Distance** between `X` and `Y`. 2. **Cosine Similarity** between `X` and `Y`. 3. **Linear Kernel** between `X` and `Y`. 4. **Polynomial Kernel** (with degree=3, coefficient=1, and gamma=1) between `X` and `Y`. 5. **Sigmoid Kernel** (with gamma=0.1 and coefficient=0) between `X` and `Y`. 6. **RBF Kernel** (with gamma=0.5) between `X` and `Y`. 7. **Chi-squared Kernel** (with gamma=0.5) between `X` and `Y`. # Input: - `X`: A 2D numpy array of shape `(n_samples_X, n_features)`. - `Y`: A 2D numpy array of shape `(n_samples_Y, n_features)`. # Output: - A dictionary with the following structure: ```python { \\"euclidean_distance\\": np.ndarray, \\"cosine_similarity\\": np.ndarray, \\"linear_kernel\\": np.ndarray, \\"polynomial_kernel\\": np.ndarray, \\"sigmoid_kernel\\": np.ndarray, \\"rbf_kernel\\": np.ndarray, \\"chi_squared_kernel\\": np.ndarray } ``` # Requirements: 1. Use appropriate functions from the `sklearn.metrics.pairwise` module. 2. Ensure that your solution is efficient and leverages vectorized operations within scikit-learn. 3. Handle any exceptions that might arise due to input data types or shapes. # Example: ```python import numpy as np # Sample Data X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) # Function Call results = compute_metrics_and_kernels(X, Y) # Expected Output (formats of numpy arrays) print(results[\\"euclidean_distance\\"]) # array of shape (n_samples_X, n_samples_Y) print(results[\\"cosine_similarity\\"]) # array of shape (n_samples_X, n_samples_Y) print(results[\\"linear_kernel\\"]) # array of shape (n_samples_X, n_samples_Y) print(results[\\"polynomial_kernel\\"]) # array of shape (n_samples_X, n_samples_Y) print(results[\\"sigmoid_kernel\\"]) # array of shape (n_samples_X, n_samples_Y) print(results[\\"rbf_kernel\\"]) # array of shape (n_samples_X, n_samples_Y) print(results[\\"chi_squared_kernel\\"]) # array of shape (n_samples_X, n_samples_Y) ```","solution":"import numpy as np from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity, linear_kernel, polynomial_kernel, sigmoid_kernel, rbf_kernel from sklearn.metrics.pairwise import chi2_kernel as sklearn_chi2_kernel def compute_metrics_and_kernels(X, Y): Computes various pairwise metrics and kernels between rows of matrices X and Y. Parameters: X (np.ndarray): 2D array of shape (n_samples_X, n_features) Y (np.ndarray): 2D array of shape (n_samples_Y, n_features) Returns: dict: A dictionary containing the computed metrics and kernels. results = { \\"euclidean_distance\\": euclidean_distances(X, Y), \\"cosine_similarity\\": cosine_similarity(X, Y), \\"linear_kernel\\": linear_kernel(X, Y), \\"polynomial_kernel\\": polynomial_kernel(X, Y, degree=3, coef0=1, gamma=1), \\"sigmoid_kernel\\": sigmoid_kernel(X, Y, gamma=0.1, coef0=0), \\"rbf_kernel\\": rbf_kernel(X, Y, gamma=0.5), \\"chi_squared_kernel\\": sklearn_chi2_kernel(X, Y, gamma=0.5) } return results"},{"question":"# Question You are provided with a dataset of passengers on the Titanic. Using the seaborn library, your task is to create several visualizations, each highlighting different aspects of the data. You will need to demonstrate your understanding of seaborn, particularly the `sns.boxplot()` function and its customization capabilities. Dataset Loading Use the seaborn library to load the Titanic dataset: ```python import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") ``` Requirements 1. **Simple Boxplot** - Create a horizontal boxplot of the passenger ages. ```python sns.boxplot(x=titanic[\\"age\\"]) ``` 2. **Categorical Grouping** - Create a vertical boxplot of passenger ages grouped by their class (First, Second, Third). ```python sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\") ``` 3. **Nested Grouping** - Create a vertical boxplot of passenger ages grouped by their class and further divided by their survival status (alive or not). ```python sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\") ``` 4. **Custom Boxplot** - Draw a boxplot for passenger ages grouped by deck, customize it to: - Remove the fill color. - Add a small gap between the boxes. - Use a narrower box width. ```python sns.boxplot(data=titanic, x=\\"age\\", y=\\"deck\\", fill=False, gap=0.1, width=0.5) ``` 5. **Whiskers Customization** - Create a vertical boxplot for passenger ages grouped by deck, with whiskers extending to the 0th and 100th percentiles of the data. ```python sns.boxplot(data=titanic, x=\\"age\\", y=\\"deck\\", whis=(0, 100)) ``` 6. **Fine-tuned Customization** - Draw a boxplot for the classes with the following customizations: - Notch enabled. - No caps on the whiskers. - Outliers marked with `x`. - Box face color semi-transparent (RGBA: 0.3, 0.5, 0.7, 0.5). - Red thick line for the median. ```python sns.boxplot( data=titanic, x=\\"age\\", y=\\"class\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (.3, .5, .7, .5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2}, ) ``` Instructions 1. Implement each of the requirements as specified above. 2. Ensure each plot is clearly labeled and contains any necessary legends. 3. Export the scripts and visualizations. ***Note:*** Each task solution should be implemented in separate code cells for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Simple Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"age\\"]) plt.title(\\"Simple Boxplot of Passenger Ages\\") plt.show() # 2. Categorical Grouping plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\") plt.title(\\"Boxplot of Passenger Ages Grouped by Class\\") plt.show() # 3. Nested Grouping plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\") plt.title(\\"Boxplot of Passenger Ages Grouped by Class and Survival Status\\") plt.show() # 4. Custom Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"age\\", y=\\"deck\\", color=\'gray\') plt.title(\\"Custom Boxplot of Passenger Ages Grouped by Deck\\") plt.show() # 5. Whiskers Customization plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"age\\", y=\\"deck\\", whis=(0, 100)) plt.title(\\"Boxplot of Passenger Ages Grouped by Deck with Custom Whiskers\\") plt.show() # 6. Fine-tuned Customization plt.figure(figsize=(10, 6)) sns.boxplot( data=titanic, x=\\"age\\", y=\\"class\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (.3, .5, .7, .5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2}, ) plt.title(\\"Fine-tuned Customization Boxplot of Passenger Ages Grouped by Class\\") plt.show()"},{"question":"# PyTorch Intermediate Representation (IR) Operator Implementation Objective: Implement a PyTorch function using Core Aten IR and Prims IR to show your understanding of these intermediate representations. Problem Statement: You are given two 2D tensors, `A` and `B`. Your task is to: 1. Implement matrix multiplication between `A` and `B` using Core Aten IR. 2. Implement element-wise addition of a scalar to a tensor using Prims IR. Requirements: 1. **Matrix Multiplication**: - Function signature: `def matmul_core_aten_ir(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:` - Use Core Aten IR functions only. - Do not use PyTorch\'s high-level `torch.matmul` or similar functions directly. 2. **Element-wise Scalar Addition**: - Function signature: `def add_scalar_prims_ir(tensor: torch.Tensor, scalar: float) -> torch.Tensor:` - Use Prims IR functions only. - Ensure the scalar is added correctly to each element of the tensor. Input: - `A` and `B` are 2D torch tensors for `matmul_core_aten_ir`. - `tensor` is a 2D torch tensor, and `scalar` is a floating-point number for `add_scalar_prims_ir`. Output: - The output of `matmul_core_aten_ir` is a 2D torch tensor resulting from the matrix multiplication of `A` and `B`. - The output of `add_scalar_prims_ir` is a 2D torch tensor with `scalar` added to each element of `tensor`. Constraints: - Assume that the input dimensions for `A` and `B` are compatible for matrix multiplication. - Ensure that the operations are implemented strictly using the specified IRs. Examples: ```python import torch # Example for matrix multiplication using Core Aten IR A = torch.tensor([[1, 2], [3, 4]]) B = torch.tensor([[5, 6], [7, 8]]) result = matmul_core_aten_ir(A, B) # Expected output: tensor([[19, 22], [43, 50]]) # Example for element-wise scalar addition using Prims IR tensor = torch.tensor([[1, 2], [3, 4]]) scalar = 5 result = add_scalar_prims_ir(tensor, scalar) # Expected output: tensor([[6, 7], [8, 9]]) ``` Notes: - The use of any high-level PyTorch functions for matrix operations or element-wise operations other than the specified IR functions is prohibited. - You may refer to the PyTorch documentation for Core Aten IR and Prims IR to ensure correct usage of IR functions in your implementation. Deliverables: - Implementation of `matmul_core_aten_ir`. - Implementation of `add_scalar_prims_ir`.","solution":"import torch def matmul_core_aten_ir(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Implement matrix multiplication using Core Aten IR. C = torch.zeros((A.size(0), B.size(1)), dtype=A.dtype, device=A.device) for i in range(A.size(0)): for j in range(B.size(1)): for k in range(A.size(1)): C[i, j] += A[i, k] * B[k, j] return C def add_scalar_prims_ir(tensor: torch.Tensor, scalar: float) -> torch.Tensor: Implement element-wise addition of a scalar to a tensor using Prims IR. result = torch.empty_like(tensor) it = tensor.numel() for i in range(it): result.view(-1)[i] = tensor.view(-1)[i] + scalar return result"},{"question":"You are given a MacOS system with a Metal Performance Shaders (MPS) enabled GPU and PyTorch properly installed. Your task is to implement a simple neural network model in PyTorch and demonstrate training it on the MPS device. Here\'s what you need to do: 1. **Define a Neural Network**: Implement a simple feed-forward neural network with one hidden layer using `torch.nn.Module`. 2. **Check MPS Availability**: Write a function to check if the MPS device is available and provide appropriate error messages if not. 3. **Prepare Dataset**: Create a synthetic dataset with random inputs and outputs using `torch`. 4. **Train the Model**: Train your neural network model on the synthetic dataset and ensure the training runs on the MPS device. 5. **Evaluate the Model**: Evaluate your model to ensure it produces an output. # Input - There are no specific inputs for this task as you need to create synthetic data. # Output - Print the model\'s performance metric (e.g., loss) after training. # Constraints - Your model should contain at least one hidden layer. - Ensure all tensors and the model are moved to the MPS device if available. # Hints - Use `torch.backends.mps.is_available()` to check MPS availability. - Use `torch.device(\\"mps\\")` to define the MPS device. # Example Code ```python import torch import torch.nn as nn import torch.optim as optim # Hint: Define a simple MLP model class SimpleMLP(nn.Module): def __init__(self): super(SimpleMLP, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Hint: Check if MPS is available and set the device def get_device(): if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): raise EnvironmentError(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: raise EnvironmentError(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return torch.device(\\"mps\\") # Hint: Create a synthetic dataset def create_synthetic_data(): x = torch.randn(100, 10) y = torch.randn(100, 1) return x, y # Main function if __name__ == \\"__main__\\": device = get_device() model = SimpleMLP().to(device) x, y = create_synthetic_data() x, y = x.to(device), y.to(device) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(100): # 100 epochs model.train() optimizer.zero_grad() outputs = model(x) loss = criterion(outputs, y) loss.backward() optimizer.step() print(f\\"Final loss: {loss.item()}\\") ```","solution":"import torch import torch.nn as nn import torch.optim as optim class SimpleMLP(nn.Module): A simple feed-forward neural network with one hidden layer. def __init__(self): super(SimpleMLP, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def get_device(): Checks if MPS (Metal Performance Shaders) device is available and returns it. Raises an error with a relevant message if the device is not available. if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): raise EnvironmentError(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: raise EnvironmentError(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return torch.device(\\"mps\\") def create_synthetic_data(): Creates a synthetic dataset with random inputs and outputs. x = torch.randn(100, 10) y = torch.randn(100, 1) return x, y def train_model(model, x, y, device, epochs=100): Trains the model on the given data and returns the final loss. criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(epochs): model.train() optimizer.zero_grad() outputs = model(x) loss = criterion(outputs, y) loss.backward() optimizer.step() return loss.item() if __name__ == \\"__main__\\": device = get_device() model = SimpleMLP().to(device) x, y = create_synthetic_data() x, y = x.to(device), y.to(device) final_loss = train_model(model, x, y, device) print(f\\"Final loss: {final_loss}\\")"},{"question":"# Email Parser and Sender **Objective:** Implement a function that reads a MIME email from a file, retrieves certain information from its headers, and then forwards the email to a different address, optionally adding a custom text attachment. **Requirements:** 1. **Function Name:** `forward_email` 2. **Parameters:** - `input_file` (str): Path to the input file containing the MIME email. - `from_address` (str): The email address from which the email should be forwarded. - `to_address` (str): The email address to which the email should be forwarded. - `smtp_server` (str, optional): The SMTP server to use for sending the email. Default: \'localhost\'. - `custom_text` (str, optional): Optional custom text to be added as a new attachment to the forwarded email. Default: None. 3. **Functionality:** - Read the MIME email from the `input_file`. - Extract and print the \'From\', \'To\', and \'Subject\' headers. - Create a new email message with the same content, but change the \'From\' header to `from_address` and the \'To\' header to `to_address`. - If `custom_text` is provided, attach this text as a new text/plain part. - Send the email using the SMTP server specified by `smtp_server`. 4. **Return Value:** - The function should return a boolean value indicating whether the email was sent successfully (`True`) or if there was an error (`False`). **Constraints:** - The function should handle potential errors gracefully, including file I/O issues and SMTP connection problems. - Ensure that the email forwarding maintains the integrity of the original message content, including any attachments. **Sample Usage:** ```python result = forward_email( input_file=\'original_email.eml\', from_address=\'your_email@example.com\', to_address=\'recipient@example.com\', smtp_server=\'smtp.example.com\', custom_text=\'Check out this additional information attached!\' ) print(f\\"Email sent successfully: {result}\\") ``` **Notes:** - The function should internally validate the email addresses. - You may assume that the SMTP server specified contains no authentication mechanism. - Be mindful of multipart messages when adding attachments.","solution":"import email import smtplib from email.policy import default from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.parser import BytesParser def forward_email(input_file, from_address, to_address, smtp_server=\'localhost\', custom_text=None): try: # Open and parse the email file with open(input_file, \'rb\') as f: original_email = BytesParser(policy=default).parse(f) # Extract original headers original_from = original_email[\'From\'] original_to = original_email[\'To\'] original_subject = original_email[\'Subject\'] print(f\\"Original From: {original_from}\\") print(f\\"Original To: {original_to}\\") print(f\\"Original Subject: {original_subject}\\") # Prepare the forwarded email new_email = MIMEMultipart() new_email[\'From\'] = from_address new_email[\'To\'] = to_address new_email[\'Subject\'] = original_email[\'Subject\'] # Add the original email contents to the new email for part in original_email.iter_parts(): new_email.attach(part) # Add custom text attachment if provided if custom_text: attachment = MIMEText(custom_text, \'plain\') new_email.attach(attachment) # Send the email with smtplib.SMTP(smtp_server) as server: server.send_message(new_email) return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Regex Pattern Matcher for E-mail Subject Lines You are tasked with writing a Python function that uses regular expressions to process and extract information from a list of e-mail subject lines. Each subject line follows a specific format and contains important information that needs to be extracted. The format of each subject line is as follows: ``` Subject Tag: [Tag] - Message ID: [ID] - Priority: [P] - Content: [Content] ``` Where: - `[Tag]` is a single word. - `[ID]` is a string of alphanumeric characters (both uppercase and lowercase) and hyphens. - `[P]` is a priority level that can either be `HIGH`, `MEDIUM`, or `LOW`. - `[Content]` is the actual message content, which can include spaces and any printable ASCII characters. **Function Specification:** Implement the function `extract_information(subject_lines: List[str]) -> List[Dict[str, str]]` that takes a list of e-mail subject lines as input and returns a list of dictionaries. Each dictionary contains the extracted components for a corresponding e-mail subject line with the following keys: `Tag`, `ID`, `Priority`, and `Content`. # Input: - `subject_lines`: a list of strings where each string is formatted as described above. # Output: - A list of dictionaries with the keys `Tag`, `ID`, `Priority`, and `Content`. Each dictionary corresponds to a processed subject line. # Constraints: 1. Each subject line will always follow the provided format. 2. The input list will contain between 1 and 1000 subject lines. 3. The lengths of `[Tag]`, `[ID]`, `[P]`, and `[Content]` are within reasonable limits that can fit within a typical e-mail subject line (255 characters max). # Example: ```python subject_lines = [ \\"Subject Tag: Update - Message ID: 123-ABC-456 - Priority: HIGH - Content: System update required\\", \\"Subject Tag: Alert - Message ID: XYZ-789 - Priority: MEDIUM - Content: Undefined behavior detected\\" ] output = extract_information(subject_lines) # Expected Output: # [ # { # \\"Tag\\": \\"Update\\", # \\"ID\\": \\"123-ABC-456\\", # \\"Priority\\": \\"HIGH\\", # \\"Content\\": \\"System update required\\" # }, # { # \\"Tag\\": \\"Alert\\", # \\"ID\\": \\"XYZ-789\\", # \\"Priority\\": \\"MEDIUM\\", # \\"Content\\": \\"Undefined behavior detected\\" # } # ] ``` # Requirements: 1. Use the `re` module to write the regular expression. 2. Ensure your solution handles each subject line independently. 3. Optimize the solution for readability and performance given the constraints. # Hints: - Utilize named groups to simplify extraction of each component. - Carefully handle the potential presence of special characters in the `[Content]` field. Good luck!","solution":"import re from typing import List, Dict def extract_information(subject_lines: List[str]) -> List[Dict[str, str]]: pattern = re.compile( r\\"Subject Tag: (?P<Tag>w+) - Message ID: (?P<ID>[w-]+) - Priority: (?P<Priority>HIGH|MEDIUM|LOW) - Content: (?P<Content>.+)\\" ) extracted_information = [] for line in subject_lines: match = pattern.match(line) if match: extracted_info = match.groupdict() extracted_information.append(extracted_info) return extracted_information"},{"question":"**Question: Analyzing Sales Data with pandas** You are provided with a CSV file containing sales data for an online store. The file, named `sales_data.csv`, contains the following columns: - `order_id`: Order ID for each sale. - `product_id`: ID of the products sold. - `quantity`: Number of units sold per product. - `price_per_unit`: Price per unit of the product. - `order_date`: Date when the order was placed (in the format YYYY-MM-DD). - `category`: Category of the product sold. - `customer_id`: ID of the customer who placed the order. Your task is to use pandas to analyze this sales data and complete the following tasks: 1. **Load the Data:** - Write a function to load the CSV file into a pandas DataFrame. ```python import pandas as pd def load_data(file_path: str) -> pd.DataFrame: Loads the sales data from a CSV file into a pandas DataFrame. Args: file_path (str): The path to the sales_data.csv file. Returns: pd.DataFrame: The sales data DataFrame. pass ``` 2. **Total Sales Calculation:** - Write a function to calculate the total sales (quantity * price_per_unit) for each order and add it as a new column named `total_sales`. ```python def calculate_total_sales(df: pd.DataFrame) -> pd.DataFrame: Calculates the total sales for each order and adds a new column \'total_sales\'. Args: df (pd.DataFrame): The sales data DataFrame. Returns: pd.DataFrame: DataFrame with the \'total_sales\' column added. pass ``` 3. **Sales By Category:** - Write a function to calculate total sales for each product category and return it as a new DataFrame with columns `category` and `total_sales`. ```python def total_sales_by_category(df: pd.DataFrame) -> pd.DataFrame: Calculates total sales for each product category. Args: df (pd.DataFrame): The sales data DataFrame. Returns: pd.DataFrame: DataFrame with \'category\' and \'total_sales\' columns. pass ``` 4. **Monthly Sales Trend:** - Write a function to calculate the monthly sales trend and return it as a DataFrame with columns `month` (in the format YYYY-MM) and `total_sales`. ```python def monthly_sales_trend(df: pd.DataFrame) -> pd.DataFrame: Calculates monthly sales trend. Args: df (pd.DataFrame): The sales data DataFrame. Returns: pd.DataFrame: DataFrame with \'month\' and \'total_sales\' columns. pass ``` 5. **Top Customers:** - Write a function to identify the top 5 customers based on total sales. Return their `customer_id` and corresponding `total_sales`. ```python def top_customers(df: pd.DataFrame) -> pd.DataFrame: Identifies the top 5 customers based on total sales. Args: df (pd.DataFrame): The sales data DataFrame. Returns: pd.DataFrame: DataFrame with \'customer_id\' and \'total_sales\' columns of the top 5 customers. pass ``` **Note:** Handle any missing data appropriately by filling or dropping them as you see fit for the analysis. **Constraints:** - You should not use loops; use pandas methods for all operations. - Ensure that your functions are efficient and handle large datasets gracefully. Provide the implementations of the above functions based on the provided file path `sales_data.csv`.","solution":"import pandas as pd def load_data(file_path: str) -> pd.DataFrame: Loads the sales data from a CSV file into a pandas DataFrame. Args: file_path (str): The path to the sales_data.csv file. Returns: pd.DataFrame: The sales data DataFrame. return pd.read_csv(file_path) def calculate_total_sales(df: pd.DataFrame) -> pd.DataFrame: Calculates the total sales for each order and adds a new column \'total_sales\'. Args: df (pd.DataFrame): The sales data DataFrame. Returns: pd.DataFrame: DataFrame with the \'total_sales\' column added. df[\'total_sales\'] = df[\'quantity\'] * df[\'price_per_unit\'] return df def total_sales_by_category(df: pd.DataFrame) -> pd.DataFrame: Calculates total sales for each product category. Args: df (pd.DataFrame): The sales data DataFrame. Returns: pd.DataFrame: DataFrame with \'category\' and \'total_sales\' columns. return df.groupby(\'category\')[\'total_sales\'].sum().reset_index() def monthly_sales_trend(df: pd.DataFrame) -> pd.DataFrame: Calculates monthly sales trend. Args: df (pd.DataFrame): The sales data DataFrame. Returns: pd.DataFrame: DataFrame with \'month\' and \'total_sales\' columns. df[\'month\'] = df[\'order_date\'].str[:7] return df.groupby(\'month\')[\'total_sales\'].sum().reset_index() def top_customers(df: pd.DataFrame) -> pd.DataFrame: Identifies the top 5 customers based on total sales. Args: df (pd.DataFrame): The sales data DataFrame. Returns: pd.DataFrame: DataFrame with \'customer_id\' and \'total_sales\' columns of the top 5 customers. top_customers_df = df.groupby(\'customer_id\')[\'total_sales\'].sum().reset_index() return top_customers_df.sort_values(by=\'total_sales\', ascending=False).head(5)"},{"question":"# Advanced Programming Project: Logging Configuration Objective In this project, you are required to create a Python program that demonstrates proficiency in configuring and managing the logging system for a complex application. You will need to use the `logging.config` module to set up your logging based on a configuration file. Task 1. **Create a Configuration File:** Create a logging configuration file in the INI format that includes: - A root logger that logs all messages. - At least one other logger that logs messages at the DEBUG level. - Multiple handlers including: - `StreamHandler` to log messages to the console. - `FileHandler` to log messages to a file. - `SMTPHandler` to send an email for ERROR level messages. - Appropriate formatters for each handler to include timestamps, log levels, and messages. 2. **Implement the Logger Setup:** Write a Python function named `setup_logging` that: - Reads and parses the configuration file. - Configures the logging module using `logging.config.fileConfig`. - Ensures that existing loggers are not disabled. 3. **Demonstrate Logging:** Implement a demonstration script that: - Calls `setup_logging`. - Logs messages at different levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). - Verifies that all loggers and handlers are functioning as expected. Example Configuration File (`logging_config.ini`): ``` [loggers] keys=root,example_logger [handlers] keys=consoleHandler,fileHandler,emailHandler [formatters] keys=simpleFormatter,detailedFormatter [logger_root] level=NOTSET handlers=consoleHandler [logger_example_logger] level=DEBUG handlers=consoleHandler,fileHandler,emailHandler qualname=example_logger [handler_consoleHandler] class=StreamHandler level=DEBUG formatter=simpleFormatter args=(sys.stdout,) [handler_fileHandler] class=FileHandler level=DEBUG formatter=detailedFormatter args=(\'example.log\', \'w\') [handler_emailHandler] class=handlers.SMTPHandler level=ERROR formatter=detailedFormatter args=(\'localhost\', \'from@example.com\', [\'admin@example.com\'], \'Error in logging system\') [formatter_simpleFormatter] format=%(asctime)s - %(message)s [formatter_detailedFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s ``` Requirements: - Your `setup_logging` function should be robust against errors in the configuration file. - The example script should be well-documented and demonstrate different logging scenarios. - Ensure that the email handler is only invoked for ERROR level messages. - Handle exceptions that could arise from invalid configurations. Constraints: - Only use the Python Standard Library. - The configuration file should be named `logging_config.ini`. Submission: - A Python script named `yourname_logging_demo.py` containing the implementation and the demonstration. - The `logging_config.ini` file. Evaluation Criteria: - Correctness and completeness of the configuration file. - Proper implementation of the `setup_logging` function. - Effectiveness and clarity of the demonstration script. - Code readability and use of best practices.","solution":"import logging.config def setup_logging(config_file=\'logging_config.ini\'): Set up logging configuration. :param config_file: Path to the logging configuration file. try: logging.config.fileConfig(config_file, disable_existing_loggers=False) print(\\"Logging configuration loaded successfully.\\") except Exception as e: print(f\\"Failed to load logging configuration: {e}\\")"},{"question":"# Question: Implement Dynamic Conditional Operations using `torch.cond` Objective You are tasked with implementing a PyTorch module that dynamically applies different operations to an input tensor based on a set of conditions using `torch.cond`. Requirements 1. Implement a PyTorch module `CustomCondModule` that: - Takes an input tensor `x`. - If the mean of `x` is greater than zero, applies a function that squares the tensor elements. - If the mean of `x` is less than or equal to zero, applies a function that returns the absolute value of the tensor elements. 2. Create a method to test your module with various inputs and demonstrate the dynamic behavior. Implementation Details - Define a class `CustomCondModule` inheriting from `torch.nn.Module`. - Implement the `forward` method using `torch.cond`: - `pred` should be a condition based on the mean value of `x`. - `true_fn` should square the tensor elements. - `false_fn` should compute the absolute value of the tensor elements. - Use the appropriate input and output formats as required by the `torch.cond` operator. - Ensure that your implementation works with any input tensor size and values. Constraints - Do **not** use any additional libraries except PyTorch. - Your solution must handle dynamic conditions as described using `torch.cond`. Example ```python import torch # Define the CustomCondModule class class CustomCondModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return x**2 def false_fn(x: torch.Tensor): return torch.abs(x) return torch.cond(x.mean() > 0, true_fn, false_fn, (x,)) # Instantiate the module model = CustomCondModule() # Test the module with different inputs inp1 = torch.tensor([-1.0, -2.0, -3.0]) inp2 = torch.tensor([1.0, 2.0, 3.0]) out1 = model(inp1) out2 = model(inp2) print(out1) # Expected output: tensor([1.0, 2.0, 3.0]) - computed from absolute value function print(out2) # Expected output: tensor([1.0, 4.0, 9.0]) - computed from square function ``` In this example, the output differs based on whether the mean of the input tensor is positive or non-positive, demonstrating the use of `torch.cond` for dynamic control flow.","solution":"import torch import torch.nn as nn class CustomCondModule(nn.Module): def __init__(self): super(CustomCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: if x.mean() > 0: return x ** 2 else: return torch.abs(x)"},{"question":"Thread Synchronization and Communication **Objective:** Create a multithreaded program in Python that demonstrates the use of threads, locks, and condition variables for inter-thread communication and synchronization. Your task is to simulate a simple producer-consumer problem where producers generate items and consumers consume them. **Problem Statement:** You need to implement a class `ThreadSafeQueue` that provides a thread-safe queue with a fixed size. The queue should support the following operations: - `enqueue(item)`: Add an item to the queue. If the queue is full, the calling thread should wait until space becomes available. - `dequeue()`: Remove and return an item from the queue. If the queue is empty, the calling thread should wait until an item is available. You also need to implement a `Producer` class and a `Consumer` class that use `ThreadSafeQueue` to produce and consume items, respectively. The producers should generate items and add them to the queue, and the consumers should remove items from the queue and process them. **Requirements:** 1. Implement the `ThreadSafeQueue` class with the following methods: - `__init__(self, max_size)`: Initialize the queue with a fixed size `max_size`. - `enqueue(self, item)`: Add an item to the queue. If the queue is full, the thread should wait until space becomes available. - `dequeue(self)`: Remove and return an item from the queue. If the queue is empty, the thread should wait until an item is available. 2. Use locks and condition variables to ensure thread safety and proper synchronization between producer and consumer threads. 3. Implement the `Producer` class with the following methods: - `__init__(self, queue, id)`: Initialize the producer with a reference to the queue and a unique identifier `id`. - `produce(self)`: Generate items and add them to the queue. The items can be simple integers or strings. 4. Implement the `Consumer` class with the following methods: - `__init__(self, queue, id)`: Initialize the consumer with a reference to the queue and a unique identifier `id`. - `consume(self)`: Remove items from the queue and process them. The processing can be simulated with a simple print statement. **Constraints:** - The queue size `max_size` is a positive integer greater than 0. - The program should correctly handle multiple producers and consumers running concurrently. **Example:** ```python import threading import time import random class ThreadSafeQueue: def __init__(self, max_size): self.queue = [] self.max_size = max_size self.lock = threading.Lock() self.not_empty = threading.Condition(self.lock) self.not_full = threading.Condition(self.lock) def enqueue(self, item): with self.lock: while len(self.queue) >= self.max_size: self.not_full.wait() self.queue.append(item) self.not_empty.notify() def dequeue(self): with self.lock: while len(self.queue) == 0: self.not_empty.wait() item = self.queue.pop(0) self.not_full.notify() return item class Producer(threading.Thread): def __init__(self, queue, id): threading.Thread.__init__(self) self.queue = queue self.id = id def run(self): while True: item = random.randint(1, 100) # Producing a random item self.queue.enqueue(item) print(f\'Producer {self.id} produced {item}\') time.sleep(random.uniform(0.1, 1)) # Simulating production time class Consumer(threading.Thread): def __init__(self, queue, id): threading.Thread.__init__(self) self.queue = queue self.id = id def run(self): while True: item = self.queue.dequeue() print(f\'Consumer {self.id} consumed {item}\') time.sleep(random.uniform(0.1, 1)) # Simulating consumption time if __name__ == \'__main__\': queue = ThreadSafeQueue(max_size=10) producers = [Producer(queue, i) for i in range(2)] consumers = [Consumer(queue, i) for i in range(2)] for producer in producers: producer.start() for consumer in consumers: consumer.start() for producer in producers: producer.join() for consumer in consumers: consumer.join() ``` **Submission:** Submit your implementation of the `ThreadSafeQueue`, `Producer`, and `Consumer` classes, along with a main function that demonstrates their usage as shown in the example.","solution":"import threading import time import random class ThreadSafeQueue: def __init__(self, max_size): self.queue = [] self.max_size = max_size self.lock = threading.Lock() self.not_empty = threading.Condition(self.lock) self.not_full = threading.Condition(self.lock) def enqueue(self, item): with self.lock: while len(self.queue) >= self.max_size: self.not_full.wait() self.queue.append(item) self.not_empty.notify() def dequeue(self): with self.lock: while len(self.queue) == 0: self.not_empty.wait() item = self.queue.pop(0) self.not_full.notify() return item class Producer(threading.Thread): def __init__(self, queue, id): threading.Thread.__init__(self) self.queue = queue self.id = id def run(self): while True: item = random.randint(1, 100) # Producing a random item self.queue.enqueue(item) print(f\'Producer {self.id} produced {item}\') time.sleep(random.uniform(0.1, 1)) # Simulating production time class Consumer(threading.Thread): def __init__(self, queue, id): threading.Thread.__init__(self) self.queue = queue self.id = id def run(self): while True: item = self.queue.dequeue() print(f\'Consumer {self.id} consumed {item}\') time.sleep(random.uniform(0.1, 1)) # Simulating consumption time if __name__ == \'__main__\': # Example usage queue = ThreadSafeQueue(max_size=10) producers = [Producer(queue, i) for i in range(2)] consumers = [Consumer(queue, i) for i in range(2)] for producer in producers: producer.start() for consumer in consumers: consumer.start() for producer in producers: producer.join() for consumer in consumers: consumer.join()"},{"question":"You are tasked with creating a series of color palettes using the seaborn library to visualize various data scenarios. Utilize the `sns.light_palette` function in seaborn to accomplish the following tasks: 1. Create a sequential color palette that transitions from light gray to \\"blue\\". Display this palette as a colormap. 2. Create a sequential color palette with the specific hex color `#FF5733` and increase the number of colors to 10. Display this palette. 3. Create a color palette using the HUSL color system with parameters `(30, 80, 70)` and display it. 4. Create a continuous colormap transitioning from light gray to the color \\"purple\\" and display it. For each task, write a function that: - Takes the required inputs necessary to generate the palette. - Returns or displays a corresponding seaborn colormap or palette. You are required to write the following functions: ```python # Function for task 1 def create_blue_palette_as_cmap(): Creates and displays a sequential color palette transitioning from light gray to blue as a colormap. pass # Function for task 2 def create_hex_palette(hex_color=\'#FF5733\', num_colors=10): Creates and displays a sequential color palette transitioning from light gray to the specified hex color with num_colors colors. Parameters: hex_color (str): Hex code of the target color. num_colors (int): Number of colors in the palette. pass # Function for task 3 def create_husl_palette(): Creates and displays a color palette using the HUSL color system with specific parameters (30, 80, 70). Parameters: None pass # Function for task 4 def create_purple_continuous_cmap(): Creates and displays a continuous colormap transitioning from light gray to purple. Parameters: None pass ``` **Constraints:** - Use only the `sns.light_palette` function to create these palettes and colormaps. - Ensure all functions utilize seaborn\'s plotting capabilities to display the palettes and colormaps clearly. **Performance Requirements:** - The functions should execute efficiently using seaborn\'s internal optimizations. **Input and Output Formats:** - Inputs will be provided by the function parameters. - Outputs will be the displayed colormaps and palettes, each visualized using seaborn. Ensure your code is well-documented and follows best practices for readability and maintainability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_blue_palette_as_cmap(): Creates and displays a sequential color palette transitioning from light gray to blue as a colormap. palette = sns.light_palette(\\"blue\\", as_cmap=True) sns.heatmap([[0, 1], [2, 3]], cmap=palette) plt.show() def create_hex_palette(hex_color=\'#FF5733\', num_colors=10): Creates and displays a sequential color palette transitioning from light gray to the specified hex color with num_colors colors. Parameters: hex_color (str): Hex code of the target color. num_colors (int): Number of colors in the palette. palette = sns.light_palette(hex_color, n_colors=num_colors) sns.palplot(palette) plt.show() def create_husl_palette(): Creates and displays a color palette using the HUSL color system with specific parameters (30, 80, 70). Parameters: None palette = sns.light_palette(sns.husl_palette(1, h=30, s=80, l=70)[0]) sns.palplot(palette) plt.show() def create_purple_continuous_cmap(): Creates and displays a continuous colormap transitioning from light gray to purple. Parameters: None palette = sns.light_palette(\\"purple\\", as_cmap=True) sns.heatmap([[0, 1], [2, 3]], cmap=palette) plt.show()"},{"question":"# XML Parsing with Custom ContentHandler You are required to write a Python script that uses the `xml.sax` package to parse an XML document and extract specific information using a custom ContentHandler. Task Write a Python function called `extract_item_details(xml_string)` that parses the provided XML string and extracts specific details from it. The XML string will represent a list of items, and each item will contain details like `id`, `name`, and `price`. The function should return a list of dictionaries, where each dictionary contains the `id`, `name`, and `price` of an item. Input - `xml_string` (str): An XML string representation of a list of items. Output - List[Dict[str, str]]: A list of dictionaries, where each dictionary contains the keys `id`, `name`, and `price`, with their corresponding values extracted from the XML string. Example ```python xml_string = \'\'\' <items> <item> <id>1</id> <name>Item One</name> <price>9.99</price> </item> <item> <id>2</id> <name>Item Two</name> <price>19.99</price> </item> </items> \'\'\' print(extract_item_details(xml_string)) ``` **Output:** ```python [ {\'id\': \'1\', \'name\': \'Item One\', \'price\': \'9.99\'}, {\'id\': \'2\', \'name\': \'Item Two\', \'price\': \'19.99\'} ] ``` Requirements 1. Define a custom ContentHandler that overrides the necessary methods to capture the required information. 2. Use `xml.sax.parseString` to parse the provided XML string and utilize your custom ContentHandler. 3. Ensure that the resulting list of dictionaries accurately captures the details of each `item`. Constraints - Assume that all XML strings provided to your function are well-formed. - Each item will always contain `id`, `name`, and `price` elements. Notes - Pay attention to element nesting and be sure to capture the text content between tags accurately. - Handle any necessary type conversions where appropriate.","solution":"import xml.sax from xml.sax.handler import ContentHandler class ItemContentHandler(ContentHandler): def __init__(self): super().__init__() self.items = [] self.current_item = {} self.current_element = \\"\\" def startElement(self, name, attrs): self.current_element = name if name == \\"item\\": self.current_item = {} def endElement(self, name): if name == \\"item\\": self.items.append(self.current_item) self.current_element = \\"\\" def characters(self, content): if self.current_element in [\\"id\\", \\"name\\", \\"price\\"]: self.current_item[self.current_element] = content.strip() def extract_item_details(xml_string): handler = ItemContentHandler() xml.sax.parseString(xml_string, handler) return handler.items"},{"question":"Title: Manipulate and Serialize Plist Data **Problem Statement:** You are given a binary plist file that contains configuration data for an application. The configuration data is structured as a dictionary and includes nested dictionaries, lists, and various data types (strings, integers, floats, booleans, and datetime objects). Your task is to write a function `update_plist(in_file: str, out_file: str)` that performs the following operations: 1. Reads the binary plist file from the specified input file path (`in_file`). 2. Updates the configuration data by applying the following changes: - Add a new key-value pair: `\\"newKey\\": \\"newValue\\"`. - Increment all integer values by 1. - Append the string `\\"Updated\\"` to all string values. 3. Writes the updated configuration data to a new binary plist file at the specified output file path (`out_file`). **Function Signature:** ```python def update_plist(in_file: str, out_file: str) -> None: ``` **Input:** - `in_file` (str): The file path to the input binary plist file. - `out_file` (str): The file path where the modified binary plist file should be saved. **Output:** - No return value. The function should create a new binary plist file at the specified output path. **Constraints:** - Assume the input plist file is always in binary format and well-formed. - Always use binary format for the output plist file. - The nested dictionaries can have varying levels of depth. - The input file path and output file path are guaranteed to be valid. **Examples:** Suppose `input.plist` contains the following configuration data: ```python { \\"config\\": { \\"version\\": 1, \\"name\\": \\"example\\", \\"enabled\\": True, \\"settings\\": { \\"retry\\": 3, \\"timeout\\": 30.5, \\"servers\\": [\\"server1\\", \\"server2\\"] } }, \\"updated_at\\": datetime.datetime(2023, 10, 1) } ``` After running `update_plist(\'input.plist\', \'output.plist\')`, the `output.plist` should contain: ```python { \\"config\\": { \\"version\\": 2, \\"name\\": \\"exampleUpdated\\", \\"enabled\\": True, \\"settings\\": { \\"retry\\": 4, \\"timeout\\": 30.5, \\"servers\\": [\\"server1Updated\\", \\"server2Updated\\"] } }, \\"updated_at\\": datetime.datetime(2023, 10, 1), \\"newKey\\": \\"newValue\\" } ``` **Note:** - Use the `plistlib` module for reading and writing the plist data. - Ensure you handle all specified data types when updating the integer values and appending to string values, including nested dictionaries.","solution":"import plistlib def update_plist(in_file: str, out_file: str) -> None: with open(in_file, \'rb\') as f: data = plistlib.load(f) def update_dict(d): for key, value in d.items(): if isinstance(value, int): d[key] = value + 1 elif isinstance(value, str): d[key] = value + \\"Updated\\" elif isinstance(value, dict): update_dict(value) elif isinstance(value, list): update_list(value) def update_list(l): for i in range(len(l)): if isinstance(l[i], int): l[i] = l[i] + 1 elif isinstance(l[i], str): l[i] = l[i] + \\"Updated\\" elif isinstance(l[i], dict): update_dict(l[i]) if isinstance(data, dict): update_dict(data) data[\\"newKey\\"] = \\"newValue\\" with open(out_file, \'wb\') as f: plistlib.dump(data, f)"},{"question":"Python Coding Assessment: Mocking with `unittest.mock` # Objective: Assess your understanding of the `unittest.mock` library to effectively mock dependencies and assert calls within Python unit tests. # Problem Statement: You are tasked with writing a Python function and corresponding unit tests using the `unittest.mock` library. The function, `process_data`, interacts with an external API to fetch user information and perform some processing. In the unit tests, you must mock the API calls and ensure the functionality and interactions are as expected. # Function Specification: ```python def process_data(api_client, user_id): Fetches user data from an API client, processes it, and returns the processed data. Args: api_client: An instance of the API client with a method `get_user_data(user_id)` that returns user data. user_id: The ID of the user to fetch data for. Returns: A dictionary containing processed user data. user_data = api_client.get_user_data(user_id) processed_data = { \\"id\\": user_data[\\"id\\"], \\"name\\": user_data[\\"name\\"].upper(), # Convert the name to uppercase \\"email\\": user_data[\\"email\\"], \\"processed\\": True } return processed_data ``` # Requirements: 1. **Mock the API Call**: Use `unittest.mock` to mock the `get_user_data` method of the `api_client` to return predefined user data. 2. **Assertions**: Assert that the `get_user_data` method is called with the correct `user_id`. 3. **Test Cases**: Write multiple test cases to cover: - The normal case where the API returns valid user data. - Edge cases (e.g., when the user data is missing fields). 4. **Error Handling**: Ensure that your function handles and tests scenarios where the API raises an exception (e.g., `ValueError` if the user doesn\'t exist). # Constraints: - Assume the `api_client` always has a `get_user_data` method. - You are not required to implement the actual API client; only the function and its tests. # Performance Requirements: Your tests should be efficient and should not make any actual network calls or depend on external APIs. # Examples: ```python # Example user data returned by the API mock_user_data = { \\"id\\": 123, \\"name\\": \\"John Doe\\", \\"email\\": \\"john.doe@example.com\\" } # Example processed data expected_processed_data = { \\"id\\": 123, \\"name\\": \\"JOHN DOE\\", \\"email\\": \\"john.doe@example.com\\", \\"processed\\": True } # Asserting that the function returns the expected processed data assert process_data(mock_api_client, 123) == expected_processed_data # Example of mocking the API call and checking interactions mock_api_client.get_user_data.assert_called_with(123) ``` # Submission: Submit the above function and a corresponding set of unit tests utilizing `unittest.mock`.","solution":"def process_data(api_client, user_id): Fetches user data from an API client, processes it, and returns the processed data. Args: api_client: An instance of the API client with a method `get_user_data(user_id)` that returns user data. user_id: The ID of the user to fetch data for. Returns: A dictionary containing processed user data. try: user_data = api_client.get_user_data(user_id) processed_data = { \\"id\\": user_data.get(\\"id\\"), \\"name\\": user_data.get(\\"name\\", \\"\\").upper(), # Convert the name to uppercase \\"email\\": user_data.get(\\"email\\"), \\"processed\\": True } return processed_data except Exception as e: raise ValueError(f\\"Failed to process data for user_id {user_id}: {e}\\")"},{"question":"**Question: Implement a Custom Rendezvous Method for PyTorch Distributed Elastic** In the context of Distributed PyTorch, a rendezvous handler is crucial for synchronizing different nodes and ensuring they can communicate properly. Implementing a custom rendezvous method will demonstrate your understanding of distributed systems and fault tolerance within the PyTorch framework. # Task You are required to implement a custom rendezvous handler for PyTorch Distributed Elastic. This handler will be used to synchronize nodes in a distributed training setup. Your task is to create a basic rendezvous handler that: 1. Supports node registration and de-registration. 2. Provides a mechanism for nodes to wait until the required number of nodes have joined (barrier synchronization). 3. Handles node restarts and ensures that the entire system can recover gracefully. # Requirements 1. **Node Registration:** - Implement a function to register a node. Each node should provide a unique identifier. ```python def register_node(self, node_id: str) -> None: pass ``` 2. **Node De-registration:** - Implement a function to de-register a node, which removes it from the list of active nodes. ```python def deregister_node(self, node_id: str) -> None: pass ``` 3. **Barrier Synchronization:** - Implement a function to wait until a specified number of nodes have registered. This function should block until the required number of nodes have joined. ```python def wait_for_nodes(self, num_nodes: int, timeout: Optional[float] = None) -> bool: pass ``` 4. **Fault Tolerance:** - Ensure your rendezvous method handles node restarts and failures. When a node fails or restarts, it should re-register, and the system should adjust accordingly without crashing. ```python def handle_node_failure(self, node_id: str) -> None: pass ``` # Input Format - Nodes are identified using unique string IDs. - The number of required nodes is an integer. - Functions may take additional parameters as required. # Output Format - The functions do not return any values but should internally manage the state of the nodes using suitable data structures. # Constraints 1. Use thread-safe mechanisms to ensure that the operations are safe in a distributed environment. 2. The wait_for_nodes function should handle timeouts gracefully and return `False` if the required number of nodes have not joined within the specified timeout. 3. Ensure no node gets permanently stuck waiting due to a dead node or other issues. # Example Usage ```python rendezvous = CustomRendezvous() rendezvous.register_node(\'node1\') rendezvous.register_node(\'node2\') # Simulate more node registrations assert rendezvous.wait_for_nodes(5, timeout=10.0) == True rendezvous.handle_node_failure(\'node2\') # Node2 recount and re-register rendezvous.register_node(\'node2\') assert rendezvous.wait_for_nodes(5, timeout=10.0) == True ``` # Performance Ensure that your implementation can handle up to 1000 nodes with minimal overhead. **Note:** This problem assumes that a basic understanding of threading or asynchronous programming in Python and a familiarity with PyTorch\'s distributed training concepts are in place. Implement your `CustomRendezvous` class to complete this task.","solution":"import threading from typing import Optional class CustomRendezvous: def __init__(self): self.nodes = set() self.lock = threading.Lock() self.condition = threading.Condition(self.lock) def register_node(self, node_id: str) -> None: with self.lock: self.nodes.add(node_id) self.condition.notify_all() def deregister_node(self, node_id: str) -> None: with self.lock: if node_id in self.nodes: self.nodes.remove(node_id) self.condition.notify_all() def wait_for_nodes(self, num_nodes: int, timeout: Optional[float] = None) -> bool: with self.lock: all_nodes_registered = self.condition.wait_for(lambda: len(self.nodes) >= num_nodes, timeout) return all_nodes_registered def handle_node_failure(self, node_id: str) -> None: self.deregister_node(node_id) # Additional logic for handling node failures can be added here"},{"question":"Design a function to simulate the parsing of email messages that: 1. Identifies and raises specific exceptions based on conditions. 2. Records defects found during parsing into a list. # Function Signature ```python def parse_email_message(message: str) -> list: pass ``` # Inputs - `message` (str): A string containing the raw email message. # Outputs - `defects` (list): A list of strings representing the defects found during parsing. # Requirements 1. The function should: - Raise an `email.errors.HeaderParseError` if the message starts with a line without leading whitespace but does not contain a colon (i.e., not a valid header). - Raise an `email.errors.MultipartConversionError` if a message claims to be a multipart but has no boundary parameter. 2. Detect the following defects and add them to the `defects` list: - `NoBoundaryInMultipartDefect` if a multipart message has no boundary parameter. - `FirstHeaderLineIsContinuationDefect` if the first header line is a continuation line. - `MissingHeaderBodySeparatorDefect` if a line found while parsing headers has no leading whitespace and contains no \':\'. # Example ```python message = MIME-Version: 1.0 Content-Type: multipart/mixed; --001a1140ba2c11e3e05 Content-Type: text/plain; charset=\\"UTF-8\\" Hello! --001a1140ba2c11e3e05-- ``` - If this is the email message that get parsed, the function should raise an `email.errors.MultipartConversionError` and add `NoBoundaryInMultipartDefect` to the `defects` list because the content type is `multipart/mixed` but there is no boundary parameter. # Constraints - The function should efficiently handle email messages up to 1MB in size. - Use only standard libraries available in Python 3.10.","solution":"import email from email import errors from email.policy import default def parse_email_message(message: str) -> list: defects = [] header_lines = message.split(\\"n\\") # Check for header parse error if header_lines: if header_lines[0] and not header_lines[0][0].isspace() and \':\' not in header_lines[0]: raise errors.HeaderParseError(\\"The first line is not a valid header.\\") # Check for first header line is a continuation line if header_lines and header_lines[0].startswith(\' \'): defects.append(\\"FirstHeaderLineIsContinuationDefect\\") mime_email = email.message_from_string(message, policy=default) content_type = mime_email.get_content_type() if content_type.startswith(\'multipart/\') and not mime_email.get_boundary(): raise errors.MultipartConversionError(\\"No boundary parameter in a multipart message.\\") if content_type.startswith(\'multipart/\') and not mime_email.get_boundary(): defects.append(\\"NoBoundaryInMultipartDefect\\") # Check for missing header body separator defect in_headers = True for line in message.split(\'n\'): if in_headers: if not line.strip(): in_headers = False continue if not line.startswith(\' \') and \':\' not in line: defects.append(\\"MissingHeaderBodySeparatorDefect\\") else: break return defects"},{"question":"# Custom Dataset and DataLoader with Multi-process Loading Objective: To assess your understanding and ability to work with PyTorch\'s `torch.utils.data` module, including creating custom datasets, using DataLoader for batching, and implementing multi-process data loading. Problem Statement: You are required to implement a custom data loading solution in PyTorch. This includes: 1. Defining a custom `MapStyleDataset` class that simulates loading data from a list of file paths. 2. Implementing a DataLoader with multi-process functionality. 3. Using custom collation and memory pinning to optimize data loading. Requirements: 1. **Custom Dataset**: - Create a `CustomMapDataset` class that inherits from `torch.utils.data.Dataset`. - The dataset should accept a list of file paths (simulated as string data) and return each file\'s content (which will be simulated as sequential numbers) when indexed. 2. **Custom Collate Function**: - Implement a custom `collate_fn` that, for each batch, converts the data to PyTorch tensors and pads sequences to the maximum length in the batch. 3. **DataLoader Configuration**: - Set up DataLoader to use multi-process loading with 4 worker processes. - Enable memory pinning for fetched data. Input and Output Formats: 1. `CustomMapDataset` Class: - **Input**: List of file paths (e.g., `[\\"file1\\", \\"file2\\", ...]`). - **Output**: Each index returns a list of sequential numbers simulating the file content (e.g., `[0, 1, 2, 3, ...]`). 2. `Custom Collate Function`: - **Input**: List of lists with sequential numbers. - **Output**: Padded tensor of shape (batch_size, max_sequence_length). 3. **DataLoader**: - Configure using the custom dataset, custom collate function, num_workers=4, and pin_memory=True. - When iterated, should yield batches of padded tensors. Constraints: - Minimum file path length is 5 characters. - Maximum sequence length in simulation is 100. Example: ```python import torch from torch.utils.data import Dataset, DataLoader import numpy as np class CustomMapDataset(Dataset): def __init__(self, file_paths): self.file_paths = file_paths def __getitem__(self, index): # Simulate file content as sequential numbers data_length = min(len(self.file_paths[index]), 100) # Constrained max length return list(range(data_length)) def __len__(self): return len(self.file_paths) def custom_collate_fn(batch): max_len = max(len(item) for item in batch) padded_batch = [item + [0] * (max_len - len(item)) for item in batch] return torch.tensor(padded_batch, dtype=torch.int64) # Create the dataset and DataLoader file_paths = [\\"file1\\", \\"file2_longer\\", \\"file3\\"] dataset = CustomMapDataset(file_paths) dataloader = DataLoader(dataset, batch_size=2, collate_fn=custom_collate_fn, num_workers=4, pin_memory=True) # Example iteration for batch in dataloader: print(batch) ``` This should output padded tensors of the sequences. Notes: - Handle any edge cases where file path length is less than 5 characters (raise ValueError). - Ensure that the DataLoader uses the custom dataset and collate function correctly in multi-process loading mode. Submission: Please submit your implementation of the `CustomMapDataset` class, the custom collate function, and the DataLoader setup code. Also, include a test case to demonstrate that your DataLoader works as expected.","solution":"import torch from torch.utils.data import Dataset, DataLoader class CustomMapDataset(Dataset): def __init__(self, file_paths): for file_path in file_paths: if len(file_path) < 5: raise ValueError(f\\"File path \'{file_path}\' is too short, must be at least 5 characters.\\") self.file_paths = file_paths def __getitem__(self, index): data_length = min(len(self.file_paths[index]), 100) return list(range(data_length)) def __len__(self): return len(self.file_paths) def custom_collate_fn(batch): max_len = max(len(item) for item in batch) padded_batch = [item + [0] * (max_len - len(item)) for item in batch] return torch.tensor(padded_batch, dtype=torch.int64) # Example usage file_paths = [\\"file1\\", \\"file2_longer\\", \\"file3\\"] dataset = CustomMapDataset(file_paths) dataloader = DataLoader(dataset, batch_size=2, collate_fn=custom_collate_fn, num_workers=4, pin_memory=True)"},{"question":"# Python Profiling Assessment You are given a large dataset processing function which takes a significant amount of time to execute. Your task is to profile this function using the `cProfile` and `pstats` modules to find performance bottlenecks. Function to be Profiled ```python def process_data(data): processed_data = [] for record in data: # Simulate some complex processing for _ in range(1000): processed_record = record ** 2 processed_data.append(processed_record) return processed_data ``` # Task 1. **Write a function `profile_processing` that profiles `process_data` function execution.** - Function signature: `def profile_processing(data: list) -> None:` - Use `cProfile` to profile the `process_data` function. - Save the profiling results to a file named \\"profile_stats.prof\\". - Create a report of the ten functions that take the most cumulative time to execute, and print this report to the console. Constraints - `data` will be a list of integers. - Make sure the profiling does not significantly impact the readability and modularity of your code. # Example Usage ```python data = list(range(1, 10000)) profile_processing(data) ``` Expected console output should show a detailed report of the top ten functions by cumulative time. ```plaintext Tue Oct 12 06:47:41 2023 profile_stats.prof 19995 function calls (19995 primitive calls) in 0.056 seconds Ordered by: cumulative time List reduced from 45 to 10 due to restriction <10> ncalls tottime percall cumtime percall filename:lineno(function) 1 0.004 0.004 0.056 0.056 <ipython-input-1-db05f26a1e72>:1(process_data) 9999 0.040 0.000 0.040 0.000 <ipython-input-1-db05f26a1e72>:4(<listcomp>) 9999 0.012 0.000 0.012 0.000 <ipython-input-1-db05f26a1e72>:5(<listcomp>) ``` # Notes - Use appropriate methods and classes from the `cProfile` and `pstats` modules to perform the profiling. - Ensure to print only the top ten functions by cumulative time.","solution":"import cProfile import pstats def process_data(data): processed_data = [] for record in data: # Simulate some complex processing for _ in range(1000): processed_record = record ** 2 processed_data.append(processed_record) return processed_data def profile_processing(data): profiler = cProfile.Profile() profiler.enable() process_data(data) profiler.disable() profiler.dump_stats(\\"profile_stats.prof\\") with open(\\"profile_report.txt\\", \\"w\\") as f: stats = pstats.Stats(profiler, stream=f) stats.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(10) stats = pstats.Stats(\\"profile_stats.prof\\") stats.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(10)"},{"question":"Objective Write PyTorch code to demonstrate your understanding of tensor storage and advanced manipulations. Problem Statement 1. Create a tensor of shape `(3, 3)` filled with random values using PyTorch\'s `torch.rand` function. 2. Access its untyped storage and fill it with zeros. 3. Verify that the original tensor is modified accordingly. 4. Create another tensor view of the updated tensor by reshaping it to shape `(1, 9)`. 5. Show that both the original tensor and the new view share the same storage. 6. Add a non-zero gradient to the original tensor and demonstrate that the tensor\'s main storage and its gradient have different storages. 7. Verify access to these storages by the `data_ptr` method, ensuring that you can print the different storage details including main, view, and gradient. Input No input is required for this task. Expected Output - The updated tensor values and their storage details. - Verification print statements for shared storage between the original tensor and its view. - Details of the different storages for tensor data and gradient. Constraints - Do not use external libraries other than PyTorch. - Handle potential exceptions where applicable to ensure robust code. Requirements - Demonstrated use of `torch.Tensor.untyped_storage`. - Proper reshaping of the tensor to create a view with shared storage. - Explicit operations involving `data_ptr` to reveal underlying storage details. Example Output ```python Initial Tensor: [[a, b, c], [d, e, f], [g, h, i]] Updated Tensor with Zero Storage: [[0, 0, 0], [0, 0, 0], [0, 0, 0]] Reshaped Tensor (View): [[0, 0, 0, 0, 0, 0, 0, 0, 0]] Storage verified to be shared between the original tensor and its view. Original Tensor Storage Data Pointer: 140336614778440 View Tensor Storage Data Pointer: 140336614778440 Gradient Storage Data Pointer: 140336614779000 ``` Provide your implementation below: ```python # Sample implementation (do not include in the question, shown here for clarification) import torch # Step 1: Create a tensor t = torch.rand((3, 3)) # Step 2-3: Access its storage and fill it with zeros s = t.untyped_storage() s.fill_(0) # Verifying tensor is updated print(\\"Updated Tensor with Zero Storage:\\", t) # Step 4: Create a view of the updated tensor v = t.view((1, 9)) # Step 5: Verify shared storage assert t.untyped_storage().data_ptr() == v.untyped_storage().data_ptr() print(\\"Storage verified to be shared between the original tensor and its view.\\") # Step 6: Add gradient and verify different storages t.requires_grad_(True) t.grad = torch.rand((3, 3)) # Step 7: Print storage details print(\\"Original Tensor Storage Data Pointer:\\", t.untyped_storage().data_ptr()) print(\\"View Tensor Storage Data Pointer:\\", v.untyped_storage().data_ptr()) print(\\"Gradient Storage Data Pointer:\\", t.grad.untyped_storage().data_ptr()) ```","solution":"import torch def tensor_manipulations(): # Step 1: Create a tensor filled with random values t = torch.rand((3, 3)) # Step 2-3: Access its untyped storage and fill it with zeros s = t.untyped_storage() s.fill_(0) # Step 4: Create a tensor view by reshaping v = t.view((1, 9)) # Step 5: Verify shared storage between the original tensor and its view shared_storage_verified = t.untyped_storage().data_ptr() == v.untyped_storage().data_ptr() # Step 6: Add a non-zero gradient to the original tensor t.requires_grad_(True) t.grad = torch.rand((3, 3)) return { \\"initial_tensor\\": t, \\"updated_tensor\\": t, \\"reshaped_view\\": v, \\"shared_storage_verified\\": shared_storage_verified, \\"original_tensor_data_ptr\\": t.untyped_storage().data_ptr(), \\"view_tensor_data_ptr\\": v.untyped_storage().data_ptr(), \\"gradient_data_ptr\\": t.grad.untyped_storage().data_ptr() }"},{"question":"# Advanced Output Formatting and File Operations Problem Statement You are provided with a list of student records where each record is a dictionary containing the student’s name, their grades for three subjects (Math, Science, and Literature), and their overall grade. Your task is to: 1. Format and print these records in a tabular format. 2. Save these records to a JSON file. 3. Read the JSON file back into a list of dictionaries. 4. Format and print the records read from the JSON file to ensure they match the original input. Specifications **Input Format**: A list of dictionaries where each dictionary has the following keys: - `name`: string - `math`: integer - `science`: integer - `literature`: integer - `overall`: float Example: ```python students = [ {\\"name\\": \\"Alice\\", \\"math\\": 88, \\"science\\": 92, \\"literature\\": 85, \\"overall\\": 88.33}, {\\"name\\": \\"Bob\\", \\"math\\": 75, \\"science\\": 78, \\"literature\\": 80, \\"overall\\": 77.67} ] ``` **Output Format**: 1. **Formatted Table**: ``` ------------------------------------------------------- | Name | Math | Science | Literature | Overall Grade | ------------------------------------------------------- | Alice | 88 | 92 | 85 | 88.33 | | Bob | 75 | 78 | 80 | 77.67 | ------------------------------------------------------- ``` 2. **JSON File Contents**: - The contents of the JSON file should reflect the list of dictionaries exactly. 3. **Verification**: - Print the records read from the JSON file in the same tabular format to ensure data integrity. Constraints: - Use formatted string literals (f-strings) or the `str.format()` method for output formatting. - Utilize file operations to read and write the JSON file properly. - Ensure proper error handling for file operations. Function Signature: ```python def process_student_records(students: list): # Format and print the records # Save to JSON file # Read from JSON file and verify pass ``` **Implementation Details:** 1. **Formatting and Printing Records**: - Use formatted string literals (f-strings) or `str.format()` to create a neatly formatted table. 2. **Saving to JSON File**: - Use the `json.dump()` method to save the list of dictionaries to a JSON file. 3. **Reading from JSON File**: - Use the `json.load()` method to read the list of dictionaries from the JSON file. 4. **Printing Read Records**: - Format and print the records read from the JSON file to verify correctness. ```python import json def process_student_records(students: list): # File name for storing the records file_name = \'student_records.json\' # Step 1: Format and print the records header = f\\"| {\'Name\':<8} | {\'Math\':<5} | {\'Science\':<8} | {\'Literature\':<10} | {\'Overall Grade\':<13} |\\" separator = \'-\' * len(header) print(separator) print(header) print(separator) for student in students: row = f\\"| {student[\'name\']:<8} | {student[\'math\']:<5} | {student[\'science\']:<8} | {student[\'literature\']:<10} | {student[\'overall\']:<13.2f} |\\" print(row) print(separator) # Step 2: Save to JSON file with open(file_name, \'w\', encoding=\'utf-8\') as f: json.dump(students, f, ensure_ascii=False, indent=4) # Step 3: Read from JSON file with open(file_name, \'r\', encoding=\'utf-8\') as f: read_students = json.load(f) # Step 4: Format and print the records read from the JSON file to verify correctness print(\\"Verifying records read from the JSON file:\\") print(separator) print(header) print(separator) for student in read_students: row = f\\"| {student[\'name\']:<8} | {student[\'math\']:<5} | {student[\'science\']:<8} | {student[\'literature\']:<10} | {student[\'overall\']:<13.2f} |\\" print(row) print(separator) # Example usage students = [ {\\"name\\": \\"Alice\\", \\"math\\": 88, \\"science\\": 92, \\"literature\\": 85, \\"overall\\": 88.33}, {\\"name\\": \\"Bob\\", \\"math\\": 75, \\"science\\": 78, \\"literature\\": 80, \\"overall\\": 77.67} ] process_student_records(students) ``` **Note**: Your implementation should handle any potential errors that may arise during file operations, such as the file not being found or issues with reading/writing JSON data.","solution":"import json def process_student_records(students: list): # File name for storing the records file_name = \'student_records.json\' # Step 1: Format and print the records header = f\\"| {\'Name\':<8} | {\'Math\':<5} | {\'Science\':<8} | {\'Literature\':<10} | {\'Overall Grade\':<13} |\\" separator = \'-\' * len(header) print(separator) print(header) print(separator) for student in students: row = f\\"| {student[\'name\']:<8} | {student[\'math\']:<5} | {student[\'science\']:<8} | {student[\'literature\']:<10} | {student[\'overall\']:<13.2f} |\\" print(row) print(separator) # Step 2: Save to JSON file with open(file_name, \'w\', encoding=\'utf-8\') as f: json.dump(students, f, ensure_ascii=False, indent=4) # Step 3: Read from JSON file with open(file_name, \'r\', encoding=\'utf-8\') as f: read_students = json.load(f) # Step 4: Format and print the records read from the JSON file to verify correctness print(\\"Verifying records read from the JSON file:\\") print(separator) print(header) print(separator) for student in read_students: row = f\\"| {student[\'name\']:<8} | {student[\'math\']:<5} | {student[\'science\']:<8} | {student[\'literature\']:<10} | {student[\'overall\']:<13.2f} |\\" print(row) print(separator) # Example usage students = [ {\\"name\\": \\"Alice\\", \\"math\\": 88, \\"science\\": 92, \\"literature\\": 85, \\"overall\\": 88.33}, {\\"name\\": \\"Bob\\", \\"math\\": 75, \\"science\\": 78, \\"literature\\": 80, \\"overall\\": 77.67} ] process_student_records(students)"},{"question":"# Advanced Command-Line Argument Parsing with argparse **Objective**: Write a Python script using the `argparse` module to create a command-line utility that processes file paths and performs operations on them. **Requirements**: 1. **Positional Arguments**: - `input_file`: The path to the input file. This argument is mandatory. - `operation`: The operation to perform on the input file. It can be one of the following: `count_lines`, `count_words`, or `count_chars`. 2. **Optional Arguments**: - `-o` or `--output`: The path to output the results. If not provided, the results should be printed to the console. - `-v` or `--verbose`: This flag increases the verbosity of the program\'s output. Multiple `-v` flags should increase verbosity level (e.g., `-vv` for very verbose). 3. **Mutually Exclusive Flags**: - `--dry-run`: If specified, the script should simulate the operation without making any changes or creating output files. - `--force`: This flag, when given alongside `output`, should allow the script to overwrite existing files without warning. **Functionality**: Your script should: 1. Read the `input_file`. 2. Perform the specified `operation` on the file. Each operation should have the following behavior: - `count_lines`: Count the number of lines in the file. - `count_words`: Count the number of words in the file. - `count_chars`: Count the number of characters in the file. 3. Print the result to the console or write it to the specified `output` file, based on the provided options. 4. Respect verbosity levels to provide detailed information based on how many `-v` flags are provided. 5. Handle mutually exclusive options `--dry-run` and `--force` appropriately. **Performance Requirements**: - The script should handle large files efficiently. - The argument parsing should handle incorrect or conflicting arguments gracefully, providing useful error messages. **Example Command-Line Usage**: ```bash python3 file_processor.py input.txt count_words -o output.txt --verbose python3 file_processor.py input.txt count_lines -vv --dry-run python3 file_processor.py input.txt count_chars -o result.txt --force ``` **Implementation Template**: ```python import argparse def count_lines(file_path): with open(file_path, \'r\') as file: return len(file.readlines()) def count_words(file_path): with open(file_path, \'r\') as file: return len(file.read().split()) def count_chars(file_path): with open(file_path, \'r\') as file: return len(file.read()) def main(): parser = argparse.ArgumentParser(description=\\"Process a file to count lines, words, or characters.\\") # Positional arguments parser.add_argument(\\"input_file\\", type=str, help=\\"Path to the input file\\") parser.add_argument(\\"operation\\", type=str, choices=[\\"count_lines\\", \\"count_words\\", \\"count_chars\\"], help=\\"Operation to perform on the input file\\") # Optional arguments parser.add_argument(\\"-o\\", \\"--output\\", type=str, help=\\"Path to output the results\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"Increase output verbosity\\") # Mutually exclusive group group = parser.add_mutually_exclusive_group() group.add_argument(\\"--dry-run\\", action=\\"store_true\\", help=\\"Simulate the operation without making any changes\\") group.add_argument(\\"--force\\", action=\\"store_true\\", help=\\"Overwrite existing files without warning\\") args = parser.parse_args() if args.verbose >= 1: print(f\\"Processing file: {args.input_file}\\") print(f\\"Operation: {args.operation}\\") if args.verbose >= 2: print(f\\"Arguments: {args}\\") if args.operation == \\"count_lines\\": result = count_lines(args.input_file) elif args.operation == \\"count_words\\": result = count_words(args.input_file) elif args.operation == \\"count_chars\\": result = count_chars(args.input_file) if args.dry_run: print(f\\"Dry run - result: {result}\\") else: if args.output: if args.force or not os.path.exists(args.output): with open(args.output, \'w\') as file: file.write(str(result)) if args.verbose >= 1: print(f\\"Result written to {args.output}\\") else: print(f\\"Error: Output file {args.output} already exists. Use --force to overwrite.\\") else: print(result) if __name__ == \\"__main__\\": main() ``` **Constraints**: - The input file should exist and be readable. - The output file path, if provided, should not overwrite an existing file unless `--force` is specified. **Tips**: - Make use of `argparse` documentation for advanced features and error handling. - Ensure that your script is modular, and each function has a clear responsibility. **Evaluation**: Submissions will be evaluated based on: - Correctness: The script performs the specified tasks accurately. - Robustness: The script handles errors and edge cases gracefully. - Code Quality: The solution is readable, and well-commented and adheres to Python coding conventions.","solution":"import argparse import os def count_lines(file_path): with open(file_path, \'r\') as file: return len(file.readlines()) def count_words(file_path): with open(file_path, \'r\') as file: return len(file.read().split()) def count_chars(file_path): with open(file_path, \'r\') as file: return len(file.read()) def main(): parser = argparse.ArgumentParser(description=\\"Process a file to count lines, words, or characters.\\") # Positional arguments parser.add_argument(\\"input_file\\", type=str, help=\\"Path to the input file\\") parser.add_argument(\\"operation\\", type=str, choices=[\\"count_lines\\", \\"count_words\\", \\"count_chars\\"], help=\\"Operation to perform on the input file\\") # Optional arguments parser.add_argument(\\"-o\\", \\"--output\\", type=str, help=\\"Path to output the results\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"Increase output verbosity\\") # Mutually exclusive group group = parser.add_mutually_exclusive_group() group.add_argument(\\"--dry-run\\", action=\\"store_true\\", help=\\"Simulate the operation without making any changes\\") group.add_argument(\\"--force\\", action=\\"store_true\\", help=\\"Overwrite existing files without warning\\") args = parser.parse_args() if args.verbose >= 1: print(f\\"Processing file: {args.input_file}\\") print(f\\"Operation: {args.operation}\\") if args.verbose >= 2: print(f\\"Arguments: {args}\\") if args.operation == \\"count_lines\\": result = count_lines(args.input_file) elif args.operation == \\"count_words\\": result = count_words(args.input_file) elif args.operation == \\"count_chars\\": result = count_chars(args.input_file) if args.dry_run: print(f\\"Dry run - result: {result}\\") else: if args.output: if args.force or not os.path.exists(args.output): with open(args.output, \'w\') as file: file.write(str(result)) if args.verbose >= 1: print(f\\"Result written to {args.output}\\") else: print(f\\"Error: Output file {args.output} already exists. Use --force to overwrite.\\") else: print(result) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Coding Assessment **Objective:** Assess the student\'s capability to use Python\'s `urllib.request` module to create an HTTP client capable of sending customized HTTP requests and handling responses and errors efficiently. **Question:** You are required to implement a Python function `fetch_url_content` that fetches the content from a given URL with the following specifications: 1. **Inputs:** - `url` (string): The URL to fetch content from. - `method` (string): The HTTP method to use (`GET` or `POST`). - `data` (dictionary, optional): Data to be sent with the request (for POST requests only). - `headers` (dictionary, optional): Custom headers to be included in the request. - `timeout` (integer, optional): Timeout for the request in seconds. - `auth` (tuple, optional): A tuple of `(username, password)` for basic authentication. 2. **Outputs:** - Returns the content of the response if the request is successful. - Returns an error string if the request fails, formatted as `\\"Error <code>: <reason>\\"`. 3. **Constraints:** - The `url` string must be a valid URL. - The `method` must be either `\\"GET\\"` or `\\"POST\\"`. - The `timeout` value must be a positive integer. 4. **Performance Requirements:** - The function should handle network-related errors gracefully. - The function should respect the specified timeout period. - The function should authenticate using the provided credentials if `auth` is not `None`. **Function Signature:** ```python def fetch_url_content( url: str, method: str, data: dict = None, headers: dict = None, timeout: int = None, auth: tuple = None ) -> str: pass ``` **Example Usage:** ```python # Example input url = \\"http://example.com/api/data\\" method = \\"POST\\" data = {\'key1\': \'value1\', \'key2\': \'value2\'} headers = {\'User-Agent\': \'Mozilla/5.0\'} timeout = 10 auth = (\\"user\\", \\"pass\\") # Example function call response = fetch_url_content(url, method, data, headers, timeout, auth) # Expected output if the request is successful # \\"Response content as a string\\" # Expected output if the request fails (e.g., 404 Not Found or timeout) # \\"Error 404: Not Found\\" # \\"Error: <reason> (if URLError)\\" ``` # Additional Information: 1. **HTTP Methods:** - Use `GET` requests if `method` is `\\"GET\\"` and do not use the `data` parameter. - Use `POST` requests if `method` is `\\"POST\\"` and send the `data` parameter, properly encoded as bytes. 2. **Headers:** - Add any custom headers specified in the `headers` dictionary. 3. **Timeouts:** - Ensure the request respects the specified timeout period, if provided. 4. **Authentication:** - Utilize basic authentication using `HTTPBasicAuthHandler` if the `auth` parameter is provided. 5. **Error Handling:** - Catch and handle `URLError` and `HTTPError` exceptions. - Return error messages in the format `\\"Error <code>: <reason>\\"` for `HTTPError`. - Return a generic error message for `URLError`. Implement the function `fetch_url_content` based on the above specifications. Ensure your function is modular, handles different scenarios, and provides meaningful error messages.","solution":"import urllib.request import urllib.error import urllib.parse import base64 def fetch_url_content( url: str, method: str, data: dict = None, headers: dict = None, timeout: int = None, auth: tuple = None ) -> str: try: req_data = None if data is not None: req_data = urllib.parse.urlencode(data).encode() if headers is None: headers = {} if auth is not None: auth_str = f\\"{auth[0]}:{auth[1]}\\".encode() b64_auth_str = base64.b64encode(auth_str).decode(\\"utf-8\\") headers[\'Authorization\'] = f\'Basic {b64_auth_str}\' req = urllib.request.Request(url, data=req_data, headers=headers, method=method) response = urllib.request.urlopen(req, timeout=timeout) return response.read().decode() except urllib.error.HTTPError as e: return f\\"Error {e.code}: {e.reason}\\" except urllib.error.URLError as e: return f\\"Error: {e.reason}\\""},{"question":"**Problem Statement:** You are tasked with creating a system to encode and decode text using custom and standard codecs. The following operations must be supported: 1. **Encode and Decode Text**: Using any of the standard codecs provided by the `codecs` module, encode a given string into bytes and decode it back into a string. 2. **Custom Encoding and Decoding**: Implement a custom codec for a simple Caesar Cipher with a shift value of 3 for both encoding and decoding. The custom codec should support both function-based and stream-based encoding/decoding. 3. **Error Handling**: Implement a custom error handler that replaces any non-ASCII character with the string \\"uFFFD\\" during encoding. Implement the following functions: 1. `encode_text(text: str, encoding: str) -> bytes` - **Input**: A string `text` and the encoding `encoding` to use (e.g., \'utf-8\', \'latin-1\'). - **Output**: The encoded byte sequence. - **Constraint**: Use `codecs.encode()` to perform the encoding. 2. `decode_text(encoded_bytes: bytes, encoding: str) -> str` - **Input**: A byte sequence `encoded_bytes` and the encoding `encoding` used originally. - **Output**: The decoded string. - **Constraint**: Use `codecs.decode()` to perform the decoding. 3. `caesar_encode(text: str) -> str` - **Input**: A string `text`. - **Output**: The Caesar Cipher encoded string with a shift of 3. - **Constraint**: Implement this function without using any built-in encoding libraries. 4. `caesar_decode(text: str) -> str` - **Input**: A Caesar Cipher encoded string `text`. - **Output**: The decoded string. - **Constraint**: Implement this function without using any built-in decoding libraries. 5. `register_caesar_codec()` - **Functionality**: Register the custom Caesar Cipher codec with the Python codec registry using `codecs.register()`. - **Constraint**: Create a `CodecInfo` object for the Caesar Cipher and use `codecs.register()` to register the codec. 6. `custom_encode(text: str, encoding: str, errors: str = \'caesar_replace\') -> bytes` - **Input**: A string `text`, the encoding `encoding`, and the error handling scheme `errors`. - **Output**: The encoded byte sequence. - **Constraint**: Use the custom codec and custom error handler if specified; otherwise, use the standard ones. 7. `custom_error_handler(exception: Exception) -> (str, int)` - **Input**: An exception object `exception`. - **Output**: A tuple containing the replacement string and the position to continue encoding. - **Constraint**: For any exception, return `(\\"uFFFD\\", exception.start + 1)`. After implementing the above functions, write a main function to demonstrate the following: 1. Encoding and decoding a given string using \'utf-8\' and \'latin-1\'. 2. Encoding and decoding a string using the custom Caesar Cipher codec. 3. Encoding a string containing non-ASCII characters with the custom error handler. **Example:** ```python text = \\"Hello, World!\\" # 1. Standard Encoding and Decoding encoded_utf8 = encode_text(text, \'utf-8\') decoded_utf8 = decode_text(encoded_utf8, \'utf-8\') encoded_latin1 = encode_text(text, \'latin-1\') decoded_latin1 = decode_text(encoded_latin1, \'latin-1\') print(encoded_utf8, decoded_utf8) print(encoded_latin1, decoded_latin1) # 2. Custom Caesar Cipher Encoding and Decoding register_caesar_codec() caesar_encoded = caesar_encode(text) caesar_decoded = caesar_decode(caesar_encoded) print(caesar_encoded, caesar_decoded) # 3. Custom Encoding with Error Handling non_ascii_text = \\"Café\\" codecs.register_error(\'caesar_replace\', custom_error_handler) custom_encoded = custom_encode(non_ascii_text, \'ascii\', \'caesar_replace\') print(custom_encoded) ``` **Notes:** - Implement necessary checks for valid inputs. - Handle exceptions appropriately. - Ensure efficient and readable code structure.","solution":"import codecs # 1. Encode text function def encode_text(text, encoding): return codecs.encode(text, encoding, errors=\'replace\') # 2. Decode text function def decode_text(encoded_bytes, encoding): return codecs.decode(encoded_bytes, encoding, errors=\'replace\') # 3. Caesar Cipher encode def caesar_encode(text): shift = 3 encoded_chars = [] for char in text: if \'a\' <= char <= \'z\': encoded_chars.append(chr((ord(char) - ord(\'a\') + shift) % 26 + ord(\'a\'))) elif \'A\' <= char <= \'Z\': encoded_chars.append(chr((ord(char) - ord(\'A\') + shift) % 26 + ord(\'A\'))) else: encoded_chars.append(char) return \'\'.join(encoded_chars) # 4. Caesar Cipher decode def caesar_decode(encoded_text): shift = 3 decoded_chars = [] for char in encoded_text: if \'a\' <= char <= \'z\': decoded_chars.append(chr((ord(char) - ord(\'a\') - shift) % 26 + ord(\'a\'))) elif \'A\' <= char <= \'Z\': decoded_chars.append(chr((ord(char) - ord(\'A\') - shift) % 26 + ord(\'A\'))) else: decoded_chars.append(char) return \'\'.join(decoded_chars) # 5. Register caesar codec def caesar_codec(name): if name == \'caesar\': return codecs.CodecInfo(encode=caesar_encode, decode=caesar_decode) return None def register_caesar_codec(): codecs.register(caesar_codec) # 6. Custom encode function def custom_encode(text, encoding, errors=\'caesar_replace\'): return codecs.encode(text, encoding, errors) # 7. Custom error handler def custom_error_handler(exception): return (\'uFFFD\', exception.start + 1) # Main function for tests def main(): text = \\"Hello, World!\\" # 1. Standard Encoding and Decoding encoded_utf8 = encode_text(text, \'utf-8\') decoded_utf8 = decode_text(encoded_utf8, \'utf-8\') encoded_latin1 = encode_text(text, \'latin-1\') decoded_latin1 = decode_text(encoded_latin1, \'latin-1\') print(encoded_utf8, decoded_utf8) print(encoded_latin1, decoded_latin1) # 2. Custom Caesar Cipher Encoding and Decoding register_caesar_codec() caesar_encoded = caesar_encode(text) caesar_decoded = caesar_decode(caesar_encoded) print(caesar_encoded, caesar_decoded) # 3. Custom Encoding with Error Handling non_ascii_text = \\"Café\\" codecs.register_error(\'caesar_replace\', custom_error_handler) custom_encoded = custom_encode(non_ascii_text, \'ascii\', \'caesar_replace\') print(custom_encoded) if __name__ == \\"__main__\\": main()"},{"question":"Objective To assess the student\'s understanding of the seaborn.objects Plot class and its methods for creating and customizing visualizations with multiple variables. Task You are provided with a dataset called `penguins` from the seaborn library. Your task is to create a series of plots using the seaborn.objects Plot class as described below. Requirements 1. **Load the penguins dataset**: - Use the `load_dataset` function from seaborn to load the `penguins` dataset. 2. **Create a plot**: - Plot the `flipper_length_mm` against both `body_mass_g` and `bill_length_mm` using the `pair` method. - Customize the plot labels to use \\"Flipper Length (mm)\\", \\"Body Mass (g)\\", and \\"Bill Length (mm)\\". - Add dots to the plot. 3. **Create a pairwise relationship plot**: - Plot multiple pairwise relationships by passing lists to both `x` and `y`: - `x` variables: `body_mass_g`, `bill_length_mm` - `y` variables: `flipper_length_mm`, `bill_depth_mm` - Customize the labels of the plot appropriately. - Add dots to the plot. 4. **Combine pairing with faceting**: - Create a plot that pairs `flipper_length_mm` with `body_mass_g` and facets them by the variable `species`. - Add dots to the plot. 5. **Wrap the subplots**: - Create a plot that pairs `flipper_length_mm` with `body_mass_g`, `bill_length_mm`, and `bill_depth_mm`, and wraps the subplots into a 2-column grid. - Customize the labels of the plot appropriately. - Add dots to the plot. Constraints - For all plots, use the seaborn.objects Plot class and its methods. - Ensure your code is well-organized and makes use of seaborn\'s available functionalities effectively. Input - No input required. The dataset is loaded within the code. Output - The code should generate four visualizations as described in the requirements. Example ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Requirement 2: Plot flipper_length_mm against body_mass_g and bill_length_mm plot1 = ( so.Plot(penguins, y=\\"flipper_length_mm\\") .pair(x=[\\"body_mass_g\\", \\"bill_length_mm\\"]) .label(x0=\\"Body Mass (g)\\", x1=\\"Bill Length (mm)\\", y=\\"Flipper Length (mm)\\") .add(so.Dots()) ) # Requirement 3: Pairwise plot with multiple x and y plot2 = ( so.Plot(penguins) .pair(x=[\\"body_mass_g\\", \\"bill_length_mm\\"], y=[\\"flipper_length_mm\\", \\"bill_depth_mm\\"]) .label(x0=\\"Body Mass (g)\\", x1=\\"Bill Length (mm)\\", y0=\\"Flipper Length (mm)\\", y1=\\"Bill Depth (mm)\\") .add(so.Dots()) ) # Requirement 4: Pairing with faceting by species plot3 = ( so.Plot(penguins, x=\\"flipper_length_mm\\") .pair(y=[\\"body_mass_g\\"]) .facet(col=\\"species\\") .add(so.Dots()) ) # Requirement 5: Wrapping subplots plot4 = ( so.Plot(penguins, y=\\"flipper_length_mm\\") .pair(x=[\\"body_mass_g\\", \\"bill_length_mm\\", \\"bill_depth_mm\\"], wrap=2) .label(x0=\\"Body Mass (g)\\", x1=\\"Bill Length (mm)\\", x2=\\"Bill Depth (mm)\\", y=\\"Flipper Length (mm)\\") .add(so.Dots()) ) ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Requirement 2: Plot flipper_length_mm against body_mass_g and bill_length_mm def create_plot1(data): plot1 = ( so.Plot(data, y=\\"flipper_length_mm\\") .pair(x=[\\"body_mass_g\\", \\"bill_length_mm\\"]) .label(x0=\\"Body Mass (g)\\", x1=\\"Bill Length (mm)\\", y=\\"Flipper Length (mm)\\") .add(so.Dots()) ) return plot1 # Requirement 3: Pairwise plot with multiple x and y def create_plot2(data): plot2 = ( so.Plot(data) .pair(x=[\\"body_mass_g\\", \\"bill_length_mm\\"], y=[\\"flipper_length_mm\\", \\"bill_depth_mm\\"]) .label(x0=\\"Body Mass (g)\\", x1=\\"Bill Length (mm)\\", y0=\\"Flipper Length (mm)\\", y1=\\"Bill Depth (mm)\\") .add(so.Dots()) ) return plot2 # Requirement 4: Pairing with faceting by species def create_plot3(data): plot3 = ( so.Plot(data, x=\\"flipper_length_mm\\") .pair(y=[\\"body_mass_g\\"]) .facet(col=\\"species\\") .add(so.Dots()) ) return plot3 # Requirement 5: Wrapping subplots def create_plot4(data): plot4 = ( so.Plot(data, y=\\"flipper_length_mm\\") .pair(x=[\\"body_mass_g\\", \\"bill_length_mm\\", \\"bill_depth_mm\\"], wrap=2) .label(x0=\\"Body Mass (g)\\", x1=\\"Bill Length (mm)\\", x2=\\"Bill Depth (mm)\\", y=\\"Flipper Length (mm)\\") .add(so.Dots()) ) return plot4 # Ensure the functions create the plots plot1 = create_plot1(penguins) plot2 = create_plot2(penguins) plot3 = create_plot3(penguins) plot4 = create_plot4(penguins)"},{"question":"# Asynchronous Task Scheduler **Objective:** Implement a custom asynchronous task scheduler using `asyncio.Queue` that manages tasks based on priority. Your implementation should handle tasks added to the queue, ensure tasks are processed by workers concurrently, and handle exceptions if the queue is empty or full. **Requirements:** 1. Implement a class named `TaskScheduler`: - It should include an `asyncio.PriorityQueue` to manage tasks. - It should have methods to add tasks to the queue and process tasks concurrently using worker coroutines. - Tasks should be tuples of the form `(priority_number, task_function)` where `priority_number` determines the order in which tasks are processed (lower numbers indicate higher priority). 2. Implement the following methods in `TaskScheduler`: - `add_task(priority: int, task: callable)`: Adds a task to the queue with the specified priority. - `start_workers(worker_count: int)`: Starts the specified number of worker coroutines to process tasks from the queue. - `stop_workers()`: Stops all worker coroutines, ensuring they finish processing any current tasks. 3. Ensure that the task processing includes proper handling of the `asyncio.QueueEmpty` and `asyncio.QueueFull` exceptions. **Input:** - Tasks to be scheduled and processed are provided by calling `add_task(priority, task)` on a `TaskScheduler` instance. - The number of worker coroutines is specified in `start_workers(worker_count)`. **Output:** - Print statements included in task processing functions to indicate the start and end of processing each task. **Example:** ```python import asyncio async def example_task(task_id): print(f\\"Starting task {task_id}\\") await asyncio.sleep(1) print(f\\"Finished task {task_id}\\") # Implementation here... # Usage Example: async def main(): scheduler = TaskScheduler(maxsize=10) # Add tasks with different priorities scheduler.add_task(1, example_task(1)) scheduler.add_task(3, example_task(2)) scheduler.add_task(2, example_task(3)) # Start workers scheduler.start_workers(3) # Allow some time for task processing await asyncio.sleep(5) # Stop workers scheduler.stop_workers() asyncio.run(main()) ``` **Constraints:** - Tasks are to be implemented as asynchronous functions. - The scheduler should handle up to `maxsize` tasks in the queue. - Workers should process tasks concurrently. **Notes:** - You can implement additional helper methods and classes if necessary. - Use appropriate exception handling for the queue operations.","solution":"import asyncio class TaskScheduler: def __init__(self, maxsize=10): self.queue = asyncio.PriorityQueue(maxsize=maxsize) self.workers = [] self._stop = False async def _worker(self): while not self._stop or not self.queue.empty(): try: priority, task = await self.queue.get() await task self.queue.task_done() except asyncio.QueueEmpty: await asyncio.sleep(0.1) def add_task(self, priority: int, task: asyncio.coroutine): self.queue.put_nowait((priority, task)) def start_workers(self, worker_count: int): for _ in range(worker_count): worker = asyncio.create_task(self._worker()) self.workers.append(worker) def stop_workers(self): self._stop = True async def wait_till_all_tasks_done(self): await self.queue.join()"},{"question":"Problem Statement # Overview As part of a newsreader client, you are required to interact with an NNTP server to retrieve and display recent articles from specified newsgroups. Your task is to implement a function that connects to an NNTP server, retrieves the subjects of the most recent articles from specified newsgroups, and displays them. # Function Signature ```python def fetch_recent_articles(host: str, port: int, newsgroups: list[str], num_articles: int) -> dict[str, list[str]]: pass ``` # Input - `host` (str): The hostname of the NNTP server. - `port` (int): The port number of the NNTP server. - `newsgroups` (list of str): A list of newsgroups to fetch articles from. - `num_articles` (int): The number of most recent articles to fetch from each newsgroup. # Output - Returns a dictionary where the keys are newsgroup names, and the values are lists of article subjects (str) representing the most recent articles. # Constraints - You can assume the NNTP server is always available and the provided newsgroups exist on the server. - The `num_articles` value will always be a positive integer. - Handle any necessary exceptions gracefully and ensure the connection to the NNTP server is properly closed. # Example ```python result = fetch_recent_articles( host=\'news.gmane.io\', port=119, newsgroups=[\'gmane.comp.python.committers\', \'gmane.comp.python.devel\'], num_articles=5 ) # Example expected output structure (the actual subjects will vary): # { # \'gmane.comp.python.committers\': [ # \'Re: Commit privileges for Łukasz Langa\', # \'Re: 3.2 alpha 2 freeze\', # ... # ], # \'gmane.comp.python.devel\': [ # \'Updated ssh key\', # \'Re: Updated ssh key\', # ... # ] # } ``` # Implementation Notes 1. Use the `nntplib.NNTP` class to establish a connection to the NNTP server. 2. Utilize the `group` method to select a newsgroup and retrieve article range information. 3. Use the `over` method to fetch the overview information for the specified number of most recent articles. 4. Parse and decode article subjects using the `nntplib.decode_header` function. 5. Handle exceptions such as `nntplib.NNTPError` to ensure robust error handling. 6. Ensure the NNTP connection is properly closed using the `quit` method or the `with` statement.","solution":"import nntplib def fetch_recent_articles(host: str, port: int, newsgroups: list[str], num_articles: int) -> dict[str, list[str]]: Connects to an NNTP server, retrieves subjects of the most recent articles from specified newsgroups, and returns them as a dictionary. Args: - host (str): The hostname of the NNTP server. - port (int): The port number of the NNTP server. - newsgroups (list of str): A list of newsgroups to fetch articles from. - num_articles (int): The number of most recent articles to fetch from each newsgroup. Returns: dict[str, list[str]]: A dictionary where keys are newsgroup names and values are lists of article subjects. recent_articles = {} try: with nntplib.NNTP(host, port) as server: for group in newsgroups: resp, count, first, last, name = server.group(group) start = max(int(last) - num_articles + 1, int(first)) resp, overviews = server.over((start, last)) subjects = [] for id, over in overviews: subject = over[\'subject\'] subjects.append(subject) recent_articles[group] = subjects except nntplib.NNTPError as e: print(f\\"An error occurred: {e}\\") return recent_articles"},{"question":"# Asynchronous Task Management with `asyncio` You have been tasked with developing an asynchronous Python application which simulates a manufacturing unit. This unit has workers that concurrently produce different parts and assemble them into a final product. Each worker operation has a specific duration, and the assembly step can only begin once all parts are produced. **Objective:** Implement a function `run_factory` using `asyncio` that starts worker tasks concurrently, waits for their completion, and then assembles the parts. Requirements: - The function should simulate three workers producing parts with different production times. - Schedule the worker coroutines to run concurrently. - After all parts are produced, assemble the parts. - Handle potential timeouts for the worker tasks. Function Details: ```python async def run_factory(worker_productions): Simulates an asynchronous manufacturing unit. Args: worker_productions (list of tuples): A list containing tuples where each tuple consists of (part_name (str), production_time (float), timeout (float)). Returns: str: A message indicating successful assembly or timeout. pass ``` Input Parameters: - `worker_productions`: A list of tuples where each tuple contains: - `part_name (str)`: The name of the part produced by a worker. - `production_time (float)`: The time taken by the worker to produce the part (in seconds). - `timeout (float)`: The maximum allowed production time for the worker (in seconds). Expected Behavior: - For each worker, create a coroutine that simulates part production by sleeping for the specified production time. - Use `asyncio.create_task` to schedule each worker\'s task concurrently. - Use `asyncio.wait_for` to apply the specified timeout to each worker\'s task. - Collect the results from all workers once they are done. - If any worker exceeds the timeout, handle this scenario appropriately. - After all parts are produced within their timeouts, return a success message. Constraints: - Ensure to catch any `asyncio.TimeoutError` exceptions if a worker exceeds the timeout. - Assume there are exactly three worker tasks to manage. Example Usage: ```python import asyncio async def run_factory(worker_productions): async def produce_part(part_name, production_time, timeout): try: await asyncio.sleep(production_time) # Simulate production time return f\\"{part_name} produced\\" except asyncio.TimeoutError: return f\\"{part_name} production timeout\\" tasks = [asyncio.create_task(asyncio.wait_for(produce_part(part_name, production_time, timeout), timeout=timeout)) for part_name, production_time, timeout in worker_productions] produced_parts = await asyncio.gather(*tasks, return_exceptions=True) for part in produced_parts: if isinstance(part, asyncio.TimeoutError): return \\"Production failed due to timeout\\" return \\"All parts produced. Assembling the final product...\\" # Simulating workers with various production times and timeouts worker_productions = [(\\"Part A\\", 2, 3), (\\"Part B\\", 3, 4), (\\"Part C\\", 1, 2)] print(asyncio.run(run_factory(worker_productions))) # Expected: \\"All parts produced. Assembling the final product...\\" worker_productions_timeout = [(\\"Part A\\", 5, 3), (\\"Part B\\", 2, 4), (\\"Part C\\", 1, 2)] print(asyncio.run(run_factory(worker_productions_timeout))) # Expected: \\"Production failed due to timeout\\" ``` Implement the `run_factory` function to handle the specified requirements and constraints.","solution":"import asyncio async def run_factory(worker_productions): async def produce_part(part_name, production_time, timeout): try: await asyncio.sleep(production_time) # Simulate production time return f\\"{part_name} produced\\" except asyncio.TimeoutError: return f\\"{part_name} production timeout\\" tasks = [asyncio.create_task(asyncio.wait_for(produce_part(part_name, production_time, timeout), timeout=timeout)) for part_name, production_time, timeout in worker_productions] produced_parts = await asyncio.gather(*tasks, return_exceptions=True) for part in produced_parts: if isinstance(part, asyncio.TimeoutError): return \\"Production failed due to timeout\\" return \\"All parts produced. Assembling the final product...\\""},{"question":"**PyTorch Distributed Computing: Worker Node Control** In this assessment, you are required to implement a function that manages worker nodes in a distributed PyTorch application. Your function will need to interact with worker nodes, handle their registration, and implement a simple task distribution mechanism. You will use the `torch.distributed.elastic.control_plane` helper functions and ensure your implementation is suitable for a distributed computing setup. # Function Signature ```python def manage_worker_nodes(worker_addresses: List[str], task_func: Callable[[int], Any]) -> Dict[int, Any]: pass ``` # Parameters - `worker_addresses` (List[str]): A list of addresses (as strings) for each worker node in the distributed setting. - `task_func` (Callable[[int], Any]): A callable task function that takes an integer (representing a worker ID) and performs a specific task. # Returns - `Dict[int, Any]`: A dictionary mapping worker IDs to their respective task results. # Constraints - Each worker is identified by an index based on their position in the `worker_addresses` list. - Assume that `torch.distributed.elastic.control_plane` provides necessary methods to communicate with workers (like `worker_main`). - Your function should handle possible exceptions during task execution and log relevant errors. # Performance Requirements - Your solution should be efficient in terms of network communication and task allocation. - Aim to minimize the overhead introduced by controlling and distributing tasks among workers. # Example Usage ```python from typing import List, Callable, Any, Dict def example_task(worker_id: int) -> str: return f\\"Processed by worker {worker_id}\\" worker_addresses = [\\"worker1.address\\", \\"worker2.address\\", \\"worker3.address\\"] results = manage_worker_nodes(worker_addresses, example_task) print(results) # Output example (depending on worker implementation and handling): # {0: \\"Processed by worker 0\\", 1: \\"Processed by worker 1\\", 2: \\"Processed by worker 2\\"} ``` # Notes - Your implementation should use proper synchronization mechanisms to handle distributed computing tasks. - You are free to mock the `torch.distributed.elastic.control_plane.worker_main` if needed for testing purposes.","solution":"from typing import List, Callable, Any, Dict import logging def manage_worker_nodes(worker_addresses: List[str], task_func: Callable[[int], Any]) -> Dict[int, Any]: Manages worker nodes in a distributed PyTorch application, distributing tasks to them. Parameters: - worker_addresses (List[str]): A list of addresses for each worker node. - task_func (Callable[[int], Any]): A task function that accepts a worker ID and returns the task result. Returns: - Dict[int, Any]: A dictionary mapping worker IDs to their respective task results. results = {} logger = logging.getLogger(__name__) for worker_id, worker_addr in enumerate(worker_addresses): try: # Simulate the task execution on each worker node result = task_func(worker_id) results[worker_id] = result except Exception as e: logger.error(f\\"Task execution failed for worker {worker_id} at address {worker_addr}: {e}\\") results[worker_id] = None # Record failure with a `None` value return results"},{"question":"**Problem Statement:** You are working on a data processing pipeline that requires efficient handling and transformation of a stream of data using functional programming concepts in Python. Your task is to implement a series of functions that will process a list of events. Each event is represented as a dictionary with the following keys: `id`, `type`, and `value`. The `pipeline` function should use this series of functions to transform and filter the list of events. 1. **Implement the following functions:** - **generate_values(events)**: This generator function yields the `value` field from each event in the provided list of `events`. ```python def generate_values(events): # Your implementation here ``` - **filter_events(events, event_type)**: This function returns an iterator that filters the events, only including those that match the specified `event_type`. ```python def filter_events(events, event_type): # Your implementation here ``` - **sum_values(values)**: This function takes an iterator of numeric values and returns the sum of these values using `functools.reduce`. ```python def sum_values(values): # Your implementation here ``` - **enumerate_events(events)**: This function returns a list of tuples, where each tuple contains the index and the event itself. ```python def enumerate_events(events): # Your implementation here ``` 2. **Using the functions above, implement the `pipeline(events, event_type)` function which processes the list of events as follows:** - Filters the events to include only those of type `event_type`. - Generates the values from the filtered events. - Sums the generated values. - Enumerates the filtered events to provide their indices. The result of the pipeline function should be a tuple containing: - The sum of the values from the filtered events. - A list of tuples with event indices and the events themselves. ```python def pipeline(events, event_type): # Your implementation here ``` **Input:** - `events`: A list of dictionaries, where each dictionary represents an event with keys `id` (int), `type` (str), and `value` (int or float). - `event_type`: A string representing the type of events to filter. **Output:** - A tuple containing: - The sum of the values from the filtered events. - A list of tuples with event indices and the events themselves. **Constraints:** - The `id` fields of the events are unique integers. - The `value` fields are numeric (either integer or float). - The pipeline should be able to handle an empty list of events efficiently. **Example:** ```python events = [ {\\"id\\": 1, \\"type\\": \\"click\\", \\"value\\": 3.5}, {\\"id\\": 2, \\"type\\": \\"click\\", \\"value\\": 2.0}, {\\"id\\": 3, \\"type\\": \\"view\\", \\"value\\": 1.0}, {\\"id\\": 4, \\"type\\": \\"click\\", \\"value\\": 4.5}, ] event_type = \\"click\\" result = pipeline(events, event_type) print(result) # Output should be: (10.0, [(0, {\\"id\\": 1, \\"type\\": \\"click\\", \\"value\\": 3.5}), (1, {\\"id\\": 2, \\"type\\": \\"click\\", \\"value\\": 2.0}), (2, {\\"id\\": 4, \\"type\\": \\"click\\", \\"value\\": 4.5})]) ``` **Note:** Ensure you make use of generator expressions, list comprehensions, and relevant functions from the `functools` and `itertools` modules for efficient processing.","solution":"from functools import reduce def generate_values(events): Generator function that yields the value field from each event in the provided list of events. for event in events: yield event[\'value\'] def filter_events(events, event_type): Filters the events, only including those that match the specified event_type. return (event for event in events if event[\'type\'] == event_type) def sum_values(values): Takes an iterator of numeric values and returns the sum of these values using functools.reduce. return reduce(lambda x, y: x + y, values, 0) def enumerate_events(events): Returns a list of tuples where each tuple contains the index and the event itself. return list(enumerate(events)) def pipeline(events, event_type): Processes the list of events by: 1. Filtering the events to only include those of type event_type. 2. Generating the values from the filtered events. 3. Summing the generated values. 4. Enumerating the filtered events to provide their indices. Returns a tuple containing the sum of the values from the filtered events and a list of tuples with event indices and the events themselves. filtered_events = filter_events(events, event_type) values = generate_values(filtered_events) sum_of_values = sum_values(values) filtered_events_for_enumeration = filter_events(events, event_type) # recreate the iterator as it is exhausted enumerated_events = enumerate_events(filtered_events_for_enumeration) return sum_of_values, enumerated_events"},{"question":"**Memory Buffer Manipulation with MemoryView Objects** In this exercise, you are tasked with implementing a Python function that reads, manipulates, and writes data using memoryview objects efficiently. You are required to perform several operations on a memory buffer without creating any unnecessary copies to ensure optimal performance. **Function Specifications:** ```python def transform_buffer(data: bytearray): This function takes a bytearray \'data\' as input, creates a memoryview object to access its buffer, and performs the following transformations: 1. Reverse the order of all bytes in the buffer. 2. Convert each byte to its bitwise complement. The function must modify the original bytearray in place and return a reference to it. :param data: bytearray - The input bytearray to be transformed. :return: bytearray - The modified input bytearray. pass ``` **Example:** ```python input_data = bytearray([1, 2, 3, 4]) result = transform_buffer(input_data) print(result) # Output: bytearray([251, 252, 253, 254]) ``` **Explanation:** - The input bytearray `[1, 2, 3, 4]` gets reversed to `[4, 3, 2, 1]`. - Each byte is then converted to its bitwise complement: `~4 = 251`, `~3 = 252`, `~2 = 253`, `~1 = 254`. **Constraints:** - You must use a memoryview object to manipulate the memory buffer directly. - Avoid creating any extra copies of the buffer or its parts. - Assume the input bytearray contains at least one byte. **Performance Requirements:** - The function should have linear time complexity O(n), where n is the number of bytes in the input bytearray. - The function should ensure minimal memory overhead by leveraging the in-place modifications feature provided by memoryview. Ensure your implementation adheres to the constraints and efficiently performs the required transformations using memoryview objects.","solution":"def transform_buffer(data: bytearray): This function takes a bytearray \'data\' as input, creates a memoryview object to access its buffer, and performs the following transformations: 1. Reverse the order of all bytes in the buffer. 2. Convert each byte to its bitwise complement. The function must modify the original bytearray in place and return a reference to it. :param data: bytearray - The input bytearray to be transformed. :return: bytearray - The modified input bytearray. if not data: return data # Create memoryview view = memoryview(data) # Reverse the order of all bytes in place for i in range(len(view) // 2): tmp = view[i] view[i] = view[-(i + 1)] view[-(i + 1)] = tmp # Convert each byte to its bitwise complement for i in range(len(view)): view[i] = ~view[i] & 0xFF return data"},{"question":"Managing Virtual Environments and Packages **Objective**: Create a script that demonstrates the creation, activation, and management of virtual environments and packages using Python. **Task**: Write a Python script that performs the following tasks: 1. **Create a Virtual Environment**: - Create a virtual environment named `test-env` in the current directory. 2. **Activate the Virtual Environment**: - Activate the virtual environment (assume the script will be executed in a Unix/MacOS environment). 3. **Install Packages**: - Install the latest version of two packages: `requests` and `numpy`. 4. **Generate a `requirements.txt` file**: - Generate a `requirements.txt` file for the installed packages. 5. **Upgrade a Package**: - Upgrade the `requests` package to its latest version. 6. **List Installed Packages**: - List all installed packages after the upgrade. **Expected Input and Output**: - No specific input from users is required. - The script should print output at each step indicating the actions being performed. **Constraints**: - The script should be runnable on a Unix/MacOS system with Python installed. - Ensure to handle any potential errors gracefully, such as virtual environment creation failures or network issues during package installation. **Performance Requirements**: - The script should run efficiently and handle external operations like package installation and upgrades without excessive delays. **Example Run**: ```shell python manage_env.py Creating virtual environment \'test-env\' ... Virtual environment created successfully. Activating virtual environment... Installing \'requests\' and \'numpy\' ... Packages installed successfully. Generating \'requirements.txt\' ... \'requirements.txt\' created successfully. Upgrading \'requests\' to the latest version ... \'requests\' upgraded successfully. Listing all installed packages ... Installed packages: requests: x.x.x numpy: x.x.x ``` Note: The script won\'t actually perform the activation of virtual environments as it is a function of the shell and not executable within a Python script, but it should display the necessary activation command for the user to manually execute in their shell.","solution":"import os import subprocess def execute_command(command): try: subprocess.run(command, check=True, shell=True) except subprocess.CalledProcessError as e: print(f\\"An error occurred while executing: {command}nError: {str(e)}\\") def create_virtual_environment(env_name=\'test-env\'): print(f\\"Creating virtual environment \'{env_name}\' ...\\") execute_command(f\\"python3 -m venv {env_name}\\") if os.path.isdir(env_name): print(\\"Virtual environment created successfully.\\") else: print(\\"Failed to create virtual environment.\\") def activate_virtual_environment(env_name=\'test-env\'): print(\\"Activating virtual environment...\\") activation_command = f\\"source ./{env_name}/bin/activate\\" print(f\\"Run the following command in your shell to activate the virtual environment:n{activation_command}\\") def install_packages(packages): for package in packages: print(f\\"Installing \'{package}\' ...\\") execute_command(f\\"./test-env/bin/pip install {package}\\") print(f\\"\'{package}\' installed successfully.\\") def generate_requirements(env_name=\'test-env\'): print(\\"Generating \'requirements.txt\' ...\\") execute_command(f\\"./{env_name}/bin/pip freeze > requirements.txt\\") if os.path.isfile(\\"requirements.txt\\"): print(\\"\'requirements.txt\' created successfully.\\") else: print(\\"Failed to create \'requirements.txt\'.\\") def upgrade_package(package): print(f\\"Upgrading \'{package}\' to the latest version ...\\") execute_command(f\\"./test-env/bin/pip install --upgrade {package}\\") print(f\\"\'{package}\' upgraded successfully.\\") def list_installed_packages(env_name=\'test-env\'): print(\\"Listing all installed packages ...\\") result = subprocess.run(f\\"./{env_name}/bin/pip list\\", capture_output=True, text=True, shell=True) print(\\"Installed packages:n\\", result.stdout) def main(): env_name = \'test-env\' packages = [\'requests\', \'numpy\'] create_virtual_environment(env_name) activate_virtual_environment(env_name) install_packages(packages) generate_requirements(env_name) upgrade_package(\'requests\') list_installed_packages(env_name) if __name__ == \\"__main__\\": main()"},{"question":"# Question **Title**: Implement a Simple Cache System using DBM **Objective**: Create a simple caching system using the `dbm` module where frequently accessed data can be stored, retrieved, and deleted. The cache should support operations to add, retrieve, and delete entries, as well as methods to compact the database and synchronize it with the disk. **Requirements**: - Implement a class `SimpleCache` that interacts with `dbm` databases. - The cache should support the following methods: - `__init__(self, filename: str, flag: str = \'c\')`: Initialize the cache with a given filename and flag. - `set(self, key: str, value: str) -> None`: Store a key-value pair in the cache. - `get(self, key: str, default: Optional[str] = None) -> Optional[str]`: Retrieve a value by key. If the key does not exist, return the default value. - `delete(self, key: str) -> None`: Delete a key-value pair from the cache. - `contains(self, key: str) -> bool`: Check if a key exists in the cache. - `keys(self) -> List[str]`: Return a list of all keys in the cache. - `compact(self) -> None`: Compact the database to reclaim space. - `sync(self) -> None`: Synchronize the database with the disk. - `close(self) -> None`: Close the database. **Constraints**: - The cache must work with all variants of `dbm` (dbm.gnu, dbm.ndbm, and dbm.dumb). - Keys and values must be strings that will be stored as bytes in the database. - The cache must handle potential `dbm` errors gracefully. - The `compact` method should work only if the underlying database supports a reorganization method. If the method is not supported, it should do nothing. **Input Format**: - Methods will be called on an instance of `SimpleCache`. **Output Format**: - The methods should perform their operations without returning any output, except for `get` and `keys` methods. **Example**: ```python # Example usage of the SimpleCache class. cache = SimpleCache(\'mycache.db\', \'c\') cache.set(\'hello\', \'world\') print(cache.get(\'hello\')) # Output: world cache.set(\'foo\', \'bar\') print(cache.contains(\'foo\')) # Output: True print(cache.keys()) # Output: [b\'hello\', b\'foo\'] cache.delete(\'hello\') print(cache.get(\'hello\')) # Output: None cache.set(\'ping\', \'pong\') cache.set(\'fizz\', \'buzz\') cache.compact() # Compact the database if supported cache.sync() # Synchronize the database with the disk cache.close() # Close the cache ``` Implement the `SimpleCache` class based on the description provided.","solution":"import dbm class SimpleCache: def __init__(self, filename: str, flag: str = \'c\'): self.filename = filename self.flag = flag self.db = dbm.open(filename, flag) def set(self, key: str, value: str) -> None: self.db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') def get(self, key: str, default: str = None) -> str: return self.db.get(key.encode(\'utf-8\'), default.encode(\'utf-8\') if default else None).decode(\'utf-8\') if self.db.get(key.encode(\'utf-8\')) else default def delete(self, key: str) -> None: if key.encode(\'utf-8\') in self.db: del self.db[key.encode(\'utf-8\')] def contains(self, key: str) -> bool: return key.encode(\'utf-8\') in self.db def keys(self) -> list: return [key.decode(\'utf-8\') for key in self.db.keys()] def compact(self) -> None: # This is a no-op for most dbm types pass def sync(self) -> None: if hasattr(self.db, \'sync\'): self.db.sync() def close(self) -> None: self.db.close()"},{"question":"Write a function `custom_residplot` that generates and saves a customized residual plot using the seaborn library. Your function should adhere to the following specifications: 1. **Function Signature**: ```python def custom_residplot(data, x, y, order=1, lowess=False, line_color=\\"r\\", save_path=\\"residual_plot.png\\"): pass ``` 2. **Input Parameters**: - `data`: A pandas DataFrame containing the data set. - `x`: The column name from the DataFrame to be used as the independent variable (string). - `y`: The column name from the DataFrame to be used as the dependent variable (string). - `order`: An integer indicating the order of the polynomial for the regression fit (default is 1 for linear fit). - `lowess`: A boolean indicating whether to add a LOWESS curve to the plot (default is `False`). - `line_color`: The color of the LOWESS curve, if `lowess` is `True` (default is \\"r\\" for red). - `save_path`: A string representing the file path where the plot image will be saved (default is \\"residual_plot.png\\"). 3. **Function Requirements**: - The function should create a residual plot using the `sns.residplot` method from seaborn. - If `order` is specified, adjust the polynomial order of the regression fit. - If `lowess` is `True`, add a LOWESS curve to the plot with the specified `line_color`. - Save the resultant plot image to the file path provided in `save_path`. 4. **Constraints**: - You must use the seaborn library for plotting. - Assume `data` is always a valid pandas DataFrame with appropriate columns. - The function should not display the plot but only save it to the provided path. # Example Usage ```python import seaborn as sns # Load example dataset mpg = sns.load_dataset(\\"mpg\\") # Generate and save the default residual plot custom_residplot(mpg, \\"horsepower\\", \\"mpg\\") # Generate and save a residual plot with a quadratic fit and a LOWESS curve custom_residplot(mpg, \\"horsepower\\", \\"mpg\\", order=2, lowess=True, line_color=\\"blue\\", save_path=\\"custom_residual_plot.png\\") ``` # Expected Output The function should generate and save plots based on the provided input parameters. For example, the last command should save a plot named \\"custom_residual_plot.png\\" with a quadratic regression fit and a blue LOWESS curve.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_residplot(data, x, y, order=1, lowess=False, line_color=\\"r\\", save_path=\\"residual_plot.png\\"): Generates and saves a customized residual plot using seaborn. Parameters: - data: pandas DataFrame containing the dataset. - x: Column name from the DataFrame to be used as the independent variable (string). - y: Column name from the DataFrame to be used as the dependent variable (string). - order: Polynomial order for the regression fit (default is 1 for linear fit). - lowess: Whether to add a LOWESS curve to the plot (default is False). - line_color: Color of the LOWESS curve if lowess is True (default is \\"r\\"). - save_path: File path where the plot image will be saved (default is \\"residual_plot.png\\"). # Initialize the plot plt.figure(figsize=(10, 6)) # Create residual plot sns.residplot(x=x, y=y, data=data, order=order, lowess=lowess, line_kws={\'color\': line_color}) # Save the plot plt.savefig(save_path) # Close the plot to avoid display plt.close()"},{"question":"**Question: Implementing Custom Copy Methods** You are given a Python class that represents a Node in a tree. Your task is to implement the custom shallow and deep copy operations for this class. The class is provided as follows: ```python class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) ``` # Requirements: 1. Implement the `__copy__()` method to create a shallow copy of the Node instance. 2. Implement the `__deepcopy__()` method to create a deep copy of the Node instance, using a memo dictionary to avoid recursive loops. # Input - An instance of the `Node` class. For testing purposes, the tree may contain multiple nodes with various structures. # Output - A new `Node` instance that is a shallow or deep copy of the original, based on the implemented method. # Constraints - Consider edge cases such as nodes with no children, nodes with multiple children, and recursive structures. - Performance should be efficient and handle reasonably large trees within acceptable time limits. # Example Given: ```python root = Node(1) child1 = Node(2) child2 = Node(3) root.add_child(child1) root.add_child(child2) child1.add_child(root) # Creating a recursive structure. ``` Example of shallow copy: ```python shallow_copy_root = copy.copy(root) ``` Here, `shallow_copy_root` should have the same structure, with references to the same child nodes. Example of deep copy: ```python deep_copy_root = copy.deepcopy(root) ``` Here, `deep_copy_root` should be a completely independent copy of the entire tree, with no shared references. # Implementation Complete the missing methods in the `Node` class: ```python import copy class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): # Implement shallow copy logic here. pass def __deepcopy__(self, memo): # Implement deep copy logic here using memo dictionary. pass ``` Ensure your implementation handles all edge cases and performs efficiently.","solution":"import copy class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): Create a shallow copy of the Node instance. new_node = Node(self.value) new_node.children = self.children.copy() # Shallow copy of children list return new_node def __deepcopy__(self, memo): Create a deep copy of the Node instance using a memo dictionary. if self in memo: return memo[self] new_node = Node(self.value) # Store new_node in memo dictionary to avoid cyclic references memo[self] = new_node new_node.children = [copy.deepcopy(child, memo) for child in self.children] return new_node"},{"question":"**Objective**: Demonstrate an understanding of Python\'s module importing using the `PyImport_ImportModule` function from the Python/C API. **Background**: Python provides a `PyImport_ImportModule` function, which allows you to import a module programmatically in the Python interpreter. This can be particularly useful in embedded Python applications or when integrating Python functionality into other programming languages using the Python/C API. **Task**: Implement a Python function `import_module_with_fallback(module_name)`. This function tries to import a module by name. If the module cannot be imported, it should attempt to import a fallback module. If both attempts fail, it should return `None`. **Requirements**: 1. The function should take one string argument, `module_name`, which is the name of the primary module to import. 2. The function will attempt to import the module specified by `module_name` using `PyImport_ImportModule`. 3. If importing `module_name` fails, it should then attempt to import a fallback module named \\"fallback_module\\". 4. If both module imports fail, the function should return `None`. 5. Implement appropriate error handling to manage import failures and return informative error messages when applicable. **Constraints**: - You may assume that the fallback module name is always \\"fallback_module\\". - Do not use Python\'s built-in `__import__` function or any similar high-level import mechanisms; instead, rely on the `PyImport_ImportModule` function as documented. - Ensure to handle any exceptions and provide clear error messages. **Example Usage**: ```python result = import_module_with_fallback(\\"nonexistent_module\\") if result is None: print(\\"Both primary and fallback module imports failed.\\") else: print(f\\"Module successfully imported: {result.__name__}\\") ``` **Expected Output**: - If \\"nonexistent_module\\" and \\"fallback_module\\" do not exist, the function should return `None` and print \\"Both primary and fallback module imports failed.\\" - If \\"fallback_module\\" exists, it should import \\"fallback_module\\" and print \\"Module successfully imported: fallback_module\\". **Hints**: - Use the `ctypes` library to call C functions from Python. - You may need to manage references to the imported modules to prevent memory leaks. **References**: - Detailed documentation on `PyImport_ImportModule` and related functions can be found in the provided documentation excerpt.","solution":"import ctypes import sys def import_module_with_fallback(module_name): Attempts to import a module by name. If the import fails, it tries to import a fallback module named \\"fallback_module\\". Returns the module if successful, otherwise returns None. # Initialize the Python/C API Py_Initialize = ctypes.pythonapi.Py_Initialize Py_Initialize() # Function pointers for PyImport_ImportModule and PyErr_Clear PyImport_ImportModule = ctypes.pythonapi.PyImport_ImportModule PyImport_ImportModule.argtypes = [ctypes.c_char_p] PyImport_ImportModule.restype = ctypes.py_object PyErr_Clear = ctypes.pythonapi.PyErr_Clear # Try importing the primary module try: module = PyImport_ImportModule(module_name.encode(\'utf-8\')) return module except: PyErr_Clear() # Try importing the fallback module try: fallback_module = PyImport_ImportModule(b\\"fallback_module\\") return fallback_module except: PyErr_Clear() return None"},{"question":"**Objective**: Demonstrate comprehension of Python\'s `os` module by solving a file system-related problem. **Problem Statement**: You are tasked with creating a Python script that organizes files in a specified directory based on their file extensions. Your script should perform the following tasks: 1. Traverse a given directory to collect all file paths. 2. Create subdirectories for each unique file extension within the specified directory. If a subdirectory for a file extension already exists, use it. 3. Move each file into the corresponding subdirectory based on its file extension. 4. Provide an option to undo the file organization, restoring the original structure of the directory. # Function Signature The primary function you need to implement will have the following signature: ```python def organize_files(directory: str, undo: bool = False) -> None: Organize files in the given directory by their file extensions. Args: directory (str): The path to the directory to organize. undo (bool): If True, undo the organization and restore the original structure. Returns: None pass ``` # Input - `directory`: A string representing the path to the directory to organize. - `undo`: A boolean flag to indicate whether to undo the organization. The default value is `False`. # Output - The function has no return value but should move the files as specified. # Constraints - You may assume all files have one-point file extensions or none (e.g., `.txt`, `.jpg`, etc.). - The `undo` functionality should reverse the changes made by the organization process, returning all files to their original location. # Example ```python # Assume the directory structure before running the script is: # /example_dir/ # |- file1.txt # |- file2.txt # |- script.py # |- photo.jpg organize_files(\'/example_dir\') # The directory structure after running the script should be: # /example_dir/ # |- txt/ # | |- file1.txt # | |- file2.txt # |- py/ # | |- script.py # |- jpg/ # |- photo.jpg organize_files(\'/example_dir\', undo=True) # The directory structure should revert to: # /example_dir/ # |- file1.txt # |- file2.txt # |- script.py # |- photo.jpg ``` # Additional Requirements - You should create the necessary directories only if they do not already exist. - The script should handle any errors gracefully, such as invalid paths or permissions issues. # Performance - The solution should handle directories with up to 10,000 files efficiently. - Aim for a time complexity of O(n) where n is the number of files in the directory.","solution":"import os import shutil def organize_files(directory: str, undo: bool = False) -> None: Organize files in the given directory by their file extensions. Args: directory (str): The path to the directory to organize. undo (bool): If True, undo the organization and restore the original structure. Returns: None if not os.path.isdir(directory): raise ValueError(f\\"Provided directory \'{directory}\' is not valid.\\") # File to track original paths track_file = os.path.join(directory, \\".file_organization_track\\") if undo: if not os.path.isfile(track_file): raise ValueError(\\"Undo file not found. Cannot undo the organization.\\") # Read original file paths with open(track_file, \'r\') as f: for line in f: original_path, current_path = line.strip().split(\',\') shutil.move(current_path, original_path) # Delete the tracking file os.remove(track_file) return file_paths = [] for root, _, files in os.walk(directory): for file in files: full_path = os.path.join(root, file) if full_path != track_file: file_paths.append(full_path) file_map = {} for file_path in file_paths: base_name = os.path.basename(file_path) extension = os.path.splitext(base_name)[1][1:] ext_dir = os.path.join(directory, extension) if extension not in file_map: file_map[extension] = ext_dir # Create extension directories and move files with open(track_file, \'w\') as track: for file_path in file_paths: base_name = os.path.basename(file_path) extension = os.path.splitext(base_name)[1][1:] ext_dir = file_map[extension] if not os.path.exists(ext_dir): os.makedirs(ext_dir) new_path = os.path.join(ext_dir, base_name) shutil.move(file_path, new_path) track.write(f\\"{file_path},{new_path}n\\")"},{"question":"**Context & Objectives:** You are tasked with improving the efficiency and flexibility of a hypothetical data processing system. Your solution should leverage the `functools` module’s features to optimize and enhance the system. **Problem Statement:** You need to implement the following functionalities: 1. A **memoized factorial function** to avoid recomputation. 2. A **customizable data cleaning function** using `partial` for predefining certain operations (such as stripping whitespace, lowercasing, and removing certain characters). 3. A **type-based processing function** using `singledispatch` to handle different kinds of data types (e.g., `int`, `str`, `list`). **Requirements:** 1. Implement a `memoized_factorial` function using `@functools.cache`. 2. Implement a `data_cleaner` function which takes a string and applies the following transformations: - `strip_whitespace` (default: `True`) - `to_lowercase` (default: `True`) - `remove_chars` (default: `None`, takes a string of characters to be removed) Utilize `functools.partial` to create a `default_data_cleaner` with the default values. 3. Implement a `process_data` function using `singledispatch` which: - For `int`: Returns the square of the number. - For `str`: Cleans the string using the `default_data_cleaner`. - For `list`: Returns a list with each element\'s `process_data` applied. - For any other type, return `\\"unsupported type\\"`. **Input:** 1. For `memoized_factorial`: An integer `n`. 2. For `data_cleaner`: A string `data` and optional keyword arguments `strip_whitespace`, `to_lowercase`, `remove_chars`. 3. For `process_data`: A single argument that can be `int`, `str`, `list`, or any other type. **Output:** 1. For `memoized_factorial`: The factorial of `n`. 2. For `data_cleaner`: Transformed string based on the provided arguments. 3. For `process_data`: The processed data based on its type. **Constraints:** - Assume `n` is a non-negative integer for `memoized_factorial`. - Assume `data` in `data_cleaner` is always a string. - For `process_data`, `list` will contain items that are either `int`, `str`, or `list`. **Implementation Instructions:** 1. Define the function `memoized_factorial` using `@functools.cache`. 2. Define the function `data_cleaner` using the specified keyword arguments and `functools.partial` to create `default_data_cleaner`. 3. Define the function `process_data` using `@singledispatch` and register overloaded implementations for `int`, `str`, and `list`. --- **Example Usage:** ```python # Part 1: Memoized Factorial print(memoized_factorial(5)) # Output: 120 print(memoized_factorial(6)) # Output: 720 # Part 2: Data Cleaner print(data_cleaner(\\" Hello World! \\", remove_chars=\\"!\\")) # Output: \\"hello world\\" print(default_data_cleaner(\\" Hello World! \\")) # Output: \\"hello world!\\" # Part 3: Process Data print(process_data(4)) # Output: 16 print(process_data(\\" Hello! \\")) # Output: \\"hello\\" print(process_data([1, \\" ABC \\", 2])) # Output: [1, \\"abc\\", 4] ``` --- Implementing these functions demonstrates the ability to use Python\'s `functools` module effectively and efficiently. **Note:** Ensure to thoroughly test all edge cases and validate the robustness of your implementations.","solution":"import functools from functools import partial, singledispatch # Part 1: Memoized Factorial @functools.cache def memoized_factorial(n): if n < 2: return 1 return n * memoized_factorial(n - 1) # Part 2: Data Cleaner def data_cleaner(data, strip_whitespace=True, to_lowercase=True, remove_chars=None): if strip_whitespace: data = data.strip() if to_lowercase: data = data.lower() if remove_chars: data = \'\'.join(char for char in data if char not in remove_chars) return data default_data_cleaner = partial(data_cleaner, strip_whitespace=True, to_lowercase=True, remove_chars=None) # Part 3: Process Data @singledispatch def process_data(data): return \\"unsupported type\\" @process_data.register def _(data: int): return data * data @process_data.register def _(data: str): return default_data_cleaner(data) @process_data.register def _(data: list): return [process_data(item) for item in data]"},{"question":"# Bytearray Manipulation and Analysis You are tasked with implementing a series of functions that demonstrate the foundational and advanced concepts related to Python\'s `bytearray` objects using the Python C API functions outlined in the provided documentation. Your implementations should work with `bytearray` objects to perform the following actions: 1. **Check Bytearray**: Verify if a given Python object is a `bytearray`. 2. **Create Bytearray**: Create a new `bytearray` from a given string. 3. **Concatenate Bytearrays**: Concatenate two `bytearray` objects. 4. **Resize Bytearray**: Resize a given `bytearray` to a specified length. 5. **Analyze Bytearray**: Return the contents and size of a given `bytearray`. Here\'s the detailed specification for each function: 1. **Check Bytearray**: ```python def is_bytearray(obj: Any) -> bool: Check if the given object is a bytearray. Args: obj: Any - The object to check Returns: bool - True if the object is a bytearray, False otherwise # Implementation ``` 2. **Create Bytearray**: ```python def create_bytearray_from_string(input_string: str) -> bytearray: Create a new bytearray from the provided input string. Args: input_string: str - The string to convert into a bytearray Returns: bytearray - The created bytearray # Implementation ``` 3. **Concatenate Bytearrays**: ```python def concatenate_bytearrays(b1: bytearray, b2: bytearray) -> bytearray: Concatenate two bytearrays and return the result. Args: b1: bytearray - The first bytearray b2: bytearray - The second bytearray Returns: bytearray - The concatenated bytearray # Implementation ``` 4. **Resize Bytearray**: ```python def resize_bytearray(b: bytearray, new_size: int) -> bytearray: Resize the given bytearray to the new specified size. Args: b: bytearray - The bytearray to resize new_size: int - The new size for the bytearray Returns: bytearray - The resized bytearray # Implementation ``` 5. **Analyze Bytearray**: ```python def analyze_bytearray(b: bytearray) -> Tuple[str, int]: Return the contents of the bytearray as a string and its size. Args: b: bytearray - The bytearray to analyze Returns: Tuple[str, int] - A tuple containing the contents of the bytearray as a string and its size # Implementation ``` # Constraints: - All inputs will be valid and within reasonable size limits for Python objects. - The `resize_bytearray` function\'s `new_size` will always be non-negative. # Performance Requirements: - Solutions should be optimized for performance where applicable, considering the size of typical `bytearray` objects. Make sure you handle edge cases, such as empty strings for bytearray creation and resizing a bytearray to zero length. You can use the provided macros and functions from the documentation to implement these functionalities efficiently.","solution":"def is_bytearray(obj): Check if the given object is a bytearray. Args: obj: Any - The object to check Returns: bool - True if the object is a bytearray, False otherwise return isinstance(obj, bytearray) def create_bytearray_from_string(input_string): Create a new bytearray from the provided input string. Args: input_string: str - The string to convert into a bytearray Returns: bytearray - The created bytearray return bytearray(input_string, \'utf-8\') def concatenate_bytearrays(b1, b2): Concatenate two bytearrays and return the result. Args: b1: bytearray - The first bytearray b2: bytearray - The second bytearray Returns: bytearray - The concatenated bytearray return b1 + b2 def resize_bytearray(b, new_size): Resize the given bytearray to the new specified size. Args: b: bytearray - The bytearray to resize new_size: int - The new size for the bytearray Returns: bytearray - The resized bytearray b = b[:new_size] return b def analyze_bytearray(b): Return the contents of the bytearray as a string and its size. Args: b: bytearray - The bytearray to analyze Returns: Tuple[str, int] - A tuple containing the contents of the bytearray as a string and its size return (b.decode(\'utf-8\'), len(b))"},{"question":"**Objective:** Configure a custom logging system using a dictionary and the `logging.config.dictConfig` method. **Task:** Write a Python function `setup_logging(config_dict: dict) -> None` that takes a logging configuration dictionary and sets up the logging configuration using the `dictConfig` function from the `logging.config` module. You need to include custom formatters, handlers, and loggers in your configuration dictionary. **Requirements:** 1. The configuration dictionary should configure at least: - Two formatters: one for simple messages and one for detailed messages with timestamps. - Two handlers: one that logs to the console and another that logs to a file. - Two loggers: a root logger and a custom logger named \'myLogger\'. 2. The configuration dictionary should have the necessary keys and values to properly instantiate these logging components. 3. The function should handle and raise exceptions in case of incorrect configurations. **Input:** - `config_dict`: A dictionary containing the logging configuration. **Output:** - None, but the function should set the logging configuration. **Constraints:** - Ensure the root logger and \'myLogger\' logger have appropriate handler and level configurations. **Example:** ```python import logging import logging.config def setup_logging(config_dict: dict) -> None: # Implement your logic here pass logging_config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(levelname)s: %(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s %(levelname)-8s %(name)-15s %(message)s\' }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\' }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'detailed\', \'level\': \'INFO\', \'filename\': \'app.log\' }, }, \'loggers\': { \'myLogger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'] } }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'] } } # You should be able to call setup_logging with this dictionary setup_logging(logging_config) ``` Your solution should initialize the logging configuration when the function is called with the provided dictionary, enabling logging based on the specified formatters, handlers, and loggers. **Note:** Ensure proper error handling for invalid configurations and provide meaningful error messages.","solution":"import logging import logging.config def setup_logging(config_dict: dict) -> None: Sets up the logging configuration using the given config dictionary. :param config_dict: A dictionary containing the logging configuration. :raises: ValueError if the configuration dictionary is invalid. try: logging.config.dictConfig(config_dict) except Exception as e: raise ValueError(f\\"Invalid logging configuration: {e}\\") # Example configuration dictionary logging_config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(levelname)s: %(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s %(levelname)-8s %(name)-15s %(message)s\' }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\' }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'detailed\', \'level\': \'INFO\', \'filename\': \'app.log\' }, }, \'loggers\': { \'myLogger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'] } }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'] } }"},{"question":"**Pandas Indexing and Data Selection** In this assessment, you will be working with the `pandas` library to practice selecting and manipulating data. You will be required to use indexing methods effectively to solve a real-world inspired problem. # Task Overview You are given a DataFrame containing data of people, with their names, ages, cities, and scores. Your task is to efficiently filter, select, and return specific subsets of this DataFrame following the instructions given below. # Provided Data You are provided with the following data in a pandas DataFrame: ```python import pandas as pd data = { \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\', \'Frank\', \'Grace\'], \'Age\': [24, 30, 22, 25, 29, 21, 23], \'City\': [\'New York\', \'Los Angeles\', \'Chicago\', \'Houston\', \'Phoenix\', \'Philadelphia\', \'San Antonio\'], \'Score\': [85, 72, 90, 88, 75, 68, 80] } df = pd.DataFrame(data) ``` # Problem Statement 1. Write a function `get_people_in_age_range(df, min_age, max_age)` that returns a DataFrame of all individuals within a specific age range (inclusive). Function Signature: `def get_people_in_age_range(df: pd.DataFrame, min_age: int, max_age: int) -> pd.DataFrame:` Input: - `df`: The pandas DataFrame containing the data. - `min_age`: The minimum age (inclusive). - `max_age`: The maximum age (inclusive). Output: - A pandas DataFrame with the individuals who are within the specified age range. 2. Write a function `get_top_n_scorers(df, n)` that returns a DataFrame of the top `n` individuals with the highest scores. Function Signature: `def get_top_n_scorers(df: pd.DataFrame, n: int) -> pd.DataFrame:` Input: - `df`: The pandas DataFrame containing the data. - `n`: The number of top scorers to return. Output: - A pandas DataFrame sorted by the \'Score\' column in descending order containing the top `n` scorers. 3. Write a function `swap_column_values(df, col1, col2)` to swap the values of two specified columns. Function Signature: `def swap_column_values(df: pd.DataFrame, col1: str, col2: str) -> pd.DataFrame:` Input: - `df`: The pandas DataFrame containing the data. - `col1`: The first column whose values you want to swap. - `col2`: The second column whose values you want to swap. Output: - The modified pandas DataFrame with the values of `col1` and `col2` swapped in-place. # Constraints - Assume that the input DataFrame always contains valid data and the specified columns exist. - The data ranges for ages and scores are within realistic values. - You should aim for efficient and idiomatic pandas code. # Examples ```python # Example 1 print(get_people_in_age_range(df, 23, 25)) # Output: ``` Name Age City Score 0 Alice 24 New York 85 3 David 25 Houston 88 6 Grace 23 San Antonio 80 ```python # Example 2 print(get_top_n_scorers(df, 3)) # Output: ``` Name Age City Score 2 Charlie 22 Chicago 90 3 David 25 Houston 88 0 Alice 24 New York 85 ```python # Example 3 new_df = swap_column_values(df, \'Name\', \'City\') print(new_df) # Output: Name Age City Score 0 New York 24 Alice 85 1 Los Angeles 30 Bob 72 2 Chicago 22 Charlie 90 3 Houston 25 David 88 4 Phoenix 29 Eve 75 5 Philadelphia 21 Frank 68 6 San Antonio 23 Grace 80 ``` # Submission Please submit a .py file with the implemented functions.","solution":"import pandas as pd def get_people_in_age_range(df: pd.DataFrame, min_age: int, max_age: int) -> pd.DataFrame: Returns a DataFrame of all individuals within a specific age range (inclusive). return df[(df[\'Age\'] >= min_age) & (df[\'Age\'] <= max_age)] def get_top_n_scorers(df: pd.DataFrame, n: int) -> pd.DataFrame: Returns a DataFrame of the top n individuals with the highest scores. return df.nlargest(n, \'Score\') def swap_column_values(df: pd.DataFrame, col1: str, col2: str) -> pd.DataFrame: Returns a DataFrame with the values of col1 and col2 swapped. df_copy = df.copy() df_copy[col1], df_copy[col2] = df_copy[col2], df_copy[col1] return df_copy"},{"question":"# Objective: To assess the understanding and application of shallow and deep copying in Python using the `copy` module, as well as the ability to customize copying for user-defined classes. # Problem Statement: Implement a class `CustomList` that represents a list of numbers with the following capabilities: 1. It should allow creating a shallow and deep copy using the `copy` module. 2. The class should include methods to calculate and access the sum of its elements `sum_elements()`. 3. Overload the `__copy__()` and `__deepcopy__()` methods to demonstrate shallow and deep copy functionalities. # Requirements: 1. Implement the constructor `__init__` to initialize the list. 2. Implement the `sum_elements()` method to calculate the sum of the elements in the list. 3. Overload the `__copy__()` method to create a shallow copy of the object. 4. Overload the `__deepcopy__()` method to create a deep copy of the object. # Input Format: - Initial list of elements provided during the creation of a `CustomList` object. # Output Format: - When creating a deep copy, ensure the new object\'s sum is recalculated correctly. - When creating a shallow copy, ensure it retains references to the original elements. # Sample Use: ```python import copy class CustomList: def __init__(self, elements): self.elements = elements def sum_elements(self): return sum(self.elements) def __copy__(self): new_list = CustomList(self.elements) return new_list def __deepcopy__(self, memo): new_list = CustomList(copy.deepcopy(self.elements, memo)) memo[id(self)] = new_list return new_list # Original list original = CustomList([1, 2, 3, 4]) # Shallow copy shallow_copied = copy.copy(original) print(shallow_copied.sum_elements()) # Output should be the sum of elements # Deep copy deep_copied = copy.deepcopy(original) print(deep_copied.sum_elements()) # Output should also be the sum of elements # Modifying the original list original.elements.append(5) # Check sums again print(original.sum_elements()) # Sum of modified original list print(shallow_copied.sum_elements()) # Should reflect the change if shallow copied correctly print(deep_copied.sum_elements()) # Should not reflect the change if deep copied correctly ``` # Constraints: - Elements of the `CustomList` should be numeric. - Assume list operations are within standard performance constraints. # Difficulty Level: - Intermediate to Advanced","solution":"import copy class CustomList: def __init__(self, elements): self.elements = elements def sum_elements(self): return sum(self.elements) def __copy__(self): new_list = CustomList(self.elements) return new_list def __deepcopy__(self, memo): new_list = CustomList(copy.deepcopy(self.elements, memo)) memo[id(self)] = new_list return new_list"},{"question":"# Asyncio Exception Handling **Objective:** Implement a function that simulates an asynchronous file reading operation using asyncio, demonstrating the handling of various asyncio exceptions. **Function Signature:** ```python async def async_file_read(file_path: str, buffer_size: int, timeout: int) -> bytes: pass ``` **Input:** - `file_path` (str): The path to the file to be read. - `buffer_size` (int): The buffer size limit for reading operations. - `timeout` (int): The time in seconds before the read operation times out. **Output:** - `bytes`: The content read from the file. **Constraints:** - If the read operation exceeds the `timeout`, an `asyncio.TimeoutError` should be raised and caught. - If the file cannot be found or opened, a custom `asyncio.SendfileNotAvailableError` should be raised. - If the read operation does not complete (e.g., due to partial read), catch the `asyncio.IncompleteReadError` and return the partial content read so far. - If the buffer size limit is reached during reading, catch the `asyncio.LimitOverrunError` and raise a custom error message indicating the buffer overrun. **Performance Requirements:** - The function should efficiently read potentially large files, respecting the given buffer size limit and timeout constraints. **Example:** ```python import asyncio async def async_file_read(file_path: str, buffer_size: int, timeout: int) -> bytes: pass # Example usage: # Suppose \'example.txt\' is a file in the current directory try: content = await async_file_read(\'example.txt\', buffer_size=1024, timeout=5) print(content) except Exception as e: print(f\\"An error occurred: {e}\\") ``` **Notes:** - Utilize `asyncio.open_connection` for simulating file read operation via network. - Make sure to handle all specified exceptions properly, providing meaningful messages or actions as required. - Use async/await syntax to ensure the reading operation is performed asynchronously.","solution":"import asyncio import os class SendfileNotAvailableError(Exception): pass class BufferOverrunError(Exception): pass async def async_file_read(file_path: str, buffer_size: int, timeout: int) -> bytes: try: if not os.path.exists(file_path): raise SendfileNotAvailableError(\\"File not found\\") async with asyncio.open_connection(file_path) as reader, writer: try: data = await asyncio.wait_for(reader.read(buffer_size), timeout=timeout) except asyncio.TimeoutError: raise asyncio.TimeoutError(\\"Reading operation timed out\\") except asyncio.IncompleteReadError as e: return e.partial except asyncio.LimitOverrunError: raise BufferOverrunError(\\"Buffer size limit exceeded\\") return data except SendfileNotAvailableError: raise except Exception as e: raise RuntimeError(f\\"Unexpected error: {e}\\")"},{"question":"# Objective Write a Python function to manipulate a list of strings and numbers based on specific rules. # Instructions 1. **Function Name**: `modify_list` 2. **Input**: A list `lst` containing strings and numbers mixed together (e.g., `[\'apple\', 2, \'banana\', 3.5, \'cherry\', 4]`). 3. **Output**: A new list where: - Strings are converted to their upper case format. - Numbers: - If integer and divisible by 2, it is replaced by its square. - If float or not divisible by 2, it is replaced by its negative value. # Function Signature ```python def modify_list(lst: list) -> list: pass ``` # Example Input: ```python [\'apple\', 2, \'banana\', 3.5, \'cherry\', 4] ``` Output: ```python [\'APPLE\', 4, \'BANANA\', -3.5, \'CHERRY\', 16] ``` # Constraints 1. The input list can contain up to 100 elements. 2. The input list will not be empty. 3. Only basic data types (strings, integers, and floats) will be present in the list. # Requirements - Use list comprehensions where applicable. - Handle edge cases, such as negative numbers and zero appropriately. # Notes - The solution should demonstrate efficient data handling and manipulation. - The solution should make use of Python\'s built-in capabilities for string and numerical operations. # Hints - Remember to check the type of each element in the list. - Utilize string methods and numerical operations effectively.","solution":"def modify_list(lst): Modifies a list of mixed strings and numbers based on specified rules. Arguments: lst -- list containing strings and numbers Returns: new_list -- modified list with processed strings and numbers modified_list = [] for element in lst: if isinstance(element, str): modified_list.append(element.upper()) elif isinstance(element, int): if element % 2 == 0: modified_list.append(element ** 2) else: modified_list.append(-element) elif isinstance(element, float): modified_list.append(-element) return modified_list"},{"question":"You are tasked with writing a Python function to extract and manipulate Unix user account information using the `pwd` module. This function will process the password database and return specific information based on certain criteria. # Function: `get_users_with_shell` Input: - `shell_name (str)`: The exact name of the shell to filter users by (e.g., `/bin/bash`). Output: - A list of dictionaries where each dictionary represents a user whose shell matches the provided `shell_name`. Each dictionary should contain the following keys and corresponding values: - `name`: The login name of the user. - `uid`: The numerical user ID of the user. - `home`: The home directory of the user. Constraints: - You must use the `pwd.getpwall()` function to retrieve the list of all users. - Only include users that have the specified shell. - The output list should be sorted by user ID in ascending order. Example: ```python def get_users_with_shell(shell_name: str): # Your implementation here # Example usage: print(get_users_with_shell(\'/bin/bash\')) ``` **Expected Output:** If there are users with `/bin/bash` as their shell, the function should return something similar to: ```python [ {\'name\': \'user1\', \'uid\': 1001, \'home\': \'/home/user1\'}, {\'name\': \'user2\', \'uid\': 1002, \'home\': \'/home/user2\'}, ... ] ``` Note: You should handle cases where no user matches the provided shell, returning an empty list in such scenarios. The function should be efficient and make use of the `pwd.getpwall()` function to ensure accurate and complete data retrieval.","solution":"import pwd def get_users_with_shell(shell_name: str): Returns users with the specified shell. Args: shell_name (str): The exact name of the shell to filter users by. Returns: list: A list of dictionaries where each dictionary represents a user. users = pwd.getpwall() filtered_users = [ {\'name\': user.pw_name, \'uid\': user.pw_uid, \'home\': user.pw_dir} for user in users if user.pw_shell == shell_name ] return sorted(filtered_users, key=lambda x: x[\'uid\']) # Example usage: # print(get_users_with_shell(\'/bin/bash\'))"},{"question":"Objective Create a CGI script using Python to process a user registration form. The form will collect a username, email address, and password from the user. Your CGI script should perform the following tasks: 1. Validate the form data: - Ensure that all fields (username, email, password) are provided. - Ensure that the email address is in a valid format. - Ensure that the password is at least 8 characters long. 2. Handle errors gracefully by displaying appropriate error messages to the user. 3. If the form data is valid, display a confirmation message with the submitted username and email. The password should not be displayed. Input The CGI script should handle HTTP POST requests containing form data with the fields: `username`, `email`, and `password`. Output The output of the CGI script should be in HTML format and will either display: 1. Error messages if the validation fails. 2. A confirmation message if the validation is successful. Constraints 1. You must use the `cgi` module to handle form data. 2. Use appropriate methods from the `cgi` module to retrieve and validate the form data. 3. Implement error handling using the `cgitb` module for debugging during script development. Sample HTML Form Your CGI script will be processing data from an HTML form like the following: ```html <form method=\\"POST\\" action=\\"/cgi-bin/registration.py\\"> <label for=\\"username\\">Username:</label> <input type=\\"text\\" id=\\"username\\" name=\\"username\\"><br> <label for=\\"email\\">Email:</label> <input type=\\"email\\" id=\\"email\\" name=\\"email\\"><br> <label for=\\"password\\">Password:</label> <input type=\\"password\\" id=\\"password\\" name=\\"password\\"><br> <input type=\\"submit\\" value=\\"Register\\"> </form> ``` Implementation Requirements 1. Create a Python CGI script named `registration.py`. 2. In the script: - Import the necessary modules (`cgi`, `cgitb`, `re` for regex validation of email). - Enable the `cgitb` module for debugging. - Use the `FieldStorage` class to access form data. - Validate the form fields. - Generate and print the appropriate HTML response. 3. Ensure the script handles missing form data and displays error messages accordingly. 4. Ensure that the script is secure and does not display sensitive information like the password. Example ```python #!/usr/bin/env python3 import cgi import cgitb import re cgitb.enable() print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() username = form.getfirst(\\"username\\", \\"\\").strip() email = form.getfirst(\\"email\\", \\"\\").strip() password = form.getfirst(\\"password\\", \\"\\").strip() errors = [] if not username: errors.append(\\"Username is required.\\") if not email: errors.append(\\"Email is required.\\") elif not re.match(r\\"[^@]+@[^@]+.[^@]+\\", email): errors.append(\\"Invalid email format.\\") if not password: errors.append(\\"Password is required.\\") elif len(password) < 8: errors.append(\\"Password must be at least 8 characters long.\\") if errors: print(\\"<h1>Error</h1>\\") for error in errors: print(f\\"<p>{error}</p>\\") else: print(\\"<h1>Registration Successful</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Email: {email}</p>\\") ``` Ensure that you test the script thoroughly in a CGI-enabled environment and handle various edge cases as discussed in the documentation.","solution":"#!/usr/bin/env python3 import cgi import cgitb import re cgitb.enable() def validate_form_data(username, email, password): errors = [] if not username: errors.append(\\"Username is required.\\") if not email: errors.append(\\"Email is required.\\") elif not re.match(r\\"[^@]+@[^@]+.[^@]+\\", email): errors.append(\\"Invalid email format.\\") if not password: errors.append(\\"Password is required.\\") elif len(password) < 8: errors.append(\\"Password must be at least 8 characters long.\\") return errors def registration_form(): print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() username = form.getfirst(\\"username\\", \\"\\").strip() email = form.getfirst(\\"email\\", \\"\\").strip() password = form.getfirst(\\"password\\", \\"\\").strip() errors = validate_form_data(username, email, password) if errors: print(\\"<h1>Error</h1>\\") for error in errors: print(f\\"<p>{error}</p>\\") else: print(\\"<h1>Registration Successful</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Email: {email}</p>\\") if __name__ == \\"__main__\\": registration_form()"},{"question":"You are given a task to manage configurations for an application, which can come from multiple sources: default configurations, environment variables, and user-specified command-line arguments. You need to create a flexible and efficient configuration handler that updates from these sources without losing track of priorities. Additionally, you should be able to count how often certain configurations are accessed during runtime. To achieve this, you will use `ChainMap` from the `collections` module to manage nested dictionaries of configurations and `Counter` to track access frequencies. **Requirements:** 1. **Configuration Handler Class:** * Create a class `ConfigHandler` that uses `ChainMap` to manage multiple configuration dictionaries with the following methods: * `add_source(source: dict)`: Adds a new source of configurations. * `get_config(key: str)`: Fetches the configuration value for the given key. Updates the access count of the key using `Counter`. * `most_accessed_configs(n: int) -> list`: Returns a list of the `n` most accessed configuration keys and their access counts. 2. **Handling Configuration Updates:** * Default configurations always have the lowest priority and user-specified arguments have the highest. **Implementation Details:** 1. Use the `ChainMap` to combine these sources, with the highest priority source at the start of the chain. 2. Use the `Counter` class to count accesses. **Function Signatures:** ```python from collections import ChainMap, Counter class ConfigHandler: def __init__(self, *sources: dict): \'\'\' Initialize the ConfigHandler with any number of initial configuration sources. The sources are combined using ChainMap. \'\'\' pass def add_source(self, source: dict): \'\'\' Add a new source of configurations. This source will have higher priority over the existing ones. :param source: A dictionary representing the new configuration source. \'\'\' pass def get_config(self, key: str): \'\'\' Fetch the configuration value for the specified key and update the access count. :param key: The key for the configuration value to fetch. :return: The configuration value corresponding to the key. \'\'\' pass def most_accessed_configs(self, n: int) -> list: \'\'\' Return a list of the n most accessed configuration keys and their access counts. :param n: The number of top accessed configuration keys to return. :return: A list of tuples where each tuple contains a key and its access count. \'\'\' pass ``` **Constraints:** * The configurations are case-sensitive. * The `most_accessed_configs` method should return elements sorted by frequency (most accessed first). In case of ties, the order should be based on the keys\' order of first occurrence. **Example Usage:** ```python # Initialize with default configurations default_config = {\'theme\': \'light\', \'language\': \'en\', \'timeout\': 30} env_config = {\'timeout\': 60, \'version\': \'1.2.3\'} user_config = {\'theme\': \'dark\', \'version\': \'2.0.0\'} config_handler = ConfigHandler(default_config, env_config) # Add user-specific configurations config_handler.add_source(user_config) # Fetch configurations print(config_handler.get_config(\'theme\')) # Output: dark print(config_handler.get_config(\'language\')) # Output: en print(config_handler.get_config(\'timeout\')) # Output: 60 print(config_handler.get_config(\'version\')) # Output: 2.0.0 # Get most accessed configurations print(config_handler.most_accessed_configs(2)) # Output: [(\'theme\', 1), (\'language\', 1)] ```","solution":"from collections import ChainMap, Counter class ConfigHandler: def __init__(self, *sources: dict): Initialize the ConfigHandler with any number of initial configuration sources. The sources are combined using ChainMap. self.configs = ChainMap(*sources) self.access_count = Counter() def add_source(self, source: dict): Add a new source of configurations. This source will have higher priority over the existing ones. :param source: A dictionary representing the new configuration source. self.configs = self.configs.new_child(source) def get_config(self, key: str): Fetch the configuration value for the specified key and update the access count. :param key: The key for the configuration value to fetch. :return: The configuration value corresponding to the key. value = self.configs.get(key) # Use get to avoid KeyError; returns None if key is not found if value is not None: self.access_count[key] += 1 return value def most_accessed_configs(self, n: int) -> list: Return a list of the `n` most accessed configuration keys and their access counts. :param n: The number of top accessed configuration keys to return. :return: A list of tuples where each tuple contains a key and its access count. return self.access_count.most_common(n)"},{"question":"# Question You need to design a Python function `process_numbers` that takes a list of integers as input and performs the following operations based on the values: 1. If the integer is negative, it should be converted to zero. 2. If the integer is zero, it should stay zero. 3. If the integer is a prime number greater than zero, it should be left unchanged. 4. If the integer is a positive multiple of ten, it should be doubled. 5. For any other positive number, the function should return the integer incremented by 1. Your function should then return a new list with all values updated according to the above rules. # Input Format - A list of integers `nums`, where each integer is in the range of -1000 to 1000 inclusive. # Output Format - A list of integers where each integer is the result of applying the above rules to the corresponding integer in the input list. # Constraints - The length of the list `nums` will be between 1 and 1000. # Function Signature ```python def process_numbers(nums: List[int]) -> List[int]: ``` # Examples ```python # Example 1 input: [0, 5, -3, 20, 7, 8] output: [0, 5, 0, 40, 7, 9] # Example 2 input: [4, -10, 0, 30, 11, -2] output: [5, 0, 0, 60, 11, 0] ``` **Note:** - Make sure to use the `match` statement as part of your solution. - You may also make use of helper functions if necessary, to keep the code clean and readable. # Solution Template ```python from typing import List def process_numbers(nums: List[int]) -> List[int]: def is_prime(n: int) -> bool: if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True result = [] for num in nums: match num: case x if x < 0: result.append(0) case 0: result.append(0) case x if is_prime(x): result.append(x) case x if x > 0 and x % 10 == 0: result.append(x * 2) case x if x > 0: result.append(x + 1) return result ```","solution":"from typing import List def process_numbers(nums: List[int]) -> List[int]: def is_prime(n: int) -> bool: if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True result = [] for num in nums: match num: case x if x < 0: result.append(0) case 0: result.append(0) case x if is_prime(x): result.append(x) case x if x > 0 and x % 10 == 0: result.append(x * 2) case x if x > 0: result.append(x + 1) return result"},{"question":"Question: Advanced Seaborn Visualizations You are provided with a dataset in CSV format. Your task is to analyze and visualize the data using seaborn. The dataset contains information about different species of penguins, like their body measurements and other features. # Dataset The dataset is in CSV format with the following columns: - `species`: The species of the penguin. - `island`: The island where the penguin was observed. - `bill_length_mm`: The length of the penguin\'s bill (in millimeters). - `bill_depth_mm`: The depth of the penguin\'s bill (in millimeters). - `flipper_length_mm`: The length of the penguin\'s flipper (in millimeters). - `body_mass_g`: The body mass of the penguin (in grams). - `sex`: The sex of the penguin. - `year`: The year of observation. # Objective 1. **Data Loading and Preparation**: - Load the dataset using pandas. - Handle any missing data by dropping rows with missing values. 2. **Visualization Tasks**: - **Task 1**: Create a scatter plot showing the relationship between `bill_length_mm` and `bill_depth_mm`. Use different colors for different species. - **Task 2**: Generate a pair plot to visualize the relationships between all numerical variables. Use different colors for different species. - **Task 3**: Create a bar plot showing the average `body_mass_g` for each species, separated by sex. - **Task 4**: Produce a joint plot for `flipper_length_mm` and `body_mass_g`, including marginal histograms. - **Task 5**: Create a violin plot to show the distribution of `body_mass_g` for each species, separated by the island. 3. **Customization**: - Apply a theme of your choice to all plots. - Customize the scatter plot from **Task 1** to include a regression line with confidence intervals. # Constraints: - Use seaborn for all visualizations. - Ensure that all plots have appropriate labels, titles, and legends for clarity. - Customize the visual aesthetics to make the plots publication-ready. # Input: - A CSV file with the described dataset format. # Output: - Python code that generates the required visualizations. - Commentary on each step explaining the choices made in data preparation and visualization. # Example: Here\'s an example of how to start with the first task: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'penguins.csv\') # Drop rows with missing values df = df.dropna() # Set the theme sns.set_theme(style=\\"whitegrid\\") # Task 1: Scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\') plt.title(\'Bill Length vs Bill Depth by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.legend(title=\'Species\') plt.show() ``` Your task is to expand on this example and complete the remaining tasks.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_and_clean_data(filepath): Load the dataset from a CSV file and drop rows with missing values. df = pd.read_csv(filepath) df = df.dropna() return df def plot_scatter(df): Create a scatter plot showing the relationship between bill_length_mm and bill_depth_mm. Use different colors for different species and include a regression line with confidence intervals. sns.set_theme(style=\\"whitegrid\\") plt.figure(figsize=(10, 6)) scatter = sns.scatterplot(data=df, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', style=\'species\') regplot = sns.regplot(data=df, x=\'bill_length_mm\', y=\'bill_depth_mm\', scatter=False, color=\'gray\') plt.title(\'Bill Length vs Bill Depth by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.legend(title=\'Species\') plt.show() def plot_pair(df): Generate a pair plot to visualize the relationships between all numerical variables. Use different colors for different species. sns.pairplot(df, hue=\'species\') plt.show() def plot_bar(df): Create a bar plot showing the average body_mass_g for each species, separated by sex. plt.figure(figsize=(12, 8)) sns.barplot(data=df, x=\'species\', y=\'body_mass_g\', hue=\'sex\', ci=None) plt.title(\'Average Body Mass of Penguins by Species and Sex\') plt.xlabel(\'Species\') plt.ylabel(\'Body Mass (g)\') plt.legend(title=\'Sex\') plt.show() def plot_joint(df): Produce a joint plot for flipper_length_mm and body_mass_g, including marginal histograms. sns.jointplot(data=df, x=\'flipper_length_mm\', y=\'body_mass_g\', hue=\'species\', kind=\'scatter\', marginal_kws=dict(bins=25, fill=True)) plt.show() def plot_violin(df): Create a violin plot to show the distribution of body_mass_g for each species, separated by the island. plt.figure(figsize=(12, 8)) sns.violinplot(data=df, x=\'species\', y=\'body_mass_g\', hue=\'island\', split=True) plt.title(\'Distribution of Body Mass by Species and Island\') plt.xlabel(\'Species\') plt.ylabel(\'Body Mass (g)\') plt.legend(title=\'Island\') plt.show() def main(filepath): df = load_and_clean_data(filepath) plot_scatter(df) plot_pair(df) plot_bar(df) plot_joint(df) plot_violin(df) if __name__ == \\"__main__\\": # Replace \'penguins.csv\' with your actual dataset file path main(\'penguins.csv\')"},{"question":"# Custom K-Nearest Neighbors Classifier with Precomputed Graphs You are required to implement a custom K-Nearest Neighbors Classifier utilizing the scikit-learn `sklearn.neighbors` module. Your classifier should be able to handle both dense and sparse datasets efficiently by precomputing the nearest neighbors graph. Additionally, your classifier should use the Minkowski distance metric. Requirements: 1. **Class Definition:** Define a class `CustomKNNClassifier` with the following methods: - `__init__(self, n_neighbors=5, p=2, algorithm=\'auto\', mode=\'distance\')`: Initializes the classifier with the number of neighbors (`n_neighbors`), Minkowski distance parameter (`p`), the algorithm for neighbors search (`algorithm`), and mode for the graph (\'distance\' or \'connectivity\'). - `fit(self, X, y)`: Fits the classifier using the training data `X` and labels `y`. - `predict(self, X)`: Predicts the labels for the input data `X` using the fitted model. - `score(self, X, y)`: Returns the mean accuracy on the given test data and labels. 2. **Parameters:** - `n_neighbors`: The number of neighbors to use for classification (default is 5). - `p`: Power parameter for the Minkowski distance metric (default is 2, which corresponds to the Euclidean distance). - `algorithm`: Algorithm used to compute the nearest neighbors (`\'auto\'`, `\'ball_tree\'`, `\'kd_tree\'`, `\'brute\'`). - `mode`: Mode for the nearest neighbors graph (`\'distance\'` or `\'connectivity\'`). 3. **Precomputation:** - Compute the nearest neighbors graph during the `fit` method using `kneighbors_graph` or `radius_neighbors_graph`, based on the specified mode. 4. **Class Prediction:** - Implement the `predict` method to classify new data points by the majority vote of their nearest neighbors. - Use uniform weights for voting. 5. **Performance:** - Ensure that your implementation is efficient for both dense and sparse matrices. Example Usage: ```python from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from scipy import sparse # Generate a random classification problem X, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, random_state=42) X_sparse = sparse.csr_matrix(X) # Convert to sparse matrix # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.2, random_state=42) # Initialize and train the custom KNN classifier clf = CustomKNNClassifier(n_neighbors=3, p=2, algorithm=\'auto\', mode=\'distance\') clf.fit(X_train, y_train) # Predict and evaluate predictions = clf.predict(X_test) accuracy = clf.score(X_test, y_test) print(f\\"Accuracy: {accuracy:.2f}\\") ``` Expected Output Format: - Implement the `CustomKNNClassifier` class with the specified methods and parameters. - Ensure your code runs efficiently on both dense and sparse datasets. - The `accuracy` output should reflect the mean accuracy of the classifier on the test data.","solution":"from sklearn.neighbors import NearestNeighbors from sklearn.metrics import accuracy_score import numpy as np class CustomKNNClassifier: def __init__(self, n_neighbors=5, p=2, algorithm=\'auto\', mode=\'distance\'): self.n_neighbors = n_neighbors self.p = p self.algorithm = algorithm self.mode = mode self.nn_model = None self.X_train = None self.y_train = None def fit(self, X, y): self.X_train = X self.y_train = y self.nn_model = NearestNeighbors(n_neighbors=self.n_neighbors, p=self.p, algorithm=self.algorithm) self.nn_model.fit(X) def predict(self, X): distances, indices = self.nn_model.kneighbors(X) predictions = [] for idx in range(X.shape[0]): nearest_labels = self.y_train[indices[idx]] predicted_label = np.bincount(nearest_labels).argmax() predictions.append(predicted_label) return np.array(predictions) def score(self, X, y): predictions = self.predict(X) return accuracy_score(y, predictions)"},{"question":"**Question: Creating and Applying Custom Color Palettes with Seaborn** You are given a dataset containing information about different species of flowers, including their sepal length, sepal width, petal length, and petal width. Your task is to create a custom color palette by blending specified colors and then apply this palette to visualize the distribution of sepal length for different species using a seaborn violin plot. **Objective:** 1. Define and create a custom color palette using `seaborn.blend_palette`. 2. Apply this palette to a violin plot to visualize the distribution of sepal length for different species. **Input Data:** A sample of the dataset (assuming it has columns `species`, `sepal_length`, `sepal_width`, `petal_length`, `petal_width`) is provided below: ```python import pandas as pd data = { \'species\': [\'setosa\', \'versicolor\', \'virginica\', \'setosa\', \'versicolor\', ...], \'sepal_length\': [5.1, 7.0, 6.3, 4.9, 6.4, ...], \'sepal_width\': [3.5, 3.2, 3.3, 3.0, 2.9, ...], \'petal_length\': [1.4, 4.7, 6.0, 1.4, 4.5, ...], \'petal_width\': [0.2, 1.4, 2.5, 0.2, 1.5, ...] } df = pd.DataFrame(data) ``` **Requirements:** 1. Use seaborn to create a custom color palette by blending three colors: `\'#f4a582\'`, `\'#92c5de\'`, and `\'xkcd:sunflower\'`. 2. Visualize the distribution of `sepal_length` for each species using a violin plot, applying your custom palette. **Function Signature:** ```python def create_and_apply_palette(df: pd.DataFrame) -> None: pass ``` **Constraints:** - You must use the `seaborn.blend_palette` function for creating the palette. - The plot should be easy to interpret with proper labels and title. **Example:** The following is an example of the required output, demonstrating a violin plot for the `sepal_length` distribution across different species: ```text <violin plot image here showing different colors for each species> ``` Use appropriate seaborn functions to add labels and title to your plot. **Note:** Ensure that your solution is efficient and leverages seaborn functionalities effectively.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_and_apply_palette(df: pd.DataFrame) -> None: Creates a custom color palette and applies it to a violin plot to visualize the distribution of sepal length for different species. Parameters: df (pd.DataFrame): The input dataframe containing species and measurements of flowers. Returns: None # Create a custom color palette by blending specified colors custom_palette = sns.blend_palette([\'#f4a582\', \'#92c5de\', \'xkcd:sunflower\'], n_colors=len(df[\'species\'].unique())) # Create the violin plot with the custom palette sns.violinplot(x=\'species\', y=\'sepal_length\', data=df, palette=custom_palette) # Add title and labels plt.title(\\"Distribution of Sepal Length by Species\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Length\\") # Show the plot plt.show()"},{"question":"Objective Implement a class-based solution to manage reading and writing data in both text and binary formats using Python\'s `io` module. Your solution should demonstrate an understanding of encoding and decoding, buffering, and different types of I/O streams. Problem Statement You are required to implement a class `FileManager` that can handle both text and binary files. This class should support reading from and writing to files in various formats. Class Definition ```python class FileManager: def __init__(self, file_path, mode=\'r\', encoding=\'utf-8\', buffer_size=None): Initialize the FileManager instance. :param file_path: str, Path to the file to be managed. :param mode: str, Mode in which the file is opened (default \'r\'). :param encoding: str, Encoding for the text files (default \'utf-8\'). :param buffer_size: int, Buffer size for the buffered I/O (None for default). # Implementation goes here def read_text(self): Read the entire content of the text file. :return: str, Content of the file. # Implementation goes here def write_text(self, data): Write the given data to the text file. :param data: str, Data to write. # Implementation goes here def read_binary(self): Read the entire content of the binary file. :return: bytes, Content of the file. # Implementation goes here def write_binary(self, data): Write the given data to the binary file. :param data: bytes, Data to write. # Implementation goes here def read_lines(self): Read the lines of a text file and return them as a list of strings. :return: list of str, Lines in the file. # Implementation goes here def write_lines(self, lines): Write the given list of lines to the text file. :param lines: list of str, Lines to write. # Implementation goes here def close(self): Close the file. # Implementation goes here ``` Constraints - The `FileManager` class should manage file resources effectively, ensuring files are closed appropriately after operations. - The methods `read_text` and `write_text` should handle text files. - The methods `read_binary` and `write_binary` should handle binary files. - The `read_lines` method should read a text file and return the lines as a list of strings. - The `write_lines` method should write a list of strings as lines to a text file. - Buffering should be handled appropriately where specified. Example Usage ```python # Example usage of FileManager class file_manager = FileManager(\'example.txt\', mode=\'w+\', encoding=\'utf-8\') # Write text to the file file_manager.write_text(\\"Hello World!\\") # Read text from the file print(file_manager.read_text()) # Write lines to the file file_manager.write_lines([\\"Line 1\\", \\"Line 2\\", \\"Line 3\\"]) # Read lines from the file print(file_manager.read_lines()) # Close the file file_manager.close() # Binary file handling binary_manager = FileManager(\'example.bin\', mode=\'wb+\') # Write binary data to the file binary_manager.write_binary(b\\"x00x01x02x03\\") # Read binary data from the file print(binary_manager.read_binary()) # Close the binary file binary_manager.close() ``` Good luck!","solution":"import io class FileManager: def __init__(self, file_path, mode=\'r+\', encoding=\'utf-8\', buffer_size=None): self.file_path = file_path self.mode = mode self.encoding = encoding self.buffer_size = buffer_size self.file = None if buffer_size: self.file = open(file_path, mode, buffering=buffer_size, encoding=encoding if \'b\' not in mode else None) else: self.file = open(file_path, mode, encoding=encoding if \'b\' not in mode else None) def read_text(self): with open(self.file_path, \'r\', encoding=self.encoding) as file: return file.read() def write_text(self, data): with open(self.file_path, \'w\', encoding=self.encoding) as file: file.write(data) def read_binary(self): with open(self.file_path, \'rb\') as file: return file.read() def write_binary(self, data): with open(self.file_path, \'wb\') as file: file.write(data) def read_lines(self): with open(self.file_path, \'r\', encoding=self.encoding) as file: return file.readlines() def write_lines(self, lines): with open(self.file_path, \'w\', encoding=self.encoding) as file: file.writelines(lines) def close(self): if self.file: self.file.close() self.file = None"},{"question":"Objective Your task is to demonstrate your knowledge and understanding of the seaborn library to create and customize scatter plots. You will be provided with a dataset, and you are required to generate multiple scatter plots that clearly and effectively communicate the relationships between variables in the data. Dataset You will use the \\"tips\\" dataset available through seaborn. To load this dataset, you can use the following code: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` The dataset contains the following columns: - `total_bill`: Total bill for the meal. - `tip`: Tip given by the customer. - `sex`: Gender of the person paying the bill. - `smoker`: Whether the person is a smoker or not. - `day`: Day of the week. - `time`: Time of the day (Lunch or Dinner). - `size`: Number of people at the table. Task Write a function `create_custom_scatterplots` that generates the following scatter plots: 1. A scatter plot comparing the `total_bill` and `tip` columns. 2. A scatter plot similar to Plot 1 but differentiated by `time` using the `hue` parameter. 3. A scatter plot similar to Plot 2 but also differs marker styles based on the `day` of the week using the `style` parameter. 4. A scatter plot treating the `size` column as a quantitative variable and mapping its values to both `hue` and `size`. 5. Use `relplot` to create scatter plots of `total_bill` vs `tip`, grouped by `time` and `day`, with different subplots for each combination. Function Signature and Specifications ```python def create_custom_scatterplots(): This function generates and displays the required scatter plots using the seaborn library. ``` Constraints - Your code should follow good coding practices. - Include appropriate titles and axis labels for each plot. - Customize the plots to enhance readability and aesthetics, such as adjusting `legend`, `sizes`, `colors`, etc., where appropriate. - Performance is not a key concern as long as the plots are generated accurately. Example Output When your function `create_custom_scatterplots` is called, it should generate and display five customized scatter plots as specified above. **Note**: The function does not need to take any input parameters and does not need to return any value. It should only generate and display the plots. ```python def create_custom_scatterplots(): import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Plot 1: Simple Scatter Plot plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Scatter Plot of Total Bill vs Tip\\") plt.show() # Plot 2: Scatter Plot with hue by \'time\' plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.title(\\"Scatter Plot of Total Bill vs Tip with Hue by Time\\") plt.show() # Plot 3: Scatter Plot with hue by \'time\' and style by \'day\' plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\\"Scatter Plot of Total Bill vs Tip with Hue by Time and Style by Day\\") plt.show() # Plot 4: Scatter Plot with size mapped to quantitative \'size\' and hue to \'size\' plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Scatter Plot of Total Bill vs Tip with Hue and Size by Size\\") plt.show() # Plot 5: Relplot scatter plot grouped by \'time\' and \'day\' sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.show() ```","solution":"def create_custom_scatterplots(): import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Plot 1: Simple Scatter Plot plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Scatter Plot of Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show() # Plot 2: Scatter Plot with hue by \'time\' plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.title(\\"Scatter Plot of Total Bill vs Tip with Hue by Time\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show() # Plot 3: Scatter Plot with hue by \'time\' and style by \'day\' plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\\"Scatter Plot of Total Bill vs Tip with Hue by Time and Style by Day\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Time and Day\\") plt.show() # Plot 4: Scatter Plot with size mapped to quantitative \'size\' and hue to \'size\' plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Scatter Plot of Total Bill vs Tip with Hue and Size by Size\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Size of the Table\\") plt.show() # Plot 5: Relplot scatter plot grouped by \'time\' and \'day\' sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.show()"},{"question":"**Objective**: Demonstrate your understanding of Python\'s `distutils` module by creating a custom setup script using available metadata dynamically. # Problem Statement You are required to write a custom setup script in Python using the `distutils` module. This script should dynamically generate a `setup` call based on metadata provided as input. Your solution should be robust enough to handle different types of distributions (e.g., pure Python, extension modules). # Input: A JSON string containing metadata about the Python package. The structure of the JSON is as follows: ```json { \\"name\\": \\"example_pkg\\", \\"version\\": \\"0.1\\", \\"description\\": \\"An example package\\", \\"packages\\": [\\"example_pkg\\"], \\"module\\": \\"example_module\\", \\"author\\": \\"Author Name\\", \\"author_email\\": \\"author@example.com\\" } ``` # Output: A dynamically generated setup script (`setup.py`) as a string, which can be subsequently executed to build and distribute the package. # Constraints: 1. If `\\"packages\\"` key is present, use it to list packages in the `setup` call. 2. If `\\"module\\"` key is present, use it to list individual modules. 3. Handle cases where only metadata without packages or modules is provided. 4. Ensure your script is executable and correctly identifies the provided metadata keys to generate the `setup` script. 5. Validate the input to ensure all required keys are present. # Example: **Input:** ```json { \\"name\\": \\"example_pkg\\", \\"version\\": \\"0.1\\", \\"description\\": \\"An example package\\", \\"packages\\": [\\"example_pkg\\"], \\"author\\": \\"Author Name\\", \\"author_email\\": \\"author@example.com\\" } ``` **Output:** ```python from distutils.core import setup setup( name=\'example_pkg\', version=\'0.1\', description=\'An example package\', packages=[\'example_pkg\'], author=\'Author Name\', author_email=\'author@example.com\' ) ``` Note: You might include extension modules if specified in the input. # Evaluation Criteria: 1. Correctness of the generated `setup` script. 2. Handling of different metadata inputs. 3. Robustness and error handling in case of missing or malformed inputs. 4. Code readability and documentation.","solution":"import json def generate_setup_script(json_input): Generate a setup script from JSON input containing package metadata. Args: json_input (str): String representation of the JSON input containing package metadata. Returns: str: A string containing the setup script. data = json.loads(json_input) # Validate input required_keys = [\\"name\\", \\"version\\", \\"description\\", \\"author\\", \\"author_email\\"] for key in required_keys: if key not in data: raise ValueError(f\\"Missing required key: {key}\\") # Start generating the setup script lines = [ \\"from distutils.core import setup\\", \\"\\", \\"setup(\\" ] # Add basic metadata for key in required_keys: lines.append(f\\" {key}=\'{data[key]}\',\\") # Add packages if provided if \\"packages\\" in data: lines.append(f\\" packages={data[\'packages\']},\\") # Add module if provided if \\"module\\" in data: lines.append(f\\" py_modules=[\'{data[\'module\']}\'],\\") # Finalize the script lines.append(\\")\\") return \\"n\\".join(lines)"},{"question":"# Question: Implementing a Simplified Object Management System in Python Python manages objects in memory using a complex system involving reference counts, type objects, and garbage collection. In this exercise, you\'ll implement a simplified version of an object management system in Python. Requirements: 1. **SimulatedObject**: Implement a class `SimulatedObject` that mimics a Python object with the following attributes: - `type`: A string representing the type of the object. - `reference_count`: An integer representing the reference count of the object. - `size`: An integer representing the size of the object in bytes (default is 1). 2. **ObjectManager**: Implement a class `ObjectManager` that manages `SimulatedObject` instances and supports the following operations: - `create_object(type_name: str, size: int = 1) -> SimulatedObject`: Creates a new `SimulatedObject` with the specified `type` and `size`. The new object should have an initial reference count of 1. - `increase_ref(obj: SimulatedObject) -> None`: Increases the reference count of the specified object by 1. - `decrease_ref(obj: SimulatedObject) -> None`: Decreases the reference count of the specified object by 1. If the reference count reaches 0, the object should be deleted (i.e., removed from the internal tracking system). - `delete_object(obj: SimulatedObject) -> None`: Deletes the specified object regardless of its reference count. - `get_object_info(obj: SimulatedObject) -> str`: Returns a string containing the object\'s type, reference count, and size. 3. **Constraints**: - Ensure that when an object\'s reference count reaches 0, it is properly deleted. - Perform proper error handling for operations such as trying to increase/decrease the reference count of an object that does not exist. Example Usage: ```python # Create an object manager manager = ObjectManager() # Create a new object of type \'CustomType\' obj1 = manager.create_object(\'CustomType\', size=10) print(manager.get_object_info(obj1)) # Output: \\"Type: CustomType, Ref Count: 1, Size: 10\\" # Increase the reference count manager.increase_ref(obj1) print(manager.get_object_info(obj1)) # Output: \\"Type: CustomType, Ref Count: 2, Size: 10\\" # Decrease the reference count manager.decrease_ref(obj1) print(manager.get_object_info(obj1)) # Output: \\"Type: CustomType, Ref Count: 1, Size: 10\\" # Decrease the reference count to 0, object should be deleted manager.decrease_ref(obj1) print(manager.get_object_info(obj1)) # Output: Error (object does not exist) # Create another object obj2 = manager.create_object(\'AnotherType\', size=5) print(manager.get_object_info(obj2)) # Output: \\"Type: AnotherType, Ref Count: 1, Size: 5\\" manager.delete_object(obj2) print(manager.get_object_info(obj2)) # Output: Error (object does not exist) ``` Your implementation should ensure efficient and correct management of simulated objects, reflecting a basic understanding of Python\'s internal object handling mechanisms.","solution":"class SimulatedObject: def __init__(self, type: str, size: int = 1): self.type = type self.size = size self.reference_count = 1 class ObjectManager: def __init__(self): self.objects = set() def create_object(self, type_name: str, size: int = 1) -> SimulatedObject: new_object = SimulatedObject(type_name, size) self.objects.add(new_object) return new_object def increase_ref(self, obj: SimulatedObject) -> None: if obj in self.objects: obj.reference_count += 1 def decrease_ref(self, obj: SimulatedObject) -> None: if obj in self.objects: obj.reference_count -= 1 if obj.reference_count == 0: self.objects.remove(obj) def delete_object(self, obj: SimulatedObject) -> None: if obj in self.objects: self.objects.remove(obj) def get_object_info(self, obj: SimulatedObject) -> str: if obj in self.objects: return f\\"Type: {obj.type}, Ref Count: {obj.reference_count}, Size: {obj.size}\\" else: raise ValueError(\\"Object does not exist.\\")"},{"question":"Given the \'penguins\' dataset from seaborn\'s sample datasets, write a function `custom_plot` that generates a complex plot using the seaborn `displot` function. The function should be capable of producing a detailed visualization including multiple subplots, customized axis labels, and titles. **Function Signature:** ```python def custom_plot() -> None: pass ``` # Requirements: 1. Load the \'penguins\' dataset using `sns.load_dataset(\\"penguins\\")`. 2. Create a KDE (Kernel Density Estimate) plot for `flipper_length_mm` and `bill_length_mm` with a rug plot at the bottom using `sns.displot`. 3. The KDE plot should be split into separate subplots (facets) based on the `species` and `sex` columns in the dataset. 4. The height of each subplot should be 5 and the aspect ratio should be 1. 5. Customize the main plot to have: - X-axis label: \\"Flipper Length (mm)\\" - Y-axis label: \\"Bill Length (mm)\\" - Titles for each subplot indicating the `species` name and `sex` (formatted as: \\"Species: {species_name}, Sex: {sex}\\") 6. Return the resulting seaborn `FacetGrid` object. # Example: ```python import seaborn as sns import matplotlib.pyplot as plt def custom_plot(): penguins = sns.load_dataset(\\"penguins\\") g = sns.displot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\", rug=True, col=\\"species\\", row=\\"sex\\", height=5, aspect=1) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Bill Length (mm)\\") g.set_titles(\\"Species: {col_name}, Sex: {row_name}\\") plt.show() return g # To verify the function, simply call it: custom_plot() ``` **Constraints:** - Ensure the function runs without errors and generates the required plot. - Use the seaborn and matplotlib packages as needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plot(): Loads the penguins dataset and generates a KDE plot with subplots based on species and sex. The plots include rug plots and customized axis labels and titles. penguins = sns.load_dataset(\\"penguins\\") # Generate the displot with KDE and rug plots, faceted by species and sex g = sns.displot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\", rug=True, col=\\"species\\", row=\\"sex\\", height=5, aspect=1 ) # Customize the axis labels and subplot titles g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Bill Length (mm)\\") g.set_titles(\\"Species: {col_name}, Sex: {row_name}\\") # Show the plot plt.show() return g"},{"question":"# Telnet Interaction Assessment You are required to implement a function that connects to a Telnet server, logs in with given credentials, executes a series of commands, and retrieves the output. The function should also handle different possible Telnet server responses appropriately, utilizing the various methods provided by the `telnetlib` module. Function Description Implement the function `telnet_interaction(host, user, password, commands)` that performs the following steps: 1. Establishes a connection to the Telnet server running at `host`. 2. Logs in using the provided `user` and `password`. 3. Executes each command in the `commands` list sequentially on the server. 4. Collects and returns the output of these commands as a single string. Parameters - `host` (str): The hostname or IP address of the Telnet server. - `user` (str): The username for the login. - `password` (str): The password for the login. - `commands` (List[str]): A list of command strings to be executed sequentially on the server. Returns - `output` (str): A single string containing the concatenated output of all executed commands. Constraints - You may assume that the Telnet server uses the default Telnet port (23). - The Telnet server prompts are standard (`\\"login: \\"` for username and `\\"Password: \\"` for password). Example ```python host = \\"localhost\\" user = \\"my_user\\" password = \\"my_password\\" commands = [\\"ls\\", \\"pwd\\", \\"whoami\\"] output = telnet_interaction(host, user, password, commands) print(output) ``` Implementation Details - Ensure to handle situations where the connection is closed unexpectedly by catching `EOFError` and other relevant exceptions. - Use `read_until` method for waiting for specific prompts. - Encode the username and password as ASCII bytes before writing to the connection. - Handle possible timeouts during read operations. Below is a skeletal implementation to get you started: ```python import telnetlib import getpass def telnet_interaction(host, user, password, commands): try: with telnetlib.Telnet(host) as tn: tn.read_until(b\\"login: \\") tn.write(user.encode(\'ascii\') + b\\"n\\") if password: tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") output = [] for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") output.append(tn.read_until(b\\" \\").decode(\'ascii\')) # Adjust the prompt based on the server configuration return \'\'.join(output) except Exception as e: return str(e) # Example usage: host = \\"localhost\\" user = input(\\"Enter your remote account: \\") password = getpass.getpass() commands = [\\"ls\\", \\"pwd\\", \\"whoami\\"] output = telnet_interaction(host, user, password, commands) print(output) ``` Ensure to test your function thoroughly with a Telnet server to validate the accuracy and robustness of your implementation.","solution":"import telnetlib def telnet_interaction(host, user, password, commands): Connects to a Telnet server, logs in, executes provided commands, and returns the output. Parameters: host (str): The hostname or IP address of the Telnet server. user (str): The username for the login. password (str): The password for the login. commands (list of str): A list of command strings to be executed sequentially on the server. Returns: str: A single string containing the concatenated output of all executed commands. try: with telnetlib.Telnet(host) as tn: tn.read_until(b\\"login: \\") tn.write(user.encode(\'ascii\') + b\\"n\\") if password: tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") output = [] for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") output.append(tn.read_until(b\\" \\").decode(\'ascii\')) # Adjust the prompt based on the server configuration return \'\'.join(output) except EOFError: return \\"Connection closed unexpectedly.\\" except Exception as e: return str(e)"},{"question":"# PyTorch Coding Assessment **Objective**: Implement a function in PyTorch that performs specific tensor computations while ensuring numerical stability. **Description**: You\'ve been given a task to implement a function that calculates the inverse of a given matrix and its condition number. This function should handle potential numerical instability and extreme values. # Function Signature: ```python import torch def stable_matrix_inverse_and_cond(matrix: torch.Tensor) -> (torch.Tensor, float): Computes the inverse of a given matrix and its condition number. Args: - matrix (torch.Tensor): A 2D tensor representing the input matrix. The matrix should be of scientific significance, implying it deals with real-world fluctuating values (not perfectly conditioned). Returns: - inverse (torch.Tensor): The inverse of the input matrix. - condition_number (float): The condition number of the input matrix. Constraints: - The input tensor will always be a 2D square matrix (n x n). - The function should detect and handle non-invertible matrices gracefully. - Use appropriate numerical precision where necessary to ensure stability. Note: - Make sure to handle tensors on both CPU and GPU. - Use PyTorch\'s linear algebra functions. pass ``` # Requirements: 1. **Input Specifications**: - The input tensor `matrix` is a 2D square tensor of shape `(n, n)` where `n` is between 1 and 1000. Values can vary widely in magnitude. 2. **Output Specifications**: - Return a tuple where the first element is the inverse of the input matrix and the second element is the condition number of the matrix. - If the matrix is non-invertible, the function should return `(None, inf)`. 3. **Constraints**: - Ensure that the computations are stable. Use `float64` precision if necessary and handle potential overflow or underflow issues. - Detect non-invertible cases and return appropriate values without crashing. - Implement proper error handling for numerical issues such as `nan` or `inf` values. 4. **Performance**: - The implementation should be efficient and able to handle matrices up to size `1000 x 1000` within reasonable time limits. # Example: ```python # Example input matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32) # Function call inverse, cond = stable_matrix_inverse_and_cond(matrix) # Expected output # (tensor([[-2.0000, 1.0000], # [ 1.5000, -0.5000]], dtype=torch.float64), # 14.9331) ``` Use PyTorch functions like `torch.inverse`, `torch.linalg.cond`, and other linear algebra functions to implement the solution while maintaining numerical stability.","solution":"import torch def stable_matrix_inverse_and_cond(matrix: torch.Tensor) -> (torch.Tensor, float): Computes the inverse of a given matrix and its condition number. Args: - matrix (torch.Tensor): A 2D tensor representing the input matrix. The matrix should be of scientific significance, implying it deals with real-world fluctuating values (not perfectly conditioned). Returns: - inverse (torch.Tensor): The inverse of the input matrix. - condition_number (float): The condition number of the input matrix. matrix = matrix.to(dtype=torch.float64) try: inverse = torch.linalg.inv(matrix) except RuntimeError: # Matrix is non-invertible return (None, float(\'inf\')) condition_number = torch.linalg.cond(matrix).item() return (inverse, condition_number)"},{"question":"# Command Line Tool to Organize Files You are tasked with creating a command-line tool using Python that helps users organize files in a directory. This tool will use the `argparse` module to parse command-line arguments. The tool should support the following functionalities: 1. **List Files**: List all files in the directory. 2. **Move Files**: Move files to a specified folder based on their extension. 3. **Delete Files**: Delete all files with a specified extension. Your task is to implement the `organize_files.py` script with the following features: **Required Positional Arguments**: - `action`: The action to be performed. Can be `list`, `move`, or `delete`. - `directory`: The target directory to perform the action on. **Optional Arguments**: - `-e`, `--extension`: File extension to filter by (e.g., `.txt`, `.jpg`). Needed for `move` and `delete` actions. - `-d`, `--destination`: Destination directory for the `move` action. This argument is required if the action is `move`. # Examples: 1. **List all files** in the `./documents` directory: ```bash python organize_files.py list ./documents ``` 2. **Move all `.txt` files** from the `./downloads` directory to the `./texts` directory: ```bash python organize_files.py move ./downloads -e .txt -d ./texts ``` 3. **Delete all `.jpg` files** in the `./pictures` directory: ```bash python organize_files.py delete ./pictures -e .jpg ``` # Constraints: - If the action is `move` or `delete`, the `--extension` argument is required. - If the action is `move`, the `--destination` argument is required. - The script should provide user-friendly output messages for each action. - Handle errors gracefully, such as invalid directories or missing required arguments. # Implementation Tips: - Utilize the `argparse` module to handle command-line arguments. - Implement file operations using modules such as `os` and `shutil`. - Ensure proper validation of inputs and provide helpful error messages. # Sample Usage: 1. List all files in a directory: ```python import argparse import os parser = argparse.ArgumentParser(description=\\"Organize files in a directory.\\") parser.add_argument(\\"action\\", choices=[\\"list\\", \\"move\\", \\"delete\\"], help=\\"Action to perform\\") parser.add_argument(\\"directory\\", help=\\"Target directory\\") parser.add_argument(\\"-e\\", \\"--extension\\", help=\\"File extension to filter by\\") parser.add_argument(\\"-d\\", \\"--destination\\", help=\\"Destination directory for move action\\") args = parser.parse_args() if args.action == \\"list\\": # Code to list all files elif args.action == \\"move\\": # Code to move files elif args.action == \\"delete\\": # Code to delete files ``` Complete the implementation for the above sample usage by implementing the actual file operations for each action (`list`, `move`, `delete`) keeping in mind the constraints and requirements.","solution":"import argparse import os import shutil import sys def list_files(directory): try: files = os.listdir(directory) for file in files: print(file) except FileNotFoundError: print(f\\"Error: The directory {directory} does not exist.\\") def move_files(directory, extension, destination): try: if not os.path.exists(destination): os.makedirs(destination) files_moved = False for file in os.listdir(directory): if file.endswith(extension): src = os.path.join(directory, file) dst = os.path.join(destination, file) shutil.move(src, dst) print(f\\"Moved: {file}\\") files_moved = True if not files_moved: print(f\\"No files with extension {extension} found in {directory}.\\") except FileNotFoundError: print(f\\"Error: The directory {directory} does not exist.\\") def delete_files(directory, extension): try: files_deleted = False for file in os.listdir(directory): if file.endswith(extension): os.remove(os.path.join(directory, file)) print(f\\"Deleted: {file}\\") files_deleted = True if not files_deleted: print(f\\"No files with extension {extension} found in {directory}.\\") except FileNotFoundError: print(f\\"Error: The directory {directory} does not exist.\\") except Exception as e: print(f\\"Error: {e}\\") def main(): parser = argparse.ArgumentParser(description=\\"Organize files in a directory.\\") parser.add_argument(\\"action\\", choices=[\\"list\\", \\"move\\", \\"delete\\"], help=\\"Action to perform\\") parser.add_argument(\\"directory\\", help=\\"Target directory\\") parser.add_argument(\\"-e\\", \\"--extension\\", help=\\"File extension to filter by\\", required=\'move\' in sys.argv or \'delete\' in sys.argv) parser.add_argument(\\"-d\\", \\"--destination\\", help=\\"Destination directory for move action\\", required=\'move\' in sys.argv) args = parser.parse_args() if args.action == \\"list\\": list_files(args.directory) elif args.action == \\"move\\": move_files(args.directory, args.extension, args.destination) elif args.action == \\"delete\\": delete_files(args.directory, args.extension) if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: To evaluate the student\'s understanding of the `codecs` module in Python, specifically their ability to create and register a custom codec with encoding, decoding, error handling, and stream support. **Question**: You are required to implement a custom codec for encoding and decoding using a simple substitution cipher known as the \\"Caesar Cipher\\". The Caesar Cipher works by shifting each letter of the text by a fixed number of positions in the alphabet. For example, with a shift of 3, \'A\' becomes \'D\', \'B\' becomes \'E\', and so on. This codec should support both encoding and decoding of text. Additionally, you need to implement incremental encoding and decoding for handling streams of data. Ensure that your codec also supports different error handling schemes, such as \\"ignore\\" and \\"replace\\". **Requirements**: 1. Implement the `CaesarCodec` class with the following methods: - `encode(input, errors=\'strict\')` - `decode(input, errors=\'strict\')` 2. Implement the `IncrementalCaesarEncoder` and `IncrementalCaesarDecoder` classes to handle incremental encoding and decoding. 3. Register your custom codec so that it can be used with the `codecs` module functions like `codecs.encode()`, `codecs.decode()`, and `codecs.open()`. 4. Your codec should handle the following error schemes: - `\'strict\'` - `\'ignore\'` - `\'replace\'` 5. Create a stream encoder and decoder using your custom codec. 6. Write a test function `test_caesar_codec()` to validate the functionality of your codec with different inputs and error schemes. **Function Definitions**: - `class CaesarCodec` - `def encode(self, input: str, errors: str = \'strict\') -> Tuple[str, int]` - `def decode(self, input: bytes, errors: str = \'strict\') -> Tuple[str, int]` - `class IncrementalCaesarEncoder(codecs.IncrementalEncoder)` - `def encode(self, input: str, final: bool = False) -> bytes` - `class IncrementalCaesarDecoder(codecs.IncrementalDecoder)` - `def decode(self, input: bytes, final: bool = False) -> str` - `def test_caesar_codec() -> None` **Constraints**: - The shift for the Caesar Cipher should be hardcoded as 3. - Assume the input text will only contain alphabetic characters and spaces. - The encoder should convert all input text to uppercase before encoding. # Example Usage: ```python import codecs # Register the custom Caesar codec codecs.register(CaesarCodec().encode) # Encode using the custom codec encoded_text = codecs.encode(\\"HELLO WORLD\\", encoding=\\"caesar\\") print(encoded_text) # Output: KHOOR ZRUOG # Decode using the custom codec decoded_text = codecs.decode(encoded_text, encoding=\\"caesar\\") print(decoded_text) # Output: HELLO WORLD # Use the incremental encoder and decoder with streams with codecs.open(\\"sample.txt\\", mode=\\"w\\", encoding=\\"caesar\\") as f: f.write(\\"PIECE BY PIECE WE TRANSFORM TEXT.\\") # Read back the encoded content with codecs.open(\\"sample.txt\\", mode=\\"r\\", encoding=\\"caesar\\") as f: content = f.read() print(content) # Output should be the original text # Test the codec functionality test_caesar_codec() ``` **Submission Requirements**: Submit the implementation of the `CaesarCodec`, `IncrementalCaesarEncoder`, `IncrementalCaesarDecoder`, the registration of the codec, and the `test_caesar_codec` function.","solution":"import codecs class CaesarCodec(codecs.Codec): shift = 3 def encode(self, input, errors=\'strict\'): return (self.caesar_shift(input), len(input)) def decode(self, input, errors=\'strict\'): return (self.caesar_shift(input, decode=True), len(input)) def caesar_shift(self, text, decode=False): shifted_text = [] shift = -self.shift if decode else self.shift for char in text: if char.isalpha(): start = ord(\'A\') if char.isupper() else ord(\'a\') shifted_text.append(chr((ord(char) - start + shift) % 26 + start)) else: shifted_text.append(char) return \'\'.join(shifted_text) class IncrementalCaesarEncoder(codecs.IncrementalEncoder): def encode(self, input, final=False): return CaesarCodec().encode(input)[0] class IncrementalCaesarDecoder(codecs.IncrementalDecoder): def decode(self, input, final=False): return CaesarCodec().decode(input)[0] def caesar_search_function(encoding_name): if encoding_name == \'caesar\': return codecs.CodecInfo( name=\'caesar\', encode=CaesarCodec().encode, decode=CaesarCodec().decode, incrementalencoder=IncrementalCaesarEncoder, incrementaldecoder=IncrementalCaesarDecoder, streamreader=codecs.StreamReader, streamwriter=codecs.StreamWriter) codecs.register(caesar_search_function)"},{"question":"# PyTorch HIP Device and Memory Management You are required to implement a function that performs tensor calculations on HIP devices using PyTorch. The function should: 1. Allocate tensors on specific HIP devices. 2. Perform tensor operations on these devices. 3. Monitor and manage memory usage. 4. Check for HIP or CUDA availability and handle operations accordingly. Function Signature ```python import torch def hip_tensor_operations(): pass ``` Requirements 1. **Device Allocation**: - Allocate a tensor `x` with values `[1., 2., 3., 4.]` on HIP device 0. - Allocate another tensor `y` with values `[5., 6., 7., 8.]` on HIP device 1. 2. **Tensor Operations**: - Transfer `y` to HIP device 0 and perform element-wise addition with `x`. Store the result in `z`. - Allocate a tensor `a` on HIP device 1 with random values of shape `(2, 2)`. - Transfer `a` to HIP device 0 and compute the product with a 2x2 identity matrix. Store the result in `b`. 3. **Memory Management**: - Print the memory allocated and reserved on HIP device 0 before and after the tensor operations. - Clear the unused cached memory on HIP device 0 after the tensor operations. 4. **Availability Check**: - Ensure that the function checks if HIP or CUDA is available. If neither is available, print an appropriate message. Constraints - Use `torch.device`, `.cuda()`, `.to()`, and other necessary PyTorch methods for the operations. - Ensure that memory management methods `torch.cuda.memory_allocated` and `torch.cuda.memory_reserved` are used correctly. - Handle scenarios where HIP is not available by printing \\"HIP not available\\". Output - Function should not return anything but print the required memory information and result tensors. Example Output ``` Memory allocated on HIP device 0 before operations: ... Memory reserved on HIP device 0 before operations: ... Memory allocated on HIP device 0 after operations: ... Memory reserved on HIP device 0 after operations: ... Clearing unused cached memory... HIP not available ```","solution":"import torch def hip_tensor_operations(): if torch.backends.cuda.is_built(): device_0 = torch.device(\'cuda:0\') device_1 = torch.device(\'cuda:1\') print(f\\"Memory allocated on HIP device 0 before operations: {torch.cuda.memory_allocated(device_0)}\\") print(f\\"Memory reserved on HIP device 0 before operations: {torch.cuda.memory_reserved(device_0)}\\") # Allocate tensors on specific devices x = torch.tensor([1., 2., 3., 4.], device=device_0) y = torch.tensor([5., 6., 7., 8.], device=device_1) # Transfer y to HIP device 0 y = y.to(device_0) # Perform element-wise addition z = x + y print(f\\"z (after addition): {z}\\") # Allocate a random tensor on HIP device 1 a = torch.randn((2, 2), device=device_1) # Transfer a to HIP device 0 a = a.to(device_0) # Calculate the product with identity matrix identity_matrix = torch.eye(2, device=device_0) b = torch.matmul(a, identity_matrix) print(f\\"b (after matrix product): {b}\\") print(f\\"Memory allocated on HIP device 0 after operations: {torch.cuda.memory_allocated(device_0)}\\") print(f\\"Memory reserved on HIP device 0 after operations: {torch.cuda.memory_reserved(device_0)}\\") # Clear unused cached memory torch.cuda.empty_cache() print(\\"Clearing unused cached memory...\\") else: print(\\"HIP not available\\")"},{"question":"# **Coding Assessment Question** Objective Write a Python function that wraps the built-in `open` function to read a file, but with the additional functionality of converting all alphabetic characters in the file to lowercase before returning the content. This will test your understanding of the `builtins` module and handling file operations in Python. Function Signature ```python def custom_open(path: str) -> str: Opens a file, reads its content, and returns the content with all alphabetic characters converted to lowercase. Parameters: path (str): The path to the file to be opened. Returns: str: The content of the file with all alphabetic characters converted to lowercase. ``` Input - A single string `path` representing the path to the file to be read. Output - A string representing the content of the file with all alphabetic characters converted to lowercase. Constraints - The file located at `path` is guaranteed to exist and be readable. - You must use the `builtins.open` function within your solution to ensure the function is correctly wrapped. Example ```python # Assume a file at the path \'./example.txt\' with the following content: # \\"Hello, WORLD!\\" print(custom_open(\'./example.txt\')) # Output: \\"hello, world!\\" ``` Requirements - Import the `builtins` module. - Use the `builtins.open` function to open and read the file. - Convert the content to lowercase before returning it. Note - Do not use any external libraries for this task. - Focus on correctly wrapping and using the built-in `open` function.","solution":"import builtins def custom_open(path: str) -> str: Opens a file, reads its content, and returns the content with all alphabetic characters converted to lowercase. Parameters: path (str): The path to the file to be opened. Returns: str: The content of the file with all alphabetic characters converted to lowercase. with builtins.open(path, \'r\') as file: content = file.read() return content.lower()"},{"question":"# PyTorch Named Tensors: Batch Matrix Multiplication with Named Dimensions Objective: Implement a function `batch_named_matmul` that performs a batch matrix multiplication on two named tensors. The function should ensure the batch dimensions are aligned correctly and propagate the resulting names to the output tensor according to the rules specified in the documentation. Function Signature: ```python import torch def batch_named_matmul(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: pass ``` Input: - `tensor1`: A PyTorch named tensor with at least 2 dimensions. The tensor contains named dimensions. - `tensor2`: A PyTorch named tensor with at least 2 dimensions. The tensor contains named dimensions. Both `tensor1` and `tensor2` must satisfy the alignment and broadcasting requirements for their batch dimensions as specified in the documentation. Output: - Output should be a named tensor resulting from the batch matrix multiplication of `tensor1` and `tensor2`. Constraints: 1. If the batch dimensions of `tensor1` and `tensor2` are not aligned and broadcastable, the function should raise a `RuntimeError`. 2. The function should retain and correctly propagate the names of the dimensions in the output tensor, following the name inference rules. Example: ```python # Example Usage: tensor1 = torch.randn(3, 4, 4, names=(\'batch\', \'N\', \'M\')) tensor2 = torch.randn(3, 4, names=(\'batch\', \'M\', \'P\')) result = batch_named_matmul(tensor1, tensor2) print(result.names) # Expected names: (\'batch\', \'N\', \'P\') assuming dimensions align correctly for matmul print(result.shape) # Expected shape: torch.Size([3, 4, 4]) ``` Notes: - In this function, no additional libraries apart from PyTorch should be used. - Pay attention to the handling of named dimensions, especially when performing operations that involve dimension alignment and broadcasting. - The focus is on correctly aligning dimensions by names and propagating dimension names to the output tensor.","solution":"import torch def batch_named_matmul(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Perform batch matrix multiplication on two named tensors and return the result as a named tensor. Args: tensor1 (torch.Tensor): The first named tensor, must have at least 2 dimensions. tensor2 (torch.Tensor): The second named tensor, must have at least 2 dimensions. Returns: torch.Tensor: The result of batch matrix multiplication. if tensor1.ndim < 2 or tensor2.ndim < 2: raise ValueError(\\"Both tensors must have at least 2 dimensions.\\") # Validate batch dimensions and get names batch_names_1 = tensor1.names[:-2] batch_names_2 = tensor2.names[:-2] if batch_names_1 != batch_names_2: raise RuntimeError(\\"Batch dimensions of tensor1 and tensor2 are not aligned.\\") # Validate matmul dimensions if tensor1.names[-1] != tensor2.names[-2]: raise RuntimeError(\\"Inner dimensions for matrix multiplication do not align.\\") # Perform the batch matrix multiplication result = torch.matmul(tensor1, tensor2) # Construct the resulting dimension names result_names = batch_names_1 + (tensor1.names[-2], tensor2.names[-1]) result = result.refine_names(*result_names) return result"},{"question":"# JSON Handling and Custom Serialization in Python **Objective:** Implement a custom JSON serialization and deserialization logic to handle specific Python objects, demonstrating a thorough understanding of Python\'s `json` module and its advanced features. Background: You are tasked with serializing and deserializing a Python object which consists of not only basic data types but also custom objects. Specifically, the `Employee` class has a composition relationship with a `Department` class. Classes structure: ```python class Department: def __init__(self, name: str, floor: int): self.name = name self.floor = floor class Employee: def __init__(self, emp_id: int, name: str, department: Department, salary: float, is_active: bool): self.emp_id = emp_id self.name = name self.department = department self.salary = salary self.is_active = is_active ``` Task: 1. **Serialization:** - Implement a custom encoder `EmployeeEncoder` subclassing from `json.JSONEncoder` that can serialize an `Employee` object into JSON format. - Ensure that the `Employee` object, including its `Department`, is correctly serialized into JSON. 2. **Deserialization:** - Implement a custom decoder function that converts the JSON back into a corresponding `Employee` object. - Use the `object_hook` parameter to facilitate this custom deserialization while calling `json.loads()`. Function Signatures: ```python import json class EmployeeEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Employee): return { \'emp_id\': obj.emp_id, \'name\': obj.name, \'department\': { \'name\': obj.department.name, \'floor\': obj.department.floor }, \'salary\': obj.salary, \'is_active\': obj.is_active } if isinstance(obj, Department): return { \'name\': obj.name, \'floor\': obj.floor } return json.JSONEncoder.default(self, obj) def employee_decoder(dct): if \'emp_id\' in dct and \'department\' in dct: dept = dct[\'department\'] return Employee( emp_id=dct[\'emp_id\'], name=dct[\'name\'], department=Department(dept[\'name\'], dept[\'floor\']), salary=dct[\'salary\'], is_active=dct[\'is_active\'] ) return dct ``` Constraints: - Ensure the serialization output is in a human-readable formatted JSON. - Handle unexpected data gracefully by raising appropriate exceptions where necessary. Input: - An instance of the `Employee` class objects. Output: - JSON string representation of the `Employee` instance when serialized. - An `Employee` instance reconstructed from JSON. Examples: ```python # Creating an Employee object dept = Department(\\"Software\\", 5) emp = Employee(101, \\"Jane Doe\\", dept, 75000.00, True) # Serializing the Employee object json_str = json.dumps(emp, cls=EmployeeEncoder, indent=4) print(json_str) # Output should be a pretty-printed JSON string representing the Employee object # Deserializing back to an Employee object emp_obj = json.loads(json_str, object_hook=employee_decoder) print(emp_obj) # The output should be an Employee object with equivalent properties ``` Try to implement the `EmployeeEncoder` class and the `employee_decoder` function, then test them with various `Employee` instances to ensure everything works correctly.","solution":"import json class Department: def __init__(self, name: str, floor: int): self.name = name self.floor = floor class Employee: def __init__(self, emp_id: int, name: str, department: Department, salary: float, is_active: bool): self.emp_id = emp_id self.name = name self.department = department self.salary = salary self.is_active = is_active class EmployeeEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Employee): return { \'emp_id\': obj.emp_id, \'name\': obj.name, \'department\': { \'name\': obj.department.name, \'floor\': obj.department.floor }, \'salary\': obj.salary, \'is_active\': obj.is_active } return json.JSONEncoder.default(self, obj) def employee_decoder(dct): if \'emp_id\' in dct and \'department\' in dct: dept = dct[\'department\'] return Employee( emp_id=dct[\'emp_id\'], name=dct[\'name\'], department=Department(dept[\'name\'], dept[\'floor\']), salary=dct[\'salary\'], is_active=dct[\'is_active\'] ) return dct # Example usage: dept = Department(\\"Software\\", 5) emp = Employee(101, \\"Jane Doe\\", dept, 75000.00, True) # Serializing the Employee object json_str = json.dumps(emp, cls=EmployeeEncoder, indent=4) print(json_str) # Deserializing back to an Employee object emp_obj = json.loads(json_str, object_hook=employee_decoder) print(vars(emp_obj)) print(vars(emp_obj.department))"},{"question":"Objective The objective of this question is to assess your understanding of implementing custom kernels in scikit-learn and applying them for classification tasks. You will need to implement a custom kernel function and use it in a Support Vector Machine (SVM) classifier using the `scikit-learn` library. Problem Statement You are required to: 1. Implement a custom kernel function, `custom_kernel`, which combines linear and polynomial kernels as defined below: [ k(mathbf{x}, mathbf{y}) = (mathbf{x}^top mathbf{y}) + (gamma mathbf{x}^top mathbf{y} + c_0)^d ] where: - (mathbf{x}) and (mathbf{y}) are the input vectors. - (gamma), (c_0), and (d) are hyperparameters. 2. Use the `custom_kernel` in an SVM classifier to classify the Iris dataset, a classic dataset for pattern recognition. Instructions 1. **Function Implementation** - Implement the function `custom_kernel(X, Y, gamma=1.0, coef0=1, degree=3)`: - `X`, `Y`: numpy arrays of shape `(n_samples_1, n_features)` and `(n_samples_2, n_features)`. - `gamma: float`, `coef0: float`, and `degree: int`: hyperparameters for the polynomial part of the kernel. 2. **Apply SVM Classifier** - Load the Iris dataset using `sklearn.datasets.load_iris`. - Split the dataset into training and testing sets (80% train, 20% test). - Train an SVM classifier using your `custom_kernel`. - Evaluate the classifier on the test set and print the accuracy. Expected Outputs - The `custom_kernel` function should return a kernel matrix of shape `(n_samples_1, n_samples_2)`. - The accuracy of the SVM classifier on the test dataset. Constraints - You cannot use built-in kernels or kernel functions from sklearn for the custom kernel part. - You must demonstrate understanding of kernel functions by implementing your custom kernel from scratch. Example ```python import numpy as np from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Implement the custom kernel function def custom_kernel(X, Y, gamma=1.0, coef0=1, degree=3): linear_part = np.dot(X, Y.T) polynomial_part = (gamma * np.dot(X, Y.T) + coef0) ** degree return linear_part + polynomial_part # Load Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the data into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train SVM classifier with the custom kernel svm = SVC(kernel=custom_kernel) svm.fit(X_train, y_train) # Predict and evaluate y_pred = svm.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy * 100:.2f}%\\") ``` You need to submit the implementation of the `custom_kernel` function and the rest of the code using the Iris dataset for training and evaluating the SVM classifier.","solution":"import numpy as np from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Implement the custom kernel function def custom_kernel(X, Y, gamma=1.0, coef0=1, degree=3): Custom kernel combining linear and polynomial kernels. Parameters: - X, Y: Input vectors. - gamma: Hyperparameter for polynomial part. - coef0: Hyperparameter for polynomial part. - degree: Degree of the polynomial. Returns: Kernel matrix. linear_part = np.dot(X, Y.T) polynomial_part = (gamma * np.dot(X, Y.T) + coef0) ** degree return linear_part + polynomial_part # Load Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the data into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train SVM classifier with the custom kernel svm = SVC(kernel=custom_kernel) svm.fit(X_train, y_train) # Predict and evaluate y_pred = svm.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy * 100:.2f}%\\")"},{"question":"# Seaborn Plotting Assessment You are tasked with creating a function that demonstrates the usage of Seaborn\'s `objects` module to generate and customize plots. To show a comprehensive understanding of the plotting capabilities, your function should illustrate various aspects of plot limit handling. Function Signature ```python def create_custom_plot(x_values, y_values, x_limits=None, y_limits=None): Create a custom plot using Seaborn\'s objects interface with specified limits for axes. Args: x_values (list of int/float): The x-axis data points. y_values (list of int/float): The y-axis data points. x_limits (tuple of two int/float, default=None): Limits for the x-axis (min, max). None to maintain default. y_limits (tuple of two int/float, default=None): Limits for the y-axis (min, max). None to maintain default. Returns: matplotlib.figure.Figure: The figure object of the constructed plot. pass ``` Requirements 1. **Plot Creation**: The function should create a line plot with markers using Seaborn\'s `so.Plot` and `so.Line`. 2. **Axis Limits**: Implement axis limits using the `limit` method. - If `x_limits` is provided, use it to set the x-axis limits. - If `y_limits` is provided, use it to set the y-axis limits. - Allow for either `None` to maintain the default limits when only one side is specified. 3. **Edge Cases**: - Handle both positive and negative values. - Test inverting axis limits by providing `min` greater than `max`. 4. **Return Value**: The function should return the figure object of the constructed plot. Example Usage ```python fig = create_custom_plot( x_values=[1, 2, 3, 4], y_values=[10, 15, 13, 17], x_limits=(0, 5), y_limits=(8, 18) ) fig.show() ``` In this example, the plot will have x-axis limits from 0 to 5 and y-axis limits from 8 to 18.","solution":"import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_plot(x_values, y_values, x_limits=None, y_limits=None): Create a custom plot using Seaborn\'s objects interface with specified limits for axes. Args: x_values (list of int/float): The x-axis data points. y_values (list of int/float): The y-axis data points. x_limits (tuple of two int/float, default=None): Limits for the x-axis (min, max). None to maintain default. y_limits (tuple of two int/float, default=None): Limits for the y-axis (min, max). None to maintain default. Returns: matplotlib.figure.Figure: The figure object of the constructed plot. # Create the plot object p = so.Plot(x=x_values, y=y_values).add(so.Line(marker=\'o\')) # Apply axis limits if provided if x_limits: p = p.limit(x=x_limits) if y_limits: p = p.limit(y=y_limits) # Create the figure fig, ax = plt.subplots() p.on(ax).plot() return fig"},{"question":"# Coding Assessment: Working with Meta Tensors in PyTorch Problem Description You are provided with a PyTorch model defined as a sequence of linear layers. Your task is to write a function that initializes a meta tensor model, performs a few operations on the meta tensor model, and then reinitializes the model on the CPU with actual data. The provided model `SimpleNN` is as follows: ```python import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x ``` Function Signature ```python def meta_to_cpu_model(input_size: int, hidden_size: int, output_size: int): Create a meta tensor model, simulate operations and reinitialize the model on CPU with data. Parameters: input_size (int): Size of the input layer. hidden_size (int): Size of the hidden layer. output_size (int): Size of the output layer. Returns: nn.Module: The reinitialized model on CPU with uninitialized weights. # Your code here ``` Requirements 1. **Model Initialization**: - Initialize the `SimpleNN` model using a meta device. 2. **Model Manipulation**: - Perform at least one operation on the model in the meta device, such as printing the model summary. 3. **Reinitialization**: - Reinitialize the model on the CPU using the `to_empty` method without initializing the parameters. 4. **Return the Model**: - The function should return the reinitialized model that resides on the CPU. Constraints - You should use the provided context manager techniques and `to_empty` method as described. - You are not required to reinitialize the weights of the final model returned. Example Usage ```python model = meta_to_cpu_model(10, 20, 2) print(model) ``` This should print a description of the model now residing on the CPU without initialized weights: ``` SimpleNN( (fc1): Linear(in_features=10, out_features=20, bias=True) (fc2): Linear(in_features=20, out_features=2, bias=True) ) ``` This question assesses your understanding of the meta device in PyTorch, specifically around model handling, initialization, and transitioning meta tensors to actual devices (CPU) appropriately.","solution":"import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def meta_to_cpu_model(input_size: int, hidden_size: int, output_size: int): Create a meta tensor model, simulate operations and reinitialize the model on CPU with data. Parameters: input_size (int): Size of the input layer. hidden_size (int): Size of the hidden layer. output_size (int): Size of the output layer. Returns: nn.Module: The reinitialized model on CPU with uninitialized weights. # Step 1: Initialize the model on a meta device (fake tensor) meta_model = SimpleNN(input_size, hidden_size, output_size).to(device=\'meta\') # Step 2: Perform at least one operation on the meta model # For demonstration, here we just obtain the summary (which doesn\'t initialize params) model_summary = str(meta_model) print(\\"Meta model summary:\\", model_summary) # Step 3: Reinitialize the model on CPU with uninitialized weights cpu_model = SimpleNN(input_size, hidden_size, output_size).to_empty(device=\'cpu\') return cpu_model"},{"question":"# Asynchronous Network Request Aggregator **Problem Statement:** You are tasked with designing a asynchronous network request aggregator using the asyncio library. The aim is to fetch data concurrently from multiple URLs and process the collected data asynchronously. **Function to Implement:** ```python import asyncio import aiohttp async def fetch_data(session, url): Fetch data from a given URL using aiohttp session. Args: session (aiohttp.ClientSession): The aiohttp session to perform the request. url (str): The URL to fetch data from. Returns: str: The text content of the response. async with session.get(url) as response: return await response.text() async def aggregate_requests(urls): Fetch data from multiple URLs asynchronously and aggregate the results. Args: urls (list): A list of URL strings to fetch data from. Returns: dict: A dictionary mapping each URL to its fetched content. async with aiohttp.ClientSession() as session: tasks = [fetch_data(session, url) for url in urls] responses = await asyncio.gather(*tasks) return dict(zip(urls, responses)) async def process_aggregated_data(urls): Process aggregated data fetched from multiple URLs. Args: urls (list): A list of URL strings to fetch data from. Returns: dict: A dictionary mapping each URL to the length of its fetched content. aggregated_data = await aggregate_requests(urls) # Simulate data processing processed_data = {url: len(content) for url, content in aggregated_data.items()} return processed_data # Example Usage urls = [ \\"https://jsonplaceholder.typicode.com/posts\\", \\"https://jsonplaceholder.typicode.com/comments\\", \\"https://jsonplaceholder.typicode.com/albums\\" ] result = asyncio.run(process_aggregated_data(urls)) print(result) ``` **Expected Input and Output:** - The input is a list of URLs (strings). - The output is a dictionary where keys are the input URLs, and values are integers representing the length of the content fetched from these URLs. **Constraints:** - Use the `fetch_data` function to fetch data asynchronously from each provided URL. - Aggregate data using asyncio\'s capabilities (e.g., `asyncio.gather`). - Handle responses as strings for simplicity. - Simulate data processing by calculating the length of fetched content. - Assume the URLs provided are valid and return data correctly. **Performance Requirements:** - The solution should efficiently handle multiple URLs, exploiting asyncio\'s concurrency features. - Ensure proper use of asynchronous context managers for resource management.","solution":"import asyncio import aiohttp async def fetch_data(session, url): Fetch data from a given URL using aiohttp session. Args: session (aiohttp.ClientSession): The aiohttp session to perform the request. url (str): The URL to fetch data from. Returns: str: The text content of the response. async with session.get(url) as response: response.raise_for_status() # Ensure we raise an error for bad responses return await response.text() async def aggregate_requests(urls): Fetch data from multiple URLs asynchronously and aggregate the results. Args: urls (list): A list of URL strings to fetch data from. Returns: dict: A dictionary mapping each URL to its fetched content. async with aiohttp.ClientSession() as session: tasks = [fetch_data(session, url) for url in urls] responses = await asyncio.gather(*tasks) return dict(zip(urls, responses)) async def process_aggregated_data(urls): Process aggregated data fetched from multiple URLs. Args: urls (list): A list of URL strings to fetch data from. Returns: dict: A dictionary mapping each URL to the length of its fetched content. aggregated_data = await aggregate_requests(urls) # Simulate data processing processed_data = {url: len(content) for url, content in aggregated_data.items()} return processed_data # Example Usage urls = [ \\"https://jsonplaceholder.typicode.com/posts\\", \\"https://jsonplaceholder.typicode.com/comments\\", \\"https://jsonplaceholder.typicode.com/albums\\" ] # result = asyncio.run(process_aggregated_data(urls)) # Uncomment to run. # print(result)"},{"question":"Data Analysis with pandas Objective: Your task is to analyze and process a dataset using the pandas library in Python. This will assess your understanding of various pandas functionalities, including data loading, cleaning, grouping, aggregation, and data merging. Dataset Description: You will be provided two CSV files, `students.csv` and `scores.csv`. 1. `students.csv`: Contains information about students. - **Columns**: - `student_id`: Unique identifier for each student - `name`: Student\'s name - `age`: Student\'s age - `gender`: Student\'s gender 2. `scores.csv`: Contains information about students\' scores in various subjects. - **Columns**: - `student_id`: Unique identifier for each student (corresponds to `student_id` in `students.csv`) - `subject`: Subject name - `score`: Score obtained in the subject Task: Write a function `analyze_student_data` that performs the following operations: 1. **Load the Data**: - Read the `students.csv` and `scores.csv` files into two separate pandas DataFrames. 2. **Data Cleaning**: - Check for and handle any missing values in both DataFrames appropriately. For simplicity, you can fill missing numerical values with the mean of that column and missing categorical values with the mode. If a column is entirely missing, you can drop it. 3. **Data Aggregation and Grouping**: - Calculate the average score for each subject and return a DataFrame containing `subject` and `average_score`. - Calculate the average score for each student across all subjects and add this information as a new column `average_score` in the `students.csv` DataFrame. 4. **Data Merging**: - Merge the students\' DataFrame with the scores DataFrame on the `student_id` column. 5. **Advanced Analysis**: - Find the top 3 students with the highest overall average score and return their names and the respective average scores. Function Signature and Expected Output: ```python def analyze_student_data(students_file: str, scores_file: str): # Your code here pass ``` **Input**: - `students_file` (str): The file path to `students.csv`. - `scores_file` (str): The file path to `scores.csv`. **Output**: - A tuple containing: 1. A DataFrame with the average score for each subject. 2. The modified `students` DataFrame with an additional `average_score` column for each student. 3. A list of tuples with names and average scores of the top 3 students. **Constraints**: - Assume the datasets are not very large (less than 1000 rows each). You should ensure your code is efficient and handles potential edge cases (e.g., missing data) effectively. Example: Suppose you have the following data in `students.csv`: ``` student_id,name,age,gender 1,John Doe,20,M 2,Jane Smith,21,F 3,Bob Johnson,22,M ``` And the following data in `scores.csv`: ``` student_id,subject,score 1,Math,85 1,English,78 2,Math,92 2,English,88 3,Math,60 3,English,75 ``` Your function call: ```python analyze_student_data(\'students.csv\', \'scores.csv\') ``` Should output: 1. DataFrame with average scores per subject. 2. Modified students DataFrame with the `average_scores` column. 3. List of top 3 students with the highest average scores. Ensure to write and run tests to validate each part of your implementation.","solution":"import pandas as pd def analyze_student_data(students_file: str, scores_file: str): # Load the data students_df = pd.read_csv(students_file) scores_df = pd.read_csv(scores_file) # Data Cleaning: Handle missing values # Filling numerical columns with the mean if there are missing values students_df.fillna(students_df.mean(numeric_only=True), inplace=True) # Filling categorical columns with the mode for column in students_df.select_dtypes(include=[\'object\']).columns: students_df[column].fillna(students_df[column].mode()[0], inplace=True) scores_df.fillna(scores_df.mean(numeric_only=True), inplace=True) for column in scores_df.select_dtypes(include=[\'object\']).columns: scores_df[column].fillna(scores_df[column].mode()[0], inplace=True) # Data Aggregation and Grouping: Average score for each subject subject_avg_scores_df = scores_df.groupby(\'subject\', as_index=False)[\'score\'].mean() subject_avg_scores_df.rename(columns={\'score\': \'average_score\'}, inplace=True) # Calculate the average score for each student student_avg_scores_df = scores_df.groupby(\'student_id\', as_index=False)[\'score\'].mean() student_avg_scores_df.rename(columns={\'score\': \'average_score\'}, inplace=True) # Merge the average score data with the students DataFrame students_df = students_df.merge(student_avg_scores_df, on=\'student_id\', how=\'left\') # Advanced Analysis: Top 3 students with the highest average score top_students_df = students_df.nlargest(3, \'average_score\') top_students = list(zip(top_students_df[\'name\'], top_students_df[\'average_score\'])) return subject_avg_scores_df, students_df, top_students"},{"question":"You are provided with a list of Python package names. Your task is to write a Python function that retrieves various metadata for each of these packages using the `importlib.metadata` library. Specifically, you need to return the version, list of entry points, and requirements for each package. Function Signature ```python def get_package_metadata(package_names: list) -> dict: pass ``` Input - `package_names`: A list of strings representing the names of the Python packages for which metadata needs to be retrieved. *(1 <= len(package_names) <= 100)* Output - A dictionary where the keys are package names and values are nested dictionaries containing: - `\'version\'`: A string representing the version of the package. - `\'entry_points\'`: A dictionary where keys are entry point groups and values are lists of entry point names. - `\'requires\'`: A list of strings representing the requirements of the package. If there are no requirements, it should be an empty list. Example Input: ```python package_names = [\\"wheel\\", \\"setuptools\\"] ``` Output: ```python { \\"wheel\\": { \\"version\\": \\"0.32.3\\", \\"entry_points\\": { \\"console_scripts\\": [\\"wheel\\"] }, \\"requires\\": [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] }, \\"setuptools\\": { \\"version\\": \\"...\\", \\"entry_points\\": { \\"...\\": [\\"...\\"] }, \\"requires\\": [\\"...\\"] } } ``` Constraints - Assume all package names provided are valid and installed in the environment. - You can utilize any method or combination of methods provided by the `importlib.metadata` library to achieve the desired results. Notes - You may need to handle missing metadata keys appropriately (e.g., if a package has no entry points, the `entry_points` key should have an empty dictionary). - The entries in the resulting dictionary should maintain the order of the input package names. Performance Considerations - Aim to minimize the number of calls to the `importlib.metadata` library to ensure that the solution is efficient and scales well with the number of packages.","solution":"import importlib.metadata def get_package_metadata(package_names): metadata_dict = {} for package in package_names: try: dist = importlib.metadata.distribution(package) version = dist.version entry_points = {} for entry_point in dist.entry_points: entry_points.setdefault(entry_point.group, []).append(entry_point.name) requires = dist.requires or [] except importlib.metadata.PackageNotFoundError: version = \\"Package not found\\" entry_points = {} requires = [] metadata_dict[package] = { \\"version\\": version, \\"entry_points\\": entry_points, \\"requires\\": requires } return metadata_dict"},{"question":"You are required to write a Python script that connects to an NNTP server to perform the following tasks: 1. **Connect to an NNTP Server**: Initialize a connection to the NNTP server running at `news.gmane.io`. 2. **List Newsgroups**: List all available newsgroups on the server and filter out groups related to Python (those that contain the substring \\"python\\" in their names). 3. **Fetch Latest Articles**: - Select one of the filtered newsgroups. - Fetch metadata (such as article number, subject, author, and date) for the last 5 articles in that newsgroup. 4. **Download an Article**: - Download the full content (headers + body) of the latest article from the selected newsgroup. - Save this article to a file named `latest_article.txt`. # Requirements: - Use the `nntplib` module to interact with the NNTP server. - Handle exceptions appropriately to manage connectivity issues or unexpected server responses. - Make sure the connection to the server is gracefully closed after completing your tasks. # Constraints: - The server might not always be responsive or might fail to provide data within a reasonable amount of time. Implement appropriate timeout mechanisms. - Only articles with an ASCII subject should be considered; skip articles with non-ASCII subjects. # Expected Input and Output: - **Input**: The script explicitly specifies the server `news.gmane.io` and does not take external input. - **Output**: The script should output: 1. The list of Python-related newsgroups. 2. Metadata of the last 5 articles in the selected newsgroup. 3. A file `latest_article.txt` containing the full content of the latest article. # Implementation: ```python import nntplib from datetime import datetime def list_python_newsgroups(server): resp, groups = server.list() python_groups = [group for group, _, _, _ in groups if \\"python\\" in group.lower()] return python_groups def fetch_latest_articles(server, group, count=5): resp, _, first, last, _ = server.group(group) resp, articles = server.over((last - count + 1, last)) return articles def download_article(server, message_id, file_name): resp, article_data = server.article(message_id) with open(file_name, \'wb\') as f: for line in article_data.lines: f.write(line + b\'n\') def main(): server = nntplib.NNTP(\'news.gmane.io\') try: python_groups = list_python_newsgroups(server) print(\\"Python-related newsgroups:\\") for group in python_groups: print(group) if python_groups: chosen_group = python_groups[0] print(f\\"nFetching latest articles from {chosen_group}n\\") articles = fetch_latest_articles(server, chosen_group) for art_num, over in articles: subject = nntplib.decode_header(over[\'subject\']) from_ = nntplib.decode_header(over[\'from\']) date = nntplib.decode_header(over[\'date\']) print(f\\"Article {art_num}: {subject} by {from_} on {date}\\") latest_message_id = articles[-1][1][\'message-id\'] download_article(server, latest_message_id, \'latest_article.txt\') print(\\"nLatest article has been downloaded to \'latest_article.txt\'.\\") finally: server.quit() if __name__ == \\"__main__\\": main() ``` Explain your implementation and how it aligns with the requirements provided.","solution":"import nntplib from datetime import datetime def list_python_newsgroups(server): resp, groups = server.list() python_groups = [group for group, _, _, _ in groups if \\"python\\" in group.lower()] return python_groups def fetch_latest_articles(server, group, count=5): resp, _, first, last, _ = server.group(group) start = max(first, last - count + 1) # Ensure we don\'t go below the first article resp, articles_overview = server.over((start, last)) articles = [] for art_num, over in articles_overview: subject = nntplib.decode_header(over[\'subject\']) author = nntplib.decode_header(over[\'from\']) date = nntplib.decode_header(over[\'date\']) if all(isinstance(s, str) for s in [subject, author, date]): # Check if all fields are ASCII articles.append((art_num, subject, author, date, over[\'message-id\'])) return articles def download_article(server, message_id, file_name): resp, article_data = server.article(message_id) with open(file_name, \'wb\') as f: for line in article_data.lines: f.write(line + b\'n\') def main(): server = nntplib.NNTP(\'news.gmane.io\', timeout=10) try: python_groups = list_python_newsgroups(server) print(\\"Python-related newsgroups:\\") for group in python_groups: print(group) if python_groups: chosen_group = python_groups[0] print(f\\"nFetching latest articles from {chosen_group}n\\") articles = fetch_latest_articles(server, chosen_group) for art_num, subject, author, date, msg_id in articles: print(f\\"Article {art_num}: {subject} by {author} on {date}\\") if articles: latest_message_id = articles[-1][-1] download_article(server, latest_message_id, \'latest_article.txt\') print(\\"nLatest article has been downloaded to \'latest_article.txt\'.\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: server.quit() if __name__ == \\"__main__\\": main()"},{"question":"# K-Nearest Neighbors Classification Assessment Objective: Demonstrate your understanding of the k-nearest neighbors (KNN) algorithm for classification using scikit-learn. Problem Statement: You are given a dataset containing feature vectors and their respective class labels. Your task is to implement a KNN classifier using the `KNeighborsClassifier` from scikit-learn. You will test your classifier with different values of `k` and different algorithms (`ball_tree`, `kd_tree`, and `brute`). You must evaluate the performance of each configuration using accuracy on a test set. Dataset: The `Iris` dataset will be used for this task. It can be loaded using scikit-learn\'s dataset module. Task: 1. Load the `Iris` dataset. 2. Split the dataset into a training set (70%) and a testing set (30%). 3. For `k` values in `[3, 5, 7]`, and for each algorithm in `[\'ball_tree\', \'kd_tree\', \'brute\']`: - Train a `KNeighborsClassifier` with the specified `k` and algorithm. - Predict the labels of the test set. - Compute and store the accuracy of the classifier. 4. Output the accuracies for each configuration in a readable format. Constraints: - Use scikit-learn\'s `train_test_split` for splitting the data. - Use scikit-learn\'s `KNeighborsClassifier` for the implementation. Expected Functions: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def evaluate_knn(): # Load dataset data = load_iris() X = data.data # features y = data.target # labels # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) k_values = [3, 5, 7] algorithms = [\'ball_tree\', \'kd_tree\', \'brute\'] results = [] # To store the result tuples of (k, algorithm, accuracy) for k in k_values: for algo in algorithms: # Initialize KNN classifier with k and algorithm knn = KNeighborsClassifier(n_neighbors=k, algorithm=algo) # Train the classifier knn.fit(X_train, y_train) # Predict test set labels y_pred = knn.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Store results results.append((k, algo, accuracy)) # Print the results for k, algo, acc in results: print(f\\"k={k}, algorithm={algo}, accuracy={acc:.4f}\\") # Call the function to see results evaluate_knn() ``` Additional Notes: - Make sure to handle the `random_state` parameter in `train_test_split` for reproducibility. - Use appropriate metric (accuracy) to evaluate the performance. Submission: Submit your implementation as a Python script or Jupyter notebook, ensuring it meets the specified requirements and constraints.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def evaluate_knn(): # Load dataset data = load_iris() X = data.data # features y = data.target # labels # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) k_values = [3, 5, 7] algorithms = [\'ball_tree\', \'kd_tree\', \'brute\'] results = [] # To store the result tuples of (k, algorithm, accuracy) for k in k_values: for algo in algorithms: # Initialize KNN classifier with k and algorithm knn = KNeighborsClassifier(n_neighbors=k, algorithm=algo) # Train the classifier knn.fit(X_train, y_train) # Predict test set labels y_pred = knn.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Store results results.append((k, algo, accuracy)) # Return the results as a list of tuples return results # Call the function to see results evaluate_knn()"},{"question":"**Q: Implement a Distributed Training Subprocess Manager** In distributed machine learning training, it is often necessary to manage multiple subprocesses working together to train a model. Using PyTorch\'s `SubprocessHandler`, write a class `DistributedTrainingManager` that does the following: 1. **Initializes the subprocesses** required for training. 2. **Monitors** each subprocess and ensures they are running correctly. 3. **Handles errors** by restarting any failed subprocess. 4. **Gracefully terminates** all subprocesses when training is complete. Your implementation should include the following methods: - `__init__(self, num_processes: int, training_function: Callable)`: Initializes the subprocess handler with the specified number of processes running the `training_function`. - `start_processes(self)`: Starts all training subprocesses. - `monitor_processes(self)`: Continuously monitors the subprocesses, restarting any that fail, and terminating all processes if the training is complete. - `terminate_processes(self)`: Gracefully terminates all running subprocesses. You can assume the `training_function` is a callable that performs the training step. **Constraints:** - Assume the training function is idempotent and can safely be restarted. - Your solution should ensure that no zombie processes are left running after termination. - Use appropriate synchronization mechanisms as needed to coordinate between processes. **Example Usage:** ```python def dummy_training_function(): # Simulate a training step import time time.sleep(1) manager = DistributedTrainingManager(num_processes=4, training_function=dummy_training_function) manager.start_processes() manager.monitor_processes() # After training is done manager.terminate_processes() ``` **Expected Output:** - All subprocesses should start and run the `dummy_training_function`. - If any subprocess fails, it should be restarted automatically. - All subprocesses should be terminated gracefully after training is complete. **Note:** Ensure that your solution leverages the `torch.distributed.elastic.multiprocessing.subprocess_handler` for handling subprocesses, and includes appropriate error handling and logging for monitoring purposes.","solution":"import torch.distributed.elastic.multiprocessing.subprocess_handler as subprocess_handler from multiprocessing import Process import time import logging from typing import Callable class DistributedTrainingManager: def __init__(self, num_processes: int, training_function: Callable): self.num_processes = num_processes self.training_function = training_function self.processes = [] self.logging = logging.getLogger() self.logging.setLevel(logging.INFO) def start_processes(self): self.logging.info(\\"Starting subprocesses...\\") for _ in range(self.num_processes): process = Process(target=self.training_function) process.start() self.processes.append(process) self.logging.info(f\\"Started {self.num_processes} subprocesses.\\") def monitor_processes(self): try: while any(p.is_alive() for p in self.processes): for i, process in enumerate(self.processes): if not process.is_alive() and process.exitcode != 0: self.logging.warning(f\\"Process {i} failed with exit code {process.exitcode}. Restarting...\\") new_process = Process(target=self.training_function) new_process.start() self.processes[i] = new_process time.sleep(1) except KeyboardInterrupt: self.logging.info(\\"Monitoring interrupted by user.\\") finally: self.terminate_processes() def terminate_processes(self): self.logging.info(\\"Terminating subprocesses...\\") for process in self.processes: process.terminate() process.join() self.logging.info(\\"All subprocesses terminated.\\") def dummy_training_function(): time.sleep(1)"},{"question":"You are tasked with creating a utility that processes event logs from a specified log file. Each log entry consists of a timestamp, event type, and a message. Your goal is to read the log file, format the log entries, and save the processed logs to a new file in a more readable format. Input Format 1. A log file where each line represents a log entry in the following format: ``` <timestamp> <event_type> <message> ``` Example: ``` 2023-10-01T12:00:00Z INFO User logged in 2023-10-01T12:05:00Z ERROR Disk space low ``` 2. The output file name to save the processed logs. Output Format The processed logs should be saved in the output file. Each log entry should be formatted as follows: ``` [<timestamp>] <event_type>: <message> ``` Example of a well-formatted log entry: ``` [2023-10-01T12:00:00Z] INFO: User logged in ``` Constraints - The log file will not exceed 1MB in size. - The log file must be read and processed line by line to avoid memory overflow. - Each line in the log file will have a valid format as described. Performance Requirements - The solution should efficiently handle file operations. - Proper error handling should be implemented for file operations (e.g., file not found, read/write errors). Function Signature ```python def process_log_file(input_file: str, output_file: str): Parameters: input_file (str): The path to the input log file. output_file (str): The path to the output file where the processed logs will be saved. Returns: None ``` # Example Given the input file `logs.txt` with the content: ``` 2023-10-01T12:00:00Z INFO User logged in 2023-10-01T12:05:00Z ERROR Disk space low ``` And calling the function `process_log_file(\'logs.txt\', \'processed_logs.txt\')`. The output file `processed_logs.txt` should contain: ``` [2023-10-01T12:00:00Z] INFO: User logged in [2023-10-01T12:05:00Z] ERROR: Disk space low ``` # Notes - Use formatted string literals (f-strings) for formatting log entries. - Ensure the output file is properly closed after writing the processed logs.","solution":"def process_log_file(input_file: str, output_file: str): Processes the input log file and saves the formatted logs to the output file. Parameters: input_file (str): path to the input log file output_file (str): path to the output file to save the processed logs try: with open(input_file, \'r\') as infile, open(output_file, \'w\') as outfile: for line in infile: timestamp, event_type, message = line.strip().split(\' \', 2) formatted_line = f\'[{timestamp}] {event_type}: {message}\' outfile.write(formatted_line + \'n\') except FileNotFoundError: print(f\\"The file {input_file} was not found.\\") except IOError as e: print(f\\"An error occurred while reading or writing a file: {e}\\")"},{"question":"# Seaborn Advanced Plotting: Faceted Bar Plot with Customizations You are given a dataset about penguins. Your task is to create a detailed and informative bar plot using Seaborn that meets specific requirements laid out below. This task will test your ability to understand and utilize various features and customizations of the Seaborn library. Requirements: 1. **Load the Dataset:** - Load the `penguins` dataset included with Seaborn. 2. **Create a Faceted Bar Plot:** - Use `sns.catplot` to create a faceted bar plot. - The plot should show the average `body_mass_g` on the y-axis. - The x-axis should categorize penguins based on the `island` they inhabit. - Create separate facets (`col`) for each `species` of penguins. 3. **Customize the Plot:** - Set the height of each facet to 4 and the aspect ratio to 0.75. - Use the `hue` parameter to further categorize data within each bar based on the `sex` of the penguins. - Add error bars representing the standard deviation of the data. 4. **Enhanced Customization (Advanced):** - Customize the appearance of the bars such that: - Bars have a linewidth of 1.5 and an edge color of `.2`. - Use a transparent color for the face of the bars (RGBA color with alpha < 1). - Ensure that each bar has an embedded text label showing its mean value in grams. 5. **Save the Plot:** - Save your final plot as `faceted_penguin_barplot.png`. Expected Output: - A PNG file `faceted_penguin_barplot.png` containing the described faceted bar plot with all customizations applied. - The plot should provide clear insights into the body mass distribution of penguins across different islands, species, and sexes. Example Function Signature: ```python def create_faceted_barplot(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the faceted bar plot with customizations g = sns.catplot( data=penguins, kind=\\"bar\\", x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", col=\\"species\\", height=4, aspect=0.75, errorbar=\\"sd\\", edgecolor=\\".2\\", linewidth=1.5, facecolor=(0, 0, 0, 0.7) ) # Add mean value labels for ax in g.axes.flat: for container in ax.containers: ax.bar_label(container, fmt=\'%.2f\') # Save the plot plt.savefig(\\"faceted_penguin_barplot.png\\") ``` Use the above function as a guide to structure your solution. Ensure all customizations and plot properties meet the given criteria.","solution":"def create_faceted_barplot(): import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the faceted bar plot with customizations g = sns.catplot( data=penguins, kind=\\"bar\\", x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", col=\\"species\\", height=4, aspect=0.75, ci=\\"sd\\", # ci=\\"sd\\" to show standard deviation edgecolor=\\".2\\", linewidth=1.5 ) # Customizing the bars with transparent colors def rgba_color(r, g, b, a): return (r/255, g/255, b/255, a) transparent_color = rgba_color(0, 0, 0, 0.7) for ax in g.axes.flat: for patch in ax.patches: patch.set_facecolor(transparent_color) # Adding mean value labels to the bars for ax in g.axes.flat: for container in ax.containers: ax.bar_label(container, fmt=\'%.2f\', label_type=\'edge\', padding=2) # Save the plot to a file plt.savefig(\\"faceted_penguin_barplot.png\\")"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},q={class:"search-container"},D={class:"card-container"},F={key:0,class:"empty-state"},R=["disabled"],N={key:0},O={key:1};function j(s,e,l,m,i,o){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",q,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",D,[(a(!0),n(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),n("div",F,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),n("span",O,"Loading...")):(a(),n("span",N,"See more"))],8,R)):d("",!0)])}const M=p(z,[["render",j],["__scopeId","data-v-e762cd8f"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/68.md","filePath":"deepseek/68.md"}'),L={name:"deepseek/68.md"},B=Object.assign(L,{setup(s){return(e,l)=>(a(),n("div",null,[x(M)]))}});export{Y as __pageData,B as default};
