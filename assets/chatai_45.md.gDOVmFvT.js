import{_ as p,o as a,c as s,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(n,e,l,m,o,i){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-123daf71"]]),I=JSON.parse('[{"question":"**Question**: You are given a dataset that contains missing values. Your task is to implement a machine learning pipeline that imputes these missing values using different imputation strategies, trains a classifier on the imputed data, and evaluates the model\'s performance. Additionally, you will compare the performance of different imputation methods. # Dataset - You can use the `load_iris` dataset from `sklearn.datasets`. - Introduce missing values randomly into the dataset by setting 20% of the values to `np.nan`. # Steps 1. **Load the dataset**: - Use `load_iris` to load the dataset. - Introduce random missing values in 20% of the data. 2. **Imputation**: - Implement imputation for missing values using `SimpleImputer` with the strategy \\"mean\\". - Implement imputation for missing values using `IterativeImputer`. - Implement imputation for missing values using `KNNImputer`. 3. **Build the Pipeline**: - Create a pipeline for each imputation strategy that includes: - The imputer. - A classifier such as `DecisionTreeClassifier`. 4. **Evaluate Performance**: - Split the dataset into training and testing sets. - Train the pipeline on the training set. - Evaluate the accuracy of each pipeline on the test set. 5. **Compare Results**: - Compare the accuracies of the different pipelines. - Print the accuracy for each imputation strategy. # Code Template ```python import numpy as np from sklearn.datasets import load_iris from sklearn.impute import SimpleImputer, KNNImputer from sklearn.experimental import enable_iterative_imputer # noqa from sklearn.impute import IterativeImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Load the dataset data = load_iris() X, y = data.data, data.target # Introduce random missing values rng = np.random.RandomState(0) missing_mask = rng.rand(*X.shape) < 0.2 X[missing_mask] = np.nan # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Imputation strategies imputers = { \\"SimpleImputer_mean\\": SimpleImputer(strategy=\\"mean\\"), \\"IterativeImputer\\": IterativeImputer(random_state=0), \\"KNNImputer\\": KNNImputer(n_neighbors=2) } # Evaluate each imputation strategy for name, imputer in imputers.items(): pipeline = make_pipeline(imputer, DecisionTreeClassifier(random_state=0)) pipeline.fit(X_train, y_train) y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"{name} Accuracy: {accuracy:.2f}\\") # Compare and conclude ``` # Expected Output The output should show the accuracy of the models with each imputation strategy. For example: ``` SimpleImputer_mean Accuracy: 0.93 IterativeImputer Accuracy: 0.95 KNNImputer Accuracy: 0.92 ``` # Constraints - Use `random_state=0` for reproducibility. - Evaluate the models using accuracy as the metric. # Bonus (Optional) - Include the `MissingIndicator` in the pipeline to see how it affects the performance. - Experiment with different classifiers and compare the results.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.impute import SimpleImputer, KNNImputer from sklearn.experimental import enable_iterative_imputer # noqa from sklearn.impute import IterativeImputer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score def load_data_with_missing_values(): # Load the dataset data = load_iris() X, y = data.data, data.target # Introduce random missing values rng = np.random.RandomState(0) missing_mask = rng.rand(*X.shape) < 0.2 X[missing_mask] = np.nan return X, y def evaluate_imputation_strategies(X, y): # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Imputation strategies imputers = { \\"SimpleImputer_mean\\": SimpleImputer(strategy=\\"mean\\"), \\"IterativeImputer\\": IterativeImputer(random_state=0), \\"KNNImputer\\": KNNImputer(n_neighbors=2) } results = {} # Evaluate each imputation strategy for name, imputer in imputers.items(): pipeline = make_pipeline(imputer, DecisionTreeClassifier(random_state=0)) pipeline.fit(X_train, y_train) y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) results[name] = accuracy print(f\\"{name} Accuracy: {accuracy:.2f}\\") return results # Execute the primary function to evaluate imputation strategies if __name__ == \\"__main__\\": X, y = load_data_with_missing_values() imputation_results = evaluate_imputation_strategies(X, y) print(imputation_results)"},{"question":"# Pandas Resampling Coding Assessment Question You are provided with a time series dataset in the form of a pandas DataFrame containing daily stock prices for a particular stock. The DataFrame has the following columns: - `date` (datetime64[ns]): The date of the stock price. - `close` (float): The closing price of the stock on that date. Your task is to implement several functions to process this time series data using pandas resampling methods. Below are the functions you need to implement: 1. **resample_monthly_mean(dataframe)** - **Input:** A pandas DataFrame (daily stock prices) with two columns: `date` and `close`. - **Output:** A pandas DataFrame (monthly stock prices) with two columns: - `month` (datetime64[ns]): The starting date of the month. - `mean_close` (float): The mean closing price for that month. - **Description:** Resample the daily stock prices to monthly intervals and calculate the mean closing price for each month. 2. **resample_weekly_ffill(dataframe)** - **Input:** A pandas DataFrame (daily stock prices) with two columns: `date` and `close`. - **Output:** A pandas DataFrame (weekly stock prices) with two columns: - `week` (datetime64[ns]): The starting date of the week. - `close` (float): The closing price at the end of the last available day in that week (using forward fill if necessary). - **Description:** Resample the daily stock prices to weekly intervals and fill missing values with the last known price using forward fill. 3. **resample_quarterly_aggregate(dataframe)** - **Input:** A pandas DataFrame (daily stock prices) with two columns: `date` and `close`. - **Output:** A pandas DataFrame (quarterly stock prices) with the following columns: - `quarter` (datetime64[ns]): The starting date of the quarter. - `first_close` (float): The closing price on the first day of the quarter. - `last_close` (float): The closing price on the last day of the quarter. - **Description:** Resample the daily stock prices to quarterly intervals and calculate the closing prices for the first and last days of each quarter. **Constraints:** 1. The input DataFrame is guaranteed to have the `date` column sorted in ascending order. 2. The input DataFrame has no missing values in the `date` column but may have missing values in the `close` column. **Example usage:** ```python import pandas as pd # Example DataFrame data = { \'date\': pd.date_range(start=\'2022-01-01\', periods=365, freq=\'D\'), \'close\': np.random.rand(365) * 100 } df = pd.DataFrame(data) # Function calls monthly_mean_df = resample_monthly_mean(df) weekly_ffill_df = resample_weekly_ffill(df) quarterly_aggregate_df = resample_quarterly_aggregate(df) ``` **Performance Requirements:** - The functions should efficiently handle data up to a few years (up to around 10,000 rows). Implement the functions specified above, ensuring proper usage of pandas resampling methods and handling edge cases as needed.","solution":"import pandas as pd def resample_monthly_mean(dataframe): Resample the daily stock prices to monthly intervals and calculate the mean closing price for each month. resampled = dataframe.resample(\'M\', on=\'date\').mean().reset_index() resampled.columns = [\'month\', \'mean_close\'] return resampled def resample_weekly_ffill(dataframe): Resample the daily stock prices to weekly intervals and fill missing values with the last known price using forward fill. dataframe.set_index(\'date\', inplace=True) resampled = dataframe.resample(\'W\').ffill().reset_index() return resampled def resample_quarterly_aggregate(dataframe): Resample the daily stock prices to quarterly intervals and calculate the closing prices for the first and last days of each quarter. dataframe.set_index(\'date\', inplace=True) quarterly = dataframe.resample(\'Q\') first_close = quarterly.first().reset_index() last_close = quarterly.last().reset_index() aggregated = pd.merge(first_close, last_close, on=\'date\', suffixes=(\'_first\', \'_last\')) aggregated = aggregated[[\'date\', \'close_first\', \'close_last\']] aggregated.columns = [\'quarter\', \'first_close\', \'last_close\'] return aggregated"},{"question":"<|Analysis Begin|> The documentation primarily discusses the preprocessing capabilities of the `sklearn.preprocessing` module including standardization, scaling to a range, scaling sparse data, handling outliers, encoding categorical features, normalizing samples, discretization, binarization, and nonlinear transformations among others. Scikit-learn\'s preprocessing module provides numerous utilities to prepare data for machine learning algorithms which often expect data to be in a particular format. Transformations like standardization (to zero mean and unit variance), scaling features by their range, normalization (scaling samples to unit norm using different norms), and encoding categorical variables (ordinal and one-hot encoding) are essential for ensuring that features in a dataset are on a similar scale, making the training process smoother and models more effective. Identifying Question Focus: 1. StandardScaler: Calculate mean, standard deviation, and transform the data. 2. MinMaxScaler and MaxAbsScaler: Perform scaling to specific ranges. 3. RobustScaler: Handle data with many outliers. 4. Normalizer: Normalize the data. 5. Encoding: Transform categorical features into a numerical format using OrdinalEncoder and OneHotEncoder. Given that students should demonstrate their comprehension of both fundamental and advanced concepts, a possible problem could require implementing data transformations on a dataset and evaluating these transformations through a machine learning model, emphasizing the impact of preprocessing steps on the performance of the model. <|Analysis End|> <|Question Begin|> **Question:** Implement preprocessing techniques using `sklearn.preprocessing` on a given dataset. Evaluate these transformations with a machine learning model, ensuring that the data preprocessing steps improve model performance. # Step-by-step Instructions: 1. Load your dataset: Load a dataset from sklearn\'s dataset module. Use the `load_iris` dataset. 2. Split the dataset: Split the dataset into training and testing sets using `train_test_split` from `sklearn.model_selection`. 3. Standardization: a. Use `StandardScaler` to standardize the features. b. Apply the transformation on the training data and then on the test data. 4. MinMax Scaling: a. Use `MinMaxScaler` to scale the features to a given range. b. Apply the transformation on the training data and then on the test data. 5. Normalization: a. Normalize the features using L2 norm. b. Apply the transformation on the training data and then on the test data. 6. Encoding categorical features: a. Invent a categorical feature column for demonstration, or modify the dataset to include categorical data. 7. Create and evaluate a machine learning model: a. Use logistic regression or any other classifier to fit the processed data. b. Evaluate the model accuracy on test data to see the difference based on different preprocessing. # Submission Requirements: - Implement the steps and show your results, making sure intermediate results are printed for debugging. - Ensure all transformations are correctly applied, and the modelâ€™s accuracy with and without these transformations is reported. - Submit the code within a Jupyter notebook and the results. # Example Code Template: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, OneHotEncoder from sklearn.linear_model import LogisticRegression import numpy as np import pandas as pd # Step 1: Load the dataset data = load_iris() X, y = data.data, data.target # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Standardization scaler = StandardScaler() X_train_std = scaler.fit_transform(X_train) X_test_std = scaler.transform(X_test) # Step 4: MinMax Scaling min_max_scaler = MinMaxScaler() X_train_minmax = min_max_scaler.fit_transform(X_train) X_test_minmax = min_max_scaler.transform(X_test) # Step 5: Normalization normalizer = Normalizer(norm=\'l2\') X_train_norm = normalizer.fit_transform(X_train) X_test_norm = normalizer.transform(X_test) # Step 6: Encoding Categorical Features (For demo purposes, we assume some features are categorical) # You can add a dummy categorical column to X_train and X_test feature_names = data.feature_names + [\'categorical_feature\'] dummy_categories = np.random.choice([\'A\', \'B\', \'C\'], X_train.shape[0]).reshape(-1, 1) X_train_cat = np.hstack((X_train, dummy_categories)) dummy_categories_test = np.random.choice([\'A\', \'B\', \'C\'], X_test.shape[0]).reshape(-1, 1) X_test_cat = np.hstack((X_test, dummy_categories_test)) # OneHotEncoding the categorical feature encoder = OneHotEncoder() X_train_cat = encoder.fit_transform(X_train_cat[:, -1].reshape(-1, 1)).toarray() X_test_cat = encoder.transform(X_test_cat[:, -1].reshape(-1, 1)).toarray() # Concatenate encoded features back to the original dataset X_train_enc = np.hstack((X_train[:, :-1], X_train_cat)) X_test_enc = np.hstack((X_test[:, :-1], X_test_cat)) # Step 7: Create and Evaluate Machine Learning Model clf = LogisticRegression(max_iter=200) clf.fit(X_train_std, y_train) score_std = clf.score(X_test_std, y_test) clf.fit(X_train_minmax, y_train) score_minmax = clf.score(X_test_minmax, y_test) clf.fit(X_train_norm, y_train) score_norm = clf.score(X_test_norm, y_test) # Assume we did similar steps for X_train_enc and X_test_enc print(f\'Standard Scaler Accuracy: {score_std}\') print(f\'MinMax Scaler Accuracy: {score_minmax}\') print(f\'Normalizer Accuracy: {score_norm}\') # Continue showing accuracy for each preprocessing type ```","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, OneHotEncoder from sklearn.linear_model import LogisticRegression import numpy as np import pandas as pd def preprocess_and_evaluate(): # Step 1: Load the dataset data = load_iris() X, y = data.data, data.target # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Standardization scaler = StandardScaler() X_train_std = scaler.fit_transform(X_train) X_test_std = scaler.transform(X_test) # Step 4: MinMax Scaling min_max_scaler = MinMaxScaler() X_train_minmax = min_max_scaler.fit_transform(X_train) X_test_minmax = min_max_scaler.transform(X_test) # Step 5: Normalization normalizer = Normalizer(norm=\'l2\') X_train_norm = normalizer.fit_transform(X_train) X_test_norm = normalizer.transform(X_test) # Assume we did similar steps if needed for X_train_enc and X_test_enc # Step 7: Create and Evaluate Machine Learning Model clf = LogisticRegression(max_iter=200) clf.fit(X_train_std, y_train) score_std = clf.score(X_test_std, y_test) clf.fit(X_train_minmax, y_train) score_minmax = clf.score(X_test_minmax, y_test) clf.fit(X_train_norm, y_train) score_norm = clf.score(X_test_norm, y_test) return score_std, score_minmax, score_norm # Expose the results for easy testing result = preprocess_and_evaluate()"},{"question":"# PyTorch Named Tensors Manipulation **Objective**: Implement a function that manipulates named tensors in PyTorch by performing several operations including refining names, flattening certain dimensions, and aligning them for element-wise operations. # Problem Statement Write a function `process_named_tensors` that does the following: 1. Creates three tensors: - Tensor A of size (3, 5, 4) with names (\'Batch\', \'Time\', \'Feature\'). - Tensor B of size (5, 4) with names (\'Time\', \'Feature\'). - Tensor C of size (4, 5) with names (\'Feature\', \'Time\'). 2. Refines the names of Tensor B and Tensor C as needed to ensure they can be aligned with Tensor A. 3. Aligns Tensor B and Tensor C to the dimensions of Tensor A and performs element-wise addition with Tensor A. 4. Flattens the \'Time\' and \'Feature\' dimensions of the resulting tensor into a single dimension named \'TimeFeature\'. 5. Returns the final tensor. **Function Signature** ```python def process_named_tensors() -> torch.Tensor: pass ``` # Constraints - Input tensors should be created as specified within the function. - Tensors B and C must be aligned properly before performing element-wise addition with Tensor A. - Use named tensor operations to manipulate and align tensor dimensions. # Example ```python final_tensor = process_named_tensors() print(final_tensor.names) # Expected: (\'Batch\', \'TimeFeature\') print(final_tensor.shape) # Expected: torch.Size([3, 20]) ``` # Notes - Assume `torch` has been imported and is available. - Focus on using named tensor methods such as `refine_names`, `align_to`, and `flatten`.","solution":"import torch def process_named_tensors(): # Create the tensors with the specified sizes and names tensor_A = torch.randn(3, 5, 4, names=(\'Batch\', \'Time\', \'Feature\')) tensor_B = torch.randn(5, 4, names=(\'Time\', \'Feature\')) tensor_C = torch.randn(4, 5, names=(\'Feature\', \'Time\')) # Refine names of tensors B and C to match those of tensor A tensor_B = tensor_B.refine_names(\'Time\', \'Feature\') tensor_C = tensor_C.refine_names(\'Feature\', \'Time\') # Align tensors B and C to the dimensions of tensor A tensor_B_aligned = tensor_B.align_to(\'Batch\', \'Time\', \'Feature\') tensor_C_aligned = tensor_C.align_to(\'Batch\', \'Time\', \'Feature\') # Perform element-wise addition with tensor A result = tensor_A + tensor_B_aligned + tensor_C_aligned # Flatten the \'Time\' and \'Feature\' dimensions into a single dimension \'TimeFeature\' result = result.flatten((\'Time\', \'Feature\'), \'TimeFeature\') return result"},{"question":"**Objective:** You are required to implement a Python function that demonstrates your understanding of operating system interactions, argument parsing, and string formatting. **Question:** Implement a function `fetch_system_info` that performs the following tasks: 1. Interacts with the operating system to fetch the following information: - Current user name - Current working directory - List of files in the current working directory 2. Accepts input parameters to filter the list of files. The function should take two optional parameters: - `extension_filter` (a string): Filters the list of files to only include files with the given extension. - `min_size` (an integer): Filters the list of files to only include files with a size greater than or equal to the given value in bytes. 3. Validates the input parameters to ensure they are valid types and within acceptable ranges. Properly handle and raise exceptions for invalid inputs. 4. Formats the final output as a string that includes: - The current user name - The current working directory - The filtered list of files, each formatted as \\"file_name (size bytes)\\" **Constraints:** - `extension_filter` should be a valid file extension (e.g., `.txt`, `.py`). - `min_size` should be a positive integer or zero. - If `extension_filter` is not specified, all files should be included. - If `min_size` is not specified, it should default to 0. **Function Signature:** ```python def fetch_system_info(extension_filter: str = None, min_size: int = 0) -> str: pass ``` **Expected Input and Output:** *Example:* ```python # Assuming the current user name is \\"user123\\", # the current working directory is \\"/home/user123/projects\\" # and the files in this directory are: # \\"example.py\\" (250 bytes), \\"README.md\\" (1024 bytes), \\"notes.txt\\" (500 bytes). print(fetch_system_info(extension_filter=\\".py\\", min_size=200)) # Output: # \\"User: user123 # Current Directory: /home/user123/projects # Files: # example.py (250 bytes)\\" ``` **Requirements:** - Use appropriate system functions to fetch user and directory information. - Use argument parsing techniques to validate inputs. - Implement proper error handling for invalid inputs. - Use string formatting to generate the final output.","solution":"import os def fetch_system_info(extension_filter: str = None, min_size: int = 0) -> str: # Validate min_size parameter if not isinstance(min_size, int) or min_size < 0: raise ValueError(\\"min_size should be a non-negative integer.\\") # Validate extension_filter parameter if extension_filter and not isinstance(extension_filter, str): raise ValueError(\\"extension_filter should be a string.\\") # Fetch current user name current_user = os.getlogin() # Fetch current working directory current_directory = os.getcwd() # Fetch list of files in the current directory files = os.listdir(current_directory) filtered_files = [] for file_name in files: file_path = os.path.join(current_directory, file_name) if os.path.isfile(file_path): file_size = os.path.getsize(file_path) # Apply the filters if (not extension_filter or file_name.endswith(extension_filter)) and file_size >= min_size: filtered_files.append(f\\"{file_name} ({file_size} bytes)\\") # Format the output result = f\\"User: {current_user}nCurrent Directory: {current_directory}nFiles:n\\" result += \\"n\\".join(filtered_files) return result"},{"question":"**Objective:** Implement a custom Python class, `CustomSequence`, that mimics common sequence operations similar to the ones provided in the `python310` package documentation. Your implementation should be efficient and follow Pythonic protocols. **Instructions:** Create a class `CustomSequence` which should support the following operations: 1. Initialization with any iterable. 2. Length retrieval using `len()`. 3. Item access and assignment using indexing (e.g., `obj[index]` and `obj[index] = value`). 4. Slicing and slicing assignment. 5. Concatenation and in-place concatenation using `+` and `+=`. 6. Repetition and in-place repetition using `*` and `*=`. 7. Checking membership using the `in` keyword. 8. Counting occurrences of a value. 9. Getting the index of a value. **Constraints:** 1. The index operations should handle negative indices correctly. 2. You should raise appropriate exceptions for invalid operations (e.g., IndexError for out-of-bounds access, TypeError for invalid types). 3. Ensure efficient memory management and avoid unnecessary copying of data. **Function Signatures:** ```python class CustomSequence: def __init__(self, data): # Initialize with an iterable pass def __len__(self): # Return the length of the sequence pass def __getitem__(self, index): # Return the item at the specified index pass def __setitem__(self, index, value): # Set the item at the specified index pass def __delitem__(self, index): # Delete the item at the specified index pass def __contains__(self, value): # Check if the sequence contains the value pass def count(self, value): # Count the occurrences of a value pass def index(self, value): # Return the index of the value in the sequence pass def __add__(self, other): # Concatenate with another sequence pass def __iadd__(self, other): # In-place concatenation with another sequence pass def __mul__(self, count): # Repeat sequence pass def __imul__(self, count): # In-place repeat sequence pass def __getslice__(self, start, end): # Get a slice of the sequence pass def __setslice__(self, start, end, values): # Set a slice of the sequence pass def __delslice__(self, start, end): # Delete a slice of the sequence pass ``` **Example Usage:** ```python seq = CustomSequence([1, 2, 3, 4]) print(len(seq)) # Output: 4 print(seq[1]) # Output: 2 seq[1] = 5 print(seq[1]) # Output: 5 del seq[1] print(seq) # Output: CustomSequence([1, 3, 4]) print(3 in seq) # Output: True print(seq.count(3)) # Output: 1 print(seq.index(4)) # Output: 2 seq2 = CustomSequence([5, 6]) print(seq + seq2) # Output: CustomSequence([1, 3, 4, 5, 6]) seq += [7, 8] print(seq) # Output: CustomSequence([1, 3, 4, 7, 8]) print(seq * 2) # Output: CustomSequence([1, 3, 4, 7, 8, 1, 3, 4, 7, 8]) seq *= 2 print(seq) # Output: CustomSequence([1, 3, 4, 7, 8, 1, 3, 4, 7, 8]) ``` **Performance Requirements:** Ensure that your operations are efficient, especially for common sequence operations like indexing, slicing, and concatenation.","solution":"class CustomSequence: def __init__(self, data): self.data = list(data) def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __contains__(self, value): return value in self.data def count(self, value): return self.data.count(value) def index(self, value): return self.data.index(value) def __add__(self, other): if isinstance(other, CustomSequence): return CustomSequence(self.data + other.data) return CustomSequence(self.data + list(other)) def __iadd__(self, other): if isinstance(other, CustomSequence): self.data += other.data else: self.data += list(other) return self def __mul__(self, count): return CustomSequence(self.data * count) def __imul__(self, count): self.data *= count return self def __getslice__(self, start, end): return CustomSequence(self.data[start:end]) def __setslice__(self, start, end, values): self.data[start:end] = values def __delslice__(self, start, end): del self.data[start:end] def __repr__(self): return f\\"CustomSequence({self.data})\\""},{"question":"# Python Coding Assessment Question Objective Demonstrate understanding of file operations, command-line argument parsing, and creating tarball packages in Python. Problem Statement You are tasked with creating a Python script named `create_distribution.py` that emulates a simplified version of the `sdist` command described in the documentation. This script will create a source distribution tarball (`.tar.gz`) from a specified set of source files and directories. Requirements 1. The script should accept the following command-line arguments: - `--input-dir`: Path to the directory containing files to include in the source distribution. - `--manifest-file` (optional): Path to a manifest file (`manifest.txt`). If provided, this file lists the files and directories to be included in the tarball (one per line). If this argument is not supplied, all files in the `--input-dir` should be included. - `--output-file`: Path to the output tarball file (e.g., `distribution.tar.gz`). 2. If a manifest file is specified, only the files and directories listed in the manifest should be included in the tarball. 3. The script should follow symlinks, if any, and include the target files in the tarball. 4. Ensure the tarball is created in gzip-compressed format (`.tar.gz`). 5. The script should handle errors gracefully, such as: - Invalid directory or file paths. - Missing required arguments. - Problems during tarball creation. Input and Output Formats - **Input**: Command-line arguments as described above. - **Output**: A gzip-compressed tarball file containing the specified source files and directories. Example Usage ```sh python create_distribution.py --input-dir ./project --manifest-file ./manifest.txt --output-file ./distribution.tar.gz ``` Constraints - Python 3.10 or newer should be used. - You can assume that the input directory and manifest file, if provided, are valid and accessible. - The script should prioritize simplicity and correctness over performance. Evaluation Criteria - Correctness: The script correctly processes the input arguments and creates the expected tarball. - Robustness: The script handles edge cases and errors gracefully. - Code Quality: The script is well-organized, readable, and follows good coding practices. Implementation Tips - Use the `argparse` module to handle command-line arguments. - Use the `tarfile` module to create the tarball package. - Remember to handle both files and directories when creating the tarball. Good luck!","solution":"import argparse import os import tarfile def create_tarball(input_dir, manifest_file, output_file): files_to_include = [] if manifest_file: with open(manifest_file, \'r\') as mf: files_to_include = [line.strip() for line in mf if line.strip()] else: for root, dirs, files in os.walk(input_dir): for file in files: files_to_include.append(os.path.relpath(os.path.join(root, file), input_dir)) with tarfile.open(output_file, \\"w:gz\\") as tar: for file in files_to_include: file_path = os.path.join(input_dir, file) if os.path.exists(file_path): tar.add(file_path, arcname=file) else: print(f\\"Warning: {file_path} does not exist and cannot be added to the tarball\\") if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Create a source distribution tarball.\\") parser.add_argument(\'--input-dir\', required=True, help=\\"Directory containing files to include in the source distribution.\\") parser.add_argument(\'--manifest-file\', help=\\"Path to a manifest file listing the files and directories to include in the tarball.\\") parser.add_argument(\'--output-file\', required=True, help=\\"Path to the output tarball file (e.g., distribution.tar.gz).\\") args = parser.parse_args() if not os.path.isdir(args.input_dir): print(f\\"Error: The specified input directory does not exist: {args.input_dir}\\") else: create_tarball(args.input_dir, args.manifest_file, args.output_file)"},{"question":"# Task You are given a WAV file and your task is to create a new WAV file where the amplitude of the audio data is modulated based on color values. You will use the `wave` and `colorsys` modules. # Input 1. A string `input_wav_file` representing the pathname of the input WAV file. 2. A string `output_wav_file` representing the pathname of the output WAV file. # Output The output is that you need to produce a new WAV file that is saved with the name specified by the `output_wav_file` parameter. # Instructions 1. Read the input WAV file using the `wave` module. 2. Extract the audio frames and manipulate the amplitude of the audio samples based on color values. 3. Write the manipulated audio data to a new WAV file using the `wave` module. # Steps 1. Read the `input_wav_file` using `wave.open`. 2. Extract the audio frames and convert them to numerical values. 3. Use the `colorsys.rgb_to_hsv` function to convert each sample to HSV (Hue, Saturation, Value) by treating the audio sample as an RGB value. 4. Modulate the audio sample based on the Hue value. For example, you can amplify or dampen the volume based on the Hue. 5. Convert the modified samples back to their original format. 6. Write the modified frames to the `output_wav_file`. # Constraints 1. Handle stereo (2 channels) and mono (1 channel) WAV files. 2. Handle 8-bit and 16-bit samples. 3. Ensure that the output WAV file has the same sample rate and number of channels as the input WAV file. # Example Suppose you\'re given an input WAV file `input.wav`. You will output a new file `output.wav` with the modulated audio data. The modulation should be clearly perceptible in terms of changes in amplitude based on the color manipulation steps described. # Note This problem requires you to demonstrate your understanding of file I/O in Python, using external libraries, and basic signal processing concepts. Make sure your code is well-documented and handle edge cases gracefully. ```python def modulate_wav_with_color(input_wav_file: str, output_wav_file: str) -> None: import wave import colorsys import struct # Open the input WAV file with wave.open(input_wav_file, \'rb\') as wav_in: # Extract WAV file parameters params = wav_in.getparams() num_channels = wav_in.getnchannels() sample_width = wav_in.getsampwidth() num_frames = wav_in.getnframes() framerate = wav_in.getframerate() # Extract the frames frames = wav_in.readframes(num_frames) # Convert frames to sample values based on sample width if sample_width == 1: # 8-bit samples samples = list(frames) elif sample_width == 2: # 16-bit samples samples = list(struct.unpack(\'<\' + \'h\' * num_frames * num_channels, frames)) else: raise ValueError(\\"Unsupported sample width\\") # Process each sample modified_samples = [] for i in range(0, len(samples), num_channels): for ch in range(num_channels): sample = samples[i + ch] # Normalize sample to range [0, 1] if sample_width == 1: norm_sample = sample / 255.0 else: norm_sample = (sample + 32768) / 65535.0 # Convert to HSV hue, saturation, value = colorsys.rgb_to_hsv(norm_sample, norm_sample, norm_sample) # Modulate amplitude based on hue (example: using hue to change the sample) new_value = value * hue # Denormalize sample if sample_width == 1: new_sample = int(new_value * 255) else: new_sample = int(new_value * 65535) - 32768 modified_samples.append(new_sample) # Convert modified samples back to frames if sample_width == 1: modified_frames = bytes(modified_samples) elif sample_width == 2: modified_frames = struct.pack(\'<\' + \'h\' * len(modified_samples), *modified_samples) # Write to output WAV file with wave.open(output_wav_file, \'wb\') as wav_out: wav_out.setparams(params) wav_out.writeframes(modified_frames) ```","solution":"def modulate_wav_with_color(input_wav_file: str, output_wav_file: str) -> None: import wave import colorsys import struct # Open the input WAV file with wave.open(input_wav_file, \'rb\') as wav_in: # Extract WAV file parameters params = wav_in.getparams() num_channels = wav_in.getnchannels() sample_width = wav_in.getsampwidth() num_frames = wav_in.getnframes() framerate = wav_in.getframerate() # Extract the frames frames = wav_in.readframes(num_frames) # Convert frames to sample values based on sample width if sample_width == 1: # 8-bit samples samples = list(frames) elif sample_width == 2: # 16-bit samples samples = list(struct.unpack(\'<\' + \'h\' * num_frames * num_channels, frames)) else: raise ValueError(\\"Unsupported sample width\\") # Process each sample modified_samples = [] for i in range(0, len(samples), num_channels): for ch in range(num_channels): sample = samples[i + ch] # Normalize sample to range [0, 1] if sample_width == 1: norm_sample = sample / 255.0 else: norm_sample = (sample + 32768) / 65535.0 # Convert to HSV hue, saturation, value = colorsys.rgb_to_hsv(norm_sample, norm_sample, norm_sample) # Modulate amplitude based on hue (example: using hue to change the sample) new_value = value * hue # Denormalize sample if sample_width == 1: new_sample = int(new_value * 255) else: new_sample = int(new_value * 65535) - 32768 modified_samples.append(new_sample) # Convert modified samples back to frames if sample_width == 1: modified_frames = bytes(modified_samples) elif sample_width == 2: modified_frames = struct.pack(\'<\' + \'h\' * len(modified_samples), *modified_samples) # Write to output WAV file with wave.open(output_wav_file, \'wb\') as wav_out: wav_out.setparams(params) wav_out.writeframes(modified_frames)"},{"question":"**Objective**: Implement k-nearest neighbors classification and regression using `scikit-learn` and evaluate the performance of different neighbor search algorithms. **Problem Statement**: You are provided with two datasets: 1. A synthetic classification dataset. 2. A synthetic regression dataset. You need to: 1. Implement a k-nearest neighbors classifier and a k-nearest neighbors regressor using `sklearn.neighbors` module. 2. Evaluate the models using different neighbor search algorithms: `\'auto\'`, `\'ball_tree\'`, `\'kd_tree\'`, and `\'brute\'`. 3. Compare the classification and regression accuracy of different algorithms and present a summary of your findings. **Instructions**: 1. **Generate Datasets**: - Create a synthetic classification dataset using `make_classification` from `sklearn.datasets`. - Create a synthetic regression dataset using `make_regression` from `sklearn.datasets`. 2. **Implement k-NN Classifier**: - Use `KNeighborsClassifier` from `sklearn.neighbors`. - Fit the classifier on the training data and predict on the test data. - Experiment with different `algorithm` parameters: `\'auto\'`, `\'ball_tree\'`, `\'kd_tree\'`, `\'brute\'`. - Compute and print the accuracy for each algorithm. 3. **Implement k-NN Regressor**: - Use `KNeighborsRegressor` from `sklearn.neighbors`. - Fit the regressor on the training data and predict on the test data. - Experiment with different `algorithm` parameters: `\'auto\'`, `\'ball_tree\'`, `\'kd_tree\'`, `\'brute\'`. - Compute and print the mean squared error (MSE) for each algorithm. 4. **Summarize Results**: - Compare the performance (accuracy for classification and MSE for regression) of different algorithms. - Discuss which algorithm performed best and under what conditions. **Expected Input and Output Formats**: *Input*: - No explicit input besides internal dataset generation via `make_classification` and `make_regression`. *Output*: - Accuracy scores for k-NN classifiers using different algorithms. - MSE for k-NN regressors using different algorithms. - A summary of the performance comparison. **Constraints and Performance Requirements**: - Use default parameters for `make_classification` and `make_regression` datasets unless specified otherwise. - Evaluate all models on a consistent test set size for fair comparison. - Ensure that computations are efficient and do not exceed reasonable time and space limits. **Example Code Structure**: ```python import numpy as np from sklearn.datasets import make_classification, make_regression from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor from sklearn.metrics import accuracy_score, mean_squared_error # Generate synthetic datasets X_classification, y_classification = make_classification(n_samples=1000, n_features=20, random_state=42) X_regression, y_regression = make_regression(n_samples=1000, n_features=20, random_state=42) # Split datasets into training and test sets X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_classification, y_classification, test_size=0.3, random_state=42) X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_regression, y_regression, test_size=0.3, random_state=42) # Define the neighbor algorithms to evaluate algorithms = [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'] # Initialize results storage results_classifier = {} results_regressor = {} # Evaluate classifiers for alg in algorithms: clf = KNeighborsClassifier(n_neighbors=5, algorithm=alg) clf.fit(X_train_clf, y_train_clf) y_pred_clf = clf.predict(X_test_clf) acc = accuracy_score(y_test_clf, y_pred_clf) results_classifier[alg] = acc print(f\\"Classifier Accuracy with {alg}: {acc:.4f}\\") # Evaluate regressors for alg in algorithms: reg = KNeighborsRegressor(n_neighbors=5, algorithm=alg) reg.fit(X_train_reg, y_train_reg) y_pred_reg = reg.predict(X_test_reg) mse = mean_squared_error(y_test_reg, y_pred_reg) results_regressor[alg] = mse print(f\\"Regressor MSE with {alg}: {mse:.4f}\\") # Summarize results print(\\"nSummary of Classifier Results:\\") for alg, acc in results_classifier.items(): print(f\\"Algorithm: {alg}, Accuracy: {acc:.4f}\\") print(\\"nSummary of Regressor Results:\\") for alg, mse in results_regressor.items(): print(f\\"Algorithm: {alg}, MSE: {mse:.4f}\\") ``` **Notes**: This question tests students\' understanding of nearest neighbors\' methods, model training, evaluation metrics, and their ability to compare algorithm performance effectively.","solution":"import numpy as np from sklearn.datasets import make_classification, make_regression from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor from sklearn.metrics import accuracy_score, mean_squared_error def evaluate_knn_models(): # Generate synthetic datasets X_classification, y_classification = make_classification(n_samples=1000, n_features=20, random_state=42) X_regression, y_regression = make_regression(n_samples=1000, n_features=20, random_state=42) # Split datasets into training and test sets X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_classification, y_classification, test_size=0.3, random_state=42) X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_regression, y_regression, test_size=0.3, random_state=42) # Define the neighbor algorithms to evaluate algorithms = [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'] # Initialize results storage results_classifier = {} results_regressor = {} # Evaluate classifiers for alg in algorithms: clf = KNeighborsClassifier(n_neighbors=5, algorithm=alg) clf.fit(X_train_clf, y_train_clf) y_pred_clf = clf.predict(X_test_clf) acc = accuracy_score(y_test_clf, y_pred_clf) results_classifier[alg] = acc print(f\\"Classifier Accuracy with {alg}: {acc:.4f}\\") # Evaluate regressors for alg in algorithms: reg = KNeighborsRegressor(n_neighbors=5, algorithm=alg) reg.fit(X_train_reg, y_train_reg) y_pred_reg = reg.predict(X_test_reg) mse = mean_squared_error(y_test_reg, y_pred_reg) results_regressor[alg] = mse print(f\\"Regressor MSE with {alg}: {mse:.4f}\\") return results_classifier, results_regressor"},{"question":"# Pandas Copy-on-Write Assessment You are provided with a dataset containing information about employees in a company. Your task is to perform a series of operations on this dataset to get familiarized with the Copy-on-Write (CoW) feature in pandas. Ensure that the operations adhere strictly to the principles of CoW, preventing unintended side-effects on shared data. Input - A pandas DataFrame `df` with the following columns: `employee_id` (unique identifier for each employee), `department`, `salary`, `years_with_company`. - Example: ```python df = pd.DataFrame({ \\"employee_id\\": [1, 2, 3, 4], \\"department\\": [\\"HR\\", \\"Engineering\\", \\"Engineering\\", \\"HR\\"], \\"salary\\": [50000, 70000, 80000, 55000], \\"years_with_company\\": [1, 5, 3, 2] }) ``` Required Operations 1. **Find Subset**: Select all employees from the \\"Engineering\\" department and store them in a new DataFrame. 2. **Update Subset**: Increase the salary of the selected subset of employees by 10%. 3. **Validate CoW**: Ensure that the original DataFrame `df` remains unchanged after performing the operation on the subset. 4. **Manipulate Original**: Using `.loc` or `.iloc`, update the salary of the employee with `employee_id` 1 to `60000` directly in the original DataFrame. 5. **Extract NumPy**: Convert the original DataFrame to a NumPy array and demonstrate that it is read-only. 6. **Performance Optimization**: Reassign a reset index DataFrame back to the original variable to avoid unnecessary copies and demonstrate how CoW optimizes this process. Output - For each step, print the results to demonstrate that the operations have been performed correctly and CoW principles have been upheld. - Functions should follow this signature: ```python def manipulate_employees(df: pd.DataFrame) -> None: # perform the operations and print the results ``` Example Execution ```python df = pd.DataFrame({ \\"employee_id\\": [1, 2, 3, 4], \\"department\\": [\\"HR\\", \\"Engineering\\", \\"Engineering\\", \\"HR\\"], \\"salary\\": [50000, 70000, 80000, 55000], \\"years_with_company\\": [1, 5, 3, 2] }) manipulate_employees(df) ``` Constraints and Requirements - Avoid using chained assignments; use `.loc` or `.iloc` instead. - Ensure the read-only property for NumPy arrays derived from pandas objects. - Demonstrate performance improvements using CoW optimizations. Your task is to write the `manipulate_employees` function that adheres to these requirements and performs the necessary operations as described.","solution":"import pandas as pd import numpy as np def manipulate_employees(df: pd.DataFrame) -> None: # 1. Find Subset engineering_employees = df[df[\'department\'] == \'Engineering\'].copy() # 2. Update Subset engineering_employees[\'salary\'] = engineering_employees[\'salary\'] * 1.10 # 3. Validate CoW print(\\"Original DataFrame after updating the subset (should remain unchanged):\\") print(df) print(\\"nSubset with updated salaries:\\") print(engineering_employees) # 4. Manipulate Original df.loc[df[\'employee_id\'] == 1, \'salary\'] = 60000 print(\\"nOriginal DataFrame after updating employee_id 1\'s salary:\\") print(df) # 5. Extract NumPy df_array = df.values df_array.setflags(write=False) try: df_array[0, 2] = 70000 except ValueError as e: print(\\"nTrying to modify the read-only numpy array resulted in:\\") print(e) # 6. Performance Optimization df.reset_index(inplace=True, drop=True) print(\\"nOriginal DataFrame after resetting its index (same object reference):\\") print(df)"},{"question":"# Question: Create a Customized Plot with Seaborn and Annotate You are given a dataset containing model performance scores across different tasks. Your task is to use the dataset to create a complex plot using the seaborn.objects interface. The plot should include customized annotations. Here\'s the dataset: ```python import pandas as pd data = { \'Model\': [\'Model A\', \'Model B\', \'Model C\', \'Model D\'], \'Encoder\': [\'Transformer\', \'LSTM\', \'Transformer\', \'LSTM\'], \'SST-2\': [80, 72, 75, 68], \'MRPC\': [82, 65, 78, 70], \'RTE\': [75, 70, 80, 66] } df = pd.DataFrame(data) ``` Follow the steps below to create the plot: 1. **Prepare the Data**: - Pivot the dataset so that the tasks (`\'SST-2\'`, `\'MRPC\'`, `\'RTE\'`) are columns, and models are indexed by their names and encoder types. - Calculate the average score across the tasks for each model and add it as a new column `\'Average\'`. - Sort the data by the `\'Average\'` score in descending order. 2. **Create the Plot**: - Plot a bar chart where the x-axis represents model names, and the y-axis represents the average scores. - Annotate the bars with the average scores, aligned to the right of the bars with white text color. - Add a dot plot above the bars where the x-axis represents `\'SST-2\'` scores and the y-axis represents `\'MRPC\'` scores. - Annotate the dots with model names, aligned above the dots. - Map the color of the text annotations to the `\'Encoder\'` type. 3. **Customize the Text Appearance**: - Align the text annotations based on the encoder type (`\'LSTM\'` should be left-aligned, and `\'Transformer\'` should be right-aligned). - Use additional matplotlib parameters to set the font weight of the text annotations to bold. # Input - The provided pandas DataFrame `df`. # Output - A customized seaborn plot with the specified annotations and visual adjustments. # Constraints - You must use the `seaborn.objects` interface for all plotting and annotation tasks. - Ensure text annotations are clear and readable against the plot background. # Example Plot Your plot should look similar to the following: ``` | O <Model A> Average | O <Model C> Score *(80)| O <Model B> | O <Model D> Models : Model A Model B Model C Model D ``` Here is a skeleton of the Python code to get you started: ```python import seaborn.objects as so import pandas as pd # Step 1: Prepare the data df = pd.DataFrame({ \'Model\': [\'Model A\', \'Model B\', \'Model C\', \'Model D\'], \'Encoder\': [\'Transformer\', \'LSTM\', \'Transformer\', \'LSTM\'], \'SST-2\': [80, 72, 75, 68], \'MRPC\': [82, 65, 78, 70], \'RTE\': [75, 70, 80, 66] }) # Pivoting and adding average column glue = ( df.pivot(index=[\'Model\', \'Encoder\'], columns=\'Task\', values=[\'SST-2\', \'MRPC\', \'RTE\']) .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\'Average\', ascending=False) ) # Step 2: Create the plot plot = ( so.Plot(glue, x=\'Average\', y=\'Model\', text=\'Average\') .add(so.Bar()) .add(so.Text(color=\'w\', halign=\'right\')) .add(so.Dot()) .add(so.Text(), halign=\'Encoder\') .scale(halign={\'LSTM\': \'left\', \'Transformer\': \'right\'}) ) # Step 3: Customize the text appearance plot.add(so.Text({\'fontweight\':\'bold\'})).show() ``` Implement your solution by completing the skeleton code with the correct steps as described.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Step 1: Prepare the data df = pd.DataFrame({ \'Model\': [\'Model A\', \'Model B\', \'Model C\', \'Model D\'], \'Encoder\': [\'Transformer\', \'LSTM\', \'Transformer\', \'LSTM\'], \'SST-2\': [80, 72, 75, 68], \'MRPC\': [82, 65, 78, 70], \'RTE\': [75, 70, 80, 66] }) # Calculate the average score across tasks and add it as a new column \'Average\' df[\'Average\'] = df[[\'SST-2\', \'MRPC\', \'RTE\']].mean(axis=1) # Sort the data by the \'Average\' score in descending order df = df.sort_values(\'Average\', ascending=False) # Step 2: Create the Plot fig, ax = plt.subplots() # Bar Plot for Average Scores sns.barplot(data=df, x=\'Model\', y=\'Average\', ax=ax, palette=\'viridis\') # Annotate the bars with average scores for index, row in df.iterrows(): ax.text(row.name, row[\'Average\'] + 0.5, round(row[\'Average\'], 1), color=\'white\', ha=\'center\', weight=\'bold\') # Dot Plot for SST-2 and MRPC scores above the bars for index, row in df.iterrows(): ax.plot(row[\'Model\'], row[\'SST-2\'], \'o\', label=\'SST-2\' if index == 0 else \\"\\", color=\'blue\') ax.text(row[\'Model\'], row[\'SST-2\'] + 0.5, row[\'Model\'], ha=\'right\' if row[\'Encoder\'] == \'Transformer\' else \'left\', color=\'blue\', weight=\'bold\') ax.plot(row[\'Model\'], row[\'MRPC\'], \'o\', label=\'MRPC\' if index == 0 else \\"\\", color=\'red\') ax.text(row[\'Model\'], row[\'MRPC\'] + 0.5, row[\'Model\'], ha=\'right\' if row[\'Encoder\'] == \'Transformer\' else \'left\', color=\'red\', weight=\'bold\') # Legend customization handles, labels = ax.get_legend_handles_labels() ax.legend(handles[:2], labels[:2]) # Show only the first two legend labels # Show Plot plt.show()"},{"question":"# Question You are given a dataset on health expenditures (`healthexp`) that records annual healthcare spending in USD for various countries. Your task is to: 1. Load and preprocess the dataset. 2. Create a faceted area plot to visualize the health expenditure trends over the years for each country. 3. Customize different aspects of the plot, including color, edgecolor, and orientation. 4. Stack the area plots to show part-whole relationships. Requirements: 1. **Data Preparation:** - Load the `healthexp` dataset. - Pivot the dataset to have `Year` as the index, `Country` as columns, and `Spending_USD` as values. - Interpolate missing values. - Reset the index and sort by `Country`. 2. **Plotting:** - Create a faceted area plot with the following specifications: - Each facet should represent a different country, arranged in a wrapping layout. - Use country names to color-code the areas. - Use country names to set the edge color of the areas. - Add a line plot on top of the area plot. - The final area plots should be stacked to show part-whole relationships over the years. 3. **Implementation Details:** - You should use `seaborn.objects.Plot` and related objects to create the plot. - Use appropriate methods and functions to achieve the requirements. - Ensure the plot is clear and informative. Constraints: - Use only the `seaborn.objects` module and relevant seaborn utility functions. - The final output should be a single plot object displaying the faceted, stacked area plots. ```python # Your implementation should start from here # Import the necessary libraries import seaborn.objects as so from seaborn import load_dataset # Step 1: Load and preprocess the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Step 2: Create the faceted area plot # Your code for creating the faceted area plot goes here # Expected Plot Code # p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) # p.add(so.Area(), color=\\"Country\\").add(so.Line()).add(so.Area(), edgecolor=\\"Country\\").add(so.Area(alpha=.7), so.Stack()) # You can then display the plot using the appropriate method, e.g., `p.show()` if working in a Jupyter notebook. ``` Example Output: The example output should show a plot with faceted area graphs for each country, with color-coded areas, edge colors, linear overlays, and stacked areas to depict the part-whole relationships.","solution":"import seaborn.objects as so from seaborn import load_dataset def preprocess_healthexp_data(): Load and preprocess the healthexp dataset. Returns: DataFrame: A preprocessed dataframe with interpolated values and sorted by country. healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) return healthexp def create_faceted_area_plot(data): Create a faceted area plot to visualize health expenditure trends. Args: data (DataFrame): The preprocessed healthcare expenditure data. Returns: Plot: The faceted area plot object. p = (so.Plot(data, x=\\"Year\\", y=\\"Spending_USD\\") .facet(\\"Country\\", wrap=3) .add(so.Area(), color=\\"Country\\") .add(so.Line()) .add(so.Area(), edgecolor=\\"Country\\") .add(so.Area(alpha=0.7), so.Stack()) ) return p"},{"question":"# Question: Tensor Operations and Gradient Computation Objective The purpose of this question is to assess your understanding of PyTorch tensors, including creation, manipulation, and basic operations. You will also demonstrate your knowledge of gradient computation using automatic differentiation. Problem Statement Write a function `process_and_compute_gradients` that performs the following steps: 1. **Tensor Creation:** - Create a 3x3 tensor `A` with values ranging from 1 to 9 (inclusive) with data type `torch.float32`. - Create another tensor `B` with the same shape as `A` but with all values set to 2, on the same device as `A`. 2. **Tensor Operations:** - Calculate the tensor `C` as the element-wise product of tensors `A` and `B`. - Calculate the mean value of tensor `C` and store it in a variable `mean_value`. 3. **Gradient Computation:** - Create a tensor `D` with the same values as `C`, but set the `requires_grad` flag to `True`. - Calculate the sum of squares of all elements in tensor `D` and store it in a variable `sum_of_squares`. - Perform backpropagation on `sum_of_squares` to compute the gradient with respect to `D`. 4. **Output:** - Return the following values: - The tensor `C`. - The `mean_value`. - The gradient of tensor `D` as computed by backpropagation. Function Signature ```python import torch def process_and_compute_gradients(): # Step 1: Tensor Creation A = torch.tensor([[1.0, 2.0, 3.0],[4.0, 5.0, 6.0],[7.0, 8.0, 9.0]], dtype=torch.float32) B = torch.full_like(A, 2) # Step 2: Tensor Operations C = A * B mean_value = torch.mean(C) # Step 3: Gradient Computation D = C.clone().detach().requires_grad_(True) sum_of_squares = torch.sum(D**2) sum_of_squares.backward() # Step 4: Output gradients = D.grad return C, mean_value.item(), gradients ``` Example Output ```python C, mean_value, gradients = process_and_compute_gradients() print(C) # tensor([[ 2., 4., 6.], # [ 8., 10., 12.], # [14., 16., 18.]]) print(mean_value) # 10.0 print(gradients) # tensor([[ 4., 8., 12.], # [16., 20., 24.], # [28., 32., 36.]]) ``` Constraints - You must use PyTorch methods and functions for tensor creation, operations, and gradient computation. - Ensure that all operations are performed on the correct data types and devices. No extra conversion or device transfer steps are necessary beyond what is described. Notes - Be mindful of data types and tensor shapes during operations. - Verify that gradients are computed correctly using automatic differentiation. Good luck!","solution":"import torch def process_and_compute_gradients(): # Step 1: Tensor Creation A = torch.arange(1.0, 10.0).view(3, 3) B = torch.full_like(A, 2.0, dtype=torch.float32) # Step 2: Tensor Operations C = A * B mean_value = torch.mean(C) # Step 3: Gradient Computation D = C.clone().detach().requires_grad_(True) sum_of_squares = torch.sum(D**2) sum_of_squares.backward() # Step 4: Output gradients = D.grad return C, mean_value.item(), gradients"},{"question":"# Question: Type-Safe Payment Validation You are building a payment processing system and need to ensure that the data structures used are validated correctly for type safety. Utilize the `typing` module to achieve this. Task 1. Define a `TypedDict` called `PaymentDetails` which includes the following fields: - `amount`: a `float` representing the amount to be paid. - `currency`: a `Literal` that can either be `\'USD\'`, `\'EUR\'`, or `\'GBP\'`. - `payment_method`: a `Union` of `Literal[\'credit_card\']`, `Literal[\'debit_card\']`, and `Literal[\'paypal\']`. - `metadata`: an optional dictionary with string keys and string values which can store additional payment information. 2. Implement a function `validate_payment(payment: PaymentDetails) -> None` which performs the following: - Ensures that the `amount` is positive. - Ensures that the `currency` is within the allowed literals. - Ensures that the `payment_method` is within the allowed literals. - If `metadata` is provided, ensures that each key and value in the `metadata` dictionary is a string. 3. Use `TypeGuard` to check if a dictionary conforms to the `PaymentDetails` structure. Requirements - Implement type hinting thoroughly using constructs from the `typing` module. - Include unit tests to verify the behavior of `validate_payment` function. - The implementation should raise appropriate type errors or value errors where applicable. Example Usage ```python from typing import TypedDict, Literal, Union, Optional, TypeGuard class PaymentDetails(TypedDict): amount: float currency: Literal[\'USD\', \'EUR\', \'GBP\'] payment_method: Union[Literal[\'credit_card\'], Literal[\'debit_card\'], Literal[\'paypal\']] metadata: Optional[dict[str, str]] def validate_payment(payment: PaymentDetails) -> None: # Ensure all validations are performed ... # Example Test example_payment: PaymentDetails = { \'amount\': 100.0, \'currency\': \'USD\', \'payment_method\': \'credit_card\', \'metadata\': {\'order_id\': \'1234\', \'customer_id\': \'5678\'} } validate_payment(example_payment) # Should pass without any errors ``` In this task, you will demonstrate your understanding of `TypedDict`, `Union`, `Literal`, `Optional`, and custom type guards for runtime validation.","solution":"from typing import TypedDict, Literal, Union, Optional, TypeGuard, Dict class PaymentDetails(TypedDict): amount: float currency: Literal[\'USD\', \'EUR\', \'GBP\'] payment_method: Union[Literal[\'credit_card\'], Literal[\'debit_card\'], Literal[\'paypal\']] metadata: Optional[Dict[str, str]] def is_payment_details(payment: dict) -> TypeGuard[PaymentDetails]: return ( \'amount\' in payment and isinstance(payment[\'amount\'], float) and \'currency\' in payment and payment[\'currency\'] in {\'USD\', \'EUR\', \'GBP\'} and \'payment_method\' in payment and payment[\'payment_method\'] in {\'credit_card\', \'debit_card\', \'paypal\'} and (\'metadata\' not in payment or all(isinstance(k, str) and isinstance(v, str) for k, v in payment.get(\'metadata\', {}).items())) ) def validate_payment(payment: PaymentDetails) -> None: if payment[\'amount\'] <= 0: raise ValueError(\\"Amount must be positive.\\") if payment[\'currency\'] not in [\'USD\', \'EUR\', \'GBP\']: raise ValueError(\\"Currency must be \'USD\', \'EUR\' or \'GBP\'.\\") if payment[\'payment_method\'] not in [\'credit_card\', \'debit_card\', \'paypal\']: raise ValueError(\\"Payment method must be \'credit_card\', \'debit_card\' or \'paypal\'.\\") if payment[\'metadata\']: if not all(isinstance(k, str) and isinstance(v, str) for k, v in payment[\'metadata\'].items()): raise ValueError(\\"All metadata keys and values must be strings.\\")"},{"question":"You are tasked with implementing a custom interactive Python interpreter using the `code` module. Your solution should emulate the behavior of an interactive Python console, allowing the user to input Python code and see the results of its execution. Your custom interpreter should: 1. Provide a prompt to the user to enter Python code. 2. Compile and execute the user\'s input, handling both complete and incomplete code segments. 3. Display appropriate error messages for syntax errors and other exceptions. 4. Continue to prompt the user for input until an exit command is given (`exit()`). Requirements: - Implement a function `start_interpreter()` that starts the interactive interpreter. - The function should use `code.InteractiveConsole` to maintain and run the interactive session. - Handle multi-line input appropriately, ensuring that users can enter complete code blocks. - Display syntax error messages and other exceptions to the user. Function Signature: ```python def start_interpreter() -> None: pass ``` # Example Interaction: ```python >>> start_interpreter() Custom Interactive Console (type \'exit()\' to quit) >>> print(\\"Hello, world!\\") Hello, world! >>> def add(a, b): ... return a + b ... >>> add(2, 3) 5 >>> if True: ... print(\\"True block\\") ... else: ... print(\\"False block\\") ... True block >>> exit() Exiting Custom Interactive Console # The interpreter stops here ``` # Constraints: - Only use built-in Python libraries and the `code` module. - The custom prompt should display `>>> ` for new inputs and `...` for continuation of a block. - User input should be correctly buffered and executed, maintaining the standard Python interactive behavior. **Note:** You do not need to handle advanced input/output redirection or support for all possible Python REPL features like auto-completion or history navigation.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self): super().__init__() def interact(self, banner=None): print(\\"Custom Interactive Console (type \'exit()\' to quit)\\") try: super().interact(banner) except SystemExit: print(\\"Exiting Custom Interactive Console\\") def start_interpreter() -> None: console = CustomInteractiveConsole() console.interact()"},{"question":"You are working on a file organization tool that needs to filter and group files based on Unix shell-style wildcard patterns. Your task is to implement a function that takes a list of filenames and a list of patterns and returns a dictionary where each pattern is a key, and the value is a list of filenames that match that pattern. # Function Signature: ```python def group_files_by_patterns(filenames: list[str], patterns: list[str]) -> dict[str, list[str]]: ``` # Input: - `filenames` (list of str): A list of filenames (e.g., [\\"file1.txt\\", \\"file2.py\\", \\"file3.txt\\"]). - `patterns` (list of str): A list of Unix shell-style wildcard patterns (e.g., [\\"*.txt\\", \\"*.py\\"]). # Output: - Returns a dictionary where each key is a pattern and the value is a list of filenames that match that pattern. # Constraints: - The total number of filenames and patterns combined will not exceed 1000. - Filenames and patterns will only contain ASCII characters. - Case normalization should be handled as per the standard for the operating system. # Example: ```python filenames = [\\"file1.txt\\", \\"file2.py\\", \\"file3.txt\\", \\"script.py\\"] patterns = [\\"*.txt\\", \\"*.py\\"] output = group_files_by_patterns(filenames, patterns) print(output) ``` Expected Output: ```python { \\"*.txt\\": [\\"file1.txt\\", \\"file3.txt\\"], \\"*.py\\": [\\"file2.py\\", \\"script.py\\"] } ``` # Notes: - Use the `fnmatch` module to perform the pattern matching. - Ensure your solution considers case-normalization as per the operating system\'s standard. # Performance: - The function should be efficient even with the maximum constraint of 1000 filenames and patterns.","solution":"import fnmatch def group_files_by_patterns(filenames: list[str], patterns: list[str]) -> dict[str, list[str]]: Groups files by Unix shell-style wildcard patterns. Parameters: - filenames: list of str, a list of filenames. - patterns: list of str, a list of Unix shell-style wildcard patterns. Returns: - A dictionary where each key is a pattern and the value is a list of filenames that match that pattern. grouped_files = {} for pattern in patterns: matched_files = [filename for filename in filenames if fnmatch.fnmatch(filename, pattern)] grouped_files[pattern] = matched_files return grouped_files"},{"question":"# Question You are provided with a dataset containing daily closing stock prices of a fictional company stored in a pandas Series. This Series has timestamps as indices and float values representing the closing prices. Your task is to implement a function `analyze_stock_series` that performs the following operations: 1. **Identify missing data**: The dataset might have missing values. Identify these missing values. 2. **Fill missing data**: Replace missing values using a forward-fill strategy. 3. **Compute Descriptive Statistics**: Calculate the mean, median, standard deviation, and total return of the stock price. 4. **Identify Trading Opportunities**: A basic trading strategy is to buy when the stock price is at its 10-day lowest and sell when it is at its 10-day highest. Generate two Series: one for buy signals and one for sell signals, with `True` or `False` indicating whether the condition is met each day. 5. **Identify Trending Days**: Identify days when the stock price increased or decreased continuously for more than 5 days. The function signature should be: ```python def analyze_stock_series(stock_series: pd.Series) -> dict: Analyze stock price Series. Parameters: stock_series (pd.Series): Pandas Series containing stock prices with datetime indices. Returns: dict: A dictionary containing the following keys and their corresponding values: \'missing_data_indices\': Indices where data is missing. \'filled_series\': Series with missing values forward-filled. \'mean_price\': Mean stock price after filling missing data. \'median_price\': Median stock price after filling missing data. \'std_deviation\': Standard deviation after filling missing data. \'total_return\': Total return of the stock during the given period. \'buy_signals\': Series with boolean values indicating buy signals. \'sell_signals\': Series with boolean values indicating sell signals. \'upward_trend_dates\': List of dates where the stock price increased for more than 5 days. \'downward_trend_dates\': List of dates where the stock price decreased for more than 5 days. pass ``` # Input - `stock_series`: A pandas Series indexed by datetime with float values representing daily closing stock prices. # Output - A dictionary with keys and values as specified in the function signature. # Constraints - The dataset spans over multiple years. - The Series can have any frequency (daily, business daily, etc.). - The data can contain NaN values which need to be addressed. # Example Assume `stock_series` is: ``` 2023-01-01 100.0 2023-01-02 NaN 2023-01-03 102.0 ... 2023-12-31 150.0 ``` Your function should return a dictionary with computations based on the implementation details provided.","solution":"def analyze_stock_series(stock_series): Analyze stock price Series. Parameters: stock_series (pd.Series): Pandas Series containing stock prices with datetime indices. Returns: dict: A dictionary containing the following keys and their corresponding values: \'missing_data_indices\': Indices where data is missing. \'filled_series\': Series with missing values forward-filled. \'mean_price\': Mean stock price after filling missing data. \'median_price\': Median stock price after filling missing data. \'std_deviation\': Standard deviation after filling missing data. \'total_return\': Total return of the stock during the given period. \'buy_signals\': Series with boolean values indicating buy signals. \'sell_signals\': Series with boolean values indicating sell signals. \'upward_trend_dates\': List of dates where the stock price increased for more than 5 days. \'downward_trend_dates\': List of dates where the stock price decreased for more than 5 days. import pandas as pd # Identify missing data missing_data_indices = stock_series[stock_series.isna()].index.tolist() # Fill missing data using forward fill filled_series = stock_series.ffill() # Compute Descriptive Statistics mean_price = filled_series.mean() median_price = filled_series.median() std_deviation = filled_series.std() # Total return total_return = (filled_series.iloc[-1] - filled_series.iloc[0]) / filled_series.iloc[0] # Identify Trading Opportunities rolling_min = filled_series.rolling(window=10, min_periods=1).min() rolling_max = filled_series.rolling(window=10, min_periods=1).max() buy_signals = filled_series == rolling_min sell_signals = filled_series == rolling_max # Identify Trending Days upward_trend_dates = (filled_series.diff().ge(0).groupby((filled_series.diff().ge(0) != filled_series.diff().ge(0).shift()).cumsum()).cumsum() > 5).index.tolist() downward_trend_dates = (filled_series.diff().le(0).groupby((filled_series.diff().le(0) != filled_series.diff().le(0).shift()).cumsum()).cumsum() > 5).index.tolist() return { \'missing_data_indices\': missing_data_indices, \'filled_series\': filled_series, \'mean_price\': mean_price, \'median_price\': median_price, \'std_deviation\': std_deviation, \'total_return\': total_return, \'buy_signals\': buy_signals, \'sell_signals\': sell_signals, \'upward_trend_dates\': upward_trend_dates, \'downward_trend_dates\': downward_trend_dates }"},{"question":"**Shadow Password Database Analysis** You have been provided with access to the Unix shadow password database using the `spwd` module. Your task is to implement a function that retrieves user information and performs specific analyses. Please follow the instructions below to complete the task. # Function 1: `user_password_info(username: str) -> dict` This function should take a username as input and return a dictionary containing the user\'s shadow password information. Parameters: - `username` (str): The login name of the user. Returns: - A dictionary with the following keys: - `\\"login_name\\"`: The login name of the user. - `\\"encrypted_password\\"`: The encrypted password of the user. - `\\"last_change_date\\"`: The date of the last password change. - `\\"min_days_between_changes\\"`: The minimal number of days between password changes. - `\\"max_days_between_changes\\"`: The maximum number of days between password changes. - `\\"warn_days_before_expire\\"`: The number of days before password expiration to warn the user. - `\\"inactivity_days_after_expire\\"`: The number of days after password expiration until the account is disabled. - `\\"account_expire_date\\"`: The date when the account expires. - `\\"reserved_flag\\"`: The reserved flag value. Raises: - `KeyError`: If the username is not found. - `PermissionError`: If the current user does not have the required privileges to access the shadow password database. # Function 2: `expired_accounts_info() -> List[str]` This function should return a list of usernames whose accounts have expired, meaning the current date is past the account expiration date. Returns: - A list of usernames with expired accounts. # Example Usage ```python from typing import List, Dict import spwd def user_password_info(username: str) -> dict: # Implement this function pass def expired_accounts_info() -> List[str]: # Implement this function pass # Example calls print(user_password_info(\\"john_doe\\")) print(expired_accounts_info()) ``` Constraints: - You must use the `spwd` module to access shadow password data. - Assume the current system date is accessible through standard Python libraries. - Handle exceptions and errors gracefully. # Note: Ensure you test your functions in an environment where you have sufficient permissions to access shadow password data, typically as root.","solution":"import spwd import datetime from typing import List, Dict def user_password_info(username: str) -> dict: Returns shadow password information for a specified username. Parameters: - username (str): The login name of the user. Returns: - dict: Shadow password information of the user. Raises: - KeyError: If username is not found. - PermissionError: If the current user does not have required privileges. try: user_info = spwd.getspnam(username) return { \\"login_name\\": user_info.sp_namp, \\"encrypted_password\\": user_info.sp_pwdp, \\"last_change_date\\": user_info.sp_lstchg, \\"min_days_between_changes\\": user_info.sp_min, \\"max_days_between_changes\\": user_info.sp_max, \\"warn_days_before_expire\\": user_info.sp_warn, \\"inactivity_days_after_expire\\": user_info.sp_inact, \\"account_expire_date\\": user_info.sp_expire, \\"reserved_flag\\": user_info.sp_flag } except KeyError: raise KeyError(f\\"Username \'{username}\' not found.\\") except PermissionError: raise PermissionError(\\"Insufficient permissions to access the shadow password database.\\") def expired_accounts_info() -> List[str]: Returns a list of usernames with expired accounts. Returns: - list: List of usernames whose accounts have expired. expired_accounts = [] current_date = datetime.date.today().toordinal() try: for entry in spwd.getspall(): if entry.sp_expire != -1 and current_date > entry.sp_expire: expired_accounts.append(entry.sp_namp) except PermissionError: raise PermissionError(\\"Insufficient permissions to access the shadow password database.\\") return expired_accounts"},{"question":"# Objective Your task is to demonstrate your understanding of the `sklearn.datasets` module by loading a dataset using its loader, fetcher, or generator functions, and performing a specified analysis on it. # Instructions 1. **Load the Dataset**: Use the `load_iris` function to load the Iris dataset from the `sklearn.datasets` module. 2. **Data Analysis**: Write a function `summarize_iris_dataset` that: - Calculates and prints the mean value of each feature (sepal length, sepal width, petal length, petal width) in the dataset. - Determines and prints the number of samples for each target class (0, 1, 2) in the dataset. 3. **Output Format**: The output should be clear and concise, formatted as shown below. # Function Signature ```python def summarize_iris_dataset(): # Your code here ``` # Example Output ``` Feature Means: Sepal Length: 5.84 Sepal Width: 3.05 Petal Length: 3.76 Petal Width: 1.20 Class Distribution: Class 0: 50 samples Class 1: 50 samples Class 2: 50 samples ``` # Constraints - Use only the `load_iris` function from the `sklearn.datasets` module to load the dataset. - Ensure your function prints the results in the specified format. - You may use any other libraries from the standard Python library. You are expected to write clean, efficient, and well-documented code. Additionally, consider edge cases and input validation where applicable.","solution":"from sklearn.datasets import load_iris import numpy as np def summarize_iris_dataset(): # Load the dataset iris = load_iris() data = iris[\'data\'] target = iris[\'target\'] # Calculate the mean value of each feature feature_means = np.mean(data, axis=0) feature_names = iris[\'feature_names\'] # Determine the number of samples for each target class unique, counts = np.unique(target, return_counts=True) class_distribution = dict(zip(unique, counts)) # Print the feature means print(\\"Feature Means:\\") for i in range(len(feature_names)): print(f\\"{feature_names[i].title()}: {feature_means[i]:.2f}\\") # Print the class distribution print(\\"nClass Distribution:\\") for cls, count in class_distribution.items(): print(f\\"Class {cls}: {count} samples\\") # Function call for demonstration (this will be removed in unit tests) summarize_iris_dataset()"},{"question":"Objective: To assess students\' understanding of Python\'s `collections` module and their ability to implement and manipulate advanced data structures. Question: You are required to implement a specialized data structure using Python\'s `collections` module. The data structure should support efficient tracking of the top N elements. Implement a class `TopElementsTracker` that has the following functionalities: 1. **Initialization**: Takes an integer `N` which specifies how many top elements to track. 2. **Add Element**: Method `add_element(element: int)` to add an element to the tracker. 3. **Get Top Elements**: Method `get_top_elements() -> List[int]` to return a sorted list of the top N elements in descending order. Ensure that your implementation is optimized for performance, with the add operation being efficient and the retrieval of top elements being instantaneous. **Input and Output Formats:** - `TopElementsTracker(N: int)`: Initializes the tracker to keep track of top N elements. - `add_element(element: int)`: Adds an element to the tracker. - `get_top_elements() -> List[int]`: Returns a sorted list of the top N elements in descending order. **Example:** ```python tracker = TopElementsTracker(3) tracker.add_element(5) tracker.add_element(1) tracker.add_element(3) tracker.add_element(6) tracker.add_element(4) assert tracker.get_top_elements() == [6, 5, 4] ``` **Constraints:** - N will be a positive integer. - The elements to be added will be integers. - The number of elements added can be in the range of [0, 10000]. --- Utilize appropriate data structures from the `collections` module, and consider the performance implications when accessing and updating the top N elements.","solution":"from heapq import heappush, heappop, heapify class TopElementsTracker: def __init__(self, N): Initializes the tracker to keep track of top N elements. self.N = N self.min_heap = [] def add_element(self, element: int): Adds an element to the tracker. if len(self.min_heap) < self.N: heappush(self.min_heap, element) elif element > self.min_heap[0]: heappop(self.min_heap) heappush(self.min_heap, element) def get_top_elements(self): Returns a sorted list of the top N elements in descending order. return sorted(self.min_heap, reverse=True)"},{"question":"# Question You are tasked with creating a Python function that works in conjunction with another system, where specific tasks executed in multiple steps are expected to be interfaced with a Python script. This requires understanding of function creation, data manipulation, and error handling. Task Write a Python function `process_and_call` that takes a list of integers and a string representation of a mathematical operation (either \\"add\\" or \\"multiply\\"), performs the operation on the list of integers, and returns the result. Additionally, handle possible errors, ensuring the function returns the appropriate error messages in specific cases. 1. **Function**: `process_and_call(nums: List[int], operation: str) -> Union[int, str]` 2. **Input**: - `nums`: A list of integers to be processed. - `operation`: A string that is either \\"add\\" or \\"multiply\\". 3. **Output**: - If `operation` is \\"add\\", return the sum of the integers in `nums`. - If `operation` is \\"multiply\\", return the product of the integers in `nums`. - If `operation` is not valid, return \\"Invalid operation\\". - If `nums` is empty, return \\"Empty list\\". 4. **Constraints**: - The list `nums` may be empty. - The list `nums` will contain non-negative integers. - The `operation` string will be case-insensitive (e.g., \\"Add\\" or \\"ADD\\"). Examples 1. `process_and_call([1, 2, 3], \\"add\\")` should return `6`. 2. `process_and_call([1, 2, 3], \\"multiply\\")` should return `6`. 3. `process_and_call([], \\"add\\")` should return `\\"Empty list\\"`. 4. `process_and_call([1, 2, 3], \\"divide\\")` should return `\\"Invalid operation\\"`. Notes - Use built-in functions and standard practices for handling errors. - Ensure the function is efficiently handling data conversions and operations. # Your Solution ```python from typing import List, Union def process_and_call(nums: List[int], operation: str) -> Union[int, str]: if not nums: return \\"Empty list\\" operation = operation.lower() if operation == \\"add\\": return sum(nums) elif operation == \\"multiply\\": result = 1 for num in nums: result *= num return result else: return \\"Invalid operation\\" # Test cases for validation assert process_and_call([1, 2, 3], \\"add\\") == 6 assert process_and_call([1, 2, 3], \\"multiply\\") == 6 assert process_and_call([], \\"add\\") == \\"Empty list\\" assert process_and_call([1, 2, 3], \\"divide\\") == \\"Invalid operation\\" ```","solution":"from typing import List, Union def process_and_call(nums: List[int], operation: str) -> Union[int, str]: if not nums: return \\"Empty list\\" operation = operation.lower() if operation == \\"add\\": return sum(nums) elif operation == \\"multiply\\": result = 1 for num in nums: result *= num return result else: return \\"Invalid operation\\""},{"question":"**Objective:** You are required to write a function that uses the Seaborn `blend_palette` function to create and save a blended color palette based on different inputs. **Function Signature:** ```python def create_and_save_palette(color_list: list, as_cmap: bool = False, file_path: str = \\"palette.png\\") -> None: Creates and saves a blended color palette using seaborn\'s blend_palette function. Args: color_list (list): List of colors to interpolate. Colors can be specified in supported formats. as_cmap (bool): If True, a continuous colormap is created instead of a discrete palette. Defaults to False. file_path (str): Path to save the visual representation of the palette. Defaults to \\"palette.png\\". Returns: None ``` **Instructions:** 1. Implement the function `create_and_save_palette` which takes three parameters: - `color_list`: A list of colors specified in different formats (e.g., \\"r\\", \\"#45a872\\", \\"xkcd:golden\\"). - `as_cmap`: A boolean flag that indicates whether to create a continuous colormap (`True`) or a discrete palette (`False`). Defaults to `False`. - `file_path`: A string representing the file path where the palette image will be saved. Defaults to `\\"palette.png\\"`. 2. Use the `seaborn.blend_palette` function to create the palette based on the specified `color_list`. 3. Visualize the resulting palette using Seaborn and Matplotlib functionalities. Save the visualization to the file specified by `file_path`. 4. Make sure to handle edge cases, such as the user providing an empty `color_list`, by raising an appropriate exception. 5. Document your code and provide a few test cases to demonstrate the usage of your function. **Example:** ```python # Example usage of create_and_save_palette function try: create_and_save_palette([\\"#bdc\\", \\"#7b9\\", \\"#47a\\"], as_cmap=True, file_path=\\"continuous_palette.png\\") create_and_save_palette([\\"blue\\", \\"green\\", \\"yellow\\"], as_cmap=False, file_path=\\"discrete_palette.png\\") except ValueError as e: print(f\\"Error: {e}\\") ``` **Constraints:** - The `color_list` should contain at least two color values. - The function should handle different color formats appropriately. **Performance Requirements:** - The function should execute efficiently with a reasonable number of color values (e.g., up to 100 colors). - The image-saving process should be robust and not fail unexpectedly. **Note:** Ensure that the Seaborn and Matplotlib packages are installed in your Python environment.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_and_save_palette(color_list: list, as_cmap: bool = False, file_path: str = \\"palette.png\\") -> None: Creates and saves a blended color palette using seaborn\'s blend_palette function. Args: color_list (list): List of colors to interpolate. Colors can be specified in supported formats. as_cmap (bool): If True, a continuous colormap is created instead of a discrete palette. Defaults to False. file_path (str): Path to save the visual representation of the palette. Defaults to \\"palette.png\\". Returns: None if not color_list or len(color_list) < 2: raise ValueError(\\"color_list must contain at least two color values.\\") palette = sns.blend_palette(color_list, as_cmap=as_cmap) plt.figure(figsize=(8, 2)) if as_cmap: sns.heatmap([range(10)], cmap=palette, cbar=False, xticklabels=False, yticklabels=False) else: sns.palplot(palette) plt.savefig(file_path) plt.close()"},{"question":"# Advanced Coding Assessment Question: Novelty Detection with Local Outlier Factor **Objective:** Demonstrate your understanding of novelty detection using the `LocalOutlierFactor` class in scikit-learn by implementing a function that detects novel observations in a dataset. **Problem Description:** You are provided with two datasets: 1. `X_train`: A 2D numpy array representing the training data (inliers). 2. `X_test`: A 2D numpy array representing the new observations to be tested for novelty. Your task is to implement a function `detect_novelties(X_train, X_test)` that uses the Local Outlier Factor algorithm for novelty detection to determine which observations in `X_test` are novelties (outliers). # Function Signature ```python def detect_novelties(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: pass ``` # Input: - `X_train` (np.ndarray): A 2D numpy array of shape (n_train_samples, n_features) representing the training data. - `X_test` (np.ndarray): A 2D numpy array of shape (n_test_samples, n_features) representing the new observations to be tested. # Output: - `np.ndarray`: A 1D numpy array of shape (n_test_samples,) containing predicted labels for `X_test` where: - 1 indicates an inlier. - -1 indicates an outlier (novelty). # Constraints: - You must use the `LocalOutlierFactor` class from `sklearn.neighbors`. - The `novelty` parameter should be set to `True`. - Do not use `fit_predict` method on the test data as it is meant for outlier detection, not novelty detection. - Ensure proper usage of `predict`, `decision_function`, and `score_samples` methods only on the new unseen data (i.e., `X_test`). # Example: ```python import numpy as np from sklearn.neighbors import LocalOutlierFactor def detect_novelties(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: # Initialize the LOF model for novelty detection lof = LocalOutlierFactor(novelty=True) # Fit the model on the training data lof.fit(X_train) # Predict novelties in the test data return lof.predict(X_test) # Sample data X_train = np.array([[1.0, 2.0], [1.5, 1.8], [1.2, 1.9], [2.0, 2.1], [1.8, 1.7]]) X_test = np.array([[1.0, 2.2], [10.0, 10.0]]) # Run the function results = detect_novelties(X_train, X_test) print(results) # Expected output: [1, -1] indicating that the second point in X_test is an outlier ``` # Instructions: 1. Import the necessary modules and classes from sklearn. 2. Initialize the `LocalOutlierFactor` model with the `novelty=True` parameter. 3. Fit the model on the `X_train` data. 4. Use the fitted model to predict whether each of the observations in `X_test` is an inlier or outlier. 5. Return the predicted labels as a 1D numpy array. Note: Ensure your code is clean, well-documented, and follows best practices in Python programming.","solution":"import numpy as np from sklearn.neighbors import LocalOutlierFactor def detect_novelties(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: Detects novel observations in the test data using Local Outlier Factor. Parameters: - X_train (np.ndarray): The training data of shape (n_train_samples, n_features) - X_test (np.ndarray): The test data to be checked for novelties of shape (n_test_samples, n_features) Returns: - np.ndarray: An array of shape (n_test_samples,) with 1 for inliers and -1 for outliers. # Initialize the Local Outlier Factor model for novelty detection lof = LocalOutlierFactor(novelty=True) # Fit the model on the training data lof.fit(X_train) # Predict novelties in the test data return lof.predict(X_test)"},{"question":"**Coding Assessment Question:** You are provided a file containing data in the EA IFF 85 chunk format. Your task is to write a Python function that reads this file and prints out information about each chunk contained within it. You must use the `chunk` module and specifically the `chunk.Chunk` class as outlined in the provided documentation. # Objective Implement the function `extract_chunk_info(file_path: str) -> None` that reads the file located at `file_path` and prints the following information about each chunk: 1. The chunk ID (name). 2. The chunk size. 3. The chunk data (up to 20 bytes; print `...` if the chunk data exceeds 20 bytes). # Example Usage For a file structured as: ``` +-----------+----------+---------------------------------+ | Offset | Length | Contents | |===========|==========|=================================| | 0 | 4 | \'TEST\' | | 4 | 4 | 10 (size of chunk data) | | 8 | 10 | \'ABCDEFGHIJ\' | |-----------|----------|---------------------------------| | ... | ... | ... | +-----------+----------+---------------------------------+ ``` The function should output: ``` Chunk ID: TEST Chunk Size: 10 Chunk Data: ABCDEFGHIJ ``` # Function Signature ```python def extract_chunk_info(file_path: str) -> None: ``` # Constraints 1. Assume that the file exists and is correctly formatted. 2. If a chunk\'s data is more than 20 bytes in size, print only the first 20 bytes of data followed by `...`. 3. The function should handle multiple chunks in the file and print information sequentially until the end of the file. # Additional Information Use the methods of the `chunk.Chunk` class as outlined in the documentation to: - Read the chunk\'s name using `getname()`. - Read the chunk\'s size using `getsize()`. - Read the chunk\'s data using `read(size)`. - Handle chunk navigation using `skip()`, `seek()`, `tell()`, etc., if necessary. # Note The module `chunk` is deprecated since Python 3.11, but for the purpose of this exercise, pretend that it is still valid and in use.","solution":"import chunk def extract_chunk_info(file_path: str) -> None: with open(file_path, \'rb\') as file: while True: try: ch = chunk.Chunk(file, bigendian=False) chunk_id = ch.getname() chunk_size = ch.getsize() chunk_data = ch.read(min(chunk_size, 20)) print(f\\"Chunk ID: {chunk_id}\\") print(f\\"Chunk Size: {chunk_size}\\") if chunk_size > 20: chunk_data = chunk_data[:20] + b\'...\' print(f\\"Chunk Data: {chunk_data}\\") # Move to the next chunk ch.skip() except EOFError: break"},{"question":"Objective: Your task is to demonstrate your understanding of Python cell objects by implementing a class in Python that mimics the behavior of the described cell objects. This class should support creating a cell, getting the current value, setting a new value, and checking if an object is a cell. Problem Statement: Implement a class `Cell` in Python with the following methods: 1. `__init__(self, value=None)`: - Initializes a new cell object containing the value `value`. If no value is provided, the cell should be initialized with `None`. 2. `is_cell(obj)`: - A static method that takes an object and returns `True` if the object is an instance of `Cell` and `False` otherwise. 3. `get(self)`: - Returns the current value stored in the cell. 4. `set(self, value)`: - Updates the cell to contain the new value `value`. Input and Output Formats: - `__init__(self, value=None)`: - Input: `value` (Any data type, default is `None`) - Output: No output (initializes the cell object) - `is_cell(obj)`: - Input: `obj` (Any Python object) - Output: `True` or `False` (Boolean indicating if the object is a cell) - `get(self)`: - Input: None - Output: The value contained in the cell (Any data type) - `set(self, value)`: - Input: `value` (Any data type) - Output: No output (sets the cell to the new value) Constraints: - The class should manage the values correctly and ensure that setting a new value replaces the old one. - The `is_cell` method should correctly identify instances of the `Cell` class. Example: ```python # Example Usage: cell = Cell(10) print(cell.get()) # Output: 10 Cell.set(cell, 20) print(Cell.get(cell)) # Output: 20 print(Cell.is_cell(cell)) # Output: True print(Cell.is_cell(10)) # Output: False # Testing initialization with None cell_none = Cell() print(cell_none.get()) # Output: None ``` Ensure your `Cell` class correctly mimics the behavior expected from the documentation, including handling of the `None` values and proper type checking.","solution":"class Cell: def __init__(self, value=None): Initializes a new cell object containing the value `value`. If no value is provided, the cell should be initialized with `None`. self.value = value @staticmethod def is_cell(obj): A static method that takes an object and returns `True` if the object is an instance of `Cell` and `False` otherwise. return isinstance(obj, Cell) def get(self): Returns the current value stored in the cell. return self.value def set(self, value): Updates the cell to contain the new value `value`. self.value = value"},{"question":"# Asynchronous Queue Task Management You are tasked with creating an asynchronous task management system utilizing the `asyncio.Queue` class in Python. This system will distribute tasks to multiple worker tasks and manage them based on the task priority. Requirements: 1. Implement a task manager using a priority queue (`asyncio.PriorityQueue`). Tasks with higher priority (lower priority number) should be processed first. 2. The manager should: - Generate a series of random tasks, each with a random priority level and processing time. - Divide the workload among worker tasks. 3. A worker should: - Retrieve a task from the queue. - Simulate the task processing by sleeping for the specified duration. - Mark the task as done once processing is complete. 4. Main coordinator function should: - Initialize the queue and worker tasks. - Add tasks to the queue. - Wait until all tasks have been processed. - Calculate and print the total time elapsed and the total expected sleep time. Input: - `num_tasks` (integer): Number of tasks to be generated. - `num_workers` (integer): Number of worker tasks to process the queue. - `max_sleep_time` (float): Maximum sleep time for a task to simulate processing time. Output: - Print the total time elapsed for processing all tasks. - Print the total expected sleep time considering all generated tasks. Constraints: - Task sleep time should be a random float between `0` and `max_sleep_time`. - Task priorities should be random integers between `1` and `10`. - Proper handling of `task_done` calls for each completed task. Implementation: Implement the following functions: 1. `generate_tasks(queue: asyncio.PriorityQueue, num_tasks: int, max_sleep_time: float) -> float`: - Generates `num_tasks` tasks with random priorities and sleep times. - Adds each task as a tuple `(priority, sleep_time)` to the queue. - Returns the total expected sleep time. 2. `worker(name: str, queue: asyncio.PriorityQueue)`: - Continuously retrieves and processes tasks from the queue. - Sleep for the `sleep_time` of the task and then mark it as done. 3. `main(num_tasks: int, num_workers: int, max_sleep_time: float)`: - Initializes the priority queue. - Calls `generate_tasks` to populate the queue. - Creates worker tasks. - Waits for all tasks to be processed. - Calculates and prints the total time elapsed and the total expected sleep time. ```python import asyncio import random import time async def generate_tasks(queue: asyncio.PriorityQueue, num_tasks: int, max_sleep_time: float) -> float: total_sleep_time = 0 for _ in range(num_tasks): priority = random.randint(1, 10) sleep_time = random.uniform(0, max_sleep_time) total_sleep_time += sleep_time await queue.put((priority, sleep_time)) return total_sleep_time async def worker(name: str, queue: asyncio.PriorityQueue): while True: priority, sleep_time = await queue.get() await asyncio.sleep(sleep_time) queue.task_done() print(f\'{name} processed task with priority {priority} and slept for {sleep_time:.2f} seconds\') async def main(num_tasks: int, num_workers: int, max_sleep_time: float): queue = asyncio.PriorityQueue() total_sleep_time = await generate_tasks(queue, num_tasks, max_sleep_time) workers = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(num_workers)] start_time = time.monotonic() await queue.join() elapsed_time = time.monotonic() - start_time for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) print(\'====\') print(f\'Total time taken: {elapsed_time:.2f} seconds\') print(f\'Total expected sleep time: {total_sleep_time:.2f} seconds\') # Execute the main function num_tasks = 20 num_workers = 3 max_sleep_time = 1.0 asyncio.run(main(num_tasks, num_workers, max_sleep_time)) ``` Note: - Make sure to handle cancellations properly to avoid warning messages during task cleanup. - Feel free to add additional print statements for debugging purposes, but ensure they don\'t interfere with the required output format.","solution":"import asyncio import random import time async def generate_tasks(queue: asyncio.PriorityQueue, num_tasks: int, max_sleep_time: float) -> float: total_sleep_time = 0 for _ in range(num_tasks): priority = random.randint(1, 10) sleep_time = random.uniform(0, max_sleep_time) total_sleep_time += sleep_time await queue.put((priority, sleep_time)) return total_sleep_time async def worker(name: str, queue: asyncio.PriorityQueue): while True: priority, sleep_time = await queue.get() await asyncio.sleep(sleep_time) queue.task_done() print(f\'{name} processed task with priority {priority} and slept for {sleep_time:.2f} seconds\') async def main(num_tasks: int, num_workers: int, max_sleep_time: float): queue = asyncio.PriorityQueue() total_sleep_time = await generate_tasks(queue, num_tasks, max_sleep_time) workers = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(num_workers)] start_time = time.monotonic() await queue.join() elapsed_time = time.monotonic() - start_time for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) print(\'====\') print(f\'Total time taken: {elapsed_time:.2f} seconds\') print(f\'Total expected sleep time: {total_sleep_time:.2f} seconds\')"},{"question":"# Custom Python Initialization You are tasked with creating a customized Python initialization configuration that supports running Python scripts with specific command-line arguments and handling errors robustly. The goal is to demonstrate your understanding of the Python initialization configuration API, particularly in configuring and initializing Python using the `PyConfig` and `PyPreConfig` structures. Part 1: Create a Customized Python Configuration Write a Python function `initialize_custom_python()` that performs the following tasks: 1. Initializes a `PyConfig` structure with the Python Configuration. 2. Sets the program name to `\\"custom_python\\"`. 3. Reads all the configuration settings. 4. Appends a custom search path `\\"/path/to/custom/modules\\"` to `sys.path`. 5. Sets a command-line argument list to include `[\\"custom_python\\", \\"-c\\", \\"print(\'Hello from custom Python!\')\\"]`. 6. Initializes Python from this configuration. 7. Handles and prints any errors encountered during initialization. Part 2: Isolated Mode Initialization Extend the function `initialize_custom_python()` to also support initialization in isolated mode, where Python ignores environment variables and leaves locale settings unchanged. Input No input arguments for the function. Output The function should print a message indicating successful initialization or any errors encountered. Constraints - Use the `PyConfig` and `PyPreConfig` structures and functions as detailed in the documentation. - Ensure proper error handling using `PyStatus`. Example Usage: ```python def main(): status = initialize_custom_python(isolated=False) # Normal initialization if status: print(\\"Initialization Successful\\") status = initialize_custom_python(isolated=True) # Isolated mode initialization if status: print(\\"Isolated Initialization Successful\\") if __name__ == \\"__main__\\": main() ``` Notes - Ensure that the Python environment is properly setup to be called from a C function as illustrated in the documentation examples. - Properly manage memory and handle all potential exceptions during the initialization process.","solution":"import sys def initialize_custom_python(isolated=False): Initializes a custom Python configuration. Parameters: isolated (bool): If True, initialize Python in isolated mode. Returns: bool: True if initialization was successful, False otherwise. try: # Set the program name program_name = \\"custom_python\\" sys.executable = program_name # Set the command line arguments sys.argv = [\\"custom_python\\", \\"-c\\", \\"print(\'Hello from custom Python!\')\\"] # Append custom search path sys.path.append(\\"/path/to/custom/modules\\") # Check if isolated mode is requested if isolated: # In isolated mode, ignore environment variables and don\'t modify the locale sys._xoptions = {\\"-I\\": \\"\\"} # Simulate python initialization print(\'Python Initialized with the following configuration:\') print(f\'Program name: {sys.executable}\') print(f\'Arguments: {sys.argv}\') print(f\'Custom search paths: {sys.path}\') print(f\'Isolated mode: {isolated}\') return True except Exception as e: print(f\'Error during initialization: {e}\') return False"},{"question":"# Python Exception Handling and Custom Exception Implementation Objective To assess your understanding of Python\'s exception handling mechanisms, custom exception creation, and proper usage of exception functions. Problem Statement You are required to implement a Python function that simulates an operation involving multiple steps, where each step might fail and raise an exception. You will also implement a custom exception class and demonstrate how to handle and raise exceptions using this class. Task 1. Implement a custom exception class `CustomError` that inherits from Python\'s built-in `Exception` class. 2. Implement a function `perform_operations(steps)` that takes a list of step descriptions as input and: - Iterates through each step. - For each step, checks the step type and either raises the appropriate standard Python exception or your `CustomError`. - Catches and handles exceptions appropriately. Logs the exception type, message, and, if applicable, the traceback. - If a fatal error (e.g., `SystemExit`, `KeyboardInterrupt`) is encountered, it should re-raise the exception to terminate the process. 3. Implement a helper function `log_exception(exc)` to print or log the details of an exception, including its type, message, and traceback. Input Format - A list of step descriptions, where each step is a dictionary with at least a `type` key and additional keys depending on the step type. Output Format - Print/log details of each exception caught. - Return a dictionary summarizing the results of each step, where keys are step indices and values are booleans indicating success (True) or failure (False). Constraints - Handle at least the following exception types: `ValueError`, `TypeError`, `CustomError`. - Ensure that the function gracefully handles unknown or unexpected exception types. - Use appropriate exception handling functions provided in the documentation. Example Input ```python steps = [ {\\"type\\": \\"value_error\\", \\"description\\": \\"Raise a ValueError\\"}, {\\"type\\": \\"type_error\\", \\"description\\": \\"Raise a TypeError\\"}, {\\"type\\": \\"custom_error\\", \\"description\\": \\"Raise a CustomError with a custom message\\"}, {\\"type\\": \\"success\\", \\"description\\": \\"Successful step\\"} ] ``` Example Output ```python { 0: False, 1: False, 2: False, 3: True } ``` # Function Signature ```python class CustomError(Exception): pass def perform_operations(steps: list) -> dict: def log_exception(exc: Exception): pass pass ``` # Solution Requirements 1. **Custom Exception Class (`CustomError`)**: - Inherit from `Exception`. - Provide an appropriate docstring. 2. **Logging Function (`log_exception`)**: - Use `sys.exc_info()` to capture exception details. - Log/Print the exception type, message, and traceback. - Optionally, use logging module for proper logging. 3. **Main Function (`perform_operations`)**: - Iterate through steps. - For each step, raise the specified exception. - Catch and handle exceptions: - Use `log_exception` to log details. - Continue iteration for non-fatal exceptions. - Re-raise fatal exceptions (`SystemExit`, `KeyboardInterrupt`). - Return a summary dictionary of step execution results. This question assesses the understanding of exception handling, creating custom exceptions, and using exception-related functions as per the given documentation.","solution":"import sys import traceback class CustomError(Exception): Custom error for specific application needs. pass def log_exception(exc: Exception): Logs the details of an exception, including its type, message, and traceback. exc_type, exc_value, exc_traceback = sys.exc_info() print(f\\"Exception type: {exc_type.__name__}\\") print(f\\"Exception message: {exc_value}\\") print(\\"Traceback:\\") traceback.print_tb(exc_traceback, file=sys.stdout) def perform_operations(steps: list) -> dict: results = {} for i, step in enumerate(steps): try: if step[\\"type\\"] == \\"value_error\\": raise ValueError(step.get(\\"description\\", \\"\\")) elif step[\\"type\\"] == \\"type_error\\": raise TypeError(step.get(\\"description\\", \\"\\")) elif step[\\"type\\"] == \\"custom_error\\": raise CustomError(step.get(\\"description\\", \\"\\")) elif step[\\"type\\"] == \\"success\\": # Simulate successful operation results[i] = True continue else: print(f\\"Unknown step type: {step[\'type\']}\\") results[i] = False except (SystemExit, KeyboardInterrupt): raise except Exception as e: log_exception(e) results[i] = False return results"},{"question":"In this assessment, you are required to implement a classification pipeline using `SGDClassifier` from the `scikit-learn` library. Your task is to load a dataset, preprocess the features, train an SGD classifier, and evaluate its performance. # Dataset - Use the Iris dataset available via `sklearn.datasets.load_iris`. # Requirements 1. **Data Loading and Preprocessing**: - Load the Iris dataset. - Split the data into training and test sets using an 80-20 split. Use `random_state=42` for reproducibility. - Standardize the features using `StandardScaler`. 2. **Model Training**: - Train an `SGDClassifier` with the following parameters: - `loss=\'hinge\'` - `penalty=\'l2\'` - `max_iter=1000` - `tol=1e-3` - `random_state=42` - Use a pipeline to streamline the preprocessing and training steps. 3. **Model Evaluation**: - Predict the labels for the test set. - Calculate the accuracy of the model on the test set. # Implementation Details - Define a function `train_sgd_classifier()` which: - Takes no parameters. - Loads and splits the Iris dataset. - Standardizes the features. - Trains an `SGDClassifier` using a pipeline. - Returns the accuracy score on the test set. # Constraints - Use only the `scikit-learn` library for machine learning tasks. - Ensure the solution is clear, efficient, and follows Python coding best practices. # Example Usage ```python def train_sgd_classifier(): # Your implementation here pass accuracy = train_sgd_classifier() print(f\\"Test set accuracy: {accuracy:.4f}\\") ``` # Expected Output Upon running your implementation, the output should be the accuracy of the trained `SGDClassifier` on the test set, printed with four decimal places.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def train_sgd_classifier(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline with a StandardScaler and SGDClassifier pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42)) # Train the classifier pipeline.fit(X_train, y_train) # Predict the labels for the test set y_pred = pipeline.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Profiling and Analyzing Python Code **Objective:** Write a Python function that profiles another function and outputs a sorted analysis report containing the function call statistics. **Task:** 1. **Profile the Function:** Use the `cProfile` module to profile a given function. 2. **Save Profile Data:** Save the profiling results to a temporary in-memory file using `io.StringIO`. 3. **Analyze the Profile Data:** Use the `pstats` module to load and analyze the profiling data. 4. **Sort and Filter:** Sort the results by cumulative time and filter to display only the top N functions. 5. **Report Generation:** Generate and return a formatted string with the profile analysis. **Function Signature:** ```python import cProfile import pstats import io def analyze_function_performance(func, *args, **kwargs): Profiles the given function and returns a formatted string of the analysis report. Parameters: func (callable): The function to profile. *args: Positional arguments to pass to the function. **kwargs: Keyword arguments to pass to the function. Returns: str: The formatted profile analysis report. pass ``` **Requirements:** - The function should use `cProfile` to create a profile. - Profile results should be stored using `io.StringIO`. - Use `pstats` to read, sort, and filter the profile data. - The results should be sorted by cumulative time and include only the top 10 functions. - Return the profile statistics as a formatted string. **Example Usage:** ```python def example_function(n): total = 0 for i in range(n): total += i * i return total profile_report = analyze_function_performance(example_function, 1000) print(profile_report) ``` **Constraints:** - The function `func` will not perform I/O operations (e.g., file reading/writing, network communication). - The function `func` will complete in a reasonable amount of time (within a few seconds). Your task is to implement the `analyze_function_performance` function following the above requirements. Ensure the output is user-friendly and easy to understand. **Note:** - You might find these methods from the `pstats.Stats` class useful: `strip_dirs()`, `sort_stats()`, and `print_stats()`. - Use `SortKey.CUMULATIVE` to sort by cumulative time.","solution":"import cProfile import pstats import io def analyze_function_performance(func, *args, **kwargs): Profiles the given function and returns a formatted string of the analysis report. Parameters: func (callable): The function to profile. *args: Positional arguments to pass to the function. **kwargs: Keyword arguments to pass to the function. Returns: str: The formatted profile analysis report. # Create a profile object profile = cProfile.Profile() # Start profiling profile.enable() # Execute the function with provided arguments func(*args, **kwargs) # Stop profiling profile.disable() # Create a string stream to hold the profile results stream = io.StringIO() # Create Stats object and configure for output stats = pstats.Stats(profile, stream=stream).strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE) # Print the top 10 functions stats.print_stats(10) # Retrieve the profile report from the string stream report = stream.getvalue() return report"},{"question":"# Advanced Python Coding Assessment Question Objective To assess the understanding of WSGI fundamentals and the usage of `wsgiref` utilities in Python web applications. Problem Statement You are required to create a WSGI application that serves a dynamic text page. This application should meet the following requirements: 1. **Dynamic Response**: Your response should customize the content based on the presence of a query parameter `name` in the request URL. The response should greet the user by name if provided, or greet \\"World\\" otherwise. 2. **Headers Manipulation**: Use the `Headers` class from `wsgiref.headers` to set the necessary response headers correctly. 3. **Testing Environment Setup**: Use the `setup_testing_defaults` function from `wsgiref.util` to setup the WSGI environment for testing. 4. **Serving the Application**: Use the `wsgiref.simple_server` to serve your WSGI application on port 8000. Requirements - The expected input is a WSGI `environ` dictionary and a callable `start_response`. - The output should be a list of bytes, representing the body of your HTTP response. - You must implement the necessary function `my_wsgi_app` and setup a simple server for this application. Constraints - You should not use any third-party libraries except the Python standard library. - Ensure your code is well-structured and handles common edge cases, like missing query parameters. Detailed Steps 1. Implement a function `my_wsgi_app(environ, start_response)` that dynamically constructs a response based on the query parameter `name`. 2. Create an HTTP response header using the `Headers` class. 3. Setup the WSGI environment for testing using `setup_testing_defaults`. 4. Use `make_server` to setup and run a WSGI server serving your application. Example Usage ```python from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults from wsgiref.headers import Headers import urllib.parse def my_wsgi_app(environ, start_response): setup_testing_defaults(environ) headers = Headers([(\'Content-type\', \'text/plain; charset=utf-8\')]) status = \'200 OK\' start_response(status, headers.items()) query_string = environ.get(\'QUERY_STRING\', \'\') params = urllib.parse.parse_qs(query_string) name = params.get(\'name\', [\'World\'])[0] response_body = f\'Hello, {name}!\' return [response_body.encode(\'utf-8\')] if __name__ == \'__main__\': with make_server(\'\', 8000, my_wsgi_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Submission Please submit a Python file containing the following: - The necessary function `my_wsgi_app`. - Your server setup to serve the application.","solution":"from wsgiref.util import setup_testing_defaults from wsgiref.headers import Headers import urllib.parse def my_wsgi_app(environ, start_response): # Setup the testing environment setup_testing_defaults(environ) # Create the headers object and set content-type headers = Headers([(\'Content-type\', \'text/plain; charset=utf-8\')]) # Define the HTTP response status status = \'200 OK\' start_response(status, headers.items()) # Parse query string query_string = environ.get(\'QUERY_STRING\', \'\') params = urllib.parse.parse_qs(query_string) # Retrieve name parameter if exists, otherwise default to \'World\' name = params.get(\'name\', [\'World\'])[0] # Construct response body response_body = f\'Hello, {name}!\' # Return response body as list of bytes return [response_body.encode(\'utf-8\')]"},{"question":"Objective The goal of this question is to assess your understanding of PyTorch\'s `torch.nn.functional` module by implementing a custom neural network layer using provided functions. You will need to combine several functional components from the module to achieve the desired operations. Task You need to implement a class `CustomConvLayer` that includes the following layers: 1. A 2D convolutional layer. 2. A ReLU activation layer. 3. A 2D max pooling layer. 4. A dropout layer. Functionality Implement the `CustomConvLayer` with the following specifications: 1. **Initialization**: - The `__init__` method should accept parameters for the convolutional layer and dropout rate. ```python def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dropout_rate=0.5): Parameters: - in_channels (int): Number of channels in the input. - out_channels (int): Number of channels produced by the convolution. - kernel_size (int or tuple): Size of the convolving kernel. - stride (int or tuple): Stride of the convolution. Default is 1. - padding (int or tuple): Zero-padding added to both sides of the input. Default is 0. - dropout_rate (float): Probability of an element to be zeroed. Default is 0.5. ``` 2. **Forward Pass**: - The `forward` method should accept a tensor `x` and perform the sequential operations: convolution, ReLU activation, max pooling, and dropout. ```python def forward(self, x): Parameters: - x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width). Returns: - torch.Tensor: Output tensor after applying convolution, ReLU, max pooling, and dropout. ``` Constraints - Use `torch.nn.functional` functions (e.g., `conv2d`, `relu`, `max_pool2d`, `dropout`) to implement the `CustomConvLayer`. - Assume the input to the `forward` method is always a 4D tensor with shape (batch_size, in_channels, height, width). Example ```python import torch import torch.nn.functional as F class CustomConvLayer: def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dropout_rate=0.5): self.in_channels = in_channels self.out_channels = out_channels self.kernel_size = kernel_size self.stride = stride self.padding = padding self.dropout_rate = dropout_rate self.weight = torch.randn(out_channels, in_channels, kernel_size, kernel_size, requires_grad=True) self.bias = torch.randn(out_channels, requires_grad=True) def forward(self, x): x = F.conv2d(x, self.weight, self.bias, stride=self.stride, padding=self.padding) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2) x = F.dropout(x, p=self.dropout_rate) return x # Example usage layer = CustomConvLayer(3, 16, 3) input_tensor = torch.randn(1, 3, 32, 32) output_tensor = layer.forward(input_tensor) print(output_tensor.shape) # Should print the shape of the output tensor ```","solution":"import torch import torch.nn.functional as F class CustomConvLayer: def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dropout_rate=0.5): self.in_channels = in_channels self.out_channels = out_channels self.kernel_size = kernel_size self.stride = stride self.padding = padding self.dropout_rate = dropout_rate # Define the weights and bias for the convolutional layer self.weight = torch.randn(out_channels, in_channels, kernel_size, kernel_size, requires_grad=True) self.bias = torch.randn(out_channels, requires_grad=True) def forward(self, x): # Apply 2D convolution x = F.conv2d(x, self.weight, self.bias, stride=self.stride, padding=self.padding) # Apply ReLU activation x = F.relu(x) # Apply 2D Max Pooling with kernel size 2 and stride 2 x = F.max_pool2d(x, kernel_size=2) # Apply dropout x = F.dropout(x, p=self.dropout_rate) return x"},{"question":"# Secure Token and Password Generation Description You are tasked with creating a secure system for generating random passwords and security tokens for a web application. Your implementation must ensure strong security guarantees by using Python\'s `secrets` module. You need to implement the following functions: 1. **generate_password(length: int, criteria: str) -> str** - This function generates a password of a given length. - The `criteria` parameter specifies the required characteristics of the password and can have the following values: - `\\"alphanumeric\\"`: The password should contain uppercase letters, lowercase letters, and digits. - `\\"complex\\"`: The password should contain at least one uppercase letter, one lowercase letter, one digit, and one special character (!@#%^&*). - The password must be generated using `secrets.choice` to ensure cryptographic security. 2. **generate_security_token(token_type: str, nbytes: int = None) -> str** - This function generates a secure token string. - The `token_type` parameter specifies the type of token and can have the following values: - `\\"hex\\"`: The token should be a hexadecimal string. - `\\"urlsafe\\"`: The token should be a URL-safe string. - The `nbytes` parameter specifies the number of random bytes to use for the token. If not provided, a default value will be used. Requirements - You must use the `secrets` module for all random generation. - Do not use any other random number generation libraries (e.g., `random`). Examples ```python # Example 1: Generate an alphanumeric password of length 12 password1 = generate_password(12, \\"alphanumeric\\") print(password1) # Example Output: \'a8B3jK5lQ2tG\' # Example 2: Generate a complex password of length 16 password2 = generate_password(16, \\"complex\\") print(password2) # Example Output: \'a8B#3jK^5l!Q2G*\' # Example 3: Generate a hexadecimal token of 16 bytes token1 = generate_security_token(\\"hex\\", 16) print(token1) # Example Output: \'f9bf78b9a18ce6d46a0cd2b0b86df9da\' # Example 4: Generate a URL-safe token of the default number of bytes token2 = generate_security_token(\\"urlsafe\\") print(token2) # Example Output: \'Drmhze6EPcv0fN_81Bj-nA\' ``` Constraints - The password length `length` for `generate_password` must be at least 8 and at most 128. - The number of bytes `nbytes` for `generate_security_token` must be between 1 and 64, inclusive. Starter Code Template ```python import secrets import string def generate_password(length: int, criteria: str) -> str: # Your implementation here pass def generate_security_token(token_type: str, nbytes: int = None) -> str: # Your implementation here pass ```","solution":"import secrets import string def generate_password(length: int, criteria: str) -> str: if not (8 <= length <= 128): raise ValueError(\\"Password length must be between 8 and 128.\\") if criteria not in [\\"alphanumeric\\", \\"complex\\"]: raise ValueError(\\"Invalid criteria. Must be \'alphanumeric\' or \'complex\'.\\") if criteria == \\"alphanumeric\\": characters = string.ascii_letters + string.digits elif criteria == \\"complex\\": characters = string.ascii_letters + string.digits + string.punctuation while True: password = \'\'.join(secrets.choice(characters) for _ in range(length)) if criteria == \\"complex\\": if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password) and any(c in string.punctuation for c in password)): break else: break return password def generate_security_token(token_type: str, nbytes: int = None) -> str: if nbytes is not None and not (1 <= nbytes <= 64): raise ValueError(\\"Number of bytes must be between 1 and 64.\\") if token_type == \\"hex\\": return secrets.token_hex(nbytes if nbytes else 32) elif token_type == \\"urlsafe\\": return secrets.token_urlsafe(nbytes if nbytes else 32) else: raise ValueError(\\"Invalid token type. Must be \'hex\' or \'urlsafe\'.\\")"},{"question":"**Objective:** To assess your understanding of the `seaborn` library, specifically focusing on the `sns.pointplot` function, you are required to write a function that creates a series of customized point plots from a given dataset. **Problem Statement:** Implement a function `create_custom_pointplots` that takes a dataset and creates a series of seaborn point plots demonstrating various customizations. The function should generate the following plots: 1. **Basic Point Plot**: - Group data by a categorical variable and plot the aggregated values with confidence intervals. - Display this plot for the `flights` dataset, showing the average number of passengers each month. 2. **Grouped Point Plot with Hue**: - Include another categorical variable to differentiate within the groups using color (`hue` parameter). - Display this plot for the `penguins` dataset, showing `body_mass_g` for each `island`, grouped by `sex`. 3. **Styled Point Plot**: - Use different markers and linestyles for the levels of hue. - Display this for the `penguins` dataset, showing `body_mass_g` for each `island`, grouped by `sex`, with markers and linestyles. 4. **Customized Error Bars**: - Use error bars to represent the standard deviation of each distribution. - Display this for the `penguins` dataset, showing `body_mass_g` for each `island`. 5. **Advanced Customization**: - Customize the appearance of the plot, including changing the markers, line styles, and managing dodge patterns to reduce overplot. - Demonstrate this for the `penguins` dataset, showing `bill_depth_mm` for each `species`, grouped by `sex`. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_pointplots(penguins, flights): # Your code here pass ``` # Requirements: - The function must include code to correctly load the datasets if not provided. - Each plot must be unique and incorporate specific seaborn functionalities presented in the documentation. - Use titles and labels to clearly differentiate each plot type. - Ensure plots are neatly showcased using `matplotlib.pyplot`. **Datasets to be used:** - Load the `penguins` dataset using `sns.load_dataset(\\"penguins\\")`. - Load the `flights` dataset using `sns.load_dataset(\\"flights\\")`. # Expected Output: When `create_custom_pointplots` is called, it should generate the following plots: 1. A point plot showing the average number of passengers each month from the `flights` dataset. 2. A point plot of `body_mass_g` for each `island`, grouped by `sex`, from the `penguins` dataset. 3. A customized point plot of `body_mass_g` for each `island`, grouped by `sex`, with different markers and linestyles from the `penguins` dataset. 4. A point plot representing the standard deviation error bars for `body_mass_g` on each `island` from the `penguins` dataset. 5. A highly customized point plot of `bill_depth_mm` for each `species`, grouped by `sex`, with advanced styling from the `penguins` dataset. Ensure the function is well-documented with comments guiding through each customization applied.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_custom_pointplots(penguins=None, flights=None): if penguins is None: penguins = sns.load_dataset(\\"penguins\\") if flights is None: flights = sns.load_dataset(\\"flights\\") # Basic Point Plot plt.figure(figsize=(10, 6)) sns.pointplot(x=\'month\', y=\'passengers\', data=flights) plt.title(\'Average Number of Passengers Each Month\') plt.show() # Grouped Point Plot with Hue plt.figure(figsize=(10, 6)) sns.pointplot(x=\'island\', y=\'body_mass_g\', hue=\'sex\', data=penguins) plt.title(\'Body Mass of Penguins on Each Island Grouped by Sex\') plt.show() # Styled Point Plot plt.figure(figsize=(10, 6)) sns.pointplot(x=\'island\', y=\'body_mass_g\', hue=\'sex\', data=penguins, markers=[\'o\', \'X\'], linestyles=[\'-\', \'--\']) plt.title(\'Styled Body Mass of Penguins on Each Island Grouped by Sex\') plt.show() # Customized Error Bars using Standard Deviation plt.figure(figsize=(10, 6)) sns.pointplot(x=\'island\', y=\'body_mass_g\', data=penguins, ci=\'sd\') plt.title(\'Body Mass of Penguins on Each Island with Standard Deviation Error Bars\') plt.show() # Advanced Customization plt.figure(figsize=(10, 6)) sns.pointplot(x=\'species\', y=\'bill_depth_mm\', hue=\'sex\', palette=\'muted\', markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], dodge=True, data=penguins) plt.title(\'Advanced Customized Plot of Bill Depth of Penguins by Species and Sex\') plt.show()"},{"question":"# PyTorch Multiprocessing Distributed Computations You are tasked with implementing a function that leverages PyTorch\'s `torch.distributed.elastic.multiprocessing` capabilities to perform distributed data processing. Specifically, you will parallelize a simple task of computing the squared values of an array across multiple processes. Function Signature: ```python def distributed_square_computation(data: List[int], num_processes: int) -> List[int]: ``` Input: * `data`: A list of integers, representing the input data to be squared. * `num_processes`: An integer, representing the number of parallel processes to be used. Output: * A list of integers, containing the squared values of the input data. The order of processing does not need to be preserved in the output. Constraints: * You must use PyTorch\'s multiprocessing tools provided in the `torch.distributed.elastic.multiprocessing` module. * Handle parallel processing efficiently to ensure all available processes are utilized. * Validate inputs to ensure the number of processes is positive and does not exceed the length of the data. Requirements: 1. Create a worker function that calculates the square of each integer. 2. Use the `start_processes` function to initiate the worker processes. 3. Gather and return the squared values from all processes. Example: ```python data = [1, 2, 3, 4, 5] num_processes = 2 output = distributed_square_computation(data, num_processes) print(output) # Possible output: [1, 4, 9, 16, 25] or any order permutation ``` Hint: Refer to the PyTorch documentation for `torch.distributed.elastic.multiprocessing.start_processes` and related context classes to help implement the distributed computation. Implement the `distributed_square_computation` function in Python using PyTorch.","solution":"import torch.multiprocessing as mp from typing import List def worker(data_chunk, result_queue): Worker function that computes the square of each element in the data_chunk and puts the result into the result_queue. # Compute squares squared_values = [x**2 for x in data_chunk] # Put results into the queue result_queue.put(squared_values) def distributed_square_computation(data: List[int], num_processes: int) -> List[int]: Function to compute the square of each element in the data using multiprocessing. if num_processes <= 0: raise ValueError(\\"Number of processes must be greater than 0\\") if num_processes > len(data): raise ValueError(\\"Number of processes cannot be greater than the length of the data\\") # Split data into chunks for each process chunk_size = len(data) // num_processes data_chunks = [data[i * chunk_size : (i + 1) * chunk_size] for i in range(num_processes)] if len(data) % num_processes != 0: data_chunks[-1].extend(data[num_processes * chunk_size:]) # Create a queue to collect results result_queue = mp.Queue() # Launch the worker processes processes = [mp.Process(target=worker, args=(chunk, result_queue)) for chunk in data_chunks] for p in processes: p.start() for p in processes: p.join() # Collect the results from the queue results = [] while not result_queue.empty(): results.extend(result_queue.get()) return results"},{"question":"Seaborn Coding Assessment Question # Objective The goal of this question is to assess your understanding of the `seaborn` library, specifically the `displot` function, for creating and customizing statistical plots. # Problem Statement Using the Seaborn `displot` function, you are required to perform the following tasks: 1. **Load the `tips` dataset** from Seaborn\'s built-in datasets. 2. **Create a histogram** of the `total_bill` column. 3. **Create a KDE plot** of the `total_bill` column with a `rug` plot added. 4. **Create a bivariate KDE plot** using the `total_bill` and `tip` columns. 5. **Create an ECDF plot** of the `total_bill` column, categorizing it by `sex`. 6. **Create faceted subplots** of KDE plots for the `total_bill` column based on `time` and `day`. Each facet should show KDE plots categorized by `sex`. 7. **Customize the plot** further by setting the axis labels, titles and adjusting figure size and aspect ratio appropriately. # Expected Code Structure ```python import seaborn as sns # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Create a histogram of the total_bill column sns.displot(data=tips, x=\\"total_bill\\") # Task 2: Create a KDE plot of the total_bill column with rug plot sns.displot(data=tips, x=\\"total_bill\\", kind=\\"kde\\", rug=True) # Task 3: Create a bivariate KDE plot for total_bill and tip sns.displot(data=tips, x=\\"total_bill\\", y=\\"tip\\", kind=\\"kde\\") # Task 4: Create an ECDF plot of the total_bill column categorized by sex sns.displot(data=tips, x=\\"total_bill\\", hue=\\"sex\\", kind=\\"ecdf\\") # Task 5: Create faceted subplots of KDE plots for total_bill column based on time and day, categorized by sex g = sns.displot(data=tips, x=\\"total_bill\\", hue=\\"sex\\", col=\\"time\\", row=\\"day\\", kind=\\"kde\\") g.set_axis_labels(\\"Total Bill ()\\", \\"Density\\") g.set_titles(\\"{row_name}, {col_name}\\") # Task 6: Customize further by setting height and aspect ratio g = sns.displot(data=tips, x=\\"total_bill\\", hue=\\"sex\\", col=\\"time\\", row=\\"day\\", kind=\\"kde\\", height=4, aspect=0.7) g.set_axis_labels(\\"Total Bill ()\\", \\"Density\\") g.set_titles(\\"{row_name}, {col_name}\\") ``` # Submission Guidelines - Your solution should be well-documented and readable. - Ensure that you handle the necessary imports and data loading within your function. - Include comments to explain your steps and thought process. - The visualizations generated should be meaningful and provide insight into the dataset. # Constraints - Follow the code structure outlined above. - Ensure that all plots are correctly rendered and appropriately customized. - Make sure to handle any edge cases or possible errors gracefully. By completing this task, you will demonstrate your ability to utilize Seaborn for advanced data visualization, an essential skill for any data scientist.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_plots(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Create a histogram of the total_bill column sns.displot(data=tips, x=\\"total_bill\\") plt.title(\'Histogram of Total Bill\') plt.xlabel(\'Total Bill ()\') plt.ylabel(\'Frequency\') plt.show() # Task 2: Create a KDE plot of the total_bill column with rug plot sns.displot(data=tips, x=\\"total_bill\\", kind=\\"kde\\", rug=True) plt.title(\'KDE Plot of Total Bill with Rug\') plt.xlabel(\'Total Bill ()\') plt.ylabel(\'Density\') plt.show() # Task 3: Create a bivariate KDE plot for total_bill and tip sns.displot(data=tips, x=\\"total_bill\\", y=\\"tip\\", kind=\\"kde\\") plt.title(\'Bivariate KDE Plot of Total Bill and Tip\') plt.xlabel(\'Total Bill ()\') plt.ylabel(\'Tip ()\') plt.show() # Task 4: Create an ECDF plot of the total_bill column categorized by sex sns.displot(data=tips, x=\\"total_bill\\", hue=\\"sex\\", kind=\\"ecdf\\") plt.title(\'ECDF Plot of Total Bill by Sex\') plt.xlabel(\'Total Bill ()\') plt.ylabel(\'ECDF\') plt.legend(title=\'Sex\') plt.show() # Task 5: Create faceted subplots of KDE plots for total_bill column based on time and day, categorized by sex g = sns.displot(data=tips, x=\\"total_bill\\", hue=\\"sex\\", col=\\"time\\", row=\\"day\\", kind=\\"kde\\") g.set_axis_labels(\\"Total Bill ()\\", \\"Density\\") g.set_titles(\\"{row_name}, {col_name} Time\\") plt.show() # Task 6: Customize further by setting height and aspect ratio g = sns.displot(data=tips, x=\\"total_bill\\", hue=\\"sex\\", col=\\"time\\", row=\\"day\\", kind=\\"kde\\", height=4, aspect=0.7) g.set_axis_labels(\\"Total Bill ()\\", \\"Density\\") g.set_titles(\\"{row_name}, {col_name} Time\\") plt.show()"},{"question":"# Assessment Question ***Objective:*** You are tasked with analyzing the performance of various models on different tasks in the GLUE benchmark dataset. You will use Seaborn objects to create visualizations, add text annotations, and customize their appearance. ***Instructions:*** 1. **Data Preparation:** - Load the GLUE dataset using `seaborn.load_dataset(\\"glue\\")`. - Convert the dataset to a wide-format by pivoting on \'Model\', \'Encoder\' as indices, \'Task\' as columns, and \'Score\' as values. - Compute the average score across tasks for each model and add it as a new column named \'Average\'. - Sort the dataframe in descending order based on the \'Average\' column. 2. **Create Visualizations:** - Create a scatter plot visualizing the scores of models on \'SST-2\' and \'MRPC\' tasks. Annotate each point with the model name. - Create a bar plot showing the average score of each model. Annotate the bars with the average score, align the text to the right, and adjust its offset. 3. **Customize Text Annotations:** - Create a scatter plot visualizing the scores of models on \'SST-2\' and \'MRPC\' tasks. Color the points based on the \'Encoder\' type and add the model names above the points. - Create a scatter plot visualizing the scores of models on \'RTE\' and \'MRPC\' tasks. Color the points based on the \'Encoder\' type and align the model names based on the encoder type (left for LSTM, right for Transformer). Bold the text annotations. ***Expected Code Implementation:*** ```python import seaborn.objects as so from seaborn import load_dataset # Data Preparation glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Scatter plot with text annotations ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", text=\\"Model\\") .add(so.Text()) ) # Bar plot with average scores and text annotations ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Scatter plot with text annotations and color coding by \'Encoder\' ( so.Pplot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) # Scatter plot with text annotations and alignment customizations ( so.Pplot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text({\\"fontweight\\": \\"bold\\"}), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ) ``` Your code should correctly implement data manipulation and visualization using the instructions and constraints provided. Ensure your plots are clean and informative with appropriate text annotations.","solution":"import pandas as pd import seaborn as sns import seaborn.objects as so from seaborn import load_dataset # Data Preparation glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Scatter plot with text annotations scatter1 = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", text=\\"Model\\") .add(so.Text()) ) # Bar plot with average scores and text annotations bar_plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Scatter plot with text annotations and color coding by \'Encoder\' scatter2 = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) # Scatter plot with text annotations and alignment customizations scatter3 = ( so.Plot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text({\\"fontweight\\": \\"bold\\"}), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) )"},{"question":"**Objective**: Write a Python script to demonstrate your understanding of the `contextvars` module and its usage in an asynchronous environment. **Instructions**: 1. Create a context variable called `request_id` to store request identifiers for an asynchronous web server. 2. Implement a function `handle_request` that simulates handling client requests, sets the `request_id` for each request, and prints the ID during request handling. 3. Use the `Context` class to ensure that each request has its unique context and does not interfere with others. 4. Simulate multiple client requests using asyncio, demonstrating how context is maintained separately for each client. **Requirements**: 1. Your script should define the following: - A `ContextVar` named `request_id`. - An asynchronous function `handle_request`, which sets a unique `request_id` and prints it. - An async function `simulate_requests` to simulate handling multiple client requests concurrently. 2. Ensure that the `request_id` is correctly isolated per request using context management. 3. Use the `copy_context` method to demonstrate copying and running a context. **Constraints**: - Each simulated request should have a unique `request_id`. - `handle_request` should print the `request_id` before and after processing the request to show the isolation of contexts. **Performance Requirements**: - The script should handle at least 5 concurrent requests efficiently. # Example Output ``` Handling request with ID: 1 Handling request with ID: 2 Handling request with ID: 3 Handling request with ID: 4 Handling request with ID: 5 Finished handling request with ID: 1 Finished handling request with ID: 2 Finished handling request with ID: 3 Finished handling request with ID: 4 Finished handling request with ID: 5 ``` **Starter Code**: ```python import asyncio import contextvars # Define the context variable request_id = contextvars.ContextVar(\\"request_id\\") async def handle_request(id): # Set the request_id for the context token = request_id.set(id) print(f\\"Handling request with ID: {request_id.get()}\\") # Simulate request handling await asyncio.sleep(1) print(f\\"Finished handling request with ID: {request_id.get()}\\") request_id.reset(token) async def simulate_requests(): ctx = contextvars.copy_context() # Simulate 5 concurrent requests await asyncio.gather( ctx.run(handle_request, 1), ctx.run(handle_request, 2), ctx.run(handle_request, 3), ctx.run(handle_request, 4), ctx.run(handle_request, 5), ) if __name__ == \\"__main__\\": asyncio.run(simulate_requests()) ``` **Note**: You may use any unique identifier for `request_id`, such as an integer or a UUID.","solution":"import asyncio import contextvars # Define the context variable request_id = contextvars.ContextVar(\\"request_id\\") async def handle_request(id): Simulates handling a client request by setting the request_id and printing the ID before and after processing. # Set the request_id for the context token = request_id.set(id) print(f\\"Handling request with ID: {request_id.get()}\\") # Simulate request handling await asyncio.sleep(1) print(f\\"Finished handling request with ID: {request_id.get()}\\") request_id.reset(token) async def simulate_requests(): Simulates multiple client requests by creating and running several tasks concurrently. ctx = contextvars.copy_context() # Simulate 5 concurrent requests await asyncio.gather( ctx.run(handle_request, 1), ctx.run(handle_request, 2), ctx.run(handle_request, 3), ctx.run(handle_request, 4), ctx.run(handle_request, 5), ) if __name__ == \\"__main__\\": asyncio.run(simulate_requests())"},{"question":"# Pandas Coding Assessment Question # Objective: To assess the student\'s ability to manipulate DataFrames using pandas by integrating multiple pandas operations including data aggregation, transformation, missing data handling, and function application. # Problem Statement: You work for an e-commerce company, and you are given a sales dataset that tracks sales over several years. The dataset contains the following columns: - `Date`: The date when the sale was made (Format: YYYY-MM-DD). - `Category`: The category of the product sold. - `Sub-Category`: The sub-category of the product sold. - `Sales`: The sales amount for that transaction. - `Profit`: The profit made from that transaction. # Task: Write a function `analyze_sales_data` that takes a pandas DataFrame as input and performs the following tasks: 1. **Data Preprocessing:** - Parse the `Date` column to datetime objects. - Extract the month and year from the `Date` column and create new columns `Month` and `Year` respectively. 2. **Handle Missing Values:** - If there are any missing values in the `Sales` or `Profit` columns, fill these missing values with the mean value of the respective column. 3. **Category and Sub-Category Analysis:** - Group the data by `Category` and `Sub-Category` and then aggregate the data to find: - Total Sales - Total Profit - Average Monthly Sales 4. **Sales Trend Analysis:** - Create a new DataFrame that shows the total monthly sales for each category and sub-category. - For each category and sub-category, calculate the month-over-month percentage change in sales. 5. **Output:** - A DataFrame that contains the aggregated data (total sales, total profit, and average monthly sales) grouped by `Category` and `Sub-Category`. - A DataFrame showing the month-over-month percentage change in sales for each category and sub-category. # Constraints: - You can assume that the input DataFrame will always contain the columns: `Date`, `Category`, `Sub-Category`, `Sales`, and `Profit`. - Do not use any external libraries other than pandas and numpy. # Example Usage: ```python import pandas as pd data = { \\"Date\\": [\\"2022-01-01\\", \\"2022-01-05\\", \\"2022-02-01\\", \\"2022-02-15\\", \\"2022-03-01\\"], \\"Category\\": [\\"Electronics\\", \\"Electronics\\", \\"Furniture\\", \\"Furniture\\", \\"Electronics\\"], \\"Sub-Category\\": [\\"Mobile\\", \\"Laptop\\", \\"Chair\\", \\"Table\\", \\"Mobile\\"], \\"Sales\\": [1000, 1500, 800, 1200, 1300], \\"Profit\\": [200, 300, 100, 150, 250] } df = pd.DataFrame(data) result_aggregated, result_trend = analyze_sales_data(df) print(result_aggregated) print(result_trend) ``` # Expected Output: Two DataFrames that summarize the sales data and the sales trend analysis will be printed.","solution":"import pandas as pd import numpy as np def analyze_sales_data(df): # Data Preprocessing df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.month df[\'Year\'] = df[\'Date\'].dt.year # Handle Missing Values df[\'Sales\'].fillna(df[\'Sales\'].mean(), inplace=True) df[\'Profit\'].fillna(df[\'Profit\'].mean(), inplace=True) # Category and Sub-Category Analysis aggregated_data = df.groupby([\'Category\', \'Sub-Category\']).agg( Total_Sales=(\'Sales\', \'sum\'), Total_Profit=(\'Profit\', \'sum\'), Average_Monthly_Sales=(\'Sales\', \'mean\') ).reset_index() # Sales Trend Analysis monthly_sales = df.groupby([\'Year\', \'Month\', \'Category\', \'Sub-Category\'])[\'Sales\'].sum().reset_index() monthly_sales[\'Previous_Sales\'] = monthly_sales.groupby([\'Category\', \'Sub-Category\'])[\'Sales\'].shift(1) monthly_sales[\'Mon_Month_Percentage_Change\'] = (monthly_sales[\'Sales\'] - monthly_sales[\'Previous_Sales\']) / monthly_sales[\'Previous_Sales\'] * 100 monthly_sales.drop(columns=[\'Previous_Sales\'], inplace=True) return aggregated_data, monthly_sales"},{"question":"Advanced Seaborn Visualization Objective: To demonstrate your proficiency with Python\'s seaborn library for data visualization by creating a complex, customized plot using given data and requirements. Problem Statement: You are given the Titanic dataset from seaborn\'s built-in datasets. Your task is to create a multi-faceted visualization that will help analyze the survival rates of different passenger classes across different age groups and sex. Requirements: 1. **Load the Titanic Dataset**: Use seaborn to load the dataset. 2. **Set Theme**: Set the theme to `whitegrid`. 3. **Subplots**: Create a faceted grid of bar plots where: - Columns represent different passenger classes (`col=\\"class\\"`). - Each subplot shows the survival rate (`y=\\"survived\\"`) for different age groups. - Age groups should be categorized into \'Child\' (0-17), \'Adult\' (18-64), and \'Senior\' (65 and above). 4. **Color Encoding**: Use the hue parameter to differentiate between male and female passengers. 5. **Customizations**: - Set the height of each subplot to `4` and aspect ratio to `1`. - Label the y-axis as \\"Survival Rate\\". - Label the x-axis as \\"Age Group\\". - Give the subplot appropriate titles for each passenger class. - Set the y-axis limits between 0 and 1. Input and Output Format: - **Input**: No direct input is needed; you will use seaborn\'s built-in dataset. - **Output**: A composite plot meeting the provided requirements. Constraints: - Ensure your code is efficient and runs within a reasonable time frame. Example: ```python import seaborn as sns import pandas as pd # Load dataset df = sns.load_dataset(\\"titanic\\") # Set theme sns.set_theme(style=\\"whitegrid\\") # Create age group column df[\'age_group\'] = pd.cut(df[\'age\'], bins=[0, 18, 65, df[\'age\'].max()], labels=[\'Child\', \'Adult\', \'Senior\']) # Create catplot with subplots and customizations g = sns.catplot( data=df, x=\\"age_group\\", y=\\"survived\\", col=\\"class\\", hue=\\"sex\\", kind=\\"bar\\", height=4, aspect=1, ) # Apply customizations g.set_axis_labels(\\"Age Group\\", \\"Survival Rate\\") g.set_titles(\\"{col_name} Passenger Class\\") g.set(ylim=(0, 1)) g.despine(left=True) # Display plot g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\'Survival Rates by Age Group, Class, and Sex\', fontsize=16) ``` This example demonstrates the structure you need to follow. Customize your plot as specified and make sure the final visualization is clear and informative.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_titanic_survival_plot(): # Load dataset df = sns.load_dataset(\\"titanic\\") # Add age group column df[\'age_group\'] = pd.cut(df[\'age\'], bins=[0, 18, 65, df[\'age\'].max()], labels=[\'Child\', \'Adult\', \'Senior\']) # Set theme sns.set_theme(style=\\"whitegrid\\") # Create catplot with subplots and customizations g = sns.catplot( data=df, x=\\"age_group\\", y=\\"survived\\", col=\\"class\\", hue=\\"sex\\", kind=\\"bar\\", height=4, aspect=1, ) # Apply customizations g.set_axis_labels(\\"Age Group\\", \\"Survival Rate\\") g.set_titles(\\"{col_name} Passenger Class\\") g.set(ylim=(0, 1)) g.despine(left=True) # Add a main title g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\'Survival Rates by Age Group, Class, and Sex\', fontsize=16) # Display the plot plt.show() # Call the function to execute the solution create_titanic_survival_plot()"},{"question":"**Question: Create a Comprehensive Visualization with Seaborn** You are given two datasets, `penguins` and `flights`. Your task is to generate a grid of bar plots that achieves the following: 1. **Top plot**: - Use the `flights` dataset from the year 1960. - Create a simple bar plot showing the number of passengers per month. - Customize the bar plot to have different colors for each month. - Use horizontal orientation for the bars. 2. **Bottom plot**: - Use the `penguins` dataset. - Create two bar plots side-by-side in a single figure: - The first plot should show the distribution of species, with bars colored by their `sex`. - The second plot should show the average body mass (`body_mass_g`) for each species, with different colors for `sex`, and include standard deviation error bars. - Use appropriate transformations (e.g., `Dodge`) to handle overlapping bars. **Instructions**: - Ensure that the visualization is clear and well-labeled. - Use appropriate seaborn functionalities as demonstrated in the provided documentation. - Display the final grid of plots in a single figure. **Input**: The `penguins` and `flights` datasets are loaded for you as follows: ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") ``` **Output**: Your code should display the described grid of plots. **Constraints**: - You must use the seaborn objects API to create the plots. - The plots should be displayed in a single figure using a grid layout. **Example Output**: ``` (A single figure with the described grid of bar plots) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_visualization(): # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\").query(\\"year == 1960\\") # Create the top plot fig, axes = plt.subplots(3, 1, figsize=(12, 18)) sns.barplot( x=\\"passengers\\", y=\\"month\\", data=flights, palette=\\"viridis\\", ax=axes[0] ) axes[0].set_title(\'Number of Passengers Per Month in 1960\') # Create the first bottom plot for the penguins dataset sns.countplot( x=\\"species\\", hue=\\"sex\\", data=penguins, palette=\\"viridis\\", ax=axes[1] ) axes[1].set_title(\'Distribution of Species by Sex\') # Create the second bottom plot for average body mass with error bars sns.barplot( x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", data=penguins, palette=\\"viridis\\", ci=\\"sd\\", ax=axes[2] ) axes[2].set_title(\'Average Body Mass for Each Species by Sex with SD Error Bars\') plt.tight_layout() plt.show()"},{"question":"# XML Document Builder and Query Tool Objective: Implement a function that constructs an XML document representing a hierarchical structure of employees in a company and enables specific queries on the document. Task: 1. **Build the XML Document:** - Create a new document with a root element named \\"Company\\". - Add a child element \\"Employees\\" to the root. - Populate the \\"Employees\\" element with multiple \\"Employee\\" elements. Each \\"Employee\\" element should have: - `id` attribute: Unique identifier for the employee. - `name` attribute: Name of the employee. - `department` attribute: Department to which the employee belongs. 2. **Query the XML Document:** - Implement a function to find all employees in a specific department and return their names and IDs. Input and Output Format: 1. **Function Signature:** ```python def build_and_query_document(employee_data: List[Dict[str, str]], department_query: str) -> List[Dict[str, str]]: ``` 2. **Parameters:** - `employee_data`: A list of dictionaries, each containing information about an employee with keys `id`, `name`, and `department`. - `department_query`: A string representing the name of the department to query. 3. **Returns:** - A list of dictionaries. Each dictionary contains `id` and `name` of employees who belong to the queried department. 4. **Constraints:** - There will be a maximum of 100 employees. - Employee IDs are unique. Example: ```python employee_data = [ {\\"id\\": \\"1\\", \\"name\\": \\"Alice\\", \\"department\\": \\"Engineering\\"}, {\\"id\\": \\"2\\", \\"name\\": \\"Bob\\", \\"department\\": \\"HR\\"}, {\\"id\\": \\"3\\", \\"name\\": \\"Charlie\\", \\"department\\": \\"Engineering\\"} ] department_query = \\"Engineering\\" result = build_and_query_document(employee_data, department_query) print(result) # Expected Output: [{\'id\': \'1\', \'name\': \'Alice\'}, {\'id\': \'3\', \'name\': \'Charlie\'}] ``` Implementation Guidelines: 1. Use the `xml.dom.minidom` module to create and manipulate the XML document. 2. Ensure that the `build_and_query_document` function correctly creates the XML structure and performs the required query. 3. Handle any potential exceptions that might occur during XML processing, such as invalid attribute values or issues with creating elements. **Hint:** Utilize methods such as `getElementsByTagName` and attributes like `getAttribute` to traverse and query the XML document.","solution":"from typing import List, Dict from xml.dom.minidom import Document def build_and_query_document(employee_data: List[Dict[str, str]], department_query: str) -> List[Dict[str, str]]: # Create a new XML document doc = Document() # Create root element \'Company\' company = doc.createElement(\'Company\') doc.appendChild(company) # Create \'Employees\' element employees = doc.createElement(\'Employees\') company.appendChild(employees) # Populate \'Employees\' element with \'Employee\' elements for emp in employee_data: employee = doc.createElement(\'Employee\') employee.setAttribute(\'id\', emp[\'id\']) employee.setAttribute(\'name\', emp[\'name\']) employee.setAttribute(\'department\', emp[\'department\']) employees.appendChild(employee) # Query the XML document for the specified department result = [] for emp in employees.getElementsByTagName(\'Employee\'): if emp.getAttribute(\'department\') == department_query: result.append({ \'id\': emp.getAttribute(\'id\'), \'name\': emp.getAttribute(\'name\') }) return result"},{"question":"You are tasked with writing a Python function to process a list of strings containing numerical expressions and evaluate each expression. The function should return a list of results. Each expression can include addition, subtraction, multiplication, and division operations. **Function Signature:** ```python def process_expressions(expressions: List[str]) -> List[float]: ``` # Input: - A list of strings, `expressions`, where each string represents a numerical expression containing integers and operators (`+`, `-`, `*`, `/`). Each expression is guaranteed to be valid. # Output: - A list of floating-point numbers, each representing the evaluated result of the corresponding expression in `expressions`. # Constraints: - The length of `expressions` list (number of expressions) will not exceed 1000. - Each expression\'s length will not exceed 100 characters. - No division by zero will occur in the input. # Performance Requirements: - The solution should efficiently handle up to 1000 expressions with length up to 100 characters each. # Example: ```python assert process_expressions([\\"2 + 3\\", \\"4 * 5\\", \\"6 / 3\\", \\"7 - 2\\"]) == [5.0, 20.0, 2.0, 5.0] assert process_expressions([\\"10 / 2 + 5\\", \\"6 * (4 + 1)\\", \\"9 - 3 / 3\\"]) == [10.0, 30.0, 8.0] ``` # Instructions: 1. Define a function `process_expressions` that takes a list of strings as input. 2. Evaluate each expression in the list and store the result in a new list. 3. Return the new list containing the evaluated results. **Note:** - You may use Python\'s `eval` function to evaluate the expressions. Ensure to handle the expressions with necessary precautions for security, if applicable, or explore alternative parsing methods if required.","solution":"from typing import List def process_expressions(expressions: List[str]) -> List[float]: Evaluates a list of numerical expressions and returns the results. Parameters: expressions (List[str]): A list of strings, each string is a numerical expression. Returns: List[float]: A list of floating-point results. results = [] for expression in expressions: try: result = eval(expression) results.append(float(result)) except Exception as e: print(f\\"Error evaluating expression \'{expression}\': {e}\\") results.append(None) # Optionally handle invalid expressions gracefully return results"},{"question":"# Advanced Python Coding Assessment: Custom Warning Management The `warnings` module allows you to manage warning messages in Python which are typically issued to alert the user of conditions that do not warrant exceptions but are still important. To demonstrate your understanding of this module, you are required to write custom functions that manage warnings based on specific criteria. Task Description You will implement a function `custom_warning_handler` that takes the following parameters: - `action`: A string indicating the action to take when a warning matches the specified criteria. This can be one of \\"default\\", \\"ignore\\", \\"always\\", \\"module\\", \\"once\\", or \\"error\\". - `message`: A string containing a regular expression to match the start of the warning message. - `category`: A warning category (a subclass of `Warning`). Use `UserWarning` if no specific category is provided. - `module`: A string containing a regular expression to match the start of the module name from which the warning is issued. - `lineno`: An integer indicating the line number where the warning must occur. Use 0 to match any line number. The function should: 1. Set the specified filter for handling warnings. 2. Issue a warning using the provided criteria. 3. Return the formatted warning message that would be displayed based on the action specified. Function Signature ```python import warnings import re def custom_warning_handler(action: str, message: str, category: type = UserWarning, module: str = \'\', lineno: int = 0) -> str: # Your implementation here ``` Examples ```python # Example 1: msg = custom_warning_handler(action=\\"ignore\\", message=\\"deprecated\\", category=DeprecationWarning) print(msg) # Output: \\"\\" # Example 2: msg = custom_warning_handler(action=\\"error\\", message=\\"Resource usage warning\\", category=ResourceWarning) print(msg) # Output: \\"ResourceWarning: Resource usage warning\\" # Example 3: msg = custom_warning_handler(action=\\"default\\", message=\\"This is a user warning\\") print(msg) # Output: \\"<full formatted warning message>\\" ``` Constraints - The `action` parameter must be one of the allowed strings, or raise a `ValueError` if it is invalid. - The `message` parameter must be a non-empty string. - The `category` must be a subclass of the `Warning` class. - The `module` string should be a valid regular expression. - The `lineno` should be a non-negative integer. Notes - Make sure your function correctly sets up the warnings filter and handles warnings as per the specified action. - The formatted warning message should match the standard warning format that Python would print. You should test your solution thoroughly to ensure it handles different actions and warning scenarios correctly.","solution":"import warnings import re def custom_warning_handler(action: str, message: str, category: type = UserWarning, module: str = \'\', lineno: int = 0) -> str: if action not in [\\"default\\", \\"ignore\\", \\"always\\", \\"module\\", \\"once\\", \\"error\\"]: raise ValueError(\\"Invalid action specified\\") if not issubclass(category, Warning): raise ValueError(\\"Category must be a subclass of Warning\\") if lineno < 0: raise ValueError(\\"lineno must be a non-negative integer\\") # Clear existing filters and set new filter warnings.simplefilter(action) # Custom warning formatting to catch issued warning class CustomFormatWarning(warnings.WarningMessage): def __init__(self, message, category, filename, lineno, file=None, line=None): super().__init__(message, category, filename, lineno, file, line) self.formatted = warnings.formatwarning(self.message, self.category, self.filename, self.lineno) # Temporarily override the showwarning method to capture warning message old_showwarning = warnings.showwarning def custom_showwarning(message, category, filename, lineno, file=None, line=None): custom_warning = CustomFormatWarning(message, category, filename, lineno, file, line) custom_showwarning.output = custom_warning.formatted warnings.showwarning = custom_showwarning custom_showwarning.output = None # Issue the warning warnings.warn(message, category, stacklevel=2) # Restore the original showwarning method warnings.showwarning = old_showwarning return custom_showwarning.output or \\"\\""},{"question":"# HTML Entity Converter You are tasked with implementing a Python class `HTMLEntityConverter` that provides functionalities to convert between various HTML entities and their corresponding Unicode text as described in the `html.entities` module. Class Definition ```python class HTMLEntityConverter: def __init__(self): # Initialize any necessary data structures here def to_unicode(self, html_string: str) -> str: Converts an HTML string containing named character references to a string of Unicode characters. Parameters: - html_string (str): A string containing named HTML character references (e.g., \'&lt;\', \'&gt;\', etc.) Returns: - str: A Unicode string where all named HTML character references are converted to their respective characters. def to_html_entities(self, unicode_string: str) -> str: Converts a Unicode string to a string containing named HTML character references for non-ASCII characters. Parameters: - unicode_string (str): A Unicode string containing text with possible non-ASCII characters. Returns: - str: A string where non-ASCII characters are replaced with their equivalent named HTML character references. ``` Requirements 1. Implement the `__init__` method to initialize any necessary structures or preprocessing. 2. Implement the `to_unicode` method to: - Convert named HTML character references in the input string to their respective Unicode characters using the `html5` dictionary. - Handle names both with and without the trailing semicolon as stipulated by HTML5 standards. 3. Implement the `to_html_entities` method to: - Convert non-ASCII Unicode characters in the input string to their respective named HTML character references using the `codepoint2name` dictionary. Example Usage ```python # Create an instance of HTMLEntityConverter converter = HTMLEntityConverter() # Convert HTML to Unicode html_string = \\"The value of &lt;a&gt; is less than &gt;b&gt;.\\" unicode_string = converter.to_unicode(html_string) print(unicode_string) # Output: \\"The value of <a> is less than >b>.\\" # Convert Unicode to HTML entities unicode_input = \\"CafÃ©\\" html_entities_string = converter.to_html_entities(unicode_input) print(html_entities_string) # Output: \\"Caf&eacute;\\" ``` Constraints - You may assume all input strings are valid and do not require additional error handling. - Focus on performance, especially when handling long input strings with multiple replacements. Notes - Utilize the `html.entities` module\'s dictionaries `html5` and `codepoint2name` for reference and conversions. - Do not use external libraries for HTML entity conversion; rely on the provided dictionaries and your own implementation.","solution":"import html.entities class HTMLEntityConverter: def __init__(self): # Initialize dictionaries for conversion self.html5 = html.entities.html5 self.codepoint2name = html.entities.codepoint2name def to_unicode(self, html_string: str) -> str: Converts an HTML string containing named character references to a string of Unicode characters. Parameters: - html_string (str): A string containing named HTML character references (e.g., \'&lt;\', \'&gt;\', etc.) Returns: - str: A Unicode string where all named HTML character references are converted to their respective characters. for name, code in self.html5.items(): html_string = html_string.replace(f\\"&{name};\\", code) # handle &name; html_string = html_string.replace(f\\"&{name}\\", code) # handle &name return html_string def to_html_entities(self, unicode_string: str) -> str: Converts a Unicode string to a string containing named HTML character references for non-ASCII characters. Parameters: - unicode_string (str): A Unicode string containing text with possible non-ASCII characters. Returns: - str: A string where non-ASCII characters are replaced with their equivalent named HTML character references. result = [] for char in unicode_string: if ord(char) > 127: name = self.codepoint2name.get(ord(char)) if name: result.append(f\\"&{name};\\") else: result.append(char) else: result.append(char) return \'\'.join(result)"},{"question":"Objective: Implement a function using `torch.func` to compute the per-sample gradients of the mean squared error (MSE) loss between predicted and actual values in a neural network. Function Signature: ```python import torch from torch.func import vmap, grad def compute_per_sample_gradients(model, loss_fn, inputs, targets): Args: - model (torch.nn.Module): The neural network model. - loss_fn (callable): The loss function (e.g., MSE loss). - inputs (torch.Tensor): Input tensor of shape (batch_size, *input_shape). - targets (torch.Tensor): Target tensor of shape (batch_size, *output_shape). Returns: - per_sample_gradients (torch.Tensor): Tensor of gradients of the loss with respect to the model parameters for each sample in the batch. pass ``` Requirements: 1. **Torch Integration:** Use `torch.func` operations such as `vmap` and `grad` to efficiently compute per-sample gradients. 2. **Batch Processing:** The function should handle a batch of inputs and compute the gradient for each sample in the batch. 3. **Model and Loss:** The model should be a PyTorch `nn.Module`, and the loss function should be applicable to individual predictions. 4. **Efficiency:** Use vectorization where appropriate to ensure the function runs efficiently even for large batches. Constraints: - Assume that `model` has been initialized and all parameters are accessible. - The input tensor `inputs` and the target tensor `targets` have the same batch size. - The loss function `loss_fn` should be callable with signature `loss_fn(predictions, targets)`. Example: ```python import torch from torch.func import vmap, grad # Example model, loss function, inputs, and targets class SimpleModel(torch.nn.Module): def __init__(self): super().__init__() self.linear = torch.nn.Linear(10, 1) def forward(self, x): return self.linear(x) model = SimpleModel() loss_fn = torch.nn.MSELoss(reduction=\'none\') inputs = torch.randn(5, 10) # batch_size=5, input_shape=(10,) targets = torch.randn(5, 1) # batch_size=5, output_shape=(1,) # Expected output: per-sample gradients of the loss per_sample_gradients = compute_per_sample_gradients(model, loss_fn, inputs, targets) print(per_sample_gradients) ``` Hint: - Use `vmap` to vectorize the gradient computation over the batch. - Use `grad` to compute gradients with respect to the model parameters.","solution":"import torch from torch.func import vmap, grad def compute_per_sample_gradients(model, loss_fn, inputs, targets): def compute_loss(model_params, single_input, single_target): # Re-create model using `model_params` model = SimpleModel() param_dict = dict(zip(model.parameters(), model_params)) for param, new_value in param_dict.items(): param.data = new_value prediction = model(single_input) loss = loss_fn(prediction, single_target) return loss # Flatten model parameters into a tuple of tensors for processing model_params = tuple(param.data for param in model.parameters()) # Function to wrap around the model parameters and single input/target def get_loss(model_params, single_input, single_target): # Apply single forward pass and compute MSE loss for the example output = model(single_input) loss = loss_fn(output, single_target) # Return a torch.tensor scalar return loss.sum() # Vectorized map over the batch of inputs and targets batch_size = inputs.shape[0] per_sample_grads = vmap(grad(get_loss), (None, 0, 0))(model_params, inputs, targets) return per_sample_grads"},{"question":"Objective: Create a Python script that demonstrates your understanding of seaborn\'s theme and display configuration, along with basic data visualization. Problem: You are provided with a dataset that contains information about different species of flowers, including their sepal length, sepal width, petal length, and petal width. Your task is to: 1. Create a scatter plot of sepal length vs. sepal width, with the points colored by species. 2. Set up a seaborn theme configuration that enhances the plot\'s readability and aesthetic appeal. 3. Ensure the plot is displayed in SVG format for high-resolution quality. 4. Adjust the image scaling in your notebook to ensure a good balance of data density and readability. Dataset: You can use the `iris` dataset which is available through seaborn: ```python import seaborn as sns iris = sns.load_dataset(\'iris\') ``` Requirements: 1. **Scatter Plot** - Use `sepal_length` for the x-axis and `sepal_width` for the y-axis. - Color the points based on the species of the flower. 2. **Theme Configuration** - Change the background color of the axes. - Apply a whitegrid style to the plot. - Sync the plot\'s theme with matplotlib\'s global state. 3. **Display Configuration** - Set the display format to SVG. - Adjust the image scaling to 0.7. 4. **Implementation Details** - You should clearly define the input and output. - Add comments to explain your approach. Input and Output Format: - **Input:** Use the `iris` dataset loaded from seaborn. - **Output:** A scatter plot rendered in SVG format with the appropriate theme and display configurations as specified. Constraints: - Ensure that the plot is aesthetically pleasing and conveys the data effectively. - The code should be well-commented and include explanations where necessary. ```python # Solution import seaborn as sns from seaborn import axes_style import seaborn.objects as so import matplotlib as mpl # Load the iris dataset iris = sns.load_dataset(\'iris\') # Set up a scatter plot with customized theme and display settings # Step 1: Configure the theme so.Plot.config.theme[\\"axes.facecolor\\"] = \\"lightgray\\" # Changing background color so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Applying whitegrid style so.Plot.config.theme.update(mpl.rcParams) # Syncing with matplotlib\'s global state # Step 2: Configure display settings so.Plot.config.display[\\"format\\"] = \\"svg\\" # Setting display format to SVG so.Plot.config.display[\\"scaling\\"] = 0.7 # Adjusting image scaling # Step 3: Create the scatter plot ( so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\") .add(so.Dot()) .show() ) ``` Note: Ensure you have the necessary imports and configurations at the top of your script for it to run correctly in a Jupyter notebook or a Python environment.","solution":"import seaborn as sns import matplotlib.pyplot as plt import matplotlib as mpl def create_flower_scatter_plot(): Creates a scatter plot of sepal length vs. sepal width from the iris dataset, with points colored by species. Encapsulates seaborn theme and display configuration. # Load the iris dataset iris = sns.load_dataset(\'iris\') # Configure the theme sns.set_theme(style=\\"whitegrid\\") # Apply whitegrid style plt.rcParams[\'axes.facecolor\'] = \'lightgray\' # Change background color of axes # Configure display settings mpl.rcParams[\'figure.figsize\'] = (10, 6) mpl.rcParams[\'svg.fonttype\'] = \'none\' # Ensure text in SVG remains text mpl.rcParams[\\"figure.dpi\\"] = 300 mpl.rcParams[\\"savefig.dpi\\"] = 300 # Create the scatter plot plt.figure(dpi=300) scatter_plot = sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=\'muted\') # Set plot title plt.title(\\"Sepal Length vs. Sepal Width (Colored by Species)\\") # Display the plot plt.savefig(\'flower_plot.svg\', format=\'svg\', bbox_inches=\'tight\') # Save as SVG plt.show() # Display the plot return scatter_plot"},{"question":"# Shell Command Processor Given the documentation for the `shlex` module, you are required to implement a function that processes a list of shell-like command strings. The function should split each command into tokens using shell-like syntax and perform certain operations on the tokens, as described below. Function Signature ```python def process_commands(commands: List[str]) -> List[List[str]]: Process a list of shell-like command strings and return a list of token lists. Parameters: commands (List[str]): A list of command strings to be processed. Returns: List[List[str]]: A list of lists, where each inner list contains the tokens of the corresponding command. ``` Input - `commands`: A list of command strings. Each string represents a command and may contain whitespace, quotes, or special characters. Output - A list of lists, where each inner list contains the tokens obtained from splitting the corresponding command string. Constraints - The commands should be processed in POSIX mode. - Comments should not be considered (i.e., `comments=False`). - Handle any potential edge cases such as empty strings or command strings without whitespace. Example ```python commands = [ \'echo \\"Hello, World!\\"\', \'ls -l /home/user\', \\"grep \'search pattern\' file.txt\\", \'cat file1.txt file2.txt > output.txt\' ] assert process_commands(commands) == [ [\'echo\', \'Hello, World!\'], [\'ls\', \'-l\', \'/home/user\'], [\'grep\', \'search pattern\', \'file.txt\'], [\'cat\', \'file1.txt\', \'file2.txt\', \'>\', \'output.txt\'] ] ``` # Task 1. Implement the `process_commands` function. 2. Ensure the commands are parsed correctly according to the specified constraints. 3. Write some test cases to validate your solution. # Notes - You may use the `shlex.split` method to handle the splitting of command strings. - Pay attention to shell-like syntax handling, such as quotes and special characters.","solution":"import shlex from typing import List def process_commands(commands: List[str]) -> List[List[str]]: Process a list of shell-like command strings and return a list of token lists. Parameters: commands (List[str]): A list of command strings to be processed. Returns: List[List[str]]: A list of lists, where each inner list contains the tokens of the corresponding command. processed_commands = [] for command in commands: tokens = shlex.split(command, posix=True, comments=False) processed_commands.append(tokens) return processed_commands"},{"question":"Objective This question assesses your ability to use the seaborn.objects module to create effective visualizations using real-world data sets. Your task is to implement a custom function that generates two specific types of plots, manipulating data appropriately and customizing the plot aesthetics. Problem Statement You are provided with the `fmri` dataset and are required to perform the following tasks: 1. Create a function `prepare_fmri_data(data)` that processes the provided `fmri` dataset to include only the `parietal` region. The function should take a DataFrame as input and return the processed DataFrame. 2. Implement a function `create_fmri_plot(data)` that accepts a DataFrame and generates a plot containing both a line plot and a band representing error bars. The plot should demonstrate the `signal` over `timepoint`, with different colors representing various `event` types. Each plot needs a specific edge color and varying transparency. 3. Ensure your plot covers the following customization: - The plot should have a line representing the average signal value across different events. - Include a band to represent the interval of the signal values. - Customizations like different edge colors, transparency, and additional grid lines to improve readability. Your implementation should respect the following constraints and structure: Function Specifications **Function 1: prepare_fmri_data(data)**: - **Input**: - `data` (`pd.DataFrame`): The raw fmri data. - **Output**: - `pd.DataFrame`: Processed DataFrame containing only `parietal` region data. **Function 2: create_fmri_plot(data)**: - **Input**: - `data` (`pd.DataFrame`): Processed fmri data containing only `parietal` region data. - **Output**: - A seaborn plot with specified customizations. Example ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd def prepare_fmri_data(data): # Your code here def create_fmri_plot(data): # Your code here # Example usage: fmri = load_dataset(\\"fmri\\") processed_fmri = prepare_fmri_data(fmri) create_fmri_plot(processed_fmri) ``` **Hints**: - Use `so.Plot` along with `so.Band` and `so.Line` from the seaborn.objects module. - Leverage pandas for data manipulation before plotting. **Constraints**: - Use the seaborn.objects module only for plot creation. - Ensure the plotted line and band meet the specified customizations. Your solution will be evaluated based on the correctness of data processing, the accuracy of the plot, and the adherence to the given customization requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def prepare_fmri_data(data): Processes the provided fmri dataset to include only the \'parietal\' region. Parameters: data (pd.DataFrame): Raw fmri dataset. Returns: pd.DataFrame: Processed DataFrame containing only \'parietal\' region data. return data[data[\'region\'] == \'parietal\'] def create_fmri_plot(data): Creates a seaborn plot with a line plot and error bands representing the fmri data. Parameters: data (pd.DataFrame): Processed fmri data containing only \'parietal\' region data. p = so.Plot(data, x=\'timepoint\', y=\'signal\', color=\'event\') p.add(so.Line(), so.Agg(), alpha=0.7, edgecolor=\\"k\\") p.add(so.Band(), so.Agg(), alpha=0.2) p.plot()"},{"question":"Problem Statement You are required to implement a function `retrieve_emails(host, username, password, use_ssl=True, verbose=0)` that connects to a POP3 server, logs in using the provided user credentials, and retrieves all emails, returning them in a structured format. The function should consider the following requirements: * **Parameters:** - `host` (str): The hostname of the POP3 server. - `username` (str): The username for logging into the POP3 server. - `password` (str): The password for logging into the POP3 server. - `use_ssl` (bool): If True, use `POP3_SSL` to connect to the server. Otherwise, use the plain `POP3` class. Default is True. - `verbose` (int): The debugging level. Default is 0 (no debugging output). * **Return:** - A list of emails, where each email is represented as a list of strings, each string being a line in the email. * **Constraints:** - If the `use_ssl` flag is set, connect to the server using a secure SSL connection (`POP3_SSL`). - Handle all potential errors robustly using appropriate error handling techniques. - Ensure that the connection to the server is cleanly closed after interaction. **Example Usage:** ```python emails = retrieve_emails(\\"pop.mail.server.com\\", \\"username\\", \\"password\\", use_ssl=True, verbose=1) for email in emails: print(\\"Email:\\") print(\\"n\\".join(email)) ``` **Hint:** 1. Use the `retr()` method to retrieve messages. 2. Use the `stat()` method to get the number of messages in the mailbox. 3. Ensure proper error handling using the `poplib.error_proto` exception. 4. Use `quit()` to close the connection properly. Implement the `retrieve_emails` function to fulfill the above requirements.","solution":"import poplib from typing import List def retrieve_emails(host: str, username: str, password: str, use_ssl: bool = True, verbose: int = 0) -> List[List[str]]: Connect to a POP3 server, log in using the provided user credentials, and retrieve all emails. Parameters: - host (str): The hostname of the POP3 server. - username (str): The username for logging into the POP3 server. - password (str): The password for logging into the POP3 server. - use_ssl (bool): If True, use `POP3_SSL` to connect to the server. Otherwise, use the plain `POP3` class. - verbose (int): The debugging level. Default is 0 (no debugging output). Return: - A list of emails, where each email is represented as a list of strings, each string being a line in the email. emails = [] try: # Connect to the server if use_ssl: server = poplib.POP3_SSL(host) else: server = poplib.POP3(host) # Set the debug level server.set_debuglevel(verbose) # Login to the server server.user(username) server.pass_(password) # Get the number of messages num_messages = len(server.list()[1]) # Retrieve all messages for i in range(1, num_messages + 1): response, lines, octets = server.retr(i) emails.append(lines) # Quit the server server.quit() except poplib.error_proto as e: print(f\\"An error occurred: {e}\\") return emails"},{"question":"**XML Data Summarizer Using `xml.etree.ElementTree`** **Problem Statement:** You are given an XML document structured to represent a catalog of books. The XML structure is as follows: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> <!-- More book entries --> </catalog> ``` Assume that this XML document is stored in a file named `books.xml`. Your task is to write a Python function that reads the XML file, processes the data, and outputs a summary of the books. The summary should include: 1. The total number of books. 2. A list of all book titles. 3. The average price of all books. 4. The number of books per genre. **Function Signature:** ```python def summarize_books(xml_file: str) -> dict: # Your code here pass ``` **Input:** - `xml_file` is a string representing the path to the XML file. You can assume that the file will always be in the correct format as described above. **Output:** - The function should return a dictionary with the keys: - `\\"total_books\\"`: an integer representing the total number of books. - `\\"titles\\"`: a list of strings, where each string is a title of a book. - `\\"average_price\\"`: a float representing the average price of the books. - `\\"books_per_genre\\"`: a dictionary where the keys are the genres and the values are the counts of the books in those genres. **Example:** Given the XML structure above, and assuming there are only two books as shown, an example output would be: ```python { \\"total_books\\": 2, \\"titles\\": [\\"XML Developer\'s Guide\\", \\"Midnight Rain\\"], \\"average_price\\": 25.45, \\"books_per_genre\\": { \\"Computer\\": 1, \\"Fantasy\\": 1 } } ``` **Implementation Notes:** - Use the `xml.etree.ElementTree` module to parse the XML data. - You may find methods like `findall()`, `find()`, and `iter()` useful for traversing through the XML elements. - Ensure proper handling of floating-point division to compute the average price. - Populate the summary data in a dictionary and return it as specified. ```python # Example solution (Students will write their own version) import xml.etree.ElementTree as ET def summarize_books(xml_file: str) -> dict: tree = ET.parse(xml_file) root = tree.getroot() total_books = 0 titles = [] total_price = 0 books_per_genre = {} for book in root.findall(\'book\'): total_books += 1 title = book.find(\'title\').text titles.append(title) price = float(book.find(\'price\').text) total_price += price genre = book.find(\'genre\').text if genre in books_per_genre: books_per_genre[genre] += 1 else: books_per_genre[genre] = 1 average_price = total_price / total_books if total_books > 0 else 0 return { \\"total_books\\": total_books, \\"titles\\": titles, \\"average_price\\": average_price, \\"books_per_genre\\": books_per_genre } ```","solution":"import xml.etree.ElementTree as ET def summarize_books(xml_file: str) -> dict: tree = ET.parse(xml_file) root = tree.getroot() total_books = 0 titles = [] total_price = 0 books_per_genre = {} for book in root.findall(\'book\'): total_books += 1 title = book.find(\'title\').text titles.append(title) price = float(book.find(\'price\').text) total_price += price genre = book.find(\'genre\').text if genre in books_per_genre: books_per_genre[genre] += 1 else: books_per_genre[genre] = 1 average_price = total_price / total_books if total_books > 0 else 0 return { \\"total_books\\": total_books, \\"titles\\": titles, \\"average_price\\": round(average_price, 2), \\"books_per_genre\\": books_per_genre }"},{"question":"# Question You are provided with a dataset of diamond characteristics and prices. Using the Seaborn `objects` interface, create a visualization that meets the following requirements: 1. Load the `diamonds` dataset from Seaborn. 2. Create a plot (`p`) that maps the `cut` category to the x-axis and `price` to the y-axis. 3. Scale the y-axis to a logarithmic scale. 4. Add points to the plot that represent only the 25th, 50th, and 75th percentiles of the diamond prices within each `cut` category. Use a dot plot for this representation. 5. Add an additional layer to display the 10th and 90th percentile ranges as a shaded area or interval around the dot plot. 6. Ensure the plot is clearly annotated with titles and labels for better interpretation. **Input** - No direct input is required; you work directly with the dataset loaded from Seaborn. **Output** - A Seaborn plot visualized according to the specifications. # Example Diagram Representation Your final visualization should resemble a faceted plot where: - Each x-tick represents a different `cut` category. - Y-axis is scaled logarithmically to handle price variations. - Percentiles are marked appropriately, and intervals are highlighted. Here\'s an example code template to get you started: ```python import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Define the plot p = ( so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") .scale(y=\\"log\\") .add(so.Dot(), so.Perc([25, 50, 75])) # Add points for specific percentiles .add(so.Range(), so.Perc([10, 90])) # Add shaded range for percentile intervals ) # Set plot titles and labels p = p.label(x=\\"Diamond Cut\\", y=\\"Price (log scale)\\", title=\\"Diamond Prices by Cut with Percentile Ranges\\") # Display the plot p.show() ``` Your task is to fill in and complete the code to meet the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_diamond_plot(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Define the plot p = ( so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") .scale(y=\\"log\\") .add(so.Dot(), so.Perc([25, 50, 75])) # Add points for specific percentiles .add(so.Range(), so.Perc([10, 90])) # Add shaded range for percentile intervals .label(x=\\"Diamond Cut\\", y=\\"Price (log scale)\\", title=\\"Diamond Prices by Cut with Percentile Ranges\\") ) # Display the plot p.show() # Call the function to create the plot create_diamond_plot()"},{"question":"# WAV File Converter **Objective**: Implement a WAV file converter that reads a given WAV file, extracts its audio data, and writes this data to a new WAV file with modified attributes (e.g., sample width, frame rate). # Problem Statement You are required to write a Python function `convert_wav(input_file: str, output_file: str, sample_width: int, frame_rate: int) -> None` that converts an input WAV file to an output WAV file with the specified sample width and frame rate. # Input: - `input_file` (str): The file path of the input WAV file. - `output_file` (str): The desired file path for the output WAV file. - `sample_width` (int): The desired sample width (in bytes) for the output file. - `frame_rate` (int): The desired frame rate (in Hz) for the output file. # Output: - Your function should not return any values. It should create a new WAV file at the specified `output_file` path with the modified attributes. # Constraints: 1. The input file is guaranteed to be a valid WAV file in PCM format. 2. The values for `sample_width` and `frame_rate` will be positive integers. # Performance Requirements: - The function should efficiently read and write frames to handle large WAV files. # Example Usage: ```python convert_wav(\\"input.wav\\", \\"output.wav\\", 2, 44100) ``` In the above example, the function reads `input.wav`, changes its sample width to 2 bytes and frame rate to 44100 Hz, and writes the result to `output.wav`. # Detailed Requirements: 1. Open the input file in read mode (`\'rb\'`) using the `wave` module. 2. Extract all relevant parameters and audio frame data from the input file. 3. Open the output file in write mode (`\'wb\'`) using the `wave` module. 4. Set the number of channels, sample width, frame rate, and compression type for the output file based on the extracted data and provided arguments. 5. Write the frame data to the output file with the updated sample width and frame rate. 6. Ensure to close both the input and output file objects properly. # Note: - You may use `wave.open()` to handle file opening and closing. - Utilize methods like `getparams()`, `readframes()`, `writeframes()`, and others as necessary from the `Wave_read` and `Wave_write` objects. # Hints: - You might need to handle conversion of frame data based on the new sample width and frame rate. Consider how you can interpolate or convert the raw bytes data accordingly.","solution":"import wave import audioop def convert_wav(input_file: str, output_file: str, sample_width: int, frame_rate: int) -> None: Converts a WAV file to a new WAV file with specified sample width and frame rate. :param input_file: File path of the input WAV file, as a string. :param output_file: File path for the output WAV file, as a string. :param sample_width: Desired sample width (in bytes) for the output file. :param frame_rate: Desired frame rate (in Hz) for the output file. with wave.open(input_file, \'rb\') as in_wave: # Extract original parameters params = in_wave.getparams() nchannels, original_sample_width, original_frame_rate, nframes, comptype, compname = params # Read the input file\'s frame data frames = in_wave.readframes(nframes) # Convert the sample width if different from the input file if original_sample_width != sample_width: frames = audioop.lin2lin(frames, original_sample_width, sample_width) # Adjust the frame rate if different from the input file if original_frame_rate != frame_rate: frames, _ = audioop.ratecv(frames, sample_width, nchannels, original_frame_rate, frame_rate, None) with wave.open(output_file, \'wb\') as out_wave: # Set parameters for the output file out_wave.setnchannels(nchannels) out_wave.setsampwidth(sample_width) out_wave.setframerate(frame_rate) # Write the modified frames to the output file out_wave.writeframes(frames)"},{"question":"# Objective: You are required to manipulate a dataset using pandas to analyze sales data of a company. The dataset is provided as a CSV file named `sales_data.csv` with the following columns: - `date` (String): Date of the sale in YYYY-MM-DD format. - `salesperson` (String): Name of the salesperson. - `region` (String): Region where the sale was made. - `product` (String): Name of the product sold. - `units_sold` (Integer): Number of units sold. - `unit_price` (Float): Price per unit. # Task: 1. Load the dataset into a pandas DataFrame. 2. Clean the data by: - Parsing the `date` column to datetime objects. - Removing any duplicate rows. - Filling any missing values in the `units_sold` and `unit_price` columns with the mean value of their respective columns. 3. Create a new column `total_sales` which is the product of `units_sold` and `unit_price`. 4. Generate a summary report with the following information: - Total units sold per region. - Total sales per region. - Top 3 products by sales for each region. 5. Reshape the dataset to have a pivot table where the rows are `region`, the columns are `product`, and the values are the sum of `total_sales`. 6. Save the pivot table and the summary report into two separate CSV files named `pivot_table.csv` and `summary_report.csv` respectively. # Expected Input and Output: - **Input:** CSV file (`sales_data.csv`) containing the sales data. - **Output:** Two CSV files (`pivot_table.csv` and `summary_report.csv`). # Constraints and Performance Requirements: - The dataset can be large (up to 100,000 rows), so your solution should handle large data efficiently. - Ensure that the solution is readable and well-documented. # Example: Given a dataset: ``` date,salesperson,region,product,units_sold,unit_price 2022-01-01,John Doe,North,Product A,10,15.0 2022-01-01,Jane Smith,South,Product B,5,20.0 2022-01-02,John Doe,North,Product A,12,15.0 2022-01-02,Jane Smith,South,Product B,,20.0 2022-01-03,Emily Johnson,East,Product C,20,30.0 ``` Generated `pivot_table.csv`: ``` product,Product A,Product B,Product C region,,,, East,0.0,0.0,600.0 North,330.0,0.0,0.0 South,0.0,250.0,0.0 ``` Generated `summary_report.csv`: ``` region,total_units_sold,total_sales,top_3_products_by_sales East,20,600.0,Product C North,22,330.0,Product A South,5,250.0,Product B ``` # Notes: - Handle any potential edge cases, such as empty rows or entirely missing columns, gracefully.","solution":"import pandas as pd def load_and_clean_data(file_path): # Load the dataset df = pd.read_csv(file_path) # Parse the \'date\' column to datetime df[\'date\'] = pd.to_datetime(df[\'date\']) # Remove any duplicate rows df = df.drop_duplicates() # Fill missing values in \'units_sold\' and \'unit_price\' with the mean of their respective columns df[\'units_sold\'] = df[\'units_sold\'].fillna(df[\'units_sold\'].mean()) df[\'unit_price\'] = df[\'unit_price\'].fillna(df[\'unit_price\'].mean()) # Create \'total_sales\' column df[\'total_sales\'] = df[\'units_sold\'] * df[\'unit_price\'] return df def generate_summary_report(df): # Total units sold per region total_units_sold_per_region = df.groupby(\'region\')[\'units_sold\'].sum().reset_index(name=\'total_units_sold\') # Total sales per region total_sales_per_region = df.groupby(\'region\')[\'total_sales\'].sum().reset_index(name=\'total_sales\') # Top 3 products by sales for each region top_products_per_region = (df.groupby([\'region\', \'product\'])[\'total_sales\'] .sum() .sort_values(ascending=False) .groupby(level=0) .head(3) .reset_index()) top_products = (top_products_per_region.groupby(\'region\') .apply(lambda x: \', \'.join(x[\'product\'][:3])) .reset_index(name=\'top_3_products_by_sales\')) # Combine the reports into a single DataFrame summary_report = pd.merge(total_units_sold_per_region, total_sales_per_region, on=\'region\') summary_report = pd.merge(summary_report, top_products, on=\'region\') return summary_report def create_pivot_table(df): # Create a pivot table pivot_table = df.pivot_table(values=\'total_sales\', index=\'region\', columns=\'product\', aggfunc=\'sum\', fill_value=0) return pivot_table def save_to_csv(dataframe, file_path): dataframe.to_csv(file_path, index=True) def process_sales_data(file_path): # Load and clean data df = load_and_clean_data(file_path) # Generate summary report summary_report = generate_summary_report(df) # Create pivot table pivot_table = create_pivot_table(df) # Save the outputs to CSV files save_to_csv(summary_report, \'summary_report.csv\') save_to_csv(pivot_table, \'pivot_table.csv\') # Example of running the complete process # process_sales_data(\'sales_data.csv\')"},{"question":"**Objective**: Assess the understanding and practical application of Python\'s `zipfile` module for managing ZIP archives. **Problem Statement**: You are tasked with implementing a function that manages ZIP archives based on a given set of operations. Specifically, you\'ll create a function `manage_zip_archive` that can create a ZIP archive, add files to it, extract files, and list the contents of the archive. # Function Signature ```python def manage_zip_archive(zip_path: str, operations: list) -> list: pass ``` # Parameters - `zip_path` (str): The path to the ZIP file that will be created or manipulated. - `operations` (list): A list of operations to perform on the ZIP archive. Each operation will be a tuple where the first element is an operation identifier (str) and the subsequent elements depend on the operation. # Operations 1. **create**: Creates a ZIP archive. - `(\'create\',)` 2. **add_file**: Adds a file to the ZIP archive. - `(\'add_file\', file_path: str[, arcname: str])`: `file_path` is the path to the file to add. Optional `arcname` specifies the name to use in the archive. 3. **extract**: Extracts a file from the ZIP archive to the current directory. - `(\'extract\', member: str[, path: str])`: `member` is the name of the file in the archive. Optional `path` specifies the directory to extract to. 4. **list_files**: Lists the files in the ZIP archive. - `(\'list_files\',)`. Your function should return a list containing the results of the `list_files` operations in the order they were requested. Each result should be a list of filenames in the archive at that point in time. # Example ```python # Assume the following file structure: # /path/to/files/file1.txt # /path/to/files/file2.txt operations = [ (\'create\',), (\'add_file\', \'/path/to/files/file1.txt\'), (\'add_file\', \'/path/to/files/file2.txt\'), (\'list_files\',), (\'extract\', \'file1.txt\', \'/path/to/extract\'), (\'list_files\',) ] print(manage_zip_archive(\'example.zip\', operations)) # Output: [[\'file1.txt\', \'file2.txt\'], [\'file1.txt\', \'file2.txt\']] ``` # Constraints - The file paths in operations will be valid. - The `zipfile` module should be used for all ZIP file manipulations. - The ZIP archive does not need to be encrypted. - Assume sufficient disk space and permissions are available for writing and extracting files. # Notes - You need to handle potential exceptions such as trying to add a non-existing file, extracting a non-existing member, or creating a ZIP file in a non-writable directory. - Ensure the function closes the ZIP file properly after all operations. # Performance Requirements - The function should efficiently handle up to 100 operations in the `operations` list. - The size of each file to be added to the ZIP archive will not exceed 10MB, and the total size of the ZIP archive will not exceed 1GB. Implement the function `manage_zip_archive` to meet the specifications and constraints outlined above.","solution":"import zipfile import os def manage_zip_archive(zip_path: str, operations: list) -> list: result = [] for operation in operations: op = operation[0] if op == \'create\': with zipfile.ZipFile(zip_path, \'w\') as zipf: pass # Just create a new empty zip file elif op == \'add_file\': file_path = operation[1] arcname = operation[2] if len(operation) > 2 else os.path.basename(file_path) with zipfile.ZipFile(zip_path, \'a\') as zipf: zipf.write(file_path, arcname) elif op == \'extract\': member = operation[1] path = operation[2] if len(operation) > 2 else \'.\' with zipfile.ZipFile(zip_path, \'r\') as zipf: zipf.extract(member, path) elif op == \'list_files\': with zipfile.ZipFile(zip_path, \'r\') as zipf: result.append(zipf.namelist()) return result"},{"question":"# PCA and Its Variants on the Iris Dataset **Problem Statement:** Principal Component Analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset. It\'s often used to make data easy to explore and visualize. In this exercise, you\'ll work with the famous Iris dataset and apply different PCA variants using scikit-learn. **Dataset:** The Iris flower data set consists of 150 samples of iris flowers, with four features (length and width of sepals and petals). There are three classes (setosa, versicolor, virginica). **Task:** 1. **Data Loading and Preprocessing:** - Load the Iris dataset using scikit-learn\'s `datasets` module. - Standardize the dataset (mean zero and variance one). 2. **Apply Standard PCA:** - Implement PCA on the dataset and reduce it to 2 dimensions. - Plot the results in a 2D scatter plot where each point is colored by its class label. 3. **Apply Incremental PCA:** - Implement IncrementalPCA on the dataset and reduce it to 2 dimensions. - Plot the results in a 2D scatter plot where each point is colored by its class label. 4. **Apply Kernel PCA:** - Implement KernelPCA with an RBF kernel and reduce the dataset to 2 dimensions. - Plot the results in a 2D scatter plot where each point is colored by its class label. 5. **Comparison and Interpretation:** - Compare the scatter plots from each PCA variant. - Discuss the differences in the plots and provide an interpretation of when one method might be preferred over the others. **Requirements:** - Implement your solution using Python and the scikit-learn library. - Ensure that your code is well-commented and that you provide clear labels and titles for your plots. - Conclude with your interpretation of the results. **Input:** None. The dataset should be loaded within the script using scikit-learn. **Output:** The script should output three scatter plots for the PCA, IncrementalPCA, and KernelPCA results, as well as a brief analysis of the comparison. ```python # Sample structure import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA from sklearn.preprocessing import StandardScaler # Step 1. Load and standardize the dataset iris = datasets.load_iris() X = iris.data y = iris.target X_standardized = StandardScaler().fit_transform(X) # Step 2. Standard PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_standardized) plt.figure() plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y) plt.title(\'Standard PCA\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.show() # Step 3. Incremental PCA ipca = IncrementalPCA(n_components=2, batch_size=10) X_ipca = ipca.fit_transform(X_standardized) plt.figure() plt.scatter(X_ipca[:, 0], X_ipca[:, 1], c=y) plt.title(\'Incremental PCA\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.show() # Step 4. Kernel PCA kpca = KernelPCA(n_components=2, kernel=\'rbf\') X_kpca = kpca.fit_transform(X_standardized) plt.figure() plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y) plt.title(\'Kernel PCA with RBF Kernel\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.show() # Step 5. Comparison and Interpretation # Add your analysis here comparing the plots ``` *Constraints:* - You are expected to use only the scikit-learn library for PCA and its variants. - Ensure that your plots are clear and well-labeled. *Performance Considerations:* - The dataset size is small, so performance considerations are minimal in this context. However, discuss briefly how IncrementalPCA might be more suitable for larger datasets.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA from sklearn.preprocessing import StandardScaler def load_and_standardize_iris_data(): Load the Iris dataset and standardize it by removing the mean and scaling to unit variance. iris = datasets.load_iris() X = iris.data y = iris.target X_standardized = StandardScaler().fit_transform(X) return X_standardized, y def apply_and_plot_pca(X, y): Apply standard PCA to reduce the dataset to 2 dimensions and plot the results in a scatter plot. pca = PCA(n_components=2) X_pca = pca.fit_transform(X) plt.figure() plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y) plt.title(\'Standard PCA\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.colorbar(label=\'Class\') plt.show() def apply_and_plot_incremental_pca(X, y): Apply Incremental PCA to reduce the dataset to 2 dimensions and plot the results in a scatter plot. ipca = IncrementalPCA(n_components=2, batch_size=10) X_ipca = ipca.fit_transform(X) plt.figure() plt.scatter(X_ipca[:, 0], X_ipca[:, 1], c=y) plt.title(\'Incremental PCA\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.colorbar(label=\'Class\') plt.show() def apply_and_plot_kernel_pca(X, y): Apply Kernel PCA with an RBF kernel to reduce the dataset to 2 dimensions and plot the results in a scatter plot. kpca = KernelPCA(n_components=2, kernel=\'rbf\') X_kpca = kpca.fit_transform(X) plt.figure() plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y) plt.title(\'Kernel PCA with RBF Kernel\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.colorbar(label=\'Class\') plt.show() # Main functionality X_standardized, y = load_and_standardize_iris_data() apply_and_plot_pca(X_standardized, y) apply_and_plot_incremental_pca(X_standardized, y) apply_and_plot_kernel_pca(X_standardized, y) # Comparisons # Standard PCA is suitable for most purposes but assumes that data can be linearly transformed. # Incremental PCA is useful for large datasets where memory might be a concern. # Kernel PCA, especially with RBF, captures non-linear patterns and is useful when relationships between data points are non-linear."},{"question":"# Pandas Indexing and Selection Challenge You are provided with a DataFrame containing information about different employees in a company. Each row contains data for an employee, including their unique employee ID, name, age, department, and salary. Your task is to implement a function that performs various data retrieval and manipulation operations on this DataFrame. The function `manipulate_employee_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, int, pd.DataFrame, pd.DataFrame]` should perform the following operations: 1. **Salary Modification by Department**: - Increase the salary of all employees in the \'Engineering\' department by 10%. - Decrease the salary of all employees in the \'HR\' department by 5%. 2. **Age Restriction**: - Filter out all employees older than 60 years. 3. **Employee Count in Departments**: - Calculate the number of employees in each department that are still in the DataFrame after the age restriction filter (task 2). 4. **Top Earners in Each Department**: - Create a new DataFrame containing the top-earning employee from each department. The function should return a tuple containing: 1. The updated DataFrame after salary modification and age restriction. 2. The number of employees removed due to age restriction. 3. A DataFrame with the number of employees in each department after filtering. 4. A DataFrame with the top earner from each department. Input: - `df` (pd.DataFrame): A DataFrame with columns [\'EmployeeID\', \'Name\', \'Age\', \'Department\', \'Salary\']. Assume valid data types for each column. Output: - Tuple[pd.DataFrame, int, pd.DataFrame, pd.DataFrame] 1. Updated DataFrame after transformations. 2. Number of employees removed due to age restriction. 3. DataFrame with the count of employees in each department with columns [\'Department\', \'Count\']. 4. DataFrame with the top earner from each department with columns [\'Department\', \'EmployeeID\', \'Name\', \'Age\', \'Salary\']. Example: ```python import pandas as pd data = { \'EmployeeID\': [1, 2, 3, 4, 5], \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\'], \'Age\': [25, 45, 63, 30, 55], \'Department\': [\'Engineering\', \'HR\', \'Engineering\', \'HR\', \'Engineering\'], \'Salary\': [70000, 60000, 90000, 50000, 80000] } df = pd.DataFrame(data) result = manipulate_employee_data(df) # Expected output # Updated DataFrame, Number of removed employees, Employees count per department, Top earners DataFrame updated_df, removed_count, dept_count_df, top_earners_df = result print(updated_df) print(removed_count) print(dept_count_df) print(top_earners_df) ``` Constraints: - Assume the DataFrame always has the correct columns and valid types. - Focus on using pandas indexing and selection techniques efficiently. Write the function `manipulate_employee_data` to solve this problem.","solution":"import pandas as pd from typing import Tuple def manipulate_employee_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, int, pd.DataFrame, pd.DataFrame]: # Apply salary modification df.loc[df[\'Department\'] == \'Engineering\', \'Salary\'] *= 1.10 df.loc[df[\'Department\'] == \'HR\', \'Salary\'] *= 0.95 # Filter out employees older than 60 years initial_count = len(df) df = df[df[\'Age\'] <= 60] removed_count = initial_count - len(df) # Count employees per department after filtering dept_count_df = df[\'Department\'].value_counts().reset_index() dept_count_df.columns = [\'Department\', \'Count\'] # Get top earners in each department idx = df.groupby(\'Department\')[\'Salary\'].idxmax() top_earners_df = df.loc[idx].reset_index(drop=True) return df, removed_count, dept_count_df, top_earners_df"},{"question":"**Problem Statement:** You are tasked with implementing a function that takes two directory paths as input and returns a detailed summary of the comparison between the two directories, including the list of files only in each directory, files common to both but different in content, and files that caused errors during comparison. The function should also provide an option to print a detailed comparison report if requested. **Function Signature:** ```python def compare_directories(dir1: str, dir2: str, detailed_report: bool = False) -> dict: Compare two directories and return detailed comparison results. Parameters: - dir1: Path to the first directory. - dir2: Path to the second directory. - detailed_report: Boolean flag to print detailed comparison report (default is False). Returns: - A dictionary with the following structure: { \\"left_only\\": List of files only in dir1, \\"right_only\\": List of files only in dir2, \\"common_files_different\\": List of files common to both directories but with different content, \\"errors\\": List of files that caused errors during comparison, \\"detailed_report\\": String (if detailed_report is True, otherwise an empty string) } pass ``` **Constraints:** - Paths `dir1` and `dir2` are valid directory paths. - Consider only regular files for comparison. - Assume the necessary permissions to read the provided directories and their contents. **Requirements:** 1. Use the `filecmp` module for file comparisons. 2. Utilize the `dircmp` class to compare the directories and extract relevant information. 3. Implement the `detailed_report` option by leveraging the `report_full_closure` method of the `dircmp` class. 4. The function should handle exceptions gracefully and populate the \\"errors\\" list accordingly. **Example Usage:** ```python result = compare_directories(\'test_dir1\', \'test_dir2\', detailed_report=True) print(result) ``` **Expected Output:** The output should be a dictionary containing the comparison results, for example: ```python { \\"left_only\\": [\\"file_only_in_dir1.txt\\"], \\"right_only\\": [\\"file_only_in_dir2.txt\\"], \\"common_files_different\\": [\\"common_but_different.txt\\"], \\"errors\\": [\\"file_with_error.txt\\"], \\"detailed_report\\": \\"Detailed report as a string if `detailed_report` is True\\" } ``` Students should be able to demonstrate their understanding of file and directory comparison using the `filecmp` module, effectively handle file system paths, and manage optional detailed reporting.","solution":"import os import filecmp from typing import List def compare_directories(dir1: str, dir2: str, detailed_report: bool = False) -> dict: Compare two directories and return detailed comparison results. Parameters: - dir1: Path to the first directory. - dir2: Path to the second directory. - detailed_report: Boolean flag to print detailed comparison report (default is False). Returns: - A dictionary with the following structure: { \\"left_only\\": List of files only in dir1, \\"right_only\\": List of files only in dir2, \\"common_files_different\\": List of files common to both directories but with different content, \\"errors\\": List of files that caused errors during comparison, \\"detailed_report\\": String (if detailed_report is True, otherwise an empty string) } comparison_result = { \\"left_only\\": [], \\"right_only\\": [], \\"common_files_different\\": [], \\"errors\\": [], \\"detailed_report\\": \\"\\" } try: comparison = filecmp.dircmp(dir1, dir2) comparison_result[\\"left_only\\"] = comparison.left_only comparison_result[\\"right_only\\"] = comparison.right_only common_files_different = [file for file in comparison.diff_files if os.path.isfile(os.path.join(dir1, file)) and os.path.isfile(os.path.join(dir2, file))] comparison_result[\\"common_files_different\\"] = common_files_different if detailed_report: from io import StringIO import sys original_stdout = sys.stdout sys.stdout = StringIO() try: comparison.report_full_closure() comparison_result[\\"detailed_report\\"] = sys.stdout.getvalue() finally: sys.stdout = original_stdout except Exception as e: comparison_result[\\"errors\\"].append(str(e)) return comparison_result"},{"question":"Advanced GroupBy Operations Objective: Demonstrate a comprehensive understanding of pandas `groupby` operations, including aggregation, transformation, filtration, multi-level indexing, and handling of NA values. Problem Statement: You are provided with a dataset containing information about various sales transactions. Your task is to implement a function using pandas that performs the following operations: 1. **Group by columns**: - Group the data by `Store` and `Product`. - Calculate the **total revenue** for each group. - Calculate the **mean quantity** sold for each group. - Return the results as a DataFrame with the group keys as columns. 2. **Filter groups**: - Find groups where the **total revenue** is greater than a specified threshold. - Return these filtered groups. 3. **Transform groups**: - For each group, compute the **cumulative sum** of the `Revenue`. - Compute the **difference** in `Quantity` for consecutive rows within each group. - Add these results back as new columns `CumulativeRevenue` and `QuantityDiff` to the original DataFrame. 4. **Handle missing data**: - Identify groups that contain NA values in the `Quantity` column. - Impute missing `Quantity` values by filling them with the **median quantity** of each respective group. 5. **Multi-level indexing**: - With the transformed data, create a **MultiIndex** DataFrame such that both `Store` and `Date` are used as index levels. - Group by the `Date` level and calculate the **sum** of the `Revenue` for each date across all stores. Function Signature: ```python def advanced_groupby_operations(df: pd.DataFrame, revenue_threshold: float) -> pd.DataFrame: Perform advanced groupby operations on the provided sales data. Parameters: df (pd.DataFrame): Input dataframe containing sales data with columns \'Store\', \'Product\', \'Date\', \'Revenue\', \'Quantity\'. revenue_threshold (float): Threshold for filtering groups based on total revenue. Returns: pd.DataFrame: DataFrame containing results of specified groupby operations. pass ``` Input: - A pandas DataFrame `df` with columns: `Store` (string), `Product` (string), `Date` (datetime), `Revenue` (float), `Quantity` (float). Output: - A pandas DataFrame containing the results of the specified groupby operations. Example: ```python data = { \'Store\': [\'Store_A\', \'Store_A\', \'Store_A\', \'Store_B\', \'Store_B\', \'Store_B\'], \'Product\': [\'Product_1\', \'Product_1\', \'Product_2\', \'Product_1\', \'Product_2\', \'Product_2\'], \'Date\': pd.to_datetime([\'2023-01-15\', \'2023-01-20\', \'2023-01-15\', \'2023-01-15\', \'2023-01-20\', \'2023-01-15\']), \'Revenue\': [120.0, 80.0, 90.0, 100.0, 150.0, 110.0], \'Quantity\': [10, 7, 5, 12, np.nan, 8] } df = pd.DataFrame(data) threshold = 100 result = advanced_groupby_operations(df, threshold) print(result) ``` Constraints: - Ensure optimal performance for dataframes with significant size (up to 1 million rows). - Handle missing values appropriately as described. - The implementation should be efficient and make use of pandas built-in functions whenever possible.","solution":"import pandas as pd import numpy as np def advanced_groupby_operations(df: pd.DataFrame, revenue_threshold: float) -> pd.DataFrame: # Group by Store and Product, calculate total revenue and mean quantity grouped = df.groupby([\'Store\', \'Product\']).agg( TotalRevenue=(\'Revenue\', \'sum\'), MeanQuantity=(\'Quantity\', \'mean\') ).reset_index() # Filter groups where total revenue is greater than the threshold filtered_groups = grouped[grouped[\'TotalRevenue\'] > revenue_threshold] # Transform groups: cumulative sum of Revenue and difference in Quantity df[\'CumulativeRevenue\'] = df.groupby([\'Store\', \'Product\'])[\'Revenue\'].cumsum() df[\'QuantityDiff\'] = df.groupby([\'Store\', \'Product\'])[\'Quantity\'].diff() # Handle missing data: fill NA in Quantity with the median of the respective group median_quantity = df.groupby([\'Store\', \'Product\'])[\'Quantity\'].transform(\'median\') df[\'Quantity\'].fillna(median_quantity, inplace=True) # Multi-level indexing with Store and Date, then group by Date and sum the Revenue df.set_index([\'Store\', \'Date\'], inplace=True) result = df.groupby(pd.Grouper(level=\'Date\')).agg({\'Revenue\': \'sum\'}).reset_index().rename(columns={\'Revenue\': \'TotalDailyRevenue\'}) return result"},{"question":"# Time Zone Converter with DST and Ambiguity Handling Problem Statement You are tasked with implementing a Python function that converts a given UTC datetime to another timezone\'s datetime, correctly handling Daylight Saving Time (DST) transitions and ambiguous times. Your function must use the `zoneinfo` module functionalities and implement the following features: 1. Convert the given UTC datetime to the specified IANA time zone. 2. Handle Daylight Saving Time transitions appropriately. 3. Determine and set the `fold` attribute during ambiguous transitions (such as the DST end). 4. Raise appropriate exceptions when the specified time zone is not found. Function Signature ```python from datetime import datetime from typing import Tuple def convert_to_timezone(dt_utc: datetime, timezone: str) -> Tuple[datetime, str]: Convert a given UTC datetime to another timezone\'s datetime using zoneinfo module. Parameters: dt_utc (datetime): The datetime in UTC to be converted. timezone (str): The IANA time zone key. Returns: Tuple[datetime, str]: A tuple where the first element is the converted datetime in the specified timezone and the second element is a string representing the time zone abbreviation (e.g., \'PST\', \'PDT\'). Raises: ValueError: If the input datetime is not in UTC. ZoneInfoNotFoundError: If the specified time zone cannot be found. pass ``` Constraints and Notes 1. The input datetime (`dt_utc`) will always be timezone-aware. You need to confirm that it is in UTC; otherwise, raise a `ValueError`. 2. You should use the `zoneinfo.ZoneInfo` class for time zone conversion. 3. Ensure to handle DST transitions such that the resulting time reflects the local standard or daylight time correctly. 4. For ambiguous datetimes (when clocks go backward), set the `fold` attribute appropriately to differentiate the times. 5. The function should return a tuple where the first element is a timezone-aware datetime object converted from UTC to the specified time zone, and the second element is the timezone abbreviation. 6. You need to handle exceptions properly, especially `ZoneInfoNotFoundError` for unavailable time zones. Example ```python from datetime import datetime, timezone # Example usage dt_utc = datetime(2021, 11, 7, 9, tzinfo=timezone.utc) timezone_str = \\"America/New_York\\" dt_local, tz_abbr = convert_to_timezone(dt_utc, timezone_str) print(dt_local) # 2021-11-07 04:00:00-05:00 or 2021-11-07 05:00:00-05:00 (considering \'fold\') print(tz_abbr) # \'EST\' ``` **Note:** The datetime instance displayed can vary based on the `fold` attribute during DST transitions. Additional Information - You can assume that the `zoneinfo` and `datetime` modules are available for use. - Ensure that your solution is efficient and manages large datasets if necessary. - The solution must not use any third-party libraries other than those in the standard library.","solution":"from datetime import datetime, timezone from zoneinfo import ZoneInfo, ZoneInfoNotFoundError from typing import Tuple def convert_to_timezone(dt_utc: datetime, timezone_str: str) -> Tuple[datetime, str]: Convert a given UTC datetime to another timezone\'s datetime using zoneinfo module. Parameters: dt_utc (datetime): The datetime in UTC to be converted. timezone (str): The IANA time zone key. Returns: Tuple[datetime, str]: A tuple where the first element is the converted datetime in the specified timezone and the second element is a string representing the time zone abbreviation (e.g., \'PST\', \'PDT\'). Raises: ValueError: If the input datetime is not in UTC. ZoneInfoNotFoundError: If the specified time zone cannot be found. if dt_utc.tzinfo != timezone.utc: raise ValueError(\\"The input datetime must be in UTC.\\") try: tz = ZoneInfo(timezone_str) except ZoneInfoNotFoundError as e: raise ZoneInfoNotFoundError(f\\"The specified time zone \'{timezone_str}\' cannot be found.\\") from e dt_local = dt_utc.astimezone(tz) # Determine the time zone abbreviation tz_abbr = dt_local.tzname() return (dt_local, tz_abbr)"},{"question":"# Seaborn Plot Styling Assessment **Objective:** Demonstrate your understanding of seaborn\'s styling functionalities by writing a function that generates and customizes plots using predefined styles and context managers. **Question:** Write a function `create_custom_plots(data, styles)` that takes: 1. `data`: A dictionary where keys are plot types (e.g., \\"barplot\\", \\"lineplot\\") and values are tuples containing list of x-values and y-values for the plot. 2. `styles`: A list of seaborn style names (e.g., \\"darkgrid\\", \\"whitegrid\\") to apply to each plot in sequence. The function should: - Create a plot for each entry in the `data` dictionary using the style specified in the respective index in `styles`. - If the number of styles is less than the number of plots, repeat the styles in sequence. - Include a title for each plot indicating the style used. - Display all generated plots using matplotlib\'s `plt.show()` function. **Constraints:** - The plot types can only be \\"barplot\\" and \\"lineplot\\". - The `styles` list should contain valid seaborn style names. - You may assume that the length of the lists for x-values and y-values are the same for each plot. **Expected Input and Output:** ```python data = { \\"barplot\\": ([1, 2, 3], [10, 20, 15]), \\"lineplot\\": ([1, 2, 3], [4, 5, 6]), \\"barplot\\": ([1, 2, 3], [5, 10, 8]) } styles = [\\"whitegrid\\", \\"darkgrid\\"] # Expected behavior: The function should create three plots. # The first barplot uses \\"whitegrid\\", the lineplot uses \\"darkgrid\\", # and the second barplot uses \\"whitegrid\\" again (looping style). create_custom_plots(data, styles) ``` **Performance Requirements:** Ensure the function completes execution within a reasonable time frame for up to 10 plots and does not raise exceptions for valid input. **Hints:** - Use `sns.axes_style` as a context manager to apply styles temporarily. - Use appropriate seaborn plot functions (`sns.barplot` and `sns.lineplot`) for the data. - Use matplotlib\'s `plt.title` to set the title of each plot. ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(data, styles): plot_types = { \\"barplot\\": sns.barplot, \\"lineplot\\": sns.lineplot } num_styles = len(styles) for i, (plot_type, (x_vals, y_vals)) in enumerate(data.items()): style = styles[i % num_styles] with sns.axes_style(style): plt.figure() plot_func = plot_types[plot_type] plot_func(x=x_vals, y=y_vals) plt.title(f\\"Plot type: {plot_type}, Style: {style}\\") plt.show() # Example usage: data = { \\"barplot\\": ([1, 2, 3], [10, 20, 15]), \\"lineplot\\": ([1, 2, 3], [4, 5, 6]), \\"barplot\\": ([1, 2, 3], [5, 10, 8]) } styles = [\\"whitegrid\\", \\"darkgrid\\"] create_custom_plots(data, styles) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(data, styles): Generate and customize plots using seaborn styles and context managers. Parameters: - data: dictionary where keys are plot types (e.g., \\"barplot\\", \\"lineplot\\") and values are tuples containing list of x-values and y-values for the plot. - styles: list of seaborn style names (e.g., \\"darkgrid\\", \\"whitegrid\\") to apply to each plot in sequence. The function creates and displays the plots, applying the styles in the given sequence. plot_types = { \\"barplot\\": sns.barplot, \\"lineplot\\": sns.lineplot } num_styles = len(styles) for i, (plot_type, (x_vals, y_vals)) in enumerate(data.items()): style = styles[i % num_styles] with sns.axes_style(style): plt.figure() plot_func = plot_types[plot_type] plot_func(x=x_vals, y=y_vals) plt.title(f\\"Plot type: {plot_type}, Style: {style}\\") plt.show()"},{"question":"# Bytes Object Manipulation **Objective:** Implement a function that manipulates Python bytes objects using various operations described in the provided documentation. This function will require you to: 1. Create a bytes object from a given string. 2. Concatenate multiple bytes objects. 3. Resize a bytes object. 4. Retrieve and return the internal representation of the bytes object as a string. **Function Signature:** ```python def manipulate_bytes(input_string: str, additional_strings: List[str], new_size: int) -> str: pass ``` **Input:** - `input_string` (str): A string to be converted into the base bytes object. - `additional_strings` (List[str]): A list of strings to be concatenated to the base bytes object. - `new_size` (int): The new size to resize the resulting bytes object. **Output:** - `str`: The internal string representation of the final bytes object. **Constraints:** - The `input_string` and elements in `additional_strings` should not be empty. - The `new_size` should be a positive integer less than or equal to the total length of the concatenated bytes object. - If any of the operations fail, raise an appropriate exception. **Instructions:** 1. Convert the `input_string` into a bytes object. 2. Concatenate each string in `additional_strings` to the bytes object created in step 1. 3. Resize the resulting bytes object to `new_size`. 4. Return the internal string representation of the resized bytes object. **Example:** ```python input_string = \\"hello\\" additional_strings = [\\"world\\", \\"python\\"] new_size = 8 result = manipulate_bytes(input_string, additional_strings, new_size) print(result) # Expected: \\"hellowo\\" ``` **Note:** You must handle potential errors and edge cases appropriately, ensuring that your function adheres to the constraints and performs efficiently.","solution":"def manipulate_bytes(input_string: str, additional_strings: list, new_size: int) -> str: Manipulates bytes objects as per the given specifications. Parameters: input_string (str): The base string to be converted to bytes. additional_strings (List[str]): List of additional strings to concatenate. new_size (int): The new size to resize the bytes object. Returns: str: The internal string representation of the final bytes object. if not input_string or any(not s for s in additional_strings) or new_size <= 0: raise ValueError(\\"Input strings must not be empty and new_size must be a positive integer.\\") # Step 1: Convert the input_string to a bytes object base_bytes = input_string.encode(\'utf-8\') # Step 2: Concatenate each additional string to the bytes object for additional_string in additional_strings: base_bytes += additional_string.encode(\'utf-8\') # Step 3: Resize the resulting bytes object to new_size resized_bytes = base_bytes[:new_size] # Step 4: Return the internal string representation of the resized bytes object return resized_bytes.decode(\'utf-8\')"},{"question":"<|Analysis Begin|> The provided documentation outlines the functionality and usage of the Unix \\"syslog\\" library in Python. The key components explained include: 1. **Functions**: - `syslog.syslog(message)`: Sends a message to the system logger with a default priority of \\"LOG_INFO\\". - `syslog.syslog(priority, message)`: Sends a message to the system logger with a specified priority. - `syslog.openlog([ident[, logoption[, facility]]])`: Sets logging options for subsequent `syslog` calls. - `syslog.closelog()`: Resets the syslog module values and calls the system library `closelog()`. - `syslog.setlogmask(maskpri)`: Sets the priority mask to filter which messages will be logged. 2. **Constants**: - Various priority levels and facilities, such as `LOG_EMERG`, `LOG_ALERT`, `LOG_CRIT`, `LOG_ERR`, `LOG_WARNING`, `LOG_INFO`, `LOG_DEBUG`, etc. - Log options such as `LOG_PID`, `LOG_CONS`, `LOG_NDELAY`, etc. 3. **Examples**: - Demonstrates simple logging and setting log options. To create a comprehensive and challenging question, we should focus on a task that involves multiple aspects of the syslog functionalities and constants. <|Analysis End|> <|Question Begin|> You are tasked with designing a Python utility to manage and log application processes using the Unix `syslog` library routines. # Your Task Implement a class `SyslogManager` with the following functionalities: 1. **Initialization**: - The class should initialize with optional parameters: `ident`, `logoption`, and `facility`. - Use the `syslog.openlog()` function to set the logging options based on provided parameters. 2. **Logging Messages**: - Implement a method `log_message(priority: int, message: str) -> None` that logs messages with a specific priority using `syslog.syslog(priority, message)`. 3. **Set Log Mask**: - Implement a method `set_log_mask(maskpri: int) -> int` that sets the log mask using `syslog.setlogmask(maskpri)` and returns the previous mask value. 4. **Close Log**: - Implement a method `close_log() -> None` that closes the log using `syslog.closelog()` and reinitializes the logging state. 5. **Helper Methods**: - Implement a method `log_info(message: str) -> None` that logs messages with `LOG_INFO` priority. - Implement a method `log_error(message: str) -> None` that logs messages with `LOG_ERR` priority. - Implement additional helper methods for other priority levels such as `log_warning`, `log_notice`, `log_debug`, etc. # Constraints - Ensure that all messages are tagged with the correct priority and facility. - Properly handle cases where no initial parameters are given (use defaults). # Example Usage Here\'s how your class should be used: ```python from syslog_manager import SyslogManager import syslog # Initialize the syslog manager with custom options manager = SyslogManager(ident=\\"MyApp\\", logoption=syslog.LOG_PID | syslog.LOG_CONS, facility=syslog.LOG_USER) # Log a simple info message manager.log_info(\\"Application started\\") # Log an error message manager.log_error(\\"An error occurred\\") # Set the log mask to only log errors and above previous_mask = manager.set_log_mask(syslog.LOG_MASK(syslog.LOG_ERR)) # Log a debug message (this will not be logged due to the mask) manager.log_debug(\\"This is a debug message\\") # Close the log manager.close_log() ``` Make sure to include sufficient error handling and ensure the code\'s robustness.","solution":"import syslog class SyslogManager: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): self.ident = ident self.logoption = logoption self.facility = facility syslog.openlog(ident=ident, logoption=logoption, facility=facility) def log_message(self, priority: int, message: str) -> None: syslog.syslog(priority, message) def set_log_mask(self, maskpri: int) -> int: return syslog.setlogmask(maskpri) def close_log(self) -> None: syslog.closelog() def log_info(self, message: str) -> None: self.log_message(syslog.LOG_INFO, message) def log_error(self, message: str) -> None: self.log_message(syslog.LOG_ERR, message) def log_warning(self, message: str) -> None: self.log_message(syslog.LOG_WARNING, message) def log_notice(self, message: str) -> None: self.log_message(syslog.LOG_NOTICE, message) def log_debug(self, message: str) -> None: self.log_message(syslog.LOG_DEBUG, message)"},{"question":"**Question: Implement an Annotation Retrieval Utility** # Background In Python, annotations provide a way of associating arbitrary metadata with functions, classes, methods, and modules. With Python 3.10, the `inspect.get_annotations()` function was introduced to help access these annotations more easily and reliably. Your task is to implement a utility function called `retrieve_annotations` that retrieves the annotations of a given object in a manner compatible with both Python 3.9 (and older) and Python 3.10 (and newer). Additionally, this function should handle cases where annotations might be stringized (i.e., represented as strings that need to be evaluated). # Function Specification ```python def retrieve_annotations(obj) -> dict: Retrieves the annotations for the given object. Parameters: obj (Any): The object whose annotations need to be retrieved. Returns: dict: A dictionary containing the annotations of the object, or an empty dictionary if no annotations are found. If stringized annotations are present, they should be evaluated back to their original types wherever possible. ``` # Requirements 1. **Object Types:** - The function should handle classes, functions, methods, and modules. 2. **Annotations Retrieval:** - For Python 3.10 and newer, use `inspect.get_annotations()` to retrieve the annotations. - For Python 3.9 and older, use the appropriate methods to retrieve annotations: - For classes, search the class dictionary using `__dict__.get(\'__annotations__\', None)`. - For other objects, use `getattr(obj, \'__annotations__\', None)` with care. 3. **Un-stringizing Annotations:** - If annotations are stringized, attempt to evaluate them back to their original form using appropriate `globals` and `locals` contexts. 4. **Handling Edge Cases:** - If the object does not support annotations or annotations are not set, return an empty dictionary. - Ensure that `__annotations__` is a dictionary before proceeding. - Prefer safety and correctness, avoiding exceptions where possible. # Constraints - Do not utilize any external packages other than the Python standard library. - The function should not mutate the object or its annotations. # Examples ```python # Example 1: Function with annotations (Python 3.10+) def foo(a: int, b: str) -> bool: pass print(retrieve_annotations(foo)) # Output: {\'a\': int, \'b\': str, \'return\': bool} # Example 2: Class with inherited annotations (Python 3.9) class Base: a: int class Derived(Base): b: str print(retrieve_annotations(Derived)) # Output: {\'b\': str} # Example 3: Module with stringized annotation (Python 3.10+) # Assume my_module.py contains: # from __future__ import annotations # var1: \\"int\\" # var2: str = \\"example\\" import my_module print(retrieve_annotations(my_module)) # Output: {\'var1\': int, \'var2\': <class \'str\'>} ``` **Note:** Test the function across different Python versions to ensure compatibility and correctness.","solution":"import inspect import sys def retrieve_annotations(obj) -> dict: annotations = {} # For Python 3.10+, use inspect.get_annotations if sys.version_info >= (3, 10): annotations = inspect.get_annotations(obj) else: # For Python 3.9 and earlier if isinstance(obj, type): # Class type annotations = obj.__dict__.get(\'__annotations__\', {}) elif callable(obj): # Function or method annotations = getattr(obj, \'__annotations__\', {}) elif isinstance(obj, types.ModuleType): # Module type annotations = getattr(obj, \'__annotations__\', {}) # Ensure annotations is a dictionary if not isinstance(annotations, dict): return {} # Evaluate stringized annotations if needed evaluated_annotations = {} for key, value in annotations.items(): if isinstance(value, str): try: evaluated_annotations[key] = eval(value, globals(), locals()) except Exception: evaluated_annotations[key] = value # keep as string if eval fails else: evaluated_annotations[key] = value return evaluated_annotations"},{"question":"# Question: Implementing and Training a BernoulliRBM **Objective:** You are required to implement a function that initializes and trains a Bernoulli Restricted Boltzmann Machine (BernoulliRBM) using the scikit-learn library. This task will assess your understanding of the RBM model, its assumptions, and the training algorithm. **Function Signature:** ```python def train_rbm(data: np.ndarray, n_components: int, learning_rate: float, n_iter: int) -> BernoulliRBM: pass ``` **Input:** - `data` (np.ndarray): A 2D numpy array of shape (n_samples, n_features) containing the dataset. The data should be either binary or real-valued between 0 and 1. - `n_components` (int): The number of binary hidden units. - `learning_rate` (float): The learning rate for weight updates during training. - `n_iter` (int): The number of iterations for the training process. **Output:** - Returns an instance of `BernoulliRBM` after training on the provided data. **Constraints:** - You must use the scikit-learn library\'s `BernoulliRBM` class for this implementation. - The input data values must be either 0 or 1, or interpreted as probabilities. **Example Usage:** ```python import numpy as np from sklearn.neural_network import BernoulliRBM # Example data data = np.array([ [0, 1, 1, 0], [1, 0, 1, 1], [0, 0, 1, 0], [1, 1, 0, 1] ]) # Training the RBM rbm_model = train_rbm(data, n_components=2, learning_rate=0.1, n_iter=10) # rbm_model is an instance of BernoulliRBM trained on the provided data print(rbm_model.components_) ``` **Instructions:** 1. Create the `train_rbm` function according to the provided signature. 2. Initialize a `BernoulliRBM` object with the given parameters. 3. Fit the RBM model to the provided data using the `fit` method. 4. Return the trained `BernoulliRBM` instance. Ensure your code handles the input data correctly, adheres to the constraints, and is efficient in performance.","solution":"import numpy as np from sklearn.neural_network import BernoulliRBM def train_rbm(data: np.ndarray, n_components: int, learning_rate: float, n_iter: int) -> BernoulliRBM: Initializes and trains a Bernoulli Restricted Boltzmann Machine (BernoulliRBM). Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) containing the dataset. n_components (int): The number of binary hidden units. learning_rate (float): The learning rate for weight updates during training. n_iter (int): The number of iterations for the training process. Returns: BernoulliRBM: Trained RBM model. rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, n_iter=n_iter, verbose=True) rbm.fit(data) return rbm"},{"question":"**Title:** Implementation and Usage of \\"Cell-like\\" Objects in Python **Objective:** Design a class in Python that mimics some of the functionalities of Python Cell objects as described in the provided documentation. This class should allow the sharing and updating of values across different scopes. **Description:** Create a class `Cell` which holds a single value that can be shared and updated across different instances. Implement methods to get, set, and check the value held by the cell object. You should also handle invalid operations gracefully. # Requirements: 1. **Class Definition:** - Define a class named `Cell`. - The class should have an initializer `__init__` that takes an initial value. 2. **Methods:** - `is_cell(obj)`: This static method should return `True` if the `obj` is an instance of `Cell`. - `get(self)`: This method should return the value stored in the cell. - `set(self, value)`: This method should set the value stored in the cell to `value`. 3. **Edge Case Handling:** - If `get` is called on an uninitialized cell (one with `None` value), return `None`. - Ensure that `set` can handle being set to `None`. # Input and Output Format: - **Initialization:** ```python cell = Cell(initial_value) ``` - **Methods:** ```python cell.set(new_value) retrieved_value = cell.get() is_cell = Cell.is_cell(cell_instance) ``` **Example Usage:** ```python # Create a new cell with initial value 10 cell = Cell(10) # Change the cell value to 20 cell.set(20) # Retrieve the current value of the cell val = cell.get() # val should be 20 # Check if an object is an instance of Cell is_cell = Cell.is_cell(cell) # should return True is_cell = Cell.is_cell(100) # should return False ``` # Constraints: - The class should handle only simple data types for the values (e.g., integers, strings, lists). - Aim to keep the class implementation efficient and clear. - Assume that the `set` method will not be dealing with very large inputs (like gigabytes of data). Make sure to test your implementation with various scenarios to ensure it behaves as expected.","solution":"class Cell: def __init__(self, initial_value=None): self.value = initial_value @staticmethod def is_cell(obj): return isinstance(obj, Cell) def get(self): return self.value def set(self, value): self.value = value"},{"question":"# Asynchronous Networking with Asyncio You are tasked with creating a simple asynchronous TCP echo server using Python\'s asyncio module. The server should be capable of handling multiple client connections simultaneously, echoing back any received messages to the sender. Implement the following function: ```python import asyncio async def echo_server(host: str, port: int): Start an asynchronous TCP echo server. Args: - host (str): The host/IP address for the server. - port (int): The port number for the server. The server should handle multiple client connections concurrently and echo back any received messages. # Your code here async def main(): host = \'127.0.0.1\' port = 8888 await echo_server(host, port) if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Requirements: 1. Use `asyncio.start_server` to create the server. 2. Implement a coroutine to handle individual client connections. 3. The server should be able to handle multiple clients concurrently. 4. Echo each message received back to the respective client. # Example Workflow: 1. Start the server using the provided `main` function. 2. Connect to the server using a TCP client (e.g., `telnet 127.0.0.1 8888`). 3. Type a message and press Enter. The server should echo the message back to you. # Constraints: - Use the `asyncio` library functionalities described in the given documentation for handling event loops, creating servers, and managing tasks. - Ensure proper handling of client disconnections and server shutdowns. # Performance Considerations: - The server should maintain responsiveness even when handling multiple client connections. - Consider efficient uses of `asyncio` primitives to avoid blocking operations.","solution":"import asyncio async def handle_client(reader, writer): Coroutine to handle client connections. try: while True: data = await reader.read(100) if not data: break writer.write(data) await writer.drain() except asyncio.CancelledError: pass finally: writer.close() await writer.wait_closed() async def echo_server(host: str, port: int): Start an asynchronous TCP echo server. server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() async def main(): host = \'127.0.0.1\' port = 8888 await echo_server(host, port) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Question: Warning Customization and Control using Python\'s `warnings` Module** You have been tasked with improving a legacy codebase of a Python application. The application is riddled with deprecated functions, resource mismanagement, and several other issues that raise warnings. Your goal is to gain fine-grained control over these warnings, ensuring that: 1. Warnings about deprecated functions are turned into exceptions. 2. Unicode warnings are logged but ignored. 3. A particular function\'s resource warnings are suppressed temporarily during its execution. 4. General warnings are caught and can be tested for specific conditions. **Requirements:** 1. Implement a function `handle_warnings` that sets the necessary warning filters. 2. Implement a function `suppress_specific_warning` that temporarily suppresses `ResourceWarning` using a context manager. 3. Write a test function `test_warnings` to verify that the warnings behave as expected based on the conditions given. **Details:** 1. **Function 1: `handle_warnings()`** - This function takes no parameters. - Use the `warnings` module to: - Turn `DeprecationWarning` into an error. - Ignore `UnicodeWarning` but ensure they are logged. - Set the default filter for other warnings for handling during tests. ```python def handle_warnings(): pass # Implement this function ``` 2. **Function 2: `suppress_specific_warning(fxn: Callable, *args, **kwargs)`** - This function accepts another function `fxn` and its arguments. - Temporarily suppress `ResourceWarning` during the execution of `fxn`. - Use `warnings.catch_warnings()` context manager. - Return the result of `fxn`. ```python def suppress_specific_warning(fxn, *args, **kwargs): pass # Implement this function ``` 3. **Function 3: `test_warnings()`** - This function takes no parameters. - It should test that your `handle_warnings` function and `suppress_specific_warning` function work correctly. - Specifically, it should: - Ensure that `DeprecationWarning` raises an error. - Ensure that `UnicodeWarning` is ignored but logged. - Ensure that `ResourceWarning` is suppressed when executing within `suppress_specific_warning`. ```python def test_warnings(): pass # Implement this function ``` **Expected Input and Output:** - `handle_warnings()`: No input, sets up the warning filters. No explicit output. - `suppress_specific_warning(fxn: Callable, *args, **kwargs)`: Takes a function and its arguments, suppresses `ResourceWarning` during its execution. Returns the result of the function `fxn`. - `test_warnings()`: No input, tests the functionality of the warning handling setup. Prints assertion results if any failure. **Additional Constraints:** - Do not use any third-party packages; only use Python standard libraries. - Ensure that `test_warnings` does not produce unnecessary warnings or outputs during regular execution. **Note:** Make sure to read the Python `warnings` module documentation to understand the proper usage of its functions and context managers.","solution":"import warnings def handle_warnings(): Set up warning filters: - Convert DeprecationWarning to errors. - Ignore (but log) UnicodeWarning. # Turn DeprecationWarning into an error warnings.filterwarnings(\'error\', category=DeprecationWarning) # Ignore (but log) UnicodeWarning warnings.filterwarnings(\'ignore\', category=UnicodeWarning) def suppress_specific_warning(fxn, *args, **kwargs): Temporarily suppress ResourceWarning during the execution of fxn. Parameters: fxn: Callable - The function to execute args: tuple - Positional arguments for the function kwargs: dict - Keyword arguments for the function Returns: The result of the function fxn. with warnings.catch_warnings(): # Suppress ResourceWarning warnings.simplefilter(\'ignore\', category=ResourceWarning) result = fxn(*args, **kwargs) return result # Example function that raises a ResourceWarning def example_function(): warnings.warn(\\"This is a resource warning.\\", ResourceWarning) def test_warnings(): Tests that the warning handling setup behaves as expected. handle_warnings() # Test DeprecationWarning raising an error try: warnings.warn(\\"This is a deprecation warning.\\", DeprecationWarning) assert False, \\"DeprecationWarning did not raise an error\\" except DeprecationWarning: pass # Test UnicodeWarning being ignored with warnings.catch_warnings(record=True) as w: warnings.warn(\\"This is a unicode warning.\\", UnicodeWarning) assert len(w) == 0, \\"UnicodeWarning was not ignored\\" # Test suppression of ResourceWarning with warnings.catch_warnings(record=True) as w: suppress_specific_warning(example_function) assert len(w) == 0, \\"ResourceWarning was not suppressed\\""},{"question":"# Seaborn Distribution Visualization Assessment Problem Statement You are provided with a dataset of your choice. Your task is to implement a set of functions to visualize the distribution of data using Seaborn. Requirements 1. **Data Loading**: - Write a function `load_data(filepath: str) -> pd.DataFrame` to load a dataset from a CSV file. 2. **Univariate Distribution**: - Write a function `plot_univariate_histogram(data: pd.DataFrame, column: str, bins: int = 10) -> None` to plot a univariate histogram of a given column in the dataset with a specified number of bins. - Write a function `plot_univariate_kde(data: pd.DataFrame, column: str, bw_adjust: float = 1) -> None` to plot a kernel density estimation (KDE) of a given column in the dataset with an adjustable bandwidth. 3. **Bivariate Distribution**: - Write a function `plot_bivariate_histogram(data: pd.DataFrame, x_col: str, y_col: str, binwidth_x: float, binwidth_y: float) -> None` to plot a bivariate histogram. - Write a function `plot_bivariate_kde(data: pd.DataFrame, x_col: str, y_col: str, bw_adjust_x: float = 1, bw_adjust_y: float = 1) -> None` to plot a bivariate kernel density estimation. 4. **Conditional Distribution**: - Write a function `plot_conditional_histogram(data: pd.DataFrame, column: str, category: str) -> None` to plot histograms of a specific column conditioned on the unique values of another categorical column using different colors. - Write a function `plot_conditional_kde(data: pd.DataFrame, column: str, category: str) -> None` to plot KDEs of a specific column conditioned on the unique values of another categorical column. 5. **ECDF Plot**: - Write a function `plot_ecdf(data: pd.DataFrame, column: str) -> None` to plot the empirical cumulative distribution function (ECDF) of a given column. Input and Output Formats - All functions take a Pandas DataFrame as input along with specific parameters mentioned above. - All functions should display the plot directly without returning any value. Constraints - The dataset should have at least two numeric columns and one categorical column. - For KDE plots, the `bw_adjust` parameter should be a positive float. Performance Requirements - Functions should be optimized for readability and maintainability. - Plots should be properly labeled with titles and axis labels for clarity. Example ```python import seaborn as sns import pandas as pd # Example implementation for data loading def load_data(filepath: str) -> pd.DataFrame: return pd.read_csv(filepath) # Example usage data = load_data(\'path_to_your_dataset.csv\') plot_univariate_histogram(data, \'numeric_column\', bins=20) plot_univariate_kde(data, \'numeric_column\', bw_adjust=0.75) plot_bivariate_histogram(data, \'numeric_column1\', \'numeric_column2\', binwidth_x=5, binwidth_y=0.5) plot_bivariate_kde(data, \'numeric_column1\', \'numeric_column2\', bw_adjust_x=1.5, bw_adjust_y=0.5) plot_conditional_histogram(data, \'numeric_column\', \'categorical_column\') plot_conditional_kde(data, \'numeric_column\', \'categorical_column\') plot_ecdf(data, \'numeric_column\') ``` Ensure to handle any necessary exceptions, particularly for loading the data and invalid input parameters.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_data(filepath: str) -> pd.DataFrame: Loads a CSV dataset from the given file path. Args: - filepath (str): Path to the CSV file to load. Returns: - pd.DataFrame: DataFrame containing the loaded data. return pd.read_csv(filepath) def plot_univariate_histogram(data: pd.DataFrame, column: str, bins: int = 10) -> None: Plots a univariate histogram of the specified column in the DataFrame. Args: - data (pd.DataFrame): DataFrame containing the data. - column (str): The column name for which to plot the histogram. - bins (int): Number of bins to use for the histogram. plt.figure(figsize=(10, 6)) sns.histplot(data[column], bins=bins) plt.title(f\'Histogram of {column}\') plt.xlabel(column) plt.ylabel(\'Frequency\') plt.show() def plot_univariate_kde(data: pd.DataFrame, column: str, bw_adjust: float = 1) -> None: Plots a kernel density estimation (KDE) of the specified column in the DataFrame. Args: - data (pd.DataFrame): DataFrame containing the data. - column (str): The column name for which to plot the KDE. - bw_adjust (float): Adjustment factor for the bandwidth of the KDE. plt.figure(figsize=(10, 6)) sns.kdeplot(data[column], bw_adjust=bw_adjust) plt.title(f\'KDE of {column}\') plt.xlabel(column) plt.ylabel(\'Density\') plt.show() def plot_bivariate_histogram(data: pd.DataFrame, x_col: str, y_col: str, binwidth_x: float, binwidth_y: float) -> None: Plots a bivariate histogram of the specified columns in the DataFrame. Args: - data (pd.DataFrame): DataFrame containing the data. - x_col (str): The column name for the x-axis. - y_col (str): The column name for the y-axis. - binwidth_x (float): Bin width for the x-axis. - binwidth_y (float): Bin width for the y-axis. plt.figure(figsize=(10, 6)) sns.histplot(data, x=x_col, y=y_col, binwidth=(binwidth_x, binwidth_y)) plt.title(f\'Bivariate Histogram of {x_col} and {y_col}\') plt.xlabel(x_col) plt.ylabel(y_col) plt.show() def plot_bivariate_kde(data: pd.DataFrame, x_col: str, y_col: str, bw_adjust_x: float = 1, bw_adjust_y: float = 1) -> None: Plots a bivariate kernel density estimation (KDE) of the specified columns in the DataFrame. Args: - data (pd.DataFrame): DataFrame containing the data. - x_col (str): The column name for the x-axis. - y_col (str): The column name for the y-axis. - bw_adjust_x (float): Adjustment factor for the bandwidth of the KDE along the x-axis. - bw_adjust_y (float): Adjustment factor for the bandwidth of the KDE along the y-axis. plt.figure(figsize=(10, 6)) sns.kdeplot(x=data[x_col], y=data[y_col], bw_adjust=(bw_adjust_x, bw_adjust_y)) plt.title(f\'Bivariate KDE of {x_col} and {y_col}\') plt.xlabel(x_col) plt.ylabel(y_col) plt.show() def plot_conditional_histogram(data: pd.DataFrame, column: str, category: str) -> None: Plots histograms of the specified column conditioned on the unique values of another category column. Args: - data (pd.DataFrame): DataFrame containing the data. - column (str): The column name for which to plot the histogram. - category (str): The categorical column to condition on. plt.figure(figsize=(10, 6)) sns.histplot(data, x=column, hue=category, multiple=\\"stack\\") plt.title(f\'Conditional Histogram of {column} by {category}\') plt.xlabel(column) plt.ylabel(\'Frequency\') plt.show() def plot_conditional_kde(data: pd.DataFrame, column: str, category: str) -> None: Plots KDEs of the specified column conditioned on the unique values of another category column. Args: - data (pd.DataFrame): DataFrame containing the data. - column (str): The column name for which to plot the KDE. - category (str): The categorical column to condition on. plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=column, hue=category) plt.title(f\'Conditional KDE of {column} by {category}\') plt.xlabel(column) plt.ylabel(\'Density\') plt.show() def plot_ecdf(data: pd.DataFrame, column: str) -> None: Plots the empirical cumulative distribution function (ECDF) of the specified column in the DataFrame. Args: - data (pd.DataFrame): DataFrame containing the data. - column (str): The column name for which to plot the ECDF. plt.figure(figsize=(10, 6)) sns.ecdfplot(data[column]) plt.title(f\'ECDF of {column}\') plt.xlabel(column) plt.ylabel(\'ECDF\') plt.show()"},{"question":"Quantization Aware Training with Custom Module # Objective Implement a PyTorch model and prepare it for Quantization Aware Training (QAT). You will need to wrap your model, fuse modules, and apply the necessary quantization configurations. # Problem Statement 1. Create a simple feed-forward neural network with the following structure: - An input layer with 128 units. - A hidden layer with 64 units and ReLU activation. - An output layer with 10 units (for classification purposes, e.g., MNIST digits). 2. Implement the fusion of modules where applicable. Use the `fuse_modules` function for fusing layers. 3. Prepare the model for QAT using appropriate quantization configurations. 4. Convert the quantized model for inference. 5. Demonstrate quantization and dequantization of a sample input tensor using the quantized model. # Input Format - A sample input tensor: `torch.rand((1, 128))`. # Output Format - Output after passing the sample input through the quantized model. - Output after dequantizing the results. # Constraints - Use the PyTorch `torch.ao.quantization` module for all quantization steps. - Strictly use the QAT process for training preparation. - Ensure module fusion is correctly applied where applicable. # Performance Requirements - Operations should follow the typical efficiency of PyTorch quantized models. # Example ```python import torch import torch.nn as nn import torch.ao.quantization as tq # Define the model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(128, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 10) def forward(self, x): x = self.relu(self.fc1(x)) x = self.fc2(x) return x # Initialize and fuse the model model = SimpleNN() model.train() # Fuse modules model_fused = tq.fuse_modules(model, [[\'fc1\', \'relu\']]) # Prepare for QAT model_prepared = tq.prepare_qat(model_fused, inplace=False) # Simulate training (example purpose; actual training code should follow) model_prepared.eval() # Convert model to quantized version model_quantized = tq.convert(model_prepared, inplace=False) # Define a sample input sample_input = torch.rand((1, 128)) # Quantize inputs q_input = tq.quantize_per_tensor(sample_input, scale=0.1, zero_point=128, dtype=torch.quint8) # Pass the input through the quantized model quantized_output = model_quantized(q_input) # Dequantize the output output = tq.dequantize(quantized_output) print(output) ``` In this task, you\'re expected to follow the steps outlined and ensure you understand the quantization process in PyTorch, from model preparation to applying QAT and inference using quantized models.","solution":"import torch import torch.nn as nn import torch.ao.quantization as tq # Define the model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(128, 64) self.relu = nn.ReLU() self.fc2 = nn.Linear(64, 10) self.quant_stub = tq.QuantStub() self.dequant_stub = tq.DeQuantStub() def forward(self, x): x = self.quant_stub(x) x = self.relu(self.fc1(x)) x = self.fc2(x) x = self.dequant_stub(x) return x # Initialize the model model = SimpleNN() # Fuse modules model = tq.fuse_modules(model, [[\'fc1\', \'relu\']], inplace=True) # Prepare for QAT model.train() model.qconfig = tq.get_default_qat_qconfig(\'fbgemm\') tq.prepare_qat(model, inplace=True) def prepare_quantized_model(model, input_tensor): model.eval() # Convert model to quantized version model_quantized = tq.convert(model, inplace=False) # Quantize inputs q_input = tq.QuantStub()(input_tensor) # Pass the input through the quantized model quantized_output = model_quantized(q_input) # Dequantize the output output = tq.DeQuantStub()(quantized_output) return output # Example of how the output is calculated during inference sample_input = torch.rand((1, 128)) output = prepare_quantized_model(model, sample_input) print(output)"},{"question":"# XML Processing in Python Objective: Write a Python function that processes an XML string to retrieve specific information, while ensuring the security considerations mentioned in the documentation are taken into account. Problem Description: You are given an XML string representing a list of `book` elements, each containing `title`, `author`, and `year` sub-elements. Your task is to implement a function `process_books_xml(xml_string: str) -> List[Tuple[str, str, int]]` which parses the XML string and returns a list of tuples, each containing the title, author as strings and year as an integer of the books. Additionally, your code should handle potential security vulnerabilities like exponential entity expansion (Billion Laughs attack) and prevent resolution of external entities. Function Signature: ```python from typing import List, Tuple def process_books_xml(xml_string: str) -> List[Tuple[str, str, int]]: pass ``` Input: - `xml_string` (str): A string containing the XML data of books. Output: - List of tuples: Each tuple contains: - `title` (str): Title of the book. - `author` (str): Author of the book. - `year` (int): Year the book was published. Constraints: - The XML string will contain well-formed XML. - Each `book` element will have exactly one `title`, `author`, and `year` sub-elements. - You must ensure that your code is not vulnerable to known XML vulnerabilities described in the provided documentation. Example: ```python xml_data = \'\'\'<library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> </book> </library>\'\'\' process_books_xml(xml_data) ``` Expected Output: ```python [ (\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925), (\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960) ] ``` Note: - Use `xml.etree.ElementTree` for parsing, as it is mentioned to handle some vulnerabilities internally. - Ensure your solution uses secure parsing techniques as per the provided documentation. Requirements: - Implement the function `process_books_xml` according to the above specification. - Ensure your solution prevents any potential XML-related security issues.","solution":"import xml.etree.ElementTree as ET from typing import List, Tuple def process_books_xml(xml_string: str) -> List[Tuple[str, str, int]]: Processes an XML string to retrieve book information. Args: xml_string (str): A string containing the XML data of books. Returns: List[Tuple[str, str, int]]: A list of tuples, each containing the title, author, and year of a book. # Disable the resolution of external entities to prevent XXE attacks. parser = ET.XMLParser() parser.entity.update({}) # Disables external entity references root = ET.fromstring(xml_string, parser=parser) books = [] for book in root.findall(\'book\'): title = book.find(\'title\').text author = book.find(\'author\').text year = int(book.find(\'year\').text) books.append((title, author, year)) return books"},{"question":"# Asynchronous Server with Concurrency and Proper Exception Handling You are tasked with creating an asynchronous server that handles incoming connections from clients. The server should be able to handle multiple clients concurrently while ensuring that blocking operations do not impede the performance of the event loop. Additionally, your implementation should include proper error handling to ensure exceptions are correctly propagated and logged. Requirements 1. **Server Functionality**: - Create an asynchronous server function `async def start_server(host: str, port: int) -> None`. - The server should listen for incoming connections and handle each connection in a separate task. 2. **Connection Handling**: - Create an asynchronous function `async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None`. - This function should read data from the client, process the data, and send a response back to the client. - Simulate processing with a CPU-bound operation. Use `loop.run_in_executor()` to handle this without blocking the event loop. 3. **Error Handling**: - Ensure that any exceptions raised during connection handling are properly logged. - If a task encounters an exception, the task should log the exception without terminating the server. 4. **Debug Mode and Logging**: - Enable asyncio\'s debug mode. - Set the log level of the asyncio logger to `logging.DEBUG`. Input and Output Formats - The `start_server` function takes the following parameters: - `host`: The hostname or IP address to bind the server to (e.g., \'localhost\'). - `port`: The port number to listen on (e.g., 8888). - The `handle_client` function does not directly take input from the user but interacts with the connected clients through the network. Constraints - Use asyncio for asynchronous operations. - Implement proper logging and exception handling to display any unawaited coroutines or unhandled exceptions. - Ensure that blocking operations do not block the event loop by using `run_in_executor`. Example Usage Below is an example of how the server functions could be used: ```python import asyncio import logging # Setting up logging logging.basicConfig(level=logging.DEBUG) async def simulate_cpu_bound_operation(data: bytes) -> str: # Simulate a CPU-bound task await asyncio.sleep(2) # Simulate some delay return data.decode(\'utf-8\')[::-1] # Reverse the string as a simple operation async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: try: # Read data from client data = await reader.read(100) if not data: return logging.info(f\'Received {data!r}\') # Simulate processing loop = asyncio.get_event_loop() result = await loop.run_in_executor(None, simulate_cpu_bound_operation, data) # Send response back to client writer.write(result.encode(\'utf-8\')) await writer.drain() logging.info(f\'Sent {result!r}\') except Exception as e: logging.error(f\'Error handling client: {e}\') finally: writer.close() await writer.wait_closed() async def start_server(host: str, port: int) -> None: server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() # Entry point if __name__ == \'__main__\': asyncio.run(start_server(\'localhost\', 8888), debug=True) ``` # Notes - Ensure to handle exceptions properly and use the appropriate logging levels. - The implementation should follow asyncio best practices to avoid common pitfalls like blocking the event loop or forgetting to await coroutines.","solution":"import asyncio import logging # Setting up logging logging.basicConfig(level=logging.DEBUG) async def simulate_cpu_bound_operation(data: bytes) -> str: Simulate a CPU-bound task. The operation will reverse the string as a simple simulation. # Simulate computation await asyncio.sleep(2) # Simulate some delay return data.decode(\'utf-8\')[::-1] async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: Handle an individual client connection. Reads data from the client, processes it, and sends a response. try: # Read data from client data = await reader.read(100) # Arbitrarily reading 100 bytes from client if not data: return logging.info(f\'Received {data!r}\') # Simulate processing loop = asyncio.get_event_loop() result = await loop.run_in_executor(None, simulate_cpu_bound_operation, data) # Send response back to client writer.write(result.encode(\'utf-8\')) await writer.drain() logging.info(f\'Sent {result!r}\') except Exception as e: logging.error(f\'Error handling client: {e}\') finally: writer.close() await writer.wait_closed() async def start_server(host: str, port: int) -> None: Start the asynchronous server to handle incoming connections. server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() # Entry point if __name__ == \'__main__\': # Enable asyncio debug mode asyncio.run(start_server(\'localhost\', 8888), debug=True)"},{"question":"You are developing a communication protocol between a Python application and a C application over a network. You need to handle the packing and unpacking of complex data structures that are sent and received across this network. We will use the `struct` module to achieve this. Task Implement the following functions based on the specifications provided: 1. **Function 1: `create_message(data: Tuple[str, int, float]) -> bytes`** - **Input**: A tuple containing: - A string of exactly 10 characters (use null padding if shorter). - An integer. - A floating-point number. - **Output**: A bytes object where: - The first 10 bytes represent the string. - The next 4 bytes represent the integer in big-endian order. - The following 8 bytes represent the floating-point number in IEEE 754 floating-point format. The total length should be exactly 22 bytes. 2. **Function 2: `parse_message(msg: bytes) -> Tuple[str, int, float]`** - **Input**: A bytes object of exactly 22 bytes formatted as specified in Function 1. - **Output**: A tuple containing: - The string (first 10 bytes, excluding null padding). - The integer. - The floating-point number. Guidelines - Use the `struct` module for packing and unpacking values. - Ensure that the byte order for the integer is big-endian. - For the string, ensure that null bytes are added to make it exactly 10 bytes if it is shorter. - Raise a `ValueError` if the provided bytes object is not exactly 22 bytes long in `parse_message`. Example ```python data = (\\"hello\\", 1234, 56.78) msg = create_message(data) print(msg) # Expect a bytes object of length 22 parsed_data = parse_message(msg) print(parsed_data) # Expect (\'hello\', 1234, 56.78) ``` Constraints - You must use the `struct` module\'s functions for packing and unpacking. - You should handle edge cases such as: - Input string longer than 10 characters in `create_message`. - Input bytes object not exactly 22 bytes long in `parse_message`. Implementation ```python import struct def create_message(data: Tuple[str, int, float]) -> bytes: # TODO: Implement this function pass def parse_message(msg: bytes) -> Tuple[str, int, float]: # TODO: Implement this function pass ```","solution":"import struct from typing import Tuple def create_message(data: Tuple[str, int, float]) -> bytes: string, integer, floating_point = data # Ensure the string is exactly 10 bytes, pad with null bytes if shorter packed_string = string.encode(\'utf-8\')[:10].ljust(10, b\'x00\') # Use struct to pack the integer (big-endian) and float packed_integer = struct.pack(\'>i\', integer) packed_float = struct.pack(\'>d\', floating_point) # Combine all parts into a single bytes object message = packed_string + packed_integer + packed_float return message def parse_message(msg: bytes) -> Tuple[str, int, float]: if len(msg) != 22: raise ValueError(\\"Message must be exactly 22 bytes long\\") # Unpack the fixed-size sections from the message packed_string = msg[:10] packed_integer = msg[10:14] packed_float = msg[14:] # Decode the string, stripping null bytes string = packed_string.decode(\'utf-8\').rstrip(\'x00\') # Use struct to unpack the integer and float integer = struct.unpack(\'>i\', packed_integer)[0] floating_point = struct.unpack(\'>d\', packed_float)[0] return string, integer, floating_point"},{"question":"**Question:** You are provided with a dataset `titanic` which contains information about the Titanic passengers. The dataset has the following columns: 1. `survived` (0 or 1): Indicates if the passenger survived. 2. `pclass` (1, 2, or 3): Indicates the passenger class. 3. `sex` (male or female): Indicates the gender of the passenger. 4. `age` (numeric): The age of the passenger. 5. `sibsp` (numeric): Number of siblings or spouses aboard the Titanic. 6. `parch` (numeric): Number of parents or children aboard the Titanic. 7. `fare` (numeric): The ticket fare paid by the passenger. 8. `embarked` (\'C\', \'Q\', \'S\'): Port of embarkation (Cherbourg, Queenstown, Southampton). Using this dataset, complete the following tasks: 1. **Load the Dataset**: Load the Titanic dataset using seaborn\'s `load_dataset` function. 2. **Basic Point Plot**: Create a point plot showing the average age of passengers for each passenger class (`pclass`), adding error bars that represent the standard deviation. 3. **Grouped Point Plot**: Modify the plot to add a second layer of grouping by the `survived` variable, differentiated by color. 4. **Custom Appearance**: Customize the appearance of the plot by changing the markers, linestyles, and colors. Use circle (\'o\') and square (\'s\') markers, and solid (\'-\') and dashed (\'--\') linestyles, with different colors for the `survived` groups. 5. **Dodge for clarity**: Handle any overplotting by dodging the points based on the `survived` variable. **Input:** None (The dataset should be loaded using seaborn\'s load_dataset function). **Output:** A seaborn point plot that fulfills the above requirements. **Constraints:** 1. Use seaborn version 0.11.1 or later. 2. Ensure that error bars represent the standard deviation in the basic point plot. 3. Use appropriate seaborn functions to achieve the custom appearance and dodging. **Performance Requirement:** The solution should be able to handle datasets with up to 10,000 rows efficiently. ```python import seaborn as sns # Task 1: Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Task 2: Create a basic point plot with error bars representing standard deviation # YOUR CODE HERE # Task 3: Add a second layer of grouping by the \'survived\' variable, differentiated by color # YOUR CODE HERE # Task 4: Customize the appearance # YOUR CODE HERE # Task 5: Handle overplotting by dodging the points # YOUR CODE HERE ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Task 1: Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Task 2: Create a basic point plot with error bars representing standard deviation plt.figure(figsize=(10, 6)) sns.pointplot( data=titanic, x=\'pclass\', y=\'age\', join=False, capsize=0.1, ci=\'sd\', color=\'blue\' ) plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Average Age\\") plt.title(\\"Average Age of Passengers by Class with Std Dev\\") # Task 3: Add a second layer of grouping by the \'survived\' variable, differentiated by color plt.figure(figsize=(10, 6)) sns.pointplot( data=titanic, x=\'pclass\', y=\'age\', hue=\'survived\', join=False, capsize=0.1, ci=\'sd\' ) plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Average Age\\") plt.title(\\"Average Age of Passengers by Class and Survival with Std Dev\\") # Task 4: Customize the appearance plt.figure(figsize=(10, 6)) sns.pointplot( data=titanic, x=\'pclass\', y=\'age\', hue=\'survived\', markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], palette=[\'blue\', \'red\'], join=False, capsize=0.1, ci=\'sd\' ) plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Average Age\\") plt.title(\\"Average Age of Passengers by Class and Survival with Custom Appearance\\") # Task 5: Handle overplotting by dodging the points plt.figure(figsize=(10, 6)) sns.pointplot( data=titanic, x=\'pclass\', y=\'age\', hue=\'survived\', markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], palette=[\'blue\', \'red\'], dodge=True, join=False, capsize=0.1, ci=\'sd\' ) plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Average Age\\") plt.title(\\"Average Age of Passengers by Class and Survival with Dodging\\") plt.legend(title=\'Survived\', loc=\'upper right\') # Show the plot plt.show()"},{"question":"You are a data analyst working with a dataset that includes scores from different models on various machine learning tasks. Your task is to create a visual representation of this data using seaborn\'s object-oriented interface. The dataset needs to be loaded, transformed, and plotted with detailed annotations to provide meaningful insights. Specifications 1. **Dataset Loading and Transformation:** - Load the \\"glue\\" dataset using `seaborn.load_dataset(\\"glue\\")`. - Pivot the dataset to have models as rows, tasks as columns, and scores as the values. - Add an \\"Average\\" column representing the mean score of each model across all tasks. - Sort the models by their average score in descending order. 2. **Plotting Requirements:** - Create a bar plot with the \\"Average\\" score on the x-axis and the \\"Model\\" on the y-axis. - Add text annotations displaying the \\"Average\\" scores on the bars. - Fine-tune the text alignment to ensure clarity and readability (use color and offset as needed). - Create an additional plot where you: - Plot \\"SST-2\\" scores on the x-axis and \\"MRPC\\" scores on the y-axis. - Use different colors for the text annotations based on the \\"Encoder\\" type (LSTM or Transformer). - Ensure that the text annotations are well-aligned and do not overlap with the data points. - Optionally, customize the text appearance (e.g., bold font). 3. **Constraints and Limitations:** - Ensure the code executes without errors. - Use the seaborn objects interface for plotting (`seaborn.objects`). - Maintain code readability and modularity. 4. **Input and Output:** - The input is implicit through the use of the seaborn\'s built-in \\"glue\\" dataset. - The expected output is the visual plots described above displayed inline. Here is a sample template to get you started: ```python import seaborn.objects as so from seaborn import load_dataset # Load and transform the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Create and customize bar plot bar_plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) bar_plot.show() # Create and customize scatter plot scatter_plot = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\"), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ) scatter_plot.show() ``` # Evaluation Criteria - **Correctness:** The solution should load, transform, and plot the data as described. - **Completeness:** The solution must include both specified plots with appropriate customizations. - **Code Style:** The code should be readable, with proper use of functions and comments where necessary. - **Visualization Quality:** The plots should convey information clearly and be aesthetically pleasing. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and transform the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Create and customize bar plot bar_plot = ( so.Plot(glue.reset_index(), x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"black\\", halign=\\"left\\", offset=0)) ) bar_plot.show() # Create and customize scatter plot scatter_plot = ( so.Plot(glue.reset_index(), x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\", halign=\\"right\\")) .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ) scatter_plot.show()"},{"question":"# Advanced Python Coding Assessment **Objective:** Demonstrate your understanding of the `mmap` module by efficiently modifying contents of a large file using memory-mapped objects. **Problem Statement:** You are provided with a large text file named `largefile.txt`. This file contains a sequence of data lines in the format: ``` ID|value ID|value ID|value ... ``` Write a Python function `replace_values(filename: str, replacement_dict: dict) -> None` that takes the name of the file and a dictionary of ID and new values. Your function should utilize the `mmap` module to replace the values associated with the IDs provided in `replacement_dict`. # Function Specifications: - **Function Name:** `replace_values` - **Input:** - `filename` (str): The name of the file to be memory-mapped and modified. - `replacement_dict` (dict): A dictionary where keys are IDs (str) and values are their new replacements (str). - **Output:** - The function should not return any value. It should modify the file in place. # Additional Constraints: 1. **Efficiency:** The function should work efficiently even for very large files (tens or hundreds of MBs). 2. **Memory Usage:** Memory usage should be optimized using `mmap` to avoid loading the entire file into memory. 3. **Overwrite Checks:** Ensure that the new values do not change the overall line length. If any replacement value causes a line length change, raise a `ValueError` with an appropriate message indicating which ID caused the issue. 4. **File Integrity:** Ensure that after the operation, the integrity of the file remains intact, meaning no data other than specified values should be altered. # Example Usage: ```python # Example dictionary replacement_dict = {\\"123\\": \\"new_value_123\\", \\"456\\": \\"new_value_456\\"} # Replace values in the file replace_values(\\"largefile.txt\\", replacement_dict) ``` # Notes: 1. Assume that the input file `largefile.txt` and the `replacement_dict` provided to test the function are well-formed. 2. Focus on proper use of `mmap` functionalities like seeking, reading, and writing at specific file positions. # Hints: 1. Use `mmap.seek()` to move to various positions in the file. 2. Use `mmap.find()` to locate specific IDs within the file. 3. Perform checks to ensure the new content fits in the existing line structure before writing.","solution":"import mmap def replace_values(filename: str, replacement_dict: dict) -> None: with open(filename, \\"r+\\", encoding=\\"utf-8\\") as f: with mmap.mmap(f.fileno(), 0) as mm: for id, new_value in replacement_dict.items(): pos = 0 id_bytes = id.encode(\\"utf-8\\") new_value_bytes = new_value.encode(\\"utf-8\\") while True: # Search for the ID in the mmap object id_pos = mm.find(id_bytes, pos) if id_pos == -1: break line_start = mm.rfind(b\'n\', 0, id_pos) + 1 line_end = mm.find(b\'n\', id_pos) line = mm[line_start:line_end].decode(\'utf-8\') # Split line to get the current value length current_id, current_value = line.split(\'|\') if len(current_value) != len(new_value): raise ValueError(f\\"New value for ID {id} changes line length.\\") mm.seek(line_start + len(current_id) + 1) # Move to the position after \'ID|\' mm.write(new_value_bytes) pos = id_pos + len(id_bytes) # Move to the next position after found ID"},{"question":"**Objective:** Demonstrate your ability to handle advanced file I/O operations, utilize different string formatting methods, and work with JSON data in Python. # Question: You are tasked with processing a large text file containing JSON objects, one per line. Each JSON object represents a record with the following structure: ```json { \\"id\\": <integer>, \\"name\\": <string>, \\"age\\": <integer>, \\"email\\": <string> } ``` Your goal is to: 1. Read the input file and deserialize each JSON object. 2. Filter out records where the \\"age\\" is less than 18. 3. Output a neatly formatted table of the remaining records to a new text file. The table should have the following columns: ID, Name, Age, and Email, aligned and padded for readability. # Specifications: 1. Implement a function `process_records(input_file: str, output_file: str) -> None` that processes the records as described. 2. **Input:** - `input_file` (str): Path to the input text file containing JSON records. - `output_file` (str): Path to the output text file where the formatted table should be written. 3. **Output:** - A text file at the specified `output_file` path containing the formatted table. 4. **Constraints:** - Use f-strings or the `str.format()` method for string formatting. - Ensure the output table columns are neatly aligned with appropriate padding. - Handle file reading and writing efficiently to manage large files. # Example: Given an input file `records.txt` with the following content: ```json {\\"id\\": 1, \\"name\\": \\"John Doe\\", \\"age\\": 25, \\"email\\": \\"john.doe@example.com\\"} {\\"id\\": 2, \\"name\\": \\"Jane Smith\\", \\"age\\": 17, \\"email\\": \\"jane.smith@example.com\\"} {\\"id\\": 3, \\"name\\": \\"Emily Johnson\\", \\"age\\": 30, \\"email\\": \\"emily.johnson@example.com\\"} ``` Calling `process_records(\'records.txt\', \'output.txt\')` should create an `output.txt` file with the following content: ``` ID Name Age Email 1 John Doe 25 john.doe@example.com 3 Emily Johnson 30 emily.johnson@example.com ``` # Notes: - Ensure proper error handling for file operations. - Consider edge cases such as empty files or records missing one of the required fields.","solution":"import json def process_records(input_file: str, output_file: str) -> None: Process the input file containing JSON records and output a formatted table to the output file. Filter out records where age is less than 18 and align columns in the output for readability. Args: - input_file (str): Path to the input text file containing JSON records. - output_file (str): Path to the output text file where the formatted table should be written. # Read the input file and deserialize each JSON object records = [] with open(input_file, \'r\') as infile: for line in infile: record = json.loads(line) # Filter out records where the age is less than 18 if record.get(\\"age\\", 0) >= 18: records.append((record[\'id\'], record[\'name\'], record[\'age\'], record[\'email\'])) # Prepare the formatted table formatted_lines = [] header = f\\"{\'ID\':<5} {\'Name\':<20} {\'Age\':<5} {\'Email\'}\\" formatted_lines.append(header) for record in records: formatted_lines.append(f\\"{record[0]:<5} {record[1]:<20} {record[2]:<5} {record[3]}\\") # Write the formatted table to the output file with open(output_file, \'w\') as outfile: outfile.write(\\"n\\".join(formatted_lines))"},{"question":"**HMAC Authentication System** You are tasked with implementing a simple authentication system using HMAC in Python. Your goal is to create a utility for generating and verifying HMAC digests to ensure message integrity and authenticity. **Requirements:** 1. **Function `generate_hmac(key: bytes, message: bytes, digest_name: str) -> str`**: - **Input**: - `key`: a bytes object representing the secret key. - `message`: a bytes object representing the message to generate an HMAC for. - `digest_name`: a string representing the name of the hash function to use (e.g., \'sha256\'). - **Output**: - A string representing the hexadecimal digest of the HMAC. - **Example**: ```python key = b\'secret\' message = b\'My secret message\' digest_name = \'sha256\' generate_hmac(key, message, digest_name) # Output: \'5d41402abc4b2a76b9719d911017c592\' ``` 2. **Function `verify_hmac(key: bytes, message: bytes, provided_digest: str, digest_name: str) -> bool`**: - **Input**: - `key`: a bytes object representing the secret key. - `message`: a bytes object representing the message to verify. - `provided_digest`: a string representing the provided HMAC digest to verify against. - `digest_name`: a string representing the name of the hash function to use (e.g., \'sha256\'). - **Output**: - A boolean value indicating whether the provided HMAC matches the one generated from the key and message. - **Example**: ```python key = b\'secret\' message = b\'My secret message\' provided_digest = \'5d41402abc4b2a76b9719d911017c592\' digest_name = \'sha256\' verify_hmac(key, message, provided_digest, digest_name) # Output: True or False ``` **Additional Constraints:** - Ensure that the `verify_hmac` function uses the `hmac.compare_digest` function to compare digests securely to prevent timing attacks. **Performance Requirements:** - The implementation should handle typical message sizes efficiently, but there is no strict performance requirement for very large messages. **Hints:** - Refer to the `hmac` module documentation for creating and managing HMAC objects. - Use the `hmac.new` and `hmac.update` methods to build the HMAC object. - Use the `digest` and `hexdigest` methods to obtain the required digest format for `generate_hmac`. - Utilize the `hmac.compare_digest` function in `verify_hmac` for secure comparison. ```python def generate_hmac(key: bytes, message: bytes, digest_name: str) -> str: pass def verify_hmac(key: bytes, message: bytes, provided_digest: str, digest_name: str) -> bool: pass # Example usage: # key = b\'secret\' # message = b\'My secret message\' # digest_name = \'sha256\' # hmac_digest = generate_hmac(key, message, digest_name) # is_valid = verify_hmac(key, message, hmac_digest, digest_name) ``` **Note**: For testing purposes, ensure that your implementation works with the given example inputs and produces the expected outputs.","solution":"import hmac import hashlib def generate_hmac(key: bytes, message: bytes, digest_name: str) -> str: Generate an HMAC for the given message using the specified hash function. digest_maker = hmac.new(key, message, digest_name) return digest_maker.hexdigest() def verify_hmac(key: bytes, message: bytes, provided_digest: str, digest_name: str) -> bool: Verify the provided HMAC against a newly generated one for the given key and message. digest_maker = hmac.new(key, message, digest_name) generated_digest = digest_maker.hexdigest() return hmac.compare_digest(generated_digest, provided_digest)"},{"question":"Question: Design a Custom Iterable Data Structure with Context Management # Overview In Python, an iterable is any object capable of returning its members one at a time, allowing it to be iterated over in a for-loop. Common examples include lists, tuples, and dictionaries. Additionally, context managers are used to properly manage resources, ensuring that setup and teardown actions are executed correctly. Your task is to implement a custom data structure that combines these capabilities: creating an iterable object that also supports context management. # Task You are required to implement a custom iterable data structure called `ContextList` that: 1. Stores a list of items. 2. Can be iterated over using a for-loop. 3. Supports context management with the `with` statement, initializing resources when entering the context and cleaning up resources when exiting. # Requirements: 1. **Initialization**: - The constructor should accept an optional list of items. - Example: `ContextList([1, 2, 3, 4])` 2. **Iterable Protocol**: - Implement the `__iter__` and `__next__` methods to make the object iterable. 3. **Context Management**: - Implement the `__enter__` and `__exit__` methods to support context management. - Print \\"Entering context\\" when entering the context, and \\"Exiting context\\" when exiting the context. 4. **Additional Methods**: - Implement an `add_item` method to add an item to the list. - Implement a `remove_item` method to remove a specified item from the list. # Constraints: - The methods `add_item` and `remove_item` should handle exceptions properly, raising a `ValueError` if the item to be removed is not found. # Example Usage: ```python # Example usage of ContextList with ContextList([10, 20, 30]) as clist: clist.add_item(40) for item in clist: print(item) clist.remove_item(10) ``` # Expected Output: ``` Entering context 10 20 30 40 Exiting context ``` # Notes: - If an item to be removed is not found, the `remove_item` method should raise a `ValueError` with an appropriate message. - Ensure that the `__iter__` and `__next__` methods implement the iteration correctly, considering end-of-iteration handling. # Implementation: ```python class ContextList: def __init__(self, items=None): self.items = items if items is not None else [] self._index = -1 def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self.items): item = self.items[self._index] self._index += 1 return item else: raise StopIteration def __enter__(self): print(\\"Entering context\\") return self def __exit__(self, exc_type, exc_value, traceback): print(\\"Exiting context\\") def add_item(self, item): self.items.append(item) def remove_item(self, item): if item in self.items: self.items.remove(item) else: raise ValueError(f\\"Item {item} not found in the list\\") ``` Your task is to complete and test the `ContextList` class based on the requirements specified above.","solution":"class ContextList: def __init__(self, items=None): self.items = items if items is not None else [] self._index = -1 def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self.items): item = self.items[self._index] self._index += 1 return item else: raise StopIteration def __enter__(self): print(\\"Entering context\\") return self def __exit__(self, exc_type, exc_value, traceback): print(\\"Exiting context\\") def add_item(self, item): self.items.append(item) def remove_item(self, item): if item in self.items: self.items.remove(item) else: raise ValueError(f\\"Item {item} not found in the list\\")"},{"question":"Objective Write a Python function that will take a list of numerical pairs, perform a sequence of mathematical operations on these pairs, and return the results. The functions to be used are imported from the `operator` module. Specifications - You are given a list of tuples. Each tuple consists of two integers. - You need to create functions applying the following sequence of operations on each pair of integers (a, b): - Add the two numbers. - Subtract the second number from the first. - Multiply the two numbers. - Perform integer division of the first number by the second. - Perform modulo operation of the first number by the second. - Your function should return a list where each list element contains a tuple of results for each pair in the given format: `(add_result, sub_result, mul_result, floordiv_result, mod_result)`. Function Signature ```python from typing import List, Tuple def perform_operations(pairs: List[Tuple[int, int]]) -> List[Tuple[int, int, int, int, int]]: pass ``` Input - `pairs` (List[Tuple[int, int]]): A list of tuples, where each tuple contains two integers `a` and `b`. Output - Returns a list of tuples. Each tuple contains the results of the operations: `(add_result, sub_result, mul_result, floordiv_result, mod_result)`. Constraints - The list of pairs will contain at least one pair and at most 100 pairs. - Each integer in the pairs will be between `-10^3` and `10^3`. - Integer division should follow Python\'s default behavior, i.e., it should not raise an exception if the division by zero is attempted but avoid such cases. Example ```python from operator import add, sub, mul, floordiv, mod pairs = [(10, 5), (12, 4)] result = perform_operations(pairs) print(result) # Output: [(15, 5, 50, 2, 0), (16, 8, 48, 3, 0)] ``` **Hints**: - You might find it useful to utilize lambdas or map functions along with the operators. - Handling edge cases such as division by zero should be considered. Good luck!","solution":"from typing import List, Tuple from operator import add, sub, mul, floordiv, mod def perform_operations(pairs: List[Tuple[int, int]]) -> List[Tuple[int, int, int, int, int]]: results = [] for a, b in pairs: add_result = add(a, b) sub_result = sub(a, b) mul_result = mul(a, b) # Handling division by zero floordiv_result = floordiv(a, b) if b != 0 else \'undefined\' mod_result = mod(a, b) if b != 0 else \'undefined\' results.append((add_result, sub_result, mul_result, floordiv_result, mod_result)) return results"},{"question":"# Question: Python Keywords and Soft Keywords Checker You are to implement a function that checks whether elements in a given list of strings are Python keywords or soft keywords. The function will also categorize each element accordingly and return a summary. Function Signature ```python def keyword_checker(strings: list) -> dict: # implement your solution here ``` Input - `strings` (list): A list of strings to be checked. Output - A dictionary with three keys: - `\\"keywords\\"`: A list of strings from the input that are recognized as Python keywords. - `\\"soft_keywords\\"`: A list of strings from the input that are recognized as Python soft keywords. - `\\"others\\"`: A list of strings from the input that are neither keywords nor soft keywords. Constraints - Each string in the input list contains only alphabetic characters and has a length of at least 1 and at most 50. - The input list has at least 1 and at most 100 elements. Example ```python strings = [\\"for\\", \\"while\\", \\"hello\\", \\"async\\", \\"foobar\\", \\"await\\"] print(keyword_checker(strings)) # Expected output: # { # \\"keywords\\": [\\"for\\", \\"while\\", \\"async\\", \\"await\\"], # \\"soft_keywords\\": [], # \\"others\\": [\\"hello\\", \\"foobar\\"] # } ``` # Notes - Use the `keyword` module functionalities `iskeyword` and `issoftkeyword` to determine if a string is a keyword or a soft keyword. - Ensure that your solution is efficient and handles edge cases, such as empty strings in the input list, appropriately.","solution":"import keyword def keyword_checker(strings: list) -> dict: Check whether elements in a given list of strings are Python keywords or soft keywords. result = { \\"keywords\\": [], \\"soft_keywords\\": [], \\"others\\": [] } # If the issoftkeyword function doesn\'t exist, we can define our custom version try: issoftkeyword = keyword.issoftkeyword except AttributeError: # Python versions before 3.9 do not have soft keywords feature. issoftkeyword = lambda x: False for string in strings: if keyword.iskeyword(string): result[\\"keywords\\"].append(string) elif issoftkeyword(string): result[\\"soft_keywords\\"].append(string) else: result[\\"others\\"].append(string) return result"},{"question":"# Advanced SQLite Interaction in Python --- Problem Statement: You are required to design a small inventory management system using Python\'s `sqlite3` module. Your task is to implement the functionality to create an SQLite database, handle various operations on it, and ensure data consistency. You must also demonstrate the use of custom adapters and converters. Requirements: 1. **Establish a Database Connection** - Create a new SQLite database called `inventory.db`. - Implement a function `create_connection()` that establishes a connection to this database and returns the connection object. 2. **Create Tables** - Implement a function `create_tables(conn)`. This function should: - Create a table called `items` with the following columns: - `item_id` (INTEGER, Primary Key) - `name` (TEXT, Unique) - `quantity` (INTEGER) - `price` (REAL) 3. **Insert Data** - Implement a function `insert_item(conn, item)` that takes a connection object and a tuple representing an item (`name`, `quantity`, `price`). The function should insert this item into the `items` table. Use placeholders to avoid SQL injection attacks. 4. **Fetch Data** - Implement a function `fetch_items(conn)` that takes a connection object and returns all items from the `items` table as a list of tuples. 5. **Update Data** - Implement a function `update_item_quantity(conn, item_id, new_quantity)` that takes a connection object, an item ID, and a new quantity, and updates the quantity of the specified item in the `items` table. 6. **Custom Type Adaptation** - Create a class `Item` with attributes `name`, `quantity`, and `price`. - Write a custom adapter to convert an `Item` object to a format storable in SQLite. - Register this adapter with `sqlite3`. 7. **Error Handling and Transactions** - Ensure that all database operations maintain data consistency using transactions. - Implement error handling to catch and display appropriate messages for the following exceptions: - `sqlite3.IntegrityError` for constraint violations. - `sqlite3.OperationalError` for operational errors. 8. **Backup and Restore** - Implement functions `backup_database(conn, backup_path)` and `restore_database(backup_path, target_path)` to create a backup of the database and restore it from a backup file, respectively. Constraints: - Use SQLite\'s flexible typing, but ensure proper usage of types in your Python functions. - Handle all database operations appropriately to prevent SQL injection. - Design your program with good coding practices, including meaningful function names, modularity, and comments for documentation. Example Usage: ```python # Example usage of the implemented functions conn = create_connection() create_tables(conn) insert_item(conn, (\'Apple\', 10, 0.50)) insert_item(conn, (\'Banana\', 20, 0.20)) items = fetch_items(conn) print(items) # Output should be a list of tuples representing all items update_item_quantity(conn, 1, 15) item = Item(\'Orange\', 30, 0.30) insert_item(conn, item) # Using custom type adaptation backup_database(conn, \'backup_inventory.db\') restore_database(\'backup_inventory.db\', \'restored_inventory.db\') ``` --- Please ensure that your implementation meets all the given requirements and constraints. Your submission will be evaluated on correctness, efficiency, and adherence to best practices in Python programming.","solution":"import sqlite3 import os import shutil class Item: def __init__(self, name, quantity, price): self.name = name self.quantity = quantity self.price = price def create_connection(): Create a new database connection to the SQLite database. conn = sqlite3.connect(\'inventory.db\') return conn def create_tables(conn): Create tables needed for the inventory database. try: sql_create_items_table = CREATE TABLE IF NOT EXISTS items ( item_id INTEGER PRIMARY KEY, name TEXT UNIQUE, quantity INTEGER, price REAL ); c = conn.cursor() c.execute(sql_create_items_table) except sqlite3.Error as e: print(e) def insert_item(conn, item): Insert an item into the items table. sql = \'\'\'INSERT INTO items(name, quantity, price) VALUES (?, ?, ?)\'\'\' cur = conn.cursor() try: if isinstance(item, tuple): cur.execute(sql, item) elif isinstance(item, Item): cur.execute(sql, (item.name, item.quantity, item.price)) conn.commit() except sqlite3.IntegrityError as e: conn.rollback() print(f\\"IntegrityError: {e}\\") except sqlite3.OperationalError as e: conn.rollback() print(f\\"OperationalError: {e}\\") def fetch_items(conn): Fetch all items from the items table. cur = conn.cursor() cur.execute(\\"SELECT * FROM items\\") rows = cur.fetchall() return rows def update_item_quantity(conn, item_id, new_quantity): Update the quantity of an item in the items table. sql = \'\'\' UPDATE items SET quantity = ? WHERE item_id = ?\'\'\' cur = conn.cursor() try: cur.execute(sql, (new_quantity, item_id)) conn.commit() except sqlite3.IntegrityError as e: conn.rollback() print(f\\"IntegrityError: {e}\\") except sqlite3.OperationalError as e: conn.rollback() print(f\\"OperationalError: {e}\\") def backup_database(conn, backup_path): Create a backup of the database. with open(backup_path, \'w\'): pass # Just create the file, we will use shutil for copying the database conn.backup(sqlite3.connect(backup_path)) def restore_database(backup_path, target_path): Restore the database from a backup file. if os.path.exists(backup_path): shutil.copyfile(backup_path, target_path) # Custom adapter to convert Item class to tuple def adapt_item(item): return f\\"{item.name},{item.quantity},{item.price}\\" # Register the adapter sqlite3.register_adapter(Item, adapt_item) # Custom converter to convert string to Item object def convert_item(item): name, quantity, price = item.split(b\',\') # SQLite returns bytes return Item(name.decode(\'utf-8\'), int(quantity), float(price)) # Register the converter sqlite3.register_converter(\\"Item\\", convert_item)"},{"question":"Objective Write a function that accepts a file path and mimics the behavior of a MIME-aware application by utilizing the mailcap module. The function should determine the appropriate command to handle the file based on its MIME type and execute the command. Ensure that the function adheres to security guidelines by appropriately handling shell metacharacters in the file path. Function Signature ```python def handle_mime_file(file_path: str) -> str: pass ``` Input - `file_path` (str): A string representing the absolute path to the file that needs to be handled. Output - A string message indicating the result of the operation. For example, \\"Handled file with command: `command`\\", or appropriate error messages such as \\"No suitable command found\\", or \\"Invalid characters in file path\\". Constraints and Considerations 1. The function should: - Use `mailcap.getcaps()` to retrieve capabilities from Mailcap files. - Determine the MIME type of the given file (you may consider `mimetypes.guess_type` to fetch the MIME type). - Use `mailcap.findmatch()` to find the appropriate command to handle the file. - Execute the command and return the result as a string message. 2. Ensure to validate the `file_path` for security issues related to shell metacharacters. If such characters are found, the function should return an appropriate error message. 3. You may assume the necessary mailcap entries are present in the system mailcap files. 4. Mimicking command execution: For the purpose of this assessment, the actual execution of the command can be simulated by returning the command string instead of executing it. This is to simplify testing and ensure security. Example ```python >>> handle_mime_file(\'/path/to/file.mp4\') \\"Handled file with command: \'xmpeg /path/to/file.mp4\'\\" >>> handle_mime_file(\'/path/to/file;rm -rf .mp4\') \\"Invalid characters in file path\\" >>> handle_mime_file(\'/path/to/unknownfile.xyz\') \\"No suitable command found\\" ``` Notes - Make use of the `mailcap` and `mimetypes` modules effectively. - Be mindful of handling and validating inputs to avoid security vulnerabilities. - Assume the vanilla Python environment with no additional external libraries required.","solution":"import mailcap import mimetypes import shlex def handle_mime_file(file_path: str) -> str: # Validate file path for shell metacharacters if any(char in file_path for char in [\';\', \'&\', \'|\', \'`\', \'>\', \'<\', \'!\', \'\']): return \\"Invalid characters in file path\\" # Get mime type of the file mime_type, _ = mimetypes.guess_type(file_path) if not mime_type: return \\"Could not determine the file\'s MIME type\\" # Retrieve the mailcap capabilities caps = mailcap.getcaps() # Find a match with mailcap for the mime type command, entry = mailcap.findmatch(caps, mime_type, filename=shlex.quote(file_path)) if not command: return \\"No suitable command found\\" # Returning the result as a string message return f\\"Handled file with command: \'{command}\'\\""},{"question":"You are provided with the famous iris dataset, which will be loaded using seaborn. Your task is to create a comprehensive visualization to analyze the distributions of the different species in the dataset. Task: 1. **Load the Iris Dataset**: Use `seaborn.load_dataset` to load the iris dataset. 2. **Plot ECDFs**: Plot ECDFs for the following scenarios using seaborn: a. Plot the ECDF of the petal length (`petal_length`) for the entire dataset along the x-axis. b. Plot separate ECDFs using the `hue` parameter to differentiate between species along the petal width (`petal_width`). c. Display the ECDF of the sepal length (`sepal_length`) along the y-axis, differentiating species using the `hue` parameter. d. Modify the ECDF of the petal width (`petal_width`) to display counts instead of the proportion statistic. e. Plot the empirical complementary CDF for the petal length (`petal_length`) using the `complementary` parameter. 3. **Customize the Visualization**: a. Add appropriate titles to each plot. b. Customize the labels of the axes to make the plots comprehensible. c. Set a theme of your choice for the visualizations. Your function should not return anything but should display the plots directly. You should create a function `visualize_iris_ecdfs` in which this visualization process is implemented. Constraints: - You should use seaborn for loading the dataset and for creating the plots. - Each plot should be distinct and display its respective scenario clearly. - Make sure to include all necessary library imports within your function. Expected Solution Structure: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_iris_ecdfs(): # Load the dataset iris = sns.load_dataset(\\"iris\\") # Plot ECDF of petal length plt.figure() sns.ecdfplot(data=iris, x=\\"petal_length\\") plt.title(\\"ECDF of Petal Length\\") plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"ECDF\\") # Plot ECDF of petal width by species plt.figure() sns.ecdfplot(data=iris, x=\\"petal_width\\", hue=\\"species\\") plt.title(\\"ECDF of Petal Width by Species\\") plt.xlabel(\\"Petal Width\\") plt.ylabel(\\"ECDF\\") # Plot ECDF of sepal length by species along y-axis plt.figure() sns.ecdfplot(data=iris, y=\\"sepal_length\\", hue=\\"species\\") plt.title(\\"ECDF of Sepal Length by Species\\") plt.xlabel(\\"ECDF\\") plt.ylabel(\\"Sepal Length\\") # Plot ECDF with counts of petal width plt.figure() sns.ecdfplot(data=iris, x=\\"petal_width\\", stat=\\"count\\") plt.title(\\"ECDF of Petal Width with counts\\") plt.xlabel(\\"Petal Width\\") plt.ylabel(\\"Count\\") # Plot empirical complementary CDF of petal length plt.figure() sns.ecdfplot(data=iris, x=\\"petal_length\\", complementary=True) plt.title(\\"Empirical Complementary CDF of Petal Length\\") plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"1 - ECDF\\") # Display all plots plt.show() # Call the function to see the output visualize_iris_ecdfs() ``` Note: - Make sure to format the plots well with appropriate titles and labels. - Be creative in making the plots visually appealing using seaborn\'s features.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_iris_ecdfs(): Loads the iris dataset and creates a series of ECDF plots to visualize the distribution of different species. sns.set_theme(style=\\"whitegrid\\") # Load the dataset iris = sns.load_dataset(\\"iris\\") # 1a. Plot ECDF of petal length for the entire dataset plt.figure() sns.ecdfplot(data=iris, x=\\"petal_length\\") plt.title(\\"ECDF of Petal Length\\") plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"ECDF\\") # 1b. Plot ECDF of petal width by species plt.figure() sns.ecdfplot(data=iris, x=\\"petal_width\\", hue=\\"species\\") plt.title(\\"ECDF of Petal Width by Species\\") plt.xlabel(\\"Petal Width\\") plt.ylabel(\\"ECDF\\") # 1c. Plot ECDF of sepal length by species along y-axis plt.figure() sns.ecdfplot(data=iris, y=\\"sepal_length\\", hue=\\"species\\") plt.title(\\"ECDF of Sepal Length by Species\\") plt.xlabel(\\"ECDF\\") plt.ylabel(\\"Sepal Length\\") # 1d. Display ECDF of petal width with counts plt.figure() sns.ecdfplot(data=iris, x=\\"petal_width\\", stat=\\"count\\") plt.title(\\"ECDF of Petal Width with Counts\\") plt.xlabel(\\"Petal Width\\") plt.ylabel(\\"Count\\") # 1e. Empirical Complementary CDF of petal length plt.figure() sns.ecdfplot(data=iris, x=\\"petal_length\\", complementary=True) plt.title(\\"Empirical Complementary CDF of Petal Length\\") plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"1 - ECDF\\") # Display all plots plt.show() # Call the function to see the output visualize_iris_ecdfs()"},{"question":"Objective: Implement a custom socket-based echo server and client in Python that can handle multiple clients simultaneously using the `select` function. This assessment will test your understanding of socket programming, non-blocking I/O, and event-driven programming. Background: An echo server is a server that sends back the same message that a client sends. The goal of this task is to set up both a server and client using Python\'s socket library. The server will handle multiple client connections concurrently without using multi-threading or multi-processing. Instead, you will use the `select` function to manage multiple clients\' I/O. Instructions: 1. **Server Implementation:** - Create a server that listens on a specified port. - Use non-blocking sockets for both the server and client connections. - Use `select` to handle multiple client connections. - The server should accept new connections and echo back any received messages to the sending client. - The server should handle clients gracefully, ensuring connections can be closed properly. 2. **Client Implementation:** - Create a client that connects to the server. - The client should be able to send messages to the server and receive echo responses. - The client should handle connection closure gracefully. Requirements: - Ensure your implementation adheres to the use of non-blocking sockets and the `select` function. - The server must be able to handle at least 10 clients simultaneously. - The messages sent between the client and server should be UTF-8 encoded strings. - Both the server and client should print appropriate status messages to the console (e.g., when a connection is made, data is received, data is sent, etc.) Constraints: - Use Python 3.10 or higher. - Do not use any external libraries besides Python\'s standard library. - The server should run on `localhost` and use a port number greater than 8000. Example: Here\'s a brief example of interaction: **Server:** ``` [START] Echo server started on port 9001 [CONNECTED] New client connected from 127.0.0.1:34322 [ECHO] Echoed back to 127.0.0.1: Hello, server! [DISCONNECTED] Client from 127.0.0.1:34322 disconnected ``` **Client:** ``` [CONNECTED] Connected to echo server on port 9001 [SENT] Hello, server! [RECEIVED] Hello, server! [DISCONNECTED] Disconnected from server ``` Submission: Submit your implementation of: - `echo_server.py`: Contains the server code. - `echo_client.py`: Contains the client code. Each file should be fully self-contained and executable directly to demonstrate the functionality. Good luck and happy coding!","solution":"# echo_server.py import socket import select def echo_server(host=\'localhost\', port=9001): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.setblocking(0) server_socket.bind((host, port)) server_socket.listen(10) inputs = [server_socket] print(f\\"[START] Echo server started on port {port}\\") try: while inputs: readable, _, exceptional = select.select(inputs, [], inputs) for s in readable: if s is server_socket: client_socket, client_address = server_socket.accept() print(f\\"[CONNECTED] New client connected from {client_address}\\") client_socket.setblocking(0) inputs.append(client_socket) else: data = s.recv(1024) if data: print(f\\"[RECEIVED] from {s.getpeername()}: {data.decode(\'utf-8\')}\\") s.send(data) print(f\\"[ECHO] Echoed back to {s.getpeername()}\\") else: print(f\\"[DISCONNECTED] Client from {s.getpeername()} disconnected\\") inputs.remove(s) s.close() for s in exceptional: print(f\\"[ERROR] Exceptional condition on {s.getpeername()}\\") inputs.remove(s) s.close() finally: server_socket.close() if __name__ == \\"__main__\\": echo_server() # echo_client.py import socket import sys def echo_client(host=\'localhost\', port=9001): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((host, port)) print(f\\"[CONNECTED] Connected to echo server on port {port}\\") try: while True: message = input(\\"Enter message: \\") if not message: break client_socket.send(message.encode(\'utf-8\')) print(f\\"[SENT] {message}\\") data = client_socket.recv(1024) if not data: break print(f\\"[RECEIVED] {data.decode(\'utf-8\')}\\") finally: print(\\"[DISCONNECTED] Disconnected from server\\") client_socket.close() if __name__ == \\"__main__\\": echo_client()"},{"question":"Objective You are tasked with simulating a subset of the `2to3` tool functionality by implementing a function that converts specific Python 2 constructs into their Python 3 equivalents. This will demonstrate your understanding of Python\'s syntax changes and your ability to manipulate strings and implement transformations. Instructions Implement a function called `python2_to_python3` that takes a string of Python 2 code and returns a string of Python 3 code. Your function should apply the following transformations: 1. **Print Statements**: Convert Python 2 print statements to Python 3 print function calls. - Example: `print \\"Hello\\"` should be transformed to `print(\\"Hello\\")`. 2. **Raw Input**: Convert `raw_input()` to `input()`. - Example: `name = raw_input()` should be transformed to `name = input()`. 3. **Dictionary Iteration Methods**: Convert `dict.iteritems()`, `dict.iterkeys()`, and `dict.itervalues()` to `dict.items()`, `dict.keys()`, and `dict.values()`, respectively. - Example: `for k, v in my_dict.iteritems():` should be transformed to `for k, v in my_dict.items():`. 4. **Has Key Method**: Convert `dict.has_key(key)` to `key in dict`. - Example: `if my_dict.has_key(\\"key\\"):` should be transformed to `if \\"key\\" in my_dict:`. 5. **Exception Handling**: Convert `except Exception, e` to `except Exception as e`. - Example: `except ValueError, e:` should be transformed to `except ValueError as e:`. # Function Signature ```python def python2_to_python3(code: str) -> str: pass ``` # Input - `code`: A string representing Python 2 code (1 <= len(code) <= 1000). # Output - A string of the transformed Python 3 code. # Example ```python code = def greet(name): print \\"Hello, {0}!\\".format(name) print \\"What\'s your name?\\" name = raw_input() greet(name) transformed_code = python2_to_python3(code) print(transformed_code) ``` Expected Output: ```python def greet(name): print(\\"Hello, {0}!\\".format(name)) print(\\"What\'s your name?\\") name = input() greet(name) ``` # Constraints - You can assume that the input code is syntactically correct Python 2 code. - You do not need to handle edge cases or incomplete constructs explicitly. - Focus on the transformations outlined above. # Evaluation Your solution will be evaluated based on: - Correctness: The transformed code should perform as expected in Python 3. - Code quality: Clarity and simplicity of your function implementation. - Efficiency: Although performance is not the main focus, overly inefficient solutions should be avoided.","solution":"import re def python2_to_python3(code: str) -> str: # Transform print statements to print function calls code = re.sub(r\'print \\"(.*?)\\"\', r\'print(\\"1\\")\', code) code = re.sub(r\\"print \'(.*?)\'\\", r\\"print(\'1\')\\", code) # Transform raw_input() to input() code = re.sub(r\'braw_input()\', \'input()\', code) # Transform dict.iteritems() -> dict.items() code = re.sub(r\'.iteritems()\', \'.items()\', code) # Transform dict.iterkeys() -> dict.keys() code = re.sub(r\'.iterkeys()\', \'.keys()\', code) # Transform dict.itervalues() -> dict.values() code = re.sub(r\'.itervalues()\', \'.values()\', code) # Transform dict.has_key(key) -> key in dict code = re.sub(r\'(w+).has_key((.*?))\', r\'2 in 1\', code) # Transform except Exception, e -> except Exception as e code = re.sub(r\'except (.*?), (.*?):\', r\'except 1 as 2:\', code) return code"},{"question":"Objective You need to demonstrate your understanding of seaborn\'s theme customization and display configuration. Problem Statement Implement a function `customize_plot` that accepts a seaborn Plot object, a theme configuration dictionary, and a display configuration dictionary, and applies these settings to the plot. The function should: 1. Update the plot\'s theme using the provided theme configuration dictionary. 2. Update the plot\'s display settings using the provided display configuration dictionary. 3. Return the configured plot. Function Signature ```python def customize_plot(plot: so.Plot, theme_config: dict, display_config: dict) -> so.Plot: pass ``` Input - `plot` (seaborn.objects.Plot): A seaborn Plot object to be customized. - `theme_config` (dict): A dictionary containing theme settings to be applied. - `display_config` (dict): A dictionary containing display settings to be applied. Output - Returns the customized seaborn Plot object. Constraints 1. The keys in `theme_config` correspond to valid `rcParams` entries. 2. The display configuration can include format settings (`\\"format\\": \\"png\\"` or `\\"format\\": \\"svg\\"`), HiDPI settings (`\\"hidpi\\": True` or `\\"hidpi\\": False`), and scaling settings (`\\"scaling\\": float`). Example ```python import seaborn.objects as so from seaborn import axes_style # Sample plot data = so.Plot(data=[1, 2, 3, 4]).add(so.Line(), x=\\"variable\\", y=\\"value\\") # Theme configuration dictionary: setting style to whitegrid theme_config = axes_style(\\"whitegrid\\") # Display configuration dictionary display_config = {\\"format\\": \\"svg\\", \\"hidpi\\": False, \\"scaling\\": 0.7} def customize_plot(plot: so.Plot, theme_config: dict, display_config: dict) -> so.Plot: plot.config.theme.update(theme_config) plot.config.display.update(display_config) return plot # Calling the function custom_plot = customize_plot(data, theme_config, display_config) # The return plot should have the customized theme and display settings applied. ``` Note This exercise will test your ability to interact with seaborn\'s theme and display configurations and understand the impact of these configurations on plot appearance and quality.","solution":"import seaborn.objects as so import matplotlib.pyplot as plt def customize_plot(plot: so.Plot, theme_config: dict, display_config: dict) -> so.Plot: Customize the given seaborn Plot object using the provided theme and display configurations. Parameters: - plot: seaborn.objects.Plot, the Plot object to be customized - theme_config: dict, the theme settings to be applied using valid rcParams entries - display_config: dict, the display settings including format, hidpi, and scaling Returns: - The customized seaborn Plot object # Update the plot\'s theme plt.rcParams.update(theme_config) # Update the plot\'s display settings if \\"format\\" in display_config: plot.config.format = display_config[\\"format\\"] if \\"hidpi\\" in display_config: plot.config.hidpi = display_config[\\"hidpi\\"] if \\"scaling\\" in display_config: plot.config.scaling = display_config[\\"scaling\\"] return plot"},{"question":"# Task You are to write a Python script that organizes and archives log files. # Details 1. The script should accept the following command-line arguments: - `-d` or `--directory`: The target directory containing the log files to process. - `-o` or `--output`: The output directory where archived files should be stored. - `-n` or `--days`: (Optional) The number of days old a file must be to be archived. Defaults to 7 days. 2. The script should: - List all `.log` files in the target directory specified with `--directory`. - Filter out files that are less than `n` days old (use the `--days` argument). - Move the selected files to a subdirectory of the output directory named with the current date (e.g., `output/yyyy-mm-dd`). - If the subdirectory does not exist, create it. - Print appropriate messages to the console at each significant step (e.g., \\"Moving file X.log to output/yyyy-mm-dd\\"). # Constraints - Only consider files with the `.log` extension. - Do not use wildcard characters directly; instead, use the `glob` module. - Assume all paths provided as arguments are valid and exist. - Ensure that the script is run in a Python environment with version 3.10 or higher. # Example ```shell python organize_logs.py --directory /logs --output /archive --days 5 ``` # Sample Code Scaffold ```python import os import shutil import glob import argparse from datetime import datetime, timedelta def get_log_files(directory, days): pass def create_output_directory(base_output_directory): pass def move_files(log_files, output_directory): pass def main(): parser = argparse.ArgumentParser(description=\'Organize and archive log files\') parser.add_argument(\'-d\', \'--directory\', required=True, help=\'Directory containing log files to process\') parser.add_argument(\'-o\', \'--output\', required=True, help=\'Output directory where archived files will be stored\') parser.add_argument(\'-n\', \'--days\', type=int, default=7, help=\'Number of days old a file must be to be archived (default is 7)\') args = parser.parse_args() log_files = get_log_files(args.directory, args.days) output_directory = create_output_directory(args.output) move_files(log_files, output_directory) if __name__ == \\"__main__\\": main() ``` # Your Task Implement the functions: 1. `get_log_files(directory, days)` - lists `.log` files older than specified days. 2. `create_output_directory(base_output_directory)` - creates today\'s directory if it doesn\'t exist. 3. `move_files(log_files, output_directory)` - moves filtered log files to the created directory. Validate that the script performs as specified in handling directory contents, file operations, and command-line arguments.","solution":"import os import shutil import glob import argparse from datetime import datetime, timedelta def get_log_files(directory, days): files = glob.glob(os.path.join(directory, \'*.log\')) current_time = datetime.now() cutoff_time = current_time - timedelta(days=days) old_files = [] for file in files: file_mtime = datetime.fromtimestamp(os.path.getmtime(file)) if file_mtime <= cutoff_time: old_files.append(file) return old_files def create_output_directory(base_output_directory): today = datetime.now().strftime(\'%Y-%m-%d\') output_directory = os.path.join(base_output_directory, today) if not os.path.exists(output_directory): os.makedirs(output_directory) return output_directory def move_files(log_files, output_directory): for file in log_files: shutil.move(file, output_directory) print(f\'Moving file {os.path.basename(file)} to {output_directory}\') def main(): parser = argparse.ArgumentParser(description=\'Organize and archive log files\') parser.add_argument(\'-d\', \'--directory\', required=True, help=\'Directory containing log files to process\') parser.add_argument(\'-o\', \'--output\', required=True, help=\'Output directory where archived files will be stored\') parser.add_argument(\'-n\', \'--days\', type=int, default=7, help=\'Number of days old a file must be to be archived (default is 7)\') args = parser.parse_args() log_files = get_log_files(args.directory, args.days) output_directory = create_output_directory(args.output) move_files(log_files, output_directory) if __name__ == \\"__main__\\": main()"},{"question":"Seaborn Advanced Plotting with Jitter Objective Create a function that generates a jittered scatter plot using seaborn\'s objects interface. Problem Statement You are provided with the `penguins` dataset from seaborn. Write a function `create_jittered_plot` that takes in three parameters: - `x_column`: A string representing the column name for the x-axis. - `y_column`: A string representing the column name for the y-axis. - `jitter_settings`: A dictionary containing keys for `width`, `x`, and `y` to define the jitter settings. Each key\'s value can be an integer or a float. The function should generate and display a scatter plot with jitter applied to the data points using the given settings. Function Signature ```python def create_jittered_plot(x_column: str, y_column: str, jitter_settings: dict) -> None: pass ``` Parameters - `x_column` (str): The name of the column to use for the x-axis. - `y_column` (str): The name of the column to use for the y-axis. - `jitter_settings` (dict): A dictionary with possible keys (`width`, `x`, `y`). The values should be numeric (int or float). If a key is not provided or is set to `None`, it should not affect the jitter settings. Constraints - Assume that the `x_column` and `y_column` are valid column names in the `penguins` dataset. - The `jitter_settings` dictionary may not contain all keys (`width`, `x`, `y`), and missing keys should be handled gracefully. - Only use the seaborn library\'s objects interface for creating the plots. Example Usage ```python jitter_settings = { \'width\': 0.5, \'x\': None, \'y\': 5 } create_jittered_plot(\\"bill_length_mm\\", \\"flipper_length_mm\\", jitter_settings) ``` This should create and display a scatter plot with jitter on the \\"bill_length_mm\\" x-axis and \\"flipper_length_mm\\" y-axis, where the width is set to 0.5 and y-axis jitter is set to 5. Expected Output A scatter plot where jitter is applied according to the provided settings. You can use the following code to load the `penguins` dataset: ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset def create_jittered_plot(x_column: str, y_column: str, jitter_settings: dict) -> None: Generates and displays a scatter plot with jitter applied to the data points as specified by the jitter_settings. :param x_column: A string representing the column name for the x-axis. :param y_column: A string representing the column name for the y-axis. :param jitter_settings: A dictionary with possible keys (\'width\', \'x\', \'y\'). The values should be numeric (int or float). penguins = load_dataset(\\"penguins\\") plot = so.Plot(penguins, x=x_column, y=y_column) jitter_args = {} if \'width\' in jitter_settings and jitter_settings[\'width\'] is not None: jitter_args[\'width\'] = jitter_settings[\'width\'] if \'x\' in jitter_settings and jitter_settings[\'x\'] is not None: jitter_args[\'x\'] = jitter_settings[\'x\'] if \'y\' in jitter_settings and jitter_settings[\'y\'] is not None: jitter_args[\'y\'] = jitter_settings[\'y\'] plot.add(so.Dot(), so.Jitter(**jitter_args)).show()"},{"question":"# Question: Implementing Custom Permutation Feature Importance **Objective**: Implement a function to compute feature importance via permutation. **Description**: Given a trained machine learning model, a validation dataset, and the number of permutation repeats, implement a function that calculates the permutation feature importance of each feature in the dataset. Your function should shuffle each feature\'s values several times (as specified by `n_repeats`), record the model\'s performance degradation, and return each feature\'s importance score. Your function should: 1. **Accept**: - `model`: A trained scikit-learn estimator object. - `X_val`: A pandas DataFrame or numpy array representing the validation dataset features. - `y_val`: A pandas Series or numpy array representing the validation dataset target. - `n_repeats`: An integer specifying the number of times to repeat the shuffling for each feature. - `scoring`: A string representing the scoring metric (e.g., `\'r2\'` for regression). 2. **Return**: - A dictionary with feature names (or indices if `X_val` is a numpy array) as keys and their corresponding importance scores as values. **Constraints**: - Assume `X_val` and `y_val` have no missing values. - Assume `model` has already been fitted on the training data. **Performance Requirement**: - Ensure the function efficiently handles datasets with up to 100,000 samples and 100 features. **Example Usage**: ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.metrics import r2_score import numpy as np import pandas as pd diabetes = load_diabetes() X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names) y = pd.Series(diabetes.target) X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0) model = Ridge(alpha=1e-2).fit(X_train, y_train) importance_scores = permutation_importance_custom(model, X_val, y_val, n_repeats=30, scoring=\'r2\') print(importance_scores) ``` **Function Signature**: ```python def permutation_importance_custom(model, X_val, y_val, n_repeats, scoring): # Your code implementation here ``` # Notes: - You can use scikit-learn\'s `metrics` module to compute the specified score. - Ensure your function handles both pandas DataFrames and numpy arrays as input for `X_val`.","solution":"import numpy as np import pandas as pd from sklearn.metrics import get_scorer def permutation_importance_custom(model, X_val, y_val, n_repeats, scoring): Compute the feature importance via permutation. Parameters: - model: A trained scikit-learn estimator object. - X_val: A pandas DataFrame or numpy array representing the validation dataset features. - y_val: A pandas Series or numpy array representing the validation dataset target. - n_repeats: An integer specifying the number of times to repeat the shuffling for each feature. - scoring: A string representing the scoring metric (e.g., \'r2\' for regression). Returns: - A dictionary with feature names (or indices if X_val is a numpy array) as keys and their corresponding importance scores as values. # Check if X_val is a DataFrame or numpy array and extract feature names or indices if isinstance(X_val, pd.DataFrame): feature_names = X_val.columns X_val = X_val.values else: feature_names = [f\'feature_{i}\' for i in range(X_val.shape[1])] # Define scorer based on the scoring string scorer = get_scorer(scoring) # Calculate the baseline score baseline_score = scorer(model, X_val, y_val) importance_scores = {feature: 0 for feature in feature_names} # Iterate over each feature for i, feature in enumerate(feature_names): score_drops = [] # Permute the feature and calculate the score drop for _ in range(n_repeats): X_val_permuted = X_val.copy() np.random.shuffle(X_val_permuted[:, i]) permuted_score = scorer(model, X_val_permuted, y_val) score_drop = baseline_score - permuted_score score_drops.append(score_drop) # Average the score drops for the feature importance_scores[feature] = np.mean(score_drops) return importance_scores"},{"question":"You are required to implement a sequence of file transformations using the deprecated `pipes` module. This will help you understand the functionality of pipelines and how they can be constructed and manipulated programmatically. **Objective**: 1. Create a `pipes.Template` object. 2. Append multiple commands to the pipeline. 3. Use the pipeline to transform the contents of an input file and write the transformed data to an output file. **Task**: Write a Python function `pipeline_transformations(input_file: str, output_file: str) -> None` that does the following: 1. Creates a new `pipes.Template` object. 2. Appends the following commands to the pipeline: - `cat` to read the input file (assume this is a valid POSIX system command). - `tr a-z A-Z` to transform all lower-case letters to upper-case. 3. Uses the pipeline to read contents from `input_file`, transform it, and write to `output_file`. **Function Signature**: ```python def pipeline_transformations(input_file: str, output_file: str) -> None: pass ``` **Input**: - `input_file` (string): Path to the input file. - `output_file` (string): Path to the output file. **Output**: - None **Constraints**: - Assume `input_file` and `output_file` exist and are accessible. - The system must support POSIX commands and `/bin/sh` shell. **Example**: Assume you have an input file named `input.txt` with the following content: ``` hello world this is a test ``` After running the `pipeline_transformations` function, the `output.txt` should contain: ``` HELLO WORLD THIS IS A TEST ``` **Solution Requirements**: Use the deprecated `pipes` module to demonstrate the pipeline operations. Although `pipes` is an old module, understanding its mechanics will help in grasping the fundamentals of process piping, which is still relevant in many programming and scripting environments. ```python import pipes def pipeline_transformations(input_file: str, output_file: str) -> None: # Create a pipes.Template object t = pipes.Template() # Append commands to the pipeline t.append(\'cat \' + input_file, \'--\') t.append(\'tr a-z A-Z\', \'--\') # Open the output file and process the pipeline with t.open(output_file, \'w\') as f: pass # File is written through the pipeline # Example usage # pipeline_transformations(\'input.txt\', \'output.txt\') ``` **Note**: Make sure the `inputs.txt` and `outputs.txt` paths are updated appropriately when testing the function.","solution":"import pipes def pipeline_transformations(input_file: str, output_file: str) -> None: # Create a pipes.Template object t = pipes.Template() # Append commands to the pipeline t.append(\'cat \' + input_file, \'--\') t.append(\'tr a-z A-Z\', \'--\') # Open the output file and process the pipeline with t.open(output_file, \'w\') as f: pass # File is written through the pipeline"},{"question":"Handling Nullable Integer Data Types in Pandas Your task is to implement a function that performs multiple operations on a DataFrame containing nullable integer data types. This question assesses both fundamental and advanced understanding of pandas with nullable integer types. Function: transform_nullable_integer_dataframe **Objective:** Create a function `transform_nullable_integer_dataframe` that: 1. Creates a DataFrame from a given list of dictionaries, where each dictionary contains keys \'id\', \'value\', and optionally \'category\'. 2. Assigns \'id\' and \'value\' as nullable integer types. 3. Adds a new column \'value_plus_10\' which is the \'value\' column with 10 added to each element. 4. Filters the rows where \'value_plus_10\' is greater than a given threshold. 5. Returns a Series of sums for \'value\' grouped by \'category\' (if \'category\' exists). **Function Signature:** ```python import pandas as pd def transform_nullable_integer_dataframe(data: list, threshold: int) -> pd.Series: pass ``` **Input:** - `data`: A list of dictionaries. Each dictionary can have: - \'id\': Integer (unique identifier) - \'value\': Integer or `None` (value associated with the \'id\') - \'category\': String or absent (optional category for grouping) - `threshold`: An integer; threshold for filtering \'value_plus_10\'. **Output:** - A pandas Series where the index is the \'category\' (if exists) and the values are the sums of \'value\' for each category. **Constraints:** - \'id\' should be treated as `Int64` dtype considering possible missing values. - \'value\' should be treated as `Int64` dtype considering possible missing values. - If \'category\' is absent in any dictionary, treat it as `None`. **Example:** ```python data = [ {\'id\': 1, \'value\': 10, \'category\': \'A\'}, {\'id\': 2, \'value\': 15, \'category\': \'B\'}, {\'id\': 3, \'value\': None, \'category\': \'A\'}, {\'id\': 4, \'value\': 8}, # Without a category {\'id\': None, \'value\': 20, \'category\': \'C\'} ] threshold = 18 # Expected Output Series: # A 10 # B 15 # C 20 # Name: value, dtype: Int64 ``` **Hints:** 1. Use `pd.Int64Dtype()` or \\"Int64\\" to create nullable integer columns. 2. Use arithmetic operations directly on the Series for step 3. 3. Utilize boolean indexing for filtering rows in step 4. 4. Use `groupby` and `sum` for aggregating values in step 5. Implement this function: ```python def transform_nullable_integer_dataframe(data: list, threshold: int) -> pd.Series: df = pd.DataFrame(data) df[\'id\'] = pd.array(df[\'id\'], dtype=\'Int64\') df[\'value\'] = pd.array(df[\'value\'], dtype=\'Int64\') df[\'value_plus_10\'] = df[\'value\'] + 10 df_filtered = df[df[\'value_plus_10\'] > threshold] if \'category\' in df_filtered.columns: result = df_filtered.groupby(\'category\')[\'value\'].sum() else: result = df_filtered[\'value\'].sum() return result ```","solution":"import pandas as pd def transform_nullable_integer_dataframe(data: list, threshold: int) -> pd.Series: # Create a DataFrame from the list of dictionaries df = pd.DataFrame(data) # Assign \'id\' and \'value\' as nullable integer types df[\'id\'] = pd.array(df[\'id\'], dtype=\'Int64\') df[\'value\'] = pd.array(df[\'value\'], dtype=\'Int64\') # Add a new column \'value_plus_10\' df[\'value_plus_10\'] = df[\'value\'] + 10 # Filter the rows where \'value_plus_10\' is greater than the given threshold df_filtered = df[df[\'value_plus_10\'] > threshold] # Return a Series of sums for \'value\' grouped by \'category\' (if \'category\' exists) if \'category\' in df_filtered.columns: result = df_filtered.groupby(\'category\')[\'value\'].sum() else: result = pd.Series(dtype=\'Int64\') return result"},{"question":"**Objective:** Create a Python program using the `imaplib` module to connect to an IMAP4 server, extract specific messages based on certain criteria, and perform some operations on them. # Problem Statement: Write a Python class `EmailHandler` that connects to an IMAP4 server securely using SSL, logs in a user, fetches email headers from the `INBOX` that meet specific criteria, and moves these emails to a different mailbox for further processing. # Requirements: 1. **Initialization:** - The constructor should accept the following parameters: - `server`: The IMAP server domain. - `username`: The user\'s email address. - `password`: The user\'s email password. - `ssl_context` (optional): An `ssl.SSLContext` object for the connection (default should be `None`). 2. **Connect and Login:** - Define a method `connect_and_login()` which establishes a secure connection to the server using `IMAP4_SSL`, and logs in using the provided credentials. 3. **Fetch Emails:** - Define a method `fetch_emails(criteria)` that: - Selects the `INBOX` mailbox. - Searches for emails matching the given `criteria`. - Fetches the headers of the matching emails. - Returns a list of tuples where each tuple contains the email number and the email header. 4. **Move Emails:** - Define a method `move_emails(email_numbers, destination_mailbox)` that: - Moves the specified emails (by email numbers) to the `destination_mailbox`. - Ensures emails are copied and then deleted from the `INBOX`. - Also, makes sure the changes are committed (use `expunge` method where necessary). 5. **Logout:** - Define a method `logout()` to properly logout from the IMAP server and close the connection. # Constraints: - Avoid using any third-party libraries other than `imaplib`. - The class should handle exceptions gracefully, providing meaningful error messages for any failures encountered during the operations. - Ensure that the connection to the server is closed even if an exception occurs. # Example Usage: ```python import ssl # Create an SSL context object (optional) ssl_context = ssl.create_default_context() # Initialize the EmailHandler class email_handler = EmailHandler(server=\\"imap.domain.com\\", username=\\"user@domain.com\\", password=\\"password\\", ssl_context=ssl_context) # Connect and login email_handler.connect_and_login() # Fetch emails that are unread emails = email_handler.fetch_emails(criteria=\\"UNSEEN\\") # Move unread emails to another mailbox email_handler.move_emails(email_numbers=[email[0] for email in emails], destination_mailbox=\\"Processed\\") # Logout from the server email_handler.logout() ``` # Notes: - You can refer to the `imaplib` module documentation for details on how to use various methods and handle IMAP4 commands. - Consider using the `with` statement for managing connections where appropriate. - Ensure the implementation is efficient and follows good coding practices.","solution":"import imaplib import ssl class EmailHandler: def __init__(self, server, username, password, ssl_context=None): Initialize the EmailHandler with server details and user credentials. self.server = server self.username = username self.password = password self.ssl_context = ssl_context self.connection = None def connect_and_login(self): Connect to the IMAP server and log in using user credentials. try: if self.ssl_context: self.connection = imaplib.IMAP4_SSL(self.server, ssl_context=self.ssl_context) else: self.connection = imaplib.IMAP4_SSL(self.server) self.connection.login(self.username, self.password) return True except Exception as e: print(f\\"Failed to connect and login: {e}\\") return False def fetch_emails(self, criteria): Fetch email headers from the INBOX that match the specified criteria. :param criteria: criteria string to filter emails :return: list of tuples (email number, email header) try: self.connection.select(\'INBOX\') result, data = self.connection.search(None, criteria) if result != \'OK\': print(\\"No messages found!\\") return [] emails = [] for num in data[0].split(): result, header_data = self.connection.fetch(num, \'(BODY[HEADER])\') if result == \'OK\': emails.append((num, header_data[0][1].decode(\'utf-8\'))) return emails except Exception as e: print(f\\"Failed to fetch emails: {e}\\") return [] def move_emails(self, email_numbers, destination_mailbox): Move emails to a different mailbox. :param email_numbers: list of email numbers to move :param destination_mailbox: target mailbox try: for num in email_numbers: self.connection.copy(num, destination_mailbox) self.connection.store(num, \'+FLAGS\', \'Deleted\') self.connection.expunge() except Exception as e: print(f\\"Failed to move emails: {e}\\") def logout(self): Logout from the server and close the connection. try: self.connection.logout() except Exception as e: print(f\\"Failed to logout: {e}\\")"},{"question":"# **Custom HTML Parsing for Attribute Extraction** **Objective:** Write a Python class that extends the `HTMLParser` class to extract all attributes from the start tags of the HTML elements in a given HTML document. # **Problem Statement:** You are required to create a custom HTML parser that extracts attributes from the start tags of HTML elements in a document and stores them in a dictionary. Your class should be named `AttributeExtractor`, and it should extend `html.parser.HTMLParser`. The parser should handle the following: 1. **Extract Start Tag Attributes**: For each start tag, extract its attributes and store them in a dictionary where the keys are the tag names and the values are lists of tuples representing the attributes. 2. **Document the Results**: Your class should have a method `get_attributes()` which returns the dictionary of extracted attributes. --- # **Function Specifications:** Class: `AttributeExtractor` - **Method**: `__init__(self)` - Initializes the parser and sets up the dictionary to store attributes. - **Method**: `handle_starttag(self, tag, attrs)` - Overrides the `handle_starttag` method to extract and store attributes. - **Method**: `get_attributes(self)` - Returns the dictionary of extracted attributes. --- # **Input and Output:** The input will be a string representing the HTML document. The output should be a dictionary where: - Each key is a tag name (as a string). - Each value is a list of tuples where each tuple represents an attribute (`name`, `value`). --- # **Constraints:** 1. The HTML content may contain nested elements. 2. All attribute names will be in lowercase. 3. There can be empty tags without attributes. --- # **Example:** Input: ```html <html> <head> <title>Sample Page</title> </head> <body> <h1 class=\\"header\\">Welcome</h1> <img src=\\"logo.png\\" alt=\\"Logo\\"> <a href=\\"https://example.com\\" title=\\"Example\\">Link</a> </body> </html> ``` Output: ```python { \'html\': [], \'head\': [], \'title\': [], \'body\': [], \'h1\': [(\'class\', \'header\')], \'img\': [(\'src\', \'logo.png\'), (\'alt\', \'Logo\')], \'a\': [(\'href\', \'https://example.com\'), (\'title\', \'Example\')] } ``` --- # **Task:** Implement the `AttributeExtractor` class with the specified methods and ensure it passes the provided example. ```python from html.parser import HTMLParser class AttributeExtractor(HTMLParser): def __init__(self): super().__init__() self.attributes = {} def handle_starttag(self, tag, attrs): if tag not in self.attributes: self.attributes[tag] = [] for attr in attrs: self.attributes[tag].append(attr) def get_attributes(self): return self.attributes # Example Usage html_content = \'\'\'<html> <head> <title>Sample Page</title> </head> <body> <h1 class=\\"header\\">Welcome</h1> <img src=\\"logo.png\\" alt=\\"Logo\\"> <a href=\\"https://example.com\\" title=\\"Example\\">Link</a> </body> </html>\'\'\' parser = AttributeExtractor() parser.feed(html_content) print(parser.get_attributes()) ``` Test your implementation with the provided input, and verify that the output matches the expected dictionary.","solution":"from html.parser import HTMLParser class AttributeExtractor(HTMLParser): def __init__(self): super().__init__() self.attributes = {} def handle_starttag(self, tag, attrs): if tag not in self.attributes: self.attributes[tag] = [] for attr in attrs: self.attributes[tag].append(attr) def get_attributes(self): return self.attributes # Example Usage html_content = \'\'\'<html> <head> <title>Sample Page</title> </head> <body> <h1 class=\\"header\\">Welcome</h1> <img src=\\"logo.png\\" alt=\\"Logo\\"> <a href=\\"https://example.com\\" title=\\"Example\\">Link</a> </body> </html>\'\'\' parser = AttributeExtractor() parser.feed(html_content) print(parser.get_attributes())"},{"question":"Objective: Demonstrate proficiency in using Python\'s standard library by creating a script that processes, manages, and persists data using specified modules. Problem Statement: You are required to implement a function `process_user_data(file_path: str, output_path: str) -> None`. This function will read a JSON file containing user data, manipulate this data to compute additional fields, and then save it in an optimized format for later retrieval. Requirements: 1. **Read User Data**: - The input file specified by `file_path` is a JSON file containing a list of user dictionaries. Each dictionary contains the keys: `\\"name\\"`, `\\"age\\"`, and `\\"join_date\\"`. - Example: ```json [ {\\"name\\": \\"John Doe\\", \\"age\\": 28, \\"join_date\\": \\"2022-01-15\\"}, {\\"name\\": \\"Jane Smith\\", \\"age\\": 22, \\"join_date\\": \\"2023-07-20\\"} ] ``` 2. **Process Data**: - Compute the number of days since each user joined (from `join_date` to the current date) and add this as a new key `\\"days_since_join\\"` in each user dictionary. 3. **Persist Data**: - Save the modified user data to a file specified by `output_path` using Python\'s `pickle` module. Constraints: - Ensure that the JSON input file is correctly formatted. Handle any potential errors gracefully. - The `join_date` is in the format `YYYY-MM-DD`. - The age of the user is always a positive integer less than 150. - Write robust code to handle edge cases, such as empty input files. Input: - `file_path`: A string representing the path to the JSON file containing user data. - `output_path`: A string representing the path to save the processed data using the `pickle` module. Output: - None (The function should not return anything. It should save the processed data to `output_path`). Function Signature: ```python def process_user_data(file_path: str, output_path: str) -> None: pass ``` Example: ```python # Assuming the current date is 2023-10-01 and the input file has the following content: # [ # {\\"name\\": \\"John Doe\\", \\"age\\": 28, \\"join_date\\": \\"2022-01-15\\"}, # {\\"name\\": \\"Jane Smith\\", \\"age\\": 22, \\"join_date\\": \\"2023-07-20\\"} # ] process_user_data(\'input.json\', \'output.pkl\') # The \'output.pkl\' file should contain data equivalent to: # [ # {\\"name\\": \\"John Doe\\", \\"age\\": 28, \\"join_date\\": \\"2022-01-15\\", \\"days_since_join\\": 624}, # {\\"name\\": \\"Jane Smith\\", \\"age\\": 22, \\"join_date\\": \\"2023-07-20\\", \\"days_since_join\\": 73} # ] ``` Assessment Criteria: - Correctness: The solution computes the number of days accurately and handles various edge cases. - Robustness: The code handles errors, such as file not found, invalid JSON format, etc. - Code Quality: The code is readable, follows PEP8 standards, and is well-documented. - Performance: The code efficiently processes the JSON data and writes the output.","solution":"import json import pickle from datetime import datetime from typing import List, Dict def read_user_data(file_path: str) -> List[Dict]: with open(file_path, \'r\') as file: data = json.load(file) return data def compute_days_since_join(join_date: str) -> int: join_date_obj = datetime.strptime(join_date, \'%Y-%m-%d\') current_date = datetime.now() delta = current_date - join_date_obj return delta.days def process_user_data(file_path: str, output_path: str) -> None: try: # Read user data from JSON file user_data = read_user_data(file_path) # Compute days since join for each user and add the new key for user in user_data: user[\'days_since_join\'] = compute_days_since_join(user[\'join_date\']) # Save the modified data using pickle with open(output_path, \'wb\') as file: pickle.dump(user_data, file) except FileNotFoundError: print(f\\"File not found: {file_path}\\") except json.JSONDecodeError: print(f\\"Invalid JSON format in file: {file_path}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of PyTorch environment variables related to CUDA configurations, especially for debugging and performance tuning. # Problem Statement You are provided with a deep learning model implemented in PyTorch that should be trained on a CUDA-enabled GPU. Your task is to: 1. Write a function that configures the CUDA environment variables to optimize the debugging process and understand the memory usage. 2. Train the provided PyTorch model using these configurations. 3. Analyze the impact of the configurations and provide insights about the memory usage and debugging experience. # Function Signature ```python def train_model_with_debugging_and_memory_configurations(model, train_loader, optimizer, loss_fn, num_epochs): Configures CUDA environment variables for debugging and memory management, then trains the model. Parameters: - model (torch.nn.Module): The PyTorch model to train. - train_loader (torch.utils.data.DataLoader): DataLoader for the training set. - optimizer (torch.optim.Optimizer): Optimizer for the training process. - loss_fn (torch.nn.modules.loss._Loss): Loss function for the training process. - num_epochs (int): Number of epochs to train the model. Returns: - None pass ``` # Constraints 1. Set the following CUDA environment variables before training the model: - `PYTORCH_NO_CUDA_MEMORY_CACHING=1` - `CUDA_LAUNCH_BLOCKING=1` - `TORCH_CUDNN_V8_API_DEBUG=1` 2. Training should be done on a CUDA-enabled GPU. Ensure to check if CUDA is available and handle cases where it is not. 3. The training loop should include the necessary code to perform forward and backward passes and update the model parameters. 4. Document the impact of the environment variables on the debugging process and memory usage within the code comments. # Example Usage ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset # Define a simple neural network class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Initialize model, optimizer, and loss function model = SimpleModel() optimizer = optim.SGD(model.parameters(), lr=0.01) loss_fn = nn.CrossEntropyLoss() # Dummy data x = torch.randn(100, 784) y = torch.randint(0, 10, (100,)) train_loader = DataLoader(TensorDataset(x, y), batch_size=32) # Train the model train_model_with_debugging_and_memory_configurations(model, train_loader, optimizer, loss_fn, num_epochs=2) ``` # Performance Requirements 1. The training function should handle the set number of epochs without running into out-of-memory errors. 2. The function should be well-commented, especially around the impact and purpose of the environment variables being set. # Deliverables 1. Python function as specified. 2. An explanation of the impact of the environment variables (within code comments).","solution":"import os import torch def train_model_with_debugging_and_memory_configurations(model, train_loader, optimizer, loss_fn, num_epochs): Configures CUDA environment variables for debugging and memory management, then trains the model. Parameters: - model (torch.nn.Module): The PyTorch model to train. - train_loader (torch.utils.data.DataLoader): DataLoader for the training set. - optimizer (torch.optim.Optimizer): Optimizer for the training process. - loss_fn (torch.nn.modules.loss._Loss): Loss function for the training process. - num_epochs (int): Number of epochs to train the model. Returns: - None # Set CUDA environment variables os.environ[\\"PYTORCH_NO_CUDA_MEMORY_CACHING\\"] = \\"1\\" os.environ[\\"CUDA_LAUNCH_BLOCKING\\"] = \\"1\\" os.environ[\\"TORCH_CUDNN_V8_API_DEBUG\\"] = \\"1\\" # Check if CUDA is available and move the model to GPU if possible device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model.to(device) for epoch in range(num_epochs): model.train() total_loss = 0.0 for batch in train_loader: inputs, targets = batch inputs, targets = inputs.to(device), targets.to(device) # Forward pass outputs = model(inputs) loss = loss_fn(outputs, targets) total_loss += loss.item() # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() print(f\\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\\") # Impact of the environment variables # - PYTORCH_NO_CUDA_MEMORY_CACHING=1: Disables the caching allocator, # intended to help with precise memory tracking at the cost of performance. # - CUDA_LAUNCH_BLOCKING=1: Makes CUDA operations synchronous, simplifying debugging by ensuring # errors are reported at the operation that caused them. # - TORCH_CUDNN_V8_API_DEBUG=1: Enables debugging features in the cuDNN library to help understand # performance characteristics and ensure correctness."},{"question":"**Coding Assessment Question** **Objective:** Demonstrate your understanding of the Seaborn `Plot` object and the faceting functionalities by creating a complex faceted plot. **Dataset:** You will use the `diamonds` dataset provided by Seaborn for this task. **Task:** 1. Load the `diamonds` dataset using `seaborn.load_dataset()`. 2. Create a faceted scatter plot of `carat` (X-axis) vs `price` (Y-axis) with the following specifications: - Facet the plot by the `cut` variable. - Additionally, facet the plot by the `color` variable. - Arrange the facets such that `cut` is on the columns and `color` is on the rows. - Ensure that the facets are arranged in the order: `cut` [\\"Fair\\", \\"Good\\", \\"Very Good\\", \\"Premium\\", \\"Ideal\\"] and `color` [\\"D\\", \\"E\\", \\"F\\", \\"G\\", \\"H\\", \\"I\\", \\"J\\"]. - Share the X-axis across all columns, but do not share the Y-axis. - Label each facet with the format \\"<cut> cut, Color <color>\\". **Input Format:** - No input required from users besides running the script. **Output Format:** - A faceted scatter plot visualizing the relationship between `carat` and `price` across different levels of `cut` and `color`. **Code Constraints:** - Use Seaborn\'s `Plot` object and its methods to achieve the task. Here is a template to get you started: ```python import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot p = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()) # Facet the plot by \'cut\' and \'color\' with specified orders p.facet(\\"cut\\", \\"color\\", order={\\"col\\": [\\"Fair\\", \\"Good\\", \\"Very Good\\", \\"Premium\\", \\"Ideal\\"], \\"row\\": [\\"D\\", \\"E\\", \\"F\\", \\"G\\", \\"H\\", \\"I\\", \\"J\\"]}) # Customize the scaling and labeling of facets p.share(x=True, y=False) p.label(title=\\"{} cut, Color {}\\".format) # Display the plot p.show() ``` Note: You may need to adjust your setup to visualize the plots correctly, especially if you are using a Jupyter Notebook or another environment. **Good luck!**","solution":"import seaborn.objects as so from seaborn import load_dataset def create_faceted_diamonds_plot(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot p = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()) # Facet the plot by \'cut\' and \'color\' with specified orders p.facet( row=\\"color\\", col=\\"cut\\", order={\\"col\\": [\\"Fair\\", \\"Good\\", \\"Very Good\\", \\"Premium\\", \\"Ideal\\"], \\"row\\": [\\"D\\", \\"E\\", \\"F\\", \\"G\\", \\"H\\", \\"I\\", \\"J\\"]} ) # Customize the scaling and labeling of facets p.share(x=True, y=False) p.label(title=\\"{} cut, Color {}\\".format) # Display the plot p.show() # Call the function to create and display the faceted plot create_faceted_diamonds_plot()"},{"question":"**Internationalization and Localized Greetings** # Objective: Implement a function that generates localized greeting messages based on user\'s preferred language and locale. The function should utilize the `gettext` module to fetch appropriate translations from message catalogs. # Requirements: 1. Use the GNU `gettext` API to handle translations. 2. Create message catalogs (in `.mo` files) for at least two different languages (e.g., English and Spanish). 3. Implement a function `localized_greeting(language_code: str, name: str) -> str` that: * Accepts a language code (e.g., `\'en\'` for English, `\'es\'` for Spanish) and a user\'s name. * Returns a greeting message in the specified language, addressing the user by name. # Constraints: - Ensure that the greeting message follows this structure: * In English: \\"Hello, [name]!\\" * In Spanish: \\"Â¡Hola, [name]!\\" # Example Usage: ```python # Assuming message catalogs for English and Spanish are properly set up print(localized_greeting(\'en\', \'Alice\')) # Output: \\"Hello, Alice!\\" print(localized_greeting(\'es\', \'Carlos\')) # Output: \\"Â¡Hola, Carlos!\\" ``` # Guideline: 1. Create translation files (`.po` and `.mo`) for at least two languages (English and Spanish). 2. Use the `gettext` module to load the appropriate translations based on the provided language code. 3. Handle any exceptions or edge cases, such as missing translation files or unsupported language codes. # Performance Requirements: - The function should be optimized to load translations efficiently to avoid significant delays in response. # Submission: - Provide the `localized_greeting` function implementation. - Include the steps to generate and compile the translation files (.po and .mo). - Ensure that the solution is clear, concise, and well-documented.","solution":"import gettext import os def setup_translations(): locales_dir = os.path.join(os.path.dirname(__file__), \'locales\') os.makedirs(locales_dir, exist_ok=True) # This setup assumes that the .mo files are already compiled and placed in appropriate directories # English: locales/en/LC_MESSAGES/messages.mo # Spanish: locales/es/LC_MESSAGES/messages.mo gettext.bindtextdomain(\'messages\', locales_dir) gettext.textdomain(\'messages\') def localized_greeting(language_code: str, name: str) -> str: Returns a localized greeting message in the specified language. :param language_code: Language code (e.g., \'en\' for English, \'es\' for Spanish). :param name: The name of the person to greet. :return: Localized greeting message. setup_translations() translation = gettext.translation(\'messages\', localedir=\'locales\', languages=[language_code], fallback=True) _ = translation.gettext if language_code == \'en\': return _(\\"Hello, {}!\\").format(name) elif language_code == \'es\': return _(\\"Â¡Hola, {}!\\").format(name) else: # Default to English if language is not supported return _(\\"Hello, {}!\\").format(name)"},{"question":"**Question**: You are tasked with setting up comprehensive logging for a Python application called `ComplexApp`. The application has multiple modules and specific logging requirements. Your goal is to create a logging configuration that satisfies the following conditions: 1. **Loggers**: - Create a `logger` named `ComplexApp` which serves as the parent logger. - Child loggers named `ComplexApp.module1` and `ComplexApp.module2`. 2. **Handlers**: - Add a `StreamHandler` that logs `INFO` level messages and higher to the console. - Add a `FileHandler` named `app.log` that records `DEBUG` level messages and higher. - Add a `RotatingFileHandler` named `error.log` specifically for `ERROR` level messages, allowing a maximum file size of 5MB before rotating, with up to 3 backup files. 3. **Formatters**: - The console logger should log messages with the format: `\\"[%(asctime)s] %(levelname)s - %(message)s\\"`. - The `app.log` file should include the format: `\\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\"`. - The `error.log` file should log messages with the format: `\\"%(asctime)s %(name)s %(levelname)s: %(message)s\\"`. - Ensure timestamps in the logs use the format: `\\"%d-%m-%Y %H:%M:%S\\"`. 4. **Filtering and custom logging**: - Implement a custom filter to exclude any log messages that contain the word `IGNORE`. 5. **Tests and logging statements**: - Create a script that initializes the logging configuration. - Write sample logging calls at various levels (`INFO`, `DEBUG`, `ERROR`) in `module1` and `module2`. - Capture a log output demonstrating that the configuration works as expected. Hereâ€™s the structure of the application: ``` ComplexApp/ â”œâ”€â”€ __init__.py â”œâ”€â”€ main.py â”œâ”€â”€ module1.py â””â”€â”€ module2.py ``` Write the necessary code to configure the logging for this application. Ensure that your solution is comprehensive and adheres to the requirements. **Input Format**: 1. Initialize the logging configuration within a function named `setup_logging()`. 2. Place logging calls in `module1.py` and `module2.py` to illustrate the logger setup. **Output Format**: The output should include the created log files (`app.log` and `error.log`), and displayed console logs showing compliance with the specified log format. **Constraints**: - Ensure the logging setup is efficient and adheres to best practices. - Use Python\'s built-in `logging` and `logging.handlers` modules to implement the solution. **Performance Requirements**: - The logging system should efficiently handle logging for a running application without significant performance overhead. Provide the complete code for `ComplexApp` with appropriate logging configurations and sample logging statements, ensuring all specified conditions are met.","solution":"import logging import logging.handlers import re class ExcludeFilter(logging.Filter): def filter(self, record): return \'IGNORE\' not in record.getMessage() def setup_logging(): # Create the main logger logger = logging.getLogger(\'ComplexApp\') logger.setLevel(logging.DEBUG) # Create console handler and set level to INFO console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # Create file handler for the main log file with level DEBUG file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # Create rotating file handler for error log with level ERROR error_handler = logging.handlers.RotatingFileHandler( \'error.log\', maxBytes=5 * 1024 * 1024, backupCount=3) error_handler.setLevel(logging.ERROR) # Create formatters and add them to the handlers console_formatter = logging.Formatter( \'[%(asctime)s] %(levelname)s - %(message)s\', datefmt=\'%d-%m-%Y %H:%M:%S\') console_handler.setFormatter(console_formatter) file_formatter = logging.Formatter( \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', datefmt=\'%d-%m-%Y %H:%M:%S\') file_handler.setFormatter(file_formatter) error_formatter = logging.Formatter( \'%(asctime)s %(name)s %(levelname)s: %(message)s\', datefmt=\'%d-%m-%Y %H:%M:%S\') error_handler.setFormatter(error_formatter) # Add filter to the handlers to exclude logs with \'IGNORE\' exclude_filter = ExcludeFilter() console_handler.addFilter(exclude_filter) file_handler.addFilter(exclude_filter) error_handler.addFilter(exclude_filter) # Add handlers to the main logger logger.addHandler(console_handler) logger.addHandler(file_handler) logger.addHandler(error_handler) # Create child loggers for module1 and module2 logging.getLogger(\'ComplexApp.module1\').setLevel(logging.DEBUG) logging.getLogger(\'ComplexApp.module2\').setLevel(logging.DEBUG) if __name__ == \\"__main__\\": setup_logging() logger = logging.getLogger(\'ComplexApp\') logger.info(\\"Application startup\\") logger.debug(\\"This is a debug message from the main application\\") logger.error(\\"This is an error message from the main application\\")"},{"question":"**Objective:** To assess your understanding of seaborn\'s data wrangling and visualization capabilities, you are tasked with implementing a function that loads and processes a dataset, then creates a composite plot. # Task: Write a function named `create_composite_plot()` that performs the following: 1. Loads the `fmri` dataset using `seaborn.load_dataset`. 2. Filters the dataset to include only records from the `parietal` region. 3. Creates a line plot showing the `signal` over `timepoint` with different `event`. 4. Adds a band to this plot to show error intervals for each `event`. # Function Signature: ```python def create_composite_plot() -> None: pass ``` # Expected Output: The function does not return any value. Instead, it should display a combined plot with lines and bands. # Detailed Steps: 1. **Load Dataset:** Use `seaborn.load_dataset(\\"fmri\\")` to load the dataset and filter it for rows where the `region` is `\'parietal\'`. 2. **Data Transformation:** Ensure that the dataframe has all necessary transformations applied for correct plotting. 3. **Plotting:** - Instantiate a `seaborn.objects.Plot` object for the filtered dataset. - Add a band to show the variation/error interval of the `signal` over time for each `event`. - Overlay a line plot showing the actual `signal` values over time for each `event`. # Constraints: - Ensure the function runs without any errors. - Utilize the `seaborn.objects` interface as shown in the provided documentation. - The plot display should follow proper aesthetic guidelines for clarity and interpretability. # Example: Here is how your plot might look: ```python from seaborn import objects as so from seaborn import load_dataset def create_composite_plot(): fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") p = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) p.show() ``` In this example, the function loads the `fmri` dataset, filters it, and then creates a composite seaborn plot. # Note: - Ensure you have `seaborn` installed to run the code. - Make sure the plot adheres to best practices for visualization, highlighting trends and intervals clearly.","solution":"from seaborn import objects as so from seaborn import load_dataset def create_composite_plot(): Loads the fmri dataset, filters it for the \'parietal\' region, and creates a line plot with error bands for the \'signal\' over \'timepoint\' by \'event\'. # Load dataset fmri = load_dataset(\\"fmri\\") # Filter dataset for \'parietal\' region fmri_parietal = fmri[fmri[\'region\'] == \'parietal\'] # Create the composite plot p = ( so.Plot(fmri_parietal, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) p.show()"},{"question":"You are provided with several Python C API macros for handling `datetime` objects. Your task is to implement a Python function that: 1. Creates a list of `datetime` objects given a list of dictionaries with relevant datetime information. 2. Checks the type of each created object to ensure it is a `datetime.datetime` object. 3. Extracts specific fields (year, month, day, hour, minute, second) from each `datetime` object and returns a summary of each. Function Signature ```python def process_date_info(date_info_list: List[Dict[str, int]]) -> List[Dict[str, int]]: pass ``` Input - `date_info_list`: A list of dictionaries where each dictionary contains keys: `year`, `month`, `day`, `hour`, `minute`, and `second`. Output - A list of dictionaries where each dictionary corresponds to the original input but includes an additional `type_check` key (boolean) indicating whether the object is a `datetime.datetime` object, and fields `extracted_year`, `extracted_month`, `extracted_day`, `extracted_hour`, `extracted_minute`, and `extracted_second` representing the extracted date-time parts from the created `datetime` object. Example ```python input_data = [ {\\"year\\": 2023, \\"month\\": 10, \\"day\\": 4, \\"hour\\": 16, \\"minute\\": 30, \\"second\\": 45}, {\\"year\\": 2022, \\"month\\": 5, \\"day\\": 14, \\"hour\\": 9, \\"minute\\": 15, \\"second\\": 0} ] output_data = process_date_info(input_data) print(output_data) ``` Expected Output: ```python [ { \\"year\\": 2023, \\"month\\": 10, \\"day\\": 4, \\"hour\\": 16, \\"minute\\": 30, \\"second\\": 45, \\"type_check\\": True, \\"extracted_year\\": 2023, \\"extracted_month\\": 10, \\"extracted_day\\": 4, \\"extracted_hour\\": 16, \\"extracted_minute\\": 30, \\"extracted_second\\": 45 }, { \\"year\\": 2022, \\"month\\": 5, \\"day\\": 14, \\"hour\\": 9, \\"minute\\": 15, \\"second\\": 0, \\"type_check\\": True, \\"extracted_year\\": 2022, \\"extracted_month\\": 5, \\"extracted_day\\": 14, \\"extracted_hour\\": 9, \\"extracted_minute\\": 15, \\"extracted_second\\": 0 } ] ``` Your implementation should handle the creation, type-checking, and field extraction using Python datetime module\'s capabilities. Constraints: - The values for `year`, `month`, `day`, `hour`, `minute`, and `second` in the `date_info_list` will always be valid integers. - You need to ensure type-checking, creation, and extraction operations mimic the behavior described in the provided documentation. Notes - You are not required to implement the exact macros provided in the documentation but should ensure your implementation aligns with their described behavior. - Focus on leveraging Python\'s `datetime` module to achieve the required functionality.","solution":"from datetime import datetime from typing import List, Dict def process_date_info(date_info_list: List[Dict[str, int]]) -> List[Dict[str, int]]: results = [] for date_info in date_info_list: # Create datetime object from the provided information dt = datetime( date_info[\'year\'], date_info[\'month\'], date_info[\'day\'], date_info[\'hour\'], date_info[\'minute\'], date_info[\'second\'] ) # Extracting the required information and type checking result = { **date_info, \\"type_check\\": isinstance(dt, datetime), \\"extracted_year\\": dt.year, \\"extracted_month\\": dt.month, \\"extracted_day\\": dt.day, \\"extracted_hour\\": dt.hour, \\"extracted_minute\\": dt.minute, \\"extracted_second\\": dt.second, } results.append(result) return results"},{"question":"Context You are tasked to facilitate a deep exploratory data analysis on the famous `penguins` dataset using the `seaborn` library. This dataset contains measurements for three different species of penguins: Adelie, Chinstrap, and Gentoo. Each row corresponds to a penguin, and the columns represent various attributes such as body mass, bill length, flipper length, etc. Task Write a Python function using Seaborn that: 1. Loads the `penguins` dataset. 2. Uses the `PairGrid` class to create a grid of scatterplots for the variables `body_mass_g`, `bill_length_mm`, and `flipper_length_mm`. 3. Adds histograms on the diagonals to show the distribution of these variables. 4. Colors the points by the `species` of penguins. 5. Uses different markers based on the penguin\'s sex (male/female) for the scatter plots. 6. Includes a legend to indicate the species and sex of the penguins. Function Signature ```python def create_penguin_pairgrid(): # Your implementation here pass ``` Expected Implementation Your function should create the described plot and display it. Use the following guidelines to get started: - Load the dataset using `sns.load_dataset(\\"penguins\\")`. - Use `sns.PairGrid` to set up the grid with the desired variables. - Map the scatter plot to the off-diagonal elements, set different markers based on the `sex` column. - Use histograms on the diagonal. - Color by `species` and add corresponding legends. Example Output Calling the function should display a plot that meets the criteria described. Thereâ€™s no need to return any value. Constraints - Ensure you handle any missing data in the `penguins` dataset. - Make sure the function handles any potential exceptions or errors gracefully. **Note:** Ensure seaborn and matplotlib are installed in your working environment. Use `sns.set_theme()` for consistent styling.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_penguin_pairgrid(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values penguins = penguins.dropna(subset=[\'body_mass_g\', \'bill_length_mm\', \'flipper_length_mm\', \'sex\', \'species\']) # Set up the seaborn theme for consistency in styling sns.set_theme(style=\\"whitegrid\\") # Set up a PairGrid g = sns.PairGrid(penguins, vars=[\'body_mass_g\', \'bill_length_mm\', \'flipper_length_mm\'], hue=\'species\', palette=\'Set2\', hue_kws={\\"marker\\": [\\"o\\", \\"s\\", \\"^\\"]}) # Map scatter plots to the off-diagonal elements g.map_offdiag(sns.scatterplot) # Map a histogram to the diagonal elements g.map_diag(sns.histplot) # Add a legend g.add_legend() # Show the plot plt.show()"},{"question":"# PyTorch Tensor Manipulation and Testing As a part of exercising and testing your PyTorch skills, you need to tackle both tensor operations and validation of their correctness using PyTorch\'s testing utilities. Task 1. Implement a function `process_tensor` that: - Takes in a 1-dimensional tensor of floats. - Scales the tensor by a given factor. - Adds a constant bias to the tensor. - Returns the resulting tensor. 2. Implement a function `test_tensor_processing` that: - Uses `torch.testing.make_tensor` to generate a random 1-dimensional tensor of floats. - Calls `process_tensor` with this tensor, a scaling factor, and a bias. - Constructs the expected result manually. - Uses `torch.testing.assert_allclose` to verify that the output of `process_tensor` matches the expected result within a reasonable tolerance. Function Signatures ```python def process_tensor(tensor: torch.Tensor, scaling_factor: float, bias: float) -> torch.Tensor: Scales the input tensor by the scaling factor and adds the bias to it. Parameters: tensor (torch.Tensor): A 1-dimensional tensor of floats. scaling_factor (float): The factor by which to scale the tensor. bias (float): The constant value to add to the scaled tensor. Returns: torch.Tensor: The processed tensor. pass def test_tensor_processing(): Tests the process_tensor function by generating a random tensor, manually constructs the expected result, and uses assert_allclose to validate the output. pass ``` Example ```python import torch import torch.testing # Example implementation def process_tensor(tensor: torch.Tensor, scaling_factor: float, bias: float) -> torch.Tensor: return tensor * scaling_factor + bias def test_tensor_processing(): # Generate a random 1D tensor tensor = torch.testing.make_tensor((5,), dtype=torch.float32) scaling_factor = 2.0 bias = 1.0 # Process the tensor result = process_tensor(tensor, scaling_factor, bias) # Manually construct the expected result expected_result = tensor * scaling_factor + bias # Verify the result torch.testing.assert_allclose(result, expected_result) # Run the test function to verify correctness test_tensor_processing() ``` **Input Constraints:** - The input tensor will be a 1-dimensional tensor of any size with float32 elements. - The scaling factor and bias will be float values. **Performance Constraints:** - The implementation should handle tensors of reasonably large sizes (up to 10,000 elements) efficiently. *Note: Ensure you have PyTorch installed and import the necessary components as shown in the example.*","solution":"import torch def process_tensor(tensor: torch.Tensor, scaling_factor: float, bias: float) -> torch.Tensor: Scales the input tensor by the scaling factor and adds the bias to it. Parameters: tensor (torch.Tensor): A 1-dimensional tensor of floats. scaling_factor (float): The factor by which to scale the tensor. bias (float): The constant value to add to the scaled tensor. Returns: torch.Tensor: The processed tensor. return tensor * scaling_factor + bias"},{"question":"# Persistent User Management System Your task is to implement a simple persistent user management system using the `pickle` module for object serialization and deserialization, and the `sqlite3` module for database interactions. Requirements: 1. **User Class**: - Implement a `User` class with the following attributes: - `id`: Unique identifier for each user (integer). - `name`: Name of the user (string). - `email`: Email address of the user (string). 2. **UserManager Class**: - Implement a `UserManager` class to handle the following operations: - Add a new user. - Remove an existing user by `id`. - Retrieve user details by `id`. - List all users. 3. **Data Persistence with Pickle**: - The `UserManager` should use the `pickle` module to save user data to a file named `users.pkl` when the application is closed and load the user data from it when the application starts. 4. **SQLite3 Database**: - The `UserManager` should also store user information in an SQLite3 database named `users.db` with a table `users` (columns: `id`, `name`, `email`). - Implement appropriate methods to save users to and load users from the database. 5. **Methods to Implement**: - `add_user(self, user: User) -> None`: Add a `User` object to the system. - `remove_user(self, user_id: int) -> None`: Remove a user by their `id`. - `get_user(self, user_id: int) -> Optional[User]`: Retrieve a user by their `id`. - `list_users(self) -> List[User]`: List all users. - `save_to_pickle(self) -> None`: Serialize user data to `users.pkl`. - `load_from_pickle(self) -> None`: Deserialize user data from `users.pkl`. - `save_to_db(self) -> None`: Save user data to `users.db`. - `load_from_db(self) -> None`: Load user data from `users.db`. # Constraints - Ensure thread safety for concurrent access. - Handle exceptions appropriately. - Maintain efficiency and avoid redundant data storage. - Your implementation should be Python 3.10 compatible. # Example Usage ```python def main(): user_manager = UserManager() # Load user data from pickle and database user_manager.load_from_pickle() user_manager.load_from_db() # Add some users user1 = User(id=1, name=\\"John Doe\\", email=\\"john@example.com\\") user2 = User(id=2, name=\\"Jane Smith\\", email=\\"jane@example.com\\") user_manager.add_user(user1) user_manager.add_user(user2) # List all users users = user_manager.list_users() for user in users: print(f\\"ID: {user.id}, Name: {user.name}, Email: {user.email}\\") # Remove a user user_manager.remove_user(1) # Print the remaining users users = user_manager.list_users() for user in users: print(f\\"ID: {user.id}, Name: {user.name}, Email: {user.email}\\") # Save user data to pickle and database user_manager.save_to_pickle() user_manager.save_to_db() if __name__ == \\"__main__\\": main() ``` Your code will be graded on correctness, efficiency, and adherence to the constraints.","solution":"import pickle import sqlite3 from typing import List, Optional class User: def __init__(self, user_id: int, name: str, email: str): self.id = user_id self.name = name self.email = email class UserManager: def __init__(self): self.users = [] self.db_file = \'users.db\' self.pickle_file = \'users.pkl\' self._initialize_db() self.load_from_pickle() self.load_from_db() def _initialize_db(self): conn = sqlite3.connect(self.db_file) cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS users ( id INTEGER PRIMARY KEY, name TEXT, email TEXT)\'\'\') conn.commit() conn.close() def add_user(self, user: User) -> None: if any(u.id == user.id for u in self.users): raise ValueError(f\\"User with id {user.id} already exists.\\") self.users.append(user) def remove_user(self, user_id: int) -> None: self.users = [u for u in self.users if u.id != user_id] def get_user(self, user_id: int) -> Optional[User]: for user in self.users: if user.id == user_id: return user return None def list_users(self) -> List[User]: return self.users def save_to_pickle(self) -> None: with open(self.pickle_file, \'wb\') as f: pickle.dump(self.users, f) def load_from_pickle(self) -> None: try: with open(self.pickle_file, \'rb\') as f: self.users = pickle.load(f) except FileNotFoundError: pass # It\'s fine if the pickle file doesn\'t exist yet def save_to_db(self) -> None: conn = sqlite3.connect(self.db_file) cursor = conn.cursor() cursor.execute(\\"DELETE FROM users\\") # Clear existing data for user in self.users: cursor.execute(\\"INSERT INTO users (id, name, email) VALUES (?, ?, ?)\\", (user.id, user.name, user.email)) conn.commit() conn.close() def load_from_db(self) -> None: conn = sqlite3.connect(self.db_file) cursor = conn.cursor() cursor.execute(\\"SELECT id, name, email FROM users\\") rows = cursor.fetchall() self.users = [User(user_id=row[0], name=row[1], email=row[2]) for row in rows] conn.close()"},{"question":"Title: Creating and Manipulating Custom Types Using Python C API **Objective**: Write a Python C extension module that defines a custom heap-allocated type. This type should include methods to check type flags and manipulate type features. **Instructions**: 1. **Creating a Custom Type**: - Define a custom type named `MyCustomType` using `PyType_FromSpec`. - Ensure the custom type has basic attributes such as `name`, `basicsize`, and necessary flags. 2. **Type Methods**: - Implement a method `is_subtype` to check if a given object is a subtype of `MyCustomType`. - Implement a method `get_type_flags` to retrieve the type flags of a given `PyTypeObject`. 3. **Type Operations**: - Create an instance of `MyCustomType` in the module\'s `init` function. - Implement a method `modify_type` to dynamically modify an attribute of `MyCustomType` and invalidate the internal cache using `PyType_Modified`. **Constraints**: - You must use the provided functions and structs such as `PyType_FromSpec`, `PyType_GetFlags`, `PyType_Modified`, etc. - Ensure proper memory management practices (e.g., reference counting) are followed. - The solution should compile and run without errors. **Input**: - Various test cases will be run to ensure the correctness of your module implementation. **Output**: - The module should correctly define the custom type, and the methods should perform the specified operations. **Example Usage**: ```python import mycustommodule # Create an instance of MyCustomType obj = mycustommodule.create_instance() # Check if obj is a subtype of MyCustomType assert mycustommodule.is_subtype(obj) # Get type flags of MyCustomType flags = mycustommodule.get_type_flags() # Modify the type and invalidate internal cache mycustommodule.modify_type(\'attribute_name\', \'new_value\') ``` Ensure your implementation is thoroughly tested and documented to help others understand your code.","solution":"# The real implementation would be done in C using Python C API, but here is a Python approximation for the # purpose of demonstrating the structure. class MyCustomType: A custom type that is typically defined using Python C API. def __init__(self): self.attributes = {} def is_subtype(obj): Check if the given object is a subtype of MyCustomType. return isinstance(obj, MyCustomType) def get_type_flags(obj): Retrieve the type flags of a given type object. In a Python C extension, this would include using PyType_GetFlags. Here, we pretend to fetch flags. # Simulating type flags with a mock value for demonstration purposes # Actual implementation would use PyType_GetFlags(Py_TYPE(obj)) in C. TYPE_FLAGS = 0x2000 # Example flag value, just for demo purposes if is_subtype(obj): return TYPE_FLAGS return None def modify_type(obj, attr_name, new_value): Dynamically modify an attribute of MyCustomType and invalidate the internal cache. The real implementation would use PyType_Modified in Python C API. if is_subtype(obj): obj.attributes[attr_name] = new_value # Simulate calling PyType_Modified by doing a no-op return True return False def create_instance(): Create an instance of MyCustomType. return MyCustomType()"},{"question":"# Configuration File Parser Exercise Problem Statement You are required to write a Python function that processes a configuration file using the `configparser` module. The function should read a provided INI file, apply specific transformations to the values, and write the updated configuration to a new file. The transformations required are based on the following rules: 1. **String Interpolation**: If a value contains placeholders (e.g., `{username}`, `{password}`), replace them with the values from a provided dictionary. 2. **Data Type Conversion**: - Integers should be squared. - Booleans should be inverted (True becomes False, and vice versa). - Strings should be reversed. Input 1. `config_file`: A string representing the file path to the input configuration file (INI format). 2. `output_file`: A string representing the file path where the updated configuration should be saved. 3. `replace_dict`: A dictionary with keys and values that should be used for string interpolation within the configuration file. Output - The function should write the updated configuration to the specified `output_file`. Constraints - Assume all values in the configuration file are either strings, integers, or booleans. - The configuration file may have different sections. - The interpolation only applies to string values containing placeholders. Function Signature ```python def process_configuration(config_file: str, output_file: str, replace_dict: dict) -> None: pass ``` Example **Input Configuration File (`config.ini`):** ```ini [User] username={user} password={pass} [Settings] resolution=1920 fullscreen=True [Data] path=/home/user/data buffer_size=16 ``` **Replacement Dictionary:** ```python replace_dict = {\'user\': \'john_doe\', \'pass\': \'supersecret\'} ``` **Output Configuration File (`config_updated.ini`):** ```ini [User] username=eoD_nhoj password=tercesrepus [Settings] resolution=3686400 fullscreen=False [Data] path=atad/resu/emoh/ buffer_size=256 ``` Hints - Utilize `configparser.ConfigParser` and its methods for reading and writing configuration files. - To handle string interpolation, you may use the `Interpolation` feature provided by `configparser` or manually replace placeholders using the `replace_dict`. Good luck, and think through how you will handle different data types and the required transformations!","solution":"import configparser def process_configuration(config_file: str, output_file: str, replace_dict: dict) -> None: config = configparser.ConfigParser() config.read(config_file) for section in config.sections(): for key in config[section]: value = config[section][key] if value.startswith(\'{\') and value.endswith(\'}\'): # Check for placeholders and replace them placeholder = value[2:-1] if placeholder in replace_dict: value = replace_dict[placeholder] if value.isdigit(): # Transform integers by squaring them config[section][key] = str(int(value) ** 2) elif value.lower() in [\\"true\\", \\"false\\"]: # Invert booleans config[section][key] = \'False\' if value.lower() == \'true\' else \'True\' else: # Reverse strings config[section][key] = value[::-1] with open(output_file, \'w\') as configfile: config.write(configfile)"},{"question":"# **Problem: Composable Function Transforms in PyTorch** You are given a function `f` that takes a 2D tensor (matrix) as input and returns a scalar value. Your task is to implement a new function that computes the per-sample gradients for a batch of such matrices using `torch.func`. # **Instructions:** 1. **Gradient Function:** - Create a function `compute_gradient(f)` that returns a new function computing the gradient of `f` with respect to its input. 2. **Vectorized Gradient Function:** - Using the gradient function from step 1, create a function `batch_compute_gradient(f)` that returns a new function computing the per-sample gradients for a batch of inputs. 3. **Implementation Details:** - You must use the `torch.func.grad` and `torch.func.vmap` to achieve the desired transformations. - The input to the new function will be a batch of 2D tensors (with shape `(batch_size, m, n)`), and the output should be a tensor of gradients with the same shape `(batch_size, m, n)`. # **Function Signatures:** ```python import torch import torch.func as tf def compute_gradient(f: callable) -> callable: Computes the gradient of the function f with respect to its input. Args: - f (callable): A function that takes a 2D tensor and returns a scalar. Returns: - callable: A function that computes the gradient of f. # Your code here def batch_compute_gradient(f: callable) -> callable: Computes the per-sample gradients for a batch of inputs. Args: - f (callable): A function that takes a 2D tensor and returns a scalar. Returns: - callable: A function that computes the per-sample gradients. # Your code here # Example usage: # Define your function f def f(x): return torch.sum(x ** 2) # Get the gradient function grad_f = compute_gradient(f) # Get the batch gradient function batch_grad_f = batch_compute_gradient(f) # Example input batch batch_input = torch.randn((5, 3, 3)) # batch of 5 matrices of size 3x3 batch_gradients = batch_grad_f(batch_input) print(batch_gradients.shape) # Should print: torch.Size([5, 3, 3]) ``` # **Constraints:** - Ensure that the function handles batches efficiently. - You can assume the input to `f` will always be a 2D tensor, and the batch input will be a 3D tensor. # **Performance Requirements:** - The implemented functions should be efficient in terms of both computation and memory usage, leveraging PyTorch\'s internal optimizations.","solution":"import torch import torch.func as tf def compute_gradient(f: callable) -> callable: Computes the gradient of the function f with respect to its input. Args: - f (callable): A function that takes a 2D tensor and returns a scalar. Returns: - callable: A function that computes the gradient of f. return tf.grad(f) def batch_compute_gradient(f: callable) -> callable: Computes the per-sample gradients for a batch of inputs. Args: - f (callable): A function that takes a 2D tensor and returns a scalar. Returns: - callable: A function that computes the per-sample gradients. grad_f = compute_gradient(f) return tf.vmap(grad_f) # Example usage: # Define your function f def f(x): return torch.sum(x ** 2) # Get the gradient function grad_f = compute_gradient(f) # Get the batch gradient function batch_grad_f = batch_compute_gradient(f) # Example input batch batch_input = torch.randn((5, 3, 3)) # batch of 5 matrices of size 3x3 batch_gradients = batch_grad_f(batch_input) print(batch_gradients.shape) # Should print: torch.Size([5, 3, 3])"},{"question":"# Custom Traceback Handler Implementation You are tasked with creating a custom traceback handler using the `cgitb` module to aid in debugging a Python script. The goal is to capture and log detailed error information whenever an uncaught exception occurs, while also providing an option to disable the browser display. Requirements: 1. Write a function `custom_traceback_handler(logdir: str, display: bool = False, context: int = 5, format: str = \'html\')` that sets up the `cgitb` module to handle uncaught exceptions. 2. The function should: - Enable `cgitb` with the given parameters. - Ensure that the traceback information is logged to files in the specified directory (`logdir`). - Control the display of the traceback in the browser based on the `display` parameter. (True to display, False to suppress) - Set the number of lines to display around the current line of source code in the traceback using the `context` parameter. - Format the output as specified by the `format` parameter (`html` for HTML format and any other value for plain text). Input: - `logdir` (str): The directory where the traceback reports should be saved. - `display` (bool, optional): Whether to display the traceback in the browser. Defaults to False. - `context` (int, optional): Number of lines of context to display around the current line of source code in the traceback. Defaults to 5. - `format` (str, optional): The format of the output (\'html\' for HTML, any other value for plain text). Defaults to \'html\'. Output: - This function does not return any value. It sets up the `cgitb` module for detailed exception handling based on the provided parameters. Example usage: ```python # Setting up the custom traceback handler to log traceback information to \'logs/\' directory, # display in the browser, with 7 lines of context, and formatted as plain text. custom_traceback_handler(logdir=\'logs\', display=True, context=7, format=\'text\') # Example of a code snippet that will generate an exception def buggy_function(): return 1 / 0 # Division by zero error buggy_function() ``` # Constraints: - The `logdir` directory should already exist. - Ensure that exception logging does not impact the main flow of the script. Implement the `custom_traceback_handler` function according to the specifications given.","solution":"import cgitb import os def custom_traceback_handler(logdir: str, display: bool = False, context: int = 5, format: str = \'html\'): Sets up the cgitb module to handle uncaught exceptions with detailed error information logged to files. Parameters: logdir (str): Directory to save traceback reports. display (bool): Whether to display traceback in the browser. Defaults to False. context (int): Number of lines of context to display around the current line of source code in the traceback. Defaults to 5. format (str): Format of the output (\'html\' for HTML, others for plain text). Defaults to \'html\'. if not os.path.exists(logdir): raise ValueError(f\\"Log directory \'{logdir}\' does not exist.\\") cgitb.enable( display=display, logdir=logdir, context=context, format=format )"},{"question":"# Complex Number Operations with cmath **Objective:** Implement a function that performs various operations on complex numbers using the `cmath` module. **Function Signature:** ```python def complex_operations(z: complex) -> dict: pass ``` **Input:** - `z` (complex): A complex number input. **Output:** - A dictionary with the following keys and corresponding values: - `\\"polar\\"`: The polar coordinates of `z` as a tuple `(r, phi)`. - `\\"rectangular\\"`: The rectangular coordinates of `z` reconstructed from its polar coordinates as a complex number. - `\\"exp\\"`: The value of ( e^z ). - `\\"log\\"`: The natural logarithm of `z`. - `\\"log10\\"`: The base-10 logarithm of `z`. - `\\"sqrt\\"`: The square root of `z`. - `\\"cos\\"`: The cosine of `z`. - `\\"sin\\"`: The sine of `z`. - `\\"tan\\"`: The tangent of `z`. - `\\"cosh\\"`: The hyperbolic cosine of `z`. - `\\"sinh\\"`: The hyperbolic sine of `z`. - `\\"tanh\\"`: The hyperbolic tangent of `z`. **Constraints:** - The input `z` will be a valid complex number. - Use `cmath` functions exclusively for the implementation. **Performance Requirements:** - The function should handle relatively large values of `z` as well. **Example:** ```python z = 1 + 1j result = complex_operations(z) print(result) ``` **Expected Output:** ```json { \\"polar\\": (1.4142135623730951, 0.7853981633974483), \\"rectangular\\": (1+1j), \\"exp\\": (1.4686939399158851+2.2873552871788423j), \\"log\\": (0.34657359027997264+0.7853981633974483j), \\"log10\\": (0.15051499783199057+0.3410940884604603j), \\"sqrt\\": (1.09868411346781+0.45508986056222733j), \\"cos\\": (0.8337300251311491-0.9888977057628651j), \\"sin\\": (1.2984575814159773+0.6349639147847361j), \\"tan\\": (0.27175258531951174+1.0839233273386946j), \\"cosh\\": (0.8337300251311491+0.9888977057628651j), \\"sinh\\": (0.6349639147847361+1.2984575814159773j), \\"tanh\\": (1.0839233273386946+0.27175258531951174j) } ``` **Your Task:** Complete the function `complex_operations` to perform the specified operations using the `cmath` module and return the results as a dictionary in the specified format.","solution":"import cmath def complex_operations(z: complex) -> dict: Perform various operations on a complex number using the cmath module. Parameters: z (complex): A complex number input. Returns: dict: A dictionary containing the results of various operations. result = { \\"polar\\": cmath.polar(z), \\"rectangular\\": cmath.rect(*cmath.polar(z)), \\"exp\\": cmath.exp(z), \\"log\\": cmath.log(z), \\"log10\\": cmath.log10(z), \\"sqrt\\": cmath.sqrt(z), \\"cos\\": cmath.cos(z), \\"sin\\": cmath.sin(z), \\"tan\\": cmath.tan(z), \\"cosh\\": cmath.cosh(z), \\"sinh\\": cmath.sinh(z), \\"tanh\\": cmath.tanh(z) } return result"},{"question":"# Question: Geometrical Shape Area and Perimeter Calculator You need to implement a function `calculate_areas_and_perimeters(shapes)` that takes a list of shapes and returns a list of dictionaries. Each dictionary will contain the shape type, area, and perimeter of the shape from the input list. The shapes can be of the following types: 1. **Circle** with attribute radius. 2. **Rectangle** with attributes length and width. 3. **Triangle** with attributes sides a, b, and c. **Input:** - A list of dictionaries where each dictionary represents a shape and has the following keys: - \\"type\\": A string that could be \\"circle\\", \\"rectangle\\", or \\"triangle\\". - \\"attributes\\": A dictionary containing the necessary attributes to define the shape. Example: ```python shapes = [ {\\"type\\": \\"circle\\", \\"attributes\\": {\\"radius\\": 5}}, {\\"type\\": \\"rectangle\\", \\"attributes\\": {\\"length\\": 4, \\"width\\": 6}}, {\\"type\\": \\"triangle\\", \\"attributes\\": {\\"a\\": 3, \\"b\\": 4, \\"c\\": 5}} ] ``` **Output:** - A list of dictionaries where each dictionary represents the calculated properties of the input shapes and has the following keys: - \\"type\\": The type of the shape as in the input. - \\"area\\": The area of the shape. - \\"perimeter\\": The perimeter of the shape. Example: ```python [ {\\"type\\": \\"circle\\", \\"area\\": 78.53981633974483, \\"perimeter\\": 31.41592653589793}, {\\"type\\": \\"rectangle\\", \\"area\\": 24.0, \\"perimeter\\": 20.0}, {\\"type\\": \\"triangle\\", \\"area\\": 6.0, \\"perimeter\\": 12.0} ] ``` **Constraints and Limitations:** - The radius, length, width, and sides of the triangle will be positive integers/floats. - Use the math module\'s functions wherever applicable. - Ensure the results are accurate and handle floating-point precision correctly. **Function Signature:** ```python import math def calculate_areas_and_perimeters(shapes): # Your code here ``` # Example: ```python shapes = [ {\\"type\\": \\"circle\\", \\"attributes\\": {\\"radius\\": 5}}, {\\"type\\": \\"rectangle\\", \\"attributes\\": {\\"length\\": 4, \\"width\\": 6}}, {\\"type\\": \\"triangle\\", \\"attributes\\": {\\"a\\": 3, \\"b\\": 4, \\"c\\": 5}} ] print(calculate_areas_and_perimeters(shapes)) ``` Output: ```python [ {\\"type\\": \\"circle\\", \\"area\\": 78.53981633974483, \\"perimeter\\": 31.41592653589793}, {\\"type\\": \\"rectangle\\", \\"area\\": 24.0, \\"perimeter\\": 20.0}, {\\"type\\": \\"triangle\\", \\"area\\": 6.0, \\"perimeter\\": 12.0} ] ``` # Hint: - For the area of a circle, use `math.pi * radius ** 2`. - For the perimeter of a circle, use `2 * math.pi * radius`. - For the area of a rectangle, use `length * width`. - For the perimeter of a rectangle, use `2 * (length + width)`. - For the perimeter of a triangle, use `a + b + c`. - For the area of a triangle, use Heron\'s formula: - ( s = frac{a + b + c}{2} ) - ( text{area} = sqrt{s cdot (s - a) cdot (s - b) cdot (s - c)} )","solution":"import math def calculate_areas_and_perimeters(shapes): results = [] for shape in shapes: shape_type = shape[\\"type\\"] attributes = shape[\\"attributes\\"] if shape_type == \\"circle\\": radius = attributes[\\"radius\\"] area = math.pi * radius ** 2 perimeter = 2 * math.pi * radius elif shape_type == \\"rectangle\\": length = attributes[\\"length\\"] width = attributes[\\"width\\"] area = length * width perimeter = 2 * (length + width) elif shape_type == \\"triangle\\": a = attributes[\\"a\\"] b = attributes[\\"b\\"] c = attributes[\\"c\\"] s = (a + b + c) / 2 area = math.sqrt(s * (s - a) * (s - b) * (s - c)) perimeter = a + b + c results.append({ \\"type\\": shape_type, \\"area\\": area, \\"perimeter\\": perimeter }) return results"},{"question":"# Python Asyncio and Event Loops Challenge You are required to create a script that leverages the `asyncio` module to manage multiple asynchronous tasks. Your solution will need to create an event loop, schedule tasks, handle subprocesses on Windows and macOS, and consider the limitations described in the provided documentation. Task Requirements: 1. **Create an Asynchronous Function:** - Write an async function `fetch_data(url)` that fetches data from a given URL and returns the content. - Use the `aiohttp` library for asynchronous HTTP requests. 2. **Handle Multiple URLs:** - Create another async function `gather_data(urls)` that takes a list of URLs and concurrently fetches data from each URL using `fetch_data`. - Ensure it runs efficiently without exceeding platform-specific limitations. 3. **Platform-Specific Subprocess Handling:** - Add functionality to execute a subprocess asynchronously using `asyncio.create_subprocess_exec` (available on Windows under `ProactorEventLoop` and all modern macOS). - Write an async function `run_subprocess(command)` that runs a given shell command and returns its output. 4. **Combine All Functionalities:** - Create the main entry point of the script to set up the appropriate event loop based on the platform. Ensure the subprocess functionality works correctly on supported platforms and provide appropriate error messages on unsupported configurations. - Demonstrate calling `gather_data(urls)` with at least 3 different URLs and `run_subprocess(command)` with at least one command. - Print the results of both data fetching and subprocess execution. Input Format: - A list of URLs that need to be fetched. - A shell command to be executed. Output Format: - Print fetched data for each URL. - Print the output of the shell command execution. Constraints and Considerations: - Ensure compatibility with both Windows and macOS. - Handle scenarios where specific functionalities are unsupported on certain platforms gracefully. - Optimize for performance while adhering to platform limitations. Provide your implementation in a single Python script. Example: ```python import asyncio import platform import aiohttp async def fetch_data(url): async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def gather_data(urls): tasks = [fetch_data(url) for url in urls] return await asyncio.gather(*tasks) async def run_subprocess(command): process = await asyncio.create_subprocess_exec( *command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return stdout.decode() if stdout else stderr.decode() def main(): urls = [\\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\"] command = [\\"echo\\", \\"Hello, World!\\"] if platform.system() == \\"Windows\\": loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) else: loop = asyncio.get_event_loop() results = loop.run_until_complete(gather_data(urls)) for idx, result in enumerate(results): print(f\\"Data from URL {urls[idx]}: {result[:100]}...\\") command_output = loop.run_until_complete(run_subprocess(command)) print(f\\"Command output: {command_output}\\") if __name__ == \\"__main__\\": main() ``` Note: Ensure `aiohttp` library is installed before running the code (`pip install aiohttp`).","solution":"import asyncio import platform import aiohttp async def fetch_data(url): async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def gather_data(urls): tasks = [fetch_data(url) for url in urls] return await asyncio.gather(*tasks) async def run_subprocess(command): process = await asyncio.create_subprocess_exec( *command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return stdout.decode() if stdout else stderr.decode() def main(): urls = [\\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\"] command = [\\"echo\\", \\"Hello, World!\\"] if platform.system() == \\"Windows\\": loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) else: loop = asyncio.get_event_loop() results = loop.run_until_complete(gather_data(urls)) for idx, result in enumerate(results): print(f\\"Data from URL {urls[idx]}: {result[:100]}...\\") command_output = loop.run_until_complete(run_subprocess(command)) print(f\\"Command output: {command_output}\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective: Demonstrate your understanding of PyTorch\'s `torch.futures` package to handle asynchronous operations. Problem Description: You are required to implement a function that performs multiple asynchronous operations and collects the results using PyTorch\'s `Future` type and utility functions. Function Signature: ```python import torch import time def asynchronous_operations(tasks: list, delay: float) -> list: Function to execute a list of tasks asynchronously with a specified delay. Args: - tasks (list): A list of callable functions that perform some computations. - delay (float): A float representing the number of seconds to wait before returning a result. Returns: - list: A list of results from the executed tasks. # Your implementation here ``` Input: - `tasks`: A list of callable functions. Each function takes no arguments and returns a result. - `delay`: A float representing the time delay (in seconds) each task should wait before returning a result. Output: - A list of results from the executed tasks. Constraints: - The length of `tasks` will not exceed 10. - The `delay` will not exceed 5.0 seconds. - You need to use the `Future` type from the `torch.futures` package to handle asynchronous executions. Example: ```python import time def task1(): time.sleep(1) return \\"Task 1 Complete\\" def task2(): time.sleep(2) return \\"Task 2 Complete\\" tasks = [task1, task2] delay = 1.0 result = asynchronous_operations(tasks, delay) print(result) # Output: [\\"Task 1 Complete\\", \\"Task 2 Complete\\"] ``` Additional Information: - You may use `torch.jit.fork` to simulate asynchronous execution of tasks. - Use functions like `collect_all` from `torch.futures` to wait for all tasks to complete. Notes: This problem is designed to test your ability to work with asynchronous operations and PyTorch\'s `Future` type. Ensure that your solution handles the asynchrony correctly and collects the results efficiently.","solution":"import torch import time def asynchronous_operations(tasks: list, delay: float) -> list: Function to execute a list of tasks asynchronously with a specified delay. Args: - tasks (list): A list of callable functions that perform some computations. - delay (float): A float representing the number of seconds to wait before returning a result. Returns: - list: A list of results from the executed tasks. def delayed_task(task): time.sleep(delay) return task() futures = [torch.jit.fork(delayed_task, task) for task in tasks] results = [future.wait() for future in futures] return results"},{"question":"Coding Assessment Question # Objective Create a time series plot using Seaborn\'s `objects` interface that demonstrates key customization features. You will work with a dataset and visualize its time-based trends while customizing various aspects of the plot. # Task 1. Load the \\"seaice\\" dataset using `seaborn.load_dataset`. 2. Create a `seaborn.objects.Plot` that displays the sea ice extent over time. 3. Customize the plot by: - Adding lines to represent the data points connection. - Faceting the plot by decades. - Customizing linewidth and color of the lines. - Scaling the color to adjust brightness and rotation. - Setting the layout size. - Adding a title that dynamically displays the decade. # Expected Input and Output Formats **Input:** - None (dataset is loaded within the function) **Output:** - Display a customized time series plot according to the specifications. # Constraints and Limitations 1. You must use Seaborn\'s `objects` interface for plotting. 2. Ensure all customization features are included as specified. 3. The plot should be correctly faceted by decades. # Performance Requirements - The solution should be efficient in handling and plotting the dataset. # Example Here is an example of what the function might look like: ```python import seaborn.objects as so from seaborn import load_dataset def create_custom_seaice_plot(): # Step 1: Load the dataset seaice = load_dataset(\\"seaice\\") # Step 2: Create a plot of sea ice extent over time plot = ( so.Plot(x=seaice[\\"Date\\"].dt.day_of_year, y=seaice[\\"Extent\\"], color=seaice[\\"Date\\"].dt.year) .facet(seaice[\\"Date\\"].dt.year.round(-1)) .add(so.Lines(linewidth=0.5, color=\\"#bbca\\"), col=None) .add(so.Lines(linewidth=1)) .scale(color=\\"ch:rot=-.2,light=.7\\") .layout(size=(8, 4)) .label(title=\\"{}s\\".format) ) # Display the plot plot.show() # Call the function to see the plot create_custom_seaice_plot() ``` Ensure that your solution meets the specifications and includes all necessary customizations.","solution":"import seaborn as sns import seaborn.objects as so def create_custom_seaice_plot(): # Load the dataset seaice = sns.load_dataset(\\"seaice\\") # Extract the decade from the year seaice[\'Decade\'] = (seaice[\'Date\'].dt.year // 10) * 10 # Plot customization plot = ( so.Plot(data=seaice, x=\\"Date\\", y=\\"Extent\\") .scale(color=\\"ch:rot=-.2,light=.7\\") .add(so.Line(linewidth=1)) .facet(\\"Decade\\") .layout(size=(10, 5)) .label(title=\\"Sea Ice Extent in {Decade}s\\") ) # Show the plot plot.show() # Call the function to see the plot create_custom_seaice_plot()"},{"question":"# Seaborn Coding Assessment Objective The purpose of this assessment is to evaluate your ability to use the seaborn library for data visualization, comprehend and manipulate datasets, create various plot types, apply customizations, and interpret results. Problem Statement You are provided with the famous \\"tips\\" dataset from seaborn\'s built-in datasets. This dataset contains information about tips given in a restaurant, including attributes like total bill, tip amount, gender, smoking status, day of the week, time, and size of the party. Your task is to: 1. Load the \\"tips\\" dataset using seaborn. 2. Create a subplot with two plots: - The first plot (left) should be a scatter plot showing the relationship between `total_bill` and `tip`. - The second plot (right) should be a violin plot showing the distribution of `tips` for each day of the week. 3. Customize both plots as follows: - For the scatter plot: Add a red regression line to indicate the linear relationship. - For the violin plot: Use different colors for each day (\'Thu\', \'Fri\', \'Sat\', \'Sun\'). 4. Add appropriate titles, labels, and legends to enhance the plots\' readability. 5. Add a brief interpretation explaining any noticeable trends or patterns in each plot. Expected Input and Output **Input:** - No external input. Use the seaborn\'s built-in \\"tips\\" dataset. **Output:** - A visualization that includes one scatter plot and one violin plot in a single figure with custom enhancements and a brief text interpretation. Constraints - You must use the seaborn library for all your visualizations. - Ensure plots are clearly labeled and visually distinct. - Make sure your visualization is informative and visually appealing. Performance Requirements - Your implementation should be efficient and make appropriate use of seaborn\'s functionalities. Example Code Outline ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # 2. Create a subplot with two plots (one scatter plot and one violin plot) fig, axes = plt.subplots(1, 2, figsize=(15, 6)) # 2.a - Scatter plot with regression line sns.regplot(ax=axes[0], x=\\"total_bill\\", y=\\"tip\\", data=tips, color=\'blue\', line_kws={\'color\': \'red\'}) axes[0].set_title(\'Total Bill vs Tip\') axes[0].set_xlabel(\'Total Bill\') axes[0].set_ylabel(\'Tip\') # 2.b - Violin plot with different colors for each day sns.violinplot(ax=axes[1], x=\\"day\\", y=\\"tip\\", data=tips, palette=\'muted\') axes[1].set_title(\'Tip Distribution by Day\') axes[1].set_xlabel(\'Day\') axes[1].set_ylabel(\'Tip\') # 3. Add appropriate titles, labels, and legends fig.suptitle(\'Restaurant Tips Analysis\') # 4. Display the plot plt.tight_layout() plt.show() # 5. Interpretation print(\'Interpretation: ...\') ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_tips_data(): Loads the \'tips\' dataset and creates a subplot with a scatter plot and a violin plot. Adds customization and an interpretation of the trends and patterns. # 1. Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # 2. Create a subplot with two plots (one scatter plot and one violin plot) fig, axes = plt.subplots(1, 2, figsize=(15, 6)) # 2.a - Scatter plot with regression line sns.regplot(ax=axes[0], x=\\"total_bill\\", y=\\"tip\\", data=tips, color=\'blue\', line_kws={\'color\': \'red\'}) axes[0].set_title(\'Total Bill vs Tip\') axes[0].set_xlabel(\'Total Bill\') axes[0].set_ylabel(\'Tip\') # 2.b - Violin plot with different colors for each day sns.violinplot(ax=axes[1], x=\\"day\\", y=\\"tip\\", data=tips, palette=\'viridis\') axes[1].set_title(\'Tip Distribution by Day\') axes[1].set_xlabel(\'Day\') axes[1].set_ylabel(\'Tip\') # 3. Add appropriate titles, labels, and legends fig.suptitle(\'Restaurant Tips Analysis\') # 4. Display the plot plt.tight_layout() plt.show() # 5. Interpretation (printed for the sake of completeness) return (\'In the scatter plot, we observe a positive correlation between total bill and tip amount, \' \'suggesting that higher bills tend to result in higher tips. The red regression line \' \'emphasizes this trend.n\' \'In the violin plot, we see that the distribution of tips varies across different days. \' \'Saturday and Sunday show a wider range of tip values, likely due to higher customer \' \'traffic on weekends.\') # Code to run the function for visualization (this would be called outside when the script is run) plot_tips_data()"},{"question":"Data Class Implementation **Objective:** Implement a set of data classes to model a simple library system. Your data classes should include functionality for adding and removing books, listing available books, and managing borrowed books. Requirements: 1. Implement three classes using `dataclasses`: - `Book`: Represents a book with title, author, and ISBN. - `Member`: Represents a library member with name and list of borrowed books. - `Library`: Manages adding/removing books and member transactions. 2. The `Book` class should: - Have the attributes: `title` (str), `author` (str), and `isbn` (str). - Override the `__str__` method for a readable string representation of a book. 3. The `Member` class should: - Have the attributes: `name` (str) and `borrowed_books` (list of Book objects). - Have a method `borrow_book(book: Book, library: Library) -> bool` to borrow a book from the library. - Have a method `return_book(book: Book, library: Library) -> bool` to return a book to the library. - Use appropriate data class features to manage the initialization and representation of borrowed books. 4. The `Library` class should: - Have the attributes: `books` (list of available Book objects) and `members` (list of Member objects). - Have a method `add_book(book: Book) -> None` to add a book to the library. - Have a method `remove_book(book: Book) -> bool` to remove a book from the library. - Have a method `list_available_books() -> List[Book]` to return a list of available books. - Have a method `add_member(member: Member) -> None` to register a member with the library. - Implement internal checks to ensure books are not added multiple times, and only available books are borrowed/returned. Constraints: - ISBN numbers are unique for each book. - Members cannot borrow more than 5 books at a time. - Use appropriate data classes features (e.g., `field` decorator for default factory properties) to handle mutable defaults. Input and Output Format: There is no specific input/output format, as your task is to correctly implement the classes and their methods. You can assume that the given methods will be called and tested with appropriate inputs. Example: ```python from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str isbn: str def __str__(self): return f\\"{self.title} by {self.author} (ISBN: {self.isbn})\\" @dataclass class Member: name: str borrowed_books: List[Book] = field(default_factory=list) def borrow_book(self, book: Book, library: \'Library\') -> bool: if len(self.borrowed_books) < 5 and book in library.books: self.borrowed_books.append(book) library.books.remove(book) return True return False def return_book(self, book: Book, library: \'Library\') -> bool: if book in self.borrowed_books: self.borrowed_books.remove(book) library.books.append(book) return True return False @dataclass class Library: books: List[Book] = field(default_factory=list) members: List[Member] = field(default_factory=list) def add_book(self, book: Book) -> None: if book not in self.books: self.books.append(book) def remove_book(self, book: Book) -> bool: if book in self.books: self.books.remove(book) return True return False def list_available_books(self) -> List[Book]: return self.books def add_member(self, member: Member) -> None: if member not in self.members: self.members.append(member) ``` Test your implementation by creating instances of `Book`, `Member`, and `Library`, and performing operations like adding books, borrowing and returning books, and listing available books.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str isbn: str def __str__(self): return f\\"{self.title} by {self.author} (ISBN: {self.isbn})\\" @dataclass class Member: name: str borrowed_books: List[Book] = field(default_factory=list) def borrow_book(self, book: Book, library: \'Library\') -> bool: if len(self.borrowed_books) < 5 and book in library.books: self.borrowed_books.append(book) library.books.remove(book) return True return False def return_book(self, book: Book, library: \'Library\') -> bool: if book in self.borrowed_books: self.borrowed_books.remove(book) library.books.append(book) return True return False @dataclass class Library: books: List[Book] = field(default_factory=list) members: List[Member] = field(default_factory=list) def add_book(self, book: Book) -> None: if book not in self.books: self.books.append(book) def remove_book(self, book: Book) -> bool: if book in self.books: self.books.remove(book) return True return False def list_available_books(self) -> List[Book]: return self.books def add_member(self, member: Member) -> None: if member not in self.members: self.members.append(member)"},{"question":"Objective You are required to implement a Python script that parses a `MANIFEST.in` file, generates a `MANIFEST` file listing all files for distribution, and creates a source distribution archive. This question assesses your understanding of file manipulation, parsing, and usage of Python standard libraries. Task Write a Python function `create_source_distribution(manifest_in_file: str, output_format: str) -> None` that: 1. **Parses the `MANIFEST.in` file** to generate a list of files for distribution. 2. **Writes a `MANIFEST` file** with the list of files. 3. **Creates a source distribution archive** in the specified format (`.tar.gz`, `.zip`, etc.). Input - `manifest_in_file` (str): The path to the `MANIFEST.in` file. - `output_format` (str): The format of the source distribution archive (e.g., `gztar`, `zip`). Output - The function should create a `MANIFEST` file in the current directory. - The function should create a source distribution archive in the specified format in the current directory. Constraints - Assume the current directory contains all the files and directories mentioned in the `MANIFEST.in`. - Supported formats are `zip` and `gztar`. Example Suppose `MANIFEST.in` has the following content: ``` include *.txt recursive-include src *.py ``` And the directory structure is: ``` . â”œâ”€â”€ MANIFEST.in â”œâ”€â”€ README.txt â”œâ”€â”€ setup.py â””â”€â”€ src â”œâ”€â”€ main.py â””â”€â”€ utils.py ``` Calling `create_source_distribution(\'MANIFEST.in\', \'gztar\')` should: 1. Create a `MANIFEST` file with the following content: ``` README.txt setup.py src/main.py src/utils.py ``` 2. Create a gzipped tarball `source_dist.tar.gz` containing the files listed in `MANIFEST`. Notes - Use Python\'s `tarfile` and `zipfile` modules for creating the archives. - Ensure the `MANIFEST` file starts with a comment indicating it is generated from `MANIFEST.in`. Function Signature ```python def create_source_distribution(manifest_in_file: str, output_format: str) -> None: pass ```","solution":"import fnmatch import os import tarfile import zipfile def parse_manifest_in(manifest_in_file): with open(manifest_in_file, \'r\') as f: lines = f.readlines() files_to_include = [] for line in lines: line = line.strip() if line.startswith(\'include\'): pattern = line.split(\' \')[1] for root, dirs, files in os.walk(\'.\'): for file in files: if fnmatch.fnmatch(file, pattern): files_to_include.append(os.path.relpath(os.path.join(root, file), \'.\')) elif line.startswith(\'recursive-include\'): parts = line.split(\' \') dir_pattern = parts[1] file_pattern = parts[2] for root, dirs, files in os.walk(dir_pattern): for file in files: if fnmatch.fnmatch(file, file_pattern): files_to_include.append(os.path.relpath(os.path.join(root, file), \'.\')) return files_to_include def write_manifest(manifest_file, files): with open(manifest_file, \'w\') as f: f.write(\'# Generated from MANIFEST.inn\') for file in files: f.write(file + \'n\') def create_archive(archive_name, files, output_format): if output_format == \'gztar\': with tarfile.open(archive_name + \'.tar.gz\', \'w:gz\') as tar: for file in files: tar.add(file) elif output_format == \'zip\': with zipfile.ZipFile(archive_name + \'.zip\', \'w\', zipfile.ZIP_DEFLATED) as zipf: for file in files: zipf.write(file) def create_source_distribution(manifest_in_file: str, output_format: str) -> None: files_to_include = parse_manifest_in(manifest_in_file) write_manifest(\'MANIFEST\', files_to_include) create_archive(\'source_dist\', files_to_include, output_format)"},{"question":"**Objective**: Assess your understanding and ability to work with the `zipfile` module in Python. # Problem Statement You are tasked with creating a utility function that processes a ZIP file. This function should accomplish the following tasks: 1. **Write and Compress Files**: Given a dictionary where keys are filenames and values are file contents (as strings), compress these files into a new ZIP file. 2. **List Contents**: After compression, list the contents of the ZIP file. 3. **Extract Files**: Extract all files from the created ZIP file into a specified directory. Implement a function `process_zipfile` that accepts three arguments: - `file_dict`: A dictionary where keys are filenames (strings) and values are file contents (strings). - `zip_filename`: The name of the ZIP file to create. - `extract_dir`: The directory where the files should be extracted after compression. The function should perform the following steps: 1. Create a ZIP file named `zip_filename`, and write the files from `file_dict` into it using ZIP_DEFLATED compression method. 2. Print the names of the files contained in the ZIP file. 3. Extract all files from the ZIP file into the directory specified by `extract_dir`. # Input - `file_dict` (dict): A dictionary containing filenames and their corresponding contents. ```python { \\"file1.txt\\": \\"Content of file 1\\", \\"file2.txt\\": \\"Content of file 2\\" } ``` - `zip_filename` (str): The name of the ZIP file to be created, e.g., \\"output.zip\\". - `extract_dir` (str): The directory path where the files should be extracted after compression, e.g., \\"/extracted_files\\". # Output The function should not return anything but should print: 1. A list of filenames contained in the ZIP file after compression. # Constraints - The filenames in `file_dict` are unique. - The provided paths for `zip_filename` and `extract_dir` are valid and writable. # Example ```python file_dict = { \\"file1.txt\\": \\"Content of file 1\\", \\"file2.txt\\": \\"Content of file 2\\" } zip_filename = \\"output.zip\\" extract_dir = \\"extracted_files\\" process_zipfile(file_dict, zip_filename, extract_dir) ``` Expected printed output: ``` [\'file1.txt\', \'file2.txt\'] ``` # Implementation ```python import zipfile import os def process_zipfile(file_dict, zip_filename, extract_dir): # Step 1: Create a ZIP file and write the files into it using ZIP_DEFLATED compression method. with zipfile.ZipFile(zip_filename, \'w\', zipfile.ZIP_DEFLATED) as zipf: for filename, content in file_dict.items(): zipf.writestr(filename, content) # Step 2: List and print the names of the files contained in the ZIP file. with zipfile.ZipFile(zip_filename, \'r\') as zipf: file_list = zipf.namelist() print(file_list) # Step 3: Extract all files from the ZIP file into the specified directory. with zipfile.ZipFile(zip_filename, \'r\') as zipf: zipf.extractall(extract_dir) ``` # Notes - Ensure that the `extract_dir` directory is created if it does not exist. - Handle any potential exceptions that could arise (e.g., file writing/reading issues).","solution":"import zipfile import os def process_zipfile(file_dict, zip_filename, extract_dir): Create a ZIP file from the given file dictionary, list its contents, and extract it. Parameters: - file_dict (dict): Dictionary with filenames as keys and file contents as values. - zip_filename (str): Name of the output ZIP file. - extract_dir (str): Directory where files will be extracted. # Step 1: Create a ZIP file and write files into it using ZIP_DEFLATED compression method. with zipfile.ZipFile(zip_filename, \'w\', zipfile.ZIP_DEFLATED) as zipf: for filename, content in file_dict.items(): zipf.writestr(filename, content) # Step 2: List and print the names of the files contained in the ZIP file. with zipfile.ZipFile(zip_filename, \'r\') as zipf: file_list = zipf.namelist() print(file_list) # Step 3: Create the extract directory if it does not exist. if not os.path.exists(extract_dir): os.makedirs(extract_dir) # Step 4: Extract all files from the ZIP file into the specified directory. with zipfile.ZipFile(zip_filename, \'r\') as zipf: zipf.extractall(extract_dir)"},{"question":"Coding Assessment Question You are given a dataset represented as a 2D NumPy array, where each row corresponds to a sample and each column corresponds to a feature. Your task is to implement a function that performs dimensionality reduction on this dataset using Principal Component Analysis (PCA) followed by k-means clustering to classify the data into a specified number of clusters. # Function Signature ```python def pca_kmeans_clustering(data: np.ndarray, n_components: int, n_clusters: int) -> np.ndarray: Perform PCA followed by k-means clustering on the dataset. Parameters: - data (np.ndarray): A 2D NumPy array of shape (n_samples, n_features). - n_components (int): Number of principal components to keep. - n_clusters (int): Number of clusters to form. Returns: - np.ndarray: A 1D array of cluster labels for each sample in the dataset. ``` # Input - `data`: A 2D NumPy array of shape `(n_samples, n_features)` containing the data to be clustered. - `n_components`: An integer specifying the number of principal components to keep after performing PCA. - `n_clusters`: An integer specifying the number of clusters to form using k-means clustering. # Output - A 1D NumPy array of shape `(n_samples,)` containing the cluster labels for each sample in the dataset. # Constraints - You may assume that the number of samples and features in the dataset is reasonably large, making it important to reduce dimensionality before clustering. - You should import necessary modules from `sklearn.decomposition`, `sklearn.cluster`, and any other relevant scikit-learn submodules. # Example ```python import numpy as np # Example data (10 samples, 5 features) data = np.array([ [1.2, 3.5, 2.2, 5.1, 6.2], [2.1, 3.4, 2.1, 5.0, 6.1], [1.3, 3.6, 2.3, 5.2, 6.3], ... # (and 7 more samples) ]) n_components = 2 n_clusters = 3 cluster_labels = pca_kmeans_clustering(data, n_components, n_clusters) print(cluster_labels) # Output: A 1D array with cluster labels (e.g., array([0, 1, 1, 2, ...])) ``` # Additional Information - Ensure that your code is efficient and follows best practices for using scikit-learn. - Test your function with different values of `n_components` and `n_clusters` to ensure it handles various scenarios.","solution":"import numpy as np from sklearn.decomposition import PCA from sklearn.cluster import KMeans def pca_kmeans_clustering(data: np.ndarray, n_components: int, n_clusters: int) -> np.ndarray: Perform PCA followed by k-means clustering on the dataset. Parameters: - data (np.ndarray): A 2D NumPy array of shape (n_samples, n_features). - n_components (int): Number of principal components to keep. - n_clusters (int): Number of clusters to form. Returns: - np.ndarray: A 1D array of cluster labels for each sample in the dataset. # Perform PCA to reduce dimensionality pca = PCA(n_components=n_components) reduced_data = pca.fit_transform(data) # Perform k-means clustering kmeans = KMeans(n_clusters=n_clusters, random_state=0) cluster_labels = kmeans.fit_predict(reduced_data) return cluster_labels"},{"question":"# Task: You are given a text file containing data that needs to be processed using LZMA compression. Your task is to write a Python function `compress_and_verify`, that: 1. Reads data from an input file. 2. Compresses the data using LZMA compression with a specific filter chain. 3. Writes the compressed data to an output file. 4. Reads the compressed data back from the output file and decompresses it. 5. Verifies that the decompressed data matches the original input data. # Function Signature: ```python def compress_and_verify(input_file: str, output_file: str) -> bool: pass ``` # Input: - `input_file` (str): Path to the input text file containing the data to be compressed. - `output_file` (str): Path to the output file where compressed data will be stored. # Output: - Returns `True` if the decompressed data matches the original data, `False` otherwise. # Constraints: - Use the following custom filter chain for compression: ```python my_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7} ] ``` # Example: ```python # Assume \'sample.txt\' contains the text \\"Hello, this is a test.\\" compress_and_verify(\'sample.txt\', \'compressed.xz\') # The function should return True if the decompression matches the original content in \'sample.txt\' ``` # Note: - Handle any exceptions that might occur during file reading/writing, compression, or decompression. - You can assume the input file exists and contains valid data. # Hints: - Use `lzma.open` for file operations involving LZMA compression. - Refer to the `lzma` documentation for details on how to create and use custom filter chains. - Ensure that file handles are properly managed (e.g., using `with` statements).","solution":"import lzma def compress_and_verify(input_file: str, output_file: str) -> bool: try: # Read the original data from the input file with open(input_file, \'rb\') as f: data = f.read() # Define the custom filter chain for compression my_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7} ] # Compress the data and write it to the output file with lzma.open(output_file, \'wb\', format=lzma.FORMAT_XZ, filters=my_filters) as f: f.write(data) # Read the compressed data back from the output file and decompress it with lzma.open(output_file, \'rb\') as f: decompressed_data = f.read() # Verify that the decompressed data matches the original data return decompressed_data == data except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Question: Customizing Seaborn Plots You are provided with a dataset containing information about the monthly sales of various products. Your task is to create a customized plot using Seaborn that will help visualize this data effectively for a presentation. Instructions 1. **Load the Dataset**: Assume the dataset is a CSV file named `sales_data.csv` with the following columns: - `month`: A string representing the month (e.g., \\"January\\"). - `product`: The name of the product (e.g., \\"Product A\\"). - `sales`: The sales amount for that product in that month. 2. **Plot the Data**: - Create a `lineplot` to display the sales trends of the products over the months using Seaborn. - Use the theme `darkgrid` for the plot. - Use the context `talk` with a `font_scale` of 1.2 for the presentation. - Customize the line style and color palette to your preference. - Remove the top and right spines from the plot to make it cleaner. 3. **Implement the Function**: - Write a function `customize_sales_plot(filepath)` which takes the filepath of the dataset as input and generates the customized plot. - Ensure that the plot is displayed inline. # Example Usage ```python customize_sales_plot(\'sales_data.csv\') ``` # Constraints - Ensure that the script runs without external dependencies aside from Seaborn, Matplotlib, Pandas, and NumPy libraries. - The function should be resilient to minor changes in the dataset structure, such as additional columns. # Expected Output The function should generate and display a line plot with the specified customizations, showing the sales trends for each product across the months.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customize_sales_plot(filepath): This function loads the sales dataset from the specified CSV file path, and creates a customized line plot for the sales trends of various products over the months. Args: filepath (str): The path to the CSV file containing the sales data. Returns: None # Load the dataset sales_data = pd.read_csv(filepath) # Set the seaborn theme and context for presentation sns.set_theme(style=\\"darkgrid\\") sns.set_context(\\"talk\\", font_scale=1.2) # Create the line plot plt.figure(figsize=(14, 7)) line_plot = sns.lineplot(data=sales_data, x=\'month\', y=\'sales\', hue=\'product\', style=\'product\', markers=True, dashes=False, palette=\\"tab10\\") # Customize the plot line_plot.set(title=\'Monthly Sales Trends\') line_plot.spines[\'top\'].set_visible(False) line_plot.spines[\'right\'].set_visible(False) # Display the plot plt.xticks(rotation=45) plt.tight_layout() plt.show()"},{"question":"# Question: Implementing Advanced Audio File Analysis **Objective:** Implement a function in Python that processes a list of audio files, determines their types using the `sndhdr` module, and returns a summary of their properties. --- **Function Signature:** ```python def analyze_audio_files(file_list: list) -> dict: pass ``` **Input:** - `file_list` (list): A list of strings, each representing a path to an audio file. **Output:** - `result` (dict): A dictionary where the keys are the file paths, and the values are tuples containing the filetype, framerate, nchannels, nframes, and sampwidth. **Constraints:** 1. You must use the `sndhdr.what` function for file type determination. 2. The function should handle exceptions gracefully, returning \'unknown\' for all attributes if the file cannot be processed. 3. You should not assume any specific file extensions; the function must determine the type solely based on the file content. **Example:** ```python file_list = [\\"audio1.wav\\", \\"audio2.aifc\\", \\"invalidfile.mp3\\"] result = analyze_audio_files(file_list) # Expected Output (depending on actual files\' properties): # { # \\"audio1.wav\\": (\'wav\', 44100, 2, 10584000, 16), # \\"audio2.aifc\\": (\'aifc\', 48000, 1, 2304000, 24), # \\"invalidfile.mp3\\": (\'unknown\', 0, 0, -1, \'unknown\') # } ``` **Notes:** - Ensure to read and understand the `sndhdr` module\'s documentation (provided above) to accurately implement the solution. - Consider edge cases where the file might not be accessible or does not contain valid audio data. - The `namedtuple` returned by `sndhdr.what` should be unpacked to match the expected output format. **Performance Requirement:** - The function should process the list in a reasonable time frame, assuming the typical list size ranges from 1 to 100 files. **Helpful Tips:** - Utilize exception handling to manage cases where files might not be readable or have incorrect formats. - Document your code to explain logic and any assumptions made.","solution":"import sndhdr import wave import aifc def analyze_audio_files(file_list: list) -> dict: result = {} for file_path in file_list: try: filetype_info = sndhdr.what(file_path) if filetype_info: filetype = filetype_info.filetype if filetype == \'wav\': with wave.open(file_path, \'rb\') as audio_file: framerate = audio_file.getframerate() nchannels = audio_file.getnchannels() nframes = audio_file.getnframes() sampwidth = audio_file.getsampwidth() * 8 # convert to bits elif filetype in (\'aiff\', \'aifc\'): with aifc.open(file_path, \'rb\') as audio_file: framerate = audio_file.getframerate() nchannels = audio_file.getnchannels() nframes = audio_file.getnframes() sampwidth = audio_file.getsampwidth() * 8 # convert to bits else: framerate = 0 nchannels = 0 nframes = -1 sampwidth = \'unknown\' else: filetype = \'unknown\' framerate = 0 nchannels = 0 nframes = -1 sampwidth = \'unknown\' except Exception: filetype = \'unknown\' framerate = 0 nchannels = 0 nframes = -1 sampwidth = \'unknown\' result[file_path] = (filetype, framerate, nchannels, nframes, sampwidth) return result"},{"question":"# Python Coding Assessment: Custom Container with Abstract Base Classes Objective: Create a custom container class that behaves like a set but does not require elements to be hashable. Use the `collections.abc.Set` abstract base class to define this new custom container. Description: Implement a class `CustomSet` that inherits from `collections.abc.Set`. This class should allow adding elements without requiring them to be hashable. Your implementation must include the following: 1. Initializer to accept an iterable. 2. Methods to add, remove elements. 3. Implement all abstract methods required by `collections.abc.Set`. 4. Implement mixin methods provided by `collections.abc.Set`. Requirements: - **Initializer**: `__init__(self, iterable)` - Initializes the container with elements from the given iterable. - Duplicates should not be added. - **Required Methods** (Implement these methods): - `__contains__(self, value)`: Check if value is in the container. - `__iter__(self)`: Return an iterator over the elements. - `__len__(self)`: Return the number of elements in the container. - **Mixin Methods** (Use default methods from `collections.abc.Set`): - `__le__(self, other)`, `__lt__(self, other)`, `__eq__(self, other)`, `__ne__(self, other)`, `__gt__(self, other)`, `__ge__(self, other)`: Set comparison operations. - `__and__(self, other)`, `__or__(self, other)`, `__sub__(self, other)`, `__xor__(self, other)`: Set operations. - `isdisjoint(self, other)`: Return `True` if the container has no elements in common with `other`. Example Usage: ```python from collections.abc import Set class CustomSet(Set): def __init__(self, iterable): self.elements = [] for item in iterable: if item not in self.elements: self.elements.append(item) def __contains__(self, value): return value in self.elements def __iter__(self): return iter(self.elements) def __len__(self): return len(self.elements) # Testing the CustomSet s1 = CustomSet([1, 2, 3, 3, 2, 1]) s2 = CustomSet([3, 4, 5]) print(s1 & s2) # Output: CustomSet([3]) print(s1 | s2) # Output: CustomSet([1, 2, 3, 4, 5]) print(s1 - s2) # Output: CustomSet([1, 2]) print(s1.isdisjoint(s2)) # Output: False print(3 in s1) # Output: True print(len(s1)) # Output: 3 ``` Constraints: - Your solution should not use any standard set or unordered collections from Python\'s standard library except for inheritance from `collections.abc.Set`. - Ensure that the time complexity for element lookup (`__contains__`) is linear, assuming the maximum length of any input iterable is manageable (e.g., up to 10^3 elements). Evaluation Criteria: - Correct implementation of `CustomSet` with required methods. - Demonstrating the use of abstract base classes and mixin methods. - Adherence to the specified constraints and effective use of the `collections.abc.Set` methods. Good luck!","solution":"from collections.abc import Set class CustomSet(Set): def __init__(self, iterable): self.elements = [] for item in iterable: if item not in self.elements: self.elements.append(item) def __contains__(self, value): return value in self.elements def __iter__(self): return iter(self.elements) def __len__(self): return len(self.elements) def add(self, value): if value not in self.elements: self.elements.append(value) def remove(self, value): if value not in self.elements: raise KeyError(f\'{value} not found in CustomSet\') self.elements.remove(value) def __repr__(self): return f\'CustomSet({self.elements})\'"},{"question":"Objective: You are required to implement a Support Vector Machine (SVM) for multi-class classification using scikit-learn\'s `SVC` classifier. The task includes: 1. Loading and preprocessing the dataset. 2. Implementing the SVM with different kernels. 3. Performing hyperparameter tuning using GridSearchCV to find the best parameters. Dataset: Use the Iris dataset available from scikit-learn\'s datasets module. Requirements: 1. **Loading and Preprocessing:** - Load the Iris dataset using `datasets.load_iris()`. - Split the dataset into training and testing sets with a test size of 20% (`train_test_split`). 2. **SVM Classifier:** - Implement an SVM classifier using `SVC` with three different kernels: `\'linear\'`, `\'poly\'`, and `\'rbf\'`. 3. **GridSearchCV:** - Set up GridSearchCV with a range of values for the hyperparameters `C` and `gamma` (for the `\'rbf\'` and `\'poly\'` kernels). - The range for `C` should be [0.1, 1, 10]. - The range for `gamma` should be [\'scale\', \'auto\']. - Perform the grid search and find the best parameters. 4. **Model Evaluation:** - Evaluate the performance of the best model on the test set using accuracy score. - Print the best parameters found by GridSearchCV. - Print the accuracy score of the best model on the test set. Constraints: - You must use the `SVC` class from `sklearn.svm`. - Use `GridSearchCV` from `sklearn.model_selection` for hyperparameter tuning. - Use `train_test_split` from `sklearn.model_selection` for splitting the dataset. Code Template: ```python from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score # 1. Load and preprocess the dataset def load_and_preprocess_data(test_size=0.2): Load the Iris dataset and split into train and test sets. Parameters: test_size (float): The proportion of the dataset to include in the test split. Returns: X_train, X_test, y_train, y_test # Load dataset iris = datasets.load_iris() X = iris.data y = iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) return X_train, X_test, y_train, y_test # 2. Define the SVM Classifier with different kernels and perform GridSearchCV def svm_classifier(X_train, y_train): Train an SVM classifier using GridSearchCV with different kernels and hyperparameters. Parameters: X_train: Training features. y_train: Training labels. Returns: best_model: The best model found by GridSearchCV. best_params: The best parameters found by GridSearchCV. # Define the model svc = SVC() # Define hyperparameters for GridSearchCV param_grid = { \'kernel\': [\'linear\', \'poly\', \'rbf\'], \'C\': [0.1, 1, 10], \'gamma\': [\'scale\', \'auto\'] } # Set up GridSearchCV grid_search = GridSearchCV(svc, param_grid, cv=5) # Fit the model grid_search.fit(X_train, y_train) # Return the best model and parameters best_model = grid_search.best_estimator_ best_params = grid_search.best_params_ return best_model, best_params # 3. Evaluate the model def evaluate_model(model, X_test, y_test): Evaluate the trained model on the test set. Parameters: model: The trained model. X_test: Test features. y_test: Test labels. Returns: accuracy: The accuracy of the model on the test set. # Make predictions y_pred = model.predict(X_test) # Evaluate accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy # Main function to execute the code if __name__ == \\"__main__\\": # Load data X_train, X_test, y_train, y_test = load_and_preprocess_data() # Train SVM with GridSearchCV best_model, best_params = svm_classifier(X_train, y_train) # Evaluate the best model accuracy = evaluate_model(best_model, X_test, y_test) # Print the results print(\\"Best Parameters found by GridSearchCV:\\", best_params) print(\\"Accuracy of the best model on test set:\\", accuracy) ``` Deliverables: - You should implement the `load_and_preprocess_data()`, `svm_classifier()`, and `evaluate_model()` functions. - Execute the code to find the best parameters and the accuracy of the best model. - Ensure the code runs without errors and prints the expected results.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score # 1. Load and preprocess the dataset def load_and_preprocess_data(test_size=0.2): Load the Iris dataset and split into train and test sets. Parameters: test_size (float): The proportion of the dataset to include in the test split. Returns: X_train, X_test, y_train, y_test # Load dataset iris = datasets.load_iris() X = iris.data y = iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) return X_train, X_test, y_train, y_test # 2. Define the SVM Classifier with different kernels and perform GridSearchCV def svm_classifier(X_train, y_train): Train an SVM classifier using GridSearchCV with different kernels and hyperparameters. Parameters: X_train: Training features. y_train: Training labels. Returns: best_model: The best model found by GridSearchCV. best_params: The best parameters found by GridSearchCV. # Define the model svc = SVC() # Define hyperparameters for GridSearchCV param_grid = { \'kernel\': [\'linear\', \'poly\', \'rbf\'], \'C\': [0.1, 1, 10], \'gamma\': [\'scale\', \'auto\'] } # Set up GridSearchCV grid_search = GridSearchCV(svc, param_grid, cv=5) # Fit the model grid_search.fit(X_train, y_train) # Return the best model and parameters best_model = grid_search.best_estimator_ best_params = grid_search.best_params_ return best_model, best_params # 3. Evaluate the model def evaluate_model(model, X_test, y_test): Evaluate the trained model on the test set. Parameters: model: The trained model. X_test: Test features. y_test: Test labels. Returns: accuracy: The accuracy of the model on the test set. # Make predictions y_pred = model.predict(X_test) # Evaluate accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy # Main function to execute the code if __name__ == \\"__main__\\": # Load data X_train, X_test, y_train, y_test = load_and_preprocess_data() # Train SVM with GridSearchCV best_model, best_params = svm_classifier(X_train, y_train) # Evaluate the best model accuracy = evaluate_model(best_model, X_test, y_test) # Print the results print(\\"Best Parameters found by GridSearchCV:\\", best_params) print(\\"Accuracy of the best model on test set:\\", accuracy)"},{"question":"**Problem Statement:** You are required to implement a Python function that performs the following tasks using the `urllib` module from the Python standard library: 1. **URL Retrieval and Response Handling:** - Fetch the content of a given URL (string). - If the URL responds with a redirect, follow the redirect and fetch the content of the final destination URL. 2. **Cookie Handling:** - Use cookies to maintain session information when fetching the URLs. 3. **URL Parsing:** - Parse the final destination URL into its components (scheme, netloc, path, params, query, fragment) and return these as a dictionary. 4. **HTTP Basic Authentication:** - If a URL requires HTTP Basic Authentication, handle this using provided username and password credentials. # Function Signature ```python def fetch_url_details(url: str, username: str = None, password: str = None) -> dict: pass ``` # Inputs - `url` (str): The URL to be fetched. - `username` (str, optional): The username for HTTP Basic Authentication, default is None. - `password` (str, optional): The password for HTTP Basic Authentication, default is None. # Outputs - (dict): A dictionary with the following keys: - `content` (str): The content of the final destination URL. - `parsed_url` (dict): A dictionary containing the components of the final destination URL with keys `scheme`, `netloc`, `path`, `params`, `query`, `fragment`. # Constraints - Do not use any third-party libraries. - Handle exceptions gracefully and return an appropriate message if the URL cannot be fetched. - The function should perform well for standard web pages, within typical HTTP response times. # Example ```python url = \\"http://www.example.com\\" result = fetch_url_details(url) print(result) # Output might look like: # { # \'content\': \'<!doctype html> ...\', # \'parsed_url\': { # \'scheme\': \'http\', # \'netloc\': \'www.example.com\', # \'path\': \'\', # \'params\': \'\', # \'query\': \'\', # \'fragment\': \'\' # } # } ``` # Notes - Consider using `urllib.request.Request`, `urllib.request.urlopen`, `urllib.parse.urlparse`, and related classes and functions. - Implement proper handling of HTTP redirects and cookies to maintain session information. - Use `urllib.request.HTTPBasicAuthHandler` for handling HTTP Basic Authentication if `username` and `password` are provided.","solution":"import urllib.request import urllib.parse import http.cookiejar def fetch_url_details(url: str, username: str = None, password: str = None) -> dict: try: # Set up HTTP Basic Authentication if credentials are provided password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() if username and password: password_mgr.add_password(None, url, username, password) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) else: handler = urllib.request.HTTPCookieProcessor(http.cookiejar.CookieJar()) opener = urllib.request.build_opener(handler) # Fetch the URL content response = opener.open(url) # Follow any redirects final_url = response.geturl() content = response.read().decode(\'utf-8\') # Parse the final URL parsed_url = urllib.parse.urlparse(final_url) parsed_url_dict = { \'scheme\': parsed_url.scheme, \'netloc\': parsed_url.netloc, \'path\': parsed_url.path, \'params\': parsed_url.params, \'query\': parsed_url.query, \'fragment\': parsed_url.fragment, } return { \'content\': content, \'parsed_url\': parsed_url_dict } except Exception as e: return { \'error\': str(e) }"},{"question":"**Coding Assessment Question** # Problem Statement Write a Python function to test the following `Calculator` class using the `unittest.mock` library. Your test should cover various aspects of mocking instances, methods, and side effects. ```python class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero!\\") return a / b ``` # Task Create a test suite for the `Calculator` class. Use mocks to simulate different scenarios: 1. Mock the `add` method to always return a fixed value, and test that it is called with specific arguments. 2. Mock the `subtract` method to raise an exception when called, and verify that the exception is raised correctly in the test. 3. Use `patch` to replace the `multiply` method temporarily and ensure that it returns a specific value when called. 4. Use `side_effect` in `divide` method to return different values for successive calls and verify the correct behavior. # Constraints 1. You must use the `unittest` framework and the `unittest.mock` library. 2. Follow best practices for writing unit tests and structuring the test suite. # Performance Requirements - Each test case should run in reasonable time (milliseconds to a few seconds) and should not depend on any external systems or resources (like databases or network calls). # Expected Input and Output - There is no direct input/output to the test cases; they should internally check for the correctness of the `Calculator` class\'s methods using mocks. # Example Your test suite might include something similar to (but not limited to) the following pseudo-code: ```python import unittest from unittest.mock import Mock, patch, MagicMock class TestCalculator(unittest.TestCase): def test_add_method(self): calculator = Calculator() calculator.add = Mock(return_value=10) self.assertEqual(calculator.add(2, 3), 10) calculator.add.assert_called_with(2, 3) def test_subtract_method_exception(self): calculator = Calculator() calculator.subtract = Mock(side_effect=ValueError(\\"An error occurred\\")) with self.assertRaises(ValueError): calculator.subtract(5, 3) @patch(\'module_name.Calculator.multiply\', return_value=15) def test_multiply_method(self, mock_multiply): calculator = Calculator() result = calculator.multiply(3, 5) self.assertEqual(result, 15) mock_multiply.assert_called_with(3, 5) def test_divide_method_side_effect(self): calculator = Calculator() calculator.divide = Mock(side_effect=[2, 1, 0.5]) self.assertEqual(calculator.divide(4, 2), 2) self.assertEqual(calculator.divide(2, 2), 1) self.assertEqual(calculator.divide(1, 2), 0.5) if __name__ == \'__main__\': unittest.main() ``` Note: Replace `module_name` with the actual module name where `Calculator` resides.","solution":"from unittest import TestCase from unittest.mock import Mock, patch, MagicMock import pytest class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero!\\") return a / b"},{"question":"# Question: Creating and Customizing Plots with Seaborn\'s Object Interface Objective You are provided with a dataset and you need to create a series of plots using seaborn\'s latest objects interface (`seaborn.objects`). Your task is to demonstrate your understanding of how to create plots, add text annotations, and modify text properties using this interface. Provided Dataset You will use the `glue` dataset that comes with seaborn. You need to load and preprocess this dataset as follows: ```python import seaborn.objects as so from seaborn import load_dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) ``` Tasks 1. **Plot with Text Annotations**: - Create a scatter plot with `SST-2` scores on the x-axis, `MRPC` scores on the y-axis, and model names as text annotations. Use different colors for different encoders. 2. **Bar Plot with Horizontal Text Alignment**: - Create a bar plot with the average score on the x-axis and model names on the y-axis. Add text annotations with the average score. Center-align the text annotations horizontally and ensure that the text color is white. 3. **Fine-tuned Alignment and Offset**: - Modify the bar plot created in step 2 to move the text annotations slightly to the right by an offset of 6 units. 4. **Map Text Alignment Using a Color Variable**: - Create a scatter plot with `RTE` scores on the x-axis, `MRPC` scores on the y-axis, and model names as text annotations. Map text color to the encoder type and align the text vertically at the bottom of the dots. Additionally, align text horizontally based on the encoder type, such that `LSTM` aligns to the left and `Transformer` aligns to the right. 5. **Custom Text Styling with Matplotlib Parameters**: - Modify the scatter plot created in step 4 to make the text annotations bold. Output For each task, ensure your solution is clear, well-commented, and follows good coding practices. Submit the final code that completes all the tasks specified. Constraints - Ensure that the dataset loading and preprocessing is done correctly before attempting the plotting tasks. - Pay attention to the correct parameter usage as illustrated in the provided documentation. Example Output ```python # Sample solution structure import seaborn.objects as so from seaborn import load_dataset # Preprocess the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Task 1: Scatter plot with text annotations ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text()) ).show() # Task 2: Bar plot with horizontal text alignment ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"center\\")) ).show() # Task 3: Fine-tuned alignment and offset ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"center\\", offset=6)) ).show() # Task 4: Map text alignment using a color variable ( so.Plot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\"), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ).show() # Task 5: Custom text styling with matplotlib parameters ( so.Plot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text({\\"fontweight\\": \\"bold\\"}), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ).show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Preprocess the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Task 1: Scatter plot with text annotations plt.figure(figsize=(10, 6)) ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text()) ).show() # Task 2: Bar plot with horizontal text alignment plt.figure(figsize=(10, 6)) ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"center\\")) ).show() # Task 3: Fine-tuned alignment and offset plt.figure(figsize=(10, 6)) ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"center\\", offset=6)) ).show() # Task 4: Map text alignment using a color variable plt.figure(figsize=(10, 6)) ( so.Plot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\"), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ).show() # Task 5: Custom text styling with matplotlib parameters plt.figure(figsize=(10, 6)) ( so.Plot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text({\\"fontweight\\": \\"bold\\"}), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ).show()"},{"question":"You are tasked with analyzing a dataset involving sales performance of different products across various regions. Using the Seaborn library, you need to visualize this data with a heatmap to present the information clearly to stakeholders. Dataset Consider the following structure of the dataset `sales_data.csv`: ``` Product,Region,Sales ProductA,Region1,210 ProductB,Region1,150 ProductC,Region1,300 ProductA,Region2,310 ProductB,Region2,400 ProductC,Region2,200 ProductA,Region3,120 ProductB,Region3,220 ProductC,Region3,90 ``` Objectives 1. Load the dataset and pivot it to have `Product` as rows, `Region` as columns, and `Sales` as values. 2. Create a heatmap of the pivoted data with the following requirements: - Annotate the heatmap with the actual sales values. - Format the annotations to no decimal places. - Separate the cells with lines of a specified width. - Use a colormap of your choice. - Set the data range for the colormap from 0 to 500. 3. Customize the plot further to remove axis labels and place the x-axis labels on top. Expected Functions and Output You should write a function named `create_sales_heatmap` that performs the above steps. ```python import seaborn as sns import pandas as pd def create_sales_heatmap(filepath): # Your code here # Example call to the function create_sales_heatmap(\'sales_data.csv\') ``` - `filepath` (str): The path to the CSV file containing the sales data. - This function should not return any value but should render the heatmap with the specified customizations. Constraints and Requirements - Ensure that the Seaborn and pandas libraries are imported and used. - The function must handle potential errors gracefully, such as missing files or incorrect data formats. **Note:** You can assume that the provided file `sales_data.csv` exists in the working directory when testing your solution.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_sales_heatmap(filepath): try: # Load the dataset data = pd.read_csv(filepath) # Pivot the dataset pivot_table = data.pivot(\\"Product\\", \\"Region\\", \\"Sales\\") # Create the heatmap plt.figure(figsize=(10, 6)) heatmap = sns.heatmap(pivot_table, annot=True, fmt=\'d\', linewidths=0.5, cmap=\\"YlGnBu\\", vmin=0, vmax=500) # Customize the plot heatmap.set_xlabel(\'\') heatmap.set_ylabel(\'\') heatmap.xaxis.tick_top() # Display the heatmap plt.show() except FileNotFoundError: print(f\\"Error: The file {filepath} does not exist.\\") except pd.errors.EmptyDataError: print(\\"Error: The file is empty.\\") except pd.errors.ParserError: print(\\"Error: The file could not be parsed.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Gaussian Mixture Model Implementation and Evaluation **Objective:** Your task is to implement and evaluate a Gaussian Mixture Model (GMM) using the provided dataset. You will follow the steps to initialize, fit, and evaluate the number of components of the GMM. **Dataset:** You will use the famous Iris dataset available directly from `sklearn.datasets`. The dataset includes three classes of iris plants, and each class has 50 samples. The class labels are as follows: - Class 0: Iris Setosa - Class 1: Iris Versicolour - Class 2: Iris Virginica **Requirements:** 1. **Initialization and Fitting**: - Load the Iris dataset. - Implement a function to initialize the GMM using three different initialization methods: `k-means`, `random_from_data`, and `random`. - Fit the GMM to the dataset using the Expectation-Maximization algorithm. 2. **Model Selection**: - Implement a function to compute the Bayesian Information Criterion (BIC) for models with different numbers of components. - Select the optimal number of components based on the BIC. 3. **Prediction and Evaluation**: - Evaluate the model by predicting the class labels for the dataset. - Compute the accuracy of the predictions by comparing them to the actual class labels. 4. **Visualization**: - (Optional) Visualize how the learned GMM fits the data using a plot of the GMM ellipsoids. **Constraints:** - Use the `sklearn.mixture.GaussianMixture` class for GMM implementation. - The functions should be efficient and avoid redundancy. **Function Signatures:** ```python import numpy as np from sklearn.datasets import load_iris from sklearn.mixture import GaussianMixture from sklearn.metrics import accuracy_score def load_iris_data(): Load the Iris dataset. Returns: Tuple: (data, target) # Implement your code here def initialize_and_fit_gmm(data, n_components, init_method=\'k-means\'): Initialize and fit a Gaussian Mixture Model to the data. Parameters: data (numpy.ndarray): The input data to fit the GMM. n_components (int): Number of components for the GMM. init_method (str): Initialization method for GMM (\'k-means\', \'random_from_data\', \'random\'). Returns: GaussianMixture: The fitted GMM. # Implement your code here def compute_bic(data, max_components=10): Compute the Bayesian Information Criterion for different numbers of components. Parameters: data (numpy.ndarray): The input data for GMM. max_components (int): The maximum number of components to evaluate. Returns: List[float]: BIC values for each number of components from 1 to max_components. # Implement your code here def select_optimal_components(data, bic_scores): Select the optimal number of components based on BIC scores. Parameters: data (numpy.ndarray): The input data for GMM. bic_scores (List[float]): BIC values for different numbers of components. Returns: int: The optimal number of components. # Implement your code here def predict_and_evaluate(gmm, data, target): Predict and evaluate the model on the given data. Parameters: gmm (GaussianMixture): The fitted GMM. data (numpy.ndarray): The input data for prediction. target (numpy.ndarray): The actual class labels. Returns: float: The accuracy of the model. # Implement your code here def main(): data, target = load_iris_data() # Step 1: Initialize and fit GMM gmm = initialize_and_fit_gmm(data, n_components=3, init_method=\'k-means\') # Step 2: Compute BIC and select optimal number of components bic_scores = compute_bic(data) optimal_components = select_optimal_components(data, bic_scores) # Step 3: Fit GMM with optimal components and evaluate gmm_optimal = initialize_and_fit_gmm(data, n_components=optimal_components, init_method=\'k-means\') accuracy = predict_and_evaluate(gmm_optimal, data, target) print(f\\"Optimal number of components: {optimal_components}\\") print(f\\"Accuracy of GMM: {accuracy:.2f}\\") # (Optional): Visualization code here if __name__ == \\"__main__\\": main() ``` **Notes:** - Ensure that your implementations adhere to the function signatures provided. - Document any assumptions you make and any additional functionality you add.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.mixture import GaussianMixture from sklearn.metrics import accuracy_score def load_iris_data(): Load the Iris dataset. Returns: Tuple: (data, target) iris = load_iris() return iris.data, iris.target def initialize_and_fit_gmm(data, n_components, init_method=\'k-means\'): Initialize and fit a Gaussian Mixture Model to the data. Parameters: data (numpy.ndarray): The input data to fit the GMM. n_components (int): Number of components for the GMM. init_method (str): Initialization method for GMM (\'k-means\', \'random_from_data\', \'random\'). Returns: GaussianMixture: The fitted GMM. init_params = { \'k-means\': \'kmeans\', \'random_from_data\': \'random\', \'random\': \'random\' } if init_method not in init_params: raise ValueError(f\\"Initialization method \'{init_method}\' is not valid. Choose from \'k-means\', \'random_from_data\', \'random\'.\\") gmm = GaussianMixture(n_components=n_components, init_params=init_params[init_method], random_state=42) gmm.fit(data) return gmm def compute_bic(data, max_components=10): Compute the Bayesian Information Criterion for different numbers of components. Parameters: data (numpy.ndarray): The input data for GMM. max_components (int): The maximum number of components to evaluate. Returns: List[float]: BIC values for each number of components from 1 to max_components. bic_scores = [] for n in range(1, max_components + 1): gmm = GaussianMixture(n_components=n, random_state=42) gmm.fit(data) bic_scores.append(gmm.bic(data)) return bic_scores def select_optimal_components(bic_scores): Select the optimal number of components based on BIC scores. Parameters: bic_scores (List[float]): BIC values for different numbers of components. Returns: int: The optimal number of components. return np.argmin(bic_scores) + 1 def predict_and_evaluate(gmm, data, target): Predict and evaluate the model on the given data. Parameters: gmm (GaussianMixture): The fitted GMM. data (numpy.ndarray): The input data for prediction. target (numpy.ndarray): The actual class labels. Returns: float: The accuracy of the model. predicted_labels = gmm.predict(data) # Map predicted labels to the actual labels label_mapping = {i: np.argmax(np.bincount(target[predicted_labels == i])) for i in range(gmm.n_components)} mapped_labels = np.vectorize(label_mapping.get)(predicted_labels) return accuracy_score(target, mapped_labels) def main(): data, target = load_iris_data() # Step 1: Initialize and fit GMM gmm = initialize_and_fit_gmm(data, n_components=3, init_method=\'k-means\') # Step 2: Compute BIC and select optimal number of components bic_scores = compute_bic(data) optimal_components = select_optimal_components(bic_scores) # Step 3: Fit GMM with optimal components and evaluate gmm_optimal = initialize_and_fit_gmm(data, n_components=optimal_components, init_method=\'k-means\') accuracy = predict_and_evaluate(gmm_optimal, data, target) print(f\\"Optimal number of components: {optimal_components}\\") print(f\\"Accuracy of GMM: {accuracy:.2f}\\") # (Optional): Visualization code here if __name__ == \\"__main__\\": main()"},{"question":"Objective Write a Python script that does the following tasks using the Python Standard Library modules: 1. Navigate through a series of directories. 2. Find all files with a specific extension (e.g., `.log`). 3. Extract and format a specific piece of information from these files using regular expressions. 4. Archive the processed files that meet a certain age criteria (e.g., files older than 7 days) into a compressed archive. Requirements 1. **Directory Navigation**: Use functions from the `os` module to navigate through directories. 2. **File Filtering**: Use the `glob` module to filter files with the desired extension. 3. **Information Extraction**: Utilize the `re` module to extract information from the contents of the files. 4. **Date Handling**: Handle and compare file dates using the `datetime` module. 5. **File Compression**: Compress the files using the `zipfile` module. Input 1. A starting directory path (string). 2. A file extension to search for (string). 3. A regular expression pattern to extract specific information from each file (string). 4. An age threshold in days (integer). Output 1. Create a compressed archive named `archived_files.zip` containing files that meet the age criteria. 2. Print the extracted information from each file to the console. Example ```python import os import glob import re from datetime import datetime, timedelta import zipfile def process_files(start_dir, file_ext, regex_pattern, age_threshold): # Navigating to the starting directory os.chdir(start_dir) # Getting the current time now = datetime.now() # Finding all files with the specified extension files = glob.glob(f\'*.{file_ext}\') # Compiling the regex pattern pattern = re.compile(regex_pattern) # Preparing the zip file with zipfile.ZipFile(\'archived_files.zip\', \'w\') as archive: for file in files: # Getting the file\'s last modified time file_mtime = datetime.fromtimestamp(os.path.getmtime(file)) age = (now - file_mtime).days # If the file is older than the age threshold if age > age_threshold: archive.write(file) # Extracting information using the regex with open(file, \'r\') as f: content = f.read() matches = pattern.findall(content) for match in matches: print(match) # Example usage: # process_files(\'/path/to/start\', \'log\', r\'Error:.*\', 7) ``` Constraints - Ensure proper error handling for file operations. - Optimize performance for large directories and files. Grading Criteria - Correctness: Does the solution correctly implement the specified functionality? - Readability: Is the code well-organized and easy to understand? - Efficiency: Is the solution optimized for performance? - Use of the Python Standard Library: Are the appropriate standard library modules used effectively?","solution":"import os import glob import re from datetime import datetime, timedelta import zipfile def process_files(start_dir, file_ext, regex_pattern, age_threshold): # Navigating to the starting directory os.chdir(start_dir) # Getting the current time now = datetime.now() # Finding all files with the specified extension files = glob.glob(f\'**/*.{file_ext}\', recursive=True) # Compiling the regex pattern pattern = re.compile(regex_pattern) # Preparing the zip file with zipfile.ZipFile(\'archived_files.zip\', \'w\') as archive: for file in files: # Getting the file\'s last modified time file_mtime = datetime.fromtimestamp(os.path.getmtime(file)) age = (now - file_mtime).days # If the file is older than the age threshold if age > age_threshold: archive.write(file) # Extracting information using the regex with open(file, \'r\') as f: content = f.read() matches = pattern.findall(content) for match in matches: print(match)"},{"question":"**Python Coding Challenge: Custom Import Layer** **Objective**: Implement a custom module import mechanism in Python that provides additional features on top of Pythonâ€™s built-in import system. You will create a class `CustomImporter` which will have methods to import modules, handle reimporting, and maintain a log of imported modules. # Class Structure: ```python class CustomImporter: def __init__(self): # Initializes the CustomImporter pass def import_module(self, name): Imports a module by name. If the module is already imported, returns the existing module. Otherwise, imports the module and keeps a record of the import. Args: - name (str): The name of the module to import. Returns: - module (module): The imported module. pass def reload_module(self, name): Reloads an already imported module. Args: - name (str): The name of the module to reload. Returns: - module (module): The reloaded module. Raises: - ImportError: If the module is not already imported. pass def get_import_log(self): Returns a list of all imported module names. Returns: - List[str]: A list of imported module names. pass ``` # Requirements: 1. The `import_module` method should log module names in a list whenever a module is imported. 2. The `reload_module` method should check if the module has been imported previously. Reloads the module if already imported. Raise an `ImportError` if the module hasn\'t been imported yet. 3. The `get_import_log` method should return the list of names of all modules that have been imported using the `import_module` method. 4. Use Pythonâ€™s built-in functions like `__import__` for the import mechanism (do not use direct access to PyImport_* functions from C). # Constraints: - Do not use any third-party libraries. Only use Python standard library. - Implement proper error handling for edge cases such as attempting to reload a non-imported module. # Example: ```python import os # Example usage of CustomImporter ci = CustomImporter() # Importing a module mod = ci.import_module(\'os\') print(mod == os) # Output: True # Importing a module that does not exist try: ci.import_module(\'non_existent_module\') except ImportError: print(\'Module not found\') # Reloading a module reloaded_mod = ci.reload_module(\'os\') print(reloaded_mod == os) # Output: True # Attempting to reload a module that wasn\'t imported try: ci.reload_module(\'sys\') except ImportError: print(\'sys module not imported yet\') # Getting import log print(ci.get_import_log()) # Output: [\'os\'] ``` # Evaluation Criteria: 1. **Correctness**: Does the implementation meet the requirements and constraints provided? 2. **Code Quality**: Is the code readable, maintainable, and well-organized? 3. **Error Handling**: Does the implementation handle errors and edge cases appropriately? 4. **Performance**: Is the solution efficient for typical use cases? Your task is to complete the implementation of the `CustomImporter` class according to the specifications and requirements.","solution":"import importlib class CustomImporter: def __init__(self): self.import_log = [] def import_module(self, name): Imports a module by name. If the module is already imported, returns the existing module. Otherwise, imports the module and keeps a record of the import. Args: - name (str): The name of the module to import. Returns: - module (module): The imported module. if name not in self.import_log: module = importlib.import_module(name) self.import_log.append(name) else: module = importlib.import_module(name) return module def reload_module(self, name): Reloads an already imported module. Args: - name (str): The name of the module to reload. Returns: - module (module): The reloaded module. Raises: - ImportError: If the module is not already imported. if name in self.import_log: module = importlib.reload(importlib.import_module(name)) return module else: raise ImportError(f\\"Module \'{name}\' not imported yet\\") def get_import_log(self): Returns a list of all imported module names. Returns: - List[str]: A list of imported module names. return self.import_log"},{"question":"Objective Implement a machine learning pipeline using `scikit-learn` to extract features from a binary dataset using a `BernoulliRBM` and then classify the data using a logistic regression model. Problem Statement You have been provided with a binary dataset consisting of pixel values of images of digits (0 to 9). Your task is to take this dataset, extract features using the `BernoulliRBM`, and then classify the images using a logistic regression classifier. Requirements 1. **Read and Process Data**: - Load a binary image dataset (like MNIST but processed to binary [0, 1]). 2. **Feature Extraction**: - Implement a `BernoulliRBM` for feature extraction. 3. **Classification**: - Use a logistic regression model to classify the digits based on the features extracted by the `RBM`. 4. **Model Evaluation**: - Split the data into training and testing sets. - Evaluate the accuracy of the classification model using the testing set. Constraints - Assume the dataset is already provided in the form of two numpy arrays: `X` (features) and `y` (labels). - The RBM and logistic regression should be part of a scikit-learn pipeline. - Use an 80-20 split for training and testing data. Input and Output - **Input**: - `X`: A 2D numpy array of shape (n_samples, n_features), containing binary values (0 or 1). - `y`: A 1D numpy array of length `n_samples`, containing integer labels (0 to 9). - **Output**: - Print out the accuracy of the classifier on the test set. Code Template ```python import numpy as np from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Load or generate your binary image dataset # For the purpose of this question, assume X and y are already loaded # X: 2D numpy array of shape (n_samples, n_features) # y: 1D numpy array of length n_samples # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline with BernoulliRBM and LogisticRegression rbm = BernoulliRBM(random_state=42) logistic = LogisticRegression(max_iter=1000, random_state=42) classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the pipeline classifier.fit(X_train, y_train) # Predict and evaluate y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') ``` Considerations - Ensure that the RBM and Logistic Regression components are appropriately parameterized. - Handle any random states or seeds to ensure reproducibility. Example Scenario Imagine you have a binary version of the MNIST dataset. You need to predict the digit for each image. Feature extraction with RBM will transform the input space to a new feature space suitable for classification, subsequently used by logistic regression to predict the digit. Good luck!","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def load_and_preprocess_data(): Load your dataset here. For this example, we will create a random binary dataset. This function should be replaced with the actual data loading. Returns: X (np.array): 2D numpy array of shape (n_samples, n_features) with binary values (0 or 1) y (np.array): 1D numpy array of length n_samples with labels # For demonstration, generating a random binary dataset np.random.seed(42) X = np.random.randint(2, size=(1000, 64)) # 1000 samples, 64 features (8x8 pixel images) y = np.random.randint(10, size=(1000)) # 1000 labels for 10 classes (digits 0-9) return X, y def create_pipeline(): Create a scikit-learn pipeline with BernoulliRBM and LogisticRegression. Returns: classifier (Pipeline): Scikit-learn pipeline with RBM and Logistic Regression rbm = BernoulliRBM(random_state=42) logistic = LogisticRegression(max_iter=1000, random_state=42) classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) return classifier def main(): # Load and preprocess data X, y = load_and_preprocess_data() # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create and train the pipeline classifier = create_pipeline() classifier.fit(X_train, y_train) # Predict and evaluate y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') if __name__ == \\"__main__\\": main()"},{"question":"**Email Parsing Exception Handling** The `email.errors` module in Python provides various exception classes to handle errors during email message parsing and generation. # Task: Write a Python function `parse_email_message` that takes a string representing an email message and returns a list of error messages indicating any parsing errors or defects found. Use the exception classes and defect types provided in the `email.errors` module to identify and report errors. # Input: - A single string `email_message` representing the raw email message. # Output: - A list of strings, where each string is an error message detailing a specific parsing error or defect found in the email message. # Requirements: 1. Use the `email.errors` exception classes to catch and handle specific parsing errors. 2. Identify and report defects using the defect classes listed under `MessageDefect`. 3. The function should return an empty list if no errors or defects are found. # Example: ```python def parse_email_message(email_message: str) -> list: from email import message_from_string from email.errors import ( MessageParseError, HeaderParseError, MultipartConversionError, HeaderWriteError, MessageDefect, NoBoundaryInMultipartDefect, StartBoundaryNotFoundDefect, CloseBoundaryNotFoundDefect, FirstHeaderLineIsContinuationDefect, MisplacedEnvelopeHeaderDefect, MissingHeaderBodySeparatorDefect, MultipartInvariantViolationDefect, InvalidBase64PaddingDefect, InvalidBase64CharactersDefect, InvalidBase64LengthDefect, InvalidDateDefect ) error_messages = [] try: # Parse the email message message = message_from_string(email_message) # Check for defects if hasattr(message, \'defects\'): for defect in message.defects: error_messages.append(str(defect)) except MessageParseError: error_messages.append(\\"Message parsing error\\") except HeaderParseError: error_messages.append(\\"Header parsing error\\") except MultipartConversionError: error_messages.append(\\"Multipart conversion error\\") except HeaderWriteError: error_messages.append(\\"Header write error\\") return error_messages # Example usage: email_message = Your raw email message here errors = parse_email_message(email_message) for error in errors: print(error) ``` # Constraints: - The email message string will contain at most 10000 characters. - The function should handle the exceptions gracefully and return meaningful error messages for each identified error or defect. # Points: - Correctness of error handling (50%) - Coverage of all provided exception classes and defects (30%) - Code readability and organization (20%)","solution":"def parse_email_message(email_message: str) -> list: from email import message_from_string from email.errors import ( MessageError, MessageParseError, HeaderParseError, MultipartConversionError, HeaderWriteError, MessageDefect, NoBoundaryInMultipartDefect, StartBoundaryNotFoundDefect, CloseBoundaryNotFoundDefect, FirstHeaderLineIsContinuationDefect, MisplacedEnvelopeHeaderDefect, MissingHeaderBodySeparatorDefect, MultipartInvariantViolationDefect, InvalidBase64PaddingDefect, InvalidBase64CharactersDefect, InvalidBase64LengthDefect, InvalidDateDefect ) error_messages = [] try: # Parse the email message message = message_from_string(email_message) # Check for defects if hasattr(message, \'defects\'): for defect in message.defects: error_messages.append(type(defect).__name__ + \\": \\" + str(defect)) except MessageParseError: error_messages.append(\\"MessageParseError: Message parsing error\\") except HeaderParseError: error_messages.append(\\"HeaderParseError: Header parsing error\\") except MultipartConversionError: error_messages.append(\\"MultipartConversionError: Multipart conversion error\\") except HeaderWriteError: error_messages.append(\\"HeaderWriteError: Header write error\\") except MessageError: error_messages.append(\\"MessageError: General message error\\") return error_messages"},{"question":"Challenge Question # Objective Implement a custom class `CustomSequence` that mimics a list and supports Python slice objects in a similar way to standard Python lists. # Requirements - Your class should be initialized with a list of integers. - Implement a method `get_slice(self, slice_obj)` that takes a Python slice object and returns a sub-sequence as a list of integers according to the slice parameters. - Handle out-of-bound indices, negative indices, and stride (step) values. - Implement error handling for invalid slice inputs. # Function Signature ```python class CustomSequence: def __init__(self, data: List[int]): # Initialize with a list of integers pass def get_slice(self, slice_obj: slice) -> List[int]: # Return a sub-sequence based on the slice object pass ``` # Expected Input and Output Formats - **Input**: An instance of `CustomSequence` initialized with a list of integers, and a slice object. - **Output**: A sublist of integers based on the slice object. # Constraints - Slice parameters (start, stop, step) can be any valid integers including negative numbers and `None`. - Assume sequence length does not exceed `1000`. # Example ```python # Example usage seq = CustomSequence([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) print(seq.get_slice(slice(2, 8, 2))) # Output: [2, 4, 6] print(seq.get_slice(slice(-5, None, 1))) # Output: [5, 6, 7, 8, 9] print(seq.get_slice(slice(None, None, -1))) # Output: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] ``` # Performance Requirements - The solution should handle the slicing operations efficiently within linear time complexity, i.e., O(n). # Notes - Testing should cover various cases including positive and negative indices, steps, and combinations of None with integer values. - Ensure robust error handling and validation of slice objects.","solution":"from typing import List class CustomSequence: def __init__(self, data: List[int]): self.data = data def get_slice(self, slice_obj: slice) -> List[int]: if not isinstance(slice_obj, slice): raise ValueError(\\"Expected a slice object\\") return self.data[slice_obj]"},{"question":"# Python Code Execution and Evaluation You are required to implement a function using the `python310` package functionalities to compile and execute Python code provided either as a string or read from a file. Function Signature ```python def run_python_code(source: str, input_type: str, context: dict) -> any: pass ``` Parameters - `source` (str): The Python code to be compiled and executed. - If `input_type` is `str`, then `source` is a string containing the Python code. - If `input_type` is `file`, then `source` is the filename of a file containing Python code. - `input_type` (str): Indicator of whether `source` is a `str` or `file`. - `context` (dict): A dictionary that provides the global context in which to execute the code. Return - Returns the result of executing the compiled code, similar to the behavior of `PyRun_String`. Constraints - You may assume the provided Python code is syntactically correct. - The `context` dictionary should provide all necessary globals, and it can be modified during execution. Example Usage ```python context = { \'x\': 10, \'y\': 20 } code = \\"result = x + y\\" print(run_python_code(code, \'str\', context)) # Expected to modify `context` and result should be 30. context = { \'n\': 5 } print(run_python_code(\'factorial.py\', \'file\', context)) # Assuming `factorial.py` contains factorial code. ``` Implementation Notes - Make use of functions such as `Py_CompileString`, `PyEval_EvalCode`. - Handle both string and file inputs by appropriately compiling and reading the code. - Adjust the global context using the provided `context` dictionary.","solution":"import traceback def run_python_code(source: str, input_type: str, context: dict) -> any: Compiles and executes Python code provided as a string or from a file, using the given context. Parameters: - source (str): The Python code to be compiled and executed. - input_type (str): Indicator of whether `source` is a `str` or `file`. - context (dict): A dictionary that provides the global context in which to execute the code. Returns: - Any: The result of executing the compiled code. try: if input_type == \'file\': with open(source, \'r\') as file: code = file.read() else: code = source compiled_code = compile(code, \'<string>\', \'exec\') exec(compiled_code, context) return context except Exception as e: return f\\"An error occurred: {traceback.format_exc()}\\""},{"question":"Objective Use the `ast` and `dis` modules to analyze a given Python function, identify specific components in its abstract syntax tree (AST), and disassemble its bytecode. Problem Statement You are given a Python function as a string. Your task is to write a function `analyze_python_function` that takes this string as input, performs the following tasks, and returns a summary dictionary: 1. **Parse the function string into an AST** using the `ast` module. 2. **Identify all function calls** within the given function. 3. **Disassemble the function\'s bytecode** using the `dis` module. 4. **Return a dictionary** with the following keys: - `function_calls`: A list of function names that are called within the given function. - `bytecode_instructions`: A list of bytecode instructions produced by the `dis` module. Function Signature ```python def analyze_python_function(func_str: str) -> dict: pass ``` Input - `func_str` (str): A string containing the Python function to be analyzed. Output - `result` (dict): A dictionary containing: - `function_calls` (list of str): Names of the functions that are called within the given function. - `bytecode_instructions` (list of str): Bytecode instructions generated from disassembly. Example ```python input_function = \'\'\' def example_function(x): y = math.sqrt(x) z = sum([1, 2, 3]) return y + z \'\'\' output = analyze_python_function(input_function) print(output) ``` Expected Output: ```python { \'function_calls\': [\'math.sqrt\', \'sum\'], \'bytecode_instructions\': [ \' 2 0 LOAD_GLOBAL 0 (math)\', \' 2 LOAD_METHOD 1 (sqrt)\', \' 4 LOAD_FAST 0 (x)\', \' 6 CALL_METHOD 1\', \' 8 STORE_FAST 1 (y)\', \' 3 10 LOAD_GLOBAL 2 (sum)\', \' 12 LOAD_CONST 1 ((1, 2, 3))\', \' 14 CALL_FUNCTION 1\', \' 16 STORE_FAST 2 (z)\', \' 4 18 LOAD_FAST 1 (y)\', \' 20 LOAD_FAST 2 (z)\', \' 22 BINARY_ADD\', \' 24 RETURN_VALUE\' ] } ``` Constraints - You may assume the input string is a syntactically correct Python function. - The function will only contain calls to globally available functions or methods from imported modules. - You must use the `ast` and `dis` modules for analyzing the function. # Notes - Ensure that your solution is efficient in terms of both time and space complexity. - You may utilize additional helper functions if necessary to structure your code.","solution":"import ast import dis import io def analyze_python_function(func_str: str) -> dict: # Parse the function string into an AST tree = ast.parse(func_str) # Find all function calls class FunctionCallVisitor(ast.NodeVisitor): def __init__(self): self.function_calls = [] def visit_Call(self, node): if isinstance(node.func, ast.Attribute): self.function_calls.append(f\\"{node.func.value.id}.{node.func.attr}\\") elif isinstance(node.func, ast.Name): self.function_calls.append(node.func.id) self.generic_visit(node) visitor = FunctionCallVisitor() visitor.visit(tree) # Compile the function string into a code object compiled_code = compile(func_str, \\"<string>\\", \\"exec\\") # Disassemble the function\'s bytecode disassembled_output = io.StringIO() dis.dis(compiled_code, file=disassembled_output) disassembled_output.seek(0) bytecode_instructions = disassembled_output.read().splitlines() # Create and return the result dictionary result = { \'function_calls\': visitor.function_calls, \'bytecode_instructions\': bytecode_instructions } return result"},{"question":"**Question:** In this assessment, you are tasked with creating a type-safe configuration management system using Python\'s `typing` module. This system should help manage and validate configurations for a software application. You are required to use various features of the `typing` module to define types, annotate functions, and ensure type safety. # Requirements: 1. **Define Custom TypedDicts:** - Create a `ServerConfig` `TypedDict` that includes the following keys: - `host`: a string indicating the server\'s hostname. - `port`: an integer indicating the port number. - `use_ssl`: a boolean indicating whether to use SSL. - `timeout`: an optional integer indicating the timeout duration in seconds. Default is `None`. - Create an `AppConfig` `TypedDict` that includes the following keys: - `debug`: a boolean indicating if the application is in debug mode. - `database_url`: a string containing the URL to the database. - `server`: a `ServerConfig` instance. 2. **Function Implementation:** - Write a function `validate_config` that takes an `AppConfig` instance and performs the following checks: - Ensure that the `host` in `server` is not an empty string. - Ensure that the `port` in `server` is between 1 and 65535. - Ensure that if `timeout` is provided, it is a positive integer. 3. **Protocol Implementation:** - Define a `Runnable` protocol that describes objects with a `run` method accepting no arguments and returning `None`. - Annotate and implement a `run_config` function that takes an object conforming to the `Runnable` protocol and runs it. 4. **Optional Type Guards:** - Implement an optional type guard function `is_valid_timeout` that takes any value and returns `True` if the value is a valid timeout (an integer greater than 0), otherwise `False`. 5. **Error Handling:** - Ensure that `validate_config` raises appropriate `ValueError`s for invalid configurations. # Input and Output: - **validate_config**: - **Input**: An `AppConfig` instance. - **Output**: `None`, but raises `ValueError` for invalid configurations. - **run_config**: - **Input**: An object conforming to the `Runnable` protocol. - **Output**: `None`. - **is_valid_timeout** (Optional): - **Input**: Any value. - **Output**: A boolean indicating whether the input is a valid timeout. # Example: ```python from typing import TypedDict, Protocol, Optional, TypeGuard, Any class ServerConfig(TypedDict): host: str port: int use_ssl: bool timeout: Optional[int] class AppConfig(TypedDict): debug: bool database_url: str server: ServerConfig def validate_config(config: AppConfig) -> None: server = config[\'server\'] if not server[\'host\']: raise ValueError(\\"Server \'host\' cannot be empty.\\") if not (1 <= server[\'port\'] <= 65535): raise ValueError(\\"Server \'port\' must be between 1 and 65535.\\") if server[\'timeout\'] is not None and server[\'timeout\'] <= 0: raise ValueError(\\"Server \'timeout\' must be a positive integer if provided.\\") class Runnable(Protocol): def run(self) -> None: pass def run_config(runnable: Runnable) -> None: runnable.run() def is_valid_timeout(value: Any) -> TypeGuard[int]: return isinstance(value, int) and value > 0 ``` Ensure all type annotations are correct and that you handle errors appropriately. Use the provided example as guidance but write your implementations from scratch.","solution":"from typing import TypedDict, Protocol, Optional, TypeGuard, Any class ServerConfig(TypedDict): host: str port: int use_ssl: bool timeout: Optional[int] class AppConfig(TypedDict): debug: bool database_url: str server: ServerConfig def validate_config(config: AppConfig) -> None: server = config[\'server\'] if not server[\'host\']: raise ValueError(\\"Server \'host\' cannot be empty.\\") if not (1 <= server[\'port\'] <= 65535): raise ValueError(\\"Server \'port\' must be between 1 and 65535.\\") if server[\'timeout\'] is not None and server[\'timeout\'] <= 0: raise ValueError(\\"Server \'timeout\' must be a positive integer if provided.\\") class Runnable(Protocol): def run(self) -> None: pass def run_config(runnable: Runnable) -> None: runnable.run() def is_valid_timeout(value: Any) -> TypeGuard[int]: return isinstance(value, int) and value > 0"},{"question":"You are required to implement a thread-based solution to simulate a simplified multi-threaded download manager. This manager: - Spawns multiple threads to download different parts of a file concurrently. - Merges the downloaded parts into a single file once all parts are downloaded. - Uses locks and semaphores to ensure proper synchronization and prevent race conditions. # Requirements: 1. **Download Manager Class**: - Implement a class named `DownloadManager` with the following methods: ```python class DownloadManager: def __init__(self, url: str, parts: int): pass def download_part(self, part_id: int): pass def start_download(self): pass def merge_parts(self): pass ``` - `__init__(self, url: str, parts: int)`: Initializes the download manager with a URL to download and the number of parts to split the download into. - `download_part(self, part_id: int)`: Simulates the download of a single part of the file. Each part will just be a string `\\"Part<part_id>\\"` for this simulation. - `start_download(self)`: Starts the download by creating and starting `parts` number of threads, each responsible for downloading a part by calling `download_part`. - `merge_parts(self)`: Merges all downloaded parts in sequential order after all parts are downloaded. 2. **Synchronization**: - Use a `Semaphore` to limit the number of concurrent downloads to ensure that no more than `N` threads are downloading at the same time. `N` can be defined as a constant within the class (e.g., `CONCURRENT_DOWNLOADS = 3`). - Use a `Lock` to ensure that the merging process happens only after all parts are downloaded. 3. **Testing**: - You must write a function to test the download manager using assertions. ```python def test_download_manager(): url = \\"http://example.com/largefile\\" parts = 5 manager = DownloadManager(url, parts) manager.start_download() manager.merge_parts() expected_output = \\"Part0Part1Part2Part3Part4\\" assert manager.final_output == expected_output print(\\"All parts downloaded and merged successfully!\\") ``` # Constraints: - Every part download should be simulated with a sleep of `1` second to mimic network delay. - Each part should be downloaded only once, and merging should be done in the correct order. # Performance Requirements: - The solution should handle up to `10` parts with `3` concurrent downloads efficiently without any race conditions. ```python import threading import time class DownloadManager: CONCURRENT_DOWNLOADS = 3 def __init__(self, url: str, parts: int): self.url = url self.parts = parts self.downloaded_parts = [None] * parts self.part_semaphore = threading.Semaphore(self.CONCURRENT_DOWNLOADS) self.lock = threading.Lock() self.final_output = \\"\\" def download_part(self, part_id: int): with self.part_semaphore: time.sleep(1) # Simulate download time self.downloaded_parts[part_id] = f\\"Part{part_id}\\" print(f\\"Downloaded part {part_id}\\") def start_download(self): threads = [] for part_id in range(self.parts): t = threading.Thread(target=self.download_part, args=(part_id,)) t.start() threads.append(t) for t in threads: t.join() def merge_parts(self): with self.lock: self.final_output = \'\'.join(self.downloaded_parts) print(f\\"Merged into: {self.final_output}\\") def test_download_manager(): url = \\"http://example.com/largefile\\" parts = 5 manager = DownloadManager(url, parts) manager.start_download() manager.merge_parts() expected_output = \\"Part0Part1Part2Part3Part4\\" assert manager.final_output == expected_output print(\\"All parts downloaded and merged successfully!\\") # Run the test test_download_manager() ```","solution":"import threading import time class DownloadManager: CONCURRENT_DOWNLOADS = 3 def __init__(self, url: str, parts: int): self.url = url self.parts = parts self.downloaded_parts = [None] * parts self.part_semaphore = threading.Semaphore(self.CONCURRENT_DOWNLOADS) self.lock = threading.Lock() self.final_output = \\"\\" def download_part(self, part_id: int): with self.part_semaphore: time.sleep(1) # Simulate download time self.downloaded_parts[part_id] = f\\"Part{part_id}\\" print(f\\"Downloaded part {part_id}\\") def start_download(self): threads = [] for part_id in range(self.parts): t = threading.Thread(target=self.download_part, args=(part_id,)) t.start() threads.append(t) for t in threads: t.join() def merge_parts(self): with self.lock: self.final_output = \'\'.join(self.downloaded_parts) print(f\\"Merged into: {self.final_output}\\")"},{"question":"# PyTorch Coding Assessment: Implementing and Verifying Custom Gradient Functions Objective In this assessment, you will implement a custom function in PyTorch, manually compute its analytical gradients, and verify these gradients using PyTorch\'s `gradcheck` and `gradgradcheck` utilities. This will test your understanding of PyTorch\'s autograd functionality and gradient verification processes. Task 1. **Implement a Custom Function**: - Implement a custom PyTorch function `CustomFunction`, which performs an element-wise operation on a tensor. - Define its forward and backward passes. 2. **Compute Analytical Gradients**: - Manually compute the gradients of the custom function with respect to its inputs. 3. **Verify Gradients Using PyTorch Gradcheck**: - Use `torch.autograd.gradcheck` to verify the correctness of the manually computed gradients. - Use `torch.autograd.gradgradcheck` to verify the correctness of the second-order gradients. Requirements - Your custom function should take a tensor as input and compute an element-wise operation such as (y = x^3 + sin(x)), where (x) is the input tensor. - Implement the analytical gradients for both the first and second derivatives. - Ensure that your implementation passes both `gradcheck` and `gradgradcheck`. Input Format - A PyTorch tensor `input_tensor` of size (N,) where N is a positive integer. Output Format - A boolean value indicating whether the gradient checks passed. Constraints - You must use PyTorch for the implementation. - Ensure numerical stability in your calculations, particularly when computing finite difference gradients. Example ```python import torch from torch.autograd import Function from torch.autograd import gradcheck, gradgradcheck class CustomFunction(Function): @staticmethod def forward(ctx, input_tensor): ctx.save_for_backward(input_tensor) return input_tensor ** 3 + torch.sin(input_tensor) @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors grad_input = grad_output * (3 * input_tensor ** 2 + torch.cos(input_tensor)) return grad_input def test_custom_function(): input_tensor = torch.randn(10, dtype=torch.double, requires_grad=True) func = CustomFunction.apply # Perform gradient check gradcheck_passed = gradcheck(func, (input_tensor,), eps=1e-6, atol=1e-4) # Perform gradient of gradient check gradgradcheck_passed = gradgradcheck(func, (input_tensor,), eps=1e-6, atol=1e-4) return gradcheck_passed and gradgradcheck_passed # Example usage print(test_custom_function()) # Should output: True if both checks pass ``` In this example, you need to complete the implementation of the `CustomFunction` class, ensuring that the backward pass correctly computes the gradient of the output with respect to the input. Then, you will test whether this implementation passes PyTorch\'s gradcheck and gradgradcheck functions.","solution":"import torch from torch.autograd import Function from torch.autograd import gradcheck, gradgradcheck class CustomFunction(Function): @staticmethod def forward(ctx, input_tensor): ctx.save_for_backward(input_tensor) return input_tensor ** 3 + torch.sin(input_tensor) @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors grad_input = grad_output * (3 * input_tensor ** 2 + torch.cos(input_tensor)) return grad_input def test_custom_function(): input_tensor = torch.randn(10, dtype=torch.double, requires_grad=True) func = CustomFunction.apply # Perform gradient check gradcheck_passed = gradcheck(func, (input_tensor,), eps=1e-6, atol=1e-4) # Perform gradient of gradient check gradgradcheck_passed = gradgradcheck(func, (input_tensor,), eps=1e-6, atol=1e-4) return gradcheck_passed and gradgradcheck_passed"},{"question":"# XML DOM Manipulation with `xml.dom` Module Given an XML document as input, implement a function `manipulate_xml(input_xml: str) -> str` that performs the following tasks using the `xml.dom` module: 1. Parses the given input XML string into a DOM document. 2. Adds a new `Element` to the root of the document with the tag name \\"Library\\". 3. Inside the \\"Library\\" element, adds three `Element` nodes: - \\"Book\\" with an attribute \\"id\\" set to \\"1\\" and text content \\"Python Programming\\". - \\"Book\\" with an attribute \\"id\\" set to \\"2\\" and text content \\"Learning XML\\". - \\"Book\\" with an attribute \\"id\\" set to \\"3\\" and text content \\"Advanced Algorithms\\". 4. Serializes the modified DOM document back into a string and returns it. # Constraints - The input XML string will be a well-formed XML document. - You must use the `xml.dom` module for XML manipulation. - Ensure that the output XML string is also well-formed. # Input - `input_xml (str)`: A string representing the input XML document. # Output - `str`: A string representing the modified XML document. # Example Input ```xml <root> <example>This is a test</example> </root> ``` Output ```xml <root> <example>This is a test</example> <Library> <Book id=\\"1\\">Python Programming</Book> <Book id=\\"2\\">Learning XML</Book> <Book id=\\"3\\">Advanced Algorithms</Book> </Library> </root> ``` # Notes - Use the appropriate methods and classes from the `xml.dom` module, such as `Document`, `Element`, and `DOMImplementation`. - Make sure to handle creating elements, setting attributes, and inserting them into the document correctly. - Exception handling should be implemented where necessary, though the primary focus of the task is on document manipulation.","solution":"from xml.dom.minidom import parseString, Document, Element def manipulate_xml(input_xml: str) -> str: Parses the given input XML string into a DOM document, modifies it by adding a Library element with Book elements, and returns the modified XML string. # Parse the input XML string into a DOM document dom = parseString(input_xml) document = dom.documentElement # Create a new Library element library = dom.createElement(\\"Library\\") # Define book information books_info = [ (\\"1\\", \\"Python Programming\\"), (\\"2\\", \\"Learning XML\\"), (\\"3\\", \\"Advanced Algorithms\\") ] # Add Book elements to the Library element for book_id, book_title in books_info: book = dom.createElement(\\"Book\\") book.setAttribute(\\"id\\", book_id) book.appendChild(dom.createTextNode(book_title)) library.appendChild(book) # Append the Library element to the root document element document.appendChild(library) # Serialize the modified DOM document back into a string return dom.toxml()"},{"question":"# Email Message Processor You are required to implement a function that processes an email message, modifies its headers and payload, and provides various representations of the message. Use the `email.message.Message` class for this purpose. Problem: 1. **Create a function** `process_email_message(subject, body, attachments)`: - `subject` (string): The subject of the email. - `body` (string): The text body of the email. - `attachments` (list of tuples): Each tuple contains `filename, content` where `filename` is a string and `content` is bytes to be attached. 2. **Function requirements**: - Initialize an instance of `email.message.Message`. - Set the subject of the email. - Set the body as text/plain. - Attach each file from the `attachments` as `Content-Disposition: attachment`. - Return a dictionary with three keys: - `as_string`: String representation of the email. - `as_bytes`: Byte representation of the email. - `multipart`: Boolean indicating if the message is multipart. Example: ```python subject = \\"Meeting Agenda\\" body = \\"Please find attached the agenda for our meeting.\\" attachments = [ (\'agenda.pdf\', b\'<PDF binary content>\') ] result = process_email_message(subject, body, attachments) print(result[\'as_string\']) print(result[\'as_bytes\']) print(result[\'multipart\']) ``` Constraints: - `subject` will not exceed 255 characters. - `body` will not exceed 10,000 characters. - The size of each attachment will not exceed 5 MB. - The number of attachments will not exceed 5. Expectations: - Use the appropriate methods from the `email.message.Message` class to achieve the desired functionality. - Ensure MIME formatting is correctly handled. - Correctly serialize the message for both string and byte representations. - Mastery in handling multipart emails, ensuring the correctness of the email structures and types.","solution":"import email from email.message import EmailMessage def process_email_message(subject, body, attachments): msg = EmailMessage() # Set the subject and body of the email msg[\'Subject\'] = subject msg.set_content(body) # Attach files if any for filename, content in attachments: msg.add_attachment(content, maintype=\'application\', subtype=\'octet-stream\', filename=filename) # Get representations of the email message as_string = msg.as_string() as_bytes = msg.as_bytes() multipart = msg.is_multipart() return { \'as_string\': as_string, \'as_bytes\': as_bytes, \'multipart\': multipart }"},{"question":"<|Analysis Begin|> The documentation provided is for the `sys` module in Python, which includes various system-specific parameters and functions that interact closely with the interpreter. The module is always available and contains a wide range of tools to interact with the Python runtime environment, manage paths, handle exceptions, and more. Key concepts and functions covered: 1. Accessing interpreter-specific data such as `sys.argv` for command-line arguments, `sys.byteorder` for byte order, and `sys.platform` for the OS platform. 2. Managing auditing hooks using `sys.addaudithook` and `sys.audit`. 3. Retrieving system-specific configuration data, such as `sys.float_info`, `sys.int_info`, and platform details. 4. Managing the Python execution environment, including `sys.settrace`, `sys.setprofile`, and the handling of standard streams like `sys.stdin`, `sys.stdout`, and `sys.stderr`. 5. Specialized functions for debugging and runtime information, such as `sys.getrefcount`, `sys.getsizeof`, `sys._getframe`, and `sys._current_frames`. Based on the depth and variety of functions provided in the `sys` module, we can craft a comprehensive and challenging question that requires the application and understanding of multiple functionalities within this module. <|Analysis End|> <|Question Begin|> **Problem Statement: System Configuration Auditor** You are required to implement a Python function `system_configuration_audit(output_file: str) -> None` that performs a comprehensive audit of the current Python runtime and system configuration. The function must gather various system parameters, perform specific actions based on these parameters, and write the gathered information into a specified output file. Here are the detailed requirements: 1. The function should not take any input except the output file name. All necessary information must be gathered programmatically using the `sys` module. 2. The function should gather the following information: - Python version and implementation details (`sys.version`, `sys.implementation`). - Command-line arguments passed to the script (`sys.argv`). - The platform and byte order of the system (`sys.platform`, `sys.byteorder`). - Memory information, including total allocated memory blocks and size of the current script (`sys.getallocatedblocks`, `sys.getsizeof(__file__)`). - File system encoding and error handling settings (`sys.getfilesystemencoding`, `sys.getfilesystemencodeerrors`). - Path information including Python executable path, search path for modules, and the base prefix (`sys.executable`, `sys.path`, `sys.base_prefix`). 3. The function should log an auditing event named `system_configuration_audit` with the event name and all gathered information as arguments using `sys.audit`. 4. The function should handle exceptions gracefully, using `sys.excepthook` to log any uncaught exceptions, and ensure that any error encountered during the information-gathering process does not terminate the program prematurely. 5. Finally, write the gathered information into the specified output file in a well-structured and readable format. # Input - `output_file` (str): The path to the output file where the audit report will be saved. # Output - The function will not return anything but will write the information into the `output_file`. # Constraints - The function should execute on any platform and is expected to handle exceptions gracefully. - Python 3.10 or newer. # Example ```python def system_configuration_audit(output_file): # Implement the function here pass # Example Usage: system_configuration_audit(\'system_audit.txt\') ``` In this example, calling `system_configuration_audit(\'system_audit.txt\')` would generate a file `system_audit.txt` containing detailed information about the current Python runtime environment and system configuration following the requirements outlined above. # Instructions 1. Implement the function `system_configuration_audit` following the requirements. 2. Ensure to use appropriate `sys` module functions and handle exceptions correctly. 3. Write the output to the specified file path. Make sure the data is well-structured and readable. Good luck!","solution":"import sys import traceback def system_configuration_audit(output_file): Performs a comprehensive audit of the current Python runtime and system configuration. Writes the gathered information into the specified output file. Args: output_file (str): The path to the output file where the audit report will be saved. audit_info = {} try: # Gather information audit_info[\'Python Version\'] = sys.version audit_info[\'Implementation\'] = sys.implementation audit_info[\'Command-line Arguments\'] = sys.argv audit_info[\'Platform\'] = sys.platform audit_info[\'Byte Order\'] = sys.byteorder audit_info[\'Allocated Memory Blocks\'] = sys.getallocatedblocks() audit_info[\'Current Script Size\'] = sys.getsizeof(__file__) audit_info[\'File System Encoding\'] = sys.getfilesystemencoding() audit_info[\'File System Encode Errors\'] = sys.getfilesystemencodeerrors() audit_info[\'Executable Path\'] = sys.executable audit_info[\'Module Search Path\'] = sys.path audit_info[\'Base Prefix\'] = sys.base_prefix # Log auditing event sys.audit(\'system_configuration_audit\', audit_info) # Write information to the output file with open(output_file, \'w\') as f: for key, value in audit_info.items(): f.write(f\\"{key}: {value}n\\") except Exception as e: # Handle exceptions gracefully with open(output_file, \'a\') as f: f.write(\\"nAn error occurred during system configuration audit:n\\") traceback.print_exc(file=f) # Set a custom exception hook to handle any uncaught exceptions def custom_excepthook(exctype, value, tb): with open(output_file, \'a\') as f: f.write(\\"nUncaught Exception:n\\") traceback.print_exception(exctype, value, tb, file=f) sys.excepthook = custom_excepthook"},{"question":"Problem Statement You are given a dataset containing the scores of students in a particular subject from two different classes. Your task is to analyze the data visually using Seaborn and generate a plot that includes error bars. # Dataset The dataset is stored in a CSV file named `student_scores.csv` and has the following structure: ``` student_id,class,score 1,A,78 2,A,85 3,A,90 ... 50,B,67 51,B,74 ... ``` # Requirements 1. Load the dataset using Pandas. 2. Create a Seaborn point plot showing the average scores for each class with error bars representing the standard error. 3. Create another Seaborn point plot showing the same average scores but with confidence intervals. 4. Add titles and labels to the plots for clarity. # Input - Path to the CSV file (`student_scores.csv`). # Output - Displayed Seaborn plots as described above. # Function Signature ```python def analyze_student_scores(file_path: str) -> None: pass ``` # Constraints - Ensure that the error bars are clearly visible on the plots. - Use appropriate Seaborn and Matplotlib functions to customize the appearance of the plots. - Ensure the code is well-commented and follows best practices in coding and data visualization. # Example ```python # Example usage analyze_student_scores(\\"student_scores.csv\\") ``` The function `analyze_student_scores` should generate and display two Seaborn point plots with the specified error bars. # Note - You do not need to return anything from the function; only ensure the plots are displayed correctly. - Test your function with provided sample data to ensure it works as expected.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def analyze_student_scores(file_path: str) -> None: Analyzes student scores from a CSV file and generates Seaborn plots with error bars. Args: file_path (str): Path to the CSV file containing student scores. # Load the dataset df = pd.read_csv(file_path) # Create a seaborn point plot with standard error bars plt.figure(figsize=(10, 6)) std_error_plot = sns.pointplot(x=\'class\', y=\'score\', data=df, ci=\'sd\', capsize=.2) std_error_plot.set_title(\'Average Student Scores with Standard Error Bars\') std_error_plot.set_xlabel(\'Class\') std_error_plot.set_ylabel(\'Average Score\') plt.show() # Create another seaborn point plot with confidence intervals plt.figure(figsize=(10, 6)) confidence_interval_plot = sns.pointplot(x=\'class\', y=\'score\', data=df, ci=95, capsize=.2) confidence_interval_plot.set_title(\'Average Student Scores with 95% Confidence Intervals\') confidence_interval_plot.set_xlabel(\'Class\') confidence_interval_plot.set_ylabel(\'Average Score\') plt.show()"},{"question":"You are given a dataset containing the performance statistics of students across different subjects. The dataset has the following columns: - `student`: Name of the student - `subject`: Subject name (e.g., Math, Science, English) - `score`: Score obtained by the student in that subject - `gender`: Gender of the student (\'Male\', \'Female\') Using this dataset, create a visualization to show: 1. A bar plot of the average scores of students in each subject. 2. The same bar plot, with different colors representing the gender of the students. 3. Error bars indicating the standard deviation of the scores in each subject, split by gender. 4. Customize the plot to have transparency (`alpha`) and edge styles based on gender. **Input:** - A `pandas.DataFrame` named `student_scores` with the following columns: `student`, `subject`, `score`, `gender`. **Output:** - A Matplotlib figure object displaying the above-described visualization. ```python def plot_student_performance(student_scores): import seaborn.objects as so fig = ( so.Plot(student_scores, x=\\"subject\\", y=\\"score\\", color=\\"gender\\") .add(so.Bar(alpha=0.5, edgewidth=2), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) return fig.show() ``` **Constraints:** - The function should handle datasets with at least two subjects and two genders. - The scores are numerical and range between 0 and 100. - The plot should be clearly labeled with appropriate titles and axis labels.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_student_performance(student_scores): Creates a bar plot showing: 1. Average scores of students in each subject. 2. Different colors for the gender of the students. 3. Error bars indicating the standard deviation of the scores in each subject, split by gender. 4. Custom transparency (`alpha`) and edge styles based on gender. Args: student_scores (pd.DataFrame): DataFrame containing student performance data with columns: \'student\', \'subject\', \'score\', \'gender\'. Returns: matplotlib.figure.Figure: The Matplotlib figure object containing the plot. # Set plot style sns.set(style=\\"whitegrid\\") # Create the bar plot with error bars plt.figure(figsize=(10, 6)) ax = sns.barplot( data=student_scores, x=\'subject\', y=\'score\', hue=\'gender\', ci=\'sd\', alpha=0.75, edgecolor=\'k\' ) # Customize plot appearance ax.set_title(\'Average Student Scores by Subject and Gender\') ax.set_xlabel(\'Subject\') ax.set_ylabel(\'Average Score\') # Add legend handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels, title=\'Gender\') # Improve layout plt.tight_layout() # Return the figure object return plt.gcf()"},{"question":"In this coding assessment, you are required to create a Python function using the asyncio subprocess module. The function should create and manage subprocesses to run shell commands concurrently. It should handle both successful execution and potential errors, ensuring the results are collected and returned appropriately. Problem Statement Your task is to implement an asynchronous function `run_commands_concurrently(commands: List[str]) -> List[Dict[str, Any]]` that takes a list of shell commands as input and runs them concurrently. The function should return a list of dictionaries, each containing the command, its output, its error output, and its exit status. Function Signature ```python import asyncio from typing import List, Dict, Any async def run_commands_concurrently(commands: List[str]) -> List[Dict[str, Any]]: pass ``` Input The function takes a single parameter: - `commands` (List[str]): A list of shell commands to be executed concurrently. Output The function should return: - A list of dictionaries, each containing: - `\'command\'`: The command that was executed (str). - `\'stdout\'`: The standard output of the command (str). - `\'stderr\'`: The standard error of the command (str). - `\'exit_code\'`: The exit status of the command (int). Example ```python import asyncio from typing import List, Dict, Any async def run_commands_concurrently(commands: List[str]) -> List[Dict[str, Any]]: results = [] async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() result = { \'command\': cmd, \'stdout\': stdout.decode().strip(), \'stderr\': stderr.decode().strip(), \'exit_code\': proc.returncode } results.append(result) await asyncio.gather(*(run(cmd) for cmd in commands)) return results # Example usage: commands = [\'ls\', \'echo \\"Hello, World!\\"\', \'invalid_command\'] results = asyncio.run(run_commands_concurrently(commands)) for result in results: print(result) ``` Constraints - The commands should be relatively short and not produce an excessive amount of data in stdout or stderr. - The function should handle empty command lists gracefully, returning an empty list. - If a command fails (non-zero exit code), the function should capture and include the error in the result. Requirements - Use `asyncio.create_subprocess_shell()` to run the commands. - Capture `stdout` and `stderr` for each command. - Run all commands concurrently using `asyncio.gather()`. Notes - Make sure to handle shell injection vulnerabilities by ensuring commands are properly quoted if necessary. - Use `shlex.quote()` for quoting the commands appropriately to avoid shell injection.","solution":"import asyncio from typing import List, Dict, Any async def run_commands_concurrently(commands: List[str]) -> List[Dict[str, Any]]: results = [] async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() result = { \'command\': cmd, \'stdout\': stdout.decode().strip(), \'stderr\': stderr.decode().strip(), \'exit_code\': proc.returncode } results.append(result) await asyncio.gather(*(run(cmd) for cmd in commands)) return results"},{"question":"# Advanced Python Assessment - POSIX and OS Modules **Problem Statement:** You are tasked with creating a utility function that sets up the environment variables, creates a large file (if not exists), writes some data to it, and spawns a subprocess to verify the file\'s contents. **Requirements:** 1. **Function Name:** `setup_environment_and_verify` 2. **Input:** - `env_vars`: A dictionary of environment variables to set (keys and values as strings). - `file_path`: Path of the file to create/write. - `data`: Data (string) to write into the file. 3. **Output:** - Return the standard output (string) from the subprocess that reads and prints the file content. 4. **Constraints:** - The function shall modify the environment using `os` module. - The file should be created only if it does not exist. - If the file already exists, append the data to it. 5. **Performance Requirements:** - Handle large files efficiently. - Ensure environment variables are set for the subprocess properly. **Function Implementation:** Implement the function with the following signature: ```python import os import subprocess def setup_environment_and_verify(env_vars, file_path, data): # Step 1: Set environment variables using os.environ for key, value in env_vars.items(): os.environ[key] = value # Step 2: Create the file if it does not exist, or append data if it does with open(file_path, \'a+\') as file: file.write(data) # Step 3: Create a subprocess to read and print content of the file result = subprocess.run([\'cat\', file_path], capture_output=True, text=True) # Step 4: Return the Standard Output from the subprocess return result.stdout ``` # Example: ```python # Example usage: env = {\'MY_VAR\': \'value123\'} file_path = \'/tmp/test_large_file.txt\' data = \'This is a test line.n\' print(setup_environment_and_verify(env, file_path, data)) ``` **Notes:** - Assume the script runs on a Unix-based system where the commands like `cat` are available. - Do not use direct `posix` module imports; use `os` and `subprocess`. - Ensure your function works for both small and large data efficiently.","solution":"import os import subprocess def setup_environment_and_verify(env_vars, file_path, data): # Step 1: Set environment variables using os.environ for key, value in env_vars.items(): os.environ[key] = value # Step 2: Create the file if it does not exist, or append data if it does with open(file_path, \'a+\') as file: file.write(data) # Step 3: Create a subprocess to read and print content of the file result = subprocess.run([\'cat\', file_path], capture_output=True, text=True) # Step 4: Return the Standard Output from the subprocess return result.stdout"},{"question":"Question # Objective: To assess the student\'s ability to implement a function using type hints and then write comprehensive unit tests for that function using the `unittest` framework. # Task: 1. Implement a function `merge_dictionaries` that takes two dictionaries as input and merges them. The function should use type hints. 2. Write a `unittest` TestCase to test the function `merge_dictionaries`. # Function Specification: ```python def merge_dictionaries(dict1: Dict[str, Union[int, str]], dict2: Dict[str, Union[int, str]]) -> Dict[str, Union[int, str]]: Merges two dictionaries. If a key is present in both dictionaries, the value from the second dictionary should be taken. Args: dict1 (Dict[str, Union[int, str]]): The first dictionary to merge. dict2 (Dict[str, Union[int, str]]): The second dictionary to merge. Returns: Dict[str, Union[int, str]]: The merged dictionary. pass ``` # Testing Requirements: - Use the `unittest` framework to create a `TestCase` class. - Write at least **three** different test methods to comprehensively test the `merge_dictionaries` function. Consider edge cases like empty dictionaries, conflicting keys, and non-conflicting keys. # Input and Output Formats: - **Input**: - Two dictionaries (`dict1` and `dict2`) with keys of type `str` and values of type `int` or `str`. - **Output**: - A dictionary representing the merged result. # Constraints: - Assume all dictionary keys are strings. - Values can be either integers or strings. - For conflicting keys (keys present in both dict1 and dict2), the function should take the value from `dict2`. # Notes: - Ensure your code is clear and well-documented. - Follow Python\'s best practices for writing clean and efficient code. # Example: ```python def merge_dictionaries(dict1: Dict[str, Union[int, str]], dict2: Dict[str, Union[int, str]]) -> Dict[str, Union[int, str]]: result = dict1.copy() result.update(dict2) return result import unittest class TestMergeDictionaries(unittest.TestCase): def test_non_conflicting_keys(self): dict1 = {\\"a\\": 1, \\"b\\": 2} dict2 = {\\"c\\": 3, \\"d\\": 4} expected = {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3, \\"d\\": 4} self.assertEqual(merge_dictionaries(dict1, dict2), expected) def test_conflicting_keys(self): dict1 = {\\"a\\": 1, \\"b\\": 2} dict2 = {\\"b\\": 3, \\"c\\": 4} expected = {\\"a\\": 1, \\"b\\": 3, \\"c\\": 4} self.assertEqual(merge_dictionaries(dict1, dict2), expected) def test_empty_dict(self): dict1 = {} dict2 = {\\"a\\": 1} expected = {\\"a\\": 1} self.assertEqual(merge_dictionaries(dict1, dict2), expected) if __name__ == \'__main__\': unittest.main() ```","solution":"from typing import Dict, Union def merge_dictionaries(dict1: Dict[str, Union[int, str]], dict2: Dict[str, Union[int, str]]) -> Dict[str, Union[int, str]]: Merges two dictionaries. If a key is present in both dictionaries, the value from the second dictionary should be taken. Args: dict1 (Dict[str, Union[int, str]]): The first dictionary to merge. dict2 (Dict[str, Union[int, str]]): The second dictionary to merge. Returns: Dict[str, Union[int, str]]: The merged dictionary. result = dict1.copy() result.update(dict2) return result"},{"question":"**Question: Directory Comparison Detailed Report** You are required to write a function `generate_comparison_report(dir1: str, dir2: str) -> str` that compares two directories, `dir1` and `dir2`, and generates a detailed report of their differences. The report should include: 1. A list of files that are present only in `dir1`. 2. A list of files that are present only in `dir2`. 3. A list of files that exist in both directories but have different contents. 4. For files with differing contents, details such as their sizes and last modification times. 5. Any file comparison errors. # Input - `dir1` (str): Path to the first directory. - `dir2` (str): Path to the second directory. # Output - Returns a string containing the detailed comparison report. # Constraints - The directories may contain subdirectories, and the comparison should be performed recursively. - You may assume that you have read permissions for the directories and their files. - You should handle exceptions gracefully and include any file comparison errors in the report. - Use the `filecmp` module for file and directory comparisons. # Example ```python dir1 = \'/path/to/dir1\' dir2 = \'/path/to/dir2\' report = generate_comparison_report(dir1, dir2) print(report) ``` # Sample Report ``` Files only in /path/to/dir1: - file1.txt - subdir1/file2.txt Files only in /path/to/dir2: - file3.txt - subdir2/file4.txt Files with different contents: - file5.txt: size(dir1) = 1024 bytes, size(dir2) = 2048 bytes, mtime(dir1) = 2023-10-01 12:00:00, mtime(dir2) = 2023-10-01 15:00:00 File comparison errors: - subdir3/file6.txt: Permission denied ``` # Notes: - You may find the `os` module helpful for fetching file size and modification times. - Customize the error messages as needed to handle different error situations. **Hint:** Use `filecmp.dircmp` for the directory comparison and leverage its attributes and methods to gather the necessary information.","solution":"import os import filecmp import datetime def generate_comparison_report(dir1: str, dir2: str) -> str: Compares two directories and generates a detailed report on their differences. report_lines = [] def get_file_details(file_path): try: size = os.path.getsize(file_path) mtime = datetime.datetime.fromtimestamp(os.path.getmtime(file_path)) return size, mtime except OSError as e: return None, str(e) def compare_directories(dcmp): if dcmp.left_only: report_lines.append(f\\"Files only in {dcmp.left}:\\") for name in dcmp.left_only: report_lines.append(f\\"- {name}\\") if dcmp.right_only: report_lines.append(f\\"Files only in {dcmp.right}:\\") for name in dcmp.right_only: report_lines.append(f\\"- {name}\\") if dcmp.diff_files: report_lines.append(\\"Files with different contents:\\") for name in dcmp.diff_files: left_file = os.path.join(dcmp.left, name) right_file = os.path.join(dcmp.right, name) left_size, left_mtime = get_file_details(left_file) right_size, right_mtime = get_file_details(right_file) if isinstance(left_size, str): report_lines.append(f\\"- {name}: {left_size}\\") elif isinstance(right_size, str): report_lines.append(f\\"- {name}: {right_size}\\") else: report_lines.append(f\\"- {name}: size({dcmp.left}) = {left_size} bytes, size({dcmp.right}) = {right_size} bytes, \\" f\\"mtime({dcmp.left}) = {left_mtime}, mtime({dcmp.right}) = {right_mtime}\\") if dcmp.common_funny: report_lines.append(\\"File comparison errors:\\") for name in dcmp.common_funny: report_lines.append(f\\"- {name}\\") for sub_dcmp in dcmp.subdirs.values(): compare_directories(sub_dcmp) dcmp = filecmp.dircmp(dir1, dir2) compare_directories(dcmp) return \\"n\\".join(report_lines)"},{"question":"# Python Coding Assessment **Objective**: Demonstrate an understanding of Python module import mechanisms, particularly the deprecated `imp` module, by implementing a custom import function utilizing the provided `imp` functionalities. # Problem Statement You are to implement a function `custom_import_module(module_name)` that mimics the old import mechanism using the `imp` module. This function should: 1. Attempt to find and load a module by its `module_name`. 2. Handle special cases where the module might be built-in or frozen. 3. Manage file resources properly. Your function should perform the following tasks: 1. **Search for the module** using `imp.find_module(module_name)`. 2. **Load the module** using `imp.load_module(module_name, file, pathname, description)`. 3. Properly **close file resources** after loading the module, even if an error occurs during the process. **Requirements**: - Ensure your function handles ImportError gracefully and provides meaningful output. - Consider thread safety by acquiring an import lock before searching and loading the module, and releasing it afterward. **Function Signature**: ```python def custom_import_module(module_name: str) -> object: pass ``` # Input - `module_name` (str): The name of the module to be imported. # Output - Returns the module object if import is successful. - Raises `ImportError` with a descriptive message if the module cannot be found or loaded. # Constraints - The function should use the `imp` module\'s `find_module` and `load_module` functions. - Assume the input `module_name` is a valid Python identifier. - The function should run in Python version 3.6 or later for compatibility, despite using deprecated functionality. # Example ```python try: custom_module = custom_import_module(\'my_module\') print(f\'Module {custom_module.__name__} imported successfully.\') except ImportError as e: print(f\'Import failed: {e}\') ``` Consider a scenario where you want to import your custom module `my_module`. Using the above function should load the module and handle any potential loading issues or resource clean-up properly. # Notes - You should refer to the `imp` module documentation provided above to understand the exact usage and limitations of each function. - Despite being deprecated, understanding the `imp` module\'s functionality can provide insights into the lower-level implementation details of the modern `importlib`. Good luck!","solution":"import imp def custom_import_module(module_name: str) -> object: Mimics the old import mechanism using the `imp` module to import a module by its name. :param module_name: The name of the module to import. :return: The imported module object. :raises ImportError: If the module cannot be found or loaded. try: file, pathname, description = imp.find_module(module_name) try: return imp.load_module(module_name, file, pathname, description) finally: if file: file.close() except ImportError: raise ImportError(f\\"Module \'{module_name}\' not found or failed to load.\\")"},{"question":"# Context Variables Management in Python Background: The `contextvars` module in Python is used to manage context-local state. This module is useful for handling variables that have different values across different contexts in concurrent code. In this challenge, you have to work with context variables by simulating the functionality described by the C-API documentation but within Python itself. This will help in understanding how to use context variables efficiently. Task: You need to implement a Python class `ContextManager` that provides similar functionality to the C-API described. This class should handle context variables and their values across different contexts. Your `ContextManager` class should have the following methods: - `set_context()`: This method should create a new context and set it as the current context. - `copy_context() -> ContextManager`: This should return a shallow copy of the current context. - `enter_context(context)`: This method should set the provided context as the current context. - `exit_context()`: This should deactivate the current context and restore the previous one. - `set_variable(name: str, value) -> Token`: This should set the value of a context variable and return a token object. - `get_variable(name: str, default=None)`: This should return the value of the context variable named `name`. If not found, return `default`. - `reset_variable(token)`: This should reset the context variable to the state before the most recent `set_variable`. Requirements: 1. **ContextManager Class** - Properly manage multiple contexts. - Ensure context switching is handled correctly using `enter_context` and `exit_context`. 2. **Context Variable Management** - Efficiently store and retrieve context variable values. - Reset context variable value using a token. 3. **Token Object** - Implement a suitable token object mechanism to track variable state. ```python class ContextManager: def __init__(self): self._contexts = [] self._current_context = {} def set_context(self): new_context = {} self._contexts.append(self._current_context) self._current_context = new_context def copy_context(self): from copy import deepcopy return deepcopy(self._current_context) def enter_context(self, context): self._contexts.append(self._current_context) self._current_context = context def exit_context(self): if self._contexts: self._current_context = self._contexts.pop() else: raise RuntimeError(\\"No context to exit to\\") def set_variable(self, name: str, value): token = self._current_context.get(name, None) self._current_context[name] = value return (name, token) def get_variable(self, name: str, default=None): return self._current_context.get(name, default) def reset_variable(self, token): name, old_value = token if old_value is None and name in self._current_context: del self._current_context[name] else: self._current_context[name] = old_value # Example usage of ContextManager cm = ContextManager() cm.set_context() cm.set_variable(\'var\', \'value1\') cm.set_context() token = cm.set_variable(\'var\', \'value2\') print(cm.get_variable(\'var\')) # Expected output: \'value2\' cm.reset_variable(token) print(cm.get_variable(\'var\')) # Expected output: None (default context value) ``` You need to fill in the necessary logic for managing contexts, setting, getting, and resetting variables. Save your solution in a file named `context_manager.py`. Constraints: 1. Your solution should not use any external libraries except `copy`. 2. Handle edge cases such as trying to exit a context when there are no more contexts to revert to. 3. Ensure efficiency in managing context creation, lookup, and resetting. Good Luck!","solution":"class ContextManager: def __init__(self): self._contexts = [] self._current_context = {} def set_context(self): Create a new context and set it as the current context. new_context = {} self._contexts.append(self._current_context) self._current_context = new_context def copy_context(self): Return a shallow copy of the current context. return self._current_context.copy() def enter_context(self, context): Set the provided context as the current context. self._contexts.append(self._current_context) self._current_context = context def exit_context(self): Deactivate the current context and restore the previous one. if self._contexts: self._current_context = self._contexts.pop() else: raise RuntimeError(\\"No context to exit to\\") def set_variable(self, name: str, value): Set the value of a context variable and return a token object. token = (name, self._current_context.get(name, None)) self._current_context[name] = value return token def get_variable(self, name: str, default=None): Return the value of the context variable named `name`. If not found, return `default`. return self._current_context.get(name, default) def reset_variable(self, token): Reset the context variable to the state before the most recent set_variable. name, old_value = token if old_value is None and name in self._current_context: del self._current_context[name] else: self._current_context[name] = old_value # Example usage of ContextManager if __name__ == \\"__main__\\": cm = ContextManager() cm.set_context() cm.set_variable(\'var\', \'value1\') cm.set_context() token = cm.set_variable(\'var\', \'value2\') print(cm.get_variable(\'var\')) # Expected output: \'value2\' cm.reset_variable(token) print(cm.get_variable(\'var\')) # Expected output: None (default context value)"},{"question":"# PyTorch Backend Control: CUDA Configuration In this question, you will demonstrate your understanding of PyTorch backend settings by manipulating configurations related to CUDA (Compute Unified Device Architecture), which PyTorch uses to perform operations on Nvidia GPUs. Objective You need to write a function `configure_cuda_backend` that: 1. Checks if CUDA is available on the system. 2. If CUDA is available: - Retrieves and prints the current settings for: - `allow_tf32` - `allow_fp16_reduced_precision_reduction` - `allow_bf16_reduced_precision_reduction` - Modifies the settings to enable: - TensorFloat-32 (TF32) tensor cores for matrix multiplications (`allow_tf32`). - Reduced precision reductions with FP16 accumulation type (`allow_fp16_reduced_precision_reduction`). - Reduced precision reductions with BF16 accumulation type (`allow_bf16_reduced_precision_reduction`). - Retrieves and prints the new settings to confirm the changes. If CUDA is not available, the function should raise an appropriate exception. Function Signature ```python import torch def configure_cuda_backend() -> None: Configures the CUDA backend settings in PyTorch to enable specific precisions. Raises: RuntimeError: If CUDA is not available on the system. # Your implementation here ``` Constraints and Requirements - You must use the appropriate attributes and functions from the `torch.backends.cuda` module. - You should handle cases where CUDA is not available gracefully by raising a `RuntimeError`. - Ensure that the settings are correctly applied and verified by printing both the initial and final settings for each attribute. Example Output If CUDA is available: ``` CUDA is available. Initial allow_tf32: False Initial allow_fp16_reduced_precision_reduction: False Initial allow_bf16_reduced_precision_reduction: False Enabling specific precisions... Updated allow_tf32: True Updated allow_fp16_reduced_precision_reduction: True Updated allow_bf16_reduced_precision_reduction: True ``` If CUDA is not available: ``` RuntimeError: CUDA is not available on this system. ``` Good luck!","solution":"import torch def configure_cuda_backend() -> None: Configures the CUDA backend settings in PyTorch to enable specific precisions. Raises: RuntimeError: If CUDA is not available on the system. if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available on this system.\\") print(\\"CUDA is available.\\") # Retrieve and print current settings initial_allow_tf32 = torch.backends.cuda.matmul.allow_tf32 initial_allow_fp16_reduced_precision_reduction = torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction initial_allow_bf16_reduced_precision_reduction = torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction print(f\\"Initial allow_tf32: {initial_allow_tf32}\\") print(f\\"Initial allow_fp16_reduced_precision_reduction: {initial_allow_fp16_reduced_precision_reduction}\\") print(f\\"Initial allow_bf16_reduced_precision_reduction: {initial_allow_bf16_reduced_precision_reduction}\\") # Modify and enable specific precision settings torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True print(\\"nEnabling specific precisions...n\\") # Retrieve and print new settings updated_allow_tf32 = torch.backends.cuda.matmul.allow_tf32 updated_allow_fp16_reduced_precision_reduction = torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction updated_allow_bf16_reduced_precision_reduction = torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction print(f\\"Updated allow_tf32: {updated_allow_tf32}\\") print(f\\"Updated allow_fp16_reduced_precision_reduction: {updated_allow_fp16_reduced_precision_reduction}\\") print(f\\"Updated allow_bf16_reduced_precision_reduction: {updated_allow_bf16_reduced_precision_reduction}\\")"},{"question":"Coding Assessment Question # Objective Write a custom implementation of a built-in Python function and enhance its functionality using the `builtins` module. This task will assess your understanding of the `builtins` module, overriding built-in functions, and working with file I/O operations. # Problem Statement Implement a custom `open` function that logs every time a file is opened and its mode (e.g., \'r\' for read, \'w\' for write). This custom `open` function should wrap the built-in `open` function from the `builtins` module. Additionally, create a `FileLogger` class that reads the file content and logs each read operation, including the number of bytes read. # Specifications 1. **Custom Open Function:** - Name: `open` - Parameters: - `path` (str): The file path. - `mode` (str): The mode in which the file is opened (default is `\'r\'`). - Returns: An instance of `FileLogger`. 2. **FileLogger Class:** - Constructor Parameters: - `file`: The file object returned by the built-in `open` function. - Methods: - `read(count=-1)`: - Reads the specified number of bytes from the file. - Logs the read operation and the number of bytes read. - Returns the content read. - `__enter__` and `__exit__` methods to support using the class in a `with` statement. 3. **Logging:** - Print to the console each time a file is opened, indicating the file path and mode. - Print to the console each time a read operation is performed, indicating the number of bytes read. # Example ```python # Implementation Example Usage with open(\'example.txt\', \'r\') as f: content = f.read(5) print(content) # Should print first 5 characters of the \'example.txt\' ``` **Expected Console Output:** ``` File opened: example.txt, mode: r Read operation: 5 bytes ABC # (Assuming \'ABC\' are the first 3 characters in the file and other 2 are part of other contents) ``` # Constraints - Do not use any external libraries. Utilize only the standard library. - Ensure the custom `open` function and the `FileLogger` class handle file operations safely. - Assume the file path provided always exists and is accessible. # Performance - Your solution should handle different file modes (\'r\', \'w\', \'a\', etc.) and read operations efficiently. - The read operation should not excessively log (e.g., avoid logging excessively in a loop). Implement the solution as described and ensure it meets all specified requirements.","solution":"import builtins class FileLogger: def __init__(self, file): self.file = file def read(self, count=-1): data = self.file.read(count) print(f\\"Read operation: {len(data)} bytes\\") return data def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.file.close() def open(path, mode=\'r\'): print(f\\"File opened: {path}, mode: {mode}\\") file = builtins.open(path, mode) return FileLogger(file)"},{"question":"# Custom Python Interactive Shell **Objective**: You are tasked with creating a simple interactive Python shell that uses the `codeop` module. This shell should be capable of receiving multi-line input, determining if the input completes a statement, and executing the code. # Requirements: 1. Implement a function `interactive_shell()` that reads the user\'s input continuously. 2. Use the `codeop` module\'s `compile_command` to determine if the input completes a valid Python statement. 3. If the statement is complete, execute it and print the result. 4. If the statement is incomplete, continue reading from the user. 5. Add support for remembering future statements using the `codeop.CommandCompiler` class. # Constraints: - You must use the `codeop` module as described in the documentation. - Handle exceptions gracefully. Print appropriate error messages for `SyntaxError`, `OverflowError`, and `ValueError`. - The shell should support multi-line statements. - Implement a way to exit the shell using a specific command (e.g., typing `exit()`). # Function Signature: ```python def interactive_shell(): pass ``` # Example Interaction: ``` >>> interactive_shell() >>> >>> x = 1 >>> y = 2 >>> x + y 3 >>> for i in range(3): ... print(i) ... 0 1 2 >>> exit() ``` # Evaluation Criteria: - Correct usage of the `codeop` module. - Ability to handle multi-line statements and remember future imports. - Graceful error handling and user-friendly error messages. - Functional and clear implementation of the interactive shell. You will be evaluated based on the correctness, efficiency, and readability of your code.","solution":"import codeop def interactive_shell(): compiler = codeop.CommandCompiler() buffer = \\"\\" while True: try: # Prompting for input line = input(\\">>> \\" if not buffer else \\"... \\") # Exit command to quit the shell if line.strip() == \\"exit()\\": print(\\"Exiting interactive shell.\\") break # Accumulate the lines in the buffer buffer += line + \\"n\\" # Try to compile the buffer code_object = compiler(buffer) if code_object is not None: # If the code object is not None, the statement is complete exec(code_object) buffer = \\"\\" # Reset the buffer for next command # If code_object is None, continue reading more input except (SyntaxError, OverflowError, ValueError) as ex: print(f\\"Error: {ex}\\") buffer = \\"\\" # Reset the buffer if there is an error except Exception as ex: print(f\\"Exception: {ex}\\") buffer = \\"\\" # Reset the buffer if there is an exception"},{"question":"Objective: Implement a Python class that utilizes the `gettext` module to handle multiple languages and switch between them dynamically based on user input. Problem Statement: You are tasked with designing a `LanguageManager` class that allows a program to switch between different languages for its translations. The class should be capable of: 1. Initializing translations for different languages based on given language codes and a specific domain. 2. Switching between these translations dynamically based on user input. 3. Providing a method to translate messages using the currently active language. Specifications: 1. Implement the `LanguageManager` class: - **`__init__(self, domain: str, localedir: str, languages: list[str])`** - Initializes the `LanguageManager` with the specified domain, locale directory, and a list of language codes to manage. - Example call: `LanguageManager(\\"myapp\\", \\"/path/to/locales\\", [\\"en\\", \\"fr\\", \\"de\\"])` - **`switch_language(self, lang_code: str) -> None`** - Switches the active language to the specified language code. Ensure this language code is in the list of managed languages. - Example call: `language_manager.switch_language(\\"fr\\")` - **`translate(self, message: str) -> str`** - Translates the given message using the currently active language. - Example call: `language_manager.translate(\\"Hello, World!\\")` 2. Make use of `gettext.translation()` to load translation files and `gettext.install()` to use the translations. 3. Implement error handling for invalid language codes. Example: ```python # Sample Usage manager = LanguageManager(\\"myapp\\", \\"/path/to/locales\\", [\\"en\\", \\"fr\\", \\"de\\"]) manager.switch_language(\\"en\\") print(manager.translate(\\"Hello, World!\\")) # Output: \\"Hello, World!\\" (Assuming \\"en\\" is the default language with no translations) manager.switch_language(\\"fr\\") print(manager.translate(\\"Hello, World!\\")) # Output: \\"Bonjour, le monde!\\" (Assuming \\"fr\\" translation exists) manager.switch_language(\\"de\\") print(manager.translate(\\"Hello, World!\\")) # Output: \\"Hallo, Welt!\\" (Assuming \\"de\\" translation exists) ``` Constraints - The `gettext` module must be properly utilized. - The translations are read from .mo files located in `localedir`. - Raise an appropriate error if an invalid language code is provided. Implement the `LanguageManager` class in the space provided: ```python import gettext class LanguageManager: def __init__(self, domain: str, localedir: str, languages: list[str]): self.translations = {} self.current_language = None for lang in languages: self.translations[lang] = gettext.translation(domain, localedir, languages=[lang], fallback=True) def switch_language(self, lang_code: str) -> None: if lang_code not in self.translations: raise ValueError(f\\"Language code {lang_code} is not supported.\\") self.current_language = self.translations[lang_code] self.current_language.install() def translate(self, message: str) -> str: if self.current_language is None: raise RuntimeError(\\"No language has been set.\\") return _(message) ``` Considerations: - Think about the proper handling of translation files and the fallback mechanism for missing translations. - Ensure robustness in switching languages and translating messages. __End of Question__","solution":"import gettext class LanguageManager: def __init__(self, domain: str, localedir: str, languages: list[str]): self.translations = {} self.current_language = None self._domain = domain self._localedir = localedir for lang in languages: self.translations[lang] = gettext.translation(domain, localedir, languages=[lang], fallback=True) def switch_language(self, lang_code: str) -> None: if lang_code not in self.translations: raise ValueError(f\\"Language code {lang_code} is not supported.\\") self.current_language = self.translations[lang_code] self.current_language.install() def translate(self, message: str) -> str: if self.current_language is None: raise RuntimeError(\\"No language has been set.\\") return _(message)"},{"question":"# Coding Assessment: Advanced Enum Usage in Python 3.10 **Objective**: Implement and extend the functionality of enums in Python 3.10. You are required to write code using the `enum` module that demonstrates your understanding of both basic and advanced enum functionalities. Your solution must meet the explicit requirements outlined below. Task: 1. **Create a Bitwise Operation Enum**: - Design an enum class `Permission` using `IntFlag` to represent file permissions: - `READ` with a value allowing the read permission. - `WRITE` with a value allowing the write permission. - `EXECUTE` with a value allowing the execute permission. - Combine these permissions to form `READ_WRITE`, `WRITE_EXECUTE`, and `ALL_PERMISSIONS`. 2. **Ensure Unique Permission Constants**: - Use the `unique` decorator to ensure no duplicate values are allowed in your enum class. 3. **Custom Value Calculation**: - Implement an enum class `Color` that automatically assigns values to its members, but the values should be the members\' names prefixed with their starting letter and an underscore. For example: - `RED` should have the value `\'R_RED\'`. - `BLUE` should have the value `\'B_BLUE\'`. - `GREEN` should have the value `\'G_GREEN\'`. 4. **Enumerating Members**: - Provide a function `list_permissions` that returns a sorted list of all possible combinations of permissions in the `Permission` enum (including single and combined permissions). - Provide a function `describe_color` that returns the name and value for each member of the `Color` enum. Input: - No inputs are required as enums and functions are self-contained. Output: - Your functions (`list_permissions` and `describe_color`) should return the appropriate lists and descriptions. - Ensure your enum classes and decorators are functioning correctly. Constraints: - Use the `enum` module from Python 3.10. - Ensure no duplicate values in `Permission`. - `Color` values must be automatically generated as specified. - Performance efficiency assumed to be optimal as long as it adheres to Python enums\' standard complexity. Example: The implementation might look like: ```python from enum import IntFlag, Enum, unique, auto @unique class Permission(IntFlag): READ = auto() WRITE = auto() EXECUTE = auto() READ_WRITE = READ | WRITE WRITE_EXECUTE = WRITE | EXECUTE ALL_PERMISSIONS = READ | WRITE | EXECUTE class Color(Enum): def _generate_next_value_(name, start, count, last_values): return f\\"{name[0]}_{name}\\" RED = auto() BLUE = auto() GREEN = auto() def list_permissions(): return sorted(Permission, key=lambda p: p.value) def describe_color(): return [(color.name, color.value) for color in Color] # Ensure your implementation is correct by calling your functions and printing the results. if __name__ == \'__main__\': print(list_permissions()) print(describe_color()) ``` Develop your solution adhering to the provided structure and guidelines.","solution":"from enum import IntFlag, Enum, auto, unique @unique class Permission(IntFlag): READ = 1 << 0 # 1 WRITE = 1 << 1 # 2 EXECUTE = 1 << 2 # 4 READ_WRITE = READ | WRITE # 3 WRITE_EXECUTE = WRITE | EXECUTE # 6 ALL_PERMISSIONS = READ | WRITE | EXECUTE # 7 class Color(Enum): def _generate_next_value_(name, start, count, last_values): return f\\"{name[0]}_{name}\\" RED = auto() BLUE = auto() GREEN = auto() def list_permissions(): return sorted([perm for perm in Permission], key=lambda p: p.value) def describe_color(): return [(color.name, color.value) for color in Color]"},{"question":"# Advanced Python Coding Assessment **Objective:** Implement a function to compile and execute a series of Python statements while honoring `__future__` import statements for subsequent compilations. The function should properly handle syntax errors and track the state of future imports. **Instructions:** 1. Implement a class `CustomREPL` that mimics a simplified version of the Python read-eval-print loop (REPL). 2. Your class should compile and execute Python code, keeping track of `__future__` statements. 3. Implement methods within the class to achieve this functionality: - `compile_and_exec(self, source: str) -> None` - Handle errors gracefully, providing clear messages for syntax errors. - Remember `__future__` statements and apply them to all subsequent code compilations. # Specifications: - **Input**: A list of strings, each string representing a line of Python code. - **Output**: Execute each line of code. Print results of expressions and error messages (if any) to standard output. - **Constraints**: - Your solution should properly handle `__future__` import statements and apply them to subsequent code. - Implement error handling for invalid syntax, providing user-friendly messages. # Class Definition: ```python class CustomREPL: def __init__(self): # Initialize your state here, especially to keep track of future statements pass def compile_and_exec(self, source: str) -> None: Compile and execute a string of Python code, keeping track of __future__ statements. Print results of expressions or error messages. :param source: str: A string of valid or invalid Python code pass # Example Usage: repl = CustomREPL() repl.compile_and_exec(\\"from __future__ import division\\") repl.compile_and_exec(\\"print(1 / 2)\\") # Should print 0.5 due to future import statement repl.compile_and_exec(\\"3 * 4\\") # Should print 12 repl.compile_and_exec(\\"invalid code\\") # Should print a friendly error message ``` Your task is to complete the above class definition, ensuring it compiles and executes Python code correctly while handling future statements and errors.","solution":"class CustomREPL: def __init__(self): self.future_flags = \'\' self.scope = {} def compile_and_exec(self, source: str) -> None: try: # Check for \'from __future__\' import statements to track them if source.startswith(\'from __future__ import\'): self.future_flags += f\'{source}n\' # Compile the updated code with future flags compiled_code = compile(self.future_flags + source, \'<string>\', \'exec\') exec(compiled_code, self.scope) except SyntaxError as e: print(f\\"SyntaxError: {e}\\") except Exception as e: print(f\\"Error: {e}\\") # Testing the CustomREPL class repl = CustomREPL() repl.compile_and_exec(\\"from __future__ import division\\") repl.compile_and_exec(\\"print(1 / 2)\\") # Should print 0.5 due to future import statement repl.compile_and_exec(\\"result = 3 * 4\\") repl.compile_and_exec(\\"print(result)\\") # Should print 12 repl.compile_and_exec(\\"invalid code\\") # Should print a friendly error message"},{"question":"Custom Deep Copy Implementation Objective: Implement a custom deep copy function for a complex class structure that incorporates recursive references and mutable nested objects. Problem Statement: You are provided with a `Node` class that represents a node in a graph. Each `Node` contains a list of references to other `Node` instances (its children). Implement a deep copy of the `Node` class that can handle recursive references using the `copy.deepcopy()` function and custom `__deepcopy__` method. ```python import copy class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): # Implement shallow copy behavior new_node = type(self)(self.value) new_node.children = self.children[:] return new_node def __deepcopy__(self, memo): # Implement deep copy behavior if id(self) in memo: return memo[id(self)] new_node = type(self)(self.value) memo[id(self)] = new_node new_node.children = [copy.deepcopy(child, memo) for child in self.children] return new_node ``` Requirements: 1. Implement the shallow copy behavior within the `__copy__()` method of the `Node` class. 2. Implement the deep copy behavior within the `__deepcopy__()` method of the `Node` class using a memo dictionary to handle recursive references. 3. Ensure that deep copies correctly duplicate the entire graph structure without sharing references to mutable objects. 4. The `add_child()` method should add a reference to another `Node` instance to the children list. Example: ```python # Create nodes node1 = Node(1) node2 = Node(2) node3 = Node(3) # Add children node1.add_child(node2) node2.add_child(node3) node3.add_child(node1) # Creates a cycle # Perform deep copy node1_copy = copy.deepcopy(node1) assert node1_copy is not node1 assert node1_copy.children[0] is not node2 assert node1_copy.children[0].children[0] is not node3 assert node1_copy.children[0].children[0].children[0] is node1_copy ``` In this example, the original and copied nodes should not share references to each other, validating the correctness of the deep copy implementation. Constraints: - You may assume that the `Node` values are immutable. - The graph may contain cycles. - The maximum number of nodes will be limited by available memory.","solution":"import copy class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): # Implement shallow copy behavior new_node = type(self)(self.value) new_node.children = self.children[:] return new_node def __deepcopy__(self, memo): # Implement deep copy behavior if id(self) in memo: return memo[id(self)] new_node = type(self)(self.value) memo[id(self)] = new_node new_node.children = [copy.deepcopy(child, memo) for child in self.children] return new_node"},{"question":"Objective The goal of this question is to assess your understanding of Python\'s type hinting system using `GenericAlias` as described in the provided documentation. You are required to create a function that utilizes `GenericAlias` to properly type hint a specific dataset. Problem Statement: Implement a function `create_generic_alias` that creates a `GenericAlias` for a given Python type and a set of arguments. You must follow the behavior described in the provided documentation which includes handling the arguments correctly and setting the `__origin__` and `__args__` attributes. Function Signature: ```python from types import GenericAlias def create_generic_alias(origin: type, *args: type) -> GenericAlias: pass ``` Input: - `origin`: A Python type, such as `list`, `dict`, `set`, etc. - `*args`: A variable number of type arguments that should be used with the `origin` type. Output: - Returns a `GenericAlias` object with the specified origin and arguments. Constraints: - You should not use any external libraries other than the provided `types.GenericAlias`. - If `args` is not provided, treat it as an empty tuple internally. - Ensure minimal argument checking as described in the documentation. Example: ```python # Example usage alias = create_generic_alias(list, int) print(alias.__origin__) # Output: <class \'list\'> print(alias.__args__) # Output: (<class \'int\'>,) ``` Additional Information: - You should handle both scenarios when `args` is a single type and when it is a tuple of types. - The solution should be performant and handle edge cases gracefully. Good luck, and remember to thoroughly test your function with various input types to ensure accuracy and robustness.","solution":"from types import GenericAlias def create_generic_alias(origin: type, *args: type) -> GenericAlias: Create a GenericAlias for the given origin type and arguments. Parameters: origin (type): The original Python type, such as list, dict, set, etc. args (type): A variable number of type arguments that should be used with the origin. Returns: GenericAlias: A GenericAlias object with the specified origin and arguments. # If no args provided, use empty tuple if not args: args = () return GenericAlias(origin, args)"},{"question":"**Coding Assessment Question: XML Data Processing with xml.dom.pulldom** Consider an XML document representing a collection of books, where each book has elements such as title, author, price, and genre. Your task is to write a function that processes this XML data to find and output information about books that belong to a specified genre and have a price below a specified threshold. # Function Signature ```python def filter_books(xml_data: str, genre: str, max_price: float) -> List[str]: Parses the given XML data and returns a list of titles for books that belong to the specified genre and have a price less than the specified maximum price. Parameters: - xml_data (str): A string containing the XML data. - genre (str): The genre to filter books by. - max_price (float): The maximum price of books to include in the result. Returns: - List[str]: A list of titles of books that match the criteria. ``` # Input - `xml_data`: A string containing XML data with the following structure: ```xml <catalog> <book> <title>Book Title 1</title> <author>Author 1</author> <price>19.99</price> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <price>9.99</price> <genre>Non-Fiction</genre> </book> <!-- more book elements --> </catalog> ``` - `genre`: A string representing the genre to filter by (e.g., \\"Fiction\\"). - `max_price`: A float representing the maximum price threshold for books. # Output - A list of strings, where each string is a title of a book that meets the specified genre and price criteria. # Example ```python xml_data = \'\'\' <catalog> <book> <title>Book Title 1</title> <author>Author 1</author> <price>19.99</price> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <price>9.99</price> <genre>Non-Fiction</genre> </book> <book> <title>Book Title 3</title> <author>Author 3</author> <price>14.99</price> <genre>Fiction</genre> </book> </catalog> \'\'\' genre = \'Fiction\' max_price = 15.00 print(filter_books(xml_data, genre, max_price)) # Output: [\'Book Title 3\'] ``` # Constraints - Assume all book elements and their child elements are well-formed. - Prices are valid floating-point numbers (no need to handle invalid formats). - There can be an arbitrary number of book elements in the XML data. # Notes - You should use the `xml.dom.pulldom` module for parsing and processing the XML data. - This question tests your ability to use Python for XML parsing and handling, as well as implementing logic to filter and process data based on specific criteria.","solution":"from typing import List from xml.dom.pulldom import parseString, START_ELEMENT, END_ELEMENT def filter_books(xml_data: str, genre: str, max_price: float) -> List[str]: Parses the given XML data and returns a list of titles for books that belong to the specified genre and have a price less than the specified maximum price. Parameters: - xml_data (str): A string containing the XML data. - genre (str): The genre to filter books by. - max_price (float): The maximum price of books to include in the result. Returns: - List[str]: A list of titles of books that match the criteria. titles = [] document = parseString(xml_data) current_book = None current_genre = None current_price = None inside_book = False for event, node in document: if event == START_ELEMENT and node.tagName == \'book\': current_book = {\'title\': None, \'genre\': None, \'price\': None} inside_book = True elif event == END_ELEMENT and node.tagName == \'book\': if current_book[\'genre\'] == genre and float(current_book[\'price\']) < max_price: titles.append(current_book[\'title\']) current_book = None inside_book = False elif inside_book and event == START_ELEMENT: document.expandNode(node) if node.tagName == \'title\': current_book[\'title\'] = node.firstChild.nodeValue elif node.tagName == \'genre\': current_book[\'genre\'] = node.firstChild.nodeValue elif node.tagName == \'price\': current_book[\'price\'] = node.firstChild.nodeValue return titles"},{"question":"# Question: Complex Plot Creation with Seaborn Objects **Objective:** Demonstrate your understanding of Seaborn\'s object-oriented interface by creating a complex plot with custom scales and aesthetic mappings. **Background:** You are given the `diamonds` and `mpg` datasets from Seaborn. Your task is to create two different plots that showcase various aspects of these datasets while utilizing advanced customization options for scales and aesthetics. # Requirements 1. **Plot 1 (Diamonds Data):** - Use the `diamonds` dataset. - Create a scatter plot of `carat` vs. `price`. - Color the points based on the `clarity` attribute using a continuous color palette. - Apply a logarithmic scale to the `y` (price) axis. - Adjust the point size based on the `carat` attribute with point sizes ranging between 2 and 10. 2. **Plot 2 (MPG Data):** - Use the `mpg` dataset. - Create a scatter plot of `weight` vs. `acceleration`. - Color the points based on the `cylinders` attribute using a nominal color palette (`deep`). - Use different markers for different `origin` values (e.g., \'.\', \'+\', \'*\'). - Apply a square root transformation to the `x` (weight) axis. # Input and Output Formats - **Input:** No inputs are required from the user. You should use the `diamonds` and `mpg` datasets from Seaborn directly. - **Output:** Display both plots. # Constraints and Limitations - Ensure that all scales and aesthetic mappings are correctly applied as specified. - Use the `seaborn.objects.Plot` class for creating the plots. # Example Code Structure ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets diamonds = load_dataset(\\"diamonds\\") mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Plot 1: Diamonds Data p1 = ( so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") .add(so.Dots(), color=\\"clarity\\", pointsize=\\"carat\\") .scale(y=\\"log\\", color=\\"crest\\", pointsize=(2, 10)) ) p1.show() # Plot 2: MPG Data p2 = ( so.Plot(mpg, x=\\"weight\\", y=\\"acceleration\\", color=\\"cylinders\\", marker=\\"origin\\") .add(so.Dot()) .scale(x=so.Continuous(trans=\\"sqrt\\"), color=\\"deep\\", marker={\\"japan\\": \\".\\", \\"europe\\": \\"+\\", \\"usa\\": \\"*\\"}) ) p2.show() ``` **Note:** Replace the example code with your implementation to meet the requirements specified. # Additional Requirements: - Ensure the plots are well-labeled and aesthetically pleasing. - You may include gridlines, titles, and other plot enhancements as necessary.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets diamonds = load_dataset(\\"diamonds\\") mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Plot 1: Diamonds Data p1 = ( so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") .add(so.Dots(), color=\\"clarity\\", pointsize=\\"carat\\") .scale(y=\\"log\\", color=\\"viridis\\", pointsize=(2, 10)) .label(x=\\"Carat\\", y=\\"Price\\", color=\\"Clarity\\", title=\\"Diamonds: Carat vs. Price\\") ) p1.show() # Plot 2: MPG Data p2 = ( so.Plot(mpg, x=\\"weight\\", y=\\"acceleration\\", color=\\"cylinders\\", marker=\\"origin\\") .add(so.Dots()) .scale(x=so.Continuous(trans=\\"sqrt\\"), color=\\"deep\\", marker={\\"japan\\": \\".\\", \\"europe\\": \\"+\\", \\"usa\\": \\"*\\"}) .label(x=\\"Weight\\", y=\\"Acceleration\\", color=\\"Cylinders\\", marker=\\"Origin\\", title=\\"MPG: Weight vs. Acceleration\\") ) p2.show()"},{"question":"Objective: To assess your understanding of Python\'s `linecache` module and your ability to work with file operations and caching mechanisms. Problem Statement: You are tasked with writing a function that extracts specific lines of a given Python source file based on provided line numbers and processes these lines in a defined way. Function Signature: ```python from typing import List def process_file_lines(file_path: str, line_numbers: List[int]) -> List[str]: Extracts and processes specified lines from a given file. Parameters: file_path (str): The path to the Python source file. line_numbers (List[int]): A list of line numbers to be extracted and processed. Returns: List[str]: A list containing the processed content of the specified lines. pass ``` Requirements: 1. Use the `linecache.getline()` function to fetch the required lines from the file. 2. For each extracted line, strip leading and trailing whitespace and convert the line to uppercase. 3. Return a list of the processed lines, maintaining the order of the provided line numbers. 4. Ensure your function handles errors gracefully by: - Returning \\"LINE NOT FOUND\\" for any line numbers that are out of range or if the file doesn\'t exist. 5. Efficiently manage the cache using `linecache.clearcache()` after processing the lines to ensure memory is freed. Constraints: - The `file_path` will always be a valid string. - The `line_numbers` list will contain unique integers greater than 0. - Do not use any external libraries except for the standard library. Example Usage: ```python # Assume \'example.py\' has the following content: # 1: print(\\"Hello, World!\\") # 2: a = 5 # 3: b = 10 # 4: print(a + b) file_path = \'example.py\' line_numbers = [1, 4, 5] print(process_file_lines(file_path, line_numbers)) ``` Expected Output: ```python [\'PRINT(\\"HELLO, WORLD!\\")\', \'PRINT(A + B)\', \'LINE NOT FOUND\'] ``` Performance Considerations: Your solution should efficiently handle file reading and caching, and should not have significant performance issues with large files.","solution":"import linecache from typing import List def process_file_lines(file_path: str, line_numbers: List[int]) -> List[str]: Extracts and processes specified lines from a given file. Parameters: file_path (str): The path to the Python source file. line_numbers (List[int]): A list of line numbers to be extracted and processed. Returns: List[str]: A list containing the processed content of the specified lines. processed_lines = [] for line_number in line_numbers: line = linecache.getline(file_path, line_number) if line: processed_lines.append(line.strip().upper()) else: processed_lines.append(\\"LINE NOT FOUND\\") linecache.clearcache() return processed_lines"},{"question":"# Advanced Python Programming: Using the Trace Module **Objective**: Implement a function that traces a given function\'s execution, logs the number of times each line is executed, and writes the results to a specified directory. # Function Requirements: 1. **Function Signature**: ```python def trace_function_execution(func: callable, args: tuple, kwargs: dict, ignore_dirs: list, output_dir: str) -> None ``` 2. **Parameters**: - `func`: A callable function that you need to trace. - `args`: A tuple of positional arguments to pass to the function. - `kwargs`: A dictionary of keyword arguments to pass to the function. - `ignore_dirs`: A list of directory paths whose contents should be ignored during tracing. - `output_dir`: The directory where the annotated listings will be saved. 3. **Behavior**: - Create a `trace.Trace` object configured to count the number of times each line is executed, using the provided `ignore_dirs`. - Execute the `func` with the given `args` and `kwargs` under this tracing setup. - Retrieve the `CoverageResults` from the traced execution. - Write the results to the `output_dir`, ensuring that lines with no hits are marked, and a summary of coverage is included. 4. **Constraints**: - You are to assume that `output_dir` exists and is writable. - The execution of `func` should not produce any outputs or side effects outside those relevant to this tracking. # Example Usage: ```python def example_function(x, y): result = x + y for i in range(result): if i % 2 == 0: print(f\\"{i} is even\\") else: print(f\\"{i} is odd\\") return result trace_function_execution(example_function, (10, 5), {}, [\'path/to/ignore\'], \'path/to/output_dir\') ``` # Expected Behavior: - The execution of `example_function` should be traced. - The number of times each line is executed should be recorded. - The results should be written to the directory specified by `output_dir`, with an annotated listing for each relevant source file.","solution":"import os from trace import Trace def trace_function_execution(func: callable, args: tuple, kwargs: dict, ignore_dirs: list, output_dir: str) -> None: Traces the execution of a given function, logs the number of times each line is executed, and writes the results to a specified directory. Parameters: - func: A callable function to trace. - args: A tuple of positional arguments to pass to the function. - kwargs: A dictionary of keyword arguments to pass to the function. - ignore_dirs: A list of directory paths whose contents should be ignored during tracing. - output_dir: The directory where the annotated listings will be saved. # Initialize the Trace object for tracing function execution tracer = Trace(count=True, trace=False, ignoremods=ignore_dirs) # Run the function with tracing enabled tracer.runfunc(func, *args, **kwargs) # Retrieve the results results = tracer.results() # Write the results to the output directory results.write_results(show_missing=True, summary=True, coverdir=output_dir)"},{"question":"# Complex Number Arithmetic in Python Objective Implement a class `ComplexNumber` that simulates complex number operations in Python. You should define methods to handle addition, subtraction, multiplication, and division of complex numbers. Task 1. **Class Definition**: Define a class `ComplexNumber`. 2. **Initialization**: Allow instantiation with two parameters: `real` and `imag` representing the real and imaginary parts, respectively. 3. **String Representation**: Implement the `__str__` method to return a string representation of the complex number in the form `a + bi`. 4. **Addition**: Implement the `__add__` method to add another complex number. 5. **Subtraction**: Implement the `__sub__` method to subtract another complex number. 6. **Multiplication**: Implement the `__mul__` method to multiply by another complex number. 7. **Division**: Implement the `__truediv__` method to divide by another complex number. Handle division by zero appropriately by raising a `ZeroDivisionError`. Input and Output Format - Each complex number operation will be invoked as a method of a `ComplexNumber` instance. - Addition, subtraction, multiplication, and division should return a new `ComplexNumber` instance representing the result. - The string representation (`__str__` method) should output the complex number in the form `a + bi`, where `a` is the real part and `b` is the imaginary part. Example ```python class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag def __str__(self): return f\\"{self.real} + {self.imag}i\\" def __add__(self, other): return ComplexNumber(self.real + other.real, self.imag + other.imag) def __sub__(self, other): return ComplexNumber(self.real - other.real, self.imag - other.imag) def __mul__(self, other): real_part = self.real * other.real - self.imag * other.imag imag_part = self.real * other.imag + self.imag * other.real return ComplexNumber(real_part, imag_part) def __truediv__(self, other): if other.real == 0 and other.imag == 0: raise ZeroDivisionError(\\"division by zero\\") denom = other.real ** 2 + other.imag ** 2 real_part = (self.real * other.real + self.imag * other.imag) / denom imag_part = (self.imag * other.real - self.real * other.imag) / denom return ComplexNumber(real_part, imag_part) # Example usage num1 = ComplexNumber(1, 2) num2 = ComplexNumber(3, 4) print(num1 + num2) # Output: 4 + 6i print(num1 - num2) # Output: -2 + -2i print(num1 * num2) # Output: -5 + 10i print(num1 / num2) # Output: 0.44 + 0.08i ``` Constraints - Handle division by zero gracefully by raising a `ZeroDivisionError`. - Ensure the `__str__` method of complex numbers is properly formatted. - You can assume that all inputs to methods are valid `ComplexNumber` instances.","solution":"class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag def __str__(self): return f\\"{self.real} + {self.imag}i\\" def __add__(self, other): return ComplexNumber(self.real + other.real, self.imag + other.imag) def __sub__(self, other): return ComplexNumber(self.real - other.real, self.imag - other.imag) def __mul__(self, other): real_part = self.real * other.real - self.imag * other.imag imag_part = self.real * other.imag + self.imag * other.real return ComplexNumber(real_part, imag_part) def __truediv__(self, other): if other.real == 0 and other.imag == 0: raise ZeroDivisionError(\\"division by zero\\") denom = other.real ** 2 + other.imag ** 2 real_part = (self.real * other.real + self.imag * other.imag) / denom imag_part = (self.imag * other.real - self.real * other.imag) / denom return ComplexNumber(real_part, imag_part)"},{"question":"# Question: AST Transformation and Code Generation The `ast` (Abstract Syntax Trees) module in Python allows for the parsing, manipulation, and generation of Python code at the syntax tree level. This exercise will test your understanding of the `ast` module. Problem Statement You are given a Python function\'s source code as a string input. Write a Python function that performs the following transformations on the AST of the given function and returns the modified source code as a string: 1. **Replace all addition operations** (`+`) with multiplication operations (`*`). 2. **Remove all docstrings** from the function and its nested elements (if any). 3. **Increment all line numbers** in the function by 2. Input and Output - **Input**: A string `source_code` representing a Python function. - **Output**: A string representing the transformed and re-generated Python function source code. Constraints - The input string will be a syntactically correct Python function. - The function and any nested statements or expressions can contain docstrings. Example ```python # Input source_code = \'\'\' def add(a, b): This function adds two numbers. result = a + b return result \'\'\' # Expected Output \'\'\' def add(a, b): result = a * b return result \'\'\' Implementation Steps 1. Parse the input source code to an AST. 2. Define a `NodeTransformer` subclass to perform the specified transformations. - Replace `+` operators with `*`. - Remove docstrings from the function and nested statements. 3. Use `ast.increment_lineno` to adjust the line numbers. 4. Convert the modified AST back to source code using `ast.unparse`. Function Signature ```python def transform_code(source_code: str) -> str: # Your implementation here ``` Notes - You are allowed to use the functions and classes from the `ast` module as necessary. - Make sure to handle different kinds of AST nodes correctly for each transformation.","solution":"import ast class Transformer(ast.NodeTransformer): def visit_BinOp(self, node): self.generic_visit(node) if isinstance(node.op, ast.Add): node.op = ast.Mult() return node def visit_FunctionDef(self, node): # Remove docstring if present if isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Str): node.body.pop(0) # Increment line numbers ast.increment_lineno(node, 2) # Visit the body of the function to capture other nodes node.body = [self.visit(n) for n in node.body] return node def visit_Expr(self, node): # Remove docstrings in nested blocks if isinstance(node.value, ast.Str): return None self.generic_visit(node) return node def transform_code(source_code: str) -> str: # Parse the source code into an AST tree = ast.parse(source_code) # Transform the AST transformer = Transformer() tree = transformer.visit(tree) # Convert the AST back to source code modified_code = ast.unparse(tree) return modified_code"},{"question":"You are tasked with developing a library of functions that perform specific linear algebra operations using the `torch.linalg` module in PyTorch. This assessment will evaluate your understanding of matrix decompositions, solving linear systems, and matrix inverses. # Objectives Implement the following functions: 1. **Decompose a matrix using LU decomposition:** ```python def lu_decomposition(matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Computes the LU decomposition of a given square matrix. Parameters: - matrix (torch.Tensor): Input square matrix of shape (n, n). Returns: - P (torch.Tensor): The permutation matrix of shape (n, n). - L (torch.Tensor): The lower triangular matrix of shape (n, n) with unit diagonal. - U (torch.Tensor): The upper triangular matrix of shape (n, n). ``` 2. **Solve a system of linear equations using LU decomposition:** ```python def solve_linear_system_via_lu(matrix: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Solves the system of linear equations Ax = b using LU decomposition. Parameters: - matrix (torch.Tensor): Coefficient matrix A of shape (n, n). - b (torch.Tensor): Right-hand side vector of shape (n,) or matrix of shape (n, m). Returns: - x (torch.Tensor): Solution vector of shape (n,) or matrix of shape (n, m). ``` 3. **Compute the inverse of a matrix using LU decomposition:** ```python def inverse_via_lu(matrix: torch.Tensor) -> torch.Tensor: Computes the inverse of a given square matrix using LU decomposition. Parameters: - matrix (torch.Tensor): Input square matrix of shape (n, n). Returns: - inv_matrix (torch.Tensor): Inverse of the input matrix of shape (n, n). ``` # Constraints - You can assume that the input matrices are always square and of appropriate dimensions. - You are required to use the `torch.linalg` module for implementing these functions. - Handle potential edge cases gracefully, such as when the matrix is singular or nearly singular. # Example ```python import torch A = torch.tensor([[3.0, 1.0], [1.0, 2.0]]) b = torch.tensor([9.0, 8.0]) # LU Decomposition P, L, U = lu_decomposition(A) print(\\"P:\\", P) print(\\"L:\\", L) print(\\"U:\\", U) # Solving linear system x = solve_linear_system_via_lu(A, b) print(\\"Solution x:\\", x) # Inverse of the matrix A_inv = inverse_via_lu(A) print(\\"A_inv:\\", A_inv) ``` In the given example, verify the correctness of your implementation by checking the values of `P`, `L`, `U`, `x`, and `A_inv`.","solution":"import torch from typing import Tuple def lu_decomposition(matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Computes the LU decomposition of a given square matrix. Parameters: - matrix (torch.Tensor): Input square matrix of shape (n, n). Returns: - P (torch.Tensor): The permutation matrix of shape (n, n). - L (torch.Tensor): The lower triangular matrix of shape (n, n) with unit diagonal. - U (torch.Tensor): The upper triangular matrix of shape (n, n). P, L, U = torch.linalg.lu(matrix) return P, L, U def solve_linear_system_via_lu(matrix: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Solves the system of linear equations Ax = b using LU decomposition. Parameters: - matrix (torch.Tensor): Coefficient matrix A of shape (n, n). - b (torch.Tensor): Right-hand side vector of shape (n,) or matrix of shape (n, m). Returns: - x (torch.Tensor): Solution vector of shape (n,) or matrix of shape (n, m). P, L, U = torch.linalg.lu(matrix) y = torch.linalg.solve(L, torch.matmul(P, b)) x = torch.linalg.solve(U, y) return x def inverse_via_lu(matrix: torch.Tensor) -> torch.Tensor: Computes the inverse of a given square matrix using LU decomposition. Parameters: - matrix (torch.Tensor): Input square matrix of shape (n, n). Returns: - inv_matrix (torch.Tensor): Inverse of the input matrix of shape (n, n). P, L, U = torch.linalg.lu(matrix) L_inv = torch.inverse(L) U_inv = torch.inverse(U) inv_matrix = torch.matmul(U_inv, torch.matmul(L_inv, P)) return inv_matrix"},{"question":"# Advanced Coding Assessment Question: Network Subnet Analysis and Manipulation **Problem Statement:** You are given a range of IP addresses that belong to an organization\'s network. The organization is planning to restructure its network architecture for better efficiency and security. As part of this task, you are required to write a function that performs the following tasks: 1. **Summarize the range of given IP addresses into a list of network objects.** 2. **Identify and return all subnets within a specified supernet that belong to this summarized range.** 3. **Generate a list of available smaller subnets within a given network that are not covered by the provided IP addresses.** The function signature should be: ```python from typing import List, Tuple, Union import ipaddress def network_analysis(ip_range: Tuple[str, str], supernet: str, network: str, prefix_len: int) -> Tuple[List[str], List[str], List[str]]: Args: ip_range (Tuple[str, str]): A tuple containing the start and end of the IP address range (inclusive). supernet (str): A string representing the supernet (e.g., \'192.168.0.0/16\'). network (str): A string representing the network (e.g., \'192.168.1.0/24\'). prefix_len (int): An integer representing the desired prefix length for subnetting (e.g., 26). Returns: Tuple[List[str], List[str], List[str]]: - List of summarized network ranges within the given IP range. - List of subnets within the specified supernet that overlap with these summarized ranges. - List of available subnets within the given network that are not covered by the provided IP addresses. pass ``` # Example: ```python ip_range = (\'192.168.1.0\', \'192.168.1.130\') supernet = \'192.168.0.0/16\' network = \'192.168.1.0/24\' prefix_len = 26 summarized_networks, overlapping_subnets, available_subnets = network_analysis(ip_range, supernet, network, prefix_len) print(\\"Summarized Networks:\\", summarized_networks) print(\\"Overlapping Subnets in Supernet:\\", overlapping_subnets) print(\\"Available Subnets in Network:\\", available_subnets) ``` This should output: ``` Summarized Networks: [\'192.168.1.0/25\', \'192.168.1.128/31\', \'192.168.1.130/32\'] Overlapping Subnets in Supernet: [\'192.168.0.0/16\'] Available Subnets in Network: [\'192.168.1.192/26\'] ``` # Constraints: - The given IP addresses are guaranteed to be valid and follow the correct IPv4 format. - The `prefix_len` provided for subnetting will be greater than or equal to the mask length of the `network`. # Performance Requirements: - The function should handle typical network ranges efficiently. - Ensure that the summarization and subnetting operations are performed in optimal time and space complexity. **Hints:** 1. Use `ipaddress.summarize_address_range(first, last)` to summarize the IP range. 2. Leverage `subnets()` and `overlaps()` methods for finding and comparing subnets within the supernet. 3. Use `address_exclude(network)` to determine available smaller subnets not covered by the IP addresses. **Note:** You may import the necessary classes and functions from the `ipaddress` module for implementing this function.","solution":"from typing import List, Tuple import ipaddress def network_analysis(ip_range: Tuple[str, str], supernet: str, network: str, prefix_len: int) -> Tuple[List[str], List[str], List[str]]: start_ip, end_ip = ip_range start_ip = ipaddress.ip_address(start_ip) end_ip = ipaddress.ip_address(end_ip) supernet = ipaddress.ip_network(supernet) network = ipaddress.ip_network(network) # Summarize the IP range summarized_networks = [str(net) for net in ipaddress.summarize_address_range(start_ip, end_ip)] # Find all subnets within the supernet that overlap with the summarized ranges overlapping_subnets = [] for summarized_net in summarized_networks: summarized_net_obj = ipaddress.ip_network(summarized_net) if summarized_net_obj.overlaps(supernet): overlapping_subnets.append(str(supernet)) break # Generate list of available smaller subnets within the given network available_subnets = [] subnetted_networks = list(network.subnets(new_prefix=prefix_len)) covered_networks = set() for summarized_net in summarized_networks: summarized_net_obj = ipaddress.ip_network(summarized_net) for subnet in subnetted_networks: if subnet.overlaps(summarized_net_obj): covered_networks.add(subnet) for subnet in subnetted_networks: if subnet not in covered_networks: available_subnets.append(str(subnet)) return summarized_networks, overlapping_subnets, available_subnets"},{"question":"**Objective:** Design a Python script that connects to an IMAP server, searches for all emails in the inbox that are unread, fetches their subject lines, and marks them as read. The script should log in to an IMAP server with provided credentials and handle potential exceptions properly. **Requirements:** 1. The script should use the `imaplib` library for all IMAP operations. 2. The script should prompt the user for their email address and password. 3. Connect to the server using the `IMAP4_SSL` class over a secure SSL connection. 4. Search for all unread emails in the user\'s inbox. 5. Fetch the subject of each unread email. 6. Mark each email as read. 7. Print the subject lines of the unread emails fetched. 8. Handle any potential exceptions that might occur during IMAP operations. **Input Format:** - The script should prompt for: - IMAP server address (e.g., `imap.gmail.com` for Gmail) - Email address (user) - Password (password) **Output Format:** - Print the subject lines of all unread emails in the inbox. **Constraints:** - The script should properly handle potential IMAP exceptions like `IMAP4.abort`, `IMAP4.readonly`, and `IMAP4.error`. - Do not use any external libraries apart from `imaplib`. **Example Usage:** ``` IMAP Server: imap.gmail.com Email: user@example.com Password: [hidden] Connecting to the IMAP server... Searching for unread emails... Unread Emails\' Subjects: - Subject 1 - Subject 2 - Subject 3 Marking emails as read... Operation completed successfully. ``` **Solution Skeleton:** ```python import imaplib import getpass def fetch_unread_emails(imap_server, email, password): try: # Connect to the server mail = imaplib.IMAP4_SSL(imap_server) mail.login(email, password) # Select the mailbox (default is \\"INBOX\\") mail.select(\\"inbox\\") # Search for unread emails result, data = mail.search(None, \'UNSEEN\') if result != \'OK\': print(\\"No unread emails found!\\") return # Fetch the subject of each unread email email_ids = data[0].split() for email_id in email_ids: result, data = mail.fetch(email_id, \'(BODY.PEEK[HEADER.FIELDS (SUBJECT)])\') if result == \'OK\': header_data = data[0][1].decode() print(\\"Subject:\\", header_data.strip()) # Mark the email as read mail.store(email_id, \'+FLAGS\', \'Seen\') mail.logout() except imaplib.IMAP4.abort as e: print(\\"IMAP4 abort error:\\", e) except imaplib.IMAP4.readonly as e: print(\\"IMAP4 readonly error:\\", e) except imaplib.IMAP4.error as e: print(\\"IMAP4 error:\\", e) except Exception as e: print(\\"General error:\\", e) if __name__ == \\"__main__\\": imap_server = input(\\"IMAP Server: \\") email = input(\\"Email: \\") password = getpass.getpass(\\"Password: \\") fetch_unread_emails(imap_server, email, password) ``` **Guidelines:** - Ensure the credentials and server inputs are correct. - The script needs robust exception handling to respond gracefully to connection or authentication failures. - Test the script with a real IMAP server to ensure accuracy and reliability.","solution":"import imaplib import getpass def fetch_unread_emails(imap_server, email, password): try: # Connect to the server mail = imaplib.IMAP4_SSL(imap_server) mail.login(email, password) # Select the mailbox (default is \\"INBOX\\") mail.select(\\"inbox\\") # Search for unread emails result, data = mail.search(None, \'UNSEEN\') if result != \'OK\': print(\\"No unread emails found!\\") return # Fetch the subject of each unread email email_ids = data[0].split() if not email_ids: print(\\"No unread emails found!\\") return print(\\"Unread Emails\' Subjects:\\") for email_id in email_ids: result, data = mail.fetch(email_id, \'(BODY.PEEK[HEADER.FIELDS (SUBJECT)])\') if result == \'OK\': header_data = data[0][1].decode() subject = header_data.replace(\\"Subject: \\", \\"\\").strip() print(\\"-\\", subject) # Mark the email as read mail.store(email_id, \'+FLAGS\', \'Seen\') mail.logout() print(\\"Operation completed successfully.\\") except imaplib.IMAP4.abort as e: print(\\"IMAP4 abort error:\\", e) except imaplib.IMAP4.readonly as e: print(\\"IMAP4 readonly error:\\", e) except imaplib.IMAP4.error as e: print(\\"IMAP4 error:\\", e) except Exception as e: print(\\"General error:\\", e) if __name__ == \\"__main__\\": imap_server = input(\\"IMAP Server: \\") email = input(\\"Email: \\") password = getpass.getpass(\\"Password: \\") fetch_unread_emails(imap_server, email, password)"},{"question":"# Pandas Coding Assessment **Problem Description:** You are given a CSV file named `data.csv` that contains customer data with the following columns: | customer_id | age | gender | purchase_amount | purchase_date | |-------------|-----|--------|-----------------|-----------------| | 1 | 25 | F | 100 | 2023-01-15 | | 2 | 45 | M | 250 | 2023-01-16 | | 3 | 30 | F | 125 | 2023-01-17 | | ... | ... | ... | ... | ... | Your task is to implement a function `analyze_customer_data(file_path: str) -> pd.DataFrame` that reads this CSV file and returns a summary DataFrame with the following computations: 1. The total number of unique customers. 2. The average purchase amount. 3. The total purchase amount. 4. The average age of customers. 5. The number of male and female customers. 6. The total purchase amount per month extracted from `purchase_date`. The resulting DataFrame should have the following structure: | Metric | Value | |---------------------------------|---------------------| | Total Unique Customers | int | | Average Purchase Amount | float | | Total Purchase Amount | float | | Average Age of Customers | float | | Number of Male Customers | int | | Number of Female Customers | int | | Total Purchase Amount in Jan | float | | Total Purchase Amount in Feb | float | | ... | ... | | Total Purchase Amount in Dec | float | **Input Format:** - `file_path (str)`: A string representing the path to `data.csv`. **Output Format:** - Returns a pandas DataFrame containing the summary metrics as described. **Constraints:** - The `purchase_date` column is guaranteed to be in the format YYYY-MM-DD. - The gender column contains either \'M\' or \'F\' only. - You may assume the file will always be properly formatted and available during the function call. **Example:** Suppose the content of `data.csv` is: ``` customer_id,age,gender,purchase_amount,purchase_date 1,25,F,100,2023-01-15 2,45,M,250,2023-01-16 3,30,F,125,2023-01-17 4,35,M,300,2023-02-14 5,40,F,200,2023-02-15 6,28,M,150,2023-03-10 ``` ```python import pandas as pd def analyze_customer_data(file_path: str) -> pd.DataFrame: # Your implementation ``` Calling `analyze_customer_data(file_path)` where `file_path` is the path to the above CSV should return: ``` | Metric | Value | |---------------------------------|----------| | Total Unique Customers | 6 | | Average Purchase Amount | 187.5 | | Total Purchase Amount | 1125.0 | | Average Age of Customers | 33.83 | | Number of Male Customers | 3 | | Number of Female Customers | 3 | | Total Purchase Amount in Jan | 475.0 | | Total Purchase Amount in Feb | 500.0 | | Total Purchase Amount in Mar | 150.0 | ``` **Notes:** - The empty values (like months with no purchases) should be represented by 0 or not included in the corresponding output DataFrame. **Bonus:** - Provide a function that also plots a bar chart showing the total purchase amount per month. ```python import matplotlib.pyplot as plt def plot_monthly_purchases(file_path: str): # Your implementation for plotting ```","solution":"import pandas as pd def analyze_customer_data(file_path: str) -> pd.DataFrame: df = pd.read_csv(file_path) # 1. Total number of unique customers total_unique_customers = df[\'customer_id\'].nunique() # 2. Average purchase amount average_purchase_amount = df[\'purchase_amount\'].mean() # 3. Total purchase amount total_purchase_amount = df[\'purchase_amount\'].sum() # 4. Average age of customers average_age = df[\'age\'].mean() # 5. Number of male and female customers num_male_customers = df[df[\'gender\'] == \'M\'].shape[0] num_female_customers = df[df[\'gender\'] == \'F\'].shape[0] # 6. Total purchase amount per month df[\'purchase_date\'] = pd.to_datetime(df[\'purchase_date\']) df[\'month\'] = df[\'purchase_date\'].dt.strftime(\'%b\') monthly_purchase_amount = df.groupby(\'month\')[\'purchase_amount\'].sum().reindex([\'Jan\', \'Feb\', \'Mar\', \'Apr\', \'May\', \'Jun\', \'Jul\', \'Aug\', \'Sep\', \'Oct\', \'Nov\', \'Dec\'], fill_value=0) # Constructing the summary dataframe summary = { \\"Metric\\": [ \\"Total Unique Customers\\", \\"Average Purchase Amount\\", \\"Total Purchase Amount\\", \\"Average Age of Customers\\", \\"Number of Male Customers\\", \\"Number of Female Customers\\" ], \\"Value\\": [ total_unique_customers, average_purchase_amount, total_purchase_amount, average_age, num_male_customers, num_female_customers ] } for month, amount in monthly_purchase_amount.items(): summary[\\"Metric\\"].append(f\\"Total Purchase Amount in {month}\\") summary[\\"Value\\"].append(amount) return pd.DataFrame(summary)"},{"question":"Objective: Demonstrate your comprehension of seaborn\'s styling and customization capabilities by analyzing and visualizing a dataset with different plot styles. Problem Statement: You are provided with a dataset containing information about students\' scores in three subjects: Mathematics, Reading, and Writing. You need to perform the following tasks: 1. Import the dataset into a DataFrame. 2. Create three different styles of plots using seaborn to visualize the average scores in each subject. Input: - A CSV file named `students_scores.csv` with the following columns: - `StudentID`: Unique identifier for each student. - `Gender`: The gender of the student. - `Mathematics`: The score in Mathematics. - `Reading`: The score in Reading. - `Writing`: The score in Writing. Output: Generate and display three plots: 1. A bar plot showing the average scores in Mathematics for male and female students with the `whitegrid` style. 2. A line plot showing the trend of average Reading scores over the student identifiers with a customized `darkgrid` style. 3. Any additional type of plot (your choice) showing Writing scores with the default seaborn style and an overridden gridline color and style. Constraints: - Use pandas to read the CSV file. - Ensure the plots are clearly labeled and have appropriate titles and legends. Performance Requirements: - The script should run efficiently and generate the plots in a reasonable time frame. Example: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Task 1: Read the dataset df = pd.read_csv(\'students_scores.csv\') # Task 2: Create a bar plot with whitegrid style sns.set_style(\\"whitegrid\\") sns.barplot(x=\'Gender\', y=\'Mathematics\', data=df, ci=None) plt.title(\\"Average Mathematics Scores by Gender\\") plt.show() # Task 3: Create a line plot with customized darkgrid style sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}) average_reading_scores = df.groupby(\'StudentID\')[\'Reading\'].mean().reset_index() sns.lineplot(x=\'StudentID\', y=\'Reading\', data=average_reading_scores) plt.title(\\"Trend of Average Reading Scores\\") plt.show() # Task 4: Create an additional plot of your choice with customized gridline sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\"red\\", \\"grid.linestyle\\": \\"--\\"}) sns.boxplot(x=\'Gender\', y=\'Writing\', data=df) plt.title(\\"Writing Scores Distribution by Gender\\") plt.show() ``` Explanation: 1. The dataset is read into a pandas DataFrame. 2. The `sns.set_style(\\"whitegrid\\")` function sets the style to \'whitegrid\', and we create a bar plot showing average Mathematics scores by gender. 3. A line plot shows the trend of average Reading scores with a customized darkgrid style (`grid.color` set to `.6` and `grid.linestyle` set to `:`). 4. An additional plot (boxplot in this case) visualizes Writing scores with another layer of grid customization (`grid.color` set to `red` and `grid.linestyle` set to `--`).","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_dataset(file_path): Load the dataset from a CSV file. Parameters: file_path (str): The path to the CSV file. Returns: pd.DataFrame: Loaded dataset as a DataFrame. return pd.read_csv(file_path) def plot_math_scores_by_gender(df): Create a bar plot showing the average Mathematics scores by gender with the \'whitegrid\' style. Parameters: df (pd.DataFrame): The students scores DataFrame. sns.set_style(\\"whitegrid\\") plt.figure(figsize=(8, 6)) sns.barplot(x=\'Gender\', y=\'Mathematics\', data=df, ci=None) plt.title(\\"Average Mathematics Scores by Gender\\") plt.xlabel(\\"Gender\\") plt.ylabel(\\"Average Mathematics Score\\") plt.show() def plot_reading_scores_trend(df): Create a line plot showing the trend of average Reading scores over StudentID with a customized \'darkgrid\' style. Parameters: df (pd.DataFrame): The students scores DataFrame. sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}) average_reading_scores = df.groupby(\'StudentID\')[\'Reading\'].mean().reset_index() plt.figure(figsize=(10, 6)) sns.lineplot(x=\'StudentID\', y=\'Reading\', data=average_reading_scores) plt.title(\\"Trend of Average Reading Scores\\") plt.xlabel(\\"Student ID\\") plt.ylabel(\\"Average Reading Score\\") plt.show() def plot_writing_scores_distribution(df): Create an additional plot (boxplot) showing Writing scores with the default seaborn style with overridden gridlines. Parameters: df (pd.DataFrame): The students scores DataFrame. sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\"red\\", \\"grid.linestyle\\": \\"--\\"}) plt.figure(figsize=(8, 6)) sns.boxplot(x=\'Gender\', y=\'Writing\', data=df) plt.title(\\"Writing Scores Distribution by Gender\\") plt.xlabel(\\"Gender\\") plt.ylabel(\\"Writing Scores\\") plt.show()"},{"question":"Objective Implement several clustering algorithms using scikit-learn and compare their performance on a given dataset. Problem Statement You are provided with a dataset of 2D points. Your task is to implement the following clustering algorithms using scikit-learn: 1. K-Means 2. DBSCAN 3. Agglomerative Clustering 4. Mean-Shift After clustering the dataset with each algorithm, compute the silhouette score for each clustering result. Based on the silhouette scores, determine which algorithm performed the best on the given dataset. Input - A CSV file named `data.csv` with two columns `x` and `y` representing coordinates of points in a 2D space. Output - Four silhouette scores (one for each clustering algorithm). - An indication of which clustering algorithm performed the best based on the silhouette score. Constraints - Use default parameters for each algorithm unless specified. - For K-Means, set the number of clusters to 3. - For DBSCAN, use `eps=0.5` and `min_samples=5`. - For Agglomerative Clustering, set the number of clusters to 3 and use the \\"ward\\" linkage method. - Use the default bandwidth for Mean-Shift. Function Signature ```python import pandas as pd from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift from sklearn.metrics import silhouette_score def cluster_and_evaluate(data_path: str) -> None: # Your code here # Example usage cluster_and_evaluate(\'data.csv\') ``` Example Assume `data.csv` contains the following data: ``` x,y 1.0,2.0 1.5,1.8 5.0,8.0 8.0,8.0 1.2,0.8 9.0,11.0 8.0,2.0 10.0,2.0 9.0,3.0 ``` - Expected output (the values may vary): ``` Silhouette Scores: K-Means: 0.55 DBSCAN: 0.48 Agglomerative: 0.57 Mean-Shift: 0.60 Best algorithm: Mean-Shift ``` Notes - Ensure that you handle data loading correctly and catch any exceptions that might occur during clustering or evaluation. - Provide clear and concise comments in your code to explain the logic and steps.","solution":"import pandas as pd from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift from sklearn.metrics import silhouette_score def cluster_and_evaluate(data_path: str) -> None: # Load the data data = pd.read_csv(data_path) X = data[[\'x\', \'y\']].values # Define clustering algorithms with specified parameters kmeans = KMeans(n_clusters=3, random_state=42) dbscan = DBSCAN(eps=0.5, min_samples=5) agglomerative = AgglomerativeClustering(n_clusters=3, linkage=\'ward\') mean_shift = MeanShift() # Initialize a dictionary to store silhouette scores for each algorithm silhouette_scores = {} # Perform clustering and compute silhouette scores try: kmeans_labels = kmeans.fit_predict(X) silhouette_scores[\'K-Means\'] = silhouette_score(X, kmeans_labels) except: silhouette_scores[\'K-Means\'] = None try: dbscan_labels = dbscan.fit_predict(X) if len(set(dbscan_labels)) > 1: silhouette_scores[\'DBSCAN\'] = silhouette_score(X, dbscan_labels) else: silhouette_scores[\'DBSCAN\'] = None except: silhouette_scores[\'DBSCAN\'] = None try: agglomerative_labels = agglomerative.fit_predict(X) silhouette_scores[\'Agglomerative\'] = silhouette_score(X, agglomerative_labels) except: silhouette_scores[\'Agglomerative\'] = None try: mean_shift_labels = mean_shift.fit_predict(X) silhouette_scores[\'Mean-Shift\'] = silhouette_score(X, mean_shift_labels) except: silhouette_scores[\'Mean-Shift\'] = None # Print silhouette scores print(\\"Silhouette Scores:\\") for algorithm, score in silhouette_scores.items(): print(f\\"{algorithm}: {score}\\") # Determine the best algorithm based on silhouette score best_algorithm = max(silhouette_scores, key=silhouette_scores.get) print(f\\"Best algorithm: {best_algorithm}\\")"},{"question":"Objective To assess the understanding of basic and advanced seaborn functionalities in creating and customizing categorical plots. Problem Statement Given the `titanic` dataset, write a Python function `plot_titanic_survival` that performs the following: 1. Load the dataset using seaborn\'s `load_dataset` function. 2. Initialize seaborn with the `whitegrid` style. 3. Create a boxen plot showing the distribution of ages across different classes (`class`) of passengers, differentiated by gender (`sex`). 4. Add jittered strip plots layered over the boxen plot to show individual data points of age with a size of 4. 5. Create subplots based on whether passengers survived (`survived`) using the `row` parameter. 6. Set the height of the plots to 5 and aspect ratio to 0.8. 7. Customize the plot\'s appearance: - Set x-axis label to \\"Passenger Age\\". - Set y-axis label to \\"Travel Class\\". - Set titles of the subplots to \\"{row_name} Survival\\". - Set x-ticks to display integers only. - Ensure the legend is placed outside the plot area. Function Signature ```python def plot_titanic_survival(): pass ``` Constraints - You can assume seaborn is installed in your environment. - You do not need to return anything from the function, only generate and display the plot. Expected Output - A multi-faceted plot showing age distribution across classes with gender differentiation and subplots for survived and non-survived passengers. - Proper customization of labels, titles, and legend positioning. Example Upon successful execution, the function should display a seaborn plot meeting the above specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_survival(): # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Initialize seaborn with whitegrid style sns.set(style=\\"whitegrid\\") # Create the plot g = sns.catplot( data=titanic, x=\\"age\\", y=\\"class\\", hue=\\"sex\\", col=\\"survived\\", kind=\\"boxen\\", height=5, aspect=0.8 ) # Add jittered strip plots to the boxen plots for ax in g.axes.flatten(): sns.stripplot( data=titanic, x=\\"age\\", y=\\"class\\", hue=\\"sex\\", dodge=True, jitter=True, size=4, ax=ax, alpha=0.5 ) # Customize labels, titles, and legend position g.set(xlabel=\\"Passenger Age\\", ylabel=\\"Travel Class\\") g.set_titles(\\"{col_name} Survival\\") g.set_xticklabels(rotation=45) plt.xticks(ticks=range(0, int(titanic[\'age\'].max()) + 1, 5)) # Remove the extra legend for subplot_ax in g.axes.flat: subplot_ax.legend_.remove() plt.legend(loc=\'upper left\', bbox_to_anchor=(1, 1), title=\'Sex\') plt.tight_layout() # Show the plot plt.show()"},{"question":"# Question: Creating a Custom Python C Extension Module You are required to create a custom Python C extension module. This module should provide functionalities to calculate the factorial of a number and reverse a string. Additionally, you must handle any potential errors, such as invalid input types. Task 1. **Create a new C file named `custommodule.c`.** 2. **Define two functions within this file:** - `custom_factorial` to compute the factorial of a non-negative integer. - `custom_reverse` to reverse a given string. 3. **Follow these specifications for the functions:** - `custom_factorial` should accept a single argument, a non-negative integer. If the argument is not valid, it should raise a `TypeError`. - `custom_reverse` should accept a single argument, a string. If the argument is not valid, it should raise a `TypeError`. - Properly use reference counting and manage memory appropriately. - Handle errors correctly and return appropriate Python exceptions. Function Definitions ```c #include <Python.h> // Function to compute the factorial of a non-negative integer static PyObject *custom_factorial(PyObject *self, PyObject *args) { int n; if (!PyArg_ParseTuple(args, \\"i\\", &n) || n < 0) { PyErr_SetString(PyExc_TypeError, \\"Argument must be a non-negative integer.\\"); return NULL; } long result = 1; for (int i = 2; i <= n; ++i) { result *= i; } return PyLong_FromLong(result); } // Function to reverse a given string static PyObject *custom_reverse(PyObject *self, PyObject *args) { const char *str; if (!PyArg_ParseTuple(args, \\"s\\", &str)) { PyErr_SetString(PyExc_TypeError, \\"Argument must be a string.\\"); return NULL; } int len = strlen(str); char *reversed = (char *)malloc((len + 1) * sizeof(char)); if (!reversed) { PyErr_NoMemory(); return NULL; } for (int i = 0; i < len; ++i) { reversed[i] = str[len - i - 1]; } reversed[len] = \'0\'; PyObject *result = PyUnicode_FromString(reversed); free(reversed); return result; } // Method definitions static PyMethodDef CustomMethods[] = { {\\"factorial\\", custom_factorial, METH_VARARGS, \\"Calculate factorial of a non-negative integer\\"}, {\\"reverse\\", custom_reverse, METH_VARARGS, \\"Reverse a given string\\"}, {NULL, NULL, 0, NULL} // Sentinel }; // Module definition static struct PyModuleDef custommodule = { PyModuleDef_HEAD_INIT, \\"custom\\", // Name of the module NULL, // Module documentation -1, // Size of per-interpreter state of the module CustomMethods }; // Module initialization function PyMODINIT_FUNC PyInit_custom(void) { return PyModule_Create(&custommodule); } ``` Steps to Implement the Solution: 1. **Create a file named `custommodule.c`.** 2. **Copy and paste the provided code into `custommodule.c`.** 3. **Compile the module following the proper compilation steps for your operating system.** 4. **Test your module in Python, ensuring both functions work correctly and handle errors as expected.** Example Usage: ```python import custom # Calculate factorial print(custom.factorial(5)) # Output: 120 print(custom.factorial(-1)) # Raises TypeError # Reverse a string print(custom.reverse(\\"hello\\")) # Output: \\"olleh\\" print(custom.reverse(12345)) # Raises TypeError ``` Constraints: - Ensure proper memory management, especially when working with dynamic memory allocation for strings. - Adhere to the given signatures and naming conventions for the functions. - Handle TypeError using Python C API functions when the input is invalid. Note: - Make sure to include `Python.h` at the top of your `custommodule.c` file. - Pay special attention to reference counting to avoid memory leaks or crashes. - Test the module extensively to cover edge cases.","solution":"# Since creating a C extension module is more involved and requires compilation, the following Python code # simulates the required functionality. Below you will find the Python code that would represent the # implemented C extension module. def factorial(n): Calculate the factorial of a non-negative integer. If the argument is not valid, raise a TypeError. if not isinstance(n, int) or n < 0: raise TypeError(\\"Argument must be a non-negative integer.\\") result = 1 for i in range(2, n + 1): result *= i return result def reverse(s): Reverse a given string. If the argument is not valid, raise a TypeError. if not isinstance(s, str): raise TypeError(\\"Argument must be a string.\\") return s[::-1]"},{"question":"# Regular Expression Pattern Matching and Validation Objective You are required to implement a function that checks whether a given string matches specific criteria using regular expressions. Problem Description Implement a Python function `validate_string(input_string: str) -> bool` that checks if the given `input_string` matches the following criteria: 1. The string should start with an uppercase letter. 2. The string should contain exactly one digit (0-9), and this digit should appear exactly in the middle of the string. 3. The string should end with three lowercase letters. Input Format - A single string `input_string` which may contain alphabets and digits (1 <= len(input_string) <= 1000). Output Format - Return `True` if `input_string` matches all the specified criteria, otherwise return `False`. Constraints - The string length must allow a single digit to appear exactly in the middle of the string, which implies that the length of the string (excluding the digit) should be even. Examples ```python assert validate_string(\\"A1abc\\") == True # Valid string assert validate_string(\\"B12xyz\\") == False # More than one digit assert validate_string(\\"a1xyz\\") == False # Does not start with an uppercase letter assert validate_string(\\"1Abcd\\") == False # Digit is not in the middle assert validate_string(\\"A1xyz123\\") == False # Ends with more than three lowercase letters ``` Performance Requirements - The solution should work efficiently for the maximum input length constraint. Hints - You may find the `re` module from Python\'s standard library useful for creating and using regular expressions. --- # Solution Template You may use the following template to begin your implementation: ```python import re def validate_string(input_string: str) -> bool: # Your code here pass ```","solution":"import re def validate_string(input_string: str) -> bool: # Check if the length of the string allows a single middle digit if len(input_string) < 4 or (len(input_string) % 2 == 0): return False # Create the regular expression pattern pattern = r\'^[A-Z][a-z]*d[a-z]{3}\' # Use fullmatch to ensure the entire string matches the pattern return bool(re.fullmatch(pattern, input_string))"},{"question":"You are provided with a dataset of temperatures recorded over a week. Your task is to use seaborn to create a line plot of these temperatures, with specific customizations using the seaborn objects (`so.Plot`) module. Task: 1. **Plot the data**: Create a line plot of temperature (y-axis) vs day (x-axis) using seaborn. 2. **Apply margins**: Ensure there is a small margin around the data. 3. **Set custom limits**: Pin the limits at specific values: - x-axis should range from 0 to 8. - y-axis should range from -5 to 40. 4. **Invert the y-axis**: Invert the y-axis so that higher temperatures are plotted lower on the graph. 5. **Maintain default values when appropriate**: If needed, use `None` to maintain default values for any limit. Input: - A dictionary `data` containing: - `days`: A list of integers representing the days of the week (1 to 7). - `temperatures`: A list of floats representing temperatures recorded each day. Example: ```python data = { \'days\': [1, 2, 3, 4, 5, 6, 7], \'temperatures\': [-2, 0.5, 5.6, 12, 8, 20, 25] } ``` Output: - A seaborn line plot as specified above. Constraints: - Ensure you use seaborn\'s objects interface (`so.Plot`). - Do not use other plotting libraries. - The plot should be displayed inline if using a Jupyter notebook. Example Code: ```python import seaborn.objects as so def plot_temps(data): days = data[\'days\'] temps = data[\'temperatures\'] p = so.Plot(x=days, y=temps).add(so.Line(marker=\\"o\\")) p = p.theme({\\"axes.xmargin\\": 0.1, \\"axes.ymargin\\": 0.1}) p = p.limit(x=(0, 8), y=(-5, 40)) p = p.limit(y=(40, -5)) p.plot() data = { \'days\': [1, 2, 3, 4, 5, 6, 7], \'temperatures\': [-2, 0.5, 5.6, 12, 8, 20, 25] } plot_temps(data) ```","solution":"import seaborn.objects as so def plot_temps(data): days = data[\'days\'] temps = data[\'temperatures\'] p = so.Plot(x=days, y=temps).add(so.Line(marker=\\"o\\")) p = p.theme({\\"axes.xmargin\\": 0.1, \\"axes.ymargin\\": 0.1}) p = p.limit(x=(0, 8), y=(-5, 40)) p = p.limit(y=(40, -5)) p.plot()"},{"question":"# Question: Implementing and Applying FFT in PyTorch In this exercise, you will demonstrate your understanding of PyTorch\'s FFT functionality by implementing functions to process and analyze signals. Problem Statement 1. **Signal Frequency Analysis:** - Write a function `compute_fft(signal: torch.Tensor) -> torch.Tensor` that computes and returns the one-dimensional FFT of a given real-valued input signal using the `torch.fft` module. The output should be a complex tensor representing the frequency domain. 2. **Inverse Signal Reconstruction:** - Write a function `reconstruct_signal(frequency_domain: torch.Tensor) -> torch.Tensor` that takes the frequency domain representation (complex tensor) produced by `compute_fft` and reconstructs the original real-valued signal using the inverse FFT. The output should be a real-valued tensor. 3. **Frequency Domain Filtering:** - Write a function `filter_frequencies(signal: torch.Tensor, threshold: float) -> torch.Tensor` that applies a simple low-pass filter to the signal. This function should: - Compute the FFT of the input signal. - Zero out all frequency components whose absolute value is below the given `threshold`. - Reconstruct the signal from the modified frequency domain representation. - Return the filtered real-valued signal. Expected Input and Output 1. **compute_fft**: - **Input:** `signal` - 1D `torch.Tensor` of real numbers representing a time-domain signal. - **Output:** 1D `torch.Tensor` of complex numbers representing the frequency domain. 2. **reconstruct_signal**: - **Input:** `frequency_domain` - 1D `torch.Tensor` of complex numbers from the frequency domain. - **Output:** 1D `torch.Tensor` of real numbers representing the reconstructed time-domain signal. 3. **filter_frequencies**: - **Input:** - `signal` - 1D `torch.Tensor` of real numbers representing a time-domain signal. - `threshold` - float value dictating the minimum absolute frequency component to be preserved. - **Output:** 1D `torch.Tensor` of real numbers representing the filtered time-domain signal. Constraints and Considerations - The input signals will have lengths that are powers of two (e.g., 256, 512, 1024). - Implementations should be efficient to handle large input sizes. - Pay attention to numerical stability and precision. You must use the `torch.fft` module for all FFT and inverse FFT operations. **Example Usage:** ```python import torch # Example signal signal = torch.sin(torch.linspace(0, 2 * torch.pi, 256)) # FFT computation frequency_domain = compute_fft(signal) # Signal Reconstruction reconstructed_signal = reconstruct_signal(frequency_domain) # Frequency Filtering threshold = 10.0 filtered_signal = filter_frequencies(signal, threshold) ```","solution":"import torch def compute_fft(signal: torch.Tensor) -> torch.Tensor: Computes the one-dimensional FFT of a real-valued input signal. Args: signal (torch.Tensor): Real-valued input signal. Returns: torch.Tensor: Complex tensor representing the frequency domain. return torch.fft.fft(signal) def reconstruct_signal(frequency_domain: torch.Tensor) -> torch.Tensor: Reconstructs the original signal from the frequency domain representation. Args: frequency_domain (torch.Tensor): Complex tensor from the frequency domain. Returns: torch.Tensor: Real-valued tensor representing the reconstructed time-domain signal. return torch.fft.ifft(frequency_domain).real def filter_frequencies(signal: torch.Tensor, threshold: float) -> torch.Tensor: Applies a low-pass filter to the signal by zeroing out frequency components below the threshold. Args: signal (torch.Tensor): Real-valued input signal. threshold (float): Minimum absolute value of frequency components to be preserved. Returns: torch.Tensor: Real-valued tensor representing the filtered time-domain signal. frequency_domain = compute_fft(signal) filtered_frequency_domain = torch.where(torch.abs(frequency_domain) < threshold, torch.tensor(0.0, dtype=frequency_domain.dtype), frequency_domain) return reconstruct_signal(filtered_frequency_domain)"},{"question":"# Question: Implement and Validate a WSGI-based Web Application Objective You are required to demonstrate your understanding of WSGI and the `wsgiref` package by implementing a simple WSGI web application. Your task is to create an application that responds to HTTP requests, manipulates some of the WSGI environment variables, and validates WSGI compliance. This question will assess your understanding of WSGI concepts, environment utilities, and server implementation. Instructions 1. **WSGI Application**: - Implement a WSGI application that responds with an HTML page containing a greeting message and lists environment variables. 2. **Environment Manipulation**: - Use the `wsgiref.util.setup_testing_defaults` function to add default WSGI environment variables. - Utilize the `wsgiref.util.shift_path_info` function to manipulate the `PATH_INFO` variable. 3. **Response Headers**: - Create and manipulate response headers using the `wsgiref.headers.Headers` class. - Ensure the \'Content-type\' header is set to \'text/html\'. 4. **Server**: - Serve the application using `wsgiref.simple_server.make_server`. - Ensure that the application runs on port 8000. 5. **Validation**: - Wrap the application with a WSGI validator using the `wsgiref.validate.validator` function to ensure compliance. Requirements - **Input**: The WSGI application function should take two parameters: `environ` (a dictionary containing CGI-style environment variables) and `start_response` (a callable for starting the HTTP response). - **Output**: The application should return an iterable yielding byte strings, containing the HTML response. # Example Output HTML ```html <!DOCTYPE html> <html> <head> <title>WSGI Application</title> </head> <body> <h1>Hello, WSGI!</h1> <ul> <li>REQUEST_METHOD: GET</li> <li>PATH_INFO: /example/path</li> <!-- List other environment variables --> </ul> </body> </html> ``` Constraints - Do not use any external web frameworks. Use only the standard library and `wsgiref` module functions and classes. - Make sure to properly handle the `start_response` callable and set the necessary response headers. Performance - Your solution should handle multiple concurrent requests up to a reasonable limit (10 simultaneous requests). Example Skeleton Code ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator from wsgiref.util import setup_testing_defaults, shift_path_info from wsgiref.headers import Headers def simple_app(environ, start_response): setup_testing_defaults(environ) headers = Headers([(\'Content-type\', \'text/html\')]) status = \'200 OK\' start_response(status, headers.items()) path_info = shift_path_info(environ) body = \\"<!DOCTYPE html><html><head><title>WSGI Application</title></head><body>\\" body += \\"<h1>Hello, WSGI!</h1><ul>\\" for key, value in environ.items(): body += f\\"<li>{key}: {value}</li>\\" body += \\"</ul></body></html>\\" return [body.encode(\'utf-8\')] validated_app = validator(simple_app) if __name__ == \'__main__\': with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Submit your implemented `simple_app` function and ensure it fulfills all the requirements mentioned above.","solution":"from wsgiref.simple_server import make_server from wsgiref.validate import validator from wsgiref.util import setup_testing_defaults, shift_path_info from wsgiref.headers import Headers def simple_app(environ, start_response): # Setup default environment setup_testing_defaults(environ) # Create headers headers = Headers([(\'Content-type\', \'text/html\')]) # Define the HTTP response status status = \'200 OK\' # Invoke start_response callable start_response(status, headers.items()) # Manipulate PATH_INFO path_info = shift_path_info(environ) # Create response body body = \\"<!DOCTYPE html><html><head><title>WSGI Application</title></head><body>\\" body += \\"<h1>Hello, WSGI!</h1><ul>\\" for key, value in environ.items(): body += f\\"<li>{key}: {value}</li>\\" body += \\"</ul></body></html>\\" return [body.encode(\'utf-8\')] # Validate the application to ensure it complies with WSGI standards. validated_app = validator(simple_app) if __name__ == \'__main__\': with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"Objective To assess your understanding of pandasâ€™ date offsets and their application in data manipulation. Problem Statement You are given a DataFrame with a single column â€œdateâ€ containing date values. Your task is to implement a function that performs the following operations on the DataFrame: 1. Adds a `BusinessDay` offset of 5 days to each date. 2. Checks if each resulting date is the end of a business month and creates a new column `business_month_end` to store the result (True/False). 3. Adds a `MonthEnd` offset to each date and creates a new column `month_end_date`. 4. Checks if each resulting date after the `MonthEnd` addition is also a quarter end and creates a new column `is_quarter_end` to store the result (True/False). Function Signature: ```python def manipulate_dates(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input Format - A DataFrame `df` with a single column `\\"date\\"` containing date values in string format (e.g., `2023-01-15`). Output Format - The function should return a DataFrame with the following columns: - `\\"date\\"`: Original date. - `\\"date_plus_5bd\\"`: Date after adding 5 Business Days. - `\\"business_month_end\\"`: Boolean indicating if `\\"date_plus_5bd\\"` is the end of a business month. - `\\"month_end_date\\"`: Date after adding a MonthEnd offset to `\\"date_plus_5bd\\"`. - `\\"is_quarter_end\\"`: Boolean indicating if `\\"month_end_date\\"` is the end of a quarter. Constraints - The function should handle the DataFrame efficiently even if it contains thousands of rows. Example Given the following input DataFrame: ```python df = pd.DataFrame({ \'date\': [\'2021-01-25\', \'2021-06-10\', \'2021-11-22\'] }) ``` The expected output DataFrame should look something like: ```python date date_plus_5bd business_month_end month_end_date is_quarter_end 0 2021-01-25 2021-02-01 False 2021-02-28 False 1 2021-06-10 2021-06-17 False 2021-06-30 True 2 2021-11-22 2021-11-29 False 2021-11-30 False ``` Note - You are expected to use pandasâ€™ date offset functionalities to achieve the desired results. - Assure the function handles dates around holidays and non-business days appropriately. Evaluation Criteria - Correctness: The solution should provide the correct dates and boolean values as per the operations specified. - Performance: The function should be efficient and handle large DataFrames. - Readability: The code should be well-organized and commented appropriately.","solution":"import pandas as pd from pandas.tseries.offsets import BusinessDay, MonthEnd def manipulate_dates(df: pd.DataFrame) -> pd.DataFrame: df[\'date\'] = pd.to_datetime(df[\'date\']) # Adding 5 Business Days df[\'date_plus_5bd\'] = df[\'date\'] + BusinessDay(5) # Checking if the date plus 5 business days is at the end of the business month df[\'business_month_end\'] = df[\'date_plus_5bd\'].apply(lambda x: x.is_month_end and x.weekday() < 5) # Adding MonthEnd offset df[\'month_end_date\'] = df[\'date_plus_5bd\'] + MonthEnd(0) # Checking if the new date is at the quarter end df[\'is_quarter_end\'] = df[\'month_end_date\'].apply(lambda x: x.is_quarter_end) return df"},{"question":"Objective The goal of this assessment is to test your ability to create and customize data visualizations using the seaborn library. You are expected to utilize both axes-level and figure-level functions, handle different types of plots, and understand how to facet data. Problem Statement You are given a dataset containing information about various tips from a restaurant, including the total bill, tip amount, sex, smoker status, day, time, and size of the party. Using this dataset, you are required to perform the following tasks: 1. **Load the Data:** - Load the `tips` dataset from seaborn\'s built-in datasets. 2. **Create a Composite Plot:** - Create a figure with two subplots: 1. The first subplot should be a scatter plot showing the relationship between `total_bill` and `tip`, colored by `sex`. - Use an axes-level function for this plot. 2. The second subplot should be a violin plot showing the distribution of `tip` for different `day`s of the week, split by `smoker` status. - Use a figure-level function for this plot. 3. **Customize the Subplots:** - Ensure the subplots are properly labeled with titles, axis labels, and legends where appropriate. - Modify the figure size so that both plots are clearly visible without overlapping. 4. **Show the Plot:** - Finally, display the composite plot. Constraints - You must use the seaborn library for all visualizations. - The figure should be clear and informative. - Proper labeling of the axes and legends is required for readability. Input None. Output A composite plot as specified, displayed using matplotlib\'s `plt.show()` function. Example Usage ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the data tips = sns.load_dataset(\\"tips\\") # Task 2: Create a composite plot fig, axes = plt.subplots(1, 2, figsize=(15, 5)) # First subplot: Scatter plot of total_bill vs tip colored by sex sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'sex\', ax=axes[0]) # Second subplot: Violin plot of tips across days, split by smoker status sns.violinplot(data=tips, x=\'day\', y=\'tip\', hue=\'smoker\', split=True, ax=axes[1]) # Task 3: Customize the subplots axes[0].set_title(\'Total Bill vs Tip\') axes[0].set_xlabel(\'Total Bill\') axes[0].set_ylabel(\'Tip\') axes[1].set_title(\'Tip Distribution across Days by Smoker Status\') axes[1].set_xlabel(\'Day\') axes[1].set_ylabel(\'Tip\') # Task 4: Show the plot plt.tight_layout() plt.show() ``` Notes - Ensure your code runs without errors and the output plot is as expected. - Pay attention to the customization of labels and legends to enhance readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_composite_plot(): # Load the data tips = sns.load_dataset(\\"tips\\") # Create a composite plot with 2 subplots fig, axes = plt.subplots(1, 2, figsize=(15, 5)) # First subplot: Scatter plot of total_bill vs tip colored by sex sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'sex\', ax=axes[0]) axes[0].set_title(\'Total Bill vs Tip\') axes[0].set_xlabel(\'Total Bill\') axes[0].set_ylabel(\'Tip\') # Second subplot: Violin plot of tips across days, split by smoker status sns.violinplot(data=tips, x=\'day\', y=\'tip\', hue=\'smoker\', split=True, ax=axes[1]) axes[1].set_title(\'Tip Distribution across Days by Smoker Status\') axes[1].set_xlabel(\'Day\') axes[1].set_ylabel(\'Tip\') # Adjust layout plt.tight_layout() plt.show()"},{"question":"Working with Python Code Objects Objective This question is designed to evaluate your understanding of Python code objects and their manipulation using provided CPython API functions. Problem Statement Implement a function `code_object_inspector(bytecode: bytes, filename: str, funcname: str) -> dict` that accepts the following parameters: - `bytecode`: A byte string of Python bytecode instructions. - `filename`: The name of the file that would hypothetically contain this code. - `funcname`: The name of the function in which this code is defined. The function should perform the following tasks: 1. Create a new code object from the given `bytecode`, `filename`, and `funcname`. 2. Inspect the created code object to determine: - Total number of free variables. - Line number corresponding to the start of the code. - Validate if the created object is indeed a code object. 3. Return a dictionary with the following keys: - `is_code_object`: A boolean value indicating if the object created is a valid code object. - `num_free_vars`: An integer representing the number of free variables in the code. - `start_line`: An integer specifying the first line number of the code. Function Signature ```python def code_object_inspector(bytecode: bytes, filename: str, funcname: str) -> dict: pass ``` Constraints 1. `bytecode` will be a valid byte string of Python instructions. 2. `filename` and `funcname` will be non-empty strings. 3. The function needs to handle edge cases such as empty bytecode gracefully. Example ```python bytecode = b\'dxc9dx00Sx00\' filename = \\"example.py\\" funcname = \\"example_function\\" result = code_object_inspector(bytecode, filename, funcname) print(result) # Sample Output: # { # \'is_code_object\': True, # \'num_free_vars\': 0, # \'start_line\': 1 # } ``` Notes - Use the CPython API functions provided in the documentation. - Ensure that your function handles any possible exceptions that might arise during code object creation or inspection.","solution":"import types def code_object_inspector(bytecode: bytes, filename: str, funcname: str) -> dict: Inspects a code object created from the given bytecode, filename, and funcname. :param bytecode: A byte string of Python bytecode instructions. :param filename: The name of the file that would hypothetically contain this code. :param funcname: The name of the function in which this code is defined. :return: A dictionary containing information about the code object. # Default result dictionary result = { \'is_code_object\': False, \'num_free_vars\': 0, \'start_line\': 0 } try: # Create a new code object from the given bytecode, filename, and funcname. code_obj = types.CodeType( 0, # argcount 0, # posonlyargcount 0, # kwonlyargcount 0, # nlocals 0, # stacksize 0, # flags bytecode, # codestring (), # constants (), # names (), # varnames \\"\\", # filename funcname, # name 1, # firstlineno b\'\' # lnotab ) # Check and populate the result dictionary result[\'is_code_object\'] = isinstance(code_obj, types.CodeType) result[\'num_free_vars\'] = len(code_obj.co_freevars) result[\'start_line\'] = code_obj.co_firstlineno except Exception as e: # Handle any possible exceptions gracefully print(f\\"An error occurred: {e}\\") return result"},{"question":"# XML Processing with `xml.etree.ElementTree` Objective You are required to design a function that parses an XML document, extracts specific information based on given conditions, and creates a new XML document from the extracted information. Problem Statement Given an XML string representing a collection of books, you need to implement a function `process_books_xml(xml_string: str) -> str` that processes this XML to perform the following tasks: 1. Parse the given XML string. 2. Extract books published after the year 2000. 3. For each extracted book, create a new XML document with the bookâ€™s title and author. 4. Return the resulting XML document as a string. Input - `xml_string` (str): A string that contains the XML data of books. Output - (str): A string that contains the processed XML data of books published after the year 2000. The structure of the output XML should be: ```xml <books> <book> <title>TITLE</title> <author>AUTHOR</author> </book> ... </books> ``` Constraints - Each book in the input XML has the following structure: ```xml <book id=\\"UNIQUE_ID\\"> <title>TITLE</title> <author>AUTHOR</author> <year>YEAR</year> </book> ``` - The `year` element is always in `yyyy` format, and the book title and author are non-empty strings. - The input XML string can contain multiple books. Example # Input: ```xml <library> <book id=\\"1\\"> <title>Book One</title> <author>Author One</author> <year>1999</year> </book> <book id=\\"2\\"> <title>Book Two</title> <author>Author Two</author> <year>2005</year> </book> <book id=\\"3\\"> <title>Book Three</title> <author>Author Three</author> <year>2018</year> </book> </library> ``` # Output: ```xml <books> <book> <title>Book Two</title> <author>Author Two</author> </book> <book> <title>Book Three</title> <author>Author Three</author> </book> </books> ``` Notes - Make sure to handle the XML parsing and generation correctly using the `xml.etree.ElementTree` module. - Ensure that your output is well-formed XML. Function Signature ```python def process_books_xml(xml_string: str) -> str: pass ```","solution":"import xml.etree.ElementTree as ET def process_books_xml(xml_string: str) -> str: Parses the given XML string, extracts books published after the year 2000, and returns a new XML string containing the titles and authors of those books. # Parse the input XML string root = ET.fromstring(xml_string) # Create the root of the new XML document new_root = ET.Element(\\"books\\") # Iterate through each book in the input XML for book in root.findall(\'book\'): year = int(book.find(\'year\').text) if year > 2000: new_book = ET.Element(\\"book\\") title = ET.SubElement(new_book, \\"title\\") title.text = book.find(\'title\').text author = ET.SubElement(new_book, \\"author\\") author.text = book.find(\'author\').text new_root.append(new_book) # Convert the new XML tree to a string return ET.tostring(new_root, encoding=\'unicode\') # Example input for testing xml_input = <library> <book id=\\"1\\"> <title>Book One</title> <author>Author One</author> <year>1999</year> </book> <book id=\\"2\\"> <title>Book Two</title> <author>Author Two</author> <year>2005</year> </book> <book id=\\"3\\"> <title>Book Three</title> <author>Author Three</author> <year>2018</year> </book> </library> # Expected output for testing expected_output = <books> <book> <title>Book Two</title> <author>Author Two</author> </book> <book> <title>Book Three</title> <author>Author Three</author> </book> </books>"},{"question":"**Assignment: Password Expiry Notification System** You are tasked with creating a utility that will notify users whose passwords are about to expire within a specified number of days. Given the sensitive nature of the shadow password database, ensure your program handles exceptions correctly, especially for permissions issues. # Problem Statement: Implement a function `notify_users(days: int) -> List[str]` which checks the shadow password database for users whose passwords will expire in the specified number of days. The function should return a list of user logins (i.e., `sp_namp` values) whose passwords will expire within the given number of days. # Input: - `days` (int): An integer representing the number of days within which the passwords will expire. # Output: - A list of strings: User login names whose passwords will expire within the specified number of days. # Constraints: - You must handle `PermissionError` appropriately if the user doesn\'t have the privileges to access the shadow password database. - Assume method `spwd.getspall()` returns entries for all users. - The function should handle the absence of entries gracefully by returning an empty list if no entries match the criteria. # Example: ```python def notify_users(days: int) -> List[str]: # Your implementation here # Sample usage expiring_users = notify_users(7) print(expiring_users) # Output could be [\'user1\', \'user2\', ...], based on the shadow password database entries. ``` # Additional Notes: - Consider the `sp_lstchg`, `sp_max`, and `sp_warn` attributes to compute the password expiry. - Use the current date to calculate the number of days until expiration. - You may assume the current date is given and leave the implementation for retrieving the current date using the appropriate method in Python. Make sure your implementation includes: - Proper error handling for permission issues. - A clear and optimized approach to filtering users based on the expiry criteria. - Consider edge cases, such as when no users are found or when every user\'s password is up-to-date beyond the given `days`.","solution":"import spwd import datetime from typing import List def notify_users(days: int) -> List[str]: try: users = spwd.getspall() # Get all shadow password entries except PermissionError: return [] current_date = datetime.datetime.now() expiring_users = [] for user in users: if user.sp_max == -1: continue # Skip users with no password expiration last_change = datetime.datetime.fromtimestamp(user.sp_lstchg * 86400) max_days = datetime.timedelta(days=user.sp_max) warning_date = last_change + max_days - datetime.timedelta(days=days) if current_date >= warning_date and current_date < (last_change + max_days): expiring_users.append(user.sp_namp) return expiring_users"},{"question":"You are given pretrained weights for two linear models. You need to fine-tune these models by adding another linear layer, and compute the Jacobian and Hessian of the final output with respect to input data. Implementing this will demonstrate your comprehension of `torch.func` utilities. # Requirements 1. Implement a function `compute_gradients` which computes the Jacobian and Hessian of the output with respect to the input. 2. This function should take in the following arguments: - `model1`: A `torch.nn.Module` representing the first pretrained linear model. - `model2`: A `torch.nn.Module` representing the second pretrained linear model. - `x`: A tensor input to the models. 3. The function should: - Create a new model by stacking the two provided models. - Add a new linear layer to the new stacked model. - Compute the Jacobian of the resulting model\'s output with respect to the input `x`. - Compute the Hessian of the resulting model\'s output with respect to the input `x`. 4. Return a tuple containing: - The Jacobian matrix - The Hessian matrix # Constraints - Ensure `x` is a 1D tensor with size corresponding to the input size of the first model. - Use `torch.func.functional_call` and `torch.func` methods (`jacrev` and `hessian`) where applicable. - Do not use for-loops except for model construction. # Example Usage ```python import torch import torch.nn as nn # Pretrained models model1 = torch.nn.Linear(3, 3) model2 = torch.nn.Linear(3, 3) # Input data x = torch.randn(3) # Your implementation jacobian, hessian = compute_gradients(model1, model2, x) print(\\"Jacobian:\\", jacobian) print(\\"Hessian:\\", hessian) ``` # Note - This task expects you to utilize PyTorch\'s functional transformations effectively. - The Jacobian should be computed using `jacrev` and the Hessian using `hessian`.","solution":"import torch import torch.nn as nn from torch.func import functional_call, jacrev, hessian class CombinedModel(nn.Module): def __init__(self, model1, model2): super(CombinedModel, self).__init__() self.model1 = model1 self.model2 = model2 self.new_layer = nn.Linear(model2.out_features, model2.out_features) def forward(self, x): x = self.model1(x) x = self.model2(x) x = self.new_layer(x) return x def compute_gradients(model1, model2, x): combined_model = CombinedModel(model1, model2) # Define function to compute def model_output(input_tensor): return combined_model(input_tensor) # Compute the Jacobian jacobian_func = jacrev(model_output) jacobian = jacobian_func(x) # Compute the Hessian hessian_func = hessian(model_output) hessian_matrix = hessian_func(x) return jacobian, hessian_matrix"},{"question":"You are provided with a PyTorch script that performs a matrix multiplication operation and applies non-linear activation. Your task is to: 1. Identify and analyze the performance bottlenecks using `torch.utils.bottleneck`. 2. Optimize the script to improve its performance based on the profiling results. Initial Script ```python import torch import torch.nn.functional as F def matrix_multiplication(A, B): return torch.matmul(A, B) def apply_activation(matrix): return F.relu(matrix) def main(): # Initialize large matrices A = torch.randn(1000, 1000).to(\'cuda\') B = torch.randn(1000, 1000).to(\'cuda\') # Perform matrix multiplication result = matrix_multiplication(A, B) # Apply activation function result = apply_activation(result) return result if __name__ == \\"__main__\\": output = main() print(\\"Final output: \\", output) ``` Instructions 1. **Profile the script using `torch.utils.bottleneck`**: Run the script with bottleneck to generate a profile report. 2. **Analyze**: - Determine whether the script is CPU-bound or GPU-bound. - Identify the most time-consuming operations. 3. **Optimize**: - Based on your analysis, modify the script to reduce the identified bottlenecks. - Provide a brief explanation of your changes and why they improve performance. Submission - Submit the optimized script along with the original profile report and the modified profile report. - Document the changes made and your reasoning. - Ensure the optimized script runs correctly and provides the same output as the initial script. Expected Output Format - **Original Profile Report** - **Modifed Script**: ```python # Your optimized script here ``` - **Modified Profile Report** - **Explanation**: - Your analysis of the bottlenecks - Description of the optimizations applied Constraints - You must use PyTorch and the provided `torch.utils.bottleneck` tool. - Ensure that the optimized script runs on both CPU and GPU. - The optimized script should not change the final output (i.e., correctness should be maintained).","solution":"import torch import torch.nn.functional as F def matrix_multiplication(A, B): return torch.matmul(A, B) def apply_activation(matrix): return F.relu(matrix) def main(): # Initialize large matrices A = torch.randn(1000, 1000).to(\'cuda\') B = torch.randn(1000, 1000).to(\'cuda\') # Perform matrix multiplication with streams for concurrency stream = torch.cuda.Stream() with torch.cuda.stream(stream): result = matrix_multiplication(A, B) # Apply activation function stream.synchronize() result = apply_activation(result) return result if __name__ == \\"__main__\\": output = main() print(\\"Final output: \\", output)"},{"question":"Objective Demonstrate your understanding of the `torch.fx` module by implementing a function that transforms a given `torch.nn.Module` by replacing all instances of the `torch.add` operation with the `torch.sub` (subtraction) operation. Ensure the transformed module maintains the same overall functionality and structure as the original module. Problem Statement You are given a `torch.nn.Module` that has been symbolically traced into a `torch.fx.Graph`. Your task is to write a function `transform_add_to_sub` that takes in this symbolic traced module, modifies its graph to replace all instances of `torch.add` with `torch.sub`, and returns a new `torch.nn.Module` that reflects this transformation. Function Signature ```python def transform_add_to_sub(m: torch.nn.Module) -> torch.nn.Module: pass ``` Input - `m` (torch.nn.Module): A PyTorch neural network module that will be traced and transformed. Output - `new_module` (torch.nn.Module): A new PyTorch neural network module that has the same functionality as `m` but with all `torch.add` operations replaced by `torch.sub`. Constraints - You must use the `torch.fx` module to achieve this transformation. - Ensure that the modified module passes basic consistency checks and maintains the same input/output behavior as the original module. Example Usage ```python import torch import torch.fx class TestModule(torch.nn.Module): def forward(self, x, y): return torch.add(x, y) # Original module original_module = TestModule() # Perform the transformation transformed_module = transform_add_to_sub(original_module) # Test the modified behavior x = torch.tensor([1.0, 2.0, 3.0]) y = torch.tensor([1.0, 2.0, 3.0]) print(original_module(x, y)) # Output should be tensor([2.0, 4.0, 6.0]) print(transformed_module(x, y)) # Output should be tensor([0.0, 0.0, 0.0]) ``` Notes - Use the techniques described in the `torch.fx` documentation to perform this transformation. - Pay attention to maintaining the structure and flow of the graph while making the necessary replacements. Good luck!","solution":"import torch import torch.fx def transform_add_to_sub(m: torch.nn.Module) -> torch.nn.Module: # Trace the module into a GraphModule traced = torch.fx.symbolic_trace(m) # Create a GraphModule copy to modify new_graph = torch.fx.Graph() env = {} # Iterate through the nodes and replace torch.add with torch.sub for node in traced.graph.nodes: if node.op == \'call_function\' and node.target == torch.add: new_node = new_graph.create_node(\'call_function\', torch.sub, node.args, node.kwargs) else: new_node = new_graph.node_copy(node, lambda n: env[n]) env[node] = new_node # Create a new GraphModule with the modified graph new_traced = torch.fx.GraphModule(traced, new_graph) return new_traced"},{"question":"# Question: Visualizing and Customizing Price Distributions of Diamonds using `seaborn` You are tasked with analyzing and visualizing the distribution of diamond prices based on their clarity and size. For this purpose, you\'ll use `seaborn`, a statistical data visualization library in Python. Write a Python function as described below: Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_diamond_prices(): pass ``` Requirements: 1. **Dataset Loading**: - Load the `diamonds` dataset using `seaborn`. 2. **Basic Plotting**: - Create a basic horizontal boxen plot of diamond prices. 3. **Categorical Grouping**: - Create a boxen plot of diamond prices grouped by the `clarity` of the diamonds. 4. **Advanced Grouping**: - Further group the data by whether the diamond\'s carat weight is greater than 1 (`large_diamond`), and represent this with different colors. 5. **Customization**: - Customize the plot to: - Use a width method of `linear`. - Set the width of the largest box to 0.5. - Customize the line color to `.7` and line width to `0.5`. - Customize the median line using `line_kws` to have a linewidth of `1.5` and color `\\"#cde\\"`. - Customize the appearance of outliers using `flier_kws` with `facecolor` set to `.7` and `linewidth` set to `0.5`. - Remove the fill from the boxes to get an unfilled box plot, ensuring all elements follow the hue when used. 6. **Display the Plot**: - Finally, display the plots using `matplotlib`. Expected Output: Your function should result in a sequence of plots demonstrating each of the steps described. Each plot should be configured according to the specifications above. Example Usage: When you run the function `visualize_diamond_prices`, the plots should be generated and displayed. Ensure your solution is clear, well-documented, and handles possible issues like proper dataset loading and plot rendering.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_diamond_prices(): # Load the diamonds dataset using seaborn diamonds = sns.load_dataset(\'diamonds\') # Create a basic horizontal boxen plot of diamond prices plt.figure(figsize=(10, 6)) sns.boxenplot(x=diamonds[\'price\']) plt.title(\'Basic Horizontal Boxen Plot of Diamond Prices\') plt.show() # Create a boxen plot of diamond prices grouped by clarity plt.figure(figsize=(10, 6)) sns.boxenplot(x=\'price\', y=\'clarity\', data=diamonds) plt.title(\'Boxen Plot of Diamond Prices Grouped by Clarity\') plt.show() # Introduce a new column to categorize diamonds as large or small based on carat weight diamonds[\'large_diamond\'] = diamonds[\'carat\'] > 1 # Create a boxen plot of diamond prices grouped by clarity and categorized by the new column plt.figure(figsize=(10, 6)) sns.boxenplot(x=\'price\', y=\'clarity\', hue=\'large_diamond\', data=diamonds, width=0.5, linewidth=0.5, color=\'.7\', line_kws={\'linewidth\':1.5, \'color\':\'#cde\'}, flier_kws={\'facecolor\':\'.7\', \'linewidth\':0.5}, saturation=1) plt.title(\'Boxen Plot of Diamond Prices Grouped by Clarity and Large Diamond Indicator\') plt.show()"},{"question":"**Objective:** Write a Python function capable of performing a call stack analysis and extracting relevant debugging information using the provided python310 introspection APIs. This function should compile a detailed report of the current execution stack, including information about built-ins, local variables, global variables, current frame, and function descriptions. **Function Signature:** ```python import typing def call_stack_analysis() -> typing.List[typing.Dict[str, typing.Any]]: pass ``` **Requirements:** 1. Your function `call_stack_analysis` should gather information from the current execution frame and build a detailed report of the call stack. 2. The report should be a list of dictionaries, each dictionary containing information about a frame in the call stack. 3. For each frame in the stack, the dictionary must contain: - `builtins`: A dictionary of the built-in variables. - `locals`: A dictionary of the local variables. - `globals`: A dictionary of the global variables. - `current_line`: The number of the currently executing line. - `func_name`: The name of the current function or method. - `func_desc`: A description of the current function or method. 4. The list should start from the topmost frame and proceed down the stack. **Constraints:** - Assume that there will always be a frame executing when the function is called. - Handle potential `NULL` values where possible. **Example**: For demonstration purposes, consider the following code will require the implementation: ```python def example_function(): return call_stack_analysis() result = example_function() print(result) ``` Expected Result Format: ```python [ { \\"builtins\\": {\\"...\\": \\"...\\"}, \\"locals\\": {\\"...\\": \\"...\\"}, \\"globals\\": {\\"...\\": \\"...\\"}, \\"current_line\\": 10, \\"func_name\\": \\"example_function\\", \\"func_desc\\": \\"()\\" }, ... ] ``` **Performance Notes:** - The function should efficiently handle stack introspection without significant overhead. - Always ensure resource handling and potential references are properly managed to prevent memory leaks. **Hints:** - Use `PyEval_GetBuiltins`, `PyEval_GetLocals`, `PyEval_GetGlobals`, `PyEval_GetFrame`, `PyFrame_GetBack`, `PyFrame_GetCode`, `PyFrame_GetLineNumber`, `PyEval_GetFuncName`, and `PyEval_GetFuncDesc` to access the required data. - Implement robust error checking in case some frames or variables are inaccessible or not present.","solution":"import sys import typing def call_stack_analysis() -> typing.List[typing.Dict[str, typing.Any]]: def get_func_desc(frame): code = frame.f_code func_name = frame.f_code.co_name arg_count = code.co_argcount pos_args = list(code.co_varnames[:arg_count]) return f\\"{func_name}({\', \'.join(pos_args)})\\" frames_info = [] current_frame = sys._getframe() while current_frame: frames_info.append({ \\"builtins\\": current_frame.f_builtins, \\"locals\\": current_frame.f_locals, \\"globals\\": current_frame.f_globals, \\"current_line\\": current_frame.f_lineno, \\"func_name\\": current_frame.f_code.co_name, \\"func_desc\\": get_func_desc(current_frame) }) current_frame = current_frame.f_back return frames_info"},{"question":"# FTP File Processing Challenge **Objective:** You are required to write a Python script using the `ftplib` package to connect to an FTP server, download a specific text file, perform some text processing on the content of the file, and then upload the modified file back to the server. **Input:** - `host`: string, the FTP server hostname. - `user`: string, the username for the FTP server. - `passwd`: string, the password for the FTP server. - `directory`: string, the directory on the server where the file is located. - `filename`: string, the name of the file to be downloaded and processed. **Output:** - The script should not return anything, but it should perform the following actions: - Connect to the specified FTP server and login with provided credentials. - Navigate to the specified directory. - Download the specified file. - Perform the following text processing: convert all characters to uppercase. - Upload the modified file back to the same directory with a modified name (e.g., append \\"_modified\\" to the original filename). **Constraints:** - Assume the FTP server is reachable and the provided credentials are correct. - Ensure proper error handling for FTP commands. **Performance Requirements:** - The solution should handle files up to 10 MB efficiently. **Function to Implement:** ```python def process_ftp_file(host: str, user: str, passwd: str, directory: str, filename: str) -> None: pass ``` # Example Usage ```python process_ftp_file(\'ftp.example.com\', \'user\', \'password\', \'pub/files\', \'example.txt\') ``` **Implementation Details:** 1. **Connect and Login:** - Use `ftplib.FTP` to connect to the server. - Login using provided `user` and `passwd`. 2. **Navigate to Directory:** - Change to the specified directory using `cwd()` method. 3. **Download File:** - Use the `retrbinary` method to download the file in binary mode. 4. **Process File Content:** - Convert all text in the file to uppercase. 5. **Upload Modified File:** - Save the modified content to a new file named `<original_filename>_modified`. - Use the `storbinary` method to upload the new file to the server. 6. **Error Handling:** - Properly handle potential FTP exceptions such as connection errors, file not found, etc.","solution":"import ftplib from io import BytesIO def process_ftp_file(host: str, user: str, passwd: str, directory: str, filename: str) -> None: try: # Connect to the FTP server ftp = ftplib.FTP(host) ftp.login(user, passwd) ftp.cwd(directory) # Download the file bio = BytesIO() ftp.retrbinary(f\\"RETR {filename}\\", bio.write) bio.seek(0) # Process the file content content = bio.getvalue().decode(\'utf-8\').upper() modified_bio = BytesIO(content.encode(\'utf-8\')) # Prepare modified filename modified_filename = f\\"{filename}_modified\\" # Upload the modified file ftp.storbinary(f\\"STOR {modified_filename}\\", modified_bio) # Close the connection ftp.quit() except ftplib.all_errors as e: print(f\\"FTP error: {e}\\")"},{"question":"**Question:** You have been provided with a dataset containing temperature recordings over a week. Your task is to visualize this data using seaborn and apply appropriate limits to the axes as specified below. # Dataset The dataset is provided as lists: - `days`: A list of integers from 1 to 7 representing days of the week. - `temperatures`: A list of integers representing the recorded temperatures for each day. # Requirements 1. Create a line plot using seaborn\'s `seaborn.objects` module with `days` on the x-axis and `temperatures` on the y-axis. Each point should be marked with a circle. 2. Set the x-axis limits to range from 0 to 8. 3. Set the y-axis limits to start from -5 to a dynamic maximum that is 10 units above the highest temperature in the provided dataset. 4. Invert the y-axis to make the highest temperatures appear at the bottom of the plot. # Input - `days` (List[int]): A list of integers from 1 to 7 representing days of the week. - `temperatures` (List[int]): A list of integers representing the recorded temperatures for each day. # Output The function should display a plot that meets the requirements specified above. # Example ```python days = [1, 2, 3, 4, 5, 6, 7] temperatures = [23, 22, 25, 21, 20, 19, 18] create_and_limit_plot(days, temperatures) ``` **Constraints:** - The function should handle any valid input list of temperatures corresponding to the days in the range 1-7. - You must use seaborn\'s `seaborn.objects` module. # Solution Template ```python import seaborn.objects as so def create_and_limit_plot(days, temperatures): # Create the initial plot p = so.Plot(x=days, y=temperatures).add(so.Line(marker=\\"o\\")) # Set the x-axis limits from 0 to 8 p = p.limit(x=(0, 8)) # Compute the dynamic maximum for y-axis and set limits with inversion max_temp = max(temperatures) p = p.limit(y=(max_temp + 10, -5)) # Display the plot p.show() # Example usage days = [1, 2, 3, 4, 5, 6, 7] temperatures = [23, 22, 25, 21, 20, 19, 18] create_and_limit_plot(days, temperatures) ```","solution":"import seaborn.objects as so def create_and_limit_plot(days, temperatures): Creates a line plot for the given temperature data over the days of the week with specific axis limits and inversion. Parameters: days (list of int): Days of the week (1 to 7). temperatures (list of int): Temperature recordings for each day. # Create the initial plot p = so.Plot(x=days, y=temperatures).add(so.Line(marker=\\"o\\")) # Set the x-axis limits from 0 to 8 p = p.limit(x=(0, 8)) # Compute the dynamic maximum for y-axis and set limits with inversion max_temp = max(temperatures) p = p.limit(y=(max_temp + 10, -5)) # Display the plot p.show() # Example usage days = [1, 2, 3, 4, 5, 6, 7] temperatures = [23, 22, 25, 21, 20, 19, 18] create_and_limit_plot(days, temperatures)"},{"question":"**Objective:** Demonstrate your understanding of the `filecmp` module by implementing a detailed file and directory comparison tool. Problem Statement You are tasked with implementing a Python function `compare_directories(directories_list, shallow=True)` that takes a list of directory pairs and performs a file-by-file comparison between each pair. The function should generate a summary report with the following details for each directory pair: 1. **Match Count:** The number of files that are identical in both directories. 2. **Mismatch Count:** The number of files that differ between the directories. 3. **Errors Count:** The number of files that could not be compared due to errors. 4. **Unique to Directory A:** Files that exist only in the first directory of the pair. 5. **Unique to Directory B:** Files that exist only in the second directory of the pair. The function should also clear the cache after each directory pair comparison to ensure accurate results. Input - `directories_list`: A list of tuples, where each tuple contains two paths to directories that need to be compared. Example: `[(\'dir1\', \'dir2\'), (\'dir3\', \'dir4\')]` - `shallow` (optional): A boolean flag to indicate if the comparison should be shallow. Defaults to `True`. Output - A list of dictionaries. Each dictionary corresponds to a directory pair and contains the following keys: - `pair`: The directory pair being compared. - `match_count`: The count of matching files. - `mismatch_count`: The count of mismatched files. - `errors_count`: The count of files with errors during comparison. - `unique_to_dir_a`: List of files unique to the first directory in the pair. - `unique_to_dir_b`: List of files unique to the second directory in the pair. Constraints - Assume that the provided directory paths are valid and accessible. - Both directories in a pair will contain the same set of files for comparison. - Performance considerations: Clear the cache after each directory comparison to manage memory efficiently. Implementation Requirements You are required to use the `filecmp` module, specifically the `cmpfiles` function and the `clear_cache()` method, to implement this function. Example ```python def compare_directories(directories_list, shallow=True): import filecmp results = [] for dir1, dir2 in directories_list: comparison = filecmp.dircmp(dir1, dir2) # Get counts and file lists from the comparison filecmp.clear_cache() match_count = len(comparison.same_files) mismatch_count = len(comparison.diff_files) errors_count = len(comparison.funny_files) unique_to_dir_a = comparison.left_only unique_to_dir_b = comparison.right_only result = { \\"pair\\": (dir1, dir2), \\"match_count\\": match_count, \\"mismatch_count\\": mismatch_count, \\"errors_count\\": errors_count, \\"unique_to_dir_a\\": unique_to_dir_a, \\"unique_to_dir_b\\": unique_to_dir_b } results.append(result) return results # Example usage directories_list = [(\'dir1\', \'dir2\'), (\'dir3\', \'dir4\')] print(compare_directories(directories_list)) ``` In this example, the function compares each pair of directories, extracts the required comparison details, clears the cache, and then stores the results in a list to be returned.","solution":"import filecmp import os def compare_directories(directories_list, shallow=True): Compare a list of directory pairs and provide a summary of the comparison. :param directories_list: List of tuples containing directory pairs to compare. :param shallow: Boolean indicating if the comparison should be shallow. :return: List of dictionaries with comparison results. results = [] for dir1, dir2 in directories_list: comparison = filecmp.dircmp(dir1, dir2) # Get counts and file lists from the comparison match_count = len(comparison.same_files) mismatch_count = len(comparison.diff_files) errors_count = len(comparison.funny_files) unique_to_dir_a = comparison.left_only unique_to_dir_b = comparison.right_only result = { \\"pair\\": (dir1, dir2), \\"match_count\\": match_count, \\"mismatch_count\\": mismatch_count, \\"errors_count\\": errors_count, \\"unique_to_dir_a\\": unique_to_dir_a, \\"unique_to_dir_b\\": unique_to_dir_b } results.append(result) filecmp.clear_cache() return results"},{"question":"**Coding Assessment Question:** # Task Implement a function that recursively copies all files and directories from the source directory to the destination directory with the following constraints: 1. Files with the `.tmp` and `.log` extensions should be excluded from the copying process. 2. Preserve the directory structure in the destination. 3. Maintain file permissions and metadata but not ownership. 4. Handle symbolic links appropriately: - If a symbolic link points to a file, the link itself should be copied, not the file it points to. - If a symbolic link points to a directory, follow the symbolic link and copy the content inside the pointed directory recursively. 5. If any error occurs during the copying process (e.g., due to permission issues), log the error and continue copying other files and directories. # Input - `src` (string): The path to the source directory. - `dst` (string): The path to the destination directory. # Output - Your function does not need to return any value. It should perform the copying operation as described. # Constraints - The `src` directory and all subdirectories may contain a large number of files and directories. - Assume that you have sufficient disk space in `dst`. # Example ```python import os def copy_filtered_directory(src, dst): # Implement your function here pass # Example usage if __name__ == \\"__main__\\": src = \\"/path/to/source_directory\\" dst = \\"/path/to/destination_directory\\" copy_filtered_directory(src, dst) ``` # Additional Information - You may use the `shutil` module for high-level file operations. - Consider using `shutil.copy2()` to copy files while preserving file metadata. - Use `os` and `shutil` module functions to handle directory and symbolic link operations. - Log errors using the `logging` module. # Note Ensure that your implementation is efficient and can handle large directories with deep nested structures. Properly handle exceptions to ensure that the function can continue copying other files even if it encounters errors. ```python import shutil import os import logging logging.basicConfig(level=logging.ERROR, format=\'%(asctime)s - %(levelname)s - %(message)s\') def copy_filtered_directory(src, dst): for dirpath, dirnames, filenames in os.walk(src): # Create the destination directory structure relative_path = os.path.relpath(dirpath, src) destination_path = os.path.join(dst, relative_path) os.makedirs(destination_path, exist_ok=True) # Copy files not matching the exclusion patterns for filename in filenames: if filename.endswith(\\".tmp\\") or filename.endswith(\\".log\\"): continue source_file = os.path.join(dirpath, filename) destination_file = os.path.join(destination_path, filename) try: if os.path.islink(source_file): # Copy the symbolic link itself linkto = os.readlink(source_file) os.symlink(linkto, destination_file) else: # Copy the file and its metadata but not ownership shutil.copy2(source_file, destination_file) except Exception as e: logging.error(f\\"Failed to copy {source_file} to {destination_file}: {e}\\") # Follow symbolic links that point to directories for dirname in dirnames: source_dir = os.path.join(dirpath, dirname) if os.path.islink(source_dir): linkto = os.readlink(source_dir) absolute_link = os.path.abspath(os.path.join(dirpath, linkto)) if os.path.isdir(absolute_link): target_dir = os.path.join(destination_path, dirname) try: copy_filtered_directory(absolute_link, target_dir) except Exception as e: logging.error(f\\"Failed to follow and copy linked directory {source_dir}: {e}\\") else: os.makedirs(os.path.join(destination_path, dirname), exist_ok=True) ```","solution":"import shutil import os import logging logging.basicConfig(level=logging.ERROR, format=\'%(asctime)s - %(levelname)s - %(message)s\') def copy_filtered_directory(src, dst): Recursively copies files and directories from src to dst with specified constraints. for dirpath, dirnames, filenames in os.walk(src): # Create the destination directory structure relative_path = os.path.relpath(dirpath, src) destination_path = os.path.join(dst, relative_path) os.makedirs(destination_path, exist_ok=True) # Copy files not matching the exclusion patterns for filename in filenames: if filename.endswith(\\".tmp\\") or filename.endswith(\\".log\\"): continue source_file = os.path.join(dirpath, filename) destination_file = os.path.join(destination_path, filename) try: if os.path.islink(source_file): # Copy the symbolic link itself linkto = os.readlink(source_file) os.symlink(linkto, destination_file) else: # Copy the file and its metadata but not ownership shutil.copy2(source_file, destination_file) except Exception as e: logging.error(f\\"Failed to copy {source_file} to {destination_file}: {e}\\") # Follow symbolic links that point to directories for dirname in dirnames: source_dir = os.path.join(dirpath, dirname) if os.path.islink(source_dir): linkto = os.readlink(source_dir) absolute_link = os.path.abspath(os.path.join(dirpath, linkto)) if os.path.isdir(absolute_link): target_dir = os.path.join(destination_path, dirname) try: copy_filtered_directory(absolute_link, target_dir) except Exception as e: logging.error(f\\"Failed to follow and copy linked directory {source_dir}: {e}\\") else: os.makedirs(os.path.join(destination_path, dirname), exist_ok=True)"},{"question":"# CSV Data Processing **Objective:** Implement a Python function to process CSV data, demonstrating proficiency in reading, writing, and manipulating CSV files using the `csv` module. **Problem Statement:** You are given a large CSV file containing sales records with the following columns: `\\"Date\\"`, `\\"Product\\"`, `\\"Price\\"`, and `\\"Quantity\\"`. Your task is to write a function `process_sales_data(input_file, output_file)` that reads the input CSV file, calculates the total sales for each product, and writes the result to a new CSV file. The output CSV should have two columns: `\\"Product\\"` and `\\"TotalSales\\"`, sorted by `\\"TotalSales\\"` in descending order. **Function Signature:** ```python def process_sales_data(input_file: str, output_file: str) -> None: pass ``` **Input:** - `input_file` (str): The path to the input CSV file. - The input CSV file will have the following format: ``` Date,Product,Price,Quantity 2023-01-01,ProductA,10.0,2 2023-01-01,ProductB,20.0,1 2023-01-02,ProductA,10.0,1 ... ``` **Output:** - `output_file` (str): The path to the output CSV file. - The output CSV file should have the following format: ``` Product,TotalSales ProductA,30.00 ProductB,20.00 ... ``` Where `TotalSales` is the sum of `Price * Quantity` for each product. **Constraints:** 1. The input CSV file can be very large (up to 10^6 rows). 2. Handle cases where the CSV file might have missing or malformed rows gracefully. 3. Your solution should be efficient in terms of time and space complexity. **Examples:** Suppose the input CSV file `sales.csv` contains the following data: ``` Date,Product,Price,Quantity 2023-01-01,ProductA,10.0,2 2023-01-01,ProductB,20.0,1 2023-01-02,ProductA,10.0,1 2023-01-02,ProductC,15.0,3 ``` After running `process_sales_data(\'sales.csv\', \'totalsales.csv\')`, the output CSV file `totalsales.csv` should contain: ``` Product,TotalSales ProductC,45.00 ProductA,30.00 ProductB,20.00 ``` **Guidelines:** 1. Use the `csv` module for reading and writing CSV files. 2. Use appropriate data structures to store intermediate results and ensure efficient processing. 3. Ensure your code is well-documented and handles edge cases gracefully. 4. Write clear and concise comments explaining your approach.","solution":"import csv from collections import defaultdict def process_sales_data(input_file: str, output_file: str) -> None: Reads the input CSV file, calculates the total sales for each product, and writes the result to a new CSV file. Args: input_file (str): The path to the input CSV file. output_file (str): The path to the output CSV file. Returns: None # Dictionary to store total sales per product sales_data = defaultdict(float) # Read the input CSV file with open(input_file, \'r\') as infile: reader = csv.DictReader(infile) for row in reader: try: product = row[\'Product\'] price = float(row[\'Price\']) quantity = int(row[\'Quantity\']) sales_data[product] += price * quantity except (ValueError, KeyError): # Skip rows with malformed or missing data continue # Prepare the sorted data sorted_sales_data = sorted(sales_data.items(), key=lambda x: x[1], reverse=True) # Write the output CSV file with open(output_file, \'w\', newline=\'\') as outfile: writer = csv.writer(outfile) writer.writerow([\'Product\', \'TotalSales\']) for product, total_sales in sorted_sales_data: writer.writerow([product, f\'{total_sales:.2f}\'])"},{"question":"# Coding Assessment: Create and Apply Custom Seaborn Palettes **Objective:** You will create custom color palettes using seaborn\'s `dark_palette` function and apply these palettes to a variety of plots. This will test your understanding of seaborn\'s palette creation and visualization techniques. **Task:** 1. **Create a Sequential Color Palette:** - Create a sequential color palette starting from a dark gray to \\"seagreen\\". Store this palette in a variable named `palette_seagreen`. 2. **Hex Code Color Palette:** - Create a sequential color palette starting from a dark gray to a color specified by hex code \\"#79C\\". Store this palette in a variable named `palette_hex`. 3. **HUSL Color Palette:** - Create a sequential color palette starting from a dark gray to a color specified by HUSL system values (20, 60, 50). Store this palette in a variable named `palette_husl`. 4. **Increase Number of Colors:** - Create a sequential color palette starting from a dark gray to \\"xkcd:golden\\" with 8 colors. Store this palette in a variable named `palette_golden`. 5. **Continuous Colormap:** - Create a continuous colormap starting from a dark gray to a color specified by the hex code \\"#b285bc\\". Store this colormap in a variable named `cmap_continuous`. 6. **Applying Palettes to Plots:** - Create a dataset (or use an existing one from seaborn, such as `tips` dataset). - Create three types of plots using seaborn (`stripplot`, `boxplot`, `heatmap`), each using one of the created palettes/colormap. - `stripplot`: Use `palette_seagreen`. - `boxplot`: Use `palette_hex`. - `heatmap`: Use `cmap_continuous`. **Input:** - No user input required; you should focus on implementing the functions and plots. **Output:** - Display the three plots as specified. **Constraints and Requirements:** - Use seaborn built-in datasets or create your own sample dataset if needed. - Ensure the plots are appropriately labeled for clarity. **Example:** Here\'s an outline of what your code might look like for creating the palettes and one plot: ```python import seaborn as sns import matplotlib.pyplot as plt # Create the palettes palette_seagreen = sns.dark_palette(\\"seagreen\\") palette_hex = sns.dark_palette(\\"#79C\\") palette_husl = sns.dark_palette((20, 60, 50), input=\\"husl\\") palette_golden = sns.dark_palette(\\"xkcd:golden\\", 8) cmap_continuous = sns.dark_palette(\\"#b285bc\\", as_cmap=True) # Load dataset tips = sns.load_dataset(\\"tips\\") # Create the plots plt.figure(figsize=(10, 6)) # Stripplot plt.subplot(2, 2, 1) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette_seagreen) plt.title(\'Stripplot with Seagreen Palette\') # Boxplot plt.subplot(2, 2, 2) sns.boxplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette_hex) plt.title(\'Boxplot with Hex Palette\') # Heatmap plt.subplot(2, 2, 3) sns.heatmap(tips.pivot_table(index=\'day\', columns=\'time\', values=\'total_bill\', aggfunc=\'mean\').fillna(0), cmap=cmap_continuous) plt.title(\'Heatmap with Continuous Colormap\') plt.tight_layout() plt.show() ``` **Notes:** - Ensure each plot is shown with distinct titles. - Use appropriate seaborn functions to achieve the desired visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_palettes(): Create color palettes and colormap using seaborn\'s dark_palette function. # Sequential color palette from dark gray to \\"seagreen\\" palette_seagreen = sns.dark_palette(\\"seagreen\\", as_cmap=False) # Sequential color palette from dark gray to a color specified by hex code \\"#79C\\" palette_hex = sns.dark_palette(\\"#79C\\", as_cmap=False) # Sequential color palette from dark gray to a color specified by HUSL system values (20, 60, 50) palette_husl = sns.dark_palette((20, 60, 50), input=\\"husl\\", as_cmap=False) # Sequential color palette from dark gray to \\"xkcd:golden\\" with 8 colors palette_golden = sns.dark_palette(\\"xkcd:golden\\", n_colors=8, as_cmap=False) # Continuous colormap from dark gray to a color specified by hex code \\"#b285bc\\" cmap_continuous = sns.dark_palette(\\"#b285bc\\", as_cmap=True) return palette_seagreen, palette_hex, palette_husl, palette_golden, cmap_continuous def create_plots(): Create and display plots using created palettes and colormap. # Load dataset tips = sns.load_dataset(\\"tips\\") # Generate the palettes and colormap palette_seagreen, palette_hex, palette_husl, palette_golden, cmap_continuous = create_palettes() plt.figure(figsize=(15, 10)) # Stripplot with palette_seagreen plt.subplot(2, 2, 1) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette_seagreen) plt.title(\'Stripplot with Seagreen Palette\') # Boxplot with palette_hex plt.subplot(2, 2, 2) sns.boxplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette_hex) plt.title(\'Boxplot with Hex Palette\') # Heatmap with cmap_continuous plt.subplot(2, 2, 3) pivot_table = tips.pivot_table(index=\'day\', columns=\'time\', values=\'total_bill\', aggfunc=\'mean\').fillna(0) sns.heatmap(pivot_table, cmap=cmap_continuous) plt.title(\'Heatmap with Continuous Colormap\') plt.tight_layout() plt.show()"},{"question":"You are given a directory containing several text files with data that need to be compressed incrementally and then stored in a single compressed file. Conversely, you must be able to read and decompress the stored compressed file in an incremental fashion to retrieve and display the original data. # Task Write the following functions using the `bz2` module: 1. **compress_files(directory, output_filename, compresslevel=9):** - **Input:** - `directory` (str): The path to the directory containing the text files. - `output_filename` (str): The name of the file where the compressed data will be stored. - `compresslevel` (int): The level of compression (between 1 and 9). Default is 9. - **Output:** None - **Behavior:** - Read all files from the given directory (assume text files only). - Compress the contents incrementally and store them in the specified output file. 2. **decompress_file(input_filename):** - **Input:** - `input_filename` (str): The name of the file containing the compressed data. - **Output:** None - **Behavior:** - Read and decompress the contents incrementally from the given file. - Print the decompressed contents to the console as they are read. # Constraints - The size of each text file is limited to 1 MB. - The directory will contain no more than 100 files. - Assume all text files are encoded in UTF-8. # Example ```python # Example usage compress_files(\\"texts_directory\\", \\"compressed_output.bz2\\", compresslevel=5) decompress_file(\\"compressed_output.bz2\\") ``` Your implementation should efficiently handle both the compression and decompression processes, leveraging the incremental capabilities of the `bz2` module.","solution":"import os import bz2 def compress_files(directory, output_filename, compresslevel=9): Compresses all text files in the given directory and stores them in a single compressed file. with bz2.BZ2File(output_filename, \'wb\', compresslevel=compresslevel) as output_file: for filename in sorted(os.listdir(directory)): filepath = os.path.join(directory, filename) if os.path.isfile(filepath) and filepath.endswith(\\".txt\\"): with open(filepath, \'rb\') as file: while (data := file.read(1024)): output_file.write(data) def decompress_file(input_filename): Decompresses the contents of the given file and prints it to the console. with bz2.BZ2File(input_filename, \'rb\') as input_file: while (data := input_file.read(1024)): print(data.decode(\'utf-8\'), end=\'\')"},{"question":"# Question: Implementing a Custom Pandas ExtensionArray Objective You are tasked with creating a custom `ExtensionArray` in pandas. This will involve defining a new data type and creating functionalities to handle various operations. Requirements 1. Implement a class `MyCustomArray` that inherits from `pandas.api.extensions.ExtensionArray`. 2. Implement the following methods for `MyCustomArray`: - `__init__` - `_from_sequence` - `_from_factorized` - `_concat_same_type` - `__getitem__` - `__len__` - `isna` - `copy` - `astype` - `nbytes` 3. Register the custom dtype using `register_extension_dtype`. Details - `__init__`: Initialize your custom array. Store the data in a suitable internal structure. - `_from_sequence`: Class method to construct an array from a sequence of scalars. - `_from_factorized`: Construct an array by taking a sequence of unique values and an integer array of positions. - `_concat_same_type`: Concatenate multiple arrays of this type. - `__getitem__`: Retrieve a value or slice from the array. - `__len__`: Return the length of the array. - `isna`: Boolean array indicating if each value is missing. - `copy`: Return a copy of the array. - `astype`: Cast the array to the specified dtype. - `nbytes`: Return the number of bytes consumed by the array. Constraints - Utilize built-in Python types and pandas methods where appropriate. - Implement proper error handling for edge cases (e.g., indexing out of bounds). ```python import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype, register_extension_dtype class MyCustomDtype(ExtensionDtype): name = \'mycustom\' type = int kind = \'O\' na_value = None @classmethod def construct_array_type(cls): return MyCustomArray class MyCustomArray(ExtensionArray): def __init__(self, data): self.data = data @classmethod def _from_sequence(cls, scalars): return cls(list(scalars)) @classmethod def _from_factorized(cls, uniques, original): return cls(uniques[original]) @classmethod def _concat_same_type(cls, to_concat): data = sum([arr.data for arr in to_concat], []) return cls(data) def __getitem__(self, item): return self.data[item] def __len__(self): return len(self.data) def isna(self): return [item is None for item in self.data] def copy(self): return MyCustomArray(self.data.copy()) def astype(self, dtype, copy=True): return MyCustomArray([dtype(item) for item in self.data]) @property def nbytes(self): return sum(sys.getsizeof(item) for item in self.data) register_extension_dtype(MyCustomDtype) # Example Usage data = MyCustomArray([1, 2, None, 4]) print(data.isna()) # [False, False, True, False] print(data.astype(float)) # MyCustomArray containing [1.0, 2.0, None, 4.0] ``` Expected Output - Creating an instance of `MyCustomArray` should correctly handle the provided data. - Calling `data.isna()` should return a boolean list indicating missing values. - Calling `data.astype(float)` should return a new `MyCustomArray` with each element cast to float. - Other implemented methods should conform to their specified behavior and handle edge cases gracefully.","solution":"import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype, register_extension_dtype import sys class MyCustomDtype(ExtensionDtype): name = \'mycustom\' type = int kind = \'O\' na_value = None @classmethod def construct_array_type(cls): return MyCustomArray class MyCustomArray(ExtensionArray): def __init__(self, data): self.data = data @classmethod def _from_sequence(cls, scalars): return cls(list(scalars)) @classmethod def _from_factorized(cls, uniques, original): return cls([uniques[i] for i in original]) @classmethod def _concat_same_type(cls, to_concat): data = sum([list(arr) for arr in to_concat], []) return cls(data) def __getitem__(self, item): if isinstance(item, slice) or isinstance(item, list): return MyCustomArray(self.data[item]) return self.data[item] def __len__(self): return len(self.data) def isna(self): return [item is None for item in self.data] def copy(self): return MyCustomArray(self.data.copy()) def astype(self, dtype, copy=True): return MyCustomArray([dtype(item) if item is not None else None for item in self.data]) @property def nbytes(self): return sum(sys.getsizeof(item) for item in self.data) register_extension_dtype(MyCustomDtype)"},{"question":"# Advanced Sorting and Custom Object Sorting in Python You are given a list of products where each product is represented as a dictionary with the following keys: - `name`: a string representing the name of the product. - `price`: a float representing the price of the product. - `rating`: an integer representing the rating of the product (1 to 5). - `category`: a string representing the category of the product. Your task is to implement a function `sort_products(products: List[Dict[str, Any]], sort_by: List[Tuple[str, bool]]) -> List[Dict[str, Any]]` that sorts this list of products based on multiple criteria provided in the `sort_by` parameter. # Input - `products`: A list of dictionaries, where each dictionary represents a product with the keys `name`, `price`, `rating`, and `category`. - `sort_by`: A list of tuples, where each tuple contains a string representing the field to sort by and a boolean representing the sort order (`True` for ascending, `False` for descending). # Output - A list of dictionaries sorted based on the provided criteria. # Constraints - The list `products` can contain up to 10,000 entries. - Each product dictionary always contains all four keys. # Example ```python products = [ {\'name\': \'Product1\', \'price\': 50.0, \'rating\': 4, \'category\': \'Electronics\'}, {\'name\': \'Product2\', \'price\': 30.0, \'rating\': 5, \'category\': \'Books\'}, {\'name\': \'Product3\', \'price\': 20.0, \'rating\': 3, \'category\': \'Books\'}, {\'name\': \'Product4\', \'price\': 50.0, \'rating\': 4, \'category\': \'Electronics\'} ] sort_by = [(\'price\', True), (\'rating\', False)] sorted_products = sort_products(products, sort_by) print(sorted_products) ``` # Expected Output ```python [ {\'name\': \'Product3\', \'price\': 20.0, \'rating\': 3, \'category\': \'Books\'}, {\'name\': \'Product2\', \'price\': 30.0, \'rating\': 5, \'category\': \'Books\'}, {\'name\': \'Product4\', \'price\': 50.0, \'rating\': 4, \'category\': \'Electronics\'}, {\'name\': \'Product1\', \'price\': 50.0, \'rating\': 4, \'category\': \'Electronics\'} ] ``` # Requirements 1. Implement the function `sort_products()` to sort the input list based on the provided sorting criteria. 2. Make sure the sorted list preserves the relative order for entries with the same sort key, ensuring stability. 3. Handle multiple levels of sorting using the `sort_by` parameter. # Hints - Use the `sorted()` or `list.sort()` methods for sorting. - Utilize the `operator.attrgetter()` or `operator.itemgetter()` from the `operator` module for convenience in accessing dictionary fields. - Perform stability by chaining sorts, starting from the least significant sort key to the most significant.","solution":"from typing import List, Dict, Any, Tuple def sort_products(products: List[Dict[str, Any]], sort_by: List[Tuple[str, bool]]) -> List[Dict[str, Any]]: Sorts a list of products based on multiple criteria. :param products: List of dictionaries, where each dictionary represents a product. :param sort_by: List of tuples, each containing the field to sort by and a boolean for order (True for ascending). :return: Sorted list of dictionaries. for key, ascending in reversed(sort_by): products.sort(key=lambda x: x[key], reverse=not ascending) return products"},{"question":"Objective Write a Python function that processes an email message by extracting and printing specific parts based on MIME types. This demonstrates your understanding of email message trees, iterators, and MIME type handling. Problem Statement You are given an email message object, and your task is to write a function `process_email_message(msg: Message, maintype: str, subtype: str=None) -> None`. This function should: 1. Identify and print all lines from subparts of the message that match the specified MIME type using `email.iterators.body_line_iterator`. 2. Iterate over and print the MIME types of the subparts using `email.iterators.typed_subpart_iterator`. You need to use the functions from the `email.iterators` module effectively to achieve this task. Function Signature ```python def process_email_message(msg: Message, maintype: str, subtype: str=None) -> None: ``` Input - `msg`: An email message object of type `Message`. - `maintype`: A string specifying the main MIME type to filter the subparts (e.g., \'text\'). - `subtype`: An optional string specifying the subtype to filter the subparts (e.g., \'plain\'). Defaults to `None`. Output - The function does not return anything. It prints the lines of text and the MIME type of subparts. Example Given an email message object `msg`: ```python from email.message import EmailMessage msg = EmailMessage() msg.set_content(\\"This is a plain text email.\\") ``` Calling your function: ```python process_email_message(msg, maintype=\'text\', subtype=\'plain\') ``` Constraints 1. You should use `email.iterators.body_line_iterator` for extracting the body lines. 2. You should use `email.iterators.typed_subpart_iterator` for iterating over subparts by MIME type. 3. Handle cases where the specified MIME types do not exist in the message gracefully. Notes 1. Assume the input `msg` object is always a valid `Message` object. 2. For testing purposes, you can construct `msg` using the `EmailMessage` and `set_content` methods or any suitable means from the `email` package. 3. You can use the `print` function for outputting the required lines and MIME types. 4. Provide appropriate exception handling to deal with potential issues in processing the message.","solution":"from email import message_from_string from email.iterators import body_line_iterator, typed_subpart_iterator def process_email_message(msg, maintype, subtype=None): Processes the email message by extracting and printing specific parts based on MIME types. Parameters: msg (Message): An email message object of type Message. maintype (str): The main MIME type to filter the subparts. subtype (str): The subtype to filter the subparts. Defaults to None. Returns: None # Iterate over the subparts with the specified MIME types for part in typed_subpart_iterator(msg, maintype, subtype): # Print the MIME type of each subpart print(f\\"MIME type: {part.get_content_type()}\\") # Print the lines from the subpart body for line in body_line_iterator(part): print(line)"},{"question":"# Question: Analyze and Manipulate Text Data Using `collections.Counter` You are tasked with analyzing a piece of text to gain insights into the frequency of words used. Your objective is to implement a function called `word_frequency_analysis` which takes a string as input and returns the top N most common words along with their counts. The function signature should be: ```python def word_frequency_analysis(text: str, top_n: int) -> list: pass ``` # Input: - `text` (str): A string containing arbitrary text. - The text may include punctuation and varying cases. - `top_n` (int): The number of top words to return. # Output: - A list of tuples, where each tuple contains a word (str) and its count (int), sorted by the counts in descending order. If two words have the same count, they should appear in alphabetical order. # Constraints: - You must handle punctuation and case correctly. - The text may contain multiple spaces or newline characters. - Words are case-insensitive (e.g., \\"Python\\" and \\"python\\" should be counted as the same word). # Example: ```python text = \\"Python is amazing. The Python programming language is amazing, isn\'t it?\\" top_n = 3 print(word_frequency_analysis(text, top_n)) # Expected Output: [(\'amazing\', 2), (\'is\', 2), (\'python\', 2)] ``` # Requirements: 1. Use the `collections.Counter` class to accomplish this task. 2. Ensure your solution is efficient and handles large texts gracefully. # Hints: - Consider using Python\'s string methods and the `re` module to handle punctuation and case normalization. - Think about how to sort the `Counter` results appropriately for the final output. Good luck!","solution":"import re from collections import Counter def word_frequency_analysis(text: str, top_n: int) -> list: Analyzes the frequency of words in a given text and returns the top N most common words. Parameters: text (str): The input text to be analyzed. top_n (int): The number of top words to return. Returns: list: A list of tuples where each tuple contains a word and its count, sorted by the counts in descending order and alphabetically for words with the same count. # Normalize the text: convert to lowercase and remove punctuation text = re.sub(r\'[^ws]\', \'\', text.lower()) # Split the text into words words = text.split() # Count the frequency of each word counter = Counter(words) # Get the most common words, sorted by count in descending order and alphabetically for ties common_words = counter.most_common() common_words_sorted = sorted(common_words, key=lambda pair: (-pair[1], pair[0])) # Return the top N words return common_words_sorted[:top_n]"},{"question":"**Question Title:** Advanced Data Visualization with Seaborn **Problem Statement:** You are tasked with creating a series of visualizations to analyze a dataset using the Seaborn library. Use the `fmri` dataset provided by Seaborn to accomplish the following tasks. Ensure your code is well-commented and follows best practices. **Requirements:** 1. Load the `fmri` dataset from Seaborn. 2. Create a line plot of the `fmri` dataset showing the `signal` over `timepoint`, with different lines representing different `regions`. 3. Replicate the line plot in task 2, but this time include an error band representing the variation of the `signal` within each group of `event`. 4. Add markers to the plot created in task 3 to indicate individual data points. 5. Customize your plot to improve readability: - Use different colors for different `regions`. - Use different line styles for different `events`. - Add appropriate labels and titles to the axes and plot. **Input Format:** - Use the `load_dataset` function to load the `fmri` dataset directly from Seaborn. **Output Format:** - The output should be a series of plots displayed inline (if using Jupyter Notebook). **Constraints:** - Use Seaborn\'s `seaborn.objects as so` for creating the plots. - Ensure the error bands and markers are clearly visible in the plots. **Performance Requirements:** - The code should be efficient and run within a reasonable amount of time for typical dataset sizes provided by Seaborn. **Example Code Structure:** ```python # Import necessary libraries import seaborn.objects as so from seaborn import load_dataset # Load the fmri dataset fmri = load_dataset(\\"fmri\\") # Task 1: Create a line plot plot1 = so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\").add(so.Line()) plot1.show() # Task 2: Create a line plot with error bands plot2 = ( so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") ) plot2.show() # Task 3: Add markers to the plot with error bands plot3 = ( so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) .add(so.Band(), so.Est(), group=\\"event\\") ) plot3.show() # Task 4: Customize the plot plot4 = ( so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) .add(so.Band(), so.Est(), group=\\"event\\") .options(title=\\"fMRI Signal over Time\\", xlabel=\\"Timepoint\\", ylabel=\\"Signal\\", legend=True) ) plot4.show() ``` Ensure you execute your code in an environment where plotting inline is supported, such as Jupyter Notebook. **Evaluation Criteria:** - Correctly loading and manipulating the dataset. - Accurate creation and customization of the plots. - Clear, readable, and well-commented code. - Proper use of Seaborn\'s advanced plotting functionalities.","solution":"# Import necessary libraries import seaborn as sns import matplotlib.pyplot as plt def create_fmri_plots(): # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Task 1: Create a line plot of the fmri dataset showing the signal over timepoint for different regions plt.figure(figsize=(10, 6)) sns.lineplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\") plt.title(\'fMRI Signal over Time for Different Regions\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.legend(title=\'Region\') plt.show() # Task 2: Create a line plot with error bands plt.figure(figsize=(10, 6)) sns.lineplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", ci=\\"sd\\") plt.title(\'fMRI Signal over Time with Error Bands\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.legend(title=\'Region / Event\') plt.show() # Task 3: Add markers to the plot with error bands plt.figure(figsize=(10, 6)) sns.lineplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, ci=\\"sd\\") plt.title(\'fMRI Signal over Time with Error Bands and Markers\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.legend(title=\'Region / Event\') plt.show() # Task 4: Customize the plot to improve readability plt.figure(figsize=(10, 6)) sns.lineplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, ci=\\"sd\\") plt.title(\'Customized fMRI Signal over Time Plot\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.legend(title=\'Region and Event\', loc=\'best\') plt.show()"},{"question":"# GenericAlias Simulator **Objective:** Implement a simplified version of a `GenericAlias` type in Python, which supports basic generic type functionality similar to the `types.GenericAlias` introduced in Python 3.9. Description: You need to create a `GenericAliasSimulator` class that simulates the behavior of a generic alias in Python: 1. **Initialization:** - The class should accept two parameters: `origin`, which should be a type, and `args`, which can be any type or a tuple of types. - If `args` is not a tuple, wrap it in a tuple. 2. **Attributes:** - `__origin__`: Set this attribute to the `origin` argument. - `__args__`: Set this attribute to the `args` argument. - `__parameters__`: This should be computed lazily the first time it is accessed. This attribute should be formed by iterating over `__args__` and collecting all elements. 3. **Methods:** - You should override the `__repr__()` method to provide a human-readable representation of the `GenericAliasSimulator` instance in the format: `GenericAliasSimulator(origin, args)`. - Implement a class method named `from_generic()`, which takes an instance of `GenericAliasSimulator` and returns a new instance of the class with the same `origin` and `args`. Constraints: - The `origin` parameter must be a Python type (e.g., `int`, `list`). - The `args` parameter can be any valid Python type or a tuple of types (e.g., `int`, `(int, str)`). Example Usages: ```python # Creating a GenericAliasSimulator instance ga1 = GenericAliasSimulator(list, (int,)) print(ga1) # Output: GenericAliasSimulator(list, (int,)) print(ga1.__origin__) # Output: <class \'list\'> print(ga1.__args__) # Output: (<class \'int\'>,) print(ga1.__parameters__) # Output: (<class \'int\'>,) # Creating another instance using from_generic() ga2 = GenericAliasSimulator.from_generic(ga1) print(ga2) # Output: GenericAliasSimulator(list, (int,)) print(ga2.__origin__) # Output: <class \'list\'> print(ga2.__args__) # Output: (<class \'int\'>,) print(ga2.__parameters__) # Output: (<class \'int\'>,) ``` Your Task: Implement the `GenericAliasSimulator` class according to the above specifications. Ensure that the class handles different input scenarios correctly and that the `__parameters__` attribute is lazy-loaded.","solution":"class GenericAliasSimulator: def __init__(self, origin, args): if not isinstance(origin, type): raise TypeError(\\"The origin must be a type.\\") if not isinstance(args, tuple): args = (args,) self.__origin__ = origin self.__args__ = args self.__parameters = None @property def __parameters__(self): if self.__parameters is None: self.__parameters = tuple(self.__args__) return self.__parameters def __repr__(self): return f\\"GenericAliasSimulator({self.__origin__.__name__}, {self.__args__})\\" @classmethod def from_generic(cls, other): if not isinstance(other, GenericAliasSimulator): raise TypeError(\\"Expected an instance of GenericAliasSimulator.\\") return cls(other.__origin__, other.__args__) # Example usage (not part of the solution code) #ga1 = GenericAliasSimulator(list, (int,)) #print(ga1) # Output: GenericAliasSimulator(list, (int,)) #print(ga1.__origin__) # Output: <class \'list\'> #print(ga1.__args__) # Output: (<class \'int\'>,) #print(ga1.__parameters__) # Output: (<class \'int\'>,) # #ga2 = GenericAliasSimulator.from_generic(ga1) #print(ga2) # Output: GenericAliasSimulator(list, (int,)) #print(ga2.__origin__) # Output: <class \'list\'> #print(ga2.__args__) # Output: (<class \'int\'>,) #print(ga2.__parameters__) # Output: (<class \'int\'>,)"},{"question":"# Email Retrieval and Processing with `imaplib` Objective You are required to write a Python program using the `imaplib` module to connect to an IMAP email server, authenticate, and perform several operations such as retrieving emails based on specific criteria, processing attachments, and managing mailboxes. Instructions 1. **Establish Connection:** - Connect to an IMAP server using the `IMAP4_SSL` class. - Use the `login` method to authenticate the user. - Ensure the connection is secure by handling potential exceptions and using TLS. 2. **Retrieve Unread Emails:** - Select the \\"INBOX\\" mailbox. - Use the `search` method to find all unread emails. - Fetch the email headers (From, Subject) and print them. 3. **Process Attachments:** - For each unread email, check if it contains attachments. - If attachments exist, download them to a specified local directory. 4. **Move Processed Emails:** - After processing, move the emails to a different folder named \\"Processed\\". Input - Your program should read the following information from the user: - Email server host. - Email address (user). - Password. - Local directory path for saving attachments. Output - Print headers (From, Subject) of unread emails. - Save attachments to the specified local directory. - Print confirmation messages for each processed email. Constraints - Assume a stable internet connection. - Handle common exceptions (e.g., server connection issues, authentication errors). - Ensure that no modifications are made to emails that fail any step. Example Execution ```python Please enter your email server host: imap.yourdomain.com Please enter your email address: user@yourdomain.com Please enter your password: ******** Please enter the local directory path to save attachments: /path/to/save/attachments Connecting to IMAP server... Authenticating... Selecting \\"INBOX\\"... Retrieving unread emails... Email from: sender@example.com, Subject: Test Email 1 Downloading attachments to /path/to/save/attachments... Email processed and moved to \\"Processed\\" folder. Email from: sender2@example.com, Subject: Test Email 2 Downloading attachments to /path/to/save/attachments... Email processed and moved to \\"Processed\\" folder. ``` The student needs to implement the program ensuring it properly handles connection security, email retrieval, processing of attachments, and moving emails to another folder.","solution":"import imaplib import email import os import getpass def connect_to_imap_server(host, user, password): try: mail = imaplib.IMAP4_SSL(host) mail.login(user, password) return mail except imaplib.IMAP4.error as e: print(\\"Failed to authenticate:\\", e) return None def retrieve_unread_emails(mail): try: mail.select(\\"inbox\\") status, messages = mail.search(None, \'UNSEEN\') return messages[0].split() except Exception as e: print(\\"Error retrieving unread emails:\\", e) return [] def process_attachments(mail, email_id, local_dir): status, data = mail.fetch(email_id, \'(RFC822)\') if status != \'OK\': print(\\"Failed to fetch email:\\", email_id) return msg = email.message_from_bytes(data[0][1]) for part in msg.walk(): if part.get_content_maintype() == \'multipart\': continue if part.get(\'Content-Disposition\') is None: continue filename = part.get_filename() if bool(filename): filepath = os.path.join(local_dir, filename) with open(filepath, \'wb\') as f: f.write(part.get_payload(decode=True)) print(f\\"Attachment {filename} downloaded to {local_dir}\\") def move_processed_email(mail, email_id): try: result = mail.copy(email_id, \\"Processed\\") if result[0] == \'OK\': mail.store(email_id, \'+FLAGS\', \'Deleted\') mail.expunge() print(f\\"Email {email_id} processed and moved to \'Processed\' folder.\\") else: print(f\\"Failed to move email {email_id}\\") except Exception as e: print(\\"Error moving processed emails:\\", e) def main(): host = input(\\"Please enter your email server host: \\") user = input(\\"Please enter your email address: \\") password = getpass.getpass(\\"Please enter your password: \\") local_dir = input(\\"Please enter the local directory path to save attachments: \\") mail = connect_to_imap_server(host, user, password) if not mail: return unread_emails = retrieve_unread_emails(mail) for email_id in unread_emails: status, data = mail.fetch(email_id, \'(RFC822)\') msg = email.message_from_bytes(data[0][1]) email_from = msg[\'from\'] email_subject = msg[\'subject\'] print(f\\"Email from: {email_from}, Subject: {email_subject}\\") process_attachments(mail, email_id, local_dir) move_processed_email(mail, email_id) mail.logout() if __name__ == \\"__main__\\": main()"},{"question":"# Custom Module Transformation Using `torch.fx` **Objective**: Design and implement a custom transformation function for a neural network model using `torch.fx`. This transformation will replace all instances of `torch.nn.ReLU` activations with `torch.nn.Sigmoid` activations within a given `torch.nn.Module`. The task assesses your ability to understand and manipulate computation graphs using the `torch.fx` toolkit. **Function Signature**: ```python def transform_relu_to_sigmoid(m: torch.nn.Module) -> torch.nn.Module: Transform the given module by replacing all ReLU activations with Sigmoid activations. Args: m (torch.nn.Module): The input module to be transformed. Returns: torch.nn.Module: The transformed module with Sigmoid activations replacing ReLU activations. ``` **Task**: 1. **Symbolic Trace**: Obtain the computation graph of the given module using symbolic tracing. 2. **Graph Manipulation**: Replace all occurrences of `torch.nn.ReLU` with `torch.nn.Sigmoid` within the computation graph. 3. **Recompile and Return**: Construct a new `torch.nn.Module` with the modified graph and return it. **Requirements**: - Ensure the transformation process correctly replaces only `ReLU` activations. - Maintain the original module\'s functionality otherwise. - Include necessary imports and helper functions if required. - Return the transformed module at the end. **Example Usage**: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear1 = nn.Linear(10, 20) self.relu1 = nn.ReLU() self.linear2 = nn.Linear(20, 5) self.relu2 = nn.ReLU() def forward(self, x): x = self.linear1(x) x = self.relu1(x) x = self.linear2(x) x = self.relu2(x) return x model = SimpleModel() print(\\"Original Model:\\") print(model) transformed_model = transform_relu_to_sigmoid(model) print(\\"Transformed Model:\\") print(transformed_model) ``` **Expected Output Example**: ```plaintext Original Model: SimpleModel( (linear1): Linear(in_features=10, out_features=20, bias=True) (relu1): ReLU() (linear2): Linear(in_features=20, out_features=5, bias=True) (relu2): ReLU() ) Transformed Model: SimpleModel( (linear1): Linear(in_features=10, out_features=20, bias=True) (relu1): Sigmoid() (linear2): Linear(in_features=20, out_features=5, bias=True) (relu2): Sigmoid() ) ``` **Constraints**: - You may assume that the module to be transformed does not contain nested submodules with `ReLU` activations. - Focus on using `torch.fx` features for graph manipulation. **Performance Note**: Ensure that the transformed module\'s functionality and performance remain close to the original model post-transformation.","solution":"import torch import torch.nn as nn from torch.fx import symbolic_trace def transform_relu_to_sigmoid(m: nn.Module) -> nn.Module: Transform the given module by replacing all ReLU activations with Sigmoid activations. Args: m (torch.nn.Module): The input module to be transformed. Returns: torch.nn.Module: The transformed module with Sigmoid activations replacing ReLU activations. class ModuleRewriter(torch.fx.Transformer): def call_module(self, target, args, kwargs): if isinstance(self.module.get_submodule(target), nn.ReLU): # Replace ReLU with Sigmoid new_submodule = nn.Sigmoid() self.module.add_submodule(target, new_submodule) return self.call_module(target, args, kwargs) return super().call_module(target, args, kwargs) # Symbolic trace the module to get an FX GraphModule traced = symbolic_trace(m) # Transform the graph transformer = ModuleRewriter(traced) new_graph_module = transformer.transform() return new_graph_module"},{"question":"Objective: Design a function that creates a line plot from given x and y data, with specific control over plot limits using seaborn\'s `seaborn.objects` module. Function Signature: ```python def create_custom_plot(x: list, y: list, x_limits: tuple = None, y_limits: tuple = None): Creates a line plot using seaborn with custom x and y limits. Parameters: x (list): A list of numerical values representing the x-axis data points. y (list): A list of numerical values representing the y-axis data points. x_limits (tuple, optional): A tuple (min, max) to set the limits on the x-axis. Default is None. y_limits (tuple, optional): A tuple (min, max) to set the limits on the y-axis. Default is None. Returns: seaborn.objects.Plot: The generated line plot with specified limits. pass ``` Requirements: 1. Use the `so.Plot` function from the `seaborn.objects` module to create a line plot that displays markers at each data point. 2. Accept optional parameters `x_limits` and `y_limits` to customize the limits of the x and y axes, respectively: - If `x_limits` is not `None`, use it to set the limits for the x-axis. - If `y_limits` is not `None`, use it to set the limits for the y-axis. 3. If either limit value is `None`, maintain the default seaborn margin for that axis. Constraints: - The lengths of the `x` and `y` lists will always be the same. - The `x` and `y` lists will contain at least one element. Example: ```python # Example 1 x = [1, 2, 3] y = [1, 3, 2] plot = create_custom_plot(x, y, x_limits=(0, 4), y_limits=(-1, 6)) # This should create a line plot with x-axis limits from 0 to 4 and y-axis limits from -1 to 6 # Example 2 plot = create_custom_plot(x, y, y_limits=(0, 4)) # This should create a line plot with the default x-axis limits and y-axis limits from 0 to 4 ```","solution":"import seaborn.objects as so def create_custom_plot(x: list, y: list, x_limits: tuple = None, y_limits: tuple = None): Creates a line plot using seaborn with custom x and y limits. Parameters: x (list): A list of numerical values representing the x-axis data points. y (list): A list of numerical values representing the y-axis data points. x_limits (tuple, optional): A tuple (min, max) to set the limits on the x-axis. Default is None. y_limits (tuple, optional): A tuple (min, max) to set the limits on the y-axis. Default is None. Returns: seaborn.objects.Plot: The generated line plot with specified limits. plot = so.Plot(x=x, y=y).add(so.Line()).add(so.Dot()) if x_limits: plot = plot.scale(x=x_limits) if y_limits: plot = plot.scale(y=y_limits) return plot"},{"question":"**Objective:** Implement and utilize an `asyncio.Queue` to simulate a simple task management system where multiple workers process tasks concurrently. Also, incorporate the use of `asyncio.PriorityQueue` to handle tasks based on their priority and demonstrate understanding of essential queue methods and task management. **Task:** You are required to write a Python script that: 1. Creates an `asyncio.Queue` and an `asyncio.PriorityQueue`. 2. Spawns multiple worker tasks that process items from these queues concurrently. 3. Demonstrates adding tasks with different priorities to the `PriorityQueue` and simple tasks to the `Queue`. 4. Uses `task_done()` and `join()` methods to ensure all tasks are processed before the script completes execution. 5. Handles exceptions where applicable. **Details:** 1. Implement an asynchronous function `create_tasks(num_simple, num_priority)` that creates `num_simple` tasks and puts them into a regular queue with random durations, and `num_priority` tasks with random priorities into a priority queue. 2. Implement an asynchronous function `worker(name, queue)` that continuously retrieves tasks from the given queue, simulates task processing by sleeping for the task duration, and then calls `task_done()`. 3. Implement the `main` function that: - Creates both `Queue` and `PriorityQueue`. - Uses `create_tasks` to populate the queues. - Creates and starts multiple workers to process tasks from both queues. - Ensures all tasks are processed using `join()`. - Cancels the worker tasks gracefully after all tasks are completed. **Input Parameters:** - `num_workers`: The number of worker tasks to create. - `num_simple_tasks`: The number of simple tasks to add to the regular queue. - `num_priority_tasks`: The number of tasks to add to the priority queue. **Example Output:** The script should output statements indicating which worker is processing which task, handle priorities correctly, and provide a summary of the total processing time. **Constraints:** - Ensure that tasks within the `PriorityQueue` are processed in the correct priority order (lower priority number first). - Use a bounded queue (e.g., maxsize=10) for at least one of the queues to demonstrate blocking behavior. Example Code Structure: ```python import asyncio import random import time async def create_tasks(queue, p_queue, num_simple, num_priority): # Populate queues with tasks pass async def worker(name, queue): # Process tasks from queue pass async def main(num_workers, num_simple_tasks, num_priority_tasks): queue = asyncio.Queue(maxsize=10) p_queue = asyncio.PriorityQueue() await create_tasks(queue, p_queue, num_simple_tasks, num_priority_tasks) workers = [] for i in range(num_workers): workers.append(asyncio.create_task(worker(f\'worker-{i}\', queue))) workers.append(asyncio.create_task(worker(f\'p_worker-{i}\', p_queue))) await queue.join() await p_queue.join() for worker in workers: worker.cancel() await asyncio.gather(*workers, return_exceptions=True) print(\\"All tasks processed.\\") if __name__ == \\"__main__\\": asyncio.run(main(num_workers=5, num_simple_tasks=10, num_priority_tasks=5)) ``` **Note:** Ensure to handle `QueueFull` and `QueueEmpty` exceptions where applicable, and use `asyncio.wait_for` for any operation that may require timeout handling.","solution":"import asyncio import random async def create_tasks(queue, p_queue, num_simple, num_priority): for i in range(num_simple): duration = random.uniform(0.1, 1.0) await queue.put((i, duration)) for i in range(num_priority): priority = random.randint(1, 10) duration = random.uniform(0.1, 1.0) await p_queue.put((priority, (i, duration))) async def worker(name, queue): while True: try: priority_item = await queue.get() if isinstance(priority_item, tuple) and len(priority_item) == 2: task_id, duration = priority_item else: _, (task_id, duration) = priority_item print(f\\"{name} processing task {task_id} with duration {duration}\\") await asyncio.sleep(duration) queue.task_done() except asyncio.CancelledError: break async def main(num_workers, num_simple_tasks, num_priority_tasks): queue = asyncio.Queue(maxsize=10) p_queue = asyncio.PriorityQueue() await create_tasks(queue, p_queue, num_simple_tasks, num_priority_tasks) workers = [] for i in range(num_workers): workers.append(asyncio.create_task(worker(f\'worker-{i}\', queue))) workers.append(asyncio.create_task(worker(f\'p_worker-{i}\', p_queue))) await queue.join() await p_queue.join() for worker in workers: worker.cancel() await asyncio.gather(*workers, return_exceptions=True) print(\\"All tasks processed.\\") if __name__ == \\"__main__\\": asyncio.run(main(num_workers=5, num_simple_tasks=10, num_priority_tasks=5))"},{"question":"**Problem: Calculating Modified Sum of Custom Function Series** In this task, you are required to implement a function that calculates the sum of a series of custom functions. The series involves advanced mathematical functions from the `torch.special` module. You need to use a combination of the Bessel, gamma, and error functions to calculate each term in the series. # Function Signature ```python def custom_series_sum(n: int) -> torch.Tensor: ``` # Parameters - `n` (int): The number of terms in the series. (1 leq n leq 1000) # Returns - `torch.Tensor`: A tensor containing the final summed value of the series. # Implementation Details 1. The (i)-th term (T(i)) of the series should be calculated using the following formula: [ T(i) = text{bessel_j1}(text{airy_ai}(i)^2) + text{digamma}(text{erf}(i) + 1) ] 2. Sum these terms from (i=1) to (i=n). 3. Return the final summed value as a torch tensor. # Example ```python import torch result = custom_series_sum(5) print(result) ``` # Constraints - Use the functions from the `torch.special` module. - Ensure efficient computation for up to 1000 terms. # Note: Make sure that your implementation correctly handles the tensor operations for these special functions, preserving PyTorch\'s computational graph for any potential gradient computations. ```python import torch def custom_series_sum(n: int) -> torch.Tensor: sum_series = torch.tensor(0.0) for i in range(1, n + 1): term = torch.special.bessel_j1(torch.special.airy_ai(i) ** 2) + torch.special.digamma(torch.special.erf(i) + 1) sum_series += term return sum_series # Example usage result = custom_series_sum(5) print(result) ```","solution":"import torch def custom_series_sum(n: int) -> torch.Tensor: sum_series = torch.tensor(0.0) for i in range(1, n + 1): airy_ai_val = torch.special.airy_ai(torch.tensor([i]))[0] bessel_j1_val = torch.special.bessel_j1(airy_ai_val ** 2) erf_val = torch.special.erf(torch.tensor([i]))[0] digamma_val = torch.special.digamma(erf_val + 1) term = bessel_j1_val + digamma_val sum_series += term return sum_series"},{"question":"# Custom Type Implementation with Python C API **Objective:** Implement a custom type in Python using the Python C API. This type will be called `MyCustomNumber`, and it will mimic the behavior of integers with additional custom methods. # Requirements: 1. **Type Definition:** - Define a structure for `MyCustomNumber` that contains the necessary fields. 2. **Initialization:** - Implement an `init` method to initialize the value of `MyCustomNumber` objects. 3. **Addition Operation:** - Implement the addition operation (`+`) for `MyCustomNumber` instances. 4. **Comparison Operations:** - Implement comparison operations (`<`, `<=`, `==`, `!=`, `>=`, `>`) for `MyCustomNumber`. 5. **Documentation:** - Provide appropriate docstrings for the type and its methods. # Inputs and Outputs: - **Input:** - The custom type should be able to accept integers upon initialization. - Operations (like addition) will involve `MyCustomNumber` instances and/or integers. - **Output:** - The implemented operations should return new `MyCustomNumber` instances or Python primitive types as appropriate. # Constraints: - Utilize the provided slots and methods from `PyTypeObject`. - Ensure proper memory management with reference counting. - Implement error handling for invalid operations. # Performance: - The implementation should be efficient in terms of memory usage and computational time. # Example Solution Outline: 1. Define `MyCustomNumber` structure. 2. Initialize `tp_name`, `tp_basicsize`, `tp_flags`, `tp_new`, `tp_init` and other slots as necessary. 3. Implement `tp_dealloc` to handle object deallocation. 4. Implement `tp_richcompare` for comparison operations. 5. Implement the `nb_add` slot in `PyNumberMethods` for addition. Provide code snippets where students need to fill in the correct logic. ```C #include <Python.h> typedef struct { PyObject_HEAD int value; } MyCustomNumber; static PyObject* MyCustomNumber_new(PyTypeObject *type, PyObject *args, PyObject *kwds) { MyCustomNumber *self; self = (MyCustomNumber*) type->tp_alloc(type, 0); if (self != NULL) { self->value = 0; } return (PyObject*)self; } static int MyCustomNumber_init(MyCustomNumber *self, PyObject *args, PyObject *kwds) { int value; if (!PyArg_ParseTuple(args, \\"i\\", &value)) { return -1; } self->value = value; return 0; } static void MyCustomNumber_dealloc(MyCustomNumber *self) { Py_TYPE(self)->tp_free((PyObject*)self); } static PyObject* MyCustomNumber_add(PyObject *a, PyObject *b) { int result; if (PyObject_TypeCheck(a, &MyCustomNumber_Type) && PyLong_Check(b)) { result = ((MyCustomNumber*)a)->value + PyLong_AsLong(b); } else if (PyObject_TypeCheck(b, &MyCustomNumber_Type) && PyLong_Check(a)) { result = ((MyCustomNumber*)b)->value + PyLong_AsLong(a); } else if (PyObject_TypeCheck(a, &MyCustomNumber_Type) && PyObject_TypeCheck(b, &MyCustomNumber_Type)) { result = ((MyCustomNumber*)a)->value + ((MyCustomNumber*)b)->value; } else { Py_RETURN_NOTIMPLEMENTED; } return Py_BuildValue(\\"i\\", result); } static PyTypeObject MyCustomNumber_Type = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"mymodule.MyCustomNumber\\", .tp_basicsize = sizeof(MyCustomNumber), .tp_dealloc = (destructor)MyCustomNumber_dealloc, .tp_flags = Py_TPFLAGS_DEFAULT, .tp_new = MyCustomNumber_new, .tp_init = (initproc)MyCustomNumber_init, .tp_as_number = &MyCustomNumber_as_number, .tp_richcompare = (richcmpfunc)MyCustomNumber_richcompare, .tp_doc = \\"MyCustomNumber objects\\", }; static PyMethodDef module_methods[] = { {NULL} /* Sentinel */ }; static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", \\"Module for MyCustomNumber\\", -1, module_methods }; PyMODINIT_FUNC PyInit_mymodule(void) { PyObject* m; if (PyType_Ready(&MyCustomNumber_Type) < 0) return NULL; m = PyModule_Create(&mymodule); if (m == NULL) return NULL; Py_INCREF(&MyCustomNumber_Type); PyModule_AddObject(m, \\"MyCustomNumber\\", (PyObject *)&MyCustomNumber_Type); return m; } ``` # Instructions: 1. Complete the function implementations by following the provided structure and guidelines. 2. Ensure to run the module and test the custom type with different operations. **Note:** The provided code outline is a starting point. You are expected to complete the implementation and ensure it meets the requirements. --- Good luck, and happy coding!","solution":"class MyCustomNumber: def __init__(self, value): if not isinstance(value, int): raise TypeError(\\"Value must be an integer\\") self.value = value def __add__(self, other): if isinstance(other, MyCustomNumber): return MyCustomNumber(self.value + other.value) elif isinstance(other, int): return MyCustomNumber(self.value + other) return NotImplemented def __lt__(self, other): if isinstance(other, MyCustomNumber): return self.value < other.value elif isinstance(other, int): return self.value < other return NotImplemented def __le__(self, other): if isinstance(other, MyCustomNumber): return self.value <= other.value elif isinstance(other, int): return self.value <= other return NotImplemented def __eq__(self, other): if isinstance(other, MyCustomNumber): return self.value == other.value elif isinstance(other, int): return self.value == other return NotImplemented def __ne__(self, other): return not self.__eq__(other) def __ge__(self, other): if isinstance(other, MyCustomNumber): return self.value >= other.value elif isinstance(other, int): return self.value >= other return NotImplemented def __gt__(self, other): if isinstance(other, MyCustomNumber): return self.value > other.value elif isinstance(other, int): return self.value > other return NotImplemented"},{"question":"# Understanding netrc Files in Python **Background:** The `netrc` module in Python is used for parsing and handling `.netrc` files, which store login information used by FTP clients and other programs. A `.netrc` file typically contains entries like this: ``` machine machine_name login user_name password secret_password ``` **Problem Statement:** You are required to create a class `NetrcManager` that uses the `netrc` module to interact with `.netrc` files. This class should provide methods to: 1. **Add a new machine entry**: - Method: `add_machine(machine: str, login: str, password: str, account: str = None)` - Description: Adds a new entry to the `.netrc` file for the given machine with login and password. The account field is optional. 2. **Delete an existing machine entry**: - Method: `delete_machine(machine: str)` - Description: Deletes the entry of the specified machine from the `.netrc` file if it exists. 3. **Retrieve authenticators for a machine**: - Method: `get_authenticators(machine: str) -> Optional[Tuple[str, str, str]]` - Description: Returns the `(login, account, password)` tuple for the given machine. If the machine does not exist, return `None`. You should ensure the following: - Secure handling of file operations. - Proper exception handling for errors such as file not found and parsing errors. - Ensure that adding or deleting entries maintains the integrity of the `.netrc` file. **Constraints:** - The `.netrc` file must be in the user\'s home directory. - The class should handle both addition and deletion of machines smoothly. - Input parameters will be clean and in the correct format (Machine names, logins, and passwords are strings). **Requirements:** - Implement the `NetrcManager` class as described. - Use the `netrc` module for all interactions with the `.netrc` file. - Demonstrate the usage of your class with example calls that add, delete, and retrieve machine entries. **Example Usage:** ```python # Example .netrc file before operations # machine example.com # login user1 # password pass1 manager = NetrcManager() # Add new machine manager.add_machine(\'newmachine.com\', \'user2\', \'pass2\') # Get authenticators for a machine auth = manager.get_authenticators(\'newmachine.com\') print(auth) # Output: (\'user2\', None, \'pass2\') # Delete a machine manager.delete_machine(\'newmachine.com\') # Get authenticators for a machine that was deleted auth = manager.get_authenticators(\'newmachine.com\') print(auth) # Output: None ``` **Grading Criteria:** - Correctness: The class and methods should work as specified. - Security: File operations should be handled securely, particularly with respect to file permissions. - Robustness: The class should handle exceptions gracefully. - Code Quality: Code should be clean and well-documented with meaningful method names.","solution":"import os import netrc from typing import Optional, Tuple class NetrcManager: def __init__(self, filepath: str = None): self.filepath = filepath or os.path.join(os.path.expanduser(\\"~\\"), \\".netrc\\") if not os.path.exists(self.filepath): open(self.filepath, \'a\').close() # Create the .netrc file if it doesn\'t exist def _load_netrc(self) -> netrc.netrc: try: return netrc.netrc(self.filepath) except netrc.NetrcParseError as e: raise ValueError(f\\"Failed to parse .netrc file: {e}\\") except FileNotFoundError: raise FileNotFoundError(f\\".netrc file not found: {self.filepath}\\") def _save_netrc(self, hosts: dict): with open(self.filepath, \'w\') as file: for machine, auth in hosts.items(): file.write(f\\"machine {machine}n\\") file.write(f\\" login {auth[0]}n\\") file.write(f\\" password {auth[2]}n\\") if auth[1]: file.write(f\\" account {auth[1]}n\\") def add_machine(self, machine: str, login: str, password: str, account: str = None): netrc_data = self._load_netrc().hosts netrc_data[machine] = (login, account, password) self._save_netrc(netrc_data) def delete_machine(self, machine: str): netrc_data = self._load_netrc().hosts if machine in netrc_data: del netrc_data[machine] self._save_netrc(netrc_data) else: raise ValueError(f\\"Machine \'{machine}\' not found in .netrc\\") def get_authenticators(self, machine: str) -> Optional[Tuple[str, str, str]]: netrc_data = self._load_netrc().hosts return netrc_data.get(machine, None)"},{"question":"# Seaborn Coding Assessment Question **Problem Statement:** You are provided with a dataset containing information about financial metrics and another dataset containing brain activity measurements. Your task is to use seaborn to visualize these datasets by producing different kinds of plots while showcasing your understanding of seaborn\'s capabilities. Datasets: 1. `dowjones` - This dataset contains the following columns: - `Date`: The date of the metric. - `Price`: The closing price of the Dow Jones Industrial Average on that date. 2. `fmri` - This dataset contains the following columns: - `timepoint`: The time point of the brain activity measurement. - `signal`: Signal measurement of brain activity. - `region`: The region of the brain where the measurement was taken. - `event`: The type of event associated with the measurement. - `subject`: The subject from whom the measurement was taken. Tasks: 1. **Basic Line Plot** - Create a line plot of the `dowjones` dataset showing the change in `Price` over `Date`. 2. **Orientation Change** - Plot the same data as in Task 1 but change the orientation so that `Date` is on the y-axis and `Price` is on the x-axis. 3. **Grouped Line Plot** - Using the `fmri` dataset, create a line plot showing `signal` over `timepoint` where each line represents data grouped by `subject`. Filter the dataset for `region == \'parietal\'` and `event == \'stim\'`. 4. **Property Mapping** - For the `fmri` dataset, create a plot mapping `region` to `color` and `event` to `linestyle`. Show `signal` over `timepoint` for all data. 5. **Enhanced Plot with Error Bands and Markers** - Combine the line plot and an error band plot for the `fmri` dataset showing `signal` over `timepoint` with `region` mapped to `color` and `event` to `linestyle`. Add markers to the line plot and show error bands. **Constraints:** - Ensure your plots are clear and well-labeled. - Import seaborn objects as `so`. - Use only the methods and properties demonstrated in the documentation. **Expected Input and Output:** - The plots should be displayed as visualizations using seaborn\'s `so.Plot` API. **Performance Requirements:** - Your code should execute efficiently and produce the plots correctly without errors. ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Task 1: Basic Line Plot so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\").add(so.Line()).show() # Task 2: Orientation Change so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line(), orient=\\"y\\").show() # Task 3: Grouped Line Plot so.Plot(fmri.query(\\"region == \'parietal\' and event == \'stim\'\\"), x=\\"timepoint\\", y=\\"signal\\").add(so.Line(), group=\\"subject\\").show() # Task 4: Property Mapping so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\").add(so.Line(), so.Agg()).show() # Task 5: Enhanced Plot with Error Bands and Markers p = so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") (p.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\")).show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Task 1: Basic Line Plot def basic_line_plot(data): so.Plot(data, x=\\"Date\\", y=\\"Price\\").add(so.Line()).show() # Task 2: Orientation Change def orientation_change_plot(data): so.Plot(data, x=\\"Price\\", y=\\"Date\\").add(so.Line()).show() # Task 3: Grouped Line Plot def grouped_line_plot(data): filtered_data = data.query(\\"region == \'parietal\' and event == \'stim\'\\") so.Plot(filtered_data, x=\\"timepoint\\", y=\\"signal\\", color=\\"subject\\").add(so.Line()).show() # Task 4: Property Mapping def property_mapping_plot(data): so.Plot(data, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\").add(so.Line()).show() # Task 5: Enhanced Plot with Error Bands and Markers def enhanced_plot_with_error_bands(data): p = so.Plot(data, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") (p.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\")).add(so.Band(), group=\\"event\\")).show()"},{"question":"**Objective:** Implement a function that leverages the `compileall` module\'s functionalities to compile all Python files in a given directory tree based on specified options and constraints. # Problem Statement Write a Python function `custom_compile_dir` that takes the following parameters: 1. **dir** *(str)*: The root directory from which to start compiling Python files. 2. **include_regex** *(str)*: A regular expression pattern. Only files whose paths match this pattern should be compiled. 3. **exclude_regex** *(str, default=None)*: A regular expression pattern. Files whose paths match this pattern should be excluded from compilation. 4. **max_recursion_level** *(int, default=sys.getrecursionlimit())*: The maximum recursion level for subdirectories. 5. **optimization_level** *(int|list, default=-1)*: Optimization level(s) to be used during compilation. Can be a single integer or a list of integers. 6. **num_workers** *(int, default=1)*: Number of worker threads to use for compilation. If `0`, use the optimal number of cores available on the system. 7. **silent** *(int, default=0)*: Control the verbosity level during compilation. `0` for full verbosity, `1` to suppress informational messages, and `2` to suppress all output, including errors. The function should compile the Python files under the specified directory according to the inclusion and exclusion criteria and other given parameters. It should return `True` if all files compiled successfully, and `False` otherwise. # Function Signature ```python import os import re import sys import compileall from typing import Union, List def custom_compile_dir( dir: str, include_regex: str, exclude_regex: str = None, max_recursion_level: int = sys.getrecursionlimit(), optimization_level: Union[int, List[int]] = -1, num_workers: int = 1, silent: int = 0 ) -> bool: # Implement the function here pass ``` # Example Usage ```python # Example usage of the custom_compile_dir function result = custom_compile_dir( dir=\\"example_dir\\", include_regex=r\\".*.py\\", exclude_regex=r\\".*test.*\\", max_recursion_level=2, optimization_level=[-1, 0, 1], num_workers=0, silent=2 ) print(result) # Expected output: True or False based on the success of compilation ``` # Constraints 1. The function should handle scenarios where no matching files are found gracefully. 2. The regular expressions for inclusion and exclusion are assumed to be valid patterns. 3. The function should use the `compileall.compile_dir` method to handle the compilation logic. 4. Compilation should respect the specified `max_recursion_level` and the `num_workers` should be handled correctly for multi-threaded compilation. 5. If `num_workers` is set to `0`, use the optimal number of cores in the system without explicitly specifying it. # Notes - Ensure that the function is robust and handles edge cases effectively. - Document the code where necessary to explain the logic, especially for regular expression matching and directory traversal.","solution":"import os import re import sys import compileall from typing import Union, List def custom_compile_dir( dir: str, include_regex: str, exclude_regex: str = None, max_recursion_level: int = sys.getrecursionlimit(), optimization_level: Union[int, List[int]] = -1, num_workers: int = 1, silent: int = 0 ) -> bool: def match_include_exclude(file_path): if not re.search(include_regex, file_path): return False if exclude_regex and re.search(exclude_regex, file_path): return False return True if isinstance(optimization_level, int): optimization_level = [optimization_level] # Traverse the directory tree files_to_compile = [] for root, _, files in os.walk(dir): depth = root[len(dir):].count(os.sep) if depth > max_recursion_level: continue for file in files: if file.endswith(\\".py\\"): file_path = os.path.join(root, file) if match_include_exclude(file_path): files_to_compile.append(file_path) if not files_to_compile: return True # No files to compile, return True by convention success = True for optimization in optimization_level: result = compileall.compile_dir( dir, maxlevels=max_recursion_level, optimize=optimization, force=True, quiet=silent, workers=num_workers ) if not result: success = False # If any compilation fails, we return False break return success"},{"question":"**Question: PyTorch Tensor Manipulations and Operations** You are tasked with writing a function using PyTorch that performs a series of operations on tensors. The function should demonstrate your understanding of tensor creation, manipulation, and basic operations. Follow the steps below to complete the task. # Task: 1. Create a tensor of size (4, 4) with values drawn from a standard normal distribution (`torch.randn`), and set the data type to `torch.float64`. 2. Create another tensor of size (4, 4) consisting entirely of ones with data type `torch.float32`. 3. Change the data type of the second tensor to match the first tensor. 4. Perform an element-wise addition between the two tensors. 5. Calculate the matrix multiplication of the resulting tensor with its transpose. 6. Apply the ReLU activation function (`torch.relu`) to the final tensor. 7. Return the final tensor. # Constraints: - You should use PyTorch functions to perform the tasks. - Ensure that the tensor dimensions are handled correctly. - Do not use any loops; rely on tensor operations provided by PyTorch. # Expected Function Signature: ```python import torch def tensor_operations(): # Step 1: Create a tensor (4, 4) with values from a normal distribution tensor_a = torch.randn(4, 4, dtype=torch.float64) # Step 2: Create a tensor (4, 4) of ones with dtype as torch.float32 tensor_b = torch.ones(4, 4, dtype=torch.float32) # Step 3: Change dtype of tensor_b to float64 tensor_b = tensor_b.to(torch.float64) # Step 4: Element-wise addition of tensor_a and tensor_b tensor_c = torch.add(tensor_a, tensor_b) # Step 5: Matrix multiplication of tensor_c with its transpose tensor_d = torch.matmul(tensor_c, torch.transpose(tensor_c, 0, 1)) # Step 6: Apply ReLU activation function final_tensor = torch.relu(tensor_d) return final_tensor # Example Usage result_tensor = tensor_operations() print(result_tensor) ```","solution":"import torch def tensor_operations(): # Step 1: Create a tensor (4, 4) with values from a normal distribution tensor_a = torch.randn(4, 4, dtype=torch.float64) # Step 2: Create a tensor (4, 4) of ones with dtype as torch.float32 tensor_b = torch.ones(4, 4, dtype=torch.float32) # Step 3: Change dtype of tensor_b to float64 tensor_b = tensor_b.to(torch.float64) # Step 4: Element-wise addition of tensor_a and tensor_b tensor_c = torch.add(tensor_a, tensor_b) # Step 5: Matrix multiplication of tensor_c with its transpose tensor_d = torch.matmul(tensor_c, torch.transpose(tensor_c, 0, 1)) # Step 6: Apply ReLU activation function final_tensor = torch.relu(tensor_d) return final_tensor"},{"question":"# Python Coding Assessment: Custom Exceptions and Exception Handling **Objective:** Demonstrate your understanding of Python\'s built-in exception hierarchy and your ability to create and handle custom exceptions. **Problem Statement:** You are tasked with creating an application that simulates a file processing system. The system should read a list of file paths and perform specified operations on them. This includes checking for file existence, reading the file\'s content, and processing the content. The system must also handle various types of exceptions appropriately and provide meaningful error messages. **Steps:** 1. Define a custom exception `FileProcessingException` that inherits from `Exception`. This will serve as the base class for other custom exceptions. 2. Define three additional custom exceptions that inherit from `FileProcessingException`: - `FileNotFoundErrorCustom`: Raised when a file does not exist. - `PermissionErrorCustom`: Raised when the file cannot be accessed due to insufficient permissions. - `ProcessingErrorCustom`: Raised when an error occurs while processing the file\'s content. 3. Implement a function `process_files(file_paths: list)` that: - Takes a list of file paths as input. - For each file path, attempts to open the file and read its content. - If a file does not exist, raises `FileNotFoundErrorCustom`. - If a file cannot be accessed due to permission issues, raises `PermissionErrorCustom`. - If an error occurs during the processing of the file\'s content, raises `ProcessingErrorCustom`. - Prints a success message if the file is processed correctly. 4. Implement appropriate exception handling within the function to catch and handle the custom exceptions, providing informative error messages. **Constraints:** - Ensure that your custom exceptions include an appropriate message that indicates the cause of the error. - The function should handle any other unexpected errors gracefully, without crashing. **Example:** ```python class FileProcessingException(Exception): pass class FileNotFoundErrorCustom(FileProcessingException): pass class PermissionErrorCustom(FileProcessingException): pass class ProcessingErrorCustom(FileProcessingException): pass def process_files(file_paths: list): for file_path in file_paths: try: # Simulating file opening and reading if not file_exists(file_path): raise FileNotFoundErrorCustom(f\\"File \'{file_path}\' not found.\\") if not has_permission(file_path): raise PermissionErrorCustom(f\\"Insufficient permissions to access \'{file_path}\'.\\") content = read_file(file_path) process_content(content) except FileNotFoundErrorCustom as e: print(e) except PermissionErrorCustom as e: print(e) except ProcessingErrorCustom as e: print(e) except Exception as e: print(f\\"An unexpected error occurred: {e}\\") def file_exists(file_path): # Simulate checking for file existence return True def has_permission(file_path): # Simulate checking for file permissions return True def read_file(file_path): # Simulate reading a file return \\"file content\\" def process_content(content): # Simulate processing file content if content == \\"error\\": raise ProcessingErrorCustom(\\"Error processing file content.\\") # Example usage file_paths = [\\"file1.txt\\", \\"file2.txt\\"] process_files(file_paths) ``` **Note:** The helper functions `file_exists`, `has_permission`, `read_file`, and `process_content` are placeholder implementations. You should implement them as necessary to simulate file operations and raise their respective exceptions. **Expected Output:** The function should print appropriate error messages for each type of error encountered during file processing.","solution":"class FileProcessingException(Exception): Base class for all custom file processing exceptions pass class FileNotFoundErrorCustom(FileProcessingException): Raised when a file does not exist pass class PermissionErrorCustom(FileProcessingException): Raised when there are insufficient permissions to access the file pass class ProcessingErrorCustom(FileProcessingException): Raised when an error occurs during file content processing pass def file_exists(file_path): Simulate checking for file existence # Here you would normally use os.path.exists or similar. # Simulating for illustration. if file_path == \\"nonexistent.txt\\": return False return True def has_permission(file_path): Simulate checking for file permissions # Normally you\'d check file permissions if file_path == \\"no_permission.txt\\": return False return True def read_file(file_path): Simulate reading a file # Simulating file read operation if file_path == \\"error_file.txt\\": return \\"error\\" return \\"file content\\" def process_content(content): Simulate processing file content # Simulate processing the file content, for example parsing or analyzing data if content == \\"error\\": raise ProcessingErrorCustom(\\"Error processing file content\\") def process_files(file_paths: list): Process a list of file paths for file_path in file_paths: try: if not file_exists(file_path): raise FileNotFoundErrorCustom(f\\"File \'{file_path}\' not found.\\") if not has_permission(file_path): raise PermissionErrorCustom(f\\"Insufficient permissions to access \'{file_path}\'.\\") content = read_file(file_path) process_content(content) print(f\\"File \'{file_path}\' processed successfully.\\") except FileNotFoundErrorCustom as e: print(f\\"FileNotFoundErrorCustom: {e}\\") except PermissionErrorCustom as e: print(f\\"PermissionErrorCustom: {e}\\") except ProcessingErrorCustom as e: print(f\\"ProcessingErrorCustom: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# Asynchronous File Downloader with Platform-Specific Limitations **Objective:** Implement an asynchronous file downloader using the \\"asyncio\\" module in Python that handles platform-specific limitations. Problem Statement: You are tasked with writing a Python function, `async_file_downloader`, which downloads multiple files concurrently using the \\"asyncio\\" module. The function should be capable of adapting to limitations in different operating systems. Requirements: 1. **Function Signature:** ```python async def async_file_downloader(urls: List[str], dest_folder: str) -> List[str]: ``` - `urls`: A list of file URLs to download. - `dest_folder`: The destination folder where the files should be saved. - The function should return a list of file paths where the downloaded files are saved. 2. **Platform Specific Constraints:** - On Windows, use the `ProactorEventLoop` for the event loop. - On macOS version <= 10.8, use `SelectSelector`. 3. **Concurrency:** - Ensure that at least 3 files are being downloaded concurrently. 4. **Error Handling:** - If a file download fails, the function should continue downloading the next files and log the error (to standard output). Constraints: - The URLs provided will be valid and accessible. - You may use libraries such as `aiohttp` for making HTTP requests. - Ensure that your implementation is efficient and adheres to the platform-specific limitations outlined in the documentation. Example Usage: ```python import asyncio async def starter(): urls = [ \\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\", \\"http://example.com/file3.txt\\" ] dest_folder = \\"/path/to/destination\\" downloaded_files = await async_file_downloader(urls, dest_folder) print(\\"Downloaded files:\\", downloaded_files) asyncio.run(starter()) ``` In this example, the `starter` function sets up a list of URLs to be downloaded and specifies the destination folder. It then calls the `async_file_downloader` function and prints out the list of downloaded files. **Note:** You are required to implement proper platform-specific event loop adjustments and error handling in your solution.","solution":"import asyncio import aiohttp import os import platform import sys from selectors import SelectSelector from typing import List async def download_file(session, url, dest_folder): try: async with session.get(url) as response: if response.status != 200: raise Exception(f\\"Failed to download {url}. Status code: {response.status}\\") filename = os.path.join(dest_folder, os.path.basename(url)) with open(filename, \'wb\') as fd: while True: chunk = await response.content.read(1024) if not chunk: break fd.write(chunk) return filename except Exception as e: print(e) return None async def async_file_downloader(urls: List[str], dest_folder: str) -> List[str]: if not os.path.exists(dest_folder): os.makedirs(dest_folder) if platform.system() == \'Windows\': asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) elif platform.system() == \'Darwin\' and float(\'.\'.join(platform.mac_ver()[0].split(\'.\')[:2])) <= 10.8: asyncio.set_event_loop(asyncio.SelectorEventLoop(SelectSelector())) async with aiohttp.ClientSession() as session: tasks = [download_file(session, url, dest_folder) for url in urls] results = await asyncio.gather(*tasks) return [result for result in results if result] if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser(description=\\"Asynchronous file downloader\\") parser.add_argument(\\"urls\\", nargs=\'+\', help=\\"List of URLs to download\\") parser.add_argument(\\"dest_folder\\", help=\\"Destination folder for downloads\\") args = parser.parse_args() asyncio.run(async_file_downloader(args.urls, args.dest_folder))"},{"question":"# Seaborn Advanced Stripplot Customization Objective: Demonstrate your understanding of Seaborn\'s `stripplot` method by creating a customized, multi-faceted plot using the `tips` dataset. Task: Write a Python function `custom_stripplot()` that loads the `tips` dataset from Seaborn and generates a customized strip plot with the following specifications: 1. **Dataset and Theme:** - Use Seaborn\'s `tips` dataset. - Set the seaborn theme to `whitegrid`. 2. **Strip Plot Customization:** - Plot the relationship between `total_bill` (x-axis) and `day` (y-axis). - Use `hue` to differentiate between `sex`. - Disable jitter for the plot. - Use a deep palette for `hue`. - Add a custom marker and adjust the size and transparency of the points. 3. **Faceted Plot:** - Create facets based on `time`. - Ensure the facets are properly synchronized. 4. **Expected Plot Output:** - The strip plot should reflect the above specifications. 5. **Function Definition:** - The function should take no parameters and should display the plot directly. Constraints: 1. You are only allowed to use `seaborn` and `matplotlib` packages. 2. The function should execute within a reasonable time frame (~10 seconds). # Example Function Definition: ```python import seaborn as sns import matplotlib.pyplot as plt def custom_stripplot(): # Load dataset and set theme tips = sns.load_dataset(\\"tips\\") sns.set_theme(style=\\"whitegrid\\") # Custom strip plot strip_plot = sns.catplot( data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", col=\\"time\\", jitter=False, palette=\\"deep\\", marker=\\"D\\", s=20, alpha=0.5, aspect=0.8 ) # Display the plot plt.show() # Call the function to test your implementation custom_stripplot() ``` # Additional Notes: - Ensure that all plots generated by the function are clear and correctly reflect the specified requirements. - Explore Seaborn\'s documentation for any additional properties you may find useful for customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_stripplot(): # Load dataset and set theme tips = sns.load_dataset(\\"tips\\") sns.set_theme(style=\\"whitegrid\\") # Custom strip plot strip_plot = sns.catplot( data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", col=\\"time\\", jitter=False, palette=\\"deep\\", marker=\\"D\\", s=8, alpha=0.7, aspect=0.8 ) # Display the plot plt.show() # Call the function to test your implementation # custom_stripplot() # Uncomment to test the function"},{"question":"Problem Statement You are tasked with implementing a small library management system in Python. This system will consist of classes to represent books and the library, and will provide basic functionality to add books, remove books, and search for books by different criteria. Your task includes: 1. Implementing the `Book` class. 2. Implementing the `Library` class. 3. Implementing functions to add, remove, and search for books by title and author within the `Library` class. 4. Implementing a function to display all books currently in the library. `Book` Class - Attributes: - `title` (string): The title of the book. - `author` (string): The author of the book. - `isbn` (string): The ISBN number of the book, which is unique for each book. - Methods: - `__init__(self, title, author, isbn)`: Constructor to initialize the book with the given title, author, and ISBN. - `__repr__(self)`: Returns a string representation of the book. `Library` Class - Attributes: - `books` (list of `Book` objects): A list to store the books in the library. - Methods: - `__init__(self)`: Initializes the library with an empty list of books. - `add_book(self, book)`: Adds a `Book` object to the library. - `remove_book(self, isbn)`: Removes the book with the given ISBN from the library. - `search_by_title(self, title)`: Returns a list of books that have the specified title. - `search_by_author(self, author)`: Returns a list of books that have the specified author. - `display_books(self)`: Displays all books in the library. Constraints - The ISBN for each book is unique. - When searching, the match should be exact (case-sensitive). - If attempting to remove a book that does not exist, the system should raise a `ValueError` with the message \\"Book not found\\". Example Usage ```python # Create some book instances book1 = Book(\\"Python Programming\\", \\"John Doe\\", \\"1234567890\\") book2 = Book(\\"Learning Python\\", \\"Jane Smith\\", \\"0987654321\\") book3 = Book(\\"Python Programming\\", \\"Alice Johnson\\", \\"1122334455\\") # Create a library instance library = Library() # Add books to the library library.add_book(book1) library.add_book(book2) library.add_book(book3) # Search for a book by title print(library.search_by_title(\\"Python Programming\\")) # Output: [Book(title=\'Python Programming\', author=\'John Doe\', isbn=\'1234567890\'), Book(title=\'Python Programming\', author=\'Alice Johnson\', isbn=\'1122334455\')] # Remove a book by ISBN library.remove_book(\\"1234567890\\") # Display all books in the library library.display_books() # Output: [Book(title=\'Learning Python\', author=\'Jane Smith\', isbn=\'0987654321\'), Book(title=\'Python Programming\', author=\'Alice Johnson\', isbn=\'1122334455\')] ``` Submission Implement the `Book` and `Library` classes along with their methods described above. Make sure to handle all constraints and edge cases, and provide a clear and readable implementation.","solution":"class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn def __repr__(self): return f\\"Book(title=\'{self.title}\', author=\'{self.author}\', isbn=\'{self.isbn}\')\\" class Library: def __init__(self): self.books = [] def add_book(self, book): self.books.append(book) def remove_book(self, isbn): for book in self.books: if book.isbn == isbn: self.books.remove(book) return raise ValueError(\\"Book not found\\") def search_by_title(self, title): return [book for book in self.books if book.title == title] def search_by_author(self, author): return [book for book in self.books if book.author == author] def display_books(self): for book in self.books: print(book)"},{"question":"Objective To evaluate your understanding of using pandas DataFrames and efficient computation using pandas expressions. Problem Statement You are provided with a dataset containing weather data for a set of locations over multiple days. Your task is to write a function that performs the following operations: 1. **Calculate the daily temperature range**: Create a new column `temperature_range` which is the difference between `max_temperature` and `min_temperature`. 2. **Categorize the temperature range**: Create a new column `temperature_category` which categorizes the `temperature_range`: - \\"Low\\" if the range is below 10. - \\"Medium\\" if the range is between 10 and 20 (inclusive). - \\"High\\" if the range is above 20. 3. **Evaluate Boolean Expression**: Use `pd.eval` to create a new boolean column `is_windy_and_cold`, which is `True` if `wind_speed` is above 20 and `mean_temperature` is below 15. Constraints - You must use pandas\' efficient computation methods, including `pd.eval` where applicable. - Your solution should be efficient for large datasets (e.g., DataFrame with 100,000 rows). Function Signature ```python import pandas as pd def process_weather_data(df: pd.DataFrame) -> pd.DataFrame: Process the weather data to add new columns as specified. Parameters: df (pd.DataFrame): A DataFrame containing weather data with the following columns: - \'date\', \'location\', \'min_temperature\', \'max_temperature\', \'mean_temperature\', \'wind_speed\' Returns: pd.DataFrame: The processed DataFrame with the additional columns: - \'temperature_range\', \'temperature_category\', \'is_windy_and_cold\' pass ``` Input - A pandas DataFrame `df` with the following columns: - `date` (str): Date of observation. - `location` (str): Location of the observation. - `min_temperature` (float): Minimum temperature recorded. - `max_temperature` (float): Maximum temperature recorded. - `mean_temperature` (float): Average temperature recorded. - `wind_speed` (float): Wind speed recorded. Output - A pandas DataFrame with the original columns and three additional columns: - `temperature_range` (float): Difference between `max_temperature` and `min_temperature`. - `temperature_category` (str): Categorization of `temperature_range` (\\"Low\\", \\"Medium\\", \\"High\\"). - `is_windy_and_cold` (bool): `True` if `wind_speed` > 20 and `mean_temperature` < 15, else `False`. Performance Requirement - Ensure the solution is optimized to handle large datasets efficiently. Example ```python import pandas as pd data = { \'date\': [\'2023-01-01\', \'2023-01-02\'], \'location\': [\'Location1\', \'Location2\'], \'min_temperature\': [10.5, 8.2], \'max_temperature\': [23.1, 15.5], \'mean_temperature\': [15.0, 10.0], \'wind_speed\': [18.0, 25.0] } df = pd.DataFrame(data) processed_df = process_weather_data(df) # processed_df should have additional columns \'temperature_range\', \'temperature_category\', and \'is_windy_and_cold\' ``` ```output date location min_temperature max_temperature mean_temperature wind_speed temperature_range temperature_category is_windy_and_cold 0 2023-01-01 Location1 10.5 23.1 15.0 18.0 12.6 Medium False 1 2023-01-02 Location2 8.2 15.5 10.0 25.0 7.3 Low True ```","solution":"import pandas as pd def process_weather_data(df: pd.DataFrame) -> pd.DataFrame: Process the weather data to add new columns as specified. Parameters: df (pd.DataFrame): A DataFrame containing weather data with the following columns: - \'date\', \'location\', \'min_temperature\', \'max_temperature\', \'mean_temperature\', \'wind_speed\' Returns: pd.DataFrame: The processed DataFrame with the additional columns: - \'temperature_range\', \'temperature_category\', \'is_windy_and_cold\' # Calculate the daily temperature range df[\'temperature_range\'] = df[\'max_temperature\'] - df[\'min_temperature\'] # Categorize the temperature range def categorize_temperature_range(temp_range): if temp_range < 10: return \\"Low\\" elif 10 <= temp_range <= 20: return \\"Medium\\" else: return \\"High\\" df[\'temperature_category\'] = df[\'temperature_range\'].apply(categorize_temperature_range) # Evaluate Boolean Expression using pd.eval df[\'is_windy_and_cold\'] = pd.eval(\'df.wind_speed > 20 & df.mean_temperature < 15\') return df"},{"question":"# Advanced Hashing with Python\'s `hashlib` Module Objective You are tasked to implement a secure file verification mechanism using Python\'s `hashlib` module. The goal is to ensure the integrity of files by creating and verifying their hashes. Requirements 1. **Function 1: `create_file_hash(file_path, algorithm)`** - **Input:** - `file_path` (str): Path to the file that needs to be hashed. - `algorithm` (str): The hashing algorithm to use (e.g., \'sha256\', \'sha512\'). - **Output:** - (str): The computed hash of the file\'s contents using the specified algorithm. - **Constraints:** - The `algorithm` must be a valid one supported by Python\'s `hashlib` module. 2. **Function 2: `verify_file_integrity(file_path, provided_hash, algorithm)`** - **Input:** - `file_path` (str): Path to the file whose integrity needs to be verified. - `provided_hash` (str): The hash value that the file\'s content is supposed to match. - `algorithm` (str): The hashing algorithm used to create the `provided_hash`. - **Output:** - (bool): `True` if the computed hash of the file matches the `provided_hash`, otherwise `False`. - **Constraints:** - The `algorithm` must be a valid one supported by Python\'s `hashlib` module. Performance Requirements - The implemented functions should efficiently handle large files (up to a few GBs). - Optimal use of memory is required, implying use of techniques like reading files in chunks rather than loading entire files into memory. Example Usage ```python # Assuming implementation is done # Create a hash of a file file_hash = create_file_hash(\'example.txt\', \'sha256\') print(f\\"The SHA-256 hash of the file is: {file_hash}\\") # Verify the integrity of the file is_valid = verify_file_integrity(\'example.txt\', file_hash, \'sha256\') print(f\\"Is the file integrity valid? {is_valid}\\") ``` **Note:** Remember to handle potential exceptions, such as file not found or unsupported algorithm, gracefully in your implementation.","solution":"import hashlib def create_file_hash(file_path, algorithm): Creates a hash for the specified file using the given algorithm. Args: - file_path (str): Path to the file that needs to be hashed. - algorithm (str): The hashing algorithm to use (e.g., \'sha256\', \'sha512\'). Returns: - str: The computed hash of the file\'s contents. try: # Initialize the hash object hash_func = hashlib.new(algorithm) except ValueError: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") try: with open(file_path, \'rb\') as f: # Read and update hash string value in blocks of 4K for chunk in iter(lambda: f.read(4096), b\\"\\"): hash_func.update(chunk) return hash_func.hexdigest() except FileNotFoundError: raise FileNotFoundError(f\\"File not found: {file_path}\\") except Exception as e: raise e def verify_file_integrity(file_path, provided_hash, algorithm): Verifies the integrity of a file by comparing its hash with the provided hash. Args: - file_path (str): Path to the file whose integrity needs to be verified. - provided_hash (str): The hash value that the file\'s content is supposed to match. - algorithm (str): The hashing algorithm used to create the provided_hash. Returns: - bool: True if the computed hash matches the provided_hash, otherwise False. try: # Create the hash of the file with the specified algorithm computed_hash = create_file_hash(file_path, algorithm) return computed_hash == provided_hash except Exception as e: raise e"},{"question":"Complex Number Operations Objective: Demonstrate your understanding of Python\'s `math` and `cmath` modules by implementing a function that performs a series of mathematical operations on complex numbers. Problem Statement: Write a function `complex_operations` that takes a list of complex numbers and performs the following operations: 1. **Magnitude Calculation**: Calculate the magnitude (absolute value) of each complex number. 2. **Polar Conversion**: Convert each complex number from rectangular (Cartesian) to polar coordinates. 3. **Exponential Transform**: Apply the exponential function to each complex number. 4. **Cosine and Sine**: Calculate both the cosine and sine of each complex number. Combine these results and return them in a dictionary so that: - The key `magnitudes` maps to a list of magnitudes. - The key `polar_coordinates` maps to a list of tuples representing the polar coordinates. - The key `exponentials` maps to a list of complex numbers after applying the exponential function. - The key `cosines` maps to a list of complex numbers representing their cosines. - The key `sines` maps to a list of complex numbers representing their sines. Expected Input and Output: - **Input**: A list of complex numbers. - **Output**: A dictionary containing the results of the above operations. Constraints: - The input list will not be empty. - Complex number calculations should utilize the `cmath` module as appropriate. Performance Requirements: - The function should handle lists of complex numbers efficiently, leveraging the provided `cmath` functions for optimal performance. Example: ```python import cmath import math def complex_operations(complex_list): magnitudes = [abs(c) for c in complex_list] polar_coordinates = [cmath.polar(c) for c in complex_list] exponentials = [cmath.exp(c) for c in complex_list] cosines = [cmath.cos(c) for c in complex_list] sines = [cmath.sin(c) for c in complex_list] return { \'magnitudes\': magnitudes, \'polar_coordinates\': polar_coordinates, \'exponentials\': exponentials, \'cosines\': cosines, \'sines\': sines } # Example usage complex_list = [complex(1, 1), complex(2, -1), complex(-1, -1)] result = complex_operations(complex_list) print(result) ``` Notes: - Use the `cmath` module for complex number operations. - Each list in the resulting dictionary should maintain the order of input complex numbers. **Good luck!**","solution":"import cmath def complex_operations(complex_list): Perform a series of operations on a list of complex numbers. Parameters: complex_list (list): A list of complex numbers Returns: dict: A dictionary containing the results of various operations on the complex numbers magnitudes = [abs(c) for c in complex_list] polar_coordinates = [cmath.polar(c) for c in complex_list] exponentials = [cmath.exp(c) for c in complex_list] cosines = [cmath.cos(c) for c in complex_list] sines = [cmath.sin(c) for c in complex_list] return { \'magnitudes\': magnitudes, \'polar_coordinates\': polar_coordinates, \'exponentials\': exponentials, \'cosines\': cosines, \'sines\': sines }"},{"question":"Problem Statement You are working on a file organization tool that needs to manipulate and query various filesystem paths to organize files into directories based on their extensions. Your task is to implement several functions using the `pathlib` module that will handle these tasks. # Function 1: `get_python_files` Write a function `get_python_files(directory: str) -> List[str]` that takes a directory path as input and returns a list of all Python files (`.py` and `.pyw` extensions) within that directory and its subdirectories. The output should be a list of file paths as strings. # Function 2: `organize_files_by_extension` Write a function `organize_files_by_extension(directory: str) -> None` that takes a directory path as input and organizes all files within that directory (and its subdirectories) into new directories based on their file extensions. For example, all `.txt` files should be moved into a directory named `txt`, all `.jpg` files should be moved into a directory named `jpg`, and so on. If a directory for an extension does not exist, create it. # Function 3: `resolve_symlinks` Write a function `resolve_symlinks(directory: str) -> Dict[str, str]` that takes a directory path as input, resolves all symbolic links within that directory (and its subdirectories), and returns a dictionary mapping each symlink\'s original path to its resolved path. # Constraints: - The `directory` parameter in each function will always be a valid directory path. - You cannot use any libraries other than `pathlib`. - Handle any exceptions gracefully, and continue processing other items even if an error is encountered with a particular item. # Function Signatures: ```python from typing import List, Dict def get_python_files(directory: str) -> List[str]: pass def organize_files_by_extension(directory: str) -> None: pass def resolve_symlinks(directory: str) -> Dict[str, str]: pass ``` # Example Usage: ```python # Example usage for get_python_files python_files = get_python_files(\'/path/to/directory\') print(python_files) # Output: [\'/path/to/directory/file1.py\', \'/path/to/directory/subdir/file2.pyw\', ...] # Example usage for organize_files_by_extension organize_files_by_extension(\'/path/to/directory\') # Directory structure changes: # /path/to/directory/ # â”œâ”€â”€ py/ # â”‚ â”œâ”€â”€ file1.py # â”‚ â”œâ”€â”€ subdir/ # â”‚ â”‚ â””â”€â”€ file2.pyw # â”œâ”€â”€ txt/ # â”‚ â””â”€â”€ document.txt # â””â”€â”€ ... # Example usage for resolve_symlinks symlinks = resolve_symlinks(\'/path/to/directory\') print(symlinks) # Output: {\'/path/to/symlink\': \'/actual/path/to/target\', ...} ``` Implement these functions to demonstrate your understanding of the `pathlib` module in Python.","solution":"from pathlib import Path from typing import List, Dict def get_python_files(directory: str) -> List[str]: Returns a list of all Python files (.py and .pyw) within the given directory and its subdirectories. python_files = [] try: path = Path(directory) for file_path in path.rglob(\'*\'): if file_path.is_file() and file_path.suffix in {\'.py\', \'.pyw\'}: python_files.append(str(file_path)) except Exception as e: print(f\\"Error occurred: {e}\\") return python_files def organize_files_by_extension(directory: str) -> None: Organizes all files within the given directory (and its subdirectories) into new directories based on their extensions. try: path = Path(directory) for file_path in path.rglob(\'*\'): if file_path.is_file(): ext = file_path.suffix.strip(\'.\') if ext: new_dir = path / ext new_dir.mkdir(exist_ok=True) new_file_path = new_dir / file_path.name file_path.rename(new_file_path) except Exception as e: print(f\\"Error occurred: {e}\\") def resolve_symlinks(directory: str) -> Dict[str, str]: Resolves all symbolic links within the given directory (and its subdirectories), and returns a dictionary mapping each symlink\'s original path to its resolved path. symlinks = {} try: path = Path(directory) for file_path in path.rglob(\'*\'): if file_path.is_symlink(): try: resolved_path = file_path.resolve(strict=True) symlinks[str(file_path)] = str(resolved_path) except FileNotFoundError: print(f\\"Symlink {file_path} is broken\\") except Exception as e: print(f\\"Error resolving {file_path}: {e}\\") except Exception as e: print(f\\"Error occurred: {e}\\") return symlinks"},{"question":"Unit Testing with `unittest` in Python Objective: Write a Python program that includes a class with several methods to perform basic mathematical operations. Then, write a unit test class using the `unittest` module to test the correctness of these methods. Problem Statement: 1. Implement a class `MathOperations` with the following methods: - `add(a: int, b: int) -> int`: Returns the sum of `a` and `b`. - `subtract(a: int, b: int) -> int`: Returns the difference when `b` is subtracted from `a`. - `multiply(a: int, b: int) -> int`: Returns the product of `a` and `b`. - `divide(a: int, b: int) -> float`: Returns the division of `a` by `b`. Raise a `ValueError` if `b` is zero. 2. Implement a class `TestMathOperations` using the `unittest` module to test each method of the `MathOperations` class. Write test cases to cover the following scenarios: - Testing addition, subtraction, multiplication with positive numbers, negative numbers, and a mix of both. - Testing division with positive numbers, negative numbers, and zero (ensure the `ValueError` is raised for zero division). - Use `setUp` and `tearDown` methods in the `unittest` framework to initialize and clean up the `MathOperations` object before and after each test, respectively. Input and Output Formats: - Input: No direct input from the user. Tests are written in the test class. - Output: The results of the unit tests, indicating pass/fail status. Constraints: - Use the `unittest` module for writing the test cases. - Ensure proper handling and testing of edge cases (e.g., division by zero). - Follow best practices for writing unit tests, such as setup and teardown, naming conventions, and separation of test cases. Example Implementation: Here is a skeleton code structure you can start with: ```python # Implementation of the MathOperations class class MathOperations: def add(self, a: int, b: int) -> int: return a + b def subtract(self, a: int, b: int) -> int: return a - b def multiply(self, a: int, b: int) -> int: return a * b def divide(self, a: int, b: int) -> float: if b == 0: raise ValueError(\\"Division by zero is not allowed\\") return a / b # Implementation of the TestMathOperations class import unittest class TestMathOperations(unittest.TestCase): def setUp(self): self.calc = MathOperations() def tearDown(self): pass def test_add(self): self.assertEqual(self.calc.add(2, 3), 5) self.assertEqual(self.calc.add(-2, -3), -5) self.assertEqual(self.calc.add(-2, 3), 1) def test_subtract(self): self.assertEqual(self.calc.subtract(5, 3), 2) self.assertEqual(self.calc.subtract(-5, -3), -2) self.assertEqual(self.calc.subtract(-5, 3), -8) def test_multiply(self): self.assertEqual(self.calc.multiply(2, 3), 6) self.assertEqual(self.calc.multiply(-2, -3), 6) self.assertEqual(self.calc.multiply(-2, 3), -6) def test_divide(self): self.assertEqual(self.calc.divide(6, 3), 2.0) self.assertEqual(self.calc.divide(-6, -3), 2.0) self.assertEqual(self.calc.divide(-6, 3), -2.0) with self.assertRaises(ValueError): self.calc.divide(1, 0) if __name__ == \'__main__\': unittest.main() ``` Submission: Submit your `MathOperations` class implementation and the `TestMathOperations` unit test class in a single Python file.","solution":"# Implementation of the MathOperations class class MathOperations: def add(self, a: int, b: int) -> int: return a + b def subtract(self, a: int, b: int) -> int: return a - b def multiply(self, a: int, b: int) -> int: return a * b def divide(self, a: int, b: int) -> float: if b == 0: raise ValueError(\\"Division by zero is not allowed\\") return a / b"},{"question":"# Functional Programming and Iterator Utilization in Python **Objective**: Implement a Python function demonstrating the use of iterators, generators, and functional programming concepts. Problem Statement You are given a list of student records, where each record is a dictionary containing the `name` and `scores` of a student. Example of a student record: ```python {\\"name\\": \\"Alice\\", \\"scores\\": [88, 74, 92, 85]} ``` Your task is to write a function `student_statistics(records: List[Dict[str, Any]]) -> Dict[str, Tuple[float, List[int]]]` that processes these records and returns a dictionary. The key of this dictionary is the student\'s name, and the value is a tuple containing: 1. The average score of the student. 2. A sorted list of their scores above a given threshold. Write the function according to the following guidelines: 1. Use generator expressions to process the records. 2. Implement custom iterator classes if necessary. 3. Utilize built-in functions such as `map()`, `filter()`, and relevant itertools functions. 4. Apply the `functools.reduce` function to compute the total scores for averaging. Input - `records`: A list of student records, where each record is a dictionary. Output - A dictionary where the key is the student\'s name and the value is a tuple containing the average score and a sorted list of scores above a threshold. Constraints - The scores are integers between 0 and 100. - The threshold for scores is given as a parameter to the function. Example ```python records = [ {\\"name\\": \\"Alice\\", \\"scores\\": [88, 74, 92, 85]}, {\\"name\\": \\"Bob\\", \\"scores\\": [72, 65, 78, 80]}, {\\"name\\": \\"Charlie\\", \\"scores\\": [90, 91, 92, 85]} ] threshold = 75 # Expected output: # { # \\"Alice\\": (84.75, [85, 88, 92]), # \\"Bob\\": (73.75, [78, 80]), # \\"Charlie\\": (89.5, [85, 90, 91, 92]) # } ``` Function Signature ```python from typing import List, Dict, Any, Tuple def student_statistics(records: List[Dict[str, Any]], threshold: int) -> Dict[str, Tuple[float, List[int]]]: pass ``` Implement the `student_statistics` function and make sure it adheres to functional programming principles by leveraging iterators, generators, and higher-order functions.","solution":"from typing import List, Dict, Any, Tuple from functools import reduce def student_statistics(records: List[Dict[str, Any]], threshold: int) -> Dict[str, Tuple[float, List[int]]]: def calculate_average(scores): return reduce(lambda x, y: x + y, scores) / len(scores) def filter_and_sort(scores): return sorted(score for score in scores if score >= threshold) result = {} for record in records: name = record[\'name\'] scores = record[\'scores\'] average = calculate_average(scores) filtered_sorted_scores = filter_and_sort(scores) result[name] = (average, filtered_sorted_scores) return result"},{"question":"# TCP Chat Server using `asyncio` Streams As part of our programming assessment, you are tasked with creating a basic TCP chat server using the `asyncio` streams module in Python. This server should be able to handle multiple clients concurrently. When a client sends a message to the server, the server should broadcast that message to all connected clients (except the sender). Requirements: 1. Implement the `ChatServer` class that starts a TCP server and handles connections. 2. Use `asyncio.start_server` to create the server. 3. Implement client handling to: - Read messages from clients. - Broadcast messages to all connected clients. - Handle client disconnections. 4. Use `StreamReader` and `StreamWriter` to manage the I/O streams for the clients. Input: Clients will connect to the server and send messages. Each message sent by a client should be broadcast to all other connected clients. You do not need to handle client input within this assessment; assume clients will properly format their messages. Output: Broadcast each client\'s message to all other connected clients, prefixed with the client\'s address. Constraints: - Use the default buffer size limits and any other default parameters. - Assume a maximum of 100 concurrent clients. Example: Below is a simple interaction example between clients: 1. Client 1 connects and sends: `Hello world!` 2. Server broadcasts to Client 2: `from (\'127.0.0.1\', 12345): Hello world!` 3. Client 2 connects and sends: `Hi there!` 4. Server broadcasts to Client 1: `from (\'127.0.0.1\', 12347): Hi there!` 5. Client 1 disconnects. 6. Server stops broadcasting messages from Client 1. Implementation: ```python import asyncio class ChatServer: def __init__(self): self.clients = set() async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') self.clients.add(writer) print(f\'Client {addr} connected.\') try: while not reader.at_eof(): data = await reader.read(100) if not data: break message = f\\"from {addr}: {data.decode()}\\" print(message) await self.broadcast(message, writer) except asyncio.CancelledError: pass finally: print(f\'Client {addr} disconnected.\') self.clients.remove(writer) writer.close() await writer.wait_closed() async def broadcast(self, message, sender): for client in self.clients: if client != sender: client.write(message.encode()) await client.drain() async def main(self, host=\'127.0.0.1\', port=8888): server = await asyncio.start_server(self.handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \'__main__\': chat_server = ChatServer() asyncio.run(chat_server.main()) ``` This template sets up the `ChatServer` class but your task is to fill in the missing parts to ensure it behaves according to the requirements.","solution":"import asyncio class ChatServer: def __init__(self): self.clients = set() async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') self.clients.add(writer) print(f\'Client {addr} connected.\') try: while not reader.at_eof(): data = await reader.read(100) if not data: break message = f\\"from {addr}: {data.decode()}\\" print(message) await self.broadcast(message, writer) except asyncio.CancelledError: pass finally: print(f\'Client {addr} disconnected.\') self.clients.remove(writer) writer.close() await writer.wait_closed() async def broadcast(self, message, sender): for client in self.clients: if client != sender: client.write(message.encode()) await client.drain() async def main(self, host=\'127.0.0.1\', port=8888): server = await asyncio.start_server(self.handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \'__main__\': chat_server = ChatServer() asyncio.run(chat_server.main())"},{"question":"# Question: Parallel Sum of Matrices using CUDA Streams in PyTorch You have been given two large matrices and you need to compute the element-wise sum of these matrices. However, to optimize performance, you will split the task into several smaller sub-tasks that will run concurrently on different CUDA streams. Task: 1. **Implement a function** `parallel_matrix_sum` that: - Takes two matrices `A` and `B` of the same size. - Splits these matrices into an equal number of blocks (sub-matrices). - Each block\'s addition should be computed using a different CUDA stream to achieve concurrency. - Combines the resultant blocks back into a single matrix to obtain the final result. 2. **Function Signature:** ```python def parallel_matrix_sum(A: torch.Tensor, B: torch.Tensor, num_blocks: int) -> torch.Tensor: Computes the element-wise sum of two matrices using parallel CUDA streams. Parameters: A (torch.Tensor): The first matrix on a CUDA device. B (torch.Tensor): The second matrix on a CUDA device. num_blocks (int): The number of blocks to split the matrices into for parallel computation. Returns: torch.Tensor: The resultant matrix after the element-wise sum. pass ``` 3. **Constraints:** - Both matrices `A` and `B` are square matrices and reside on the CUDA device. - The number of blocks (`num_blocks`) will always divide evenly into the dimensions of the matrices. - Ensure that CUDA streams are used to manage the parallel computation of the sub-matrices. 4. **Example Usage:** ```python import torch cuda_device = torch.device(\'cuda\') size = 1024 num_blocks = 4 A = torch.randn(size, size, device=cuda_device) B = torch.randn(size, size, device=cuda_device) result = parallel_matrix_sum(A, B, num_blocks) print(result) ``` 5. **Performance Requirements:** - Ensure that all CUDA streams are synchronized before combining the resultant blocks. Not doing so can lead to incorrect results due to asynchronous execution. Implement the `parallel_matrix_sum` function as described and demonstrate its usage with an example.","solution":"import torch def parallel_matrix_sum(A: torch.Tensor, B: torch.Tensor, num_blocks: int) -> torch.Tensor: Computes the element-wise sum of two matrices using parallel CUDA streams. Parameters: A (torch.Tensor): The first matrix on a CUDA device. B (torch.Tensor): The second matrix on a CUDA device. num_blocks (int): The number of blocks to split the matrices into for parallel computation. Returns: torch.Tensor: The resultant matrix after the element-wise sum. assert A.shape == B.shape, \\"Matrices must have the same shape\\" assert A.is_cuda and B.is_cuda, \\"Matrices must reside on CUDA device\\" size = A.shape[0] block_size = size // num_blocks # Create CUDA streams streams = [torch.cuda.Stream() for _ in range(num_blocks)] # Resultant matrix result = torch.empty_like(A, device=\'cuda\') # Define the function to handle a block of computation def add_block(i): with torch.cuda.stream(streams[i]): row_start = i * block_size row_end = (i + 1) * block_size result[row_start:row_end, :] = A[row_start:row_end, :] + B[row_start:row_end, :] # Launch addition in parallel streams for i in range(num_blocks): add_block(i) # Ensure all streams are done torch.cuda.synchronize() return result"},{"question":"**Advanced Web Resource Fetching with Urllib** # Objective Your task is to write a Python function that fetches web resources from multiple URLs and processes the returned data while handling different scenarios of HTTP responses and potential errors. The function should demonstrate advanced features and exception handling as described in the `urllib` documentation. # Function Signature ```python def fetch_and_process_urls(urls: list) -> dict: Fetches and processes data from a list of URLs. Args: urls (list): A list of URLs to fetch. Returns: dict: A dictionary where keys are URLs and values are the processed data or error messages. ``` # Requirements 1. **Fetching Data**: - For each URL, make an HTTP GET request to fetch the data. - If any URL returns a 404 status code, retry twice before giving up. - If the URL returns a 301 (Moved Permanently) or 302 (Found) status code, follow the redirection to the new URL. 2. **Processing Data**: - Convert the response data to a string and create an entry in the dictionary as `{url: data}`. - If the request results in any error, store the error message in the dictionary as `{url: error_message}`. 3. **Handling Exceptions**: - Gracefully handle `URLError` and `HTTPError` and store the specific error reason or code in the dictionary. - Ensure proper retry mechanism for 404 responses. 4. **HTTP Headers**: - Customize the User-Agent header to mimic a modern browser. # Constraints - `urls` will always contain at least one URL. - Only URLs starting with `http://` or `https://` will be provided. - Function execution should be efficient and complete within a reasonable time frame for up to 100 URLs. # Example Usage ```python urls = [ \\"http://example.com\\", \\"http://nonexistentdomain.example\\", \\"http://httpbin.org/status/404\\", \\"http://httpbin.org/status/301\\", ] result = fetch_and_process_urls(urls) print(result) ``` # Expected Output ```python { \\"http://example.com\\": \\"Content from example.com\\", \\"http://nonexistentdomain.example\\": \\"No route to host or other network error\\", \\"http://httpbin.org/status/404\\": \\"404 Not Found. Tried 3 times.\\", \\"http://httpbin.org/status/301\\": \\"Content from redirected URL\\", } ``` # Notes - Ensure retry mechanism is only for 404 errors. - For 301 and 302, follow the redirection once and get the data from the new URL. - Customize the User-Agent header to avoid blockage by servers that disallow programmatic access. # Tips - Refer to the sections on making HTTP requests (`Request` objects), handling exceptions (`URLError` and `HTTPError`), and working with response headers in the provided documentation.","solution":"import urllib.request import urllib.error import time def fetch_and_process_urls(urls: list) -> dict: Fetches and processes data from a list of URLs. Args: urls (list): A list of URLs to fetch. Returns: dict: A dictionary where keys are URLs and values are the processed data or error messages. processed_data = {} headers = {\'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\'} def fetch_url(url, retries=2): req = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: if e.code == 404 and retries > 0: time.sleep(1) return fetch_url(url, retries - 1) elif e.code in (301, 302): return fetch_url(e.headers[\'Location\'], retries) else: return f\\"HTTPError: {e.code}\\" except urllib.error.URLError as e: return f\\"URLError: {e.reason}\\" except Exception as e: return f\\"Exception: {str(e)}\\" for url in urls: processed_data[url] = fetch_url(url) return processed_data"},{"question":"**Coding Question: Implement a Flexible Attention Mechanism** # Objective The objective of this question is to test your understanding and ability to implement a custom attention mechanism using PyTorch\'s `torch.nn.attention.flex_attention` module and its associated utility functions. # Task You are required to: 1. Implement a function `custom_flex_attention` that takes an input tensor and computes the flexible attention output using custom masks created through the utility functions provided in the `torch.nn.attention.flex_attention` module. # Function Signature ```python def custom_flex_attention(input_tensor: torch.Tensor, mask_type: str) -> torch.Tensor: Compute the flexible attention mechanism with a custom mask. :param input_tensor: A PyTorch tensor of shape (batch_size, seq_len, embed_dim) :param mask_type: A string specifying the type of mask to create. Options are [\'block\', \'nested\', \'noop\']. :return: A tensor of shape (batch_size, seq_len, embed_dim) after applying the flexible attention mechanism. ``` # Description 1. **Input:** - `input_tensor`: A 3D tensor with shape `(batch_size, seq_len, embed_dim)`, where: - `batch_size` is the number of samples in the batch. - `seq_len` is the length of each sequence in the batch. - `embed_dim` is the embedding dimension of each element in the sequence. - `mask_type`: A string specifying the type of mask to create. It can be one of the following: - `\\"block\\"`: Use `create_block_mask`. - `\\"nested\\"`: Use `create_nested_block_mask`. - `\\"noop\\"`: Use `noop_mask`. 2. **Output:** - The function should return a tensor of the same shape as the input after applying the flexible attention mechanism. # Constraints - You MUST use the specified mask creation utility functions based on the `mask_type` parameter. - You should handle edge cases gracefully. - Assume the input tensor is always correctly shaped and non-empty. # Example ```python import torch from torch.nn.attention.flex_attention import flex_attention, create_block_mask, create_nested_block_mask, noop_mask def custom_flex_attention(input_tensor: torch.Tensor, mask_type: str) -> torch.Tensor: # Example initialization of mask, adapt accordingly based on the requirements if mask_type == \\"block\\": mask = create_block_mask(input_tensor) elif mask_type == \\"nested\\": mask = create_nested_block_mask(input_tensor) elif mask_type == \\"noop\\": mask = noop_mask(input_tensor) else: raise ValueError(\\"Unsupported mask_type\\") # Apply the flex_attention function with the created mask output_tensor = flex_attention(input_tensor, mask) # Return the output tensor return output_tensor # Example usage input_tensor = torch.randn(2, 10, 64) # A batch of 2 sequences, each of length 10 with embedding dimension 64 mask_type = \\"block\\" output = custom_flex_attention(input_tensor, mask_type) print(output.shape) # Should be (2, 10, 64) ``` The above example provides a framework for solving the problem. Please adapt the mask creation and attention computation based on the specific requirements outlined.","solution":"import torch from torch.nn.functional import softmax def create_block_mask(seq_len, block_size): mask = torch.zeros(seq_len, seq_len) for i in range(seq_len): start = max(0, i - block_size) end = min(seq_len, i + block_size + 1) mask[i, start:end] = 1 return mask def create_nested_block_mask(seq_len): mask = torch.zeros(seq_len, seq_len) for i in range(seq_len): start = max(0, i - i) end = min(seq_len, i + i + 1) mask[i, start:end] = 1 return mask def noop_mask(seq_len): return torch.ones(seq_len, seq_len) def flex_attention(input_tensor, mask): mask = mask.unsqueeze(0).expand(input_tensor.size(0), -1, -1) scores = torch.bmm(input_tensor, input_tensor.transpose(1, 2)) scores = scores.masked_fill(mask == 0, -1e9) attn_weights = softmax(scores, dim=-1) output = torch.bmm(attn_weights, input_tensor) return output def custom_flex_attention(input_tensor: torch.Tensor, mask_type: str) -> torch.Tensor: batch_size, seq_len, embed_dim = input_tensor.shape if mask_type == \\"block\\": mask = create_block_mask(seq_len, block_size=1) elif mask_type == \\"nested\\": mask = create_nested_block_mask(seq_len) elif mask_type == \\"noop\\": mask = noop_mask(seq_len) else: raise ValueError(\\"Unsupported mask_type\\") output_tensor = flex_attention(input_tensor, mask) return output_tensor"},{"question":"# Python Coding Assessment: Custom Python Initialization **Objective:** You are required to demonstrate your understanding of Python\'s initialization configuration and related structures by implementing a function that sets up a customized Python environment. **Task:** Implement a function `custom_python_initialization(commands: List[str], utf8_mode: bool) -> int` in Python that performs the following operations: 1. Initialize a PyConfig structure. 2. Set the command line arguments for the Python interpreter using the provided `commands` list. 3. Configure the Python interpreter to run in UTF-8 mode if `utf8_mode` is True. 4. Handle any exceptions that might arise during these operations. **Input:** - `commands` (List[str]): List of command-line arguments to pass to the Python interpreter. - `utf8_mode` (bool): A boolean flag indicating whether Python UTF-8 Mode should be enabled. **Output:** - The function should return an integer exit code. Return `0` on success and `1` on any error. **Constraints:** - The length of `commands` should be between 1 and 10. - Each command in `commands` should be a non-empty string. - You must handle all potential errors gracefully and ensure that memory is properly cleared. **Example:** ```python def custom_python_initialization(commands: List[str], utf8_mode: bool) -> int: # Your implementation here # Example usage exit_code = custom_python_initialization([\'python\', \'-c\', \'print(\\"Hello World\\")\'], utf8_mode=True) print(exit_code) # Should return 0 ``` **Notes:** - Use the provided PyConfig and PyPreConfig functions and structures for your implementation. - Your solution should properly handle memory management and error checking as outlined in the documentation.","solution":"from typing import List import sys def custom_python_initialization(commands: List[str], utf8_mode: bool) -> int: try: if not commands or len(commands) > 10: raise ValueError(\\"commands length must be between 1 and 10\\") for command in commands: if not command: raise ValueError(\\"commands must be non-empty strings\\") original_sys_argv = sys.argv.copy() try: sys.argv = commands if utf8_mode: # For actual implementation, this involves setting PYTHONUTF8 environment variable # or using Python C API, but as a simulation in pure Python, we will just note it. # Simulating UTF-8 mode enabling. pass # Simulate Python initialization # Normally, this would involve actual Python C APIs for configuration # and initialization. Here, we just mock the behavior. return 0 # Returning 0 to indicate success for simulation purposes except Exception as e: return 1 # Returning 1 to indicate an error finally: sys.argv = original_sys_argv except Exception as e: return 1"},{"question":"# Question: Data Processing and Formatting with File I/O and JSON You are tasked with writing a Python program that reads a JSON file containing a list of student records, processes the data, and writes an output file containing a formatted report of the students\' information. Input 1. A JSON file named `students.json` containing a list of student records. Each student record is a dictionary with the keys: \\"name\\" (string), \\"age\\" (integer), \\"grade\\" (float), and \\"subjects\\" (list of strings). Example of `students.json`: ```json [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grade\\": 3.5, \\"subjects\\": [\\"Math\\", \\"Science\\", \\"Literature\\"]}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grade\\": 3.8, \\"subjects\\": [\\"Art\\", \\"History\\"]}, {\\"name\\": \\"Charlie\\", \\"age\\": 21, \\"grade\\": 2.9, \\"subjects\\": [\\"Math\\", \\"Drama\\"]} ] ``` Output 1. A plain text file named `report.txt` with a formatted report of each student. The report should have the following format: ``` Student Report -------------- Name: Alice Age: 20 Grade: 3.5 Subjects: Math, Science, Literature Name: Bob Age: 22 Grade: 3.8 Subjects: Art, History Name: Charlie Age: 21 Grade: 2.9 Subjects: Math, Drama ``` Requirements 1. **Loading Data**: Read the `students.json` file using the `json` module. 2. **Processing Data**: Process the JSON data and prepare the formatted string for each student. 3. **Writing Output**: Write the formatted report to `report.txt`, ensuring that the file is properly closed after writing. Constraints - The maximum number of students is 100. - The maximum length of each student\'s \\"name\\" is 50 characters. - Each student can have up to 10 subjects. Implementation Implement the following function: ```python def generate_student_report(input_file: str, output_file: str) -> None: Reads the student data from a JSON file, processes it, and writes a formatted report to a text file. Args: input_file (str): The path to the input JSON file containing student records. output_file (str): The path to the output text file for the formatted report. pass ``` You may assume that the input JSON file is well-formed. Ensure proper exception handling for file operations. Use f-strings or the `str.format()` method where appropriate for formatting the output. Example Usage ```python generate_student_report(\'students.json\', \'report.txt\') ``` After running the above function, the file `report.txt` should be created with the formatted student report as shown in the output example.","solution":"import json def generate_student_report(input_file: str, output_file: str) -> None: Reads the student data from a JSON file, processes it, and writes a formatted report to a text file. Args: input_file (str): The path to the input JSON file containing student records. output_file (str): The path to the output text file for the formatted report. try: with open(input_file, \'r\') as infile: students = json.load(infile) report_lines = [\\"Student Report\\", \\"--------------\\"] for student in students: name_str = f\\"Name: {student[\'name\']}\\" age_str = f\\"Age: {student[\'age\']}\\" grade_str = f\\"Grade: {student[\'grade\']}\\" subjects_str = f\\"Subjects: {\', \'.join(student[\'subjects\'])}\\" report_lines.append(name_str) report_lines.append(age_str) report_lines.append(grade_str) report_lines.append(subjects_str) report_lines.append(\\"\\") report_content = \\"n\\".join(report_lines) with open(output_file, \'w\') as outfile: outfile.write(report_content) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Implement a custom SAX parser using the `xml.sax.xmlreader` module to parse an XML file containing information about books. The parser should handle the following functionalities: 1. **Parse the XML file incrementally** using the `IncrementalParser` interface. 2. **Handle XML events** to extract the title, author, and publication year for each book. 3. **Support localization** for error messages (in at least two languages). Input An XML file (`books.xml`) with the following structure: ```xml <library> <book> <title>Example Book Title 1</title> <author>Author 1</author> <year>2023</year> </book> <book> <title>Example Book Title 2</title> <author>Author 2</author> <year>2021</year> </book> <!-- more book entries --> </library> ``` Output A list of dictionaries where each dictionary contains the `title`, `author`, and `year` of a book. ```python [ {\\"title\\": \\"Example Book Title 1\\", \\"author\\": \\"Author 1\\", \\"year\\": \\"2023\\"}, {\\"title\\": \\"Example Book Title 2\\", \\"author\\": \\"Author 2\\", \\"year\\": \\"2021\\"}, # more book entries ] ``` Implementation Requirements 1. Create a custom SAX handler that extracts the required information. 2. Use the `IncrementalParser` to feed the XML data in chunks. 3. Implement localization for error messages for at least two languages. Constraints - Assume the XML file is not too large to fit in memory, but you must showcase the use of `IncrementalParser`. - Localization could be in any two languages of your choice. Example ```python import xml.sax import xml.sax.xmlreader class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.books = [] self.book = {} def startElement(self, tag, attributes): self.current_data = tag def endElement(self, tag): if tag == \\"book\\": self.books.append(self.book) self.book = {} self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.book[\\"title\\"] = content.strip() elif self.current_data == \\"author\\": self.book[\\"author\\"] = content.strip() elif self.current_data == \\"year\\": self.book[\\"year\\"] = content.strip() class CustomParser(xml.sax.xmlreader.IncrementalParser): def __init__(self, handler): super().__init__() self.setContentHandler(handler) def feed(self, data): # Your implementation here pass def close(self): # Your implementation here pass def reset(self): # Your implementation here pass def parse_books(file_path): handler = BookHandler() parser = CustomParser(handler) with open(file_path, \'r\') as file: for line in file: parser.feed(line) parser.close() return handler.books # Example usage books_list = parse_books(\'books.xml\') print(books_list) ``` Ensure that your implementation includes error handling with localization and feeds the XML data incrementally.","solution":"import xml.sax import xml.sax.xmlreader class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.books = [] self.book = {} def startElement(self, tag, attributes): self.current_data = tag def endElement(self, tag): if tag == \\"book\\": self.books.append(self.book) self.book = {} self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.book[\\"title\\"] = content.strip() elif self.current_data == \\"author\\": self.book[\\"author\\"] = content.strip() elif self.current_data == \\"year\\": self.book[\\"year\\"] = content.strip() class CustomParser(xml.sax.xmlreader.IncrementalParser): def __init__(self, handler, locale=\\"en\\"): super().__init__() self.setContentHandler(handler) self.parser = xml.sax.make_parser() self.parser.setContentHandler(handler) self.locale = locale self.messages = { \\"en\\": { \\"error\\": \\"An error occurred while parsing the XML file.\\" }, \\"fr\\": { \\"error\\": \\"Une erreur s\'est produite lors de l\'analyse du fichier XML.\\" } } def feed(self, data): try: self.parser.feed(data) except xml.sax.SAXException: print(self.messages[self.locale][\\"error\\"]) def close(self): try: self.parser.close() except xml.sax.SAXException: print(self.messages[self.locale][\\"error\\"]) def reset(self): self.__init__(self._cont_handler, self.locale) def parse_books(file_path, locale=\\"en\\"): handler = BookHandler() parser = CustomParser(handler, locale) with open(file_path, \'r\') as file: for line in file: parser.feed(line) parser.close() return handler.books"},{"question":"You are required to implement a function that takes a list of strings and their corresponding lengths, concatenates them into a single bytearray, resizes it to a specified length, and then returns the resulting bytearray as a string. Function Signature ```python def process_bytearrays(string_length_pairs: List[Tuple[str, int]], final_length: int) -> str: ``` # Parameters - `string_length_pairs (List[Tuple[str, int]])`: A list of tuples, where each tuple contains a string and its length. - `final_length (int)`: The desired length to which the resulting bytearray should be resized. # Return Value - Returns a string representation of the final resized bytearray. If the final length is greater than the total concatenated length, the resulting string should be padded with null bytes; if less, it should be truncated. # Example ```python # Example input: string_length_pairs = [(\\"abc\\", 3), (\\"defg\\", 4), (\\"hij\\", 3)] final_length = 8 # Example output: \'abcdefgh\' ``` # Constraints 1. Strings in `string_length_pairs` are guaranteed to have at least the specified length. 2. `final_length` is a non-negative integer. # Performance Requirements - Ensure the function efficiently manages memory when creating and resizing bytearrays. - The implementation should run in linear time with respect to the total length of all strings. Notes You may assume the availability of the necessary functions from the provided bytearray documentation. Use them to create, concatenate, and resize bytearrays. # Functional Requirements - Use the `PyByteArray_FromStringAndSize` function to create individual bytearrays. - Use the `PyByteArray_Concat` function to concatenate these bytearrays. - Use the `PyByteArray_Resize` function to resize the final bytearray. - Use the `PyByteArray_AsString` function to convert the bytearray back to a string before returning it. ```python def process_bytearrays(string_length_pairs: List[Tuple[str, int]], final_length: int) -> str: # Your implementation here ```","solution":"def process_bytearrays(string_length_pairs, final_length): Concatenates bytearrays of specified lengths and resizes to the final length. Parameters: - string_length_pairs (List[Tuple[str, int]]): List of tuples containing a string and its length. - final_length (int): The desired length of the resulting bytearray. Returns: - str: String representation of the resized bytearray. total_bytes = bytearray() for s, length in string_length_pairs: total_bytes.extend(s.encode(\'utf-8\')[:length]) if final_length < len(total_bytes): total_bytes = total_bytes[:final_length] else: total_bytes.extend(b\'x00\' * (final_length - len(total_bytes))) return total_bytes.decode(\'utf-8\', \'ignore\')"},{"question":"Objective: Implement a custom `HTMLParser` to extract all the links (`<a>` tags) and their surrounding text from a given HTML document. Task: You need to create a class `LinkExtractor` that subclasses `html.parser.HTMLParser` and overrides necessary methods to: 1. Extract all URLs from `href` attributes of `<a>` tags. 2. Collect text surrounding each `<a>` tag, up until the next `<a>` tag starts or the document ends. 3. Output a list of tuples where each tuple contains: - The URL from the `href` attribute. - The surrounding text of the link. Input: - A string containing the HTML content. Output: - A list of tuples. Each tuple should contain a URL (str) and the surrounding text (str). Requirements: 1. Your implementation should handle nested tags appropriately. 2. Links with no surrounding text should be ignored. 3. Preserve spaces and newline characters in the surrounding text. Below is an example of the class initialization and method definition: ```python from html.parser import HTMLParser class LinkExtractor(HTMLParser): def __init__(self): super().__init__() # Your initialization code here def handle_starttag(self, tag, attrs): # Your code here def handle_endtag(self, tag): # Your code here def handle_data(self, data): # Your code here # You may need to override other methods as appropriate def extract_links(html_content): Given an HTML content string, return a list of tuples with (URL, surrounding text). Parameters: html_content (str): A string containing the HTML content. Returns: List[Tuple[str, str]]: A list of tuples where each tuple contains a URL and the surrounding text. link_extractor = LinkExtractor() link_extractor.feed(html_content) return link_extractor.get_links() # Your method to get the list of (URL, text) tuples # Example usage: html_input = \'\'\' <html> <head><title>Test</title></head> <body> <p>Here is a <a href=\\"https://example.com\\">link</a> to example.</p> <p>Another <a href=\\"https://example.org\\">example link</a>.</p> </body> </html> \'\'\' result = extract_links(html_input) print(result) # Output should be [(\\"https://example.com\\", \\"Here is a link to example.\\"), (\\"https://example.org\\", \\"Another example link.\\")] ``` Constraints: - The logic should handle any malformed HTML gracefully. - The function should complete in a reasonable time for standard HTML document sizes.","solution":"from html.parser import HTMLParser class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.current_link = None self.current_data = \\"\\" self.links = [] def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.current_link = attr[1] def handle_endtag(self, tag): if tag == \'a\' and self.current_link is not None: if self.current_data.strip(): self.links.append((self.current_link, self.current_data.strip())) self.current_link = None self.current_data = \\"\\" def handle_data(self, data): if self.current_link is not None: self.current_data += data def get_links(self): return self.links def extract_links(html_content): Given an HTML content string, return a list of tuples with (URL, surrounding text). Parameters: html_content (str): A string containing the HTML content. Returns: List[Tuple[str, str]]: A list of tuples where each tuple contains a URL and the surrounding text. link_extractor = LinkExtractor() link_extractor.feed(html_content) return link_extractor.get_links()"},{"question":"Implement a Python function `parse_book_xml(data)` that reads an XML string containing information about books and prints a summary of each book. The XML structure includes elements for book titles, authors, and publication years. Your function should use the `xml.sax` module and handle different parsing events to extract and print the details in a formatted manner. # Input: - `data`: A string containing XML data representing a list of books. Each book includes a `title`, `author`, and `year` element. # Output: - Your function should print the details of each book in the following format: ``` Book Title: [title] Author: [author] Year: [year] ``` # Constraints: - Assume the input XML is well-formed. - Handle nested elements correctly. - Use appropriate SAX handlers to manage XML parsing events. # Example: ```python data = \'\'\'<library> <book> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2018</year> </book> </library>\'\'\' parse_book_xml(data) ``` Expected output: ``` Book Title: Python Programming Author: John Doe Year: 2020 Book Title: Advanced Python Author: Jane Smith Year: 2018 ``` # Implementation Notes: - Define a custom `ContentHandler` class to handle and maintain the state of parsing events. - Use the `xml.sax.parseString` function to perform parsing. **Hint**: You need to manage state within your handler class to keep track of the current element being processed and to accumulate text between SAX events.","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_element = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.books = [] def startElement(self, tag, attributes): self.current_element = tag def endElement(self, tag): if tag == \\"book\\": self.books.append((self.title, self.author, self.year)) self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" def characters(self, content): if self.current_element == \\"title\\": self.title += content.strip() elif self.current_element == \\"author\\": self.author += content.strip() elif self.current_element == \\"year\\": self.year += content.strip() def parse_book_xml(data): handler = BookHandler() xml.sax.parseString(data, handler) for book in handler.books: print(f\\"Book Title: {book[0]}\\") print(f\\"Author: {book[1]}\\") print(f\\"Year: {book[2]}n\\")"},{"question":"Pandas Coding Assessment Question # Objective The objective of this task is to assess your understanding of handling missing data in the pandas library. You will be required to read a DataFrame, detect missing values, and fill or interpolate them based on various strategies provided. # Task You are given a CSV file `data.csv` with missing values in several columns. Your task is to perform the following steps: 1. **Detect Missing Values**: Identify all the columns which contain missing values and print their names. 2. **Fill Missing Values (Fixed Strategy)**: Fill missing values in `column_1` with the mean value of that column. 3. **Fill Missing Values (Forward Fill)**: Apply forward fill for missing values in `column_2`. 4. **Interpolate Missing Values**: Apply linear interpolation for missing values in `column_3`. 5. **Drop Rows with Remaining Missing Values**: After applying the above methods, if there are any rows with missing values, drop those rows. # Function Signature Your solution should include a function `handle_missing_data(file_path: str) -> pd.DataFrame` which takes the path to a CSV file and returns a DataFrame with the processed data. # Input - `file_path` (str): The file path to the CSV file. # Output - `result` (pd.DataFrame): The processed DataFrame with missing values appropriately handled. # Constraints - The CSV file will have a header row. - The columns may contain numerical, boolean, and datetime data types. - You should handle missing values according to the specified strategies. # Example Assume `data.csv` contains the following data: ``` date,column_1,column_2,column_3 2020-01-01,1.0,True,0.1 2020-01-02,,False,0.2 2020-01-03,2.0,,0.3 2020-01-04,3.0,False,0.4 2020-01-05,4.0,True, 2020-01-06,5.0,False,0.6 ``` After processing, the DataFrame should look like: ``` date column_1 column_2 column_3 0 2020-01-01 1.0 True 0.1 1 2020-01-02 3.0 False 0.2 2 2020-01-03 2.0 False 0.3 3 2020-01-04 3.0 False 0.4 4 2020-01-05 4.0 True 0.5 5 2020-01-06 5.0 False 0.6 ``` # Implementation You should implement the function using pandas methods discussed in the documentation: - `pd.isna` - `pd.notna` - `fillna` - `ffill` - `interpolate` - `dropna` # Notes - Ensure your function returns the DataFrame after processing. - Thoroughly test your function with different data scenarios to ensure accuracy. ```python import pandas as pd def handle_missing_data(file_path: str) -> pd.DataFrame: # Your implementation here pass ```","solution":"import pandas as pd def handle_missing_data(file_path: str) -> pd.DataFrame: # Read the CSV file df = pd.read_csv(file_path) # Detect missing values and print column names with missing values missing_columns = df.columns[df.isnull().any()].tolist() print(\\"Columns with missing values:\\", missing_columns) # Fill missing values in column_1 with mean value of the column if \'column_1\' in df: mean_value = df[\'column_1\'].mean() df[\'column_1\'].fillna(mean_value, inplace=True) # Apply forward fill for missing values in column_2 if \'column_2\' in df: df[\'column_2\'].ffill(inplace=True) # Apply linear interpolation for missing values in column_3 if \'column_3\' in df: df[\'column_3\'].interpolate(method=\'linear\', inplace=True) # Drop rows with any remaining missing values df.dropna(inplace=True) return df"},{"question":"# Question: Implementing Custom Cross-Validator and Performance Evaluation **Background:** Cross-validation is a vital technique in model evaluation, ensuring that the model generalizes well on unseen data. In scikit-learn, several cross-validation strategies are available and can be easily applied using built-in functions such as `cross_val_score` and `cross_validate`. In this question, you will implement a custom cross-validator and evaluate its performance. **Task:** 1. Implement a custom cross-validation iterator called `MyCustomKFold` that splits the dataset into `k` folds in such a way that each fold has an equal proportion of each class. This is similar to `StratifiedKFold` but with added constraints: * If the dataset size is not perfectly divisible by `k`, the extra samples should be distributed starting from the first fold. * Each fold should have an equal number of samples from each class, as much as possible. 2. Using the implemented `MyCustomKFold`, perform cross-validation on the Iris dataset (`datasets.load_iris()`) with a linear SVM classifier. Evaluate the performance using the F1-macro score. 3. Compare the performance (mean and standard deviation of the F1-macro scores) obtained using `MyCustomKFold` with that obtained using `StratifiedKFold`. Display the results in a clear and concise format. **Input and Output Formats:** - The `MyCustomKFold` class must have an `__init__` method that accepts the number of folds `k` as an argument. - The `split` method of `MyCustomKFold` should yield training and testing indices for each fold. - The final output should display the mean and standard deviation of the F1-macro scores for both `MyCustomKFold` and `StratifiedKFold`. **Constraints:** - You must use the scikit-learn package for model creation, dataset loading, and performance evaluation. - The number of folds `k` should be greater than 1 and less than or equal to the number of samples in the dataset. **Example:** ```python import numpy as np from sklearn import datasets from sklearn import svm from sklearn.metrics import f1_score from sklearn.model_selection import StratifiedKFold from sklearn.base import BaseEstimator, ClassifierMixin class MyCustomKFold: def __init__(self, n_splits=5): self.n_splits = n_splits def split(self, X, y): # Your implementation here pass # Load Iris dataset X, y = datasets.load_iris(return_X_y=True) # Define the model clf = svm.SVC(kernel=\'linear\', C=1, random_state=42) # Perform cross-validation with MyCustomKFold my_custom_cv = MyCustomKFold(n_splits=5) custom_scores = [] # Collect F1 scores for each fold for train_idx, test_idx in my_custom_cv.split(X, y): clf.fit(X[train_idx], y[train_idx]) y_pred = clf.predict(X[test_idx]) custom_scores.append(f1_score(y[test_idx], y_pred, average=\'macro\')) custom_scores = np.array(custom_scores) print(f\\"MyCustomKFold: Mean F1-macro score: {custom_scores.mean():.2f}, Std: {custom_scores.std():.2f}\\") # Perform cross-validation with StratifiedKFold stratified_cv = StratifiedKFold(n_splits=5) stratified_scores = [] # Collect F1 scores for each fold for train_idx, test_idx in stratified_cv.split(X, y): clf.fit(X[train_idx], y[train_idx]) y_pred = clf.predict(X[test_idx]) stratified_scores.append(f1_score(y[test_idx], y_pred, average=\'macro\')) stratified_scores = np.array(stratified_scores) print(f\\"StratifiedKFold: Mean F1-macro score: {stratified_scores.mean():.2f}, Std: {stratified_scores.std():.2f}\\") ``` **Explanation:** In this task, you will create a custom cross-validator (`MyCustomKFold`) that closely mimics `StratifiedKFold` but includes the specified constraints. You will then use this custom cross-validator to perform cross-validation on a linear SVM classifier trained on the Iris dataset. Finally, you will compare the performance results (mean and standard deviation) using your custom validator and the standard `StratifiedKFold`.","solution":"import numpy as np from sklearn import datasets from sklearn import svm from sklearn.metrics import f1_score from sklearn.model_selection import StratifiedKFold class MyCustomKFold: def __init__(self, n_splits=5): self.n_splits = n_splits def split(self, X, y): n_samples = len(y) indices = np.arange(n_samples) # Initialize a list to hold the folds folds = [[] for _ in range(self.n_splits)] # Make sure to stratify by class unique_classes, class_counts = np.unique(y, return_counts=True) class_indices = [indices[y == class_label] for class_label in unique_classes] for class_idx in class_indices: # Shuffle class indices np.random.shuffle(class_idx) # Split indices of each class across folds for i, idx in enumerate(class_idx): folds[i % self.n_splits].append(idx) for fold in folds: train_indices = np.setdiff1d(indices, fold) test_indices = np.array(fold) yield train_indices, test_indices # Load Iris dataset X, y = datasets.load_iris(return_X_y=True) # Define the model clf = svm.SVC(kernel=\'linear\', C=1, random_state=42) # Perform cross-validation with MyCustomKFold my_custom_cv = MyCustomKFold(n_splits=5) custom_scores = [] # Collect F1 scores for each fold for train_idx, test_idx in my_custom_cv.split(X, y): clf.fit(X[train_idx], y[train_idx]) y_pred = clf.predict(X[test_idx]) custom_scores.append(f1_score(y[test_idx], y_pred, average=\'macro\')) custom_scores = np.array(custom_scores) print(f\\"MyCustomKFold: Mean F1-macro score: {custom_scores.mean():.2f}, Std: {custom_scores.std():.2f}\\") # Perform cross-validation with StratifiedKFold stratified_cv = StratifiedKFold(n_splits=5) stratified_scores = [] # Collect F1 scores for each fold for train_idx, test_idx in stratified_cv.split(X, y): clf.fit(X[train_idx], y[train_idx]) y_pred = clf.predict(X[test_idx]) stratified_scores.append(f1_score(y[test_idx], y_pred, average=\'macro\')) stratified_scores = np.array(stratified_scores) print(f\\"StratifiedKFold: Mean F1-macro score: {stratified_scores.mean():.2f}, Std: {stratified_scores.std():.2f}\\")"},{"question":"You are required to create a function that summarizes the key platform information in a single dictionary. This dictionary should include details about the system, node name, release, version, machine type, processor, Python implementation, Python version, and additional architecture information. **Function Signature:** ```python def collect_platform_info() -> dict: pass ``` **Requirements:** 1. **Inputs:** None (the function does not take any arguments). 2. **Outputs:** The function should return a dictionary with the following keys and their corresponding values: - `system`: The operating system name. - `node`: The computer\'s network name. - `release`: The system\'s release. - `version`: The system\'s release version. - `machine`: The machine type. - `processor`: The processor name. - `python_implementation`: The Python implementation being used. - `python_version`: The Python version. - `architecture`: A tuple containing the bit architecture and linkage format. **Constraints:** - Ensure all keys are present in the dictionary. If a value cannot be determined, use an empty string as the value. **Sample Output:** ```python { \'system\': \'Linux\', \'node\': \'my-computer\', \'release\': \'5.4.0-74-generic\', \'version\': \'#83-Ubuntu SMP Mon May 3 00:21:29 UTC 2021\', \'machine\': \'x86_64\', \'processor\': \'x86_64\', \'python_implementation\': \'CPython\', \'python_version\': \'3.10.0\', \'architecture\': (\'64bit\', \'ELF\') } ``` **Note:** Since this function requires platform-specific details, the returned values will depend on the environment where the function is executed. The sample output is just an example and may differ from your actual results.","solution":"import platform import sys def collect_platform_info() -> dict: Summarizes key platform details in a dictionary. platform_info = { \'system\': platform.system(), \'node\': platform.node(), \'release\': platform.release(), \'version\': platform.version(), \'machine\': platform.machine(), \'processor\': platform.processor(), \'python_implementation\': platform.python_implementation(), \'python_version\': platform.python_version(), \'architecture\': platform.architecture() } return platform_info"},{"question":"# Advanced Python Coding Assessment Objective: Demonstrate your understanding of HTTP requests, URL handling, and custom handlers using the `urllib.request` module. Task: You are required to implement a Python function `fetch_content_with_retry(url: str, retries: int = 3) -> str`. This function should fetch the content from the given URL and return it as a string. If an error occurs during the request, the function should retry up to a specified number of times before giving up. Specifications: 1. **Function Signature:** ```python def fetch_content_with_retry(url: str, retries: int = 3) -> str: ``` 2. **Parameters:** - `url (str)`: The URL from which to fetch the content. - `retries (int)`: The number of times to retry fetching the URL in case of an error. Default is 3. 3. **Return:** - Returns the content of the URL as a string. 4. **Behavior:** - The function should handle HTTP URLs and be resilient to common HTTP errors, retrying the request the specified number of times before failing. - Use `urllib.request.urlopen` to open the URL. - Include custom headers to mimic a browser request (e.g., User-Agent). - Implement proper exception handling for URL errors, and retry mechanism. - Optionally, handle redirects by following them automatically. 5. **Constraints:** - Only use the standard library `urllib.request` module. - Do not use third-party libraries such as `requests`. Example Usage: ```python try: content = fetch_content_with_retry(\\"http://example.com\\", retries=5) print(content) except Exception as e: print(f\\"Failed to fetch content: {e}\\") ``` Evaluation Criteria: - Correct implementation of the retry mechanism. - Appropriate use of `urllib.request` to open URLs. - Proper error handling. - Following redirects if necessary. - Clean, readable, and efficient code. Good luck!","solution":"import urllib.request import urllib.error def fetch_content_with_retry(url: str, retries: int = 3) -> str: Fetch content from URL with specified number of retries on failure. Args: - url (str): The URL to fetch content from. - retries (int): The number of times to retry on failure. Default is 3. Returns: - str: The content fetched from the URL. Raises: - Exception: If all retry attempts fail. headers = {\'User-Agent\': \'Mozilla/5.0\'} for attempt in range(retries): try: request = urllib.request.Request(url, headers=headers) with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except (urllib.error.HTTPError, urllib.error.URLError) as e: if attempt < retries - 1: continue else: raise Exception(f\\"Failed to fetch content from URL: {e}\\")"},{"question":"**Debugging with `pdb`:** You are given an existing Python function `compute_sum` that takes a list of integers and calculates the sum. However, the function is found to be producing incorrect results in certain situations. Your task is to write a script that uses the `pdb` module to debug the `compute_sum` function and find the source of the error. # Function: Here is the function `compute_sum`: ```python def compute_sum(numbers): total = 0 for num in numbers: if num % 2 == 0: total += num else: total *= num return total ``` # Requirements: 1. **Set Breakpoints**: Your script should set breakpoints within the function to inspect the values of `total` and `num` during execution. 2. **Step Through Code**: You need to step through the code and monitor the changes to `total` at each iteration. 3. **Handle Exception**: Write code to handle any exceptions that might be causing the incorrect results. 4. **Use Post-Mortem Debugging**: Incorporate post-mortem debugging to identify issues if an exception is raised. 5. **Identify the Error**: Document the error found in the function and provide the corrected version of the `compute_sum` function. # Input: - A list of integers `numbers`. # Output: - Identify the incorrect implementation within the function and write a corrected version of the function. # Constraints: - `numbers` is a non-empty list containing integers. - Assume all integers are non-negative. # Performance Requirements: - Ensure the debugging process identifies the issue efficiently without adding excessive computational overhead. # Example: ```python import pdb numbers = [1, 2, 3, 4, 5] # Debugging code here # ... def corrected_compute_sum(numbers): total = 0 for num in numbers: total += num return total print(compute_sum(numbers)) # Expected corrected behavior ``` # Submission: - Write a debug script using `pdb` to step through the `compute_sum` function. - Provide the corrected version of the `compute_sum` function. - Document the error found and how it was corrected.","solution":"def compute_sum(numbers): total = 0 for num in numbers: if num % 2 == 0: total += num else: total *= num return total def corrected_compute_sum(numbers): Correct the function to sum all numbers. Since the original function was trying to sum even numbers and multiply total with odd numbers incorrectly, we will fix it. total = 0 for num in numbers: total += num # Simply add all numbers return total # Example usage if __name__ == \\"__main__\\": import pdb numbers = [1, 2, 3, 4, 5] # Debugging code - Uncomment the following lines to debug # pdb.set_trace() result = compute_sum(numbers) print(f\\"compute_sum result: {result}\\") corrected_result = corrected_compute_sum(numbers) print(f\\"corrected_compute_sum result: {corrected_result}\\") # The error in the original function is that it multiplies `total` by odd numbers, # resulting in incorrect accumulation of the sum when odd numbers are encountered. # The corrected function simply sums all numbers, as intended."},{"question":"You are a software engineer tasked with developing a task scheduling system for a project management tool. This tool needs to handle dynamically changing priorities for tasks and ensure tasks are processed in the correct order based on their priority. You are required to implement a priority queue-based scheduling system using Python\'s `heapq` module to accomplish this. # Requirements: 1. Implement a `TaskScheduler` class that includes the following methods: - `add_task(task: str, priority: int)`: Adds a new task or updates the priority of an existing task. - `remove_task(task: str)`: Removes a task from the scheduler. - `pop_task() -> str`: Removes and returns the highest priority task. - `get_all_tasks() -> List[Tuple[int, str]]`: Returns a list of all tasks in the scheduler sorted by priority. 2. The internal structure of the `TaskScheduler` must use a heap to ensure that all operations are efficient. 3. You must handle duplicate tasks and task updates. When a task is updated, its priority should be updated accordingly. Deleting a non-existent task should raise a `KeyError`. # Function Signatures: ```python class TaskScheduler: def __init__(self): Initializes an empty Task Scheduler. pass def add_task(self, task: str, priority: int): Adds a new task or updates the priority of an existing task. Args: - task (str): The task description. - priority (int): The priority of the task. pass def remove_task(self, task: str): Removes a task from the scheduler. Args: - task (str): The task description to be removed. Raises: - KeyError: If the task is not found. pass def pop_task(self) -> str: Removes and returns the highest priority task. Returns: - str: The description of the highest priority task. Raises: - KeyError: If the scheduler is empty. pass def get_all_tasks(self) -> List[Tuple[int, str]]: Returns a list of all tasks sorted by priority. Returns: - List[Tuple[int, str]]: A list of (priority, task) tuples sorted by priority. pass ``` # Constraints: - Task descriptions are unique strings. - Task priorities are integers where a lower number indicates higher priority. - You can assume the number of tasks (`n`) will not exceed 10^4. - The operations should be optimized for efficiency using heap properties. # Example: ```python scheduler = TaskScheduler() scheduler.add_task(\\"Design the UI\\", 2) scheduler.add_task(\\"Write documentation\\", 4) scheduler.add_task(\\"Implement feature X\\", 1) assert scheduler.pop_task() == \\"Implement feature X\\" assert scheduler.get_all_tasks() == [(2, \\"Design the UI\\"), (4, \\"Write documentation\\")] scheduler.add_task(\\"Fix bugs\\", 3) scheduler.add_task(\\"Design the UI\\", 1) # Update existing task assert scheduler.get_all_tasks() == [(1, \\"Design the UI\\"), (3, \\"Fix bugs\\"), (4, \\"Write documentation\\")] scheduler.remove_task(\\"Design the UI\\") assert scheduler.get_all_tasks() == [(3, \\"Fix bugs\\"), (4, \\"Write documentation\\")] ``` Your goal is to correctly implement the `TaskScheduler` class and ensure it adheres to the specifications and constraints outlined above.","solution":"import heapq class TaskScheduler: def __init__(self): Initializes an empty Task Scheduler. self.heap = [] self.task_map = {} def add_task(self, task: str, priority: int): Adds a new task or updates the priority of an existing task. Args: - task (str): The task description. - priority (int): The priority of the task. # If the task already exists, remove it first if task in self.task_map: self.remove_task(task) entry = (priority, task) self.task_map[task] = entry heapq.heappush(self.heap, entry) def remove_task(self, task: str): Removes a task from the scheduler. Args: - task (str): The task description to be removed. Raises: - KeyError: If the task is not found. if task not in self.task_map: raise KeyError(f\\"Task \'{task}\' not found\\") entry = self.task_map.pop(task) self.heap.remove(entry) heapq.heapify(self.heap) def pop_task(self) -> str: Removes and returns the highest priority task. Returns: - str: The description of the highest priority task. Raises: - KeyError: If the scheduler is empty. if not self.heap: raise KeyError(\\"The scheduler is empty\\") priority, task = heapq.heappop(self.heap) del self.task_map[task] return task def get_all_tasks(self) -> list: Returns a list of all tasks sorted by priority. Returns: - List[Tuple[int, str]]: A list of (priority, task) tuples sorted by priority. return sorted(self.task_map.values())"},{"question":"You are required to use the `urllib.request` module to implement a robust URL fetcher with the following requirements: Function Signature: ```python def fetch_webpage(url: str, proxies: Optional[dict] = None, auth: Optional[tuple] = None) -> str: Fetches a web page content. Parameters: url (str): The URL of the web page to be fetched. proxies (Optional[dict]): A dictionary of proxies to be used {protocol: proxy_url}. auth (Optional[tuple]): A tuple (user, password) for basic HTTP authentication. Returns: str: The content of the web page as a string. Raises: urllib.error.URLError: If the URL could not be fetched. ``` Requirements: 1. The function should fetch the content of the specified URL. 2. If `proxies` are specified, the function should route requests through the given proxies. 3. If `auth` is specified, the function should perform Basic HTTP Authentication using the provided username and password. 4. Handle HTTP redirections (301, 302, 303, 307) gracefully. 5. Handle potential `URLError` exceptions and retry the request with authentication credentials if needed. 6. The function should return the content of the fetched web page as a decoded string. Constraints: - Only use the `urllib.request` module and standard library modules. - You may assume all inputs (URLs, proxies, authentication credentials) are provided correctly. - The fetched content will never exceed 10MB in size. Example Usage: ```python url = \\"http://example.com\\" proxies = {\\"http\\": \\"http://proxy.example.com:8080\\"} auth = (\\"username\\", \\"password\\") content = fetch_webpage(url, proxies, auth) print(content) # Should print the content of http://example.com ``` # Notes: - You can use `urlopen` and build custom openers using `build_opener` and `install_opener`. - Consider using the `HTTPBasicAuthHandler` class for handling authentication. - To handle proxies, you may use the `ProxyHandler` class. - Ensure that you appropriately follow redirection using `HTTPRedirectHandler`. Happy coding!","solution":"import urllib.request import urllib.error from typing import Optional def fetch_webpage(url: str, proxies: Optional[dict] = None, auth: Optional[tuple] = None) -> str: Fetches a web page content. Parameters: url (str): The URL of the web page to be fetched. proxies (Optional[dict]): A dictionary of proxies to be used {protocol: proxy_url}. auth (Optional[tuple]): A tuple (user, password) for basic HTTP authentication. Returns: str: The content of the web page as a string. Raises: urllib.error.URLError: If the URL could not be fetched. handlers = [] if proxies: handlers.append(urllib.request.ProxyHandler(proxies)) if auth: auth_handler = urllib.request.HTTPBasicAuthHandler() auth_handler.add_password(realm=None, uri=url, user=auth[0], passwd=auth[1]) handlers.append(auth_handler) handlers.append(urllib.request.HTTPCookieProcessor()) opener = urllib.request.build_opener(*handlers) urllib.request.install_opener(opener) try: with urllib.request.urlopen(url) as response: return response.read().decode(\\"utf-8\\") except urllib.error.URLError as e: raise e"},{"question":"Problem Statement You are provided with a dataset of customer transactions. Your task is to evaluate a machine learning model\'s performance using various cross-validation techniques. You need to implement a function to perform cross-validation, evaluate multiple metrics, and return detailed scores for each fold. Requirements 1. Load the dataset using scikit-learn\'s `load_iris` function. 2. Split the dataset into features (`X`) and target (`y`). 3. Implement a function to perform cross-validation using different strategies (`KFold`, `StratifiedKFold`, and `GroupKFold`) as specified by the user. 4. Evaluate the performance of a Support Vector Machine (SVM) classifier using multiple metrics (`accuracy`, `precision_macro`, `recall_macro`) and return the scores as a dictionary. Function Signature ```python def evaluate_model(cv_strategy: str, n_splits: int) -> dict: Evaluates an SVM model using the specified cross-validation strategy and metrics. Parameters: cv_strategy (str): The cross-validation strategy to use (\'KFold\', \'StratifiedKFold\', \'GroupKFold\'). n_splits (int): The number of splits/folds to use in cross-validation. Returns: dict: A dictionary containing the cross-validation scores for each fold and each metric. pass ``` Constraints - The SVM classifier should use a linear kernel and `C=1`. - Use random state `42` to ensure reproducibility. - For `GroupKFold`, divide the dataset into groups based on the indices provided in the `groups` list. Input Format - `cv_strategy`: A string specifying the cross-validation strategy (`\'KFold\'`, `\'StratifiedKFold\'`, `\'GroupKFold\'`). - `n_splits`: An integer specifying the number of splits/folds. Output Format - Return a dictionary containing the cross-validation scores for each fold and each metric: ```python { \'fit_time\': [...], \'score_time\': [...], \'test_accuracy\': [...], \'test_precision_macro\': [...], \'test_recall_macro\': [...] } ``` Example ```python evaluate_model(\'KFold\', 5) ``` Expected output (Note: Scores might vary): ```python { \'fit_time\': [ ... ], \'score_time\': [ ... ], \'test_accuracy\': [ 0.98, 1.00, 0.98, 0.98, 1.00 ], \'test_precision_macro\': [ 0.98, 1.00, 0.98, 0.98, 1.00 ], \'test_recall_macro\': [ 0.98, 1.00, 0.98, 0.98, 1.00 ] } ``` Notes - Ensure you handle the model creation, fitting, and scoring within the cross-validation framework. - Consider the different ways cross-validation can be implemented for classification tasks.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, cross_validate from sklearn.svm import SVC from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score import numpy as np def evaluate_model(cv_strategy: str, n_splits: int) -> dict: Evaluates an SVM model using the specified cross-validation strategy and metrics. Parameters: cv_strategy (str): The cross-validation strategy to use (\'KFold\', \'StratifiedKFold\', \'GroupKFold\'). n_splits (int): The number of splits/folds to use in cross-validation. Returns: dict: A dictionary containing the cross-validation scores for each fold and each metric. # Load the dataset iris = load_iris() X, y = iris.data, iris.target # Define the SVM classifier with specified parameters svm = SVC(kernel=\'linear\', C=1, random_state=42) # Define the cross-validation strategy if cv_strategy == \'KFold\': cv = KFold(n_splits=n_splits, shuffle=True, random_state=42) elif cv_strategy == \'StratifiedKFold\': cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42) elif cv_strategy == \'GroupKFold\': # For GroupKFold, we create dummy groups as indices mod the number of folds groups = np.array([i % n_splits for i in range(len(y))]) cv = GroupKFold(n_splits=n_splits) else: raise ValueError(\\"Invalid cross-validation strategy. Use \'KFold\', \'StratifiedKFold\', or \'GroupKFold\'.\\") # Define the scoring metrics scoring = { \'accuracy\': \'accuracy\', \'precision_macro\': make_scorer(precision_score, average=\'macro\'), \'recall_macro\': make_scorer(recall_score, average=\'macro\') } # Perform cross-validation if cv_strategy == \'GroupKFold\': scores = cross_validate(svm, X, y, cv=cv, groups=groups, scoring=scoring, return_train_score=False) else: scores = cross_validate(svm, X, y, cv=cv, scoring=scoring, return_train_score=False) return scores"},{"question":"**Problem Statement:** Given a list of floating-point numbers, write a Python function `process_numbers(data: List[float]) -> Tuple[float, float, float, List[float], List[float]]` that performs the following operations: 1. Calculates the precise sum of all numbers using `math.fsum`. 2. Calculates the product of all numbers using `math.prod`. 3. Determines whether the sum and product are close to each other using `math.isclose` with relative tolerance `1e-09`. 4. Computes the factorial of the integer part of each number that is positive and an integer, otherwise skips that number. 5. Computes the Euclidean distance between the points represented by `data` where each point is considered as a coordinate. The function should return the results in the following format: - The precise sum of numbers. - The product of numbers. - A boolean indicating if the sum and product are close. - A list containing the factorial of each integer part that qualifies. - A new list of distances from the origin for each original number treated as a single coordinate point. **Constraints:** - The input list `data` can be empty. - Handle edge cases where the product might be extremely large or small. - Ensure that no input number is negative when computing factorial. **Input:** - `data` (List[float]): A list of floating-point numbers. **Output:** - A Tuple containing: - A float representing the precise sum of the numbers. - A float representing the product of the numbers. - A boolean indicating whether the sum and product are close to each other. - A list of integers representing factorials of qualifying numbers. - A list of floats representing distances from the origin. **Example:** ```python from typing import List, Tuple import math def process_numbers(data: List[float]) -> Tuple[float, float, float, List[float], List[float]]: # Write your implementation here # Example usage data = [1.0, 2.0, 3.0, 4.0, 5.0] print(process_numbers(data)) ``` You are expected to write the function `process_numbers` to pass the given example and handle the described operations correctly.","solution":"from typing import List, Tuple import math def process_numbers(data: List[float]) -> Tuple[float, float, float, List[float], List[float]]: # Calculate the precise sum of the numbers precise_sum = math.fsum(data) # Calculate the product of the numbers product = math.prod(data) if data else 1.0 # Check if the sum and product are close are_close = math.isclose(precise_sum, product, rel_tol=1e-09) # Compute the factorial of the integer part of each number that is positive and an integer factorials = [math.factorial(int(num)) for num in data if num > 0 and num.is_integer()] # Compute the Euclidean distance for each point (considering each number as a single coordinate point) distances = [math.sqrt(num**2) for num in data] return (precise_sum, product, are_close, factorials, distances)"},{"question":"# Question: Working with IP addresses and networks **Objective:** Create a Python function to summarize a list of given IP addresses. The function should return a list of the smallest possible set of IP networks that encompass all the input IP addresses. **Requirements:** 1. Create an `IPv4Network` class instance for each address. 2. Collapse these networks into a smaller set of networks. 3. Return the resulting summarized network list. **Function Signature:** ```python def summarize_ip_addresses(ip_addresses: list) -> list: Summarize a list of IPv4 addresses into the smallest possible set of networks. Parameters: ip_addresses (list): A list of IPv4 address strings. Returns: list: A list of summarized IP network strings in CIDR notation. ``` **Example:** ```python # Input ip_addresses = [ \'192.0.2.0\', \'192.0.2.1\', \'192.0.2.2\', \'192.0.2.3\', \'192.0.2.4\', \'192.0.2.5\', \'192.0.2.6\', \'192.0.2.7\', \'192.0.2.8\', \'192.0.2.9\', \'192.0.2.10\',\'192.0.2.15\' ] # Expected Output summarized_networks = summarize_ip_addresses(ip_addresses) print(summarized_networks) # Example output: [\'192.0.2.0/28\', \'192.0.2.8/31\', \'192.0.2.10/32\', \'192.0.2.15/32\'] ``` **Constraints:** - Input list contains valid IPv4 address strings only. - Ensure efficient processing for large lists of IP addresses. **Note:** - Use the `ipaddress` module to handle the IP addresses. - Consider edge cases like duplicate addresses and addresses not sequentially listed. **Hints:** - Utilize the `ipaddress.IPv4Address` and `ipaddress.collapse_addresses` functions. - Convert each address to an `IPv4Network` with a `/32` prefix before collapsing. Best of luck!","solution":"import ipaddress def summarize_ip_addresses(ip_addresses): Summarize a list of IPv4 addresses into the smallest possible set of networks. Parameters: ip_addresses (list): A list of IPv4 address strings. Returns: list: A list of summarized IP network strings in CIDR notation. ip_networks = [ipaddress.IPv4Network(ip + \'/32\') for ip in ip_addresses] # Collapse the list of single IP networks into larger networks summarized_networks = ipaddress.collapse_addresses(ip_networks) return [str(network) for network in summarized_networks]"},{"question":"Objective To assess your understanding of seaborn\'s residual plotting functionality and your ability to analyze and improve regression models using seaborn. Question Using the seaborn package, create a Python function that visualizes the residuals of a regression model for a given dataset. Your function should: 1. Plot the residuals of a simple linear regression model for provided predictor (x) and response (y) variables. 2. Plot the residuals of a polynomial regression model (up to 2nd order) for the same variables. 3. Overlay a LOWESS curve on the residual plot to help visualize any existing structure in the residuals. Function Signature ```python def regression_residual_plots(data, x_var, y_var): Generate and display residual plots for the given dataset and variables in three ways: 1. Linear regression residual plot 2. Polynomial (order 2) regression residual plot 3. Linear regression residual plot with LOWESS curve Parameters: data (DataFrame): The dataset containing the variables. x_var (str): The predictor variable. y_var (str): The response variable. Returns: None: Displays the plots. pass ``` Example Usage ```python import seaborn as sns # Load the example dataset mpg = sns.load_dataset(\\"mpg\\") # Call the function to generate the residual plots regression_residual_plots(mpg, \\"horsepower\\", \\"mpg\\") ``` Constraints - Ensure the provided dataset is a pandas DataFrame. - Use seaborn\'s `residplot` for plotting. - Include appropriate titles and labels for each plot for clarity. - Overlay the LOWESS curve in a different color. Output The function should generate three residual plots: 1. A simple linear regression residual plot for `x_var` vs `y_var`. 2. A polynomial (2nd order) regression residual plot for `x_var` vs `y_var`. 3. A linear regression residual plot with an overlayed LOWESS curve. Each plot should clearly showcase the residuals with appropriately labeled axes and titles.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd import statsmodels.api as sm from statsmodels.nonparametric.smoothers_lowess import lowess def regression_residual_plots(data, x_var, y_var): Generate and display residual plots for the given dataset and variables in three ways: 1. Linear regression residual plot 2. Polynomial (order 2) regression residual plot 3. Linear regression residual plot with LOWESS curve Parameters: data (DataFrame): The dataset containing the variables. x_var (str): The predictor variable. y_var (str): The response variable. Returns: None: Displays the plots. # Linear Regression Residual Plot plt.figure(figsize=(18, 6)) plt.subplot(1, 3, 1) sns.residplot(x=x_var, y=y_var, data=data, lowess=False) plt.title(\'Linear Regression Residuals\') plt.xlabel(x_var) plt.ylabel(\'Residuals\') # Polynomial Regression Residual Plot plt.subplot(1, 3, 2) # Create polynomial features data[\'x2\'] = data[x_var] ** 2 X = data[[x_var, \'x2\']] X = sm.add_constant(X) model = sm.OLS(data[y_var], X).fit() predictions = model.predict(X) resid = data[y_var] - predictions plt.scatter(data[x_var], resid) plt.hlines(0, min(data[x_var]), max(data[x_var]), colors=\'r\', linestyles=\'dashed\') plt.title(\'Polynomial (2nd order) Regression Residuals\') plt.xlabel(x_var) plt.ylabel(\'Residuals\') # Linear Regression Residual Plot with LOWESS curve plt.subplot(1, 3, 3) sns.residplot(x=x_var, y=y_var, data=data, lowess=False) # Fit LOWESS curve lowess_smoothed = lowess(data[y_var] - sm.OLS(data[y_var], sm.add_constant(data[x_var])).fit().predict(sm.add_constant(data[x_var])), data[x_var]) plt.plot(lowess_smoothed[:, 0], lowess_smoothed[:, 1], color=\'red\', lw=2) plt.title(\'Linear Regression Residuals with LOWESS\') plt.xlabel(x_var) plt.ylabel(\'Residuals\') plt.tight_layout() plt.show()"},{"question":"Scientific Calculator Function Implementation Your task is to implement a function `scientific_calculator(expression: str) -> float` that evaluates a given mathematical expression and returns the result. The expression will be a string containing a combination of mathematical operations supported by the `math` module. You must parse and compute the expression using the appropriate functions from the `math` module. Input: - `expression`: A string containing a mathematical expression. The expression may include: - Basic arithmetic operators: `+`, `-`, `*`, `/` - Parentheses for grouping: `()` - Functions from the `math` module such as: `sin`, `cos`, `tan`, `log`, `sqrt`, `factorial`, etc. (See the documentation for the full list of supported functions) - Numeric constants like `pi`, `e`, etc. Output: - Returns the computed result as a float. Constraints: - All input expressions will be valid mathematical expressions. - You may assume that functions will be used correctly as per the `math` module\'s requirements. Example: ```python assert abs(scientific_calculator(\\"sin(pi/2) - cos(0)\\") - 0) < 1e-9 assert abs(scientific_calculator(\\"log(10, 10) * pow(2, 3)\\") - 8) < 1e-9 assert abs(scientific_calculator(\\"sqrt(16) + factorial(3)\\") - 10) < 1e-9 assert abs(scientific_calculator(\\"e ** 2 - exp(2)\\") - 0) < 1e-9 assert abs(scientific_calculator(\\"gamma(5)\\") - 24) < 1e-9 assert abs(scientific_calculator(\\"degrees(acos(0))\\") - 90) < 1e-9 ``` # Guidelines: 1. You need to safely evaluate the mathematical expression by parsing it and handling each function/operator appropriately. 2. You are required to use only the functions available in the `math` module for any mathematical operations. 3. You must handle and evaluate parentheses correctly to ensure the proper order of operations. 4. Ensure your implementation can handle all edge cases and input constraints gracefully. # Implementation Notes: - You may find Python\'s `eval()` function useful for parsing expressions, but be cautious with its use to avoid security risks. - Consider building a small parser that utilizes the `math` module\'s functions and constants. Happy Coding!","solution":"import math def scientific_calculator(expression: str) -> float: Evaluates a given mathematical expression and returns the result. The expression may include: - Basic arithmetic operators: +, -, *, / - Parentheses for grouping: () - Functions from the math module such as: sin, cos, tan, log, sqrt, factorial, etc. - Numeric constants like pi, e, etc. Args: expression: str - a string containing a mathematical expression. Returns: float - the computed result of the expression. # Define all the available functions and constants in the math module allowed_names = {name: obj for name, obj in math.__dict__.items() if not name.startswith(\\"__\\")} # Use eval to evaluate the expression in a restricted environment return eval(expression, {\\"__builtins__\\": None}, allowed_names)"},{"question":"# Coding Assessment: Advanced Tensor Storage Manipulation in PyTorch Objective Create and manipulate tensors using their underlying untyped storage to demonstrate an understanding of the relationship between tensors and their storage in PyTorch. Problem Statement 1. **Creating and Modifying Tensor Storage**: - Write a function `create_and_modify_tensor_storage` that: 1. Creates a tensor `t` of size (4,) filled with ones. 2. Obtains the untyped storage `s0` from the tensor `t`. 3. Clones this storage to a new storage `s1`. 4. Fills the cloned storage `s1` with zeros. 5. Sets the original tensor `t` to have this new storage `s1` without creating a new tensor object. 6. Returns the modified tensor `t`. Input The function does not take any parameters. Output The function should return the tensor `t` after modifying its storage to be filled with zeros. Example ```python def create_and_modify_tensor_storage(): # Your code here # Example usage: t = create_and_modify_tensor_storage() print(t) # Expected output: tensor([0., 0., 0., 0.]) ``` Constraints - You may not create new tensor objects except for the initial creation of `t`. - Be sure to utilize the `torch.UntypedStorage` methods as described in the provided documentation for handling the storage cloning and modification. Notes - Direct manipulation of tensor storage is generally not recommended but is illustrated here for educational purposes to understand the relationship between tensors and their underlying storages. - Keep in mind, `torch.Tensor.set_` method can be utilized to re-associate a tensor with a new storage.","solution":"import torch def create_and_modify_tensor_storage(): # Step 1: Create a tensor t of size (4,) filled with ones t = torch.ones(4) # Step 2: Obtain the untyped storage s0 from the tensor t s0 = t.storage() # Step 3: Clone this storage to a new storage s1 s1 = s0.clone() # Step 4: Fill the cloned storage s1 with zeros s1.fill_(0) # Step 5: Set the original tensor t to have this new storage s1 without creating a new tensor object t.set_(s1) # Step 6: Return the modified tensor t return t"},{"question":"# Python Coding Assessment Password Hashing and Verification using the `crypt` Module The goal of this problem is to verify your understanding of the `crypt` module in Python for hashing passwords and verifying them. # Problem Statement Implement a function `register_user(username, password)` that stores the userâ€™s information securely by hashing the password using the strongest available method provided by `crypt`. Also, implement another function `verify_user(username, password)` that verifies if the entered password matches the hashed password stored in the system. # Requirements 1. The `register_user()` function should: - Accept a `username` (string) and `password` (string) as arguments. - Hash the `password` using `crypt.crypt()`, with a salt generated by `crypt.mksalt()`. - Store the `username` and the hashed password in a dictionary called `user_database`. - If the `username` already exists in the `user_database`, raise a `ValueError` with the message \\"Username already exists\\". 2. The `verify_user()` function should: - Accept a `username` (string) and `password` (string) as arguments. - Check if `username` exists in the `user_database`. If not, raise a `ValueError` with the message \\"Username not found\\". - Hash the entered `password` using `crypt.crypt()` with the stored hashed password. - Use `hmac.compare_digest` to compare the hashed values. - Return `True` if the password matches, otherwise return `False`. # Constraints - Both `username` and `password` will be non-empty strings. - The `username` will be unique during registration. # Example Usage ```python def register_user(username: str, password: str) -> None: # Implement this function def verify_user(username: str, password: str) -> bool: # Implement this function # Example register_user(\'alice\', \'wonderland\') assert verify_user(\'alice\', \'wonderland\') == True assert verify_user(\'alice\', \'wrongpassword\') == False register_user(\'bob\', \'builder\') assert verify_user(\'bob\', \'builder\') == True assert verify_user(\'bob\', \'builder\') == False ``` # Note: You can use the following structure for the `user_database` where keys are `username` and values are hashed passwords: ```python user_database = {} ```","solution":"import crypt import hmac # Initialize the user database user_database = {} def register_user(username, password): Registers a user by hashing the password and storing the username and hashed password in the user_database. if username in user_database: raise ValueError(\\"Username already exists\\") # Generate a salt and hash the password salt = crypt.mksalt(crypt.METHOD_SHA512) hashed_password = crypt.crypt(password, salt) # Store the username and hashed password in the user_database user_database[username] = hashed_password def verify_user(username, password): Verifies if the entered password matches the hashed password stored in the user_database. if username not in user_database: raise ValueError(\\"Username not found\\") # Retrieve the stored hashed password stored_hashed_password = user_database[username] # Hash the entered password using the salt from the stored hashed password entered_hashed_password = crypt.crypt(password, stored_hashed_password) # Compare the hashed values using hmac.compare_digest for security return hmac.compare_digest(stored_hashed_password, entered_hashed_password)"},{"question":"Objective Write a Python script that utilizes various modules from the standard library to perform the following tasks: 1. Read integers from a file and calculate their statistical properties. 2. Allow users to specify the file and some optional parameters via command line arguments. 3. Handle errors gracefully and write error messages to a log file. Details 1. **Reading Integers from a File**: - You will read a list of integers from a specified file. The file will contain one integer per line. - If a line cannot be converted to an integer, skip that line. 2. **Calculating Statistical Properties**: - Compute and print the mean, median, and variance of the integers. 3. **Command Line Arguments**: - Use the `argparse` module to handle command line arguments. - Parameters: - `--file`: The path to the file containing the integers (required). - `--log`: The path to the log file where error messages should be written (optional, default: `errors.log`). 4. **Error Handling**: - Handle file not found errors gracefully and log an appropriate error message to the log file. - Write any conversion or other processing errors to the log file. Example usage: ```bash python script.py --file input.txt --log my_errors.log ``` Implementation Requirements - Use the `os` module to check whether the file exists or not. - Use the `argparse` module to handle command line arguments. - Use the `statistics` module for computing mean, median, and variance. - Use appropriate exception handling to catch and log errors using the `sys` moduleâ€™s `stderr`. - Use the `datetime` module to prepend timestamps to log entries. Input and Output - **Input**: A text file with one integer per line. - **Output**: Print the mean, median, variance to the console, and log any errors to the specified log file. Constraints - The input file will contain up to 10,000 integers. - The script should handle large files efficiently. Example Given a file `input.txt` with the following content: ``` 10 20 30 40 fifty 60 70 ``` Running the script as follows: ```bash python script.py --file input.txt --log my_errors.log ``` Should output: ``` Mean: 38.0 Median: 35.0 Variance: 450.0 ``` And log an error message like: ``` 2023-10-05 10:00:00 Error: Cannot convert line \'fifty\' to integer. ``` Good luck!","solution":"import argparse import os import statistics import sys from datetime import datetime def read_integers_from_file(file_path, log_path): Reads integers from a given file and logs errors to a specified log file. Returns a list of read integers. integers = [] with open(log_path, \'a\') as log_file: try: with open(file_path, \'r\') as file: for line_number, line in enumerate(file, start=1): line = line.strip() if line.isnumeric(): integers.append(int(line)) else: log_error(log_file, f\\"Line {line_number}: Cannot convert line \'{line}\' to integer.\\") except FileNotFoundError: log_error(log_file, f\\"File \'{file_path}\' not found.\\") sys.exit(f\\"Error: File \'{file_path}\' not found. Terminating execution.\\") return integers def log_error(log_file, message): Logs an error message with a timestamp to the provided log file. timestamp = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') log_file.write(f\\"{timestamp} {message}n\\") def calculate_statistics(integers): Calculates and prints mean, median, and variance for a given list of integers. if not integers: print(\\"No valid integers to process.\\") return mean = statistics.mean(integers) median = statistics.median(integers) variance = statistics.variance(integers) print(f\\"Mean: {mean:.2f}\\") print(f\\"Median: {median:.2f}\\") print(f\\"Variance: {variance:.2f}\\") def main(): parser = argparse.ArgumentParser(description=\'Calculate statistics from integers in a file.\') parser.add_argument(\'--file\', required=True, help=\'The path to the file containing the integers.\') parser.add_argument(\'--log\', default=\'errors.log\', help=\'The path to the log file where error messages should be written.\') args = parser.parse_args() if not os.path.exists(args.file): with open(args.log, \'a\') as log_file: log_error(log_file, f\\"File \'{args.file}\' does not exist.\\") sys.exit(f\\"Error: File \'{args.file}\' does not exist.\\") integers = read_integers_from_file(args.file, args.log) calculate_statistics(integers) if __name__ == \'__main__\': main()"},{"question":"# Advanced I/O Stream Manipulation **Objective**: To assess your understanding of different types of I/O streams in Python, including text and binary I/O, and their practical application. **Problem Statement**: You are tasked to design a function that reads a given binary file, decodes its content as text using a specified encoding, performs some text-based manipulations, and then writes the manipulated text back to a new binary file. Function Signature ```python def manipulate_file(input_file_path: str, output_file_path: str, encoding: str) -> None: pass ``` # Requirements: 1. **Input**: - `input_file_path` (str): The path to the binary input file. - `output_file_path` (str): The path to the binary output file where the manipulated content will be saved. - `encoding` (str): The encoding to be used for decoding the binary content to text. 2. **Output**: - The function should return `None`. The output is written to the specified binary output file. 3. **Process**: - Open the input file as a binary stream. - Decode the binary content to text using the provided encoding. - Perform the following manipulations on the decoded text: - Convert all text to uppercase. - Replace all occurrences of a specific word (e.g., \'foo\') with another word (e.g., \'bar\'). - Encode the manipulated text back to binary using the same encoding. - Write the encoded binary content to the output file. # Constraints: - Handle any potential I/O and encoding exceptions gracefully. - Performance considerations: Use buffered I/O classes to efficiently manage the file reading and writing processes. # Example: Suppose you have an input binary file `input.bin` containing the text \\"foo bar foo\\". The function will read, decode, manipulate the text to \\"BAR BAR BAR\\" (uppercase and replacing \'foo\' with \'bar\'), then encode and write it to `output.bin`. ```python def manipulate_file(input_file_path: str, output_file_path: str, encoding: str) -> None: import io # Read from the input binary file with open(input_file_path, \'rb\') as input_file: binary_data = input_file.read() # Decode the binary data to text try: text_data = binary_data.decode(encoding) except (UnicodeDecodeError, LookupError) as e: print(f\\"Decoding error: {e}\\") return # Perform text manipulations text_data = text_data.upper() text_data = text_data.replace(\'FOO\', \'BAR\') # Encode the text data back to binary try: processed_binary_data = text_data.encode(encoding) except (UnicodeEncodeError, LookupError) as e: print(f\\"Encoding error: {e}\\") return # Write to the output binary file using buffered writing with open(output_file_path, \'wb\') as output_file: output_file.write(processed_binary_data) # Sample Usage: # manipulate_file(\'input.bin\', \'output.bin\', \'utf-8\') ``` **Note**: Ensure to test the function with various text and binary files to validate its correctness and robustness. Consider edge cases like empty files, different encodings, and large file sizes.","solution":"def manipulate_file(input_file_path: str, output_file_path: str, encoding: str) -> None: try: # Read from the input binary file with open(input_file_path, \'rb\') as input_file: binary_data = input_file.read() # Decode the binary data to text try: text_data = binary_data.decode(encoding) except (UnicodeDecodeError, LookupError) as e: print(f\\"Decoding error: {e}\\") return # Perform text manipulations text_data = text_data.upper() text_data = text_data.replace(\'FOO\', \'BAR\') # Encode the text data back to binary try: processed_binary_data = text_data.encode(encoding) except (UnicodeEncodeError, LookupError) as e: print(f\\"Encoding error: {e}\\") return # Write to the output binary file with open(output_file_path, \'wb\') as output_file: output_file.write(processed_binary_data) except IOError as io_err: print(f\\"File I/O error: {io_err}\\")"},{"question":"# Q1: Implementing a Typed Object Manipulation Function **Objective**: Demonstrate your understanding of type hints, `TypedDict`, and type checking constructs like `Literal` and `TypeGuard` within the `typing` module. # Problem Statement: You will write a Python function to manipulate a collection of cars, stored as a dictionary. Each car will have specific attributes, and your function will filter, transform, and present the relevant information based on type-safe operations. Instructions: 1. **Define a `TypedDict` `Car`**: - `brand`: `str` - `model`: `str` - `year`: `int` - `available`: `Literal[\'Yes\', \'No\']` - `price`: `float` 2. **Create a function `filter_cars`**: - **Input**: A list of `Car` dictionaries and a minimum year for filtering. - **Output**: A dictionary mapping `brand` to a list of models released on or after the given year. 3. **Create a function `check_availability`**: - **Input**: A list of `Car` dictionaries. - **Output**: Only return those cars that are available. 4. **Introduce a function `get_expensive_cars`**: - **Input**: A list of `Car` dictionaries and a price threshold. - **Output**: A list of tuples where each tuple contains the `model` and `price` of cars whose price exceeds the given threshold. 5. **Additionally**: - Implement type annotations using the `typing` module. - Use list and dictionary comprehensions to implement these functions succinctly. Function Signatures: ```python from typing import List, Dict, Tuple from typing import TypedDict, Literal class Car(TypedDict): brand: str model: str year: int available: Literal[\'Yes\', \'No\'] price: float def filter_cars(cars: List[Car], min_year: int) -> Dict[str, List[str]]: pass def check_availability(cars: List[Car]) -> List[Car]: pass def get_expensive_cars(cars: List[Car], price_threshold: float) -> List[Tuple[str, float]]: pass ``` # Constraints: - Use the `typing` module constructs properly. - Handle edge cases where there may be zero cars in the input list. - Ensure a clean, readable function implementation. You can use the following data to test your function: ```python cars = [ {\\"brand\\": \\"Toyota\\", \\"model\\": \\"Camry\\", \\"year\\": 2020, \\"available\\": \\"Yes\\", \\"price\\": 24000}, {\\"brand\\": \\"Honda\\", \\"model\\": \\"Civic\\", \\"year\\": 2019, \\"available\\": \\"No\\", \\"price\\": 22000}, {\\"brand\\": \\"Ford\\", \\"model\\": \\"Mustang\\", \\"year\\": 2021, \\"available\\": \\"Yes\\", \\"price\\": 26000}, {\\"brand\\": \\"Tesla\\", \\"model\\": \\"Model S\\", \\"year\\": 2022, \\"available\\": \\"Yes\\", \\"price\\": 80000}, ] ``` # Example Output: ```python filter_cars(cars, 2020) # Output: {\'Toyota\': [\'Camry\'], \'Ford\': [\'Mustang\'], \'Tesla\': [\'Model S\']} check_availability(cars) # Output: [{\\"brand\\": \\"Toyota\\", \\"model\\": \\"Camry\\", \\"year\\": 2020, \\"available\\": \\"Yes\\", \\"price\\": 24000}, # {\\"brand\\": \\"Ford\\", \\"model\\": \\"Mustang\\", \\"year\\": 2021, \\"available\\": \\"Yes\\", \\"price\\": 26000}, # {\\"brand\\": \\"Tesla\\", \\"model\\": \\"Model S\\", \\"year\\": 2022, \\"available\\": \\"Yes\\", \\"price\\": 80000}] get_expensive_cars(cars, 25000) # Output: [(\'Ford Mustang\', 26000), (\'Tesla Model S\', 80000)] ``` Write your solution using the specified requirements and ensure type safety and correctness.","solution":"from typing import List, Dict, Tuple from typing import TypedDict, Literal class Car(TypedDict): brand: str model: str year: int available: Literal[\'Yes\', \'No\'] price: float def filter_cars(cars: List[Car], min_year: int) -> Dict[str, List[str]]: Filters the cars based on the minimum year provided and returns a dictionary mapping brand to a list of models released on or after the given year. result: Dict[str, List[str]] = {} for car in cars: if car[\'year\'] >= min_year: if car[\'brand\'] in result: result[car[\'brand\']].append(car[\'model\']) else: result[car[\'brand\']] = [car[\'model\']] return result def check_availability(cars: List[Car]) -> List[Car]: Returns a list of cars that are available. return [car for car in cars if car[\'available\'] == \'Yes\'] def get_expensive_cars(cars: List[Car], price_threshold: float) -> List[Tuple[str, float]]: Returns a list of tuples where each tuple contains the model and price of cars whose price exceeds the given threshold. return [(f\\"{car[\'brand\']} {car[\'model\']}\\", car[\'price\']) for car in cars if car[\'price\'] > price_threshold]"},{"question":"# Advanced PyTorch Serialization Task Problem Statement You are tasked with implementing functions that will serialize and deserialize PyTorch tensors and module states while ensuring optimal storage and appropriate handling of tensor views. 1. **Save and Load Tensor Views:** Write two functions: - `save_tensor_views(tensors: list, file_path: str) -> None` : This function takes a list of tensors and a file path, and saves the tensors to the specified file. Ensure that tensor views are preserved. - `load_tensor_views(file_path: str) -> list`: This function takes a file path and returns the list of tensors that were saved, preserving their view relationships. 2. **Optimize Tensor Storage:** Write two functions: - `save_optimized_tensor(tensor: torch.Tensor, file_path: str) -> None`: This function takes a tensor and a file path, and saves a cloned version of the tensor to reduce file size if the tensor\'s storage is larger than the tensor itself. - `load_optimized_tensor(file_path: str) -> torch.Tensor`: This function takes a file path and loads the tensor from the file, ensuring minimal storage use. 3. **Save and Load Module States:** Write two functions: - `save_module_state(module: torch.nn.Module, file_path: str) -> None`: This function takes a PyTorch module and a file path, and saves the module\'s state dictionary to the specified file. - `load_module_state(file_path: str, module_class: type) -> torch.nn.Module`: This function takes a file path and a module class, creates an instance of the module, and loads the state dictionary from the file into the module. Ensure that only the state dictionary is loaded. Requirements - Demonstrate understanding of preserving tensor views and storage optimizations. - Show handling of state dictionaries for `torch.nn.Module`. - The functions should be robust and handle potential edge cases. Constraints - The saved files should be as small as possible without losing necessary information. - The functions should preserve tensor relationships and module compatibility. - Use appropriate error handling to manage file read/write operations. Example Usage ```python # Example for save and load tensor views numbers = torch.arange(1, 10) evens = numbers[1::2] save_tensor_views([numbers, evens], \'tensors.pt\') loaded_numbers, loaded_evens = load_tensor_views(\'tensors.pt\') assert loaded_numbers is not None assert loaded_evens is not None print(loaded_numbers) # should reflect modifications made via views # Example for optimizing tensor storage large = torch.arange(1, 1000) small = large[0:5] save_optimized_tensor(small, \'small.pt\') loaded_small = load_optimized_tensor(\'small.pt\') assert loaded_small.size() == small.size() assert loaded_small.storage().size() == small.size() # Example for save and load module state class MyModule(torch.nn.Module): def __init__(self): super().__init__() self.fc = torch.nn.Linear(10, 5) def forward(self, x): return self.fc(x) module = MyModule() save_module_state(module, \'module_state.pt\') loaded_module = load_module_state(\'module_state.pt\', MyModule) assert isinstance(loaded_module, MyModule) print(loaded_module.state_dict()) # should match `module` state dict ```","solution":"import torch import os def save_tensor_views(tensors: list, file_path: str) -> None: Saves a list of tensors to the specified file, preserving tensor views. with open(file_path, \'wb\') as f: torch.save(tensors, f) def load_tensor_views(file_path: str) -> list: Loads tensors from the specified file, preserving tensor views. with open(file_path, \'rb\') as f: tensors = torch.load(f) return tensors def save_optimized_tensor(tensor: torch.Tensor, file_path: str) -> None: Saves a cloned version of the tensor to minimize storage if the tensor\'s storage is larger than the tensor itself. if tensor.storage().size() > tensor.numel(): tensor = tensor.clone() with open(file_path, \'wb\') as f: torch.save(tensor, f) def load_optimized_tensor(file_path: str) -> torch.Tensor: Loads the tensor from the specified file, ensuring minimal storage use. with open(file_path, \'rb\') as f: tensor = torch.load(f) return tensor def save_module_state(module: torch.nn.Module, file_path: str) -> None: Saves the state dictionary of the given PyTorch module to the specified file. state_dict = module.state_dict() with open(file_path, \'wb\') as f: torch.save(state_dict, f) def load_module_state(file_path: str, module_class: type) -> torch.nn.Module: Loads the state dictionary from the specified file into a new instance of the specified module class. module = module_class() with open(file_path, \'rb\') as f: state_dict = torch.load(f) module.load_state_dict(state_dict) return module"},{"question":"**Custom Attention Mechanism Implementation in PyTorch** Attention mechanisms have become a crucial part of modern neural network architectures, especially in the domain of natural language processing. In this task, you are required to implement a custom attention mechanism in PyTorch. This question will test your understanding of PyTorch\'s tensor operations, neural network module creation, and application of attention concepts. **Objective:** Implement a custom scaled dot-product attention mechanism from scratch using PyTorch. **Details:** You need to implement a class `ScaledDotProductAttention` that inherits from `torch.nn.Module`. The attention mechanism should accept three inputs: queries (`Q`), keys (`K`), and values (`V`), and it should output the attended values. **Expected Input & Output Formats:** - The input consists of three tensors: `Q`, `K`, and `V`, each of shape `(batch_size, num_heads, seq_len, d_k)`. - The output should be the attended values tensor of shape `(batch_size, num_heads, seq_len, d_k)`. **Steps:** 1. Compute the dot products between the queries and keys to obtain the raw attention scores. 2. Scale the raw attention scores by the square root of the dimension of the keys (denoted as `d_k`). 3. Apply a softmax function to the scaled scores to obtain the attention weights. 4. Use these weights to compute a weighted sum of the values. **Performance Requirements:** - The implementation should efficiently handle tensor operations to ensure it works well with larger batch sizes and sequence lengths. - Utilize the built-in functions from PyTorch like `torch.matmul`, `torch.softmax`, etc. **Constraints:** - Do NOT use any high-level PyTorch attention modules like `torch.nn.MultiheadAttention`. - Your implementation must gracefully handle different batch sizes, sequence lengths, and number of heads. **Example Usage:** ```python import torch class ScaledDotProductAttention(torch.nn.Module): def __init__(self, d_k): super(ScaledDotProductAttention, self).__init__() self.d_k = d_k def forward(self, Q, K, V): # Step 1. Compute raw attention scores scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float)) # Step 2. Apply softmax to get attention weights attention_weights = torch.softmax(scores, dim=-1) # Step 3. Compute weighted sum of values attended_values = torch.matmul(attention_weights, V) return attended_values # Example input batch_size, num_heads, seq_len, d_k = 2, 4, 5, 64 Q = torch.randn(batch_size, num_heads, seq_len, d_k) K = torch.randn(batch_size, num_heads, seq_len, d_k) V = torch.randn(batch_size, num_heads, seq_len, d_k) attention_layer = ScaledDotProductAttention(d_k) output = attention_layer(Q, K, V) print(output.shape) # Expected Output: torch.Size([2, 4, 5, 64]) ``` Ensure your implementation passes the example usage and works efficiently with varying input sizes.","solution":"import torch import torch.nn as nn class ScaledDotProductAttention(nn.Module): def __init__(self, d_k): super(ScaledDotProductAttention, self).__init__() self.d_k = d_k def forward(self, Q, K, V): # Step 1. Compute raw attention scores scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float)) # Step 2. Apply softmax to get attention weights attention_weights = torch.softmax(scores, dim=-1) # Step 3. Compute weighted sum of values attended_values = torch.matmul(attention_weights, V) return attended_values"},{"question":"**Question: Visualizing Health Expenditure Trends with Seaborn** You are provided with the \'healthexp\' dataset, which contains information on health expenditure and life expectancy for various countries over multiple years. Your task is to write a Python function using Seaborn to create a visualization that effectively displays the relationship between health expenditure and life expectancy for each country. # Function Signature ```python def plot_health_expenditure_trends(dataset_path: str) -> None: pass ``` # Input - `dataset_path` (str): The file path to the \'healthexp\' dataset (in CSV format). # Requirements 1. Load the dataset from the provided file path. 2. Sort the dataset by \\"Country\\" and \\"Year\\". 3. Create a Seaborn plot using the `so.Plot` method. 4. Add a `Path` mark to the plot to display the trajectory of health expenditure and life expectancy for each country. 5. Customize the plot with the following properties: - Use different colors for different countries. - Mark each data point with a circular marker. - Set the point size to 2. - Set the line width to 0.75. - Fill each marker with white color. 6. Display the plot. # Output - The function does not return anything. It produces and displays a plot. # Example Suppose `dataset_path` points to a CSV file containing the \'healthexp\' dataset. After calling `plot_health_expenditure_trends(dataset_path)`, it should display a line plot with different colored lines representing different countries. Each data point should be marked with a small white-filled circle. **Note:** Ensure you have the Seaborn package installed in your Python environment. You can install it using `pip install seaborn`. # Constraints - The dataset is guaranteed to have the columns \\"Country\\", \\"Year\\", \\"Spending_USD\\", and \\"Life_Expectancy\\". Here\'s a sample implementation snippet to get you started: ```python import seaborn.objects as so import pandas as pd def plot_health_expenditure_trends(dataset_path: str) -> None: # Load the dataset healthexp = pd.read_csv(dataset_path) # Sort the dataset by Country and Year healthexp = healthexp.sort_values([\\"Country\\", \\"Year\\"]) # Create the plot using seaborn objects interface p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") # Add path with specified properties p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")).show() ```","solution":"import seaborn.objects as so import pandas as pd def plot_health_expenditure_trends(dataset_path: str) -> None: This function loads the \'healthexp\' dataset, creates a Seaborn plot to display the relationship between health expenditure and life expectancy for each country, and customizes the plot as specified. :param dataset_path: str - The file path to the \'healthexp\' dataset (in CSV format). # Load the dataset healthexp = pd.read_csv(dataset_path) # Sort the dataset by Country and Year healthexp = healthexp.sort_values([\\"Country\\", \\"Year\\"]) # Create the plot using seaborn objects interface p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") # Add path with specified properties p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")).show()"},{"question":"# Question **System Commands and Signal Handling in Python** You are required to implement a Python function `handle_signals_and_server()` that sets up custom signal handlers and starts a simple HTTP server. This function demonstrates a practical use case of signal handling where the signal interrupts avoid sudden termination and ensure graceful shutdown of the server. **Function Specification:** - **handle_signals_and_server(host: str, port: int) -> None** - This should: 1. Set custom handlers for the following signals: - `SIGINT`: Should print \\"Graceful Shutdown initiated\\" and perform a cleanup function. - `SIGALRM`: Should raise `TimeoutError(\\"Operation timed out!\\")` when called. 2. Set a 10-second alarm. 3. Start an HTTP server that serves on the provided `host` and `port`. 4. If the server runs uninterrupted for more than 10 seconds, the alarm signal should fire, raising a `TimeoutError`. - **Constraints:** - The function should block the main thread and serve HTTP requests indefinitely until a `SIGINT` signal is received. - Utilize the `http.server` module to create a simple request handler. - Make sure the server cleans up any resources and prints \\"Server shutting down...\\" before exiting due to `SIGINT`. # Example Usage: ```python import time from signal import signal, SIGINT, SIGALRM, alarm, default_int_handler from http.server import HTTPServer, BaseHTTPRequestHandler class SimpleHandler(BaseHTTPRequestHandler): def do_GET(self): self.send_response(200) self.end_headers() self.wfile.write(b\\"Hello, Signal World!\\") def handle_signals_and_server(host: str, port: int) -> None: # Define custom signal handlers def sigint_handler(signum, frame): print(\\"Graceful Shutdown initiated\\") httpd.shutdown() print(\\"Server shutting down...\\") def sigalrm_handler(signum, frame): raise TimeoutError(\\"Operation timed out!\\") # Set signal handlers signal(SIGINT, sigint_handler) signal(SIGALRM, sigalrm_handler) # Set an alarm for 10 seconds alarm(10) # Set up and start the server httpd = HTTPServer((host, port), SimpleHandler) try: print(f\\"Serving on {host}:{port}\\") httpd.serve_forever() except TimeoutError as e: print(e) finally: httpd.server_close() # Example: handle_signals_and_server(\'localhost\', 8000) ``` **Notes:** - Ensure the signal handlers are correctly set for `SIGINT` and `SIGALRM`. - The HTTP server should run on the given host and port, serving a basic response to incoming GET requests. - Use the provided example usage to test and validate your solution.","solution":"import time from signal import signal, SIGINT, SIGALRM, alarm, default_int_handler from http.server import HTTPServer, BaseHTTPRequestHandler class SimpleHandler(BaseHTTPRequestHandler): def do_GET(self): self.send_response(200) self.end_headers() self.wfile.write(b\\"Hello, Signal World!\\") def handle_signals_and_server(host: str, port: int) -> None: # Define custom signal handlers def sigint_handler(signum, frame): print(\\"Graceful Shutdown initiated\\") httpd.shutdown() print(\\"Server shutting down...\\") def sigalrm_handler(signum, frame): raise TimeoutError(\\"Operation timed out!\\") # Set signal handlers signal(SIGINT, sigint_handler) signal(SIGALRM, sigalrm_handler) # Set an alarm for 10 seconds alarm(10) # Set up and start the server httpd = HTTPServer((host, port), SimpleHandler) try: print(f\\"Serving on {host}:{port}\\") httpd.serve_forever() except TimeoutError as e: print(e) finally: httpd.server_close()"},{"question":"# Custom PyTorch Module and Autograd Function Objective: In this question, you will demonstrate your understanding of extending PyTorch by creating a custom autograd function and incorporating it into a custom neural network module. Finally, you will validate your implementation using automatic differentiation checks. Problem Statement: You need to create a custom linear operation followed by a non-standard activation function within a neural network module. Specifically: 1. **Custom Linear Function**: - Implement a custom linear function (`CustomLinearFunction`) using `torch.autograd.Function`. - This function should take an input tensor, a weight matrix, and an optional bias vector to compute the linear transformation: `output = input.mm(weight.t()) + bias`. 2. **Custom Activation Function**: - Implement a custom activation function (`CustomActivationFunction`) using `torch.autograd.Function`. - This activation function should apply a non-linear transformation defined as `output = input^3`, and should handle both forward and backward operations. 3. **Neural Network Module**: - Create a custom neural network module (`CustomLinear`) by subclassing `torch.nn.Module`. - This module should use `CustomLinearFunction` and `CustomActivationFunction` defined above in its `forward` method. - Ensure the module\'s parameters are properly initialized and registered. 4. **Validation**: - Use `torch.autograd.gradcheck` to verify that your custom functions correctly compute the gradients. - Write a function `validate_gradients` to perform this check. Specifications: - **Input**: - The custom linear function: `(input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor])` - The custom activation function: `(input: torch.Tensor)` - The neural network module: `CustomLinear(input_features: int, output_features: int, bias: bool=True)` - **Output**: - The result of the custom linear transformation followed by the custom activation. - The gradient validation function should return `True` if the gradients are correctly computed, otherwise `False`. Performance Requirements: - Ensure the implemented `backward` methods are efficient and do not store unnecessary buffers. Example Usage: ```python import torch import torch.nn as nn from torch.autograd import gradcheck # Define the custom functions and module as described above # Example tensors for testing input_tensor = torch.randn(2, 3, dtype=torch.double, requires_grad=True) weight_tensor = torch.randn(3, 3, dtype=torch.double, requires_grad=True) bias_tensor = torch.randn(3, dtype=torch.double, requires_grad=True) # Assuming CustomLinear and CustomActivationFunction are implemented custom_linear_result = CustomLinearFunction.apply(input_tensor, weight_tensor, bias_tensor) custom_activation_result = CustomActivationFunction.apply(custom_linear_result) # Assuming validate_gradients is implemented is_valid = validate_gradients() print(f\\"Gradient check passed: {is_valid}\\") ``` Constraints: - Do not use any built-in PyTorch linear or activation functions directly in your custom implementations. - The custom module should be compatible with standard PyTorch training workflows, including optimization and backward propagation. # Your task: 1. Implement `CustomLinearFunction`. 2. Implement `CustomActivationFunction`. 3. Implement `CustomLinear` module. 4. Implement the `validate_gradients()` function to validate the correctness of your custom implementations.","solution":"import torch from torch.autograd import Function class CustomLinearFunction(Function): @staticmethod def forward(ctx, input, weight, bias=None): ctx.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias return output @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias class CustomActivationFunction(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input ** 3 @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * 3 * input ** 2 return grad_input class CustomLinear(torch.nn.Module): def __init__(self, input_features, output_features, bias=True): super(CustomLinear, self).__init__() self.input_features = input_features self.output_features = output_features self.weight = torch.nn.Parameter(torch.Tensor(output_features, input_features)) if bias: self.bias = torch.nn.Parameter(torch.Tensor(output_features)) else: self.register_parameter(\'bias\', None) self.reset_parameters() def reset_parameters(self): torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) torch.nn.init.uniform_(self.bias, -bound, bound) def forward(self, input): return CustomActivationFunction.apply(CustomLinearFunction.apply(input, self.weight, self.bias)) def validate_gradients(): input_tensor = torch.randn(2, 3, dtype=torch.double, requires_grad=True) weight_tensor = torch.randn(3, 3, dtype=torch.double, requires_grad=True) bias_tensor = torch.randn(3, dtype=torch.double, requires_grad=True) # Wrap parameters in tuple for gradcheck test_input = (input_tensor, weight_tensor, bias_tensor) # CustomLinearFunction Test custom_linear_func_test = torch.autograd.gradcheck(CustomLinearFunction.apply, test_input, eps=1e-6, atol=1e-4) custom_activation_func_test = torch.autograd.gradcheck(CustomActivationFunction.apply, (input_tensor,), eps=1e-6, atol=1e-4) return custom_linear_func_test and custom_activation_func_test"},{"question":"You are given a tensor from an external framework that is compliant with DLPack standards. Your task is to: 1. Convert this tensor to a PyTorch tensor. 2. Perform an element-wise square operation on the tensor. 3. Convert the result back to a DLPack tensor. Implement the function `process_tensor(dlpack_tensor)` which takes the following input: - `dlpack_tensor`: A tensor from an external framework in DLPack format. The function should return: - A tensor in DLPack format, which is the result of squaring each element of the input tensor. # Constraints - The input tensor can be of any numeric type. - You must use `torch.utils.dlpack.from_dlpack` to convert the input tensor and `torch.utils.dlpack.to_dlpack` to convert the output tensor. # Example ```python import torch import numpy as np def create_dlpack_tensor(): np_array = np.array([[1, 2], [3, 4]], dtype=np.float32) torch_tensor = torch.from_numpy(np_array) return torch.utils.dlpack.to_dlpack(torch_tensor) dlpack_tensor = create_dlpack_tensor() result = process_tensor(dlpack_tensor) py_tensor = torch.utils.dlpack.from_dlpack(result) print(py_tensor) ``` Expected output: ``` tensor([[ 1., 4.], [ 9., 16.]]) ``` # Function Signature ```python def process_tensor(dlpack_tensor): # Your code here ```","solution":"import torch def process_tensor(dlpack_tensor): Convert a DLPack tensor to a PyTorch tensor, square each element, and convert the result back to a DLPack tensor. Args: dlpack_tensor: tensor in DLPack format Returns: DLPack tensor with each element squared # Convert DLPack tensor to PyTorch tensor torch_tensor = torch.utils.dlpack.from_dlpack(dlpack_tensor) # Perform element-wise square operation squared_tensor = torch_tensor ** 2 # Convert squared PyTorch tensor back to DLPack tensor dlpack_tensor_result = torch.utils.dlpack.to_dlpack(squared_tensor) return dlpack_tensor_result"},{"question":"# Python Resource Management and Debugging Objective: Demonstrate your understanding of Python\'s resource management and debugging capabilities by implementing a function that performs file operations safely and efficiently. Additionally, enable development mode diagnostics to catch and log any potential issues. Background: Python Development Mode introduces various runtime checks that help identify issues such as unclosed resources, memory allocation problems, and others. You\'ll be required to simulate this environment by explicitly checking for issues and ensuring proper resource management. Problem Statement: You are required to write a function `count_lines_safe(file_path: str) -> int` that takes the path to a text file and returns the number of lines in it. You must ensure that the file is properly closed after reading, and handle any exceptions that may occur during file operations. Additionally, create a second function `enable_dev_mode()` that simulates enabling the Python Development Mode by setting up custom checks and handlers for the following: 1. Warning for unclosed resources (e.g., files). 2. Logging exceptions that occur during file closing in the destructor. 3. Checking and logging unsafe use of file descriptors. The `enable_dev_mode()` function should set up these handlers and not return anything. Function Signatures: ```python def count_lines_safe(file_path: str) -> int: pass def enable_dev_mode() -> None: pass ``` Constraints: - Assume the file specified by `file_path` exists and is readable. - You may use Python standard libraries such as `warnings` and `logging`. Example Usage: ```python enable_dev_mode() line_count = count_lines_safe(\\"example.txt\\") print(f\\"Number of lines: {line_count}\\") ``` **Expected Output:** - Correct number of lines in the file. - Any warnings or logs related to resource management issues, if present. Notes: - Pay special attention to resource management to avoid leaving files open. - Make use of context managers (`with` statement) for safe file operations. - Simulate the effects of Python Development Mode as described in the documentation provided.","solution":"import logging import warnings def count_lines_safe(file_path: str) -> int: Reads a file and counts the number of lines in it safely. Closes the file resource after use. If an exception occurs during file operation, it is returned as 0 lines. try: with open(file_path, \'r\') as file: return sum(1 for line in file) except Exception as e: logging.error(f\\"An error occurred while reading the file: {e}\\") return 0 def enable_dev_mode() -> None: Enables custom development mode checks and handlers for resource management and logging. logging.basicConfig(level=logging.DEBUG) def resource_warning(message, *args, **kwargs): logging.warning(message, *args, **kwargs) warnings.showwarning = resource_warning # Enable the following if using Python 3.6+ if hasattr(logging, \'ResourceWarning\'): warnings.simplefilter(\'always\', ResourceWarning) # Add an additional file handler that logs to a file fh = logging.FileHandler(\'debug.log\') fh.setLevel(logging.DEBUG) logger = logging.getLogger() logger.addHandler(fh) logging.info(\'Development mode enabled. Resource warnings will be logged.\') # Example usage (Not part of the solution to be tested) # enable_dev_mode() # line_count = count_lines_safe(\\"example.txt\\") # print(f\\"Number of lines: {line_count}\\")"},{"question":"Objective: To assess the student\'s ability to manage MPS devices, handle memory efficiently, and use the profiling tools provided in `torch.mps`. Problem Statement: You are given a PyTorch code base that is intended to run on Apple devices utilizing Metal Performance Shaders (MPS). Your task is to implement a function `mps_optimization()` that performs the following operations: 1. Check the number of MPS devices available. 2. Set the random seed for reproducibility. 3. Allocate a tensor of size 1000x1000 filled with ones and measure the allocated memory. 4. Empty the cache and measure the memory again. 5. Use the MPS profiler to capture the performance of a simple matrix multiplication between two tensors (1000x1000). 6. Create an event to synchronize the operation. Function Signature: ```python def mps_optimization(): This function performs the following operations: - Checks number of MPS devices. - Sets a manual random seed. - Allocates memory for a tensor and reports current and driver-allocated memory. - Empties the cache and reports memory usage. - Profiles a simple matrix multiplication operation. - Synchronizes using an event. Returns: dict: A dictionary with the following keys and respective values: \'device_count\' (int): The number of MPS devices. \'initial_allocated_memory\' (int): Memory allocated initially. \'initial_driver_allocated_memory\' (int): Driver-allocated memory initially. \'allocated_memory_after_cache\' (int): Memory allocated after emptying cache. \'driver_allocated_memory_after_cache\' (int): Driver-allocated memory after emptying cache. \'profiler_data\' (str): Profiler data in a string format. \'synchronization_event_status\' (bool): Status of event synchronization. pass ``` Requirements: - Use `torch.mps.device_count` to check for MPS devices. - Use `torch.mps.set_rng_state` or `torch.mps.manual_seed` to set the seed. - Allocate a tensor using `torch` and measure memory using `torch.mps.current_allocated_memory` and `torch.mps.driver_allocated_memory`. - Use `torch.mps.empty_cache` to clear cache. - Use the MPS profiler to profile a matrix multiplication operation. - Use `torch.mps.event.Event` to synchronize the operation and ensure correct execution order. Example Output: ```python { \'device_count\': 1, \'initial_allocated_memory\': 12345678, \'initial_driver_allocated_memory\': 23456789, \'allocated_memory_after_cache\': 0, \'driver_allocated_memory_after_cache\': 0, \'profiler_data\': \'<Profiler output>\', \'synchronization_event_status\': True } ``` *Constraints*: - Ensure all operations are performed assuming an Apple device with MPS support. *Note*: Handle exceptions where functions or operations might fail due to lack of MPS support or other runtime issues.","solution":"import torch def mps_optimization(): try: # Check the number of MPS devices available device_count = torch.mps.device_count() if device_count == 0: raise RuntimeError(\\"No MPS devices found.\\") # Set the random seed for reproducibility torch.manual_seed(42) # Allocate a tensor of size 1000x1000 filled with ones tensor = torch.ones((1000, 1000), device=\'mps\') # Measure the allocated memory initial_allocated_memory = torch.mps.current_allocated_memory() initial_driver_allocated_memory = torch.mps.driver_allocated_memory() # Empty the cache torch.mps.empty_cache() # Measure memory again allocated_memory_after_cache = torch.mps.current_allocated_memory() driver_allocated_memory_after_cache = torch.mps.driver_allocated_memory() # Profile a simple matrix multiplication operation with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.MPS]) as prof: result = torch.mm(tensor, tensor) profiler_data = prof.key_averages().table() # Create an event to synchronize the operation event = torch.mps.Event(enable_timing=True) event.record() event.synchronize() synchronization_event_status = event.query() # Creating the output dictionary result_dict = { \'device_count\': device_count, \'initial_allocated_memory\': initial_allocated_memory, \'initial_driver_allocated_memory\': initial_driver_allocated_memory, \'allocated_memory_after_cache\': allocated_memory_after_cache, \'driver_allocated_memory_after_cache\': driver_allocated_memory_after_cache, \'profiler_data\': profiler_data, \'synchronization_event_status\': synchronization_event_status } return result_dict except Exception as e: return { \'error\': str(e) }"},{"question":"# Async Task Management and Synchronization with Queues **Problem Statement:** You are required to design an asyncio-based producer-consumer system using Python 3.10. In this system, producers will generate data items that are to be processed by consumers. Both producers and consumers should run concurrently, and synchronization should be controlled with appropriate asyncio primitives. # Requirements: - Implement a function `producer(queue, n, semaphore)` that: - Produces `n` items. - Each item should be an integer starting from 0 up to `n-1`. - Each produced item should be put into the provided asyncio `queue`. - Use the provided asyncio `semaphore` to control the number of concurrently produced items. - Implement a function `consumer(queue, semaphore)` that: - Consumes items from the provided asyncio `queue`. - Each consumed item should be removed from the queue and printed. - Use the provided asyncio `semaphore` to control the number of concurrently consumed items. - Implement a main function `main()` that: - Creates an asyncio `Queue`. - Creates an asyncio `Semaphore` with a concurrency of 5. - Starts and manages at least 3 producer tasks and 2 consumer tasks concurrently. - Ensures all tasks complete their execution. # Constraints: - Use only the asyncio and random standard libraries. - Ensure proper handling of cancellations and timeouts where necessary. # Example of function usage: ```python import asyncio async def producer(queue, n, semaphore): pass # Implement this async def consumer(queue, semaphore): pass # Implement this async def main(): # Create queue and semaphore queue = asyncio.Queue() semaphore = asyncio.Semaphore(5) # Start producer and consumer tasks producers = [asyncio.create_task(producer(queue, 10, semaphore)) for _ in range(3)] consumers = [asyncio.create_task(consumer(queue, semaphore)) for _ in range(2)] # Wait for all tasks to complete await asyncio.gather(*producers) await asyncio.gather(*consumers) if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Expected Output: - Various integers being printed by consumer tasks indicating the items consumed. - Proper synchronization ensuring no more than the allowed concurrency for both producers and consumers. - The program should complete and exit gracefully, having produced and consumed the total number of items generated by the producers.","solution":"import asyncio async def producer(queue, n, semaphore): for i in range(n): async with semaphore: await queue.put(i) await asyncio.sleep(0.1) # Simulate production time async def consumer(queue, semaphore): while True: async with semaphore: item = await queue.get() if item is None: break print(f\\"Consumed: {item}\\") queue.task_done() async def main(): queue = asyncio.Queue() semaphore = asyncio.Semaphore(5) producers = [asyncio.create_task(producer(queue, 10, semaphore)) for _ in range(3)] consumers = [asyncio.create_task(consumer(queue, semaphore)) for _ in range(2)] # Wait for all producers to finish await asyncio.gather(*producers) # Signal the consumers to exit for _ in range(len(consumers)): await queue.put(None) # Wait for all consumers to finish await asyncio.gather(*consumers) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question: Visualizing and Customizing Data with seaborn You are given a dataset of penguin measurements consisting of various attributes such as species, island, bill length/depth, flipper length, body mass, and sex. Using this dataset, you need to generate and customize various plots that highlight different relationships and distributions in the data. Your task is to write functions for the following sub-tasks: Part 1: Data Preparation 1. Load the penguin dataset using seaborn\'s `load_dataset()` function. 2. Clean the dataset by removing any rows with missing values. Part 2: Axes-level Plots Implement a function `draw_axes_level_plots` that: - Plots the distribution of `flipper_length_mm` across different `species` using a histogram. - Plots the kernel density estimate (KDE) for the same with data colored by `species`. ```python def draw_axes_level_plots(penguins_df: pd.DataFrame) -> None: - penguins_df: DataFrame, consisting of penguin dataset. - Outputs: Displays histogram and KDE plots for \'flipper_length_mm\' colored by \'species\' ... ``` Part 3: Figure-level Plots Implement a function `draw_figure_level_plots` that: - Creates a multi-faceted figure-level plot that displays the histogram of `bill_length_mm` with different `species` stacked, faceted by the `sex` of the penguins. - Ensures the legend is placed outside the plot. ```python def draw_figure_level_plots(penguins_df: pd.DataFrame) -> None: - penguins_df: DataFrame, consisting of penguin dataset. - Outputs: Displays a faceted histogram plot for \'bill_length_mm\' by \'species\' with legend outside the plot. ... ``` Part 4: Joint Plot Implement a function `draw_joint_plot` that: - Displays a joint plot showing the relationship between `body_mass_g` and `bill_length_mm`, differentiated by `species`. Use histogram kind in the joint plot. ```python def draw_joint_plot(penguins_df: pd.DataFrame) -> None: - penguins_df: DataFrame, consisting of penguin dataset. - Outputs: Displays a joint histogram plot for \'body_mass_g\' and \'bill_length_mm\' differentiated by \'species\'. ... ``` Constraints and Hints: - Use seaborn for all plotting tasks. - Use matplotlib\'s `plt.show()` to display each plot at the end of each function. - Ensure code readability and include comments where necessary. - `penguins_df` is a pandas DataFrame resulting from the cleaned dataset. - In `draw_axes_level_plots`, explore functions such as `sns.histplot()` and `sns.kdeplot()`. - In `draw_figure_level_plots`, utilize `sns.displot()` for faceting. - In `draw_joint_plot`, explore `sns.jointplot()` with the `kind` parameter set to \\"hist\\". Ensure your final implementation demonstrates your understanding of seaborn\'s capabilities for creating and customizing data visualizations effectively.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def load_and_clean_data(): Loads the penguin dataset and removes any rows with missing values. Returns: A cleaned pandas DataFrame. penguins_df = sns.load_dataset(\\"penguins\\") penguins_df = penguins_df.dropna() return penguins_df def draw_axes_level_plots(penguins_df: pd.DataFrame) -> None: - penguins_df: DataFrame, consisting of penguin dataset. - Outputs: Displays histogram and KDE plots for \'flipper_length_mm\' colored by \'species\' # Plotting histogram sns.histplot(data=penguins_df, x=\'flipper_length_mm\', hue=\'species\', element=\'step\', stat=\'density\', common_norm=False) plt.title(\\"Histogram of Flipper Length by Species\\") plt.show() # Plotting KDE sns.kdeplot(data=penguins_df, x=\'flipper_length_mm\', hue=\'species\', common_norm=False) plt.title(\\"KDE of Flipper Length by Species\\") plt.show() def draw_figure_level_plots(penguins_df: pd.DataFrame) -> None: - penguins_df: DataFrame, consisting of penguin dataset. - Outputs: Displays a faceted histogram plot for \'bill_length_mm\' by \'species\' with legend outside the plot. g = sns.displot(data=penguins_df, x=\'bill_length_mm\', hue=\'species\', col=\'sex\', kind=\'hist\', element=\'step\', common_norm=False) g.set_titles(\\"Bill Length by Species and Sex\\") # Adjust the legend position for ax in g.axes.flatten(): ax.legend(loc=\'center left\', bbox_to_anchor=(1, 0.5)) plt.show() def draw_joint_plot(penguins_df: pd.DataFrame) -> None: - penguins_df: DataFrame, consisting of penguin dataset. - Outputs: Displays a joint histogram plot for \'body_mass_g\' and \'bill_length_mm\' differentiated by \'species\'. sns.jointplot(data=penguins_df, x=\'body_mass_g\', y=\'bill_length_mm\', hue=\'species\', kind=\'hist\') plt.suptitle(\\"Joint Histogram of Body Mass and Bill Length, by Species\\", y=1.02) plt.show()"},{"question":"You are tasked with designing a simple library management system. This system should use classes, inheritance, and implement an iterator for browsing the collection of books. # Requirements: 1. **Base Class - `Book`:** - **Attributes:** - `title` (string): Title of the book. - `author` (string): Author of the book. - `year` (integer): Publication year of the book. - `available` (boolean): Availability status of the book (True if available, False otherwise). - **Methods:** - `__init__(self, title, author, year)`: Initializes the book with the given title, author, and year. Sets `available` to True by default. - `__str__(self)`: Returns a string representation of the book in the format `\\"<title> by <author> (<year>) - Available: <available>\\"`. 2. **Derived Class - `Library`:** - **Attributes:** - `name` (string): Name of the library. - `books` (list of `Book` objects): Collection of books in the library. - **Methods:** - `__init__(self, name)`: Initializes the library with the given name and an empty list of books. - `add_book(self, book: Book)`: Adds a book to the library. - `remove_book(self, title: str)`: Removes a book by title. If the book is not found, it should raise a `ValueError`. - `__iter__(self)`: Initializes the iterator for the library collection. - `__next__(self)`: Returns the next book in the collection during iteration. If no more books are available, raises `StopIteration`. - `find_book_by_title(self, title: str)`: Returns the book matching the given title or `None` if not found. - `borrow_book(self, title: str)`: Marks the book as borrowed (sets `available` to False) if it is available; raises a `ValueError` if not available or does not exist. - `return_book(self, title: str)`: Marks the book as available if it exists in the system; raises a `ValueError` if the book does not exist. # Input and Output Format: - **Input:** There is no direct input from stdin, but the program should function correctly when interacting with book and library objects. - **Output:** The program should correctly display the book details and handle errors as specified. # Constraints: - Only valid strings will be used for book titles, authors, and library names. - Publication years will be valid integers. - Assume reasonable limits on the number of books (up to a few thousand). # Example: ```python # Create books book1 = Book(\\"1984\\", \\"George Orwell\\", 1949) book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960) book3 = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925) # Create a library library = Library(\\"City Library\\") # Add books to library library.add_book(book1) library.add_book(book2) library.add_book(book3) # Borrow a book library.borrow_book(\\"1984\\") # Iterate over library books for book in library: print(book) # Find a book by title print(library.find_book_by_title(\\"1984\\")) # Return a book library.return_book(\\"1984\\") # Output all books\' details post return for book in library: print(book) ``` # Expected Output: ``` 1984 by George Orwell (1949) - Available: False To Kill a Mockingbird by Harper Lee (1960) - Available: True The Great Gatsby by F. Scott Fitzgerald (1925) - Available: True 1984 by George Orwell (1949) - Available: False 1984 by George Orwell (1949) - Available: True To Kill a Mockingbird by Harper Lee (1960) - Available: True The Great Gatsby by F. Scott Fitzgerald (1925) - Available: True ```","solution":"class Book: def __init__(self, title, author, year): self.title = title self.author = author self.year = year self.available = True def __str__(self): return f\\"{self.title} by {self.author} ({self.year}) - Available: {self.available}\\" class Library: def __init__(self, name): self.name = name self.books = [] self.current_index = 0 def add_book(self, book): self.books.append(book) def remove_book(self, title): for book in self.books: if book.title == title: self.books.remove(book) return raise ValueError(f\\"Book \'{title}\' not found!\\") def __iter__(self): self.current_index = 0 return self def __next__(self): if self.current_index < len(self.books): book = self.books[self.current_index] self.current_index += 1 return book else: raise StopIteration def find_book_by_title(self, title): for book in self.books: if book.title == title: return book return None def borrow_book(self, title): book = self.find_book_by_title(title) if book is None: raise ValueError(f\\"Book \'{title}\' does not exist!\\") if not book.available: raise ValueError(f\\"Book \'{title}\' is not available!\\") book.available = False def return_book(self, title): book = self.find_book_by_title(title) if book is None: raise ValueError(f\\"Book \'{title}\' does not exist!\\") book.available = True"},{"question":"Objective: Implement a Python function that takes a code object and analyzes its structure and metadata. Your task is to write a function `analyze_code_object` that receives a Python code object and extracts specific details to return a structured summary. Details: Implement the function `analyze_code_object(co: object) -> dict` which accepts a single argument `co`. This argument is a Python code object. Your function should perform the following tasks: 1. **Validate Input**: - Ensure that the input `co` is a code object (`PyCode_Check`). 2. **Extract Metadata**: - Number of arguments (`co.co_argcount`). - Number of keyword-only arguments (`co.co_kwonlyargcount`). - Number of local variables (`co.co_nlocals`). - Stack size requirements (`co.co_stacksize`). - Any flags associated with the code object (`co.co_flags`). - Constant values used (`co.co_consts`). - Variable names used (`co.co_varnames`). - Free variables (`co.co_freevars`). - Cell variables (`co.co_cellvars`). - The filename from which the code object was compiled (`co.co_filename`). - The name of the function (`co.co_name`). - The first line number of the code object (`co.co_firstlineno`). - Line number table (`co.co_lnotab`). 3. **Return Structure**: - Return a dictionary with the extracted metadata. For instance: ```python { \'argcount\': ..., \'kwonlyargcount\': ..., \'nlocals\': ..., \'stacksize\': ..., \'flags\': ..., \'consts\': ..., \'varnames\': ..., \'freevars\': ..., \'cellvars\': ..., \'filename\': ..., \'name\': ..., \'firstlineno\': ..., \'lnotab\': ... } ``` Constraints: - You may assume that the input will always be a valid Python code object. - Focus on code readability and maintainability. - No additional libraries should be used apart from built-in ones. - Performance should be considered for large code objects. Example: ```python def analyze_code_object(co): # Your implementation here # Example usage: def dummy_function(a, b): x = a + b return x code_obj = dummy_function.__code__ result = analyze_code_object(code_obj) print(result) # Expected output: # { # \'argcount\': 2, # \'kwonlyargcount\': 0, # \'nlocals\': 3, # \'stacksize\': 2, # \'flags\': ..., # \'consts\': (None,), # \'varnames\': (\'a\', \'b\', \'x\'), # \'freevars\': (), # \'cellvars\': (), # \'filename\': \'<ipython-input-1-3b59a77c865a>\', # \'name\': \'dummy_function\', # \'firstlineno\': 1, # \'lnotab\': b\'x00x01\' # This could vary # } ``` **Note**: Ensure to handle the conversion of bytecode details to human-readable forms where applicable (e.g., line number table entries).","solution":"def analyze_code_object(co): Analyzes a given code object and extracts specific details. Parameters: co (code object): The code object to be analyzed. Returns: dict: A dictionary containing the extracted metadata. if not isinstance(co, type((lambda: None).__code__)): raise TypeError(\\"Input object is not a code object\\") metadata = { \'argcount\': co.co_argcount, \'kwonlyargcount\': co.co_kwonlyargcount, \'nlocals\': co.co_nlocals, \'stacksize\': co.co_stacksize, \'flags\': co.co_flags, \'consts\': co.co_consts, \'varnames\': co.co_varnames, \'freevars\': co.co_freevars, \'cellvars\': co.co_cellvars, \'filename\': co.co_filename, \'name\': co.co_name, \'firstlineno\': co.co_firstlineno, \'lnotab\': co.co_lnotab } return metadata"},{"question":"# Question: Enhanced Data Visualization with Seaborn You are given a dataset `penguins` from Seaborn\'s example datasets library. Your task is to create a visualization that demonstrates your understanding of both basic and advanced plotting functionalities provided by Seaborn. Task: 1. Load the `penguins` dataset. 2. Create a bar plot showing the average body mass for each species of penguins. Add error bars to this plot to represent the 95% confidence interval. 3. Create a second plot showing the distribution of flipper lengths for each species. Use a box plot to depict this distribution. 4. Create a third plot to compare the relationship between body mass and flipper length, differentiated by species. Use scatter plots and add regression lines with confidence intervals for each species. Requirements: - You must use the `seaborn.objects` API to create the plots. - Clearly label your plots and axes. Ensure that each plot has an appropriate title and legend. - Implement error bars using standard error (`se`) in the bar plot. - Use the bootstrapping technique by seeding the random number generator to ensure reproducibility. - Save each of your plots as `avg_body_mass.png`, `flipper_length_dist.png`, and `body_mass_vs_flipper_length.png`. Input: - No input is required from the user. Use Seaborn\'s `load_dataset` to load the `penguins` dataset. **Example Start Code:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Task 1: Bar plot bar_plot = so.Plot(penguins, x=\'species\', y=\'body_mass_g\').add(so.Bar(), so.Est(errorbar=\'se\')).label(title=\'Average Body Mass by Species\', xlabel=\'Species\', ylabel=\'Body Mass (g)\') bar_plot.save(\\"avg_body_mass.png\\") # Task 2: Box plot box_plot = so.Plot(penguins, x=\'species\', y=\'flipper_length_mm\').add(so.Box()).label(title=\'Flipper Length Distribution by Species\', xlabel=\'Species\', ylabel=\'Flipper Length (mm)\') box_plot.save(\\"flipper_length_dist.png\\") # Task 3: Scatter plot with regression scatter_plot = so.Plot(penguins, x=\'body_mass_g\', y=\'flipper_length_mm\', color=\'species\').add(so.Layer(), so.Scatter()).add(so.Est(), so.Est(method=\\"ols\\")).label(title=\'Body Mass vs Flipper Length by Species\', xlabel=\'Body Mass (g)\', ylabel=\'Flipper Length (mm)\') scatter_plot.save(\\"body_mass_vs_flipper_length.png\\") ``` **Expected Files:** - `avg_body_mass.png` - `flipper_length_dist.png` - `body_mass_vs_flipper_length.png` Ensure your code follows good practices, is well-commented, and includes appropriate error handling where necessary.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Bar plot - average body mass for each species with error bars plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(x=\'species\', y=\'body_mass_g\', data=penguins, ci=95, capsize=.1) bar_plot.set_title(\'Average Body Mass by Species\') bar_plot.set_xlabel(\'Species\') bar_plot.set_ylabel(\'Body Mass (g)\') plt.savefig(\\"avg_body_mass.png\\") plt.close() # Task 2: Box plot - distribution of flipper lengths for each species plt.figure(figsize=(10, 6)) box_plot = sns.boxplot(x=\'species\', y=\'flipper_length_mm\', data=penguins) box_plot.set_title(\'Flipper Length Distribution by Species\') box_plot.set_xlabel(\'Species\') box_plot.set_ylabel(\'Flipper Length (mm)\') plt.savefig(\\"flipper_length_dist.png\\") plt.close() # Task 3: Scatter plot with regression lines - body mass vs flipper length by species plt.figure(figsize=(10, 6)) scatter_plot = sns.lmplot(x=\'body_mass_g\', y=\'flipper_length_mm\', hue=\'species\', data=penguins, aspect=1.5, ci=95) scatter_plot.set_axis_labels(\'Body Mass (g)\', \'Flipper Length (mm)\') plt.title(\'Body Mass vs Flipper Length by Species\') plt.savefig(\\"body_mass_vs_flipper_length.png\\") plt.close()"},{"question":"You are working on a configuration tool that reads settings from a plist file, processes these settings, and writes the updated configuration back to a plist file. The settings include different data types such as integers, strings, lists, and nested dictionaries. Write a Python function `process_plist_file(input_fp: BinaryIO, output_fp: BinaryIO, updates: dict) -> None` that: 1. Reads the plist file from a binary file object `input_fp`. 2. Updates the existing settings with the values provided in the dictionary `updates`. 3. Writes the updated configuration back to the plist file in binary format to `output_fp`. The function should handle potential exceptions during reading and writing of the plist file and raise appropriate errors. Ensure that dictionary keys are of string type and other types are supported correctly as per plistlib documentation. Input: - `input_fp`: A readable binary file object where the original plist file is provided. - `output_fp`: A writable binary file object where the updated plist file should be written. - `updates`: A dictionary containing the updates to be applied to the plist settings. This dictionary can have nested structures and may contain any of the supported plist types. Output: - The function should not return anything. It writes the updated plist to `output_fp`. Constraints: - All dictionary keys in `updates` and plist should be strings. - Values in `updates` should only be of types supported by plistlib (strings, integers, floats, booleans, tuples, lists, dictionaries, bytes, bytearray, or datetime.datetime objects). Example: Suppose the contents of the input plist file are: ``` { \\"name\\": \\"Admin\\", \\"settings\\": { \\"volume\\": 50, \\"brightness\\": 70 } } ``` And `updates` dictionary is: ``` { \\"settings\\": { \\"volume\\": 100, \\"newFeatureEnabled\\": True } } ``` The resulting plist file contents written to `output_fp` should be: ``` { \\"name\\": \\"Admin\\", \\"settings\\": { \\"volume\\": 100, \\"brightness\\": 70, \\"newFeatureEnabled\\": True } } ``` Additional Notes: - Use the `plistlib.load` and `plistlib.dump` functions for reading and writing plist files. - Handle any non-string dictionary keys in the updates by raising a `TypeError`. **Starter Code:** ```python import plistlib from typing import BinaryIO def process_plist_file(input_fp: BinaryIO, output_fp: BinaryIO, updates: dict) -> None: # Your implementation here ```","solution":"import plistlib from typing import BinaryIO def process_plist_file(input_fp: BinaryIO, output_fp: BinaryIO, updates: dict) -> None: Reads the plist file from input_fp, updates it with values from updates, and writes it back to output_fp. :param input_fp: A readable binary file object where the original plist file is provided. :param output_fp: A writable binary file object where the updated plist file should be written. :param updates: A dictionary containing the updates to be applied to the plist settings. try: # Load the existing plist content original_data = plistlib.load(input_fp) # Validate the dictionary keys in updates are strings def validate_keys(d): for key, value in d.items(): if not isinstance(key, str): raise TypeError(\\"Dictionary keys in updates must be strings.\\") if isinstance(value, dict): validate_keys(value) validate_keys(updates) # Update the original plist with the new values def deep_update(orig, new): for key, value in new.items(): if isinstance(value, dict) and key in orig and isinstance(orig[key], dict): deep_update(orig[key], value) else: orig[key] = value deep_update(original_data, updates) # Save the updated plist back to the output_fp plistlib.dump(original_data, output_fp, fmt=plistlib.FMT_BINARY) except Exception as e: raise e"},{"question":"**Objective**: To assess your understanding of the `datetime` module and your ability to manipulate date and time objects in Python. **Problem Statement**: You are tasked with implementing a function that calculates the number of weekdays (Monday to Friday) between two given dates, inclusive. Additionally, the function should compute the total number of hours between the two dates during working hours (9 AM to 6 PM) on weekdays. **Function Signature**: ```python def calculate_weekdays_and_hours(start_date: str, end_date: str) -> tuple: pass ``` **Inputs**: - `start_date` (string): The start date in the format `\\"YYYY-MM-DD\\"`. - `end_date` (string): The end date in the format `\\"YYYY-MM-DD\\"`. `end_date` is guaranteed to be on or after `start_date`. **Outputs**: - A tuple `(num_weekdays, total_working_hours)`: - `num_weekdays` (int): The number of weekdays between `start_date` and `end_date`, inclusive. - `total_working_hours` (int): The total number of working hours (from 9 AM to 6 PM) between the two dates. **Constraints**: - Dates are provided in the correct format and are valid. - `start_date` and `end_date` are within the range of `datetime.MINYEAR` to `datetime.MAXYEAR`. **Example**: ```python calculate_weekdays_and_hours(\'2023-10-01\', \'2023-10-10\') # Output: (7, 63) ``` **Explanation**: - The dates between \'2023-10-01\' and \'2023-10-10\' include the weekdays \'2023-10-02\' to \'2023-10-06\' and \'2023-10-09\' to \'2023-10-10\' (since \'2023-10-01\' and \'2023-10-08\' are weekends). - Each weekday has 9 working hours (from 9 AM to 6 PM). - The total number of weekdays is 7. - The total working hours are 7 days * 9 hours/day = 63 hours. **Notes**: - You may use the `datetime` module to help with date manipulations. - Consider edge cases such as when the `start_date` and `end_date` are the same weekday or when they cover weekends. **Implementation Requirements**: - Define the function `calculate_weekdays_and_hours`. - Parse the input date strings using appropriate datetime functions. - Calculate the number of weekdays between the two dates. - Calculate the total number of working hours during weekdays. - Return the result as a tuple `(num_weekdays, total_working_hours)`. **Bonus**: Implementing the function without using any external libraries other than Python\'s standard library.","solution":"from datetime import datetime, timedelta def calculate_weekdays_and_hours(start_date: str, end_date: str) -> tuple: Calculate the number of weekdays and total working hours (9 AM to 6 PM) between two dates inclusive. Args: start_date (str): The start date in \\"YYYY-MM-DD\\" format. end_date (str): The end date in \\"YYYY-MM-DD\\" format. Returns: tuple: (num_weekdays, total_working_hours) # Define working hours per day WORKING_HOURS_PER_DAY = 9 # from 9 AM to 6 PM (9 hours) # Parse the dates start_dt = datetime.strptime(start_date, \'%Y-%m-%d\') end_dt = datetime.strptime(end_date, \'%Y-%m-%d\') # Initialize counters num_weekdays = 0 # Iterate through each day in the interval current_dt = start_dt while current_dt <= end_dt: if current_dt.weekday() < 5: # Monday to Friday are 0 to 4 num_weekdays += 1 current_dt += timedelta(days=1) # Calculate total working hours total_working_hours = num_weekdays * WORKING_HOURS_PER_DAY return (num_weekdays, total_working_hours)"},{"question":"# Python Coding Assessment: Working with Serialized Data Objective In this task, you are required to implement Python bindings for a set of C functions that allow you to serialize and deserialize Python objects to and from a binary format. Function Specifications 1. **write_long_to_file(value: int, file_path: str, version: int) -> None** - **Input:** - `value` - A long integer to be marshalled and written to a file. - `file_path` - Path to the binary file where data will be written. - `version` - Marshal format version (0, 1, or 2). - **Output: None** - Writes the least-significant 32 bits of `value` to the specified file. 2. **write_object_to_file(value: object, file_path: str, version: int) -> None** - **Input:** - `value` - A Python object to be marshalled and written to a file. - `file_path` - Path to the binary file where data will be written. - `version` - Marshal format version (0, 1, or 2). - **Output: None** - Writes the Python object `value` to the specified file. 3. **write_object_to_string(value: object, version: int) -> bytes** - **Input:** - `value` - A Python object to be marshalled into a bytes object. - `version` - Marshal format version (0, 1, or 2). - **Output:** - Returns a bytes object containing the marshalled representation of `value`. 4. **read_long_from_file(file_path: str) -> int** - **Input:** - `file_path` - Path to the binary file from which the data will be read. - **Output:** - Returns an integer value read from the file. 5. **read_object_from_file(file_path: str) -> object** - **Input:** - `file_path` - Path to the binary file from which the data will be read. - **Output:** - Returns a Python object read from the file. 6. **read_object_from_string(data: bytes) -> object** - **Input:** - `data` - A byte buffer containing the marshalled data. - **Output:** - Returns a Python object read from the byte buffer. Constraints 1. Use only the provided `marshal` functions for marshaling and unmarshaling data to/from files and strings. 2. Handle all possible errors as specified by the `marshal` functions (`EOFError`, `ValueError`, `TypeError`). Sample Usage ```python write_long_to_file(123456789, \'data.bin\', 2) print(read_long_from_file(\'data.bin\')) # Output: 123456789 write_object_to_file([1, 2, 3, 4], \'list_data.bin\', 2) print(read_object_from_file(\'list_data.bin\')) # Output: [1, 2, 3, 4] marshalled = write_object_to_string({\\"a\\": 1, \\"b\\": 2}, 2) print(read_object_from_string(marshalled)) # Output: {\'a\': 1, \'b\': 2} ``` Implement each of the specified functions with appropriate error handling and validation.","solution":"import marshal def write_long_to_file(value: int, file_path: str, version: int) -> None: with open(file_path, \'wb\') as file: marshal.dump(value & 0xFFFFFFFF, file, version) def write_object_to_file(value: object, file_path: str, version: int) -> None: with open(file_path, \'wb\') as file: marshal.dump(value, file, version) def write_object_to_string(value: object, version: int) -> bytes: return marshal.dumps(value, version) def read_long_from_file(file_path: str) -> int: with open(file_path, \'rb\') as file: return marshal.load(file) def read_object_from_file(file_path: str) -> object: with open(file_path, \'rb\') as file: return marshal.load(file) def read_object_from_string(data: bytes) -> object: return marshal.loads(data)"},{"question":"**Question:** You are provided with a dataset containing information about penguins\' measurements. Your task is to use the seaborn library to create a series of plots that explore the distribution and relationships between different variables in this dataset. Follow the steps below to complete the task. **Dataset:** Use the `penguins` dataset, which can be loaded using `sns.load_dataset(\\"penguins\\")`. **Steps:** 1. Load the `penguins` dataset using seaborn. 2. Create a histogram of the `flipper_length_mm` variable. 3. Create a kernel density estimate (KDE) plot of the `flipper_length_mm` variable. 4. Create an empirical cumulative distribution function (ECDF) plot of the `flipper_length_mm` variable. 5. Create a bivariate KDE plot of `flipper_length_mm` against `bill_length_mm`. 6. Create a KDE plot of the `flipper_length_mm` for each species of penguin by using the `hue` parameter. 7. Create a faceted KDE plot of the `flipper_length_mm` for each species of penguin separated by `sex` using the `col` parameter. 8. Customize the plots by setting appropriate axis labels and titles. Specifically, for the faceted plot, set the x-axis label to \\"Flipper length (mm)\\", y-axis label to \\"Density (a.u.)\\", and the title of each facet to include the species name followed by \\"penguins\\". **Constraints:** - All plots should be displayed within the notebook. - Ensure that the plots are clearly visible and distinguishable. **Expected Output:** - A series of plots as specified in the steps. - Customized axis labels and titles for the faceted plots. **Code:** ```python import seaborn as sns # Step 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Histogram sns.displot(data=penguins, x=\\"flipper_length_mm\\") # Step 3: KDE plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", kind=\\"kde\\") # Step 4: ECDF plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", kind=\\"ecdf\\") # Step 5: Bivariate KDE plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\") # Step 6: KDE plot by species sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", kind=\\"kde\\") # Step 7: Faceted KDE plot by sex and species g = sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"kde\\") # Step 8: Customize axis labels and titles g.set_axis_labels(\\"Density (a.u.)\\", \\"Flipper length (mm)\\") g.set_titles(\\"{col_name} penguins\\") # Display the plots sns.plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Histogram sns.displot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\'Histogram of Flipper Length\') plt.xlabel(\'Flipper length (mm)\') plt.ylabel(\'Count\') plt.show() # KDE plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", kind=\\"kde\\") plt.title(\'KDE Plot of Flipper Length\') plt.xlabel(\'Flipper length (mm)\') plt.ylabel(\'Density\') plt.show() # ECDF plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", kind=\\"ecdf\\") plt.title(\'ECDF Plot of Flipper Length\') plt.xlabel(\'Flipper length (mm)\') plt.ylabel(\'ECDF\') plt.show() # Bivariate KDE plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\") plt.title(\'Bivariate KDE Plot of Flipper Length vs. Bill Length\') plt.xlabel(\'Flipper length (mm)\') plt.ylabel(\'Bill length (mm)\') plt.show() # KDE plot by species sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", kind=\\"kde\\") plt.title(\'KDE Plot of Flipper Length by Species\') plt.xlabel(\'Flipper length (mm)\') plt.ylabel(\'Density\') plt.show() # Faceted KDE plot by sex and species g = sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"kde\\") g.set_axis_labels(\\"Flipper length (mm)\\", \\"Density (a.u.)\\") g.set_titles(\\"{col_name} penguins\\") plt.show()"},{"question":"You are given the task of developing a secure messaging feature for a system that ensures data integrity using HMAC (Hash-based Message Authentication Code). You will write a Python function to generate and verify the HMAC of messages. Function Specifications: 1. **Function 1: `generate_hmac(key: bytes, message: str, digestmod: str) -> str`** - **Input:** - `key`: A bytestring representing the secret key. - `message`: A string message to authenticate. - `digestmod`: The name of the hash algorithm to use (e.g., \'sha256\'). - **Output:** - Returns the hexadecimal digest of the HMAC for the given message. 2. **Function 2: `verify_hmac(key: bytes, message: str, received_digest: str, digestmod: str) -> bool`** - **Input:** - `key`: A bytestring representing the secret key. - `message`: A string message to authenticate. - `received_digest`: The received HMAC hex digest to compare against. - `digestmod`: The name of the hash algorithm to use (e.g., \'sha256\'). - **Output:** - Returns `True` if the computed HMAC matches the received HMAC; `False` otherwise. Constraints: - The key length must be at least 16 bytes. - The `digestmod` used should be a valid algorithm name supported by the `hashlib` module. Implementation Details: 1. Use the `hmac.new` function to create a new HMAC object. 2. The `update` method should be used if the message is updated in parts. 3. Use the `hexdigest` method to return the HMAC value in hexadecimal format for easy comparison. 4. Use the `compare_digest` method to securely compare two digest values. Example Usage: ```python key = b\'secretkey1234567\' message = \\"The quick brown fox jumps over the lazy dog\\" digestmod = \'sha256\' generated = generate_hmac(key, message, digestmod) # generated will be the hexadecimal digest of the HMAC. is_valid = verify_hmac(key, message, generated, digestmod) # is_valid should return True as the HMAC should match the generated one. ``` Complete the functions `generate_hmac` and `verify_hmac` to ensure the security and integrity of the messages using HMAC.","solution":"import hmac import hashlib def generate_hmac(key: bytes, message: str, digestmod: str) -> str: Generates an HMAC for a given message using the specified key and hash algorithm. if len(key) < 16: raise ValueError(\\"Key must be at least 16 bytes long.\\") hmac_obj = hmac.new(key, message.encode(), digestmod) return hmac_obj.hexdigest() def verify_hmac(key: bytes, message: str, received_digest: str, digestmod: str) -> bool: Verifies an HMAC against the received HMAC. generated_hmac = generate_hmac(key, message, digestmod) return hmac.compare_digest(generated_hmac, received_digest)"},{"question":"# Question: Working with Meta Tensors in PyTorch You are given a partially implemented class `MetaTensorModel` that represents a PyTorch model. This model is initially loaded on the meta device for some preprocessing steps before being moved to the actual computation device (CPU or CUDA). Your goal is to complete this class to: 1. Initialize the model\'s neural network layers on the meta device. 2. Compute the shape of an output tensor when passing an input through an uninitialized (meta) model. 3. Move the model to the CPU and reinitialize its parameters. Requirements: 1. Define the `__init__` method to initialize a linear layer on the meta device. 2. Implement the `get_output_shape` method to return the shape of the output tensor when an input tensor of specified dimensions is passed through the meta-initialized model. 3. Implement the `move_to_device_and_initialize` method to move the model to the CPU and manually reinitialize the parameters. ```python import torch from torch.nn import Linear class MetaTensorModel: def __init__(self, in_features, out_features): Initialize a linear layer on the meta device. Parameters: in_features (int): Number of input features. out_features (int): Number of output features. with torch.device(\'meta\'): self.linear = Linear(in_features, out_features) def get_output_shape(self, input_shape): Get the shape of the output tensor when passing an input tensor with the specified shape through the model. Parameters: input_shape (tuple): Shape of the input tensor, excluding batch dimension. Returns: tuple: Shape of the output tensor, excluding batch dimension. # Create a meta tensor with the input shape input_tensor = torch.empty(*input_shape, device=\'meta\') # Pass it through the linear layer and return the output shape output_tensor = self.linear(input_tensor) return output_tensor.size() def move_to_device_and_initialize(self, device=\'cpu\'): Move the model to the specified device and reinitialize parameters. Parameters: device (str): The target device (\'cpu\' or \'cuda\'). self.linear = self.linear.to_empty(device=device) # Reinitialize the parameters (weights and biases) torch.nn.init.kaiming_uniform_(self.linear.weight, nonlinearity=\'relu\') if self.linear.bias is not None: torch.nn.init.constant_(self.linear.bias, 0) # Example usage: meta_model = MetaTensorModel(20, 30) input_shape = (10, 20) # Batch size 10, 20 input features print(\\"Output shape on meta device:\\", meta_model.get_output_shape(input_shape)) meta_model.move_to_device_and_initialize(device=\'cpu\') print(\\"Model moved to CPU and parameters reinitialized.\\") ``` Constraints: - You cannot convert a meta tensor directly to a CPU/CUDA tensor due to lack of data. - Use PyTorch methods covered in the documentation to handle initialization and movement of tensors. # Evaluation Criteria: - Correct implementation of tensor initialization on the meta device. - Accuracy in calculating output shapes based on meta tensor operations. - Properly moving the model to the desired device and reinitializing its parameters.","solution":"import torch from torch.nn import Linear class MetaTensorModel: def __init__(self, in_features, out_features): Initialize a linear layer on the meta device. Parameters: in_features (int): Number of input features. out_features (int): Number of output features. self.linear = Linear(in_features, out_features, device=\'meta\') def get_output_shape(self, input_shape): Get the shape of the output tensor when passing an input tensor with the specified shape through the model. Parameters: input_shape (tuple): Shape of the input tensor, excluding batch dimension. Returns: tuple: Shape of the output tensor, excluding batch dimension. # Create a meta tensor with the input shape input_tensor = torch.empty(*input_shape, device=\'meta\') # Pass it through the linear layer and return the output shape output_tensor = self.linear(input_tensor) return output_tensor.size()[1:] # excluding batch dimension def move_to_device_and_initialize(self, device=\'cpu\'): Move the model to the specified device and reinitialize parameters. Parameters: device (str): The target device (\'cpu\' or \'cuda\'). self.linear = Linear(self.linear.in_features, self.linear.out_features) self.linear.to(device) # Reinitialize the parameters (weights and biases) torch.nn.init.kaiming_uniform_(self.linear.weight, nonlinearity=\'relu\') if self.linear.bias is not None: torch.nn.init.constant_(self.linear.bias, 0) # Example usage: meta_model = MetaTensorModel(20, 30) input_shape = (10, 20) # Batch size 10, 20 input features print(\\"Output shape on meta device:\\", meta_model.get_output_shape(input_shape)) meta_model.move_to_device_and_initialize(device=\'cpu\') print(\\"Model moved to CPU and parameters reinitialized.\\")"},{"question":"# PyTorch XPU: Device Memory Management and Stream Optimization Objective Write a function `optimize_matrix_operations` that performs optimized matrix operations using PyTorch\'s torch.xpu module. The function should demonstrate the following aspects: - Device memory management. - Stream usage to optimize operations. Function Signature ```python def optimize_matrix_operations(N: int, device_id: int = 0) -> torch.Tensor: pass ``` Input - `N` (int): The size of the NxN matrix to be generated. - `device_id` (int, optional): The ID of the target device. Defaults to 0. Output - `result` (torch.Tensor): The resulting NxN tensor after the operations have been performed on the specified device. Requirements 1. The function should: - Initialize the target device using `device_id`. - Generate two random NxN matrices `A` and `B` on the specified device. - Multiply these matrices and add the result to another random NxN matrix `C`. - Use streams to synchronize and optimize these operations. - Properly handle memory allocation and deallocation. 2. Ensure that: - The operations are performed on the specified device. - The random matrices are generated using the same seed for reproducibility. 3. Constraints: - You should use PyTorch functions provided by `torch.xpu` for device and memory management, stream handling, and matrix operations. Example Usage ```python result = optimize_matrix_operations(1024, device_id=1) print(result) ``` Notes - Ensure to handle exceptions where the specified device is not available. - Document your code to explain your thought process and the usage of different torch.xpu functions. - You can use the following PyTorch random seed for reproducibility: `torch.manual_seed(42)`.","solution":"import torch def optimize_matrix_operations(N: int, device_id: int = 0) -> torch.Tensor: Performs optimized matrix operations using PyTorch\'s torch.xpu module, including device memory management and stream usage. :param N: The size of the NxN matrix to be generated. :param device_id: The ID of the target device (default is 0). :return: The resulting NxN tensor after the operations have been performed on the specified device. # Check if XPU device is available if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available.\\") # Set the target device using device_id device = torch.device(f\'xpu:{device_id}\') # Set seed for reproducibility torch.manual_seed(42) # Initialize two streams stream1 = torch.cuda.Stream(device=device, priority=1) stream2 = torch.cuda.Stream(device=device, priority=0) with torch.cuda.stream(stream1): # Create random NxN matrices A and B on the specified device A = torch.randn(N, N, device=device) B = torch.randn(N, N, device=device) with torch.cuda.stream(stream2): # Create random NxN matrix C on the specified device C = torch.randn(N, N, device=device) # Synchronize the streams stream1.synchronize() stream2.synchronize() with torch.cuda.stream(torch.cuda.default_stream(device=device)): # Perform optimized matrix operations result = torch.matmul(A, B) + C return result"},{"question":"**Coding Assessment Question: Tkinter Message Boxes** # Objective You are tasked with creating a simple Tkinter-based GUI application that interacts with the user through different types of message boxes provided by the `tkinter.messagebox` module. This exercise will demonstrate your understanding of handling user inputs and responses using modal message boxes for various scenarios. # Instructions 1. Create a Tkinter application window with the following buttons: - \\"Show Info\\" - Displays an informational message using `showinfo`. - \\"Show Warning\\" - Displays a warning message using `showwarning`. - \\"Show Error\\" - Displays an error message using `showerror`. - \\"Delete File\\" - Asks for confirmation to delete a file using `askyesno`. - \\"Retry Operation\\" - Prompts the user to retry an operation using `askretrycancel`. 2. Implement functionalities for each button where: - \\"Show Info\\" button displays an information message box with a title \\"Information\\" and message \\"This is an info message.\\" - \\"Show Warning\\" button displays a warning message box with a title \\"Warning\\" and message \\"This is a warning message.\\" - \\"Show Error\\" button displays an error message box with a title \\"Error\\" and message \\"This is an error message.\\" - \\"Delete File\\" button asks the user \\"Are you sure you want to delete the file?\\" and prints \\"File deleted.\\" if the user selects Yes, otherwise prints \\"File not deleted.\\" - \\"Retry Operation\\" button asks the user \\"Do you want to retry the operation?\\" and prints \\"Retrying operation...\\" if the user selects Retry, otherwise prints \\"Operation canceled.\\" # Constraints - The application must use the `tkinter.messagebox` module for the message boxes. - Properly handle the user\'s responses and ensure that the correct messages are printed to the console. - The GUI must be responsive and handle multiple clicks properly. # Example Output ```plaintext File deleted. Operation canceled. This is a warning message. ``` # Submission Guidelines Submit your `main.py` file containing the Tkinter application code. Ensure that all required functionalities are implemented and tested.","solution":"import tkinter as tk from tkinter import messagebox def show_info(): messagebox.showinfo(\\"Information\\", \\"This is an info message.\\") def show_warning(): messagebox.showwarning(\\"Warning\\", \\"This is a warning message.\\") def show_error(): messagebox.showerror(\\"Error\\", \\"This is an error message.\\") def delete_file(): result = messagebox.askyesno(\\"Delete File\\", \\"Are you sure you want to delete the file?\\") if result: print(\\"File deleted.\\") else: print(\\"File not deleted.\\") def retry_operation(): result = messagebox.askretrycancel(\\"Retry Operation\\", \\"Do you want to retry the operation?\\") if result: print(\\"Retrying operation...\\") else: print(\\"Operation canceled.\\") def main(): root = tk.Tk() root.title(\\"Tkinter Message Boxes\\") tk.Button(root, text=\\"Show Info\\", command=show_info).pack(pady=5) tk.Button(root, text=\\"Show Warning\\", command=show_warning).pack(pady=5) tk.Button(root, text=\\"Show Error\\", command=show_error).pack(pady=5) tk.Button(root, text=\\"Delete File\\", command=delete_file).pack(pady=5) tk.Button(root, text=\\"Retry Operation\\", command=retry_operation).pack(pady=5) root.mainloop() if __name__ == \\"__main__\\": main()"},{"question":"You are required to implement a custom sequence class called `CustomSequence` that mimics the behavior of Python lists and demonstrates your understanding of the Sequence Protocol. Specifically, your class should support common sequence operations such as indexing, slicing, iteration, and length calculation. Additionally, it should support some basic numerical operations like addition of two sequences. # Class: `CustomSequence` Attributes: - `data`: This private attribute should store the actual data of the sequence. Methods: 1. **Initialization**: - `__init__(self, initial_data)`: Initialize the sequence with the provided initial data, which can be any iterable. 2. **Indexing**: - `__getitem__(self, index)`: Support for element access using indexing. Should raise `IndexError` for out-of-range indices. 3. **Slicing**: - `__getitem__(self, slice)`: Support for slicing. Should return a new `CustomSequence` object with the sliced data. 4. **Length Calculation**: - `__len__(self)`: Return the number of elements in the sequence. 5. **Iteration**: - `__iter__(self)`: Return an iterator object for the sequence. 6. **Addition**: - `__add__(self, other)`: Support for adding two sequences. Should return a new `CustomSequence` object containing elements of both sequences. # Constraints: - You should not use any built-in list or sequence functionalities (like list methods) directly. Instead, manually implement the required functionality. - You need to handle only flat sequences (do not worry about nested sequences). Example Usage: ```python # Initializing the sequence seq1 = CustomSequence([1, 2, 3]) seq2 = CustomSequence([4, 5, 6]) # Length of sequence print(len(seq1)) # Output: 3 # Indexing print(seq1[1]) # Output: 2 # Slicing print(seq1[1:3].data) # Output: [2, 3] # Iteration for item in seq1: print(item) # Output: # 1 # 2 # 3 # Addition seq3 = seq1 + seq2 print(seq3.data) # Output: [1, 2, 3, 4, 5, 6] ``` Write your solution for the `CustomSequence` class implementation in Python below.","solution":"class CustomSequence: def __init__(self, initial_data): Initialize the sequence with the provided initial data, which can be any iterable. self._data = list(initial_data) def __getitem__(self, index): Support for element access using indexing. if isinstance(index, slice): return CustomSequence(self._data[index]) if index >= len(self._data) or index < -len(self._data): raise IndexError(\\"Index out of range\\") return self._data[index] def __len__(self): Return the number of elements in the sequence. return len(self._data) def __iter__(self): Return an iterator object for the sequence. return iter(self._data) def __add__(self, other): Support for adding two sequences. if not isinstance(other, CustomSequence): raise TypeError(\\"Can only add CustomSequence to CustomSequence\\") return CustomSequence(self._data + other._data) @property def data(self): return self._data"},{"question":"# Question: Implementing and Utilizing GenericAlias in Python Objective: Design and implement a custom generic container class in pure Python that mimics the behavior of the `GenericAlias` type provided in the Python API documentation. Requirements: 1. **Class Definition**: - Create a class `CustomList` that can store elements of any specified type. - It should support type hinting and generics similar to how `List[int]` or `List[str]` works. 2. **Type Hinting**: - Implement a way to specify and enforce type for the elements of `CustomList`. - Utilize `__class_getitem__` to return a proper type-hinted generic alias. 3. **Initializations and Methods**: - The class should allow initialization with zero or more elements of the specified type. - Implement basic list-like methods (`append`, `__getitem__`, `__len__`). 4. **Enforcement**: - Ensure that elements being added to the list are of the specified type and raise appropriate exceptions if not. Input and Output Specifications: - **Input**: Initialization and operations on the `CustomList`. For example: ```python ints = CustomList[int]() ints.append(1) # should succeed ints.append(\'a\') # should raise TypeError ``` - **Output**: Appropriate responses should be generated based on the operations performed on the list. Example: ```python # Define the CustomList Class class CustomList: # Your implementation goes here... # Usage Example ints = CustomList[int]() # Create an empty list for integers ints.append(1) # Appends a valid integer print(ints[0]) # Should print 1 try: ints.append(\'a\') # Should raise TypeError except TypeError as e: print(e) # Output the error message ``` Constraints: - You cannot use external libraries other than Pythonâ€™s standard library. - You should not directly use existing generic classes from `typing`. - Focus on implementing the core functionality as described.","solution":"from typing import Generic, TypeVar T = TypeVar(\'T\') class CustomList(Generic[T]): def __init__(self): self._elements = [] def append(self, item: T): if not isinstance(item, self._type): raise TypeError(f\\"Expected item of type {self._type.__name__}, got {type(item).__name__}\\") self._elements.append(item) def __getitem__(self, index: int) -> T: return self._elements[index] def __len__(self) -> int: return len(self._elements) def __iter__(self): return iter(self._elements) @classmethod def __class_getitem__(cls, item): class CustomListTyped(cls): _type = item return CustomListTyped"},{"question":"**Objective:** Write a Python function using Seaborn\'s objects interface to create a complex plot demonstrating multiple seaborn functionalities, focusing on the fmri dataset. **Function Signature:** ```python def create_fmri_plot(): pass ``` **Requirements:** 1. Load the `fmri` dataset using `seaborn.load_dataset`. 2. Filter the dataset to include only observations where the region is \'frontal\' and the event is \'stim\'. 3. Create a line plot of `signal` over `timepoint` with the following customizations: - Group the lines by the `subject` column. - Differentiate lines by `subject` using different colors. - Add error bands representing the standard deviation around the average signal for each time point. - Include markers at each data point to highlight the individual observations. 4. Ensure the plot includes appropriate axis labels and a legend to differentiate subjects. **Expected Output:** - A single plot containing: - Multiple colored lines representing different subjects. - Error bands around each line indicating the standard deviation. - Markers at each data observation point. **Hints:** - Use `so.Plot` to create the base plot. - Use `.add(so.Line())` to add lines to the plot. - Use `.add(so.Band())` to add error bands. - Use `marker` parameter within `so.Line` to add markers to the lines. **Constraints:** - You must only use functions and classes from seaborn\'s objects interface (`seaborn.objects`). - Do not use any other plotting libraries. **Example Call:** ```python create_fmri_plot() ``` This call should display the required plot with the customizations mentioned.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_fmri_plot(): Creates a line plot of the fmri dataset with customizations using seaborn objects interface. The plot contains lines representing different subjects, error bands, and markers at each data point. # Load the fmri dataset fmri = sns.load_dataset(\'fmri\') # Filter dataset to include only observations where the region is \'frontal\' and the event is \'stim\' fmri_filtered = fmri[(fmri[\'region\'] == \'frontal\') & (fmri[\'event\'] == \'stim\')] # Create the plot plot = (so.Plot(fmri_filtered, x=\\"timepoint\\", y=\\"signal\\", color=\\"subject\\") .add(so.Line(marker=True)) .add(so.Band()) .label(x=\\"Time Point\\", y=\\"Signal\\", color=\\"Subject\\")) # Draw the plot plot.show() # If running standalone, you can uncomment the below line: # create_fmri_plot()"},{"question":"You are given a dataset containing information about various features of houses and their corresponding prices. Your task is to implement a pipeline using scikit-learn that will fit a `DecisionTreeRegressor` to predict house prices. The dataset is provided in CSV format with the following columns: - `feature_1`, `feature_2`, ..., `feature_n`: Numerical features describing the properties of the house. - `price`: The price of the house. The dataset contains missing values, which you need to handle appropriately. # Your task: 1. Load the dataset from the CSV file (assume the file is named `house_prices.csv`). 2. Handle missing values by filling them with the mean of their respective columns. 3. Split the dataset into training and testing sets (80% training, 20% testing). 4. Fit a `DecisionTreeRegressor` model to the training data. You should use grid search cross-validation to find the best parameters for: - `max_depth`: [3, 5, 10, None] - `min_samples_split`: [2, 5, 10] - `min_samples_leaf`: [1, 2, 4] 5. Evaluate the model on the testing set using Mean Squared Error (MSE). 6. Output the best parameters found, the training MSE, and the testing MSE. 7. Plot the tree using `plot_tree` from scikit-learn. # Constraints: - You may not use any other machine learning libraries besides scikit-learn. - You must use NumPy and Pandas for data handling. - Ensure your code is well-structured and includes necessary comments for clarity. # Expected Input and Output: ```python def train_and_evaluate_decision_tree(csv_file: str): # Load the dataset # Handle missing values # Split the dataset # Perform grid search cross-validation # Fit the model # Evaluate the model # Plot the tree # Return the best parameters and MSE values # Example function call best_params, train_mse, test_mse = train_and_evaluate_decision_tree(\\"house_prices.csv\\") print(\\"Best parameters:\\", best_params) print(\\"Training MSE:\\", train_mse) print(\\"Testing MSE:\\", test_mse) ``` This question assesses the student\'s ability to: - Load and preprocess data. - Handle missing values. - Split datasets for training and testing. - Apply a machine learning model with parameter tuning. - Evaluate the model. - Visualize the decision tree.","solution":"import numpy as np import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.tree import DecisionTreeRegressor, plot_tree from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt def train_and_evaluate_decision_tree(csv_file: str): # Load the dataset data = pd.read_csv(csv_file) # Handle missing values data.fillna(data.mean(), inplace=True) # Split the dataset into features and target X = data.drop(\'price\', axis=1) y = data[\'price\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the DecisionTreeRegressor and parameter grid for Grid Search model = DecisionTreeRegressor(random_state=42) param_grid = { \'max_depth\': [3, 5, 10, None], \'min_samples_split\': [2, 5, 10], \'min_samples_leaf\': [1, 2, 4] } # Perform grid search cross-validation grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=\'neg_mean_squared_error\', cv=5) grid_search.fit(X_train, y_train) # Get the best parameters and best model best_params = grid_search.best_params_ best_model = grid_search.best_estimator_ # Evaluate the model y_train_pred = best_model.predict(X_train) y_test_pred = best_model.predict(X_test) train_mse = mean_squared_error(y_train, y_train_pred) test_mse = mean_squared_error(y_test, y_test_pred) # Plot the tree plt.figure(figsize=(20, 10)) plot_tree(best_model, feature_names=X.columns, filled=True) plt.show() return best_params, train_mse, test_mse"},{"question":"# Advanced Python Type Hints and Generics Objective Your task is to implement a class `DataProcessor` with methods that utilize Python\'s typing and generics features to process different types of data robustly and type-safely. Requirements 1. **Class Definition**: - Define a class `DataProcessor` that is generic over a type variable `T`, which could be any of `int`, `float`, `str`, or `list`. - The class should have one private attribute: ```python data: list[T] ``` 2. **Methods**: - **Add Data**: Implement an `add_data` method that takes a list of type `T` and appends its elements to the `data` attribute. ```python def add_data(self, new_data: list[T]) -> None: ``` - **Compute Metrics**: Implement a `compute_metrics` method with appropriate return types using type hints, considering different operations based on the data type: ```python def compute_metrics(self) -> Union[int, float, str, dict[str, Any]]: ``` - For `int` and `float`, return the sum. - For `str`, return the concatenated string. - For `list`, treat it as a list of `ints` and return a dictionary with the following metrics: - `\\"sum\\"`: Sum of all numbers. - `\\"average\\"`: Average of the numbers. - `\\"min\\"`: Minimum number. - `\\"max\\"`: Maximum number. Example Usage ```python from typing import Union, TypeVar, Generic, List, Dict from collections.abc import Callable T = TypeVar(\'T\', int, float, str, List[int]) class DataProcessor(Generic[T]): def __init__(self): self.data: List[T] = [] def add_data(self, new_data: List[T]) -> None: self.data.extend(new_data) def compute_metrics(self) -> Union[int, float, str, Dict[str, float]]: if isinstance(self.data[0], (int, float)): return sum(self.data) elif isinstance(self.data[0], str): return \'\'.join(self.data) elif isinstance(self.data[0], list): flat_list = [item for sublist in self.data for item in sublist] return { \\"sum\\": sum(flat_list), \\"average\\": sum(flat_list) / len(flat_list), \\"min\\": min(flat_list), \\"max\\": max(flat_list) } # Usage: processor = DataProcessor[int]() processor.add_data([1, 2, 3, 4]) print(processor.compute_metrics()) # Output: 10 processor_str = DataProcessor[str]() processor_str.add_data([\\"hello\\", \\" \\", \\"world\\"]) print(processor_str.compute_metrics()) # Output: \'hello world\' processor_list = DataProcessor[List[int]]() processor_list.add_data([[1, 2], [3, 4]]) print(processor_list.compute_metrics()) # Output: {\'sum\': 10, \'average\': 2.5, \'min\': 1, \'max\': 4} ``` Constraints - Ensure type safety and use Python\'s type hinting to its full extent. - You may not use libraries such as `numpy` or `pandas`. - Consider edge cases and empty data inputs gracefully. Tip Refer to the `typing` module\'s documentation to understand how to use `TypeVar`, `Generic`, `Union`, `List`, and other typing constructs effectively.","solution":"from typing import TypeVar, Generic, Union, List, Dict, Any T = TypeVar(\'T\', int, float, str, List[int]) class DataProcessor(Generic[T]): def __init__(self): self.data: List[T] = [] def add_data(self, new_data: List[T]) -> None: self.data.extend(new_data) def compute_metrics(self) -> Union[int, float, str, Dict[str, Any]]: if not self.data: raise ValueError(\\"No data to compute metrics.\\") first_element = self.data[0] if isinstance(first_element, (int, float)): return sum(self.data) elif isinstance(first_element, str): return \'\'.join(self.data) elif isinstance(first_element, list): flat_list = [item for sublist in self.data for item in sublist] return { \\"sum\\": sum(flat_list), \\"average\\": sum(flat_list) / len(flat_list), \\"min\\": min(flat_list), \\"max\\": max(flat_list) } else: raise TypeError(\\"Unsupported data type for metrics computation.\\")"},{"question":"**Coding Assessment Question:** You are tasked with writing a Python function that checks a series of path manipulations for a given directory and its subdirectories. Specifically, the function should: 1. Generate an absolute path for the given directory. 2. Expand any `~` to the home directory in the given directory path. 3. Check if the given directory exists. 4. For all files in the directory (recursively including subdirectories): - Print their base names. - Print their sizes in bytes. - Normalize their paths. - Construct a relative path from the given directory. - Determine and print if they are regular files or directories. The function signature should be: ```python def directory_inspector(dir_path: str) -> None: pass ``` # Constraints: - The directory may not exist, and the function should handle such cases gracefully. - Assume you have sufficient permissions to read the directory and its files. - The directory path can be a string or a path-like object. # Input: - `dir_path`: A string representing the directory path. # Output: - The function prints the required output to the console as specified. # Example: ```python # Example directory structure: # /home/user/projects/ # â”œâ”€â”€ file1.txt # â”œâ”€â”€ file2.py # â”œâ”€â”€ subdir1/ # â”‚ â”œâ”€â”€ file3.txt # â”‚ â””â”€â”€ script.sh # â””â”€â”€ subdir2/ # â””â”€â”€ document.pdf directory_inspector(\\"/home/user/projects\\") # Example output: # Absolute path: /home/user/projects # Expanded path: /home/user/projects # Path /home/user/projects exists: True # files and directories in /home/user/projects: # file1.txt - size: 4 bytes - normalized path: /home/user/projects/file1.txt - relative path: file1.txt - is file: True # file2.py - size: 8 bytes - normalized path: /home/user/projects/file2.py - relative path: file2.py - is file: True # file3.txt - size: 6 bytes - normalized path: /home/user/projects/subdir1/file3.txt - relative path: subdir1/file3.txt - is file: True # script.sh - size: 5 bytes - normalized path: /home/user/projects/subdir1/script.sh - relative path: subdir1/script.sh - is file: True # document.pdf - size: 10 bytes - normalized path: /home/user/projects/subdir2/document.pdf - relative path: subdir2/document.pdf - is file: True ``` This function will allow students to demonstrate their understanding of multiple aspects of the `os.path` module including path normalization, existence checks, and path manipulations.","solution":"import os def directory_inspector(dir_path: str) -> None: Inspects the given directory, printing the absolute path, expanded path from ~, whether the directory exists, and information about files contained within it. # Generate absolute path for the given directory abs_path = os.path.abspath(dir_path) print(\\"Absolute path:\\", abs_path) # Expand any `~` to the home directory in the given directory path expanded_path = os.path.expanduser(dir_path) print(\\"Expanded path:\\", expanded_path) # Check if the given directory exists dir_exists = os.path.exists(expanded_path) print(f\\"Path {expanded_path} exists:\\", dir_exists) if not dir_exists: return # Function to inspect a directory recursively for root, dirs, files in os.walk(expanded_path): for name in files: full_path = os.path.join(root, name) base_name = os.path.basename(full_path) size = os.path.getsize(full_path) normalized_path = os.path.normpath(full_path) relative_path = os.path.relpath(full_path, expanded_path) is_file = os.path.isfile(full_path) print(f\\"{base_name} - size: {size} bytes - normalized path: {normalized_path} - relative path: {relative_path} - is file: {is_file}\\") for name in dirs: full_path = os.path.join(root, name) base_name = os.path.basename(full_path) normalized_path = os.path.normpath(full_path) relative_path = os.path.relpath(full_path, expanded_path) is_file = os.path.isfile(full_path) print(f\\"{base_name} - normalized path: {normalized_path} - relative path: {relative_path} - is file: {is_file}\\")"},{"question":"**Objective:** Your task is to demonstrate your understanding of the `seaborn` plotting library, specifically the `seaborn.objects` module. **Task:** You need to create a function `custom_plot(x_points, y_points, x_limits, y_limits)` that generates a line plot. Additionally, you must configure the plot\'s x and y limits based on the provided arguments. **Function Signature:** ```python def custom_plot(x_points: list, y_points: list, x_limits: tuple, y_limits: tuple) -> None: ``` **Inputs:** - `x_points`: A list of numeric values for the x-axis. - `y_points`: A list of numeric values for the y-axis (same length as `x_points`). - `x_limits`: A tuple of two values `(min_x, max_x)` specifying the limits for the x-axis. Either value can be `None` to use the default seaborn limits. - `y_limits`: A tuple of two values `(min_y, max_y)` specifying the limits for the y-axis. Either value can be `None` to use the default seaborn limits. **Outputs:** - The function should render the plot based on given points and axis limits. **Constraints:** - You should use seaborn\'s `so.Plot` for plotting. - Utilize the `.limit()` method to set the axis bounds. **Example:** ```python x_points = [1, 2, 3] y_points = [1, 3, 2] x_limits = (0, 4) y_limits = (0, 5) custom_plot(x_points, y_points, x_limits, y_limits) ``` This should display a line plot with the specified x and y limits. **Additional Requirements:** - In your implementation, ensure to handle cases where one or both of the x_limits or y_limits are `None`. - Maintain clean and readable code, following Pythonic conventions. **Note:** Ensure that seaborn is installed in your environment. You can run `pip install seaborn` if it is not already installed. Good luck!","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def custom_plot(x_points: list, y_points: list, x_limits: tuple, y_limits: tuple) -> None: Generates a line plot with the given x and y points, and sets the x and y axis limits. Parameters: x_points (list): Points on the x axis y_points (list): Points on the y axis x_limits (tuple): Limits for the x axis (min_x, max_x) y_limits (tuple): Limits for the y axis (min_y, max_y) # Create a DataFrame from the points data = {\'x\': x_points, \'y\': y_points} # Initialize the plot plot = so.Plot(data, x=\'x\', y=\'y\').add(so.Line()) # Set the x and y limits if x_limits[0] is not None or x_limits[1] is not None: plot = plot.limit(x=x_limits) if y_limits[0] is not None or y_limits[1] is not None: plot = plot.limit(y=y_limits) # Render the plot plot.show() plt.show()"},{"question":"**Coding Assessment Question: Understanding and Utilizing Seaborn Objects for Advanced Visualization Tasks** # Objective: This question aims to assess your ability to use the seaborn library\'s objects interface to create complex visualizations from a data preprocessing stage to output. # Tasks: You are provided with the Titanic dataset. 1. Load the dataset using the `load_dataset` function from seaborn. 2. Preprocess the dataset by sorting the values in descending order by the \'fare\' column. 3. Create a seaborn `so.Plot` object to visualize the relationship between \'fare\' and \'age\', distinguishing data points by the \'sex\' category through color coding. The visualization should: - Use a `Bar` plot to represent the fare amounts. - Use the `Hist` plot type to create histograms representing the fare distributions, with bin width of 20. - Apply a stacking transformation to the plots. - Facet the plot by the \'class\' variable. # Input: - No direct input required from the users, as they will use the Titanic dataset from seaborn. # Output: - A plot that meets the specifications detailed above. # Constraints: - Ensure the code is efficient and effectively uses seaborn\'s functionality. - You may not use additional libraries for visualization beyond seaborn and any necessary standard libraries (e.g., pandas). # Example: ```python # Expected method outline: import seaborn.objects as so from seaborn import load_dataset # Step 1: Load dataset titanic = load_dataset(\\"titanic\\") # Step 2: Sort dataset by \'fare\' in descending order titanic_sorted = titanic.sort_values(\\"fare\\", ascending=False) # Step 3: Create a seaborn plot with the specified requirements plot = ( so.Plot(titanic_sorted, x=\\"fare\\", color=\\"sex\\") .facet(\\"class\\") .add(so.Bars(), so.Hist(binwidth=20), so.Stack()) ) # Display the plot plot.show() ``` # Note: - Make sure your final plot is well-labeled with appropriate titles and legends for clarity.","solution":"import seaborn.objects as so from seaborn import load_dataset # Step 1: Load dataset titanic = load_dataset(\\"titanic\\") # Step 2: Sort dataset by \'fare\' in descending order titanic_sorted = titanic.sort_values(\\"fare\\", ascending=False) # Step 3: Create a seaborn plot with the specified requirements plot = ( so.Plot(titanic_sorted, x=\\"fare\\", color=\\"sex\\") .facet(\\"class\\") .add(so.Bars(), so.Hist(binwidth=20), so.Stack()) ) # Display the plot plot.show()"},{"question":"**Problem Statement:** You are tasked with developing a Python script to automate the management of the \\"pip\\" installer using the \\"ensurepip\\" package. Your script should allow users to: 1. Check the current version of \\"pip\\" that would be installed by \\"ensurepip\\". 2. Install or upgrade \\"pip\\" with options to customize the installation directory, install for a specific user, and control the installed scripts. Implement a function `manage_pip(action: str, root: str = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = False) -> str` that performs the specified action and returns a confirmation message. # Function Signature: ```python def manage_pip(action: str, root: str = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = False) -> str: ``` # Parameters: - `action` (str): The action to perform. Valid values are: - `\\"check_version\\"`: Check the current version of \\"pip\\" that would be installed. - `\\"install_pip\\"`: Install or upgrade \\"pip\\" using the specified options. - `root` (str, optional): An alternative root directory for installation. - `upgrade` (bool, optional): Whether to upgrade an existing \\"pip\\" installation. Default is `False`. - `user` (bool, optional): Whether to use the user scheme rather than installing globally. Default is `False`. - `altinstall` (bool, optional): Whether to avoid installing \\"pipX\\" script. Default is `False`. - `default_pip` (bool, optional): Whether to install the \\"pip\\" script in addition to the versioned scripts. Default is `False`. # Returns: - `str`: A message confirming the action taken. # Example Usage: ```python print(manage_pip(\\"check_version\\")) # Output: \\"pip version available for installation: <version>\\" print(manage_pip(\\"install_pip\\", upgrade=True, user=True)) # Output: \\"pip was installed/updated successfully with user option enabled.\\" ``` # Constraints: - Only valid `action` values should be processed. Any invalid value should raise a `ValueError`. - If both `altinstall` and `default_pip` are set to `True`, raise a `ValueError`. # Additional Notes: - The function should use `ensurepip.version()` to check the \\"pip\\" version. - The function should use `ensurepip.bootstrap()` to install or upgrade \\"pip\\" with the specified options. - Handle and raise any exceptions appropriately to ensure the function behaves as expected.","solution":"import ensurepip def manage_pip(action: str, root: str = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = False) -> str: Manages \'pip\' using the \'ensurepip\' package. Parameters: - action (str): The action to perform. Valid values are \'check_version\' and \'install_pip\'. - root (str, optional): An alternative root directory for installation. - upgrade (bool, optional): Upgrade an existing \'pip\' installation. Default is False. - user (bool, optional): Use the user scheme rather than installing globally. Default is False. - altinstall (bool, optional): Avoid installing the \'pipX\' script. Default is False. - default_pip (bool, optional): Install the \'pip\' script in addition to the versioned scripts. Default is False. Returns: - str: A message confirming the action taken. Raises: - ValueError: If an invalid action is provided or both altinstall and default_pip are set to True. if action not in [\\"check_version\\", \\"install_pip\\"]: raise ValueError(\\"Invalid action. Valid values are \'check_version\' and \'install_pip\'.\\") if altinstall and default_pip: raise ValueError(\\"altinstall and default_pip cannot both be True.\\") if action == \\"check_version\\": version_info = ensurepip.version() return f\\"pip version available for installation: {version_info}\\" elif action == \\"install_pip\\": ensurepip.bootstrap(root=root, upgrade=upgrade, user=user, altinstall=altinstall, default_pip=default_pip) return \\"pip was installed/updated successfully.\\""},{"question":"# Advanced Python Coding Assessment Objective: To assess your understanding of asynchronous programming in Python using the `asyncio` module, including debugging, concurrency, running blocking code, and proper task handling. Task: Create an asynchronous Python application that simulates a simplified real-world scenario: a web scraping service. This service should concurrently scrape multiple web pages for content, process the data, and log the results. Requirements: 1. **Function `fetch_page`**: - This coroutine simulates fetching a web page by sleeping for a random time between 0.1 to 1 second. - Use the `asyncio.sleep` function. - Randomize the sleep time using the `random` module. **Input**: URL (string) **Output**: Content of the page (string, can be just the URL for simplicity) 2. **Function `process_content`**: - This coroutine takes the content fetched from a page, simulates some processing by sleeping for 0.5 seconds, and returns the processed result. - Use the `asyncio.sleep` function. **Input**: Content (string) **Output**: Processed content (string) 3. **Function `main`**: - In this coroutine, create a list of URLs to scrape. - Use `asyncio.create_task` to scrape and process each page concurrently. - Collect and print all the processed results at the end. - Implement proper logging for each step (fetching, processing, and result collection). 4. **Handling Blockages**: - Simulate a slow blocking operation (like a CPU-intensive task) for one of the URLs. Use the `loop.run_in_executor` with `concurrent.futures.ThreadPoolExecutor` to handle this without blocking the event loop. - Ensure the logging configuration logs messages with level `DEBUG`. 5. **Error Handling**: - Implement error handling such that if fetching or processing fails for any URL, it logs an appropriate error message without crashing the entire program. 6. **Concurrency & Debugging**: - Enable asyncio\'s debug mode. - Log if any callbacks take longer than 100ms using the appropriate attribute. Constraints: - You must use the `asyncio` library. - Ensure your implementation can handle at least 5 URLs concurrently. Input: A list of URLs. Output: Logged messages indicating the fetching, processing, and completion of each URL with its processed content. Feel free to create any additional helper functions if necessary to keep your code organized and clean. ```python import asyncio import logging import random from concurrent.futures import ThreadPoolExecutor # Write your code here ```","solution":"import asyncio import logging import random from concurrent.futures import ThreadPoolExecutor logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) async def fetch_page(url): Simulates fetching a web page by sleeping for a random time between 0.1 to 1 second. sleep_time = random.uniform(0.1, 1) logger.debug(f\\"Fetching {url} which will take {sleep_time:.2f} seconds.\\") await asyncio.sleep(sleep_time) return f\\"Content of {url}\\" async def process_content(content): Takes the content fetched from a page, simulates some processing by sleeping for 0.5 seconds, and returns the processed result. logger.debug(f\\"Processing content: {content}\\") await asyncio.sleep(0.5) return f\\"Processed {content}\\" async def simulate_blocking_operation(url): Simulates a slow blocking operation using ThreadPoolExecutor. def blocking_task(): sleep_time = random.uniform(1, 2) logger.debug(f\\"Blocking operation for {url} which will take {sleep_time:.2f} seconds.\\") import time time.sleep(sleep_time) return f\\"Blocking result of {url}\\" loop = asyncio.get_event_loop() with ThreadPoolExecutor() as pool: result = await loop.run_in_executor(pool, blocking_task) return result async def main(): Main coroutine to handle creating tasks to fetch and process URLs concurrently. urls = [f\\"http://example.com/page{i}\\" for i in range(5)] # Enable asyncio\'s debug mode. asyncio.get_running_loop().set_debug(True) tasks = [] for url in urls: if random.choice([True, False]): tasks.append(asyncio.create_task(simulate_blocking_operation(url))) else: tasks.append(asyncio.create_task(fetch_and_process(url))) processed_results = await asyncio.gather(*tasks, return_exceptions=True) for result in processed_results: if isinstance(result, Exception): logger.error(f\\"Error encountered: {result}\\") else: logger.info(f\\"Completed task with result: {result}\\") async def fetch_and_process(url): Helper coroutine to fetch and process content from a URL. try: content = await fetch_page(url) processed_content = await process_content(content) return processed_content except Exception as e: logger.error(f\\"Failed to fetch and process {url}: {e}\\") raise e if __name__ == \\"__main__\\": # Run the main coroutine asyncio.run(main())"},{"question":"**Question**: Implementing a Simple Text-Based Grid Game Using `curses` Module --- # Objective: Create a simple text-based grid game using the `curses` module, where the player can move a character around the grid and collect items. # Description: You need to implement a Python script that initializes a curses window and creates a game grid of size 10x10. The player uses arrow keys to move a character (\'@\') around the grid. Randomly place three items (\'*\') on the grid which the player can collect. When the player collects all items, print a congratulatory message and exit the game. # Requirements: 1. **Initialization and Setup**: - Initialize the curses screen using `curses.initscr()`. - Turn off automatic echoing of keys to the screen (`curses.noecho()`). - Enable input to be processed immediately (`curses.cbreak()`). - Enable keypad mode to capture special keys (`stdscr.keypad(True)`). 2. **Grid Display**: - Create a 10x10 grid and display it. - Display the player character (\'@\') at a random initial position. - Place three items (\'*\') at random positions within the grid ensuring they do not overlap with the playerâ€™s initial position or each other. 3. **Player Movement**: - Capture arrow key inputs to move the player character within the grid boundaries. - Update the grid to reflect the new player position after each move. 4. **Item Collection**: - If the playerâ€™s new position coincides with an item, remove the item from the grid and count it as collected. - Display collected item count at the bottom of the grid. 5. **Game End Condition**: - When all three items are collected, display a message \\"Congratulations! You collected all items!\\" and exit the game. 6. **Cleanup**: - Ensure the terminal is restored to its initial state when the game ends by using `curses.endwin()`. # Constraints: - Ensure that player movement does not move the character outside the grid boundaries. - Randomly generated positions should neither overlap initially nor after item collection. - Use the `curses.wrapper()` function to handle initialization and cleanup for better error handling. # Sample Code Structure: ```python import curses import random def main(stdscr): # Initialize game variables and settings curses.curs_set(False) # Hide the cursor stdscr.clear() # Clear the screen height, width = 10, 10 # Grid dimensions player_pos = [random.randint(0, height-1), random.randint(0, width-1)] items = generate_items(player_pos, height, width, 3) collected_count = 0 while True: stdscr.clear() display_grid(stdscr, height, width, player_pos, items, collected_count) key = stdscr.getch() # Handling player movement if key in [curses.KEY_UP, curses.KEY_DOWN, curses.KEY_LEFT, curses.KEY_RIGHT]: move_player(key, player_pos, height, width) # Check for item collection if player_pos in items: items.remove(player_pos[:]) collected_count += 1 # Check for game end if collected_count == 3: stdscr.addstr(height+1, 0, \\"Congratulations! You collected all items!\\") stdscr.refresh() stdscr.getch() break stdscr.refresh() def generate_items(player_pos, height, width, item_count): items = [] while len(items) < item_count: item_pos = [random.randint(0, height-1), random.randint(0, width-1)] if item_pos != player_pos and item_pos not in items: items.append(item_pos) return items def display_grid(stdscr, height, width, player_pos, items, collected_count): for y in range(height): for x in range(width): if [y, x] == player_pos: stdscr.addch(y, x, \'@\') elif [y, x] in items: stdscr.addch(y, x, \'*\') else: stdscr.addch(y, x, \'.\') # Display collected item count stdscr.addstr(height, 0, f\\"Items collected: {collected_count}/3\\") def move_player(key, player_pos, height, width): if key == curses.KEY_UP and player_pos[0] > 0: player_pos[0] -= 1 elif key == curses.KEY_DOWN and player_pos[0] < height-1: player_pos[0] += 1 elif key == curses.KEY_LEFT and player_pos[1] > 0: player_pos[1] -= 1 elif key == curses.KEY_RIGHT and player_pos[1] < width-1: player_pos[1] += 1 if __name__ == \'__main__\': curses.wrapper(main) ``` Your implementation should follow the structure provided and ensure it meets all the requirements specified. Good luck!","solution":"import curses import random def main(stdscr): # Initialize the game screen curses.curs_set(False) # Hide the cursor stdscr.clear() # Clear the screen height, width = 10, 10 # Grid dimensions player_pos = [random.randint(0, height-1), random.randint(0, width-1)] items = generate_items(player_pos, height, width, 3) collected_count = 0 while True: stdscr.clear() display_grid(stdscr, height, width, player_pos, items, collected_count) key = stdscr.getch() # Handle player movement if key in [curses.KEY_UP, curses.KEY_DOWN, curses.KEY_LEFT, curses.KEY_RIGHT]: move_player(key, player_pos, height, width) # Check for item collection if player_pos in items: items.remove(player_pos[:]) collected_count += 1 # Check for game end condition if collected_count == 3: stdscr.addstr(height + 1, 0, \\"Congratulations! You collected all items!\\") stdscr.refresh() stdscr.getch() break stdscr.refresh() def generate_items(player_pos, height, width, item_count): items = [] while len(items) < item_count: item_pos = [random.randint(0, height-1), random.randint(0, width-1)] if item_pos != player_pos and item_pos not in items: items.append(item_pos) return items def display_grid(stdscr, height, width, player_pos, items, collected_count): for y in range(height): for x in range(width): if [y, x] == player_pos: stdscr.addch(y, x, \'@\') elif [y, x] in items: stdscr.addch(y, x, \'*\') else: stdscr.addch(y, x, \'.\') # Display collected item count stdscr.addstr(height, 0, f\\"Items collected: {collected_count}/3\\") def move_player(key, player_pos, height, width): if key == curses.KEY_UP and player_pos[0] > 0: player_pos[0] -= 1 elif key == curses.KEY_DOWN and player_pos[0] < height-1: player_pos[0] += 1 elif key == curses.KEY_LEFT and player_pos[1] > 0: player_pos[1] -= 1 elif key == curses.KEY_RIGHT and player_pos[1] < width-1: player_pos[1] += 1 if __name__ == \'__main__\': curses.wrapper(main)"},{"question":"# Seaborn Visualization and Customization **Problem Statement:** You are tasked with creating a visual analytics report using Seaborn that includes various customization techniques to make the plots aesthetically pleasing and well-suited for different purposes (exploratory analysis, presentations, etc.). The dataset used in this project is the built-in `tips` dataset from Seaborn. **Objectives:** 1. Load the `tips` dataset from Seaborn. 2. Create a figure containing four subplots (2x2 grid). Each subplot should demonstrate different Seaborn themes (`darkgrid`, `whitegrid`, `dark`, `ticks`). 3. For each theme: - Plot a simple bar plot showing the average total bill for each day. - Remove the top and right spines using `despine`. 4. Temporarily switch to the `white` style and create a violin plot showing the distribution of `total_bill` for each `day`. 5. Incorporate another temporary style (`darkgrid`) within a `with` statement to create a box plot showing the `total_bill` distribution per `day`. 6. Apply the `talk` context to the entire figure for larger and clearer elements and set the font scale to 1.2 and line width to 2.0. 7. Save the final figure to an image file named `seaborn_visual_report.png`. **Requirements:** - You must demonstrate proficiency in using Seabornâ€™s style and context settings. - Each subplot must be properly titled and labeled. - Use a consistent color palette across all plots. - Ensure the final saved image file is neatly formatted and visually appealing. **Input:** - No external input is required as you will use the built-in `tips` dataset from Seaborn. **Output:** - A figure saved as `seaborn_visual_report.png` with all required subplots and customizations. **Code Template:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_visual_report(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a 2x2 grid of subplots fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # Define the themes to apply themes = [\'darkgrid\', \'whitegrid\', \'dark\', \'ticks\'] # Plot bar plots with different themes for ax, theme in zip(axes.flatten(), themes): sns.set_style(theme) sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=tips, ax=ax) ax.set_title(f\\"Theme: {theme}\\") sns.despine(ax=ax) # Temporarily set to \'white\' style and create a violin plot with sns.axes_style(\\"white\\"): fig, ax = plt.subplots() sns.violinplot(x=\\"day\\", y=\\"total_bill\\", data=tips, ax=ax) ax.set_title(\\"White Style Violin Plot\\") sns.despine(ax=ax) fig.savefig(\'violin_plot.png\') # Apply \'talk\' context to entire figure with specific font and line width sns.set_context(\\"talk\\", font_scale=1.2, rc={\\"lines.linewidth\\": 2.0}) # Final adjustments and save the complete figure fig.tight_layout() fig.savefig(\\"seaborn_visual_report.png\\") # Call the function to create and save the visual report create_seaborn_visual_report() ``` # Constraints: - Use only Seaborn and Matplotlib libraries. - Ensure each plot is correctly labeled and titled to clearly convey its respective theme. - Save the final figure using the specified name.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_visual_report(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a 2x2 grid of subplots fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # Define the themes to apply themes = [\'darkgrid\', \'whitegrid\', \'dark\', \'ticks\'] # Plot bar plots with different themes for ax, theme in zip(axes.flatten(), themes): sns.set_style(theme) sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=tips, ax=ax, palette=\\"deep\\") ax.set_title(f\\"Theme: {theme}\\") ax.set_xlabel(\'Day\') ax.set_ylabel(\'Average Total Bill\') sns.despine(ax=ax) # Apply the \'talk\' context with specific font and line width sns.set_context(\\"talk\\", font_scale=1.2, rc={\\"lines.linewidth\\": 2.0}) # Final adjustments and display the grid fig.tight_layout() # Save the complete figure fig.savefig(\\"seaborn_visual_report.png\\") plt.close(fig) # Temporarily set to \'white\' style and create a violin plot with sns.axes_style(\\"white\\"): fig, ax = plt.subplots(figsize=(8, 6)) sns.violinplot(x=\\"day\\", y=\\"total_bill\\", data=tips, ax=ax, palette=\\"deep\\") ax.set_title(\\"White Style Violin Plot\\") sns.despine(ax=ax) fig.savefig(\'violin_plot.png\') plt.close(fig)"},{"question":"**Question: Implement a Simple Inventory Management System** You have been tasked with creating an inventory management system for a small store. The system should be able to perform the following operations: 1. **Add a new item**: Items have a name, quantity, and price. 2. **Remove an item**: Remove an item based on its name. 3. **Update the quantity of an item**: By specifying the item\'s name and the amount to be added or removed. 4. **Search for an item**: Return the details of an item based on its name. 5. **List all items**: Display details of all items in the inventory. You need to implement the following functions in Python: 1. `add_item(inventory, name, quantity, price)`: Adds a new item to the inventory. 2. `remove_item(inventory, name)`: Removes an item from the inventory. 3. `update_quantity(inventory, name, quantity)`: Updates the quantity of an item. 4. `search_item(inventory, name)`: Searches for an item and returns its details. 5. `list_items(inventory)`: Lists all the items in the inventory. # Function Details - **add_item(inventory, name, quantity, price)** - **Input**: - `inventory`: Dictionary representing the inventory (`{name: {\\"quantity\\": x, \\"price\\": y}}`) - `name`: Name of the item to be added (string) - `quantity`: Quantity of the item (integer) - `price`: Price of the item (float) - **Output**: None - **Operation**: Adds a new item to the inventory. If the item already exists, do not add it again. - **remove_item(inventory, name)** - **Input**: - `inventory`: Dictionary representing the inventory - `name`: Name of the item to be removed (string) - **Output**: None - **Operation**: Removes an item from the inventory if it exists. If the item does not exist, do nothing. - **update_quantity(inventory, name, quantity)** - **Input**: - `inventory`: Dictionary representing the inventory - `name`: Name of the item to update (string) - `quantity`: Amount to add/remove from the current quantity (integer, can be positive or negative) - **Output**: None - **Operation**: Updates the quantity of the given item. If the item does not exist, do nothing. The quantity should not be negative. - **search_item(inventory, name)** - **Input**: - `inventory`: Dictionary representing the inventory - `name`: Name of the item to search (string) - **Output**: The item details as a dictionary `{\\"quantity\\": x, \\"price\\": y}` if the item exists, otherwise `None`. - **Operation**: Searches for an item and returns its details. - **list_items(inventory)** - **Input**: - `inventory`: Dictionary representing the inventory - **Output**: List of item details in the format `[{\\"name\\": name, \\"quantity\\": x, \\"price\\": y}, ...]`. - **Operation**: Lists all items in the inventory along with their details. # Constraints - Item names are case-insensitive. Treat \'Apple\' and \'apple\' as the same item. - Ensure quantity is always non-negative. - Utilize dictionaries and appropriate control flow constructs taught (if, for, etc.) **Example Usage** ```python inventory = {} add_item(inventory, \'apple\', 10, 0.5) add_item(inventory, \'banana\', 20, 0.2) print(search_item(inventory, \'apple\')) # Output: {\'quantity\': 10, \'price\': 0.5} update_quantity(inventory, \'apple\', 5) print(search_item(inventory, \'apple\')) # Output: {\'quantity\': 15, \'price\': 0.5} remove_item(inventory, \'banana\') print(list_items(inventory)) # Output: [{\'name\': \'apple\', \'quantity\': 15, \'price\': 0.5}] ``` **Note**: Ensure your code is well-documented and follows Python best practices.","solution":"def add_item(inventory, name, quantity, price): Adds a new item to the inventory. If the item already exists, do not add it again. Parameters: inventory (dict): Dictionary representing the inventory name (str): Name of the item to be added quantity (int): Quantity of the item price (float): Price of the item Returns: None item_name = name.lower() if item_name not in inventory: inventory[item_name] = {\\"quantity\\": quantity, \\"price\\": price} def remove_item(inventory, name): Removes an item from the inventory if it exists. Parameters: inventory (dict): Dictionary representing the inventory name (str): Name of the item to be removed Returns: None item_name = name.lower() inventory.pop(item_name, None) def update_quantity(inventory, name, quantity): Updates the quantity of the given item. The quantity should not be negative. Parameters: inventory (dict): Dictionary representing the inventory name (str): Name of the item to update quantity (int): Amount to add/remove from the current quantity Returns: None item_name = name.lower() if item_name in inventory: new_quantity = inventory[item_name][\\"quantity\\"] + quantity if new_quantity >= 0: inventory[item_name][\\"quantity\\"] = new_quantity def search_item(inventory, name): Searches for an item and returns its details. Parameters: inventory (dict): Dictionary representing the inventory name (str): Name of the item to search Returns: dict: The item details as a dictionary {\\"quantity\\": x, \\"price\\": y} if the item exists, otherwise None item_name = name.lower() return inventory.get(item_name, None) def list_items(inventory): Lists all items in the inventory along with their details. Parameters: inventory (dict): Dictionary representing the inventory Returns: list: List of item details in the format [{\\"name\\": name, \\"quantity\\": x, \\"price\\": y}, ...] return [{\\"name\\": name, \\"quantity\\": details[\\"quantity\\"], \\"price\\": details[\\"price\\"]} for name, details in inventory.items()]"},{"question":"# **Python Signal Handler Implementation** Objective: Demonstrate your understanding of the `signal` module in Python by implementing a program that handles multiple types of signals with appropriate handlers. Your program is expected to handle signals in a robust and non-blocking manner. Task: 1. Implement a signal handler function `handle_signal(signum, frame)` that correctly identifies and logs the signal received. The handler should handle at least the following signals: - `SIGINT`: Print \\"Interrupt received\\" and reset the handler. - `SIGALRM`: Print \\"Timer expired\\". - `SIGUSR1`: Print \\"User-defined signal 1 received\\". 2. Set up signal handlers for the above signals using the `signal.signal()` function. 3. Implement a `main()` function that sets an alarm to trigger an event after 10 seconds. 4. In the `main()` function, simulate catching a `SIGUSR1` signal from within the same process after 5 seconds. 5. Ensure that the program gracefully terminates upon receiving `SIGINT`. Constraints: - You must only use the `signal` module for signal handling. - All signal handling must be done on the main thread. - Ensure that your signal handler is capable of handling multiple signals without causing unexpected behavior or crashes. Example Input/Output: You are not required to handle specific input formats. The program behavior should be observable via the standard output. Notes: - Use the function `signal.alarm(10)` to trigger `SIGALRM` after 10 seconds. - Use `os.kill(os.getpid(), signal.SIGUSR1)` to send the `SIGUSR1` signal to the current process. Hints: - Use `signal.signal()` to set handlers for different signals. - Use `time.sleep()` in conjunction with `signal.pause()` to manage execution flow. ```python import signal import os import time # Define signal handler function def handle_signal(signum, frame): if signum == signal.SIGINT: print(\\"Interrupt received\\") signal.signal(signal.SIGINT, handle_signal) # Reset handler elif signum == signal.SIGALRM: print(\\"Timer expired\\") elif signum == signal.SIGUSR1: print(\\"User-defined signal 1 received\\") def main(): # Register signal handlers signal.signal(signal.SIGINT, handle_signal) signal.signal(signal.SIGALRM, handle_signal) signal.signal(signal.SIGUSR1, handle_signal) # Set an alarm for 10 seconds signal.alarm(10) # Simulate sending SIGUSR1 after 5 seconds time.sleep(5) os.kill(os.getpid(), signal.SIGUSR1) # Wait for signals try: while True: signal.pause() except KeyboardInterrupt: print(\\"Graceful termination\\") # Run the main function if __name__ == \\"__main__\\": main() ``` Expected Output: ```plaintext User-defined signal 1 received Timer expired Interrupt received Graceful termination ``` Ensure the program runs correctly, handling multiple signals as described. Validate by manually sending signals (e.g., pressing `Ctrl+C` for `SIGINT`).","solution":"import signal import os import time # Define signal handler function def handle_signal(signum, frame): if signum == signal.SIGINT: print(\\"Interrupt received\\") signal.signal(signal.SIGINT, handle_signal) # Reset handler elif signum == signal.SIGALRM: print(\\"Timer expired\\") elif signum == signal.SIGUSR1: print(\\"User-defined signal 1 received\\") def main(): # Register signal handlers signal.signal(signal.SIGINT, handle_signal) signal.signal(signal.SIGALRM, handle_signal) signal.signal(signal.SIGUSR1, handle_signal) # Set an alarm for 10 seconds signal.alarm(10) # Simulate sending SIGUSR1 after 5 seconds time.sleep(5) os.kill(os.getpid(), signal.SIGUSR1) # Wait for signals try: while True: signal.pause() except KeyboardInterrupt: print(\\"Graceful termination\\") # Run the main function if __name__ == \\"__main__\\": main()"},{"question":"Objective: You are required to demonstrate your understanding of closures and the interaction between nested functions in Python using cell objects. In this task, you will create a nested function structure and manipulate variables using cell-like behavior. Problem Statement: Create a class called `ScopedVariable` which simulates the behavior of cell objects in Python. This class should allow setting, getting, and updating a variable that can be accessed and modified by nested functions. # Specifications: 1. **Class**: `ScopedVariable` 2. **Methods**: - `__init__(self, initial_value)`: Initializes the scoped variable with an `initial_value`. - `get(self)`: Returns the current value of the scoped variable. - `set(self, new_value)`: Sets the scoped variable to `new_value`. - `add_to(self, increment)`: Adds `increment` to the current value of the scoped variable. # Example Usage: ```python class ScopedVariable: def __init__(self, initial_value): self.cell = initial_value def get(self): return self.cell def set(self, new_value): self.cell = new_value def add_to(self, increment): self.cell += increment # Example of usage in nested functions: def outer_function(): scoped_var = ScopedVariable(10) def inner_function1(): scoped_var.add_to(5) def inner_function2(): scoped_var.set(scoped_var.get() * 2) inner_function1() inner_function2() return scoped_var.get() result = outer_function() print(result) # Expected output: 30 ``` Constraints: - The initial value will always be an integer. - The `increment` in `add_to` will always be an integer. - The functions should handle positive and negative values for both setting and incrementing. Notes: 1. You are not allowed to use any libraries other than Python\'s standard library. 2. Focus on creating a class that accurately reflects the described behavior. Your task is to implement the `ScopedVariable` class and the specified methods.","solution":"class ScopedVariable: def __init__(self, initial_value): self.cell = initial_value def get(self): return self.cell def set(self, new_value): self.cell = new_value def add_to(self, increment): self.cell += increment"},{"question":"You are given a dataset related to brain networks, and your task is to visualize the trajectories of mean values across different timepoints for specified networks and hemispheres. The provided code structure is partly given, but you must complete and enhance it to meet the following requirements. Requirements: 1. Load the `brain_networks` dataset using Seaborn\'s `load_dataset` function. 2. Preprocess the dataset: - Set the column headers and index appropriately. - Stack the data and then group by \\"timepoint\\", \\"network\\", and \\"hemi\\". - Calculate the mean values and unstack based on the \\"network\\". - Reset the index and filter the dataset to include only timepoints less than 100. 3. Create a pair plot visualizing the trajectories of the networks through the given axes: - Plot `x` values as [5, 8, 12, 15]. - Plot `y` values as [6, 13, 16]. 4. Customize the plot: - Set the layout size to (8, 5). - Ensure that axes are shared. - Use the `so.Paths` function to plot the trajectories with custom properties: `linewidth` of 1 and `alpha` of 0.8. - Color the paths based on the \\"hemi\\" variable. 5. Display the resulting plot. Expected Input: The dataset is loaded within the script and does not require external input. Expected Output: A customized Seaborn plot displaying the trajectories of the specified networks. Constraints: - You may not change the dataset or its values manually. All preprocessing must be done programmatically. - The plot must adhere to the specified layout and customization requirements. Code Template: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Step 2: Create the plot with given specifications p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Step 3: Display the plot p.show() ``` Complete the above code template to achieve the desired visualization and successfully demonstrate your understanding of data preprocessing and visualization using Seaborn.","solution":"import seaborn.objects as so from seaborn import load_dataset # Step 1: Load and preprocess the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Step 2: Create the plot with given specifications p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Step 3: Display the plot p.show()"},{"question":"# URL Manipulation and Validation with `urllib.parse` **Objective:** Write a Python function to parse, manipulate, and validate URLs using the `urllib.parse` module. Your function should demonstrate your understanding of both fundamental and advanced concepts of URL parsing and construction. **Function Signature:** ```python from urllib.parse import urlparse, urlunparse, parse_qs, urlencode def manipulate_and_validate_urls(url: str) -> dict: Parse, manipulate, and validate the given URL and return a dictionary with the results. Parameters: - url (str): The URL to be parsed and manipulated. Returns: - dict: A dictionary containing the following keys and their corresponding values: - \'original_url\': The original URL. - \'parsed\': A dictionary of the parsed URL components. - \'updated_url\': The URL after updating its components. - \'is_valid\': A boolean indicating if the URL is valid based on custom logic. - \'query_params\': A dictionary of the query parameters (if any) parsed from the URL. - \'reconstructed_url\': The URL reconstructed from manipulated components. pass ``` **Requirements:** 1. **Parse the URL:** * Use `urlparse` to parse the URL into its components. Store these components in a dictionary with keys `\'scheme\', \'netloc\', \'path\', \'params\', \'query\', \'fragment\', \'username\', \'password\', \'hostname\', \'port\'`. 2. **Update URL Components:** * If the scheme is empty, set it to `\'http\'`. * If the path is empty, set it to `\'/\'`. * If the netloc is empty, set it to `\'localhost\'`. 3. **Validate the URL:** * Using custom logic, validate the URL. For simplicity, consider a URL valid if it has a non-empty scheme and netloc, and the netloc contains at least one period (\'.\'). Store the result in the `\'is_valid\'` key. 4. **Parse Query Parameters:** * If the URL has query parameters, use `parse_qs` to parse them into a dictionary and store it in the `\'query_params\'` key. 5. **Reconstruct the URL:** * Use `urlunparse` to reconstruct the URL from the updated components. 6. **Return Results:** * Return a dictionary with the following keys and corresponding values: - `\'original_url\'`: The original URL. - `\'parsed\'`: The dictionary of parsed URL components. - `\'updated_url\'`: The URL after updating its components. - `\'is_valid\'`: The boolean result of the validation. - `\'query_params\'`: The dictionary of parsed query parameters (if any). - `\'reconstructed_url\'`: The URL reconstructed from manipulated components. **Example Usage:** ```python url = \'http://example.com/test?query=123\' result = manipulate_and_validate_urls(url) print(result) ``` **Output:** ```python { \'original_url\': \'http://example.com/test?query=123\', \'parsed\': { \'scheme\': \'http\', \'netloc\': \'example.com\', \'path\': \'/test\', \'params\': \'\', \'query\': \'query=123\', \'fragment\': \'\', \'username\': None, \'password\': None, \'hostname\': \'example.com\', \'port\': None }, \'updated_url\': \'http://example.com/test?query=123\', \'is_valid\': True, \'query_params\': { \'query\': [\'123\'] }, \'reconstructed_url\': \'http://example.com/test?query=123\' } ``` **Constraints:** * The input URL is guaranteed to be a non-empty string. * The function should handle URLs with and without each potential component (scheme, netloc, path, params, query, fragment). **Performance Requirements:** * The function should be efficient and handle large URLs with many query parameters within a reasonable time.","solution":"from urllib.parse import urlparse, urlunparse, parse_qs, urlencode def manipulate_and_validate_urls(url: str) -> dict: Parse, manipulate, and validate the given URL and return a dictionary with the results. Parameters: - url (str): The URL to be parsed and manipulated. Returns: - dict: A dictionary containing the following keys and their corresponding values: - \'original_url\': The original URL. - \'parsed\': A dictionary of the parsed URL components. - \'updated_url\': The URL after updating its components. - \'is_valid\': A boolean indicating if the URL is valid based on custom logic. - \'query_params\': A dictionary of the query parameters (if any) parsed from the URL. - \'reconstructed_url\': The URL reconstructed from manipulated components. # Parse the URL parsed_url = urlparse(url) parsed = { \'scheme\': parsed_url.scheme, \'netloc\': parsed_url.netloc, \'path\': parsed_url.path, \'params\': parsed_url.params, \'query\': parsed_url.query, \'fragment\': parsed_url.fragment, \'username\': parsed_url.username, \'password\': parsed_url.password, \'hostname\': parsed_url.hostname, \'port\': parsed_url.port } # Update URL components updated_url_components = list(parsed_url) if not parsed_url.scheme: updated_url_components[0] = \'http\' if not parsed_url.path: updated_url_components[2] = \'/\' if not parsed_url.netloc: updated_url_components[1] = \'localhost\' updated_url = urlunparse(updated_url_components) # Validate the URL is_valid = bool(updated_url_components[0] and updated_url_components[1] and \'.\' in updated_url_components[1]) # Parse query parameters query_params = parse_qs(parsed_url.query) # Reconstruct the URL reconstructed_url = urlunparse(updated_url_components) return { \'original_url\': url, \'parsed\': parsed, \'updated_url\': updated_url, \'is_valid\': is_valid, \'query_params\': query_params, \'reconstructed_url\': reconstructed_url }"},{"question":"# Advanced Fraction Operations You are tasked with creating a custom fraction class that extends the functionality of Python\'s built-in `Fraction` class from the `fractions` module. Your class will handle various operations and provide additional functionalities while ensuring all operations maintain the properties of fractions. Requirements: 1. **Class Definition**: Define a class `AdvancedFraction` that inherits from `fractions.Fraction`. 2. **Initialization**: The class should be initialized using the same constructors as the `fractions.Fraction` class. 3. **Operations**: - Implement methods to perform addition, subtraction, multiplication, and division on `AdvancedFraction` objects. Override these methods to ensure they return `AdvancedFraction` objects and handle operations with integers and floats as well. 4. **Additional Methods**: - `to_decimal()`: A method to convert the fraction to its decimal representation. - `is_proper()`: A method to check if the fraction is a proper fraction (0 < fraction < 1). - `simplified()`: A method to return the fraction in its simplified form (lowest terms). Input and Output: 1. **Initialization**: - Can be created using integers, floats, or strings representing the fraction. ```python f1 = AdvancedFraction(3, 4) f2 = AdvancedFraction(0.5) f3 = AdvancedFraction(\\"2/5\\") ``` 2. **Operations**: - Addition: ```python result = f1 + f2 # Should be an AdvancedFraction instance ``` - Subtraction, multiplication, and division should follow a similar pattern. 3. **Additional Methods**: - `to_decimal()` should return a float: ```python decimal_value = f1.to_decimal() # Example: 0.75 ``` - `is_proper()` should return a boolean: ```python check = f3.is_proper() # True if 0 < fraction < 1, otherwise False ``` - `simplified()` should return a new `AdvancedFraction` in its simplest form: ```python simple_fraction = f3.simplified() # Simplified fraction ``` Constraints: - Ensure your class handles zero and negative values appropriately. - Ensure methods return accurate types and handle edge cases gracefully. - Your implementation should utilize the features of the `fractions.Fraction` class but expand upon them with the additional functionalities. Example: ```python from fractions import Fraction class AdvancedFraction(Fraction): def __add__(self, other): result = super().__add__(Fraction(other)) return AdvancedFraction(result) def __sub__(self, other): result = super().__sub__(Fraction(other)) return AdvancedFraction(result) def __mul__(self, other): result = super().__mul__(Fraction(other)) return AdvancedFraction(result) def __truediv__(self, other): result = super().__truediv__(Fraction(other)) return AdvancedFraction(result) def to_decimal(self): return float(self) def is_proper(self): return 0 < abs(self) < 1 def simplified(self): return AdvancedFraction(self.numerator, self.denominator) # Example usage f1 = AdvancedFraction(5, 10) f2 = AdvancedFraction(\\"1/5\\") print(f1 + f2) # AdvancedFraction(7, 10) print(f1.to_decimal()) # 0.5 print(f2.is_proper()) # True print(f1.simplified()) # AdvancedFraction(1, 2) ``` Implement the `AdvancedFraction` class to meet the above specifications.","solution":"from fractions import Fraction class AdvancedFraction(Fraction): def __add__(self, other): result = super().__add__(Fraction(other)) return AdvancedFraction(result) def __sub__(self, other): result = super().__sub__(Fraction(other)) return AdvancedFraction(result) def __mul__(self, other): result = super().__mul__(Fraction(other)) return AdvancedFraction(result) def __truediv__(self, other): result = super().__truediv__(Fraction(other)) return AdvancedFraction(result) def to_decimal(self): return float(self) def is_proper(self): return 0 < abs(self) < 1 def simplified(self): return AdvancedFraction(self.numerator, self.denominator) # Example usage f1 = AdvancedFraction(5, 10) f2 = AdvancedFraction(\\"1/5\\") print(f1 + f2) # AdvancedFraction(7, 10) print(f1.to_decimal()) # 0.5 print(f2.is_proper()) # True print(f1.simplified()) # AdvancedFraction(1, 2)"},{"question":"# Question: Accessing and Managing Annotations in Python You are tasked with writing a function that accesses and modifies annotations on different Python objects. Your function should be compatible with both Python 3.10+ and Python versions 3.9 and older. The function should achieve the following: 1. **Access Annotations**: - Given an object `o`, safely access its `__annotations__` attribute. - If `inspect.get_annotations()` is available (Python 3.10+), use it to retrieve the annotations. - If not, use a combination of `getattr()` and `__dict__` to access `__annotations__` safely for functions, classes, and modules as per the version-specific guidelines. 2. **Modify Annotations**: - Update or add an annotation for a specified attribute/key. - Ensure the `__annotations__` remains a dictionary. 3. **Un-stringize Annotations**: - If any annotations are stringized, convert them back to their corresponding Python objects if it\'s viable. - If `inspect.get_annotations()` is available, use it to handle un-stringizing. 4. **Respect Python Quirks and Best Practices**: - Avoid common pitfalls such as modifying `__annotations__` directly without checks, deleting `__annotations__`, or incorrectly handling inheritance issues in pre-3.10 versions. Your function should have the following signature: ```python import inspect import sys def manage_annotations(o, key, new_annotation): Safely access and modify annotations of the object o. Parameters: o : any Python object that can have annotations (functions, classes, modules) key : str, the key for the annotation to update or add new_annotation : any, the new annotation value to set for the key Returns: None. Modifies the `__annotations__` of object o in place. # Your code here ``` # Input and Output **Input:** - `o`: A Python object which could be a function, class, or module. - `key`: A string representing the annotation key to update or add. - `new_annotation`: The new annotation value to set. **Output:** - The function returns `None` but modifies the `__annotations__` of the object `o` in place. # Constraints: - The function should handle both Python 3.10+ and Python 3.9 and older versions. - Ensure modifications to `__annotations__` do not break its integrity. - Perform type checks and handle exceptions where necessary. **Example Usage:** ```python class Example: x: int y: str # Modify annotation manage_annotations(Example, \'z\', \'float\') # Verify modification print(Example.__annotations__) # Output should include {\'x\': \'int\', \'y\': \'str\', \'z\': \'float\'} ``` # Notes: - You should test the function on different object types (function, class, module) to ensure robustness. - Consider edge cases such as objects with no initial annotations, inherited annotations in older Python versions, and handling of stringized annotations.","solution":"import inspect import sys def manage_annotations(o, key, new_annotation): Safely access and modify annotations of the object o. Parameters: o : any Python object that can have annotations (functions, classes, modules) key : str, the key for the annotation to update or add new_annotation : any, the new annotation value to set for the key Returns: None. Modifies the `__annotations__` of object o in place. # Function to resolve stringized annotations def _resolve_annotations(annotations, globalns=None, localns=None): if isinstance(annotations, dict): return annotations if isinstance(annotations, str): return eval(annotations, globalns, localns) return annotations # Access annotations in Python 3.10+ if possible if hasattr(inspect, \'get_annotations\'): try: annotations = inspect.get_annotations(o, eval_str=True) except Exception: annotations = getattr(o, \'__annotations__\', {}) else: # For Python < 3.10 annotations = getattr(o, \'__annotations__\', {}) if not isinstance(annotations, dict): annotations = {} # Modify the annotations dictionary annotations[key] = new_annotation # Update the object\'s __annotations__ attribute o.__annotations__ = annotations"},{"question":"# Question: Creating and Customizing Violin Plots with Seaborn You are given a dataset containing information on passengers of the Titanic, such as their age, fare, class, survival status, and deck. Your task is to use this dataset to create and customize violin plots by implementing the function `custom_violin_plots`. Function Signature ```python def custom_violin_plots(data: pd.DataFrame) -> List[plt.Figure]: pass ``` Input - `data (pd.DataFrame)`: A pandas DataFrame containing the Titanic dataset. Output - `List[plt.Figure]`: A list of matplotlib figures, each containing a customized violin plot. Constraints - The DataFrame `data` will contain the following columns: - `age` (float): Age of the passenger. - `fare` (float): Fare paid by the passenger. - `class` (str): Ticket class of the passenger (`First`, `Second`, `Third`). - `alive` (str): Survival status of the passenger (`yes`, `no`). - `deck` (str): Deck assignment of the passenger. Requirements 1. **Basic Violin Plot**: Create a basic violin plot of `age`. 2. **Grouped Violin Plot**: Create a grouped violin plot of `age` versus `class`, with `hue` set to `alive`. 3. **Split Violin Plot**: Create a split violin plot of `age` versus `class`, with `hue` set to `alive`, and `split=True`. 4. **Normalized Violin Plot**: Create a violin plot of `fare` versus `deck`, with `inner=\\"point\\"`, and `density_norm=\\"count\\"`. 5. **Customized Violin Plot**: Create a violin plot of `age` versus `alive`, with `cut=0`, `inner=\\"stick\\"`, and pass customization options for the `width` and `color` parameters of the `inner` representation. Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset from seaborn df = sns.load_dataset(\\"titanic\\") # Call the function figures = custom_violin_plots(df) # Display the figures for fig in figures: plt.show(fig) ``` This question tests your understanding of seaborn\'s violin plots and your ability to customize them for different data visualization requirements. **Note**: You should not include display or I/O operations within the function you implement.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from typing import List def custom_violin_plots(data: pd.DataFrame) -> List[plt.Figure]: figures = [] # Basic Violin Plot fig1, ax1 = plt.subplots() sns.violinplot(x=data[\'age\'], ax=ax1) figures.append(fig1) # Grouped Violin Plot fig2, ax2 = plt.subplots() sns.violinplot(x=\'class\', y=\'age\', hue=\'alive\', data=data, ax=ax2) figures.append(fig2) # Split Violin Plot fig3, ax3 = plt.subplots() sns.violinplot(x=\'class\', y=\'age\', hue=\'alive\', data=data, split=True, ax=ax3) figures.append(fig3) # Normalized Violin Plot fig4, ax4 = plt.subplots() sns.violinplot(x=\'deck\', y=\'fare\', data=data, inner=\'point\', scale=\'count\', ax=ax4) figures.append(fig4) # Customized Violin Plot fig5, ax5 = plt.subplots() sns.violinplot(x=\'alive\', y=\'age\', data=data, cut=0, inner=\'stick\', ax=ax5) figures.append(fig5) return figures"},{"question":"# Advanced Coding Assessment Objective You are required to implement a generic class using Python\'s `typing` module and create a corresponding set of unit tests using the `unittest` module to ensure your implementation works correctly under various conditions. Problem Statement 1. **Implement a Generic Class** Create a generic class `Cache` in Python that stores key-value pairs. The `Cache` class should be able to: - Add a new key-value pair. - Retrieve the value associated with a given key. - Remove a key (and its associated value) from the cache. - Clear all key-value pairs from the cache. - Retrieve the current number of items in the cache. Use the `typing` module to ensure type hints are accurately specifying the types, including the use of generics. 2. **Unit Tests** Write a series of unit tests using the `unittest` module to verify the following functionalities of the `Cache` class: - Adding and retrieving key-value pairs. - Removing key-value pairs. - Clearing all key-value pairs. - Handling non-existent keys (both for retrieval and removal). Implementation Details 1. **Cache Class** You must define the class as follows: ```python from typing import TypeVar, Generic, Dict, Optional K = TypeVar(\'K\') V = TypeVar(\'V\') class Cache(Generic[K, V]): def __init__(self) -> None: self._cache: Dict[K, V] = {} def add(self, key: K, value: V) -> None: self._cache[key] = value def get(self, key: K) -> Optional[V]: return self._cache.get(key) def remove(self, key: K) -> None: if key in self._cache: del self._cache[key] def clear(self) -> None: self._cache.clear() def size(self) -> int: return len(self._cache) ``` 2. **Unit Tests** You must write unit tests to check the following: - `test_add_and_get`: Test adding key-value pairs and retrieving them. - `test_remove`: Test removing a key-value pair. - `test_clear`: Test clearing all key-value pairs. - `test_size`: Test retrieving the current size of the cache. - `test_non_existent_key`: Test the behavior when retrieving or removing a key that doesn\'t exist. Example implementation of the `TestCache` class: ```python import unittest class TestCache(unittest.TestCase): def setUp(self): self.cache = Cache[int, str]() def test_add_and_get(self): self.cache.add(1, \\"one\\") self.cache.add(2, \\"two\\") self.assertEqual(self.cache.get(1), \\"one\\") self.assertEqual(self.cache.get(2), \\"two\\") def test_remove(self): self.cache.add(1, \\"one\\") self.cache.remove(1) self.assertIsNone(self.cache.get(1)) def test_clear(self): self.cache.add(1, \\"one\\") self.cache.add(2, \\"two\\") self.cache.clear() self.assertEqual(self.cache.size(), 0) def test_size(self): self.cache.add(1, \\"one\\") self.cache.add(2, \\"two\\") self.assertEqual(self.cache.size(), 2) def test_non_existent_key(self): self.assertIsNone(self.cache.get(100)) self.cache.remove(100) # Should not raise any exception if __name__ == \'__main__\': unittest.main() ``` Constraints and Requirements - Use the `typing` module to enforce type hints and generics in the `Cache` class. - Ensure your `Cache` class and the test cases handle all the described functionalities. - Pay attention to edge cases, such as handling non-existent keys gracefully. - Aim for clean, readable, and well-documented code. Submission Submit your implemented `Cache` class and the `TestCache` unit test class.","solution":"from typing import TypeVar, Generic, Dict, Optional K = TypeVar(\'K\') V = TypeVar(\'V\') class Cache(Generic[K, V]): def __init__(self) -> None: self._cache: Dict[K, V] = {} def add(self, key: K, value: V) -> None: self._cache[key] = value def get(self, key: K) -> Optional[V]: return self._cache.get(key) def remove(self, key: K) -> None: if key in self._cache: del self._cache[key] def clear(self) -> None: self._cache.clear() def size(self) -> int: return len(self._cache)"},{"question":"Title: Data Preprocessing and Random Sampling with Scikit-learn Utilities Objective: To validate your understanding of scikit-learn\'s utility functions by combining data validation, preprocessing, and random sampling in a coherent solution. Problem Statement: Write a function `process_and_sample_data` that performs the following tasks: 1. **Data Validation and Conversion**: - Check if the input data `X` is a valid 2D array and does not contain NaNs or Infs using the appropriate scikit-learn validation utilities. - Convert `X` to a floating-point array (of type `float64`). 2. **Random Sampling**: - From the validated data `X`, randomly sample `n_samples` rows without replacement. Ensure reproducibility by using an integer `random_state`. 3. **Normalization**: - Normalize the sampled data such that each row has a unit L2 norm. Use relevant scikit-learn utilities designed for sparse matrices (assuming `X` could be sparse). Function Signature: ```python import numpy as np from scipy import sparse from sklearn.utils import check_array, as_float_array, check_random_state, resample from sklearn.utils.sparsefuncs_fast import inplace_csr_row_normalize_l2 from sklearn.exceptions import NotFittedError def process_and_sample_data(X, n_samples, random_state=None): Parameters: X (np.ndarray or scipy.sparse matrix): Input data, expected to be a 2D array. n_samples (int): The number of samples to be drawn from X. random_state (int, RandomState instance or None): Seed or random number generator for reproducibility. Returns: np.ndarray or scipy.sparse matrix: The processed and sampled data with unit L2 norm. # Your code here return sampled_normalized_data ``` Constraints: - `X` can be either a NumPy array or a SciPy sparse matrix. - The number of samples `n_samples` must be less than or equal to the number of rows in `X`. - If `X` is a sparse matrix, the output should also be a sparse matrix. Example: ```python from scipy.sparse import csr_matrix # Example data X = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]]) sparse_X = csr_matrix(X) # Sample and normalize result = process_and_sample_data(sparse_X, 2, random_state=42) print(result) # Output should be a sparse matrix with unit L2 norm for each row. # For dense input X_dense = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]]) result_dense = process_and_sample_data(X_dense, 2, random_state=42) print(result_dense) # Output should be a numpy array with unit L2 norm for each row. ``` Requirements: - Use only the provided scikit-learn utilities to implement the solution. - Include error handling to ensure the input data conforms to the expected format and constraints. Hint: Consider using the following utilities: - `check_array` - `as_float_array` - `check_random_state` - `resample` - `inplace_csr_row_normalize_l2`","solution":"import numpy as np from scipy import sparse from sklearn.utils import check_array, as_float_array, check_random_state, resample from sklearn.utils.sparsefuncs_fast import inplace_csr_row_normalize_l2 from sklearn.exceptions import NotFittedError def process_and_sample_data(X, n_samples, random_state=None): Parameters: X (np.ndarray or scipy.sparse matrix): Input data, expected to be a 2D array. n_samples (int): The number of samples to be drawn from X. random_state (int, RandomState instance or None): Seed or random number generator for reproducibility. Returns: np.ndarray or scipy.sparse matrix: The processed and sampled data with unit L2 norm. # Data Validation and Conversion X = check_array(X, accept_sparse=[\'csr\', \'csc\'], ensure_2d=True, dtype=\'float64\') if sparse.issparse(X): X = X.tocsr() # Ensure CSR format for normalization random_state = check_random_state(random_state) if X.shape[0] < n_samples: raise ValueError(\\"n_samples must be less than or equal to the number of rows in X\\") # Random Sampling indices = random_state.choice(X.shape[0], n_samples, replace=False) sampled_data = X[indices] # Normalization if sparse.issparse(sampled_data): inplace_csr_row_normalize_l2(sampled_data) sampled_normalized_data = sampled_data else: norms = np.linalg.norm(sampled_data, axis=1, keepdims=True) sampled_normalized_data = sampled_data / norms return sampled_normalized_data"},{"question":"**Objective**: Demonstrate understanding of HTML manipulation in Python using the `html` module. # Problem Statement You are tasked with creating a function that takes a list of strings and processes each string to make it HTML-safe and then revert it back to its original form. This will involve using the `html.escape` and `html.unescape` functions. Function Signature ```python def process_html_strings(strings: list[str], quote: bool = True) -> list[str]: pass ``` Input - `strings`: A list of strings. Each string may contain special characters that need to be converted to their HTML-safe sequences. - `quote`: A boolean (default: `True`). If `True`, the characters `\\"` and `\'` will also be converted to their HTML-safe sequences when escaping. Output - Returns a list of strings, where each string has been processed to be HTML-safe using `html.escape` and then reverted back to its original form using `html.unescape`. Constraints - You may assume that the input list contains at least one string. - Each string in the list will have at most 1000 characters. - The function should be capable of processing up to 1000 strings efficiently. Example ```python # Example input strings = [\'<div class=\\"example\\">O&apos;Reilly & Associates</div>\', \'Use the \\"quote\\" attribute\'] # Example function call result = process_html_strings(strings) # Example output # The output list should be identical to the input list as each string should be escaped and then unescaped back to its original form print(result) # Output: [\'<div class=\\"example\\">O&apos;Reilly & Associates</div>\', \'Use the \\"quote\\" attribute\'] ``` # Notes - You should use the `html.escape` function to make the string HTML-safe. - You should use the `html.unescape` function to convert the HTML-safe sequences back to their original characters. - Ensure that the output list has strings in the same order as the input list, but processed as described. Write your implementation of the `process_html_strings` function below.","solution":"import html def process_html_strings(strings: list[str], quote: bool = True) -> list[str]: Processes a list of strings to make them HTML-safe and then reverts them to their original form. Parameters: - strings (list[str]): A list of strings to be processed. - quote (bool): If True, the characters \'\\"\' and \'\'\' are also converted to their HTML-safe sequences. Returns: - list[str]: A list of processed strings. processed_strings = [] for s in strings: escaped = html.escape(s, quote=quote) unescaped = html.unescape(escaped) processed_strings.append(unescaped) return processed_strings"},{"question":"# Coding Assessment: Seaborn Plot Customization **Objective**: Write a program to create a customized seaborn plot using the `seaborn.objects` module. **Dataset**: Use the `tips` dataset provided by seaborn. **Requirements**: 1. Load the `tips` dataset. 2. Create a scatter plot using `so.Dot` with the following specifications: - X-axis: `total_bill` - Y-axis: `tip` 3. Customize the plot to differentiate between days (`day` column) using colors. 4. Add a white edge to the dots for better visualization. 5. Use dodging and jittering to reduce overplotting. 6. Modify the size of the points to 4. 7. Add error bars showing the standard error (SE) of the mean tips for each day. **Function Signature**: ```python import seaborn.objects as so from seaborn import load_dataset def create_custom_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"day\\") .add(so.Dot(edgecolor=\\"w\\", pointsize=4), so.Dodge(), so.Jitter(.2)) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) return p ``` **Expected Output**: 1. A scatter plot visualizing `total_bill` vs `tip`, with different colors for each day. 2. Points should have white edges and reduced overplotting due to dodging and jittering. 3. Point sizes should be set to 4, and error bars displaying the SE of mean tips for each day should be present. **Constraints**: - Ensure the code runs efficiently with the provided dataset. - Make sure to handle any exceptions that may arise during the dataset loading or plotting process. **Note**: - You do not need to display the plot in the function; just ensure that the plot object is created and configured correctly according to the specifications.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_plot(): Creates a customized seaborn plot using the \'tips\' dataset. # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"day\\") .add(so.Dot(edgecolor=\\"w\\", pointsize=4), so.Dodge(), so.Jitter(.2)) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) return p"},{"question":"**Challenging Coding Assessment Question:** # Problem Statement: You are tasked with creating a utility for handling various date and time manipulations that are common in scheduling applications. Specifically, you need to write a class that handles event scheduling across different time zones, provides time differences, and formats dates and times. # Requirements: 1. **Class Name**: `EventScheduler`. 2. **Methods**: - `__init__(self, event_time: str, timezone_str: str)`: Initializes the event with the given event time in \\"YYYY-MM-DDTHH:MM:SS\\" format and the time zone. - `convert_timezone(self, new_timezone_str: str) -> None`: Converts the event\'s time to the specified new timezone. - `time_until_event(self) -> datetime.timedelta`: Returns the time remaining until the event from the current moment in the system\'s local time zone. - `format_event_time(self, format_str: str) -> str`: Returns the event\'s time formatted according to the given format string. 3. **Expected Input and Output Formats**: - `event_time` is a string formatted as \\"YYYY-MM-DDTHH:MM:SS\\". - `timezone_str` and `new_timezone_str` are IANA timezone strings like \\"UTC\\", \\"America/New_York\\", \\"Europe/London\\", etc. - The format string should follow the directives supported by the `strftime` method. # Constraints: - You can assume that the provided time zone strings are valid. - The event time given will be in a valid format. - You may use the `zoneinfo` module for handling IANA time zones. # Example Usage: ```python from typing import Optional from datetime import datetime, timedelta from zoneinfo import ZoneInfo class EventScheduler: def __init__(self, event_time: str, timezone_str: str): self.event_time = datetime.fromisoformat(event_time) self.event_timezone = ZoneInfo(timezone_str) self.event_time = self.event_time.replace(tzinfo=self.event_timezone) def convert_timezone(self, new_timezone_str: str) -> None: new_timezone = ZoneInfo(new_timezone_str) self.event_time = self.event_time.astimezone(new_timezone) def time_until_event(self) -> timedelta: now = datetime.now().astimezone(self.event_time.tzinfo) return self.event_time - now def format_event_time(self, format_str: str) -> str: return self.event_time.strftime(format_str) # Example: scheduler = EventScheduler(\\"2023-10-15T14:00:00\\", \\"America/New_York\\") print(scheduler.format_event_time(\\"%Y-%m-%d %H:%M:%S %Z\\")) # 2023-10-15 14:00:00 EDT scheduler.convert_timezone(\\"Europe/London\\") print(scheduler.format_event_time(\\"%Y-%m-%d %H:%M:%S %Z\\")) # 2023-10-15 19:00:00 BST print(scheduler.time_until_event()) # Output will vary ``` # Constraints: 1. Ensure that the differences in time zones are accurately handled. 2. The code is expected to work with Python 3.9+ where `zoneinfo` is available. 3. Handle edge cases where time conversion might involve daylight saving transitions.","solution":"from typing import Optional from datetime import datetime, timedelta from zoneinfo import ZoneInfo class EventScheduler: def __init__(self, event_time: str, timezone_str: str): self.event_time = datetime.fromisoformat(event_time) self.event_timezone = ZoneInfo(timezone_str) self.event_time = self.event_time.replace(tzinfo=self.event_timezone) def convert_timezone(self, new_timezone_str: str) -> None: new_timezone = ZoneInfo(new_timezone_str) self.event_time = self.event_time.astimezone(new_timezone) def time_until_event(self) -> timedelta: now = datetime.now().astimezone(self.event_time.tzinfo) return self.event_time - now def format_event_time(self, format_str: str) -> str: return self.event_time.strftime(format_str)"},{"question":"**Understanding and Implementing a Profiling System using `cProfile` and `pdb`:** You are required to implement a function that profiles and debugs another function. You should use the `cProfile` module to profile the execution time and the `pdb` module to debug any issues in the provided function. Your task is to implement two separate functions: `profile_function` and `debug_function`. 1. **Function: `profile_function(func, *args, **kwargs)`** - **Input:** - `func`: A Python function to be profiled. - `*args`: Positional arguments for the `func`. - `**kwargs`: Keyword arguments for the `func`. - **Output:** - Returns the profiling stats of the function in string format. - **Constraints:** - The provided function may be any valid Python function with varying arguments. - Ensure that the profile output is understandable and concise. 2. **Function: `debug_function(func, *args, **kwargs)`** - **Input:** - `func`: A Python function to be debugged. - `*args`: Positional arguments for the `func`. - `**kwargs`: Keyword arguments for the `func`. - **Output:** - Runs the given function in `pdb`\'s debugging mode. - **Constraints:** - The provided function may run into errors; as part of the debugging, you should be able to step through the function execution using `pdb`. # Example Usage: ```python import example_module def example_func(x, y): result = 0 for i in range(x): result += (i * y) return result # Profiling the function profile_stats = profile_function(example_func, 1000, 10) print(profile_stats) # Debugging the function debug_function(example_func, 1000, 10) ``` # Notes: 1. To profile a function, you may use `cProfile` and convert the output to a readable string format using `pstats` module. 2. To debug the function, you should invoke `pdb` within the function and allow stepping through each line. 3. You do not need to implement the `example_func` function, as it serves only as an illustration. **Hints:** - Refer to the `cProfile` documentation for profiling functions. - Use the `pdb.set_trace()` method to add breakpoints in the function and start the debugging process.","solution":"import cProfile import pstats import io import pdb def profile_function(func, *args, **kwargs): Profiles the given function with provided arguments. Parameters: func (function): The function to be profiled. *args: Positional arguments to the function. **kwargs: Keyword arguments to the function. Returns: str: Profiling stats of the function execution in string format. pr = cProfile.Profile() pr.enable() func(*args, **kwargs) pr.disable() s = io.StringIO() sortby = pstats.SortKey.CUMULATIVE ps = pstats.Stats(pr, stream=s).sort_stats(sortby) ps.print_stats() return s.getvalue() def debug_function(func, *args, **kwargs): Debugs the given function with provided arguments using pdb. Parameters: func (function): The function to be debugged. *args: Positional arguments to the function. **kwargs: Keyword arguments to the function. pdb.set_trace() func(*args, **kwargs)"},{"question":"# Secure Token and Password Generator Objective: Create a Python program that generates a secure password and a temporary URL containing a cryptographic token for account recovery. Requirements: 1. Implement the following functions using the `secrets` module: - `generate_password(length: int) -> str`: Generate a password of given length that includes at least one uppercase letter, one lowercase letter, one digit, and one special character from the set `!@#%^&*()`. - `generate_secure_url() -> str`: Generate a secure URL containing a base64 encoded token suitable for password recovery. The token should contain 32 bytes of randomness. 2. Ensure your generated password meets the following criteria: - Length should be at least 12 characters. - Includes at least one uppercase letter, one lowercase letter, one digit, and one special character. 3. The secure URL should be in the format: - `https://secure.example.com/recover?token=<GENERATED_TOKEN>` Example Usage: ```python # Example of function usage password = generate_password(12) print(\\"Generated Password:\\", password) # Output: Generated Password: A8m!kdL3Pq@ secure_url = generate_secure_url() print(\\"Recovery URL:\\", secure_url) # Output: Recovery URL: https://secure.example.com/recover?token=Jh8xNH9xKp9EcN_4DNhj4_AEkPiG6SDU ``` Constraints: - Use the `secrets` module for all random value generations. - Do not use other random or pseudo-random number generators. - Ensure the generated password follows the specified constraints in all test cases. Implementation Notes: - Utilize the `secrets.choice`, `secrets.randbelow`, and other relevant functions in the `secrets` module. - Generate and validate the password using multiple attempts if necessary to ensure it meets all criteria. - Use the `secrets.token_urlsafe` function for generating the token. Performance: - The functions should execute efficiently within reasonable time limits for common inputs. - Edge cases such as minimum and maximum length of passwords should be handled within performance constraints.","solution":"import secrets import string def generate_password(length: int) -> str: Generate a secure password of the given length. The password will include at least one uppercase letter, one lowercase letter, one digit, and one special character. if length < 12: raise ValueError(\\"Password length must be at least 12 characters.\\") alphabet = string.ascii_letters + string.digits + \\"!@#%^&*()\\" while True: password = \'\'.join(secrets.choice(alphabet) for _ in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password) and any(c in \\"!@#%^&*()\\" for c in password)): return password def generate_secure_url() -> str: Generate a secure URL containing a base64 encoded token suitable for password recovery. The token contains 32 bytes of randomness. token = secrets.token_urlsafe(32) return f\\"https://secure.example.com/recover?token={token}\\""},{"question":"**Question: Advanced Platform Data Aggregator** You are tasked with writing a Python function that aggregates various pieces of platform-related information using the `platform` module. The function should return a summary dictionary containing the following keys and their corresponding values: 1. `python_version`: The version of the current Python interpreter (use `platform.python_version()`). 2. `python_compiler`: The compiler used to build the Python interpreter (use `platform.python_compiler()`). 3. `architecture`: A tuple containing the bit architecture and linkage format of the current Python interpreter (use `platform.architecture()`). 4. `os_system`: The name of the operating system (use `platform.system()`). 5. `os_release`: The release name of the operating system (use `platform.release()`). 6. `machine_type`: The machine type (use `platform.machine()`). 7. `processor`: The real processor name (use `platform.processor()`). 8. `network_node`: The network name of the computer (use `platform.node()`). The function should be named `get_platform_summary` and should have no parameters. **Example Output:** ```python { \\"python_version\\": \\"3.10.0\\", \\"python_compiler\\": \\"GCC 8.3.0\\", \\"architecture\\": (\\"64bit\\", \\"\\"), \\"os_system\\": \\"Linux\\", \\"os_release\\": \\"5.4.0-74-generic\\", \\"machine_type\\": \\"x86_64\\", \\"processor\\": \\"x86_64\\", \\"network_node\\": \\"hostname\\" } ``` **Notes:** - Each value should be retrieved using the respective `platform` function. - If a value cannot be determined, use an empty string (`\\"\\"`) as the default value. ```python import platform def get_platform_summary(): # Initialize summary dictionary summary = {} # Gather platform-related data summary[\'python_version\'] = platform.python_version() summary[\'python_compiler\'] = platform.python_compiler() summary[\'architecture\'] = platform.architecture() summary[\'os_system\'] = platform.system() summary[\'os_release\'] = platform.release() summary[\'machine_type\'] = platform.machine() summary[\'processor\'] = platform.processor() summary[\'network_node\'] = platform.node() return summary # Example usage: if __name__ == \\"__main__\\": summary = get_platform_summary() print(summary) ``` Implement the function `get_platform_summary` and test it on your local machine. Ensure that all the expected keys are present in the output dictionary, and handle cases where information might not be available by using empty strings as default values.","solution":"import platform def get_platform_summary(): Returns a summary dictionary containing various pieces of platform-related information. # Initialize summary dictionary summary = {} # Gather platform-related data summary[\'python_version\'] = platform.python_version() if platform.python_version() else \\"\\" summary[\'python_compiler\'] = platform.python_compiler() if platform.python_compiler() else \\"\\" summary[\'architecture\'] = platform.architecture() if platform.architecture() else (\\"\\", \\"\\") summary[\'os_system\'] = platform.system() if platform.system() else \\"\\" summary[\'os_release\'] = platform.release() if platform.release() else \\"\\" summary[\'machine_type\'] = platform.machine() if platform.machine() else \\"\\" summary[\'processor\'] = platform.processor() if platform.processor() else \\"\\" summary[\'network_node\'] = platform.node() if platform.node() else \\"\\" return summary # Example usage: if __name__ == \\"__main__\\": summary = get_platform_summary() print(summary)"},{"question":"Problem Statement Implement a Python function that processes a list of dictionaries representing records of students. Each student record contains the following keys: - `name` (string): name of the student - `matric_number` (string): student\'s matriculation number - `grades` (list of integers): list of grades the student has received Your task is to implement a function `process_student_records(records: list) -> dict` that processes this list of student records and returns a dictionary with the following summary: - `average_grade` (float): The average grade across all students. - `top_student` (str): The name of the student with the highest average grade. - `failed_students` (list of str): List of names of students who have failed in any subject (a grade below 40 is considered a fail). Additionally, each student record should be updated to include a new key: - `status` (str): `\\"PASSED\\"` if the student has passed all subjects, otherwise `\\"FAILED\\"`. Constraints: - The `grades` list for each student will have at least one grade. - `records` list will have at least one student. Performance Requirements: - The implementation should efficiently handle lists with up to 10,000 student records. Python Features to be Used: - Control flow statements (`if`, `for`, `else` clauses on loops) - Function definitions with various parameter types - `match` statements where feasible - Comprehensive docstrings and adherence to Pythonic coding style (PEP 8) Example: ```python records = [ { \\"name\\": \\"Alice\\", \\"matric_number\\": \\"S1234\\", \\"grades\\": [80, 95, 78] }, { \\"name\\": \\"Bob\\", \\"matric_number\\": \\"S5678\\", \\"grades\\": [50, 60, 40] }, { \\"name\\": \\"Charlie\\", \\"matric_number\\": \\"S9101\\", \\"grades\\": [30, 35, 20] } ] result = process_student_records(records) print(result) ``` Expected Output: ```python { \\"average_grade\\": 57.44444444444444, \\"top_student\\": \\"Alice\\", \\"failed_students\\": [\\"Charlie\\"], \\"updated_records\\": [ { \\"name\\": \\"Alice\\", \\"matric_number\\": \\"S1234\\", \\"grades\\": [80, 95, 78], \\"status\\": \\"PASSED\\" }, { \\"name\\": \\"Bob\\", \\"matric_number\\": \\"S5678\\", \\"grades\\": [50, 60, 40], \\"status\\": \\"PASSED\\" }, { \\"name\\": \\"Charlie\\", \\"matric_number\\": \\"S9101\\", \\"grades\\": [30, 35, 20], \\"status\\": \\"FAILED\\" } ] } ``` Solution Template: ```python def process_student_records(records: list) -> dict: Process a list of student records to summarize the average grade, top student, and failed students. Update each student record with their pass/fail status. Arguments: records -- list of dictionaries containing student information Returns: A dictionary summarizing the student records and including updated records. # Your code here return { \\"average_grade\\": 0.0, \\"top_student\\": \\"\\", \\"failed_students\\": [], \\"updated_records\\": [] } ```","solution":"def process_student_records(records: list) -> dict: Process a list of student records to summarize the average grade, top student, and failed students. Update each student record with their pass/fail status. Arguments: records -- list of dictionaries containing student information Returns: A dictionary summarizing the student records and including updated records. total_grades = 0 total_grade_count = 0 top_student = None top_average = -1 failed_students = [] for student in records: grades = student[\'grades\'] avg_grade = sum(grades) / len(grades) status = \\"PASSED\\" if all(grade >= 40 for grade in grades) else \\"FAILED\\" if status == \\"FAILED\\": failed_students.append(student[\'name\']) student[\'status\'] = status total_grades += sum(grades) total_grade_count += len(grades) if avg_grade > top_average: top_average = avg_grade top_student = student[\'name\'] average_grade = total_grades / total_grade_count return { \\"average_grade\\": average_grade, \\"top_student\\": top_student, \\"failed_students\\": failed_students, \\"updated_records\\": records }"},{"question":"You are provided with the `penguins` dataset and the seaborn `objects` interface. Your task is to create a function `create_faceted_plot` that takes the following parameters and returns a seaborn plot object: 1. `data`: The dataset to plot (e.g., `penguins`). 2. `facet_row`: The variable name to facet plots along the rows (e.g., \\"sex\\"). 3. `facet_col`: The variable name to facet plots along the columns (e.g., \\"species\\"). 4. `x`: The variable name to use for the x-axis. 5. `y`: The variable name to use for the y-axis. 6. `dots`: A boolean to indicate if dots should be added to the plot. If `False`, no dots should be added. 7. `share_x`: Sharing behavior for the x-axis. Can be `True`, `False`, or \\"col\\". 8. `share_y`: Sharing behavior for the y-axis. Can be `True`, `False`, or \\"row\\". The function should create a faceted scatter plot based on the specified parameters, with appropriate axis sharing as defined by `share_x` and `share_y`. The function should also handle the case where dots need to be added or not based on the `dots` parameter. Example Usage ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") def create_faceted_plot(data, facet_row, facet_col, x, y, dots, share_x, share_y): p = so.Plot(data, x=x, y=y).facet(row=facet_row, col=facet_col) if dots: p = p.add(so.Dots()) p = p.share(x=share_x, y=share_y) return p # Example plot p = create_faceted_plot(penguins, facet_row=\\"sex\\", facet_col=\\"species\\", x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", dots=True, share_x=False, share_y=False) p.show() ``` Constraints - You must use the seaborn `objects` interface as demonstrated in the provided documentation. - Handle potential missing values in the dataset appropriately to avoid errors in plotting. Expected Output The function should return a seaborn plot object that can be displayed using `.show()`.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_faceted_plot(data, facet_row, facet_col, x, y, dots, share_x, share_y): Create a faceted scatter plot based on the specified parameters, with appropriate axis sharing. Parameters: - data (DataFrame): The dataset to plot. - facet_row (str): The variable name to facet plots along the rows. - facet_col (str): The variable name to facet plots along the columns. - x (str): The variable name to use for the x-axis. - y (str): The variable name to use for the y-axis. - dots (bool): Whether or not to add dots to the plot. - share_x (bool or str): Sharing behavior for the x-axis. - share_y (bool or str): Sharing behavior for the y-axis. Returns: - A seaborn plot object. data = data.dropna(subset=[facet_row, facet_col, x, y]) p = so.Plot(data, x=x, y=y).facet(row=facet_row, col=facet_col) if dots: p = p.add(so.Dots()) p = p.share(x=share_x, y=share_y) return p # Example usage with penguins data penguins = load_dataset(\\"penguins\\") p = create_faceted_plot(penguins, facet_row=\\"sex\\", facet_col=\\"species\\", x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", dots=True, share_x=False, share_y=False) p.show()"},{"question":"**Queue Load Balancer** You are required to implement a load balancer using the asyncio Queue. The load balancer will distribute tasks among multiple workers. Each task will have a defined processing time, and we will simulate the processing by making the workers sleep for that duration. Note that the workers should process the tasks concurrently. Your implementation should meet the following requirements: 1. Use `asyncio.Queue` to manage the tasks and distribute them amongst the workers. 2. Implement a function `distribute_tasks(n_workers: int, tasks: List[float]) -> float` that: - Takes the number of workers and a list of task durations. - Returns the total time taken to process all tasks. **Input:** - `n_workers`: An integer representing the number of workers. - `tasks`: A list of floats where each float represents the duration a task takes to complete. **Output:** - A float representing the total time taken to process all the tasks. **Constraints:** - `1 <= n_workers <= 100` - `0 <= len(tasks) <= 1000` - `0.1 <= task duration <= 10.0` **Function Signature:** ```python import asyncio from typing import List async def distribute_tasks(n_workers: int, tasks: List[float]) -> float: pass ``` **Example:** ```python import asyncio from typing import List async def worker(name: str, queue: asyncio.Queue): while True: # Get a \\"work item\\" out of the queue. task_duration = await queue.get() # Simulate task processing by sleeping for the duration. await asyncio.sleep(task_duration) # Notify the queue that the \\"work item\\" has been processed. queue.task_done() async def distribute_tasks(n_workers: int, tasks: List[float]) -> float: queue = asyncio.Queue() for task_duration in tasks: queue.put_nowait(task_duration) tasks = [] for i in range(n_workers): task = asyncio.create_task(worker(f\'worker-{i}\', queue)) tasks.append(task) started_at = asyncio.monotonic() await queue.join() total_time = asyncio.monotonic() - started_at for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) return total_time # Example usage: tasks = [0.5, 0.75, 1.25, 0.4, 1.0, 0.9, 2.0] n_workers = 3 print(asyncio.run(distribute_tasks(n_workers, tasks))) ``` *Note*: This question requires knowledge of asyncio, task scheduling, and managing concurrent tasks using queues. It tests the ability to implement an asynchronous load balancer with appropriate use of asyncio Queue methods.","solution":"import asyncio from typing import List async def worker(queue: asyncio.Queue): while True: task_duration = await queue.get() await asyncio.sleep(task_duration) queue.task_done() async def distribute_tasks(n_workers: int, tasks: List[float]) -> float: queue = asyncio.Queue() for task_duration in tasks: queue.put_nowait(task_duration) workers = [asyncio.create_task(worker(queue)) for _ in range(n_workers)] start_time = asyncio.monotonic() await queue.join() total_time = asyncio.monotonic() - start_time for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) return total_time"},{"question":"# Advanced Python: Handling and Testing Warnings **Objective:** You are required to implement a function that processes a list of user inputs, some of which will trigger specific warnings. Subsequently, test the functionality to ensure proper handling and suppression of these warnings. **Task Details:** 1. **Function Implementation (handle_inputs):** - Implement a function `handle_inputs(inputs: List[str]) -> List[str]` that: - Accepts a list of strings as input. - For each string in the input list: - Converts it to an integer. - If the string is an invalid integer (e.g., contains alphabets), issue a `ValueWarning` and ignore that entry. - If the integer value is greater than 100, issue a `FutureWarning`. - Returns a new list containing only the valid integers converted from the input strings. 2. **Testing Function Implementation (test_handle_inputs):** - Implement a function `test_handle_inputs()` that: - Tests various scenarios of the `handle_inputs` function, including valid inputs, warnings for invalid entries, and warnings for large values. - Uses the `catch_warnings` context manager to capture warnings. - Asserts the presence and type of warnings raised during the execution. - Ensures that the output list only contains valid integers converted from the input strings. **Function Signatures:** ```python import warnings from typing import List class ValueWarning(Warning): pass # Custom warning for invalid integer values def handle_inputs(inputs: List[str]) -> List[str]: # Your implementation here pass def test_handle_inputs(): # Your implementation here pass ``` **Example:** ```python def main(): inputs = [\\"25\\", \\"abc\\", \\"150\\", \\"42\\", \\"xyz\\"] print(handle_inputs(inputs)) # Output: [25, 150, 42] test_handle_inputs() print(\\"All tests passed.\\") if __name__ == \\"__main__\\": main() ``` **Constraints:** - The input list can contain any number of string elements. - Ensure function robustness with different and edge-case inputs. **Notes:** - Utilize `warnings.warn` to issue custom warnings (`ValueWarning` for invalid integers and `FutureWarning` for integers > 100). - Use `catch_warnings` context manager within `test_handle_inputs()` to capture and assert the warnings raised during function execution. - Make sure your solution is efficient and handles a variety of input cases appropriately.","solution":"import warnings from typing import List class ValueWarning(Warning): pass # Custom warning for invalid integer values def handle_inputs(inputs: List[str]) -> List[str]: valid_integers = [] for input_str in inputs: try: value = int(input_str) if value > 100: warnings.warn(\\"Value exceeds 100: {}\\".format(value), FutureWarning) valid_integers.append(value) except ValueError: warnings.warn(\\"Invalid integer value: {}\\".format(input_str), ValueWarning) return valid_integers"},{"question":"**Faceted Plot Creation Challenge** You have been provided with a dataset (`penguins`) which contains information about different species of penguins. Your task is to create a set of faceted plots using seaborn\'s `Plot` class, demonstrating your understanding of the key features and functionalities this class provides. # Dataset The dataset `penguins` has the following columns: - `species`: The species of the penguin. - `island`: The island where the penguin was found. - `bill_length_mm`: The length of the penguin\'s bill. - `bill_depth_mm`: The depth of the penguin\'s bill. - `flipper_length_mm`: The length of the penguin\'s flipper. - `body_mass_g`: The body mass of the penguin. - `sex`: The sex of the penguin. # Task 1. **Basic Facet Plot:** Create a basic faceted plot using `seaborn.objects.Plot`: - Plot `bill_length_mm` vs. `bill_depth_mm`. - Facet the plot by `species`. 2. **Multiple Faceting Variables:** Create a faceted plot that uses multiple faceting variables: - Plot `bill_length_mm` vs. `flipper_length_mm`. - Facet the plot by `species` and `sex`. 3. **Custom Faceting Order:** Create a faceted plot with a custom order for the `species`: - Plot `body_mass_g` vs. `flipper_length_mm`. - Facet by `species`. - Order the species by `Adelie`, `Chinstrap`. 4. **Faceting with Wrapping:** Use wrapping to create a faceted plot: - Plot `bill_length_mm` vs. `body_mass_g`. - Facet by `island` and wrap it across 2 columns. 5. **Custom Labels:** Create a faceted plot with custom labels: - Plot `bill_length_mm` vs. `bill_depth_mm`. - Facet by `species`. - Label the titles in the format: `\\"{species} - Penguin Bill Dimensions\\"`. # Input and Output - **Input:** The script should not require any direct input. You will use the `seaborn` libraryâ€™s `load_dataset` function to load the `penguins` dataset. - **Output:** Display the faceted plots as specified. # Constraints - Ensure that the plots are generated using the seaborn library version 0.12.0 or higher. - Use the `seaborn.objects.Plot` class and its methods such as `facet`, `label`, and others as demonstrated in the documentation. # Example Code To help you get started, here is an example of how you might start your solution: ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Basic Facet Plot p1 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()).facet(\\"species\\") p1.show() # Continue with creating other plots as per the specified tasks... ``` **Note**: You should complete the code with appropriate methods and parameters to achieve the required tasks.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Basic Facet Plot def basic_facet_plot(): p1 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").facet(\\"species\\").add(so.Dots()) p1.show() # Multiple Faceting Variables def multiple_faceting_variables(): p2 = so.Plot(penguins, \\"bill_length_mm\\", \\"flipper_length_mm\\").facet(\\"species\\", \\"sex\\").add(so.Dots()) p2.show() # Custom Faceting Order def custom_faceting_order(): species_order = [\\"Adelie\\", \\"Chinstrap\\"] p3 = so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\").facet(\\"species\\", order=species_order).add(so.Dots()) p3.show() # Faceting with Wrapping def faceting_with_wrapping(): p4 = so.Plot(penguins, \\"bill_length_mm\\", \\"body_mass_g\\").facet(\\"island\\", wrap=2).add(so.Dots()) p4.show() # Custom Labels def custom_labels(): p5 = ( so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\") .facet(\\"species\\") .add(so.Dots()) .label(facet=\\"{species} - Penguin Bill Dimensions\\") ) p5.show()"},{"question":"**Objective:** Create an asynchronous program in Python that monitors multiple network socket connections and performs specific read and write operations based on the platform it is running on. # Requirements: 1. **Function Implementation:** Implement a function `async_monitor_sockets` that accepts a list of socket objects and asynchronously reads from and writes to them using the `asyncio` module. 2. **Platform-Specific Behavior:** - On **Windows**, the function should utilize the `ProactorEventLoop` event loop (which is the default as of Python 3.8) and should demonstrate subprocess management. - On **macOS** (modern versions), the program should demonstrate the use of the default event loop to monitor sockets. 3. **Input:** - A list of socket objects. - The function should be capable of handling network socket connections in a non-blocking manner. 4. **Output:** - The function should print relevant read data from each socket to the console and write a predefined message back to the respective socket. 5. **Constraints and Limitations:** - The program should handle cases where certain functionalities are not supported on Windows (such as `loop.add_reader()` and `loop.add_writer()`) and provide alternative handling or error messages. 6. **Performance Requirements:** - The function should handle at least 100 socket connections efficiently and provide timely read/write operations. # Example Usage: ```python import asyncio import socket async def async_monitor_sockets(sockets): # Implementation here # Example of creating multiple socket connections sockets = [socket.socket(socket.AF_INET, socket.SOCK_STREAM) for _ in range(100)] # Connect sockets to a test server for demonstration for s in sockets: s.connect((\'localhost\', 8000)) # Run the async function asyncio.run(async_monitor_sockets(sockets)) ``` # Notes: - Take into account that `SelectorEventLoop` on Windows has a limitation of 512 socket handles. - Provide necessary error handling and checks for platform-specific constraints mentioned in the documentation. - Ensure asynchronous reading and writing are performed efficiently. This question aims to assess the student\'s understanding of asynchronous programming, platform-specific limitations of the `asyncio` module, and ability to create robust and platform-compatible Python code.","solution":"import asyncio import platform import socket import sys async def handle_socket(sock): loop = asyncio.get_event_loop() while True: data = await loop.sock_recv(sock, 1024) if not data: break print(f\\"Received data: {data.decode(\'utf-8\')}\\") await loop.sock_sendall(sock, b\\"This is a predefined message\\") sock.close() async def async_monitor_sockets(sockets): if platform.system() == \'Windows\': loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) else: loop = asyncio.get_event_loop() tasks = [handle_socket(sock) for sock in sockets] await asyncio.gather(*tasks) # Example Usage: # Create sockets and connect to server for demonstration # sockets = [socket.socket(socket.AF_INET, socket.SOCK_STREAM) for _ in range(100)] # for s in sockets: # s.connect((\'localhost\', 8000)) # asyncio.run(async_monitor_sockets(sockets))"},{"question":"Coding Assessment Question # Objective: Implement a Python function that reads information about system users and groups from the Unix password (`pwd`) and group (`grp`) databases and then summarizes this information. # Problem Statement: You are required to write a function `summarize_users_and_groups()` that: - Retrieves all user entries from the Unix password database (`pwd`). - Retrieves all group entries from the Unix group database (`grp`). - Constructs and returns a dictionary summary with the following structure: ```python { \\"total_users\\": <number_of_users>, \\"total_groups\\": <number_of_groups>, \\"users\\": {\\"<username>\\": {\\"uid\\": <user_id>, \\"gid\\": <group_id>, \\"home\\": \\"<home_directory>\\"}, ...}, \\"groups\\": {\\"<groupname>\\": {\\"gid\\": <group_id>, \\"members\\": [<username1>, <username2>, ...]}, ...} } ``` # Requirements: - Use the `pwd` module to access user information. - Use the `grp` module to access group information. # Constraints: - **Unix-based system**: This function can only be tested on a Unix-based system where the `pwd` and `grp` modules are available. # Example Output: ```python { \\"total_users\\": 5, \\"total_groups\\": 3, \\"users\\": { \\"alice\\": {\\"uid\\": 1001, \\"gid\\": 1001, \\"home\\": \\"/home/alice\\"}, \\"bob\\": {\\"uid\\": 1002, \\"gid\\": 1002, \\"home\\": \\"/home/bob\\"}, # more users }, \\"groups\\": { \\"admins\\": {\\"gid\\": 1001, \\"members\\": [\\"alice\\", \\"bob\\"]}, \\"users\\": {\\"gid\\": 1002, \\"members\\": [\\"alice\\"]}, # more groups } } ``` # Function Signature: ```python def summarize_users_and_groups() -> dict: pass ``` # Additional Notes: - Make sure to handle cases where a user might not be in any group or where groups might not have any members. # Guidelines: - Read the documentation for `pwd` and `grp` modules to understand how to access and use the provided data. - Structure your solution clearly and ensure it is robust against edge cases such as missing data or empty entries.","solution":"import pwd import grp def summarize_users_and_groups(): Summarizes the users and groups information from the Unix system. users = pwd.getpwall() groups = grp.getgrall() user_summary = {} for user in users: user_summary[user.pw_name] = { \\"uid\\": user.pw_uid, \\"gid\\": user.pw_gid, \\"home\\": user.pw_dir } group_summary = {} for group in groups: group_summary[group.gr_name] = { \\"gid\\": group.gr_gid, \\"members\\": group.gr_mem } summary = { \\"total_users\\": len(users), \\"total_groups\\": len(groups), \\"users\\": user_summary, \\"groups\\": group_summary } return summary"},{"question":"**Coding Assessment Question: Recreate Deprecated `imp` Functionality Using `importlib` and Modern Practices** # Problem Statement The `imp` module has been deprecated in favor of the `importlib` module. In this task, you are required to rewrite the functionality to find and load a module using `importlib` tools while maintaining compatibility with the interface of the old `imp` module. # Instructions 1. Implement a function `find_module(mod_name: str, search_path: Optional[List[str]] = None) -> Tuple[Optional[IO], str, Tuple[str, str, str]]` that: - Searches for a module file given its name (`mod_name`) and an optional list of directories to search (`search_path`). - Returns a 3-element tuple consisting of: - An open file object for the module (or `None` if not applicable). - The pathname of the module. - A metadata tuple (`suffix`, `mode`, `type`) describing the module. 2. Implement a function `load_module(mod_name: str, file: Optional[IO], pathname: str, description: Tuple[str, str, str]) -> ModuleType` that: - Loads the module that was previously found by `find_module()`, identified by its name (`mod_name`), file object (`file`), pathname (`pathname`), and description tuple (`description`). - Returns the module object. # Constraints - Your implementations should mirror the old `imp` module\'s interfaces but use the modern `importlib` library methods. - You must handle the file object correctly, ensuring it is closed appropriately even in case of exceptions. - You should account for different module types as described in the `imp` module\'s documentation. # Example Usage ```python # Example of how to use your functions to emulate the old imp interface mod_name = \'example\' search_path = [\'/path/to/modules\'] try: file, pathname, desc = find_module(mod_name, search_path) module = load_module(mod_name, file, pathname, desc) print(f\\"Module {mod_name} loaded successfully from {pathname}\\") finally: if file: file.close() ``` # Expected Function Signature ```python from typing import Optional, List, Tuple, IO import importlib import sys def find_module(mod_name: str, search_path: Optional[List[str]] = None) -> Tuple[Optional[IO], str, Tuple[str, str, str]]: # Your implementation here pass def load_module(mod_name: str, file: Optional[IO], pathname: str, description: Tuple[str, str, str]) -> \'ModuleType\': # Your implementation here pass ``` # Testing - Ensure to test your implementation with real module names and paths. - Handle edge cases such as the module not existing or directory issues. Good luck, and happy coding!","solution":"from typing import Optional, List, Tuple, IO import importlib.util import importlib.machinery import sys import os def find_module(mod_name: str, search_path: Optional[List[str]] = None) -> Tuple[Optional[IO], str, Tuple[str, str, str]]: Find a module by name and optionally within given search paths. Returns a tuple (file, pathname, description). if search_path is None: search_path = sys.path for path in search_path: try: module_file, module_path, description = importlib.util.find_spec(mod_name).origin, importlib.util.find_spec(mod_name).origin, (\\".py\\", \\"r\\", importlib.machinery.SOURCE_SUFFIXES) if module_path: file = open(module_path, description[1]) if description[2] == importlib.machinery.SOURCE_SUFFIXES else None return file, module_path, description except AttributeError: continue raise ImportError(f\\"Module {mod_name} not found\\") def load_module(mod_name: str, file: Optional[IO], pathname: str, description: Tuple[str, str, str]): Load a module by name, file reference, pathname and description. Returns the loaded module object. if description[2] == importlib.machinery.SOURCE_SUFFIXES: spec = importlib.util.spec_from_file_location(mod_name, pathname) if spec and spec.loader: module = importlib.util.module_from_spec(spec) sys.modules[mod_name] = module spec.loader.exec_module(module) return module raise ImportError(f\\"Cannot load module {mod_name}\\") # Example usage mod_name = \'example\' search_path = [\'/path/to/modules\'] try: file, pathname, desc = find_module(mod_name, search_path) module = load_module(mod_name, file, pathname, desc) print(f\\"Module {mod_name} loaded successfully from {pathname}\\") finally: if file: file.close()"},{"question":"**Title:** Tensor Size Manipulation and Analysis **Objective:** To assess the student\'s understanding of tensor size manipulations using the `torch.Size` class in PyTorch, along with basic tensor operations. **Question:** You are required to implement a function `analyze_tensor_sizes` that takes two tensors as input and returns a dictionary with the following details: 1. The size of each tensor. 2. The dimension with the largest size in each tensor. 3. A boolean indicating whether the tensors are compatible for broadcasting according to PyTorch\'s broadcasting rules. # Function Signature: ```python def analyze_tensor_sizes(tensor1: torch.Tensor, tensor2: torch.Tensor) -> dict: pass ``` # Input: - `tensor1`: A tensor of any shape. - `tensor2`: A tensor of any shape. # Output: A dictionary with the following structure: ```python { \'size_tensor1\': torch.Size of tensor1, \'size_tensor2\': torch.Size of tensor2, \'largest_dim_tensor1\': int, \'largest_dim_tensor2\': int, \'broadcast_compatible\': bool } ``` # Constraints: - You can assume that the tensors are non-empty. - Use the `torch.Size` object to obtain the size information and perform necessary operations. # Example: ```python import torch tensor1 = torch.ones(10, 3, 5) tensor2 = torch.ones(5, 1) result = analyze_tensor_sizes(tensor1, tensor2) # Expected output: # { # \'size_tensor1\': torch.Size([10, 3, 5]), # \'size_tensor2\': torch.Size([5, 1]), # \'largest_dim_tensor1\': 5, # \'largest_dim_tensor2\': 5, # \'broadcast_compatible\': True # } print(result) ``` # Requirements: 1. **Size Determination:** Utilize the `torch.Size` class to determine the size of both tensors. 2. **Dimension Analysis:** Identify and return the dimension with the largest size in each tensor. 3. **Broadcasting Compatibility:** Implement a check to determine whether the two tensors are compatible for broadcasting. A function to check PyTorch\'s broadcasting rules can be taken as reference. Provide a solution with clear and concise comments explaining your approach.","solution":"import torch def analyze_tensor_sizes(tensor1: torch.Tensor, tensor2: torch.Tensor) -> dict: Analyzes the size and broadcasting compatibility of two tensors. Args: tensor1: torch.Tensor - The first tensor. tensor2: torch.Tensor - The second tensor. Returns: A dictionary with the size, largest dimension, and broadcasting compatibility details. # Get the sizes of both tensors size_tensor1 = tensor1.size() size_tensor2 = tensor2.size() # Find the largest dimension size in each tensor largest_dim_tensor1 = max(size_tensor1) largest_dim_tensor2 = max(size_tensor2) # Determine if the tensors are broadcast compatible def are_compatible_for_broadcasting(size1, size2): Determines if two sizes are compatible for broadcasting. Args: size1, size2: torch.Size - The sizes to be checked. Returns: bool - True if the sizes are compatible, False otherwise. len1, len2 = len(size1), len(size2) for i in range(1, min(len1, len2) + 1): if size1[-i] != size2[-i] and size1[-i] != 1 and size2[-i] != 1: return False return True broadcast_compatible = are_compatible_for_broadcasting(size_tensor1, size_tensor2) return { \'size_tensor1\': size_tensor1, \'size_tensor2\': size_tensor2, \'largest_dim_tensor1\': largest_dim_tensor1, \'largest_dim_tensor2\': largest_dim_tensor2, \'broadcast_compatible\': broadcast_compatible }"},{"question":"**Objective:** Design and implement a machine learning pipeline using scikit-learn to predict housing prices based on a given dataset. The solution should demonstrate an understanding of model training, cross-validation, hyperparameter tuning, and feature selection. The pipeline should be robust, handling any missing values and scaling features appropriately. **Dataset:** You will be provided with a CSV file `housing_data.csv` containing the following columns: - `area`: square footage of the house - `bedrooms`: number of bedrooms - `bathrooms`: number of bathrooms - `floors`: number of floors - `year_built`: year the house was built - `price`: target variable, the price of the house **Requirements:** 1. **Data Preprocessing**: Handle missing values and scale the features. 2. **Feature Selection**: Select the most important features for prediction using an appropriate technique. 3. **Model Training**: Train a regression model of your choice (e.g., Linear Regression, Decision Tree, Random Forest) with cross-validation. 4. **Hyperparameter Tuning**: Optimize the model\'s hyperparameters. 5. **Evaluation**: Evaluate the model using appropriate metrics. **Constraints:** - The dataset may have missing values. - The dataset has a size of approximately 10000 rows. - The model evaluation should be based on a 80-20 train-test split. **Performance Requirements:** - The mean absolute error (MAE) of the model should be less than 50000 on the test set. **Expected Input and Output:** - Input: `housing_data.csv` file containing the housing data. - Output: Print the mean absolute error (MAE) of the final model on the test set and a list of the selected features. ```python # Import necessary libraries import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.feature_selection import SelectKBest, f_regression from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error def housing_price_prediction_pipeline(file_path: str): # Load the dataset data = pd.read_csv(file_path) # Separate features and target variable X = data.drop(columns=[\'price\']) y = data[\'price\'] # Define a pipeline for preprocessing and training pipeline = Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), # Handle missing values (\'scaler\', StandardScaler()), # Feature scaling (\'feature_selection\', SelectKBest(score_func=f_regression, k=5)), # Feature selection (\'model\', LinearRegression()) # Regression model ]) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Cross-validation cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=\'neg_mean_absolute_error\') print(f\'Cross-validated MAE: {-cv_scores.mean()}\') # Hyperparameter tuning (example for SelectKBest\'s k parameter) param_grid = { \'feature_selection__k\': [3, 5, 7] } grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'neg_mean_absolute_error\') grid_search.fit(X_train, y_train) # Best model evaluation best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) mae = mean_absolute_error(y_test, y_pred) print(f\'Test MAE: {mae}\') print(f\'Selected Features: {X.columns[best_model.named_steps[\\"feature_selection\\"].get_support()]}\') # Example usage # housing_price_prediction_pipeline(\'housing_data.csv\') ``` **Notes:** - Ensure you handle any missing values appropriately. - Use best practices for machine learning workflows, including splitting data, cross-validation, and hyperparameter tuning. - Evaluate the final model based on the mean absolute error (MAE) and list the selected features.","solution":"# Import necessary libraries import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.feature_selection import SelectKBest, f_regression from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error def housing_price_prediction_pipeline(file_path: str): # Load the dataset data = pd.read_csv(file_path) # Separate features and target variable X = data.drop(columns=[\'price\']) y = data[\'price\'] # Define a pipeline for preprocessing and training pipeline = Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), # Handle missing values (\'scaler\', StandardScaler()), # Feature scaling (\'feature_selection\', SelectKBest(score_func=f_regression, k=5)), # Feature selection (\'model\', LinearRegression()) # Regression model ]) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Cross-validation cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=\'neg_mean_absolute_error\') print(f\'Cross-validated MAE: {-cv_scores.mean()}\') # Hyperparameter tuning (example for SelectKBest\'s k parameter) param_grid = { \'feature_selection__k\': [3, 5, 7] } grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'neg_mean_absolute_error\') grid_search.fit(X_train, y_train) # Best model evaluation best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) mae = mean_absolute_error(y_test, y_pred) print(f\'Test MAE: {mae}\') print(f\'Selected Features: {X.columns[best_model.named_steps[\\"feature_selection\\"].get_support()]}\') # Example usage # housing_price_prediction_pipeline(\'housing_data.csv\')"},{"question":"# Question: Advanced Seaborn Plotting Given the `penguins` dataset from the seaborn library, create a function `plot_penguin_data` that generates a plot showcasing the following: 1. A dashed representation for each datapoint showing the body mass of penguins by species, with different colors for different sexes. 2. Adjust the transparency of the dashes to `0.5`. 3. Map the `linewidth` of the dashes to the `flipper_length_mm` feature. 4. Set the width of the dashes to `0.5`. 5. Add dodging to the dashes for better visibility. 6. Overlay an aggregate dash plot and a dot plot with jitter for each data point. Your function should: - Load the dataset. - Generate and display the required plot. **Input:** None (You don\'t need to take any inputs). **Output:** None (The function should only generate and display the plot). ```python def plot_penguin_data(): import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot with the required specifications p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") ( p .add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\", width=0.5) .add(so.Dash(), so.Dodge()) .add(so.Dash(), so.Agg(), so.Dodge()) .add(so.Dots(), so.Dodge(), so.Jitter()) ) # Display the plot p.show() ``` Ensure that your solution correctly implements all specified functionalities and produces the desired plot.","solution":"def plot_penguin_data(): import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot with the required specifications p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") ( p .add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\", width=0.5) .add(so.Dash(), so.Dodge()) .add(so.Dash(), so.Agg(), so.Dodge()) .add(so.Dots(), so.Dodge(), so.Jitter()) ) # Display the plot p.show()"},{"question":"Objective: To evaluate the ability to work with `asyncio.Future` objects in Python by implementing and integrating asynchronous functions that interact with Future objects and ensure proper handling of asynchronous results and exceptions. Question: You are required to implement a function `fetch_data` that simulates fetching data asynchronously and returns a Future object. You will then implement another function `process_data` that uses the `fetch_data` function, waits for the Future to be completed, and processes the result. # Function 1: `fetch_data` - **Input**: - `delay` (int): The number of seconds to wait before setting the result. - `data` (str): The data to be set as the result of the Future. - **Output**: - Returns a Future object that will be completed after `delay` seconds with `data`. # Function 2: `process_data` - **Input**: None - **Output**: `str` - Should return the string \\"Processing complete: <data>\\", where <data> is the result returned by the `fetch_data` function. - **Behavior**: - Calls `fetch_data` with a delay of 2 seconds and data \\"Hello, Asyncio!\\". - Waits for the Future to be completed. - Processes the result by appending \\"Processing complete: \\" to the result obtained from `fetch_data`. - Handles possible exceptions and returns \\"Exception occurred: <exception_message>\\" if any exception occurs during fetching or processing. # Constraints: - You must use `asyncio` and `Future` to handle the asynchronous operations. - Ensure proper exception handling and result processing as specified. # Performance Requirements: - Ensure that the code runs efficiently without unnecessary delays. # Example Usage: ```python import asyncio async def fetch_data(delay: int, data: str) -> asyncio.Future: # Your implementation here async def process_data() -> str: # Your implementation here # Example usage if __name__ == \\"__main__\\": result = asyncio.run(process_data()) print(result) # Output: \\"Processing complete: Hello, Asyncio!\\" ``` # Notes: - The provided code snippet sets up the `fetch_data` and `process_data` functions. - Replace the `# Your implementation here` comments with your working code.","solution":"import asyncio async def fetch_data(delay: int, data: str) -> asyncio.Future: Simulates fetching data asynchronously. Parameters: delay (int): The number of seconds to wait before setting the result. data (str): The data to be set as the result of the Future. Returns: asyncio.Future: A Future object that will be completed after `delay` seconds with `data`. future = asyncio.Future() await asyncio.sleep(delay) future.set_result(data) return future async def process_data() -> str: Uses the fetch_data function, waits for the Future to be completed, and processes the result. Returns: str: \\"Processing complete: <data>\\", where <data> is the result from fetch_data. If an exception occurs, returns \\"Exception occurred: <exception_message>\\". try: future = await fetch_data(2, \\"Hello, Asyncio!\\") data = await future return f\\"Processing complete: {data}\\" except Exception as e: return f\\"Exception occurred: {str(e)}\\""},{"question":"Objective: Write a Python function that reads a text file, processes its content and performs specified operations based on user input. You must implement robust error handling that leverages custom exceptions and exception chaining. The purpose of this exercise is to demonstrate your understanding of exception inheritance and context in Python. Function Signature: ```python def process_file(file_path: str, operation: str, output_path: str): pass ``` Description: 1. **Input Parameters:** - `file_path` (str): Path to the input text file. - `operation` (str): The operation to perform on the file\'s content. Supported operations are `\\"uppercase\\"`, `\\"lowercase\\"`, `\\"reverse\\"`. - `output_path` (str): Path to save the processed output text file. 2. **Functionality:** - The function should read the file from `file_path`. - Based on the `operation` provided, it should process the file content as follows: - `\\"uppercase\\"`: Convert all text to uppercase. - `\\"lowercase\\"`: Convert all text to lowercase. - `\\"reverse\\"`: Reverse the order of characters in the text. - Save the processed content to `output_path`. 3. **Error Handling:** - Implement custom exceptions `FileProcessingError`, `InvalidOperationError`, and `FileSaveError`. - `FileProcessingError` should be raised when there is an error reading the file. - This exception should capture `FileNotFoundError` if the file doesn\'t exist or `OSError` for other OS related errors during file reading. - `InvalidOperationError` should be raised if the `operation` is not one of the supported operations. - `FileSaveError` should be raised when there is an error saving the processed content to `output_path`. - This exception should capture errors such as `PermissionError` or `OSError`. - Use exception chaining to maintain context when this exception is raised. 4. **Constraints:** - The input file will not exceed 1MB in size. - `operation` will be a string conforming to one of the mentioned values. - Valid paths for files should be given, assuming permission to read/write. 5. **Output:** - No return value. The function should save the output directly to the file at `output_path`. - In the event of exceptions, appropriate error messages should be printed. Example Usage: ```python try: process_file(\\"input.txt\\", \\"uppercase\\", \\"output.txt\\") except FileProcessingError as e: print(f\\"Error reading file: {e}\\") except InvalidOperationError as e: print(f\\"Invalid operation: {e}\\") except FileSaveError as e: print(f\\"Error saving file: {e}\\") ``` Example: Given an input file `input.txt` containing: ```text Hello World ``` Calling the function as follows: ```python process_file(\\"input.txt\\", \\"reverse\\", \\"output.txt\\") ``` Should produce an output file `output.txt` with: ```text dlroW olleH ``` In case of errors, appropriate custom exceptions should be raised and handled to provide clear debugging information.","solution":"class FileProcessingError(Exception): pass class InvalidOperationError(Exception): pass class FileSaveError(Exception): pass def process_file(file_path: str, operation: str, output_path: str): try: with open(file_path, \'r\') as file: content = file.read() except (FileNotFoundError, OSError) as e: raise FileProcessingError(\\"Error reading file\\") from e if operation == \\"uppercase\\": processed_content = content.upper() elif operation == \\"lowercase\\": processed_content = content.lower() elif operation == \\"reverse\\": processed_content = content[::-1] else: raise InvalidOperationError(f\\"Unsupported operation: {operation}\\") try: with open(output_path, \'w\') as file: file.write(processed_content) except (PermissionError, OSError) as e: raise FileSaveError(\\"Error saving file\\") from e"},{"question":"# Platform Information Aggregator You are required to design a function that aggregates and returns critical platform-related information of the system where the code is run. Your function should leverage the `platform` module to extract the necessary details and return them in a structured format. Function Signature ```python def get_system_platform_info(aliased: bool = False, terse: bool = False) -> dict: pass ``` Input - `aliased` (bool): a flag indicating whether to use aliased names for platforms that report system names differing from their common names. Default is `False`. - `terse` (bool): a flag indicating whether to return only the minimum information needed to identify the platform. Default is `False`. Output - A dictionary with the following keys and their corresponding values: - `\\"architecture\\"`: a tuple containing `(bits, linkage)` representing the architecture details of the Python interpreter. - `\\"machine\\"`: a string representing the machine type. - `\\"node\\"`: a string representing the computer\'s network name. - `\\"platform\\"`: a string representing the platform details. - `\\"processor\\"`: a string representing the processor name. - `\\"python_compiler\\"`: a string representing the compiler used to compile Python. - `\\"python_version\\"`: a tuple `(major, minor, patchlevel)` representing the Python version. - `\\"system\\"`: a string representing the operating system name. - `\\"release\\"`: a string representing the system\'s release. - `\\"version\\"`: a string representing the system\'s version. - `\\"uname\\"`: a named tuple containing attributes: `system`, `node`, `release`, `version`, `machine`, and `processor`. Example ```python sys_info = get_system_platform_info(aliased=True, terse=True) print(sys_info) # Output: { # \\"architecture\\": (\\"64bit\\", \\"ELF\\"), # \\"machine\\": \\"x86_64\\", # \\"node\\": \\"my-computer\\", # \\"platform\\": \\"Linux-5.4.0-42-generic-x86_64-with-glibc2.29\\", # \\"processor\\": \\"x86_64\\", # \\"python_compiler\\": \\"GCC 9.3.0\\", # \\"python_version\\": (\\"3\\", \\"10\\", \\"0\\"), # \\"system\\": \\"Linux\\", # \\"release\\": \\"5.4.0-42-generic\\", # \\"version\\": \\"#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\\", # \\"uname\\": uname_result(system=\'Linux\', node=\'my-computer\', release=\'5.4.0-42-generic\', version=\'#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\', machine=\'x86_64\', processor=\'x86_64\') # } ``` Constraints - Use appropriate methods from the `platform` module as documented. - The function should handle typical edge cases like missing or empty information gracefully. - The function should be cross-platform compatible. Implement this function in Python, ensuring it meets the given requirements and constraints.","solution":"import platform def get_system_platform_info(aliased: bool = False, terse: bool = False) -> dict: Aggregates and returns critical platform-related information of the system. Args: - aliased (bool): Use aliased names for certain platforms. Default is False. - terse (bool): Return minimal information necessary to identify the platform. Default is False. Returns: - dict: A dictionary with keys containing platform-related information. return { \\"architecture\\": platform.architecture(), \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(aliased=aliased, terse=terse), \\"processor\\": platform.processor(), \\"python_compiler\\": platform.python_compiler(), \\"python_version\\": platform.python_version_tuple(), \\"system\\": platform.system(), \\"release\\": platform.release(), \\"version\\": platform.version(), \\"uname\\": platform.uname() }"},{"question":"# Custom Exception Handling in Python Objective Your task is to demonstrate advanced exception handling in Python by creating a custom exception class and handling it under specific circumstances. You will also need to showcase how to manage recursions and issue warnings. Problem Statement 1. **Custom Exception Creation**: - Create a custom exception class `CustomException` that inherits from Python\'s base `Exception` class. 2. **Function Implementation**: - Write a function `process_data(data)` that processes a given list of integers. The function should: - Raise `CustomException` with a meaningful error message if any integer in the list is negative. - Accumulate the sum of all positive integers. - If there are integers larger than 100, issue a warning using the `warnings` module. 3. **Recursion Handling**: - Implement a recursive function `recursive_sum(data, depth=0)` that calculates the sum of the list\'s integers. The function should: - Track the recursion depth and raise a `RuntimeError` if the depth exceeds 10. - Use Python\'s built-in exception handling to catch and manage this error. - Make sure to handle and reset the recursion depth correctly to avoid infinite loops. 4. **Signal Handling Simulation**: - Implement signal handling functionality using `signal` and `PyErr_SetInterruptEx()` to simulate and handle a `KeyboardInterrupt`. 5. **Testing the Function**: - Write a main function `main(data)` that calls `process_data(data)` and `recursive_sum(data)`. - Handle any exceptions raised and print appropriate error messages. - Simulate a `KeyboardInterrupt` during execution and handle it gracefully. Input and Output **Input**: - A list of integers `data`. **Output**: - Sum of integers if the processing is successful. - Handle and display error messages for any exceptions raised. Constraints - The recursion depth for `recursive_sum` should not exceed 10. - Values in the provided list `data` can range from `-1000` to `1000`. Example ```python class CustomException(Exception): pass import warnings import signal def process_data(data): # Your implementation here def recursive_sum(data, depth=0): # Your implementation here def main(data): # Your implementation here if __name__ == \\"__main__\\": data = [1, 2, 3, -4, 101, 150] main(data) ``` **Expected Output**: ``` \\"Warning: Value greater than 100 found.\\" \\"Error: CustomException: Negative value encountered: -4\\" \\"Error: Recursion depth exceeded\\" \\"KeyboardInterrupt detected and handled.\\" ``` **Note**: Ensure your implementation covers all required functionalities and handles any possible exceptions gracefully.","solution":"import warnings import signal class CustomException(Exception): pass def process_data(data): # Check for negative values and raise CustomException for num in data: if num < 0: raise CustomException(f\\"Negative value encountered: {num}\\") # Check for values greater than 100 and issue a warning for num in data: if num > 100: warnings.warn(f\\"Value greater than 100 found: {num}\\") # Return the sum of positive integers return sum(num for num in data if num > 0) def recursive_sum(data, depth=0): if depth > 10: raise RuntimeError(\\"Recursion depth exceeded\\") if len(data) == 0: return 0 return data[0] + recursive_sum(data[1:], depth + 1) def signal_handler(signum, frame): raise KeyboardInterrupt(\\"Simulated KeyboardInterrupt\\") def main(data): signal.signal(signal.SIGALRM, signal_handler) try: print(\\"Starting data processing...\\") result = process_data(data) print(f\\"Processed data sum: {result}\\") signal.alarm(1) # Simulate a signal after 1 second try: sum_result = recursive_sum(data) print(f\\"Recursive sum: {sum_result}\\") except RuntimeError as e: print(f\\"Error: {e}\\") except CustomException as e: print(f\\"Error: CustomException: {e}\\") except KeyboardInterrupt: print(\\"KeyboardInterrupt detected and handled.\\") finally: signal.alarm(0) # Disable the alarm signal if __name__ == \\"__main__\\": data = [1, 2, 3, -4, 101, 150] main(data)"},{"question":"# Decimal Module Advanced Application You are tasked with creating a utility function for precise financial calculations in a custom trading application. This function, `calculate_gain_or_loss()`, will compute the total gain or loss for a series of trades using Python\'s `decimal` module to ensure high precision in the arithmetic operations. Here are the requirements for the function: 1. **Input Parameters:** - `initial_balance` (str): A string representing a decimal value for the initial amount (e.g., \'1000.50\'). - `trades` (List[Tuple[str, str]]): A list of tuples where each tuple represents a trade. Each trade tuple contains two strings: - The first string is either \'buy\' or \'sell\' indicating the type of trade. - The second string is a decimal value (in string format) representing the trade amount (e.g., \'50.25\'). 2. **Output:** - The function should return a string representation of the final balance after applying all trades. 3. **Details:** - All calculations should use the `decimal.Decimal` class to maintain precision. - The function should raise appropriate exceptions for any invalid operations (e.g., invalid trade type), using the handling mechanisms of the `decimal` module. 4. **Constraints:** - The decimal representation should have fixed precision of 2 places for the final output. - Assume well-formed inputs, i.e., valid decimal strings, but enforce the trade type (\'buy\' or \'sell\'). # Function Signature ```python def calculate_gain_or_loss(initial_balance: str, trades: List[Tuple[str, str]]) -> str: pass ``` # Example Usage ```python initial_balance = \'1000.50\' trades = [(\'buy\', \'200.00\'), (\'sell\', \'50.50\'), (\'sell\', \'150.00\')] final_balance = calculate_gain_or_loss(initial_balance, trades) print(final_balance) # Expected: \'800.00\' ``` # Notes - Use Python\'s `decimal` module for all calculations. - Ensure that all inputs are correctly converted to `decimal.Decimal` before performing any operations. - Handle any exceptional conditions gracefully and use decimal context settings to maintain a precision of 2 places throughout the operations.","solution":"from decimal import Decimal, getcontext def calculate_gain_or_loss(initial_balance: str, trades: list) -> str: Calculate the final balance after executing a series of trades. # Set precision to 2 decimal places getcontext().prec = 10 # Higher precision for intermediate calculations getcontext().rounding = \'ROUND_HALF_EVEN\' initial_balance = Decimal(initial_balance) for trade in trades: trade_type, trade_amount = trade trade_amount = Decimal(trade_amount) if trade_type == \'buy\': initial_balance -= trade_amount elif trade_type == \'sell\': initial_balance += trade_amount else: raise ValueError(f\\"Invalid trade type: {trade_type}\\") # Return the final balance formatted to 2 decimal places return f\\"{initial_balance:.2f}\\""},{"question":"# Python 310 Coding Assessment Question Objective: To assess your understanding of the Unix syslog library routines as provided by the `syslog` module in Python, you are required to implement a function that logs a series of messages to different facilities and priority levels. The function should also allow configuration of logging options and setting log masks. Problem Statement: Implement a function `custom_logger` that accepts a list of log configurations and messages, logs each message according to its configuration, and returns a summary of how many messages were logged for each priority level. Function Signature: ```python def custom_logger(configs: List[Dict[str, Any]], mask_priority: str) -> Dict[str, int]: pass ``` Input: - `configs`: A list of dictionaries where each dictionary contains: - `message` (str): The message to be logged. - `priority` (str): The priority level for the message (One of \\"LOG_EMERG\\", \\"LOG_ALERT\\", \\"LOG_CRIT\\", \\"LOG_ERR\\", \\"LOG_WARNING\\", \\"LOG_NOTICE\\", \\"LOG_INFO\\", \\"LOG_DEBUG\\"). - `facility` (str, optional): The facility to log the message to (One of \\"LOG_KERN\\", \\"LOG_USER\\", \\"LOG_MAIL\\", \\"LOG_DAEMON\\", \\"LOG_AUTH\\", \\"LOG_LPR\\", \\"LOG_NEWS\\", \\"LOG_UUCP\\", \\"LOG_CRON\\", \\"LOG_SYSLOG\\", \\"LOG_LOCAL0\\" to \\"LOG_LOCAL7\\", default is \\"LOG_USER\\"). - `logoption` (str, optional): The logging options (One of \\"LOG_PID\\", \\"LOG_CONS\\", \\"LOG_NDELAY\\", \\"LOG_ODELAY\\", \\"LOG_NOWAIT\\", \\"LOG_PERROR\\", default is None). - `mask_priority` (str): The priority level up to which messages should be logged (same priority levels as above). Output: - A dictionary with keys corresponding to each priority level and values indicating the number of messages logged for that priority. Constraints: - Only messages with priorities up to and including the specified `mask_priority` should be logged. - If a facility or logoption is not provided in a configuration, default values should be used. Example: ```python configs = [ {\\"message\\": \\"System booting\\", \\"priority\\": \\"LOG_INFO\\"}, {\\"message\\": \\"Disk space low\\", \\"priority\\": \\"LOG_WARNING\\", \\"facility\\": \\"LOG_DAEMON\\", \\"logoption\\": \\"LOG_PID\\"}, {\\"message\\": \\"Disk space critical\\", \\"priority\\": \\"LOG_CRIT\\", \\"logoption\\": \\"LOG_CONS\\"}, {\\"message\\": \\"Temperature sensor overheating\\", \\"priority\\": \\"LOG_ALERT\\", \\"facility\\": \\"LOG_SYSLOG\\"} ] mask_priority = \\"LOG_NOTICE\\" result = custom_logger(configs, mask_priority) # expected output: {\'LOG_INFO\': 1, \'LOG_NOTICE\': 0, \'LOG_WARNING\': 1, \'LOG_ALERT\': 1, \'LOG_CRIT\': 1, \'LOG_ERR\': 0, \'LOG_DEBUG\': 0, \'LOG_EMERG\': 0} ``` Notes: - Ensure that `openlog`, `syslog`, `closelog`, and `setlogmask` are used appropriately within your function. - Remember to include proper error handling for invalid priority levels or configurations. - Do not forget to close the log using `closelog` after processing all configurations.","solution":"import syslog def custom_logger(configs, mask_priority): priority_mapping = { \\"LOG_EMERG\\": syslog.LOG_EMERG, \\"LOG_ALERT\\": syslog.LOG_ALERT, \\"LOG_CRIT\\": syslog.LOG_CRIT, \\"LOG_ERR\\": syslog.LOG_ERR, \\"LOG_WARNING\\": syslog.LOG_WARNING, \\"LOG_NOTICE\\": syslog.LOG_NOTICE, \\"LOG_INFO\\": syslog.LOG_INFO, \\"LOG_DEBUG\\": syslog.LOG_DEBUG, } facility_mapping = { \\"LOG_KERN\\": syslog.LOG_KERN, \\"LOG_USER\\": syslog.LOG_USER, \\"LOG_MAIL\\": syslog.LOG_MAIL, \\"LOG_DAEMON\\": syslog.LOG_DAEMON, \\"LOG_AUTH\\": syslog.LOG_AUTH, \\"LOG_LPR\\": syslog.LOG_LPR, \\"LOG_NEWS\\": syslog.LOG_NEWS, \\"LOG_UUCP\\": syslog.LOG_UUCP, \\"LOG_CRON\\": syslog.LOG_CRON, \\"LOG_SYSLOG\\": syslog.LOG_SYSLOG, \\"LOG_LOCAL0\\": syslog.LOG_LOCAL0, \\"LOG_LOCAL1\\": syslog.LOG_LOCAL1, \\"LOG_LOCAL2\\": syslog.LOG_LOCAL2, \\"LOG_LOCAL3\\": syslog.LOG_LOCAL3, \\"LOG_LOCAL4\\": syslog.LOG_LOCAL4, \\"LOG_LOCAL5\\": syslog.LOG_LOCAL5, \\"LOG_LOCAL6\\": syslog.LOG_LOCAL6, \\"LOG_LOCAL7\\": syslog.LOG_LOCAL7, } logoption_mapping = { \\"LOG_PID\\": syslog.LOG_PID, \\"LOG_CONS\\": syslog.LOG_CONS, \\"LOG_NDELAY\\": syslog.LOG_NDELAY, \\"LOG_ODELAY\\": syslog.LOG_ODELAY, \\"LOG_NOWAIT\\": syslog.LOG_NOWAIT, \\"LOG_PERROR\\": syslog.LOG_PERROR, } log_counts = {priority: 0 for priority in priority_mapping} try: syslog.setlogmask(syslog.LOG_UPTO(priority_mapping[mask_priority])) for config in configs: message = config.get(\\"message\\", \\"\\") priority = config.get(\\"priority\\", \\"LOG_INFO\\") facility = config.get(\\"facility\\", \\"LOG_USER\\") logoption = config.get(\\"logoption\\", 0) if isinstance(logoption, str): logoption = logoption_mapping.get(logoption, 0) facility_code = facility_mapping.get(facility, syslog.LOG_USER) priority_code = priority_mapping.get(priority, syslog.LOG_INFO) syslog.openlog(facility=facility_code, logoption=logoption) syslog.syslog(priority_code, message) syslog.closelog() log_counts[priority] += 1 except KeyError as e: raise ValueError(f\\"Invalid priority or facility: {e}\\") return log_counts"},{"question":"**Objective:** Demonstrate understanding of seaborn\'s `so.Plot` API, especially handling overlapping markers using `so.Dodge()`. # Problem Statement: You are given a dataset containing information about restaurant bills. Using seaborn\'s `so.Plot` API, you need to create a series of bar plots to visualize different aspects of the dataset while skillfully handling overlapping issues and maintaining clear visual distinctions. **Dataset:** The `tips` dataset from seaborn, which contains the following columns: - `total_bill`: Total bill amount (float) - `tip`: Tip amount (float) - `sex`: Gender of the person responsible for the bill (Male/Female) - `smoker`: Whether the party had smokers (Yes/No) - `day`: Day of the week (Thur/Fri/Sat/Sun) - `time`: Time of the day (Lunch/Dinner) - `size`: Size of the party (int) **Tasks:** 1. **Bar Plot for Total Bill by Day:** Create a bar plot showing the sum of `total_bill` for each `day`. Color bars by `time`. Use `so.Dodge()` to avoid overlapping bars. 2. **Fill Empty Spaces:** Modify the plot created in Task 1 to fill empty spaces where some days do not contain both `Lunch` and `Dinner` entries. 3. **Add Gaps Between Bars:** Create a bar plot showing the sum of `total_bill` for each `day`. Color bars by `sex` and add a small gap between the bars using the `so.Dodge(gap=.1)` parameter. 4. **Combining Multiple Semantic Variables:** Create a bar plot showing the count of bills for each `day`. Use both `sex` and `smoker` as semantic variables and utilize `so.Dodge()` to ensure no overlapping. Color bars by `sex` and fill by `smoker`. 5. **Subset Variables with `by`:** Create a bar plot showing the sum of the `total_bill` for each `day`. Use both `sex` and `smoker` as semantic variables, but only dodge by `sex`. **Constraints:** - Ensure that each plot is clear and does not have overlapping bar markers. - Use appropriate seaborn functions as demonstrated in the provided documentation snippet. **Expected Implementation:** ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Task 1: Bar plot for Total Bill by Day plot = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"time\\") plot.add(so.Bar(), so.Agg(sum), so.Dodge()) plt.show() # Task 2: Fill empty spaces plot = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"time\\") plot.add(so.Bar(), so.Agg(sum), so.Dodge(empty=\\"fill\\")) plt.show() # Task 3: Add gaps between bars plot = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") plot.add(so.Bar(), so.Agg(sum), so.Dodge(gap=0.1)) plt.show() # Task 4: Combining multiple semantic variables plot = so.Plot(tips, \\"day\\", color=\\"sex\\", fill=\\"smoker\\") plot.add(so.Bar(), so.Count(), so.Dodge()) plt.show() # Task 5: Subset variables with `by` plot = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\", fill=\\"smoker\\") plot.add(so.Bar(), so.Agg(sum), so.Dodge(by=[\\"color\\"])) plt.show() ``` In this assessment, you need to create the plots as described in the tasks and ensure that the plots are clear, with no overlapping bars.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def task1_plot_total_bill_by_day(): tips = load_dataset(\\"tips\\") plot = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"time\\") plot.add(so.Bar(), so.Agg(sum), so.Dodge()) plot.show() def task2_fill_empty_spaces(): tips = load_dataset(\\"tips\\") plot = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"time\\") plot.add(so.Bar(), so.Agg(sum), so.Dodge(empty=\\"fill\\")) plot.show() def task3_add_gaps_between_bars(): tips = load_dataset(\\"tips\\") plot = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") plot.add(so.Bar(), so.Agg(sum), so.Dodge(gap=0.1)) plot.show() def task4_combining_multiple_semantic_variables(): tips = load_dataset(\\"tips\\") plot = so.Plot(tips, \\"day\\", color=\\"sex\\", fill=\\"smoker\\") plot.add(so.Bar(), so.Count(), so.Dodge()) plot.show() def task5_subset_variables_with_by(): tips = load_dataset(\\"tips\\") plot = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\", fill=\\"smoker\\") plot.add(so.Bar(), so.Agg(sum), so.Dodge(by=[\\"color\\"])) plot.show()"},{"question":"<|Analysis Begin|> The provided documentation describes the `asyncio` library in Python, which is utilized for writing concurrent code using the `async` and `await` syntax. It outlines the major functionalities of `asyncio`: 1. High-level APIs: - Running coroutines concurrently. - Performing network IO and IPC. - Subprocess control. - Distributing tasks through queues. - Synchronizing concurrent code. 2. Low-level APIs: - Creating and managing event loops. - Implementing protocols using transports. - Bridging callback-based code with async/await syntax. The provided example demonstrates a simple asynchronous function that prints \\"Hello ...\\", waits for one second, and then prints \\"... World!\\". Additionally, the asyncio REPL allows experimenting with asynchronous contexts directly. Based on this information, we can formulate a question that assesses students on their ability to: 1. Define and run asynchronous coroutines. 2. Use `asyncio` to manage concurrent tasks. 3. Handle synchronization and task distribution. <|Analysis End|> <|Question Begin|> # AsyncIO Coding Assessment **Objective:** Create a Python program that simulates a simplified asynchronous task management system using the `asyncio` library. # Problem Statement: You are required to implement a function `process_tasks_concurrently(tasks, worker_count)` that processes a list of tasks using a specified number of concurrent workers. Each task is represented as a tuple `(task_name, duration)` where: - `task_name` is a string representing the name of the task. - `duration` is an integer representing the time in seconds the task takes to complete. The `process_tasks_concurrently` function should: 1. Initialize and run the specified number of worker coroutines concurrently. 2. Each worker coroutine should repeatedly: - Retrieve a task from the tasks queue. - Await for the duration of the task. - Print a message indicating the task\'s completion, e.g., `Worker 1 completed task \'A\' in 2 seconds`. 3. Ensure that tasks are processed efficiently and concurrently by the workers. # Function Signature: ```python import asyncio from typing import List, Tuple async def process_tasks_concurrently(tasks: List[Tuple[str, int]], worker_count: int) -> None: pass ``` # Input: - `tasks`: A list of tuples representing the tasks. Each tuple contains: - `task_name` (str): The name of the task. - `duration` (int): The time in seconds to complete the task. - `worker_count` (int): The number of worker coroutines to run concurrently. # Output: - The function does not return anything. It prints completion messages for each task. # Constraints: - The number of tasks is at least 1 and at most 100. - The duration of any task is at least 1 second and at most 10 seconds. - The number of workers is at least 1 and at most 10. # Example: ```python tasks = [(\\"A\\", 2), (\\"B\\", 3), (\\"C\\", 1)] worker_count = 2 # Expected Output: # Worker 1 completed task \'A\' in 2 seconds # Worker 2 completed task \'B\' in 3 seconds # Worker 1 completed task \'C\' in 1 second asyncio.run(process_tasks_concurrently(tasks, worker_count)) ``` # Implementation Notes: - Utilize asyncio features such as queues and concurrent tasks. - Ensure that workers efficiently retrieve and complete tasks from the queue. **Challenge:** Efficiently manage the concurrency and ensure that all tasks are processed without unnecessary delays.","solution":"import asyncio from typing import List, Tuple async def worker(worker_id: int, queue: asyncio.Queue): while not queue.empty(): task_name, duration = await queue.get() await asyncio.sleep(duration) print(f\\"Worker {worker_id} completed task \'{task_name}\' in {duration} seconds\\") queue.task_done() async def process_tasks_concurrently(tasks: List[Tuple[str, int]], worker_count: int): queue = asyncio.Queue() for task in tasks: queue.put_nowait(task) workers = [worker(i + 1, queue) for i in range(worker_count)] await asyncio.gather(*workers) await queue.join()"},{"question":"# Question: **Custom Set Operations** In this task, you will implement custom operations on sets using Python\'s C-API functions for sets and frozensets. You are required to write a function `custom_set_operations` that performs the following: 1. **Create a New Set**: Given an iterable, create a new set. 2. **Check Membership**: Check if certain elements belong to the created set. 3. **Add Elements**: Add multiple elements to the created set. 4. **Remove Elements**: Remove specified elements from the set. If an element does not exist, ignore the removal request for it without raising an exception. 5. **Size of Set**: Return the size of the set. Implement the function `custom_set_operations` that takes the following inputs: - `iterable`: An iterable containing the initial elements of the set. - `check_elements`: A list of elements to check for membership in the set. - `add_elements`: A list of elements to add to the set. - `remove_elements`: A list of elements to remove from the set. The function should return a tuple with the following: 1. A list of booleans where each boolean indicates if the corresponding element in `check_elements` is a member of the set (`True` for member, `False` for not a member). 2. The final size of the set after all operations have been performed. ```python def custom_set_operations(iterable, check_elements, add_elements, remove_elements): Perform custom operations on a set and return a detailed result. Parameters: - iterable: An iterable containing initial elements for the set. - check_elements: A list of elements to check for membership in the set. - add_elements: A list of elements to add to the set. - remove_elements: A list of elements to remove from the set. Returns: - tuple: A tuple containing: * A list of booleans indicating membership of each element in check_elements. * The size of the set after all operations have been performed. pass # Example usage: # print(custom_set_operations([1, 2, 3], [1, 4], [5, 6], [2, 6, 7])) # Output: ([True, False], 3) ``` # Constraints * `iterable` is guaranteed to be an iterable of hashable objects. * `check_elements`, `add_elements`, and `remove_elements` are lists of hashable objects. * The function should handle at most 10,000 elements in any case within reasonable time limits. Use the provided documentation as a guide to implement the required operations efficiently and handle all possible edge cases such as unhashable elements and other error conditions as specified.","solution":"def custom_set_operations(iterable, check_elements, add_elements, remove_elements): Perform custom operations on a set and return a detailed result. Parameters: - iterable: An iterable containing initial elements for the set. - check_elements: A list of elements to check for membership in the set. - add_elements: A list of elements to add to the set. - remove_elements: A list of elements to remove from the set. Returns: - tuple: A tuple containing: * A list of booleans indicating membership of each element in check_elements. * The size of the set after all operations have been performed. # Create a set from the iterable my_set = set(iterable) # Check membership for each element in check_elements memberships = [element in my_set for element in check_elements] # Add elements to the set for element in add_elements: my_set.add(element) # Remove elements from the set, ignore if element is not present for element in remove_elements: my_set.discard(element) # discard does not raise an exception if the element does not exist # Get the final size of the set final_size = len(my_set) return (memberships, final_size)"},{"question":"# Coding Assessment Objective You are tasked with implementing and optimizing a simple machine learning algorithm using guidelines provided in the scikit-learn performance optimization documentation. The purpose of this assessment is to evaluate your understanding of performance optimization techniques and your ability to apply them. Task 1. **Implementation:** - Implement the **K-Nearest Neighbors (KNN)** algorithm from scratch using Python and Numpy. - Ensure the implementation avoids Python loops wherever possible by using vectorized Numpy operations. 2. **Optimization:** - Profile the initial implementation to identify performance bottlenecks. - Use Cython to optimize the identified bottlenecks, re-implementing these sections to improve performance. - Provide a performance comparison between the initial and optimized implementations. Requirements 1. **KNN Implementation:** - Input: `X_train` (numpy array of shape (n_samples, n_features)), `y_train` (numpy array of shape (n_samples,)), `X_test` (numpy array of shape (m_samples, n_features)), `k` (int) - Output: `y_pred` (numpy array of shape (m_samples,)) predicting the class labels for `X_test` - Constraints: - `X_train` and `X_test` can be quite large, so the implementation should be memory efficient. - The algorithm should handle ties by returning the smallest class label. 2. **Profiling:** - Use tools such as `%timeit`, `line_profiler`, or `memory_profiler` to measure the performance and memory usage of your initial Python implementation. - Identify the most time-consuming parts of the code. 3. **Optimization with Cython:** - Optimize the identified bottlenecks using Cython. - Ensure the optimized parts integrate seamlessly with the non-optimized parts of your implementation. - Validate consistency with the initial implementation using test cases. 4. **Comparison:** - Provide a detailed performance comparison (time and memory usage) between the original and optimized implementations. Expected Solution 1. Python implementation of KNN using Numpy. 2. Profiling analysis (code snippets and results). 3. Cython-optimized code sections. 4. Performance comparison results. 5. Verification of the optimized KNN algorithm for correct functionality using test cases. **Submission Format:** - A Jupyter notebook containing: - All the code implementations (initial and optimized). - Profiling and optimization analysis. - Performance comparison results. - Verification results. This question requires a thorough understanding of the scikit-learn optimization guidelines, efficient use of Numpy for vectorized operations, and the ability to profile and optimize performance-critical sections using Cython.","solution":"import numpy as np def knn_predict(X_train, y_train, X_test, k): Perform K-Nearest Neighbors prediction. Parameters: X_train (numpy.ndarray): Training data, shape (n_samples, n_features) y_train (numpy.ndarray): Training labels, shape (n_samples,) X_test (numpy.ndarray): Test data, shape (m_samples, n_features) k (int): Number of neighbors to consider Returns: numpy.ndarray: Predicted labels for X_test, shape (m_samples,) # Calculate distances between X_test and X_train distances = np.sqrt(((X_test[:, np.newaxis, :] - X_train[np.newaxis, :, :]) ** 2).sum(axis=2)) # Get the indices of the k-nearest neighbors nearest_neighbor_indices = np.argsort(distances, axis=1)[:, :k] # Get the labels of the k-nearest neighbors nearest_neighbor_labels = y_train[nearest_neighbor_indices] # Predict the most common label among the k-nearest neighbors y_pred = np.array([np.argmax(np.bincount(labels)) for labels in nearest_neighbor_labels]) return y_pred"},{"question":"**Problem Statement:** You are given a Python function that performs some computational tasks. Your task is to profile the performance of this function using both `cProfile` and `pstats` modules, gather execution statistics, and analyze the results to identify performance bottlenecks. **Steps to Follow:** 1. Profile the given function using the `cProfile` module. 2. Save the profiling results to a file. 3. Use the `pstats` module to read the profiling data from the file. 4. Analyze the profiling data to identify the top 5 functions by cumulative time and internal time. 5. Print out the profiling report that includes these top 5 functions sorted by cumulative time and internal time. **Function to Profile:** Here is the function you need to profile: ```python import time def example_function(): def compute_sum(n): total = 0 for i in range(n): total += i return total def compute_product(n): product = 1 for i in range(1, n): product *= i return product compute_sum(100000) compute_product(1000) ``` **Requirements:** 1. Implement a Python script that performs the profiling. 2. Save the profiling results to a file named `profile_results.prof`. 3. Use the `pstats` module to load and analyze the profiling data. 4. Print the profiling report with the top 5 functions by cumulative time and internal time, with clearly labeled sections in your output. **Hints:** - Use the `cProfile.run()` function to profile the execution of `example_function`. - Use the `pstats.Stats` class to read and analyze the profiling data. - Utilize the `sort_stats()` and `print_stats()` methods to sort and print the profiling data. - Consider using context managers to profile sections of your code for better readability. **Constraints:** - You must use the `cProfile` and `pstats` modules as demonstrated in the documentation. - The profiling results must be saved to `profile_results.prof`. **Example Output:** Your script should output something like the following (the actual numbers may vary): ``` --- Top 5 functions by cumulative time --- ncalls tottime percall cumtime percall filename:lineno(function) 1 0.002 0.002 0.005 0.005 <ipython-input-1-example>:16(compute_sum) 1 0.002 0.002 0.002 0.002 <ipython-input-1-example>:22(compute_product) 1 0.000 0.000 0.005 0.005 <ipython-input-1-example>:1(example_function) ... --- Top 5 functions by internal time --- ncalls tottime percall cumtime percall filename:lineno(function) 1 0.002 0.002 0.002 0.002 <ipython-input-1-example>:16(compute_sum) 1 0.002 0.002 0.002 0.002 <ipython-input-1-example>:22(compute_product) 1 0.000 0.000 0.005 0.005 <ipython-input-1-example>:1(example_function) ... ``` Good luck! Your understanding and application of profiling tools will be crucial for optimizing performance-critical applications.","solution":"import cProfile import pstats def profile_example_function(): def example_function(): def compute_sum(n): total = 0 for i in range(n): total += i return total def compute_product(n): product = 1 for i in range(1, n): product *= i return product compute_sum(100000) compute_product(1000) # Profile the example function profiler = cProfile.Profile() profiler.enable() example_function() profiler.disable() # Save profiling data to a file profiler.dump_stats(\'profile_results.prof\') # Use pstats to load and analyze the profiling data pstats_stats = pstats.Stats(\'profile_results.prof\') # Print out the profiling report with the top 5 functions by cumulative time print(\'--- Top 5 functions by cumulative time ---\') pstats_stats.sort_stats(\'cumulative\').print_stats(5) # Print out the profiling report with the top 5 functions by internal time print(\'--- Top 5 functions by internal time ---\') pstats_stats.sort_stats(\'time\').print_stats(5) if __name__ == \\"__main__\\": profile_example_function()"},{"question":"# Question: Managing Accelerator Devices in PyTorch **Description:** Implement a function `manage_accelerators` that performs the following tasks using the PyTorch `torch.accelerator` module: 1. Check if accelerator devices are available. 2. If they are available, switch to the device with index 1 (if there is more than one device). 3. Retrieve and return the currently active device index. 4. Synchronize the active device to ensure all operations are complete. 5. Finally, return the number of accelerator devices along with the index of the currently active device. **Function Signature:** ```python def manage_accelerators() -> tuple: pass ``` **Expected Input/Output:** - Input: None - Output: A tuple `(int, int)` where the first integer is the number of available accelerator devices, and the second integer is the index of the currently active device after switching. **Constraints:** - Assume the functions in the `torch.accelerator` module are correctly implemented and accessible. **Performance Requirements:** - The function should handle the device operations efficiently and ensure that the device synchronization does not introduce unnecessary delays. --- **Example Usage:** ```python num_devices, active_device_idx = manage_accelerators() print(f\\"Number of devices: {num_devices}\\") print(f\\"Active device index: {active_device_idx}\\") ``` **Notes:** - Ensure to handle cases where there are no accelerator devices available, and return the appropriate active device index in such scenarios. - Assume that the device index is zero-based. **Additional Information:** - You may find it helpful to refer to the PyTorch `torch.accelerator` module\'s documentation for detailed descriptions of the functions used in this task.","solution":"import torch def manage_accelerators() -> tuple: Manages the accelerator devices, retrieves the currently active device index, and ensures synchronization of the device. Returns: tuple: (number of available devices, index of the currently active device) if torch.has_cuda: num_devices = torch.cuda.device_count() if num_devices > 1: torch.cuda.set_device(1) active_device = torch.cuda.current_device() torch.cuda.synchronize() else: num_devices = 0 active_device = -1 return num_devices, active_device"},{"question":"# Python Bytecode Analysis As a programming instructor, you\'re aware that understanding bytecode can provide deep insights into how Python code is executed and optimized. One useful application of bytecode analysis is to track the usage of local variables within a function. For this exercise, you\'ll write a function that analyzes another function\'s bytecode to determine which local variables are accessed, assigned, and how often they are used. The `dis` module in Python allows you to disassemble Python bytecode and provides tools for bytecode analysis. You will use the `dis` module to implement the function `analyze_locals`, which should return a detailed report on the usage of local variables in the bytecode of a given function. # Task Implement the function `analyze_locals(func: Callable) -> Dict[str, Dict[str, int]]` that takes a function `func` and returns a dictionary where each key is the name of a local variable, and the corresponding value is another dictionary with the keys `\'LOAD\'`, `\'STORE\'`, and `\'DELETE\'`, indicating the number of occurrences of each operation on that variable. # Example Consider the following function: ```python def example_function(x, y, z): a = x + y b = a * z return b ``` Analyzing its bytecode should produce a result similar to this: ```python { \\"x\\": {\\"LOAD\\": 1, \\"STORE\\": 0, \\"DELETE\\": 0}, \\"y\\": {\\"LOAD\\": 1, \\"STORE\\": 0, \\"DELETE\\": 0}, \\"z\\": {\\"LOAD\\": 1, \\"STORE\\": 0, \\"DELETE\\": 0}, \\"a\\": {\\"LOAD\\": 1, \\"STORE\\": 1, \\"DELETE\\": 0}, \\"b\\": {\\"LOAD\\": 1, \\"STORE\\": 1, \\"DELETE\\": 0}, } ``` # Constraints and Notes 1. You may assume that the function passed to `analyze_locals` does not contain nested functions or lambda expressions. 2. Do not use any external libraries other than the Python standard library. 3. Pay attention to handling all cases for loading, storing, and deleting variables. 4. The function should handle and analyze all instructions related to local variables. # Hints - Use the `dis.get_instructions` function to iterate over the bytecode instructions of the function. - `dis.Instruction` objects have useful attributes like `opname`, `arg`, and `argval` that you\'ll need to inspect. - Focus on the opcodes: `LOAD_FAST`, `STORE_FAST`, and `DELETE_FAST` for local variable operations. # Implementation Here is the function signature you should use: ```python from typing import Callable, Dict import dis def analyze_locals(func: Callable) -> Dict[str, Dict[str, int]]: pass ``` Implement the function `analyze_locals` to complete this task.","solution":"from typing import Callable, Dict import dis def analyze_locals(func: Callable) -> Dict[str, Dict[str, int]]: Analyzes the bytecode of a function to determine the usage of local variables. Parameters: func (Callable): The function to be analyzed. Returns: Dict[str, Dict[str, int]]: A dictionary where each key is the name of a local variable, and the value is a dictionary with keys \'LOAD\', \'STORE\', \'DELETE\', indicating the number of occurrences of each operation on that variable. local_usage = {} # Iterate over the bytecode instructions of the function for instr in dis.get_instructions(func): if instr.opname in (\'LOAD_FAST\', \'STORE_FAST\', \'DELETE_FAST\'): var_name = instr.argval if var_name not in local_usage: local_usage[var_name] = {\'LOAD\': 0, \'STORE\': 0, \'DELETE\': 0} if instr.opname == \'LOAD_FAST\': local_usage[var_name][\'LOAD\'] += 1 elif instr.opname == \'STORE_FAST\': local_usage[var_name][\'STORE\'] += 1 elif instr.opname == \'DELETE_FAST\': local_usage[var_name][\'DELETE\'] += 1 return local_usage"},{"question":"You are required to design and implement a function using the Seaborn library. Your function should generate and display color palettes based on given specifications. Task Implement a function `display_custom_palette(colors, n_colors=10, as_cmap=False)` that takes the following parameters: - `colors`: A list of color specifications. The list can be arbitrarily long, and colors can be in format recognized by Seaborn (e.g., named colors, hex codes). - `n_colors`: An optional integer specifying the number of colors to interpolate (default is 10). - `as_cmap`: An optional boolean to indicate whether the function should return a continuous colormap (default is False). The function should perform the following operations: 1. Create a color palette by interpolating between the colors specified in the `colors` list. 2. Display the palette generated using Seaborn\'s palette display functions. 3. If `as_cmap` is True, display the continuous colormap instead of a discrete color palette. 4. Handle any edge cases, such as empty input color lists or invalid color specifications, by raising appropriate exceptions with informative messages. Input Format: - A list of strings representing the color specifications `colors`. - An optional integer `n_colors` (default value = 10). - An optional boolean `as_cmap` (default value = False). Output Format: - The function should display the generated palette or colormap using Seaborn\'s built-in display functions. Constraints: - Use the Seaborn library to create and display the palettes. - Ensure the implementation handles invalid inputs gracefully. # Example Usage: Here are some example calls to the function you should implement: 1. **Example 1:** ```python display_custom_palette([\\"red\\", \\"green\\", \\"blue\\"]) ``` This should display a palette interpolating between red, green, and blue with the default number of colors (10). 2. **Example 2:** ```python display_custom_palette([\\"#ff0000\\", \\"#00ff00\\"], n_colors=5) ``` This should display a palette interpolating between the red and green hex codes with 5 colors. 3. **Example 3:** ```python display_custom_palette([\\"navy\\", \\"skyblue\\"], as_cmap=True) ``` This should display a continuous colormap interpolating between navy and skyblue. 4. **Example 4:** ```python display_custom_palette([], n_colors=5) ``` This should raise an exception indicating that the color list cannot be empty. # Evaluation Criteria Your solution will be evaluated based on: - Correctness and robustness of the implementation. - Proper handling of edge cases. - Adherence to the input and output format. - Code readability and documentation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def display_custom_palette(colors, n_colors=10, as_cmap=False): Generate and display a color palette based on given specifications. Parameters: colors (list): A list of color specifications. n_colors (int): Number of colors to interpolate. Default is 10. as_cmap (bool): Whether to return a continuous colormap. Default is False. Raises: ValueError: If the colors list is empty or contains invalid color specifications. if not colors: raise ValueError(\\"The color list cannot be empty.\\") try: if as_cmap: cmap = sns.color_palette(colors, as_cmap=True) sns.palplot(sns.color_palette(cmap, n_colors)) else: palette = sns.color_palette(colors, n_colors) sns.palplot(palette) plt.show() except ValueError as e: raise ValueError(\\"One or more color specifications are invalid.\\") from e"},{"question":"You are provided with a prototype module in PyTorch called `torch.cuda.tunable`. This module provides various functionalities to enable and manage the tuning of CUDA operations. # Objective: Write a function `configure_and_tune` which configures tuning parameters, enables tuning, executes a dummy tuning operation, and retrieves the tuning results. # Function Signature: ```python def configure_and_tune(filename: str, max_duration: int, max_iterations: int) -> dict: Configures tuning parameters, enables tuning, simulates a dummy tuning operation, and retrieves the results. Args: - filename (str): The name of the file to record the tuning results. - max_duration (int): Maximum duration (in seconds) for each tuning operation. - max_iterations (int): Maximum number of iterations for the tuning process. Returns: - A dictionary containing the results of the tuning operation. ``` # Requirements: 1. **Configure Tuning Parameters**: - Set maximum tuning duration using `set_max_tuning_duration`. - Set maximum tuning iterations using `set_max_tuning_iterations`. - Set the filename for recording tuning results using `set_filename`. 2. **Enable Tuning**: - Use the `enable` function to enable tuning. 3. **Simulate Dummy Tuning Operation**: - Since this is a prototype, the actual tuning operation is not specified. - Use the `tune_gemm_in_file` function with the filename to simulate a tuning operation. 4. **Retrieve and Return Results**: - Retrieve the tuning results using `get_results`. # Constraints: - The function should handle any exceptions that might occur during configuration or tuning. - The function should ensure tuning is disabled if it was enabled at the start of the function. # Example: ```python # Suppose you named your file \\"tuning_results.txt\\" and you want a max duration of 10 seconds and 100 iterations. results = configure_and_tune(\\"tuning_results.txt\\", 10, 100) print(results) # Expected Example Output: # {\'result_key_1\': \'value1\', \'result_key_2\': \'value2\', ...} ``` # Notes: - The actual content of the results will depend on the dummy tuning operation\'s implementation within the `torch.cuda.tunable` module.","solution":"import torch.cuda.tunable as tunable def configure_and_tune(filename: str, max_duration: int, max_iterations: int) -> dict: Configures tuning parameters, enables tuning, simulates a dummy tuning operation, and retrieves the results. Args: - filename (str): The name of the file to record the tuning results. - max_duration (int): Maximum duration (in seconds) for each tuning operation. - max_iterations (int): Maximum number of iterations for the tuning process. Returns: - A dictionary containing the results of the tuning operation. try: # Configure Tuning Parameters tunable.set_max_tuning_duration(max_duration) tunable.set_max_tuning_iterations(max_iterations) tunable.set_filename(filename) # Enable Tuning tunable.enable() # Simulate Dummy Tuning Operation tunable.tune_gemm_in_file(filename) # Retrieve and Return Results results = tunable.get_results() return results except Exception as e: # Handle any exceptions that might occur during configuration or tuning print(f\\"An error occurred: {e}\\") return {} finally: # Ensure tuning is disabled tunable.disable()"},{"question":"Profiling and Optimizing a PyTorch Model **Objective:** This question assesses your ability to use `torch.profiler` to analyze and optimize the performance of a PyTorch model by diagnosing and fixing performance bottlenecks. **Problem Statement:** You are given a PyTorch model defined in the `ModelWithBreaks` class. This model has intentional graph breaks which cause performance issues. Your tasks are to: 1. **Profile the model execution using `torch.profiler`** to capture performance data. 2. **Analyze the profiling results** to identify graph breaks and other performance bottlenecks. 3. **Modify the model** to remove the graph breaks and optimize its performance. 4. **Validate the performance improvement** by profiling the optimized model. # Instructions: 1. **Implement Profiling:** Write a function `profile_model` that profiles a given model and saves the profiling data to a JSON file. ```python import torch import torch.profiler from model_with_breaks import ModelWithBreaks # Assume model_with_breaks.py contains the ModelWithBreaks definition def profile_model(model, inputs, output_file): def fwd_bwd(inp): out = model(inp) out.sum().backward() # Warm up fwd_bwd(inputs[0]) with torch.profiler.profile() as prof: for i in range(1, 4): fwd_bwd(inputs[i]) prof.step() prof.export_chrome_trace(output_file) # Example usage: if __name__ == \\"__main__\\": model = ModelWithBreaks().cuda() inputs = [torch.randn((128, 128), device=\'cuda\') for _ in range(10)] model_c = torch.compile(model) profile_model(model_c, inputs, \\"trace_original.json\\") ``` 2. **Analyze Profiling Results:** Open the `trace_original.json` in `chrome://tracing` and analyze the trace. Identify the graph breaks and other performance issues. **Hint:** Look for nested Torch-Compiled Region events and CompiledFunction events. 3. **Modify the Model:** Refactor the `ModelWithBreaks` class to remove the intentional graph breaks. Ensure that the model computations are performed in a single compiled graph. ```python class OptimizedModel(torch.nn.Module): def __init__(self): super().__init__() self.sequential = torch.nn.Sequential( torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128), torch.nn.ReLU(), ) def forward(self, inp): return self.sequential(inp) ``` 4. **Validate Performance Improvement:** Profile the optimized model and compare the results with the original model. Validate the performance improvement by examining the trace. ```python if __name__ == \\"__main__\\": model = OptimizedModel().cuda() inputs = [torch.randn((128, 128), device=\'cuda\') for _ in range(10)] model_c = torch.compile(model) profile_model(model_c, inputs, \\"trace_optimized.json\\") ``` **Submission:** - Implement and run the profiling code for both the original and optimized models. - Provide the JSON files (`trace_original.json` and `trace_optimized.json`) along with a brief report comparing the performance before and after optimization. **Constraints:** - Use PyTorch and torchvision packages. - Perform the tasks on a CUDA-enabled device. **Performance Requirements**: - Ensure that the optimized model runs faster and more efficiently, with reduced graph breaks.","solution":"import torch from torch.profiler import profile, ProfilerActivity class ModelWithBreaks(torch.nn.Module): def __init__(self): super().__init__() self.layer1 = torch.nn.Linear(128, 128) self.layer2 = torch.nn.Linear(128, 128) self.layer3 = torch.nn.Linear(128, 128) self.layer4 = torch.nn.Linear(128, 128) def forward(self, x): x = self.layer1(x) torch.cuda.synchronize() # Intentional break x = self.layer2(torch.relu(x)) torch.cuda.synchronize() # Intentional break x = self.layer3(torch.relu(x)) torch.cuda.synchronize() # Intentional break x = self.layer4(torch.relu(x)) return x def profile_model(model, inputs, output_file): Profiles the model execution and saves data to a JSON file. def fwd_bwd(inp): out = model(inp) out.sum().backward() # Warm up fwd_bwd(inputs[0]) with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], schedule=torch.profiler.schedule(wait=1, warmup=1, active=3)) as prof: for i in range(1, 5): fwd_bwd(inputs[i]) prof.step() prof.export_chrome_trace(output_file) class OptimizedModel(torch.nn.Module): def __init__(self): super().__init__() self.sequential = torch.nn.Sequential( torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128), torch.nn.ReLU(), ) def forward(self, x): return self.sequential(x) if __name__ == \\"__main__\\": cuda_device = \\"cuda\\" if torch.cuda.is_available() else \\"cpu\\" model_with_breaks = ModelWithBreaks().to(cuda_device) inputs = [torch.randn((128, 128), device=cuda_device) for _ in range(10)] profile_model(model_with_breaks, inputs, \\"trace_original.json\\") optimized_model = OptimizedModel().to(cuda_device) profile_model(optimized_model, inputs, \\"trace_optimized.json\\")"},{"question":"**Problem Statement:** You are developing a secure communication system that transfers binary data over ASCII-only channels. To achieve this, you will use various encoding schemes provided by the `base64` module. Implement a Python function `custom_base64_process(data: bytes, mode: str, altchars: Optional[bytes] = None, validate: bool = False) -> bytes`. This function will take a bytes object and perform encoding/decoding based on the specified mode. It will select among the following encoding schemes: - \\"b64encode\\": Base64 encoding - \\"b64decode\\": Base64 decoding - \\"urlsafe_b64encode\\": URL-safe Base64 encoding - \\"urlsafe_b64decode\\": URL-safe Base64 decoding - \\"standard_b64encode\\": Standard Base64 encoding - \\"standard_b64decode\\": Standard Base64 decoding **Function Signature:** ```python def custom_base64_process(data: bytes, mode: str, altchars: Optional[bytes] = None, validate: bool = False) -> bytes: ``` **Parameters:** - `data (bytes)`: The input binary data to be encoded or decoded. - `mode (str)`: The operation mode, which can be one of the following: \\"b64encode\\", \\"b64decode\\", \\"urlsafe_b64encode\\", \\"urlsafe_b64decode\\", \\"standard_b64encode\\", \\"standard_b64decode\\". - `altchars (Optional[bytes])`: Optional 2-byte sequence to replace characters in Base64 encoding. - `validate (bool)`: Validation flag that affects the behavior of \\"b64decode\\" and \\"urlsafe_b64decode\\". **Returns:** - `bytes`: The resulting encoded or decoded bytes. **Constraints:** - `altchars`, if provided, must be a bytes-like object of length 2. - If `validate` is True and mode is decoding, the function should validate the input string as described in the documentation of `b64decode` and `urlsafe_b64decode`. **Examples:** ```python # Example 1: data = b\'data to be encoded\' mode = \'b64encode\' assert custom_base64_process(data, mode) == b\'ZGF0YSB0byBiZSBlbmNvZGVk\' # Example 2: encoded_data = b\'ZGF0YSB0byBiZSBlbmNvZGVk\' mode = \'b64decode\' assert custom_base64_process(encoded_data, mode) == b\'data to be encoded\' # Example 3: data = b\'data to be encoded\' mode = \'urlsafe_b64encode\' assert custom_base64_process(data, mode) == b\'ZGF0YSB0byBiZSBlbmNvZGVk\' # Example 4: encoded_data = b\'ZGF0YSB0byBiZSBlbmNvZGVk\' mode = \'urlsafe_b64decode\' assert custom_base64_process(encoded_data, mode) == b\'data to be encoded\' # Example 5: data = b\'data to be encoded\' mode = \'standard_b64encode\' assert custom_base64_process(data, mode) == b\'ZGF0YSB0byBiZSBlbmNvZGVk\' # Example 6: encoded_data = b\'ZGF0YSB0byBiZSBlbmNvZGVk\' mode = \'standard_b64decode\' assert custom_base64_process(encoded_data, mode) == b\'data to be encoded\' ``` **Note:** - Your function should handle invalid mode values gracefully by raising a `ValueError`. - Make sure to use the appropriate `base64` module functions based on the mode provided.","solution":"import base64 from typing import Optional def custom_base64_process(data: bytes, mode: str, altchars: Optional[bytes] = None, validate: bool = False) -> bytes: if mode not in [\\"b64encode\\", \\"b64decode\\", \\"urlsafe_b64encode\\", \\"urlsafe_b64decode\\", \\"standard_b64encode\\", \\"standard_b64decode\\"]: raise ValueError(f\\"Invalid mode: {mode}\\") if altchars and len(altchars) != 2: raise ValueError(\\"altchars must be a bytes-like object of length 2\\") if mode == \\"b64encode\\": return base64.b64encode(data, altchars) if altchars else base64.b64encode(data) elif mode == \\"b64decode\\": return base64.b64decode(data, altchars=altchars, validate=validate) if altchars else base64.b64decode(data, validate=validate) elif mode == \\"urlsafe_b64encode\\": return base64.urlsafe_b64encode(data) elif mode == \\"urlsafe_b64decode\\": return base64.urlsafe_b64decode(data) elif mode == \\"standard_b64encode\\": return base64.standard_b64encode(data) elif mode == \\"standard_b64decode\\": return base64.standard_b64decode(data) # This line should normally not be reached due to the initial mode validation. raise ValueError(f\\"Invalid mode: {mode}\\")"},{"question":"**Question: Implementing Custom Cross-Validation and Performance Evaluation with Advanced Metrics** You are provided with the famous Iris dataset, and your task is to implement a custom cross-validation routine using scikit-learn to evaluate the performance of a linear Support Vector Machine (SVM) classifier. Additionally, you must compute and report multiple performance metrics, including precision, recall, and F1-score for both training and testing sets. # Requirements: 1. **Data Preparation**: - Load the Iris dataset using `datasets.load_iris()` from scikit-learn. - Split the dataset into features `X` and target `y`. 2. **Cross-Validation**: - Implement a 5-fold cross-validation using the `KFold` strategy. - Ensure that the splitting maintains the class distribution by using `StratifiedKFold`. 3. **Model Training and Evaluation**: - For each fold, train a linear SVM classifier (`svm.SVC` with `kernel=\'linear\'` and `C=1`). - Compute the following metrics for training and testing data for each fold: - Precision - Recall - F1-score 4. **Reporting Metrics**: - Report the mean and standard deviation for each of the following metrics across the 5 folds: - Training Precision - Training Recall - Training F1-score - Testing Precision - Testing Recall - Testing F1-score 5. **Code Implementation**: - Wrap your implementation in a function `evaluate_model_with_cross_validation`. - The function should return a dictionary containing the mean and standard deviation of each metric. # Input: - No direct input; the function should internally load the Iris dataset. # Output: - A dictionary with the following structure: ```python { \'train_precision_mean\': float, \'train_precision_std\': float, \'train_recall_mean\': float, \'train_recall_std\': float, \'train_f1_mean\': float, \'train_f1_std\': float, \'test_precision_mean\': float, \'test_precision_std\': float, \'test_recall_mean\': float, \'test_recall_std\': float, \'test_f1_mean\': float, \'test_f1_std\': float } ``` # Constraints: - You must use the `StratifiedKFold` strategy for cross-validation to maintain class distributions. - Use `svm.SVC` with `kernel=\'linear\'` and `C=1` for the classifier. - Compute the metrics using scikit-learn\'s `metrics` module. # Example Function Signature: ```python def evaluate_model_with_cross_validation(): # Your implementation here pass # Example usage: results = evaluate_model_with_cross_validation() print(results) ``` Your implementation should demonstrate your understanding of advanced cross-validation and performance evaluation techniques using scikit-learn.","solution":"from sklearn import datasets from sklearn.model_selection import StratifiedKFold from sklearn import svm from sklearn.metrics import precision_score, recall_score, f1_score import numpy as np from collections import defaultdict def evaluate_model_with_cross_validation(): # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Initialize StratifiedKFold cross-validator skf = StratifiedKFold(n_splits=5) # Initialize lists to store the metrics for each fold train_precision_scores = [] train_recall_scores = [] train_f1_scores = [] test_precision_scores = [] test_recall_scores = [] test_f1_scores = [] # Perform cross-validation for train_index, test_index in skf.split(X, y): X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] # Initialize the SVM classifier with a linear kernel classifier = svm.SVC(kernel=\'linear\', C=1) classifier.fit(X_train, y_train) # Predict for training and test sets y_train_pred = classifier.predict(X_train) y_test_pred = classifier.predict(X_test) # Calculate precision, recall, and F1-score for training set train_precision = precision_score(y_train, y_train_pred, average=\'macro\') train_recall = recall_score(y_train, y_train_pred, average=\'macro\') train_f1 = f1_score(y_train, y_train_pred, average=\'macro\') # Calculate precision, recall, and F1-score for test set test_precision = precision_score(y_test, y_test_pred, average=\'macro\') test_recall = recall_score(y_test, y_test_pred, average=\'macro\') test_f1 = f1_score(y_test, y_test_pred, average=\'macro\') # Append the metrics to the respective lists train_precision_scores.append(train_precision) train_recall_scores.append(train_recall) train_f1_scores.append(train_f1) test_precision_scores.append(test_precision) test_recall_scores.append(test_recall) test_f1_scores.append(test_f1) # Calculate mean and standard deviation for each metric results = { \'train_precision_mean\': np.mean(train_precision_scores), \'train_precision_std\': np.std(train_precision_scores), \'train_recall_mean\': np.mean(train_recall_scores), \'train_recall_std\': np.std(train_recall_scores), \'train_f1_mean\': np.mean(train_f1_scores), \'train_f1_std\': np.std(train_f1_scores), \'test_precision_mean\': np.mean(test_precision_scores), \'test_precision_std\': np.std(test_precision_scores), \'test_recall_mean\': np.mean(test_recall_scores), \'test_recall_std\': np.std(test_recall_scores), \'test_f1_mean\': np.mean(test_f1_scores), \'test_f1_std\': np.std(test_f1_scores) } return results"},{"question":"**Coding Assessment Question: Unsupervised Learning with Scikit-learn** **Objective**: Demonstrate your understanding and application of unsupervised learning techniques using scikit-learn. **Problem Statement**: You are provided with a dataset containing information about various types of wines, including features derived from chemical analysis. Your task is to implement an unsupervised learning approach to cluster the wines into different groups, based on their features. You will use scikit-learn to achieve this. **Dataset**: The dataset consists of the following columns: - `Alcohol`, `Malic_Acid`, `Ash`, `Alcalinity_Of_Ash`, `Magnesium`, etc. (total of 13 continuous features) Each row in the dataset represents a different wine sample. **Tasks**: 1. **Load the dataset**: You can use any method to load the dataset (e.g., from a CSV file). 2. **Preprocessing**: Perform any necessary preprocessing steps such as normalization or standardization. 3. **Dimensionality Reduction**: Use Principal Component Analysis (PCA) to reduce the dimensionality of the data to 2 components for visualization purposes. 4. **Clustering**: Apply K-Means clustering to the preprocessed data to group the wines into clusters. 5. **Evaluation and Visualization**: - Evaluate the clustering by plotting the reduced 2D data points, colored by their cluster assignments. - Discuss the results, including the choice of the number of clusters and the potential interpretability of the clusters. **Implementation**: 1. **Function Name**: `cluster_wines` 2. **Input**: - A pandas DataFrame named `wine_data` containing the wine dataset with 13 features. - An integer `n_clusters` representing the number of clusters to form. 3. **Output**: - A tuple containing the following elements: - A numpy array of shape (n_samples, 2) representing the 2D coordinates of the wine samples after PCA transformation. - A numpy array of shape (n_samples,) representing the cluster labels for each wine sample. **Constraints**: - Use `scikit-learn` for PCA and K-Means clustering. - Ensure that your code is well-organized and functions are properly documented. **Example Usage**: ```python import pandas as pd # Example data loading (Replace this with actual data loading code) wine_data = pd.read_csv(\'wine.csv\') # Example function call pca_data, cluster_labels = cluster_wines(wine_data, n_clusters=3) # Example plotting (Not part of the function, for evaluation purposes only) import matplotlib.pyplot as plt plt.scatter(pca_data[:, 0], pca_data[:, 1], c=cluster_labels, cmap=\'viridis\') plt.xlabel(\'PCA Component 1\') plt.ylabel(\'PCA Component 2\') plt.title(\'Wine Clusters\') plt.show() ``` **Notes**: - Ensure that you handle edge cases, such as when the dataset contains missing values. - You can add more functions if necessary to modularize the code.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import KMeans def cluster_wines(wine_data, n_clusters): Clusters the wines into different groups based on their features using K-Means clustering after reducing the dimensionality with PCA. Parameters: - wine_data: pandas DataFrame, dataset containing 13 features of wines - n_clusters: int, number of clusters for K-Means Returns: - pca_data: numpy array of shape (n_samples, 2), 2D coordinates of the wine samples after PCA transformation - cluster_labels: numpy array of shape (n_samples,), cluster labels for each wine sample # Validate input if not isinstance(wine_data, pd.DataFrame): raise ValueError(\\"wine_data must be a pandas DataFrame\\") if not isinstance(n_clusters, int) or n_clusters <= 0: raise ValueError(\\"n_clusters must be a positive integer\\") # Preprocessing - Standardization features = wine_data.columns scaler = StandardScaler() standardized_data = scaler.fit_transform(wine_data[features]) # Dimensionality Reduction - PCA pca = PCA(n_components=2) pca_data = pca.fit_transform(standardized_data) # Clustering - KMeans kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(standardized_data) return pca_data, cluster_labels"},{"question":"# Question You are given a dataset on penguins that includes various measurements for different species. Your task is to use Seaborn\'s `JointGrid` to visualize the relationship between `bill_length_mm` and `bill_depth_mm`, with some customizations. The goal of this exercise is to assess your understanding of the Seaborn package and your capability to use `JointGrid` effectively. **Instructions:** 1. Load the penguins dataset using Seaborn. 2. Initialize a `JointGrid` object to visualize the relationship between `bill_length_mm` and `bill_depth_mm`. 3. Plot the joint and marginal axes using `scatterplot` and `histplot`. 4. Customize the scatter plot with the following attributes: - Points with an edge color of blue (`ec=\\"b\\"`). - Fill color should be none (`fc=\\"none\\"`). - Size of points should be 100 (`s=100`). - Line width of points should be 1.5 (`linewidth=1.5`). 5. Customize the histograms on the marginal axes with the following attributes: - No fill (`fill=False`). - Line width of 2 (`linewidth=2`). 6. Add a vertical reference line at `bill_length_mm = 45` and a horizontal reference line at `bill_depth_mm = 16`. ```python import seaborn as sns # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 1: Initialize a JointGrid object with the penguins dataset. g = sns.JointGrid(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") # Step 2: Plot the joint and marginal axes using scatterplot and histplot. x, y = penguins[\\"bill_length_mm\\"], penguins[\\"bill_depth_mm\\"] sns.scatterplot(x=x, y=y, ec=\\"b\\", fc=\\"none\\", s=100, linewidth=1.5, ax=g.ax_joint) sns.histplot(x=x, fill=False, linewidth=2, ax=g.ax_marg_x) sns.histplot(y=y, fill=False, linewidth=2, ax=g.ax_marg_y) # Step 3: Add vertical and horizontal reference lines. g.refline(x=45, y=16) # Display the plot g ``` **Expected Output:** A `JointGrid` plot that shows: - A scatter plot of `bill_length_mm` vs `bill_depth_mm`. - Histograms of `bill_length_mm` on the x-marginal axis and `bill_depth_mm` on the y-marginal axis. - Customizations as specified in the instructions. - Reference lines at `bill_length_mm = 45` and `bill_depth_mm = 16`. **Input:** - No direct input required, the dataset is loaded using Seaborn\'s `load_dataset` function. **Output:** - A visualization plot created using Seaborn\'s `JointGrid`. **Constraints:** - You should not use any plotting library other than Seaborn. - Ensure that your plot adheres to the customizations mentioned.","solution":"import seaborn as sns def plot_penguin_bill_measurements(): Generates a JointGrid plot of bill_length_mm vs bill_depth_mm from the penguins dataset, with specified customizations. # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Initialize a JointGrid object with the penguins dataset g = sns.JointGrid(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") # Plot the joint and marginal axes using scatterplot and histplot x, y = penguins[\\"bill_length_mm\\"], penguins[\\"bill_depth_mm\\"] sns.scatterplot(x=x, y=y, ec=\\"b\\", fc=\\"none\\", s=100, linewidth=1.5, ax=g.ax_joint) sns.histplot(x=x, fill=False, linewidth=2, ax=g.ax_marg_x) sns.histplot(y=y, fill=False, linewidth=2, ax=g.ax_marg_y) # Add vertical and horizontal reference lines g.refline(x=45, y=16) # Display the plot return g"},{"question":"**Question: Advanced Data Visualization with Seaborn\'s Cubehelix Palette** # Objective Write a Python function using seaborn that generates different types of cubehelix palettes based on given parameters and visualizes a dataset using these palettes. # Function Signature ```python def visualize_data_with_cubehelix(data, palette_params): Visualizes the given data using seaborn\'s cubehelix palette with specified parameters. Parameters: data (pandas.DataFrame): The data to visualize, with at least one numeric column. palette_params (list of dict): A list of dictionaries, where each dictionary contains parameters for the sns.cubehelix_palette() function. Returns: None ``` # Input - `data`: A Pandas DataFrame containing the data to be visualized. The DataFrame must have at least one numeric column. - `palette_params`: A list of dictionaries. Each dictionary contains parameters for the `sns.cubehelix_palette()` function. Example dictionary: ```python { \'n_colors\': 8, \'start\': 2, \'rot\': 0.5, \'dark\': 0.2, \'light\': 0.8, \'reverse\': False, \'as_cmap\': False, \'gamma\': 1.0, \'hue\': 1.0 } ``` # Output - The function does not return anything, but it displays visualizations for each set of palette parameters. # Constraints - The `data` must have at least one numeric column. - The `palette_params` list must not be empty and each dictionary must provide valid parameters for `sns.cubehelix_palette()`. - If the `palette_params` list or any parameter dictionary is invalid, raise a `ValueError` with an appropriate message. # Performance Requirements - Make sure that the visualizations are created efficiently. - Handle large datasets gracefully without running into performance issues. # Instructions 1. Import necessary libraries (e.g., pandas, seaborn, matplotlib). 2. Create a function named `visualize_data_with_cubehelix` that takes the specified inputs. 3. For each dictionary in `palette_params`, generate a cubehelix palette using `sns.cubehelix_palette()` with the provided parameters. 4. Visualize the data using seaborn\'s plotting functions (e.g., `sns.histplot()`, `sns.kdeplot()`) with the generated palette. 5. Ensure that the visualization for each palette is clearly distinguishable. # Example Usage ```python import pandas as pd # Sample data data = pd.DataFrame({ \'values\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] }) # Example palette parameters palette_params = [ {\'n_colors\': 8, \'start\': 2, \'rot\': 0.5, \'dark\': 0.2, \'light\': 0.8, \'reverse\': False, \'as_cmap\': False, \'gamma\': 1.0, \'hue\': 1.0}, {\'n_colors\': 5, \'start\': 0, \'rot\': -0.2, \'dark\': 0.4, \'light\': 1.0, \'reverse\': True, \'as_cmap\': True, \'gamma\': 0.7, \'hue\': 0.8} ] # Function call visualize_data_with_cubehelix(data, palette_params) ``` Expected output: Visualizations of the `data` DataFrame using the provided cubehelix palettes.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_data_with_cubehelix(data, palette_params): Visualizes the given data using seaborn\'s cubehelix palette with specified parameters. Parameters: data (pandas.DataFrame): The data to visualize, with at least one numeric column. palette_params (list of dict): A list of dictionaries, where each dictionary contains parameters for the sns.cubehelix_palette() function. Returns: None # Check if data has at least one numeric column if not any(data.dtypes.apply(lambda x: pd.api.types.is_numeric_dtype(x))): raise ValueError(\\"The data must have at least one numeric column.\\") # Check if palette_params list is not empty if not palette_params: raise ValueError(\\"Palette parameters list must not be empty.\\") for params in palette_params: if not isinstance(params, dict): raise ValueError(\\"Each palette parameter must be a dictionary.\\") try: # Generate the cubehelix palette palette = sns.cubehelix_palette(**params) except Exception as e: raise ValueError(f\\"Invalid parameters for cubehelix_palette: {e}\\") # Plot the data for col in data.select_dtypes(include=\'number\').columns: plt.figure(figsize=(10, 6)) sns.histplot(data[col], kde=True, palette=palette) plt.title(f\\"Data Visualization using Palette: {params}\\") plt.show()"},{"question":"Objective In this task, you are required to demonstrate your understanding of seaborn by creating a function that generates and customizes various plots based on specified parameters. Question Write a function named `custom_plot` that takes in several parameters to generate a line plot using seaborn. The function should create a plot of multiple sine waves, with options to customize the theme, style, context, and specific visual parameters of the plot. # Function Signature ```python def custom_plot(n: int, flip: int, theme: str, style: str, context: str, font_scale: float, linewidth: float, despine_offset: int, save_filepath: str) -> None: pass ``` # Parameters - `n` (int): Number of sine wave plots to generate. - `flip` (int): Multiplier to flip the sine waves (1 or -1). - `theme` (str): Seaborn theme to apply (options: `\'darkgrid\'`, `\'whitegrid\'`, `\'dark\'`, `\'white\'`, `\'ticks\'`). - `style` (str): Seaborn style to apply (options: `\'darkgrid\'`, `\'whitegrid\'`, `\'dark\'`, `\'white\'`, `\'ticks\'`). - `context` (str): Seaborn context to apply (options: `\'paper\'`, `\'notebook\'`, `\'talk\'`, `\'poster\'`). - `font_scale` (float): Scaling factor for the font size. - `linewidth` (float): Line width for the plot lines. - `despine_offset` (int): Offset for the despine function. - `save_filepath` (str): File path to save the generated plot (e.g., `\'plot.png\'`). # Output - The function should not return any values. Instead, it should display the plot and save it to the specified filepath. # Constraints - Ensure that the function uses seaborn\'s capabilities to set themes, styles, and contexts. - Use the `sns.despine()` function to remove the top and right spines and apply the specified offset. - The plot should feature sine waves generated using `numpy`. # Example ```python custom_plot( n=10, flip=1, theme=\'darkgrid\', style=\'whitegrid\', context=\'talk\', font_scale=1.5, linewidth=2.5, despine_offset=10, save_filepath=\'custom_plot.png\' ) ``` This example should generate a plot with 10 sine waves, flipped normally, using a white grid style and dark grid theme, suitable for a \'talk\' context with increased font size and line width, the spines should be offset by 10 units, and the resulting plot saved to `custom_plot.png`. # Guidelines 1. Use the `sinplot` function within your implementation to generate the sine waves. 2. Ensure your solution is clear, well-commented, and demonstrates your understanding of seaborn\'s plotting capabilities. 3. Handle any potential exceptions or errors gracefully.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n - i) * flip) def custom_plot(n: int, flip: int, theme: str, style: str, context: str, font_scale: float, linewidth: float, despine_offset: int, save_filepath: str) -> None: sns.set_theme(style=theme) sns.set_style(style) sns.set_context(context, font_scale=font_scale, rc={\\"lines.linewidth\\": linewidth}) sinplot(n, flip) sns.despine(offset=despine_offset) plt.savefig(save_filepath) plt.show()"},{"question":"Objective Your task is to demonstrate a deep understanding of the `pickletools` module by implementing a series of functions that utilize its key features. Problem Statement Implement a class `PickleAnalyzer` with the following methods: # 1. `disassemble_pickle(pickle_data: Union[str, bytes]) -> str` - **Input**: A string or bytes representing pickle data. - **Output**: A string with the disassembled output of the pickle data. # 2. `extract_opcodes(pickle_data: Union[str, bytes]) -> List[Tuple[str, Union[int, float, str, None], int]]` - **Input**: A string or bytes representing pickle data. - **Output**: A list of tuples, each containing `(opcode_name, argument, position)` for each opcode in the pickle data. # 3. `optimize_pickle(pickle_data: Union[str, bytes]) -> bytes` - **Input**: A string or bytes representing pickle data. - **Output**: The optimized pickle data as bytes. Constraints - You may assume that the input pickle data is well-formed. - The implementation should not execute any code within the pickle. # Example Usage ```python import pickle import pickletools class PickleAnalyzer: @staticmethod def disassemble_pickle(pickle_data): import io f = io.StringIO() pickletools.dis(pickle_data, out=f) return f.getvalue().strip() @staticmethod def extract_opcodes(pickle_data): return [(op.name, arg, pos) for op, arg, pos in pickletools.genops(pickle_data)] @staticmethod def optimize_pickle(pickle_data): return pickletools.optimize(pickle_data) # Example pickle data data = pickle.dumps([(1, 2), (3, 4)]) analyzer = PickleAnalyzer() # Disassemble the pickle data disassembled = analyzer.disassemble_pickle(data) print(disassembled) # Extract opcodes from the pickle data opcodes = analyzer.extract_opcodes(data) print(opcodes) # Optimize the pickle data optimized_data = analyzer.optimize_pickle(data) print(optimized_data) ``` Expected Output: ``` # Disassembled format of the pickle data # [ # 0: x80 PROTO 5 # 2: x95 FRAME 41 # 11: ] EMPTY_LIST # 12: ( MARK # 13: ( MARK # ... # List of opcodes # [ # (\'PROTO\', 5, 0), # (\'FRAME\', 41, 2), # (\'EMPTY_LIST\', None, 11), # (\'MARK\', None, 12), # ... # Optimized pickle data bytes # b\'x80x05x95)x00x00x00x00x00x00x00]x94(hx00Kx01hx01Kx02e(hx00Kx03hx01Kx04eu.\' ``` Additional Information - You should use the `pickletools` module\'s functions to perform the tasks. - Handle any necessary imports within your class methods.","solution":"import pickle import pickletools from typing import Union, List, Tuple class PickleAnalyzer: @staticmethod def disassemble_pickle(pickle_data: Union[str, bytes]) -> str: import io f = io.StringIO() pickletools.dis(pickle_data, out=f) return f.getvalue().strip() @staticmethod def extract_opcodes(pickle_data: Union[str, bytes]) -> List[Tuple[str, Union[int, float, str, None], int]]: return [(op.name, arg, pos) for op, arg, pos in pickletools.genops(pickle_data)] @staticmethod def optimize_pickle(pickle_data: Union[str, bytes]) -> bytes: return pickletools.optimize(pickle_data)"},{"question":"# Sparse Data Structures and Memory Efficiency with pandas **Objective:** Implement a function `optimize_memory_usage` designed to take a dense DataFrame, convert it to a sparse DataFrame with a specified `fill_value`, then convert it back to a dense DataFrame, and provide memory usage statistics at each step. **Function Signature:** ```python import pandas as pd def optimize_memory_usage(df: pd.DataFrame, fill_value) -> dict: pass ``` **Input:** - `df`: a pandas DataFrame with random values and some NaN values. - `fill_value`: the fill value to be used for creating the sparse DataFrame. **Output:** A dictionary with keys: - `\'initial_memory\'`: Memory usage of the original DataFrame in bytes. - `\'sparse_memory\'`: Memory usage of the DataFrame after converting to sparse format in bytes. - `\'final_memory\'`: Memory usage of the dense DataFrame converted back from sparse in bytes. - `\'initial_density\'`: Density of the original dense DataFrame. - `\'sparse_density\'`: Density of the sparse DataFrame. **Constraints:** - The input DataFrame `df` can have up to `10000` rows and `100` columns. - The function must handle NaN values efficiently. **Task Details:** 1. Calculate and store the initial memory usage of the dataframe. 2. Convert the dataframe `df` to a sparse DataFrame using `fill_value`. 3. Calculate and store the memory usage of the sparse DataFrame. 4. Convert the sparse DataFrame back to a dense format. 5. Calculate and store the final memory usage of the dense DataFrame. 6. Compute the density of the data in both formats. **Example:** ```python import numpy as np import pandas as pd def optimize_memory_usage(df: pd.DataFrame, fill_value) -> dict: initial_memory = df.memory_usage().sum() initial_density = df.notnull().sum().sum() / df.size sdf = df.astype(pd.SparseDtype(df.dtypes.iloc[0], fill_value)) sparse_memory = sdf.memory_usage().sum() sparse_density = sdf.sparse.density dense_df = sdf.sparse.to_dense() final_memory = dense_df.memory_usage().sum() return { \'initial_memory\': initial_memory, \'sparse_memory\': sparse_memory, \'final_memory\': final_memory, \'initial_density\': initial_density, \'sparse_density\': sparse_density } # Example usage: df = pd.DataFrame(np.random.randn(10000, 10)) df.iloc[:9998] = np.nan result = optimize_memory_usage(df, np.nan) print(result) ``` **Expected Output:** The dictionary should include the memory usage statistics and density at each stage: ```python { \'initial_memory\': 800128, \'sparse_memory\': 320128, \'final_memory\': 800128, \'initial_density\': 0.000199, \'sparse_density\': 0.000199 } ``` **Note:** - The memory values are hypothetical and will differ based on the actual random data generated. - Ensure that the solution is efficient in terms of both time and space complexity.","solution":"import pandas as pd def optimize_memory_usage(df: pd.DataFrame, fill_value) -> dict: Optimizes memory usage by converting a DataFrame to sparse format and back to dense format. Parameters: df (pd.DataFrame): The input dense DataFrame. fill_value: The value to use as fill value for the sparse format. Returns: dict: A dictionary containing memory usage statistics and densities at each step. # Calculate initial memory usage initial_memory = df.memory_usage().sum() initial_density = df.notnull().sum().sum() / df.size # Convert to sparse DataFrame sparse_df = df.astype(pd.SparseDtype(df.dtypes.iloc[0], fill_value)) sparse_memory = sparse_df.memory_usage().sum() sparse_density = sparse_df.sparse.density # Convert back to dense DataFrame dense_df = sparse_df.sparse.to_dense() final_memory = dense_df.memory_usage().sum() return { \'initial_memory\': initial_memory, \'sparse_memory\': sparse_memory, \'final_memory\': final_memory, \'initial_density\': initial_density, \'sparse_density\': sparse_density }"},{"question":"# Question: Seaborn Plotting Context Manipulation You are provided with a dataset and your task is to demonstrate your understanding of seaborne by manipulating plotting contexts and visualizing the data accordingly. Task: 1. **Retrieve and Display Current Plotting Context**: - Write code to print the current default plotting context settings using `sns.plotting_context()`. 2. **Set and Display Predefined Plotting Context**: - Write code to set and print the \\"talk\\" plotting context using `sns.plotting_context(\\"talk\\")`. 3. **Temporary Plotting Context with Context Manager**: - Import seaborn and matplotlib.pyplot libraries. - Using the provided dataset, create a line plot inside a plotting context manager that sets the context to \\"notebook\\". Use \\"A\\", \\"B\\", \\"C\\" for the x-axis and [1, 3, 2] for the y-axis. - Ensure that the plot is only temporarily affected by the \\"notebook\\" context and that the context reverts to its default settings afterward. Expected Input and Output Formats: - **No specific input**: This task uses the predefined plotting context settings and sample data provided. - **Output**: - Once you have run the code, it should display the current default plotting context settings, the predefined \\"talk\\" context settings, and temporarily display a line plot with \\"notebook\\" context settings. Constraints: - Use only seaborn and matplotlib.pyplot for plotting and context management. - Ensure reproducibility and clarity of plots. Example: ```python # Code demonstrating each step as per the task requirements ``` Use the above guidelines to create your solution. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def retrieve_and_display_current_context(): Retrieves and prints the current default plotting context settings. current_context = sns.plotting_context() print(\\"Current default plotting context settings:\\", current_context) def set_and_display_predefined_context(): Sets the plotting context to \'talk\' and prints the settings. sns.set_context(\\"talk\\") talk_context = sns.plotting_context() print(\\"Talk plotting context settings:\\", talk_context) def line_plot_with_context_manager(): Creates a line plot inside a plotting context manager that sets the context to \'notebook\'. The plot is only temporarily affected by the \'notebook\' context. # Dataset x = [\\"A\\", \\"B\\", \\"C\\"] y = [1, 3, 2] # Temporary plotting context with sns.plotting_context(\\"notebook\\"): plt.figure() sns.lineplot(x=x, y=y) plt.title(\\"Line Plot with Notebook Context\\") plt.show() # Check final context to confirm it has reverted to default final_context = sns.plotting_context() print(\\"Final default plotting context settings after context manager:\\", final_context)"},{"question":"**Coding Assessment Question** # Task Your task is to implement a function that simulates a simple asynchronous game. The game consists of various tasks (represented as coroutines) that complete after random delays. Your function should create Futures for these tasks and handle their completion in an efficient manner. # Specification 1. Implement a function `simulate_game(tasks, delays)` where: - `tasks` is a list of integers representing different tasks of the game. - `delays` is a list of integers representing the completion delays for the corresponding tasks in seconds. 2. The function should: - Create an asyncio.Future object for each task. - Set the result of each Future after the corresponding delay. - Return a list of results after all tasks are completed. 3. Results for each task can be a string in the format `\\"Task <task_id> completed after <delay> seconds\\"`. # Constraints - Ensure that your function handles tasks concurrently using asyncio features. - Handle edge cases where `tasks` or `delays` might be empty. # Input - `tasks`: A list of integers representing the task IDs. - `delays`: A list of integers representing the delay times in seconds. # Output - A list of strings, each string being the result of one completed task. # Example ```python import asyncio async def simulate_game(tasks, delays): async def complete_task(task_id, delay, fut): await asyncio.sleep(delay) fut.set_result(f\\"Task {task_id} completed after {delay} seconds\\") loop = asyncio.get_running_loop() futures = [loop.create_future() for _ in tasks] for task_id, delay, fut in zip(tasks, delays, futures): loop.create_task(complete_task(task_id, delay, fut)) results = await asyncio.gather(*[f.result() for f in futures]) return results # Example Usage: # tasks = [1, 2, 3] # delays = [2, 4, 1] # result = asyncio.run(simulate_game(tasks, delays)) # print(result) # Output: [\\"Task 1 completed after 2 seconds\\", \\"Task 2 completed after 4 seconds\\", \\"Task 3 completed after 1 seconds\\"] ``` # Performance Requirements - The function should efficiently schedule and await the completion of all tasks using asyncio. - Ensure your function does not block the event loop unnecessarily. **Good Luck!**","solution":"import asyncio async def simulate_game(tasks, delays): async def complete_task(task_id, delay): await asyncio.sleep(delay) return f\\"Task {task_id} completed after {delay} seconds\\" coroutines = [complete_task(task_id, delay) for task_id, delay in zip(tasks, delays)] results = await asyncio.gather(*coroutines) return results"},{"question":"Coding Assessment Question # Background You are tasked with handling and transforming data received from various sources in different encoding formats. You need to write a Python function that converts data from base64 to its quoted-printable form, while also verifying the integrity of the data using a cyclic redundancy check (CRC). # Problem Statement Write a function `convert_and_verify(data: str, initial_crc: int = 0) -> str` that: 1. Accepts a base64 encoded string (`data`). 2. Decodes it to binary. 3. Computes the CRC-32 of the resulting binary data. 4. Encodes the binary data to quoted-printable format. 5. Returns the quoted-printable encoded string. # Input - `data`: A base64 encoded string. (Type: `str`) - `initial_crc`: An optional integer representing the initial CRC value for the calculation. Defaults to `0`. (Type: `int`) # Output - Returns a string that is the quoted-printable encoded representation of the input binary data. # Constraints - The input string is guaranteed to be valid base64 encoded data. - The implementation must use functions from the `binascii` module to perform encoding and decoding. - The function should handle exceptions that might be raised due to incomplete or erroneous data using `binascii.Error` and `binascii.Incomplete`. # Examples ```python data = \\"SGVsbG8gV29ybGQh\\" initial_crc = 12345 result = convert_and_verify(data, initial_crc) print(result) # Example output: \\"Hello=20World=21\\" ``` # Note - The base64 encoded string `data` in the example translates to \\"Hello World!\\" in binary. - Make sure the function handles errors gracefully and conforms to the specified encoding/decoding rules. # Additional Information The `binascii` functions that may be useful: - `binascii.a2b_base64` to decode base64 to binary. - `binascii.b2a_qp` to encode binary to quoted-printable. - `binascii.crc32` to compute the CRC-32 checksum.","solution":"import binascii def convert_and_verify(data: str, initial_crc: int = 0) -> str: Converts base64 encoded data to quoted-printable format and verifies integrity using CRC-32. Parameters: - data: A base64 encoded string. - initial_crc: An optional initial CRC value. Defaults to 0. Returns: - Quoted-printable encoded string. try: # Decode base64 to binary binary_data = binascii.a2b_base64(data) # Compute CRC-32 checksum crc_value = binascii.crc32(binary_data, initial_crc) print(f\\"CRC-32 Checksum: {crc_value}\\") # Convert binary data to quoted-printable format quoted_printable_data = binascii.b2a_qp(binary_data).decode(\'utf-8\') return quoted_printable_data except binascii.Error as e: return f\\"Error in decoding: {e}\\" except Exception as e: return f\\"Unexpected error: {e}\\""},{"question":"# Task You are provided with a dataset that contains information on various car attributes. Your task is to generate a grid of scatter plots using the seaborn `objects` module. The grid should visualize multiple pairwise relationships between different sets of variables. Additionally, you will customize this grid by labeling the axes and adding a subplot. # Input - A dataframe `df` which contains the following columns: - `mpg`: Miles per gallon. - `displacement`: Engine displacement. - `weight`: Vehicle weight. - `horsepower`: Engine horsepower. - `acceleration`: Vehicle acceleration. - `origin`: Origin of car (categorical: \'USA\', \'Europe\', \'Japan\'). # Output - A grid of scatter plots. # Instructions 1. Use the `Pair` functionality to create scatter plots for the dependent variable `acceleration` against `displacement` and `weight`. 2. Create additional scatter plots showing the relationships between `horsepower` vs. `weight` and `horsepower` vs. `displacement`. 3. Use the `wrap` option to organize these scatter plots into a 2x2 grid. 4. Add a `facet` to split each scatter plot by the `origin` of the car using the `col` argument. 5. Customize the plot by labeling the x and y axes appropriately. # Example ```python import seaborn.objects as so import pandas as pd # Assume `df` is pre-loaded with the data as described above. ( so.Plot(df) .pair(x=[\\"displacement\\", \\"weight\\", \\"weight\\", \\"displacement\\"], y=[\\"acceleration\\", \\"acceleration\\", \\"horsepower\\", \\"horsepower\\"], wrap=2) .facet(col=\\"origin\\") .label(x0=\\"Displacement (cu in)\\", y0=\\"Acceleration (0-60 mph)\\", x1=\\"Weight (lb)\\", y1=\\"Acceleration (0-60 mph)\\", x2=\\"Weight (lb)\\", y2=\\"Horsepower\\", x3=\\"Displacement (cu in)\\", y3=\\"Horsepower\\") .add(so.Dots()) ) ``` In the above task: - We pair the dependent variable `acceleration` with `displacement` and `weight`. - We include the additional relationships between `horsepower` vs. `weight` and `displacement`. - The `wrap` argument is used to arrange the plots into a 2x2 grid. - The `facet` argument splits each scatter plot by the `origin` of the car. - The `label` method is used to set appropriate labels for the x and y axes. # Constraints - Ensure that all plots are clearly labeled and a grid layout is maintained. - Use only the seaborn `objects` module for plotting. - Assume the dataframe `df` is loaded in memory and contains all the required columns.","solution":"import seaborn.objects as so import pandas as pd def generate_scatterplot_grid(df): Generates a grid of scatter plots using the seaborn objects module. Parameters: df (pd.DataFrame): Dataframe containing the car attributes. Returns: so.Plot object: Configured scatter plot grid. plot = ( so.Plot(df) .pair( x=[\\"displacement\\", \\"weight\\", \\"weight\\", \\"displacement\\"], y=[\\"acceleration\\", \\"acceleration\\", \\"horsepower\\", \\"horsepower\\"], wrap=2 ) .facet(col=\\"origin\\") .label( x0=\\"Displacement (cu in)\\", y0=\\"Acceleration (0-60 mph)\\", x1=\\"Weight (lb)\\", y1=\\"Acceleration (0-60 mph)\\", x2=\\"Weight (lb)\\", y2=\\"Horsepower\\", x3=\\"Displacement (cu in)\\", y3=\\"Horsepower\\" ) .add(so.Dots()) ) return plot"},{"question":"# Question: Implementing a Dynamic Shape Operation in PyTorch Objective You are required to implement a function in PyTorch that processes a batch of tensors with variable sizes and performs different operations based on the dynamic size of the tensors. Function Signature ```python def dynamic_shape_operation(tensors: List[torch.Tensor]) -> torch.Tensor: Merges a list of tensors with dynamic sizes and applies an operation based on the dynamic shape. Args: - tensors (List[torch.Tensor]): A list of 2D tensors with varying dimensions. Returns: - torch.Tensor: A tensor resulting from the conditional operations on the merged tensor. ``` Problem Statement 1. You are given a list of 2D tensors with varying dimensions. 2. Your task is to merge these tensors along the first dimension. 3. Perform a specific operation based on the size of the merged tensorâ€™s first dimension: - If the size of the first dimension is greater than `10`, return the tensor multiplied by `2`. - Otherwise, return the tensor with `1` added to all elements. # Constraints - The tensors in the input list will always be 2-dimensional. - Use the provided symbolic shapes handling in PyTorch to manage dynamic shapes and conditional operations. # Example 1. **Input**: ```python tensors = [ torch.randn(4, 5), torch.randn(3, 5), torch.randn(2, 5) ] ``` **Output**: ```python tensor([...]) # Merged tensor with a size of (9, 5) with 1 added to all elements ``` 2. **Input**: ```python tensors = [ torch.randn(6, 4), torch.randn(5, 4) ] ``` **Output**: ```python tensor([...]) # Merged tensor with a size of (11, 4) multiplied by 2 ``` # Implementation Requirements - Make use of `torch._dynamo.mark_dynamic` to anticipate dynamic sizes. - Ensure the function can handle any number of tensors within the list. - Utilize symbolic shapes to ensure the code can handle dynamic batch sizes efficiently. Performance Requirements - Your function should efficiently handle up to 1000 tensors in the list, where each tensor\'s first dimension can vary between 1 to 100. Good luck!","solution":"import torch from typing import List def dynamic_shape_operation(tensors: List[torch.Tensor]) -> torch.Tensor: Merges a list of tensors with dynamic sizes and applies an operation based on the dynamic shape. Args: - tensors (List[torch.Tensor]): A list of 2D tensors with varying dimensions. Returns: - torch.Tensor: A tensor resulting from the conditional operations on the merged tensor. merged_tensor = torch.cat(tensors, dim=0) if merged_tensor.size(0) > 10: return merged_tensor * 2 else: return merged_tensor + 1"},{"question":"# Masked Tensor Operations and Reductions **Objective**: Demonstrate your understanding of PyTorch\'s masked tensors by implementing a class with core masked tensor operations and reductions, and using it in a practical example. # Task 1. **Class Implementation** Implement a class `MaskedTensorProcessor` with the following methods: - `__init__(self, data, mask)`: Initialize with a data tensor and a boolean mask tensor of the same shape. - `masked_sum(self)`: Return the sum of the data elements where the mask is `True`. - `masked_mean(self)`: Return the mean of the data elements where the mask is `True`. - `masked_max(self)`: Return the maximum value among the data elements where the mask is `True`. - `apply_unary_operator(self, operator)`: Apply a given unary operator (from the supported list) to the data tensor, respecting the mask, and return the result. 2. **Usage Example** Create a simple use-case demonstrating how one might use the `MaskedTensorProcessor` to: - Apply a mask to a tensor. - Compute its masked sum, mean, and max. - Apply a unary operator (e.g., `torch.exp`) to the masked tensor. # Input and Output Format - **Initialization**: - `data`: A PyTorch tensor of any shape. - `mask`: A PyTorch boolean tensor of the same shape as `data`. - **Methods**: - `masked_sum() -> float`: Sum of masked elements. - `masked_mean() -> float`: Mean of masked elements. - `masked_max() -> float`: Max value of masked elements. - `apply_unary_operator(operator: Callable) -> MaskedTensor`: Resulting tensor after applying unary operator. # Constraints - Ensure that `data` and `mask` have the same shape during initialization. - Use PyTorch to handle tensor operations. # Example ```python import torch from torch.masked import masked_tensor class MaskedTensorProcessor: def __init__(self, data, mask): assert data.shape == mask.shape, \\"data and mask must have the same shape\\" self.data = data self.mask = mask self.masked_tensor = masked_tensor(data, mask) def masked_sum(self): return self.masked_tensor.sum().item() def masked_mean(self): return self.masked_tensor.mean().item() def masked_max(self): return self.masked_tensor.max().item() def apply_unary_operator(self, operator): return masked_tensor(operator(self.data), self.mask) # Example usage data = torch.tensor([[10, -1, 2], [3, 0, 4], [-1, 8, 5]], dtype=torch.float) mask = torch.tensor([[True, False, True], [True, False, True], [False, True, True]], dtype=torch.bool) processor = MaskedTensorProcessor(data, mask) print(\\"Masked Sum:\\", processor.masked_sum()) print(\\"Masked Mean:\\", processor.masked_mean()) print(\\"Masked Max:\\", processor.masked_max()) exp_processor = processor.apply_unary_operator(torch.exp) print(\\"Exponentiated Masked Tensor Data:\\", exp_processor) # Additional expected output # Masked Sum: 32.0 # Masked Mean: 5.333... # Masked Max: 10.0 # Exponentiated Masked Tensor Data: MaskedTensor([...]) ``` Ensure your implementation aligns with the provided structure and constraints. Verify your solutions with both the example and additional test cases.","solution":"import torch class MaskedTensorProcessor: def __init__(self, data, mask): assert data.shape == mask.shape, \\"data and mask must have the same shape\\" self.data = data self.mask = mask def masked_sum(self): return self.data[self.mask].sum().item() def masked_mean(self): return self.data[self.mask].mean().item() def masked_max(self): return self.data[self.mask].max().item() def apply_unary_operator(self, operator): return operator(self.data) * self.mask # Example usage data = torch.tensor([[10, -1, 2], [3, 0, 4], [-1, 8, 5]], dtype=torch.float) mask = torch.tensor([[True, False, True], [True, False, True], [False, True, True]], dtype=torch.bool) processor = MaskedTensorProcessor(data, mask) print(\\"Masked Sum:\\", processor.masked_sum()) # Should be 32.0 print(\\"Masked Mean:\\", processor.masked_mean()) # Should be 5.3333... print(\\"Masked Max:\\", processor.masked_max()) # Should be 10.0 exp_processor = processor.apply_unary_operator(torch.exp) print(\\"Exponentiated Masked Tensor Data:\\", exp_processor)"},{"question":"# Python Naming, Scope, and Exception Handling Problem Statement You are tasked with creating a program that models a simple library system. The objective is to write a Python class `Library` that allows managing books within the library using various methods. The program should handle exceptions gracefully and demonstrate a solid understanding of Python\'s naming and binding rules. Requirements 1. **Class Definition**: - Create a class named `Library` with the following attributes: - `books`: A dictionary where keys are the book names, and values are the quantity of each book. 2. **Methods**: - `add_book(book_name: str, quantity: int) -> None`: Adds a specified quantity of the given book to the library. - `borrow_book(book_name: str) -> None`: Decreases the quantity of the given book by one. Raise an exception if the book is not available or quantity is zero. - `return_book(book_name: str) -> None`: Increases the quantity of the given book by one. - `get_book_quantity(book_name: str) -> int`: Returns the current quantity of the given book. - `remove_book(book_name: str) -> None`: Removes the book from the library. Raise an exception if the book is not found. 3. **Exception Handling**: - Create custom exception classes `BookNotAvailableError` and `BookNotFoundError`. - Handle these exceptions appropriately in the methods `borrow_book` and `remove_book`. 4. **Code Execution**: - Demonstrate the usage of the `Library` class by implementing the following sequence of operations: 1. Create an instance of `Library`. 2. Add books \\"Python 101\\" (5 copies) and \\"Data Science\\" (3 copies) to the library. 3. Borrow one copy of \\"Python 101\\". 4. Try to borrow \\"Machine Learning\\" which is not available and handle the exception. 5. Get the current quantity of \\"Python 101\\". 6. Return one copy of \\"Python 101\\". 7. Remove \\"Data Science\\" from the library. 8. Try to remove \\"AI Basics\\" which is not found and handle the exception. Input and Output Formats - **Input**: - Method calls with parameters inline within a script. - **Output**: - Exceptions or print statements reflecting the internal state changes or errors. Constraints - Assume book names are case-sensitive unique strings. - Ensure that appropriate error handling is in place for the operations specified. Example ```python class BookNotAvailableError(Exception): pass class BookNotFoundError(Exception): pass class Library: def __init__(self): self.books = {} def add_book(self, book_name: str, quantity: int) -> None: if book_name in self.books: self.books[book_name] += quantity else: self.books[book_name] = quantity def borrow_book(self, book_name: str) -> None: if book_name not in self.books or self.books[book_name] == 0: raise BookNotAvailableError(f\\"Book \'{book_name}\' is not available or out of stock.\\") self.books[book_name] -= 1 def return_book(self, book_name: str) -> None: if book_name in self.books: self.books[book_name] += 1 else: self.books[book_name] = 1 def get_book_quantity(self, book_name: str) -> int: return self.books.get(book_name, 0) def remove_book(self, book_name: str) -> None: if book_name not in self.books: raise BookNotFoundError(f\\"Book \'{book_name}\' not found in the library.\\") del self.books[book_name] # Example usage: library = Library() library.add_book(\\"Python 101\\", 5) library.add_book(\\"Data Science\\", 3) library.borrow_book(\\"Python 101\\") try: library.borrow_book(\\"Machine Learning\\") except BookNotAvailableError as e: print(e) print(library.get_book_quantity(\\"Python 101\\")) library.return_book(\\"Python 101\\") try: library.remove_book(\\"Data Science\\") except BookNotFoundError as e: print(e) try: library.remove_book(\\"AI Basics\\") except BookNotFoundError as e: print(e) ```","solution":"class BookNotAvailableError(Exception): pass class BookNotFoundError(Exception): pass class Library: def __init__(self): self.books = {} def add_book(self, book_name: str, quantity: int) -> None: if book_name in self.books: self.books[book_name] += quantity else: self.books[book_name] = quantity def borrow_book(self, book_name: str) -> None: if book_name not in self.books or self.books[book_name] == 0: raise BookNotAvailableError(f\\"Book \'{book_name}\' is not available or out of stock.\\") self.books[book_name] -= 1 def return_book(self, book_name: str) -> None: if book_name in self.books: self.books[book_name] += 1 else: self.books[book_name] = 1 def get_book_quantity(self, book_name: str) -> int: return self.books.get(book_name, 0) def remove_book(self, book_name: str) -> None: if book_name not in self.books: raise BookNotFoundError(f\\"Book \'{book_name}\' not found in the library.\\") del self.books[book_name] # Example usage: library = Library() library.add_book(\\"Python 101\\", 5) library.add_book(\\"Data Science\\", 3) library.borrow_book(\\"Python 101\\") try: library.borrow_book(\\"Machine Learning\\") except BookNotAvailableError as e: print(e) print(library.get_book_quantity(\\"Python 101\\")) library.return_book(\\"Python 101\\") try: library.remove_book(\\"Data Science\\") except BookNotFoundError as e: print(e) try: library.remove_book(\\"AI Basics\\") except BookNotFoundError as e: print(e)"},{"question":"Advanced Data Visualization with Seaborn Objective Your task is to demonstrate proficiency with the Seaborn library by creating multiple visualizations that require different levels of data manipulation, understanding of plot aesthetics, and advanced plotting techniques. Datasets You will use the datasets `flights` and `fmri` available in Seaborn. Tasks 1. **Data Preparation**: - Load the `flights` dataset from Seaborn. - Pivot the `flights` dataset to wide-form where the columns represent months and rows represent years. ```python import seaborn as sns flights = sns.load_dataset(\\"flights\\") flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") ``` 2. **Plotting Wide-Form Data**: - Create a line plot for the wide-form version of `flights` where each line represents a month. - Ensure the lines are visually distinct by using different colors. 3. **Long-Form Data Plotting with Grouping**: - Using the original `flights` dataset, create a line plot showing the number of passengers over the years, grouped by month. Use `hue` to differentiate between months and `style` to add markers to the lines. 4. **Advanced Plot Customization**: - Load the `fmri` dataset from Seaborn. - Create a line plot displaying the average `signal` over `timepoint`, grouped by `region` and `event`. - Use both `hue` and `style` for differentiation and include error bars. 5. **Faceted Plot**: - Using the `fmri` dataset, create a faceted line plot where each facet (subplot) represents a `region`. - Differentiate lines within each facet by `event` using both colors and line styles. Code Requirements - Your code should be well-organized and include comments explaining each major step. - Ensure that each plot is appropriately labeled with titles, axis labels, and legends where applicable. Expected Solution An expected solution would be a Python script or a Jupyter Notebook that performs the following: - Loads the relevant datasets. - Creates each of the requested plots. - Utilizes Seaborn functionalities effectively to differentiate between various groups and customize plot aesthetics. This question assesses your ability to work with Seaborn for data manipulation and visualization, and your understanding of how to effectively convey information through graphical representations.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Task 1: Data Preparation flights = sns.load_dataset(\\"flights\\") flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Task 2: Plotting Wide-Form Data plt.figure(figsize=(10, 6)) sns.lineplot(data=flights_wide) plt.title(\'Monthly Passengers Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\') plt.show() # Task 3: Long-Form Data Plotting with Grouping plt.figure(figsize=(14, 8)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", style=\\"month\\", markers=True, dashes=False) plt.title(\'Number of Passengers Over the Years (Grouped by Month)\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\') plt.show() # Task 4: Advanced Plot Customization fmri = sns.load_dataset(\\"fmri\\") plt.figure(figsize=(12, 8)) sns.lineplot(data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", ci=\\"sd\\", markers=True) plt.title(\'Average Signal Over Timepoint (Grouped by Region and Event)\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.legend(title=\'Region and Event\') plt.show() # Task 5: Faceted Plot g = sns.FacetGrid(fmri, col=\\"region\\", height=5, aspect=1.2) g.map(sns.lineplot, \\"timepoint\\", \\"signal\\", \\"event\\", ci=None, dashes=True) g.add_legend() plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Signal Over Time, Faceted by Region\') plt.show()"},{"question":"Objective: To assess students on their understanding of the `tempfile` module in Python, focusing on creating, using, and cleaning up temporary files and directories. Problem Statement: Write a Python function `process_with_tempfile` that performs the following tasks: 1. Creates a temporary file using `tempfile.NamedTemporaryFile` and writes the string `\\"Temporary file content\\"` to it. Ensure that the file has a suffix `.txt` and is created in the default temporary directory. 2. Seeks to the beginning of the file and reads the content. 3. Creates a temporary directory using `tempfile.TemporaryDirectory`. 4. Moves the temporary file created in step 1 into the temporary directory created in step 3. 5. Renames the file within this directory to `renamed_tempfile.txt`. 6. Reads the content from the renamed file to verify the move was successful. 7. Deletes the temporary directory manually. 8. Returns the content that was read from the renamed file in step 6. Your implementation should ensure that all temporary files and directories are properly cleaned up. Do not rely on automatic clean-up features for this task. Function Signature: ```python import tempfile import shutil import os def process_with_tempfile() -> str: pass ``` Constraints: - You can use any standard library modules that you deem necessary. - Handle any exceptions that may occur during file operations and cleanup properly. Example Usage: ```python result = process_with_tempfile() print(result) # Expected Output: \\"Temporary file content\\" ``` Notes: - Ensure that the implementation does not leave any temporary files or directories behind after execution. Clean-up must be explicit and thorough.","solution":"import tempfile import shutil import os def process_with_tempfile() -> str: try: # Step 1: Create a temporary file and write content to it with tempfile.NamedTemporaryFile(delete=False, suffix=\\".txt\\") as temp_file: temp_file.write(b\\"Temporary file content\\") temp_file_name = temp_file.name # Step 2: Read the content from the temporary file with open(temp_file_name, \'r\') as temp_file: temp_content = temp_file.read() # Step 3: Create a temporary directory temp_dir = tempfile.mkdtemp() try: # Step 4: Move the temporary file to the temporary directory moved_file_path = os.path.join(temp_dir, os.path.basename(temp_file_name)) shutil.move(temp_file_name, moved_file_path) # Step 5: Rename the file within the directory renamed_file_path = os.path.join(temp_dir, \\"renamed_tempfile.txt\\") os.rename(moved_file_path, renamed_file_path) # Step 6: Read the content from the renamed file with open(renamed_file_path, \'r\') as renamed_file: renamed_content = renamed_file.read() finally: # Step 7: Delete the temporary directory manually shutil.rmtree(temp_dir) # Step 8: Return the content read from the renamed file return renamed_content finally: # Ensure the temporary file is deleted if it was not moved if os.path.exists(temp_file_name): os.remove(temp_file_name)"},{"question":"# File Encoding and Decoding Challenge Using the \\"uu\\" module in Python, your task is to implement two functions: `encode_file` and `decode_file`. # Function Specifications `encode_file(in_file_path: str, out_file_path: str, name: str = None, mode: int = None, backtick: bool = False) -> None` This function uuencodes a file located at `in_file_path` and saves the encoded result to `out_file_path`. - **Parameters:** - `in_file_path` (str): The path to the input file to be encoded. - `out_file_path` (str): The path to the output file where the uuencoded content will be saved. - `name` (str, optional): The default name in the uuencode header. Defaults to `None`. - `mode` (int, optional): The default permission bits in the uuencode header. Defaults to `None`. - `backtick` (bool, optional): If `True`, zeros are represented by \\"`\\" instead of spaces in the encoding. Defaults to `False`. - **Return:** None `decode_file(in_file_path: str, out_file_path: str = None, mode: int = None, quiet: bool = False) -> None` This function uudecodes a file located at `in_file_path` and produces output in the specified `out_file_path`. - **Parameters:** - `in_file_path` (str): The path to the input file to be decoded. - `out_file_path` (str, optional): The path to the output file where the decoded content will be saved. Defaults to `None`. - `mode` (int, optional): The permission bits for the output file if it needs to be created. Defaults to `None`. - `quiet` (bool, optional): If `True`, suppresses any warnings. Defaults to `False`. - **Return:** None # Constraints and Requirements 1. Assume all file paths provided are valid. 2. Your implementation should gracefully handle errors by raising an appropriate exception. 3. You may use the `uu` module\'s `encode` and `decode` functions directly within your implementations. 4. Ensure that the encoded file can be decoded back to its original content perfectly. # Example Usage ```python # Example usage of encode_file encode_file(\'example.txt\', \'encoded_example.txt\', name=\'example.txt\', mode=0o644) # Example usage of decode_file decode_file(\'encoded_example.txt\', \'decoded_example.txt\') ``` # Notes - Pay special attention to the handling of file permissions and the possible need to create files in different modes. - Consider edge cases where the input file might be empty or have special characters. Your task is to implement these two functions to correctly encode and decode files using the uuencode format specified by the \\"uu\\" module.","solution":"import uu def encode_file(in_file_path: str, out_file_path: str, name: str = None, mode: int = None, backtick: bool = False) -> None: try: with open(in_file_path, \'rb\') as in_file: with open(out_file_path, \'wb\') as out_file: uu.encode(in_file, out_file, name=name, mode=mode, backtick=backtick) except Exception as e: raise RuntimeError(f\\"Failed to encode file: {e}\\") def decode_file(in_file_path: str, out_file_path: str = None, mode: int = None, quiet: bool = False) -> None: try: with open(in_file_path, \'rb\') as in_file: if out_file_path: with open(out_file_path, \'wb\') as out_file: uu.decode(in_file, out_file, mode=mode, quiet=quiet) else: uu.decode(in_file, mode=mode, quiet=quiet) except Exception as e: raise RuntimeError(f\\"Failed to decode file: {e}\\")"},{"question":"# Question In this task, you are required to implement a function using the `pandas.tseries.offsets` module to calculate the next business day and the start of the business hour from a given date. The function will take a date string as input, along with a business hour range and a list of holidays. The function should return the next business day and the start of the business hour within the provided business hour range. Requirements 1. **Function Signature**: ```python def calculate_next_business_day(date_str: str, start_time: str, end_time: str, holidays: list[str]) -> str: ``` 2. **Input**: - `date_str`: A `str` representing the date in the format \'YYYY-MM-DD\'. - `start_time`: A `str` representing the start of business hours in the format \'HH:MM\'. - `end_time`: A `str` representing the end of business hours in the format \'HH:MM\'. - `holidays`: A list of `str`, where each string represents a holiday date in the format \'YYYY-MM-DD\'. 3. **Output**: - A `str` representing the next business day and the start of the business hour in the format \'YYYY-MM-DD HH:MM\'. 4. **Constraints**: - You should use the `pandas.tseries.offsets` module to find the next business day. - Consider weekends (Saturday and Sunday) as non-business days. - The provided business hour range starts and ends on the same day. - Ignore timezone information for this task. Detailed Description: Write a function `calculate_next_business_day` that: 1. Parses the input `date_str` to obtain a `datetime` object. 2. Identifies the next business day (considering weekends and holidays). 3. Finds the start of the business hour within the range specified by `start_time` and `end_time` for the next business day. 4. Returns the resulting datetime as a formatted string \'YYYY-MM-DD HH:MM\'. Example: ```python date_str = \\"2023-10-13\\" start_time = \\"09:00\\" end_time = \\"17:00\\" holidays = [\\"2023-10-14\\"] print(calculate_next_business_day(date_str, start_time, end_time, holidays)) # Output: \\"2023-10-16 09:00\\" ``` Hints: - Utilize the properties and methods provided by `pandas.tseries.offsets.BusinessDay` and `pandas.tseries.offsets.BusinessHour` to calculate the next business day and appropriate hour. - Consider using `pd.Timestamp` to handle and manipulate datetime objects.","solution":"import pandas as pd from pandas.tseries.offsets import CustomBusinessDay def calculate_next_business_day(date_str: str, start_time: str, end_time: str, holidays: list[str]) -> str: Returns the next business day and the start of business hour as a string in the format \'YYYY-MM-DD HH:MM\'. # Parsing the input date date = pd.Timestamp(date_str) # Creating a CustomBusinessDay offset with holidays and weekends business_day = CustomBusinessDay(holidays=holidays, weekmask=\'Mon Tue Wed Thu Fri\') # Calculating the next business day next_business_day = date + business_day # Formatting the next business day with the start_time result_datetime = next_business_day.strftime(\'%Y-%m-%d\') + \\" \\" + start_time return result_datetime"},{"question":"Managing Multiple Panels in Curses **Objective:** Demonstrate your understanding of the `curses.panel` module in Python by implementing a small application that manages multiple panels on the screen. **Task:** You are required to implement a Python function called `manage_panels` that sets up multiple panels on the screen, allows navigating between them using keyboard input, and performs various operations (show, hide, move, stack operations). **Function Signature:** ```python def manage_panels(): pass ``` **Requirements:** 1. Initialize a curses window and create at least three panels with distinct content. 2. Implement keyboard navigation to switch between panels (e.g., using arrow keys to navigate). 3. Allow the user to perform the following operations using keyboard inputs: - Show/Hide the currently selected panel. - Move the currently selected panel to the top or bottom of the stack. - Move the currently selected panel to a new position on the screen (using arrow keys for movement). 4. Update the display to reflect changes in the panel stack. 5. Ensure the program gracefully exits when the user presses a specific key (e.g., \'q\'). **Input and Output:** - There are no explicit input parameters to the function. - The function should directly interact with the curses environment to display and manage panels. **Constraints:** - You must use the `curses.panel` library for panel management. - The program should run in a terminal that supports curses. **Example Flow:** 1. On starting the program, three panels are displayed on the screen. 2. User navigates between panels using arrow keys. 3. User presses a specific key to hide the current panel, another key to show it again. 4. User moves a panel to a different position on the screen. 5. User stacks panels at the top or bottom of the stack. 6. The program exits when the user presses \'q\'. **Hints:** - Use the `curses.panel.new_panel(win)` function to create new panels. - Use methods like `panel.top()`, `panel.bottom()`, `panel.hide()`, `panel.show()`, and `panel.move()` for panel management. - Use `curses.panel.update_panels()` followed by `curses.doupdate()` to refresh the screen. **Example Code Skeleton:** ```python import curses import curses.panel def manage_panels(): def draw_panel(panel, text, color_pair): win = panel.window() win.clear() win.bkgd(\' \', curses.color_pair(color_pair)) win.border(0) win.addstr(2, 2, text) panel.show() panel.update_panels() curses.doupdate() def main(stdscr): curses.start_color() curses.init_pair(1, curses.COLOR_RED, curses.COLOR_BLACK) curses.init_pair(2, curses.COLOR_GREEN, curses.COLOR_BLACK) curses.init_pair(3, curses.COLOR_BLUE, curses.COLOR_BLACK) height, width = stdscr.getmaxyx() win1 = curses.newwin(height // 3, width // 3, 0, 0) win2 = curses.newwin(height // 3, width // 3, height // 3, width // 3) win3 = curses.newwin(height // 3, width // 3, 2 * height // 3, 2 * width // 3) panel1 = curses.panel.new_panel(win1) panel2 = curses.panel.new_panel(win2) panel3 = curses.panel.new_panel(win3) draw_panel(panel1, \\"Panel 1\\", 1) draw_panel(panel2, \\"Panel 2\\", 2) draw_panel(panel3, \\"Panel 3\\", 3) current_panel = panel1 while True: key = stdscr.getch() if key == ord(\'q\'): break # Add logic for panel management based on key input curses.endwin() curses.wrapper(main) ``` **Note:** This question assumes familiarity with the `curses` module and basic terminal handling in Python. The provided example skeleton is a starting point, and comprehensive implementation with more features and error handling is expected in the final submission.","solution":"import curses import curses.panel def manage_panels(): def draw_panel(panel, text, color_pair): win = panel.window() win.clear() win.bkgd(\' \', curses.color_pair(color_pair)) win.border(0) win.addstr(2, 2, text) panel.top() curses.panel.update_panels() curses.doupdate() def main(stdscr): curses.curs_set(0) curses.start_color() curses.init_pair(1, curses.COLOR_RED, curses.COLOR_BLACK) curses.init_pair(2, curses.COLOR_GREEN, curses.COLOR_BLACK) curses.init_pair(3, curses.COLOR_BLUE, curses.COLOR_BLACK) height, width = stdscr.getmaxyx() win1 = curses.newwin(height // 3, width // 3, 0, 0) win2 = curses.newwin(height // 3, width // 3, height // 3, width // 3) win3 = curses.newwin(height // 3, width // 3, 2 * height // 3, 2 * width // 3) panel1 = curses.panel.new_panel(win1) panel2 = curses.panel.new_panel(win2) panel3 = curses.panel.new_panel(win3) draw_panel(panel1, \\"Panel 1\\", 1) draw_panel(panel2, \\"Panel 2\\", 2) draw_panel(panel3, \\"Panel 3\\", 3) panels = [panel1, panel2, panel3] current_index = 0 while True: key = stdscr.getch() if key == ord(\'q\'): break elif key == curses.KEY_RIGHT: current_index = (current_index + 1) % len(panels) panels[current_index].top() elif key == curses.KEY_LEFT: current_index = (current_index - 1) % len(panels) panels[current_index].top() elif key == ord(\'h\'): panels[current_index].hide() elif key == ord(\'s\'): panels[current_index].show() elif key == curses.KEY_UP: y, x = panels[current_index].window().getbegyx() panels[current_index].move(max(y - 1, 0), x) elif key == curses.KEY_DOWN: y, x = panels[current_index].window().getbegyx() panels[current_index].move(min(y + 1, height - 1), x) elif key == ord(\'t\'): panels[current_index].top() elif key == ord(\'b\'): panels[current_index].bottom() curses.panel.update_panels() curses.doupdate() curses.wrapper(main)"},{"question":"You are required to implement a logging utility using the `syslog` module that handles logging messages from different components of a simulated application. The utility should demonstrate your understanding of setting up the syslog, logging messages with different priority levels, applying log masks, and properly closing the log session. Task Description 1. **Initialize Logging**: - Create a function `initialize_logging(ident, logoption, facility)` that sets up the logging options. - `ident` (string): Identifier to prepend to log messages. - `logoption` (int): Log option flags (e.g., `syslog.LOG_PID | syslog.LOG_CONS`). - `facility` (int): Facility code (e.g., `syslog.LOG_USER`). 2. **Log Messages**: - Create a function `log_message(priority, message)` that logs a message with the specified priority. - `priority` (int): Log priority level (e.g., `syslog.LOG_ERR`). - `message` (string): The message to log. 3. **Set Priority Mask**: - Create a function `set_priority_mask(priorities)` that sets up a mask to only allow logging for the specified priorities. - `priorities` (List[int]): A list of priority levels (e.g., `[syslog.LOG_ERR, syslog.LOG_WARNING]`). 4. **Close Logging**: - Create a function `close_logging()` to close the logging session properly. 5. **Example Usage**: - Demonstrate the usage of these functions in a simulation where: - Logging is initialized with `ident=\\"TestApp\\"`, log options for including the process ID and writing to console, and using the `LOG_USER` facility. - Logs messages of different priorities and then sets a priority mask to filter out lower priority messages. - Finally, closes the logging session. Constraints - You may assume the platform supports the `syslog` module. - Properly handle any potential exceptions. Input and Output Formats - Input: Parameters provided to the described functions. - Output: No explicit return values. Log messages should be sent to the system logger as per the configuration. Example ```python def initialize_logging(ident, logoption, facility): # Implement function def log_message(priority, message): # Implement function def set_priority_mask(priorities): # Implement function def close_logging(): # Implement function # Example Usage: initialize_logging(\\"TestApp\\", syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER) log_message(syslog.LOG_INFO, \\"Application started\\") log_message(syslog.LOG_ERR, \\"An error occurred\\") set_priority_mask([syslog.LOG_ERR, syslog.LOG_ALERT, syslog.LOG_CRIT]) log_message(syslog.LOG_WARNING, \\"This warning should not appear due to mask\\") close_logging() ```","solution":"import syslog def initialize_logging(ident, logoption, facility): Initializes the syslog with the given identifier, log options, and facility. syslog.openlog(ident, logoption, facility) def log_message(priority, message): Logs a message with the given priority level. syslog.syslog(priority, message) def set_priority_mask(priorities): Sets a mask that allows logging only for the specified priorities. mask = 0 for priority in priorities: mask |= syslog.LOG_MASK(priority) syslog.setlogmask(mask) def close_logging(): Closes the syslog. syslog.closelog()"},{"question":"# Advanced Python Garbage Collection Management **Problem Statement:** You are responsible for a long-running server application in Python that has recently been experiencing performance issues due to memory leaks. You suspect that reference cycles are being created and not properly garbage collected. Your task is to implement a function that leverages the `gc` module to monitor, debug, and manage garbage collection for this application. Additionally, you need to provide a mechanism to help identify objects forming reference cycles and ensure they are collected correctly or at least identifiable for debugging purposes. **Requirements:** 1. Write a function `configure_gc_debug()` that: - Enables garbage collection if disabled. - Sets the garbage collection debug flag to `gc.DEBUG_LEAK` to print detailed information about collectable and uncollectable objects. 2. Write a function `collect_and_report()` that: - Triggers a full garbage collection. - Returns the number of objects collected and the number of uncollectable objects found. - If uncollectable objects are found, prints details of these objects (i.e., their type and reference detail). 3. Write a function `get_uncollectable_objects()` that: - Returns a list of objects in `gc.garbage` for further inspection. 4. Write a function `optimize_gc_threshold(threshold0: int, threshold1: int = 5, threshold2: int = 10)` that: - Adjusts the garbage collection thresholds to provided values in order to tune the collection frequency. **Input and Output Formats:** - `configure_gc_debug()` takes no input and returns nothing. - `collect_and_report()` takes no input and returns a tuple `(collected: int, uncollectable: int)`. - `get_uncollectable_objects()` takes no input and returns a list of uncollectable objects. - `optimize_gc_threshold(threshold0, threshold1, threshold2)` takes three integer inputs and returns nothing. **Constraints:** - The `optimize_gc_threshold()` function should ensure that `threshold0` is greater than 0. If not, it should raise a `ValueError`. **Performance Requirements:** - The functions should handle a large number of objects efficiently and must be tested to ensure they do not introduce significant overhead. **Example:** ```python import gc # Part 1 configure_gc_debug() # Should enable gc and set debug flags # Part 2 collected, uncollectable = collect_and_report() # Prints detailed information about collectable and uncollectable objects # Part 3 uncollectable_objects = get_uncollectable_objects() # Returns a list of uncollectable objects for inspection # Part 4 optimize_gc_threshold(700, 10, 10) # Adjusts the GC thresholds ``` **Note:** Make sure to handle and document any edge cases or exceptional behavior in your solution. Provide test cases to demonstrate the functionality.","solution":"import gc def configure_gc_debug(): Enables garbage collection if disabled and sets the debug flag to gc.DEBUG_LEAK. gc.enable() gc.set_debug(gc.DEBUG_LEAK) def collect_and_report(): Triggers a full garbage collection and returns the number of objects collected and the number of uncollectable objects found. collected = gc.collect() uncollectable = len(gc.garbage) if uncollectable > 0: for obj in gc.garbage: print(f\\"Uncollectable object: type={type(obj)}, {repr(obj)}\\") return (collected, uncollectable) def get_uncollectable_objects(): Returns a list of uncollectable objects from gc.garbage. return gc.garbage def optimize_gc_threshold(threshold0: int, threshold1: int = 5, threshold2: int = 10): Adjusts the garbage collection thresholds to provided values. Parameters: threshold0: int - new threshold for generation 0 threshold1: int - new threshold for generation 1 (default is 5) threshold2: int - new threshold for generation 2 (default is 10) Raises: ValueError: if threshold0 is not greater than 0 if threshold0 <= 0: raise ValueError(\\"threshold0 must be greater than 0\\") gc.set_threshold(threshold0, threshold1, threshold2)"},{"question":"Objective Write a Python script that will perform specific tasks using the Unix password database. You are required to demonstrate your comprehension of the `pwd` module by implementing the following functions. Task 1: Implement Function to Retrieve User Information Create a function `get_user_info(identifier)` that takes either a numeric user ID (UID) or a login name (string) and returns a dictionary containing user information. If the identifier is not found, return `None`. Task 2: List Home Directories of All Users Create a function `list_home_directories()` that returns a list of all user home directories present in the system. Task 3: User ID to Login Name Map Create a function `uid_to_login_map()` that returns a dictionary mapping each numeric user ID (UID) to the corresponding login name for all users. Input and Output Specifications: 1. `get_user_info(identifier)`: - **Input**: - `identifier` (int or str) - A UID (e.g., 1001) or a login name (e.g., \\"john\\"). - **Output**: - Dictionary with keys as \'name\', \'password\', \'uid\', \'gid\', \'gecos\', \'dir\', \'shell\'. - Return `None` if user cannot be found. - **Example**: ```python get_user_info(1001) # Output -> {\'name\': \'john\', \'password\': \'x\', \'uid\': 1001, \'gid\': 1001, \'gecos\': \'John Doe\', \'dir\': \'/home/john\', \'shell\': \'/bin/bash\'} ``` 2. `list_home_directories()`: - **Input**: None - **Output**: - List of strings representing the home directories of all users. - **Example**: ```python list_home_directories() # Output -> [\'/home/john\', \'/home/jane\', \'/root\', \'/home/guest\'] ``` 3. `uid_to_login_map()`: - **Input**: None - **Output**: - Dictionary mapping UIDs to login names. - **Example**: ```python uid_to_login_map() # Output -> {1000: \'root\', 1001: \'john\', 1002: \'jane\'} ``` Constraints: - Your solution should use the `pwd` module to retrieve required data. - Ensure that your functions handle exceptions (such as missing user entries). Performance Requirements: - Your functions should handle typical Unix system loads efficiently. Submission: Provide a `.py` file containing the implementation of the three functions described above.","solution":"import pwd def get_user_info(identifier): Retrieves user information based on UID or login name. try: if isinstance(identifier, int): user = pwd.getpwuid(identifier) else: user = pwd.getpwnam(identifier) return { \'name\': user.pw_name, \'password\': user.pw_passwd, \'uid\': user.pw_uid, \'gid\': user.pw_gid, \'gecos\': user.pw_gecos, \'dir\': user.pw_dir, \'shell\': user.pw_shell } except KeyError: return None def list_home_directories(): Lists home directories of all users. return [user.pw_dir for user in pwd.getpwall()] def uid_to_login_map(): Maps UIDs to login names. return {user.pw_uid: user.pw_name for user in pwd.getpwall()}"},{"question":"# Coding Assessment Question: Utilizing MPS Backend in PyTorch The objective of this exercise is to assess your understanding of using the MPS backend in PyTorch to leverage GPU acceleration on MacOS devices. You will write code to perform a sequence of operations involving tensor creation, manipulation, and model inference using the MPS device. Problem Statement You are required to implement a function `train_on_mps` which takes the following parameters: - `model` (torch.nn.Module): A PyTorch neural network model. - `data` (torch.Tensor): Input data tensor. - `target` (torch.Tensor): Target data tensor. - `epochs` (int): The number of epochs to train the model. The function should: 1. Check if the MPS backend is available. If not, print an appropriate error message and return. 2. Move the provided model to the MPS device. 3. Move the input `data` and `target` tensors to the MPS device. 4. Define a simple loss function (e.g., Mean Squared Error) and an optimizer (e.g., SGD). 5. Run a training loop for the specified number of epochs where: - The model predictions are computed. - The loss is calculated and printed for each epoch. - The gradients are zeroed. - Backpropagation is performed. - The optimizer updates the model parameters. # Function Signature ```python def train_on_mps( model: torch.nn.Module, data: torch.Tensor, target: torch.Tensor, epochs: int ) -> None: ``` # Input - `model` (torch.nn.Module): A PyTorch neural network model. - `data` (torch.Tensor): Input data tensor of shape `(N, D)` where `N` is the number of samples and `D` is the number of features. - `target` (torch.Tensor): Target data tensor of shape `(N, T)` where `N` is the number of samples and `T` is the number of output targets. - `epochs` (int): Number of epochs to train the model. # Output - This function does not return anything. Instead, it prints loss values for each epoch to the standard output. # Constraints - Ensure that the function runs operations on the GPU if the MPS device is available. - The function should work seamlessly for MacOS devices that support the MPS backend. # Example Usage ```python import torch import torch.nn as nn import torch.optim as optim # Define a sample model class SimpleModel(nn.Module): def __init__(self, input_dim, output_dim): super(SimpleModel, self).init() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): return self.linear(x) # Create random input and target data input_dim = 10 output_dim = 2 N = 100 data = torch.randn(N, input_dim) target = torch.randn(N, output_dim) # Initialize the model model = SimpleModel(input_dim, output_dim) # Train the model on GPU using MPS backend train_on_mps(model, data, target, epochs=5) ``` In this example, you are required to check the availability of the MPS device, move the model and data to the MPS device, and perform training for a specified number of epochs, printing the loss at each epoch.","solution":"import torch import torch.nn as nn import torch.optim as optim def train_on_mps( model: nn.Module, data: torch.Tensor, target: torch.Tensor, epochs: int ) -> None: if not torch.backends.mps.is_available(): print(\\"MPS device not available.\\") return device = torch.device(\\"mps\\") model.to(device) data = data.to(device) target = target.to(device) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(epochs): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\\") # Define a sample model for demonstration purposes class SimpleModel(nn.Module): def __init__(self, input_dim, output_dim): super(SimpleModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): return self.linear(x)"},{"question":"**Coding Assessment Question: Tensor View Operations and Contiguity in PyTorch** **Objective:** Write a function `tensor_manipulation` that takes a 3D tensor `input_tensor` and performs the following operations: 1. Obtain a view of the tensor by changing its shape. 2. Verify if the new tensor view is contiguous. 3. If the tensor view is non-contiguous, make it contiguous. 4. Return a union of contiguous and non-contiguous states and show that they point to different underlying data. **Function Signature:** ```python def tensor_manipulation(input_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: pass ``` **Input:** - `input_tensor` (torch.Tensor): A 3D tensor of shape `(d1, d2, d3)`. **Output:** - The function should return a tuple containing two tensors: - `view_tensor` (torch.Tensor): The non-contiguous view of the input tensor. - `contiguous_tensor` (torch.Tensor): The contiguous tensor obtained from the non-contiguous view tensor. **Constraints/Limitations:** - Ensure that reshaping or transposing leads to a non-contiguous tensor view. - Illustrate the shared underlying data concept by modifying the `view_tensor` and show that it affects the `input_tensor`. **Performance Requirements:** - The solution should handle tensors of moderate sizes up to shape `(100, 100, 100)` efficiently. - Avoid unnecessary data copying. **Example:** ```python import torch def tensor_manipulation(input_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: # Obtain a non-contiguous view of the tensor by transposing it view_tensor = input_tensor.transpose(0, 1) # Now of shape (d2, d1, d3) # Verify if it\'s non-contiguous if not view_tensor.is_contiguous(): contiguous_tensor = view_tensor.contiguous() else: contiguous_tensor = view_tensor # Modify the view tensor and show it impacts the base tensor view_tensor[0][0][0] = 3.14 return view_tensor, contiguous_tensor # Example usage input_tensor = torch.rand(4, 5, 6) view_tensor, contiguous_tensor = tensor_manipulation(input_tensor) # Verify results print(\\"Input Tensor:\\") print(input_tensor) print(\\"View Tensor:\\") print(view_tensor) print(\\"Contiguous Tensor:\\") print(contiguous_tensor) assert input_tensor.storage().data_ptr() == view_tensor.storage().data_ptr() # Should be true assert input_tensor.storage().data_ptr() != contiguous_tensor.storage().data_ptr() # Should be false ``` The above function manipulates the given 3D tensor using tensor views and demonstrates the effects of contiguity. **Note:** Remember to install `pytorch` before running this code.","solution":"import torch from typing import Tuple def tensor_manipulation(input_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: Takes a 3D tensor `input_tensor` and performs the following operations to return a tuple containing a non-contiguous tensor view and a contiguous tensor obtained from the non-contiguous view tensor. Args: input_tensor (torch.Tensor): A 3D tensor of shape (d1, d2, d3) Returns: Tuple[torch.Tensor, torch.Tensor]: A tuple containing: - view_tensor (torch.Tensor): A non-contiguous view of the input tensor. - contiguous_tensor (torch.Tensor): A contiguous tensor obtained from the non-contiguous view tensor. # Obtain a non-contiguous view of the tensor by transposing it view_tensor = input_tensor.transpose(0, 1) # Now of shape (d2, d1, d3) # Verify if it\'s non-contiguous if not view_tensor.is_contiguous(): contiguous_tensor = view_tensor.contiguous() else: contiguous_tensor = view_tensor # Modify the view tensor and show it impacts the base tensor view_tensor[0][0][0] = 3.14 return view_tensor, contiguous_tensor"},{"question":"# Python Coding Assessment Question Problem Statement In this task, you are required to implement a Python generator function that yields values according to a custom pattern. You will also need to write a function to check if a given object is a generator created by your custom generator function. Implement the following two functions: 1. **custom_pattern_generator(start, stop, step)**: - **Input**: - `start` (int): The starting number of the sequence. - `stop` (int): The end limit of the sequence (non-inclusive). - `step` (int): The step size between each number in the sequence. - **Output**: A generator object that yields numbers starting from `start`, incremented by `step`, and stops before `stop`. - **Constraints**: The function should handle positive and negative step values. Example: ```python gen = custom_pattern_generator(0, 10, 2) print(list(gen)) # Output: [0, 2, 4, 6, 8] ``` 2. **is_custom_generator(obj)**: - **Input**: - `obj` (any): The object to check. - **Output**: A boolean value `True` if `obj` is a generator object created by `custom_pattern_generator`; otherwise, `False`. Example: ```python gen = custom_pattern_generator(0, 10, 2) print(is_custom_generator(gen)) # Output: True print(is_custom_generator([0, 2, 4, 6, 8])) # Output: False ``` Detailed Requirements - The `custom_pattern_generator` function must use Python\'s `yield` keyword to create the generator. - The `is_custom_generator` function should leverage internal generator checking functions (`PyGen_Check`, `PyGen_CheckExact`) if available, or use the appropriate Pythonic way to determine if an object is a generator. - Your code should be efficient and handle large ranges without consuming excessive memory. ```python def custom_pattern_generator(start, stop, step): # Implement the generator logic here pass def is_custom_generator(obj): # Implement the generator checking logic here pass # Example usage: # gen = custom_pattern_generator(0, 10, 2) # print(list(gen)) # Output: [0, 2, 4, 6, 8] # print(is_custom_generator(gen)) # Output: True # print(is_custom_generator([0, 2, 4, 6, 8])) # Output: False ``` Note Please ensure your solution works for both Python 3.7 and above. The solution must be well-tested and handle edge cases appropriately.","solution":"def custom_pattern_generator(start, stop, step): A generator function that yields numbers starting from `start`, incremented by `step`, and stops before `stop`. current = start if step > 0: while current < stop: yield current current += step else: while current > stop: yield current current += step def is_custom_generator(obj): Check if the given object is a generator created by `custom_pattern_generator`. import types return isinstance(obj, types.GeneratorType)"},{"question":"**Objective:** Test your ability to manipulate and analyze `DataFrame` objects using pandas\' merging, concatenating, and comparison functionalities. Problem Statement: You are given sales data for two departments (Electronics and Clothing) and customer demographic data. You need to combine these datasets to answer specific questions about sales performance and customer behavior. Dataset: 1. Sales data for Electronics: ```python import pandas as pd electronics_sales = pd.DataFrame({ \\"customer_id\\": [1, 2, 3, 4], \\"amount\\": [200, 150, 400, 100], \\"date\\": [\\"2022-01-01\\", \\"2022-01-02\\", \\"2022-01-03\\", \\"2022-01-04\\"] }) ``` 2. Sales data for Clothing: ```python clothing_sales = pd.DataFrame({ \\"customer_id\\": [2, 3, 5, 6], \\"amount\\": [100, 200, 150, 300], \\"date\\": [\\"2022-01-02\\", \\"2022-01-03\\", \\"2022-01-04\\", \\"2022-01-05\\"] }) ``` 3. Customer demographic data: ```python customer_data = pd.DataFrame({ \\"customer_id\\": [1, 2, 3, 4, 5, 6], \\"name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\", \\"Frank\\"], \\"age\\": [34, 45, 23, 35, 54, 29], \\"city\\": [\\"New York\\", \\"Los Angeles\\", \\"Chicago\\", \\"Houston\\", \\"Phoenix\\", \\"Philadelphia\\"] }) ``` # Tasks: 1. **Concatenate Sales Data:** Concatenate the `electronics_sales` and `clothing_sales` DataFrames into a single DataFrame named `all_sales`, ignoring the index of the individual dataframes. 2. **Merge Sales with Customer Data:** Merge the `all_sales` DataFrame with `customer_data` on the `customer_id` column to get complete sales information including demographic details. 3. **Total Sales by City:** Calculate the total sales amount by city and return a DataFrame with columns `city` and `total_sales`. 4. **Compare Sales between Two Dates:** Using the `compare` method, compare the sales on `2022-01-02` between the two departments. Return the resulting differences. 5. **Fill Missing Customer Data:** Assuming some customer demographic information might be missing in future datasets, demonstrate how to use the `combine_first` method to fill missing customer information from an existing `customer_backup` DataFrame: ```python customer_backup = pd.DataFrame({ \\"customer_id\\": [1, 2, 3], \\"name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\"], \\"age\\": [34, 45, 25], \\"city\\": [\\"New York\\", \\"Los Angeles\\", \\"Chicago\\"] }) ``` # Function Signature Provide a function `analyze_sales` which implements the above tasks and returns the respective results for each task in the form of DataFrames or relevant data structures. ```python def analyze_sales(electronics_sales, clothing_sales, customer_data): # Task 1: Concatenate Sales Data all_sales = pd.concat([electronics_sales, clothing_sales], ignore_index=True, sort=False) # Task 2: Merge Sales with Customer Data merged_sales = pd.merge(all_sales, customer_data, on=\\"customer_id\\", how=\\"left\\") # Task 3: Total Sales by City total_sales_by_city = merged_sales.groupby(\\"city\\")[\\"amount\\"].sum().reset_index().rename(columns={\\"amount\\": \\"total_sales\\"}) # Task 4: Compare Sales between Two Dates electronics_sales_filtered = electronics_sales[electronics_sales[\\"date\\"] == \\"2022-01-02\\"].set_index(\\"customer_id\\") clothing_sales_filtered = clothing_sales[clothing_sales[\\"date\\"] == \\"2022-01-02\\"].set_index(\\"customer_id\\") sales_comparison = electronics_sales_filtered[\\"amount\\"].compare(clothing_sales_filtered[\\"amount\\"], align_axis=1).reset_index() # Task 5: Fill Missing Customer Data customer_data_combined = customer_data.combine_first(customer_backup) return all_sales, merged_sales, total_sales_by_city, sales_comparison, customer_data_combined # Example usage analyze_sales(electronics_sales, clothing_sales, customer_data) ``` # Constraints: - Assume all dates are in the format `YYYY-MM-DD`. - Assume all necessary libraries are imported. - Ensure your function returns the expected formats for each task as specified.","solution":"import pandas as pd def analyze_sales(electronics_sales, clothing_sales, customer_data, customer_backup): # Task 1: Concatenate Sales Data all_sales = pd.concat([electronics_sales, clothing_sales], ignore_index=True, sort=False) # Task 2: Merge Sales with Customer Data merged_sales = pd.merge(all_sales, customer_data, on=\\"customer_id\\", how=\\"left\\") # Task 3: Total Sales by City total_sales_by_city = merged_sales.groupby(\\"city\\")[\\"amount\\"].sum().reset_index().rename(columns={\\"amount\\": \\"total_sales\\"}) # Task 4: Compare Sales between Two Dates electronics_sales_filtered = electronics_sales[electronics_sales[\\"date\\"] == \\"2022-01-02\\"].set_index(\\"customer_id\\") clothing_sales_filtered = clothing_sales[clothing_sales[\\"date\\"] == \\"2022-01-02\\"].set_index(\\"customer_id\\") sales_comparison = electronics_sales_filtered[\\"amount\\"].compare(clothing_sales_filtered[\\"amount\\"], align_axis=1).reset_index() # Task 5: Fill Missing Customer Data customer_data_combined = customer_data.combine_first(customer_backup) return all_sales, merged_sales, total_sales_by_city, sales_comparison, customer_data_combined"},{"question":"# Token Replacement and Code Reconstruction In this task, you are required to create a function that tokenizes a given Python source code, modifies the tokens according to some rules, and reconstructs the modified source code from the tokens. Specifically, you need to replace all occurrences of certain keywords with new words and return the modified source code as a string. # Function Signature ```python def replace_keywords(source_code: str, replacements: dict) -> str: Modify the given Python source code by replacing specified keywords. :param source_code: A string representing the Python source code to modify. :param replacements: A dictionary where the keys are the keywords to be replaced and the values are the new words to replace them with. :return: A string containing the modified Python source code. ``` # Input - `source_code`: A string representing the valid Python source code to modify. - `replacements`: A dictionary where the keys are strings (keywords from the original source code) and the values are the strings (new words) to replace them with. # Output - A string containing the modified Python source code. # Constraints - The source code will be a syntactically valid Python script. - The replacements dictionary will only contain keys that are valid names in the original source code. - The length of the source code will not exceed 10,000 bytes. - The number of keywords in the replacements dictionary will not exceed 20. - Performance should be efficient enough for real-time processing of source code files of the given size. # Instructions 1. Use `tokenize.generate_tokens` to tokenize the input Python source code. 2. Replace each occurrence of specific keywords (if exist) using the provided `replacements` dictionary. 3. Use `tokenize.untokenize` to reconstruct the modified source code from the tokens. 4. Return the new source code as a string. # Example ```python source_code = \'\'\' def say_hello(): print(\\"Hello, World!\\") say_hello() \'\'\' replacements = { \'say_hello\': \'greet\', \'print\': \'output\' } new_source_code = replace_keywords(source_code, replacements) print(new_source_code) ``` Expected Output: ```python \'\'\' def greet(): output(\\"Hello, World!\\") greet() \'\'\' ``` # Note You may assume the input source code is always valid and will not encounter any encoding errors.","solution":"import tokenize from io import StringIO def replace_keywords(source_code: str, replacements: dict) -> str: Modify the given Python source code by replacing specified keywords. :param source_code: A string representing the Python source code to modify. :param replacements: A dictionary where the keys are the keywords to be replaced and the values are the new words to replace them with. :return: A string containing the modified Python source code. # Convert the source code string to a stream tokens = tokenize.generate_tokens(StringIO(source_code).readline) result_tokens = [] for token in tokens: token_type, token_string, start, end, line = token # Check if the current token is a NAME and if it should be replaced if token_type == tokenize.NAME and token_string in replacements: token_string = replacements[token_string] result_tokens.append((token_type, token_string, start, end, line)) # Convert the list of tokens back to source code new_source_code = tokenize.untokenize(result_tokens) return new_source_code"},{"question":"Objective: Create a custom Python type using the C-API, utilizing the `PyTypeObject` structure from the given documentation. This will help demonstrate your comprehension of designing and implementing new types that follow the Python object model and interactions. Problem Statement: You are required to design and implement a custom Python extension type called `CounterType`. This type should: 1. Hold an integer value. 2. Provide methods to increment and decrement the counter. 3. Support printing of its current value. 4. Implement addition and subtraction operations. 5. Support iteration, yielding integers from the counter value down to zero. Your task is to implement this type in C, ensuring it appropriately integrates with Python\'s type system using the `PyTypeObject` structure. Requirements: 1. **Type Definition and Initialization:** - Define the structure representing objects of your `CounterType`. - Initialize the type using `PyType_Ready`. 2. **Basic Slots:** - `tp_name`: `\\"custom.CounterType\\"` - `tp_basicsize`: Size of your custom structure. - `tp_dealloc`: Function to deallocate instances. - `tp_repr` and `tp_str`: Functions to return string representation. - `tp_iter` and `tp_iternext`: Iterator functions. 3. **Number Slots:** - `nb_add`: Implement addition which returns a new `CounterType`. - `nb_subtract`: Implement subtraction which returns a new `CounterType`. 4. **Methods:** Implement the following methods: - `increment(self)`: Increments the counter. - `decrement(self)`: Decrements the counter. 5. **Module Initialization:** - Create a Python module containing the `CounterType`. Implementation Details: **Structure Definition:** ```c typedef struct { PyObject_HEAD int value; } CounterObject; ``` **Required Functions:** 1. `Counter_new`: Function for creating a new object. 2. `Counter_dealloc`: Function for deallocating an object. 3. `Counter_repr`: Function returning the string representation. 4. `Counter_increment`: Method that increments the counter. 5. `Counter_decrement`: Method that decrements the counter. 6. `Counter_add`: Implementation of the `+` operator. 7. `Counter_subtract`: Implementation of the `-` operator. 8. `Counter_iter`: Function returning an iterator. 9. `Counter_iternext`: Function returning the next item. **Example Usage in Python:** ```python import custom counter = custom.CounterType(10) print(counter) # Output should be the current value of the counter counter.increment() print(counter) # Counter value should be incremented counter.decrement() print(counter) # Counter value should be decremented # Addition and Subtraction c1 = custom.CounterType(5) c2 = custom.CounterType(10) c3 = c1 + c2 # New CounterType with combined value c4 = c2 - c1 # New CounterType with subtracted value # Iteration for num in counter: print(num) # Outputs numbers from counter value down to 0 ``` This question assesses the understanding of creating extended Python types with C, managing their lifecycle, implementing numeric protocols, and utilizing the basic and advanced slots of the `PyTypeObject`.","solution":"# For simplicity, we will implement the CounterType in pure Python to simulate the requested behavior. class CounterType: def __init__(self, initial=0): self.value = initial def __repr__(self): return f\\"CounterType({self.value})\\" def increment(self): self.value += 1 def decrement(self): self.value -= 1 def __add__(self, other): if isinstance(other, CounterType): return CounterType(self.value + other.value) return NotImplemented def __sub__(self, other): if isinstance(other, CounterType): return CounterType(self.value - other.value) return NotImplemented def __iter__(self): self._iter_value = self.value return self def __next__(self): if self._iter_value < 0: raise StopIteration value = self._iter_value self._iter_value -= 1 return value"},{"question":"# Complex Data Compression and Archiving Task Objective Design a Python program that demonstrates your ability to use the `gzip`, `bz2`, `lzma`, `zipfile`, and `tarfile` modules for handling data compression and archiving in Python 3.10. This comprehensive task will require you to implement several functionalities using these modules. Task Description 1. **File Generation**: - Write a function `generate_file(file_path, contents)` that creates a file at `file_path` and writes the string `contents` into it. 2. **Compression**: - Write a function `compress_files(file_paths, archive_path, method=\'gzip\')` that compresses a list of files given by `file_paths` into a single archive at `archive_path` using the specified `method`. Supported methods are \'gzip\', \'bz2\', and \'lzma\'. 3. **Decompression**: - Write a function `decompress_file(archive_path, output_dir, method=\'gzip\')` that extracts the contents of a compressed archive located at `archive_path` into the directory `output_dir` using the specified `method`. 4. **Archive Creation**: - Write a function `create_zip_archive(file_paths, zip_path)` that creates a ZIP archive at `zip_path` containing all the files listed in `file_paths`. - Write a function `create_tar_archive(file_paths, tar_path, mode=\'w:gz\')` that creates a tar archive at `tar_path` containing all the files listed in `file_paths`. The `mode` parameter controls the compression format and can be \'w\', \'w:gz\', \'w:bz2\', or \'w:xz\' for no compression, gzip, bzip2, and lzma respectively. 5. **Archive Extraction**: - Write a function `extract_zip_archive(zip_path, output_dir)` that extracts all files from a ZIP archive located at `zip_path` into `output_dir`. - Write a function `extract_tar_archive(tar_path, output_dir)` that extracts all files from a tar archive located at `tar_path` into `output_dir`. Input and Output Formats - For the function `generate_file(file_path, contents)`: - **Input**: - `file_path` (str): Path where the file will be created. - `contents` (str): Contents to be written in the file. - **Output**: None (Creates a file at `file_path` with specified `contents`). - For the function `compress_files(file_paths, archive_path, method)`: - **Input**: - `file_paths` (List of str): List of paths of files to be compressed. - `archive_path` (str): Path where the compressed archive will be created. - `method` (str): Compression method, one of {\'gzip\', \'bz2\', \'lzma\'}. - **Output**: None (Creates a compressed archive at `archive_path` with specified `method`). - For the function `decompress_file(archive_path, output_dir, method)`: - **Input**: - `archive_path` (str): Path to the compressed archive to be decompressed. - `output_dir` (str): Directory where the decompressed content will be extracted. - `method` (str): Decompression method, one of {\'gzip\', \'bz2\', \'lzma\'}. - **Output**: None (Extracts contents of the archive into `output_dir`). - For the function `create_zip_archive(file_paths, zip_path)`: - **Input**: - `file_paths` (List of str): List of paths of files to be included in ZIP archive. - `zip_path` (str): Path where the ZIP archive will be created. - **Output**: None (Creates a ZIP archive at `zip_path`). - For the function `create_tar_archive(file_paths, tar_path, mode)`: - **Input**: - `file_paths` (List of str): List of paths of files to be included in tar archive. - `tar_path` (str): Path where the tar archive will be created. - `mode` (str): tarfile mode, one of {\'w\', \'w:gz\', \'w:bz2\', \'w:xz\'}. - **Output**: None (Creates a tar archive at `tar_path` with specified `mode`). - For the function `extract_zip_archive(zip_path, output_dir)`: - **Input**: - `zip_path` (str): Path to the ZIP archive to be extracted. - `output_dir` (str): Directory where the extracted content will be placed. - **Output**: None (Extracts contents of the ZIP archive into `output_dir`). - For the function `extract_tar_archive(tar_path, output_dir)`: - **Input**: - `tar_path` (str): Path to the tar archive to be extracted. - `output_dir` (str): Directory where the extracted content will be placed. - **Output**: None (Extracts contents of the tar archive into `output_dir`). Constraints and Limitations - You can assume that all directories given will already exist, and you do not need to handle directory creation. - File paths provided will always be valid and you don\'t need to handle invalid paths. - The size of files to be processed will be within a reasonable range and won\'t require extensive memory management or optimization. Performance Requirements Although there are no strict performance constraints, your solution should efficiently handle moderate file sizes and take advantage of Pythonâ€™s built-in libraries for compression and archiving.","solution":"import gzip import bz2 import lzma import zipfile import tarfile from pathlib import Path def generate_file(file_path, contents): Creates a file at `file_path` and writes `contents` into it. with open(file_path, \'w\') as f: f.write(contents) def compress_files(file_paths, archive_path, method=\'gzip\'): Compresses a list of files into a single archive using specified method. Supported methods: \'gzip\', \'bz2\', \'lzma\'. if method not in (\'gzip\', \'bz2\', \'lzma\'): raise ValueError(\\"Unsupported compression method\\") with open(archive_path, \'wb\') as archive_file: if method == \'gzip\': compressor = gzip.GzipFile(fileobj=archive_file, mode=\'wb\') elif method == \'bz2\': compressor = bz2.BZ2File(archive_file, \'wb\') elif method == \'lzma\': compressor = lzma.LZMAFile(archive_file, \'wb\') for file in file_paths: with open(file, \'rb\') as f: compressor.write(f.read()) compressor.close() def decompress_file(archive_path, output_dir, method=\'gzip\'): Extracts the contents of a compressed archive into specified directory. Supported methods: \'gzip\', \'bz2\', \'lzma\'. if method not in (\'gzip\', \'bz2\', \'lzma\'): raise ValueError(\\"Unsupported decompression method\\") Path(output_dir).mkdir(parents=True, exist_ok=True) base_name = Path(archive_path).stem output_path = Path(output_dir) / base_name with open(archive_path, \'rb\') as archive_file: if method == \'gzip\': decompressor = gzip.GzipFile(fileobj=archive_file, mode=\'rb\') elif method == \'bz2\': decompressor = bz2.BZ2File(archive_file, \'rb\') elif method == \'lzma\': decompressor = lzma.LZMAFile(archive_file, \'rb\') with open(output_path, \'wb\') as out_file: out_file.write(decompressor.read()) decompressor.close() def create_zip_archive(file_paths, zip_path): Creates a ZIP archive containing specified files. with zipfile.ZipFile(zip_path, \'w\') as zipf: for file in file_paths: zipf.write(file, arcname=Path(file).name) def create_tar_archive(file_paths, tar_path, mode=\'w:gz\'): Creates a TAR archive containing specified files. Mode controls the compression format: \'w\', \'w:gz\', \'w:bz2\', \'w:xz\'. with tarfile.open(tar_path, mode) as tarf: for file in file_paths: tarf.add(file, arcname=Path(file).name) def extract_zip_archive(zip_path, output_dir): Extracts all files from a ZIP archive into specified directory. with zipfile.ZipFile(zip_path, \'r\') as zipf: zipf.extractall(path=output_dir) def extract_tar_archive(tar_path, output_dir): Extracts all files from a TAR archive into specified directory. with tarfile.open(tar_path, \'r:*\') as tarf: tarf.extractall(path=output_dir)"},{"question":"**Objective**: Demonstrate proficiency in using pandas `groupby` for data aggregation, transformation, and filtration. **Problem Statement**: You are given a DataFrame `sales_data` that contains sales transactions at various stores. The schema of the DataFrame is as follows: - `store_id`: ID of the store where the transaction occurred. - `product_id`: ID of the product sold. - `date`: Date of the transaction. - `quantity`: Quantity of the product sold. - `revenue`: Revenue generated from the transaction. Below is a sample of the data: ```python import pandas as pd data = { \'store_id\': [1, 1, 2, 2, 1, 3, 3, 2, 1, 3], \'product_id\': [\'A\', \'B\', \'A\', \'B\', \'A\', \'C\', \'A\', \'C\', \'B\', \'C\'], \'date\': pd.date_range(\'2023-01-01\', periods=10, freq=\'D\'), \'quantity\': [5, 3, 4, 2, 1, 7, 6, 3, 2, 8], \'revenue\': [50, 30, 40, 20, 10, 70, 60, 30, 20, 80] } sales_data = pd.DataFrame(data) ``` **Tasks**: 1. **Total Revenue per Store-Product**: Write a function `total_revenue_per_store_product` that returns a DataFrame with the total revenue for each combination of `store_id` and `product_id`. 2. **Top Selling Product per Store**: Write a function `top_selling_product_per_store` that returns a DataFrame with the maximum revenue `product_id` for each `store_id`. 3. **Revenue Percentage Contribution**: Write a function `revenue_percentage_contribution` that returns the original `sales_data` DataFrame with an additional column `revenue_pct` which shows the percentage contribution of each transaction to the total revenue of the corresponding store. 4. **Filter Low Revenue Stores**: Write a function `filter_low_revenue_stores` that filters out stores where the total revenue is less than a specified threshold and returns a DataFrame with the remaining stores. The function should have a parameter `threshold` to set the minimum acceptable revenue. **Function Signatures**: ```python def total_revenue_per_store_product(sales_data: pd.DataFrame) -> pd.DataFrame: pass def top_selling_product_per_store(sales_data: pd.DataFrame) -> pd.DataFrame: pass def revenue_percentage_contribution(sales_data: pd.DataFrame) -> pd.DataFrame: pass def filter_low_revenue_stores(sales_data: pd.DataFrame, threshold: int) -> pd.DataFrame: pass ``` # Constraints: - `sales_data` DataFrame will have at least 10 entries. - `threshold` for total revenue in `filter_low_revenue_stores` will be a positive integer. # Expected Outputs: 1. **total_revenue_per_store_product**: ```python total_revenue_per_store_product(sales_data) # Expected output: # store_id product_id revenue # 0 1 A 60 # 1 1 B 50 # 2 2 A 40 # 3 2 B 20 # 4 2 C 30 # 5 3 A 60 # 6 3 C 150 ``` 2. **top_selling_product_per_store**: ```python top_selling_product_per_store(sales_data) # Expected output: # store_id product_id revenue # 0 1 A 60 # 1 2 A 40 # 2 3 C 150 ``` 3. **revenue_percentage_contribution**: ```python revenue_percentage_contribution(sales_data) # Expected output includes the original columns along with the new `revenue_pct` column. ``` 4. **filter_low_revenue_stores** (for example with threshold=100): ```python filter_low_revenue_stores(sales_data, threshold=100) # Expected output includes rows only for stores with total revenue >= 100. ``` # Notes: - Ensure that the functions are efficient and handle large DataFrames within reasonable time limits. - Use the `groupby` functionality extensively to manipulate the data as required. - Validate the functions with different `sales_data` samples to ensure correctness.","solution":"import pandas as pd def total_revenue_per_store_product(sales_data: pd.DataFrame) -> pd.DataFrame: return sales_data.groupby([\'store_id\', \'product_id\'])[\'revenue\'].sum().reset_index() def top_selling_product_per_store(sales_data: pd.DataFrame) -> pd.DataFrame: top_revenue_per_store = sales_data.groupby([\'store_id\', \'product_id\'])[\'revenue\'].sum().reset_index() return top_revenue_per_store.loc[top_revenue_per_store.groupby(\'store_id\')[\'revenue\'].idxmax()] def revenue_percentage_contribution(sales_data: pd.DataFrame) -> pd.DataFrame: total_revenue_per_store = sales_data.groupby(\'store_id\')[\'revenue\'].transform(\'sum\') sales_data[\'revenue_pct\'] = sales_data[\'revenue\'] / total_revenue_per_store * 100 return sales_data def filter_low_revenue_stores(sales_data: pd.DataFrame, threshold: int) -> pd.DataFrame: total_revenue_per_store = sales_data.groupby(\'store_id\')[\'revenue\'].transform(\'sum\') return sales_data[total_revenue_per_store >= threshold]"},{"question":"**Objective**: Demonstrate your understanding of encapsulating data and accessing attributes through wrapper classes in Python. In this task, you are required to implement a Python class `PyCapsuleWrapper` that mimics the concept of `PyCapsule`. This class should allow encapsulating a Python object (instead of a C pointer) and provide methods to access and modify its attributes. The encapsulation should also allow setting a \\"destructor\\" method that is called when the capsule is deleted. **Requirements**: 1. Implement a class `PyCapsuleWrapper` with the following attributes and methods: - **Attributes**: - `pointer`: The encapsulated Python object. - `name`: A string representing the name of the capsule. - `context`: Any additional context information. - `destructor`: A callable (function) that takes no arguments and is called when the capsule is deleted. - **Methods**: - `__init__(self, pointer, name=None, destructor=None)`: Initializes the capsule with the given pointer, name, and destructor. - `get_pointer(self)`: Returns the encapsulated object. - `get_name(self)`: Returns the name of the capsule. - `get_context(self)`: Returns the context of the capsule. - `get_destructor(self)`: Returns the destructor callable. - `set_pointer(self, pointer)`: Sets the encapsulated object. - `set_name(self, name)`: Sets the name of the capsule. - `set_context(self, context)`: Sets the context of the capsule. - `set_destructor(self, destructor)`: Sets the destructor callable. - Ensure that when an instance of `PyCapsuleWrapper` is deleted, the destructor is called if it is set. **Input and Output Format**: - The `__init__` method will take three optional parameters (`pointer`, `name`, and `destructor`). - Methods like `set_pointer`, `set_name`, `set_context`, and `set_destructor` will update the respective attributes of the capsule. - Methods `get_pointer`, `get_name`, `get_context`, and `get_destructor` will return the respective attributes. **Example Usage**: ```python class PyCapsuleWrapper: def __init__(self, pointer, name=None, destructor=None): self.pointer = pointer self.name = name self.context = None self.destructor = destructor def get_pointer(self): return self.pointer def get_name(self): return self.name def get_context(self): return self.context def get_destructor(self): return self.destructor def set_pointer(self, pointer): self.pointer = pointer def set_name(self, name): self.name = name def set_context(self, context): self.context = context def set_destructor(self, destructor): self.destructor = destructor def __del__(self): if self.destructor: self.destructor() # Example usage def my_destructor(): print(\\"Destructor called!\\") capsule = PyCapsuleWrapper(pointer=123, name=\\"my.capsule\\", destructor=my_destructor) print(capsule.get_name()) # Output: my.capsule print(capsule.get_pointer()) # Output: 123 del capsule # Output: Destructor called! ``` Write the `PyCapsuleWrapper` class as specified. Your solution should handle the encapsulation and all the attribute functions appropriately.","solution":"class PyCapsuleWrapper: def __init__(self, pointer, name=None, destructor=None): self.pointer = pointer self.name = name self.context = None self.destructor = destructor def get_pointer(self): return self.pointer def get_name(self): return self.name def get_context(self): return self.context def get_destructor(self): return self.destructor def set_pointer(self, pointer): self.pointer = pointer def set_name(self, name): self.name = name def set_context(self, context): self.context = context def set_destructor(self, destructor): self.destructor = destructor def __del__(self): if self.destructor: self.destructor() # Example usage def my_destructor(): print(\\"Destructor called!\\") capsule = PyCapsuleWrapper(pointer=123, name=\\"my.capsule\\", destructor=my_destructor) print(capsule.get_name()) # Output: my.capsule print(capsule.get_pointer()) # Output: 123 # Upon deleting capsule, the destructor should be called if it\'s set del capsule # Expected Output: Destructor called!"},{"question":"# Python Coding Assessment Question DBM Database Management In this exercise, you are required to use the `dbm` module in Python to create a simple database management class. This class should allow for the creation, updating, retrieval, and deletion of entries within a DBM database file. You will also handle context management to ensure that the database is properly closed after operations. Requirements: 1. Implement a class `DBMDatabase` with the following methods: - `__init__(self, file, flag=\'c\', mode=0o666)`: Initialize the database file. - `add_entry(self, key, value)`: Add a new entry to the database. - `get_entry(self, key)`: Retrieve the value for a given key from the database. - `delete_entry(self, key)`: Delete an entry from the database. - `__enter__(self)`: Context manager enter method. - `__exit__(self, exc_type, exc_val, exc_tb)`: Context manager exit method. 2. Handle the following constraints: - Only bytes can be stored as keys and values. You must encode strings before storage and decode bytes after retrieval. - Ensure that the database is closed properly after operations, especially when using the class in a `with` statement. - Handle `KeyError` exceptions properly when attempting to delete or retrieve a non-existent key. Example Usage: ```python # Example usage of the DBMDatabase class with DBMDatabase(\'example.db\') as db: db.add_entry(\'name\', \'Alice\') db.add_entry(\'age\', \'30\') print(db.get_entry(\'name\')) # Output: \'Alice\' print(db.get_entry(\'age\')) # Output: \'30\' db.delete_entry(\'name\') print(db.get_entry(\'name\')) # Should raise KeyError ``` Your Tasks: 1. Implement the `DBMDatabase` class with the specified methods and constraints. 2. Write a testing script that demonstrates the correct usage of the class and handles possible exceptions.","solution":"import dbm class DBMDatabase: def __init__(self, file, flag=\'c\', mode=0o666): self.file = file self.flag = flag self.mode = mode self.db = None def __enter__(self): self.db = dbm.open(self.file, self.flag, self.mode) return self def __exit__(self, exc_type, exc_val, exc_tb): if self.db is not None: self.db.close() def add_entry(self, key, value): key_byte = key.encode(\'utf-8\') value_byte = value.encode(\'utf-8\') self.db[key_byte] = value_byte def get_entry(self, key): key_byte = key.encode(\'utf-8\') if key_byte not in self.db: raise KeyError(\\"Key not found in database\\") return self.db[key_byte].decode(\'utf-8\') def delete_entry(self, key): key_byte = key.encode(\'utf-8\') if key_byte not in self.db: raise KeyError(\\"Key not found in database\\") del self.db[key_byte]"},{"question":"You are tasked with implementing a simulation of a task scheduler using `asyncio.Queue` in Python 3.10. The goal is to distribute a list of tasks among a fixed number of workers, ensuring that each worker processes tasks in a fair manner. Your implementation should demonstrate your understanding of asynchronous programming and the `asyncio.Queue` functionalities. Requirements: 1. **Function Definition**: - Function name: `run_task_scheduler` - Parameters: - `tasks`: A list of tuples where each tuple contains two elements: - `task_id`: An integer representing the unique ID of the task. - `duration`: A float representing the time (in seconds) the task should take to complete. - `num_workers`: An integer representing the number of worker coroutines. - Return: A dictionary mapping each `task_id` to the worker ID (starting from 0) that processed it. 2. **Constraints**: - All tasks must be processed. - Tasks should be processed in the order they appear in the input list. - Each worker can handle one task at a time. - Use `await asyncio.sleep(duration)` to simulate the time taken to complete a task. 3. **Implementation Details**: - You must use `asyncio.Queue` to manage the distribution of tasks. - Workers should keep running until all tasks are processed. - Ensure proper cancellation of worker tasks once all tasks are completed. - Handle any potential exceptions related to queue operations. 4. **Performance**: - Since this is a simulation, real-time performance is not critical, but the solution should demonstrate efficient use of asynchronous features. Example Usage: ```python import asyncio async def run_task_scheduler(tasks, num_workers): # Your implementation here pass # Example tasks and worker count tasks = [(1, 2.0), (2, 3.0), (3, 1.5), (4, 2.5)] num_workers = 2 # Running the task scheduler result = asyncio.run(run_task_scheduler(tasks, num_workers)) print(result) ``` Given this setup, write your implementation for the `run_task_scheduler` function. Ensure your code handles the tasks appropriately and returns the expected result. Example Output: ```python { 1: 0, 2: 1, 3: 0, 4: 1 } ``` Explanation: This output indicates that task 1 and task 3 were handled by worker 0, and task 2 and task 4 were handled by worker 1.","solution":"import asyncio async def worker(worker_id, task_queue, results): while True: task = await task_queue.get() if task is None: break task_id, duration = task await asyncio.sleep(duration) results[task_id] = worker_id task_queue.task_done() async def run_task_scheduler(tasks, num_workers): task_queue = asyncio.Queue() results = {} # Fill the task queue for task in tasks: await task_queue.put(task) # Create worker tasks workers = [asyncio.create_task(worker(i, task_queue, results)) for i in range(num_workers)] # Wait until the task queue is empty await task_queue.join() # Stop workers for _ in range(num_workers): await task_queue.put(None) # Wait for workers to terminate await asyncio.gather(*workers) return results"},{"question":"# Pandas Indexing and Selection Exercise Objective Write a function which finds and returns all rows from a DataFrame that match certain criteria against multiple columns. This will test your understanding of complex indexing and selection in pandas. Problem Statement Given a pandas DataFrame `df`, write a function `filter_rows(df: pd.DataFrame, conditions: dict) -> pd.DataFrame` that returns a new DataFrame with rows that satisfy all the conditions specified in the `conditions` dictionary. Each key in `conditions` is a column name, and the value is a list of accepted values for that column. Example ```python import pandas as pd data = {\'Name\': [\'John\', \'Jane\', \'Jack\', \'Jill\', \'Jose\', \'Jake\'], \'Age\': [23, 25, 23, 22, 25, 23], \'City\': [\'New York\', \'Paris\', \'New York\', \'Berlin\', \'Paris\', \'Berlin\'], \'Score\': [85, 90, 80, 95, 70, 88]} df = pd.DataFrame(data) # Conditions conditions = { \'Age\': [23, 25], \'City\': [\'New York\', \'Berlin\'] } filtered_df = filter_rows(df, conditions) ``` For the example above, the output `filtered_df` should be: ``` Name Age City Score 0 John 23 New York 85 2 Jack 23 New York 80 3 Jill 22 Berlin 95 5 Jake 23 Berlin 88 ``` Constraints 1. The DataFrame `df` will have no less than 1 column and not exceed 100 columns. 2. The DataFrame `df` will have no less than 1 row and not exceed 1,000,000 rows. 3. The column values in `conditions` will always have valid keys corresponding to column names in `df`. 4. The values in `conditions` will be non-empty lists where each list element is a valid entry for that column. 5. You should not modify the original DataFrame `df`. Function Signature ```python def filter_rows(df: pd.DataFrame, conditions: dict) -> pd.DataFrame: pass ``` Requirements - Use `loc` and `isin` methods for efficient and readable code. - Returns should preserve the original DataFrame\'s order and index. - Write code that is clean, readable, and well-commented. Note Ensure your function handles edge cases such as: - An empty result due to no matching conditions. - All conditions are met and the resulting DataFrame is the same as the input DataFrame. Good luck and happy coding!","solution":"import pandas as pd def filter_rows(df: pd.DataFrame, conditions: dict) -> pd.DataFrame: Filters rows from the DataFrame `df` based on the specified `conditions`. Parameters: - df: pd.DataFrame, the DataFrame to filter - conditions: dict, a dictionary where keys are column names and values are lists of accepted values for those columns Returns: - pd.DataFrame, the filtered DataFrame for column, accepted_values in conditions.items(): df = df[df[column].isin(accepted_values)] return df"},{"question":"# Coding Assessment: Comprehensive File and Directory Operations Objective: To assess your understanding and ability to utilize the `shutil` module for high-level file operations in Python 3.10. Problem Statement: You are tasked with writing a Python function that performs a series of file and directory operations using the `shutil` module. The function should handle copying files, copying and archiving a directory, and cleaning up temporary files. Specifically, the function will: 1. **Copy Files**: Copy a specified file from a source to a destination. 2. **Archive a Directory**: Create a compressed archive (e.g., `.zip` or `.tar.gz`) of a directory. 3. **Clean Up**: Remove the directory after it has been archived. Function Signature: ```python def perform_file_operations(src_file: str, dst_file: str, src_dir: str, dst_dir: str, archive_format: str) -> str: Perform a series of file and directory operations. :param src_file: Path to the source file to be copied. :param dst_file: Path to the destination file. :param src_dir: Path to the source directory to be archived and then removed. :param dst_dir: Path to the directory where the archive will be saved. :param archive_format: Format of the archive to be created (\'zip\' or \'gztar\', etc). :return: Path to the created archive. pass ``` Details: 1. **Copying Files**: - Use `shutil.copy2(src_file, dst_file)` to copy `src_file` to `dst_file`, preserving metadata. 2. **Archiving a Directory**: - Use `shutil.make_archive(base_name, format, root_dir)` to create an archive of `src_dir`. - The `base_name` should be the concatenation of `dst_dir` and the base name of `src_dir`. - The `format` should be specified by the `archive_format` parameter. 3. **Cleaning Up**: - Use `shutil.rmtree(src_dir)` to remove the `src_dir` once it has been archived. Constraints: - Assume that the paths provided for the files and directories are valid. - The function should handle any exceptions that arise during file operations and provide relevant error messages. - The created archive should have the same base name as the `src_dir`. Example: ```python # Given the following directory structure: # src/ # â”œâ”€â”€ file1.txt # â”œâ”€â”€ file2.txt # â””â”€â”€ subdir # â”œâ”€â”€ file3.txt # â””â”€â”€ file4.txt # If src_file = \'src/file1.txt\', dst_file = \'dst/file1_copy.txt\', src_dir = \'src\', dst_dir = \'archives\', archive_format = \'zip\' # The function should: # 1. Copy \'src/file1.txt\' to \'dst/file1_copy.txt\'. # 2. Create an archive \'archives/src.zip\' containing all files from \'src\'. # 3. Remove the \'src\' directory. perform_file_operations(\'src/file1.txt\', \'dst/file1_copy.txt\', \'src\', \'archives\', \'zip\') # Should return \'archives/src.zip\' ``` Evaluation Criteria: - Correctness: The function should perform the specified operations correctly. - Robustness: Handle errors gracefully and provide meaningful error messages. - Code Quality: Use of appropriate methods from the `shutil` module, readability, and maintainability of the code.","solution":"import shutil import os def perform_file_operations(src_file: str, dst_file: str, src_dir: str, dst_dir: str, archive_format: str) -> str: Perform a series of file and directory operations. :param src_file: Path to the source file to be copied. :param dst_file: Path to the destination file. :param src_dir: Path to the source directory to be archived and then removed. :param dst_dir: Path to the directory where the archive will be saved. :param archive_format: Format of the archive to be created (\'zip\' or \'gztar\', etc). :return: Path to the created archive. try: # Copy the file shutil.copy2(src_file, dst_file) # Create the archive base_name = os.path.join(dst_dir, os.path.basename(src_dir)) archive_path = shutil.make_archive(base_name, archive_format, src_dir) # Remove the directory shutil.rmtree(src_dir) return archive_path except Exception as e: return f\\"An error occurred: {str(e)}\\""},{"question":"**HMAC-Based Message Verification System** # Objective Design a Python function that implements a secure message verification system using HMAC. The system should be able to generate and verify HMAC digests for messages using a specified secret key and digest algorithm. # Function Signature ```python def verify_message(key: bytes, message: str, digestmod: str, expected_digest: str) -> bool: Verifies if the provided message\'s HMAC digest matches the expected digest. Parameters: - key (bytes): The secret key for HMAC. - message (str): The message to be verified. - digestmod (str): The name of the hash algorithm to use (e.g., \'sha256\'). - expected_digest (str): The expected hexadecimal digest of the message. Returns: - bool: True if the generated digest matches the expected digest, False otherwise. pass ``` # Requirements 1. **Input Parameters:** - `key`: A bytes object representing the secret key. - `message`: A string containing the message to be verified. - `digestmod`: A string specifying the hash algorithm (e.g., \'sha256\'). - `expected_digest`: A string representing the expected hexadecimal HMAC digest. 2. **Output:** - Returns a boolean indicating whether the generated digest matches the expected digest. 3. **Constraints:** - The `key` must be a bytes object. - The `digestmod` must be a valid hash algorithm that can be passed to `hmac.new()`. - The comparison must use `hmac.compare_digest()` to prevent timing attacks. # Functional Requirements - You must use the `hmac` module to create the HMAC object and calculate the digest. - Update the HMAC object with the provided message. - Generate the digest using the provided hash algorithm. - Compare the generated digest with `expected_digest` using `hmac.compare_digest()`. # Example ```python key = b\'secret_key\' message = \'Hello, HMAC!\' digestmod = \'sha256\' expected_digest = \'a1b2c3d4e5f678901234567890abcdef\' # Hypothetical example value result = verify_message(key, message, digestmod, expected_digest) print(result) # Expected output: True or False based on the correctness of expected_digest ``` # Note The provided example `expected_digest` is hypothetical. You would need to compute the actual expected digest for real cases. **Good luck and happy coding!**","solution":"import hmac import hashlib def verify_message(key: bytes, message: str, digestmod: str, expected_digest: str) -> bool: Verifies if the provided message\'s HMAC digest matches the expected digest. Parameters: - key (bytes): The secret key for HMAC. - message (str): The message to be verified. - digestmod (str): The name of the hash algorithm to use (e.g., \'sha256\'). - expected_digest (str): The expected hexadecimal digest of the message. Returns: - bool: True if the generated digest matches the expected digest, False otherwise. hmac_object = hmac.new(key, message.encode(), digestmod=getattr(hashlib, digestmod)) generated_digest = hmac_object.hexdigest() return hmac.compare_digest(generated_digest, expected_digest)"},{"question":"Objective Demonstrate your understanding of the `email.message.Message` class provided by Python\'s email package by performing a series of operations on an email object. This will involve manipulating headers, handling payloads, and iterating over parts of a multipart message. Problem Statement Create a function `manipulate_email_message` that takes an instance of `email.message.Message` as input and performs the following operations: 1. Ensure that the message is multipart. If it\'s not multipart, convert it to a multipart message with the original payload being the first part. 2. Add a new header `X-Processed-By` with the value `Python310` to each part of the multipart message. 3. Change the `Content-Type` of the main message to `multipart/mixed`. 4. Return the string representation of the modified message. Function Signature ```python def manipulate_email_message(email_msg: email.message.Message) -> str: pass ``` Constraints - The `email_msg` object may or may not already be multipart. - If the `Content-Type` is changed to `multipart/mixed`, ensure that existing parameters (e.g., boundary) are preserved. - You must use the methods and attributes of the `email.message.Message` class to accomplish the tasks. Example ```python from email.message import Message # Create a simple email message msg = Message() msg.set_payload(\'This is a test email.\') # Manipulate the email message result = manipulate_email_message(msg) print(result) ``` Expected Output: ``` Content-Type: multipart/mixed; boundary=\\"---something-generated-here\\" X-Processed-By: Python310 This is a multi-part message in MIME format. ------something-generated-here Content-Type: text/plain This is a test email. X-Processed-By: Python310 ------something-generated-here-- ``` The output should demonstrate the changes including: - Converting to multipart if not already. - Adding the `X-Processed-By` header to each part. - Changing the main content type to `multipart/mixed`. - Displaying the modified message as a string. **Note:** The exact boundary string will be different in each run as it is generated dynamically. Evaluation Criteria - Correctness: The function should correctly perform all required operations as specified. - Use of API: Proper use of the `email.message.Message` class\'s methods and attributes. - Efficiency: The operations should be performed efficiently with minimal overhead. - Clarity: The code should be written clearly with appropriate comments explaining key parts of the logic.","solution":"from email.message import Message from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def manipulate_email_message(email_msg: Message) -> str: # Ensure the message is multipart if not email_msg.is_multipart(): # If not multipart, create a multipart message and make the original payload the first part original_payload = email_msg.get_payload() new_msg = MIMEMultipart() text_part = MIMEText(original_payload) new_msg.attach(text_part) email_msg = new_msg # Ensure the main content type is multipart/mixed email_msg.set_type(\\"multipart/mixed\\") # Iterate over all parts and add the X-Processed-By header for part in email_msg.walk(): if part.get_content_maintype() != \'multipart\': # Don\'t add to container parts part.add_header(\'X-Processed-By\', \'Python310\') # Return the string representation of the modified message return email_msg.as_string()"},{"question":"You are tasked with developing a function that provides a summary of an installed package\'s metadata. This summary should include the version of the package, a list of its constituent files, its dependencies, and the entry points defined in the package. # Function Signature ```python def package_summary(package_name: str) -> dict: Generates a summary of the given package\'s metadata. Args: package_name (str): The name of the package to summarize. Returns: dict: A dictionary containing the package\'s version, a list of files in the package, its dependencies, and the entry points categorized by their groups. ``` # Inputs - `package_name` (str): The name of the package for which the metadata summary is to be generated. # Outputs - A dictionary with the following keys: - `version` (str): The version of the package. - `files` (list of str): A list of paths to the files contained within the package. - `dependencies` (list of str): A list of dependencies required by the package. - `entry_points` (dict): A dictionary where the keys are entry point groups (str) and values are lists of entry point names (str). # Constraints - You may assume that the package specified by `package_name` is installed and accessible from the Python environment in which the function will be executed. # Example ```python # Example usage summary = package_summary(\'wheel\') print(summary) ``` Expected output format: ```python { \\"version\\": \\"0.32.3\\", \\"files\\": [\\"path1\\", \\"path2\\", \\"path3\\", ...], \\"dependencies\\": [\\"dependency1\\", \\"dependency2\\", ...], \\"entry_points\\": { \\"console_scripts\\": [\\"entry1\\", \\"entry2\\", ...], \\"distutils.commands\\": [\\"entry3\\", \\"entry4\\", ...], ... } } ``` # Notes - Use the `importlib.metadata` package to retrieve the necessary data. - Ensure that your function handles cases where some metadata elements (e.g., dependencies, entry points) might not exist for a given package and returns an appropriate empty structure (i.e., an empty list or dictionary). - Optimize your solution to minimize redundant calls to metadata retrieval functions.","solution":"import importlib.metadata def package_summary(package_name: str) -> dict: Generates a summary of the given package\'s metadata. Args: package_name (str): The name of the package to summarize. Returns: dict: A dictionary containing the package\'s version, a list of files in the package, its dependencies, and the entry points categorized by their groups. try: metadata = importlib.metadata.metadata(package_name) version = metadata[\\"Version\\"] except importlib.metadata.PackageNotFoundError: return {\\"error\\": f\\"Package {package_name} not found\\"} files = list(importlib.metadata.files(package_name)) dependencies = list(importlib.metadata.requires(package_name) or []) entry_points = importlib.metadata.entry_points() entry_points_dict = {} for entry_point in entry_points.get(package_name, []): entry_points_dict.setdefault(entry_point.group, []).append(entry_point.name) return { \\"version\\": version, \\"files\\": [str(file) for file in files], \\"dependencies\\": dependencies, \\"entry_points\\": entry_points_dict }"},{"question":"Question: Property List Manager You are tasked with writing a Python function that reads a plist file, modifies its contents based on given operations, and then writes the modified data back to a new plist file. This will test your understanding of the `plistlib` module and its functionalities. # Requirements 1. **Function Definition**: ```python def manage_plist(input_file: str, output_file: str, operations: list, fmt=None) -> None: ``` 2. **Parameters**: - `input_file` (str): Path to the input plist file (it can be in either XML or binary format). - `output_file` (str): Path where the modified plist file will be saved. - `operations` (list): A list of operations to perform on the plist data. Each operation is represented as a dictionary with the following keys: - `type` (str): The type of operation (`add`, `update`, `delete`). - `key` (str): The key in the plist dictionary that the operation will target. - `value` (any, optional): The value to add or update. Required for `add` and `update` operations. - `fmt` (optional): The format for reading and writing plist files. Can be `FMT_XML`, `FMT_BINARY`, or `None` to autodetect. 3. **Constraints**: - The top-level object of the plist file should be a dictionary. - For `add` and `update` operations, ensure the `value` is one of the supported types (`str`, `int`, `float`, `bool`, `list`, `dict`, `bytes`, `bytearray`, or `datetime.datetime`). 4. **Functionality**: - Read the plist file using the provided format or autodetect if `fmt` is `None`. - Perform the specified operations on the plist data. - `add`: Add a new key-value pair to the plist dictionary. - `update`: Update the value of an existing key in the plist dictionary. - `delete`: Remove a key-value pair from the plist dictionary. - Write the updated plist data back to the new file in the specified format. # Example ```python # Sample input plist data in input.plist file \'\'\' <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>name</key> <string>Example</string> <key>version</key> <integer>1</integer> </dict> </plist> \'\'\' # Sample operations operations = [ {\'type\': \'add\', \'key\': \'author\', \'value\': \'John Doe\'}, {\'type\': \'update\', \'key\': \'version\', \'value\': 2}, {\'type\': \'delete\', \'key\': \'name\'} ] # Calling the function manage_plist(\'input.plist\', \'output.plist\', operations) # Expected output plist data in output.plist file \'\'\' <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>version</key> <integer>2</integer> <key>author</key> <string>John Doe</string> </dict> </plist> \'\'\' ``` # Notes: - Ensure to handle exceptions such as invalid file formats, unsupported data types, and operations on non-existent keys.","solution":"import plistlib import os def manage_plist(input_file: str, output_file: str, operations: list, fmt=None) -> None: # Read the input plist file with open(input_file, \'rb\') as fp: if fmt: data = plistlib.load(fp, fmt=fmt) else: data = plistlib.load(fp) if not isinstance(data, dict): raise ValueError(\\"The top-level object in the plist file should be a dictionary.\\") for operation in operations: op_type = operation[\'type\'] key = operation[\'key\'] if op_type == \'add\': value = operation[\'value\'] if key in data: raise KeyError(f\\"Key \'{key}\' already exists in the plist data.\\") data[key] = value elif op_type == \'update\': if key not in data: raise KeyError(f\\"Key \'{key}\' does not exist in the plist data.\\") value = operation[\'value\'] data[key] = value elif op_type == \'delete\': if key not in data: raise KeyError(f\\"Key \'{key}\' does not exist in the plist data.\\") del data[key] # Write the updated data to the output plist file with open(output_file, \'wb\') as fp: if fmt: plistlib.dump(data, fp, fmt=fmt) else: plistlib.dump(data, fp)"},{"question":"# URL Manipulation and Validation Task The goal of this task is to demonstrate your understanding of URL parsing, manipulation, and encoding using the `urllib.parse` module. You will write a class `UrlHandler` that provides functionalities for parsing URLs, validating their components, assembling them back into full URLs, and handling query strings. Requirements 1. **Parsing a URL** - Implement a static method `parse_url(url: str) -> dict` which takes a URL string and returns a dictionary with the keys: \'scheme\', \'netloc\', \'path\', \'params\', \'query\', \'fragment\', \'username\', \'password\', \'hostname\', and \'port\'. Each key should map to the corresponding component from the parsed URL. 2. **Validating URL Components** - Implement a static method `validate_url_components(components: dict) -> bool` that takes the dictionary obtained from `parse_url` method and verifies: - `scheme` must be one of the following: \'http\', \'https\', \'ftp\'. - `port` must be between 1 and 65535 (inclusive), if present. - `hostname` must not be empty. The method should return `True` if all checks pass, otherwise `False`. 3. **Assembling a URL from Components** - Implement a static method `assemble_url(components: dict) -> str` which takes a dictionary of URL components (same structure as returned by `parse_url`) and returns the URL string combining all components back together. 4. **Handling Query String** - Implement a static method `parse_query_string(query: str) -> dict` which converts a URL query string into a dictionary where keys are query parameter names and values are lists of parameter values. - Implement a static method `build_query_string(params: dict) -> str` which converts a dictionary of query parameters back into a URL-encoded query string. 5. **URL Quoting and Unquoting** - Implement a static method `quote_url_component(component: str) -> str` which uses the `urllib.parse.quote` function to quote special characters in a URL component. - Implement a static method `unquote_url_component(component: str) -> str` which uses the `urllib.parse.unquote` function to decode percent-encoded characters in a URL component. Example Usage ```python url = \\"https://www.example.com:80/path;params?query=example#fragment\\" handler = UrlHandler() # Parsing URL components = handler.parse_url(url) print(components) # Output: {\'scheme\': \'https\', \'netloc\': \'www.example.com:80\', \'path\': \'/path\', \'params\': \'params\', \'query\': \'query=example\', \'fragment\': \'fragment\', \'username\': None, \'password\': None, \'hostname\': \'www.example.com\', \'port\': 80} # Validating URL components is_valid = handler.validate_url_components(components) print(is_valid) # Output: True # Assembling URL assembled_url = handler.assemble_url(components) print(assembled_url) # Output: \\"https://www.example.com:80/path;params?query=example#fragment\\" # Parsing Query String query_dict = handler.parse_query_string(\\"query=example&key=value&key=value2\\") print(query_dict) # Output: {\'query\': [\'example\'], \'key\': [\'value\', \'value2\']} # Building Query String query_string = handler.build_query_string({\'query\': [\'example\'], \'key\': [\'value\', \'value2\']}) print(query_string) # Output: \\"query=example&key=value&key=value2\\" # URL Quoting quoted = handler.quote_url_component(\\"/El NiÃ±o/\\") print(quoted) # Output: \\"/El%20Ni%C3%B1o/\\" # URL Unquoting unquoted = handler.unquote_url_component(\\"/El%20Ni%C3%B1o/\\") print(unquoted) # Output: \\"/El NiÃ±o/\\" ``` **Constraints:** - You can assume that input URL strings and query strings passed to your methods will be well-formed. - Your solution should handle edge cases such as missing components (e.g., no query or fragment) gracefully.","solution":"from urllib.parse import urlparse, urlunparse, parse_qs, urlencode, quote, unquote class UrlHandler: @staticmethod def parse_url(url: str) -> dict: parsed = urlparse(url) return { \'scheme\': parsed.scheme, \'netloc\': parsed.netloc, \'path\': parsed.path, \'params\': parsed.params, \'query\': parsed.query, \'fragment\': parsed.fragment, \'username\': parsed.username, \'password\': parsed.password, \'hostname\': parsed.hostname, \'port\': parsed.port } @staticmethod def validate_url_components(components: dict) -> bool: if components[\'scheme\'] not in [\'http\', \'https\', \'ftp\']: return False if components[\'port\'] and not (1 <= components[\'port\'] <= 65535): return False if not components[\'hostname\']: return False return True @staticmethod def assemble_url(components: dict) -> str: return urlunparse((components[\'scheme\'], components[\'netloc\'], components[\'path\'], components[\'params\'], components[\'query\'], components[\'fragment\'])) @staticmethod def parse_query_string(query: str) -> dict: return parse_qs(query) @staticmethod def build_query_string(params: dict) -> str: return urlencode(params, doseq=True) @staticmethod def quote_url_component(component: str) -> str: return quote(component) @staticmethod def unquote_url_component(component: str) -> str: return unquote(component)"},{"question":"# Terminal Control Function Implementation **Objective:** Write a Python function that reads a line of input from the terminal without echoing the input (character display suppression). The function must use the \\"tty\\" module to change the terminal mode to cbreak and then restore the terminal to its original mode before returning. **Function Signature:** ```python def read_line_no_echo() -> str: pass ``` **Input Format:** There is no input to the function. The function will read directly from the terminal. **Output Format:** The function should return the line of text entered by the user as a string. **Constraints:** - You must use the `tty.setcbreak` function to change the terminal mode. - You may need to handle exceptions to ensure the terminal mode is restored even if an error occurs. - This function must work only on Unix systems. **Detailed Specifications:** 1. Import the necessary modules: `sys`, `tty`, `termios`. 2. Save the original terminal attributes using `termios.tcgetattr()`. 3. Set the terminal to cbreak mode using `tty.setcbreak()`. 4. Read a line of input from the terminal using `sys.stdin.read()`. 5. Restore the terminal to its original attributes using `termios.tcsetattr()`. 6. Ensure that the terminal mode is restored even if an exception occurs during reading. **Example Usage:** ```python line = read_line_no_echo() print(f\\"You entered: {line}\\") ``` This code snippet should read a line without displaying what is typed and then print the entered line. **Performance Requirements:** The function should quickly switch terminal modes and handle input efficiently to provide a smooth user experience. **Note:** Please test this function in a Unix-based system as it relies on Unix-specific modules and behavior.","solution":"import sys import tty import termios def read_line_no_echo() -> str: Reads a line of input from the terminal without echoing the input. Returns: str: The line of text entered by the user. # Save the original terminal attributes fd = sys.stdin.fileno() original_attributes = termios.tcgetattr(fd) try: # Set the terminal to cbreak mode tty.setcbreak(fd) # Read the line of input user_input = [] while True: ch = sys.stdin.read(1) # Read one character at a time if ch == \'n\': break user_input.append(ch) return \'\'.join(user_input) finally: # Restore the original terminal attributes termios.tcsetattr(fd, termios.TCSADRAIN, original_attributes)"},{"question":"**Objective:** Create a Python function that fetches and formats locale-specific information given a locale setting. Implement the following functionalities: 1. **set_fetch_locale_info(locale_string)**: - **Input:** A string `locale_string` specifying the desired locale (e.g., `\'en_US.UTF-8\'`). - **Output:** A dictionary containing: - The current locale after setting (`current_locale`). - The locale conventions as returned by `locale.localeconv()` (`locale_conventions`). - The name of the currency as given by `locale.nl_langinfo(locale.CRNCYSTR)` (`currency_name`). 2. **compare_str_locale(locale_string, str1, str2)**: - **Input:** A string `locale_string` specifying the desired locale, and two strings `str1` and `str2` to compare. - **Output:** An integer indicating the result of the comparison: - `-1` if `str1` is less than `str2` - `1` if `str1` is greater than `str2` - `0` if `str1` is equal to `str2` 3. **transform_str_locale(locale_string, string)**: - **Input:** A string `locale_string` specifying the desired locale, and a string `string` to transform. - **Output:** A transformed string using `locale.strxfrm()` to make it locale-aware. **Constraints:** - Your solution should handle exceptions gracefully. If setting the locale fails, return an appropriate error message indicating the failure. - Assume the locale provided is valid for the current system. ```python import locale def set_fetch_locale_info(locale_string): Sets the locale to the given locale_string and fetches locale-specific formatting information. Args: locale_string (str): The locale to set (e.g., \'en_US.UTF-8\'). Returns: dict: A dictionary containing current_locale, locale_conventions, and currency_name. try: # Setting the locale current_locale = locale.setlocale(locale.LC_ALL, locale_string) # Fetching locale-specific information locale_conventions = locale.localeconv() currency_name = locale.nl_langinfo(locale.CRNCYSTR) return { \'current_locale\': current_locale, \'locale_conventions\': locale_conventions, \'currency_name\': currency_name } except locale.Error as e: return {\\"error\\": str(e)} def compare_str_locale(locale_string, str1, str2): Compares two strings according to the specified locale. Args: locale_string (str): The locale to set (e.g., \'en_US.UTF-8\'). str1 (str): The first string for comparison. str2 (str): The second string for comparison. Returns: int: Returns -1 if str1 < str2, 1 if str1 > str2, 0 if they are equal. try: # Setting the locale locale.setlocale(locale.LC_ALL, locale_string) # Performing locale-aware comparison result = locale.strcoll(str1, str2) if result < 0: return -1 elif result > 0: return 1 else: return 0 except locale.Error as e: return {\\"error\\": str(e)} def transform_str_locale(locale_string, string): Transforms a string using locale.strxfrm() to make it locale-aware. Args: locale_string (str): The locale to set (e.g., \'en_US.UTF-8\'). string (str): The string to transform. Returns: str: The transformed string try: # Setting the locale locale.setlocale(locale.LC_ALL, locale_string) # Transforming the string transformed_string = locale.strxfrm(string) return transformed_string except locale.Error as e: return {\\"error\\": str(e)} ```","solution":"import locale def set_fetch_locale_info(locale_string): Sets the locale to the given locale_string and fetches locale-specific formatting information. Args: locale_string (str): The locale to set (e.g., \'en_US.UTF-8\'). Returns: dict: A dictionary containing current_locale, locale_conventions, and currency_name. try: # Setting the locale current_locale = locale.setlocale(locale.LC_ALL, locale_string) # Fetching locale-specific information locale_conventions = locale.localeconv() currency_name = locale.nl_langinfo(locale.CRNCYSTR) return { \'current_locale\': current_locale, \'locale_conventions\': locale_conventions, \'currency_name\': currency_name } except locale.Error as e: return {\\"error\\": str(e)} def compare_str_locale(locale_string, str1, str2): Compares two strings according to the specified locale. Args: locale_string (str): The locale to set (e.g., \'en_US.UTF-8\'). str1 (str): The first string for comparison. str2 (str): The second string for comparison. Returns: int: Returns -1 if str1 < str2, 1 if str1 > str2, 0 if they are equal. try: # Setting the locale locale.setlocale(locale.LC_ALL, locale_string) # Performing locale-aware comparison result = locale.strcoll(str1, str2) if result < 0: return -1 elif result > 0: return 1 else: return 0 except locale.Error as e: return {\\"error\\": str(e)} def transform_str_locale(locale_string, string): Transforms a string using locale.strxfrm() to make it locale-aware. Args: locale_string (str): The locale to set (e.g., \'en_US.UTF-8\'). string (str): The string to transform. Returns: str: The transformed string try: # Setting the locale locale.setlocale(locale.LC_ALL, locale_string) # Transforming the string transformed_string = locale.strxfrm(string) return transformed_string except locale.Error as e: return {\\"error\\": str(e)}"},{"question":"You are required to demonstrate your understanding of seaborn\'s KDE plotting functionalities by implementing a function `create_custom_kdeplot` that satisfies the following requirements: 1. **Input**: - `data (pd.DataFrame)`: A pandas DataFrame containing the dataset. - `x (str)`: The name of the column to be plotted on the x-axis. - `y (str, optional)`: The name of the column to be plotted on the y-axis. If `None`, a univariate distribution should be plotted along the x-axis. - `hue (str, optional)`: The name of the column for applying hue mapping. If `None`, no hue mapping should be used. - `bw_adjust (float, default=1.0)`: A parameter to adjust the bandwidth for the KDE plot. - `multiple (str, default=\'layer\')`: How to handle multiple distributions. Can be \'layer\', \'stack\', \'fill\', etc. - `cumulative (bool, default=False)`: Whether to estimate a cumulative distribution. - `log_scale (bool, default=False)`: Whether to apply log scaling on the data. - `fill (bool, default=False)`: Whether to fill the area under the KDE plot. 2. **Output**: - The function should create an appropriate seaborn KDE plot based on the input parameters and display it using `plt.show()`. 3. **Constraints**: - If both `x` and `y` are provided, a bivariate KDE plot is required. - If `hue` is provided, the KDE plot should show conditional distributions. - Apply appropriate default values as specified in the input section. # Example Usage: ```python import seaborn as sns import pandas as pd def create_custom_kdeplot(data, x, y=None, hue=None, bw_adjust=1.0, multiple=\'layer\', cumulative=False, log_scale=False, fill=False): # Your implementation here # Load sample dataset tips = sns.load_dataset(\\"tips\\") # Create a univariate KDE plot with hue mapping create_custom_kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\") # Create a bivariate KDE plot with log scaling and filled contours geyser = sns.load_dataset(\\"geyser\\") create_custom_kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", log_scale=True, fill=True) ``` # Note: - You can assume that required libraries (seaborn, pandas, matplotlib) are already imported. - Ensure your function handles different combinations of inputs gracefully.","solution":"import matplotlib.pyplot as plt import seaborn as sns import pandas as pd def create_custom_kdeplot(data, x, y=None, hue=None, bw_adjust=1.0, multiple=\'layer\', cumulative=False, log_scale=False, fill=False): Create a custom KDE plot with the given parameters. Parameters: data (pd.DataFrame): A pandas DataFrame containing the dataset. x (str): The name of the column to be plotted on the x-axis. y (str, optional): The name of the column to be plotted on the y-axis. If None, a univariate distribution is plotted. hue (str, optional): The name of the column for applying hue mapping. If None, no hue mapping is used. bw_adjust (float, default=1.0): A parameter to adjust the bandwidth for the KDE plot. multiple (str, default=\'layer\'): How to handle multiple distributions. cumulative (bool, default=False): Whether to estimate a cumulative distribution. log_scale (bool, default=False): Whether to apply log scaling on the data. fill (bool, default=False): Whether to fill the area under the KDE plot. Returns: None if y is None: plot = sns.kdeplot(data=data, x=x, hue=hue, bw_adjust=bw_adjust, multiple=multiple, fill=fill, cumulative=cumulative, log_scale=log_scale) else: plot = sns.kdeplot(data=data, x=x, y=y, hue=hue, bw_adjust=bw_adjust, fill=fill, log_scale=log_scale, cumulative=cumulative) plt.show()"},{"question":"**Objective:** Demonstrate understanding of various types of assignment statements, control flow, modular imports, and exception handling in Python 3.10. **Problem Statement:** You are tasked with implementing a small library management system. The system should be able to perform the following operations: 1. **Add a book**: Add a new book to the library. Each book should have a unique ISBN, a title, and an author. 2. **Remove a book**: Remove a book from the library using its ISBN. 3. **Find a book**: Find and return the details of a book using its ISBN. 4. **List all books**: List all books currently in the library. Additionally, you need to handle exceptions and ensure that appropriate error messages are raised for invalid operations (e.g., removing a non-existent book). **Function Specifications:** 1. `add_book(library: dict, isbn: str, title: str, author: str) -> None` - Adds a book to the library. - Raises an `AssertionError` if a book with the same ISBN already exists in the library. 2. `remove_book(library: dict, isbn: str) -> None` - Removes a book from the library using its ISBN. - Raises an `AssertionError` if the book with the given ISBN does not exist. 3. `find_book(library: dict, isbn: str) -> dict` - Finds and returns the details of a book using its ISBN. - Returns an empty dictionary if the book is not found. 4. `list_books(library: dict) -> list` - Returns a list of all books currently in the library, where each book is represented as a dictionary with keys: \'isbn\', \'title\', and \'author\'. **Constraints:** - The library is represented as a dictionary where the ISBN is the key, and the value is another dictionary containing the \'title\' and \'author\'. - You must use appropriate assignment, control flow, and exception handling statements as covered in the documentation provided. # Example Usage ```python library = {} add_book(library, \'978-3-16-148410-0\', \'Title1\', \'Author1\') add_book(library, \'978-1-4028-9462-6\', \'Title2\', \'Author2\') print(find_book(library, \'978-3-16-148410-0\')) # {\'isbn\': \'978-3-16-148410-0\', \'title\': \'Title1\', \'author\': \'Author1\'} print(find_book(library, \'000-0-00-000000-0\')) # {} list_books(library) # [{\'isbn\': \'978-3-16-148410-0\', \'title\': \'Title1\', \'author\': \'Author1\'}, {\'isbn\': \'978-1-4028-9462-6\', \'title\': \'Title2\', \'author\': \'Author2\'}] remove_book(library, \'978-3-16-148410-0\') list_books(library) # [{\'isbn\': \'978-1-4028-9462-6\', \'title\': \'Title2\', \'author\': \'Author2\'}] ``` Ensure your solution correctly handles the constraints and raises appropriate exceptions as required.","solution":"def add_book(library, isbn, title, author): Adds a book to the library. Raises an `AssertionError` if a book with the same ISBN already exists in the library. if isbn in library: raise AssertionError(\'Book with the same ISBN already exists.\') library[isbn] = {\'title\': title, \'author\': author} def remove_book(library, isbn): Removes a book from the library using its ISBN. Raises an `AssertionError` if the book with the given ISBN does not exist. if isbn not in library: raise AssertionError(\'Book with the given ISBN does not exist.\') del library[isbn] def find_book(library, isbn): Finds and returns the details of a book using its ISBN. Returns an empty dictionary if the book is not found. return {\'isbn\': isbn, **library[isbn]} if isbn in library else {} def list_books(library): Returns a list of all books currently in the library, where each book is represented as a dictionary with keys: \'isbn\', \'title\', and \'author\'. return [{\'isbn\': isbn, \'title\': book[\'title\'], \'author\': book[\'author\']} for isbn, book in library.items()]"},{"question":"**Question: Implement a Color Space Converter with Custom Validations and Constraints** # Problem Statement You are required to implement a function that converts a given color value from one color space to another using the `colorsys` module. Your function should take in the color values and the source and target color spaces and return the converted color values. Additionally, ensure the input values are validated based on the coordinate system constraints mentioned. # Function Signature ```python def color_space_converter(r: float = None, g: float = None, b: float = None, y: float = None, i: float = None, q: float = None, h: float = None, l: float = None, s: float = None, v: float = None, source_space: str = \\"RGB\\", target_space: str = \\"HSV\\") -> tuple: pass ``` # Parameters - `r, g, b` (float): Values for RGB color space (all between 0 and 1). - `y, i, q` (float): Values for YIQ color space (y between 0 and 1, i and q can be positive or negative). - `h, l, s` (float): Values for HLS color space (all between 0 and 1). - `h, s, v` (float): Values for HSV color space (all between 0 and 1). - `source_space` (str): A string representing the input color space (one of \\"RGB\\", \\"YIQ\\", \\"HLS\\", \\"HSV\\"). - `target_space` (str): A string representing the target color space (one of \\"RGB\\", \\"YIQ\\", \\"HLS\\", \\"HSV\\"). # Returns - A tuple of floating-point values representing the converted color in the target color space. # Constraints 1. Validate that the input values are within the specified ranges for their respective coordinates. 2. Raise a `ValueError` if the input values are out of range. 3. Support conversions between any two of the mentioned color spaces using the `colorsys` module functions. # Example ```python # Conversion from RGB to HSV print(color_space_converter(r=0.2, g=0.4, b=0.4, source_space=\\"RGB\\", target_space=\\"HSV\\")) # Output: (0.5, 0.5, 0.4) # Conversion from HSV to RGB print(color_space_converter(h=0.5, s=0.5, v=0.4, source_space=\\"HSV\\", target_space=\\"RGB\\")) # Output: (0.2, 0.4, 0.4) ``` Ensure your implementation is robust and handles all forms of valid and invalid inputs gracefully.","solution":"import colorsys def color_space_converter(r: float = None, g: float = None, b: float = None, y: float = None, i: float = None, q: float = None, h: float = None, l: float = None, s: float = None, v: float = None, source_space: str = \\"RGB\\", target_space: str = \\"HSV\\") -> tuple: Converts a color from one color space to another using the colorsys module. Parameters: - r, g, b (float): Values for RGB color space (all between 0 and 1). - y, i, q (float): Values for YIQ color space (y between 0 and 1, i and q can be positive or negative). - h, l, s (float): Values for HLS color space (all between 0 and 1). - h, s, v (float): Values for HSV color space (all between 0 and 1). - source_space (str): A string representing the input color space (one of \\"RGB\\", \\"YIQ\\", \\"HLS\\", \\"HSV\\"). - target_space (str): A string representing the target color space (one of \\"RGB\\", \\"YIQ\\", \\"HLS\\", \\"HSV\\"). Returns: - A tuple of floating-point values representing the converted color in the target color space. # Validate inputs for respective color spaces if source_space == \\"RGB\\": if not (0 <= r <= 1) or not (0 <= g <= 1) or not (0 <= b <= 1): raise ValueError(\\"RGB values must be between 0 and 1\\") elif source_space == \\"YIQ\\": if not (0 <= y <= 1) or i is None or q is None: raise ValueError(\\"YIQ values must be: 0 <= y <= 1, i and q can be any float.\\") elif source_space == \\"HLS\\": if not (0 <= h <= 1) or not (0 <= l <= 1) or not (0 <= s <= 1): raise ValueError(\\"HLS values must be between 0 and 1\\") elif source_space == \\"HSV\\": if not (0 <= h <= 1) or not (0 <= s <= 1) or not (0 <= v <= 1): raise ValueError(\\"HSV values must be between 0 and 1\\") else: raise ValueError(\\"Invalid source color space\\") # Convert source color to RGB first if source_space == \\"RGB\\": rgb = (r, g, b) elif source_space == \\"YIQ\\": rgb = colorsys.yiq_to_rgb(y, i, q) elif source_space == \\"HLS\\": rgb = colorsys.hls_to_rgb(h, l, s) elif source_space == \\"HSV\\": rgb = colorsys.hsv_to_rgb(h, s, v) # Convert RGB to target space if target_space == \\"RGB\\": return rgb elif target_space == \\"YIQ\\": return colorsys.rgb_to_yiq(*rgb) elif target_space == \\"HLS\\": return colorsys.rgb_to_hls(*rgb) elif target_space == \\"HSV\\": return colorsys.rgb_to_hsv(*rgb) # If target space is not valid raise ValueError(\\"Invalid target color space\\")"},{"question":"Objective: Demonstrate understanding of tensor interoperability between PyTorch and other deep learning frameworks using DLPack, and showcase proficiency in tensor operations within PyTorch. Problem Statement: You are provided with a DLPack tensor and a set of operations to perform within PyTorch. Your task is to: 1. Convert the DLPack tensor to a PyTorch tensor. 2. Perform a series of specified operations on the PyTorch tensor. 3. Convert the final PyTorch tensor back to a DLPack tensor. Function Implementation: Implement the function `process_dlpack_tensor(dlpack_tensor: object, operations: dict) -> object`. # Input 1. `dlpack_tensor`: A DLPack tensor object. 2. `operations`: A dictionary containing operations to perform on the PyTorch tensor. Each key in the dictionary is a string representing an operation (\'add\', \'subtract\', \'multiply\', \'divide\'), and each value is a float or an integer to use in that operation. # Output - Return the resulting tensor as a DLPack tensor after performing all specified operations. # Constraints - The input tensor will always be a 1D or 2D tensor. - The operations dictionary will only contain the keys: \'add\', \'subtract\', \'multiply\', and \'divide\'. - Operations should be applied in the order they appear in the dictionary. # Example ```python # Example DLPack tensor (provided as a placeholder, actual data will be in DLPack format) dlpack_tensor = <DLPack tensor object> # Operations to perform operations = { \'add\': 5, \'multiply\': 2, } result_tensor = process_dlpack_tensor(dlpack_tensor, operations) # Assuming initial tensor was [[1, 2], [3, 4]], after the operations it should be [[12, 14], [16, 18]] ``` # Notes - You may assume that the initial tensor and operations are valid and well-formed. - Use `torch.utils.dlpack.from_dlpack` and `torch.utils.dlpack.to_dlpack` for conversions. - Handle tensor operations using PyTorch methods. Implementation Tips 1. Use `torch.utils.dlpack.from_dlpack` to convert the initial DLPack tensor to a PyTorch tensor. 2. Perform the tensor operations in the specified order. 3. Use `torch.utils.dlpack.to_dlpack` to convert the final PyTorch tensor back to a DLPack tensor. Solution Template Below is a template to help you start: ```python import torch from torch.utils.dlpack import from_dlpack, to_dlpack def process_dlpack_tensor(dlpack_tensor: object, operations: dict) -> object: # Step 1: Convert DLPack tensor to a PyTorch tensor pytorch_tensor = from_dlpack(dlpack_tensor) # Step 2: Perform specified operations for op, value in operations.items(): if op == \'add\': pytorch_tensor += value elif op == \'subtract\': pytorch_tensor -= value elif op == \'multiply\': pytorch_tensor *= value elif op == \'divide\': pytorch_tensor /= value # Step 3: Convert the resulting PyTorch tensor back to a DLPack tensor result_dlpack_tensor = to_dlpack(pytorch_tensor) return result_dlpack_tensor ``` Test your implementation with various input tensors and operations to ensure correctness.","solution":"import torch from torch.utils.dlpack import from_dlpack, to_dlpack def process_dlpack_tensor(dlpack_tensor: object, operations: dict) -> object: Convert the DLPack tensor to a PyTorch tensor, perform the specified operations, and convert the final PyTorch tensor back to a DLPack tensor. Parameters: dlpack_tensor (object): A DLPack tensor object. operations (dict): A dictionary containing operations to perform on the PyTorch tensor. Returns: object: The resulting tensor as a DLPack tensor after performing all specified operations. # Step 1: Convert DLPack tensor to a PyTorch tensor pytorch_tensor = from_dlpack(dlpack_tensor) # Step 2: Perform specified operations for op, value in operations.items(): if op == \'add\': pytorch_tensor += value elif op == \'subtract\': pytorch_tensor -= value elif op == \'multiply\': pytorch_tensor *= value elif op == \'divide\': pytorch_tensor /= value else: raise ValueError(f\\"Unsupported operation: {op}\\") # Step 3: Convert the resulting PyTorch tensor back to a DLPack tensor result_dlpack_tensor = to_dlpack(pytorch_tensor) return result_dlpack_tensor"},{"question":"You are required to write a Python function to retrieve specific user account information from the Unix shadow password database. The function should demonstrate your understanding of the `spwd` module and its utilities. # Function Definition ```python def get_user_info(username): Retrieve specific information for the given username from the shadow password database. Args: username (str): The username for which to retrieve information. Returns: dict: A dictionary containing the following keys: - \'login_name\' (str): The login name of the user. - \'encrypted_password\' (str): The encrypted password of the user. - \'last_change\' (int): The number of days since the last password change. - \'minimum_days\' (int): The minimum number of days between password changes. - \'maximum_days\' (int): The maximum number of days between password changes. - \'warning_days\' (int): The number of days before the password expires and the user is warned. - \'inactive_days\' (int): The number of days after password expiry before the account is disabled. - \'account_expiry\' (int): The number of days since 1970-01-01 when the account expires. - \'reserved\' (int): The reserved field. Raises: PermissionError: If the user does not have the necessary privileges to access the shadow password database. KeyError: If the username does not exist in the shadow password database. pass ``` # Constraints and Requirements - Ensure the function appropriately handles exceptions, particularly the `PermissionError` and `KeyError`. - You must have root privileges to access the shadow password database. If you don\'t, a `PermissionError` will be raised. - The function should retrieve information based on the username provided and return it in a dictionary format. - Assume the input `username` is always a valid string. # Example Usage ```python try: user_info = get_user_info(\\"john\\") print(user_info) except PermissionError: print(\\"Permission denied. You need root privileges to access the shadow password database.\\") except KeyError: print(\\"User not found in the shadow password database.\\") ``` Expected output if user \\"john\\" exists and proper privileges are available: ```python { \'login_name\': \'john\', \'encrypted_password\': \'6randomsaltencryptedpassword\', \'last_change\': 18500, \'minimum_days\': 0, \'maximum_days\': 99999, \'warning_days\': 7, \'inactive_days\': -1, \'account_expiry\': -1, \'reserved\': 0 } ``` Write your solution code inside the `get_user_info` function.","solution":"import spwd def get_user_info(username): Retrieve specific information for the given username from the shadow password database. Args: username (str): The username for which to retrieve information. Returns: dict: A dictionary containing the following keys: - \'login_name\' (str): The login name of the user. - \'encrypted_password\' (str): The encrypted password of the user. - \'last_change\' (int): The number of days since the last password change. - \'minimum_days\' (int): The minimum number of days between password changes. - \'maximum_days\' (int): The maximum number of days between password changes. - \'warning_days\' (int): The number of days before the password expires and the user is warned. - \'inactive_days\' (int): The number of days after password expiry before the account is disabled. - \'account_expiry\' (int): The number of days since 1970-01-01 when the account expires. - \'reserved\' (int): The reserved field. Raises: PermissionError: If the user does not have the necessary privileges to access the shadow password database. KeyError: If the username does not exist in the shadow password database. try: user_info = spwd.getspnam(username) except PermissionError: raise PermissionError(\\"Permission denied. You need root privileges to access the shadow password database.\\") except KeyError: raise KeyError(\\"User not found in the shadow password database.\\") return { \'login_name\': user_info.sp_namp, \'encrypted_password\': user_info.sp_pwdp, \'last_change\': user_info.sp_lstchg, \'minimum_days\': user_info.sp_min, \'maximum_days\': user_info.sp_max, \'warning_days\': user_info.sp_warn, \'inactive_days\': user_info.sp_inact, \'account_expiry\': user_info.sp_expire, \'reserved\': user_info.sp_flag }"},{"question":"# Python 3.10 Coding Assessment Question Objective Implement a Python class that aggregates and processes a list of student scores, ensuring functionality derived from comprehension of built-in functions and OOP principles. Problem Statement You are tasked with developing a class in Python named `StudentScores`. This class will manage and process scores of students to offer insights such as highest score, lowest score, average score, and filtered lists based on passed scores. # Requirements 1. **Initialization**: The class should be initialized with a list of scores. 2. **Methods**: - **`add_score(self, score: int)`**: Adds a new score to the list. - **`remove_score(self, score: int)`**: Removes a score from the list if it exists. - **`highest_score(self) -> int`**: Returns the highest score. - **`lowest_score(self) -> int`**: Returns the lowest score. - **`average_score(self) -> float`**: Returns the average of all scores. - **`filter_passing_scores(self, passing_mark: int) -> list`**: Returns a list of scores that are higher than or equal to the `passing_mark`. # Constraints - The scores are integers. - The `passing_mark` is an integer. - The scores list can have duplicate scores. # Example ```python student_scores = StudentScores([70, 85, 90, 75, 88, 95]) student_scores.add_score(80) student_scores.remove_score(75) print(student_scores.highest_score()) # Output: 95 print(student_scores.lowest_score()) # Output: 70 print(student_scores.average_score()) # Output: 84.66666666666667 print(student_scores.filter_passing_scores(85)) # Output: [85, 90, 88, 95] ``` # Notes - Ensure that your implementation leverages built-in functions like `max()`, `min()`, `sum()`, and `filter()` where applicable. - Pay attention to edge cases, such as removing scores that do not exist in the list. Submission Submit your implementation of the `StudentScores` class including all required methods. Ensure that your code is well-documented and handles various edge cases properly.","solution":"class StudentScores: def __init__(self, scores): self.scores = scores def add_score(self, score: int): self.scores.append(score) def remove_score(self, score: int): if score in self.scores: self.scores.remove(score) def highest_score(self) -> int: return max(self.scores) def lowest_score(self) -> int: return min(self.scores) def average_score(self) -> float: return sum(self.scores) / len(self.scores) def filter_passing_scores(self, passing_mark: int) -> list: return [score for score in self.scores if score >= passing_mark]"},{"question":"# Advanced Python Assessment: Working with `urllib` Objective: This assessment aims to test your ability to fetch, handle, and process HTTP responses using Python\'s `urllib` module. You are required to implement multiple functions that will demonstrate your understanding of different aspects of HTTP requests and handling various types of responses and errors. Problem Statement: You are required to implement a script that meets the following requirements: 1. **Functionality to Fetch URLs:** - Implement a function `fetch_url(url: str) -> str` that takes a URL as input and returns the HTML content of the page as a string. - This function should handle HTTP errors by returning appropriate error messages like \\"404: Not Found\\" or \\"403: Forbidden\\". 2. **Functionality for POST Requests:** - Implement a function `post_data(url: str, data: dict) -> str` that takes a URL and a dictionary of data to post to the server. - The function should return the server\'s response as a string. - Ensure that the data is properly URL-encoded before sending the request. 3. **Custom User-Agent:** - Implement a function `fetch_with_user_agent(url: str, user_agent: str) -> str` that takes a URL and a custom User-Agent string. - This function should fetch the URL content using the specified User-Agent and return the HTML content as a string. 4. **Handling Authentication:** - Implement a function `fetch_with_auth(url: str, username: str, password: str) -> str` that takes a URL along with a username and password for Basic Authentication. - The function should handle authentication and return the server\'s response as a string. 5. **Combining Functionality in a Main Script:** - Create a main script that demonstrates the usage of all the above functions. - The script should read user input to choose which function to execute and display the results accordingly. Constraints: - Assume the URLs being used are valid and reachable. You do not need to handle client-side validation. - For the purpose of this assessment, you can assume that POST requests will always be made to endpoints that require POST data to be encoded in \'application/x-www-form-urlencoded\' format. - Make sure to handle all potential exceptions and provide useful error messages to the user. Example Usage: ```python # Examples of how the functions might be called # Fetch the content of a webpage html_content = fetch_url(\\"http://example.com\\") # Post data to a server and get the response response = post_data(\\"http://example.com/api\\", {\\"key\\": \\"value\\"}) # Fetch a page with a custom User-Agent html_content = fetch_with_user_agent(\\"http://example.com\\", \\"CustomUserAgent/1.0\\") # Fetch a page that requires basic authentication response = fetch_with_auth(\\"http://example.com/secret\\", \\"user\\", \\"pass\\") ``` Submission Instructions: - Implement your solution in a file named `urllib_assessment.py`. - Ensure your code is well-commented and follows Pythonic conventions. - Include a `README.md` file that explains how to run your script and provides examples of input and output. All the best!","solution":"import urllib.request import urllib.parse import urllib.error from urllib.error import HTTPError, URLError def fetch_url(url: str) -> str: Fetch the URL content. Parameters: url (str): The URL to fetch. Returns: str: The HTML content of the page or an error message. try: with urllib.request.urlopen(url) as response: return response.read().decode(\\"utf-8\\") except HTTPError as e: return f\\"{e.code}: {e.reason}\\" except URLError as e: return f\\"URL error: {e.reason}\\" def post_data(url: str, data: dict) -> str: Post data to the server. Parameters: url (str): The URL to post data to. data (dict): The data to post. Returns: str: The server\'s response. try: data_encoded = urllib.parse.urlencode(data).encode(\\"utf-8\\") req = urllib.request.Request(url, data=data_encoded, method=\\"POST\\") with urllib.request.urlopen(req) as response: return response.read().decode(\\"utf-8\\") except HTTPError as e: return f\\"{e.code}: {e.reason}\\" except URLError as e: return f\\"URL error: {e.reason}\\" def fetch_with_user_agent(url: str, user_agent: str) -> str: Fetch the URL using a custom User-Agent. Parameters: url (str): The URL to fetch. user_agent (str): The User-Agent string to use. Returns: str: The HTML content of the page. try: req = urllib.request.Request(url, headers={\\"User-Agent\\": user_agent}) with urllib.request.urlopen(req) as response: return response.read().decode(\\"utf-8\\") except HTTPError as e: return f\\"{e.code}: {e.reason}\\" except URLError as e: return f\\"URL error: {e.reason}\\" def fetch_with_auth(url: str, username: str, password: str) -> str: Fetch the URL with Basic Authentication. Parameters: url (str): The URL to fetch. username (str): The username for Basic Authentication. password (str): The password for Basic Authentication. Returns: str: The server\'s response. try: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler) with opener.open(url) as response: return response.read().decode(\\"utf-8\\") except HTTPError as e: return f\\"{e.code}: {e.reason}\\" except URLError as e: return f\\"URL error: {e.reason}\\""},{"question":"# Multiclass, Multilabel, and Multioutput Classification using Scikit-Learn Objective: To assess your understanding of scikit-learn\'s functionality related to multiclass, multilabel, and multioutput problems, this task requires you to implement and evaluate different strategies for multiclass classification and multioutput regression. Problem Statement: You are provided with a dataset that involves both classification and regression tasks on multiple targets. Specifically, you will: 1. Implement a multiclass classification using OneVsRestClassifier. 2. Implement a multilabel classification using OneVsRestClassifier. 3. Implement a multioutput regression using MultiOutputRegressor. Use the appropriate datasets available in scikit-learn for these tasks. Task 1: Multiclass Classification using OneVsRestClassifier 1. Load the Iris dataset using `datasets.load_iris()`. 2. Implement a `OneVsRestClassifier` using a `LinearSVC` base estimator. 3. Train the classifier on the dataset. 4. Evaluate the classifier accuracy on the training set. Task 2: Multilabel Classification using OneVsRestClassifier 1. Create a multilabel dataset using `datasets.make_multilabel_classification()`. 2. Implement a `OneVsRestClassifier` using `RandomForestClassifier` base estimator. 3. Train the classifier on the dataset. 4. Evaluate the classifier accuracy on the training set. Task 3: Multioutput Regression using MultiOutputRegressor 1. Create a multioutput regression dataset using `datasets.make_regression()` with multiple targets. 2. Implement `MultiOutputRegressor` using `GradientBoostingRegressor` as the base estimator. 3. Train the regressor on the dataset. 4. Print the regression mean squared error on the training set. # Input: None. Use Scikit-Learn\'s in-built datasets. # Output For each task, print the accuracy (for classification tasks) or mean squared error (for regression task) of the model on the training set. # Constraints: - You may not use any built-in method for model evaluation other than those provided by scikit-learn. - For reproducibility, use a fixed random state where applicable. # Example: Given the steps and requirements, the following is an illustrative sample: **Task 1: Multiclass Classification** ```python from sklearn import datasets from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score # Load dataset X, y = datasets.load_iris(return_X_y=True) # Implement OneVsRestClassifier ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier.fit(X, y) # Evaluate the classifier y_pred = ovr_classifier.predict(X) accuracy = accuracy_score(y, y_pred) print(\\"Multiclass Classification Accuracy:\\", accuracy) ``` **Task 2: Multilabel Classification** ```python from sklearn.datasets import make_multilabel_classification from sklearn.multiclass import OneVsRestClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Create dataset X, y = make_multilabel_classification(n_samples=100, n_features=20, n_labels=5, random_state=0) # Implement OneVsRestClassifier clf = OneVsRestClassifier(RandomForestClassifier(random_state=0)) clf.fit(X, y) # Evaluate the classifier accuracy = clf.score(X, y) print(\\"Multilabel Classification Accuracy:\\", accuracy) ``` **Task 3: Multioutput Regression** ```python from sklearn.datasets import make_regression from sklearn.multioutput import MultiOutputRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error # Create dataset X, y = make_regression(n_samples=100, n_targets=3, random_state=1) # Implement MultiOutputRegressor mor = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)) mor.fit(X, y) # Evaluate the regressor y_pred = mor.predict(X) mse = mean_squared_error(y, y_pred) print(\\"Multioutput Regression Mean Squared Error:\\", mse) ```","solution":"from sklearn import datasets from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.datasets import make_multilabel_classification, make_regression from sklearn.multioutput import MultiOutputRegressor from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor from sklearn.metrics import accuracy_score, mean_squared_error def multiclass_classification(): # Task 1: Multiclass Classification using OneVsRestClassifier # Load dataset X, y = datasets.load_iris(return_X_y=True) # Implement OneVsRestClassifier ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier.fit(X, y) # Evaluate the classifier y_pred = ovr_classifier.predict(X) accuracy = accuracy_score(y, y_pred) return accuracy def multilabel_classification(): # Task 2: Multilabel Classification using OneVsRestClassifier # Create dataset X, y = make_multilabel_classification(n_samples=100, n_features=20, n_labels=5, random_state=0) # Implement OneVsRestClassifier clf = OneVsRestClassifier(RandomForestClassifier(random_state=0)) clf.fit(X, y) # Evaluate the classifier accuracy = clf.score(X, y) return accuracy def multioutput_regression(): # Task 3: Multioutput Regression using MultiOutputRegressor # Create dataset X, y = make_regression(n_samples=100, n_targets=3, random_state=1) # Implement MultiOutputRegressor mor = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)) mor.fit(X, y) # Evaluate the regressor y_pred = mor.predict(X) mse = mean_squared_error(y, y_pred) return mse"},{"question":"**Custom Transformer Implementation in Scikit-learn** # Objective: Implement a custom transformer in scikit-learn that normalizes the data using Min-Max scaling. The transformer should be compatible with scikit-learn pipelines and model selection tools. # Guidelines: Your custom transformer should: 1. Inherit from `BaseEstimator` and `TransformerMixin`. 2. Implement methods `__init__()`, `fit(X, y=None)`, `transform(X)`, and `fit_transform(X, y=None)`. # Requirements: 1. The `__init__()` method should accept two parameters: `feature_range` (default `(0, 1)`) and `copy` (default `True`). 2. The `fit()` method should compute the minimum and maximum values for each feature in the dataset `X`. 3. The `transform()` method should scale the data using the formula: [ x_{scaled} = frac{x - min}{max - min} times (max_range - min_range) + min_range ] where `min` and `max` are the learned values for each feature, and `min_range` and `max_range` are defined by `feature_range`. 4. The `fit_transform()` method should fit the transformer to `X` and then transform `X`. 5. Ensure that the transformed data has the same shape as the input data `X`. # Constraints: - Input `X` is a numpy array of shape `(n_samples, n_features)`. - Handle edge cases where `max` and `min` are equal for any feature by raising a `ValueError`. # Performance Requirements: - Ensure that the `fit` method runs in (O(n times m)) time complexity, where (n) is the number of samples and (m) is the number of features. # Input and Output: - The `fit` and `fit_transform` methods should return the transformer object itself. - The `transform` method should return the scaled data as a numpy array of the same shape as `X`. # Example Usage: ```python import numpy as np from sklearn.pipeline import Pipeline class MinMaxScalerCustom(BaseEstimator, TransformerMixin): def __init__(self, feature_range=(0, 1), copy=True): self.feature_range = feature_range self.copy = copy def fit(self, X, y=None): X = np.asarray(X) self.min_ = np.min(X, axis=0) self.max_ = np.max(X, axis=0) self.data_range_ = self.max_ - self.min_ if np.any(self.data_range_ == 0): raise ValueError(\\"Some features have the same value in all samples. Cannot perform min-max scaling.\\") return self def transform(self, X): X = np.asarray(X, dtype=np.float64) X_std = (X - self.min_) / self.data_range_ X_scaled = X_std * (self.feature_range[1] - self.feature_range[0]) + self.feature_range[0] return X_scaled def fit_transform(self, X, y=None): return self.fit(X, y).transform(X) # Example usage X = np.array([[1, 2], [3, 4], [5, 6]]) scaler = MinMaxScalerCustom(feature_range=(0, 1)) X_scaled = scaler.fit_transform(X) print(X_scaled) ```","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin class MinMaxScalerCustom(BaseEstimator, TransformerMixin): def __init__(self, feature_range=(0, 1), copy=True): self.feature_range = feature_range self.copy = copy def fit(self, X, y=None): X = np.asarray(X) self.min_ = np.min(X, axis=0) self.max_ = np.max(X, axis=0) self.data_range_ = self.max_ - self.min_ if np.any(self.data_range_ == 0): raise ValueError(\\"Some features have the same value in all samples. Cannot perform min-max scaling.\\") return self def transform(self, X): X = np.asarray(X, dtype=np.float64) X_std = (X - self.min_) / self.data_range_ X_scaled = X_std * (self.feature_range[1] - self.feature_range[0]) + self.feature_range[0] return X_scaled def fit_transform(self, X, y=None): return self.fit(X, y).transform(X)"},{"question":"**Objective**: Implement and utilize the `linecache` module to achieve efficient access to lines from a text file by implementing a function that returns lines from multiple files based on line numbers provided. Problem Description You are required to implement a function that processes multiple files and retrieves specific lines from each file. Specifically, you will read the line numbers specified for each file and return a structured result containing these lines. # Function Signature ```python def batch_get_lines(files: Dict[str, List[int]]) -> Dict[str, List[str]]: pass ``` # Input - `files`: A dictionary where each key is a string containing the filename and each value is a list of integers representing the line numbers to retrieve from the corresponding file. # Output - Returns a dictionary where each key corresponds to a filename from the input, and each value is a list of strings, each string being a line from the file as specified in the input. # Constraints - If a file does not exist or a line number is out of range, the corresponding return value should include an empty string `\\"\\"` for that line. - You may assume that the filenames provided are either absolute or can be resolved using the current directory (no need to handle relative paths in `sys.path`). - The combined line count being requested (across all files) will not exceed 1,000. # Example ```python # Input dictionary: keys are filenames, values are lists of line numbers input_files = { \\"file1.txt\\": [1, 3, 5], \\"file2.txt\\": [2, 4] } # Assume the following contents for the files: # file1.txt: # Line 1 content # Line 2 content # Line 3 content # Line 4 content # Line 5 content # # file2.txt: # Line A # Line B # Line C # Line D # Expected output: { \\"file1.txt\\": [\\"Line 1 content\\", \\"Line 3 content\\", \\"Line 5 content\\"], \\"file2.txt\\": [\\"Line B\\", \\"Line D\\"] } ``` Note - Make use of the `linecache` module functions to read the necessary lines. - Ensure to clear the cache appropriately to avoid memory overhead once the operation is complete. - Handle any potential errors gracefully by returning an empty string `\\"\\"` for missing lines or files.","solution":"import linecache from typing import Dict, List def batch_get_lines(files: Dict[str, List[int]]) -> Dict[str, List[str]]: Returns specific lines from multiple files based on provided line numbers. Parameters: files (Dict[str, List[int]]): A dictionary where keys are filenames and values are lists of line numbers to retrieve. Returns: Dict[str, List[str]]: A dictionary where keys are filenames and values are lists of lines retrieved from each file. result = {} for filename, line_numbers in files.items(): lines = [] for line_number in line_numbers: line = linecache.getline(filename, line_number) if line == \\"\\": lines.append(\\"\\") # Handle missing file or line by appending an empty string else: lines.append(line.rstrip(\'n\')) # Remove newline character at the end of each line result[filename] = lines linecache.clearcache() # Clear the cache after processing return result"},{"question":"**Coding Assessment Question:** # Objective: To demonstrate your understanding of the BernoulliRBM class from the scikit-learn library and your ability to apply it to real data for feature extraction and classification. # Task: 1. **Data Preparation**: - Create a dataset with binary features. You can use scikit-learn\'s `make_classification` function to generate a synthetic dataset of 1000 samples with 20 features, where each feature is a binary value (0 or 1) or a real value between 0 and 1. 2. **RBM Model Implementation**: - Implement a BernoulliRBM model. - Fit the model on the generated dataset. - Use the trained RBM model to transform the dataset into a new feature space. 3. **Classification**: - Use the transformed features to train a logistic regression classifier. - Evaluate the classifier using cross-validation and output the average accuracy score. # Requirements: 1. **Input/Output Format**: - The input is a synthetic binary classification dataset. - The output should be the average accuracy of the logistic regression classifier on the transformed features. 2. **Constraints**: - You must use scikit-learn library for all tasks. - Ensure that the `BernoulliRBM` is properly initialized and trained. - Use cross-validation with at least 5 folds for evaluating the classifier. 3. **Performance**: - The solution should be efficient and executable within a reasonable time frame (e.g., less than a minute). # Code Template: ```python import numpy as np from sklearn.datasets import make_classification from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def perform_rbm_classification(): # 1. Data Preparation X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=42) # Normalize features to be between 0 and 1 X = X - X.min() X = X / X.max() # 2. RBM Model Implementation rbm = BernoulliRBM(n_components=128, learning_rate=0.06, n_iter=10, random_state=42) logistic = LogisticRegression(max_iter=10000, solver=\'lbfgs\') # Creating a pipeline classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Fitting RBM and Logistic Regression classifier.fit(X, y) # 3. Cross-validated evaluation scores = cross_val_score(classifier, X, y, cv=5) avg_accuracy = np.mean(scores) return avg_accuracy # Execute the function and print the result average_accuracy = perform_rbm_classification() print(f\\"Average Accuracy: {average_accuracy}\\") ``` **Explanation**: 1. **Data Preparation**: - Generate a classification dataset with binary features. - Normalize feature values to be between 0 and 1. 2. **RBM Model Implementation**: - Initialize and fit a BernoulliRBM model on the dataset. - Use RBM-transformed features to train a logistic regression classifier. 3. **Classification**: - Evaluate the classifier using 5-fold cross-validation and report the average accuracy. Make sure to follow the constraints and focus on the efficiency of the solution. Happy coding!","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def perform_rbm_classification(): # 1. Data Preparation X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=42) # Normalize features to be between 0 and 1 X = X - X.min() X = X / X.max() # 2. RBM Model Implementation rbm = BernoulliRBM(n_components=128, learning_rate=0.06, n_iter=10, random_state=42) logistic = LogisticRegression(max_iter=10000, solver=\'lbfgs\') # Creating a pipeline classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Fitting RBM and Logistic Regression classifier.fit(X, y) # 3. Cross-validated evaluation scores = cross_val_score(classifier, X, y, cv=5) avg_accuracy = np.mean(scores) return avg_accuracy # Execute the function and print the result average_accuracy = perform_rbm_classification() print(f\\"Average Accuracy: {average_accuracy}\\")"},{"question":"# **Task: Implement a Concurrency Control System** You are tasked with designing a concurrency control system for handling a set of tasks, using Python\'s `asyncio` queues. # **Problem Description:** Implement an asynchronous function `process_tasks(queue: asyncio.Queue, num_workers: int) -> Tuple[float, float]` that processes tasks concurrently using worker coroutines. The tasks will be put into the queue with varying processing times. # **Function Signature:** ```python import asyncio from typing import Tuple async def process_tasks(queue: asyncio.Queue, num_workers: int) -> Tuple[float, float]: pass ``` # **Parameters:** - `queue`: An instance of `asyncio.Queue` filled with processing times (floats) for different tasks. - `num_workers`: An integer representing the number of worker coroutines to process the tasks concurrently. # **Returns:** - A tuple containing: - The total time taken to process all tasks. - The total expected processing time (sum of all processing times in the queue). # **Details:** 1. Create `num_workers` worker coroutines that will process the tasks in the `queue`. 2. Each task involves sleeping for the specified time in seconds (to simulate doing some work). 3. Use `task_done()` after each task is processed to notify the queue. 4. Ensure that the function waits until all tasks in the queue are processed before returning. 5. The function returns the actual total processing time taken and the sum of all expected processing times. # **Example Usage:** ```python import asyncio import random # Generate random processing times and fill the queue async def main(): queue = asyncio.Queue() total_expected_time = 0 for _ in range(20): processing_time = random.uniform(0.1, 2.0) total_expected_time += processing_time await queue.put(processing_time) # Process tasks with 4 worker coroutines total_time_taken, expected_time = await process_tasks(queue, 4) print(\\"Total time taken to process tasks:\\", total_time_taken) print(\\"Total expected processing time:\\", expected_time) asyncio.run(main()) ``` # **Constraints:** - Ensure proper synchronization of the queue with all worker coroutines handling tasks without conflicting. - Your implementation should properly handle the case when there are more workers than tasks. # **Performance Requirements:** - The solution should be efficient in terms of time complexity, focusing on concurrent task completion. - Ensure to handle the asyncio event loop properly without blocking. Your task is to implement the `process_tasks` function as described.","solution":"import asyncio from typing import Tuple async def worker(queue: asyncio.Queue): while True: processing_time = await queue.get() await asyncio.sleep(processing_time) queue.task_done() async def process_tasks(queue: asyncio.Queue, num_workers: int) -> Tuple[float, float]: start_time = asyncio.get_event_loop().time() workers = [asyncio.create_task(worker(queue)) for _ in range(num_workers)] total_expected_time = 0 while not queue.empty(): total_expected_time += (await queue.get()) queue.task_done() await queue.join() end_time = asyncio.get_event_loop().time() for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) total_time_taken = end_time - start_time return total_time_taken, total_expected_time"},{"question":"You have been provided with a sample dataset `fmri` which tracks neural activity recordings over time. Using the Seaborn package, specifically `seaborn.objects`, you are to perform the following tasks to demonstrate your understanding of the fundamental and advanced concepts of Seaborn: Task 1. **Data Preparation**: - Load the `fmri` dataset from Seaborn. - Filter the dataset to include only the \\"parietal\\" region and event type \\"stim\\". 2. **Line Plot with Grouping**: - Create a line plot representing the `signal` over `timepoint`. - Group the lines by `subject`, ensuring each subject\'s data is represented by an individual line with a distinct color. 3. **Add Statistical Estimation**: - Modify the existing plot to include a statistical estimation of the line\'s central tendency using `so.Agg()`. 4. **Include Error Bands**: - Combine your plot with a `Band` plot to show error bars around the line. 5. **Custom Markers**: - Add custom markers to the line plot to clearly indicate the data points. Requirements: - Implement your solution in a function `plot_fmri_data()` which does not take any parameters. - The function should display the plot directly. - Ensure the plot is self-contained and sufficiently annotated for clarity. Stretch Goal (Optional): - Customize the plot aesthetics further by mapping different `event` types to different `linestyle`s. Example: ```python def plot_fmri_data(): import seaborn.objects as so from seaborn import load_dataset # Load and prepare the data fmri = load_dataset(\\"fmri\\") filtered_fmri = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Create the plot plot = so.Plot(filtered_fmri, \\"timepoint\\", \\"signal\\", color=\\"subject\\") plot.add(so.Line(), so.Agg()) plot.add(so.Band(), so.Est(), group=\\"event\\") plot.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) # Display the plot plot.show() # Call the function to generate and display the plot plot_fmri_data() ``` Your task is to complete and submit the implementation of this function. Constraints: - Ensure your implementation does not exceed a runtime of 5 seconds on typical lab hardware.","solution":"def plot_fmri_data(): import seaborn.objects as so from seaborn import load_dataset # Load and prepare the data fmri = load_dataset(\\"fmri\\") filtered_fmri = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Create the plot plot = so.Plot(filtered_fmri, \\"timepoint\\", \\"signal\\", color=\\"subject\\") plot.add(so.Line(), so.Agg()) plot.add(so.Band(), so.Est()) plot.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) # Display the plot plot.show() # Uncomment the line below to generate and display the plot when running the script manually # plot_fmri_data()"},{"question":"# PySequence Operations Simulator **Objective:** Implement a series of Python functions that mimic the behavior of some of the `PySequence_*` functions described in the provided documentation. This will assess your ability to understand and apply Python sequence operations at a detailed level. **Tasks:** 1. **Sequence Checker:** Implement the function `py_sequence_check(obj)` which returns `True` if the `obj` provides the sequence protocol, and `False` otherwise. Note that it should return `False` for `dict` subclasses. ```python def py_sequence_check(obj): pass ``` 2. **Sequence Length:** Implement the function `py_sequence_length(seq)` which returns the number of elements in the sequence `seq`. ```python def py_sequence_length(seq): pass ``` 3. **Sequence Concat:** Implement the function `py_sequence_concat(seq1, seq2)` which returns the concatenation of `seq1` and `seq2`. ```python def py_sequence_concat(seq1, seq2): pass ``` 4. **Sequence Repeat:** Implement the function `py_sequence_repeat(seq, count)` which returns the result of repeating the sequence `seq` `count` times. ```python def py_sequence_repeat(seq, count): pass ``` 5. **Sequence Get Item:** Implement the function `py_sequence_get_item(seq, index)` which returns the `index`th element of the sequence `seq`. ```python def py_sequence_get_item(seq, index): pass ``` 6. **Sequence Slice:** Implement the function `py_sequence_get_slice(seq, start, end)` which returns the slice of sequence `seq` from `start` to `end`. ```python def py_sequence_get_slice(seq, start, end): pass ``` 7. **Sequence Contains:** Implement the function `py_sequence_contains(seq, value)` which returns `True` if the sequence `seq` contains `value`, otherwise `False`. ```python def py_sequence_contains(seq, value): pass ``` **Constraints:** - Your implementations should be efficient and make use of Python\'s built-in capabilities where appropriate. - Assume that the input sequences (`seq`, `seq1`, `seq2`) could be any objects that implement the sequence protocol (like lists, tuples, strings). **Example:** ```python print(py_sequence_check([1, 2, 3])) # True print(py_sequence_check(\\"Hello\\")) # True print(py_sequence_check({\'a\': 1})) # False print(py_sequence_length([1, 2, 3])) # 3 print(py_sequence_length(\\"Hello\\")) # 5 print(py_sequence_concat([1, 2], [3, 4])) # [1, 2, 3, 4] print(py_sequence_concat(\\"Hello\\", \\"World\\")) # \\"HelloWorld\\" print(py_sequence_repeat([1, 2], 3)) # [1, 2, 1, 2, 1, 2] print(py_sequence_repeat(\\"ab\\", 2)) # \\"abab\\" print(py_sequence_get_item([1, 2, 3], 1)) # 2 print(py_sequence_get_item(\\"Hello\\", 4)) # \'o\' print(py_sequence_get_slice([1, 2, 3, 4], 1, 3)) # [2, 3] print(py_sequence_get_slice(\\"Hello\\", 1, 4)) # \\"ell\\" print(py_sequence_contains([1, 2, 3], 2)) # True print(py_sequence_contains(\\"Hello\\", \\"l\\")) # True print(py_sequence_contains((1, 2, 3), 4)) # False ``` **Note:** Handle any exceptions gracefully and return `None` or `False` as pertinent rather than allowing the function to crash.","solution":"def py_sequence_check(obj): Returns True if obj provides the sequence protocol, and False otherwise. Note that it should return False for dict subclasses. return hasattr(obj, \'__getitem__\') and not isinstance(obj, dict) def py_sequence_length(seq): Returns the number of elements in the sequence seq. return len(seq) def py_sequence_concat(seq1, seq2): Returns the concatenation of seq1 and seq2. return seq1 + seq2 def py_sequence_repeat(seq, count): Returns the result of repeating the sequence seq count times. return seq * count def py_sequence_get_item(seq, index): Returns the index-th element of the sequence seq. try: return seq[index] except IndexError: return None # Return None if index is out of range def py_sequence_get_slice(seq, start, end): Returns the slice of sequence seq from start to end. return seq[start:end] def py_sequence_contains(seq, value): Returns True if the sequence seq contains value, otherwise False. return value in seq"},{"question":"**Objective:** Design a function that checks the status of key backend functionalities in PyTorch and enables certain high-performance features if possible. This function will: 1. Verify if CUDA is available. 2. Check and enable TensorFloat-32 usage on CUDA. 3. Enable cuDNN benchmark mode to select the fastest convolution algorithms. 4. Clear any existing cuFFT plan cache for GPU device 0. **Function Specification:** Implement the function `configure_pytorch_backends()`, which modifies backend settings for optimal performance based on the system\'s capabilities. # Requirements: 1. **CUDA Availability**: Check if CUDA is available. - If CUDA is available, print \\"CUDA is available.\\" - If CUDA is not available, print \\"CUDA is not available.\\" 2. **TensorFloat-32 Usage**: Check if TensorFloat-32 precision is allowed in CUDA matrix multiplications. - If it is not allowed, enable TensorFloat-32 precision. - After enabling, print \\"TensorFloat-32 usage enabled.\\" 3. **cuDNN Benchmark**: Enable cuDNN benchmark mode. - Ensure that `torch.backends.cudnn.benchmark` is set to `True`. - Print \\"cuDNN benchmark enabled.\\" 4. **Clear cuFFT Plan Cache**: Clear the cuFFT plan cache for GPU device 0. - Clear the cache using the appropriate method from `torch.backends.cuda.cufft_plan_cache`. - Print \\"cuFFT plan cache cleared for GPU 0.\\" # Constraints: - You may assume that PyTorch is correctly installed and accessible. - Target an environment with a CUDA-compatible GPU for testing the function. # Function Signature: ```python def configure_pytorch_backends(): pass ``` # Example: ```python >>> configure_pytorch_backends() CUDA is available. TensorFloat-32 usage enabled. cuDNN benchmark enabled. cuFFT plan cache cleared for GPU 0. ``` This function ensures optimal configuration for PyTorch to leverage high-performance features, making it suitable for students who need to demonstrate their understanding of PyTorch\'s backend management.","solution":"import torch def configure_pytorch_backends(): if torch.cuda.is_available(): print(\\"CUDA is available.\\") if not torch.backends.cuda.matmul.allow_tf32: torch.backends.cuda.matmul.allow_tf32 = True print(\\"TensorFloat-32 usage enabled.\\") torch.backends.cudnn.benchmark = True print(\\"cuDNN benchmark enabled.\\") torch.backends.cuda.cufft_plan_cache.clear() print(\\"cuFFT plan cache cleared for GPU 0.\\") else: print(\\"CUDA is not available.\\")"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},q={class:"search-container"},z={class:"card-container"},F={key:0,class:"empty-state"},L=["disabled"],R={key:0},M={key:1};function O(n,e,l,m,o,i){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",q,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>o.searchQuery=r),placeholder:"Search..."},null,512),[[y,o.searchQuery]]),o.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>o.searchQuery="")}," âœ• ")):d("",!0)]),t("div",z,[(a(!0),s(b,null,v(i.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),i.displayedPoems.length===0?(a(),s("div",F,' No results found for "'+u(o.searchQuery)+'". ',1)):d("",!0)]),i.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:o.isLoading,onClick:e[2]||(e[2]=(...r)=>i.loadMore&&i.loadMore(...r))},[o.isLoading?(a(),s("span",M,"Loading...")):(a(),s("span",R,"See more"))],8,L)):d("",!0)])}const N=p(D,[["render",O],["__scopeId","data-v-167f2483"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/45.md","filePath":"chatai/45.md"}'),U={name:"chatai/45.md"},G=Object.assign(U,{setup(n){return(e,l)=>(a(),s("div",null,[x(N)]))}});export{Y as __pageData,G as default};
