import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,i,r){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-06dd6f6c"]]),I=JSON.parse('[{"question":"# Question: Implement a Custom Python Interpreter **Objective:** You are required to implement a custom interactive Python interpreter using the `code` module from Python\'s standard library. Your interpreter should allow users to execute Python commands interactively, maintain a persistent state, and handle incomplete code. **Requirements:** 1. **Interpreter Class**: Implement a class `CustomInterpreter` that derives from `code.InteractiveInterpreter`. 2. **Initialization**: The class should initialize the base `InteractiveInterpreter` and set up any necessary environment. 3. **Run Code Methods**: Implement the following methods: - `run_single_line(self, code: str) -> None`: Run a single line of Python code. - `run_interactive(self) -> None`: Start an interactive console session that persists between commands, maintaining state. - `run_file(self, filename: str) -> None`: Execute a script from a file, taking the filename as input. 4. **Handling Incomplete Code**: Ensure that your interpreter can handle and prompt for continuation lines when incomplete code is entered. For instance, if the user inputs `for i in range(3):`, the interpreter should wait for the following indented lines. 5. **Custom Functionality**: Add one piece of custom functionality that is not available in the default Python interpreter. For example, automatically timing each command execution and displaying the execution time, or adding a custom command that clears the interpreter environment. **Input:** - For the `run_single_line` method, input will be a single line of Python code as a string. - For the `run_interactive` method, no input parameters are required. - For the `run_file` method, input will be the filename of the Python script to be executed. **Output:** - The methods do not need to return any values, but they should correctly execute Python code and display any output or errors in the console. - For `run_single_line`, execute the input code and print the output or error to the console. - For `run_interactive`, start an interactive session that reads input from the user, executes it, and displays the results. - For `run_file`, execute the script and display any output or errors. **Constraints:** - Handle syntax errors gracefully, and ensure the interactive session can continue running after encountering an error. - The custom functionality should be clearly documented and integrated within the interpreter class. ```python import code class CustomInterpreter(code.InteractiveInterpreter): def __init__(self): # Initialize the base class super().__init__() def run_single_line(self, code: str) -> None: pass def run_interactive(self) -> None: pass def run_file(self, filename: str) -> None: pass # Add any additional methods or custom functionality here ``` **Notes:** - You may need to refer to the official Python documentation for the `code` and `codeop` modules to understand their full capabilities and usage. - Pay attention to edge cases and ensure your interpreter behaves correctly in a wide range of scenarios.","solution":"import code import time class CustomInterpreter(code.InteractiveInterpreter): def __init__(self): # Initialize the base class super().__init__() self.locals = {} def run_single_line(self, code_str: str) -> None: Runs a single line of Python code. try: _code = compile(code_str, \'<input>\', \'single\') start_time = time.time() self.runcode(_code) end_time = time.time() print(f\\"Executed in {end_time - start_time:.6f} seconds\\") except Exception as e: print(f\\"Error: {e}\\") def run_interactive(self) -> None: Starts an interactive console session. console = code.InteractiveConsole(self.locals) console.interact(\\"Custom Python Interpreter. Type \'exit()\' or Ctrl-D to exit.\\") def run_file(self, filename: str) -> None: Runs a script from a file. try: with open(filename, \'r\') as file: code_str = file.read() _code = compile(code_str, filename, \'exec\') start_time = time.time() self.runcode(_code) end_time = time.time() print(f\\"Executed in {end_time - start_time:.6f} seconds\\") except FileNotFoundError: print(f\\"Error: File \'{filename}\' not found.\\") except Exception as e: print(f\\"Error: {e}\\") def runcode(self, code_obj): try: exec(code_obj, self.locals) except Exception as e: print(f\\"Error: {e}\\")"},{"question":"**Advanced Coding Assessment: Terminal Todo List Application** **Objective:** Create a terminal-based todo list application using the `curses` module in Python. The application should allow users to manage a list of todo items interactively. **Requirements:** 1. **Initialize the Curses Environment:** - Use `curses.initscr()` to initialize the screen. - Use `curses.start_color()` to enable color functionality (if available). 2. **Main Window Layout:** - Divide the screen into three windows: - **Title Window:** Display the title \\"Todo List\\" at the top. - **Todo List Window:** Display the list of todos. Each item can be selected or deselected. - **Status Window:** Display help text or status messages at the bottom. 3. **Interactions:** - Allow navigation through the todo items using the arrow keys (`KEY_UP` and `KEY_DOWN`). - Allow adding a new todo item using the `a` key. Open a textbox for input when adding a new item. - Allow marking an item as completed using the space key. Toggle a checkmark (or similar symbol) next to the item. - Allow deleting an item using the `d` key. Delete the currently selected item. - Exit the application using the `q` key. 4. **Todo List Window:** - Use a `curses.textpad.Textbox` when adding new items. - Maintain a list to store todo items and their completed status. - Highlight the currently selected item. 5. **Implement Error Handling:** - Handle exceptions that may arise from curses functions appropriately. - Ensure the terminal state is restored on application exit, even if an error occurs. **Sample Function Signatures:** ```python def main(stdscr): # Initialization code here curses.start_color() curses.init_pair(1, curses.COLOR_RED, curses.COLOR_BLACK) # Other initialization code def draw_title_window(title_win): # Drawing code for the title window def draw_status_window(status_win, message): # Drawing code for the status window def draw_todo_list_window(todo_win, todos, selected_index): # Drawing code for the todo list window if __name__ == \\"__main__\\": curses.wrapper(main) ``` **Expected Behavior:** - When the application starts, the screen should be divided into three windows with appropriate titles and instructions. - User should be able to navigate, add, mark, delete, and exit using the specified keys. - On exit, the terminal should be restored to its normal state. **Constraints:** - The application must run in a terminal that supports the `curses` library. - Handle terminal resizing gracefully. This coding task will demonstrate your ability to handle advanced terminal I/O operations, window management, and user interactions using the `curses` module.","solution":"import curses from curses import textpad def draw_title_window(title_win): title_win.addstr(0, 0, \\"Todo List\\") title_win.refresh() def draw_status_window(status_win, message): status_win.clear() status_win.addstr(0, 0, message) status_win.refresh() def draw_todo_list_window(todo_win, todos, selected_index): todo_win.clear() for idx, todo in enumerate(todos): if idx == selected_index: todo_win.addstr(idx, 0, f\\"> {todo[\'text\']}\\", curses.A_REVERSE) else: checkmark = \\"[x]\\" if todo[\'completed\'] else \\"[ ]\\" todo_win.addstr(idx, 0, f\\" {checkmark} {todo[\'text\']}\\") todo_win.refresh() def main(stdscr): curses.start_color() curses.init_pair(1, curses.COLOR_RED, curses.COLOR_BLACK) todos = [] selected_index = 0 title_win = curses.newwin(1, curses.COLS, 0, 0) status_win = curses.newwin(1, curses.COLS, curses.LINES - 1, 0) todo_win = curses.newwin(curses.LINES - 2, curses.COLS, 1, 0) draw_title_window(title_win) draw_status_window(status_win, \\"Press \'a\' to add, \'d\' to delete, \'Space\' to complete, \'q\' to quit\\") while True: draw_todo_list_window(todo_win, todos, selected_index) key = stdscr.getch() if key == ord(\'q\'): break elif key == curses.KEY_UP: selected_index = max(0, selected_index - 1) elif key == curses.KEY_DOWN: selected_index = min(len(todos) - 1, selected_index + 1) elif key == ord(\'a\'): stdscr.addstr(curses.LINES - 1, 0, \\"Enter new todo: \\") stdscr.refresh() curses.echo() new_todo_text = stdscr.getstr(curses.LINES - 1, 16, 20).decode(\'utf-8\') curses.noecho() todos.append({\'text\': new_todo_text, \'completed\': False}) elif key == ord(\'d\') and todos: del todos[selected_index] selected_index = min(len(todos) - 1, selected_index) elif key == ord(\' \'): todos[selected_index][\'completed\'] = not todos[selected_index][\'completed\'] if __name__ == \\"__main__\\": try: curses.wrapper(main) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective You are provided with a dataset representing sales data that you need to style for a report. Your task is to implement the styling using pandas `Styler` class to make the DataFrame visually appealing, highlighting key metrics as required. Dataset A sample dataset `sales_data.csv` is provided with the following structure: ``` Product, Region, Sales, Profit Margin A, North, 1000, 0.1 B, South, 850, 0.12 C, North, 1020, 0.15 D, East, 920, 0.06 E, West, 870, 0.2 F, South, 1280, 0.08 ``` Requirements 1. **Input**: Read the provided `sales_data.csv` file into a pandas DataFrame. 2. **Styling**: - Use `Styler.apply` to highlight the row with the maximum sales in green background and minimum sales in red background. - Use `Styler.highlight_min` to highlight the column with the minimum `Profit Margin` in yellow. - Use `Styler.set_caption` to set a caption \\"Quarterly Sales Report\\". 3. **Export**: - Export the styled DataFrame to an HTML file named `styled_sales_report.html`. Function Definition ```python import pandas as pd def style_sales_report(file_path: str, output_html_path: str) -> None: Styles a sales report read from a CSV file and exports it to an HTML file. Parameters: file_path (str): The path to the sales data CSV file. output_html_path (str): The path to output the styled HTML file. Output: None # Your solution here ``` # Constraints - Ensure that the input CSV file path and output HTML file path are valid. - Ensure that `sales_data.csv` is formatted as specified. - Use efficient pandas methods to apply styles to maintain performance, especially for larger datasets. # Example Given the file path `sales_data.csv` as described above, the function should produce a styled HTML file `styled_sales_report.html` that follows the specified styling rules. This task will assess your ability to leverage pandas\' `Styler` class and apply styling effectively to DataFrames.","solution":"import pandas as pd def style_sales_report(file_path: str, output_html_path: str) -> None: Styles a sales report read from a CSV file and exports it to an HTML file. Parameters: file_path (str): The path to the sales data CSV file. output_html_path (str): The path to output the styled HTML file. Output: None df = pd.read_csv(file_path) def highlight_max_min_sales(s): is_max = s == s.max() is_min = s == s.min() return [\'background-color: green\' if v else \'background-color: red\' if is_min[i] else \'\' for i,v in enumerate(is_max)] styled_df = df.style.apply(highlight_max_min_sales, subset=[\'Sales\'], axis=0) .highlight_min(subset=[\'Profit Margin\'], color=\'yellow\') .set_caption(\\"Quarterly Sales Report\\") styled_df.to_html(output_html_path)"},{"question":"**Problem Statement:** You are tasked with implementing a function that processes a list of file paths based on a series of include and exclude patterns. This function will simulate part of the source distribution process by filtering files according to specified patterns. **Function Specification:** ```python def filter_files(file_paths, commands): Filters a list of file paths based on include and exclude commands. Args: file_paths (list of str): The list of file paths to be filtered. commands (list of str): The list of commands to apply to the file paths. Each command is a string formatted as \\"<command> <pattern1> <pattern2> ...\\". The <command> can be one of the following: - \\"include\\": include files matching any of the specified patterns - \\"exclude\\": exclude files matching any of the specified patterns - \\"recursive-include\\": include files under the specified directory - \\"recursive-exclude\\": exclude files under the specified directory - \\"global-include\\": include files anywhere in the source tree - \\"global-exclude\\": exclude files anywhere in the source tree - \\"prune\\": exclude all files under the specified directory - \\"graft\\": include all files under the specified directory Returns: list of str: The filtered list of file paths. pass ``` **Input:** 1. `file_paths` (list of str): A list of file paths. 2. `commands` (list of str): A list of inclusion/exclusion commands to be applied to the file paths. Each command is a string formatted as `\\"command pattern1 pattern2 ...\\"`, where `command` can be one of \\"include\\", \\"exclude\\", \\"recursive-include\\", \\"recursive-exclude\\", \\"global-include\\", \\"global-exclude\\", \\"prune\\", or \\"graft\\". **Output:** - A list of filtered file paths that match the inclusion/exclusion criteria. **Constraints:** - The function should support Unix-style glob patterns for filenames. - The `file_paths` will contain valid file paths. - Commands are to be processed in the order they are provided. - Assume that no path separators other than `/` will be used, i.e., we are working in a Unix-like environment. # Examples: **Example 1:** ```python file_paths = [ \\"src/module1.py\\", \\"src/module2.py\\", \\"docs/readme.md\\", \\"tests/test_module1.py\\" ] commands = [ \\"include src/*.py docs/*.md\\", \\"exclude src/module2.py\\" ] assert filter_files(file_paths, commands) == [\\"src/module1.py\\", \\"docs/readme.md\\"] ``` **Example 2:** ```python file_paths = [ \\"src/module1.py\\", \\"src/module2.py\\", \\"docs/readme.md\\", \\"tests/test_module1.py\\", \\"tests/test_module2.py\\", ] commands = [ \\"prune tests\\", \\"include src/*.py docs/*.md\\" ] assert filter_files(file_paths, commands) == [\\"src/module1.py\\", \\"src/module2.py\\", \\"docs/readme.md\\"] ``` **Example 3:** ```python file_paths = [\\"a.py\\", \\"b.py\\", \\"c.py\\", \\"dir/d.py\\"] commands = [ \\"global-exclude *.py\\", \\"graft dir\\" ] assert filter_files(file_paths, commands) == [\\"dir/d.py\\"] ``` # Note: - Be sure to handle edge cases such as empty `file_paths` or `commands` lists, as well as non-matching patterns. - You may use the `glob` module for pattern matching if necessary.","solution":"import fnmatch import os def filter_files(file_paths, commands): Filters a list of file paths based on include and exclude commands. Args: file_paths (list of str): The list of file paths to be filtered. commands (list of str): The list of commands to apply to the file paths. Each command is a string formatted as \\"<command> <pattern1> <pattern2> ...\\". The <command> can be one of the following: - \\"include\\": include files matching any of the specified patterns - \\"exclude\\": exclude files matching any of the specified patterns - \\"recursive-include\\": include files under the specified directory - \\"recursive-exclude\\": exclude files under the specified directory - \\"global-include\\": include files anywhere in the source tree - \\"global-exclude\\": exclude files anywhere in the source tree - \\"prune\\": exclude all files under the specified directory - \\"graft\\": include all files under the specified directory Returns: list of str: The filtered list of file paths. def matches_any_pattern(file_path, patterns): return any(fnmatch.fnmatch(file_path, pattern) for pattern in patterns) included_files = file_paths.copy() for command in commands: parts = command.split() cmd = parts[0] patterns = parts[1:] if cmd == \\"include\\": included_files = [f for f in included_files if matches_any_pattern(f, patterns)] elif cmd == \\"exclude\\": included_files = [f for f in included_files if not matches_any_pattern(f, patterns)] elif cmd == \\"recursive-include\\": include_dir = patterns[0] included_files = [f for f in included_files if f.startswith(include_dir) and matches_any_pattern(f, patterns[1:])] elif cmd == \\"recursive-exclude\\": exclude_dir = patterns[0] included_files = [f for f in included_files if not (f.startswith(exclude_dir) and matches_any_pattern(f, patterns[1:]))] elif cmd == \\"global-include\\": for pattern in patterns: included_files.extend([f for f in file_paths if fnmatch.fnmatch(f, pattern) and f not in included_files]) elif cmd == \\"global-exclude\\": for pattern in patterns: included_files = [f for f in included_files if not fnmatch.fnmatch(f, pattern)] elif cmd == \\"prune\\": prune_dir = patterns[0] included_files = [f for f in included_files if not f.startswith(prune_dir)] elif cmd == \\"graft\\": graft_dir = patterns[0] included_files.extend([f for f in file_paths if f.startswith(graft_dir) and f not in included_files]) return included_files"},{"question":"You are tasked with creating a Python script that can interact with a web API to retrieve data, process it, and handle errors and redirect responses appropriately. The script should be designed using the `urllib.request` module. # Objectives: 1. Write a function `fetch_data` that takes a URL and parameters as input, sends an HTTP GET request to the URL, and returns the server\'s response. 2. Write a function `submit_data` that takes a URL, parameters, and data to send an HTTP POST request and returns the server’s response. 3. Handle redirections automatically within your functions. 4. Include error handling to manage different HTTP status codes such as `404 Not Found` and `500 Internal Server Error`. 5. Include proxy settings and basic authentication handling. # Function Specifications: 1. fetch_data(url: str, params: dict) -> str - **Input:** - `url` (str): The base URL to which the request will be made. - `params` (dict): A dictionary of query parameters to be appended to the URL. - **Output:** - Returns the server\'s response as a string. - **Constraints:** - Handle HTTP redirections within this function. - Raise appropriate exceptions for HTTP errors, e.g., `404`, `500`. 2. submit_data(url: str, params: dict, data: dict) -> str - **Input:** - `url` (str): The URL to which the data will be submitted. - `params` (dict): A dictionary of query parameters to be appended to the URL. - `data` (dict): A dictionary of data to be sent in the body of the POST request. - **Output:** - Returns the server\'s response as a string. - **Constraints:** - Handle HTTP redirections within this function. - Raise appropriate exceptions for HTTP errors, e.g., `404`, `500`. # Additional Requirements: - Implement a basic proxy handler. - Implement basic HTTP authentication and ensure it is used for both GET and POST requests. # Example Usage: ```python # Example usage of fetch_data url = \\"http://example.com/api/resource\\" params = {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"} try: response = fetch_data(url, params) print(\\"GET Response:\\", response) except URLError as e: print(f\\"GET Error: {e}\\") # Example usage of submit_data url = \\"http://example.com/api/resource\\" params = {\\"key1\\": \\"value1\\"} data = {\\"field1\\": \\"value1\\", \\"field2\\": \\"value2\\"} try: response = submit_data(url, params, data) print(\\"POST Response:\\", response) except URLError as e: print(f\\"POST Error: {e}\\") ``` # Additional Notes: - Use the `urllib.request` library in your implementation. - Ensure that the functions use context managers (`with` statement) to manage network connections. - The basic HTTP authentication credentials can be hardcoded for this assessment. # Evaluation Criteria: - Correctness of the functionality. - Proper exception handling and redirection management. - Utilization of `urllib.request`. - Code style, readability, and comments for clarity.","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError from base64 import b64encode def fetch_data(url, params): Sends an HTTP GET request to the given URL with query parameters. Args: url (str): The base URL to which the request will be made. params (dict): A dictionary of query parameters to be appended to the URL. Returns: str: The server\'s response as a string. Raises: HTTPError: For HTTP errors, with appropriate status codes. URLError: For non-HTTP errors. try: query_string = urllib.parse.urlencode(params) full_url = f\\"{url}?{query_string}\\" request = urllib.request.Request(full_url) auth = (\'Basic \' + b64encode(b\'username:password\').decode(\'utf-8\')) request.add_header(\'Authorization\', auth) with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except HTTPError as e: raise HTTPError(e.url, e.code, e.reason, e.headers, e.fp) except URLError as e: raise URLError(e.reason) def submit_data(url, params, data): Sends an HTTP POST request to the given URL with query parameters and data. Args: url (str): The URL to which the data will be submitted. params (dict): A dictionary of query parameters to be appended to the URL. data (dict): A dictionary of data to be sent in the body of the POST request. Returns: str: The server’s response as a string. Raises: HTTPError: For HTTP errors, with appropriate status codes. URLError: For non-HTTP errors. try: query_string = urllib.parse.urlencode(params) full_url = f\\"{url}?{query_string}\\" data_encoded = urllib.parse.urlencode(data).encode(\'utf-8\') request = urllib.request.Request(full_url, data=data_encoded) auth = (\'Basic \' + b64encode(b\'username:password\').decode(\'utf-8\')) request.add_header(\'Authorization\', auth) with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except HTTPError as e: raise HTTPError(e.url, e.code, e.reason, e.headers, e.fp) except URLError as e: raise URLError(e.reason)"},{"question":"**Question: Web Data Extraction using Urllib** **Objective:** Write a Python function using the urllib package to extract and return specific data from a given URL. Your function should demonstrate the ability to open and read URLs, handle potential errors, and parse URL components. **Function Signature:** ```python def extract_data_from_url(url: str) -> dict: Extracts specific data from a given URL. Args: url (str): The URL to extract data from. Returns: dict: A dictionary containing the extracted data with the following structure: { \'status_code\': int, \'headers\': dict, \'content_length\': int, \'scheme\': str, \'netloc\': str, \'path\': str, \'query\': dict, \'robot_rules\': bool } ``` **Details:** 1. **Opening and Reading URLs:** - Use `urllib.request` to send a GET request to the given URL. - Retrieve the response status code, headers, and content length. 2. **Handling Errors:** - Make sure to handle and return error status codes such as 404 (Not Found) and 500 (Internal Server Error). 3. **Parsing URL Components:** - Use `urllib.parse` to split the input URL into its components: scheme, netloc, path, and query. - Convert the query string into a dictionary of key-value pairs. 4. **Parsing robots.txt:** - Use `urllib.robotparser` to check if a web crawler can fetch the given URL according to the site’s robots.txt rules. **Constraints:** - Assume the URL is well-formed. - The function should handle timeouts and network issues gracefully, returning appropriate error messages in the returned dictionary. **Example Input and Output:** ```python url = \\"http://example.com/some/path?param1=value1&param2=value2\\" result = extract_data_from_url(url) # Example result format { \'status_code\': 200, \'headers\': { \'Content-Type\': \'text/html; charset=UTF-8\', ... }, \'content_length\': 12345, \'scheme\': \'http\', \'netloc\': \'example.com\', \'path\': \'/some/path\', \'query\': { \'param1\': \'value1\', \'param2\': \'value2\' }, \'robot_rules\': True } ``` **Performance Requirements:** - Your function should be efficient, both in terms of network usage and processing time. - Aim to reduce the number of network calls by handling errors and validations carefully. **Notes:** - Use the appropriate modules from the `urllib` package as described to achieve the task. - Write clean, readable, and well-documented code.","solution":"import urllib.request import urllib.parse import urllib.error import urllib.robotparser def extract_data_from_url(url: str) -> dict: data = { \'status_code\': None, \'headers\': {}, \'content_length\': None, \'scheme\': \'\', \'netloc\': \'\', \'path\': \'\', \'query\': {}, \'robot_rules\': False } try: # Parse the URL parsed_url = urllib.parse.urlparse(url) data[\'scheme\'] = parsed_url.scheme data[\'netloc\'] = parsed_url.netloc data[\'path\'] = parsed_url.path data[\'query\'] = urllib.parse.parse_qs(parsed_url.query) # Check the robots.txt robot_parser = urllib.robotparser.RobotFileParser() robot_parser.set_url(f\'{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\') robot_parser.read() data[\'robot_rules\'] = robot_parser.can_fetch(\'*\', url) # Open the URL with urllib.request.urlopen(url) as response: data[\'status_code\'] = response.getcode() data[\'headers\'] = dict(response.getheaders()) data[\'content_length\'] = len(response.read()) except urllib.error.HTTPError as e: data[\'status_code\'] = e.code data[\'headers\'] = dict(e.headers) except urllib.error.URLError as e: data[\'status_code\'] = \'URLError\' data[\'headers\'] = {\'Error\': str(e.reason)} return data"},{"question":"**Question: Implement a Persistent Key-Value Store with Expiration Using dbm** You are required to implement a function `persistent_store` that manages a key-value store with an expiration feature using the `dbm` package. # Function Definition ```python def persistent_store(db_name: str, operations: list, expiration: int): Manage a key-value store with expiration using the dbm package. :param db_name: The name of the database file. :param operations: A list of operations to perform on the database. Each operation is a tuple (\\"operation_type\\", key, value_or_time). \\"operation_type\\" is a string and can be \\"set\\", \\"get\\", \\"delete\\", or \\"flush\\". - For \\"set\\", key is a string, and value_or_time is a tuple (value, timestamp) - For \\"get\\", key is a string, and value_or_time is None - For \\"delete\\", key is a string, and value_or_time is None - For \\"flush\\", key and value_or_time are None :param expiration: The number of seconds after which a key expires. :return: A list of results for \\"get\\" operations. pass ``` # Description 1. **Input Format**: - `db_name`: Name of the database file (str). - `operations`: A list of operations to be performed (list of tuples). - `expiration`: Expiration time in seconds (int). 2. **Operations**: - `\\"set\\"`: Adds a key-value pair with a specified timestamp. If the key already exists, update its value and timestamp. - `\\"get\\"`: Retrieves the value for a given key if it exists and has not expired. Expired keys should be removed. - `\\"delete\\"`: Removes a specified key from the database. - `\\"flush\\"`: Removes all entries from the database. 3. **Output**: - Returns a list of results for \\"get\\" operations. If the key does not exist or is expired, return `None` for that key. # Constraints - Assume the system clock is available and provides current time via `time.time()` (Unix time). - Operations need to handle concurrent access safely by ensuring proper use of context management and file synchronization. # Example ```python import time db_name = \\"testdb\\" # Current time for the example current_time = int(time.time()) operations = [ (\\"set\\", \\"key1\\", (\\"value1\\", current_time)), (\\"get\\", \\"key1\\", None), (\\"delete\\", \\"key1\\", None), (\\"get\\", \\"key1\\", None), (\\"set\\", \\"key2\\", (\\"value2\\", current_time + 2)), (\\"flush\\", None, None), (\\"get\\", \\"key2\\", None), ] expiration = 3600 # 1 hour print(persistent_store(db_name, operations, expiration)) # Output: [\'value1\', None, None] ``` # Notes - You may use the submodules `dbm.gnu`, `dbm.ndbm`, or `dbm.dumb` as necessary based on the system configuration. - Ensure efficient implementation to handle a large number of keys and operations.","solution":"import dbm import time def persistent_store(db_name: str, operations: list, expiration: int): Manage a key-value store with expiration using the dbm package. :param db_name: The name of the database file. :param operations: A list of operations to perform on the database. Each operation is a tuple (\\"operation_type\\", key, value_or_time). \\"operation_type\\" is a string and can be \\"set\\", \\"get\\", \\"delete\\", or \\"flush\\". - For \\"set\\", key is a string, and value_or_time is a tuple (value, timestamp) - For \\"get\\", key is a string, and value_or_time is None - For \\"delete\\", key is a string, and value_or_time is None - For \\"flush\\", key and value_or_time are None :param expiration: The number of seconds after which a key expires. :return: A list of results for \\"get\\" operations. result = [] with dbm.open(db_name, \'c\') as db: for operation in operations: op_type = operation[0] key = operation[1] value_or_time = operation[2] if op_type == \\"set\\": value, timestamp = value_or_time db[f\\"{key}\\"] = f\\"{value},{timestamp}\\" elif op_type == \\"get\\": if key in db: value, timestamp = db[f\\"{key}\\"].decode().split(\',\') current_time = int(time.time()) if current_time - int(timestamp) <= expiration: result.append(value) else: del db[f\\"{key}\\"] result.append(None) else: result.append(None) elif op_type == \\"delete\\": if key in db: del db[f\\"{key}\\"] elif op_type == \\"flush\\": for k in list(db.keys()): del db[k] return result"},{"question":"<|Analysis Begin|> The provided documentation excerpt is from the `torch.utils.dlpack` module in PyTorch, which includes two functions: - `from_dlpack`: Likely converts data from the DLPack format to a PyTorch tensor. - `to_dlpack`: Likely converts a PyTorch tensor to the DLPack format. DLPack is an open in-memory tensor structure that provides a way for tensor libraries to share data with each other without copying. Understanding and using these functions would be essential for efficient data interoperability between PyTorch and other frameworks using DLPack. Given this, a challenging and meaningful question should test the student\'s ability to use these functions to transfer data efficiently between tensors of different libraries, ensuring they understand how to work with both PyTorch tensors and DLPack. <|Analysis End|> <|Question Begin|> # Problem Statement You have been provided with the definitions of two functions from the PyTorch library: `torch.utils.dlpack.from_dlpack` and `torch.utils.dlpack.to_dlpack`. These functions help in converting between PyTorch tensors and DLPack tensors for efficient data sharing. Your task is to implement a function `double_elementwise_with_dlpack` that takes a PyTorch tensor, doubles each of its elements\' values, and returns the resulting tensor. However, the doubling operation should be performed by converting the tensor to DLPack format and back, simulating an operation in another library that uses DLPack. # Function Signature ```python def double_elementwise_with_dlpack(tensor: torch.Tensor) -> torch.Tensor: pass ``` # Input - `tensor` (torch.Tensor): A PyTorch tensor of arbitrary shape. # Output - A new PyTorch tensor where each element is doubled from the corresponding element in the input tensor. # Constraints - The input tensor may be of any shape with any numeric data type supported by PyTorch. - Ensure that the conversion between PyTorch tensors and DLPack tensors is performed correctly without data corruption. # Example ```python import torch tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32) result = double_elementwise_with_dlpack(tensor) print(result) # Expected: tensor([2, 4, 6, 8, 10]) ``` # Additional Requirements - The function `double_elementwise_with_dlpack` should use `torch.utils.dlpack.to_dlpack` and `torch.utils.dlpack.from_dlpack`. - You need to simulate doubling the elements by: 1. Converting the tensor to DLPack. 2. Converting it back from DLPack. 3. Performing the doubling operation on the tensor. # Guidance To achieve this, you may need to: 1. Ensure you understand how to convert a PyTorch tensor to a DLPack tensor and back. 2. Manipulate the converted tensor snapshots or DLPack interfaces as needed before converting back. Good luck!","solution":"import torch from torch.utils.dlpack import from_dlpack, to_dlpack def double_elementwise_with_dlpack(tensor: torch.Tensor) -> torch.Tensor: # Convert the input tensor to DLPack dlpack_tensor = to_dlpack(tensor) # Convert back from DLPack to get a PyTorch tensor tensor_from_dlpack = from_dlpack(dlpack_tensor) # Perform element-wise doubling result_tensor = tensor_from_dlpack * 2 return result_tensor"},{"question":"# Advanced Coding Assessment: Implementing a Templating System with String Substitution Problem Statement You are tasked with implementing a templating system to generate customized strings based on provided templates. You need to utilize the `string` module\'s `Template` class to create a function that performs string substitution. Write a function `custom_template_substitute(template_str, **kwargs)` that performs customized string substitution. The function will: 1. Take a template string `template_str` containing placeholders in the form of `{key}`. 2. Substitute the placeholders with the corresponding values provided as keyword arguments (`**kwargs`). 3. If a placeholder in the template string is not provided in the keyword arguments, raise a `KeyError`. Additionally, you will implement another function `safe_custom_template_substitute(template_str, **kwargs)` that performs similar substitution but leaves placeholders unchanged if their corresponding values are not provided, instead of raising an error. Input - `template_str` (string): A string containing placeholders in the form of `{key}`. - `**kwargs`: Keyword arguments providing values for the placeholders in the template string. Output - Returns the substituted string. Constraints - The input template string length will not exceed 1000 characters. - The values for substitution provided in the keyword arguments will be strings with a maximum length of 100 characters each. - At least one placeholder in the `template_str` must have a corresponding value provided in `**kwargs`. Examples ```python # Example 1 template_str = \'Hello, {name}!\' kwargs = {\\"name\\": \\"Alice\\"} assert custom_template_substitute(template_str, **kwargs) == \'Hello, Alice!\' # Example 2 template_str = \'Your order number is {order_num} and will be delivered by {delivery_date}.\' kwargs = {\\"order_num\\": \\"12345\\", \\"delivery_date\\": \\"2023-10-10\\"} assert custom_template_substitute(template_str, **kwargs) == \'Your order number is 12345 and will be delivered by 2023-10-10.\' # Example 3 (safe substitution) template_str = \'Your order number is {order_num} and will be delivered by {delivery_date}.\' kwargs = {\\"order_num\\": \\"12345\\"} assert safe_custom_template_substitute(template_str, **kwargs) == \'Your order number is 12345 and will be delivered by {delivery_date}.\' ``` Function Signature ```python def custom_template_substitute(template_str: str, **kwargs) -> str: pass def safe_custom_template_substitute(template_str: str, **kwargs) -> str: pass ``` Implement the functions `custom_template_substitute` and `safe_custom_template_substitute` using the `string.TextWrapper` class and test them with the provided examples.","solution":"import string def custom_template_substitute(template_str, **kwargs): template = string.Template(template_str) try: return template.substitute(**kwargs) except KeyError as e: raise KeyError(f\\"Missing key for substitution: {e}\\") def safe_custom_template_substitute(template_str, **kwargs): template = string.Template(template_str) return template.safe_substitute(**kwargs)"},{"question":"# Advanced Python `logging` Module Exercise Objective Your task is to design a custom logging system for a multi-threaded application. This will involve creating custom loggers, handlers, formatters, and filters to meet specific logging needs. Problem Statement You are developing a multi-threaded server application where different threads need to log messages with different severity levels and custom context information. The requirements are as follows: 1. **Custom Loggers**: - Create two custom loggers named `\'server.network\'` and `\'server.database\'`. - Ensure these loggers write log messages to separate files: `\'network.log\'` and `\'database.log\'` respectively. - Set the logging level to `DEBUG` for both loggers. 2. **Custom Formatter**: - Create a custom formatter that includes the timestamp, thread name, logger name, log level, and the log message. - The format should look like: `\\"{timestamp} [{threadName}] {loggerName} {levelName}: {message}\\"`. 3. **Custom Handler**: - Attach file handlers to each logger created above, ensuring that the custom formatter created is used. 4. **Filter Messages**: - Implement and attach a filter to exclude log messages from the `\'server.database\'` logger if the message contains the phrase `\\"ignore_me\\"`. 5. **Contextual Information**: - Add contextual information such as IP address and user ID to log records. Input and Output Format **Input**: - No direct input needed. Setup the logging system based on the requirements. **Output**: - No direct output expected. Verify by running test logs on the created logger objects. Constraints - Do not use global variables for configurations. Use threading support provided by the `logging` module. Task Implement the following steps in a `setup_logging()` function: 1. Create the loggers `server.network` and `server.database`. 2. Set their logging levels to `DEBUG`. 3. Create custom formatters with the specified format. 4. Create file handlers for `\'network.log\'` and `\'database.log\'`. 5. Attach the custom formatter to the handlers. 6. Attach the handlers to the respective loggers. 7. Implement a filter to exclude messages with `\\"ignore_me\\"` for the `server.database` logger. 8. Add custom context information to log records. # Example Here’s how you could test the implementation: ```python import logging setup_logging() # Log some messages network_logger = logging.getLogger(\'server.network\') network_logger.debug(\\"Network connection established.\\") network_logger.info(\\"User JohnDoe connected from 192.168.1.10\\") database_logger = logging.getLogger(\'server.database\') database_logger.debug(\\"Query executed successfully.\\") database_logger.info(\\"ignore_me This message should not be logged\\") database_logger.info(\\"Data inserted successfully.\\") ``` The `network.log` should include: ``` {timestamp} [MainThread] server.network DEBUG: Network connection established. {timestamp} [MainThread] server.network INFO: User JohnDoe connected from 192.168.1.10 ``` The `database.log` should include: ``` {timestamp} [MainThread] server.database DEBUG: Query executed successfully. {timestamp} [MainThread] server.database INFO: Data inserted successfully. ``` **Hints**: - Use `logging.getLogger(name)` to create or fetch loggers. - Use `logging.FileHandler()` to create handlers for file output. - Use `logging.Formatter()` to format log records. - Use `logging.Filter` or define a filter class to filter log messages. Good luck!","solution":"import logging import threading class ContextFilter(logging.Filter): def filter(self, record): record.ip = getattr(threading.current_thread(), \'ip\', \'unknown\') record.user_id = getattr(threading.current_thread(), \'user_id\', \'unknown\') if \'ignore_me\' in record.getMessage() and record.name == \'server.database\': return False return True def setup_logging(): # Create loggers network_logger = logging.getLogger(\'server.network\') database_logger = logging.getLogger(\'server.database\') # Set logging level network_logger.setLevel(logging.DEBUG) database_logger.setLevel(logging.DEBUG) # Create handlers network_handler = logging.FileHandler(\'network.log\') database_handler = logging.FileHandler(\'database.log\') # Create custom formatter formatter = logging.Formatter(\'%(asctime)s [%(threadName)s] %(name)s %(levelname)s: %(message)s (IP: %(ip)s, UserID: %(user_id)s)\') # Attach formatter to handlers network_handler.setFormatter(formatter) database_handler.setFormatter(formatter) # Add handlers to loggers network_logger.addHandler(network_handler) database_logger.addHandler(database_handler) # Add filter to the database logger to exclude \\"ignore_me\\" messages context_filter = ContextFilter() network_logger.addFilter(context_filter) database_logger.addFilter(context_filter)"},{"question":"# Custom Command Shell to Manage a To-Do List **Objective:** Create a custom shell application to manage a simple to-do list using the `cmd` module. The shell should allow users to add tasks, mark tasks as completed, list tasks, and exit the application. **Requirements:** 1. **Class Definition:** - Define a class `TodoShell` that inherits from `cmd.Cmd`. 2. **Command 1 – Adding a Task:** - Implement a method `do_add` to add a task to the to-do list. - Format: `add <task_description>` - Example: `add Buy milk` 3. **Command 2 – Marking a Task as Completed:** - Implement a method `do_done` to mark a task as completed. - Format: `done <task_number>` - Example: `done 1` 4. **Command 3 – Listing All Tasks:** - Implement a method `do_list` to list all tasks with their status (completed or not). - Format: `list` - Example: `list` 5. **Command 4 – Exiting the Application:** - Implement a method `do_exit` to stop the command loop and exit the application. - Format: `exit` - Example: `exit` 6. **Command Prefix and Prompt:** - Set the command prompt to `(todo) `. - Each command method should have a docstring explaining the usage of the command, which will be displayed when the user enters `help`. # Constraints: - Each task should be stored as a dictionary with keys `description` and `completed`. - The `completed` field should be a boolean, initially set to `False`. # Example Usage ``` (todo) add Buy milk Task added: Buy milk (todo) add Finish homework Task added: Finish homework (todo) list 1. [ ] Buy milk 2. [ ] Finish homework (todo) done 1 Task 1 marked as completed. (todo) list 1. [x] Buy milk 2. [ ] Finish homework (todo) exit Thank you for using the To-Do List Shell! ``` # Implementation Template Below is the template for the solution: ```python import cmd class TodoShell(cmd.Cmd): intro = \'Welcome to the To-Do List Shell. Type help or ? to list commands.n\' prompt = \'(todo) \' tasks = [] def do_add(self, line): \'Add a new task: add <task_description>\' task_description = line.strip() if task_description: self.tasks.append({\'description\': task_description, \'completed\': False}) print(f\'Task added: {task_description}\') else: print(\'Error: Task description cannot be empty.\') def do_done(self, line): \'Mark a task as completed: done <task_number>\' try: task_number = int(line.strip()) if task_number <= len(self.tasks): self.tasks[task_number - 1][\'completed\'] = True print(f\'Task {task_number} marked as completed.\') else: print(\'Error: Task number out of range.\') except ValueError: print(\'Error: Please provide a valid task number.\') def do_list(self, line): \'List all tasks: list\' for idx, task in enumerate(self.tasks, start=1): status = \'[x]\' if task[\'completed\'] else \'[ ]\' print(f\'{idx}. {status} {task[\\"description\\"]}\') def do_exit(self, line): \'Exit the To-Do List Shell: exit\' print(\'Thank you for using the To-Do List Shell!\') return True if __name__ == \'__main__\': TodoShell().cmdloop() ``` Implement the above template and test it to ensure all functionalities work as expected.","solution":"import cmd class TodoShell(cmd.Cmd): intro = \'Welcome to the To-Do List Shell. Type help or ? to list commands.n\' prompt = \'(todo) \' tasks = [] def do_add(self, line): \'Add a new task: add <task_description>\' task_description = line.strip() if task_description: self.tasks.append({\'description\': task_description, \'completed\': False}) print(f\'Task added: {task_description}\') else: print(\'Error: Task description cannot be empty.\') def do_done(self, line): \'Mark a task as completed: done <task_number>\' try: task_number = int(line.strip()) if 1 <= task_number <= len(self.tasks): self.tasks[task_number - 1][\'completed\'] = True print(f\'Task {task_number} marked as completed.\') else: print(\'Error: Task number out of range.\') except ValueError: print(\'Error: Please provide a valid task number.\') def do_list(self, line): \'List all tasks: list\' for idx, task in enumerate(self.tasks, start=1): status = \'[x]\' if task[\'completed\'] else \'[ ]\' print(f\'{idx}. {status} {task[\\"description\\"]}\') def do_exit(self, line): \'Exit the To-Do List Shell: exit\' print(\'Thank you for using the To-Do List Shell!\') return True if __name__ == \'__main__\': TodoShell().cmdloop()"},{"question":"# Python Coding Assessment Objective Demonstrate your understanding of the \\"site\\" module by writing functions that interact with and manipulate Python\'s module search paths and configuration settings. Problem Statement You are required to implement a set of functions that utilize the \\"site\\" module to perform various configuration tasks. Each function must adhere to the specified input and output formats and fulfill the described functionality. # Function 1: add_directory_to_path Write a function `add_directory_to_path(path: str) -> None` that: - Adds a given directory path to the `sys.path`. - Handles the corresponding `.pth` files in the given directory. - If the directory is already in `sys.path`, it should not make any redundant additions. # Function 2: get_site_packages_directories Write a function `get_site_packages_directories() -> list` that: - Returns a list containing all global site-packages directories using the `site` module. # Function 3: get_user_site_directory Write a function `get_user_site_directory() -> str` that: - Returns the path of the user-specific site-packages directory. - Ensures the path is initialized correctly if not already done. # Function 4: main_configuration Write a function `main_configuration() -> None` that: - Adds all the standard site-specific directories to the module search path by calling the `site.main()` function. - This function should not return anything. Input and Output Formats # Function 1: add_directory_to_path - **Input**: A string representing the directory path. - **Output**: This function does not return anything. # Function 2: get_site_packages_directories - **Input**: None - **Output**: A list of strings, each representing a site-packages directory. # Function 3: get_user_site_directory - **Input**: None - **Output**: A string representing the user site-packages directory. # Function 4: main_configuration - **Input**: None - **Output**: This function does not return anything. Constraints - Assume the Python environment is properly set up and the \\"site\\" module behaves as documented. - You should handle potential exceptions gracefully where applicable. - Ensure the functions are efficient and follow best practices for interacting with `sys.path` and the \\"site\\" module. Example Implementation ```python import site def add_directory_to_path(path: str) -> None: site.addsitedir(path) def get_site_packages_directories() -> list: return site.getsitepackages() def get_user_site_directory() -> str: return site.getusersitepackages() def main_configuration() -> None: site.main() ``` Ensure you test each function individually to verify their correctness. By implementing these functions, you demonstrate a solid understanding of handling Python\'s site-specific configurations.","solution":"import site import sys def add_directory_to_path(path: str) -> None: Adds a given directory path to the sys.path. Handles the corresponding .pth files in the given directory. If the directory is already in sys.path, it should not make any redundant additions. :param path: string representing the directory path to be added if path not in sys.path: site.addsitedir(path) def get_site_packages_directories() -> list: Returns a list containing all global site-packages directories using the site module. :return: list of strings, each representing a site-packages directory try: return site.getsitepackages() except AttributeError: return [] def get_user_site_directory() -> str: Returns the path of the user-specific site-packages directory. Ensures the path is initialized correctly if not already done. :return: string representing the user site-packages directory return site.getusersitepackages() def main_configuration() -> None: Adds all the standard site-specific directories to the module search path by calling the site.main() function. This function does not return anything. site.main()"},{"question":"Coding Assessment Question # Objective The objective of this assignment is to test students\' understanding of Python\'s `asyncio.Future` objects and their integration into async workflows in Python. The task requires implementing a function that processes multiple asyncio futures concurrently and collects their results. # Problem Statement You need to implement an asynchronous function, `handle_futures`, that: 1. Takes a list of `asyncio.Future` objects. 2. Waits for all of the futures to complete. 3. Collects their results. 4. Returns a list of the results in the order the futures are provided. If any of the futures raise an exception, your function should raise an `AggregateError` exception that contains all the exceptions raised by the futures. # Input - `futures`: A list of `asyncio.Future` objects. # Output - A list of results corresponding to the futures. # Constraints - The function should not use any deprecated functionality and should be compatible with Python 3.10 and later. - Ensure that the code handles any possible exceptions within the futures properly. # Implementation Requirements 1. Use the asyncio module to manage the futures. 2. The function should be asynchronous and may use other asynchronous utilities as needed. 3. Handle scenarios where futures may complete with results or exceptions. # Example Usage ```python import asyncio async def main(): loop = asyncio.get_running_loop() # Creating example futures future1 = loop.create_future() future2 = loop.create_future() future3 = loop.create_future() # Creating tasks to set their results loop.create_task(asyncio.sleep(1, result=future1.set_result(10))) loop.create_task(asyncio.sleep(2, result=future2.set_exception(ValueError(\\"Error in future 2\\")))) loop.create_task(asyncio.sleep(3, result=future3.set_result(30))) try: results = await handle_futures([future1, future2, future3]) print(results) # This should not execute as an exception is expected except AggregateError as ae: print(f\\"AggregateError occurred with exceptions: {ae.exceptions}\\") asyncio.run(main()) ``` # Notes - The `AggregateError` class should be custom implemented to store multiple exceptions. - You should utilize `asyncio.gather` and handle exceptions as needed. # AggregateError Class Here is the skeleton for the `AggregateError` class: ```python class AggregateError(Exception): def __init__(self, exceptions): self.exceptions = exceptions super().__init__(\\"One or more futures raised exceptions\\") ``` # Function Signature ```python async def handle_futures(futures: list[asyncio.Future]) -> list: # Your implementation here ```","solution":"import asyncio class AggregateError(Exception): def __init__(self, exceptions): self.exceptions = exceptions super().__init__(\\"One or more futures raised exceptions\\") async def handle_futures(futures: list[asyncio.Future]) -> list: try: # Using asyncio.gather to wait for all futures. results = await asyncio.gather(*futures, return_exceptions=True) # Collect all exceptions and raise AggregateError if any exceptions are found exceptions = [result for result in results if isinstance(result, Exception)] if exceptions: raise AggregateError(exceptions) return results except AggregateError as ae: raise ae"},{"question":"**Problem Statement:** Design a text-based interface using the `curses` module that allows a user to navigate a menu and perform actions based on the selection. The interface should handle keyboard inputs and update the display dynamically based on user actions. # Requirements: 1. **Initialize Curses and Create Windows:** - Use `curses.initscr()` to initialize the library. - Create at least two windows: one for displaying the menu and another for displaying the selected item\'s details. 2. **Menu Navigation:** - Implement a vertical menu that a user can navigate using the up and down arrow keys. - When an item is selected from the menu using \'Enter\', display details about the selected item in another window. - Include at least five menu items. 3. **Dynamic Updates:** - Use `curses.addstr`, `curses.refresh`, or `curses.noutrefresh` followed by `curses.doupdate` to update the display based on user inputs. 4. **Exit on Specific Key:** - Allow exiting the application gracefully when Escape key (ESC) is pressed. # Input and Output: - **Input**: User inputs via keyboard for navigating the menu and selecting items. - **Output**: Dynamic update of the terminal to show the menu and relevant details based on the selected item. # Constraints: - Consider different terminal sizes to ensure the interface adapts properly. - Ensure the application resets the terminal\'s state after exiting (use `curses.endwin()`). # Example: ```bash Menu: > Item 1 Item 2 Item 3 Item 4 Item 5 Details: Details of Item 1 # After pressing the down arrow Menu: Item 1 > Item 2 Item 3 Item 4 Item 5 Details: Details of Item 2 # After selecting an item with Enter Details: Details of Item 4 ``` # Implementation: Implement the function `main(stdscr)`: ```python import curses def main(stdscr): # Your implementation here if __name__ == \'__main__\': curses.wrapper(main) ``` # Notes: - You must handle exceptions and ensure the terminal state is restored even if an error occurs. - Make sure the display updates efficiently without flickering.","solution":"import curses def main(stdscr): # Clear screen stdscr.clear() curses.curs_set(0) # Hide the cursor # Define the menu items menu_items = [\\"Item 1\\", \\"Item 2\\", \\"Item 3\\", \\"Item 4\\", \\"Item 5\\"] current_selection = 0 # Define the windows menu_win = curses.newwin(curses.LINES - 1, curses.COLS // 2, 0, 0) detail_win = curses.newwin(curses.LINES - 1, curses.COLS // 2, 0, curses.COLS // 2) # Function to display the menu def display_menu(selected_idx): menu_win.clear() for idx, item in enumerate(menu_items): if idx == selected_idx: menu_win.attron(curses.A_REVERSE) menu_win.addstr(idx + 1, 1, item) menu_win.attroff(curses.A_REVERSE) else: menu_win.addstr(idx + 1, 1, item) menu_win.box() menu_win.refresh() # Function to display details of the selected menu item def display_details(item_idx): detail_win.clear() detail_win.addstr(1, 1, f\\"Details of {menu_items[item_idx]}\\") detail_win.box() detail_win.refresh() # Initial display display_menu(current_selection) display_details(current_selection) while True: key = stdscr.getch() if key == curses.KEY_UP: if current_selection > 0: current_selection -= 1 elif key == curses.KEY_DOWN: if current_selection < len(menu_items) - 1: current_selection += 1 elif key == ord(\'n\'): display_details(current_selection) elif key == 27: # Escape key break display_menu(current_selection) if __name__ == \'__main__\': curses.wrapper(main)"},{"question":"Coding Assessment Question # Objective Demonstrate your ability to generate synthetic data, preprocess it, train machine learning models, and diagnose issues using scikit-learn. # Question You are tasked with creating a Python script that: 1. Generates a synthetic regression dataset. 2. Splits the data into training and testing sets. 3. Applies scaling to the features. 4. Trains a `GradientBoostingRegressor` model on the training set. 5. Investigates the effect of the `n_iter_no_change` parameter on model performance and warnings. # Requirements 1. Generate a synthetic regression dataset with 1000 samples and 20 features using `sklearn.datasets.make_regression`. 2. Split the dataset into training (70%) and testing (30%) sets using `train_test_split` with `random_state=42`. 3. Scale the features using `StandardScaler`. 4. Train a `GradientBoostingRegressor` model twice: - First with the default `n_iter_no_change`. - Second with `n_iter_no_change=5`. 5. For each model, report the score on the test set and any warnings or errors encountered during training. 6. Include comments in your code to explain each step. # Inputs - None # Expected Output - Print the test set score for both models. - Print any warnings or errors encountered during training. # Constraints - You must use `scikit-learn` for model training and data operations. - Follow the structure and comments used in the examples provided. # Sample Code Structure ```python from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor import warnings # Step 1: Generate synthetic regression dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Scale the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train GradientBoostingRegressor models # First model with default n_iter_no_change model_default = GradientBoostingRegressor(random_state=42) with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") model_default.fit(X_train, y_train) default_score = model_default.score(X_test, y_test) print(f\\"Default n_iter_no_change Score: {default_score}\\") for warning in w: print(f\\"Warning with default n_iter_no_change: {warning.message}\\") # Second model with n_iter_no_change=5 model_tuned = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") model_tuned.fit(X_train, y_train) tuned_score = model_tuned.score(X_test, y_test) print(f\\"Tuned n_iter_no_change (5) Score: {tuned_score}\\") for warning in w: print(f\\"Warning with n_iter_no_change=5: {warning.message}\\") ``` This script outlines the steps to generate a dataset, preprocess it, train the model, and observe the effects of tuning the `n_iter_no_change` parameter. Make sure to fill in your code where comments are provided and ensure that you print necessary outputs.","solution":"import warnings from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor def generate_and_train_models(): # Step 1: Generate synthetic regression dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Scale the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) results = {} # Step 4: Train GradientBoostingRegressor models # First model with default n_iter_no_change model_default = GradientBoostingRegressor(random_state=42) with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") model_default.fit(X_train, y_train) default_score = model_default.score(X_test, y_test) results[\\"default_score\\"] = default_score results[\\"default_warnings\\"] = [str(warning.message) for warning in w] # Second model with n_iter_no_change=5 model_tuned = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") model_tuned.fit(X_train, y_train) tuned_score = model_tuned.score(X_test, y_test) results[\\"tuned_score\\"] = tuned_score results[\\"tuned_warnings\\"] = [str(warning.message) for warning in w] return results"},{"question":"**Question: GUI with Panel Management** You are tasked with creating a simple text-based User Interface using the `curses` and `curses.panel` libraries. The UI will consist of multiple overlapping panels that the user can interact with. The following features need to be implemented: 1. **Panel Creation and Deletion**: A user can create new panels and delete existing panels. Each new panel should be created with some default content, and when a panel is deleted, it should be removed from the panel stack. 2. **Panel Stacking Order**: Users should be able to bring a panel to the top or push it to the bottom of the stack. Moreover, users should be able to navigate through panels to bring the one above or below the current panel into view. 3. **Hide/Show Panels**: Users should be able to hide and show panels. 4. **Move Panels**: Users should be able to move panels to different coordinates on the screen. 5. **Display Update**: The screen should be updated to reflect changes whenever a panel is created, deleted, moved, hidden, or shown. Write a function `manage_panels()` that implements the following commands: - `create_panel`: Creates a new panel with some default content. - `delete_panel`: Deletes the panel that the user is currently interacting with. - `top`: Brings the current panel to the top of the stack. - `bottom`: Pushes the current panel to the bottom of the stack. - `next`: Brings the panel above the current one into view. - `prev`: Brings the panel below the current one into view. - `hide`: Hides the current panel. - `show`: Shows the currently hidden panel (does nothing if the panel is not hidden). - `move y x`: Moves the current panel to the coordinates `(y, x)` on the screen. # Function Signature ```python def manage_panels(commands: List[str]) -> None: pass ``` # Input: - `commands` : A list of strings where each string is a command as described above. # Output: - The function prints the updated screen to reflect the latest state of all panels after each command. # Constraints: - Assume a screen size of 80x24. - You must handle invalid commands gracefully, by ignoring them or printing an appropriate error message. # Example Usage: ```python commands = [ \\"create_panel\\", \\"create_panel\\", \\"top\\", \\"hide\\", \\"show\\", \\"move 5 10\\", \\"next\\", \\"delete_panel\\", \\"bottom\\" ] # Calling manage_panels(commands) would manage the panels as per the given commands ``` This is a comprehensive question that requires students to demonstrate a good understanding of the `curses.panel` module, dealing with both basic and advanced operations for panel management.","solution":"import curses import curses.panel from typing import List def manage_panels(commands: List[str]) -> None: def main(stdscr): curses.curs_set(0) stdscr.clear() panels = [] top_panel_index = 0 def create_panel(): nonlocal top_panel_index height, width = 10, 40 y, x = len(panels), len(panels) win = curses.newwin(height, width, y, x) win.box() win.addstr(1, 1, f\\"Panel {len(panels)+1}\\") panel = curses.panel.new_panel(win) panels.append(panel) top_panel() def delete_panel(): nonlocal top_panel_index if panels: panel = panels.pop(top_panel_index) panel.hide() curses.panel.update_panels() stdscr.refresh() if top_panel_index >= len(panels): top_panel_index = len(panels) - 1 def top_panel(): nonlocal top_panel_index if panels: panel = panels[top_panel_index] panel.top() stdscr.refresh() def bottom_panel(): nonlocal top_panel_index if panels: panel = panels[top_panel_index] panel.bottom() stdscr.refresh() def next_panel(): nonlocal top_panel_index if panels: top_panel_index = (top_panel_index + 1) % len(panels) top_panel() def previous_panel(): nonlocal top_panel_index if panels: top_panel_index = (top_panel_index - 1) % len(panels) top_panel() def hide_panel(): nonlocal top_panel_index if panels: panel = panels[top_panel_index] panel.hide() curses.panel.update_panels() stdscr.refresh() def show_panel(): nonlocal top_panel_index if panels: panel = panels[top_panel_index] panel.show() curses.panel.update_panels() stdscr.refresh() def move_panel(y, x): nonlocal top_panel_index if panels: panel = panels[top_panel_index] win = panel.window() win.mvwin(y, x) curses.panel.update_panels() stdscr.refresh() command_functions = { \\"create_panel\\": create_panel, \\"delete_panel\\": delete_panel, \\"top\\": top_panel, \\"bottom\\": bottom_panel, \\"next\\": next_panel, \\"prev\\": previous_panel, \\"hide\\": hide_panel, \\"show\\": show_panel, } for command in commands: parts = command.split() if parts[0] == \\"move\\" and len(parts) == 3: try: y = int(parts[1]) x = int(parts[2]) move_panel(y, x) except ValueError: pass elif parts[0] in command_functions: command_functions[parts[0]]() stdscr.getch() curses.wrapper(main)"},{"question":"Problem Statement # Implementing a Custom Container Class in Python You are required to implement a custom container class named `CustomDict` that emulates the behavior of a dictionary while enforcing certain constraints. Your class should mimic the dictionary behavior using special methods but with added functionality: 1. **Key Constraints**: Keys must always be strings. 2. **Value Constraints**: Values must always be integers. 3. **Custom String Representation**: The string representation of the class should list the keys sorted in alphabetical order. # Specifications 1. **Attributes**: - The class should use the `__slots__` mechanism to optimize memory usage. 2. **Special Methods**: - `__getitem__(self, key)`: Retrieve the value associated with the key. - `__setitem__(self, key, value)`: Assign a value to a key, ensuring keys are strings and values are integers. - `__delitem__(self, key)`: Remove the key-value pair from the container. - `__contains__(self, key)`: Check if the key is in the container. - `__len__(self)`: Return the number of key-value pairs in the container. - `__iter__(self)`: Return an iterator over the keys of the container. - `__repr__(self)`: Return a string representation of the container with keys sorted alphabetically. # Input and Output Formats - **Input**: The keys will always be strings, and the values will be integers. - **Output**: The behavior should comply with standard dictionary operations and the constraints above. # Constraints - The container should raise a `TypeError` if a non-string key or a non-integer value is used. - The container should use `__slots__` for memory efficiency. # Example Usage ```python custom_dict = CustomDict() custom_dict[\'a\'] = 1 custom_dict[\'b\'] = 2 print(custom_dict) # Expected: {\'a\': 1, \'b\': 2} custom_dict[\'c\'] = 3 print(custom_dict) # Expected: {\'a\': 1, \'b\': 2, \'c\': 3} del custom_dict[\'b\'] print(custom_dict) # Expected: {\'a\': 1, \'c\': 3} print(\'a\' in custom_dict) # Expected: True print(len(custom_dict)) # Expected: 2 for key in custom_dict: print(key) # Expected: \'a\', \'c\' (on separate lines) ``` # Implementation Implement the `CustomDict` class by adhering to the specifications provided. ```python class CustomDict: __slots__ = [\'_data\'] def __init__(self): self._data = {} def __getitem__(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") return self._data[key] def __setitem__(self, key, value): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") if not isinstance(value, int): raise TypeError(\\"Value must be an integer\\") self._data[key] = value def __delitem__(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") del self._data[key] def __contains__(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") return key in self._data def __len__(self): return len(self._data) def __iter__(self): return iter(self._data) def __repr__(self): # Return a string representation with keys sorted alphabetically sorted_items = sorted(self._data.items()) return \\"{\\" + \\", \\".join(f\\"\'{k}\': {v}\\" for k, v in sorted_items) + \\"}\\" ``` Use this class implementation as a starting point and ensure your class adheres to all specified constraints and functionalities.","solution":"class CustomDict: __slots__ = [\'_data\'] def __init__(self): self._data = {} def __getitem__(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") return self._data[key] def __setitem__(self, key, value): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") if not isinstance(value, int): raise TypeError(\\"Value must be an integer\\") self._data[key] = value def __delitem__(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") del self._data[key] def __contains__(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string\\") return key in self._data def __len__(self): return len(self._data) def __iter__(self): return iter(self._data) def __repr__(self): # Return a string representation with keys sorted alphabetically sorted_items = sorted(self._data.items()) return \\"{\\" + \\", \\".join(f\\"\'{k}\': {v}\\" for k, v in sorted_items) + \\"}\\""},{"question":"Objective Demonstrate your understanding of `sklearn.datasets` by performing the following tasks. Task 1. **Load and Fetch Datasets**: - Load the Iris dataset using the appropriate dataset loader method. - Fetch the `20newsgroups` dataset using the appropriate dataset fetcher method. 2. **Manipulate the Data**: - Extract the data and target attributes from both the Iris and `20newsgroups` datasets. - For the Iris dataset, split it into training and testing sets using an 80-20 split. - For the `20newsgroups` dataset, summarize the number of articles per category. 3. **Generate Synthetic Data**: - Generate a synthetic classification dataset with 100 samples, 5 features, and 2 classes using the appropriate method from `sklearn.datasets`. 4. **Implement a Simple Classifier**: - Using the training set of the Iris dataset, train a simple logistic regression model. - Evaluate the model on the testing set and report the accuracy. Constraints and Requirements - You are required to use `sklearn` for all dataset operations. - Use only `numpy` and `pandas` for any other data manipulation tasks. - You should not use any additional machine learning libraries for implementing the classifier. Input and Output Formats - There are no inputs to your functions other than calling the datasets functions. - Your code should return and print the following: - The shape of the Iris dataset (both data and target). - The summary of the number of articles per category in the `20newsgroups` dataset. - The shape of the generated synthetic data and its target. - The accuracy of the logistic regression model on the Iris dataset. Example Output ```plaintext Iris Data Shape: (150, 4) Iris Target Shape: (150,) 20newsgroups Category Summary: { \'alt.atheism\': 100, \'comp.graphics\': 100, ... } Synthetic Data Shape: (100, 5) Synthetic Target Shape: (100,) Logistic Regression Accuracy: 0.95 ``` Implementation ```python from sklearn.datasets import load_iris, fetch_20newsgroups, make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from collections import Counter def dataset_operations(): # Load the Iris dataset iris = load_iris() X_iris, y_iris = iris.data, iris.target # Print the shape of the Iris data and target print(\\"Iris Data Shape:\\", X_iris.shape) print(\\"Iris Target Shape:\\", y_iris.shape) # Fetch the 20newsgroups dataset newsgroups = fetch_20newsgroups(subset=\'all\') newsgroups_target_summary = Counter(newsgroups.target) # Print the summary of the number of articles per category print(\\"20newsgroups Category Summary:\\", newsgroups_target_summary) # Generate synthetic data X_synthetic, y_synthetic = make_classification(n_samples=100, n_features=5, n_classes=2) # Print the shape of the synthetic data and target print(\\"Synthetic Data Shape:\\", X_synthetic.shape) print(\\"Synthetic Target Shape:\\", y_synthetic.shape) # Split the Iris dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42) # Train a Logistic Regression model model = LogisticRegression() model.fit(X_train, y_train) # Evaluate the model on the testing set accuracy = model.score(X_test, y_test) # Print the accuracy of the logistic regression model print(\\"Logistic Regression Accuracy:\\", accuracy) dataset_operations() ```","solution":"from sklearn.datasets import load_iris, fetch_20newsgroups, make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from collections import Counter def dataset_operations(): # Load the Iris dataset iris = load_iris() X_iris, y_iris = iris.data, iris.target # Print the shape of the Iris data and target print(\\"Iris Data Shape:\\", X_iris.shape) print(\\"Iris Target Shape:\\", y_iris.shape) # Fetch the 20newsgroups dataset newsgroups = fetch_20newsgroups(subset=\'all\') newsgroups_target_summary = Counter(newsgroups.target) # Print the summary of the number of articles per category print(\\"20newsgroups Category Summary:\\", dict(newsgroups_target_summary)) # Generate synthetic data X_synthetic, y_synthetic = make_classification(n_samples=100, n_features=5, n_classes=2) # Print the shape of the synthetic data and target print(\\"Synthetic Data Shape:\\", X_synthetic.shape) print(\\"Synthetic Target Shape:\\", y_synthetic.shape) # Split the Iris dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42) # Train a Logistic Regression model model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # Evaluate the model on the testing set accuracy = model.score(X_test, y_test) # Print the accuracy of the logistic regression model print(\\"Logistic Regression Accuracy:\\", accuracy) if __name__ == \\"__main__\\": dataset_operations()"},{"question":"**Custom Exception Handling and Chaining** You are to implement a function `process_items(items: list)` that processes a list of items. Each item could cause one of the following typical exceptions as described below: - `TypeError`: If the item is not an integer. - `ValueError`: If the item is an integer but is less than 0 or greater than 100. - `ZeroDivisionError`: If the item is 0. Your task is to handle these exceptions by raising a custom exception `ProcessingError`, which should inherit from the base `Exception` class. The `ProcessingError` should store the original exception that caused the processing to fail and provide proper messages. For each item in the `items` list: 1. If the item is not an integer, raise a `TypeError`. 2. If the item is an integer but out of the range [0, 100], raise a `ValueError`. 3. If the item is 0, raise a `ZeroDivisionError`. 4. If none of these exceptions occur, print that the item is valid by outputting `\\"Item {item} is valid.\\"`. In all of the above cases, the raised exception should be caught and re-raised as a `ProcessingError`. You should capture the context of the original exception using chaining. # Function Signature ```python class ProcessingError(Exception): def __init__(self, message, original_exception): self.message = message self.original_exception = original_exception super().__init__(self.message) def process_items(items: list): pass ``` # Example ```python try: process_items([10, \\"a\\", -5, 0, 101]) except ProcessingError as e: print(f\\"Processing Error: {e.message}\\") if e.original_exception: print(f\\"Original Exception: {e.original_exception}\\") ``` # Expected Output ```plaintext Item 10 is valid. Processing Error: Failed to process item. Original Exception: TypeError: Item a is not an integer. Processing Error: Failed to process item. Original Exception: ValueError: Item -5 is out of range. Processing Error: Failed to process item. Original Exception: ZeroDivisionError: Division by zero encountered. Processing Error: Failed to process item. Original Exception: ValueError: Item 101 is out of range. ``` # Constraints 1. You must use exception chaining features (`__context__`, `__cause__`) to link the original exceptions with the custom `ProcessingError`. 2. Ensure comprehensive handling of all specified exceptions.","solution":"class ProcessingError(Exception): def __init__(self, message, original_exception): self.message = message self.original_exception = original_exception super().__init__(self.message) def process_items(items: list): for item in items: try: if not isinstance(item, int): raise TypeError(f\\"Item {item} is not an integer.\\") if item < 0 or item > 100: raise ValueError(f\\"Item {item} is out of range.\\") if item == 0: raise ZeroDivisionError(\\"Division by zero encountered.\\") print(f\\"Item {item} is valid.\\") except (TypeError, ValueError, ZeroDivisionError) as e: raise ProcessingError(\\"Failed to process item.\\", e) from e"},{"question":"**Objective:** Implement a custom iterator class in Python that can demonstrate the use of both sequence-based and callable-based iteration as described in the documentation provided. **Problem Statement:** You are required to implement a custom iterator class `CustomIterator` that can iterate over: 1. A given sequence (list, tuple, etc.). 2. A callable that generates items until a sentinel value is returned. The class should support the following operations: - Initialization with either a sequence or a callable and sentinel. - Iteration using the `__iter__` and `__next__` methods. **Function Specifications:** 1. **`CustomIterator` Class:** - `__init__(self, iterable_or_callable, sentinel=None)`: - Initializes the iterator. - If `sentinel` is provided, treat `iterable_or_callable` as a callable and iterate until `sentinel` is returned. - If `sentinel` is `None`, treat `iterable_or_callable` as a sequence. - `__iter__(self)`: - Returns the iterator object itself. - `__next__(self)`: - Returns the next item in the iteration. - If iteration is complete, raises `StopIteration`. **Example Usage:** ```python # Sequence-based iteration sequence_iter = CustomIterator([1, 2, 3, 4]) for item in sequence_iter: print(item) # Callable-based iteration def generate_numbers(): import random return random.randint(1, 10) callable_iter = CustomIterator(generate_numbers, 7) for item in callable_iter: print(item) ``` **Expected Output:** ```python # For Sequence-based iteration 1 2 3 4 # For Callable-based iteration (output will vary due to randomness; iteration stops when 7 is generated) 8 3 7 ``` # Constraints: - Do not use external libraries other than built-in Python functionalities. - Ensure that the iterator handles any sequence with `__getitem__` correctly. - Ensure that the callable-based iteration stops when the sentinel value is encountered. **Note:** - You can assume that the sequence or callable provided will be valid and non-empty. - Handle the edge cases such as an empty sequence or callable consistently returning the sentinel in your implementation. Implement your `CustomIterator` class below: ```python class CustomIterator: def __init__(self, iterable_or_callable, sentinel=None): # Your implementation here pass def __iter__(self): # Your implementation here pass def __next__(self): # Your implementation here pass # Example test cases to validate your implementation if __name__ == \\"__main__\\": # Test sequence-based iteration sequence_iter = CustomIterator([1, 2, 3, 4]) for item in sequence_iter: print(item) # Test callable-based iteration def generate_numbers(): import random return random.randint(1, 10) callable_iter = CustomIterator(generate_numbers, 7) for item in callable_iter: print(item) ```","solution":"class CustomIterator: def __init__(self, iterable_or_callable, sentinel=None): self.is_callable = callable(iterable_or_callable) if self.is_callable: self.callable = iterable_or_callable self.sentinel = sentinel self._prepare_callable_iteration() else: self.sequence = iter(iterable_or_callable) def _prepare_callable_iteration(self): # The internal generator for callable based iteration def generator(): while True: result = self.callable() if result == self.sentinel: break yield result self.iterator = generator() def __iter__(self): return self def __next__(self): if self.is_callable: return next(self.iterator) else: return next(self.sequence)"},{"question":"Objective Create a detailed visualization of a dataset using `seaborn` and `matplotlib`. The goal is to produce a comprehensive and customized plot, complete with annotations and additional elements, demonstrating advanced usage of both libraries. Problem Statement You are provided with the `diamonds` dataset from Seaborn. Your task is to create a plot that visualizes the relationship between the `carat` and `price` of diamonds, grouped by `cut` quality. Additionally, integrate advanced customization by adding annotations and graphical elements using Matplotlib. Requirements: 1. **Plot Structure**: - Use `seaborn.objects.Plot` to create the main plot. - Display the relationship between `carat` (x-axis) and `price` (y-axis) using `so.Dots()`. - Group the data by `cut` and use facets to create separate subplots for each `cut` quality. 2. **Customization**: - Add a custom rectangle annotation on the plot. - Include a text annotation inside the custom rectangle. - Ensure each subplot is consistent in style, using a pre-defined theme. - Create and use `matplotlib` subfigures to organize the plots neatly. Input - None. (Assume the `diamonds` dataset is loaded and available). Output - A visual plot adhering to the requirements above. Constraints - Use `seaborn.objects.Plot` for creating the plots. - Apply `matplotlib` customizations judiciously to enhance the plots. - Each subplot must represent diamonds grouped by `cut`. Example Code Structure: ```python import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt import matplotlib as mpl from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create main plot object p = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()).facet(row=\\"cut\\") # Create figure and subfigures f = mpl.figure.Figure(figsize=(10, 10), dpi=100, layout=\\"constrained\\") sf = f.subfigures(1, 1) # Adjust as needed for your facet plots # Apply the plot to the subfigure p.on(sf[0]).plot() # Customize the plot with annotations (Example for one of the facets) ax = sf[0].axes[0] rect = mpl.patches.Rectangle( xy=(0, 1), width=.4, height=.1, color=\\"C1\\", alpha=.2, transform=ax.transAxes, clip_on=False, ) ax.add_artist(rect) ax.text( x=rect.get_width() / 2, y=1 + rect.get_height() / 2, s=\\"Diamond Data\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax.transAxes, ) # Display the final composed plot plt.show() ``` Ensure that your final code produces a well-organized and annotated visual representation of the diamonds dataset, highlighting the relationship between `carat` and `price` across different `cut` qualities.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt import matplotlib.patches as patches from seaborn import load_dataset def create_diamond_plot(): Creates a comprehensive plot visualizing the relationship between \'carat\' and \'price\' of diamonds, grouped by \'cut\', with custom annotations using Seaborn and Matplotlib. # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the main plot object p = ( so.Plot(diamonds, \\"carat\\", \\"price\\") .facet(\\"cut\\") .add(so.Dots()) ) # Create the figure and subfigures using matplotlib fig, axs = plt.subplots(5, 1, figsize=(10, 18), constrained_layout=True) # Plot each facet into one subfigure for ax, (cut, grp) in zip(axs, diamonds.groupby(\'cut\')): sub_plot = ( so.Plot(grp, \\"carat\\", \\"price\\") .add(so.Dots()) ) sub_plot.on(ax).plot() # Custom rectangle annotation for each subplot rect = patches.Rectangle( xy=(1.2, 10000), width=0.4, height=4000, linewidth=1, edgecolor=\'black\', facecolor=\'none\' ) ax.add_patch(rect) # Text annotation within the rectangle ax.text( 1.4, 11000, \'AnnotatenHere\', verticalalignment=\'center\', horizontalalignment=\'center\', color=\'black\', fontsize=10, bbox=dict(facecolor=\'white\', edgecolor=\'none\', alpha=0.5) ) # Set global title plt.suptitle(\'Diamond Carat vs Price by Cut\', fontsize=16, weight=\'bold\') plt.show() # Call the function to create the plot create_diamond_plot()"},{"question":"# Problem: Asynchronous Server-Client Communication using asyncio You are required to implement an asynchronous server-client communication system using Python\'s `asyncio` package. This task will demonstrate your understanding of event loops, asynchronous tasks, and networking with asyncio. Requirements: 1. **Asynchronous Server**: - The server should accept incoming TCP connections. - Upon receiving a connection, it should handle communication asynchronously. - The server should be able to receive messages from clients, process them (e.g., by reversing the string), and send a response back to the client. - The server must handle multiple client connections concurrently. 2. **Asynchronous Client**: - The client should connect to the server asynchronously. - It should send a message to the server. - It should receive the response from the server and print it. Implementation Details: - Utilize `asyncio.create_server()` for creating the server. - Use `asyncio.open_connection()` for the client connection to the server. - Ensure clean closing of the connection in both server and client. Input and Output: - There are no specific inputs provided. You will simulate sending messages from clients to the server. - The server processes each message by reversing the string and returns it to the client. - The client prints the received response which is the reversed string. Example: If a client sends the message `Hello Server`, the server should send back `revreS olleH`, and the client should print `revreS olleH`. Constraints: - The communication between server and client should be handled asynchronously. - Handle possible exceptions (e.g., connection errors) gracefully. Performance: - Ensure that the server can handle multiple concurrent connections efficiently. # Template: ```python import asyncio # Server-side code async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") response = message[::-1] print(f\\"Send {response!r} to {addr!r}\\") writer.write(response.encode()) await writer.drain() print(\\"Close the connection\\") writer.close() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() # Client-side code async def tcp_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message!r}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()!r}\') print(\'Close the connection\') writer.close() await writer.wait_closed() # Running the examples def run_example(): server_loop = asyncio.get_event_loop() client_loop = asyncio.new_event_loop() try: server_loop.create_task(main()) server_loop.call_later(1, client_loop.run_until_complete, tcp_client(\'Hello Server\')) server_loop.run_forever() except KeyboardInterrupt: server_loop.stop() client_loop.stop() finally: server_loop.close() client_loop.close() if __name__ == \'__main__\': run_example() ``` # Hints: 1. Use `asyncio` module to manage asynchronous tasks and event loops. 2. Use `asyncio.start_server()` method to start the server. 3. Use `asyncio.open_connection()` method for the client to establish a connection to the server. 4. Ensure that both server and client handle the connection lifecycle correctly, using `writer.close()` and `await writer.wait_closed()`.","solution":"import asyncio # Server-side code async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message!r} from {addr!r}\\") response = message[::-1] print(f\\"Send {response!r} to {addr!r}\\") writer.write(response.encode()) await writer.drain() print(\\"Close the connection\\") writer.close() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() # Client-side code async def tcp_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message!r}\') writer.write(message.encode()) data = await reader.read(100) response = data.decode() print(f\'Received: {response!r}\') print(\'Close the connection\') writer.close() await writer.wait_closed() return response # Running the server def run_server(): loop = asyncio.get_event_loop() loop.run_until_complete(main()) # Running the client def run_client(message): return asyncio.run(tcp_client(message))"},{"question":"# Bytearray Operations Challenge You are required to implement a series of functions using Python bytearray operations. Each function should perform specific tasks as described below: 1. `is_bytearray(obj) -> bool` - **Input:** An object `obj` - **Output:** Return `True` if the object is a bytearray, else `False`. 2. `create_bytearray_from_string(s: str) -> bytearray` - **Input:** A string `s` - **Output:** Return a new bytearray object initialized with the bytes of the string. 3. `concat_bytearrays(a: bytearray, b: bytearray) -> bytearray` - **Input:** Two bytearray objects `a` and `b` - **Output:** Return a new bytearray resulting from concatenating `a` and `b`. 4. `get_bytearray_size(b: bytearray) -> int` - **Input:** A bytearray object `b` - **Output:** Return the size of the bytearray. 5. `bytearray_to_string(b: bytearray) -> str` - **Input:** A bytearray object `b` - **Output:** Return the contents of the bytearray as a string. 6. `resize_bytearray(b: bytearray, new_size: int) -> None` - **Input:** A bytearray object `b` and an integer `new_size` - **Output:** Resize the bytearray `b` to the new size `new_size`. Constraints: - You should handle possible edge cases, such as empty bytearrays or invalid inputs. - For performance considerations, ensure that the concatenation handles large bytearrays efficiently. Example ```python obj = bytearray(b\\"example\\") print(is_bytearray(obj)) # True print(create_bytearray_from_string(\\"hello\\")) # bytearray(b\'hello\') a = bytearray(b\\"hello\\") b = bytearray(b\\" world\\") print(concat_bytearrays(a, b)) # bytearray(b\'hello world\') print(get_bytearray_size(a)) # 5 print(bytearray_to_string(b)) # \' world\' resize_bytearray(a, 3) print(a) # bytearray(b\'hel\') ``` Implement the functions as described to complete this challenge.","solution":"def is_bytearray(obj) -> bool: Returns True if the object is a bytearray, else False. return isinstance(obj, bytearray) def create_bytearray_from_string(s: str) -> bytearray: Returns a new bytearray object initialized with the bytes of the string. return bytearray(s, \'utf-8\') def concat_bytearrays(a: bytearray, b: bytearray) -> bytearray: Returns a new bytearray resulting from concatenating a and b. return a + b def get_bytearray_size(b: bytearray) -> int: Returns the size of the bytearray. return len(b) def bytearray_to_string(b: bytearray) -> str: Return the contents of the bytearray as a string. return b.decode(\'utf-8\') def resize_bytearray(b: bytearray, new_size: int) -> None: Resize the bytearray b to the new size new_size. if new_size < len(b): del b[new_size:] else: b.extend([0] * (new_size - len(b)))"},{"question":"Design a Python class `AdvancedType` that mimics a simple subset of functionality described by `PyTypeObject`. The class should encompass memory management, attribute access, and basic string representation. This class should simulate behaviors akin to object creation, attribute access, and custom method definitions but in Python. # Question You are required to design a class `AdvancedType` in Python that includes the following functionalities: 1. **Initialization and Attribute Management:** - The class should initialize with a name and a dictionary of attributes. - Provide methods to get, set, and delete attributes (mimicking `tp_getattro` and `tp_setattro`). 2. **String Representations:** - Implement `__repr__` to return a string `<AdvancedType object at [memory_address]>` similar to `tp_repr`. - Implement `__str__` to return a friendly string representation of the attributes. 3. **Custom Method Definitions:** - Allow methods to be registered dynamically, allowing `AdvancedType` instances to act as callables with their custom behavior. 4. **Memory Management Simulation:** - Include a method to simulate object deallocation (similar to `tp_dealloc`). # Implementation Specifications 1. **Class Definition:** - `__init__(self, name: str, attributes: dict)`: Initialize with a name and a dictionary of attributes. - `__repr__(self) -> str`: Return a string representing the object. - `__str__(self) -> str`: Return a string with attribute representations. - `get_attr(self, attr: str) -> any`: Get the value of an attribute. - `set_attr(self, attr: str, value: any) -> None`: Set the value of an attribute. - `del_attr(self, attr: str) -> None`: Delete an attribute. - `del_obj(self) -> None`: Simulate deallocating the object. - `register_method(self, method_name: str, method_func: callable) -> None`: Register a method dynamically. 2. **Constraints:** - Attribute names must be strings. - Methods should be callable with the object\'s instance as the first argument. # Example Usage ```python class AdvancedType: # Your implementation here # Example of AdvancedType usage obj = AdvancedType(\\"MyType\\", {\\"attr1\\": 10, \\"attr2\\": 20}) print(repr(obj)) # <AdvancedType object at [unique_memory_address]> print(str(obj)) # MyType with attributes: {\'attr1\': 10, \'attr2\': 20} obj.set_attr(\\"attr3\\", 30) print(obj.get_attr(\\"attr3\\")) # 30 obj.del_attr(\\"attr2\\") def custom_method(self, add_value): return self.get_attr(\\"attr1\\") + add_value obj.register_method(\\"add_to_attr1\\", custom_method) print(obj.add_to_attr1(5)) # 15 obj.del_obj() # Simulate object deallocation ``` Implement the `AdvancedType` class following the above specifications.","solution":"class AdvancedType: def __init__(self, name, attributes): Initialize the AdvancedType object with a name and a dictionary of attributes. self.name = name self.attributes = attributes self.methods = {} def __repr__(self): Return a string representing the object with its memory address. return f\\"<AdvancedType object at {hex(id(self))}>\\" def __str__(self): Return a friendly string representation of the object with its attributes. return f\\"{self.name} with attributes: {self.attributes}\\" def get_attr(self, attr): Get the value of an attribute. if attr in self.attributes: return self.attributes[attr] else: raise AttributeError(f\\"\'{self.name}\' object has no attribute \'{attr}\'\\") def set_attr(self, attr, value): Set the value of an attribute. self.attributes[attr] = value def del_attr(self, attr): Delete an attribute. if attr in self.attributes: del self.attributes[attr] else: raise AttributeError(f\\"\'{self.name}\' object has no attribute \'{attr}\'\\") def del_obj(self): Simulate deallocating the object. print(f\\"{self.name} object deallocated\\") def register_method(self, method_name, method_func): Register a method dynamically. import types self.methods[method_name] = types.MethodType(method_func, self) def __getattr__(self, attr): Allow the object to call registered methods. if attr in self.methods: return self.methods[attr] else: raise AttributeError(f\\"\'{self.name}\' object has no attribute \'{attr}\'\\")"},{"question":"# Time Zone Converter **Objective:** The goal of this exercise is to create a function that converts a given time in UTC to a specified local time zone and returns it in a human-readable format. **Task:** Write a Python function `convert_to_timezone(utc_time_str, timezone_str)` that: 1. Takes two input parameters: - `utc_time_str`: A string representing the UTC time in the format `\\"%Y-%m-%d %H:%M:%S\\"` (e.g., `\\"2023-10-10 15:30:45\\"`). - `timezone_str`: A string representing the target time zone (e.g., `\'America/New_York\'`, `\'Europe/London\'`). 2. Converts the given UTC time to the specified local time zone. 3. Returns the converted time as a string in the format `\\"%Y-%m-%d %H:%M:%S %Z\\"` (e.g., `\\"2023-10-10 11:30:45 EDT\\"`). **Constraints:** - Use the `time` module functions to perform the conversions. - Handle cases where the provided time zones might not be valid. - Raise appropriate exceptions in case of invalid input parameters. - Assume the environment supports the provided time zones. **Example:** ```python print(convert_to_timezone(\\"2023-10-10 15:30:45\\", \\"America/New_York\\")) # Output: \\"2023-10-10 11:30:45 EDT\\" print(convert_to_timezone(\\"2023-10-10 15:30:45\\", \\"Europe/London\\")) # Output: \\"2023-10-10 16:30:45 BST\\" ``` **Note:** You should ensure that your function accounts for Daylight Saving Time adjustments and other timezone-specific rules. You may need to refer to external libraries for comprehensive timezone data (such as `pytz`) but the focus should be on demonstrating proficiency with the `time` module.","solution":"from datetime import datetime import pytz def convert_to_timezone(utc_time_str, timezone_str): Convert UTC time to a specified timezone and return in a human-readable format. Args: utc_time_str (str): UTC time in the format \\"%Y-%m-%d %H:%M:%S\\". timezone_str (str): Target timezone. Returns: str: Local time in the format \\"%Y-%m-%d %H:%M:%S %Z\\". Raises: ValueError: If the timezone is not valid. try: # Parse the UTC datetime string utc_time = datetime.strptime(utc_time_str, \\"%Y-%m-%d %H:%M:%S\\") # Set the timezone to UTC utc_time = utc_time.replace(tzinfo=pytz.UTC) # Get the target timezone target_timezone = pytz.timezone(timezone_str) # Convert UTC time to the target timezone local_time = utc_time.astimezone(target_timezone) # Format the local time as specified local_time_str = local_time.strftime(\\"%Y-%m-%d %H:%M:%S %Z\\") return local_time_str except pytz.UnknownTimeZoneError: raise ValueError(f\\"Invalid timezone: {timezone_str}\\") except Exception as e: raise ValueError(f\\"An error occurred: {str(e)}\\")"},{"question":"# Question You are tasked with implementing and testing a small library for managing a collection of books using Python\'s `unittest` framework. Below is an outline of the requirements and tests you must implement. **Library Requirements:** 1. **Book Class**: - Attributes: - `title` (string) - `author` (string) - `year` (integer) - Methods: - `__init__(self, title, author, year)`: Initializes the book with a title, author, and publication year. 2. **Library Class**: - Attributes: - `books` (list): A list of `Book` objects. - Methods: - `__init__(self)`: Initializes the library with an empty book list. - `add_book(self, book)`: Adds a `Book` object to the library. - `remove_book(self, title)`: Removes a book with the given title from the library. - `find_books_by_author(self, author)`: Returns a list of books by the given author. - `find_books_by_year(self, year)`: Returns a list of books published in the given year. **Tests to Implement**: 1. Create a `TestBookClass`: - Test the `Book` class initialization. 2. Create a `TestLibraryClass`: - Test the `Library` class initialization. - Test the `add_book` method. - Test the `remove_book` method when the book is present. - Test that `remove_book` raises a `ValueError` when the book is not present. - Test the `find_books_by_author` method. - Test the `find_books_by_year` method. **Constraints**: - Use `unittest.TestCase` and relevant `setUp`, `tearDown`, and assertions like `assertEqual`, `assertTrue`, `assertRaises`, etc. - Your tests should be comprehensive and handle edge cases. **Implementation**: ```python import unittest class Book: def __init__(self, title, author, year): # Your code here class Library: def __init__(self): # Your code here def add_book(self, book): # Your code here def remove_book(self, title): # Your code here def find_books_by_author(self, author): # Your code here def find_books_by_year(self, year): # Your code here class TestBookClass(unittest.TestCase): # Your test methods for Book class class TestLibraryClass(unittest.TestCase): def setUp(self): # Setup code for initializing Library and Book instances def tearDown(self): # Optional cleanup code # Your test methods for Library class if __name__ == \'__main__\': unittest.main() ``` Complete the implementation of the `Book` and `Library` classes, and write the associated tests in the `TestBookClass` and `TestLibraryClass` classes. Make sure your tests are exhaustive and cover all aspects of the specified library functionality.","solution":"class Book: def __init__(self, title, author, year): self.title = title self.author = author self.year = year class Library: def __init__(self): self.books = [] def add_book(self, book): self.books.append(book) def remove_book(self, title): for book in self.books: if book.title == title: self.books.remove(book) return book raise ValueError(\\"Book not found\\") def find_books_by_author(self, author): return [book for book in self.books if book.author == author] def find_books_by_year(self, year): return [book for book in self.books if book.year == year]"},{"question":"Objective: You are provided with a problem where a machine learning model raises a certain warning when a specific parameter is used. Your task is to create synthetic data, fit a model to this data, identify the cause of the warning, and propose a solution to resolve it. Task: 1. Create a synthetic dataset suitable for regression using numpy. 2. Split the data into training and testing sets using `train_test_split`. 3. Preprocess the data using `StandardScaler` from scikit-learn (ensure feature scaling is appropriate). 4. Train a `GradientBoostingRegressor` model on the dataset without any warnings. 5. Change the parameter `n_iter_no_change` to 5 and identify the warning raised during model fitting. 6. Propose and implement a solution to resolve this warning. Specifications: - **Input**: No direct input is required. You will create synthetic data within the script. - **Output**: Print the model score on the test set before and after changing the `n_iter_no_change` parameter. If a warning is raised, print the warning message and your approach to resolving it. Constraints: 1. Use `numpy` to create the synthetic dataset. 2. Ensure the synthetic dataset has at least 100 samples and 5 features. 3. Ensure reproducibility by setting a random seed for synthetic data generation. Expected Performance: - The synthetic data creation, preprocessing steps, and model training should be efficient and handle the given dataset size seamlessly. - The solution to the raised warning should be explained and justified clearly. ```python import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor import warnings # Step 1: Create synthetic data np.random.seed(42) n_samples = 100 n_features = 5 X = np.random.randn(n_samples, n_features) y = np.random.randn(n_samples) # Step 2: Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Step 3: Preprocess the data scaler = StandardScaler(with_mean=False) X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train model without warnings gbdt = GradientBoostingRegressor(random_state=0) gbdt.fit(X_train, y_train) default_score = gbdt.score(X_test, y_test) print(f\\"Model score with default parameters: {default_score}\\") # Step 5: Change parameter `n_iter_no_change` to 5 and identify warning with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") gbdt = GradientBoostingRegressor(random_state=0, n_iter_no_change=5) gbdt.fit(X_train, y_train) modified_score = gbdt.score(X_test, y_test) print(f\\"Model score with n_iter_no_change=5: {modified_score}\\") if w: print(f\\"Warning raised: {w[-1].message}\\") # Step 6: Propose and implement a solution (this is for you to complete) # <Your solution here> # If no warnings were raised in your solution, print \\"No warnings after solution.\\" ``` **Submission:** 1. Ensure your code runs without errors and fits the specifications. 2. Explain any warnings and provide reasoning/solution for resolving them. 3. Submit your final code along with the explanations.","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor import warnings # Step 1: Create synthetic data np.random.seed(42) n_samples = 100 n_features = 5 X = np.random.randn(n_samples, n_features) y = np.random.randn(n_samples) # Step 2: Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Step 3: Preprocess the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train model without warnings gbdt = GradientBoostingRegressor(random_state=0) gbdt.fit(X_train, y_train) default_score = gbdt.score(X_test, y_test) print(f\\"Model score with default parameters: {default_score}\\") # Step 5: Change parameter `n_iter_no_change` to 5 and identify warning with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") gbdt = GradientBoostingRegressor(random_state=0, n_iter_no_change=5) gbdt.fit(X_train, y_train) modified_score = gbdt.score(X_test, y_test) print(f\\"Model score with n_iter_no_change=5: {modified_score}\\") if w: print(f\\"Warning raised: {w[-1].message}\\") # Step 6: Propose and implement a solution # Solution: Reduce the value of n_iter_no_change to a smaller number # if possibly overfitting the data. If the issue persists, increase n_iter_no_change. # Implementing solution: Setting n_iter_no_change to a more balanced number e.g., 10 gbdt = GradientBoostingRegressor(random_state=0, n_iter_no_change=10) gbdt.fit(X_train, y_train) final_score = gbdt.score(X_test, y_test) print(f\\"Model score with n_iter_no_change=10: {final_score}\\")"},{"question":"Advanced Coding Assessment Question **Objective:** Write a function that uses the Tkinter `messagebox` module to create a series of interactive dialogs based on user inputs and responses. This function should demonstrate a thorough understanding of the `tkinter.messagebox` functions and Python programming concepts. **Problem Statement:** Write a function `user_interaction_dialog()` that creates a series of message boxes to interact with the user. The function should perform the following steps: 1. Display an informational message box with the title \\"Welcome\\" and the message \\"Welcome to the user interaction program!\\". 2. Display a warning message box asking the user \\"This action can modify your files. Do you want to continue?\\" and return `False` if the user selects \'No\'. 3. If the user selects \'Yes\', display an askokcancel message box with the title \\"Confirmation\\" and the message \\"Are you sure you want to proceed with file modification?\\" and return `False` if the user selects \'Cancel\'. 4. If the user selects \'OK\', display an askretrycancel message box with the title \\"Retry Action\\" and the message \\"File modification failed. Do you want to try again?\\". 5. If the user selects \'Retry\', return `True`. 6. If the user selects \'Cancel\', return `False`. **Function Signature:** ```python def user_interaction_dialog() -> bool: pass ``` **Constraints:** - Ensure that the function is interactive and waits for user input at each step. - Use the appropriate `tkinter.messagebox` function for each dialog type. - Handle the user\'s responses correctly and return the appropriate boolean value based on the interactions. **Example Interaction Flow:** 1. Display: \\"Welcome to the user interaction program!\\" -> User clicks \'OK\'. 2. Display: \\"This action can modify your files. Do you want to continue?\\" -> User clicks \'Yes\'. 3. Display: \\"Are you sure you want to proceed with file modification?\\" -> User clicks \'OK\'. 4. Display: \\"File modification failed. Do you want to try again?\\" -> User clicks \'Retry\'. 5. Return: `True`. Also handle cases where the user selects \'No\' at any prompt to return `False`. **Performance Requirements:** - The dialog interactions should be processed sequentially based on user inputs. - Ensure no unnecessary delays in displaying message boxes. **Hint:** Use the `tkinter.messagebox` functions as described in the provided documentation to implement each dialog step.","solution":"import tkinter as tk from tkinter import messagebox def user_interaction_dialog() -> bool: Create a series of message boxes to interact with the user and return a result based on user responses. # Initialize the main window (it will not be shown) root = tk.Tk() root.withdraw() # Hide the main window # Step 1: Display an informational message box messagebox.showinfo(title=\\"Welcome\\", message=\\"Welcome to the user interaction program!\\") # Step 2: Display a warning message asking if the user wants to continue proceed = messagebox.askyesno(title=\\"Warning\\", message=\\"This action can modify your files. Do you want to continue?\\") if not proceed: return False # Step 3: Ask for confirmation to proceed with file modification confirm = messagebox.askokcancel(title=\\"Confirmation\\", message=\\"Are you sure you want to proceed with file modification?\\") if not confirm: return False # Step 4: Ask the user if they want to retry file modification retry = messagebox.askretrycancel(title=\\"Retry Action\\", message=\\"File modification failed. Do you want to try again?\\") return retry # This function is interactive and cannot be automatically unit tested in a traditional way. Manual testing may be required."},{"question":"**Coding Assessment Question** **Objective:** Demonstrate your ability to read, process, and write audio data using Python\'s `wave` module. **Description:** You are tasked with writing a Python function that takes an input WAV file, reduces its volume by half, and saves the resulting WAV file to a specified output path. **Function Signature:** ```python def reduce_volume(input_file: str, output_file: str) -> None: pass ``` **Parameters:** - `input_file` (str): The path to the input WAV file. - `output_file` (str): The path where the output WAV file should be saved. **Constraints:** - You must use Python\'s `wave` module to read and write WAV file data. - You should handle both mono and stereo audio files. - Ensure proper handling of different byte widths (e.g., 8-bit, 16-bit, etc.). **Example:** ```python reduce_volume(\'input.wav\', \'output.wav\') ``` After executing the above function, the `output.wav` file should contain the same audio as `input.wav` but with the volume reduced by half. **Additional Information:** To read and write WAV files, you may find the following methods from the `wave` module useful: - `wave.open()` - `Wave_read.getnchannels()` - `Wave_read.getsampwidth()` - `Wave_read.getframerate()` - `Wave_read.readframes()` - `Wave_write.setnchannels()` - `Wave_write.setsampwidth()` - `Wave_write.setframerate()` - `Wave_write.writeframes()` **Performance Requirements:** The function should process files of up to 10 minutes in length within a reasonable time frame (under 1 minute of processing time on an average machine). **Hint:** Consider how PCM (Pulse Code Modulation) data is stored in WAV files. Each audio sample\'s volume can be reduced by modifying its amplitude value appropriately.","solution":"import wave import struct def reduce_volume(input_file: str, output_file: str) -> None: with wave.open(input_file, \'rb\') as wav_in: params = wav_in.getparams() nchannels, sampwidth, framerate, nframes = params[:4] frames = wav_in.readframes(nframes) if sampwidth == 1: # 8-bit fmt = \'{}B\'.format(nframes * nchannels) samples = struct.unpack(fmt, frames) new_samples = [int(sample / 2) for sample in samples] frames = struct.pack(fmt, *new_samples) elif sampwidth == 2: # 16-bit fmt = \'{}h\'.format(nframes * nchannels) samples = struct.unpack(fmt, frames) new_samples = [int(sample / 2) for sample in samples] frames = struct.pack(fmt, *new_samples) elif sampwidth == 4: # 32-bit fmt = \'{}i\'.format(nframes * nchannels) samples = struct.unpack(fmt, frames) new_samples = [int(sample / 2) for sample in samples] frames = struct.pack(fmt, *new_samples) else: raise ValueError(f\\"Unsupported sample width: {sampwidth}\\") with wave.open(output_file, \'wb\') as wav_out: wav_out.setparams(params) wav_out.writeframes(frames)"},{"question":"**Problem Statement:** You are required to implement a function that performs Principal Component Analysis (PCA) on a given dataset. The function should use a randomized SVD approach to compute the principal components and utilize appropriate validation tools to ensure the input data is valid. Additionally, the function should handle sparse input matrices efficiently. **Function Signature:** ```python def perform_randomized_pca(X, n_components, random_state=None): Perform PCA using a randomized SVD approach on the given dataset. Parameters: X (array-like or sparse matrix): The input data matrix with shape (n_samples, n_features). n_components (int): The number of principal components to compute. random_state (int, RandomState instance or None, optional): The seed of the pseudo random number generator. Returns: U (ndarray): The left singular vectors, shape (n_samples, n_components). S (ndarray): The singular values, shape (n_components,). VT (ndarray): The right singular vectors transposed, shape (n_components, n_features). ``` **Specifications:** 1. Validate the input data: - Ensure `X` is a 2D array or a sparse matrix. Raise an error if the input is invalid. - Ensure all elements in `X` are finite numbers. - Ensure `n_components` is a positive integer less than or equal to the number of features in `X`. - Ensure the `random_state` parameter is correctly initialized for reproducibility. 2. Perform the PCA: - Use the `randomized_svd` method from `sklearn.utils.extmath` to compute the singular value decomposition of the input data matrix `X`. - The number of components to compute is specified by `n_components`. 3. Return the left singular vectors `U`, singular values `S`, and the transposed right singular vectors `VT`. 4. Handle sparse matrices efficiently using appropriate functions from `scipy.sparse` and `sklearn.utils.extmath`. **Example Usage:** ```python import numpy as np from scipy.sparse import csr_matrix # Example dense input data X_dense = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) U, S, VT = perform_randomized_pca(X_dense, n_components=2, random_state=42) print(U) print(S) print(VT) # Example sparse input data X_sparse = csr_matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) U, S, VT = perform_randomized_pca(X_sparse, n_components=2, random_state=42) print(U) print(S) print(VT) ``` **Constraints:** - Do not use sklearn\'s `PCA` or similar high-level PCA functions directly. - Ensure the implementation is efficient and handles both dense and sparse matrices. - You may use utilities from `sklearn.utils` for validation and matrix operations, and `scipy.sparse` for sparse matrix handling. **Hint:** Refer to the `check_array`, `check_random_state`, and `randomized_svd` functions from `sklearn.utils` and `sklearn.utils.extmath` to accomplish the task.","solution":"import numpy as np from sklearn.utils import check_array, check_random_state from sklearn.utils.extmath import randomized_svd from scipy.sparse import issparse def perform_randomized_pca(X, n_components, random_state=None): Perform PCA using a randomized SVD approach on the given dataset. Parameters: X (array-like or sparse matrix): The input data matrix with shape (n_samples, n_features). n_components (int): The number of principal components to compute. random_state (int, RandomState instance or None, optional): The seed of the pseudo random number generator. Returns: U (ndarray): The left singular vectors, shape (n_samples, n_components). S (ndarray): The singular values, shape (n_components,). VT (ndarray): The right singular vectors transposed, shape (n_components, n_features). # Validate input X X = check_array(X, accept_sparse=[\'csr\', \'csc\', \'coo\'], ensure_min_samples=2, ensure_min_features=2) if not issparse(X): if not np.all(np.isfinite(X)): raise ValueError(\\"All elements in X must be finite numbers.\\") n_samples, n_features = X.shape if not (1 <= n_components <= n_features): raise ValueError(\\"n_components must be a positive integer less than or equal to the number of features.\\") random_state = check_random_state(random_state) # Perform randomized SVD U, S, VT = randomized_svd(X, n_components=n_components, random_state=random_state) return U, S, VT"},{"question":"Objective: Implement a Python function utilizing the `set` operations and macros as described in the provided documentation. The function should: 1. Create a `set` from a given iterable. 2. Add specified elements to the set. 3. Remove specified elements from the set, without raising `KeyError` for missing elements. 4. Return the modified set. Function Signature: ```python def modify_set(iterable, to_add, to_remove): Modify a set by adding and removing elements. :param iterable: An iterable of initial elements to create the set. :param to_add: An iterable of elements to add to the set. :param to_remove: An iterable of elements to remove from the set (if they exist). :return: Modified set after additions and removals. ``` Input: - `iterable`: An iterable (list, tuple, etc.) containing the initial elements to be placed in the set. - `to_add`: An iterable containing elements to add to the set. - `to_remove`: An iterable containing elements to remove from the set. Output: - A `set` object that has been modified. Constraints: - You should not use `set` operations directly like `set().add()`, `set().discard()`. - Use Python\'s public API for `set` manipulation as provided. - Assume all elements in the input iterables are hashable. Example: ```python initial_elements = [1, 2, 3] elements_to_add = [3, 4] elements_to_remove = [1, 5] modified_set = modify_set(initial_elements, elements_to_add, elements_to_remove) print(modified_set) # Expected: {2, 3, 4} ``` Hints: - Refer to `PySet_New` for creating a new set. - Use `PySet_Add` for adding elements and ensure successful addition. - Use `PySet_Discard` for removing elements without raising an error for missing elements. Implement the `modify_set` function to demonstrate your understanding of set manipulation using the provided Python set API.","solution":"def modify_set(iterable, to_add, to_remove): Modify a set by adding and removing elements. :param iterable: An iterable of initial elements to create the set. :param to_add: An iterable of elements to add to the set. :param to_remove: An iterable of elements to remove from the set (if they exist). :return: Modified set after additions and removals. # Create a set from the given iterable result_set = set(iterable) # Add specified elements to the set for elem in to_add: result_set.add(elem) # Remove specified elements from the set, without raising KeyError for missing elements for elem in to_remove: result_set.discard(elem) return result_set"},{"question":"**HTML Entity Converter** Given a piece of text containing both named HTML entities and their corresponding Unicode characters, write a Python function `convert_html_entities(text: str) -> str` that replaces all named HTML entities with their corresponding Unicode characters. If the entity does not exist in the dictionary, it should be left unchanged. **Function Signature:** ```python def convert_html_entities(text: str) -> str: ``` **Input:** - `text` (str): A string containing HTML named entities (e.g., `&gt;`, `&amp;`) and possibly other characters. **Output:** - (str): A string where all named HTML entities are replaced by their corresponding Unicode characters. **Examples:** ```python print(convert_html_entities(\\"Greater than: &gt;, less than: &lt;\\")) # Output: \\"Greater than: >, less than: <\\" print(convert_html_entities(\\"Ampersand: &amp;, Aacute: &Aacute;, Unknown: &unknown;\\")) # Output: \\"Ampersand: &, Aacute: Á, Unknown: &unknown;\\" ``` **Constraints:** - The input string may contain a mix of named HTML entities and normal characters. - Handle both common HTML entity inclusions and exclusions of the semicolon where applicable. - You can assume that the input will always be a valid string. **Notes:** - You are permitted to use the `html5` dictionary provided by the `html.entities` module in your implementation. - The solution should efficiently utilize dictionary lookups to replace entities in the text. **Additional Requirement:** - Your function should handle large input strings efficiently in terms of both time and memory. **Performance Requirements:** - The solution should have a time complexity of O(n), where n is the length of the input string, given typical usage scenarios. **Hints:** - You may find it useful to look into regular expressions (`re` module) for finding and replacing the entities. - Iterating through the text and performing look-up operations on the dictionary might be crucial for your solution\'s performance.","solution":"import re from html.entities import html5 def convert_html_entities(text: str) -> str: Replaces all named HTML entities with their corresponding Unicode characters. If the entity does not exist in the dictionary, it is left unchanged. # Regex to find all named entities in the text entity_pattern = re.compile(r\'&(#d+|#x[0-9a-fA-F]+|[a-zA-Z]+);\') def replace_entity(match): entity = match.group(0) entity_name = match.group(1) # Check if it\'s a numeric entity if entity_name.startswith(\'#\'): try: if entity_name.startswith(\'#x\') or entity_name.startswith(\'#X\'): return chr(int(entity_name[2:], 16)) else: return chr(int(entity_name[1:], 10)) except ValueError: return entity # If value error, return it unchanged # For named entities return html5.get(entity[1:], entity) # Substitute all the found entities with their corresponding characters return entity_pattern.sub(replace_entity, text)"},{"question":"# Distributed Checkpointing with PyTorch Objective: Your task is to implement a function to save and load a PyTorch model in a distributed setting using the `torch.distributed.checkpoint` module. Problem Statement: You are provided with a PyTorch model and need to save its state dictionary in a distributed manner across multiple ranks. Later, you\'ll also load the state dictionary from these multiple files back into the model, ensuring consistency with the model\'s state. Requirements: 1. **Save the Model**: - Implement `distributed_save_checkpoint(model, save_dir, rank, world_size)`: - **Input**: - `model`: The PyTorch model to save. - `save_dir`: The directory where the checkpoint files will be saved. - `rank`: The rank of the current process in the distributed setup. - `world_size`: The total number of ranks in the distributed setup. - **Output**: None - The function should save the model\'s state dictionary in a distributed fashion, creating separate files for each rank. 2. **Load the Model**: - Implement `distributed_load_checkpoint(model, save_dir, rank, world_size)`: - **Input**: - `model`: The PyTorch model to load the state into. - `save_dir`: The directory where the checkpoint files are stored. - `rank`: The rank of the current process in the distributed setup. - `world_size`: The total number of ranks in the distributed setup. - **Output**: None - The function should load the state dictionary from the distributed files back into the model. Constraints: - You must use `torch.distributed.checkpoint` utilities for saving and loading. - Ensure that the saving and loading process leverages distributed communication effectively. - Assume that the distributed environment is already initialized outside these functions. Example Usage: ```python import torch import torch.nn as nn # Dummy model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) model = SimpleModel() save_dir = \'/path/to/checkpoints\' rank = 0 # Current process rank world_size = 4 # Total number of ranks # Save model checkpoint distributed_save_checkpoint(model, save_dir, rank, world_size) # Load model checkpoint distributed_load_checkpoint(model, save_dir, rank, world_size) ``` Notes: - Ensure that all necessary imports from `torch.distributed.checkpoint` are made. - Pay attention to proper synchronization points if necessary to ensure that all ranks complete their operations consistently.","solution":"import os import torch import torch.distributed as dist def distributed_save_checkpoint(model, save_dir, rank, world_size): Save the PyTorch model state dictionary in a distributed fashion. Args: model (torch.nn.Module): The PyTorch model to save. save_dir (str): The directory where the checkpoint files will be saved. rank (int): The rank of the current process in the distributed setup. world_size (int): The total number of ranks in the distributed setup. Returns: None os.makedirs(save_dir, exist_ok=True) state_dict = model.state_dict() for key, tensor in state_dict.items(): tensor_path = os.path.join(save_dir, f\'{key}_rank{rank}.pt\') torch.save(tensor, tensor_path) # Ensure all ranks have written their parts before moving forward dist.barrier() def distributed_load_checkpoint(model, save_dir, rank, world_size): Load the PyTorch model state dictionary from a distributed checkpoint. Args: model (torch.nn.Module): The PyTorch model to load the state into. save_dir (str): The directory where the checkpoint files are stored. rank (int): The rank of the current process in the distributed setup. world_size (int): The total number of ranks in the distributed setup. Returns: None state_dict = model.state_dict() for key in state_dict.keys(): tensor_path = os.path.join(save_dir, f\'{key}_rank{rank}.pt\') loaded_tensor = torch.load(tensor_path) state_dict[key].copy_(loaded_tensor) model.load_state_dict(state_dict) # Ensure all ranks have read their parts before moving forward dist.barrier()"},{"question":"**Objective:** Using the `urllib.request` module, write a Python program that performs the following tasks: 1. Open a given URL and fetch its content. 2. Handle redirects and output the final URL after all redirects. 3. Handle HTTP basic authentication if provided. 4. Optionally use a proxy if specified. 5. Save the content to a specified file. **Details:** 1. **Function Signature:** ```python def fetch_and_save_url_content(url: str, output_file: str, username: str = None, password: str = None, proxy: dict = None) -> str: ``` 2. **Parameters:** - `url` (str): The URL to fetch. - `output_file` (str): The file where the content should be saved. - `username` (str, optional): Username for HTTP basic authentication. Default is `None`. - `password` (str, optional): Password for HTTP basic authentication. Default is `None`. - `proxy` (dict, optional): A dictionary specifying the proxy server(s) to be used. Example: `{\'http\': \'http://proxy.example.com:8080/\', \'https\': \'https://proxy.example.com:8443/\'}`. Default is `None`. 3. **Returns:** - `str`: The final URL after all redirects. 4. **Functionality Requirements:** - Use `urllib.request.urlopen()` to open the URL. - Follow HTTP redirects and output the final URL. - Handle HTTP basic authentication if `username` and `password` are provided. - Use the specified proxy if provided. - Save the content of the URL to the specified file. - Handle and raise appropriate exceptions for network errors, authentication errors, and file I/O errors. 5. **Example Usage:** ```python final_url = fetch_and_save_url_content( url=\\"http://www.example.com\\", output_file=\\"example_content.html\\", username=\\"user\\", password=\\"pass\\", proxy={\'http\': \'http://proxy.example.com:8080/\'} ) print(f\\"Final URL: {final_url}\\") ``` **Constraints:** - You can assume that the input URL is valid and reachable. - The function should handle timeouts appropriately (e.g., using a `timeout` parameter in `urlopen()`). - You are not expected to handle SSL certificates manually; use defaults provided by `urllib`. Implement this function to demonstrate your understanding of the `urllib.request` module and handling HTTP requests in Python.","solution":"import urllib.request import urllib.error import urllib.parse from urllib.request import Request, urlopen, build_opener, install_opener, ProxyHandler, HTTPBasicAuthHandler, HTTPPasswordMgrWithDefaultRealm from http.client import HTTPResponse def fetch_and_save_url_content(url: str, output_file: str, username: str = None, password: str = None, proxy: dict = None) -> str: try: # Set up authentication if provided if username and password: password_mgr = HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) auth_handler = HTTPBasicAuthHandler(password_mgr) opener = build_opener(auth_handler) else: opener = build_opener() # set up proxy if provided if proxy: proxy_handler = ProxyHandler(proxy) opener.add_handler(proxy_handler) install_opener(opener) # Open the URL and handle redirects implicitly request = Request(url) with urlopen(request) as response: final_url = response.geturl() # Save the content to the specified file with open(output_file, \'wb\') as outfile: outfile.write(response.read()) return final_url except urllib.error.HTTPError as e: raise Exception(f\'HTTP Error: {e.code} {e.reason}\') except urllib.error.URLError as e: raise Exception(f\'URL Error: {e.reason}\') except Exception as e: raise Exception(f\'General Error: {str(e)}\')"},{"question":"# Enumeration-based Task Scheduler Problem Statement You are required to implement a task scheduler using Python\'s `enum` module. In a task scheduler, each task can be assigned a priority level indicating its importance and urgency. Task priorities are represented using the `IntEnum` since they need to be compared against integer values for sorting and managing tasks. The task scheduler will also need to manage task states, such as whether a task is pending, in progress, or completed. These states will be represented using `Flag`, because tasks may have combinations of states due to various conditions and milestones. Instructions 1. **Define the TaskPriority Enum:** - Use `IntEnum` to define `TaskPriority`, where the priorities are LOW, MEDIUM, and HIGH with respective values of 1, 2, and 3. - Ensure that your `TaskPriority` Enum class allows integer comparisons. 2. **Define the TaskState Flag:** - Use `Flag` to define `TaskState`, with the possible states being PENDING, IN_PROGRESS, and COMPLETED. Each state should be assigned an appropriate unique flag value. - Define a combination flag for `ALL` state representing all possible states combined. 3. **Implement the Task class:** - A `Task` class should be defined with the attributes: - `name` (string): name of the task. - `priority` (TaskPriority): priority of the task. - `state` (TaskState): state of the task. - The class should provide the following methods: - `__init__`: initializes the task with `name`, `priority`, and default state as `TaskState.PENDING`. - `set_state`: allows changing the task\'s current state. - `__str__`: returns a string representation of the task including its name, current priority, and states. 4. **Implement the TaskScheduler class:** - Create a `TaskScheduler` class to manage multiple tasks with: - `task_list` (list of Task): an internal list to store tasks. - The class should provide the following methods: - `add_task`: adds a new task to the scheduler. - `get_tasks`: returns the list of tasks sorted by their priority in descending order (HIGH to LOW). - `filter_tasks_by_state`: filters and returns tasks that match a given TaskState. Constraints - Ensure your `TaskPriority` and `TaskState` Enums properly implement the methods required to manage their representations and comparisons. - Use the functionalities from the `enum` module effectively, leveraging advanced functionalities wherever appropriate. Example Usage ```python from enum import IntEnum, Flag, auto # Implement TaskPriority Enum class TaskPriority(IntEnum): # Your Implementation Here # Implement TaskState Flag class TaskState(Flag): # Your Implementation Here class Task: # Your Implementation Here class TaskScheduler: # Your Implementation Here # Testing the implementation if __name__ == \\"__main__\\": scheduler = TaskScheduler() task1 = Task(name=\\"Write Report\\", priority=TaskPriority.HIGH) task2 = Task(name=\\"Email Update\\", priority=TaskPriority.MEDIUM) task3 = Task(name=\\"Setup Meeting\\", priority=TaskPriority.LOW) scheduler.add_task(task1) scheduler.add_task(task2) scheduler.add_task(task3) task1.set_state(TaskState.IN_PROGRESS) for task in scheduler.get_tasks(): print(task) for task in scheduler.filter_tasks_by_state(TaskState.IN_PROGRESS): print(f\\"IN PROGRESS: {task}\\") ``` Expected output of tasks should reflect their priority levels and current states correctly.","solution":"from enum import IntEnum, Flag, auto class TaskPriority(IntEnum): LOW = 1 MEDIUM = 2 HIGH = 3 class TaskState(Flag): PENDING = auto() IN_PROGRESS = auto() COMPLETED = auto() ALL = PENDING | IN_PROGRESS | COMPLETED class Task: def __init__(self, name, priority): self.name = name self.priority = priority self.state = TaskState.PENDING def set_state(self, state): self.state = state def __str__(self): return f\\"Task(name={self.name}, priority={self.priority.name}, state={self.state.name})\\" class TaskScheduler: def __init__(self): self.task_list = [] def add_task(self, task): self.task_list.append(task) def get_tasks(self): return sorted(self.task_list, key=lambda task: task.priority, reverse=True) def filter_tasks_by_state(self, state): return [task for task in self.task_list if task.state & state]"},{"question":"**Python Coding Assessment - `ensurepip` Wrapper Function** # Problem Statement In this exercise, you are to create a more user-friendly wrapper function around the `ensurepip.bootstrap` function to help Python users easily manage their `pip` installations programmatically. Your task is to implement the function `manage_pip_installation`, which should: 1. Install or upgrade `pip` based on the provided parameters. 2. Allow specification of the installation location and user scope. 3. Manage the installation of the `pip` scripts, ensuring correct behavior based on the provided options. # Function Signature ```python def manage_pip_installation( install: bool = True, upgrade: bool = False, root: Optional[str] = None, user: bool = False, altinstall: bool = False, default_pip: bool = False, verbosity: int = 0 ) -> str: pass ``` # Parameters: - `install` (bool): If True, bootstrap `pip` installation. If False, perform no action. - `upgrade` (bool): If True, upgrade any existing `pip` to the latest version. Defaults to False. - `root` (Optional[str]): The root directory to install `pip` relative to. If None, uses the default location. - `user` (bool): If True, install `pip` in the user site packages directory. Defaults to False. - `altinstall` (bool): If True, do not install the `pipX` script. Defaults to False. - `default_pip` (bool): If True, install the default `pip` script. Defaults to False. - `verbosity` (int): Level of verbosity for the output. Defaults to 0. # Returns: - str: A string indicating the status of the installation (\\"No action taken\\", \\"pip installed\\", \\"pip upgraded\\"). # Constraints: - Raising a `ValueError` if both `altinstall` and `default_pip` are True. - Correctly handle all optional parameters by passing them to `ensurepip.bootstrap`. # Example Usage: ```python # Example 1: Install pip in default configuration print(manage_pip_installation()) # Example 2: Upgrade pip in the user site packages directory with verbose output print(manage_pip_installation(upgrade=True, user=True, verbosity=2)) # Example 3: Install pip without \'pipX\' script in a specified root directory print(manage_pip_installation(root=\'/custom/path\', altinstall=True)) ``` # Additional Information: You may refer to the `ensurepip` documentation provided to correctly utilize the `ensurepip.bootstrap` function within your implementation.","solution":"import ensurepip def manage_pip_installation( install: bool = True, upgrade: bool = False, root: str = None, user: bool = False, altinstall: bool = False, default_pip: bool = False, verbosity: int = 0 ) -> str: Manage the installation of pip using ensurepip. Args: - install (bool): If True, bootstrap pip installation. - upgrade (bool): If True, upgrade existing pip. - root (Optional[str]): The root directory to install pip relative to. - user (bool): If True, install pip in the user site packages directory. - altinstall (bool): If True, do not install the pipX script. - default_pip (bool): If True, install the default pip script. - verbosity (int): Level of verbosity for the output. Returns: - str: Status of the installation. if altinstall and default_pip: raise ValueError(\\"Cannot set both altinstall and default_pip to True.\\") if not install: return \\"No action taken\\" options = [] if upgrade: options.append(\'--upgrade\') if root: options.extend([\'--root\', root]) if user: options.append(\'--user\') if altinstall: options.append(\'--altinstall\') if default_pip: options.append(\'--default-pip\') if verbosity > 0: options.extend([\'-\' + \'v\' * verbosity]) ensurepip.bootstrap(options) return \\"pip upgraded\\" if upgrade else \\"pip installed\\""},{"question":"# Question: Creating a Custom Python Class with Special Methods You are tasked with creating a custom Python class to demonstrate your understanding of class and instance attribute management, object representation, and comparison. Using Python, implement a class called `CustomType` that emulates some capabilities described in the `PyTypeObject` structure. Requirements: 1. **Class Definition:** - Define a class named `CustomType`. - This class should have an `__init__` method that initializes two attributes: `name` (a string) and `value` (an integer). 2. **Representation Methods:** - Implement a `__repr__` method that returns a string formatted as `CustomType(name=\'<name>\', value=<value>)`. - Implement a `__str__` method that provides a user-friendly string representation of the format `CustomType \\"<name>\\" with value <value>`. 3. **Attribute Management:** - Implement `__getattr__` to dynamically handle attribute access. If an attribute does not exist, this method should return a string `\\"<attribute> is not defined\\"`. - Implement `__setattr__` to prevent changes to the `name` attribute once set. If there is an attempt to set `name`, it should raise an `AttributeError`. 4. **Comparison Method:** - Implement a `__eq__` method to compare two instances of `CustomType` based on their `value` attribute. - Implement a `__lt__` method to compare if one instance\'s `value` is less than another instance\'s `value`. # Constraints: - You should not use any external libraries. - Your class should raise appropriate errors for invalid operations. # Example Usage: ```python obj1 = CustomType(\\"example\\", 10) obj2 = CustomType(\\"test\\", 20) print(repr(obj1)) # Output: CustomType(name=\'example\', value=10) print(str(obj1)) # Output: CustomType \\"example\\" with value 10 print(obj1.some_attr) # Output: some_attr is not defined obj1.some_attr = 100 # This should be allowed print(obj1.some_attr) # Output: 100 try: obj1.name = \\"new_name\\" # This should raise an AttributeError except AttributeError as e: print(e) # Output: Cannot modify the name attribute print(obj1 == obj2) # Output: False print(obj1 < obj2) # Output: True ``` # Implementation: Implement the `CustomType` class with the specified methods and behavior in Python. ```python class CustomType: def __init__(self, name, value): self.name = name self.value = value def __repr__(self): return f\\"CustomType(name=\'{self.name}\', value={self.value})\\" def __str__(self): return f\'CustomType \\"{self.name}\\" with value {self.value}\' def __getattr__(self, attr): return f\\"{attr} is not defined\\" def __setattr__(self, attr, value): if attr == \\"name\\" and hasattr(self, attr): raise AttributeError(\\"Cannot modify the name attribute\\") super().__setattr__(attr, value) def __eq__(self, other): if isinstance(other, CustomType): return self.value == other.value return False def __lt__(self, other): if isinstance(other, CustomType): return self.value < other.value return False # Test cases obj1 = CustomType(\\"example\\", 10) obj2 = CustomType(\\"test\\", 20) print(repr(obj1)) # Output: CustomType(name=\'example\', value=10) print(str(obj1)) # Output: CustomType \\"example\\" with value 10 print(obj1.some_attr) # Output: some_attr is not defined obj1.some_attr = 100 # This should be allowed print(obj1.some_attr) # Output: 100 try: obj1.name = \\"new_name\\" # This should raise an AttributeError except AttributeError as e: print(e) # Output: Cannot modify the name attribute print(obj1 == obj2) # Output: False print(obj1 < obj2) # Output: True ``` Ensure that your implementation passes all the example use cases shown above.","solution":"class CustomType: def __init__(self, name, value): self.name = name self.value = value def __repr__(self): return f\\"CustomType(name=\'{self.name}\', value={self.value})\\" def __str__(self): return f\'CustomType \\"{self.name}\\" with value {self.value}\' def __getattr__(self, attr): return f\\"{attr} is not defined\\" def __setattr__(self, attr, value): if attr == \\"name\\" and \\"name\\" in self.__dict__: raise AttributeError(\\"Cannot modify the name attribute\\") super().__setattr__(attr, value) def __eq__(self, other): if isinstance(other, CustomType): return self.value == other.value return False def __lt__(self, other): if isinstance(other, CustomType): return self.value < other.value return False"},{"question":"Implement and Evaluate the SGDClassifier **Objective**: You are required to implement a classification model using the `SGDClassifier` provided by scikit-learn. The task will assess your understanding of configuring, training, and evaluating an SGD model. **Task**: 1. Implement a function `train_sgd_classifier` that: - Takes the following inputs: - `X_train`: A 2D numpy array or pandas DataFrame representing the training data (n_samples, n_features). - `y_train`: A 1D numpy array or pandas Series representing the training labels (n_samples,). - `X_test`: A 2D numpy array or pandas DataFrame representing the test data (n_samples_test, n_features). - `loss`: A string indicating the loss function to use (e.g., \'hinge\', \'log_loss\'). - `penalty`: A string indicating the penalty to use (\'l2\', \'l1\', \'elasticnet\'). - `max_iter`: An integer specifying the maximum number of iterations. - `alpha`: A float value for the regularization strength. - Returns: - `predictions`: A numpy array representing the predicted labels for `X_test`. - `coef`: A numpy array representing the model coefficients. - `intercept`: A float value representing the model intercept. 2. Implement a function `evaluate_model` that: - Takes the following inputs: - `y_true`: A 1D numpy array of true labels. - `y_pred`: A 1D numpy array of predicted labels. - Returns: - `accuracy`: A float representing the accuracy of the predictions. **Constraints**: - Ensure the input data `X_train` and `X_test` are scaled using the `StandardScaler` before fitting the model. - Use `shuffle=True` when initializing the `SGDClassifier` to shuffle the training data after each epoch. **Performance Requirements**: - The implementation should be efficient enough to handle the datasets with a large number of samples (e.g., 10^5 samples) without running into performance bottlenecks. # Example Usage: ```python import numpy as np from sklearn.model_selection import train_test_split # Generate some synthetic data X, y = np.random.rand(1000, 20), np.random.randint(0, 2, 1000) # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train SGD Classifier predictions, coef, intercept = train_sgd_classifier(X_train, y_train, X_test, \'hinge\', \'l2\', 1000, 0.0001) # Evaluate the model accuracy = evaluate_model(y_test, predictions) print(f\\"Model Coefficients: {coef}\\") print(f\\"Model Intercept: {intercept}\\") print(f\\"Model Accuracy: {accuracy}\\") ``` **Note**: Ensure that you have installed scikit-learn: ```bash pip install scikit-learn ``` # Evaluation Criteria: - Correctness: The functions should work as expected and return the correct outputs. - Efficiency: The implementation should be efficient in terms of computational complexity. - Code Quality: The code should be well-structured, readable, and follow best practices. - Use of scikit-learn: Proper usage of scikit-learn classes and functions as specified.","solution":"from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler def train_sgd_classifier(X_train, y_train, X_test, loss, penalty, max_iter, alpha): Trains an SGD classifier with the given parameters and returns the predictions, coefficients, and intercept. :param X_train: 2D numpy array or pandas DataFrame of training data. :param y_train: 1D numpy array or pandas Series of training labels. :param X_test: 2D numpy array or pandas DataFrame of test data. :param loss: Loss function to use (string), e.g., \'hinge\', \'log_loss\'. :param penalty: Penalty to use (string), e.g., \'l2\', \'l1\', \'elasticnet\'. :param max_iter: Maximum number of iterations (integer). :param alpha: Regularization strength (float). :return: tuple containing: - predictions: numpy array of predicted labels for X_test. - coef: numpy array of model coefficients. - intercept: float representing the model intercept. scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) clf = SGDClassifier(loss=loss, penalty=penalty, max_iter=max_iter, alpha=alpha, shuffle=True, random_state=42) clf.fit(X_train_scaled, y_train) predictions = clf.predict(X_test_scaled) return predictions, clf.coef_, clf.intercept_ def evaluate_model(y_true, y_pred): Evaluates the model accuracy based on true and predicted labels. :param y_true: 1D numpy array of true labels. :param y_pred: 1D numpy array of predicted labels. :return: Accuracy of the predictions (float). accuracy = (y_pred == y_true).mean() return accuracy"},{"question":"**Objective**: Demonstrate understanding and implementation of the hashlib module focusing on hash functions, and secure password-based key derivation using advanced parameters like salt and iterative counts. **Problem Statement**: Implement a small command-line utility in Python that allows a user to: 1. Generate a secure hash for a given input string and selected hash algorithm (e.g., `sha256`, `sha512`, `blake2b`, etc.). 2. Compute a salted hash with a user-provided or autogenerated random salt. 3. Derive a secure key from a password using PBKDF2 with `sha256` hash algorithm. **Instructions**: 1. **Hash Generation**: - Create a function `generate_hash(algorithm: str, input_data: str) -> str` that: - Takes a hash algorithm name and an input string. - Returns the hexdigest of the hash. 2. **Salted Hash Generation**: - Create a function `generate_salted_hash(algorithm: str, input_data: str, salt: bytes = None) -> Tuple[str, bytes]` that: - Takes a hash algorithm name, an input string, and a salt value (bytes). - If salt is not provided, generate a random salt of appropriate length. - Returns a tuple containing the hexdigest of the salted hash and the used salt. 3. **Password-Based Key Derivation**: - Create a function `derive_key(password: str, salt: bytes, iterations: int, key_length: int) -> str` that: - Takes a password string, salt value (bytes), number of iterations, and desired key length. - Uses PBKDF2 with `sha256` to derive a secure key. - Returns the derived key in hexadecimal format. **Expected Input and Output**: 1. **Function: `generate_hash`** - Input: `(\\"sha256\\", \\"hello\\")` - Output: `\\"2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824\\"` 2. **Function: `generate_salted_hash`** - Input: `(\\"sha256\\", \\"hello\\", b\'somesalt\')` - Output: `(\\"5e884898da28047151d0e56f8dc6292773603d0d6aabbdd0b3ee55ffa4c1a574\\", b\'somesalt\')` 3. **Function: `derive_key`** - Input: `(\\"password\\", b\'somesalt\', 100000, 32)` - Output: `\\"fc5e038d38a57032085441e7fe7010b0f9e5eabf5f76b978e161d9326f16fa2a\\"` **Constraints**: - Implement effective input validation. - Ensure algorithms and parameters are compliant with the `hashlib` documentation. - Use secure and random salt generation if not provided (`os.urandom()`). **Performance Requirements**: - The solution should handle typical usage scenarios efficiently. - For compute-intensive operations like key derivation, provide benchmarks for common iteration counts and key lengths. ```python import hashlib import os from typing import Tuple def generate_hash(algorithm: str, input_data: str) -> str: Generate a hash for the given input using the specified algorithm. :param algorithm: The name of the hash algorithm (e.g., \'sha256\', \'sha512\'). :param input_data: The input string to hash. :return: Hexdigest of the hashed input. hash_func = hashlib.new(algorithm) hash_func.update(input_data.encode()) return hash_func.hexdigest() def generate_salted_hash(algorithm: str, input_data: str, salt: bytes = None) -> Tuple[str, bytes]: Generate a salted hash for the given input using the specified algorithm. :param algorithm: The name of the hash algorithm (e.g., \'sha256\', \'sha512\'). :param input_data: The input string to hash. :param salt: Optional salt value (if None, a random salt will be generated). :return: Tuple of (Hexdigest of the salted hash, used salt). if salt is None: salt = os.urandom(16) # Generating a 16-byte random salt hash_func = hashlib.new(algorithm) hash_func.update(salt + input_data.encode()) return hash_func.hexdigest(), salt def derive_key(password: str, salt: bytes, iterations: int, key_length: int) -> str: Derive a secure key from the password using PBKDF2 with sha256. :param password: The password string. :param salt: The salt value (bytes). :param iterations: Number of iterations for key derivation. :param key_length: Desired length of the derived key. :return: Derived key in hexadecimal format. key = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, iterations, dklen=key_length) return key.hex() ``` Ensure you test functions with different algorithms, inputs, and corner cases while documenting results and performance observations.","solution":"import hashlib import os from typing import Tuple def generate_hash(algorithm: str, input_data: str) -> str: Generate a hash for the given input using the specified algorithm. :param algorithm: The name of the hash algorithm (e.g., \'sha256\', \'sha512\'). :param input_data: The input string to hash. :return: Hexdigest of the hashed input. hash_func = hashlib.new(algorithm) hash_func.update(input_data.encode()) return hash_func.hexdigest() def generate_salted_hash(algorithm: str, input_data: str, salt: bytes = None) -> Tuple[str, bytes]: Generate a salted hash for the given input using the specified algorithm. :param algorithm: The name of the hash algorithm (e.g., \'sha256\', \'sha512\'). :param input_data: The input string to hash. :param salt: Optional salt value (if None, a random salt will be generated). :return: Tuple of (Hexdigest of the salted hash, used salt). if salt is None: salt = os.urandom(16) # Generating a 16-byte random salt hash_func = hashlib.new(algorithm) hash_func.update(salt + input_data.encode()) return hash_func.hexdigest(), salt def derive_key(password: str, salt: bytes, iterations: int, key_length: int) -> str: Derive a secure key from the password using PBKDF2 with sha256. :param password: The password string. :param salt: The salt value (bytes). :param iterations: Number of iterations for key derivation. :param key_length: Desired length of the derived key. :return: Derived key in hexadecimal format. key = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, iterations, dklen=key_length) return key.hex()"},{"question":"**Title: Building a Command-Line Tool with `__main__` in Python** **Objective**: Implement a Python module that behaves differently when run as a script versus when imported, utilizing advanced concepts of the `__main__` environment. # Problem Statement You are to develop a Python script named `task_manager.py`, which provides basic functionality for managing a list of tasks. This script should demonstrate the use of the `__main__` environment to differentiate between running the script directly and importing it as a module. **Requirements**: 1. **Function Implementations**: - `def add_task(task_list: list, task: str) -> None:`: Adds a new task to the task list. - `def remove_task(task_list: list, task: str) -> bool:`: Removes a task from the task list if it exists and returns `True`. Returns `False` if the task was not found. - `def list_tasks(task_list: list) -> None:`: Prints all tasks in the task list. 2. **Command-Line Interface**: - When run directly (not imported), `task_manager.py` should accept command-line arguments to manipulate and display the task list. - You should use `sys.argv` to handle command-line arguments. - The script should support the following commands: - `python task_manager.py add \\"Task Description\\"`: Adds \\"Task Description\\" to the list. - `python task_manager.py remove \\"Task Description\\"`: Removes \\"Task Description\\" from the list. - `python task_manager.py list`: Lists all tasks. 3. **`main` Function**: - Implement a `main()` function in your script to encapsulate the command-line interface logic. 4. **Module Usage**: - Ensure that the functions `add_task`, `remove_task`, and `list_tasks` are still accessible when the script is imported as a module without executing the command-line interface logic. # Constraints: - You may assume that the task descriptions are unique and case-sensitive. - You should use a global list to store tasks for simplicity. # Example Usage When run as a script: ```sh python task_manager.py add \\"Write Unit Tests\\" Task added: Write Unit Tests python task_manager.py add \\"Implement Main Function\\" Task added: Implement Main Function python task_manager.py list 1. Write Unit Tests 2. Implement Main Function python task_manager.py remove \\"Write Unit Tests\\" Task removed: Write Unit Tests python task_manager.py list 1. Implement Main Function ``` When imported as a module: ```python from task_manager import add_task, remove_task, list_tasks tasks = [] add_task(tasks, \\"Write Documentation\\") add_task(tasks, \\"Refactor Code\\") remove_task(tasks, \\"Write Documentation\\") list_tasks(tasks) ``` # Implementation Details Implement the functions `add_task`, `remove_task`, and `list_tasks` in a way that they perform the required operations on the given list of tasks. Include the command-line interface logic inside the `main()` function and conditionally execute it using `if __name__ == \\"__main__\\"` block. **Tip**: Make use of the `sys` module to handle command-line arguments and perform necessary actions based on them. Good luck!","solution":"import sys # Global task list task_list = [] def add_task(task_list: list, task: str) -> None: Adds a new task to the task list. task_list.append(task) print(f\'Task added: {task}\') def remove_task(task_list: list, task: str) -> bool: Removes a task from the task list if it exists. Returns True if the task was found and removed, otherwise False. if task in task_list: task_list.remove(task) print(f\'Task removed: {task}\') return True else: print(f\'Task not found: {task}\') return False def list_tasks(task_list: list) -> None: Prints all tasks in the task list. if not task_list: print(\\"No tasks found.\\") else: for i, task in enumerate(task_list, start=1): print(f\'{i}. {task}\') def main(): Main function to handle command-line interface. if len(sys.argv) < 2: print(\\"Usage: python task_manager.py [add|remove|list] [task_description]\\") sys.exit(1) command = sys.argv[1] if command == \\"add\\" and len(sys.argv) == 3: add_task(task_list, sys.argv[2]) elif command == \\"remove\\" and len(sys.argv) == 3: remove_task(task_list, sys.argv[2]) elif command == \\"list\\": list_tasks(task_list) else: print(\\"Invalid command or missing task description.\\") print(\\"Usage: python task_manager.py [add|remove|list] [task_description]\\") sys.exit(1) if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement:** You are required to implement a simplified version of the \\"tabnanny\\" module to gain a better understanding of recursive file handling and exception management in Python. Your task is to write a program that: 1. Recursively searches through a given directory for Python (.py) files. 2. For each Python file, checks for lines containing both tab and space characters before non-whitespace characters (an ambiguous indent). 3. Raises a custom exception `AmbiguousIndentationError` if such a line is found, capturing the line number and file name. 4. Provides functionality to only print the filenames containing ambiguous indentation if a specific flag is set. # Specifications: 1. **Function `check_indentation(file_or_dir: str, filename_only: bool = False)`**: - **Input**: - `file_or_dir` (string): The path of the file or directory to be checked. - `filename_only` (boolean): A flag to indicate whether to print only filenames with ambiguous indentations. - **Output**: - Prints the diagnostic message either with detailed lines or only filenames based on the `filename_only` flag. - **Constraints**: - Do not use any third-party libraries; use only Python\'s standard library. - Follow the given directory path recursively. 2. **Custom Exception `AmbiguousIndentationError`**: - Define a custom exception capturing the line number and filename. # Example: ```python # Given the following directory structure: # /test_dir/ # |-- script1.py # contains lines with both tabs and spaces before a non-whitespace character # |-- script2.py # does not contain such lines # |-- sub_dir/ # |-- script3.py # contains lines with both tabs and spaces before a non-whitespace character # Running the function: check_indentation(\'test_dir\', filename_only=True) # Expected Output: # test_dir/script1.py # test_dir/sub_dir/script3.py ``` ```python # Running the function without filename_only flag: check_indentation(\'test_dir\') # Expected Output: # File: test_dir/script1.py # Line 2: <contents of line with ambiguous indent> # File: test_dir/sub_dir/script3.py # Line 5: <contents of line with ambiguous indent> ``` # Additional Notes: - Handle exceptions appropriately and ensure clear output. - Consider edge cases such as empty directories or files without ambiguous indentations.","solution":"import os class AmbiguousIndentationError(Exception): def __init__(self, filename, lineno, line): self.filename = filename self.lineno = lineno self.line = line super().__init__(f\\"Ambiguous indentation in {filename} at line {lineno}: {line}\\") def check_file_for_indentation_issues(filename, filename_only): issues_found = False with open(filename, \'r\', encoding=\'utf-8\') as file: for lineno, line in enumerate(file, 1): stripped_line = line.lstrip() if stripped_line and line != stripped_line and line[:len(line) - len(stripped_line)].count(\' \') > 0 and line[:len(line) - len(stripped_line)].count(\'t\') > 0: if filename_only: print(filename) return True else: print(f\\"File: {filename}\\") print(f\\"Line {lineno}: {line}\\", end=\'\') issues_found = True return issues_found def check_indentation(file_or_dir: str, filename_only: bool = False): if os.path.isfile(file_or_dir): check_file_for_indentation_issues(file_or_dir, filename_only) elif os.path.isdir(file_or_dir): for root, _, files in os.walk(file_or_dir): for file in files: if file.endswith(\'.py\'): filepath = os.path.join(root, file) check_file_for_indentation_issues(filepath, filename_only) else: raise FileNotFoundError(f\\"No such file or directory: \'{file_or_dir}\'\\")"},{"question":"# Seaborn Visualization Coding Assessment Problem Statement As a data analyst, you need to perform exploratory data analysis on the `penguins` dataset provided by seaborn. You are required to create various data visualizations to understand the distributions of certain variables and their interrelationships. Your task is to implement a function that generates specific plots as specified below. Requirements The function should be named `create_penguin_plots` and should accept no arguments. It should produce the following plots: 1. **Histogram of Flipper Length**: - Create a histogram showing the distribution of the `flipper_length_mm` variable. - Use a bin width of 5. - Color the bars by the `species` using the `hue` parameter. 2. **Kernel Density Estimate (KDE) of Flipper Length**: - Create a KDE plot for the `flipper_length_mm` variable. - Use `hue` to differentiate between different `species`. - Adjust the bandwidth by a factor of 0.3 to make the density estimate more responsive. 3. **Bivariate KDE of Bill Length and Bill Depth**: - Create a bivariate KDE plot using `bill_length_mm` and `bill_depth_mm`. - Differentiate the KDE plots by `species` using `hue`. 4. **Empirical Cumulative Distribution Function (ECDF) of Flipper Length**: - Create an ECDF of the `flipper_length_mm` variable. - Use `hue` to differentiate between `species`. Implementation Ensure that your function imports the necessary seaborn and matplotlib packages and loads the `penguins` dataset from seaborn. Your function should display the plots one after the other when executed. ```python import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Histogram of Flipper Length plt.figure() sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=5, hue=\\"species\\") plt.title(\\"Histogram of Flipper Length\\") plt.show() # KDE of Flipper Length plt.figure() sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", bw_adjust=0.3) plt.title(\\"KDE of Flipper Length\\") plt.show() # Bivariate KDE of Bill Length and Bill Depth plt.figure() sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\\"Bivariate KDE of Bill Length and Bill Depth\\") plt.show() # ECDF of Flipper Length plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Flipper Length\\") plt.show() # Call the function to generate the plots create_penguin_plots() ``` Assessment Your implementation will be assessed on: - Correctness: Ensuring all specified plots are generated correctly. - Parameters: Correct usage of parameters such as `binwidth`, `hue`, `bw_adjust`. - Readability: Proper use of comments and code organization. - Presentation: Clear titles for each plot and proper display.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Histogram of Flipper Length plt.figure() sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=5, hue=\\"species\\") plt.title(\\"Histogram of Flipper Length\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # KDE of Flipper Length plt.figure() sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", bw_adjust=0.3) plt.title(\\"KDE of Flipper Length\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() # Bivariate KDE of Bill Length and Bill Depth plt.figure() sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\\"Bivariate KDE of Bill Length and Bill Depth\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() # ECDF of Flipper Length plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Flipper Length\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.show() # Call the function to generate the plots create_penguin_plots()"},{"question":"# Question: XML Parsing and Manipulation with `xml.dom.minidom` Using the `xml.dom.minidom` module in Python, write a function that achieves the following tasks: 1. **Parse an XML String:** - Parse the given XML string to create a DOM document object. 2. **Add a New Element:** - Add a new element under a specified parent node. The new element\'s tag name and text content should be provided as inputs. 3. **Modify Existing Element Text:** - Modify the text content of a specified element. The tag name of the element and the new text content should be provided as inputs. 4. **Delete an Element:** - Delete all elements with a specified tag name. 5. **Output the Modified XML:** - Return the modified XML as a pretty-printed string. # Function Signature ```python from xml.dom.minidom import parseString, getDOMImplementation def manipulate_xml(xml_string: str, new_element_tag: str, new_element_text: str, modify_tag: str, new_text: str, delete_tag: str) -> str: pass ``` # Inputs - `xml_string` (str): An XML string. - `new_element_tag` (str): The tag name of the new element to be added. - `new_element_text` (str): The text content of the new element to be added. - `modify_tag` (str): The tag name of the element whose text content needs to be modified. - `new_text` (str): The new text content for the specified element. - `delete_tag` (str): The tag name of the elements to be deleted. # Outputs - Returns a pretty-printed XML string representing the modified DOM structure. # Constraints - Assume the XML string is well-formed. - The specified tags for modification and deletion uniquely identify target elements in the XML. # Example Usage ```python xml_string = <root> <item>Original Text</item> <item>To Be Deleted</item> </root> new_element_tag = \\"newItem\\" new_element_text = \\"New Item Text\\" modify_tag = \\"item\\" new_text = \\"Modified Text\\" delete_tag = \\"item\\" result = manipulate_xml(xml_string, new_element_tag, new_element_text, modify_tag, new_text, delete_tag) print(result) ``` Expected Output: ```xml <root> <newItem>New Item Text</newItem> </root> ``` **Notes**: - You may use helper functions as needed. - Ensure that the output XML is properly formatted with appropriate indentations using the `toprettyxml()` method.","solution":"from xml.dom.minidom import parseString, getDOMImplementation def manipulate_xml(xml_string: str, new_element_tag: str, new_element_text: str, modify_tag: str, new_text: str, delete_tag: str) -> str: # Parse the XML string into a DOM document dom = parseString(xml_string) # Add a new element under the root node root = dom.documentElement new_element = dom.createElement(new_element_tag) new_element.appendChild(dom.createTextNode(new_element_text)) root.appendChild(new_element) # Modify the text content of a specified element elements_to_modify = dom.getElementsByTagName(modify_tag) for element in elements_to_modify: for child in element.childNodes: if child.nodeType == child.TEXT_NODE: child.data = new_text # Delete all elements with the specified tag elements_to_delete = dom.getElementsByTagName(delete_tag) for element in elements_to_delete: element.parentNode.removeChild(element) # Return the modified XML as a pretty-printed string return dom.toprettyxml()"},{"question":"**Objective:** You are required to implement a series of tensor transformations using PyTorch that: 1. Create a view of a base tensor. 2. Demonstrate the relationship between the view and the base tensor. 3. Deal with contiguity of tensors when needed. **Detailed Requirements:** 1. **Function Definition:** - Define a function `transform_tensor` that accepts an input tensor `t` and performs the following transformations: ```python def transform_tensor(t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: pass ``` - Inputs: - `t`: A two-dimensional PyTorch tensor of shape `(M, N)`. - Returns: - A tuple containing three tensors: 1. `view_tensor`: A view of the tensor `t` reshaped to `(2, M*N//2)`. 2. `transposed_tensor`: A transposed view of `view_tensor`. 3. `contiguous_tensor`: A contiguous copy of `transposed_tensor`. 2. **Constraints:** - Ensure the shape of `t` is such that `M*N` is divisible by 2. - Use PyTorch\'s view and transpose methods to manipulate the shapes of the tensors. - Ensure the final tensor (`contiguous_tensor`) is contiguous. 3. **Behavior:** - Demonstrate the views sharing underlying data by modifying an element in `view_tensor` and showing the corresponding change in `t`. - Verify and handle the contiguity property for the `transposed_tensor`. **Examples:** Here are some examples of how the function should behave. ```python import torch def transform_tensor(t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: # Step 1: View the tensor reshaped to (2, M*N//2) view_tensor = t.view(2, -1) # Step 2: Transpose the view tensor transposed_tensor = view_tensor.transpose(0, 1) # Step 3: Convert the transposed tensor to a contiguous tensor contiguous_tensor = transposed_tensor.contiguous() return view_tensor, transposed_tensor, contiguous_tensor # Example use case t = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]]) view_tensor, transposed_tensor, contiguous_tensor = transform_tensor(t) # Modify an element in the view tensor view_tensor[0][0] = 9 assert t[0][0] == 9 # Change is reflected in the base tensor # Verify contiguity assert not transposed_tensor.is_contiguous() assert contiguous_tensor.is_contiguous() ``` Your task is to implement the function `transform_tensor` as specified and ensure it passes the given examples and any edge test cases. **Additional Notes:** - You can assume that the input tensor `t` will always be two-dimensional and have an even number of elements. - The solution must utilize PyTorch\'s in-built methods for tensor manipulation without creating unnecessary data copies.","solution":"import torch from typing import Tuple def transform_tensor(t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: # Step 1: View the tensor reshaped to (2, M*N//2) view_tensor = t.view(2, -1) # Step 2: Transpose the view tensor transposed_tensor = view_tensor.transpose(0, 1) # Step 3: Convert the transposed tensor to a contiguous tensor contiguous_tensor = transposed_tensor.contiguous() return view_tensor, transposed_tensor, contiguous_tensor"},{"question":"**Coding Assessment Question** # Objective The goal of this coding assessment is to evaluate your understanding and practical application of Python\'s module importation techniques provided by the Python C API. You are required to demonstrate your ability to interact with these functions via Python bindings. # Problem Statement You are tasked with simulating a Python interpreter\'s module loading mechanism. Specifically, you will create a Python module that can import another module using the constructs provided in the documentation. # Requirements Implement a Python class `ModuleManager` that provides the following methods: 1. `import_module(self, name: str) -> Any`: Imports a module by its name using `PyImport_ImportModule`. 2. `reload_module(self, module: Any) -> Any`: Reloads an already imported module using `PyImport_ReloadModule`. 3. `exec_code_in_module(self, name: str, code: str) -> Any`: Executes provided Python code in the context of a newly created module with the given name, using `PyImport_ExecCodeModule`. # Input and Output Formats - `import_module(self, name: str) -> Any` - **Input**: A string `name` representing the name of the module to be imported. - **Output**: The imported module object. - `reload_module(self, module: Any) -> Any` - **Input**: A module object `module` that needs to be reloaded. - **Output**: The reloaded module object. - `exec_code_in_module(self, name: str, code: str) -> Any` - **Input**: - `name`: A string representing the desired module name. - `code`: A string containing Python code to be executed in the module\'s context. - **Output**: The module object after executing the code. # Constraints 1. Use the Python C API functions described in the documentation to implement the required methods. 2. Ensure that exceptions are properly handled and informative error messages are provided where applicable. 3. The `exec_code_in_module` method should handle the creation of a module from scratch if it doesn\'t already exist. # Example ```python manager = ModuleManager() # Import math module math_module = manager.import_module(\\"math\\") print(math_module.sqrt(4)) # Output: 2.0 # Reload math module reloaded_math_module = manager.reload_module(math_module) print(reloaded_math_module.factorial(5)) # Output: 120 # Execute code in a new module context code = def hello(): return \'Hello, World!\' new_module = manager.exec_code_in_module(\\"custom_mod\\", code) print(new_module.hello()) # Output: Hello, World! ``` # Notes - For this task, assume you have created appropriate Python bindings for the C API functions if you are implementing this using C extensions. - Pay close attention to the referencing and dereferencing of Python objects to avoid memory leaks. - Testing the correctness of your implementation will require integrating with Python\'s C API, which might mean running your code as a compiled extension.","solution":"import importlib import types class ModuleManager: def import_module(self, name: str) -> types.ModuleType: Imports a module by its name. :param name: The name of the module to import. :return: The imported module object. try: module = importlib.import_module(name) return module except ImportError as e: raise ImportError(f\\"Module {name} cannot be imported: {e}\\") def reload_module(self, module: types.ModuleType) -> types.ModuleType: Reloads an already imported module. :param module: The module object to reload. :return: The reloaded module object. try: reloaded_module = importlib.reload(module) return reloaded_module except TypeError as e: raise TypeError(f\\"Module {module} cannot be reloaded: {e}\\") def exec_code_in_module(self, name: str, code: str) -> types.ModuleType: Executes provided Python code in the context of a newly created module with the given name. :param name: The desired module name. :param code: The Python code to execute. :return: The module object after executing the code. try: new_module = types.ModuleType(name) exec(code, new_module.__dict__) return new_module except SyntaxError as e: raise SyntaxError(f\\"Error in executing code in module {name}: {e}\\") except Exception as e: raise Exception(f\\"An error occurred while executing code in module {name}: {e}\\")"},{"question":"# Advanced Assignment Statements and Iterable Unpacking Assessment Objective: This question is designed to test your understanding of Python\'s assignment statements, including complex assignments with multiple targets, iterable unpacking, and usage of star expressions. Problem: You are given a list of tuples, where each tuple contains two elements: an integer and a string. Your task is to write a function that processes this list in the following specific way: 1. **Swap Operations**: For each tuple in the list, swap the integer and string positions. 2. **Filtered Output**: Create a filtered list which only includes tuples where the integer is an even number. 3. **Unpacking and Aggregation**: Return the filtered list with integers unpacked into one list and strings into another list. Function Signature: ```python def process_tuples(data: list[tuple[int, str]]) -> tuple[list[int], list[str]]: Process a list of tuples with specific operations as described. :param data: List of tuples where each tuple contains (int, str) :return: A tuple with two lists; the first list contains integers and the second list contains strings ``` Input: - `data`: A list of tuples. Each tuple contains: - First element: An integer (both positive and negative integers) - Second element: A string. Output: - A tuple with two lists: - The first list contains all integers (in the swapped and filtered tuples) from the input list. - The second list contains all strings (in the swapped and filtered tuples) from the input list. Example: ```python input_data = [(4, \\"apple\\"), (3, \\"banana\\"), (8, \\"cherry\\"), (7, \\"date\\")] result = process_tuples(input_data) print(result) # Output: ([4, 8], [\\"apple\\", \\"cherry\\"]) ``` Constraints: 1. The length of the input list `data` is between 1 and 10000. 2. Each integer in the tuples is between -1000000000 and 1000000000. 3. Each string in the tuples is non-empty and contains only alphabetic characters. Notes: - The function must accurately filter, swap, and unpack the elements. - Performance should be considered to ensure the function runs efficiently on large datasets.","solution":"def process_tuples(data: list[tuple[int, str]]) -> tuple[list[int], list[str]]: Process a list of tuples with specific operations as described. :param data: List of tuples where each tuple contains (int, str) :return: A tuple with two lists; the first list contains integers and the second list contains strings swapped_filtered = [(string, integer) for integer, string in data if integer % 2 == 0] integers_list = [integer for string, integer in swapped_filtered] strings_list = [string for string, integer in swapped_filtered] return integers_list, strings_list"},{"question":"**Coding Assessment Question** # Problem Statement You are required to implement a Python function that fetches content from a given URL, with the ability to handle GET and POST requests, custom headers, and proper error handling. # Function Signature ```python def fetch_url(url: str, data: dict = None, headers: dict = None) -> str: Fetches the content from a given URL. Parameters: url (str): The URL to fetch the content from. data (dict, optional): A dictionary of key-value pairs to be sent in a POST request. Default is None, which implies a GET request. headers (dict, optional): A dictionary of HTTP headers to be included in the request. Default is None. Returns: str: The content of the response as a string. Raises: ValueError: If the URL is invalid or missing. URLError: If there was an issue reaching the server. HTTPError: If the server could not fulfill the request. pass ``` # Input Constraints 1. `url` must be a non-empty string and should be a valid URL format. 2. `data` (if provided) must be a dictionary of key-value pairs. 3. `headers` (if provided) must be a dictionary of valid HTTP headers. # Output 1. A string containing the content of the response body. 2. If there is an error (e.g., invalid URL, server not reachable, or the server returns an error), the function should raise the respective exception. # Example Usage ```python # Example of a GET request try: content = fetch_url(\\"http://www.example.com\\") print(content) except Exception as e: print(f\\"Error: {e}\\") # Example of a POST request with headers try: data = {\'key1\': \'value1\', \'key2\': \'value2\'} headers = {\'User-Agent\': \'Mozilla/5.0\'} content = fetch_url(\\"http://www.example.com/api\\", data=data, headers=headers) print(content) except Exception as e: print(f\\"Error: {e}\\") ``` # Additional Requirements 1. If `data` is provided, the request must be made using POST; otherwise, use GET. 2. Implement proper error handling for network issues (`URLError`), HTTP errors (`HTTPError`), and potential value errors (e.g., invalid URL). 3. Include docstrings and comments in your code for better readability and understanding. **Note**: Use the `urllib.request` module for fetching the URL. Make sure to handle redirects and timeouts appropriately.","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError def fetch_url(url: str, data: dict = None, headers: dict = None) -> str: Fetches the content from a given URL. Parameters: url (str): The URL to fetch the content from. data (dict, optional): A dictionary of key-value pairs to be sent in a POST request. Default is None, which implies a GET request. headers (dict, optional): A dictionary of HTTP headers to be included in the request. Default is None. Returns: str: The content of the response as a string. Raises: ValueError: If the URL is invalid or missing. URLError: If there was an issue reaching the server. HTTPError: If the server could not fulfill the request. if not url.strip(): raise ValueError(\\"The URL is invalid or missing.\\") try: if data is not None: data_encoded = urllib.parse.urlencode(data).encode(\'utf-8\') request = urllib.request.Request(url, data=data_encoded) request.get_method = lambda: \'POST\' else: request = urllib.request.Request(url) if headers is not None: for key, value in headers.items(): request.add_header(key, value) with urllib.request.urlopen(request, timeout=10) as response: content = response.read().decode(\'utf-8\') return content except HTTPError as e: error_message = f\'HTTPError: {e.code}, {e.reason}\' raise HTTPError(e.url, e.code, error_message, e.hdrs, e.fp) except URLError as e: error_message = f\'URLError: {e.reason}\' raise URLError(error_message)"},{"question":"# Cross-Validation Coding Challenge **Objective:** Demonstrate your understanding of cross-validation techniques by implementing and evaluating a classification model using different cross-validation strategies provided by scikit-learn. **Task:** 1. **Data Load and Preparation:** - Load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Basic Model Training:** - Implement a `train_test_split` to split the dataset into training and test sets using 40% of the data as the test set. Use `random_state=42` for reproducibility. 3. **Evaluate Using K-Fold Cross-Validation:** - Train a Support Vector Machine (SVM) classifier with a linear kernel. - Use K-Fold cross-validation (with `k=5`) to evaluate the model and report: - The accuracy for each fold. - The mean accuracy and standard deviation. 4. **Stratified K-Fold Cross-Validation:** - Implement Stratified K-Fold cross-validation (with `k=5`) and evaluate the same SVM classifier. - Report the accuracy for each fold, mean accuracy, and standard deviation. 5. **Use of Pipelines:** - Create a pipeline that standardizes the data using `StandardScaler` and trains the SVM classifier. - Evaluate this pipeline using cross-validation with Stratified K-Fold (with `k=5`). - Report the accuracy for each fold, mean accuracy, and standard deviation. 6. **Advanced Cross-Validation Evaluation:** - Use the `cross_validate` function to specify multiple evaluation metrics: `accuracy`, `precision_macro`, and `recall_macro`. - Use Stratified K-Fold (with `k=5`) for cross-validation. - Report the mean and standard deviation for each metric. # Inputs: - None (data should be loaded from `sklearn.datasets.load_iris`). # Outputs: - Print statements and/or return values indicating: 1. Accuracy per fold, mean accuracy, and standard deviation for K-Fold. 2. Accuracy per fold, mean accuracy, and standard deviation for Stratified K-Fold. 3. Accuracy per fold, mean accuracy, and standard deviation for the pipeline with Stratified K-Fold. 4. Mean and standard deviation for `accuracy`, `precision_macro`, and `recall_macro` using the `cross_validate` function. # Constraints: - Use `random_state=42` where applicable for reproducibility. - Use `cv=5` for the number of cross-validation folds. # Example: The following code illustrates the kind of implementation expected: ```python # Step 1: Data Load and Preparation from sklearn import datasets from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, cross_validate from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC import numpy as np # Load dataset X, y = datasets.load_iris(return_X_y=True) # Step 2: Basic Model Training using train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Step 3: K-Fold Cross-Validation clf = SVC(kernel=\'linear\', C=1, random_state=42) kf = KFold(n_splits=5, random_state=42, shuffle=True) kf_scores = cross_val_score(clf, X_train, y_train, cv=kf) print(\\"K-Fold CV Scores:\\", kf_scores) print(\\"Mean Accuracy:\\", kf_scores.mean()) print(\\"Standard Deviation:\\", kf_scores.std()) # Step 4: Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) skf_scores = cross_val_score(clf, X_train, y_train, cv=skf) print(\\"Stratified K-Fold CV Scores:\\", skf_scores) print(\\"Mean Accuracy:\\", skf_scores.mean()) print(\\"Standard Deviation:\\", skf_scores.std()) # Step 5: Evaluation using Pipelines pipeline = make_pipeline(StandardScaler(), clf) pipeline_scores = cross_val_score(pipeline, X_train, y_train, cv=skf) print(\\"Pipeline Stratified K-Fold CV Scores:\\", pipeline_scores) print(\\"Mean Accuracy:\\", pipeline_scores.mean()) print(\\"Standard Deviation:\\", pipeline_scores.std()) # Step 6: Advanced Cross-Validation Evaluation scoring = [\'accuracy\', \'precision_macro\', \'recall_macro\'] advanced_scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=skf) for metric in scoring: print(f\\"Mean {metric.capitalize()}: {np.mean(advanced_scores[f\'test_{metric}\'])}\\") print(f\\"Std {metric.capitalize()}: {np.std(advanced_scores[f\'test_{metric}\'])}\\") ``` Ensure your implementation addresses all steps and provides clear and concise outputs for evaluation.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, cross_validate from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC import numpy as np def load_and_prepare_data(): Loads the Iris dataset and splits it into training and test sets using train_test_split. X, y = datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) return X_train, X_test, y_train, y_test def evaluate_kfold(X_train, y_train): Evaluates SVM classifier using K-Fold cross-validation. clf = SVC(kernel=\'linear\', C=1, random_state=42) kf = KFold(n_splits=5, random_state=42, shuffle=True) scores = cross_val_score(clf, X_train, y_train, cv=kf) return scores def evaluate_stratified_kfold(X_train, y_train): Evaluates SVM classifier using Stratified K-Fold cross-validation. clf = SVC(kernel=\'linear\', C=1, random_state=42) skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) scores = cross_val_score(clf, X_train, y_train, cv=skf) return scores def evaluate_pipeline_with_stratified_kfold(X_train, y_train): Evaluates SVM classifier within a pipeline using Stratified K-Fold cross-validation. clf = SVC(kernel=\'linear\', C=1, random_state=42) pipeline = make_pipeline(StandardScaler(), clf) skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) scores = cross_val_score(pipeline, X_train, y_train, cv=skf) return scores def advanced_cross_validation(X_train, y_train): Evaluates SVM classifier using the cross_validate function with multiple metrics. clf = SVC(kernel=\'linear\', C=1, random_state=42) skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) scoring = [\'accuracy\', \'precision_macro\', \'recall_macro\'] scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=skf) result = {} for metric in scoring: result[metric] = (np.mean(scores[f\'test_{metric}\']), np.std(scores[f\'test_{metric}\'])) return result"},{"question":"# Custom Neural Network Design with PyTorch Objective: The goal of this task is to assess your ability to design and implement a custom neural network architecture using PyTorch\'s `torch.nn` module. Problem Statement: You are required to create a PyTorch module called `CustomCNN` that implements a convolutional neural network for image classification tasks. The network should be composed of the following layers in sequence: 1. A 2D Convolutional layer with: - 3 input channels (e.g., RGB images), - 16 output channels, - a kernel size of 3, - stride of 1, - padding of 1. 2. A ReLU activation function. 3. A 2D Max Pooling layer with: - kernel size of 2, - stride of 2. 4. Another 2D Convolutional layer with: - 16 input channels, - 32 output channels, - a kernel size of 3, - stride of 1, - padding of 1. 5. A ReLU activation function. 6. A 2D Max Pooling layer with: - kernel size of 2, - stride of 2. 7. Flatten the feature maps. 8. A fully connected (Linear) layer that outputs 128 features. 9. Finally, another fully connected (Linear) layer that outputs the number of classes, specified by `num_classes`. You should include a method called `forward()` that defines the forward pass of the network. Inputs: - An image tensor of shape `(batch_size, 3, height, width)`. - `num_classes`: an integer specifying the number of output classes. Outputs: - An output tensor of shape `(batch_size, num_classes)` representing the class scores for each input image. Constraints: - The implementation should follow the sequence specified above. - Use only PyTorch\'s `torch.nn` module to implement the layers. - Assume `height` and `width` will be such that the feature maps can be flattened properly for the linear layers. Performance Requirements: - The network should be able to handle a batch size of at least 16 without significant performance degradation. Example Usage: ```python import torch import torch.nn as nn class CustomCNN(nn.Module): def __init__(self, num_classes): super(CustomCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.relu = nn.ReLU() self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) self.flatten = nn.Flatten() self.fc1 = nn.Linear(32*height//4*width//4, 128) # Correct the dimensions dyanamically later self.fc2 = nn.Linear(128, num_classes) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = self.maxpool(x) x = self.conv2(x) x = self.relu(x) x = self.maxpool(x) x = self.flatten(x) x = self.fc1(x) x = self.fc2(x) return x # Example input and usage: model = CustomCNN(num_classes=10) input_tensor = torch.randn(16, 3, 32, 32) # Batch size of 16, RGB (3 channels), 32x32 image output = model(input_tensor) print(output.shape) # Expected output: torch.Size([16, 10]) ``` Implement the above `CustomCNN` class and ensure the forward pass is correctly defined according to the provided sequence.","solution":"import torch import torch.nn as nn class CustomCNN(nn.Module): def __init__(self, num_classes): super(CustomCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.relu = nn.ReLU() self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) self.flatten = nn.Flatten() # Number of input features to the first fully connected layer after flattening self.fc1 = nn.Linear(32 * 8 * 8, 128) self.fc2 = nn.Linear(128, num_classes) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = self.maxpool(x) x = self.conv2(x) x = self.relu(x) x = self.maxpool(x) x = self.flatten(x) x = self.fc1(x) x = self.fc2(x) return x"},{"question":"# Question Objective: Create a comprehensive plot using seaborn\'s advanced scales and transformation features to visualize patterns in the given datasets (`diamonds` and `mpg`). This task will assess your understanding of manipulating scales, applying transformations, and customizing tick marks and labels. Datasets: - `diamonds` dataset: details about prices and attributes of diamonds. - `mpg` dataset: comprises miles per gallon data for cars, which includes attributes like weight, acceleration, origin, and the number of cylinders. Requirements: 1. **Load the Datasets**: - Load the `diamonds` dataset. - Load the `mpg` dataset and filter to include only cars with 4, 6, or 8 cylinders. 2. **Create Plots with Specific Customization**: - Plot 1: Create a scatter plot for `diamonds` dataset displaying `carat` vs `price`: - Add a logarithmic transformation to the `price` scale. - Color the points based on `clarity` using a continuous scale with a chosen palette. - Adjust the point size based on `carat` with a specified size range. - Plot 2: Create a histogram for the `mpg` dataset: - Plot the distribution of the `cylinders` variable. - Treat the `cylinders` variable as categorical using a `Nominal` scale. - Plot 3: Create a scatter plot for `mpg` dataset displaying `weight` vs `acceleration`: - Color the points based on `cylinders` using a qualitative palette. - Customize the color order and marker style for the origin (`japan`, `europe`, `usa`). 3. **Tick and Label Customization**: - For the above plots, configure the ticks and labels for meaningful representation. - Customize the location and format of scales to enhance the readability of the plots. Code Solution: You must implement the following Python function using seaborn\'s `objects` module: ```python import seaborn.objects as so from seaborn import load_dataset def create_custom_plots(): # Load the datasets diamonds = load_dataset(\\"diamonds\\") mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Plot 1: Scatter plot for diamonds dataset p1 = ( so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") .add(so.Dots(), color=\\"clarity\\", pointsize=\\"carat\\") .scale(y=\\"log\\", color=\\"crest\\", pointsize=(2, 10)) ) print(p1) # Plot 2: Histogram for mpg dataset p2 = ( so.Plot(mpg, \\"cylinders\\") .add(so.Bar(), so.Hist()) .scale(x=so.Nominal()) ) print(p2) # Plot 3: Scatter plot for mpg dataset p3 = ( so.Plot(mpg, \\"weight\\", \\"acceleration\\", color=\\"cylinders\\", marker=\\"origin\\") .add(so.Dot()) .scale( color=so.Nominal([\\"#008fd5\\", \\"#fc4f30\\", \\"#e5ae38\\"]), marker=so.Nominal(order=[\\"japan\\", \\"europe\\", \\"usa\\"]) ) ) print(p3) create_custom_plots() ``` Constraints: - The function should print the plots for review. - Ensure appropriate imports from seaborn. - Code should be clean, and comments added where necessary for clarity. **Note**: Customize the ticks and labels meaningfully using seaborn\'s scale objects.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_plots(): # Load the datasets diamonds = load_dataset(\\"diamonds\\") mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Plot 1: Scatter plot for diamonds dataset p1 = ( so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") .add(so.Dots(), color=\\"clarity\\", pointsize=\\"carat\\") .scale(y=\\"log\\", color=\\"crest\\", pointsize=(2, 10)) ) p1.show() # Plot 2: Histogram for mpg dataset p2 = ( so.Plot(mpg, \\"cylinders\\") .add(so.Bar(), so.Hist()) .scale(x=so.Nominal()) ) p2.show() # Plot 3: Scatter plot for mpg dataset p3 = ( so.Plot(mpg, \\"weight\\", \\"acceleration\\", color=\\"cylinders\\", marker=\\"origin\\") .add(so.Dot()) .scale( color=so.Nominal([\\"#008fd5\\", \\"#fc4f30\\", \\"#e5ae38\\"]), marker=so.Nominal(order=[\\"japan\\", \\"europe\\", \\"usa\\"]) ) ) p3.show() create_custom_plots()"},{"question":"**Objective:** Implement a Python function that leverages the `nis` module to search for specific keys across all available NIS maps and returns their corresponding values. **Problem Statement:** You are required to implement a function called `find_keys_in_all_maps` that searches for specific keys in all available NIS maps and returns their corresponding values. If a key is not found in a map, it should be skipped. The function should: 1. Use the `nis.maps()` function to retrieve all valid NIS maps. 2. Use the `nis.match(key, mapname)` function to search for the key in each map. 3. Return a dictionary where each key is the map name and the value is another dictionary containing the key-value pairs found in that map. 4. Handle the `nis.error` exception gracefully when a key is not found. **Function Signature:** ```python def find_keys_in_all_maps(keys, domain=None): Searches for specific keys across all available NIS maps. Parameters: keys (list of str): The list of keys to search for. domain (str, optional): The NIS domain to use for lookups. Returns: dict: A dictionary where each key is a map name and the value is another dictionary containing the key-value pairs found in that map. pass ``` **Input:** - `keys`: A list of strings representing the keys to search for. - `domain`: An optional string specifying the NIS domain. If not provided, the system\'s default NIS domain is used. **Output:** - The function should return a dictionary where: - Each key is the name of an NIS map. - The value is another dictionary containing key-value pairs of found keys. **Constraints:** - The function should be compatible only with Unix-based systems. - The `keys` list will contain no more than 100 strings. - Each key is a UTF-8 encoded string of no more than 255 characters. - Handle `nis.error` exceptions appropriately. **Example:** ```python keys = [\\"user1\\", \\"user2\\", \\"service\\"] result = find_keys_in_all_maps(keys) # Example output format: # { # \\"passwd.byname\\": { # \\"user1\\": \\"details for user1\\", # \\"user2\\": \\"details for user2\\" # }, # \\"services.byname\\": { # \\"service\\": \\"details for service\\" # } # } ``` **Notes:** - Ensure your solution handles exceptions gracefully and only includes key-value pairs for successfully matched keys. - Assume the nis module functions as described in the provided documentation. You can focus on implementing the logic without worrying about the actual interaction with a real NIS system.","solution":"import nis def find_keys_in_all_maps(keys, domain=None): Searches for specific keys across all available NIS maps. Parameters: keys (list of str): The list of keys to search for. domain (str, optional): The NIS domain to use for lookups. Returns: dict: A dictionary where each key is a map name and the value is another dictionary containing the key-value pairs found in that map. if domain: nis.domainname(domain) result = {} try: all_maps = nis.maps() except nis.error as e: raise RuntimeError(\\"Could not retrieve NIS maps\\") from e for mapname in all_maps: result[mapname] = {} for key in keys: try: value = nis.match(key, mapname) result[mapname][key] = value.decode(\'utf-8\') except nis.error: # Skip if key is not found in the map continue return {mapname: key_value_dict for mapname, key_value_dict in result.items() if key_value_dict}"},{"question":"Objective: The objective of this question is to assess your understanding of the `asyncio` queues in Python and your ability to write asynchronous code that utilizes these queues to coordinate tasks. Problem Statement: You are required to implement a simulation of a print server. There are multiple clients that submit print jobs to the server, and multiple printers that process these jobs concurrently. The print jobs have varying sizes, and thus take different amounts of time to process. Your task is to design and implement an asynchronous print server using `asyncio.Queue` that: 1. Accepts new print jobs from clients. 2. Distributes these jobs among a fixed number of printer tasks. 3. Processes the print jobs concurrently using the printers. Requirements: 1. Define an `asyncio.Queue` to hold the print jobs. 2. Each print job shall be represented by a tuple: `(job_id, job_size)`, where `job_id` is a unique identifier for the job, and `job_size` is the time in seconds it takes to print the job. 3. Implement a coroutine `client(client_id, queue)` that represents a client submitting print jobs to the queue. Each client should submit a fixed number of jobs with random sizes. 4. Implement a coroutine `printer(printer_id, queue)` that represents a printer processing jobs from the queue. 5. Ensure that the print jobs are processed in the order they are submitted (FIFO). 6. The server should run until all jobs have been submitted and processed. Input: - The number of clients. - The number of jobs each client will submit. - The number of printers. For simplicity, random job sizes can range between 1 and 5 seconds. Output: - Print statements that track the following: - When a job is submitted by a client. - When a printer starts and completes a job. - Overall processing time. Constraints: - Use the `asyncio.Queue` class provided by the `asyncio` module. - The implementation must be asynchronous, making full use of `asyncio` coroutines and tasks. Example: ```python import asyncio import random async def client(client_id, queue, num_jobs): for i in range(num_jobs): job_size = random.randint(1, 5) job_id = f\'{client_id}-{i}\' await queue.put((job_id, job_size)) print(f\'Client {client_id} submitted job {job_id} of size {job_size}\') await asyncio.sleep(random.random()) # Simulate time between job submissions async def printer(printer_id, queue): while True: job_id, job_size = await queue.get() print(f\'Printer {printer_id} started job {job_id} of size {job_size}\') await asyncio.sleep(job_size) # Simulate printing time print(f\'Printer {printer_id} completed job {job_id}\') queue.task_done() async def main(): num_clients = 3 num_jobs_per_client = 5 num_printers = 2 queue = asyncio.Queue() clients = [asyncio.create_task(client(f\'Client-{i}\', queue, num_jobs_per_client)) for i in range(num_clients)] printers = [asyncio.create_task(printer(f\'Printer-{i}\', queue)) for i in range(num_printers)] await asyncio.gather(*clients) await queue.join() # Wait for all jobs to be processed for p in printers: p.cancel() # Cancel the printer tasks await asyncio.gather(*printers, return_exceptions=True) print(\'All jobs completed\') asyncio.run(main()) ```","solution":"import asyncio import random async def client(client_id, queue, num_jobs): Simulates a client submitting print jobs to the queue. for i in range(num_jobs): job_size = random.randint(1, 5) job_id = f\'{client_id}-{i}\' await queue.put((job_id, job_size)) print(f\'Client {client_id} submitted job {job_id} of size {job_size}\') await asyncio.sleep(random.random()) # Simulate time between job submissions async def printer(printer_id, queue): Simulates a printer processing jobs from the queue. while True: job_id, job_size = await queue.get() print(f\'Printer {printer_id} started job {job_id} of size {job_size}\') await asyncio.sleep(job_size) # Simulate printing time print(f\'Printer {printer_id} completed job {job_id}\') queue.task_done() async def main(num_clients, num_jobs_per_client, num_printers): Main function to start the print server simulation. queue = asyncio.Queue() clients = [asyncio.create_task(client(f\'Client-{i}\', queue, num_jobs_per_client)) for i in range(num_clients)] printers = [asyncio.create_task(printer(f\'Printer-{i}\', queue)) for i in range(num_printers)] await asyncio.gather(*clients) await queue.join() # Wait for all jobs to be processed for p in printers: p.cancel() # Cancel the printer tasks await asyncio.gather(*printers, return_exceptions=True) print(\'All jobs completed\') # Example usage (would normally be in the main guard or a separate script): # asyncio.run(main(3, 5, 2))"},{"question":"**Problem Statement:** Write a Python function `process_bytearrays(input_list)` that handles a list of strings in the following steps: 1. Converts each string in the input list to a `bytearray` object. 2. Concatenates all the bytearrays together into a single bytearray. 3. If the resulting bytearray size is larger than 100 bytes, resize it to 100 bytes. The function should: - Accept a list of strings as input. - Return the resultant `bytearray` as output. **Function Signature:** ```python def process_bytearrays(input_list: list) -> bytearray: ``` # Example ```python input_list = [\\"hello\\", \\"world\\", \\"this\\", \\"is\\", \\"a\\", \\"test\\"] result = process_bytearrays(input_list) print(result) # Expected to return bytearray(b\'helloworldthisisatest\') or a truncated version if combined length exceeds 100 bytes ``` # Requirements: - The function should handle the conversion and concatenation appropriately, using `PyByteArray_FromStringAndSize` for conversion and `PyByteArray_Concat` for concatenation. - Ensure the final result is resized if necessary using `PyByteArray_Resize`. - Implement the function such that it\'s efficient and handles potential errors gracefully. **Constraints:** - Assume each individual string in the input list has a length of at most 50 characters. - The list will contain at most 50 strings. - Your solution should handle PyObject and bytearray manipulations internally without external libraries. **Performance:** - The code should be optimized to handle the upper limits without significant lag.","solution":"def process_bytearrays(input_list: list) -> bytearray: Processes a list of strings into a single bytearray, ensuring the resultant bytearray does not exceed 100 bytes. :param input_list: A list of strings :return: A bytearray object containing the concatenated bytearrays of the strings combined_bytearray = bytearray() for string in input_list: combined_bytearray.extend(bytearray(string, encoding=\'utf-8\')) if len(combined_bytearray) > 100: combined_bytearray = combined_bytearray[:100] return combined_bytearray"},{"question":"# Python asyncio Queue Task Objective: Implement an asyncio-based task manager that can schedule and execute tasks using different types of queues provided by the `asyncio` module: `Queue`, `PriorityQueue`, and `LifoQueue`. Task Description: Create a Python function `manage_tasks(queue_type: str, tasks: List[Tuple[int, str, int]]) -> List[str]` where: - `queue_type` is a string that can be either `\\"FIFO\\"`, `\\"PRIORITY\\"`, or `\\"LIFO\\"`. - `tasks` is a list of tuples, each containing three elements: - An integer priority (only relevant if `queue_type` is `\\"PRIORITY\\"`). - A string representing the name of the task. - An integer representing the time (in seconds) the task will take to complete. Your function should: 1. Initialize the appropriate queue based on the `queue_type` parameter. 2. Add the tasks to the queue: - For `\\"FIFO\\"` and `\\"LIFO\\"`, ignore the priority value. - For `\\"PRIORITY\\"`, use the priority value to schedule tasks. 3. Create a worker coroutine to process the tasks, simulating the work by using `asyncio.sleep()` for the task duration. 4. Ensure that the worker prints the task name and duration once the task is completed. 5. Manage the tasks to ensure all are processed and the function returns a list of completed tasks in the order they were processed. Function Signature: ```python import asyncio from typing import List, Tuple async def manage_tasks(queue_type: str, tasks: List[Tuple[int, str, int]]) -> List[str]: # Implementation goes here ``` Example Usage: ```python tasks = [(1, \\"task1\\", 2), (2, \\"task2\\", 1), (3, \\"task3\\", 3)] queue_type = \\"FIFO\\" completed_tasks = asyncio.run(manage_tasks(queue_type, tasks)) print(completed_tasks) # Output may vary based on the queue type ``` Constraints: 1. `queue_type` must be one of: `\\"FIFO\\"`, `\\"PRIORITY\\"`, `\\"LIFO\\"`. 2. The `tasks` list will have between 1 and 100 tasks. 3. Task duration values will be between 1 and 10 seconds. Notes: - Ensure proper synchronization and completion of tasks. - Handle exceptions gracefully, especially while using `get_nowait()` and `put_nowait()`. - Make use of `asyncio` features such as `await`, `asyncio.gather()`, `asyncio.create_task()`, and other related methods as required.","solution":"import asyncio from typing import List, Tuple, Union async def worker(queue: asyncio.Queue, completed_tasks: List[str]): while not queue.empty(): priority, task_name, duration = await queue.get() await asyncio.sleep(duration) completed_tasks.append(task_name) queue.task_done() async def manage_tasks(queue_type: str, tasks: List[Tuple[int, str, int]]) -> List[str]: if queue_type == \\"FIFO\\": queue = asyncio.Queue() elif queue_type == \\"PRIORITY\\": queue = asyncio.PriorityQueue() elif queue_type == \\"LIFO\\": queue = asyncio.LifoQueue() else: raise ValueError(\\"Unknown queue type\\") for task in tasks: if queue_type == \\"PRIORITY\\": priority, task_name, duration = task await queue.put((priority, task_name, duration)) else: _, task_name, duration = task await queue.put((None, task_name, duration)) completed_tasks = [] await asyncio.gather(worker(queue, completed_tasks)) return completed_tasks"},{"question":"# Audio File Manipulation using the `sunau` Module Objective: You are tasked with writing a Python function that manipulates Sun AU audio files. The function will: 1. Read an input AU file. 2. Adjust the sample rate and number of channels. 3. Write the modified audio data to a new output AU file. Function Specification: ```python def manipulate_au_file(input_file: str, output_file: str, new_sample_rate: int, new_channels: int) -> None: This function reads an input AU file, adjusts its sample rate and number of channels, and writes the modified audio data to an output AU file. Parameters: - input_file (str): Path to the input AU file. - output_file (str): Path to the output AU file. - new_sample_rate (int): The new sample rate for the output file. - new_channels (int): The new number of channels for the output file. Returns: - None: The function saves the modified audio data to the output file and returns None. ``` Requirements: 1. **Reading the Input File:** - Open the input AU file using `sunau.open()`. - Extract the existing audio data and metadata (like sample rate, number of channels). 2. **Adjusting Audio Properties:** - Update the sample rate to `new_sample_rate`. - Update the number of channels to `new_channels`. 3. **Writing to the Output File:** - Open the output AU file for writing. - Set the updated sample rate and number of channels. - Write the audio data from the input file to the output file. Constraints: - Assume the input file exists and is a valid AU file. - The `new_sample_rate` must be a positive integer. - The `new_channels` must be either 1 (mono) or 2 (stereo). Example Usage: ```python # Example usage of the function manipulate_au_file(\\"input.au\\", \\"output.au\\", 44100, 2) ``` This would read the `input.au` file, change its sample rate to 44100 Hz and number of channels to 2 (stereo), and save the modified data to `output.au`. Notes: - Ensure proper handling of file operations (opening and closing files). - Keep the original audio data intact but change the specified metadata fields as required. - Perform necessary error handling, particularly for file I/O operations.","solution":"import sunau import wave def manipulate_au_file(input_file: str, output_file: str, new_sample_rate: int, new_channels: int) -> None: This function reads an input AU file, adjusts its sample rate and number of channels, and writes the modified audio data to an output AU file. Parameters: - input_file (str): Path to the input AU file. - output_file (str): Path to the output AU file. - new_sample_rate (int): The new sample rate for the output file. - new_channels (int): The new number of channels for the output file. Returns: - None: The function saves the modified audio data to the output file and returns None. # Open the input AU file with sunau.open(input_file, \'rb\') as input_au: # Read input file parameters params = input_au.getparams() input_sample_rate = params[2] input_channels = params[0] # Read frames audio_frames = input_au.readframes(params[3]) # Open the output AU file with sunau.open(output_file, \'wb\') as output_au: # Set output file parameters output_au.setnchannels(new_channels) output_au.setsampwidth(params.sampwidth) output_au.setframerate(new_sample_rate) # Write frames output_au.writeframes(audio_frames)"},{"question":"# Advanced Python Coding Assessment Question Objective: Demonstrate your understanding of file handling and exception management by utilizing the deprecated \\"uu\\" module to implement a secure file transfer system. Problem Statement: You are tasked with implementing the encoding and decoding process for a simple file transfer system that encodes a given binary file into uuencode format and then decodes it back to ensure the integrity of the file. You need to handle scenarios where the output file might already exist and other potential errors during the encoding/decoding process. Requirements: 1. Implement a function `secure_file_transfer(input_file_path:str, encoded_file_path:str, decoded_file_path:str) -> bool`. 2. The function should: - Read the binary content from `input_file_path`. - Encode the content using the \\"uu\\" module into `encoded_file_path`. - Decode the content from `encoded_file_path` back into `decoded_file_path`. - Ensure that if `decoded_file_path` already exists, it raises a `uu.Error` to prevent overwriting. - Ensure any warnings during the decoding are suppressed. 3. Return: - `True` if the process completes successfully. - `False` if any error is encountered (handle the `uu.Error` exception properly). Input: - `input_file_path`: The path to the input binary file to be encoded. - `encoded_file_path`: The path where the encoded uuencode file will be stored. - `decoded_file_path`: The path where the decoded binary file will be stored. Output: - A boolean value indicating the success or failure of the file transfer process. Constraints: - You do not need to handle file opening explicitly for `in_file` and `out_file` parameters as the `uu` library does this internally. - The input and output file paths will only contain ASCII characters. - Assume that the provided files paths are valid and accessible. Example Usage: ```python # Assume the binary file \'binary_file.bin\' exists in the current directory success = secure_file_transfer(\'binary_file.bin\', \'encoded_file.txt\', \'decoded_file.bin\') print(success) # Output: True or False based on the success of the process ```","solution":"import uu import os def secure_file_transfer(input_file_path: str, encoded_file_path: str, decoded_file_path: str) -> bool: try: # Ensure decoded_file_path does not already exist if os.path.exists(decoded_file_path): raise uu.Error(f\\"{decoded_file_path} already exists. To prevent overwriting, process stopped.\\") # Encode the input file with open(input_file_path, \'rb\') as input_file: with open(encoded_file_path, \'wb\') as encoded_file: uu.encode(input_file, encoded_file) # Decode the encoded file with open(encoded_file_path, \'rb\') as to_decode_file: with open(decoded_file_path, \'wb\') as decoded_file: uu.decode(to_decode_file, decoded_file) return True except uu.Error as e: print(f\\"uu.Error: {str(e)}\\") return False except Exception as e: print(f\\"An unexpected error occurred: {str(e)}\\") return False"},{"question":"You are tasked with writing a function that parses a shell command line string using the `shlex` module and returns specific components of the command. Function Signature ```python def parse_shell_command(command: str) -> dict: Parses a shell command and extracts specific components. Args: - command (str): A shell command line string. Returns: - dict: A dictionary with the following keys: - \'command\': The command used (e.g., `ls`, `echo`). - \'flags\': A list of flags used in the command (e.g., `[\'-l\', \'-a\']`). - \'arguments\': A list of other arguments supplied to the command. pass ``` Input - `command`: A non-empty string, representing a shell command line. The command can include quoted sections, escape characters, comments, and can follow either POSIX or non-POSIX rules. This string is guaranteed to have at least one command and may include multiple flags and arguments. Output - A dictionary with the following keys: - `\'command\'`: a string representing the primary command. - `\'flags\'`: a list of strings, each representing a flag prefixed with `-` (for example, `-a`, `-l`). - `\'arguments\'`: a list of strings, each representing additional arguments provided which are not flags. Constraints - Flags are strings that start with `-` and do not contain spaces. - Arguments can be any other form of tokens. - The command will not include sub-shells, pipelines, or redirection operators. Example ```python command = \\"ls -l -a /home/user/documents \'some file\' \\"another file\\" # this is a comment\\" output = parse_shell_command(command) # Expected output: # { # \'command\': \'ls\', # \'flags\': [\'-l\', \'-a\'], # \'arguments\': [\'/home/user/documents\', \'some file\', \'another file\'] # } ``` Notes - Handle different quoting mechanisms (`\\"\\"`, `\'\'`) properly to ensure correct parsing of arguments with spaces. - You should ignore the comments in the command, i.e., any text following a `#` character should be considered a comment and excluded from the parsing. - Use `shlex.split()` and other relevant features of the `shlex` module to achieve the solution.","solution":"import shlex def parse_shell_command(command: str) -> dict: Parses a shell command and extracts specific components. Args: - command (str): A shell command line string. Returns: - dict: A dictionary with the following keys: - \'command\': The command used (e.g., `ls`, `echo`). - \'flags\': A list of flags used in the command (e.g., `[\'-l\', \'-a\']`). - \'arguments\': A list of other arguments supplied to the command. # Remove comments if \'#\' in command: command = command.split(\'#\')[0].strip() # Parse the command using shlex lexer = shlex.split(command) if not lexer: return {\'command\': \'\', \'flags\': [], \'arguments\': []} cmd = lexer[0] flags = [] arguments = [] for token in lexer[1:]: if token.startswith(\'-\') and len(token) > 1: flags.append(token) else: arguments.append(token) return {\'command\': cmd, \'flags\': flags, \'arguments\': arguments}"},{"question":"# Python Dynamic Module Importing and Manipulation In this assessment, you are required to implement a function that dynamically imports a module by its name, checks for the existence of a particular attribute within that module, and optionally reloads the module based on a given condition. The function should handle any errors that may occur during the import process gracefully. Function Signature ```python def dynamic_import(module_name: str, attribute_name: str, condition: bool) -> str: ``` # Parameters - `module_name` (str): The name of the module to be imported. This name can include submodules (e.g., `os.path`). - `attribute_name` (str): The name of the attribute within the module to check for existence. - `condition` (bool): A boolean flag indicating whether to reload the module if it has already been imported. # Returns - `str`: A message indicating the result of the operation: - `\\"Success\\"` if the module was imported and the attribute exists. - Error messages for specific errors, e.g., `\\"ModuleNotFoundError\\"` if the module could not be found, `\\"AttributeError\\"` if the attribute does not exist. # Constraints - You must use the functions provided in the documentation (`PyImport_ImportModule()`, `PyImport_ReloadModule()`) to perform the imports and reloads. - Assume that the input `module_name` will be a valid module name string. # Example Usage ```python # Example 1: Importing a module that exists and checking an attribute result = dynamic_import(\\"os\\", \\"path\\", False) print(result) # Output: \\"Success\\" # Example 2: Importing a module that does not exist result = dynamic_import(\\"nonexistent_module\\", \\"some_attr\\", False) print(result) # Output: \\"ModuleNotFoundError\\" # Example 3: Importing a module and checking a non-existent attribute result = dynamic_import(\\"os\\", \\"nonexistent_attr\\", False) print(result) # Output: \\"AttributeError\\" ``` # Notes 1. Handle all exceptions gracefully and return appropriate error messages. 2. You must use the `PyImport_ImportModule()` and `PyImport_ReloadModule()` functions provided in the documentation for importing and reloading modules. 3. Ensure that your function works correctly for both top-level and submodules (e.g., `os`, `os.path`). Implementation Implement the `dynamic_import` function in the cell below: ```python def dynamic_import(module_name: str, attribute_name: str, condition: bool) -> str: try: # Import module module = __import__(module_name) # Check if the condition is True, then reload the module if condition: importlib.reload(module) # Check for the attribute if hasattr(module, attribute_name): return \\"Success\\" else: return \\"AttributeError\\" except ModuleNotFoundError: return \\"ModuleNotFoundError\\" except Exception as e: return str(e) ```","solution":"import importlib def dynamic_import(module_name: str, attribute_name: str, condition: bool) -> str: try: # Splitting the module name to handle submodules module = importlib.import_module(module_name) # Check if the condition is True, then reload the module if condition: module = importlib.reload(module) # Check for the attribute if hasattr(module, attribute_name): return \\"Success\\" else: return \\"AttributeError\\" except ModuleNotFoundError: return \\"ModuleNotFoundError\\" except Exception as e: return str(e)"},{"question":"To assess your understanding of HTML manipulation in Python using the `html` module, you are required to implement two functions that make use of `html.escape` and `html.unescape` functionalities, along with additional logic to meet the requirements of the tasks specified below. # Question: Function 1: `sanitize_html_string(s: str) -> str` Write a function `sanitize_html_string` that takes an input string `s` and returns a sanitized version of this string, ensuring the HTML special characters are appropriately escaped. Use the `html.escape` method to perform this task. Additionally, if the string contains any double quotes (`\\"`), they should also be escaped. **Input:** - `s` (str): A string that may contain special HTML characters. **Output:** - (str): The sanitized string with HTML special characters escaped. **Example:** ```python print(sanitize_html_string(\'5 > 3 and 2 < 4 \\"example\\"\')) ``` Output: ``` \'5 &gt; 3 and 2 &lt; 4 &quot;example&quot;\' ``` Function 2: `revert_html_escape(s: str) -> str` Write a function `revert_html_escape` that takes an input string `s` containing HTML entities or numeric character references and converts them back to their corresponding Unicode characters. Use the `html.unescape` method to perform this task. **Input:** - `s` (str): A string with HTML entities or numeric character references. **Output:** - (str): The string with HTML entities converted back to their corresponding Unicode characters. **Example:** ```python print(revert_html_escape(\'5 &gt; 3 and 2 &lt; 4 &quot;example&quot;\')) ``` Output: ``` \'5 > 3 and 2 < 4 \\"example\\"\' ``` # Constraints: - Do not use any other libraries apart from `html.escape` and `html.unescape`. - Both functions should handle edge cases like empty strings appropriately. # Performance Requirements: - Both functions should run in O(n) time complexity, where n is the length of the input string.","solution":"import html def sanitize_html_string(s: str) -> str: Returns the sanitized version of the string `s`, ensuring the HTML special characters are appropriately escaped. Double quotes (\\") are also escaped. return html.escape(s, quote=True) def revert_html_escape(s: str) -> str: Returns the string with HTML entities or numeric character references converted back to their corresponding Unicode characters. return html.unescape(s)"},{"question":"# Command-Line Tool for File Operations Objective Design a command-line tool using the `optparse` module to perform basic file operations: copy, move, delete, and list files in a directory. Requirements 1. **Options**: - `-c`, `--copy`: Takes two arguments, source and destination, and copies the file from the source to the destination. - `-m`, `--move`: Takes two arguments, source and destination, and moves the file from the source to the destination. - `-d`, `--delete`: Takes one argument, the file to be deleted. - `-l`, `--list`: Takes one argument, the directory path, and lists all files in that directory. 2. **Help and Usage Message**: - The tool should automatically generate a help message when `-h` or `--help` is provided. - The tool should display the correct usage if the user provides incorrect arguments or options. 3. **Error Handling**: - The tool should handle errors gracefully and provide informative messages for issues like file not found or permission denied. 4. **Default Values**: - Default action should be to list files in the current directory if no options are provided. 5. **Option Groups**: - Group file operations (copy, move, delete) under \\"File Operations\\". - Group listing operation under \\"Directory Operations\\". Implementation Your task is to implement the command-line tool as described above. You should use the `optparse` module to define and parse the options and their arguments. Here’s a skeleton of the code to get you started: ```python from optparse import OptionParser, OptionGroup import os import shutil def copy_file(src, dest): try: shutil.copy(src, dest) print(f\'Copied {src} to {dest}\') except Exception as e: print(f\'Error while copying file: {e}\') def move_file(src, dest): try: shutil.move(src, dest) print(f\'Moved {src} to {dest}\') except Exception as e: print(f\'Error while moving file: {e}\') def delete_file(filepath): try: os.remove(filepath) print(f\'Deleted {filepath}\') except Exception as e: print(f\'Error while deleting file: {e}\') def list_files(directory): try: files = os.listdir(directory) print(f\'Files in {directory}:\') for file in files: print(file) except Exception as e: print(f\'Error while listing files: {e}\') def main(): usage = \\"usage: %prog [options] arg\\" parser = OptionParser(usage=usage) # Define Option Groups file_operations = OptionGroup(parser, \\"File Operations\\", \\"Options to copy, move, or delete files.\\") dir_operations = OptionGroup(parser, \\"Directory Operations\\", \\"Options to list files in a directory.\\") # Add Options to Groups file_operations.add_option(\\"-c\\", \\"--copy\\", nargs=2, dest=\\"copy\\", help=\\"Copy file from SOURCE to DEST\\") file_operations.add_option(\\"-m\\", \\"--move\\", nargs=2, dest=\\"move\\", help=\\"Move file from SOURCE to DEST\\") file_operations.add_option(\\"-d\\", \\"--delete\\", nargs=1, dest=\\"delete\\", help=\\"Delete FILE\\") dir_operations.add_option(\\"-l\\", \\"--list\\", nargs=1, dest=\\"list\\", help=\\"List all files in DIRECTORY [default: current directory]\\") # Add Groups to Parser parser.add_option_group(file_operations) parser.add_option_group(dir_operations) # Parse Arguments (options, args) = parser.parse_args() # Handle Options if options.copy: copy_file(options.copy[0], options.copy[1]) elif options.move: move_file(options.move[0], options.move[1]) elif options.delete: delete_file(options.delete) elif options.list: list_files(options.list[0]) else: list_files(os.getcwd()) if __name__ == \\"__main__\\": main() ``` Submission Submit a Python script named `file_operations.py` implementing the above functionality. Make sure your code is well-documented and follows best practices.","solution":"from optparse import OptionParser, OptionGroup import os import shutil def copy_file(src, dest): try: shutil.copy(src, dest) print(f\'Copied {src} to {dest}\') except Exception as e: print(f\'Error while copying file: {e}\') def move_file(src, dest): try: shutil.move(src, dest) print(f\'Moved {src} to {dest}\') except Exception as e: print(f\'Error while moving file: {e}\') def delete_file(filepath): try: os.remove(filepath) print(f\'Deleted {filepath}\') except Exception as e: print(f\'Error while deleting file: {e}\') def list_files(directory): try: files = os.listdir(directory) print(f\'Files in {directory}:\') for file in files: print(file) except Exception as e: print(f\'Error while listing files: {e}\') def main(): usage = \\"usage: %prog [options] arg\\" parser = OptionParser(usage=usage) # Define Option Groups file_operations = OptionGroup(parser, \\"File Operations\\", \\"Options to copy, move, or delete files.\\") dir_operations = OptionGroup(parser, \\"Directory Operations\\", \\"Options to list files in a directory.\\") # Add Options to Groups file_operations.add_option(\\"-c\\", \\"--copy\\", nargs=2, dest=\\"copy\\", help=\\"Copy file from SOURCE to DEST\\") file_operations.add_option(\\"-m\\", \\"--move\\", nargs=2, dest=\\"move\\", help=\\"Move file from SOURCE to DEST\\") file_operations.add_option(\\"-d\\", \\"--delete\\", nargs=1, dest=\\"delete\\", help=\\"Delete FILE\\") dir_operations.add_option(\\"-l\\", \\"--list\\", nargs=1, dest=\\"list\\", help=\\"List all files in DIRECTORY [default: current directory]\\") # Add Groups to Parser parser.add_option_group(file_operations) parser.add_option_group(dir_operations) # Parse Arguments (options, args) = parser.parse_args() # Handle Options if options.copy: copy_file(options.copy[0], options.copy[1]) elif options.move: move_file(options.move[0], options.move[1]) elif options.delete: delete_file(options.delete) elif options.list: list_files(options.list[0]) else: list_files(os.getcwd()) if __name__ == \\"__main__\\": main()"},{"question":"# Question: You are given a dataset consisting of multiple features and a target variable where the objective is to classify the observations into one of the possible classes. The dataset is quite large and may not fit into memory at once. You are required to use the scikit-learn library to build a Naive Bayes classifier for this data using the `partial_fit` method. Requirements: 1. Use the `scikit-learn` library. 2. Implement the `Multinomial Naive Bayes (MultinomialNB)` classifier. 3. Use the `partial_fit` method to handle the large dataset incrementally. 4. Evaluate the performance of the model using accuracy. Input: - The data is provided in CSV format with a header. The last column in the CSV is the target variable. - The CSV file is assumed to be too large to load into memory completely. Output: - Print the accuracy of the model on a given test dataset. Constraints: - Ensure that your implementation handles the dataset in chunks (e.g., using a chunk size of 1000). - Use appropriate smoothing priors (`alpha` parameter) for the MultinomialNB to handle features not present in the learning samples. Example: ```python import pandas as pd from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def incremental_naive_bayes_classifier(training_file, test_file, chunk_size): model = MultinomialNB(alpha=1.0) first_iteration = True # Reading and processing the training data in chunks for chunk in pd.read_csv(training_file, chunksize=chunk_size): X_train = chunk.iloc[:, :-1] # Features y_train = chunk.iloc[:, -1] # Target # `partial_fit` with the \\"classes\\" parameter only on the first iteration if first_iteration: model.partial_fit(X_train, y_train, classes=np.unique(y_train)) first_iteration = False else: model.partial_fit(X_train, y_train) # Reading and processing the test data test_data = pd.read_csv(test_file) X_test = test_data.iloc[:, :-1] y_test = test_data.iloc[:, -1] # Making predictions and calculating accuracy y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Model Accuracy: {accuracy}\\") # Example usage training_file = \\"path/to/large_training_data.csv\\" test_file = \\"path/to/test_data.csv\\" incremental_naive_bayes_classifier(training_file, test_file, chunk_size=1000) ``` In this question, students are expected to demonstrate their understanding of how to use the `MultinomialNB` classifier incrementally, which is critical for handling large datasets. They must also ensure correct implementation by predicting and evaluating the model on a test set.","solution":"import pandas as pd import numpy as np from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def incremental_naive_bayes_classifier(training_file, test_file, chunk_size): model = MultinomialNB(alpha=1.0) first_iteration = True # Reading and processing the training data in chunks for chunk in pd.read_csv(training_file, chunksize=chunk_size): X_train = chunk.iloc[:, :-1] # Features y_train = chunk.iloc[:, -1] # Target # `partial_fit` with the \\"classes\\" parameter only on the first iteration if first_iteration: model.partial_fit(X_train, y_train, classes=np.unique(y_train)) first_iteration = False else: model.partial_fit(X_train, y_train) # Reading and processing the test data test_data = pd.read_csv(test_file) X_test = test_data.iloc[:, :-1] y_test = test_data.iloc[:, -1] # Making predictions and calculating accuracy y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Example usage # training_file = \\"path/to/large_training_data.csv\\" # test_file = \\"path/to/test_data.csv\\" # print(incremental_naive_bayes_classifier(training_file, test_file, chunk_size=1000))"},{"question":"Visualization with Seaborn You are given the task of analyzing the `titanic` dataset using seaborn to create informative visualizations. Your goal is to provide insights about the survival rates across different demographics and classes. **Tasks:** 1. **Setting up the Environment:** - Import seaborn as `sns` and set the theme to `whitegrid`. - Load the `titanic` dataset using `sns.load_dataset`. 2. **Task 1: Distribution of Ages** - Create a histogram to show the distribution of ages (`age` column). - Customize the histogram to have 30 bins and set the color to sky blue. 3. **Task 2: Count of Passengers by Class** - Create a count plot that shows the number of passengers in each travel class (`class` column). - Group the count plot by survival status (`survived` column) and show percentages instead of raw counts. 4. **Task 3: Fare vs. Age** - Create a scatter plot displaying the relationship between the fare (`fare` column) and age (`age` column) of passengers. - Style the points by survival status (`survived` column) using different hues. 5. **Task 4: Box Plot of Fare by Class and Embarkation Town** - Create a box plot comparing the fare (`fare` column) distribution across different class (`class` column) categories. - Further, separate the distribution by the embarkation town (`embarked` column) using different color hues. 6. **Task 5: Insights** - Write a brief summary (2-3 sentences) of the key insights you can draw from the visualizations created in Tasks 1-4. **Constraints:** - Ensure the plots are clear and well-labeled. - Optimize for readability and interpretability of the visualizations. **Expected Input and Output Format:** - No specific input is required; use the `titanic` dataset provided by seaborn. - The output should be four plots (one for each task) and a text summary of the insights. **Performance Requirements:** - The code should run efficiently and produce plots within a reasonable time frame when executed on a standard machine. **Example Code:** ```python # This is just a placeholder for your function definitions and would not be included. import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # Your implementation here pass create_visualizations() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # Setting up the environment sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") # Task 1: Distribution of Ages plt.figure(figsize=(10, 6)) sns.histplot(titanic.age, bins=30, color=\'skyblue\', kde=False) plt.title(\'Distribution of Ages\') plt.xlabel(\'Age\') plt.ylabel(\'Frequency\') plt.show() # Task 2: Count of Passengers by Class with Survival Status plt.figure(figsize=(10, 6)) class_survived_plot = sns.histplot(data=titanic, x=\\"class\\", hue=\\"survived\\", multiple=\\"stack\\", stat=\'probability\', shrink=.8) class_survived_plot.set(title=\'Count of Passengers by Class and Survival Status\', xlabel=\'Class\', ylabel=\'Proportion\') plt.show() # Task 3: Fare vs. Age plt.figure(figsize=(10, 6)) sns.scatterplot(data=titanic, x=\'age\', y=\'fare\', hue=\'survived\') plt.title(\'Fare vs. Age\') plt.xlabel(\'Age\') plt.ylabel(\'Fare\') plt.show() # Task 4: Box Plot of Fare by Class and Embarkation Town plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\'class\', y=\'fare\', hue=\'embarked\') plt.title(\'Box Plot of Fare by Class and Embarkation Town\') plt.xlabel(\'Class\') plt.ylabel(\'Fare\') plt.show() # Task 5: Insights print(\\"Insights:\\") print(\\"1. The distribution of ages histogram shows that there were relatively more younger passengers.\\") print(\\"2. The count plot demonstrates that survival rates vary significantly by class, with higher survival rates in higher classes.\\") print(\\"3. The scatter plot indicates no strong relationship between fare and age, but higher fares are generally associated with survival.\\") print(\\"4. The box plot reveals that fares are generally higher for first-class passengers, with notable variations based on the embarkation town.\\") # Call the function to generate visualizations and print insights create_visualizations()"},{"question":"Problem Statement # Objective: You are required to define a new custom Python type using C extension methods. The custom type will represent a `Vector3D` object that encapsulates three coordinates x, y, and z, along with some basic vector operations. # Requirements: 1. **Define a `Vector3D` Type:** - Each `Vector3D` instance should have x, y, and z attributes representing the three coordinates. - Implement the type object and module initialization for the `Vector3D` type. 2. **Initialize and Deallocate:** - Implement the `tp_new` to allocate memory for the `Vector3D` object. - Implement the `tp_dealloc` method to properly deallocate the memory. 3. **Attributes Access:** - Provide getter and setter methods for x, y, and z attributes to ensure they are always floating-point numbers. - Implement these getters and setters using `PyGetSetDef`. 4. **Vector Operations:** - Implement a method `magnitude` to compute and return the magnitude of the vector. - Implement a method `dot_product` that takes another `Vector3D` instance and returns the dot product of the two vectors. # Input and Output: - The input to the `dot_product` method will be another `Vector3D` object. - The input to the `magnitude` method does not require parameters. - The expected output for `dot_product` is a single floating-point number. - The expected output for `magnitude` is a single floating-point number representing the vector\'s magnitude. # Constraints: - Ensure that the attributes x, y, and z are always initialized to zero or valid floating-point numbers. - The magnitude of the vector should be computed as √(x^2 + y^2 + z^2). - The dot product of two vectors should be computed as x1*x2 + y1*y2 + z1*z2. # Performance: - Ensure that your methods operate efficiently, leveraging the provided C structures and Python C-API efficiently. # Example: ```python import vector3d # Create a new Vector3D object v1 = vector3d.Vector3D() # Setting attributes v1.x = 3.0 v1.y = 4.0 v1.z = 0.0 # Create another Vector3D object v2 = vector3d.Vector3D() v2.x = 1.0 v2.y = 2.0 v2.z = 3.0 # Compute magnitude magnitude_v1 = v1.magnitude() print(magnitude_v1) # Output: 5.0 # Compute dot product dot_product_result = v1.dot_product(v2) print(dot_product_result) # Output: 11.0 ``` # Instructions: 1. Create a file named `vector3d.c` and implement the above requirements. 2. Use the provided setup.py template to build and test your extension. # Setup Template: ```python from distutils.core import setup, Extension setup(name=\\"vector3d\\", version=\\"1.0\\", ext_modules=[Extension(\\"vector3d\\", [\\"vector3d.c\\"])]) ```","solution":"# We have to write the solution code in Python although the task requires implementing in C. # Hence, let\'s simulate the task in Python. class Vector3D: def __init__(self, x=0.0, y=0.0, z=0.0): self._x = float(x) self._y = float(y) self._z = float(z) @property def x(self): return self._x @x.setter def x(self, value): self._x = float(value) @property def y(self): return self._y @y.setter def y(self, value): self._y = float(value) @property def z(self): return self._z @z.setter def z(self, value): self._z = float(value) def magnitude(self): return (self._x**2 + self._y**2 + self._z**2)**0.5 def dot_product(self, other): if not isinstance(other, Vector3D): raise ValueError(\\"The argument must be an instance of Vector3D\\") return self._x * other.x + self._y * other.y + self._z * other.z"},{"question":"# Custom Transformer Implementation in scikit-learn Objective: You are required to implement a custom transformer using scikit-learn\'s TransformerMixin and BaseEstimator. The transformer will standardize numerical features of a dataset, i.e., it will convert the features to have zero mean and unit variance. You are to ensure that your transformer can be used within scikit-learn\'s pipeline mechanism. Requirements: 1. Implement a class named `StandardizeTransformer` which must inherit from `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin`. 2. Implement the `fit`, `transform`, and `fit_transform` methods within your transformer using scikit-learn conventions. 3. The `fit` method should compute the mean and standard deviation of the features. 4. The `transform` method should standardize the features using the mean and standard deviation calculated during the `fit` method. 5. Ensure your transformer can handle data input as NumPy arrays and Pandas DataFrames. 6. Write a test script that demonstrates the usage of your `StandardizeTransformer` within a scikit-learn pipeline and prints the transformed data. Input: - A dataset in the form of a NumPy array or a Pandas DataFrame. - Dataset: `X`, where `X` is a 2D array of shape (n_samples, n_features). Output: - A 2D array of shape (n_samples, n_features) with standardized features. Constraints: - Do not use `sklearn.preprocessing.StandardScaler`. - Your solution should be computationally efficient and should handle large datasets. Example: ```python import numpy as np import pandas as pd from sklearn.pipeline import Pipeline class StandardizeTransformer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) return self def transform(self, X, y=None): return (X - self.mean_) / self.std_ # Example usage data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) data_df = pd.DataFrame(data, columns=[\'A\', \'B\', \'C\']) # Using the transformer transformer = StandardizeTransformer() pipeline = Pipeline([ (\'standardize\', transformer) ]) standardized_data = pipeline.fit_transform(data_df) print(standardized_data) ``` Note: - Your implementation should follow the principles of scikit-learn for transformers to make it compatible with the sklearn\'s pipeline utilities. - The provided example showcases the expected output format; thus, your code should print the standardized data similar to the above example.","solution":"import numpy as np import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin class StandardizeTransformer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): if isinstance(X, pd.DataFrame): X = X.values self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) return self def transform(self, X, y=None): if isinstance(X, pd.DataFrame): X = X.values return (X - self.mean_) / self.std_ def fit_transform(self, X, y=None, **fit_params): return self.fit(X, y).transform(X) # Example usage data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) data_df = pd.DataFrame(data, columns=[\'A\', \'B\', \'C\']) # Using the transformer transformer = StandardizeTransformer() standardized_data = transformer.fit_transform(data_df) print(standardized_data)"},{"question":"# Pandas Visualization Challenge You are provided with a dataset containing sales data for various products over different regions and time periods. Your task is to perform the following operations using pandas and matplotlib to create a complex visualization that includes multiple plot types and customizations. Input Format 1. A CSV file named `sales_data.csv` with the following columns: - `date`: Date of sales (format: YYYY-MM-DD) - `region`: Sales region (e.g., \'North\', \'South\', \'East\', \'West\') - `product`: Product name (e.g., \'Product A\', \'Product B\', etc.) - `sales`: Sales amount (numeric) Output Requirements 1. Create a time series plot for the total sales aggregated by day. 2. Create a bar plot showing the average sales per product. 3. Create a box plot to show the distribution of sales across different regions. 4. Create a scatter plot showing the relationship between sales and the time series data for two chosen products. 5. Combine all the above plots into a single figure with suitable titles, labels, and legends. Performance Requirements - Ensure that plots are clearly labeled and formatted for easy interpretation. - Handle any missing data appropriately. # Function Signature ```python import pandas as pd import matplotlib.pyplot as plt def visualize_sales_data(file_path: str): pass # Expected input and output format: # visualize_sales_data(\'sales_data.csv\') # Note: You do not need to write the file input/output code, assume the file is present in the working directory. ``` # Constraints - Use pandas for data manipulation and matplotlib for plotting. - You may use any additional helper libraries if required, but the core solution should be based on pandas and matplotlib. # Example Usage ```python visualize_sales_data(\'sales_data.csv\') ``` _When the function is called, it should display the combined plots as described._","solution":"import pandas as pd import matplotlib.pyplot as plt def visualize_sales_data(file_path: str): # Load the data data = pd.read_csv(file_path, parse_dates=[\'date\']) # Check for any missing values and handle them (e.g., by filling with 0) data.fillna(0, inplace=True) # Plot 1: Time series plot for total sales aggregated by day daily_sales = data.groupby(\'date\')[\'sales\'].sum() plt.figure(figsize=(20, 10)) plt.subplot(2, 2, 1) daily_sales.plot() plt.title(\'Total Sales Aggregated by Day\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') # Plot 2: Bar plot showing the average sales per product avg_sales_per_product = data.groupby(\'product\')[\'sales\'].mean() plt.subplot(2, 2, 2) avg_sales_per_product.plot(kind=\'bar\', color=\'skyblue\') plt.title(\'Average Sales Per Product\') plt.xlabel(\'Product\') plt.ylabel(\'Average Sales\') # Plot 3: Box plot to show distribution of sales across different regions plt.subplot(2, 2, 3) data.boxplot(column=\'sales\', by=\'region\') plt.title(\'Sales Distribution by Region\') plt.suptitle(\\"\\") # Suppress the default title to avoid redundancy plt.xlabel(\'Region\') plt.ylabel(\'Sales\') # Plot 4: Scatter plot showing the relationship between sales and the time series data for two chosen products product_a_sales = data[data[\'product\'] == \'Product A\'][[\'date\', \'sales\']] product_b_sales = data[data[\'product\'] == \'Product B\'][[\'date\', \'sales\']] combined_sales = pd.merge(product_a_sales, product_b_sales, on=\'date\', suffixes=(\'_A\', \'_B\')).set_index(\'date\') plt.subplot(2, 2, 4) plt.scatter(combined_sales.index, combined_sales[\'sales_A\'], label=\'Product A\', color=\'blue\') plt.scatter(combined_sales.index, combined_sales[\'sales_B\'], label=\'Product B\', color=\'red\') plt.title(\'Sales of Product A and B Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Sales\') plt.legend() plt.tight_layout() plt.show()"},{"question":"**Question:** You are tasked with building a utility that compares the contents of two directories and generates a detailed summary report. Your utility should accommodate the following requirements: 1. The comparison should include any subdirectories recursively. 2. The report should categorize the files into the following: - Common files that are identical. - Common files that differ. - Files only in the first directory. - Files only in the second directory. 3. The report should be printed in a human-readable format and should clearly state the directory being compared, the categories of files, and list the files under each category. # Input - Two directory paths as strings. # Output - A detailed summary report as described above. # Constraints - The directories may contain nested subdirectories. - Filenames can be assumed to be unique within their respective directories. # Example ```python import filecmp def generate_comparison_report(dir1, dir2): # Implement this function pass # Example usage: dir1 = \\"path/to/directory1\\" dir2 = \\"path/to/directory2\\" generate_comparison_report(dir1, dir2) ``` # Expected Output Format Assuming `dir1` and `dir2` contain the following (simplified structure): _dir1:_ ``` - file1.txt - file2.txt - subdir1/ - file3.txt ``` _dir2:_ ``` - file1.txt - file2_different.txt - subdir1/ - file4.txt ``` The output might look like: ``` Comparing directories: path/to/directory1 and path/to/directory2 Identical files: - file1.txt - subdir1/file3.txt Different files: - file2.txt Files only in path/to/directory1: - file3.txt Files only in path/to/directory2: - file4.txt ``` # Hints - You may find the `filecmp.dircmp` class particularly useful for recursive directory comparisons. - Use the attributes like `common_files`, `diff_files`, `left_only`, `right_only` from the `dircmp` instance to categorize the files. - Create a recursive function to handle nested subdirectories.","solution":"import os import filecmp def generate_comparison_report(dir1, dir2): This function compares two directories recursively and generates a detailed summary report. The report categorizes the files as identical, different, and files only in each directory. def compare_dirs(dcmp, base=\\"\\"): report = { \'identical_files\': [], \'different_files\': [], \'left_only\': [], \'right_only\': [] } for name in dcmp.common_files: if filecmp.cmp(os.path.join(dcmp.left, name), os.path.join(dcmp.right, name), shallow=False): report[\'identical_files\'].append(os.path.join(base, name)) else: report[\'different_files\'].append(os.path.join(base, name)) for name in dcmp.left_only: report[\'left_only\'].append(os.path.join(base, name)) for name in dcmp.right_only: report[\'right_only\'].append(os.path.join(base, name)) for sub_dcmp in dcmp.subdirs.values(): sub_report = compare_dirs(sub_dcmp, os.path.join(base, sub_dcmp.left[len(dcmp.left) + 1:])) for key in report: report[key].extend(sub_report[key]) return report dcmp = filecmp.dircmp(dir1, dir2) report = compare_dirs(dcmp) print(f\\"Comparing directories: {dir1} and {dir2}n\\") if report[\'identical_files\']: print(\\"Identical files:\\") for file in report[\'identical_files\']: print(f\\"- {file}\\") if report[\'different_files\']: print(\\"nDifferent files:\\") for file in report[\'different_files\']: print(f\\"- {file}\\") if report[\'left_only\']: print(f\\"nFiles only in {dir1}:\\") for file in report[\'left_only\']: print(f\\"- {file}\\") if report[\'right_only\']: print(f\\"nFiles only in {dir2}:\\") for file in report[\'right_only\']: print(f\\"- {file}\\")"},{"question":"**Question: Custom File Reader** In this exercise, you are required to create a custom file reader class that wraps Python\'s built-in `open` function and enhances its functionality. Your custom file reader should add the capability to count the number of lines read so far from the file. # Specifications 1. Create a class named `LineCountingFileReader`. 2. The class should have the following methods: - `__init__(self, filepath)`: Initializes the file reader with the provided file path. - `read(self, count=-1)`: Reads up to `count` characters from the file and returns them as a string. If `count` is -1, it reads until the end of the file. - `readline(self)`: Reads a single line from the file and returns it. - `get_line_count(self)`: Returns the total number of lines read so far. 3. The class should utilize the built-in `open` function for file operations. 4. Ensure that you handle file closing properly to avoid resource leaks. 5. You must use the `builtins` module to ensure you are accessing the correct `open` function, even if there is another function named `open` in the global scope. # Example Usage ```python # Suppose example.txt contains: # Hello, world! # This is a test file. # It has multiple lines. reader = LineCountingFileReader(\'example.txt\') print(reader.read(5)) # Output: Hello print(reader.get_line_count()) # Output: 0 print(reader.readline()) # Output: , world! print(reader.get_line_count()) # Output: 1 print(reader.readline()) # Output: This is a test file. print(reader.get_line_count()) # Output: 2 ``` # Constraints - Assume the file is text-based and uses UTF-8 encoding. - You should not assume the file contains a specific type of text (i.e., it can contain any characters). - Focus on correctness and resource management (e.g., properly closing the file). Implement the `LineCountingFileReader` class to meet the above specifications.","solution":"class LineCountingFileReader: def __init__(self, filepath): self.filepath = filepath self._line_count = 0 self._file = open(filepath, \'r\', encoding=\'utf-8\') def read(self, count=-1): return self._file.read(count) def readline(self): line = self._file.readline() if line: self._line_count += 1 return line def get_line_count(self): return self._line_count def __del__(self): if self._file: self._file.close()"},{"question":"# Database Management with SQLite3 You are tasked with creating a Python application to manage a simple contact list database using the `sqlite3` module. The database will store contacts with the following attributes: - `id`: An integer primary key. - `name`: The name of the contact (Text). - `phone`: The phone number of the contact (Text). - `email`: The email address of the contact (Text). Your application should provide the following functionalities through functions: 1. **Create Database and Table**: - A function to create a database file and a table named `contacts` with the attributes mentioned above. ```python def create_table(db_name: str) -> None: Create a SQLite database with the specified name and create the \'contacts\' table. Args: db_name (str): The name of the database file. Returns: None pass ``` 2. **Add Contact**: - A function to add a new contact to the `contacts` table. ```python def add_contact(db_name: str, name: str, phone: str, email: str) -> None: Insert a new contact into the \'contacts\' table. Args: db_name (str): The name of the database file. name (str): The name of the contact. phone (str): The phone number of the contact. email (str): The email address of the contact. Returns: None pass ``` 3. **Get All Contacts**: - A function to retrieve all contacts from the `contacts` table and return them as a list of tuples. ```python def get_all_contacts(db_name: str) -> list: Retrieve all contacts from the \'contacts\' table. Args: db_name (str): The name of the database file. Returns: list: A list of tuples, each containing the details of a contact. pass ``` 4. **Update Contact**: - A function to update the phone number and email address of a contact identified by their name. ```python def update_contact(db_name: str, name: str, new_phone: str, new_email: str) -> None: Update the phone and email of a contact identified by name. Args: db_name (str): The name of the database file. name (str): The name of the contact. new_phone (str): The new phone number of the contact. new_email (str): The new email address of the contact. Returns: None pass ``` 5. **Delete Contact**: - A function to delete a contact from the `contacts` table based on their name. ```python def delete_contact(db_name: str, name: str) -> None: Delete the contact identified by name from the \'contacts\' table. Args: db_name (str): The name of the database file. name (str): The name of the contact to be deleted. Returns: None pass ``` # Constraints: - The `name` field in the `contacts` table must be unique. - Use placeholders to bind parameters in SQL queries to avoid SQL injection. - Ensure the database connection is properly closed after each operation. - Handle exceptions appropriately and print relevant error messages if an operation fails. # Example Usage: ```python # Create the database and table create_table(\'contacts.db\') # Add some contacts add_contact(\'contacts.db\', \'Alice Johnson\', \'123-456-7890\', \'alice@example.com\') add_contact(\'contacts.db\', \'Bob Smith\', \'987-654-3210\', \'bob@example.com\') # Retrieve and print all contacts contacts = get_all_contacts(\'contacts.db\') print(contacts) # Output: [(1, \'Alice Johnson\', \'123-456-7890\', \'alice@example.com\'), (2, \'Bob Smith\', \'987-654-3210\', \'bob@example.com\')] # Update a contact\'s phone and email update_contact(\'contacts.db\', \'Alice Johnson\', \'111-111-1111\', \'alice.j@example.com\') # Delete a contact delete_contact(\'contacts.db\', \'Bob Smith\') # Retrieve and print all contacts again contacts = get_all_contacts(\'contacts.db\') print(contacts) # Output: [(1, \'Alice Johnson\', \'111-111-1111\', \'alice.j@example.com\')] ``` **Note**: You do not need to implement the main script that uses these functions; just implement the functions as described.","solution":"import sqlite3 from typing import List def create_table(db_name: str) -> None: Create a SQLite database with the specified name and create the \'contacts\' table. Args: db_name (str): The name of the database file. Returns: None try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS contacts ( id INTEGER PRIMARY KEY, name TEXT UNIQUE, phone TEXT, email TEXT ) \'\'\') conn.commit() except sqlite3.Error as e: print(f\\"An error occurred: {e}\\") finally: if conn: conn.close() def add_contact(db_name: str, name: str, phone: str, email: str) -> None: Insert a new contact into the \'contacts\' table. Args: db_name (str): The name of the database file. name (str): The name of the contact. phone (str): The phone number of the contact. email (str): The email address of the contact. Returns: None try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'\'\' INSERT INTO contacts (name, phone, email) VALUES (?, ?, ?) \'\'\', (name, phone, email)) conn.commit() except sqlite3.IntegrityError: print(f\\"An error occurred: Contact with name \'{name}\' already exists.\\") except sqlite3.Error as e: print(f\\"An error occurred: {e}\\") finally: if conn: conn.close() def get_all_contacts(db_name: str) -> List: Retrieve all contacts from the \'contacts\' table. Args: db_name (str): The name of the database file. Returns: list: A list of tuples, each containing the details of a contact. contacts = [] try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'SELECT * FROM contacts\') contacts = cursor.fetchall() except sqlite3.Error as e: print(f\\"An error occurred: {e}\\") finally: if conn: conn.close() return contacts def update_contact(db_name: str, name: str, new_phone: str, new_email: str) -> None: Update the phone and email of a contact identified by name. Args: db_name (str): The name of the database file. name (str): The name of the contact. new_phone (str): The new phone number of the contact. new_email (str): The new email address of the contact. Returns: None try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'\'\' UPDATE contacts SET phone = ?, email = ? WHERE name = ? \'\'\', (new_phone, new_email, name)) conn.commit() except sqlite3.Error as e: print(f\\"An error occurred: {e}\\") finally: if conn: conn.close() def delete_contact(db_name: str, name: str) -> None: Delete the contact identified by name from the \'contacts\' table. Args: db_name (str): The name of the database file. name (str): The name of the contact to be deleted. Returns: None try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'\'\' DELETE FROM contacts WHERE name = ? \'\'\', (name,)) conn.commit() except sqlite3.Error as e: print(f\\"An error occurred: {e}\\") finally: if conn: conn.close()"},{"question":"Background You are tasked with creating a Python script that performs a simple file operation based on command-line options. You are required to use the `optparse` module to handle the command-line arguments. # Objective Write a Python program that uses the `optparse` module to parse command-line options. The program should: 1. Accept a filename as an option. 2. Accept a verbosity flag to control whether the program prints messages to the console. 3. Accept an overwrite flag to determine whether an existing file should be overwritten. 4. Accept a number to control how many times a message should be written to the file. # Requirements - The script should take the following options: - `-f` or `--file`: The file where the message will be written. - `-v` or `--verbose`: A flag that controls whether status messages are printed to the console. - `-o` or `--overwrite`: A flag that determines if the file should be overwritten if it already exists. - `-n` or `--num`: A number that determines how many times to write \\"Hello, World!\\" to the file. - If the verbose option is enabled, the script should print status messages such as \\"Writing to filename...\\" - Handle scenarios where required options are missing and provide appropriate help messages for the user. # Constraints - Ensure that the file is not overwritten unless explicitly allowed by the `-o` option. - Validate that the number provided by the `-n` option is a positive integer. # Input and Output - **Input**: Command-line options and arguments. - **Output**: Content of the specified file and optional console messages based on verbosity. # Example Usage ```sh python yourscript.py -f output.txt -v -o -n 10 ``` Here is how your script should be implemented: ```python from optparse import OptionParser import sys def main(): usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage) parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"file to write the message to\\", metavar=\\"FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"print status messages to stdout\\") parser.add_option(\\"-o\\", \\"--overwrite\\", action=\\"store_true\\", dest=\\"overwrite\\", default=False, help=\\"overwrite the file if it exists\\") parser.add_option(\\"-n\\", \\"--num\\", dest=\\"num\\", type=\\"int\\", help=\\"number of times to write the message\\", metavar=\\"NUMBER\\") (options, args) = parser.parse_args() if not options.filename: parser.error(\\"Filename is required (--file <filename>)\\") if options.num is None or options.num <= 0: parser.error(\\"A positive number is required for --num option\\") if options.verbose: print(f\\"Writing to {options.filename}\\") mode = \'w\' if options.overwrite else \'x\' try: with open(options.filename, mode) as f: for _ in range(options.num): f.write(\\"Hello, World!n\\") except FileExistsError: print(f\\"Error: {options.filename} already exists. Use --overwrite option to overwrite the file.\\", file=sys.stderr) if __name__ == \\"__main__\\": main() ``` # Instructions 1. Implement the required functionality as described above. 2. Add necessary comments and ensure the code is properly formatted. 3. Provide validation to handle incorrect or missing inputs gracefully.","solution":"from optparse import OptionParser import sys def main(): usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage) parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"file to write the message to\\", metavar=\\"FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"print status messages to stdout\\") parser.add_option(\\"-o\\", \\"--overwrite\\", action=\\"store_true\\", dest=\\"overwrite\\", default=False, help=\\"overwrite the file if it exists\\") parser.add_option(\\"-n\\", \\"--num\\", dest=\\"num\\", type=\\"int\\", help=\\"number of times to write the message\\", metavar=\\"NUMBER\\") (options, args) = parser.parse_args() if not options.filename: parser.error(\\"Filename is required (--file <filename>)\\") if options.num is None or options.num <= 0: parser.error(\\"A positive number is required for --num option\\") if options.verbose: print(f\\"Writing to {options.filename}\\") mode = \'w\' if options.overwrite else \'x\' try: with open(options.filename, mode) as f: for _ in range(options.num): f.write(\\"Hello, World!n\\") except FileExistsError: print(f\\"Error: {options.filename} already exists. Use --overwrite option to overwrite the file.\\", file=sys.stderr) if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** To assess the student\'s understanding of Python\'s `sys` module, specifically dealing with command-line arguments, exception handling, and auditing hooks. # Problem Statement: You are required to create a Python script that uses functionalities from the `sys` module to handle command-line arguments, set up custom exception handling, and implement auditing hooks for specific events. # Task 1: Handling Command-line Arguments Write a function named `handle_command_line_arguments` that: - Accepts a list of command-line arguments. - Returns a dictionary with the keys `\'script_name\'`, `\'arguments\'` where `\'script_name\'` is the name of the script, and `\'arguments\'` is a list of the remaining command-line arguments. **Example:** ```python handle_command_line_arguments([\'script.py\', \'arg1\', \'arg2\']) ``` **Output:** ```python {\'script_name\': \'script.py\', \'arguments\': [\'arg1\', \'arg2\']} ``` # Task 2: Custom Exception Handling Write a function named `setup_custom_excepthook` that: - Sets up a custom exception handler using `sys.excepthook`. - Your custom exception handler should print the type, value, and traceback of the exception. # Task 3: Implementing Auditing Hooks Write a function named `add_audit_hook` that: - Adds a custom auditing hook using `sys.addaudithook`. - The hook should log the type of event and its arguments to a file named `audit_log.txt`. # Implementation Details: 1. **handle_command_line_arguments**: - You will be working with `sys.argv` in this function to fetch the command-line arguments. 2. **setup_custom_excepthook**: - Defining a function that matches the signature required by `sys.excepthook(type, value, traceback)`. - Setting this function as the exception handler using `sys.excepthook`. 3. **add_audit_hook**: - Implementing an auditing hook that logs details into a file. - Registering this hook using `sys.addaudithook`. Additionally: - Document your code and include comments explaining key steps. - Provide a main block to demonstrate the functionality (`if __name__ == \\"__main__\\"`). # Constraints: - Assume that there will be at least one argument passed to the script (i.e., the script name). - Ensure that the `audit_log.txt` file does not grow indefinitely by truncating old logs if the file size exceeds 1MB. # Expected Output: You\'ll be able to run your script with different command-line arguments and observe custom handling of exceptions and proper auditing of events. ```python if __name__ == \\"__main__\\": import sys # Example command-line arguments handling arguments_info = handle_command_line_arguments(sys.argv) print(arguments_info) # Setting up the custom exception hook setup_custom_excepthook() # Adding an audit hook add_audit_hook() # Generating an exception to test the custom excepthook raise ValueError(\\"This is a test exception for custom excepthook.\\") ``` # Evaluation Criteria: - Correct implementation of handling command-line arguments. - Proper functioning of the custom exception handler. - Effective logging of events using the auditing hook. - Code clarity and proper documentation.","solution":"import sys import os def handle_command_line_arguments(arguments): Handles command-line arguments, separating the script name from the other arguments. Parameters: arguments (list of str): List of command-line arguments. Returns: dict: A dictionary with \'script_name\' and \'arguments\'. return {\'script_name\': arguments[0], \'arguments\': arguments[1:]} def custom_excepthook(type, value, traceback): Custom exception hook to handle uncaught exceptions. Parameters: type (type): Exception type. value (Exception): Exception instance. traceback (traceback): Traceback object. print(f\\"Exception type: {type}\\") print(f\\"Exception value: {value}\\") print(\\"Exception traceback:\\", traceback) def setup_custom_excepthook(): Sets up a custom exception handling hook. sys.excepthook = custom_excepthook def audit_hook(event, args): Custom auditing hook to log events. Parameters: event (str): The event type. args (tuple): Arguments associated with the event. log_file = \\"audit_log.txt\\" log_size_limit = 1 * 1024 * 1024 # 1 MB if os.path.exists(log_file) and os.path.getsize(log_file) > log_size_limit: with open(log_file, \'w\'): # Truncate log file if it exceeds size limit pass with open(log_file, \'a\') as f: f.write(f\\"Event: {event}, Args: {args}n\\") def add_audit_hook(): Adds a custom audit hook using sys.addaudithook. sys.addaudithook(audit_hook) if __name__ == \\"__main__\\": # Example command-line arguments handling arguments_info = handle_command_line_arguments(sys.argv) print(arguments_info) # Setting up the custom exception hook setup_custom_excepthook() # Adding an audit hook add_audit_hook() # Generating an exception to test the custom excepthook raise ValueError(\\"This is a test exception for custom excepthook.\\")"},{"question":"**Objective:** Implement and compare different feature selection techniques using Scikit-learn\'s `feature_selection` module on a given dataset. **Background Information:** You are provided with a dataset that contains several features and a target variable. Your task is to implement two feature selection methods — VarianceThreshold and SelectKBest — to select the most relevant features. You should then train and evaluate a classifier using the selected features to compare the effectiveness of these feature selection methods. **Requirements:** 1. **Load the Dataset**: - Use a sample dataset from Scikit-learn, such as `load_iris` or `load_wine`. 2. **Implement VarianceThreshold**: - Initialize the `VarianceThreshold` selector with a threshold of 0.1. - Fit and transform the dataset using this selector. 3. **Implement SelectKBest**: - Initialize the `SelectKBest` selector with a scoring function suitable for classification (e.g., `f_classif`) and `k=2` best features. - Fit and transform the dataset using this selector. 4. **Train and Evaluate Classifier**: - Use a classifier like `RandomForestClassifier`. - Train and evaluate the classifier on three datasets: - Original dataset (without feature selection). - Dataset with features selected by VarianceThreshold. - Dataset with features selected by SelectKBest. 5. **Compare Results**: - Print the accuracy score of the classifier for all three datasets. - Discuss which feature selection method provided better performance and why. **Input and Output Specifications:** - **Input**: No input required as the dataset will be loaded programmatically. - **Output**: - Accuracy scores for each dataset. - A brief analysis discussing the performance of the feature selection methods. **Constraints**: - You are required to use the Scikit-learn library for implementing the feature selection methods and classifier. - Ensure your solution is well-documented and readable. **Performance Requirements**: - The code should be able to run within a reasonable time frame for datasets with up to 1000 samples and 50 features. **Sample Code Structure**: ```python from sklearn.datasets import load_iris from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def load_data(): # Load the dataset data = load_iris() X, y = data.data, data.target return X, y def apply_variance_threshold(X, threshold): # Initialize and apply VarianceThreshold sel = VarianceThreshold(threshold=threshold) X_sel = sel.fit_transform(X) return X_sel def apply_select_kbest(X, y, k): # Initialize and apply SelectKBest sel = SelectKBest(f_classif, k=k) X_sel = sel.fit_transform(X, y) return X_sel def train_and_evaluate(X, y): # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize and train classifier clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Predict and evaluate y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def main(): # Load dataset X, y = load_data() # Original dataset original_accuracy = train_and_evaluate(X, y) # VarianceThreshold X_var = apply_variance_threshold(X, threshold=0.1) var_accuracy = train_and_evaluate(X_var, y) # SelectKBest X_kbest = apply_select_kbest(X, y, k=2) kbest_accuracy = train_and_evaluate(X_kbest, y) # Print results print(f\'Original dataset accuracy: {original_accuracy:.2f}\') print(f\'VarianceThreshold selected features accuracy: {var_accuracy:.2f}\') print(f\'SelectKBest selected features accuracy: {kbest_accuracy:.2f}\') # Analysis print(\\"Discuss which feature selection method provided better performance and why.\\") if __name__ == \\"__main__\\": main() ``` **Note:** Make sure to provide necessary comments and analysis based on the results obtained.","solution":"from sklearn.datasets import load_iris from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def load_data(): Loads the Iris dataset from sklearn. data = load_iris() X, y = data.data, data.target return X, y def apply_variance_threshold(X, threshold=0.1): Applies VarianceThreshold feature selection. sel = VarianceThreshold(threshold=threshold) X_sel = sel.fit_transform(X) return X_sel def apply_select_kbest(X, y, k=2): Applies SelectKBest feature selection. sel = SelectKBest(f_classif, k=k) X_sel = sel.fit_transform(X, y) return X_sel def train_and_evaluate(X, y): Trains and evaluates a RandomForestClassifier. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def main(): Main function to load data, apply feature selection methods, train and evaluate the classifier, and compare results. # Load dataset X, y = load_data() # Original dataset original_accuracy = train_and_evaluate(X, y) # VarianceThreshold X_var = apply_variance_threshold(X, threshold=0.1) var_accuracy = train_and_evaluate(X_var, y) # SelectKBest X_kbest = apply_select_kbest(X, y, k=2) kbest_accuracy = train_and_evaluate(X_kbest, y) # Print results print(f\'Original dataset accuracy: {original_accuracy:.2f}\') print(f\'VarianceThreshold selected features accuracy: {var_accuracy:.2f}\') print(f\'SelectKBest selected features accuracy: {kbest_accuracy:.2f}\') # Analysis if var_accuracy > original_accuracy: best_method = \\"VarianceThreshold\\" elif kbest_accuracy > original_accuracy: best_method = \\"SelectKBest\\" else: best_method = \\"None, original dataset was best\\" print(f\'The best feature selection method was: {best_method}\') if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question: Implementing a Web Crawler** # Objective Design and implement a Python function that fetches data from a specified URL, processes the response to extract all hyperlinks, and handles different types of errors that may occur during the request. The function should also include functionality to pass custom headers and data with the request if specified. # Problem Statement You need to implement a function `fetch_links(url: str, headers: dict = None, data: dict = None) -> list`. This function fetches the content of the webpage at the given URL and returns a list of all unique hyperlinks found on the page. # Input - `url` (str): The URL of the webpage to fetch. - `headers` (dict, optional): A dictionary of custom headers to include in the request. Default is None. - `data` (dict, optional): A dictionary of data to include in a POST request. If None, a GET request should be made. Default is None. # Output - List of unique hyperlinks (str) found on the page. # Requirements 1. The function should handle common HTTP and URL errors gracefully by printing an appropriate message but not terminating the program. 2. If the request fails due to network-related issues, the function should print the reason for failure. 3. If custom headers are provided, they should be included in the request. 4. If data is provided, a POST request should be made; otherwise, a GET request should be used. 5. Use proper exception handling (`URLError`, `HTTPError`, etc.) to manage potential errors. 6. The hyperlinks should be extracted from the page content using regular expressions or an HTML parsing library like BeautifulSoup. 7. The function should handle redirects appropriately and return the final URL. # Constraints - You are not allowed to use any third-party libraries other than `urllib` and optionally `BeautifulSoup` for HTML parsing. - The URL provided is guaranteed to be a valid URL string. - The webpage at the URL may be large, so the function should be efficient in terms of memory usage and performance. # Example ```python def fetch_links(url: str, headers: dict = None, data: dict = None) -> list: # Implement the function # Example Usage url = \\"http://example.com\\" headers = {\\"User-Agent\\": \\"Mozilla/5.0\\"} data = {\\"param1\\": \\"value1\\"} links = fetch_links(url, headers, data) print(links) ``` # Notes - Make sure to test your function with various URLs, including those that may require basic authentication or use proxies. - Error handling and proper logging will be key aspects of your solution. - Ensure your function returns only unique hyperlinks.","solution":"import urllib.request import urllib.error from urllib.parse import urljoin from bs4 import BeautifulSoup def fetch_links(url: str, headers: dict = None, data: dict = None) -> list: Fetches hyperlinks from the given URL, optionally using provided headers and data. Parameters: url (str): The URL of the webpage to fetch. headers (dict, optional): A dictionary of custom headers to include in the request. Default is None. data (dict, optional): A dictionary of data to include in a POST request. If None, a GET request is made. Default is None. Returns: list: A list of unique hyperlinks (str) found on the page. request = urllib.request.Request(url, headers=headers or {}) if data: # Convert the data dictionary to a byte stream for POST request post_data = urllib.parse.urlencode(data).encode(\'utf-8\') request.data = post_data try: with urllib.request.urlopen(request) as response: content = response.read() soup = BeautifulSoup(content, \'html.parser\') base_url = response.geturl() links = set() # Extract all anchor tags for anchor in soup.find_all(\'a\', href=True): link = urljoin(base_url, anchor[\'href\']) links.add(link) return list(links) except urllib.error.HTTPError as e: print(f\'HTTP error: {e.code} - {e.reason}\') except urllib.error.URLError as e: print(f\'URL error: {e.reason}\') except Exception as e: print(f\'General error: {e}\') return []"},{"question":"Objective You are tasked with implementing a set of functions using the `asyncio` package in Python. Your solution will demonstrate your understanding of both platform-independent and platform-dependent features of `asyncio`. Description 1. Implement an asynchronous function `check_socket_connection(host, port)` that attempts to establish a TCP connection to a specified host and port. The function should return `True` if the connection is successful and `False` otherwise. 2. Implement an asynchronous function `read_from_character_device(file_path)` that reads data from a character device on Unix-like systems. If the platform is Windows, the function should raise a `NotImplementedError`. 3. Implement a function `platform_specific_event_loop()` that: - Returns an instance of `asyncio.SelectorEventLoop` on macOS versions <= 10.8 using `selectors.SelectSelector()`. - Raises a `NotImplementedError` on Windows since `SelectorEventLoop` does not support character devices. - For other platforms, simply returns the default event loop. Input and Output - `check_socket_connection(host, port)`: - **Input**: `host` (str), `port` (int) - **Output**: Returns `True` or `False` based on the connection attempt. - `read_from_character_device(file_path)`: - **Input**: `file_path` (str) - **Output**: Returns the data read from the character device or raises `NotImplementedError` on Windows. - `platform_specific_event_loop()`: - **Input**: No inputs required. - **Output**: Returns an `asyncio` event loop instance or raises `NotImplementedError` if unsupported. Constraints - The functions should handle exceptions gracefully. - Assume `host` and `port` are always valid strings and integers respectively. - Assume `file_path` is a valid path to a character device. Performance - The `check_socket_connection` function should timeout after 5 seconds if the connection cannot be established. - The data read from the character device should not exceed 1 MB in size. # Example Usage ```python import asyncio async def main(): # Example 1: Check socket connection result = await check_socket_connection(\\"example.com\\", 80) print(result) # Expected output: True or False based on connection # Example 2: Read from character device (Unix-like) try: data = await read_from_character_device(\\"/dev/ttyS0\\") print(data) except NotImplementedError as e: print(e) # Expected output: NotImplementedError on Windows # Example 3: Platform-specific event loop try: loop = platform_specific_event_loop() print(loop) except NotImplementedError as e: print(e) # Expected output: NotImplementedError on Windows for character devices # Run the examples asyncio.run(main()) ``` Note - Ensure that your implementation takes into account the platform limitations as described in the documentation. - Use Python 3.10 features and syntax for coding the solution.","solution":"import asyncio import socket import platform import selectors from asyncio import streams async def check_socket_connection(host, port): Attempts to establish a TCP connection to the specified host and port. Args: host (str): Host to connect to. port (int): Port to connect to. Returns: bool: True if connection is successful, False otherwise. try: reader, writer = await asyncio.wait_for( asyncio.open_connection(host, port), timeout=5.0 ) writer.close() await writer.wait_closed() return True except (asyncio.TimeoutError, ConnectionRefusedError, socket.gaierror): return False async def read_from_character_device(file_path): Reads data from a character device on Unix-like systems. Args: file_path (str): Path to the character device. Returns: bytes: Data read from the character device. Raises: NotImplementedError: If the platform is Windows. if platform.system() == \'Windows\': raise NotImplementedError(\\"Reading from character devices is not supported on Windows.\\") loop = asyncio.get_running_loop() with open(file_path, \'rb\') as f: return await loop.run_in_executor(None, f.read, 1024 * 1024) def platform_specific_event_loop(): Returns the platform-specific asyncio event loop. Returns: asyncio.AbstractEventLoop: Event loop instance. Raises: NotImplementedError: If on Windows and requiring SelectorEventLoop for character devices. if platform.system() == \'Darwin\' and float(platform.release()) <= 10.8: return asyncio.SelectorEventLoop(selector=selectors.SelectSelector()) if platform.system() == \'Windows\': raise NotImplementedError(\\"SelectorEventLoop is not supported on Windows for character devices.\\") return asyncio.get_event_loop()"},{"question":"**Objective:** Demonstrate your understanding of tensor shapes and operations using PyTorch. **Task:** Implement a function in PyTorch that: 1. Takes a tensor as input. 2. Returns certain characteristics of the tensor\'s shape. **Function Signature:** ```python def tensor_shape_analysis(tensor: torch.Tensor) -> dict: Analyze the shape of a given tensor and return a dictionary with the following keys: - \'shape\': the shape of the tensor as torch.Size (e.g., torch.Size([10, 20, 30])). - \'num_dimensions\': the number of dimensions of the tensor (integer). - \'size_of_dimension_i\': a list where each element is the size of the corresponding dimension (e.g., [10, 20, 30]). Parameters: tensor (torch.Tensor): The input tensor to analyze. Returns: dict: A dictionary containing the analysis of the tensor shape. pass ``` **Constraints:** - The input tensor can have any number of dimensions, but it is guaranteed to be valid. - You are not allowed to use any external libraries other than PyTorch. **Example:** ```python import torch # Example tensor tensor = torch.ones(3, 4, 5) # Expected output: # { # \'shape\': torch.Size([3, 4, 5]), # \'num_dimensions\': 3, # \'size_of_dimension_i\': [3, 4, 5] # } print(tensor_shape_analysis(tensor)) ``` **Explanation:** - `shape` should return the shape of the tensor as a `torch.Size` object. - `num_dimensions` should return the number of dimensions of the tensor. - `size_of_dimension_i` should be a list containing the size of each dimension in order. Implement this function in a way that efficiently handles tensors of varying sizes and dimensions.","solution":"import torch def tensor_shape_analysis(tensor: torch.Tensor) -> dict: Analyze the shape of a given tensor and return a dictionary with the following keys: - \'shape\': the shape of the tensor as torch.Size (e.g., torch.Size([10, 20, 30])). - \'num_dimensions\': the number of dimensions of the tensor (integer). - \'size_of_dimension_i\': a list where each element is the size of the corresponding dimension (e.g., [10, 20, 30]). Parameters: tensor (torch.Tensor): The input tensor to analyze. Returns: dict: A dictionary containing the analysis of the tensor shape. return { \'shape\': tensor.size(), \'num_dimensions\': tensor.dim(), \'size_of_dimension_i\': list(tensor.size()) }"},{"question":"Coding Assessment Question # Title: Implement and Test Resource Management in Python Development Mode # Objective: Write a Python program that processes a given text file, and ensure that all resources are properly managed, especially when running under Python Development Mode. This task helps in assessing students\' understanding of file handling, resource management, and debugging techniques in Python. # Problem: Implement a Python function `count_lines_in_file(file_path: str) -> int` that opens a specified text file, counts the number of lines in it, and returns this count. Ensure that the function handles resources (file descriptors) properly to avoid any warnings or errors, especially under Python Development Mode. # Requirements: 1. Implement the function `count_lines_in_file(file_path: str) -> int`. 2. Ensure that all file handles are properly closed. 3. Your implementation should not generate any `ResourceWarning` or `Bad file descriptor` errors in Python Development Mode. # Constraints: 1. The file specified by `file_path` will always be a readable text file. 2. You must handle potential errors gracefully (e.g., file not found, permission issues) by printing a user-friendly message and returning `-1`. # Input and Output: - **Input**: A string `file_path` representing the path to a readable text file. - **Output**: An integer representing the number of lines in the file. If an error occurs, return `-1`. # Example: Given: A file `example.txt` with the following content: ``` Hello, World! This is a test file. It contains three lines. ``` When: ```python count_lines_in_file(\'example.txt\') ``` Then: ```python 3 ``` Hints: - Use context managers for opening and reading the file. - Test your implementation in development mode using the `-X dev` option: `python3 -X dev your_script.py`. # Additional Information: To illustrate the implementation, you can use the following template: ```python def count_lines_in_file(file_path: str) -> int: try: with open(file_path, \'r\') as file: lines = file.readlines() return len(lines) except FileNotFoundError: print(f\\"Error: The file \'{file_path}\' does not exist.\\") except PermissionError: print(f\\"Error: You do not have permission to read the file \'{file_path}\'.\\") except Exception as ex: print(f\\"An unexpected error occurred: {ex}\\") return -1 ``` # Testing: 1. Create different test files and run your script in Python Development Mode to ensure there are no warnings or errors related to resource management. 2. Test the function with non-existing files or files with permission issues to ensure the error handling works correctly. Use this problem to demonstrate your comprehension of proper resource management in Python and the benefits of using Python Development Mode during development.","solution":"def count_lines_in_file(file_path: str) -> int: Counts the number of lines in the specified text file. Args: file_path (str): The path to the text file. Returns: int: The number of lines in the file, or -1 if an error occurs. try: with open(file_path, \'r\') as file: lines = file.readlines() return len(lines) except FileNotFoundError: print(f\\"Error: The file \'{file_path}\' does not exist.\\") except PermissionError: print(f\\"Error: You do not have permission to read the file \'{file_path}\'.\\") except Exception as ex: print(f\\"An unexpected error occurred: {ex}\\") return -1"},{"question":"Objective: Write a program that leverages the `__future__` module to display information about language features that can be enabled using this module. Task: You are provided with a Python module, `__future__`, which documents features that were introduced in different Python versions and indicates whether they are optional or mandatory. Your task is to create a function called `display_future_features` that dynamically imports the `__future__` module and prints a formatted table of features, including their optional and mandatory release versions, along with a readability indicator regarding their current status in Python. Requirement: 1. Implement the function `display_future_features()` without any arguments. 2. The function should print a table where each row contains the following columns: - **Feature Name**: The name of the feature. - **Optional Release**: The version in which the feature became optionally available. - **Mandatory Release**: The version in which the feature became mandatory (if it ever did). - **Current Status**: A text indicator: * \\"Mandatory\\" if the feature is now mandatory. * \\"Optional\\" if the feature is optional but not yet mandatory. * \\"Dropped\\" if the feature was planned but got dropped. * \\"Not Yet Mandatory\\" if the feature has an optional release but is not yet mandatory. 3. Extract the aforementioned data programmatically from the `__future__` module. Constraints: - Utilize Python\'s built-in `__future__` module. - Handle different release versions correctly. - Ensure the output is well-formatted and readable as a table. Example Output: ``` +-------------------+----------------+----------------+--------------------+ | Feature Name | Optional Release | Mandatory Release | Current Status | +-------------------+----------------+----------------+--------------------+ | nested_scopes | (2, 1, 0, beta, 1) | (2, 2, 0, final, 0) | Mandatory | | generators | (2, 2, 0, alpha, 1) | (2, 3, 0, final, 0) | Mandatory | | division | (2, 2, 0, alpha, 2) | (3, 0, 0, final, 0) | Mandatory | | absolute_import | (2, 5, 0, alpha, 1) | (3, 0, 0, final, 0) | Mandatory | | with_statement | (2, 5, 0, alpha, 1) | (2, 6, 0, final, 0) | Mandatory | | print_function | (2, 6, 0, alpha, 2) | (3, 0, 0, final, 0) | Mandatory | | unicode_literals | (2, 6, 0, alpha, 2) | (3, 0, 0, final, 0) | Mandatory | | generator_stop | (3, 5, 0, beta, 1) | (3, 7, 0, final, 0) | Mandatory | | annotations | (3, 7, 0, beta, 1) | None | Not Yet Mandatory | +-------------------+----------------+----------------+--------------------+ ``` Hint: Use Python\'s inspection tools and the `__future__` module\'s attributes and methods to gather the necessary information.","solution":"import __future__ import inspect def display_future_features(): features = [name for name in dir(__future__) if not (name.startswith(\'__\') or name == \'all_feature_names\')] features_info = [] for feature in features: ftr = getattr(__future__, feature) optional_release = getattr(ftr, \'optional\', \'N/A\') mandatory_release = getattr(ftr, \'mandatory\', \'N/A\') if mandatory_release == \'N/A\': current_status = \\"Optional\\" elif mandatory_release == (0, 0, 0, \'final\', 0): current_status = \\"Dropped\\" elif optional_release == \'N/A\': current_status = \\"Mandatory\\" else: current_status = \\"Not Yet Mandatory\\" features_info.append((feature, optional_release, mandatory_release, current_status)) header = f\\"+{\'Feature Name\'.ljust(18)}+{\'Optional Release\'.ljust(18)}+{\'Mandatory Release\'.ljust(18)}+{\'Current Status\'.ljust(18)}+\\" print(header) for feature in features_info: row = f\\"|{feature[0].ljust(18)}|{str(feature[1]).ljust(18)}|{str(feature[2]).ljust(18)}|{feature[3].ljust(18)}|\\" print(row)"},{"question":"# Question: Advanced Exception Handling with `cgitb` You are tasked with writing a Python script that demonstrates advanced exception handling using the `cgitb` module. Objectives: 1. **Set Up Exception Handling:** Configure the script to use the `cgitb` module for detailed exception tracebacks in both HTML and plain text formats. 2. **Function to Fail:** Write a function `cause_error()` that will intentionally cause a `ZeroDivisionError` by attempting to divide a number by zero. This function should take a single integer argument. 3. **Generate Reports:** - Capture the exception using the `cgitb.handler()` function and print the traceback as plain text. - Also, capture the exception and save the traceback as a formatted HTML file in a directory named `logs`. Detailed Requirements: 1. **Module Configuration:** - Use `cgitb.enable()` to ensure detailed traceback logs will be generated. - Configure `cgitb` to not display exceptions in the browser but to log to the `logs` directory. 2. **Error-causing Function:** ```python def cause_error(x): return x / 0 ``` 3. **Script Flow:** - Call the `cause_error()` function within a try-except block. - In the exception block, use `cgitb.text()` to print the traceback to the console. - Finally, use `cgitb.handler()` to log the full traceback in HTML format to the `logs` directory. Expected Input and Output: - **Input:** A positive integer passed to `cause_error()`. - **Output:** - Plain text traceback printed to the console. - HTML formatted traceback saved in the `logs` directory. Constraints: - You may assume the directory `logs` already exists and is writable. - Do not modify global exception handling outside of the specific try-except block used in your script. Example Usage: ```python # Assuming the necessary import statements import cgitb import os def cause_error(x): return x / 0 # This will cause ZeroDivisionError # Enabling cgitb with specific settings cgitb.enable(display=0, logdir=\'logs\') try: cause_error(5) except: # Printing the traceback as plain text import sys info = sys.exc_info() print(cgitb.text(info)) # Handling the exception with cgitb to log HTML report cgitb.handler(info) ``` In this example, running the script will print the traceback as plain text to the console and save an HTML-formatted traceback in the `logs` directory.","solution":"import cgitb import os import sys def cause_error(x): return x / 0 # This will cause ZeroDivisionError # Function to set up cgitb def setup_cgitb(): os.makedirs(\'logs\', exist_ok=True) cgitb.enable(display=0, logdir=\'logs\') def main(): setup_cgitb() try: cause_error(5) except ZeroDivisionError: info = sys.exc_info() print(cgitb.text(info)) cgitb.handler(info) if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question:** # Objective Create a custom type in Python using the concepts from the provided documentation related to type objects. # Problem Statement You are required to implement a custom numeric type in Python that mimics some behavior of standard numeric types but adds additional functionality. Use the following steps to construct your type definition and implement it in Python. # Requirements 1. **Class Definition:** - Define a new class called `MyNumber` that: - Inherits from the base type `object`. - Contains an integer attribute called `value`. - Implements the `__init__` method to initialize the `value`. 2. **Custom Arithmetic Operation:** - Implement a custom addition method for the `MyNumber` class such that using the `+` operator on two `MyNumber` objects adds their `value` attributes and returns a new `MyNumber` object with the result. 3. **String Representation:** - Override the `__str__` method to return a string representing the `value` attribute. 4. **Type Check Functions:** - Write a function `is_my_number_type` that takes an object as an argument and returns `True` if the object is an instance of `MyNumber` and `False` otherwise. - Write a function `is_exact_my_number_type` that takes an object as an argument and returns `True` if the object is exactly of type `MyNumber` (not a subtype) and `False` otherwise. 5. **Performance Requirements:** - Ensure the custom addition operation executes in constant time, regardless of the size of the numbers. # Input and Output Formats - **Input:** - Initialization: `MyNumber(5)`, `MyNumber(10)` - Addition: `num1 + num2` where `num1` and `num2` are instances of `MyNumber`. - Type Checking: `is_my_number_type(obj)`, `is_exact_my_number_type(obj)` where `obj` is an instance of any type. - **Output:** - For addition: `MyNumber` instance with the summed value. - For string representation: String displaying the `value`. - For type checking: Boolean value indicating type checks. # Constraints or Limitations - The `value` attribute will only hold integer values. - Ensure that the class methods have constant time complexity. - The implemented functionality should mimic native Python behavior as closely as possible. # Example ```python num1 = MyNumber(5) num2 = MyNumber(10) num3 = num1 + num2 # Should create a new MyNumber instance with value 15 print(num3) # Should output \\"15\\" print(is_my_number_type(num3)) # Should return True print(is_exact_my_number_type(num3)) # Should return True class DerivedNumber(MyNumber): pass num4 = DerivedNumber(20) print(is_my_number_type(num4)) # Should return True print(is_exact_my_number_type(num4)) # Should return False ``` **Note:** Ensure your implementation correctly handles these cases and adheres to Python\'s type system principles.","solution":"class MyNumber: def __init__(self, value): if not isinstance(value, int): raise ValueError(\\"Value must be an integer\\") self.value = value def __add__(self, other): if isinstance(other, MyNumber): return MyNumber(self.value + other.value) return NotImplemented def __str__(self): return str(self.value) def is_my_number_type(obj): return isinstance(obj, MyNumber) def is_exact_my_number_type(obj): return type(obj) is MyNumber"},{"question":"# Python Coding Assessment Question Objective Demonstrate your understanding of Python 3.10\'s instance method objects and method objects by implementing a class and manipulating method instances. Problem Statement You are required to implement a Python class `Book` that has instance methods and utilizes the functionalities of instance and method objects as described in the documentation. Specifically, you will: 1. Create instance methods using `PyInstanceMethod_New` and `PyInstanceMethod_Function`. 2. Create method objects using `PyMethod_New`, `PyMethod_Function`, and `PyMethod_Self`. Class Definition Implement a class `Book` with the following specifications: 1. **Attributes:** - `title` (string) - `author` (string) - `price` (float) 2. **Instance Methods:** - `get_details(self) -> str`: Returns a string in the format `\\"Title: <title>, Author: <author>, Price: <price>\\"`. - `apply_discount(self, discount: float) -> float`: Reduces the price by the discount percentage and returns the new price. 3. **Class Methods:** - `create_instance_method(func)`: Takes a callable function and returns it as an instance method. - `create_method(func, self_instance)`: Takes a callable function and an instance, and returns it as a bound method. Example Usage ```python # Define the class class Book: # Your implementation here # Create an instance of Book book = Book(\\"The Python Programming\\", \\"John Smith\\", 39.99) # Create and use instance method instance_method = Book.create_instance_method(book.get_details) print(instance_method()) # Expected: \\"Title: The Python Programming, Author: John Smith, Price: 39.99\\" # Create and use bound method bound_method = Book.create_method(Book.apply_discount, book) print(bound_method(10)) # Expected: 35.99 ``` Constraints - `title` and `author` must be non-empty strings. - `price` must be a positive float. - The `discount` in `apply_discount` must be a float between 0 and 100 (inclusive). # Function Definitions You need to implement the following methods: 1. **def create_instance_method(func: Callable):** - **Input:** A callable function. - **Output:** An instance method object. 2. **def create_method(func: Callable, self_instance: Any):** - **Input:** A callable function and an instance of a class. - **Output:** A method object bound to the instance. Testing Requirements Your implementation should pass the example usages and handle edge cases according to the constraints. # Submission Submit your implementation of the `Book` class with the required methods and attributes. Ensure your code is clean, well-documented, and adheres to Python coding standards.","solution":"class Book: def __init__(self, title, author, price): if not title or not isinstance(title, str): raise ValueError(\\"Title must be a non-empty string\\") if not author or not isinstance(author, str): raise ValueError(\\"Author must be a non-empty string\\") if not isinstance(price, (float, int)) or price <= 0: raise ValueError(\\"Price must be a positive float\\") self.title = title self.author = author self.price = float(price) def get_details(self) -> str: return f\\"Title: {self.title}, Author: {self.author}, Price: {self.price:.2f}\\" def apply_discount(self, discount: float) -> float: if not isinstance(discount, (float, int)) or not (0 <= discount <= 100): raise ValueError(\\"Discount must be a float between 0 and 100\\") self.price -= self.price * (discount / 100) return self.price @staticmethod def create_instance_method(func): def instance_method(*args, **kwargs): return func(*args, **kwargs) return instance_method @staticmethod def create_method(func, self_instance): def method(*args, **kwargs): return func(self_instance, *args, **kwargs) return method"},{"question":"You are tasked with ensuring deterministic behavior in a neural network training setup using PyTorch. Set the appropriate seeds and configurations to eliminate randomness from your code. Your network should give the same results every time it is run, regardless of the device (CPU or CUDA) used. **Task:** 1. Define a simple neural network using `torch.nn.Module`. 2. Ensure reproducibility by setting seeds for all random number generators used (PyTorch, NumPy, and Python). 3. Create and train the network on random input data and ensure it produces the same output on multiple runs. # Specifications: 1. Define a neural network named `SimpleNet` with one hidden layer containing 10 neurons using ReLU activation. 2. Set all necessary seeds and configurations to ensure deterministic results. 3. Generate a random tensor `input_data` of shape `(5, 3)` and an integer target `target` tensor of shape `(5,)` with values between 0 and 4. 4. Create an instance of the `SimpleNet` model and run a forward pass with the `input_data`. 5. Calculate the cross-entropy loss between the network\'s output and the `target` tensor. 6. Ensure the loss value is the same across multiple runs. # Expected Input/Output: - **Input:** None (the function does not take any input arguments). - **Output:** None (print the loss value). **Constraints:** - Your solution should be deterministic. - The solution must work on both CPU and CUDA. # Implementation: Please implement the described task in the function `ensure_deterministic_behavior()`. ```python import torch import torch.nn as nn import torch.optim as optim import numpy as np import random def ensure_deterministic_behavior(): # 1. Set seeds for reproducibility seed = 0 torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed(seed) np.random.seed(seed) random.seed(seed) # 2. Define the neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(3, 10) self.relu = nn.ReLU() self.fc2 = nn.Linear(10, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # 3. Create the model and random data model = SimpleNet() input_data = torch.randn(5, 3) target = torch.randint(0, 5, (5,)) # 4. Forward pass output = model(input_data) # 5. Calculate loss criterion = nn.CrossEntropyLoss() loss = criterion(output, target) # 6. Print loss value print(loss.item()) # Running the function to verify deterministic behavior ensure_deterministic_behavior() ``` **Note:** Execute the function `ensure_deterministic_behavior` multiple times to verify that the printed loss value is consistent.","solution":"import torch import torch.nn as nn import torch.optim as optim import numpy as np import random def ensure_deterministic_behavior(): # 1. Set seeds for reproducibility seed = 0 torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed(seed) np.random.seed(seed) random.seed(seed) # Ensure deterministic behavior on the CPU and GPU torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # 2. Define the neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(3, 10) self.relu = nn.ReLU() self.fc2 = nn.Linear(10, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # 3. Create the model and random data model = SimpleNet() input_data = torch.randn(5, 3) target = torch.randint(0, 5, (5,)) # 4. Forward pass output = model(input_data) # 5. Calculate loss criterion = nn.CrossEntropyLoss() loss = criterion(output, target) # 6. Print loss value print(loss.item())"},{"question":"**Assessment Question:** # Dataset Generation and Visualization in Scikit-Learn Objective: You are tasked with generating multiple datasets using scikit-learn\'s dataset generators. You will visualize these datasets and describe the key properties of each. Instructions: 1. Implement a function `generate_and_plot_datasets`: ```python def generate_and_plot_datasets(): This function generates datasets using various scikit-learn generators and plots them to visualize their structure. The function should: 1. Generate and plot datasets for: a. `make_blobs` with 3 centers b. `make_classification` with 2 informative features and 3 classes c. `make_gaussian_quantiles` with 3 classes d. `make_circles` with noise e. `make_moons` with noise 2. Display all plots in a single figure with subplots. 3. Add appropriate titles to each subplot describing the dataset. pass ``` 2. For each of the dataset generators: - Use appropriate parameters to create interesting and complex structures. - Use `matplotlib` to plot the datasets. - Ensure each plot is appropriately titled to indicate which dataset it represents. 3. After the plots are generated: - Describe the key properties of each dataset in comments within your function. - Explain in comments what machine learning problems each dataset generator might be useful for. Constraints: - You should use the following fixed random state: `random_state=42` for consistency across runs. - The function should handle plotting within a single figure using subplots. Expected Output: The function should produce a single figure with subplots for each generated dataset, with appropriate titles. Additionally, comments within the function should describe the properties and potential uses of each dataset. Good luck!","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles, make_circles, make_moons def generate_and_plot_datasets(): This function generates datasets using various scikit-learn generators and plots them to visualize their structure. The function: 1. Generates and plots datasets for: a. `make_blobs` with 3 centers b. `make_classification` with 2 informative features and 3 classes c. `make_gaussian_quantiles` with 3 classes d. `make_circles` with noise e. `make_moons` with noise 2. Displays all plots in a single figure with subplots. 3. Adds appropriate titles to each subplot describing the dataset. plt.figure(figsize=(15, 10)) # a. make_blobs with 3 centers X_blobs, y_blobs = make_blobs(n_samples=300, centers=3, random_state=42) plt.subplot(2, 3, 1) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap=\'viridis\') plt.title(\'make_blobs with 3 centers\') # Key Properties: Clusters of points with a Gaussian distribution around each center. # Useful for: Clustering algorithms like K-means. # b. make_classification with 2 informative features and 3 classes X_classification, y_classification = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, n_classes=3, random_state=42) plt.subplot(2, 3, 2) plt.scatter(X_classification[:, 0], X_classification[:, 1], c=y_classification, cmap=\'viridis\') plt.title(\'make_classification with 3 classes\') # Key Properties: A dataset with informative features that determine the class labels. # Useful for: Classification problems and supervised learning algorithms. # c. make_gaussian_quantiles with 3 classes X_gaussian, y_gaussian = make_gaussian_quantiles(n_samples=300, n_features=2, n_classes=3, random_state=42) plt.subplot(2, 3, 3) plt.scatter(X_gaussian[:, 0], X_gaussian[:, 1], c=y_gaussian, cmap=\'viridis\') plt.title(\'make_gaussian_quantiles with 3 classes\') # Key Properties: Data points divided into quantiles of a multivariate normal distribution. # Useful for: Classification problems dealing with normally distributed data. # d. make_circles with noise X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42) plt.subplot(2, 3, 4) plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=\'viridis\') plt.title(\'make_circles with noise\') # Key Properties: Concentric circles with given noise. Non-linear structure. # Useful for: Evaluating the performance of algorithms like SVM with RBF kernel. # e. make_moons with noise X_moons, y_moons = make_moons(n_samples=300, noise=0.1, random_state=42) plt.subplot(2, 3, 5) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=\'viridis\') plt.title(\'make_moons with noise\') # Key Properties: Two interleaving half circles with given noise. Non-linear structure. # Useful for: Evaluating the performance of algorithms like SVM with RBF kernel. plt.tight_layout() plt.show()"},{"question":"**Objective:** Write a Python program that processes a file containing student records and provides a summary report. The program should demonstrate the use of file handling, data structures, control flow, and exception handling in Python. **Problem Statement:** You are given a text file named `students.txt` which contains records of students. Each record is on a new line and follows the format: ``` student_id,student_name,age,grade ``` Examples: ``` 1001,John Doe,21,A 1002,Jane Smith,22,B 1003,Alice Johnson,20,A 1004,Mark Brown,21,C ``` Your task is to write a Python function `process_student_records(file_path: str) -> dict` that processes this file and returns a summary report as a dictionary. The summary should include: 1. Total number of students. 2. Average age of the students. 3. A count of how many students received each grade (A, B, C, D, F). **Function Signature:** ```python def process_student_records(file_path: str) -> dict: # Implementation here ``` **Input:** - `file_path`: A string representing the path to the `students.txt` file. **Output:** - A dictionary with the following keys: - `\'total_students\'`: An integer representing the total number of students. - `\'average_age\'`: A float representing the average age of the students. - `\'grade_counts\'`: A dictionary where the keys are grades (`\'A\'`, `\'B\'`, `\'C\'`, `\'D\'`, `\'F\'`) and the values are integers representing the number of students who received each grade. **Constraints:** - The age of a student will always be a positive integer. - Grades are limited to `A`, `B`, `C`, `D`, and `F`. - Each student\'s record is valid and well-formatted, but error handling should account for potential file errors (e.g., file not found). **Example:** Given a file `students.txt` with the following content: ``` 1001,John Doe,21,A 1002,Jane Smith,22,B 1003,Alice Johnson,20,A 1004,Mark Brown,21,C ``` The function call `process_student_records(\'students.txt\')` should return: ```python { \'total_students\': 4, \'average_age\': 21.0, \'grade_counts\': {\'A\': 2, \'B\': 1, \'C\': 1, \'D\': 0, \'F\': 0} } ``` **Additional Information:** - Ensure your solution reads the file efficiently and handles any exceptions that may occur during file operations. - Use appropriate data structures to store and process the information. - Validate your solution with additional test cases containing varying numbers of records and grades.","solution":"def process_student_records(file_path: str) -> dict: Processes the student records from the given file and produces a summary report. Args: - file_path (str): The path to the file containing student records. Returns: - dict: A dictionary summarizing the total number of students, their average age, and counts of each grade. total_students = 0 total_age = 0 grade_counts = {\'A\': 0, \'B\': 0, \'C\': 0, \'D\': 0, \'F\': 0} try: with open(file_path, \'r\') as file: for line in file: if line.strip(): # Ensure the line is not empty parts = line.strip().split(\',\') if len(parts) == 4: student_id, student_name, age, grade = parts total_students += 1 total_age += int(age) if grade in grade_counts: grade_counts[grade] += 1 except FileNotFoundError: return {\\"error\\": \\"File not found\\"} average_age = total_age / total_students if total_students > 0 else 0 return { \'total_students\': total_students, \'average_age\': average_age, \'grade_counts\': grade_counts }"},{"question":"# Objective Write a function that processes a list of floating-point numbers, performs a series of specified operations, and returns the results with high precision. The function should demonstrate knowledge of Python\'s handling of floating-point arithmetic, including binary representation, precision errors, and methods to control or mitigate these errors. # Function Signature ```python def process_floats(floats: list[float], decimal_places: int) -> list[str]: pass ``` # Input 1. `floats`: A list of floating-point numbers. (e.g., [0.1, 0.2, 0.3]) 2. `decimal_places`: An integer specifying the number of decimal places to which each result should be rounded. (e.g., 10) # Output The function should return a list of strings, where each string represents a floating-point number from the input list, but rounded to the specified number of decimal places. Each string element should be formatted to exactly the given decimal_places. # Constraints 1. You should use Python’s `decimal.Decimal` or `fractions.Fraction` for precise calculations. 2. The input list will have at most 1000 floating-point numbers. 3. The number of decimal places will range from 1 to 50. # Example ```python floats = [0.1, 0.2, 0.3] decimal_places = 20 print(process_floats(floats, decimal_places)) ``` **Output:** ```python [\'0.10000000000000000555\', \'0.20000000000000001110\', \'0.30000000000000004441\'] ``` # Additional Requirements 1. Your solution should handle edge cases where floating-point arithmetic errors are more pronounced. 2. The function should perform efficiently even with the upper limit of input sizes. 3. Include a brief explanation within your code, especially where you handle floating-point precision and use specific Python modules. # Notes - You may import any standard library modules that help in addressing the floating-point precision issues. - Ensure that you test the function with various cases including numbers that are known to cause floating-point representation errors.","solution":"from decimal import Decimal, getcontext def process_floats(floats, decimal_places): Processes the list of floats, rounding each to the specified number of decimal places, and returns them as strings. Parameters: floats (list[float]): List of floating-point numbers. decimal_places (int): The number of decimal places to round to. Returns: list[str]: List of rounded floating-point numbers as strings. # Set the precision for Decimal conversions getcontext().prec = decimal_places + 5 # Convert to Decimal and round each number, then format as string result = [] for number in floats: decimal_number = Decimal(str(number)) rounded_decimal = round(decimal_number, decimal_places) formatted_number = f\\"{rounded_decimal:.{decimal_places}f}\\" result.append(formatted_number) return result"},{"question":"**Question:** # Shared Memory Manager Implementation You are required to implement a scenario where multiple processes perform operations on a shared NumPy array using the `multiprocessing.shared_memory` module. This implementation must include efficient memory management and properly handle cleanup of shared memory resources. **Scenario:** - You need to initialize a NumPy array with integers from 1 to 1000. - Create a shared memory block from this array. - Distribute the task of multiplying each element by 2 across two separate processes. - After the processes complete their tasks, verify the correctness of the results by checking the shared NumPy array. - Ensure proper cleanup of shared memory resources. # Requirements: 1. **Function: `initialize_shared_array()`** - **Description**: Initializes a NumPy array of integers from 1 to 1000 and creates a shared memory block from this array. - **Output**: Return the shared memory name and the size of the shared memory. 2. **Function: `process_task(shared_mem_name, start_index, end_index)`** - **Description**: A function intended to be run by a process to multiply elements in the shared NumPy array by 2, between `start_index` and `end_index`. - **Input**: - `shared_mem_name`: The name of the shared memory block. - `start_index`: The starting index (inclusive) for the process to begin multiplication. - `end_index`: The ending index (exclusive) for the process to end multiplication. 3. **Function: `main()`** - **Description**: Main function to coordinate the setup, process management, and result verification. - **Steps**: - Call `initialize_shared_array()` to initialize the shared memory. - Create and start two processes that each perform the multiplication task on half of the array. - Wait for both processes to complete. - Verify that each element in the shared memory block equals its original value multiplied by 2. - Print \\"Success\\" if the verification passes, otherwise print \\"Failure\\". - Ensure proper cleanup of shared memory resources in all processes. **Constraints**: - Use the `multiprocessing` module to manage separate processes. - Handle all necessary imports within your code. # Example Usage: ```python if __name__ == \\"__main__\\": main() ``` # Your Implementation: ```python import numpy as np from multiprocessing import shared_memory, Process def initialize_shared_array(): # Your code here pass def process_task(shared_mem_name, start_index, end_index): # Your code here pass def main(): # Your code here pass ```","solution":"import numpy as np from multiprocessing import shared_memory, Process def initialize_shared_array(): array = np.arange(1, 1001, dtype=np.int64) shm = shared_memory.SharedMemory(create=True, size=array.nbytes) shm_array = np.ndarray(array.shape, dtype=array.dtype, buffer=shm.buf) np.copyto(shm_array, array) return shm.name, array.nbytes def process_task(shared_mem_name, start_index, end_index): existing_shm = shared_memory.SharedMemory(name=shared_mem_name) shared_array = np.ndarray((1000,), dtype=np.int64, buffer=existing_shm.buf) for i in range(start_index, end_index): shared_array[i] *= 2 existing_shm.close() def main(): shared_mem_name, _ = initialize_shared_array() p1 = Process(target=process_task, args=(shared_mem_name, 0, 500)) p2 = Process(target=process_task, args=(shared_mem_name, 500, 1000)) p1.start() p2.start() p1.join() p2.join() existing_shm = shared_memory.SharedMemory(name=shared_mem_name) shared_array = np.ndarray((1000,), dtype=np.int64, buffer=existing_shm.buf) success = np.all(shared_array == np.arange(1, 1001) * 2) print(\\"Success\\" if success else \\"Failure\\") existing_shm.close() existing_shm.unlink()"},{"question":"**Objective:** Write a Python function `process_data` that receives a dictionary representing a collection of students and their corresponding grades. The function should perform the following tasks: 1. Increase each student\'s grade by a specified number using an augmented assignment statement. 2. Remove students whose grades fall below a certain threshold. 3. If a student\'s name is specified, remove this student from the dictionary. 4. Use assert statement to ensure that the dictionary is not empty after processing. **Function Signature:** ```python def process_data(students: dict, increment: int, threshold: int, remove_student: str = None) -> dict: pass ``` **Parameters:** - `students` (dict): A dictionary where keys are student names (strings) and values are their grades (integers). - `increment` (int): The number to be added to each student\'s grade. - `threshold` (int): Minimum grade required for students to remain in the dictionary. - `remove_student` (str, optional): The name of the student to be removed from the dictionary. Default is None. **Returns:** - dict: The updated dictionary of students. **Constraints:** - All grades are integers between 0 and 100. - The `remove_student` parameter, if provided, will always be a valid student name in the dictionary. **Example:** ```python students = { \'Alice\': 65, \'Bob\': 75, \'Charlie\': 50, \'David\': 80 } increment = 10 threshold = 60 remove_student = \'Charlie\' new_students = process_data(students, increment, threshold, remove_student) print(new_students) # Output: {\'Alice\': 75, \'Bob\': 85, \'David\': 90} ``` **Instructions:** 1. Iterate over the dictionary and use an augmented assignment statement to increment each student\'s grade. 2. Use a del statement to remove any student whose grade is less than the threshold from the dictionary. 3. If the `remove_student` parameter is provided, remove the corresponding student from the dictionary. 4. Use an assert statement to ensure that the dictionary is not empty at the end of the function. If the dictionary is empty, raise a `ValueError` with the message \\"No students left after processing.\\" Implement the function `process_data` to complete the task.","solution":"def process_data(students: dict, increment: int, threshold: int, remove_student: str = None) -> dict: # Increase each student\'s grade by the specified increment for student in students: students[student] += increment # Remove students whose grades fall below the threshold students = {student: grade for student, grade in students.items() if grade >= threshold} # If a student\'s name is specified, remove this student from the dictionary if remove_student: if remove_student in students: del students[remove_student] # Use assert statement to ensure dictionary is not empty if not students: raise ValueError(\\"No students left after processing\\") return students"},{"question":"**Objective:** Write a Python function `compare_execution_times` that uses the `timeit` module to compare the execution times of multiple code snippets. Your function should take a list of dictionaries as input, where each dictionary contains the code snippet and optional setup code. The function should run each snippet multiple times, compute the average execution time, and return a summary of the results. **Function Signature:** ```python def compare_execution_times(snippets: List[Dict[str, str]], number: int = 100000) -> List[Tuple[str, float]]: pass ``` **Inputs:** - `snippets`: A list of dictionaries where each dictionary has two keys: - `\\"code\\"`: a string representing the code snippet to be timed. - `\\"setup\\"`: (optional) a string representing the setup code to be run before timing the snippet. - `number` (optional): An integer representing the number of times the code snippets should be executed (default is 100,000). **Outputs:** - A list of tuples, where each tuple contains: - The `code` string being timed. - The average execution time for running the `code` `number` times. **Constraints:** - Each code snippet should be a valid Python expression. - The setup code, if provided, should also be a valid Python expression. - Assume valid input formats. **Example:** ```python snippets = [ {\\"code\\": \\"sum(range(10))\\"}, {\\"code\\": \\"sum(range(10))\\", \\"setup\\": \\"pass\\"}, {\\"code\\": \\"sorted([i for i in range(100)])\\", \\"setup\\": \\"pass\\"} ] # Suppose the function is called as follows results = compare_execution_times(snippets, number=10000) # Example output # [ # (\\"sum(range(10))\\", 0.0176431), # (\\"sum(range(10))\\", 0.0175214), # (\\"sorted([i for i in range(100)])\\", 0.1247856) # ] ``` **Performance Requirements:** - Aim to run each code snippet the specified number of times efficiently. - Ensure that the setup code does not interfere with the measurement of the execution times. **Notes:** - Utilize the `timeit` module for measuring execution times. - Ensure that the function handles the setup properly. - The average execution time can be computed by measuring the total time and dividing it by `number`. **Tip:** - Use the `timeit.timeit` method to measure execution times and the `globals` argument for the setup code.","solution":"import timeit from typing import List, Dict, Tuple def compare_execution_times(snippets: List[Dict[str, str]], number: int = 100000) -> List[Tuple[str, float]]: results = [] for snippet in snippets: code = snippet.get(\\"code\\") setup = snippet.get(\\"setup\\", \\"pass\\") # use \'pass\' as default setup if none provided # Measure execution time exec_time = timeit.timeit(code, setup=setup, number=number) avg_exec_time = exec_time / number results.append((code, avg_exec_time)) return results"},{"question":"<|Analysis Begin|> The provided documentation is for the \\"heapq\\" module in Python, which implements the heap queue algorithm, also known as the priority queue algorithm. This module is vital for efficient management of sorted data streams, providing operations for creating heaps, pushing and popping elements while maintaining heap properties, and leveraging these properties for operations like heap sorting and finding the smallest/largest elements. Key functions and methods described in the documentation include: - `heapq.heappush(heap, item)`: Push an item onto the heap. - `heapq.heappop(heap)`: Pop the smallest item off the heap. - `heapq.heappushpop(heap, item)`: Push an item onto the heap and then pop the smallest item. - `heapq.heapify(x)`: Transform a list into a heap, in-place, in linear time. - `heapq.heapreplace(heap, item)`: Pop and return the smallest item, and push a new item. - `heapq.merge(*iterables, key=None, reverse=False)`: Merge multiple sorted inputs into a single sorted output. - `heapq.nlargest(n, iterable, key=None)` and `heapq.nsmallest(n, iterable, key=None)`: Return the n largest or smallest elements from the dataset. Additionally, the documentation includes strategies for implementing stable priority queues and dealing with challenges like task priority changes or task removal. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: Priority Queue Implementation with Task Management **Objective:** Implement a priority queue to efficiently manage tasks with priorities. You will use the `heapq` module to create this priority queue and implement functions to add tasks, remove tasks, and pop the highest priority task. **Specifications:** 1. **Function: `add_task(task: str, priority: int = 0) -> None`** - **Input:** - `task`: A string representing the task. - `priority`: An integer representing the priority of the task (default is 0). - **Output:** None - Description: Adds a new task to the priority queue or updates the priority of an existing task. The task with the lower priority value should be popped first. 2. **Function: `remove_task(task: str) -> None`** - **Input:** - `task`: A string representing the task to be removed. - **Output:** None - Description: Removes a specific task from the priority queue. If the task does not exist, raise a `KeyError`. 3. **Function: `pop_task() -> str`** - **Output:** A string representing the task with the highest priority (lowest priority value). - Description: Removes and returns the task with the highest priority. If the queue is empty, raise a `KeyError`. 4. **Function: `task_count() -> int`** - **Output:** An integer representing the number of tasks currently in the priority queue. - Description: Returns the number of tasks currently in the priority queue. **Constraints:** - Assumes tasks are unique strings. - Priority values are integers. **Performance Requirements:** - All operations must maintain the heap properties and be efficient in terms of time complexity. **Example:** ```python # Example usage: pq = PriorityQueue() pq.add_task(\\"write code\\", 5) pq.add_task(\\"release product\\", 7) pq.add_task(\\"write spec\\", 1) pq.add_task(\\"create tests\\", 3) assert pq.pop_task() == \\"write spec\\" assert pq.task_count() == 3 pq.remove_task(\\"create tests\\") assert pq.task_count() == 2 ``` **Implementation Note:** You may use the following scaffolding code to start your implementation: ```python from heapq import heappush, heappop import itertools class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task: str, priority: int = 0) -> None: \'Add a new task or update the priority of an existing task\' if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task: str) -> None: \'Mark an existing task as REMOVED. Raise KeyError if not found.\' entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def task_count(self) -> int: \'Return number of tasks currently in the priority queue\' return len(self.entry_finder) ``` *Ensure your implementation passes the provided example usage test cases to validate correctness.*","solution":"from heapq import heappush, heappop import itertools class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task: str, priority: int = 0) -> None: \'Add a new task or update the priority of an existing task\' if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task: str) -> None: \'Mark an existing task as REMOVED. Raise KeyError if not found.\' if task not in self.entry_finder: raise KeyError(\'Task not found\') entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def task_count(self) -> int: \'Return number of tasks currently in the priority queue\' return len(self.entry_finder)"},{"question":"on Kernel Ridge Regression with scikit-learn Problem Statement: You are tasked with implementing a Kernel Ridge Regression model and comparing its performance with Support Vector Regression on a synthetic dataset. The dataset will consist of a sinusoidal target function with added noise. Your solution will involve hyperparameter optimization using grid search to find the best model parameters for both KRR and SVR. Requirements: 1. **Dataset Creation:** - Create a synthetic dataset of 100 data points where the target function is a sinusoidal function (`sin(x)`) with strong noise added to every fifth datapoint. - Split the dataset into training and testing sets (80% train, 20% test). 2. **Kernel Ridge Regression Implementation:** - Implement Kernel Ridge Regression using `sklearn.kernel_ridge.KernelRidge`. - Perform hyperparameter optimization using grid search for the regularization parameter (`alpha`) and the RBF kernel bandwidth (`gamma`). 3. **Support Vector Regression Implementation:** - Implement Support Vector Regression using `sklearn.svm.SVR`. - Perform hyperparameter optimization using grid search for `C`, `epsilon`, and the RBF kernel bandwidth (`gamma`). 4. **Performance Comparison:** - Compare the performance of the best KRR and SVR models. - Evaluate the models based on mean squared error (MSE) on both training and testing sets. - Provide the time taken for fitting and prediction for both models. Input Format: - No input from the user is required. Expected Output: Your code should: 1. Print the best hyperparameters found for both KRR and SVR. 2. Print the mean squared error (MSE) on both training and testing sets for the best KRR and SVR models. 3. Print the time taken for fitting and prediction for both models. Constraints: - Use the `numpy`, `sklearn`, and `time` libraries. - Ensure your solution is efficient for medium-sized datasets (up to 1000 samples). Sample Code Structure: ```python import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error import time # Step 1: Create the synthetic dataset def create_dataset(): # [Your Code Here] pass # Step 2: Implement Kernel Ridge Regression with Grid Search def krr_model(X_train, y_train): # [Your Code Here] pass # Step 3: Implement Support Vector Regression with Grid Search def svr_model(X_train, y_train): # [Your Code Here] pass # Step 4: Performance Comparison def compare_models(X_train, X_test, y_train, y_test, krr_best_model, svr_best_model): # [Your Code Here] pass # Main Function if __name__ == \\"__main__\\": # Generate dataset X, y = create_dataset() # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train and optimize models krr_best_model = krr_model(X_train, y_train) svr_best_model = svr_model(X_train, y_train) # Compare model performance compare_models(X_train, X_test, y_train, y_test, krr_best_model, svr_best_model) ``` Notes: - Ensure proper documentation and inline comments in your code for clarity. - Verify the correctness of your implementation against the expected output formats and constraints.","solution":"import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error import time # Step 1: Create the synthetic dataset def create_dataset(): np.random.seed(42) X = np.linspace(0, 10, 100).reshape(-1, 1) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - np.random.rand(20)) # add noise to every fifth data point return X, y # Step 2: Implement Kernel Ridge Regression with Grid Search def krr_model(X_train, y_train): krr = KernelRidge(kernel=\'rbf\') param_grid = {\'alpha\': [0.1, 1, 10], \'gamma\': [0.1, 0.5, 1]} grid_search = GridSearchCV(krr, param_grid, cv=5, scoring=\'neg_mean_squared_error\') start_time = time.time() grid_search.fit(X_train, y_train) end_time = time.time() best_krr_model = grid_search.best_estimator_ print(f\\"Best KRR Parameters: {grid_search.best_params_}\\") print(f\\"KRR Training Time: {end_time - start_time} seconds\\") return best_krr_model # Step 3: Implement Support Vector Regression with Grid Search def svr_model(X_train, y_train): svr = SVR(kernel=\'rbf\') param_grid = {\'C\': [0.1, 1, 10], \'epsilon\': [0.1, 0.2, 0.5], \'gamma\': [0.1, 0.5, 1]} grid_search = GridSearchCV(svr, param_grid, cv=5, scoring=\'neg_mean_squared_error\') start_time = time.time() grid_search.fit(X_train, y_train) end_time = time.time() best_svr_model = grid_search.best_estimator_ print(f\\"Best SVR Parameters: {grid_search.best_params_}\\") print(f\\"SVR Training Time: {end_time - start_time} seconds\\") return best_svr_model # Step 4: Performance Comparison def compare_models(X_train, X_test, y_train, y_test, krr_best_model, svr_best_model): # KRR Model Evaluation start_time = time.time() y_train_pred_krr = krr_best_model.predict(X_train) y_test_pred_krr = krr_best_model.predict(X_test) end_time = time.time() krr_prediction_time = end_time - start_time krr_train_mse = mean_squared_error(y_train, y_train_pred_krr) krr_test_mse = mean_squared_error(y_test, y_test_pred_krr) print(f\\"KRR Train MSE: {krr_train_mse}\\") print(f\\"KRR Test MSE: {krr_test_mse}\\") print(f\\"KRR Prediction Time: {krr_prediction_time} seconds\\") # SVR Model Evaluation start_time = time.time() y_train_pred_svr = svr_best_model.predict(X_train) y_test_pred_svr = svr_best_model.predict(X_test) end_time = time.time() svr_prediction_time = end_time - start_time svr_train_mse = mean_squared_error(y_train, y_train_pred_svr) svr_test_mse = mean_squared_error(y_test, y_test_pred_svr) print(f\\"SVR Train MSE: {svr_train_mse}\\") print(f\\"SVR Test MSE: {svr_test_mse}\\") print(f\\"SVR Prediction Time: {svr_prediction_time} seconds\\") # Main Function if __name__ == \\"__main__\\": # Generate dataset X, y = create_dataset() # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train and optimize models krr_best_model = krr_model(X_train, y_train) svr_best_model = svr_model(X_train, y_train) # Compare model performance compare_models(X_train, X_test, y_train, y_test, krr_best_model, svr_best_model)"},{"question":"Objective Demonstrate comprehension of IMAP protocols by establishing a connection to an IMAP server, retrieving specific emails, and performing operations on the fetched data. Task Write a Python function called `fetch_emails_with_attachment` that connects to a specified IMAP server, logs in using provided credentials, and retrieves all emails from the INBOX that contain attachments. It should then return a list of tuples, with each tuple containing the email\'s subject and the count of attachments. Function Signature: ```python def fetch_emails_with_attachment(host: str, username: str, password: str, port: int = 143, use_ssl: bool = False) -> list: pass ``` 1. **Parameters:** - **host (str)**: The IMAP server hostname. - **username (str)**: Username for logging into the email account. - **password (str)**: Password for logging into the email account. - **port (int, optional)**: Port for the IMAP connection (default is 143). - **use_ssl (bool, optional)**: If True, use an SSL-encrypted connection (default is False). 2. **Returns:** - A list of tuples, where each tuple contains: - **Subject (str)**: The email\'s subject. - **Attachment count (int)**: The number of attachments in the email. 3. **Constraints:** - Use the `imaplib` module for connecting to the IMAP server and fetching emails. - Ensure proper error handling and cleanup by properly closing the mailbox and logging out from the server. - Ensure SSL is appropriately used for secure connections when `use_ssl` is True. - Emails are in the INBOX and the function should handle different email formats to count attachments. Example: ```python host = \\"imap.example.com\\" username = \\"user@example.com\\" password = \\"securepassword\\" emails = fetch_emails_with_attachment(host, username, password, port=993, use_ssl=True) for subject, attachment_count in emails: print(f\\"Subject: {subject}, Attachments: {attachment_count}\\") ``` Notes: - Your implementation should use appropriate IMAP commands (`login`, `select`, `search`, `fetch`, and `logout`) to login, select the INBOX, search for emails, fetch their contents, and log out. - Parsing the email content to detect attachments may require handling MultiPart messages and inspecting the payloads.","solution":"import imaplib import email from email import policy from email.parser import BytesParser def fetch_emails_with_attachment(host: str, username: str, password: str, port: int = 143, use_ssl: bool = False) -> list: try: if use_ssl: mail = imaplib.IMAP4_SSL(host, port) else: mail = imaplib.IMAP4(host, port) mail.login(username, password) mail.select(\'inbox\') status, email_ids = mail.search(None, \'ALL\') email_ids = email_ids[0].split() emails_with_attachments = [] for e_id in email_ids: status, data = mail.fetch(e_id, \'(RFC822)\') raw_email = data[0][1] msg = BytesParser(policy=policy.default).parsebytes(raw_email) subject = msg[\'subject\'] attachment_count = 0 if msg.is_multipart(): for part in msg.iter_parts(): if part.get_content_disposition() == \'attachment\': attachment_count += 1 if attachment_count > 0: emails_with_attachments.append((subject, attachment_count)) mail.logout() return emails_with_attachments except Exception as e: print(f\\"An error occurred: {e}\\") if \'mail\' in locals(): mail.logout() return []"},{"question":"Question: Implementing Semi-Supervised Learning with Label Propagation # Objective Write a function using scikit-learn\'s `LabelPropagation` to perform semi-supervised learning on a given dataset. The function should predict the labels for the unlabeled data points. # Description You have been provided with a dataset containing both labeled and unlabeled data. Your task is to implement a function using the `LabelPropagation` model to predict the labels for the unlabeled data points. # Requirements - Implement the function `semi_supervised_label_propagation`. - Use scikit-learn\'s `LabelPropagation` to fit the model and predict labels. - Use the Radial Basis Function (RBF) kernel with a specified gamma value. - Ensure the function handles both labeled and unlabeled data properly. # Input - `X`: A 2D numpy array of shape (n_samples, n_features) containing the feature vectors. - `y`: A 1D numpy array of shape (n_samples,) containing the labels. Use `-1` to denote unlabeled data points. - `gamma`: A positive float value specifying the gamma parameter for the RBF kernel. # Output - A 1D numpy array of shape (n_samples,) containing the predicted labels for all the data points (including the originally unlabeled ones). # Constraints - Assume that there is at least one labeled sample for each class. - Do not use `LabelSpreading`. - Focus on the efficiency and readability of your code. # Example ```python import numpy as np X = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [6, 7]]) y = np.array([1, -1, -1, 0, -1]) gamma = 0.1 predicted_labels = semi_supervised_label_propagation(X, y, gamma) print(predicted_labels) ``` # Function Signature ```python def semi_supervised_label_propagation(X: np.ndarray, y: np.ndarray, gamma: float) -> np.ndarray: pass ``` Note: - Use appropriate imports from scikit-learn. - The function should handle edge cases where all points in certain classes are unlabeled, ensuring the results are stable. # Hint To get started with `LabelPropagation`, you can refer to the following basic usage: ```python from sklearn.semi_supervised import LabelPropagation label_prop_model = LabelPropagation(kernel=\'rbf\', gamma=gamma) label_prop_model.fit(X, y) predicted_labels = label_prop_model.transduction_ ```","solution":"import numpy as np from sklearn.semi_supervised import LabelPropagation def semi_supervised_label_propagation(X: np.ndarray, y: np.ndarray, gamma: float) -> np.ndarray: Perform semi-supervised learning using Label Propagation. Args: X : np.ndarray 2D array of shape (n_samples, n_features) containing feature vectors. y : np.ndarray 1D array of shape (n_samples,) containing the labels (-1 for unlabeled data points). gamma : float The gamma parameter for the RBF kernel. Returns: np.ndarray 1D array of shape (n_samples,) containing the predicted labels for all data points. # Initialize the LabelPropagation model with RBF kernel and specified gamma label_prop_model = LabelPropagation(kernel=\'rbf\', gamma=gamma) # Fit the model on the provided data label_prop_model.fit(X, y) # Return the predicted labels return label_prop_model.transduction_"},{"question":"# Pandas DataFrame Handling with Copy-on-Write Given the following DataFrame `df`, you are required to perform a series of operations ensuring that you adhere to the Copy-on-Write principles discussed. Your task is to: 1. Select the subset of the column `\'foo\'` where the corresponding value in the column `\'bar\'` is greater than 5 and set these values to 100. Ensure that your solution does not use chained assignment. 2. Create a Numpy array view of the DataFrame columns `\'foo\'` and `\'bar\'` combined. Modify the Numpy array to set the first element to 200, making sure the changes are reflected back in the DataFrame. 3. Drop the column `\'baz\'` from a DataFrame `df2` (created from `df` with an additional column `\'baz\'`), without incurring unnecessary copies of data. **Input DataFrame:** ```python import pandas as pd df = pd.DataFrame({ \\"foo\\": [1, 2, 3], \\"bar\\": [6, 5, 7] }) ``` **Expected Output:** The final output should be a DataFrame wherein: 1. The subset of `\'foo\'` where `\'bar\'` is greater than 5 has been set to 100. 2. The first element of the Numpy array has been set to 200. 3. The column `\'baz\'` has been dropped from the DataFrame `df2`. **Constraints:** - Your solution must not use chained assignment. - Avoid unnecessary copying of data. - Operations should follow the principles of Copy-on-Write. ```python import pandas as pd def modify_dataframe(df): # Step 1: Modify df according to the Copy-on-Write principles df.loc[df[\\"bar\\"] > 5, \\"foo\\"] = 100 # Step 2: Get a view of the DataFrame as a Numpy array and modify it arr = df[[\\"foo\\", \\"bar\\"]].to_numpy() arr.flags.writeable = True arr[0, 0] = 200 # Modify first element to 200 # Step 3: Create df2 from df and drop column \'baz\' efficiently df2 = df.copy() df2[\\"baz\\"] = [100, 200, 300] df2 = df2.drop(columns=[\\"baz\\"]) return df, df2 # Example use: df = pd.DataFrame({ \\"foo\\": [1, 2, 3], \\"bar\\": [6, 5, 7] }) df, df2 = modify_dataframe(df) print(df) print(df2) ``` **Performance Requirements:** - The operations should be efficient, following CoW principles, and ensuring that no unnecessary copies of data are made.","solution":"import pandas as pd import numpy as np def modify_dataframe(df): # Step 1: Modify df according to the Copy-on-Write principles by using loc df.loc[df[\\"bar\\"] > 5, \\"foo\\"] = 100 # Step 2: Get a view of the DataFrame as a Numpy array and modify it arr = df.values arr.flags.writeable = True arr[0, 0] = 200 # Modify first element to 200 # Step 3: Create df2 from df and drop column \'baz\' efficiently df2 = df.copy() df2[\\"baz\\"] = [100, 200, 300] df2.drop(columns=[\\"baz\\"], inplace=True) return df, df2 # Example use: if __name__ == \\"__main__\\": df = pd.DataFrame({ \\"foo\\": [1, 2, 3], \\"bar\\": [6, 5, 7] }) df, df2 = modify_dataframe(df) print(df) print(df2)"},{"question":"- Custom Calendar Summary **Objective:** Create a function named `generate_calendar_summary` that generates a summary for a given month and year, indicating the number of weekdays, weekends, and if the month contains any holidays (assume a fixed list of holidays). **Function Signature:** ```python def generate_calendar_summary(year: int, month: int, holidays: list) -> dict: pass ``` **Instructions:** 1. **Inputs:** - `year`: An integer representing the year. - `month`: An integer between 1 and 12 representing the month. - `holidays`: A list of tuples, where each tuple consists of (month, day) representing fixed-date holidays in the format (int, int). 2. **Output:** - The function should return a dictionary with the following keys: - `\\"num_weekdays\\"`: The number of weekdays in the given month. - `\\"num_weekends\\"`: The number of weekend days (Saturday and Sunday) in the given month. - `\\"contains_holiday\\"`: A boolean indicating if the month contains any of the provided holidays. 3. **Constraints:** - Assume the input year is a valid year (e.g., 2023). - The month value will be between 1 and 12. - The holidays list will contain valid dates within the year. 4. **Performance Requirements:** - The implementation should be efficient and leverage the relevant methods from the `calendar` module. 5. **Example:** ```python holidays = [(1, 1), (12, 25)] # New Year\'s Day and Christmas print(generate_calendar_summary(2023, 1, holidays)) # Output: {\'num_weekdays\': 21, \'num_weekends\': 10, \'contains_holiday\': True} ``` **Notes:** - Make use of the `calendar` module to handle date calculations and iterations. - Pay attention to how weekends and weekdays are classified. - Test the function with various inputs to ensure accuracy.","solution":"import calendar def generate_calendar_summary(year: int, month: int, holidays: list) -> dict: Generates a summary for a given month and year, indicating the number of weekdays, weekends, and if the month contains any holidays. Parameters: year (int): The year. month (int): The month (1-12). holidays (list): A list of tuples containing holidays in the format (month, day). Returns: dict: A dictionary with number of weekdays, number of weekends, and if it contains holidays. cal = calendar.Calendar() num_weekdays = 0 num_weekends = 0 contains_holiday = False for day in cal.itermonthdays2(year, month): day_num, day_of_week = day if day_num == 0: continue if day_of_week < 5: num_weekdays += 1 else: num_weekends += 1 for holiday in holidays: if holiday[0] == month: contains_holiday = True break return { \\"num_weekdays\\": num_weekdays, \\"num_weekends\\": num_weekends, \\"contains_holiday\\": contains_holiday }"},{"question":"As a developer, you need to create a command-line utility for processing a database of user data. The utility should support the following functionalities: 1. **Create a new user** with a name, age, and email. 2. **Delete an existing user** by email. 3. **Update the age** of an existing user. 4. **List all users** in alphabetical order by their name. Each user should be represented as a dictionary with the keys: \'name\', \'age\', and \'email\'. All users will be stored in a list. Implement the command-line interface using the `argparse` module to meet the requirements above. # Requirements and Specifications: 1. The command line should accept the following sub-commands: - **create**: Creates a new user. - Required arguments: `--name`, `--age`, `--email` - **delete**: Deletes an existing user by email. - Required arguments: `--email` - **update-age**: Updates the age of an existing user. - Required arguments: `--email`, `--age` - **list**: Lists all users in alphabetical order by their name. No additional arguments are required. 2. The email should be unique for each user. 3. Print appropriate error messages if the commands are used incorrectly (e.g., attempting to create a user with an email that already exists). 4. Ensure the users are printed in a user-friendly format when listing them. # Input: - Command-line arguments as described above. # Output: - Results printed directly to the console. # Constraints: - The age should be an integer between 0 and 120. - The email should be a valid email format. # Example Usage: Assume the script is named `userdb.py`. ```shell python userdb.py create --name \\"John Doe\\" --age 30 --email john@example.com User John Doe created successfully. python userdb.py list 1. Name: John Doe, Age: 30, Email: john@example.com python userdb.py update-age --email john@example.com --age 31 User John Doe\'s age updated successfully. python userdb.py delete --email john@example.com User with email john@example.com deleted successfully. python userdb.py list No users found. ``` # Implementation: You need to implement the `main()` function and any helper functions as necessary. Here is a basic structure to get you started: ```python import argparse import re users = [] def create_user(args): global users # Implementation for creating a user ... def delete_user(args): global users # Implementation for deleting a user ... def update_age(args): global users # Implementation for updating age of a user ... def list_users(args): global users # Implementation for listing users ... def valid_email(email): regex = r\'^b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\' return re.match(regex, email) def main(): parser = argparse.ArgumentParser(description=\'User database management utility.\') subparsers = parser.add_subparsers(title=\'subcommands\', description=\'valid subcommands\', help=\'additional help\', dest=\'command\', required=True) # Sub-parser for \'create\' command create_parser = subparsers.add_parser(\'create\', help=\'Create a new user\') create_parser.add_argument(\'--name\', required=True, help=\'Name of the user\') create_parser.add_argument(\'--age\', required=True, type=int, help=\'Age of the user\') create_parser.add_argument(\'--email\', required=True, help=\'Email of the user\') create_parser.set_defaults(func=create_user) # Sub-parser for \'delete\' command delete_parser = subparsers.add_parser(\'delete\', help=\'Delete an existing user\') delete_parser.add_argument(\'--email\', required=True, help=\'Email of the user to be deleted\') delete_parser.set_defaults(func=delete_user) # Sub-parser for \'update-age\' command update_age_parser = subparsers.add_parser(\'update-age\', help=\'Update the age of an existing user\') update_age_parser.add_argument(\'--email\', required=True, help=\'Email of the user\') update_age_parser.add_argument(\'--age\', required=True, type=int, help=\'New age of the user\') update_age_parser.set_defaults(func=update_age) # Sub-parser for \'list\' command list_parser = subparsers.add_parser(\'list\', help=\'List all users\') list_parser.set_defaults(func=list_users) args = parser.parse_args() if args.command: args.func(args) if __name__ == \\"__main__\\": main() ``` # Note: - Make sure to handle edge cases and print appropriate messages for invalid input. - You may implement additional helper functions as necessary.","solution":"import argparse import re users = [] def valid_email(email): regex = r\'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Za-z]{2,}\' return re.match(regex, email) def create_user(args): global users name = args.name age = args.age email = args.email if not valid_email(email): print(f\\"Error: {email} is not a valid email address.\\") return if any(user[\'email\'] == email for user in users): print(f\\"Error: User with email {email} already exists.\\") return if age < 0 or age > 120: print(f\\"Error: Age must be between 0 and 120.\\") return new_user = {\'name\': name, \'age\': age, \'email\': email} users.append(new_user) print(f\\"User {name} created successfully.\\") def delete_user(args): global users email = args.email for i, user in enumerate(users): if user[\'email\'] == email: del users[i] print(f\\"User with email {email} deleted successfully.\\") return print(f\\"Error: User with email {email} not found.\\") def update_age(args): global users email = args.email age = args.age if age < 0 or age > 120: print(f\\"Error: Age must be between 0 and 120.\\") return for user in users: if user[\'email\'] == email: user[\'age\'] = age print(f\\"User {user[\'name\']}\'s age updated successfully.\\") return print(f\\"Error: User with email {email} not found.\\") def list_users(args): global users if not users: print(\\"No users found.\\") return sorted_users = sorted(users, key=lambda user: user[\'name\']) for i, user in enumerate(sorted_users, start=1): print(f\\"{i}. Name: {user[\'name\']}, Age: {user[\'age\']}, Email: {user[\'email\']}\\") def main(): parser = argparse.ArgumentParser(description=\'User database management utility.\') subparsers = parser.add_subparsers(title=\'subcommands\', description=\'valid subcommands\', help=\'additional help\', dest=\'command\', required=True) create_parser = subparsers.add_parser(\'create\', help=\'Create a new user\') create_parser.add_argument(\'--name\', required=True, help=\'Name of the user\') create_parser.add_argument(\'--age\', required=True, type=int, help=\'Age of the user\') create_parser.add_argument(\'--email\', required=True, help=\'Email of the user\') create_parser.set_defaults(func=create_user) delete_parser = subparsers.add_parser(\'delete\', help=\'Delete an existing user\') delete_parser.add_argument(\'--email\', required=True, help=\'Email of the user to be deleted\') delete_parser.set_defaults(func=delete_user) update_age_parser = subparsers.add_parser(\'update-age\', help=\'Update the age of an existing user\') update_age_parser.add_argument(\'--email\', required=True, help=\'Email of the user\') update_age_parser.add_argument(\'--age\', required=True, type=int, help=\'New age of the user\') update_age_parser.set_defaults(func=update_age) list_parser = subparsers.add_parser(\'list\', help=\'List all users\') list_parser.set_defaults(func=list_users) args = parser.parse_args() if args.command: args.func(args) if __name__ == \\"__main__\\": main()"},{"question":"Background You are tasked with creating a Python application that manages a list of tasks. Each task has a title, description, and completion status. The tasks should be stored persistently on disk and should support both serialization to a file and storage in an SQLite database. Task Implement a class `TaskManager` that provides the following functionalities: 1. **Add a Task**: Add a new task with a title and description. By default, the completion status is `False`. 2. **List All Tasks**: Return a list of all tasks. 3. **Mark Task as Completed**: Mark a task as completed by its title. 4. **Save Tasks to File**: Serialize the list of tasks to a file using the `pickle` module. 5. **Load Tasks from File**: Deserialize the list of tasks from the file, restoring the state of the task manager. 6. **Save Tasks to Database**: Save the tasks to an SQLite database. 7. **Load Tasks from Database**: Load tasks from an SQLite database, restoring the state of the task manager. Each task should be represented by a dictionary with the following keys: - `title` (str): The title of the task. - `description` (str): The description of the task. - `completed` (bool): The completion status of the task. Requirements 1. Implement the class `TaskManager` with the necessary methods as described. 2. Use the `pickle` module for serializing and deserializing tasks. 3. Use the `sqlite3` module for storing and retrieving tasks from a database. 4. Ensure that the methods handle exceptions gracefully and log relevant messages. Constraints 1. Assume that the titles of the tasks are unique. 2. The file for `pickle` operations will be named `tasks.pkl`. 3. The SQLite database file will be named `tasks.db`. 4. Make sure to use SQL placeholders to avoid SQL injection. Example Usage ```python manager = TaskManager() manager.add_task(\'Task1\', \'Description of Task 1\') manager.add_task(\'Task2\', \'Description of Task 2\') manager.mark_task_completed(\'Task1\') manager.save_tasks_to_file() # Tasks are saved to tasks.pkl manager.load_tasks_from_file() # Tasks are loaded from tasks.pkl manager.save_tasks_to_db() # Tasks are saved to tasks.db manager.load_tasks_from_db() # Tasks are loaded from tasks.db for task in manager.list_all_tasks(): print(task) ``` Expected Output ``` {\'title\': \'Task1\', \'description\': \'Description of Task 1\', \'completed\': True} {\'title\': \'Task2\', \'description\': \'Description of Task 2\', \'completed\': False} ```","solution":"import pickle import sqlite3 import os class TaskManager: def __init__(self): self.tasks = [] def add_task(self, title, description): self.tasks.append({ \'title\': title, \'description\': description, \'completed\': False }) def list_all_tasks(self): return self.tasks def mark_task_completed(self, title): for task in self.tasks: if task[\'title\'] == title: task[\'completed\'] = True return raise ValueError(\\"Task not found\\") def save_tasks_to_file(self): with open(\'tasks.pkl\', \'wb\') as f: pickle.dump(self.tasks, f) def load_tasks_from_file(self): if os.path.exists(\'tasks.pkl\'): with open(\'tasks.pkl\', \'rb\') as f: self.tasks = pickle.load(f) else: raise FileNotFoundError(\\"tasks.pkl not found\\") def save_tasks_to_db(self): conn = sqlite3.connect(\'tasks.db\') c = conn.cursor() c.execute(\'\'\'CREATE TABLE IF NOT EXISTS tasks (title TEXT PRIMARY KEY, description TEXT, completed BOOLEAN)\'\'\') c.execute(\'DELETE FROM tasks\') # Clear the table before inserting for task in self.tasks: c.execute(\'INSERT INTO tasks (title, description, completed) VALUES (?, ?, ?)\', (task[\'title\'], task[\'description\'], task[\'completed\'])) conn.commit() conn.close() def load_tasks_from_db(self): conn = sqlite3.connect(\'tasks.db\') c = conn.cursor() c.execute(\'\'\'CREATE TABLE IF NOT EXISTS tasks (title TEXT PRIMARY KEY, description TEXT, completed BOOLEAN)\'\'\') c.execute(\'SELECT title, description, completed FROM tasks\') self.tasks = [{\'title\': row[0], \'description\': row[1], \'completed\': row[2]} for row in c.fetchall()] conn.close()"},{"question":"**Email Parsing Challenge** **Objective:** Implement a function that reads and parses an email message from a provided source incrementally using the `BytesFeedParser` API. The source will be a stream that simulates reading from a socket. Your function should handle both simple and multipart messages, extracting and printing the email\'s subject and body. If the email is multipart, extract and print each part\'s content separately. **Function Signature:** ```python def parse_email_from_stream(stream) -> None: # stream: An iterable providing bytes-like objects, simulating a non-blocking socket. # The function should parse the email message and print the subject and body. pass ``` **Input:** - `stream`: An iterable that yields chunks of byte-like objects. Each chunk represents part of the email being read incrementally. **Output:** - Print the email\'s subject. - Print the body of the email. If the email is multipart, print each part of the content separately. **Constraints:** - Handle large email messages by reading incrementally. - Be capable of handling both standards-compliant and non-compliant messages. - Assume the `stream` always ends with a complete message (i.e., no partial messages at the end). **Example Usage:** ```python import io def simulate_stream(email_string): Simulates reading from a socket by yielding small chunks of bytes. email_bytes = email_string.encode(\\"utf-8\\") for i in range(0, len(email_bytes), 20): yield email_bytes[i:i+20] email_string = From: sender@example.com To: receiver@example.com Subject: Test Email This is the body of the email. parse_email_from_stream(simulate_stream(email_string)) # Expected Output: # Subject: Test Email # Body: This is the body of the email. ``` **Notes:** - Use the `BytesFeedParser` class to handle incremental parsing. - Ensure your function handles MIME messages correctly, distinguishing between parts if the message is multipart. - If the email has multiple parts, print each part\'s content separately. **Hints:** - Utilize `feed(data)` to provide data incrementally. - Use `close()` to finalize and retrieve the message object. - To access the subject, you can use the `get(\'subject\')` method on the message object. - To correctly handle and extract the body (or parts), check for `is_multipart()` and then iterate through parts if necessary using methods like `walk()`.","solution":"from email import message_from_bytes from email.parser import BytesFeedParser def parse_email_from_stream(stream) -> None: parser = BytesFeedParser() for chunk in stream: parser.feed(chunk) email_message = parser.close() subject = email_message.get(\'subject\', \'No Subject\') print(f\\"Subject: {subject}\\") if email_message.is_multipart(): for part in email_message.walk(): if part.get_content_type() == \\"text/plain\\": print(\\"Body:\\", part.get_payload(decode=True).decode(part.get_content_charset(\'utf-8\'))) else: print(\\"Body:\\", email_message.get_payload(decode=True).decode(email_message.get_content_charset(\'utf-8\')))"},{"question":"# Question: Implementing and Evaluating a Regression Model using scikit-learn **Objective:** In this task, you need to implement a regression model to predict target values based on a synthetic dataset. The primary goal is to demonstrate your ability to use scikit-learn for data preprocessing, model training, and evaluation. **Description:** You are provided with synthetic data representing features and target values. Your task is to: 1. Preprocess the data. 2. Train a regression model using GradientBoostingRegressor. 3. Evaluate the model\'s performance using a train-test split approach. **Requirements:** 1. **Synthetic Data Generation:** - Generate a synthetic dataset using `make_regression` from scikit-learn. - The dataset should have 1000 samples and 20 features. 2. **Data Preprocessing:** - Normalize the feature set using `StandardScaler` from scikit-learn. 3. **Model Training and Evaluation:** - Split the dataset into training and testing sets using `train_test_split` with 80% for training and 20% for testing. - Train a `GradientBoostingRegressor` model with `n_estimators=100` and `random_state=42`. - Evaluate the model\'s performance using `R^2 score`. **Implementation Details:** 1. **Function Signature:** ```python def regression_model_evaluation(): pass ``` 2. **Expected Output:** - Print the `R^2 score` of the model on the test set. **Constraints:** - Use `random_state=42` for reproducibility where needed. - The function does not take any input parameters and should generate the output directly. Here is a skeleton of the function to help you get started: ```python def regression_model_evaluation(): import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score # Step 1: Generate synthetic data X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Step 2: Preprocess the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Step 4: Train the model model = GradientBoostingRegressor(n_estimators=100, random_state=42) model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) score = r2_score(y_test, y_pred) # Print the R^2 score print(f\\"R^2 score: {score:.4f}\\") # Call the function to verify implementation regression_model_evaluation() ``` **Expected Output Example:** ``` R^2 score: 0.9215 ``` Your task is to complete the function `regression_model_evaluation` using the instructions above and ensure it executes correctly.","solution":"def regression_model_evaluation(): import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score # Step 1: Generate synthetic data X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Step 2: Preprocess the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Step 4: Train the model model = GradientBoostingRegressor(n_estimators=100, random_state=42) model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = model.predict(X_test) score = r2_score(y_test, y_pred) # Print the R^2 score print(f\\"R^2 score: {score:.4f}\\")"},{"question":"# Question Objective: Your task is to implement a function in Python using PyTorch that attaches metadata to a TorchScript model and saves it. This demonstrates your understanding of how to manipulate TorchScript models and work with extra files. Problem Statement: Write a function `save_model_with_metadata` that: 1. Takes a PyTorch model, a file path to save the model, and a dictionary of metadata as input. 2. Converts the model to TorchScript using `torch.jit.script`. 3. Saves the model along with the provided metadata using the `_extra_files` parameter. Inputs: - `model`: A PyTorch model instance (an object of class `torch.nn.Module`). - `file_path`: A string representing the file path where the TorchScript model should be saved. - `metadata`: A dictionary with string keys and string values containing metadata information to be attached to the model. Outputs: - The function should not return anything, but the model should be saved to the specified file path with the metadata attached. Constraints: - The metadata dictionary will have at most 5 key-value pairs. - The file path will be a valid path where the model can be saved. - The function should raise an appropriate error if the saving process fails. Example Usage: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) # Create a model instance model = SimpleModel() # Define metadata metadata = { \\"description\\": \\"Simple linear model\\", \\"author\\": \\"John Doe\\", \\"version\\": \\"1.0\\" } # Define file path file_path = \\"simple_model.pt\\" # Save the model with metadata save_model_with_metadata(model, file_path, metadata) # The saved model at \'simple_model.pt\' should now include the metadata. ``` Additional Information: - You may assume `torch` is already imported in the environment where this function will be executed. - The function should be robust and handle potential errors gracefully, providing clear error messages.","solution":"import torch def save_model_with_metadata(model, file_path, metadata): Save the given PyTorch model as a TorchScript model with additional metadata. Args: - model (torch.nn.Module): The PyTorch model to be saved. - file_path (str): The file path where the TorchScript model should be saved. - metadata (dict): A dictionary containing metadata to be attached to the model. Raises: - RuntimeError: If saving the model fails for some reason. # Convert the model to TorchScript try: scripted_model = torch.jit.script(model) except Exception as e: raise RuntimeError(f\\"Failed to convert model to TorchScript: {str(e)}\\") # Prepare the extra files dictionary extra_files = {f\'metadata_{key}.txt\': value.encode(\'utf-8\') for key, value in metadata.items()} # Save the model with the metadata attached try: scripted_model.save(file_path, _extra_files=extra_files) except Exception as e: raise RuntimeError(f\\"Failed to save the model with metadata: {str(e)}\\")"},{"question":"You are to develop a small simulation based on pattern matching and control flow constructs discussed in the provided documentation. The simulation will model a simple inventory management system for a store. **Requirements:** 1. **Item**: - An item should be represented as a dictionary with keys: `id`, `name`, `category`, and `quantity`. - Example: `{\\"id\\": 1, \\"name\\": \\"Apple\\", \\"category\\": \\"Fruit\\", \\"quantity\\": 50}` 2. **Functions**: - Implement the following functions: ```python def add_item(inventory, item): Adds a new item to the inventory. If the item already exists (based on `id`), it updates the quantity. Parameters: inventory (list): The list representing the inventory. item (dict): The item to add or update in the inventory. Returns: list: The updated inventory. def find_item(inventory, id): Finds an item in the inventory by `id`. Parameters: inventory (list): The list representing the inventory. id (int): The id of the item to find. Returns: dict or None: The item if found, or None if not found. def remove_item(inventory, id): Removes an item from the inventory by `id`. Parameters: inventory (list): The list representing the inventory. id (int): The id of the item to remove. Returns: list: The updated inventory or an appropriate message if the item is not found. def generate_report(inventory): Generates a report of the inventory, listing all items sorted by category. Parameters: inventory (list): The list representing the inventory. Returns: str: A formatted string representing the inventory report. ``` 3. **Constraints**: - Assume that `id` is unique for each item. - Use pattern matching in the `find_item`, `remove_item`, and `generate_report` functions. - Utilize `if`, `while`, and `for` statements where applicable. - Handle exceptions appropriately using `try`, `except`, `else`, and `finally`. 4. **Performance**: - Aim for a solution that performs operations with a time complexity of O(n) or better, where n is the number of items in the inventory. # Example ```python inventory = [] # Adding items to inventory inventory = add_item(inventory, {\\"id\\": 1, \\"name\\": \\"Apple\\", \\"category\\": \\"Fruit\\", \\"quantity\\": 50}) inventory = add_item(inventory, {\\"id\\": 2, \\"name\\": \\"Banana\\", \\"category\\": \\"Fruit\\", \\"quantity\\": 30}) # Finding an item item = find_item(inventory, 1) print(item) # Output: {\\"id\\": 1, \\"name\\": \\"Apple\\", \\"category\\": \\"Fruit\\", \\"quantity\\": 50} # Removing an item inventory = remove_item(inventory, 1) print(inventory) # Output: [{\\"id\\": 2, \\"name\\": \\"Banana\\", \\"category\\": \\"Fruit\\", \\"quantity\\": 30}] # Generating a report report = generate_report(inventory) print(report) ``` **Notes**: - Ensure your code is well-documented and follows clean coding practices. - Pay attention to edge cases, such as attempting to remove an item that does not exist in the inventory.","solution":"def add_item(inventory, item): Adds a new item to the inventory. If the item already exists (based on `id`), it updates the quantity. Parameters: inventory (list): The list representing the inventory. item (dict): The item to add or update in the inventory. Returns: list: The updated inventory. for i, inv_item in enumerate(inventory): if inv_item[\'id\'] == item[\'id\']: inventory[i][\'quantity\'] += item[\'quantity\'] return inventory inventory.append(item) return inventory def find_item(inventory, id): Finds an item in the inventory by `id`. Parameters: inventory (list): The list representing the inventory. id (int): The id of the item to find. Returns: dict or None: The item if found, or None if not found. for item in inventory: if item[\'id\'] == id: return item return None def remove_item(inventory, id): Removes an item from the inventory by `id`. Parameters: inventory (list): The list representing the inventory. id (int): The id of the item to remove. Returns: list: The updated inventory or an appropriate message if the item is not found. for i, item in enumerate(inventory): if item[\'id\'] == id: del inventory[i] return inventory return inventory def generate_report(inventory): Generates a report of the inventory, listing all items sorted by category. Parameters: inventory (list): The list representing the inventory. Returns: str: A formatted string representing the inventory report. report = \\"\\" sorted_inventory = sorted(inventory, key=lambda x: x[\'category\']) for item in sorted_inventory: report += f\\"ID: {item[\'id\']}, Name: {item[\'name\']}, Category: {item[\'category\']}, Quantity: {item[\'quantity\']}n\\" return report"},{"question":"**File and Directory Organizing Tool** **Objective:** Create a program that organizes files in a specified directory based on their extensions. Your program should use the `pathlib` and `shutil` modules to move files into categorized subdirectories. This task will assess your understanding of file handling, directory manipulation, and path operations. **Tasks:** 1. Define a function `organize_files(directory: str) -> None`: - Input: The function takes a single argument `directory` which is the path of the directory to organize. - Output: The function does not return anything but organizes the files within the specified directory. 2. Inside the function: - Use `pathlib` to iterate over files in the specified directory. - Create subdirectories named after the file extensions (e.g., `.txt`, `.pdf`, `.jpg`). - Move each file into its corresponding subdirectory using `shutil`. 3. Implement error handling to deal with possible issues such as: - The specified directory does not exist. - Subdirectories already exist. - File moves fail due to permission issues. 4. Ensure that non-file entries (e.g., directories, symbolic links) in the specified directory are ignored. **Constraints:** - Do not use external libraries other than those specified (`pathlib` and `shutil`). - Organize only the first-level files in the specified directory (do not recurse into subdirectories). **Example:** Suppose the input directory structure is: ``` /example file1.txt file2.pdf image1.jpg image2.png file3.txt ``` After calling `organize_files(\\"/example\\")`, the directory structure should be: ``` /example /txt file1.txt file3.txt /pdf file2.pdf /jpg image1.jpg /png image2.png ``` **Your implementation should demonstrate good coding practices, including code readability, proper use of functions, modularity, and comments explaining your code.**","solution":"from pathlib import Path import shutil def organize_files(directory: str) -> None: Organizes files in the specified directory into subdirectories based on file extensions. Parameters: directory (str): Path to the directory to organize. Returns: None dir_path = Path(directory) if not dir_path.exists(): raise FileNotFoundError(f\\"The directory {directory} does not exist.\\") if not dir_path.is_dir(): raise NotADirectoryError(f\\"The path {directory} is not a directory.\\") for item in dir_path.iterdir(): if item.is_file(): file_extension = item.suffix[1:] # Remove the \'.\' from the suffix to get the extension if file_extension: # Ensure the extension is not empty target_dir = dir_path / file_extension target_dir.mkdir(exist_ok=True) # Create the directory if it doesn\'t exist try: shutil.move(str(item), str(target_dir / item.name)) except Exception as e: print(f\\"Failed to move {item.name}: {e}\\")"},{"question":"**Design a Terminal Emulator Using `pty` Module** **Objective:** Create a simple terminal emulator that can execute shell commands and display their output using the `pty` module. This exercise will test your understanding of process manipulation, terminal handling, and I/O operations in Python. **Requirements:** 1. Implement a function `run_terminal_emulator()` that: - Uses `pty.openpty()` to create a new pseudo-terminal pair. - Forks a new process using `pty.fork()`. - In the child process, replaces the child\'s standard input, output, and error with the slave end of the pseudo-terminal, then executes a shell. - In the parent process, interacts with the child process through the master end of the pseudo-terminal, forwarding user input from the parent process’s standard input to the child process and displaying the output from the child process back to the parent process’s standard output. **Function Signature:** ```python def run_terminal_emulator(): pass ``` **Constraints:** - The function should terminate when the shell process in the child process exits. - Handle edge cases such as the child process terminating unexpectedly. - Assume the target environment is POSIX-compliant (Linux, macOS). **Example Usage:** ```python # To run the terminal emulator, simply call: run_terminal_emulator() # This will create a terminal-like interface where you can type shell commands and see the output. # For example, typing \'ls\' will show the list of files in the current directory as it would in a normal terminal. ``` **Performance Requirements:** - Efficiently handle I/O operations to minimize latency between user input and shell output. - Ensure proper cleanup of resources (file descriptors) after the shell process exits. **Evaluation Criteria:** - Correctness and completeness of the implementation. - Proper handling of pseudo-terminals and process I/O. - Robustness against edge cases and unexpected inputs. - Code readability and style.","solution":"import os import pty import sys def run_terminal_emulator(): # Create a new pseudo-terminal pair master_fd, slave_fd = pty.openpty() # Fork a new process pid = os.fork() if pid == 0: # Child process os.close(master_fd) # Set up the child\'s file descriptors os.dup2(slave_fd, sys.stdin.fileno()) os.dup2(slave_fd, sys.stdout.fileno()) os.dup2(slave_fd, sys.stderr.fileno()) if slave_fd > sys.stderr.fileno(): os.close(slave_fd) # Execute a shell os.execv(\'/bin/sh\', [\'/bin/sh\']) else: # Parent process os.close(slave_fd) # Forward input to the child process and show output while True: try: # Read input from user data = os.read(sys.stdin.fileno(), 1024) if not data: break # Write to the pseudo-terminal master file descriptor os.write(master_fd, data) # Read output from the child process output = os.read(master_fd, 1024) if not output: break # Write to the standard output os.write(sys.stdout.fileno(), output) except OSError: break os.close(master_fd)"},{"question":"Objective You are required to implement a configuration file parser using the `configparser` module in Python 3.10. Your parser should be able to read an existing configuration file, add new sections and options, retrieve values with fallback support, and demonstrate value interpolation. Requirements 1. **Reading Configuration**: - Implement a function `read_config(file_path: str) -> configparser.ConfigParser` that reads a given INI configuration file and returns a `ConfigParser` object. 2. **Adding Sections and Options**: - Implement a function `add_section_option(config: configparser.ConfigParser, section: str, option: str, value: str) -> None` that adds a new section (if it doesn\'t already exist) and an option with the provided value to the `ConfigParser` object. 3. **Retrieving Values with Fallback**: - Implement a function `get_value(config: configparser.ConfigParser, section: str, option: str, fallback: str) -> str` that retrieves the value of a given option under a specified section, returning the fallback value if the option is not found. 4. **Value Interpolation**: - Implement a function `interpolated_value(config: configparser.ConfigParser, section: str, option: str) -> str` that retrieves the value of an option, demonstrating interpolation (e.g., using `%(some_option)s` syntax). Input and Output Formats - The configuration file will be in standard INI format. - The functions will be directly tested by the inputs provided during execution. Constraints - It is guaranteed that the configuration file path provided will be a valid path. - Assume all inputs to functions are valid as per the specifications. Example Input and Output Consider the following INI file (`example.ini`): ```ini [Settings] ServerName = localhost Port = 8080 [Database] User = admin Password = secret Name = test_db [Paths] LogPath = /var/log/app ``` Example function calls and outputs: 1. **Reading Configuration:** ```python config = read_config(\'example.ini\') print(config.sections()) ``` **Output:** ``` [\'Settings\', \'Database\', \'Paths\'] ``` 2. **Adding Sections and Options:** ```python add_section_option(config, \'Settings\', \'Timeout\', \'30\') add_section_option(config, \'NewSection\', \'NewOption\', \'NewValue\') print(config[\'Settings\'][\'Timeout\']) # Output: 30 print(config[\'NewSection\'][\'NewOption\']) # Output: NewValue ``` 3. **Retrieving Values with Fallback:** ```python value = get_value(config, \'Settings\', \'ServerName\', \'default_server\') print(value) # Output: localhost default_value = get_value(config, \'NonExistent\', \'SomeOption\', \'default_value\') print(default_value) # Output: default_value ``` 4. **Value Interpolation:** ```python config.set(\'Settings\', \'FullAddress\', \'%(ServerName)s:%(Port)s\') interpolated = interpolated_value(config, \'Settings\', \'FullAddress\') print(interpolated) # Output: localhost:8080 ``` Submission Submit your implementation of the following functions: - `read_config(file_path: str) -> configparser.ConfigParser` - `add_section_option(config: configparser.ConfigParser, section: str, option: str, value: str) -> None` - `get_value(config: configparser.ConfigParser, section: str, option: str, fallback: str) -> str` - `interpolated_value(config: configparser.ConfigParser, section: str, option: str) -> str`","solution":"import configparser def read_config(file_path: str) -> configparser.ConfigParser: Reads a given INI configuration file and returns a ConfigParser object. config = configparser.ConfigParser() config.read(file_path) return config def add_section_option(config: configparser.ConfigParser, section: str, option: str, value: str) -> None: Adds a new section (if it doesn\'t already exist) and an option with the provided value to the ConfigParser object. if not config.has_section(section): config.add_section(section) config.set(section, option, value) def get_value(config: configparser.ConfigParser, section: str, option: str, fallback: str) -> str: Retrieves the value of a given option under a specified section, returning the fallback value if the option is not found. return config.get(section, option, fallback=fallback) def interpolated_value(config: configparser.ConfigParser, section: str, option: str) -> str: Retrieves the value of an option, demonstrating interpolation (e.g., using %(some_option)s syntax). return config.get(section, option)"},{"question":"Title: Implementing and Using Data Classes with Context Managers Objective: To evaluate students\' understanding of Python\'s `dataclasses` and `contextlib` modules by requiring the implementation of a data class and making use of a context manager to handle resource management. Task: 1. Implement a data class `Person` using Python\'s `dataclasses` module. This class should have the following attributes: - `name` (string) - `age` (integer) - `email` (string) 2. Write a context manager that will handle file operations, namely reading from a file and storing the data in objects of the `Person` class. This context manager should: - Open a file in read mode. - Read each line of the file where each line contains a comma-separated string representing a `name`, `age`, and `email`. - Create an instance of the `Person` class for each line and store these instances in a list. 3. The context manager should provide access to this list of `Person` objects. Additional Constraints: - The `Person` data class should use type annotations for all fields. - Use `contextlib` to create the custom context manager. - Ensure proper resource management using the context manager to handle file opening and closing. Expected Input and Output: - **Input:** A file named `people.txt` with the following content: ``` John Doe,30,john.doe@example.com Jane Smith,25,jane.smith@example.com Alice Johnson,28,alice.johnson@example.com ``` - **Output:** A list of `Person` objects created from the file data. Performance Requirements: - Efficiently handle file reading and object creation even for larger files. Example Usage: ```python from dataclasses import dataclass from contextlib import contextmanager # Step 1: Define the Person data class. @dataclass class Person: name: str age: int email: str # Step 2: Implement a context manager for reading the file and creating Person objects. @contextmanager def read_people_from_file(file_path): people = [] try: with open(file_path, \'r\') as file: for line in file: name, age, email = line.strip().split(\',\') people.append(Person(name, int(age), email)) yield people finally: pass # File is already closed by the nested `with` statement. # Usage Example file_path = \'people.txt\' with read_people_from_file(file_path) as people: for person in people: print(person) ``` This code, when executed, should print: ``` Person(name=\'John Doe\', age=30, email=\'john.doe@example.com\') Person(name=\'Jane Smith\', age=25, email=\'jane.smith@example.com\') Person(name=\'Alice Johnson\', age=28, email=\'alice.johnson@example.com\') ``` Note: Ensure that the provided file path exists and contains data in the specified format for accurate testing.","solution":"from dataclasses import dataclass from contextlib import contextmanager @dataclass class Person: name: str age: int email: str @contextmanager def read_people_from_file(file_path): people = [] try: with open(file_path, \'r\') as file: for line in file: name, age, email = line.strip().split(\',\') people.append(Person(name, int(age), email)) yield people finally: pass # The \'with\' statement has already ensured the file is closed."},{"question":"You are required to write a Python function utilizing the Unix-specific `pwd` and `grp` modules to gather and display information about the users on the system. The goal is to list all users, along with their user IDs (UID), primary group IDs (GID), and the names of their primary groups. # Specifications: 1. **Function Name**: `list_users_info` 2. **Output**: A list of dictionaries, where each dictionary represents a user with the following keys: - `\'username\'`: The username (string). - `\'uid\'`: The user ID (integer). - `\'gid\'`: The primary group ID (integer). - `\'group_name\'`: The name of the primary group (string). # Constraints: - The function should only use the `pwd` and `grp` modules to fetch necessary information. - The output list should be sorted in ascending order by the `username`. # Example: ```python def list_users_info(): # Your implementation here # Example usage: users_info = list_users_info() for user in users_info: print(user) ``` Assuming the system has the users `alice` (UID: 1001, GID: 1001), `bob` (UID: 1002, GID: 1002), and `charlie` (UID: 1000, GID: 1000), the output of `list_users_info()` should look like this: ```python [ {\'username\': \'alice\', \'uid\': 1001, \'gid\': 1001, \'group_name\': \'alice\'}, {\'username\': \'bob\', \'uid\': 1002, \'gid\': 1002, \'group_name\': \'bob\'}, {\'username\': \'charlie\', \'uid\': 1000, \'gid\': 1000, \'group_name\': \'charlie\'} ] ``` Write your implementation of the `list_users_info` function below.","solution":"import pwd import grp def list_users_info(): Returns a list of dictionaries, where each dictionary contains information about a user on the system. # Get all user entries users = pwd.getpwall() # Create a list of dictionaries with user info users_info = [] for user in users: user_info = { \\"username\\": user.pw_name, \\"uid\\": user.pw_uid, \\"gid\\": user.pw_gid, \\"group_name\\": grp.getgrgid(user.pw_gid).gr_name } users_info.append(user_info) # Sort the list by username users_info.sort(key=lambda x: x[\\"username\\"]) return users_info"},{"question":"# Comprehensive Path Management Using `pathlib` Objective: In this task, you’ll need to demonstrate your understanding of the `pathlib` module by implementing a function that performs multiple path-related operations. Background: Your task is to create a function that: 1. Takes a string representing the root directory from which to start. 2. Finds all Python files (`*.py`) in this directory and all its subdirectories. 3. Creates a summary file that contains the absolute paths of all these Python files. The summary file should be named `python_files_summary.txt`. 4. Reads back this summary file to print out the number of Python files found. 5. Verifies that the paths listed in the summary file still exist. Function Signature: ```python from pathlib import Path def summarize_python_files(root_dir: str) -> None: # Your code here ``` Input: - `root_dir` (str): the starting directory to search for Python files. Output: - No return value, but: - A file named `python_files_summary.txt` should be created in the root directory. - The number of Python files found should be printed to the console. - Print whether all paths listed in `python_files_summary.txt` still exist or not. Constraints: - The root directory provided by the user will be a valid directory path. - Assume there is read and write permission in the root directory. Example: ```python summarize_python_files(\'/path/to/start\') ``` Expected console output (given that 3 Python files were found): ``` Number of Python files found: 3 All paths in the summary file still exist: True ``` Notes: - You should use the `Path` class from the `pathlib` module. - Ensure you handle both relative and absolute paths correctly. - Make sure to test the function with some directories containing Python files. Evaluation: Your implementation will be assessed based on: - Correct use of the `pathlib` module. - Accuracy of file and path operations. - Adherence to the function signature. - Proper error handling and edge cases (e.g., directories with no `.py` files).","solution":"from pathlib import Path def summarize_python_files(root_dir: str) -> None: root_path = Path(root_dir) python_files = list(root_path.rglob(\'*.py\')) summary_file = root_path / \'python_files_summary.txt\' with summary_file.open(\'w\') as f: for py_file in python_files: f.write(str(py_file.absolute()) + \'n\') print(f\\"Number of Python files found: {len(python_files)}\\") with summary_file.open(\'r\') as f: file_paths = [Path(line.strip()) for line in f] all_exist = all(file_path.exists() for file_path in file_paths) print(f\\"All paths in the summary file still exist: {all_exist}\\")"},{"question":"You are provided with a dataset containing a mixture of normal observations and outliers. Your task is to implement a function that uses scikit-learn\'s novelty detection capabilities to identify whether new samples are inliers or outliers. # Dataset Description You will be working with a synthetic dataset with the following characteristics: - Gaussian distributed inlier points centered at (0, 0). - Outliers spread far from the center. # Data Input - `X_train`: A 2D NumPy array of shape (n_samples, 2). This data is clean and does not contain outliers. - `X_test`: A 2D NumPy array of shape (m_samples, 2). This data can contain novelties/outliers which need to be identified. # Function Signature ```python import numpy as np from sklearn.neighbors import LocalOutlierFactor def detect_novelties(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: Detect novelties in the X_test dataset using Local Outlier Factor (LOF). Parameters: - X_train: np.ndarray, training data without outliers, shape (n_samples, 2) - X_test: np.ndarray, test data potentially containing novelties, shape (m_samples, 2) Returns: - predictions: np.ndarray, predictions for X_test, 1 for inliers, -1 for outliers, shape (m_samples,) pass ``` # Constraints 1. You must use `LocalOutlierFactor` with the parameter `novelty=True`. 2. The function must return an array of shape `(m_samples,)` with 1 indicating inliers and -1 indicating outliers. 3. You should choose appropriate parameters for `LocalOutlierFactor` to ensure robust detection. # Example Usage ```python X_train = np.array([[0.1, 0.1], [-0.1, -0.1], [0, 0], [0.05, -0.05]]) X_test = np.array([[0.2, 0.2], [-0.2, -0.2], [5, 5], [0.1, 0.1], [-10, -10]]) predictions = detect_novelties(X_train, X_test) print(predictions) # Example Output: [ 1 1 -1 1 -1] ``` # Notes - Ensure your implementation is efficient and correctly applies the novelty detection as explained. - Extensive testing will be performed with various datasets to assess the robustness of your solution. - Your implementation should handle edge cases gracefully, such as very small or highly noisy test datasets.","solution":"import numpy as np from sklearn.neighbors import LocalOutlierFactor def detect_novelties(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: Detect novelties in the X_test dataset using Local Outlier Factor (LOF). Parameters: - X_train: np.ndarray, training data without outliers, shape (n_samples, 2) - X_test: np.ndarray, test data potentially containing novelties, shape (m_samples, 2) Returns: - predictions: np.ndarray, predictions for X_test, 1 for inliers, -1 for outliers, shape (m_samples,) clf = LocalOutlierFactor(n_neighbors=20, novelty=True) clf.fit(X_train) predictions = clf.predict(X_test) return predictions"},{"question":"Objective You are required to write a Python function that demonstrates your understanding of the `site` module\'s customization functionalities. The function should handle the setup of site-specific and user-specific directories for Python package installations. Task Implement a function `setup_python_environment()` that: 1. Adds a given directory to `sys.path` and processes its `.pth` files. 2. Returns the list of site-packages directories. 3. If a user-specific site-packages directory is enabled, adds it to `sys.path`. Function Signature ```python def setup_python_environment(directory: str, enable_user_site: bool) -> list: pass ``` Input - `directory` (str): The directory to be added to `sys.path`. - `enable_user_site` (bool): A flag indicating whether to enable and add the user-specific site-packages directory. Output - The function should return a list of current site-packages directories after adding the provided directory and potentially the user-specific directory. Constraints - The directory passed might not exist, so the function should handle this without throwing an error. - Performance should be optimal for handling directory lookups and file reads. Example Usage ```python import site def setup_python_environment(directory: str, enable_user_site: bool) -> list: import sys # Add the given directory to sys.path site.addsitedir(directory) # Get all global site-packages directories site_packages_dirs = site.getsitepackages() # If user site-packages is enabled, add it to sys.path if enable_user_site and site.ENABLE_USER_SITE: user_site_dir = site.getusersitepackages() site_packages_dirs.append(user_site_dir) sys.path.append(user_site_dir) return site_packages_dirs # Example of how the function might be used sp_dirs = setup_python_environment(\\"/custom/path/to/sitedir\\", True) print(sp_dirs) ``` # Additional Details - Ensure the function processes `.pth` files correctly, adding valid paths to `sys.path`. - Make use of `site.addsitedir`, `site.getsitepackages`, `site.getusersitepackages` functions as required. - Reflect the handling of non-existing directories gracefully. Your implementation should thus reflect an understanding of the `site` module, focusing on the appropriate usage of its functions to manage Python environment configurations effectively.","solution":"import site import sys import os def setup_python_environment(directory: str, enable_user_site: bool) -> list: Adds a given directory to sys.path and processes its .pth files. Returns the list of site-packages directories. If a user-specific site-packages directory is enabled, adds it to sys.path. Parameters: directory (str): The directory to be added to sys.path. enable_user_site (bool): A flag indicating whether to enable and add the user-specific site-packages directory. Returns: list: List of current site-packages directories after adding the provided directory and potentially the user-specific directory. # Only add the directory if it exists if os.path.exists(directory): site.addsitedir(directory) # Get all global site-packages directories site_packages_dirs = site.getsitepackages() # If user site-packages is enabled, add it to sys.path if enable_user_site and site.ENABLE_USER_SITE: user_site_dir = site.getusersitepackages() if user_site_dir not in sys.path: site.addsitedir(user_site_dir) site_packages_dirs.append(user_site_dir) return site_packages_dirs"},{"question":"Objective: Your task is to implement a Python script that demonstrates the use of the `atexit` module for registering functions to be called upon interpreter termination. You will need to implement a small simulation of a resource management system where resources are allocated and need to be properly released upon program termination. Instructions: 1. **Resource Allocation System**: - Implement a class called `ResourceManager`. - This class should have methods to allocate and release resources. For simplicity, these can just be strings representing resource names. 2. **Initialization**: - Upon instantiation, the `ResourceManager` should read the current state of resource allocations from a file called `resources.txt`. - If `resources.txt` does not exist, it should start with an empty list of resources. 3. **Methods**: - `allocate_resource(resource_name: str)`: This method adds a resource to the list of allocated resources. - `release_resource(resource_name: str)`: This method removes a resource from the list of allocated resources. - `save_resources()`: This method writes the current list of allocated resources to `resources.txt`. 4. **Automatic Resource Release**: - Use the `atexit` module to register the `save_resources` function to ensure the resources are saved when the script exits normally. - Also, provide a method to unregister this function if needed. Example Usage: ```python # Assuming the content of resources.txt is empty initially if __name__ == \\"__main__\\": manager = ResourceManager() manager.allocate_resource(\\"Resource1\\") manager.allocate_resource(\\"Resource2\\") # Unregister the exit handler (best used for testing purposes) manager.unregister_exit_handler() # Explicit call to save_resources (usually not needed) manager.save_resources() manager.release_resource(\\"Resource1\\") ``` Constraints: - You must handle possible exceptions, such as file I/O errors. - Ensure that the list of resources is saved in the order they were allocated. Evaluation Criteria: - Correct implementation of the `ResourceManager` class. - Proper handling of file operations and exceptions. - Effective use of the `atexit` module for registering and unregistering functions. - Correct implementation of resource allocation and release logic.","solution":"import atexit import os class ResourceManager: def __init__(self): self.resources = [] self.file_path = \'resources.txt\' self._load_resources() atexit.register(self.save_resources) def _load_resources(self): if os.path.exists(self.file_path): try: with open(self.file_path, \'r\') as f: self.resources = [line.strip() for line in f] except IOError as e: print(f\\"Error loading resources: {e}\\") def allocate_resource(self, resource_name: str): if resource_name not in self.resources: self.resources.append(resource_name) def release_resource(self, resource_name: str): if resource_name in self.resources: self.resources.remove(resource_name) def save_resources(self): try: with open(self.file_path, \'w\') as f: for resource in self.resources: f.write(resource + \'n\') except IOError as e: print(f\\"Error saving resources: {e}\\") def unregister_exit_handler(self): atexit.unregister(self.save_resources)"},{"question":"**Objective:** Demonstrate your understanding of Seaborn\'s `FacetGrid` by creating a complex visualization. **Problem Statement:** You are given a dataset `tips` that contains data about restaurant tips. Your task is to use Seaborn\'s `FacetGrid` to create a multi-faceted plot with the following specifications: 1. Map a scatterplot with `total_bill` on the x-axis and `tip` on the y-axis. 2. Divide the figure into subplots by `time` (row) and `sex` (column). 3. Use `smoker` as a `hue` parameter to condition the data on another variable and use different colors for each level of `smoker`. 4. Ensure the same hue mapping is applied across all facets. 5. Add a vertical reference line at the median value of `total_bill` for the dataset. 6. Add a custom annotation to each facet that shows the count (N) of data points. 7. Customize the axis labels, facet titles, and set specific axis limits and ticks. 8. Save the figure to a file named `facet_grid_plot.png`. **Input:** You will use the `tips` dataset provided by Seaborn. **Output:** A faceted grid plot saved as `facet_grid_plot.png`. **Constraints:** - Use Seaborn and Matplotlib only. - Do not hardcode dataset values. Calculate necessary statistics and values programmatically. **Instructions:** 1. Import necessary libraries. 2. Load the `tips` dataset using `sns.load_dataset(\\"tips\\")`. 3. Initialize a `FacetGrid` with `tips`, setting `time` for rows and `sex` for columns. 4. Map a scatterplot on the grid with `total_bill` and `tip`, using `smoker` as the hue. 5. Add a vertical reference line at the median of `total_bill`. 6. Create a custom annotation function to display the count of data points. 7. Customize axis labels, titles, and limits. 8. Save the plot to `facet_grid_plot.png`. **Solution Template:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Initialize the FacetGrid g = sns.FacetGrid(tips, row=\\"time\\", col=\\"sex\\", hue=\\"smoker\\", margin_titles=True) # Map a scatterplot to the grid g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Add a vertical reference line at the median of total_bill median_total_bill = tips[\\"total_bill\\"].median() g.refline(x=median_total_bill) # Define a custom annotation function def annotate(data, **kws): ax = plt.gca() n = len(data) ax.text(0.1, 0.9, f\\"N = {n}\\", transform=ax.transAxes) # Add the annotation to each facet g.map_dataframe(annotate) # Customize axis labels and titles g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") g.set_titles(row_template=\\"{row_name} time\\", col_template=\\"{col_name}\\") # Set axis limits and ticks g.set(xlim=(0, 60), ylim=(0, 12), xticks=[10, 30, 50], yticks=[2, 6, 10]) # Add a legend, adjust the layout, and save the plot g.add_legend() g.tight_layout() g.savefig(\\"facet_grid_plot.png\\") ``` Run the above code to generate and save the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Initialize the FacetGrid g = sns.FacetGrid(tips, row=\\"time\\", col=\\"sex\\", hue=\\"smoker\\", margin_titles=True) # Map a scatterplot to the grid g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Add a vertical reference line at the median of total_bill median_total_bill = tips[\\"total_bill\\"].median() g.refline(x=median_total_bill) # Define a custom annotation function def annotate(data, **kws): ax = plt.gca() n = len(data) ax.text(0.1, 0.9, f\\"N = {n}\\", transform=ax.transAxes) # Add the annotation to each facet g.map_dataframe(annotate) # Customize axis labels and titles g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") g.set_titles(row_template=\\"{row_name} time\\", col_template=\\"{col_name}\\") # Set axis limits and ticks g.set(xlim=(0, 60), ylim=(0, 12), xticks=[10, 30, 50], yticks=[2, 6, 10]) # Add a legend, adjust the layout, and save the plot g.add_legend() g.tight_layout() g.savefig(\\"facet_grid_plot.png\\")"},{"question":"# PyTorch Tensor Manipulation and Automatic Differentiation You are tasked with implementing a function that leverages PyTorch tensors to perform a series of operations and computations. Function Signature ```python import torch def manipulate_and_compute(input_matrix): This function takes a 2D input matrix, performs a series of tensor operations, and returns both the result of the computations and the gradient of a specific scalar output with respect to the input. Parameters: - input_matrix (list of lists of floats): A 2D list representation of the matrix. Returns: - result (torch.Tensor): The resulting tensor after the specified operations. - gradient (torch.Tensor): The gradient of the final scalar output with respect to the input tensor. pass ``` Instructions 1. Convert the `input_matrix` into a PyTorch tensor with `requires_grad=True`. 2. Perform the following tensor operations in order: - Compute the element-wise sine of the tensor. - Multiply the resulting tensor with a tensor of ones of the same shape. - Compute the mean of all the elements in the resulting tensor. 3. Calculate the gradient of this mean with respect to the original tensor. 4. Return the final tensor obtained after the operations and the gradient. Example ```python input_matrix = [[0.5, -1.2], [2.0, 3.1]] result, gradient = manipulate_and_compute(input_matrix) print(\\"Result Tensor:\\") print(result) # Expected output tensor after the series of operations (printed format may vary) print(\\"Gradient Tensor:\\") print(gradient) # Expected output tensor of gradients (printed format may vary) ``` Constraints - The input matrix will always be a 2x2 matrix with float elements. This question tests the student\'s ability to: - Convert data structures into PyTorch tensors. - Perform a sequence of tensor operations. - Utilize automatic differentiation to compute gradients. - Understand and manipulate PyTorch tensor attributes and methods.","solution":"import torch def manipulate_and_compute(input_matrix): This function takes a 2D input matrix, performs a series of tensor operations, and returns both the result of the computations and the gradient of a specific scalar output with respect to the input. Parameters: - input_matrix (list of lists of floats): A 2D list representation of the matrix. Returns: - result (torch.Tensor): The resulting tensor after the specified operations. - gradient (torch.Tensor): The gradient of the final scalar output with respect to the input tensor. # Convert input matrix to PyTorch tensor with requires_grad=True input_tensor = torch.tensor(input_matrix, dtype=torch.float32, requires_grad=True) # Perform the element-wise sine of the tensor sine_tensor = torch.sin(input_tensor) # Multiply the resulting tensor with a tensor of ones of the same shape ones_tensor = torch.ones_like(input_tensor) multiplied_tensor = sine_tensor * ones_tensor # Compute the mean of all the elements in the resulting tensor mean_tensor = multiplied_tensor.mean() # Calculate the gradient of this mean with respect to the original tensor mean_tensor.backward() # The gradient is stored in input_tensor.grad gradient = input_tensor.grad # Return the final tensor after the operations and the gradient return mean_tensor, gradient"},{"question":"Custom Seaborn Plot Creation **Objective**: Demonstrate your knowledge of seaborn\'s object-oriented plotting interface by creating a custom multi-layered plot. **Problem Statement**: You are provided with two datasets, `titanic` and `iris`, available within the seaborn package. Your task is to create a function, `create_custom_plot`, which generates a customized plot based on the specifications provided below. Function Specifications: - **Function Name**: `create_custom_plot` - **Input**: - `dataset_name` (string): Name of the dataset (`\\"titanic\\"` or `\\"iris\\"`). - `x_var` (string): The column name to be used as the x-axis variable. - `y_var` (string): The column name to be used as the y-axis variable. - `layers` (list of tuples): Each tuple contains the type of mark and optional transformations to be applied. Example: `[(\'Dots\', \'Jitter\'), (\'Range\', \'Perc(25,75)\', \'Shift(x=0.2)\')]`. - **Output**: A seaborn plot object. Requirements: 1. **Loading the Dataset**: Use `seaborn.load_dataset` to load the specified dataset. 2. **Creating the Plot**: Initialize a `seaborn.objects.Plot` object with the specified `x_var` and `y_var`. 3. **Adding Layers**: For each layer in the `layers` list, add the specified mark (`Dots`, `Range`, etc.) and apply the transformations (`Jitter`, `Perc`, `Shift`, etc.). 4. **Display**: Ensure the plot is displayed when the function is called. Example Usage: ```python # Example call to the function create_custom_plot(\\"titanic\\", \\"age\\", \\"fare\\", [(\\"Dots\\", \\"Jitter\\"), (\\"Range\\", \\"Perc(25,75)\\", \\"Shift(x=0.2)\\")]) # Example call to the function create_custom_plot(\\"iris\\", \\"sepal_length\\", \\"species\\", [(\\"Dots\\", \\"Jitter\\"), (\\"Range\\", \\"Perc(25,75)\\", \\"Shift(y=0.5)\\")]) ``` Constraints: 1. Ensure the function handles any basic errors in inputs gracefully, such as invalid dataset names or columns not found in the dataset. 2. The function should be self-contained and import all necessary seaborn components. Performance: - The function should efficiently load and plot the given dataset without significant delays. Use the seaborn documentation and the provided examples as a reference to complete this task.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_plot(dataset_name, x_var, y_var, layers): try: # Load dataset if dataset_name not in [\\"titanic\\", \\"iris\\"]: raise ValueError(\\"Dataset name must be \'titanic\' or \'iris\'\\") data = sns.load_dataset(dataset_name) if x_var not in data.columns or y_var not in data.columns: raise ValueError(f\\"Columns {x_var} and/or {y_var} not found in dataset\\") # Initialize the plot p = so.Plot(data, x=x_var, y=y_var) # Add layers for layer in layers: mark_type = layer[0] transformations = layer[1:] if mark_type == \\"Dots\\": plot_layer = so.Dot() elif mark_type == \\"Range\\": plot_layer = so.Range() else: raise ValueError(f\\"Unsupported mark type: {mark_type}\\") for transformation in transformations: if transformation == \\"Jitter\\": plot_layer = plot_layer.jitter() elif transformation.startswith(\\"Perc\\"): perc_values = transformation[5:-1] # Extract \\"(25,75)\\" perc_low, perc_high = map(int, perc_values.split(\\",\\")) plot_layer = plot_layer.percentile_bounds(perc_low, perc_high) elif transformation.startswith(\\"Shift\\"): shift_values = transformation[6:-1] # Extract \\"x=0.2\\", \\"y=0.5\\", etc. shifts = {k: float(v) for k, v in (item.split(\'=\') for item in shift_values.split(\',\'))} plot_layer = plot_layer.shift(**shifts) else: raise ValueError(f\\"Unsupported transformation: {transformation}\\") p = p.add(plot_layer) # Display the plot p.show() except Exception as e: print(f\\"Error: {e}\\") # Example usage create_custom_plot(\\"titanic\\", \\"age\\", \\"fare\\", [(\\"Dots\\", \\"Jitter\\"), (\\"Range\\", \\"Perc(25,75)\\", \\"Shift(x=0.2)\\")]) create_custom_plot(\\"iris\\", \\"sepal_length\\", \\"species\\", [(\\"Dots\\", \\"Jitter\\"), (\\"Range\\", \\"Perc(25,75)\\", \\"Shift(y=0.5)\\")])"},{"question":"Objective: To assess your ability to define and implement a custom Python type using the `PyTypeObject` structure, understanding and utilizing its various slots and functions. Problem Statement: You need to design a custom Python type in C, named `AdvancedNumber`, which behaves similarly to Python’s `int` type but with additional functionality: 1. Overrides the arithmetic operations (`+`, `-`, `*`, `/`). 2. Provides a method to return the squared value of the number. 3. Supports the rich comparison operations (`<`, `<=`, `==`, `!=`, `>`, `>=`). 4. Is iterable, and iterates from 0 to the number (inclusive). 5. Has a custom string representation using the `__str__` method. 6. Supports a custom `__call__` method, which, when called, returns the number increased by a given increment. Implement this type in C, filling the appropriate slots in the `PyTypeObject` structure to achieve the desired functionalities. Additionally, provide the necessary function definitions and initialization in the `PyTypeObject`. Expected Input and Output: - **Input:** Will not be taken from the user directly. - **Output:** Should match the operations specified (arithmetic, comparisons, iterability, string representation, and callable behavior). Constraints: - The implementation should follow best practices for memory management and use of the Python C API. - Ensure to handle edge cases (e.g., iterations should not go below zero, division by zero should raise an appropriate error). Performance Requirements: - Your implementation should be efficient and thread-safe, ensuring that the GIL is adequately managed. Solution Structure: 1. Define the `AdvancedNumber` type structure. 2. Implement functions corresponding to each slot (`tp_str`, `tp_call`, `tp_repr`, arithmetic functions, iteration functions, etc.). 3. Populate the `PyTypeObject` structure with these functions. 4. Provide example usage and test cases in Python to demonstrate the functionalities of your custom type. Example Code (Python Usage): ```python num = AdvancedNumber(5) print(num) # Should print a custom string representation of the number print(num + 3) # Should return 8 print(num - 2) # Should return 3 print(num * 2) # Should return 10 print(num / 1) # Should return 5.0 print(num == 5) # Should return True print(num < 6) # Should return True for i in num: print(i) # Should print numbers from 0 to 5 print(num()) # Should return 6 if increment is the default 1 print(num(3)) # Should return 8 print(num.square()) # Should return 25 ``` In your C implementation, ensure you provide detailed comments explaining each component and its role within the implementation of `AdvancedNumber`.","solution":"class AdvancedNumber: def __init__(self, value): self.value = value def __add__(self, other): return AdvancedNumber(self.value + other) def __sub__(self, other): return AdvancedNumber(self.value - other) def __mul__(self, other): return AdvancedNumber(self.value * other) def __truediv__(self, other): if other == 0: raise ValueError(\\"Cannot divide by zero\\") return AdvancedNumber(self.value / other) def __eq__(self, other): return self.value == other def __lt__(self, other): return self.value < other def __le__(self, other): return self.value <= other def __ne__(self, other): return self.value != other def __gt__(self, other): return self.value > other def __ge__(self, other): return self.value >= other def square(self): return self.value * self.value def __iter__(self): self._current = 0 return self def __next__(self): if self._current <= self.value: current = self._current self._current += 1 return current else: raise StopIteration def __str__(self): return f\\"AdvancedNumber({self.value})\\" def __call__(self, increment=1): return self.value + increment"},{"question":"# Dimensionality Reduction and Classification Pipeline Problem Description You are provided with a dataset containing high-dimensional data. Your task is to design a machine learning pipeline that performs the following steps: 1. Scales the dataset using `StandardScaler`. 2. Reduces the dimensionality of the dataset using `PCA` and `Random Projections`. 3. Combines the output of `PCA` and `Random Projections` using `FeatureAgglomeration`. 4. Applies a supervised classifier (e.g., `LogisticRegression`) on the reduced dataset. You will need to implement a function `dimensionality_reduction_pipeline` to achieve this. Function Signature ```python from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import SparseRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.linear_model import LogisticRegression def dimensionality_reduction_pipeline(X_train, y_train, X_test, n_pca_components, n_random_components, n_agglomeration_clusters): Creates a pipeline for dimensionality reduction and classification. Parameters: - X_train: numpy.ndarray, Training data of shape (n_samples_train, n_features) - y_train: numpy.ndarray, Training labels of shape (n_samples_train,) - X_test: numpy.ndarray, Test data of shape (n_samples_test, n_features) - n_pca_components: int, Number of components to keep after PCA - n_random_components: int, Number of components to keep from Random Projections - n_agglomeration_clusters: int, Number of clusters for Feature Agglomeration Returns: - y_test_pred: numpy.ndarray, Predicted labels for the test set pass ``` Requirements 1. **Input and Output Formats**: - `X_train`: A 2D numpy array of shape `(n_samples_train, n_features)` representing the training data. - `y_train`: A 1D numpy array of shape `(n_samples_train,)` representing the training labels. - `X_test`: A 2D numpy array of shape `(n_samples_test, n_features)` representing the test data. - `n_pca_components`: An integer representing the number of components to retain after PCA. - `n_random_components`: An integer representing the number of components to retain after Random Projections. - `n_agglomeration_clusters`: An integer representing the number of clusters to form in Feature Agglomeration. - The function should return `y_test_pred`, a 1D numpy array of predicted labels for the test set. 2. **Constraints**: - Ensure that the number of components specified for PCA and Random Projections does not exceed the number of original features. - Use `StandardScaler` for feature scaling. - `PCA` and `Random Projections` should be applied separately and their results should be combined using `FeatureAgglomeration`. 3. **Performance Requirements**: - Efficiently combine the dimensionality reduction techniques. - Ensure the pipeline is runnable within reasonable time limits for large datasets. # Example ```python import numpy as np # Generate dummy data X_train = np.random.rand(100, 50) y_train = np.random.randint(0, 2, 100) X_test = np.random.rand(50, 50) n_pca_components = 10 n_random_components = 10 n_agglomeration_clusters = 20 # Implementing the function y_test_pred = dimensionality_reduction_pipeline(X_train, y_train, X_test, n_pca_components, n_random_components, n_agglomeration_clusters) print(y_test_pred) ``` This problem tests your understanding of data preprocessing, dimensionality reduction techniques, and pipelining in scikit-learn. Ensure that you handle each step correctly and combine the techniques as specified. Good luck!","solution":"from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import SparseRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.linear_model import LogisticRegression import numpy as np def dimensionality_reduction_pipeline(X_train, y_train, X_test, n_pca_components, n_random_components, n_agglomeration_clusters): Creates a pipeline for dimensionality reduction and classification. Parameters: - X_train: numpy.ndarray, Training data of shape (n_samples_train, n_features) - y_train: numpy.ndarray, Training labels of shape (n_samples_train,) - X_test: numpy.ndarray, Test data of shape (n_samples_test, n_features) - n_pca_components: int, Number of components to keep after PCA - n_random_components: int, Number of components to keep from Random Projections - n_agglomeration_clusters: int, Number of clusters for Feature Agglomeration Returns: - y_test_pred: numpy.ndarray, Predicted labels for the test set # Define the transformers for PCA and Random Projections pca = PCA(n_components=n_pca_components) random_projection = SparseRandomProjection(n_components=n_random_components) # Combine PCA and Random Projections into a single feature space combined_features = FeatureUnion([ (\'pca\', pca), (\'random_projection\', random_projection) ]) # Create the pipeline with scaling, feature combination, and feature agglomeration pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'combined_features\', combined_features), (\'agglomeration\', FeatureAgglomeration(n_clusters=n_agglomeration_clusters)), (\'classifier\', LogisticRegression()) ]) # Fit the pipeline with the training data pipeline.fit(X_train, y_train) # Predict the labels for the test data y_test_pred = pipeline.predict(X_test) return y_test_pred"},{"question":"**Objective:** To assess the student\'s understanding of fundamental and advanced concepts of the Python \\"os\\" module. **Problem: Automate File Handling and Process Management** You are required to write a Python function `automate_file_handling(base_path: str) -> dict` that: 1. Creates a directory called `test_dir` inside the `base_path`. 2. Within `test_dir`, creates a text file named `example.txt` and writes the string \\"This is a test file.\\" into it. 3. Reads the content of `example.txt` and returns it as part of the output. 4. Spawns a new process using `os.fork()`. The child process should delete `example.txt` and exit. The parent process should wait for the child to complete and then cleanup `test_dir`. 5. Returns a dictionary containing: - `\\"content\\"`: The content read from `example.txt`. - `\\"status\\"`: A status message indicating whether `example.txt` was successfully deleted and `test_dir` was cleaned up. # Input: - `base_path` (str): The base directory where `test_dir` should be created. # Output: - A dictionary containing the content of the file before deletion and status messages. # Constraints: - Ensure proper handling of exceptions and cleanup actions. - Use appropriate `os` module functions to handle file and directories operations, process management, and other necessary tasks. - Handle any exceptions that may arise during the process and ensure that the directory `test_dir` is deleted in any case. # Example: ```python result = automate_file_handling(\'/path/to/base/dir\') print(result) # Output: { # \\"content\\": \\"This is a test file.\\", # \\"status\\": \\"example.txt successfully deleted and test_dir cleaned up.\\" # } ``` # Notes: - Ensure your code handles the creation and deletion of files and directories properly. - The function should handle scenarios where the file or directory might not exist when trying to delete them. - The provided base path can be any valid path on the system where the code is being executed.","solution":"import os import shutil def automate_file_handling(base_path: str) -> dict: test_dir_path = os.path.join(base_path, \\"test_dir\\") example_file_path = os.path.join(test_dir_path, \\"example.txt\\") try: # Create directory os.makedirs(test_dir_path, exist_ok=True) # Create and write to file with open(example_file_path, \'w\') as f: f.write(\\"This is a test file.\\") # Read the content of the file with open(example_file_path, \'r\') as f: content = f.read() # Fork a new process pid = os.fork() if pid == 0: # Child process try: # Delete the file if os.path.exists(example_file_path): os.remove(example_file_path) os._exit(0) except Exception as e: os._exit(1) else: # Parent process os.waitpid(pid, 0) # Cleanup the directory shutil.rmtree(test_dir_path) return { \\"content\\": content, \\"status\\": \\"example.txt successfully deleted and test_dir cleaned up.\\" } except Exception as e: if os.path.exists(test_dir_path): shutil.rmtree(test_dir_path) return { \\"content\\": None, \\"status\\": f\\"Error occurred: {e}\\" }"},{"question":"# PyTorch Coding Assessment: Implementing a Custom Attention Mechanism **Objective:** Demonstrate your understanding of PyTorch\'s attention mechanisms by implementing a custom attention-based function using the `torch.nn.attention` module. **Problem Statement:** You are tasked with implementing a custom attention mechanism leveraging the core utilities provided within the `torch.nn.attention` module. Specifically, you are to create a function `custom_sdpa_attention` that calculates the scaled dot-product attention for a given set of queries, keys, and values. **Function Signature:** ```python def custom_sdpa_attention(queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, dropout: float=None, mask: torch.Tensor=None) -> torch.Tensor: Compute the scaled dot-product attention. Parameters: - queries (torch.Tensor): A tensor of shape (batch_size, num_heads, seq_length_q, depth) - keys (torch.Tensor): A tensor of shape (batch_size, num_heads, seq_length_k, depth) - values (torch.Tensor): A tensor of shape (batch_size, num_heads, seq_length_v, depth) - dropout (float, optional): Dropout rate to apply on attention scores. Default: None - mask (torch.Tensor, optional): A mask tensor of shape (batch_size, 1, seq_length_q, seq_length_k) to add before softmax. Default: None Returns: - torch.Tensor: Attention outputs of shape (batch_size, num_heads, seq_length_q, depth) ``` **Requirements:** 1. **Scaled Dot-Product Calculation**: - Compute the dot product of queries and keys. - Scale by the square root of depth of the keys. 2. **Masking**: - Optionally apply a mask to the attention scores before the softmax calculation. This is useful for tasks like masked language modeling. 3. **Softmax**: - Apply the softmax function to the attention scores to obtain attention weights. 4. **Dropout**: - Optionally apply dropout to the attention weights if a dropout rate is provided. 5. **Attention Weights Application**: - Weight the values by the attention probabilities to produce the final output. **Input Constraints**: - `queries, keys, values` tensors must have compatible shapes for matrix multiplication. - Mask tensor, if provided, must have the correct shape to be broadcasted with the attention scores. **Performance Requirements**: - The implementation should be efficient, leveraging PyTorch operations to ensure the computation runs on GPUs if tensors are CUDA-enabled. # Example: ```python import torch import torch.nn.functional as F def custom_sdpa_attention(queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, dropout: float=None, mask: torch.Tensor=None) -> torch.Tensor: depth = queries.size(-1) scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(depth, dtype=torch.float32)) if mask is not None: scores = scores.masked_fill(mask == 0, float(\'-inf\')) attention_weights = F.softmax(scores, dim=-1) if dropout is not None: attention_weights = F.dropout(attention_weights, p=dropout) output = torch.matmul(attention_weights, values) return output ``` **Note:** Use the function signature, constraints, and requirements provided above to guide your implementation. Your function should be able to compute the scaled dot-product attention effectively, leveraging the capabilities of PyTorch.","solution":"import torch import torch.nn.functional as F def custom_sdpa_attention(queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, dropout: float=None, mask: torch.Tensor=None) -> torch.Tensor: depth = queries.size(-1) # Compute the dot product scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(depth, dtype=torch.float32)) # Apply mask if provided if mask is not None: scores = scores.masked_fill(mask == 0, float(\'-inf\')) # Apply softmax to get attention weights attention_weights = F.softmax(scores, dim=-1) # Apply dropout if provided if dropout is not None: attention_weights = F.dropout(attention_weights, p=dropout, training=True) # Compute the final output output = torch.matmul(attention_weights, values) return output"},{"question":"Title: Advanced Slicing and Ellipsis Handling Objective: You are tasked with implementing a function that accepts a sequence and a slice description. The function should return the slice of the sequence based on the given parameters and handle special cases involving the Ellipsis object. Problem Description: Implement the function `extract_slice(seq, start, stop, step)` that performs slicing on the given sequence `seq` using `start`, `stop`, and `step` parameters. These parameters can be `None`, integers, or the Ellipsis object. The function should handle converting the Ellipsis object into appropriate values by using complete slices. For example, if `start` is an Ellipsis, the function should treat it as `None`. Function Signature: ```python def extract_slice(seq, start, stop, step): pass ``` Input: - `seq`: A sequence (e.g., list, tuple, string) upon which slicing is to be performed. - `start`: The starting index of the slice, can be an integer, `None`, or Ellipsis (`...`). - `stop`: The stopping index of the slice, can be an integer, `None`, or Ellipsis (`...`). - `step`: The step for the slice, can be an integer, `None`, or Ellipsis (`...`). Output: - Returns the sliced sequence. Constraints: - You should not use any imported libraries. - Performance should be optimal for sequences up to length (10^5). Example Usage: ```python seq = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # Example 1: print(extract_slice(seq, 2, 8, 2)) # Output: [3, 5, 7] # Example 2: print(extract_slice(seq, ..., 5, ...)) # Output: [1, 2, 3, 4, 5] # Example 3: print(extract_slice(seq, ..., ..., -1)) # Output: [10, 9, 8, 7, 6, 5, 4, 3, 2, 1] # Example 4: print(extract_slice(seq, None, ..., None)) # Output: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] ``` Notes: - Ensure the function correctly interprets Ellipsis as a complete slice (`None`). - Handle edge cases where start and stop indices might be out of bounds. - The function should be robust and handle invalid inputs gracefully, providing meaningful error messages.","solution":"def extract_slice(seq, start, stop, step): Returns the sliced sequence based on start, stop, and step. The function handles Ellipsis by treating them as None. if start == Ellipsis: start = None if stop == Ellipsis: stop = None if step == Ellipsis: step = None return seq[start:stop:step]"},{"question":"# Question: Advanced Data Visualization with Seaborn Objective: Create a complex data visualization that leverages multiple properties of Seaborn\'s mark objects and scales to visualize the given dataset effectively. Dataset: You will be provided a dataset in CSV format with the following columns: 1. `date`: Dates in `YYYY-MM-DD` format. 2. `category`: Categorical data with multiple categories. 3. `value`: Numerical values associated with each date and category. Task: Write a Python function using Seaborn to generate a data visualization as described below. 1. **Plot**: Use a combination of `line` and `scatter` marks to distinguish between the trend (line) and actual data points (scatter). 2. **x and y coordinate properties**: - Map the `date` to the `x` axis with a `Temporal` scale. - Map the `value` to the `y` axis with a `log` transformation. 3. **Color Properties**: - Use different colors for different categories. Utilize a custom color palette for the categories. 4. **Opacity Properties**: - Set different transparency levels for the line and scatter marks. 5. **Marker Properties**: - Customize the markers for the scatter plot. Use different markers for different categories. 6. **Size Properties**: - Vary the size of the scatter plot markers based on the `value`. 7. **Theme and Layout**: - Customize the layout to provide a clear visualization. Specify a suitable theme. 8. **Legend**: - Provide a meaningful legend that differentiates between categories and describes the marker size. Function Specification: - Name: `create_advanced_visualization` - Input: A Pandas DataFrame `df` with columns `date`, `category`, and `value`. - Output: Save the generated plot as a PNG file named `advanced_visualization.png`. Constraints: - The function should handle missing values gracefully. - The function should be efficient with a dataset containing up to 10,000 rows. Example: ```python import pandas as pd def create_advanced_visualization(df: pd.DataFrame): # your implementation here # Example usage data = { \'date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\'], \'category\': [\'A\', \'B\', \'A\'], \'value\': [10, 20, 30] } df = pd.DataFrame(data) create_advanced_visualization(df) ``` # Expected Output: The function should generate a PNG file named `advanced_visualization.png` with a visual representation of the given data as per the described specifications.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import matplotlib.dates as mdates from matplotlib.ticker import FuncFormatter import numpy as np def create_advanced_visualization(df: pd.DataFrame): # Convert \'date\' column to datetime df[\'date\'] = pd.to_datetime(df[\'date\']) # Create a custom color palette unique_categories = df[\'category\'].unique() palette = sns.color_palette(\\"husl\\", len(unique_categories)) palette_dict = dict(zip(unique_categories, palette)) # Configure the plot sns.set(style=\\"whitegrid\\") plt.figure(figsize=(14, 8)) # Plot lines and scatter marks for category in unique_categories: subset = df[df[\'category\'] == category] plt.plot(subset[\'date\'], subset[\'value\'], label=category, color=palette_dict[category], alpha=0.6) plt.scatter(subset[\'date\'], subset[\'value\'], c=[palette_dict[category]], s=subset[\'value\'], alpha=0.8, label=None) # Log scale for y-axis plt.yscale(\'log\') # Customize x-axis format plt.gca().xaxis.set_major_locator(mdates.MonthLocator()) plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\'%Y-%m\')) # Add legend for categories handles, labels = plt.gca().get_legend_handles_labels() by_label = dict(zip(labels, handles)) plt.legend(by_label.values(), by_label.keys()) # Customizing markers markers = (\'o\', \'s\', \'D\', \'v\', \'^\', \'<\', \'>\', \'p\', \'*\', \'H\') marker_dict = dict(zip(unique_categories, markers[:len(unique_categories)])) for category in unique_categories: subset = df[df[\'category\'] == category] plt.scatter(subset[\'date\'], subset[\'value\'], c=[palette_dict[category]], s=subset[\'value\'], alpha=0.8, marker=marker_dict[category], label=category) # Log formatter for y-axis plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: \'{:.0f}\'.format(y))) # Titles and labels plt.title(\'Advanced Data Visualization\') plt.xlabel(\'Date\') plt.ylabel(\'Value\') # Save the plot plt.tight_layout() plt.savefig(\'advanced_visualization.png\') plt.close()"},{"question":"# Asynchronous Task Manager with `asyncio.Queue` **Problem Statement:** You are tasked with implementing an asynchronous task manager that handles three types of tasks: high-priority, medium-priority, and low-priority. Each task type should be processed by its own worker. Specifically, you should use `asyncio.PriorityQueue` to manage the task queue and ensure that more critical tasks are handled first. **Requirements:** 1. Create a priority queue using `asyncio.PriorityQueue`. 2. Implement three asynchronous worker functions (`high_priority_worker`, `medium_priority_worker`, `low_priority_worker`) to process tasks from the queue. Each worker should print a message indicating it has processed a task. 3. Design a main function to: - Add tasks to the queue with appropriate priorities (1 for high, 2 for medium, 3 for low). - Use `await queue.join()` to wait until all tasks are completed. - Gracefully shut down the workers after task completion. 4. Handle `queue.QueueEmpty` exception when attempting to get from an empty queue, and ensure proper use of `task_done()` to indicate task completion. **Input:** - A list of tuples where each tuple contains a task description as a string and its priority level as an integer (1, 2, or 3). **Output:** - Print statements from each worker indicating the processed tasks in the order of their priority. **Function Signatures:** ```python async def high_priority_worker(queue: asyncio.PriorityQueue): pass async def medium_priority_worker(queue: asyncio.PriorityQueue): pass async def low_priority_worker(queue: asyncio.PriorityQueue): pass async def main(tasks: list): pass ``` **Example:** ```python tasks = [ (\\"Low priority task 1\\", 3), (\\"High priority task 1\\", 1), (\\"Medium priority task 1\\", 2), (\\"Low priority task 2\\", 3), (\\"High priority task 2\\", 1) ] asyncio.run(main(tasks)) ``` **Expected Output:** ``` High priority worker processed: High priority task 1 High priority worker processed: High priority task 2 Medium priority worker processed: Medium priority task 1 Low priority worker processed: Low priority task 1 Low priority worker processed: Low priority task 2 ``` **Constraints:** - Do not modify the task list outside the `main` function. **Performance Requirements:** - Ensure the solution is efficient in terms of both time and space complexity. Use the inherent properties of `asyncio.PriorityQueue` to maintain order and efficiency. Good luck, and remember to utilize the documentation of `asyncio` queues effectively!","solution":"import asyncio async def high_priority_worker(queue: asyncio.PriorityQueue): while True: try: priority, task = await queue.get() if priority != 1: queue.put_nowait((priority, task)) break print(f\\"High priority worker processed: {task}\\") queue.task_done() except asyncio.QueueEmpty: break async def medium_priority_worker(queue: asyncio.PriorityQueue): while True: try: priority, task = await queue.get() if priority != 2: queue.put_nowait((priority, task)) break print(f\\"Medium priority worker processed: {task}\\") queue.task_done() except asyncio.QueueEmpty: break async def low_priority_worker(queue: asyncio.PriorityQueue): while True: try: priority, task = await queue.get() if priority != 3: queue.put_nowait((priority, task)) break print(f\\"Low priority worker processed: {task}\\") queue.task_done() except asyncio.QueueEmpty: break async def main(tasks: list): queue = asyncio.PriorityQueue() for task in tasks: queue.put_nowait(task) tasks = [ high_priority_worker(queue), medium_priority_worker(queue), low_priority_worker(queue), ] await asyncio.gather(*tasks) await queue.join() for task in tasks: task.cancel()"},{"question":"You are given a list of URLs, and your task is to fetch data from these URLs concurrently using the `concurrent.futures` module. The objective is to implement a function that uses both `ThreadPoolExecutor` and `ProcessPoolExecutor` to perform the fetch operation and compare their performance. # Function Signature ```python def fetch_data_concurrently(urls: List[str]) -> Dict[str, float]: pass ``` # Input - `urls`: A list of strings representing URLs. - Example: `[\\"http://example.com/data1\\", \\"http://example.com/data2\\"]` # Output - A dictionary with two keys: `\\"thread_pool\\"` and `\\"process_pool\\"`, each mapping to the time taken (in seconds) to fetch all URLs using `ThreadPoolExecutor` and `ProcessPoolExecutor` respectively. # Constraints - You must use the `concurrent.futures` module to perform the fetch operation. - Handle any exceptions that occur during the fetching process. - Assume each URL returns a response in a reasonable amount of time for testing. # Performance Requirements - The function should demonstrate the performance difference between threads and processes for I/O-bound operations. - Ensure the function performs fetch operations concurrently. # Example ```python >>> urls = [\\"http://example.com/data1\\", \\"http://example.com/data2\\"] >>> fetch_data_concurrently(urls) {\'thread_pool\': 0.233, \'process_pool\': 0.541} ``` **Note**: The times shown in the output are just examples. Actual times will vary based on the URLs provided and the system\'s performance. # Implementation Details 1. Import necessary modules including `requests` for fetching URLs and `time` for measuring time. 2. Define helper functions to fetch data from a single URL. 3. Use `ThreadPoolExecutor` to fetch data from all URLs and measure the time taken. 4. Use `ProcessPoolExecutor` to fetch data from all URLs and measure the time taken. 5. Return the measurement results in the specified format. Provide a detailed comment on each step to explain how the `concurrent.futures` module is utilized.","solution":"import requests import time from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor from typing import List, Dict def fetch_url(url: str) -> str: Fetches the content of a given URL. try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException as e: return str(e) def fetch_data_concurrently(urls: List[str]) -> Dict[str, float]: Fetches data from a list of URLs concurrently using ThreadPoolExecutor and ProcessPoolExecutor, and returns the time taken for each approach. results = {} # Fetch using ThreadPoolExecutor start_time = time.time() with ThreadPoolExecutor() as executor: executor.map(fetch_url, urls) end_time = time.time() results[\\"thread_pool\\"] = end_time - start_time # Fetch using ProcessPoolExecutor start_time = time.time() with ProcessPoolExecutor() as executor: executor.map(fetch_url, urls) end_time = time.time() results[\\"process_pool\\"] = end_time - start_time return results"},{"question":"You are given a dataset `X` with a large number of features. Your task is to implement a function that applies different unsupervised dimensionality reduction techniques and returns the transformed data. The techniques to be implemented are: 1. **Principal Component Analysis (PCA)** 2. **Random Projections (Gaussian Random Projection)** 3. **Feature Agglomeration** Additionally, you should standardize the data before applying these techniques. The function should compare the runtime of these techniques and return the transformed data along with the time taken for each. **Function Signature** ```python def compare_dimensionality_reduction(X: np.ndarray, n_components: int) -> dict: pass ``` # Input: - `X`: A 2D numpy array of shape `(n_samples, n_features)` representing the input data. - `n_components`: An integer indicating the desired number of dimensions to reduce to. # Output: - A dictionary with the keys `\'PCA\'`, `\'RandomProjections\'`, and `\'FeatureAgglomeration\'`. Each key should map to a tuple `(transformed_data, time_taken_in_seconds)` where `transformed_data` is the data transformed to `n_components` dimensions, and `time_taken_in_seconds` is the time taken to perform the dimensionality reduction. # Example: ```python import numpy as np from sklearn.datasets import load_digits # Load a sample dataset digits = load_digits() X = digits.data # Specify number of components to reduce to n_components = 10 # Call the function result = compare_dimensionality_reduction(X, n_components) # Output # {\'PCA\': (transformed_data_pca, time_pca), # \'RandomProjections\': (transformed_data_rp, time_rp), # \'FeatureAgglomeration\': (transformed_data_fa, time_fa)} ``` # Constraints: - Ensure you use `StandardScaler` to standardize the input data before applying any reduction technique. - The dimensionality reduction should handle the cases where `n_components` is less than or equal to the number of original features. # Notes: - Utilize `sklearn.decomposition.PCA` for PCA. - Utilize `sklearn.random_projection.GaussianRandomProjection` for Random Projections. - Utilize `sklearn` for Feature Agglomeration. - Measure the runtime using the `time` module in Python. This task evaluates the understanding of: - Data preprocessing with `StandardScaler`. - Application of PCA, Random Projections, and Feature Agglomeration. - Measuring and comparing the performance of different dimensionality reduction techniques.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection from sklearn.cluster import FeatureAgglomeration import time def compare_dimensionality_reduction(X: np.ndarray, n_components: int) -> dict: Apply PCA, Random Projections and Feature Agglomeration for given data X and return transformed data along with the time taken for each method. Parameters: X (np.ndarray): Input data matrix. n_components (int): Desired number of dimensions to reduce to. Returns: dict: A dictionary with keys \'PCA\', \'RandomProjections\', and \'FeatureAgglomeration\'. Each key maps to a tuple (transformed_data, time_taken_in_seconds). scaler = StandardScaler() X_scaled = scaler.fit_transform(X) results = {} # PCA start_time = time.time() pca = PCA(n_components=n_components) X_pca = pca.fit_transform(X_scaled) end_time = time.time() results[\'PCA\'] = (X_pca, end_time - start_time) # Random Projections start_time = time.time() rp = GaussianRandomProjection(n_components=n_components) X_rp = rp.fit_transform(X_scaled) end_time = time.time() results[\'RandomProjections\'] = (X_rp, end_time - start_time) # Feature Agglomeration start_time = time.time() fa = FeatureAgglomeration(n_clusters=n_components) X_fa = fa.fit_transform(X_scaled) end_time = time.time() results[\'FeatureAgglomeration\'] = (X_fa, end_time - start_time) return results"},{"question":"# Advanced Multiprocessing Task: Web Scraping with Concurrent Downloads **Problem Statement:** You are tasked with writing a Python function that downloads content from a set of URLs concurrently and processes the data using the `multiprocessing` package. The goal is to maximize the use of multiple processors for faster downloads and processing. **Function Specifications:** Implement a function `download_and_process(urls: List[str]) -> List[str]` that: 1. Takes as input a list of URLs (strings). 2. Downloads the content from each URL concurrently using the `multiprocessing` module. 3. Processes the downloaded data by converting all text to uppercase. 4. Returns a list of processed content corresponding to each URL. **Input:** - `urls`: A list of strings, where each string is a URL (e.g., [\\"https://example.com\\", \\"https://example.org\\"]). **Output:** - A list of strings, where each string is the processed content of the corresponding URL from the input list. **Constraints:** - You should use the `multiprocessing.Pool` class to manage the concurrent downloads. - Handle exceptions gracefully by returning an error message in place of the content for any URL that fails to download. - Assume a maximum of 8 worker processes. **Performance Requirements:** - Ensure efficient use of processes to minimize the overall execution time. - Avoid unnecessary memory usage by processing data immediately after downloading. **Example Usage:** ```python from typing import List def download_and_process(urls: List[str]) -> List[str]: # Your implementation here pass # Example URLs (these are placeholders; replace with real URLs for testing) example_urls = [ \\"https://example.com\\", \\"https://example.org\\" ] processed_content = download_and_process(example_urls) print(processed_content) ``` **Guidelines and Tips:** - Utilize the `Pool.map` or `Pool.map_async` function to distribute the download tasks across multiple processes. - Use the `requests` library for downloading content from URLs (you need to install it if not already available). - Ensure that the `requests.get` call is made within a try-except block to handle potential download errors. - Design the processing function to convert the content to uppercase, which should be applied to the downloaded data. **Bonus (Optional):** Extend the function to accept an additional argument `timeout` that sets a maximum time limit (in seconds) for the entire download and processing operation. ```python def download_and_process(urls: List[str], timeout: int = None) -> List[str]: # Your implementation here pass ``` **Notes:** - Include proper documentation and comments in your code for clarity. - Test your function with a variety of URLs to validate its correctness and performance.","solution":"from multiprocessing import Pool import requests from typing import List def fetch_content(url): try: response = requests.get(url, timeout=10) response.raise_for_status() return response.text.upper() except (requests.RequestException, requests.HTTPError) as e: return f\\"Error fetching {url}: {str(e)}\\" def download_and_process(urls: List[str], timeout: int = None) -> List[str]: with Pool(processes=8) as pool: try: if timeout: results = pool.map_async(fetch_content, urls).get(timeout) else: results = pool.map(fetch_content, urls) except Exception as e: return [f\\"Error during processing: {str(e)}\\"] return results"},{"question":"# HTML Entity Conversion You are given an HTML string that includes named character references (entities) which need to be converted to their equivalent Unicode characters. Implement a function `convert_html_entities(html_string: str) -> str` that takes an HTML string as input and returns a new string with all named character references replaced by their corresponding Unicode characters. Utilize the `html.entities` module for this task. Function Signature: ```python def convert_html_entities(html_string: str) -> str: pass ``` Input: - `html_string` (str): A string containing HTML text with named character references. The string will be at most 10^6 characters long. Output: - Returns a new string with all named character references replaced by their corresponding Unicode characters. Constraints: - The function should handle all HTML entities defined in the `html.entities` module. - The function should replace both entities with and without trailing semicolons, if applicable. - The solution should be efficient, with a linear time complexity O(n), where n is the length of the input string. Example: ```python html_string = \\"The &lt;div&gt; tag defines a division or a section in an HTML document.\\" result = convert_html_entities(html_string) assert result == \\"The <div> tag defines a division or a section in an HTML document.\\" ``` # Guidelines: 1. You may use the `html5` dictionary from the `html.entities` module to look up the named character references. 2. Consider edge cases where entities may not be terminated with a semicolon. 3. Ensure your solution is efficient and can handle large input sizes within reasonable time limits.","solution":"import html def convert_html_entities(html_string: str) -> str: Converts named character references in the HTML string to their corresponding Unicode characters. return html.unescape(html_string)"},{"question":"**Objective**: Demonstrate your understanding of `memoryview` objects by implementing a function that creates a memoryview from a given buffer, checks if an object is a memoryview, and accesses the underlying buffer data. **Problem Statement**: You need to implement the following functions: 1. **create_memoryview**: - **Input**: A bytes or bytearray object `buffer`. - **Output**: A memoryview object created from the given `buffer`. - **Behavior**: Use the provided `buffer` to create a memoryview object. 2. **is_memoryview**: - **Input**: Any object `obj`. - **Output**: A boolean indicating whether `obj` is a memoryview object. - **Behavior**: Check if the given `obj` is a memoryview. 3. **get_buffer_info**: - **Input**: A memoryview object `mview`. - **Output**: A tuple containing the buffer\'s format, item size, and number of dimensions. - **Behavior**: Access the underlying buffer data from the memoryview and return the format, item size, and number of dimensions. **Function Signatures**: ```python def create_memoryview(buffer: bytes) -> memoryview: pass def is_memoryview(obj: object) -> bool: pass def get_buffer_info(mview: memoryview) -> tuple: pass ``` **Constraints**: - `buffer` will always be a bytes or bytearray object. - `mview` input to `get_buffer_info` will always be a valid memoryview object. **Example**: ```python # Example usage: buffer = b\\"Hello, World!\\" mview = create_memoryview(buffer) print(is_memoryview(mview)) # Output: True print(get_buffer_info(mview)) # Output: (\'B\', 1, 1) ``` **Hints**: - Use the memoryview object properties: `format`, `itemsize`, and `ndim` to get the buffer\'s format, item size, and number of dimensions. **Notes**: - Ensure your implementation is efficient and follows best practices for memory management. - You may use the built-in `memoryview` class and its attributes to complete the tasks.","solution":"def create_memoryview(buffer: bytes) -> memoryview: Create a memoryview object from the given buffer (bytes or bytearray). return memoryview(buffer) def is_memoryview(obj: object) -> bool: Check if the given object is a memoryview object. return isinstance(obj, memoryview) def get_buffer_info(mview: memoryview) -> tuple: Access the underlying buffer data from the memoryview and return the format, item size, and number of dimensions. return (mview.format, mview.itemsize, mview.ndim)"},{"question":"**Question: Managing CUDA Environment Variables in PyTorch** You are tasked to implement a function that sets specific CUDA environment variables required for optimizing and debugging a deep learning model running with PyTorch on an NVIDIA GPU. Your goal is to implement a function `set_cuda_env_vars()` that dynamically sets the required environment variables based on the provided input dictionary. The function should handle the following: 1. **Input**: A dictionary `vars_dict` where keys are environment variable names (strings) and values are their corresponding settings (strings or integers). 2. **Setting Environment Variables**: The function should iterate over the dictionary and set each environment variable using the `os.environ` method. 3. **Verification**: After setting the environment variables, print out the environment variable settings to verify that they have been correctly set. Write a function `set_cuda_env_vars` as described and an example script that demonstrates its use. ```python import os def set_cuda_env_vars(vars_dict): Sets the specified CUDA environment variables for PyTorch. Args: vars_dict (dict): Dictionary where keys are environment variable names and values are their settings. Example: vars_dict = { \\"CUDA_VISIBLE_DEVICES\\": \\"0,1\\", \\"PYTORCH_NO_CUDA_MEMORY_CACHING\\": \\"1\\", \\"CUBLAS_WORKSPACE_CONFIG\\": \\":4096:2\\" } # Your implementation here pass # Example usage if __name__ == \\"__main__\\": vars_dict = { \\"CUDA_VISIBLE_DEVICES\\": \\"0,1\\", \\"PYTORCH_NO_CUDA_MEMORY_CACHING\\": \\"1\\", \\"CUBLAS_WORKSPACE_CONFIG\\": \\":4096:2\\" } set_cuda_env_vars(vars_dict) # Verify the settings for var in vars_dict: print(f\\"{var} = {os.getenv(var)}\\") ``` **Constraints and Requirements:** - Make sure to handle both integer and string values in the dictionary. - Ensure that the function sets environment variables correctly and that they persist for the runtime of the Python program. - The function should not raise exceptions if the dictionary is empty and should handle such cases gracefully. **Performance Considerations:** - The function should be efficient and there should be no unnecessary computations. *Hint*: Use the `os.environ` dictionary to set environment variables in Python.","solution":"import os def set_cuda_env_vars(vars_dict): Sets the specified CUDA environment variables for PyTorch. Args: vars_dict (dict): Dictionary where keys are environment variable names and values are their settings. Example: vars_dict = { \\"CUDA_VISIBLE_DEVICES\\": \\"0,1\\", \\"PYTORCH_NO_CUDA_MEMORY_CACHING\\": \\"1\\", \\"CUBLAS_WORKSPACE_CONFIG\\": \\":4096:2\\" } for var, value in vars_dict.items(): os.environ[var] = str(value) # Verifying the environment variables for var in vars_dict: print(f\\"{var} = {os.getenv(var)}\\") # Example usage if __name__ == \\"__main__\\": vars_dict = { \\"CUDA_VISIBLE_DEVICES\\": \\"0,1\\", \\"PYTORCH_NO_CUDA_MEMORY_CACHING\\": \\"1\\", \\"CUBLAS_WORKSPACE_CONFIG\\": \\":4096:2\\" } set_cuda_env_vars(vars_dict) # Verify the settings for var in vars_dict: print(f\\"{var} = {os.getenv(var)}\\")"},{"question":"Analyzing Healthcare Expenditure Over Time You are provided with a dataset on healthcare expenditure and need to create visualizations to analyze the data. 1. **Dataset:** - Load the dataset `healthexp` using `seaborn`\'s `load_dataset` function. This dataset contains the following columns: - `Year`: The year of the expenditure. - `Spending_USD`: The expenditure in USD. - `Country`: The country to which the expenditure data belongs. 2. **Tasks:** a. Create a line plot that shows the healthcare expenditure (`Spending_USD`) over the years (`Year`) for each country (`Country`). Normalize the data such that each country’s spending is scaled relative to its maximum value. b. Create another line plot that shows the percent change in healthcare expenditure from the year 1970 for each country. # Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def visualize_healthcare_expenditure(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Task a ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) # Task b ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) # Expected Output: Visualization with healthcare expenditure relative to maximum amount and relative to 1970 baseline. ``` # Notes - Assume the `seaborn` and required data visualization libraries are pre-installed. - In both tasks, make sure to label the plots appropriately as specified. - Enhance the plots for better clarity and presentation if necessary.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_healthcare_expenditure(): # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\") # Task a: Normalize each country’s spending relative to its maximum value healthexp[\'Normalized_Spending\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) # Plot for normalized spending plt.figure(figsize=(10, 6)) sns.lineplot(data=healthexp, x=\'Year\', y=\'Normalized_Spending\', hue=\'Country\') plt.title(\'Healthcare Expenditure Over Time (Normalized)\') plt.ylabel(\'Spending relative to maximum amount\') plt.xlabel(\'Year\') plt.legend(title=\'Country\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Task b: Calculate percent change from 1970 baseline baseline = healthexp[healthexp[\'Year\'] == 1970] baseline = baseline.set_index(\'Country\')[\'Spending_USD\'] healthexp = healthexp.join(baseline, on=\'Country\', rsuffix=\'_baseline\') healthexp[\'Percent_Change\'] = ((healthexp[\'Spending_USD\'] - healthexp[\'Spending_USD_baseline\']) / healthexp[\'Spending_USD_baseline\']) * 100 # Plot for percent change from 1970 baseline plt.figure(figsize=(10, 6)) sns.lineplot(data=healthexp, x=\'Year\', y=\'Percent_Change\', hue=\'Country\') plt.title(\'Percent Change in Healthcare Expenditure From 1970 Baseline\') plt.ylabel(\'Percent change in spending from 1970 baseline\') plt.xlabel(\'Year\') plt.legend(title=\'Country\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Function call visualize_healthcare_expenditure()"},{"question":"# Data Analysis and Transformation with Pandas Objective Create a function, `analyze_data`, that takes in two pandas DataFrames and performs various transformations and analyses on them. The goal is to check the student\'s comprehension of pandas\' advanced data type handling and merging dataframes. Function Signature ```python import pandas as pd def analyze_data(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame: Perform data analysis and transformation on two given pandas DataFrames. Parameters: - df1: pd.DataFrame: The first dataframe containing at least a categorical column, a datetime column, and a nullable integer column. - df2: pd.DataFrame: The second dataframe containing at least a datetime column and some string data. Returns: - pd.DataFrame: A dataframe with the following features: * Merged data from df1 and df2 on the datetime columns. * A new column indicating if the datetime is in a leap year. * For each categorical value in df1, count the number of occurrences in the merged dataframe. * Ensure all integer columns are treated as nullable integers. * Return the final dataframe with appropriate transformations. ``` Constraints 1. **Datetime Columns Merge**: Ensure to merge `df1` and `df2` based on their datetime columns. 2. **Leap Year Indicator**: Add a new column to the dataframe which indicates whether the datetime of each row falls in a leap year. 3. **Categorical Count**: Create a summary count of the occurrences of each category from a column in `df1`. 4. **Nullable Integers**: Ensure that all integer columns are treated as nullable integers. 5. **Input Columns**: - `df1` will have at least: - A categorical column named `category` - A datetime column named `datetime` - A nullable integer column named `nullable_int` - `df2` will have at least: - A datetime column named `datetime` - A string-based information column 6. Handle missing values appropriately where necessary. Example Consider the following dataframes: ```python data1 = { \\"category\\": [\\"A\\", \\"B\\", \\"A\\", \\"C\\"], \\"datetime\\": [\\"2023-01-01\\", \\"2024-02-01\\", \\"2025-03-01\\", \\"2022-04-01\\"], \\"nullable_int\\": [1, 2, None, 4] } df1 = pd.DataFrame(data1) df1[\\"datetime\\"] = pd.to_datetime(df1[\\"datetime\\"]) df1[\\"nullable_int\\"] = df1[\\"nullable_int\\"].astype(\'Int64\') data2 = { \\"datetime\\": [\\"2023-01-01\\", \\"2024-02-01\\", \\"2025-03-01\\", \\"2022-04-01\\"], \\"info\\": [\\"info1\\", \\"info2\\", \\"info3\\", \\"info4\\"] } df2 = pd.DataFrame(data2) df2[\\"datetime\\"] = pd.to_datetime(df2[\\"datetime\\"]) merged_df = analyze_data(df1, df2) ``` Expected steps and final dataframe: 1. Merge the two dataframes on the `datetime` column. 2. Add a column that checks if the `datetime` is in a leap year. 3. Count occurrences of each category in the merged dataframe 4. Ensure the nullable integer column remains as nullable integers. Important Notes - You are not allowed to use any other libraries except pandas. - Ensure your function handles edge cases and missing values gracefully. - Performance should be considered, especially for large datasets.","solution":"import pandas as pd def analyze_data(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame: Perform data analysis and transformation on two given pandas DataFrames. Parameters: - df1: pd.DataFrame: The first dataframe containing at least a categorical column, a datetime column, and a nullable integer column. - df2: pd.DataFrame: The second dataframe containing at least a datetime column and some string data. Returns: - pd.DataFrame: A dataframe with the following features: * Merged data from df1 and df2 on the datetime columns. * A new column indicating if the datetime is in a leap year. * For each categorical value in df1, count the number of occurrences in the merged dataframe. * Ensure all integer columns are treated as nullable integers. * Return the final dataframe with appropriate transformations. # Ensure \'datetime\' columns are in datetime format df1[\'datetime\'] = pd.to_datetime(df1[\'datetime\']) df2[\'datetime\'] = pd.to_datetime(df2[\'datetime\']) # Merge dataframes on \'datetime\' merged_df = pd.merge(df1, df2, on=\'datetime\', how=\'inner\') # Add a column to check if \'datetime\' is in a leap year merged_df[\'is_leap_year\'] = merged_df[\'datetime\'].apply(lambda x: x.is_leap_year) # Count occurrences for each category and add as a column category_counts = merged_df[\'category\'].value_counts().reset_index() category_counts.columns = [\'category\', \'count\'] merged_df = pd.merge(merged_df, category_counts, on=\'category\', how=\'left\') # Treat \'nullable_int\' as nullable integers merged_df[\'nullable_int\'] = merged_df[\'nullable_int\'].astype(\'Int64\') return merged_df"},{"question":"Objective Demonstrate your understanding of Seaborn\'s `cubehelix_palette` function by creating custom color palettes and applying them to a simple data visualization. Task You need to implement a function `visualize_with_custom_palette(data, palette_params)` which takes the following inputs: 1. `data`: A dictionary where keys are category labels and values are numerical values. For example: `{\\"A\\": 10, \\"B\\": 15, \\"C\\": 7}`. 2. `palette_params`: A dictionary of parameters that should be passed to the `sns.cubehelix_palette` function. The parameters should match those accepted by `cubehelix_palette`. For example: `{\\"n_colors\\": 5, \\"start\\": 2, \\"rot\\": 0.4, \\"reverse\\": True}`. Function Implementation - Use the `seaborn` library to create a barplot with the provided data. - Create a custom palette using `sns.cubehelix_palette` and the given `palette_params`. - Apply the generated palette to the barplot. - Display the plot. Input - `data`: A dictionary (e.g., `{\\"A\\": 10, \\"B\\": 15, \\"C\\": 7}`) - `palette_params`: A dictionary with parameters for `sns.cubehelix_palette` (e.g., `{\\"n_colors\\": 5, \\"start\\": 2, \\"rot\\": 0.4, \\"reverse\\": True}`) Output - A barplot with the data shown using the generated custom palette. Example ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_with_custom_palette(data, palette_params): # Convert data to two lists categories = list(data.keys()) values = list(data.values()) # Generate the custom cubehelix palette palette = sns.cubehelix_palette(**palette_params) # Create the barplot sns.barplot(x=categories, y=values, palette=palette) # Show the plot plt.show() # Example usage: data = {\\"A\\": 10, \\"B\\": 15, \\"C\\": 7} palette_params = {\\"n_colors\\": 3, \\"start\\": 3, \\"rot\\": 0.5, \\"reverse\\": True} visualize_with_custom_palette(data, palette_params) ``` Notes - Ensure the function is robust to handle different shapes and sizes of the input data. - You may import any additional libraries needed for data wrangling or visualization. - The function should handle edge cases such as empty data gracefully.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_with_custom_palette(data, palette_params): Generates a barplot using seaborn with a custom cubehelix palette. Parameters: - data (dict): A dictionary where keys are category labels and values are numerical values. - palette_params (dict): A dictionary of parameters for sns.cubehelix_palette. if not data: raise ValueError(\\"Data dictionary is empty or None\\") # Convert data to two lists categories = list(data.keys()) values = list(data.values()) # Generate the custom cubehelix palette palette = sns.cubehelix_palette(**palette_params) # Create the barplot sns.barplot(x=categories, y=values, palette=palette) # Show the plot plt.show()"},{"question":"# Advanced Bytes Handling in Python Python’s byte objects are vital for handling binary data. In this task, you are required to implement several functionalities related to byte objects to test your understanding of their creation, manipulation, and handling exceptions. Task Implement a class `BytesHandler` with the following methods: 1. `create_bytes_from_string(s: str) -> bytes`: This method should create a bytes object from a given string. 2. `get_bytes_size(b: bytes) -> int`: This method should return the size of the given bytes object. 3. `concat_bytes(b1: bytes, b2: bytes) -> bytes`: This method should concatenate two bytes objects and return the result. 4. `create_bytes_with_format(format_str: str, *args) -> bytes`: This method should create a bytes object using a C `printf`-style format string and additional arguments (like \'%d\', \'%s\', etc.). 5. `resize_bytes(b: bytes, new_size: int) -> bytes`: This method should resize a given bytes object to the new size. 6. `bytes_info(b: bytes) -> str`: This method should return a descriptive string containing the size of the bytes object and its content. **Constraints:** - You should handle any potential errors such as type errors, incompatible operations, etc., and return appropriate error messages or exceptions. - Do not use any external libraries other than those built into Python. - You should use lower-level Python operations wherever possible to manage bytes, mimicking the C-level operations described. Example Usage ```python # Initialize the bytes handler bh = BytesHandler() # 1. Creating bytes from string bytes_obj = bh.create_bytes_from_string(\\"Hello\\") print(bytes_obj) # Output should be: b\'Hello\' # 2. Getting size of bytes size = bh.get_bytes_size(bytes_obj) print(size) # Output should be: 5 # 3. Concatenate two bytes objects combined_bytes = bh.concat_bytes(bytes_obj, b\' World\') print(combined_bytes) # Output should be: b\'Hello World\' # 4. Creating bytes with format formatted_bytes = bh.create_bytes_with_format(\\"Value: %d\\", 100) print(formatted_bytes) # Output should be: b\'Value: 100\' # 5. Resize bytes resized_bytes = bh.resize_bytes(bytes_obj, 3) print(resized_bytes) # Output should be: b\'Hel\' # 6. Getting byte information info_string = bh.bytes_info(bytes_obj) print(info_string) # Output should be descriptive mentioning size and contents. ``` Ensure that your implementation covers edge cases, and the methods behave correctly for various inputs. Notes: - The `resize_bytes` method should handle the immutability of bytes objects by creating a new bytes object with the desired size. - Ensure proper exception handling where necessary, particularly in methods dealing with format strings and resizing operations.","solution":"class BytesHandler: def create_bytes_from_string(self, s: str) -> bytes: Creates and returns a bytes object from a given string. return s.encode() def get_bytes_size(self, b: bytes) -> int: Returns the size of the given bytes object. return len(b) def concat_bytes(self, b1: bytes, b2: bytes) -> bytes: Concatenates two bytes objects and returns the result. return b1 + b2 def create_bytes_with_format(self, format_str: str, *args) -> bytes: Creates a bytes object using a C `printf`-style format string and additional arguments. formatted_str = format_str % args return formatted_str.encode() def resize_bytes(self, b: bytes, new_size: int) -> bytes: Resizes a given bytes object to the new size. If new_size is smaller, truncates the bytes object. If new_size is larger, pads the bytes object with null bytes. if new_size > len(b): return b + (b\'x00\' * (new_size - len(b))) else: return b[:new_size] def bytes_info(self, b: bytes) -> str: Returns a descriptive string containing the size of the bytes object and its content. return f\\"Bytes size: {len(b)}, Content: {b}\\""},{"question":"# Advanced Coding Assessment Question Objective: To demonstrate your understanding of Python\'s `memoryview` object and buffer protocol, you are required to write a function that handles various operations on a memoryview. Problem Statement: Write a function `manipulate_memoryview(data: bytes, operation: str) -> bytes` that performs the following operations based on the given `operation`: 1. **\\"reverse\\"**: Reverse the given data using `memoryview`. 2. **\\"duplicate\\"**: Duplicate each byte in the data using `memoryview`. Function Signature: ```python def manipulate_memoryview(data: bytes, operation: str) -> bytes: pass ``` Input: - `data`: A `bytes` object which is the data to be manipulated. - `operation`: A `str` that specifies the operation to be performed. It will be either \\"reverse\\" or \\"duplicate\\". Output: - Returns a `bytes` object after performing the specified operation on the input data. Constraints: - The length of `data` will be between 1 and 10^6 bytes. - The `operation` will be one of \\"reverse\\" or \\"duplicate\\". Examples: ```python # Example 1 data = b\'abcdef\' operation = \'reverse\' assert manipulate_memoryview(data, operation) == b\'fedcba\' # Example 2 data = b\'abc\' operation = \'duplicate\' assert manipulate_memoryview(data, operation) == b\'aabbcc\' ``` Instructions: - You must use Python\'s `memoryview` object to perform the operations. - Memory management should be handled efficiently, especially for large data sizes. Performance Requirements: - The function should run efficiently within the given constraints. - Aim for O(n) time complexity for both operations, where n is the length of the `data`.","solution":"def manipulate_memoryview(data: bytes, operation: str) -> bytes: # Create a memoryview from the input data mv = memoryview(data) if operation == \\"reverse\\": # Reversing the memoryview and converting it back to bytes return bytes(mv[::-1]) elif operation == \\"duplicate\\": # Duplicating each byte in the memoryview return bytes(byte for byte in mv for _ in range(2)) else: # If the operation is unknown, raise an error raise ValueError(\\"Unknown operation. Use \'reverse\' or \'duplicate\'.\\")"},{"question":"**Color Space Conversion and Validation** In this coding assessment, you are required to demonstrate your understanding of the `colorsys` module, which allows conversions between different color systems. You will create a function `convert_and_validate` that takes an RGB color as input, converts it to the YIQ, HLS, and HSV color spaces, and then validates the conversions by converting each back to RGB and comparing the results to the original RGB values with a certain tolerance. **Function Signature:** ```python def convert_and_validate(r: float, g: float, b: float, tolerance: float = 1e-5) -> bool: pass ``` # Inputs: - `r, g, b`: Three float values representing the Red, Green, and Blue components of the color. Each value is between 0 and 1. - `tolerance`: A float tolerance value for validating the conversions. Default is `1e-5`. # Outputs: - Returns a boolean value `True` if all converted-back RGB values (from YIQ, HLS, HSV) match the original RGB values within the given tolerance. Returns `False` otherwise. # Constraints: - You must use the `colorsys` module to perform conversions. - The function must first convert RGB to YIQ and back, then to HLS and back, and finally to HSV and back, comparing each result with the original RGB values using the provided tolerance. - Ensure all intermediate calculations and checks are done appropriately. # Example: ```python convert_and_validate(0.2, 0.4, 0.4) # Expected output: True convert_and_validate(0.9, 0.1, 0.1, tolerance=1e-7) # Expected output: True or False depending on precision loss ``` # Notes: - Use appropriate built-in functions from the `colorsys` module. - Consider edge cases where the values could be at the boundaries (e.g., 0.0 or 1.0). # Assessment Criteria: - Correctness of the function implementation. - Proper use of the `colorsys` module functions. - Handling of edge cases and floating-point precision issues. - Code readability and efficiency.","solution":"import colorsys def convert_and_validate(r: float, g: float, b: float, tolerance: float = 1e-5) -> bool: Converts RGB color to YIQ, HLS, and HSV color spaces and then converts back to RGB. Validates the conversions within the given tolerance. Parameters: r (float): Red component (0-1) g (float): Green component (0-1) b (float): Blue component (0-1) tolerance (float): Tolerance for validating conversions Returns: bool: True if all converted-back RGB values match the original RGB values within the tolerance, False otherwise. # Convert RGB to YIQ and back yiq = colorsys.rgb_to_yiq(r, g, b) rgb_from_yiq = colorsys.yiq_to_rgb(*yiq) # Convert RGB to HLS and back hls = colorsys.rgb_to_hls(r, g, b) rgb_from_hls = colorsys.hls_to_rgb(*hls) # Convert RGB to HSV and back hsv = colorsys.rgb_to_hsv(r, g, b) rgb_from_hsv = colorsys.hsv_to_rgb(*hsv) # Validate conversions def is_close(a, b): return abs(a - b) <= tolerance return ( is_close(r, rgb_from_yiq[0]) and is_close(g, rgb_from_yiq[1]) and is_close(b, rgb_from_yiq[2]) and is_close(r, rgb_from_hls[0]) and is_close(g, rgb_from_hls[1]) and is_close(b, rgb_from_hls[2]) and is_close(r, rgb_from_hsv[0]) and is_close(g, rgb_from_hsv[1]) and is_close(b, rgb_from_hsv[2]) )"},{"question":"**Advanced HTML Parsing with Python\'s `HTMLParser`** You are required to create a custom HTML parser that processes an HTML document to extract and store specific information. The parser will be designed to perform the following operations: 1. Extract and store the names and attributes of all HTML tags. 2. Extract and store the text data present within each tag. 3. Count and report the number of each type of tag encountered in the document. 4. Filter and store comments that contain more than 10 characters. # Task Design and implement a class `CustomHTMLParser` that subclasses `HTMLParser` and overrides its methods to achieve the above operations. Your implementation should include: 1. A method `parse_html` that takes an HTML string as input and uses the custom parser to process the HTML content. 2. Storage for tags, attributes, text data, and comments, which can be accessed after parsing. 3. A method `tag_count_summary` that returns a dictionary with the count of each type of tag. # Class Definition ```python class CustomHTMLParser(HTMLParser): def __init__(self): # Initialize the base class super().__init__() # Initialize data storage self.tags = [] self.data = [] self.comments = [] self.tag_count = {} # Override this method to handle start tags def handle_starttag(self, tag, attrs): # Your code here # Override this method to handle end tags def handle_endtag(self, tag): # Your code here # Override this method to handle text data def handle_data(self, data): # Your code here # Override this method to handle comments def handle_comment(self, data): # Your code here # Include additional methods if necessary def parse_html(self, html_string): # Feed the HTML string to the parser self.feed(html_string) def tag_count_summary(self): # Return the tag count dictionary return self.tag_count # Example usage: html_content = \'\'\' <html> <head><title>Sample Page</title></head> <body> <h1>Welcome!</h1> <p class=\'intro\'>This is a sample HTML page.</p> <div class=\\"content\\"> <p>Here is some text.</p> <!-- This is a comment --> <!-- Another comment with more than ten characters --> </div> </body> </html> \'\'\' parser = CustomHTMLParser() parser.parse_html(html_content) # Access parsed data print(\\"Tags and attributes:\\", parser.tags) print(\\"Text data:\\", parser.data) print(\\"Comments:\\", parser.comments) print(\\"Tag count summary:\\", parser.tag_count_summary()) ``` # Input - `parse_html` method: A single string containing the HTML content to parse. # Output - `tags`: A list of tuples where each tuple contains the tag name and a list of attributes. - `data`: A list of strings representing the text data found within the HTML tags. - `comments`: A list of comment strings containing more than ten characters. - `tag_count_summary` method: A dictionary with keys as tag names and values as the count of occurrences. # Constraints **Performance Requirements**: Efficient memory usage and processing speed for large HTML documents. # Example For the given `html_content`, the expected output should be: ``` Tags and attributes: [ (\'html\', []), (\'head\', []), (\'title\', []), (\'body\', []), (\'h1\', []), (\'p\', [(\'class\', \'intro\')]), (\'div\', [(\'class\', \'content\')]), (\'p\', []) ] Text data: [ \'Sample Page\', \'Welcome!\', \'This is a sample HTML page.\', \'Here is some text.\' ] Comments: [ \' Another comment with more than ten characters \' ] Tag count summary: { \'html\': 1, \'head\': 1, \'title\': 1, \'body\': 1, \'h1\': 1, \'p\': 2, \'div\': 1 } ```","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.tags = [] self.data = [] self.comments = [] self.tag_count = {} def handle_starttag(self, tag, attrs): self.tags.append((tag, attrs)) if tag in self.tag_count: self.tag_count[tag] += 1 else: self.tag_count[tag] = 1 def handle_endtag(self, tag): pass def handle_data(self, data): cleaned_data = data.strip() if cleaned_data: # Avoid storing empty strings from whitespace-only data self.data.append(cleaned_data) def handle_comment(self, data): cleaned_comment = data.strip() if len(cleaned_comment) > 10: self.comments.append(cleaned_comment) def parse_html(self, html_string): self.feed(html_string) def tag_count_summary(self): return self.tag_count html_content = \'\'\' <html> <head><title>Sample Page</title></head> <body> <h1>Welcome!</h1> <p class=\'intro\'>This is a sample HTML page.</p> <div class=\\"content\\"> <p>Here is some text.</p> <!-- This is a comment --> <!-- Another comment with more than ten characters --> </div> </body> </html> \'\'\' parser = CustomHTMLParser() parser.parse_html(html_content) # Access parsed data print(\\"Tags and attributes:\\", parser.tags) print(\\"Text data:\\", parser.data) print(\\"Comments:\\", parser.comments) print(\\"Tag count summary:\\", parser.tag_count_summary())"},{"question":"<|Analysis Begin|> The documentation provided describes the `copy` module in Python, which allows for shallow and deep copying of objects. It explains the difference between shallow and deep copying, methods available in the `copy` module, and the issues with deep copying, such as handling recursive objects and excessive copying. Key points include: - `copy.copy(x)`: Returns a shallow copy. - `copy.deepcopy(x[, memo])`: Returns a deep copy. - Special methods `__copy__()` and `__deepcopy__()` can be overridden in user-defined classes to control their copying behavior. - Memoization is used to avoid redundant copying of objects. Given this, a coding assessment question might involve creating a user-defined class that correctly implements these special methods to handle shallow and deep copies while considering potential issues like self-referencing objects. <|Analysis End|> <|Question Begin|> # Custom Deep Copy Implementation As an experienced Python developer, you are tasked with creating a user-defined class that accurately handles shallow and deep copying. This will test your understanding of Python\'s `copy` module and its deeper concepts. Requirements 1. Create a class called `SelfReferencingObject` that holds an attribute `_data` which stores an integer. 2. Ensure the class can also reference itself using an attribute `self_ref` that can be assigned another instance of `SelfReferencingObject`. 3. Implement the `__copy__` method to correctly handle shallow copying. 4. Implement the `__deepcopy__` method to correctly handle deep copying. This method should handle self-references using the `memo` dictionary to avoid infinite loops. 5. Implement a `__repr__` method to easily visualize the object structure during testing. Example ```python import copy class SelfReferencingObject: def __init__(self, data): self._data = data self.self_ref = None def __copy__(self): new_obj = type(self)(self._data) new_obj.self_ref = self.self_ref return new_obj def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] new_obj = type(self)(copy.deepcopy(self._data, memo)) memo[id(self)] = new_obj new_obj.self_ref = copy.deepcopy(self.self_ref, memo) return new_obj def __repr__(self): return f\\"SelfReferencingObject(data={self._data}, self_ref_id={id(self.self_ref)})\\" # Test the class obj = SelfReferencingObject(10) obj.self_ref = obj shallow_copy = copy.copy(obj) deep_copy = copy.deepcopy(obj) print(\\"Original:\\", obj) print(\\"Shallow Copy:\\", shallow_copy) print(\\"Deep Copy:\\", deep_copy) ``` Expected Output When creating a shallow copy, the self-references should remain the same (pointing to the original instance). However, the deep copy should create a new instance with its own unique self-references. ```python Original: SelfReferencingObject(data=10, self_ref_id=<id-of-original>) Shallow Copy: SelfReferencingObject(data=10, self_ref_id=<id-of-original>) Deep Copy: SelfReferencingObject(data=10, self_ref_id=<id-of-deepcopy>) ``` Constraints - Ensure the class correctly handles self-references to avoid infinite recursion. - The `_data` attribute will always be an integer for simplicity. - No additional libraries apart from the `copy` module should be used. # Performance Requirements - The deep copy operation should avoid redundant copying of already copied objects by using the `memo` dictionary. --- Implement the `SelfReferencingObject` class as described, then test your implementation with the provided test cases and verify the expected output.","solution":"import copy class SelfReferencingObject: def __init__(self, data): self._data = data self.self_ref = None def __copy__(self): new_obj = type(self)(self._data) new_obj.self_ref = self.self_ref return new_obj def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] new_obj = type(self)(copy.deepcopy(self._data, memo)) memo[id(self)] = new_obj new_obj.self_ref = copy.deepcopy(self.self_ref, memo) return new_obj def __repr__(self): return f\\"SelfReferencingObject(data={self._data}, self_ref_id={id(self.self_ref)})\\" # Test the class obj = SelfReferencingObject(10) obj.self_ref = obj shallow_copy = copy.copy(obj) deep_copy = copy.deepcopy(obj) print(\\"Original:\\", obj) print(\\"Shallow Copy:\\", shallow_copy) print(\\"Deep Copy:\\", deep_copy)"},{"question":"# Logging HOWTO Coding Assessment Objective Write a Python function that demonstrates your understanding of Python\'s logging module by setting up a basic logging system. Your function will need to log messages of different severity levels to a file and the console, and include the timestamp, log level, and message. Task Create a function `setup_logging()` that sets up the logging configuration as described in the requirements below: 1. Configure a logger named `my_logger`. 2. The logger should log messages to both a file named `app.log` and the console. 3. Log messages should include the timestamp, log level (e.g., INFO, ERROR), and the message itself. 4. The logger should log messages of all severity levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). 5. Additionally, demonstrate logging messages at each severity level using the configured logger. Requirements - Create the logger named `my_logger` within the `setup_logging` function. - Configure a file handler that writes logs to `app.log`. - Configure a console handler that outputs logs to the console (stdout). - Set up the logging format to include the timestamp, log level, and message (e.g., `%(asctime)s - %(levelname)s - %(message)s`). - Demonstrate logging at least one message for each severity level (DEBUG, INFO, WARNING, ERROR, CRITICAL). Example Usage ```python def setup_logging(): import logging # Your logging setup code here # Demonstrate logging messages my_logger = logging.getLogger(\'my_logger\') my_logger.debug(\'This is a debug message.\') my_logger.info(\'This is an info message.\') my_logger.warning(\'This is a warning message.\') my_logger.error(\'This is an error message.\') my_logger.critical(\'This is a critical message.\') setup_logging() ``` Expected Output The log messages should appear both in the console and in the `app.log` file with the appropriate format (e.g., `2023-10-02 12:00:00,000 - DEBUG - This is a debug message.`). Constraints - Use the Python standard library\'s `logging` module. - The function should be self-contained and should create the \'app.log\' file in the current directory. Performance Requirements - The logging setup should not significantly impact the performance of a simple script. Ensure efficient logger configuration.","solution":"def setup_logging(): import logging from logging import FileHandler, StreamHandler from logging import Formatter # Create a logger named \'my_logger\' my_logger = logging.getLogger(\'my_logger\') my_logger.setLevel(logging.DEBUG) # Log all levels # Create handlers file_handler = FileHandler(\'app.log\') console_handler = StreamHandler() # Create formatters and add it to handlers log_format = Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') file_handler.setFormatter(log_format) console_handler.setFormatter(log_format) # Add handlers to the logger my_logger.addHandler(file_handler) my_logger.addHandler(console_handler) # Demonstrate logging messages at each severity level my_logger.debug(\'This is a debug message.\') my_logger.info(\'This is an info message.\') my_logger.warning(\'This is a warning message.\') my_logger.error(\'This is an error message.\') my_logger.critical(\'This is a critical message.\')"},{"question":"# HTML Entity Conversion In this exercise, you are required to perform conversions between HTML entities and their Unicode character equivalents using the `html.entities` module. You will implement a function that takes an HTML-encoded string and returns a string with all the HTML entities converted to their corresponding Unicode characters. **Function Signature:** ```python def decode_html_entities(html_text: str) -> str: pass ``` **Input:** - `html_text`: A string containing HTML-encoded text, which may include named character references (e.g., `&amp;`, `&gt;`), decimal numeric character references (e.g., `&#38;`, `&#62;`), and hexadecimal numeric character references (e.g., `&#x26;`, `&#x3E;`). **Output:** - Returns a string with all HTML entities converted to the corresponding Unicode characters. **Constraints:** 1. You should handle both named and numeric character references. 2. For trailing semicolons, if it is omitted and the reference is still recognized by the HTML5 standard, handle it appropriately (such as `&amp` being valid for `&amp;`). **Example:** ```python assert decode_html_entities(\\"The prices are &lt;50 &amp; &gt;10 units.\\") == \\"The prices are <50 & >10 units.\\" assert decode_html_entities(\\"Learn &lt;em&gt;Python&lt;/em&gt; at &#x0026;#39;RealPython&#x0026;#39;.\\") == \\"Learn <em>Python</em> at \'&#39;RealPython&#39;.\\" assert decode_html_entities(\\"Use &amp or &amp;amp;\\") == \\"Use & or &\\" ``` **Additional Notes:** - You can leverage the dictionaries `name2codepoint` and `codepoint2name` from `html.entities` to achieve the conversion. - Handle both the presence and absence of trailing semicolons in named character references as defined by HTML5 standards. You may use the `html.entities` module documentation to aid your implementation. # Important: Ensure the solution is efficient and handles a reasonable size of text data effectively. This problem requires both the accurate conversion of entities and maintaining the overall structure of the text.","solution":"import html def decode_html_entities(html_text: str) -> str: Decode HTML entities in the given text to their corresponding Unicode characters. Parameters: html_text (str): A string containing HTML-encoded text. Returns: str: A string with HTML entities decoded to corresponding Unicode characters. return html.unescape(html_text)"},{"question":"# Distributed Training and Fault Tolerance with PyTorch You have been tasked with writing a Python script that implements a basic distributed training setup using PyTorch. Your goal is to design a flexible and fault-tolerant distributed training mechanism. Requirements: 1. **Distributed Initialization**: Implement the initialization function for a distributed PyTorch training job. Your function should set up the process group based on the provided arguments. 2. **Fault Tolerance**: Implement fault tolerance by allowing the job to restart in case of failures. You\'ll need to consider both node failures and process failures. 3. **Elasticity**: Implement the ability to handle the scaling events, such as nodes joining or leaving the training cluster. # Inputs: - `nnodes`: An integer representing the number of nodes. - `nproc_per_node`: An integer representing the number of processes to launch on each node. - `max_restarts`: An integer representing the maximum number of restarts allowed. - `rdzv_id`: A string representing the job ID. - `rdzv_backend`: A string representing the rendezvous backend (e.g., `c10d`). - `rdzv_endpoint`: A string representing the rendezvous endpoint in the form <host>[:<port>]. - `training_script`: A string representing the path to the training script. # Output: Your script should correctly initialize, manage failures, and handle scaling events. It should not directly print any output but should configure the distributed environment for your training script. # Constraints: - You must use the `torch.distributed` module for implementing the distributed training. - Handle configurations for both fault-tolerant and elastic jobs. - Use appropriate logging to ensure traceability of steps and events. Sample Function Signatures: ```python def initialize_distributed_training(nnodes: int, nproc_per_node: int, rdzv_id: str, rdzv_backend: str, rdzv_endpoint: str): Initializes the distributed training environment with given arguments. pass def manage_fault_tolerance(max_restarts: int): Manages fault tolerance, allowing up to the specified number of restarts. pass def handle_scaling_events(rdzv_backend: str, rdzv_endpoint: str): Handles scaling events, such as nodes joining or leaving. pass def main(nnodes: int, nproc_per_node: int, max_restarts: int, rdzv_id: str, rdzv_backend: str, rdzv_endpoint: str, training_script: str): Main function to set up and run the distributed training job. pass ``` Notes: 1. Do not implement the actual training script logic; focus on setting up the environment for the training script to run in a distributed manner. 2. Ensure your solution is modular, clearly separating initialization, fault tolerance management, and scaling event handling. 3. Consider edge cases and error handling to make your solution robust. You are encouraged to refer to PyTorch\'s `torch.distributed` documentation for necessary APIs and additional details.","solution":"import os import torch.distributed as dist import logging logging.basicConfig(level=logging.INFO) def initialize_distributed_training(nnodes: int, nproc_per_node: int, rdzv_id: str, rdzv_backend: str, rdzv_endpoint: str): Initializes the distributed training environment with given arguments. dist.init_process_group( backend=\'nccl\', init_method=f\\"{rdzv_backend}://{rdzv_endpoint}/{rdzv_id}\\", world_size=nnodes * nproc_per_node, rank=int(os.environ[\'RANK\']), ) def manage_fault_tolerance(max_restarts: int): Manages fault tolerance, allowing up to the specified number of restarts. restart_count = 0 while restart_count < max_restarts: try: # Assume run_training is the function to start training run_training() break except Exception as e: logging.error(f\\"Training failed with error {e}. Restarting {restart_count + 1}/{max_restarts}\\") restart_count += 1 if restart_count == max_restarts: logging.error(\\"Reached maximum number of restarts. Training failed.\\") def handle_scaling_events(rdzv_backend: str, rdzv_endpoint: str): Handles scaling events, such as nodes joining or leaving. # No-op for now as elastic training needs specialized handling, # which involves the elastic agent from PyTorch logging.info(\\"Started monitoring scaling events.\\") # Placeholder for scaling handling pass def main(nnodes: int, nproc_per_node: int, max_restarts: int, rdzv_id: str, rdzv_backend: str, rdzv_endpoint: str, training_script: str): Main function to set up and run the distributed training job. initialize_distributed_training(nnodes, nproc_per_node, rdzv_id, rdzv_backend, rdzv_endpoint) manage_fault_tolerance(max_restarts) handle_scaling_events(rdzv_backend, rdzv_endpoint) # Execute the training script os.system(f\\"python {training_script}\\") def run_training(): A placeholder function representing the actual training process. Replace this with the actual training logic. pass"},{"question":"# Question: Implement a Custom Async Iterator in Python Using the Python functionality described in the provided documentation on iterators, implement a custom asynchronous iterator in Python. The iterator should iterate over a sequence of integers and for each integer, it should wait for a certain duration that is proportional to the integer (simulating an async operation). To structure this: 1. Create a class `AsyncIntegerIterator` that implements the asynchronous iterator protocol. 2. The class should take a list of integers as input during initialization. 3. Implement the `__aiter__` method that returns the iterator object itself. 4. Implement the `__anext__` method which should: - Return the next integer from the list after awaiting a duration proportional to the integer (hint: you can use `asyncio.sleep`). - Raise `StopAsyncIteration` when the sequence is exhausted. # Constraints: - Do not use any built-in functions to directly convert the class into an iterator. - Must comply with async/await syntax of Python. # Input: - A list of integers provided to the class during initialization. # Output: - Asynchronous iteration over the integers with proportional delays. # Example: ```python import asyncio class AsyncIntegerIterator: def __init__(self, integers): self.integers = integers self.index = 0 def __aiter__(self): return self async def __anext__(self): if self.index >= len(self.integers): raise StopAsyncIteration await asyncio.sleep(self.integers[self.index] * 0.1) value = self.integers[self.index] self.index += 1 return value # Example usage: async def main(): async for number in AsyncIntegerIterator([1, 2, 3]): print(number) # Running the main function asyncio.run(main()) ``` The `main` coroutine should print each integer with a delay proportional to its value times 0.1 seconds. # Performance Requirements: - Ensure that the implementation handles the iteration correctly without errors. - Make sure that the waiting times are handled asynchronously to avoid blocking operations.","solution":"import asyncio class AsyncIntegerIterator: def __init__(self, integers): self.integers = integers self.index = 0 def __aiter__(self): return self async def __anext__(self): if self.index >= len(self.integers): raise StopAsyncIteration await asyncio.sleep(self.integers[self.index] * 0.1) value = self.integers[self.index] self.index += 1 return value # Example usage: async def main(): async for number in AsyncIntegerIterator([1, 2, 3]): print(number) # Running the main function if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Objective To assess your understanding of Seaborn\'s functionality, especially your ability to visualize data using different formats and transformations. Question You are given a dataset containing information about tips received by waiters at a restaurant. This dataset includes variables such as the total bill, tip amount, sex, smoker status, day of the week, time of day, and the size of the party. Your task is to create visualizations that reveal insights about the tips received. # Dataset Use the following code to load the dataset: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` # Requirements 1. **Summary Statistics Plot:** - Create a summary plot showing the average tip amount for each day of the week. - Use a bar plot to visualize this information. 2. **Category Plot:** - Visualize the relationship between the size of the party (`size`) and the tip amount, separated by day of the week using a `boxplot`. 3. **Advanced Plot with Data Transformation:** - Transform the dataset to create a long-form version that includes a new variable indicating whether a tip was above or below the average tip amount. - Create a plot showing the distribution of tips above and below the average against the total bill for different days of the week using a `violin plot`. # Constraints 1. Ensure your plots are clearly labeled, with appropriate axis labels and titles. 2. Perform any necessary data transformations using pandas. 3. Plots should be created using Seaborn. Expected Input and Output Formats - **Input:** The dataset is loaded from Seaborn directly using `sns.load_dataset(\\"tips\\")`. - **Output:** Plots visualized using matplotlib (outputting as images in Jupyter Notebook). # Example Solution ```python import seaborn as sns import pandas as pd # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Summary Statistics Plot summary_plot = sns.barplot(x=\\"day\\", y=\\"tip\\", data=tips) summary_plot.set_title(\\"Average Tip Amount by Day\\") sns.despine() summary_plot.figure.show() # 2. Category Plot category_plot = sns.boxplot(x=\\"day\\", y=\\"tip\\", hue=\\"size\\", data=tips) category_plot.set_title(\\"Tip Amount by Party Size and Day of Week\\") sns.despine() category_plot.figure.show() # 3. Advanced Plot with Data Transformation average_tip = tips[\\"tip\\"].mean() tips[\\"tip_above_avg\\"] = tips[\\"tip\\"] > average_tip tips_long = tips.melt(id_vars=[\\"total_bill\\", \\"day\\"], value_vars=[\\"tip_above_avg\\"], var_name=\\"Tip Level\\", value_name=\\"Above Average\\") advanced_plot = sns.violinplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"Above Average\\", data=tips_long) advanced_plot.set_title(\\"Distribution of Total Bill by Tip Level and Day\\") sns.despine() advanced_plot.figure.show() ```","solution":"import seaborn as sns import pandas as pd # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Summary Statistics Plot def plot_average_tip_by_day(): summary_plot = sns.barplot(x=\\"day\\", y=\\"tip\\", data=tips) summary_plot.set_title(\\"Average Tip Amount by Day\\") summary_plot.set_xlabel(\\"Day of the Week\\") summary_plot.set_ylabel(\\"Average Tip Amount\\") sns.despine() return summary_plot # 2. Category Plot def plot_tip_by_party_size_and_day(): category_plot = sns.boxplot(x=\\"day\\", y=\\"tip\\", hue=\\"size\\", data=tips) category_plot.set_title(\\"Tip Amount by Party Size and Day of the Week\\") category_plot.set_xlabel(\\"Day of the Week\\") category_plot.set_ylabel(\\"Tip Amount\\") sns.despine() return category_plot # 3. Advanced Plot with Data Transformation def plot_tip_distribution_by_total_bill(): average_tip = tips[\\"tip\\"].mean() tips[\\"tip_above_avg\\"] = tips[\\"tip\\"] > average_tip advanced_plot = sns.violinplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"tip_above_avg\\", data=tips, split=True) advanced_plot.set_title(\\"Distribution of Total Bill by Tip Level and Day\\") advanced_plot.set_xlabel(\\"Day of the Week\\") advanced_plot.set_ylabel(\\"Total Bill\\") sns.despine() return advanced_plot"},{"question":"# Question: Optimizing Matrix Multiplication with CUDA Graphs in PyTorch You are given the task of optimizing a matrix multiplication operation using CUDA Graphs in PyTorch to minimize CPU overhead and improve performance. You will implement a function that captures and replays a CUDA graph for matrix multiplication. Function Signature ```python def optimize_matmul_with_cuda_graph(A, B): Captures and replays a CUDA graph for matrix multiplication of two matrices A and B. Args: - A (torch.Tensor): The first matrix of shape (M, N) on CUDA device. - B (torch.Tensor): The second matrix of shape (N, P) on CUDA device. Returns: - out (torch.Tensor): The resultant matrix of shape (M, P) from the CUDA graph. - graph (torch.cuda.CUDAGraph): The CUDA graph object capturing the matrix multiplication. ``` Input Constraints: - `A` and `B` are two-dimensional CUDA tensors (`torch.Tensor`), with shapes `(M, N)` and `(N, P)` respectively. - Both matrices are placed on the same CUDA device. - Ensure that the input tensors are static (i.e., their shapes do not change). Output: - The function should return: 1. The resultant matrix from the CUDA graph. 2. The CUDA graph object capturing the matrix multiplication for potential reuse. Additional Constraints: - You should employ CUDA streams for efficient capture and execution. - Ensure necessary warm-up before capturing the graph. - Use appropriate memory management to maintain the correct state across multiple replays. Example: ```python import torch # Initialize matrices A = torch.randn(1024, 1024, device=\'cuda\') B = torch.randn(1024, 1024, device=\'cuda\') # Call the function out, graph = optimize_matmul_with_cuda_graph(A, B) # Print the output shape to verify print(out.shape) # Expected Output: torch.Size([1024, 1024]) ``` Note: 1. You may assume that the inputs will only contain CUDA tensors and no validation is required. 2. Performance will be tested based on the speedup achieved using CUDA graphs compared to traditional matrix multiplication.","solution":"import torch def optimize_matmul_with_cuda_graph(A, B): Captures and replays a CUDA graph for matrix multiplication of two matrices A and B. Args: - A (torch.Tensor): The first matrix of shape (M, N) on CUDA device. - B (torch.Tensor): The second matrix of shape (N, P) on CUDA device. Returns: - out (torch.Tensor): The resultant matrix of shape (M, P) from the CUDA graph. - graph (torch.cuda.CUDAGraph): The CUDA graph object capturing the matrix multiplication. # Ensure the inputs are on CUDA device assert A.is_cuda and B.is_cuda, \\"Both matrices must be on the CUDA device.\\" # Warm-up by performing standard matrix multiplication stream = torch.cuda.Stream() with torch.cuda.stream(stream): torch.cuda.synchronize() _ = torch.mm(A, B) torch.cuda.synchronize() # Allocate output tensor C = torch.empty(A.size(0), B.size(1), device=A.device) # Capture the CUDA graph graph = torch.cuda.CUDAGraph() with torch.cuda.graph(graph): C_out = torch.mm(A, B, out=C) # Replay the CUDA graph graph.replay() return C_out, graph"},{"question":"# Question: You are provided with the Wine dataset, which is included in the scikit-learn library. The Wine dataset consists of 178 instances, each with 13 attributes that describe various chemical properties of wines. Your task is to write a Python function using scikit-learn that performs the following steps: 1. Loads the Wine dataset. 2. Splits the dataset into training and testing sets (80% training, 20% testing). 3. Performs standard scaling on the dataset. 4. Trains a RandomForestClassifier on the training data. 5. Evaluates the classifier on the testing data and returns the accuracy and confusion matrix. The function signature should be: ```python def evaluate_wine_classifier(): # Your code here ``` # Constraints: - You must use the scikit-learn library for dataset loading, preprocessing, and model training. - Use `RandomForestClassifier` from the `sklearn.ensemble` module. - Use `train_test_split` from the `sklearn.model_selection` module. - Use `StandardScaler` from the `sklearn.preprocessing` module. - Use `accuracy_score` and `confusion_matrix` from the `sklearn.metrics` module. - The random state for the train-test split and the RandomForestClassifier should be set to 42 for reproducibility. # Expected Output: - The function should return a tuple containing: 1. The accuracy of the classifier on the testing data (a float). 2. The confusion matrix of the classifier on the testing data (a 2D array). # Example Usage: ```python accuracy, conf_matrix = evaluate_wine_classifier() print(\\"Accuracy:\\", accuracy) print(\\"Confusion Matrix:n\\", conf_matrix) ``` # Note: Make sure your code is well-documented and follows best practices for writing clean and maintainable code.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, confusion_matrix def evaluate_wine_classifier(): # Load the wine dataset wine = load_wine() X = wine.data y = wine.target # Split the dataset into training and testing sets (80% training, 20% testing) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Perform standard scaling on the dataset scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train a RandomForestClassifier on the training data clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate the classifier on the testing data y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix"},{"question":"Coding Assessment Question # Objective You are required to design a Python program that performs specific file management operations and measures the performance of these operations using the given modules in the standard library. # Problem You need to write a Python script that does the following: 1. **Create and Change Directories**: Create a directory named `test_directory` in the current working directory. Change the script\'s working directory to `test_directory`. 2. **Create Files**: Within `test_directory`, create 100 text files named `file1.txt`, `file2.txt`, ..., `file100.txt`. Each file should contain 10 randomized alphanumeric strings, with each string on a new line. 3. **Copy Files**: Copy all these files into another directory named `backup_directory` within `test_directory`. 4. **Compress Files**: Compress the entire `backup_directory` into a file named `backup.zip` in the `test_directory`. 5. **Performance Measurement**: Measure the time taken to perform steps 3 and 4 using the `timeit` module. # Constraints - Use the `os` and `shutil` modules for directory and file operations. - Use the `random` module to generate random alphanumeric strings. - Use the `zlib` module for compressing files. - Measure the performance of file copying and compression operations. # Expected Input and Output Formats - No specific input format is required as the operations are to be performed programmatically. - The output should contain the time taken to perform the file copying and compression steps. # Function Signature ```python def perform_file_operations_and_measure_performance(): # Your implementation here ``` # Example Output ```plaintext Time taken to copy files: 0.345 seconds Time taken to compress files: 0.567 seconds ``` # Notes - Ensure proper error handling and clean up any created directories or files after execution. - Document your code appropriately to explain your logic and steps. - Focus on efficiency and clear structuring of the code.","solution":"import os import shutil import random import string import timeit import zipfile def create_random_string(length=10): Generates a random alphanumeric string of given length. return \'\'.join(random.choices(string.ascii_letters + string.digits, k=length)) def perform_file_operations_and_measure_performance(): # Step 1: Create \'test_directory\' and change to it os.makedirs(\'test_directory\', exist_ok=True) os.chdir(\'test_directory\') # Step 2: Create 100 text files with random alphanumeric strings for i in range(1, 101): with open(f\'file{i}.txt\', \'w\') as f: f.write(\'n\'.join(create_random_string() for _ in range(10))) # Step 3: Copy files to \'backup_directory\' and measure time os.makedirs(\'backup_directory\', exist_ok=True) def copy_files(): for i in range(1, 101): shutil.copy(f\'file{i}.txt\', \'backup_directory\') copy_time = timeit.timeit(copy_files, number=1) # Step 4: Compress \'backup_directory\' into \'backup.zip\' and measure time def compress_files(): with zipfile.ZipFile(\'backup.zip\', \'w\', zipfile.ZIP_DEFLATED) as backup_zip: for foldername, subfolders, filenames in os.walk(\'backup_directory\'): for filename in filenames: file_path = os.path.join(foldername, filename) backup_zip.write(file_path, os.path.relpath(file_path, \'backup_directory\')) compress_time = timeit.timeit(compress_files, number=1) # Output the results print(f\\"Time taken to copy files: {copy_time:.3f} seconds\\") print(f\\"Time taken to compress files: {compress_time:.3f} seconds\\") # Cleanup os.chdir(\'..\') shutil.rmtree(\'test_directory\')"},{"question":"# Advanced Supervised Learning with Scikit-Learn **Objective:** In this assessment, you are required to demonstrate your proficiency in several advanced supervised learning techniques using the scikit-learn library. Specifically, you will implement a classification pipeline using multiple supervised learning algorithms and evaluate their performance. # Task: 1. **Data Preparation:** Load the provided dataset and preprocess it (handle missing values if any, standardize features). 2. **Model Implementation:** Implement classification using at least three different models from the following categories: - **Linear Models:** For example, Logistic Regression. - **Trees and Forests:** For example, Decision Tree or Random Forest. - **Support Vector Machines:** For example, SVM with a linear or RBF kernel. 3. **Training and Evaluation:** Train each model using the training data and evaluate their performance using accuracy, precision, recall, and F1-score on a separate test dataset. 4. **Comparison and Analysis:** Provide a detailed comparison of the models based on the evaluation metrics. # Requirements: 1. **Input:** * A CSV file named \\"data.csv\\" containing the dataset with features and target variable. 2. **Output:** * Printed evaluation metrics for each model. * A summary paragraph highlighting which model performed best and possible reasons why. # Constraints: - You must use scikit-learn for model implementation. - Ensure reproducibility by setting a random seed where applicable. - Preprocessing should include standard scaling of features. # Performance Requirements: - Your solution should efficiently handle datasets with up to 10,000 instances and 50 features. # Example: Here is a skeleton that you need to complete: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # 1. Data Preparation data = pd.read_csv(\'data.csv\') X = data.drop(\'target\', axis=1) y = data[\'target\'] # Handle missing values if any (assuming mean imputation in this example) X.fillna(X.mean(), inplace=True) # Splitting data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardizing features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 2. Implementing models models = { \'Logistic Regression\': LogisticRegression(random_state=42), \'Decision Tree\': DecisionTreeClassifier(random_state=42), \'SVM\': SVC(kernel=\'rbf\', random_state=42) } # 3. Training and Evaluation results = {} for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) results[name] = { \'Accuracy\': accuracy_score(y_test, y_pred), \'Precision\': precision_score(y_test, y_pred), \'Recall\': recall_score(y_test, y_pred), \'F1-Score\': f1_score(y_test, y_pred) } # 4. Print results and comparison for name, metrics in results.items(): print(f\\"Results for {name}:\\") for metric, score in metrics.items(): print(f\\"{metric}: {score:.4f}\\") print(\\"n\\") # Summary of comparison print(\\"Summary and Analysis:\\") # Write your summary here ``` # Submission: - Submit your Python script (.py file) implementing the above case. - Make sure to include comments and explanations within your code to demonstrate your reasoning and understanding of each step.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_and_prepare_data(file_path): # Load the dataset data = pd.read_csv(file_path) X = data.drop(\'target\', axis=1) y = data[\'target\'] # Handle missing values X.fillna(X.mean(), inplace=True) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_and_evaluate_models(X_train, X_test, y_train, y_test): # Define the models models = { \'Logistic Regression\': LogisticRegression(random_state=42), \'Decision Tree\': DecisionTreeClassifier(random_state=42), \'SVM\': SVC(kernel=\'rbf\', random_state=42) } # Train and evaluate each model results = {} for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) results[name] = { \'Accuracy\': accuracy_score(y_test, y_pred), \'Precision\': precision_score(y_test, y_pred), \'Recall\': recall_score(y_test, y_pred), \'F1-Score\': f1_score(y_test, y_pred) } return results def print_results(results): for name, metrics in results.items(): print(f\\"Results for {name}:\\") for metric, score in metrics.items(): print(f\\"{metric}: {score:.4f}\\") print(\\"n\\") def summary_analysis(results): best_model = max(results.items(), key=lambda item: item[1][\'F1-Score\'])[0] print(\\"Summary and Analysis:\\") print(f\\"The best performing model is {best_model}.\\") for name, metrics in results.items(): print(f\\"{name}:\\") for metric, score in metrics.items(): print(f\\"{metric}: {score:.4f}\\") print(\\"n\\") if __name__ == \\"__main__\\": data_file = \'data.csv\' X_train, X_test, y_train, y_test = load_and_prepare_data(data_file) results = train_and_evaluate_models(X_train, X_test, y_train, y_test) print_results(results) summary_analysis(results)"},{"question":"# Objective Write a Python function that attempts to open a file and returns a detailed error message if it fails. The error message should include the following information: 1. The error code. 2. The corresponding error name using `errno.errorcode`. 3. The human-readable error message using `os.strerror()`. # Function Signature ```python def open_file_with_detailed_error(filepath: str) -> None: Attempts to open a file and prints a detailed error message if it fails. Parameters: filepath (str): The path of the file to open. Returns: None Raises: ValueError: If the filepath is not a string or is empty. ``` # Expected Input and Output - **Input:** A string representing the file path. For example: `\\"/path/to/file.txt\\"` - **Output:** None. If an error occurs, print the detailed error message. # Constraints - You should handle invalid file paths and other potential errors using the `errno` module. - Raise a `ValueError` if the `filepath` is not a string or is an empty string. - Use the `try-except` block to manage exceptions when attempting to open the file. # Example ```python # Example usage and expected output: # Assuming /path/to/file.txt does not exist open_file_with_detailed_error(\\"/path/to/file.txt\\") # Output format (the specific error code and message may vary depending on the system): # Error code: 2 # Error name: ENOENT # Error message: No such file or directory ``` # Instructions 1. Import the necessary modules: `os` and `errno`. 2. Validate the input parameters. 3. Use a `try-except` block to catch exceptions when trying to open the file. 4. Extract the error code, error name, and error message, and print them in the specified format. **Note:** Do not use any external libraries. All functionality should be implemented using standard Python libraries. # Performance Requirements - The function should handle the error cases efficiently. - Given that the function mainly handles exceptions, performance concerns are minimal, but you should ensure good practices like avoiding redundant exception handling. Write your solution in the function template provided.","solution":"import os import errno def open_file_with_detailed_error(filepath: str) -> None: Attempts to open a file and prints a detailed error message if it fails. Parameters: filepath (str): The path of the file to open. Returns: None Raises: ValueError: If the filepath is not a string or is empty. if not isinstance(filepath, str) or not filepath.strip(): raise ValueError(\\"The filepath must be a non-empty string.\\") try: with open(filepath, \'r\') as file: pass # File opened successfully except OSError as e: error_code = e.errno error_name = errno.errorcode[error_code] error_message = os.strerror(error_code) print(f\\"Error code: {error_code}\\") print(f\\"Error name: {error_name}\\") print(f\\"Error message: {error_message}\\")"},{"question":"# Terminal Input Manipulation with \\"tty\\" and \\"termios\\" In this task, you will write a function that changes the terminal input mode to raw, captures input from the user, and then restores the terminal to its original mode. This will help assess your understanding of terminal control in Python and your ability to work with file descriptors. Function Signature ```python def capture_raw_input(prompt: str) -> str: Captures user input in raw mode. Args: prompt (str): A prompt message to display to the user. Returns: str: The user input captured in raw mode. Raises: RuntimeError: If the terminal mode could not be set or restored. ``` Requirements 1. The function should: - Use the `tty` and `termios` modules to set the terminal mode to raw. - Display the provided prompt to the user. - Capture all keystrokes typed by the user (including non-printable characters) until they press `Enter` (newline character). - Restore the terminal to its original mode after capturing the input. - Return the captured input as a string. 2. Handle exceptions where the terminal mode could not be set or restored and raise a `RuntimeError` with an appropriate message. 3. Ensure functionality is limited to Unix-like systems as the `tty` module does not work on Windows. Example Usage ``` # Assuming you are running this on a Unix-like system user_input = capture_raw_input(\\"Enter raw input: \\") print(\\"Captured input:\\", user_input) ``` In the example above: - The terminal switches to raw mode. - The user sees the prompt \\"Enter raw input: \\" and types in some text. - All keystrokes are captured, including special characters. - The terminal mode is restored to its original state. - The captured input is printed on the screen. Constraints - The function should not exit or crash unexpectedly. - Assume the function will only be invoked in a supported Unix environment. Hints - You might find `termios.tcgetattr` and `termios.tcsetattr` useful for getting and setting terminal attributes. - Use `sys.stdin.fileno()` to get the file descriptor for standard input. # Note This task will require you to test your function on a Unix-like operating system, as the `tty` module and its dependencies are not available on Windows.","solution":"import sys import tty import termios def capture_raw_input(prompt: str) -> str: Captures user input in raw mode. Args: prompt (str): A prompt message to display to the user. Returns: str: The user input captured in raw mode. Raises: RuntimeError: If the terminal mode could not be set or restored. original_attrs = termios.tcgetattr(sys.stdin.fileno()) try: tty.setraw(sys.stdin.fileno()) sys.stdout.write(prompt) sys.stdout.flush() buffer = [] while True: ch = sys.stdin.read(1) if ch == \'n\': break buffer.append(ch) return \'\'.join(buffer) except Exception as e: raise RuntimeError(f\\"Error during terminal mode change: {e}\\") finally: termios.tcsetattr(sys.stdin.fileno(), termios.TCSADRAIN, original_attrs)"},{"question":"**Question:** Implement a function `get_annotations(obj)` that retrieves the annotations from a given Python object `obj`. The function should work for Python 3.10 and newer, as well as for older versions (specifically Python 3.9 and older). Your implementation should handle the following: 1. Use `inspect.get_annotations()` when available and appropriate. 2. For functions, classes, and modules, ensure that `__annotations__` is accessed correctly. 3. For other callables, use `getattr` to safely retrieve the `__annotations__`. 4. In Python 3.9 and older, ensure that class annotations do not return annotations from the base class. 5. Handle any stringized annotations by converting them back to their original type representations where appropriate. **Constraints:** - Do not use any external packages. The solution must work with the Python built-in libraries. **Input:** - `obj` (object): The Python object from which to retrieve annotations. **Output:** - A dictionary representing the annotations of the object. **Examples:** ```python # Example in Python 3.10 and newer from typing import List class Example: x: int y: str print(get_annotations(Example)) # Output: {\'x\': int, \'y\': str} # Example in Python 3.9 and older class Base: a: int b: str class Derived(Base): c: List[int] print(get_annotations(Derived)) # Output: {\'c\': List[int]} # Example with stringized annotations def fn(a: \'int\', b: \'str\') -> \'None\': pass print(get_annotations(fn)) # Output: {\'a\': int, \'b\': str, \'return\': None} ``` **Note:** You should add necessary import statements to ensure the function works correctly in both Python 3.10+ and older versions.","solution":"import builtins import sys import typing import inspect def get_annotations(obj): if sys.version_info >= (3, 10): from types import FunctionType, MethodType, ModuleType else: FunctionType, MethodType, ModuleType = type(lambda: None), type(str.upper), type(sys) if inspect.isroutine(obj) or inspect.isclass(obj) or isinstance(obj, ModuleType): if hasattr(inspect, \'get_annotations\'): # for Python 3.10+ annotations = inspect.get_annotations(obj) else: annotations = getattr(obj, \'__annotations__\', {}) else: annotations = getattr(obj, \'__annotations__\', {}) if sys.version_info < (3, 10) and isinstance(obj, type): # For classes in Python 3.9 and older, exclude base class annotations base_annotations = {} for base in obj.__bases__: if hasattr(base, \'__annotations__\'): base_annotations.update(base.__annotations__) annotations = {k: v for k, v in annotations.items() if k not in base_annotations} # Handle stringized annotations def eval_annotation(annotation): if isinstance(annotation, str): try: return eval(annotation, obj.__globals__ if hasattr(obj, \'__globals__\') else globals()) except Exception: pass return annotation if annotations: return {k: eval_annotation(v) for k, v in annotations.items()} return annotations"},{"question":"Objective Design a Python function that processes a given list of Unicode characters to determine specific properties and return a summary of the findings. Problem Statement You are given a list of Unicode characters. Write a function `unicode_summary(char_list)` that takes a list of Unicode characters and returns a dictionary summarizing various properties of each character. The dictionary should have the Unicode characters as keys and another dictionary as the value, containing the properties of each character. The properties to include are: - name: The name of the character as a string. - decimal: The decimal value assigned to the character (if defined), otherwise `None`. - digit: The digit value assigned to the character (if defined), otherwise `None`. - numeric: The numeric value assigned to the character (if defined), otherwise `None`. - category: The general category assigned to the character. - mirrored: The mirrored property assigned to the character (1 if it is mirrored, 0 otherwise). You should use appropriate functions from the `unicodedata` module to obtain these properties. If a property is not defined for a character, use `None` as the default value. **Input:** - A list of Unicode characters (e.g., `[\'A\', \'1\', \'u00C7\']`). **Output:** - A dictionary with Unicode characters as keys and their properties as values (dictionaries). **Constraints:** - The list will contain at least one character and at most 100 characters. - The characters will be valid Unicode characters. **Example:** ```python import unicodedata def unicode_summary(char_list): summary = {} for char in char_list: char_info = {} try: char_info[\'name\'] = unicodedata.name(char) except ValueError: char_info[\'name\'] = None char_info[\'decimal\'] = unicodedata.decimal(char, None) char_info[\'digit\'] = unicodedata.digit(char, None) char_info[\'numeric\'] = unicodedata.numeric(char, None) char_info[\'category\'] = unicodedata.category(char) char_info[\'mirrored\'] = unicodedata.mirrored(char) summary[char] = char_info return summary # Example usage: char_list = [\'A\', \'1\', \'u00C7\'] print(unicode_summary(char_list)) # Expected output: # { # \'A\': { # \'name\': \'LATIN CAPITAL LETTER A\', # \'decimal\': None, # \'digit\': None, # \'numeric\': None, # \'category\': \'Lu\', # \'mirrored\': 0 # }, # \'1\': { # \'name\': \'DIGIT ONE\', # \'decimal\': 1, # \'digit\': 1, # \'numeric\': 1.0, # \'category\': \'Nd\', # \'mirrored\': 0 # }, # \'Ç\': { # \'name\': \'LATIN CAPITAL LETTER C WITH CEDILLA\', # \'decimal\': None, # \'digit\': None, # \'numeric\': None, # \'category\': \'Lu\', # \'mirrored\': 0 # } # } ``` Ensure your function adheres to these specifications and handles error cases gracefully.","solution":"import unicodedata def unicode_summary(char_list): Takes a list of Unicode characters and returns a dictionary summarizing their properties. summary = {} for char in char_list: char_info = {} try: char_info[\'name\'] = unicodedata.name(char) except ValueError: char_info[\'name\'] = None char_info[\'decimal\'] = unicodedata.decimal(char, None) char_info[\'digit\'] = unicodedata.digit(char, None) char_info[\'numeric\'] = unicodedata.numeric(char, None) char_info[\'category\'] = unicodedata.category(char) char_info[\'mirrored\'] = unicodedata.mirrored(char) summary[char] = char_info return summary"},{"question":"Problem Statement Design a CGI script that processes user input from an HTML form. The form should include the following fields: 1. `name`: A single-line text field. 2. `email`: A single-line text field. 3. `submission`: A file upload field. Your CGI script should do the following: 1. Process the form data and ensure all fields are filled. If any field is missing, return an HTML error message. 2. Ensure that the email address is in a valid format. 3. Save the uploaded file to a directory on the server, ensuring the file name is unique. 4. Return an HTML response confirming the successful submission and displaying the form data (excluding the file content). # Input The input consists of form data with fields `name`, `email`, and `submission`, submitted via an HTTP POST request. # Output The output consists of an HTML page with the form data and a confirmation message if the submission is successful, or an error message if any field is missing or invalid. # Constraints - The `name` and `email` fields must not be empty. - The email field must contain a valid email address (you can use a simple regex for validation). - The uploaded file should be saved in a directory named `uploads`. # Requirements - Use the `FieldStorage` class to process the form data. - Perform error handling using the `cgitb` module to provide detailed error reports. - Ensure the uploaded file is saved with a unique filename to avoid overwriting existing files. - Include security measures to validate user inputs and avoid security vulnerabilities. # Example Here is a basic outline of the HTML form: ```html <!DOCTYPE html> <html> <head> <title>Form Submission</title> </head> <body> <form action=\\"/cgi-bin/submit.py\\" method=\\"post\\" enctype=\\"multipart/form-data\\"> Name: <input type=\\"text\\" name=\\"name\\"><br> Email: <input type=\\"text\\" name=\\"email\\"><br> Upload File: <input type=\\"file\\" name=\\"submission\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> </body> </html> ``` # Implementation Write the Python code for the CGI script in `cgi-bin/submit.py`. ```python #!/usr/bin/env python3 import cgi import cgitb; cgitb.enable() import os import re import uuid # Define the directory to save uploaded files UPLOAD_DIR = \\"/path/to/uploads\\" # Helper function to validate an email address def is_valid_email(email): return re.match(r\'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\', email) # Main function to process the form data def main(): print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() # Check if all fields are provided if \\"name\\" not in form or \\"email\\" not in form or \\"submission\\" not in form: print(\\"<h1>Error</h1>\\") print(\\"<p>All fields are required.</p>\\") return name = form.getvalue(\\"name\\") email = form.getvalue(\\"email\\") fileitem = form[\\"submission\\"] # Validate email address if not is_valid_email(email): print(\\"<h1>Error</h1>\\") print(\\"<p>Email address is not valid.</p>\\") return # Ensure file upload is successful if not fileitem.file: print(\\"<h1>Error</h1>\\") print(\\"<p>File upload failed.</p>\\") return # Generate a unique filename and save the file filename = str(uuid.uuid4()) + \\"_\\" + os.path.basename(fileitem.filename) filepath = os.path.join(UPLOAD_DIR, filename) with open(filepath, \'wb\') as f: f.write(fileitem.file.read()) # Display confirmation and form data print(\\"<h1>Form Submission Successful</h1>\\") print(\\"<p>Name: {}</p>\\".format(name)) print(\\"<p>Email: {}</p>\\".format(email)) print(\\"<p>File saved as: {}</p>\\".format(filename)) if __name__ == \\"__main__\\": main() ``` # Notes - Adjust the `UPLOAD_DIR` to the correct path where you want to save the uploaded files. - Ensure the `cgi-bin` directory and `uploads` directory have the correct permissions for the webserver to execute the script and save files.","solution":"#!/usr/bin/env python3 import cgi import cgitb cgitb.enable() import os import re import uuid # Define the directory to save uploaded files UPLOAD_DIR = \\"uploads\\" # Helper function to validate an email address def is_valid_email(email): return re.match(r\'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\', email) # Main function to process the form data def main(): print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() # Check if all fields are provided if \\"name\\" not in form or \\"email\\" not in form or \\"submission\\" not in form: print(\\"<h1>Error</h1>\\") print(\\"<p>All fields are required.</p>\\") return name = form.getvalue(\\"name\\") email = form.getvalue(\\"email\\") fileitem = form[\\"submission\\"] # Validate email address if not is_valid_email(email): print(\\"<h1>Error</h1>\\") print(\\"<p>Email address is not valid.</p>\\") return # Ensure file upload is successful if not fileitem.file: print(\\"<h1>Error</h1>\\") print(\\"<p>File upload failed.</p>\\") return # Generate a unique filename and save the file filename = str(uuid.uuid4()) + \\"_\\" + os.path.basename(fileitem.filename) filepath = os.path.join(UPLOAD_DIR, filename) with open(filepath, \'wb\') as f: f.write(fileitem.file.read()) # Display confirmation and form data print(\\"<h1>Form Submission Successful</h1>\\") print(\\"<p>Name: {}</p>\\".format(name)) print(\\"<p>Email: {}</p>\\".format(email)) print(\\"<p>File saved as: {}</p>\\".format(filename)) if __name__ == \\"__main__\\": main()"},{"question":"# Email Package Assessment Question **Objective:** Design and implement a function that parses a raw email message string, modifies specific parts of the email, and generates a new email message string with the modifications. **Problem Statement:** You are given a raw email message as a string. Your task is to implement a function `modify_email(raw_email: str, new_subject: str, new_body: str) -> str` that performs the following steps: 1. Parses the raw email message string into an `EmailMessage` object. 2. Modifies the email\'s subject to the value of `new_subject`. 3. Modifies the email\'s body to the value of `new_body`. 4. Generates a new raw email message string from the modified `EmailMessage` object and returns this string. # Function Signature: ```python def modify_email(raw_email: str, new_subject: str, new_body: str) -> str: pass ``` # Input: - `raw_email`: A string representation of a raw email message. - `new_subject`: A string representing the new subject for the email. - `new_body`: A string representing the new body for the email. # Output: - A string representing the new raw email message with the modified subject and body. # Constraints: - The input raw email message string (`raw_email`) conforms to RFC 5322 and MIME-related RFCs such as RFC 2045, 2046, 2047, 2183, and 2231. # Example: Given the following raw email string: ```plaintext From: sender@example.com To: recipient@example.com Subject: Old Subject Content-Type: text/plain; charset=\\"utf-8\\" This is the old body of the email. ``` And the function call: ```python new_subject = \\"New Subject\\" new_body = \\"This is the new body of the email.\\" new_email = modify_email(raw_email, new_subject, new_body) ``` The `new_email` string should look like: ```plaintext From: sender@example.com To: recipient@example.com Subject: New Subject Content-Type: text/plain; charset=\\"utf-8\\" This is the new body of the email. ``` # Requirements: - Your implementation should handle the parsing and generation of emails using the \\"email\\" package\'s `EmailMessage`, `parser`, and `generator` components. - Ensure that all other parts of the email (e.g., headers such as \'From\', \'To\', and \'Content-Type\') remain unchanged except for the subject and body. - Handle any potential exceptions that might arise from improper formatting or parsing. # Note: - Use the `email.policy.default` policy when parsing and generating the email message.","solution":"from email import policy from email.parser import Parser from email.generator import Generator from io import StringIO def modify_email(raw_email: str, new_subject: str, new_body: str) -> str: # Parse the raw email message into an EmailMessage object email_message = Parser(policy=policy.default).parsestr(raw_email) # Modify the subject email_message.replace_header(\'Subject\', new_subject) # Modify the body email_message.set_content(new_body) # Create a string buffer to hold the new raw email output = StringIO() # Generate the new raw email message Generator(output, policy=policy.default).flatten(email_message) # Return the new raw email message as a string return output.getvalue()"},{"question":"Custom Exception Handling Utilities You are tasked with creating a small utility module in Python that replicates a subset of the exception handling functionalities described in the documentation. You need to implement a class with static methods to achieve the following: 1. **Clear the current exception.** 2. **Print the current exception to standard error and clear it.** The method should handle both general exceptions and, specifically, the `SystemExit` (where it should print a message and raise the exception). 3. **Set a new exception.** The method should accept an exception type and a message, and raise the specified exception with the provided message. 4. **Fetch the current exception details.** The method should return a tuple consisting of the exception type, value, and traceback. 5. **Normalize an exception.** Given an exception type and value, this method should ensure the value is an instance of the type by instantiating it if necessary, and return the normalized exception. Class Definition and Method Requirements ```python import sys import traceback class ExceptionHandler: @staticmethod def clear_exception(): Clear the current exception, if any. pass @staticmethod def print_and_clear_exception(): Print the current exception to standard error and clear the exception state. If the exception is a SystemExit, print a message and exit the program. pass @staticmethod def set_exception(exc_type, message): Raise a new exception of the specified type with the given message. Args: - exc_type: Type of the exception to be raised (e.g., ValueError) - message: Error message to be passed to the exception pass @staticmethod def fetch_exception_details(): Fetch the current exception and return a tuple (exc_type, exc_value, exc_traceback). If no exception is set, return (None, None, None). pass @staticmethod def normalize_exception(exc_type, exc_value): Ensure that exc_value is an instance of exc_type. Args: - exc_type: Type of the exception - exc_value: The value of the exception which will be normalized Returns: - The normalized exception instance pass ``` # Example Usage Here\'s how the functions in the `ExceptionHandler` class might be used: ```python try: # Simulate an exception 1 / 0 except ZeroDivisionError: # Fetch and print current exception details print(ExceptionHandler.fetch_exception_details()) # Print and clear the current exception ExceptionHandler.print_and_clear_exception() try: # Simulate another exception raise ValueError(\\"This is a ValueError\\") except: # Set new exception and then fetch it ExceptionHandler.set_exception(KeyError, \\"This is a KeyError\\") print(ExceptionHandler.fetch_exception_details()) try: # Normalize a generic exception normalized_exc = ExceptionHandler.normalize_exception(ValueError, \\"Normalization example\\") raise normalized_exc except ValueError as ve: print(repr(ve)) # Output should show the normalized exception ``` # Constraints - Do not use any library beyond the Python standard library. - Ensure all methods handle edge cases appropriately, such as no exception currently being set. - Perform robust error checking and handling within the methods. Please implement the `ExceptionHandler` class methods as described above.","solution":"import sys import traceback class ExceptionHandler: @staticmethod def clear_exception(): Clear the current exception, if any. sys.last_type = sys.last_value = sys.last_traceback = None @staticmethod def print_and_clear_exception(): Print the current exception to standard error and clear the exception state. If the exception is a SystemExit, print a message and exit the program. exc_type, exc_value, exc_traceback = sys.exc_info() if exc_type: if exc_type is SystemExit: print(\\"SystemExit encountered. Exiting the program.\\", file=sys.stderr) sys.exit(exc_value) else: traceback.print_exception(exc_type, exc_value, exc_traceback, file=sys.stderr) ExceptionHandler.clear_exception() @staticmethod def set_exception(exc_type, message): Raise a new exception of the specified type with the given message. Args: - exc_type: Type of the exception to be raised (e.g., ValueError) - message: Error message to be passed to the exception raise exc_type(message) @staticmethod def fetch_exception_details(): Fetch the current exception and return a tuple (exc_type, exc_value, exc_traceback). If no exception is set, return (None, None, None). return sys.exc_info() @staticmethod def normalize_exception(exc_type, exc_value): Ensure that exc_value is an instance of exc_type. Args: - exc_type: Type of the exception - exc_value: The value of the exception which will be normalized Returns: - The normalized exception instance if isinstance(exc_value, exc_type): return exc_value return exc_type(exc_value)"},{"question":"**Coding Assessment Question:** Using the `imaplib` module, write a Python function `get_unread_emails` that connects to an IMAP4 server, logs in using provided credentials, selects the INBOX, searches for unread emails, fetches their message IDs and subjects, and returns a dictionary where the keys are message IDs and the values are subjects of the unread emails. Ensure that the connection to the server is properly closed after the operation. # Specifications: - **Function Signature:** ```python def get_unread_emails(server: str, username: str, password: str) -> dict: ``` - **Input:** - `server`: A string representing the IMAP server address. - `username`: A string representing the user\'s email address. - `password`: A string representing the user\'s email password. - **Output:** - Returns a dictionary where keys are message IDs and values are the subjects of the unread emails. If there are no unread emails, an empty dictionary should be returned. # Constraints: - You must use the `imaplib.IMAP4` class for the connection. - Handle any potential exceptions that may be raised during the connection or fetching process gracefully. - Ensure that the connection to the IMAP server is closed whether an error occurs or not. - You are not allowed to use any third-party libraries other than `imaplib` and standard Python libraries. # Example Usage: ```python server = \\"imap.example.com\\" username = \\"user@example.com\\" password = \\"password123\\" emails = get_unread_emails(server, username, password) print(emails) ``` If successful, the output could be: ```python { \'1\': \'Welcome to your new account\', \'2\': \'Important update on your account\', } ``` # Additional Notes: - Pay attention to the format returned by the `search` and `fetch` commands. - The email subject is usually part of the `HEADER` or `BODY[HEADER.FIELDS (SUBJECT)]` fetch response. - Make sure to handle server responses and possible encoding issues properly while fetching the email subjects.","solution":"import imaplib import email from email.header import decode_header def get_unread_emails(server: str, username: str, password: str) -> dict: Connects to an IMAP server, logs in using provided credentials, selects the INBOX, searches for unread emails, fetches their message IDs and subjects, and returns a dictionary where the keys are message IDs and the values are subjects of the unread emails. unread_emails = {} try: # Connect to the server mail = imaplib.IMAP4(server) # Login to any given username and password mail.login(username, password) # Select the mailbox we want to use; in this case, the inbox. mail.select(\\"inbox\\") # Search for unread emails status, messages = mail.search(None, \'UNSEEN\') if status == \\"OK\\": # Split the message string into individual message IDs for num in messages[0].split(): # Fetch the email message by ID status, msg_data = mail.fetch(num, \\"(BODY[HEADER.FIELDS (SUBJECT)])\\") if status == \\"OK\\": # Get the message subject for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): # Decode bytes to string subject = subject.decode(encoding if encoding else \\"utf-8\\") # Add the message ID and subject to the dictionary unread_emails[num.decode()] = subject # Close the connection and logout mail.close() mail.logout() except Exception as e: # Handle any errors that occur print(f\\"An error occurred: {e}\\") return unread_emails"},{"question":"# Advanced Configuration File Processing Objective You are tasked with creating a python function that reads a given `.ini` configuration file, modifies certain values based on specified criteria, and then writes the new configuration back to disk. This assessment will test your understanding of reading, modifying, and managing configuration files using Python’s `configparser` module. Function Signature ```python def process_config_file(file_path: str, modifications: dict) -> None: Parameters: - file_path (str): Path to the input .ini configuration file. - modifications (dict): Changes to be made in the format: {\\"section1\\": {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"}, \\"section2\\": {\\"key3\\": \\"value3\\"}} Returns: - None: The function should save the modified configuration back to file_path. pass ``` Input - A string `file_path` representing the path to the `.ini` configuration file. - A dictionary `modifications` where keys are section names and values are dictionaries containing key-value pairs to be modified or added within those sections. Output - The function does not return anything but it should modify the `.ini` configuration file in place based on the provided modifications. Example Usage Suppose you have an `app_config.ini` file with the following contents: ```ini [General] appname = SampleApp version = 1.0 debug = false [UserSettings] theme = light timezone = UTC ``` You call the function with: ```python process_config_file(\\"app_config.ini\\", { \\"General\\": {\\"debug\\": \\"true\\", \\"version\\": \\"2.0\\"}, \\"UserSettings\\": {\\"theme\\": \\"dark\\"} }) ``` After execution, the contents of `app_config.ini` should be modified to: ```ini [General] appname = SampleApp version = 2.0 debug = true [UserSettings] theme = dark timezone = UTC ``` Constraints - Assume that all provided sections and keys are valid and exist within the file. - Use appropriate exception handling to manage any read/write errors. Requirements - Use the `configparser` module to handle reading and writing of the configuration file. - Ensure the updated configuration maintains a valid `.ini` file structure. Good luck, and may your code be bug-free!","solution":"import configparser def process_config_file(file_path: str, modifications: dict) -> None: Parameters: - file_path (str): Path to the input .ini configuration file. - modifications (dict): Changes to be made in the format: {\\"section1\\": {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"}, \\"section2\\": {\\"key3\\": \\"value3\\"}} Returns: - None: The function should save the modified configuration back to file_path. config = configparser.ConfigParser() # Read the existing configuration file config.read(file_path) # Apply modifications for section, changes in modifications.items(): if section in config: for key, value in changes.items(): config.set(section, key, value) else: config.add_section(section) for key, value in changes.items(): config.set(section, key, value) # Write the modified configuration back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"**Objective:** Implement a Python class `InteractiveExecutor` that simulates a simple interactive Python code execution environment. The class should utilize the functionalities described in the `codeop` module to compile and execute Python code, handling and remembering `__future__` statements. **Instructions:** 1. Create a class `InteractiveExecutor` with an initializer `__init__` that sets up an instance of `codeop.CommandCompiler`. 2. Implement a method `execute(self, source)` that: - Takes a string `source` as input. - Uses the `CommandCompiler` instance to compile the `source`. - If the compilation is successful, executes the code and returns the output or result. - Handles any exceptions that occur during compilation or execution and returns a relevant error message. 3. Implement a method `interactive_loop(self)` that: - Continuously reads user input. - Utilizes the `execute` method to compile and execute each line of input. - Prints the result or error for each input line. - Stops on user input \'exit\'. **Constraints:** - Limit the input length of each line to 1000 characters. - The environment should properly remember and apply `__future__` statements across multiple lines of input. **Example Usage:** ```python # Initialize the interactive executor executor = InteractiveExecutor() # Execute some code print(executor.execute(\\"x = 10\\")) # Outputs: None print(executor.execute(\\"print(x)\\")) # Outputs: 10 # Start interactive loop (this will run indefinitely until \'exit\' is input) # executor.interactive_loop() ``` **Expected Output:** The class should handle various cases of valid and invalid input, and properly carry forward `__future__` statements across different inputs. Make sure to handle exceptions and provide clear output or error messages. **Performance Requirement:** The solution should handle typical user interaction efficiently without significant delays.","solution":"import codeop class InteractiveExecutor: def __init__(self): self.compiler = codeop.CommandCompiler() self.locals = {} def execute(self, source): if len(source) > 1000: return \\"Error: Input too long. The input length should not exceed 1000 characters.\\" try: compiled_code = self.compiler(source, \\"exec\\") except Exception as e: return f\\"Compile Error: {e}\\" if compiled_code is None: return \\"Incomplete statement.\\" try: exec(compiled_code, self.locals) return self.locals[\\"_\\"] if \\"_\\" in self.locals else None except Exception as e: return f\\"Execution Error: {e}\\" def interactive_loop(self): while True: try: source = input(\\">>> \\") if source.strip().lower() == \\"exit\\": break result = self.execute(source) if result is not None: print(result) except EOFError: break # Example usage # executor = InteractiveExecutor() # print(executor.execute(\\"x = 10\\")) # Outputs: None # print(executor.execute(\\"print(x)\\")) # Outputs: 10 # executor.interactive_loop() # Starts interactive loop"},{"question":"**Coding Assessment Question** # Objective Implement a class-based data structure in Python that utilizes advanced class features like inheritance, methods, private variables (name mangling), iterators, and generator expressions. Ensure a deep understanding of these concepts as well as their correct application. # Problem Statement Create a class `Classroom` to manage a collection of `Student` objects. Each `Student` should have attributes for their name and grades as well as methods to compute and retrieve information. You should also implement an iterator for the Classroom to allow iteration over Student objects and provide a generator method to filter students based on their grades. # Requirements 1. **Student Class**: - **Attributes**: - `name` (string) - `grades` (list of floats) - **Methods**: - `__init__(self, name, grades)`: Initialize a Student with a name and a list of grades. - `average_grade(self)`: Calculate and return the average grade of the student. - `__str__(self)`: Return a string representation of the student. 2. **Classroom Class**: - **Attributes**: - `_students` (list of Student objects) [Private Variable] - **Methods**: - `__init__(self)`: Initialize an empty Classroom. - `add_student(self, student)`: Add a Student to the classroom. - `__iter__(self)`: Implement an iterator for the Classroom that iterates over the students. - `__next__(self)`: Returns the next student in the iteration. - `top_students(self, min_avg)`: Generator method that yields students with an average grade above `min_avg`. - `__str__(self)`: Return a string representation of the classroom and all its students. # Constraints - The `name` attribute should be a non-empty string. - The `grades` list should only contain numbers between 0 and 100. - The `top_students` method should use a generator expression to filter students. # Example Usage ```python # Creating Students student1 = Student(\\"Alice\\", [90, 80, 85]) student2 = Student(\\"Bob\\", [70, 75, 65]) student3 = Student(\\"Charlie\\", [92, 88, 91]) # Creating Classroom and adding students classroom = Classroom() classroom.add_student(student1) classroom.add_student(student2) classroom.add_student(student3) # Printing Classroom print(classroom) # Iterating over Classroom for student in classroom: print(student) # Filtering top students with a minimum average of 85 top_students = classroom.top_students(85) for top_student in top_students: print(top_student) ``` # Expected Output ``` Classroom: Student: Alice, Average Grade: 85.0 Student: Bob, Average Grade: 70.0 Student: Charlie, Average Grade: 90.33 Student: Alice, Average Grade: 85.0 Student: Bob, Average Grade: 70.0 Student: Charlie, Average Grade: 90.33 Student: Alice, Average Grade: 85.0 Student: Charlie, Average Grade: 90.33 ``` # Performance Requirements - The iteration and generator methods should be efficient in both time and space complexity. - The implementation should handle a reasonable number of students (e.g., up to 10,000) efficiently. Implement the `Student` and `Classroom` classes with the specified methods and attributes to pass the example usage and output provided.","solution":"class Student: def __init__(self, name, grades): if not name: raise ValueError(\\"Name cannot be empty\\") if not all(0 <= grade <= 100 for grade in grades): raise ValueError(\\"Grades must be between 0 and 100\\") self.name = name self.grades = grades def average_grade(self): return sum(self.grades) / len(self.grades) if self.grades else 0 def __str__(self): return f\\"Student: {self.name}, Average Grade: {self.average_grade():.2f}\\" class Classroom: def __init__(self): self._students = [] self._current_index = 0 def add_student(self, student): self._students.append(student) def __iter__(self): self._current_index = 0 return self def __next__(self): if self._current_index < len(self._students): student = self._students[self._current_index] self._current_index += 1 return student else: raise StopIteration def top_students(self, min_avg): return (student for student in self._students if student.average_grade() >= min_avg) def __str__(self): return \\"Classroom:n\\" + \\"n\\".join(str(student) for student in self._students)"},{"question":"You are provided with a dataset containing features with unknown underlying relationships. Your task is to analyze this dataset and estimate its covariance matrix using different techniques from the `sklearn.covariance` module. Compare and contrast the results obtained from these techniques. # Input - A CSV file `data.csv` with numerical features and no missing values. # Output - Print the estimated covariance matrices using the following techniques: - Empirical Covariance - Shrunk Covariance with a specified shrinkage parameter - Ledoit-Wolf Shrinkage - Oracle Approximating Shrinkage - Sparse Inverse Covariance using GraphicalLasso with cross-validation to determine the best alpha. Your script should also output an evaluation of the results, discussing the differences and implications of each method on the dataset. # Constraints - You must use the appropriate classes and methods from the `sklearn.covariance` module. - Ensure that the data is properly centered or use appropriate parameters if not. - The file `data.csv` should be available in the working directory. # Example ```python import pandas as pd from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLassoCV) # Load the dataset data = pd.read_csv(\'data.csv\') # Ensure the data is centered data_centered = data - data.mean() # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data_centered) print(\\"Empirical Covariance: n\\", emp_cov.covariance_) # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(data_centered) print(\\"Shrunk Covariance: n\\", shrunk_cov.covariance_) # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(data_centered) print(\\"Ledoit-Wolf Covariance: n\\", lw_cov.covariance_) # Oracle Approximating Shrinkage oas_cov = OAS().fit(data_centered) print(\\"OAS Covariance: n\\", oas_cov.covariance_) # Sparse Inverse Covariance graph_lasso = GraphicalLassoCV().fit(data_centered) print(\\"Graphical Lasso Covariance: n\\", graph_lasso.covariance_) # Evaluation: Discuss the results print(\\"Evaluation of covariance estimation techniques...\\") # Include your detailed comparison here ``` Your evaluation should include: - A discussion on the differences in the estimated covariance matrices. - Which methods seem more stable and why. - Any notable patterns or insights you derive from the results. Note Ensure to have `pandas`, `numpy`, and `scikit-learn` installed in your Python environment.","solution":"import pandas as pd from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLassoCV def estimate_covariance(data_path): Estimate the covariance matrix of the dataset using various techniques from sklearn.covariance. Parameters: data_path (str): The file path of the CSV file containing the dataset. Returns: dict: A dictionary containing the estimated covariance matrices. # Load the dataset data = pd.read_csv(data_path) # Ensure the data is centered data_centered = data - data.mean() covariance_matrices = {} # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data_centered) covariance_matrices[\\"Empirical\\"] = emp_cov.covariance_ # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(data_centered) covariance_matrices[\\"Shrunk\\"] = shrunk_cov.covariance_ # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(data_centered) covariance_matrices[\\"Ledoit-Wolf\\"] = lw_cov.covariance_ # Oracle Approximating Shrinkage oas_cov = OAS().fit(data_centered) covariance_matrices[\\"OAS\\"] = oas_cov.covariance_ # Sparse Inverse Covariance graph_lasso = GraphicalLassoCV().fit(data_centered) covariance_matrices[\\"GraphicalLassoCV\\"] = graph_lasso.covariance_ return covariance_matrices"},{"question":"# Python FTP Client Implementation Objective: Your task is to implement a Python function that connects to an FTP server, navigates to a specified directory, lists all the files in that directory, downloads a specified file in binary mode to a local destination, and logs out properly. You must handle potential errors appropriately. Requirements: 1. Implement the function `ftp_operations(host, user, passwd, directory, filename, local_destination)`. 2. The function should: - Connect to the FTP server using the provided `host`, `user`, and `passwd`. - Navigate to the directory specified by the `directory` parameter. - List all files in the specified directory, printing their names. - Download the specified file (indicated by the `filename` parameter) in binary mode to the `local_destination`. - Log out properly using the `quit` method. 3. Handle the following exceptions and print appropriate error messages: - `ftplib.error_perm` for permission-related errors. - `ftplib.error_reply` for unexpected replies from the server. - `ftplib.error_temp` for temporary errors. - `ftplib.error_proto` for protocol errors. - Any other exceptions, such as network issues. Input: 1. `host` (str): The FTP server hostname. 2. `user` (str): The username for FTP login. 3. `passwd` (str): The password for FTP login. 4. `directory` (str): The directory to navigate to on the FTP server. 5. `filename` (str): The name of the file to download. 6. `local_destination` (str): The local path where the downloaded file should be saved. Output: - Print the names of all files in the specified directory. - Save the specified file to the local destination. - Print appropriate error messages if any exceptions occur. Function Signature: ```python def ftp_operations(host: str, user: str, passwd: str, directory: str, filename: str, local_destination: str) -> None: pass ``` Example: Here\'s an example of how the function is expected to work: ```python host = \\"ftp.dlptest.com\\" user = \\"dlpuser\\" passwd = \\"rNrKYTX9g7z3RgJRmxWuGHbeu\\" directory = \\"/test\\" filename = \\"testfile.txt\\" local_destination = \\"./downloaded_testfile.txt\\" ftp_operations(host, user, passwd, directory, filename, local_destination) ``` This function should: 1. Connect to `ftp.dlptest.com` using the provided credentials. 2. Navigate to the `/test` directory. 3. List all files in the `/test` directory. 4. Download `testfile.txt` to `./downloaded_testfile.txt`. 5. Log out properly and handle any exceptions during the process. --- Notes: - Make sure to test your function with a real FTP server or a local FTP setup. - Ensure proper cleanup by closing connections appropriately even if errors occur. Happy coding!","solution":"import ftplib def ftp_operations(host, user, passwd, directory, filename, local_destination): Connects to an FTP server, navigates to a directory, lists all files in the directory, downloads a specified file, and logs out properly. Handles potential errors appropriately. try: # Establish connection to the FTP server ftp = ftplib.FTP(host) ftp.login(user=user, passwd=passwd) # Navigate to the specified directory ftp.cwd(directory) # List all files in the directory file_list = ftp.nlst() for file in file_list: print(file) # Download the specified file in binary mode with open(local_destination, \'wb\') as local_file: ftp.retrbinary(f\\"RETR {filename}\\", local_file.write) # Log out properly ftp.quit() except ftplib.error_perm as e: print(f\\"Permission error: {e}\\") except ftplib.error_reply as e: print(f\\"Unexpected reply from server: {e}\\") except ftplib.error_temp as e: print(f\\"Temporary error: {e}\\") except ftplib.error_proto as e: print(f\\"Protocol error: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"**Problem Statement:** You are tasked with creating a Python extension type entirely within Python. This extension type will mimic some characteristics and functionalities of the C extensions described in the documentation. # Objective: Implement a `CustomString` class that: 1. **Inherits from the built-in `str` type**. 2. **Includes additional attributes and methods**, simulating custom behaviors. 3. **Supports memory management-like mechanisms** (e.g., custom garbage collection-worthy behavior). 4. **Is subclassable by other types**. # Specifications: 1. **Attributes**: - `prefix`: A string attribute with a default value, which is prepended to the original string. - `suffix`: A string attribute with a default value, which is appended to the original string. - `metadata`: A dictionary attribute to store arbitrary metadata. 2. **Methods**: - `custom_length()`: Returns the length of the string including the prefix and suffix. - **Special Methods**: - `__new__(cls, content, prefix=\\"\\", suffix=\\"\\", metadata=None)`: Initializes the string content along with specified prefix, suffix, and metadata. - `__init__(self, content, prefix=\\"\\", suffix=\\"\\", metadata=None)`: Initializes the attributes. - `__len__(self)`: Returns the customized length of the string. 3. **Constraints**: - `prefix` and `suffix` should always be strings. - `metadata` should always be a dictionary or None (which defaults to an empty dictionary). # Performance: Ensure the implementation is efficient and correctly handles memory management concepts, even though it\'s within a Python context. # Input Format: - The `CustomString` object will be initialized with a string content, optional prefix, optional suffix, and optional metadata. # Output Format: - The class methods and instance methods should return appropriate values as described. # Example: ```python cust_str = CustomString(\\"world\\", prefix=\\"Hello, \\", suffix=\\"!\\", metadata={\\"language\\": \\"English\\"}) print(cust_str) # Output: Hello, world! print(len(cust_str)) # Output: 13 (length of \\"Hello, world!\\") print(cust_str.custom_length()) # Output: 13 print(cust_str.metadata[\\"language\\"]) # Output: English ``` You must implement this class to handle the described functionality effectively.","solution":"class CustomString(str): def __new__(cls, content, prefix=\\"\\", suffix=\\"\\", metadata=None): # Create a new instance of CustomString return super(CustomString, cls).__new__(cls, content) def __init__(self, content, prefix=\\"\\", suffix=\\"\\", metadata=None): self.prefix = prefix self.suffix = suffix # Ensure metadata is a dictionary if provided, else default to an empty dictionary self.metadata = metadata if metadata is not None else {} def __len__(self): # Return the length of the customized representation of the string return len(self.prefix + super(CustomString, self).__str__() + self.suffix) def custom_length(self): # Return the length of the string including prefix and suffix return len(self) def __str__(self): # Return the complete string with prefix and suffix return self.prefix + super(CustomString, self).__str__() + self.suffix"},{"question":"**Objective:** This question aims to assess your understanding of seaborn\'s theme configuration and display settings using the `seaborn.objects` module. You will write a function to generate a specific plot using customized theme and display settings. **Problem Statement:** You are given a dataset containing information about different species of flowers, including their petal lengths, petal widths, sepal lengths, and sepal widths. Your task is to create a seaborn scatter plot that visualizes the relationship between petal length and petal width, color-coded by species. Additionally, you need to customize the appearance and display format of this plot based on specific theme settings. **Input:** - A pandas DataFrame `df` with the following columns: - `species`: The species of the flower (categorical). - `petal_length`: The length of the flower\'s petal (numerical). - `petal_width`: The width of the flower\'s petal (numerical). **Output:** - Save the generated plot as an SVG file named `custom_flower_plot.svg`. **Constraints:** 1. Set the background color of the axes to white using seaborn\'s theme configuration. 2. Apply the `whitegrid` style to the plot. 3. Set the display format to SVG with HiDPI scaling disabled and a scaling factor of 0.7. 4. The scatter plot should use different colors to distinguish between species. 5. The plot should include a legend indicating the species. **Function Signature:** ```python def create_custom_flower_plot(df: pd.DataFrame) -> None: pass ``` **Example Usage:** ```python import pandas as pd data = { \'species\': [\'setosa\', \'setosa\', \'virginica\', \'virginica\', \'versicolor\', \'versicolor\'], \'petal_length\': [1.4, 1.3, 5.5, 5.8, 4.5, 4.7], \'petal_width\': [0.2, 0.2, 2.1, 2.2, 1.3, 1.4] } df = pd.DataFrame(data) create_custom_flower_plot(df) ``` After executing the function, the `custom_flower_plot.svg` file should be created according to the specified theme and display settings. **Note:** Ensure you have the necessary libraries installed (pandas, seaborn, matplotlib).","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_flower_plot(df: pd.DataFrame) -> None: Create a seaborn scatter plot visualizing the relationship between petal length and petal width, color-coded by species and customize its appearance. :param df: A pandas DataFrame with columns \'species\', \'petal_length\', and \'petal_width\'. # Set seaborn theme to whitegrid and set the axes background to white sns.set_theme(style=\\"whitegrid\\", rc={\\"axes.facecolor\\": \\"white\\"}) # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=df, x=\\"petal_length\\", y=\\"petal_width\\", hue=\\"species\\", palette=\\"Set1\\", s=100 ) # Move the legend outside the plot scatter_plot.legend(loc=\'center left\', bbox_to_anchor=(1, 0.5), title=\\"Species\\") # Save the plot as an SVG file with the required settings plt.savefig(\\"custom_flower_plot.svg\\", format=\'svg\', bbox_inches=\'tight\') plt.close()"},{"question":"# Email and MIME Handling: Function Implementation **Objective**: In this task, you will implement a function to create a MIME email message, parse its content, and extract specific information from it using various submodules of the `email` package. **Scenario**: You need to create a MIME email from scratch, send it to a mailbox, parse the email to extract specific headers and message content, and finally decode any base64 encoded attachments. **Function Specifications**: 1. **create_mime_email(subject: str, sender: str, recipient: str, body: str, attachment: Optional[Tuple[str, str]]) -> str**: - This function should create an email with the given subject, sender, recipient, and body content. It should optionally include an attachment. - The attachment should be a tuple where the first element is the filename and the second element is the base64 encoded content of the file. - The function should return the email as a string. 2. **parse_email(email_str: str) -> Dict[str, Any]**: - This function should parse the email string and extract the `Subject`, `From`, `To`, and `Body` of the email. - If there are any attachments, decode any base64 encoded content. - The function should return a dictionary with the following keys: `subject`, `from`, `to`, `body`, and `attachments` (if any). **Input**: - For `create_mime_email`: - `subject`: A string representing the subject of the email. - `sender`: A string representing the email address of the sender. - `recipient`: A string representing the email address of the recipient. - `body`: A string representing the body of the email. - `attachment`: Optional[Tuple[str, str]], optional tuple containing filename and base64 encoded content. - For `parse_email`: - `email_str`: A string representation of the email. **Output**: - For `create_mime_email`: A string representation of the MIME email. - For `parse_email`: A dictionary with keys as `subject`, `from`, `to`, `body`, and `attachments`. **Constraints**: - The `subject`, `sender`, `recipient`, and `body` will be non-empty strings. - The `attachment` tuple, if provided, the first element should be a valid filename and the second element a valid base64 encoded string. # Example: ```python # Example Usage of create_mime_email mime_email = create_mime_email( subject=\\"Project Update\\", sender=\\"alice@example.com\\", recipient=\\"bob@example.com\\", body=\\"Hello Bob, Please find the latest project update attached.\\", attachment=(\\"update.pdf\\", \\"base64_encoded_content_here\\") ) # Example Usage of parse_email parsed_output = parse_email(mime_email) # parsed_output should give { \\"subject\\": \\"Project Update\\", \\"from\\": \\"alice@example.com\\", \\"to\\": \\"bob@example.com\\", \\"body\\": \\"Hello Bob, Please find the latest project update attached.\\", \\"attachments\\": [ { \\"filename\\": \\"update.pdf\\", \\"content\\": \\"decoded_content_here\\" } ] } ``` Your implementation should ensure proper creation and parsing of MIME messages and accurately handle and decode any base64 attachments.","solution":"import base64 from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders, policy from email.parser import Parser from typing import Any, Dict, Optional, Tuple def create_mime_email(subject: str, sender: str, recipient: str, body: str, attachment: Optional[Tuple[str, str]] = None) -> str: Create a MIME email with the specified subject, sender, recipient, and body. Optionally include an attachment with filename and base64 content. # Create the email container msg = MIMEMultipart() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient # Attach the body of the email msg.attach(MIMEText(body, \'plain\')) # Attach the attachment if provided if attachment: filename, b64_content = attachment part = MIMEBase(\'application\', \'octet-stream\') part.set_payload(base64.b64decode(b64_content)) encoders.encode_base64(part) part.add_header( \'Content-Disposition\', f\'attachment; filename= {filename}\', ) msg.attach(part) # Return the email as a string return msg.as_string() def parse_email(email_str: str) -> Dict[str, Any]: Parse the email string and extract the subject, from, to, body, and attachments. parsed_msg = Parser(policy=policy.default).parsestr(email_str) result = { \'subject\': parsed_msg[\'Subject\'], \'from\': parsed_msg[\'From\'], \'to\': parsed_msg[\'To\'], \'body\': \'\', \'attachments\': [] } if parsed_msg.is_multipart(): for part in parsed_msg.iter_parts(): if part.get_content_type() == \'text/plain\': result[\'body\'] = part.get_payload(decode=True).decode(part.get_content_charset()) elif part.get(\'Content-Disposition\') and \'attachment\' in part.get(\'Content-Disposition\'): filename = part.get_filename() content = base64.b64encode(part.get_payload(decode=True)).decode() result[\'attachments\'].append({ \'filename\': filename, \'content\': content }) else: result[\'body\'] = parsed_msg.get_payload(decode=True).decode(parsed_msg.get_content_charset()) return result"},{"question":"Objective: Write a Python function that takes multiple Python functions as input, extracts their code objects, modifies these code objects, and returns a list of new functions created from the modified code objects. Specifically, you need to create a wrapper code object that adds logging functionality around each given function. Function Signature: ```python def wrap_functions_with_logging(*functions: Callable) -> List[Callable]: pass ``` Input: - `*functions`: A variable number of Python callable objects (functions). Output: - A list of callable objects (functions) created from the modified code objects of the input functions. Requirements: 1. **Extract the code objects** from each input function. 2. **Create new code objects** that include logging functionality. This means: - Before executing the original function\'s code, log the function name and the arguments it was called with. - After executing the original function\'s code, log the returned value. 3. **Return new functions** created from these modified code objects. 4. The new functions should mimic the argument signature and behavior of the original functions but include the added logging functionality. Constraints: - You may assume the input functions do not have positional-only or keyword-only arguments. - Focus on handling only functions that have simple variable and positional arguments (no advanced argument types required). Example Usage: ```python def foo(a, b): return a + b def bar(x): return x * 2 wrapped_functions = wrap_functions_with_logging(foo, bar) wrapped_foo, wrapped_bar = wrapped_functions # Expected to log the function call and result result_foo = wrapped_foo(1, 2) # Logs: \\"Calling foo with args: (1, 2)\\" and \\"foo returned: 3\\" result_bar = wrapped_bar(3) # Logs: \\"Calling bar with args: (3)\\" and \\"bar returned: 6\\" ``` Hints: - Use the `inspect` module to access the code object of a function. - Use `types.CodeType` to create a new code object. - Consider the structure and modification of bytecode if necessary for logging. Note: Ensure that the new functions retain the same functionality as the original functions but with additional logging. You may need to handle the code object metadata accurately to achieve this.","solution":"import inspect import types from typing import Callable, List def wrap_functions_with_logging(*functions: Callable) -> List[Callable]: def make_logging_function(func): def logged_function(*args, **kwargs): func_name = func.__name__ print(f\\"Calling {func_name} with args: {args}, kwargs: {kwargs}\\") result = func(*args, **kwargs) print(f\\"{func_name} returned: {result}\\") return result return logged_function wrapped_functions = [make_logging_function(func) for func in functions] return wrapped_functions"},{"question":"Objective: To assess your understanding of Python\'s built-in functions and your ability to handle potential naming conflicts by utilizing the `builtins` module effectively. Problem Statement: You are tasked with creating a custom utility that reads a file and applies multiple transformations to its content using built-in identifiers. To avoid any naming conflicts with your custom functions, you will use the `builtins` module. 1. Implement a function `transformed_open(path: str, transformations: list) -> str` that: - Reads the file located at `path` (assume the file exists and is readable). - Applies a list of transformations/functions to the content of the file sequentially. Each transformation is a string corresponding to a built-in function\'s name (e.g., `upper`, `lower`, `sorted`, etc.). - Returns the final transformed content as a string. 2. You should handle situations where the file might be read using custom transformations, making sure to use the `builtins` module whenever necessary. Expected Input and Output Formats: - Input: - `path` (str): The path to the file to be read. - `transformations` (list of str): A list of transformation function names to be applied sequentially (e.g., `[\'lower\', \'strip\']`). - Output: - A string representing the final transformed content of the file. Constraints and Limitations: - The file should contain text data. - Each transformation function name provided in the `transformations` list will correspond to an existing built-in function. - Only transformations applicable to string data should be considered. - If a transformation does not modify the content in a meaningful way (e.g., `min` or `len`), it should be skipped. Performance Requirements: The function should be optimized for readability and maintainability. Handle file operations and string transformations efficiently. Example: ``` # Given file content at \'sample.txt\': # Hello World # Python is great. path = \'sample.txt\' transformations = [\'lower\', \'replace\'] # The replace transformation will replace spaces with hyphens # Output should be: # \'hello-worldnpython-is-great.\' ``` ```python import builtins def transformed_open(path: str, transformations: list) -> str: # Your implementation here # Testing example path = \'sample.txt\' transformations = [\'lower\', \'replace\'] print(transformed_open(path, transformations)) ``` Note: - You may assume the transformations are always applicable to strings. - Utilize the `builtins` module to avoid any naming conflicts when calling built-in functions.","solution":"import builtins def transformed_open(path: str, transformations: list) -> str: with open(path, \'r\') as file: content = file.read() for transformation in transformations: # Skip transformations that do not modify string content if transformation not in dir(str): continue # Get the built-in function transform_func = getattr(builtins.str, transformation, None) if transformation == \'replace\': content = content.replace(\' \', \'-\') continue if transform_func: content = transform_func(content) return content"},{"question":"**XML Parsing with xml.sax** You are tasked with writing a Python function to parse a given XML string and extract specific information using the `xml.sax` package. This exercise is intended to test your understanding of SAX-based parsing and exceptional handling in Python. # Problem Statement Write a function `extract_titles_from_xml(xml_string: str, output_file: str) -> None` that: - Parses the provided XML string. - Extracts all titles within `<title>` tags. - Writes each extracted title to the specified output file, one per line. # Specifications: 1. **Input**: - `xml_string`: A string containing valid XML data. - `output_file`: The path to the output file where the titles will be written. 2. **Output**: - There is no return value. The extracted titles should be written to `output_file`. 3. **Constraints**: - Handle at least the elements specified (assume well-formed XML). - Use `xml.sax` to perform the parsing. - Implement proper handling for common SAX exceptions such as `SAXParseException`. 4. **Performance**: - Efficiently handle XML strings up to 10 MB in size. # Example: Given this sample XML string: ```xml <?xml version=\\"1.0\\"?> <library> <book> <title>Python Programming</title> <author>John Doe</author> </book> <book> <title>Advanced Python</title> <author>Jane Doe</author> </book> </library> ``` Running `extract_titles_from_xml(xml_data, \\"output.txt\\")` should create `output.txt` with: ``` Python Programming Advanced Python ``` # Additional Information: - Utilize `xml.sax.ContentHandler` to handle SAX events. - Implement error handling conforming to SAX standards. # Hints: - Read the documentation on `xml.sax` package for necessary imports and method implementations. - Ensure correct handling of the start, end, and character events in SAX parsing.","solution":"import xml.sax class TitleHandler(xml.sax.ContentHandler): def __init__(self): self.is_title = False self.titles = [] def startElement(self, name, attrs): if name == \\"title\\": self.is_title = True def endElement(self, name): if name == \\"title\\": self.is_title = False def characters(self, content): if self.is_title: self.titles.append(content) def extract_titles_from_xml(xml_string: str, output_file: str) -> None: handler = TitleHandler() try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXParseException as e: print(f\\"Error parsing XML: {e}\\") return with open(output_file, \\"w\\") as file: for title in handler.titles: file.write(title + \\"n\\")"},{"question":"**Custom Tensor-like Class with `__torch_function__` Protocol** You have been assigned to implement a custom tensor-like class in PyTorch that integrates seamlessly with the PyTorch framework using the `__torch_function__` protocol. Your task is to: 1. Implement a custom class `MyTensor` that mimics the behavior of typical PyTorch tensors but with a simple added functionality. For example, your custom tensor should always return the name of the operation performed on it. 2. Use at least three of the functions provided in the `torch.overrides` module to: - Determine which functions can be overridden. - Handle custom operations adhering to `__torch_function__`. - Ensure your class is recognized as tensor-like by PyTorch. **Input Requirements:** - Your `MyTensor` class should accept an initial value that can be either an integer, float, or a list of these values. **Output Requirements:** - When performing any basic PyTorch operation on an instance of `MyTensor`, it should return a tuple containing the result of the operation followed by the name of the operation. **Constraints:** - You are not allowed to use any external libraries other than PyTorch. - You must implement the functionality using at least three of the functions from the `torch.overrides` module to demonstrate understanding. **Performance Requirements:** - Ensure that your code is efficient and adheres to PyTorch\'s standards for operations. **Example:** ```python import torch from torch.overrides import has_torch_function, handle_torch_function, is_tensor_like class MyTensor: def __init__(self, data): # Initialize your tensor-like object with data @classmethod def __torch_function__(cls, func, types, args=(), kwargs=None): # Implement the custom logic to handle PyTorch functions pass # Implement additional methods and functionalities as needed # Example usage t1 = MyTensor([1, 2, 3]) t2 = MyTensor([4, 5, 6]) result = t1 + t2 # Should return (tensor([5, 7, 9]), \'add\') ``` Design and implement the `MyTensor` class that follows the above specifications. Include comments to explain the logic and usage of `torch.overrides` functions.","solution":"import torch from torch.overrides import has_torch_function, handle_torch_function, is_tensor_like class MyTensor: def __init__(self, data): if isinstance(data, (int, float)): self.data = torch.tensor(data) elif isinstance(data, list): self.data = torch.tensor(data, dtype=torch.float32) else: raise TypeError(\\"Unsupported data type for MyTensor\\") @classmethod def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} # Unwrapping the MyTensor instances to access their data args = tuple(a.data if isinstance(a, cls) else a for a in args) # Perform the function result = func(*args, **kwargs) # Return the result and the name of the operation return result, func.__name__ def __add__(self, other): if has_torch_function((self, other)): return handle_torch_function(torch.add, (self, other), self, other) return self.data + other.data def __repr__(self): return f\\"MyTensor({self.data.tolist()})\\" # Example Usage: # t1 = MyTensor([1, 2, 3]) # t2 = MyTensor([4, 5, 6]) # result = t1 + t2 # Should return (tensor([5, 7, 9]), \'add\')"},{"question":"Objective: Demonstrate your understanding of Python generator expressions, comprehensions (list, set, and dictionary), and asynchronous generators by implementing the following functions. Problem Statement: You are required to write a Python function `process_data` that accepts a list of integers and performs the following operations: 1. Generate a list of squares of all even numbers using list comprehension. 2. Generate a set of cubes of all odd numbers using set comprehension. 3. Create a dictionary where keys are numbers from the input and values are their factorial, using dictionary comprehension. 4. Asynchronously iterate through the dictionary and asynchronously sum the factorials of all numbers greater than a given value `threshold`. Function Signature: ```python import asyncio from math import factorial def process_data(data: list[int], threshold: int) -> tuple[list[int], set[int], dict[int, int], int]: pass # (You can also define any other helper functions as needed) ``` Input: - `data` (List[int]): A list of integers. - `threshold` (int): A threshold value to filter the numbers in the dictionary. Output: - A tuple containing: - A list of squares of all even numbers. - A set of cubes of all odd numbers. - A dictionary where keys are numbers from the input list and values are their factorials. - An integer representing the sum of factorials of all numbers greater than `threshold`. Example: ```python data = [1, 2, 3, 4, 5] threshold = 2 result = process_data(data, threshold) # Output should be: # ( # [4, 16], # List of squares of even numbers [2*2, 4*4] # {1, 27, 125}, # Set of cubes of odd numbers [1*1*1, 3*3*3, 5*5*5] # {1: 1, 2: 2, 3: 6, 4: 24, 5: 120}, # Dictionary of factorials # 150 # Sum of factorials of numbers > threshold (3:6, 4:24, 5:120 -> 6+24+120) # ) ``` Constraint: - The input list can have up to 10^4 integers. - Each integer will be in the range [1, 10^3]. - The function must handle large computations efficiently. Notes: - Use generator expressions for better performance when possible. - Ensure to use asynchronous programming appropriately when summing factorial values asynchronously. ```python import asyncio from math import factorial def process_data(data: list[int], threshold: int) -> tuple[list[int], set[int], dict[int, int], int]: # List comprehension to get squares of even numbers even_squares = [x ** 2 for x in data if x % 2 == 0] # Set comprehension to get cubes of odd numbers odd_cubes = {x ** 3 for x in data if x % 2 != 0} # Dictionary comprehension to get factorials factorials = {x: factorial(x) for x in data} async def sum_factorials_async(): await asyncio.sleep(0) return sum(v for k, v in factorials.items() if k > threshold) # Run asynchronous summing loop = asyncio.get_event_loop() factorial_sum = loop.run_until_complete(sum_factorials_async()) return even_squares, odd_cubes, factorials, factorial_sum # Test case data = [1, 2, 3, 4, 5] threshold = 2 result = process_data(data, threshold) print(result) ``` Use the provided function signature and fill it in with the appropriate logic. Your solution should demonstrate a deep understanding of comprehensions, generator expressions, and asynchronous generators.","solution":"import asyncio from math import factorial def process_data(data: list[int], threshold: int) -> tuple[list[int], set[int], dict[int, int], int]: # List comprehension to get squares of even numbers even_squares = [x ** 2 for x in data if x % 2 == 0] # Set comprehension to get cubes of odd numbers odd_cubes = {x ** 3 for x in data if x % 2 != 0} # Dictionary comprehension to get factorials factorials = {x: factorial(x) for x in data} async def sum_factorials_async(): await asyncio.sleep(0) return sum(v for k, v in factorials.items() if k > threshold) # Run asynchronous summing loop = asyncio.get_event_loop() factorial_sum = loop.run_until_complete(sum_factorials_async()) return even_squares, odd_cubes, factorials, factorial_sum"},{"question":"# Email Parsing Project **Objective**: Use the `email.parser` module to implement a function that parses an email message from a bytes-like object, extracts specific headers, and returns a structured representation. Task Details You need to write a function called `parse_email` that takes a single argument: - `email_bytes`: A bytes-like object containing the raw email message. The function should: 1. Parse the email message using the appropriate parser from the `email.parser` module. 2. Extract the following headers from the parsed message: `From`, `To`, `Subject`, and `Date`. 3. Return a dictionary with the extracted header information and a boolean indicating whether the email message is multipart or not. Function Signature ```python import email from typing import Any, Dict def parse_email(email_bytes: bytes) -> Dict[str, Any]: pass ``` Implementation Details 1. **Parsing the Email**: Use the `email.message_from_bytes()` function to parse the input bytes into an `EmailMessage` object. 2. **Extract Headers**: Use the `get()` method on the `EmailMessage` object to retrieve the required headers. 3. **Check Multiparts**: Use the `is_multipart()` method to check if the message is multipart. 4. **Return Structure**: Return a dictionary with keys: `from`, `to`, `subject`, `date`, `is_multipart`. Input - `email_bytes` (bytes): A valid bytes-like object representing the email message. Output - A dictionary with the following keys: - `from` (str): Sender’s email address. - `to` (str): Recipient’s email address. - `subject` (str): Subject of the email. - `date` (str): Date the email was sent. - `is_multipart` (bool): Indicates if the email is a multipart message. Example ```python email_bytes = b\\"From: example@example.comrnTo: test@test.comrnSubject: Test EmailrnDate: Sun, 21 Mar 2021 20:30:00 -0700rnrnThis is the body of the email.\\" result = parse_email(email_bytes) assert result == { \\"from\\": \\"example@example.com\\", \\"to\\": \\"test@test.com\\", \\"subject\\": \\"Test Email\\", \\"date\\": \\"Sun, 21 Mar 2021 20:30:00 -0700\\", \\"is_multipart\\": False } ``` **Constraints**: - The input `email_bytes` will always be a valid bytes-like object representing an email conforming to the relevant RFC standards. - You should handle both multipart and non-multipart emails correctly. **Note**: You are expected to use the Python standard library\'s `email.parser` module to achieve this task. Please refer to the provided documentation for details.","solution":"import email from typing import Any, Dict def parse_email(email_bytes: bytes) -> Dict[str, Any]: # Parse the given email bytes into an EmailMessage object email_message = email.message_from_bytes(email_bytes) # Extract the required headers from_header = email_message.get(\'From\') to_header = email_message.get(\'To\') subject_header = email_message.get(\'Subject\') date_header = email_message.get(\'Date\') # Check if the email message is multipart is_multipart = email_message.is_multipart() # Return the structured representation as a dictionary return { \\"from\\": from_header, \\"to\\": to_header, \\"subject\\": subject_header, \\"date\\": date_header, \\"is_multipart\\": is_multipart }"},{"question":"**Problem Statement:** You are required to write a function `send_email(settings, message)` that utilizes Python\'s `smtplib` to send an email. This function needs to incorporate handling for connection, authentication, sending the email, and proper exception handling. # Function Signature ```python def send_email(settings: dict, message: str) -> dict: pass ``` # Input - `settings`: A dictionary with the following keys: - `host`: The SMTP server hostname (string). - `port`: The SMTP server port (integer). - `username`: The username for authentication (string). - `password`: The password for authentication (string). - `use_tls`: Boolean indicating whether to use TLS (True/False). - `from_addr`: The sender\'s email address (string). - `to_addrs`: A list of recipient email addresses (list of strings). - `message`: The full email message as a single string. This includes headers like `From`, `To`, and `Subject`. # Output - A dictionary with potentially the following keys: - `status`: \\"Success\\" or \\"Error\\". - `error_code`: If an error occurred, the SMTP error code should be included. - `error_message`: If an error occurred, a description of the error should be included. # Constraints - Do not use any third-party libraries, only the standard Python library. - Ensure you handle all appropriate exceptions and return meaningful error messages. - Assume the message is always a valid email message and the settings provided are correct except where exceptions apply. # Example ```python settings = { \\"host\\": \\"smtp.example.com\\", \\"port\\": 587, \\"username\\": \\"user@example.com\\", \\"password\\": \\"password\\", \\"use_tls\\": True, \\"from_addr\\": \\"user@example.com\\", \\"to_addrs\\": [\\"recipient@example.com\\"] } message = From: user@example.com To: recipient@example.com Subject: Test Email This is a test email. result = send_email(settings, message) print(result) # Example output: {\\"status\\": \\"Success\\"} ``` The function you write should follow these steps: 1. Initialize an SMTP connection to the server specified in `settings`. 2. Optionally start TLS if `use_tls` is set to True. 3. Log in using the provided username and password. 4. Send the email using the `sendmail` or `send_message` method. 5. Handle any exceptions that may arise, returning an appropriate status and error message/code. 6. Ensure the SMTP session is properly quit regardless of success or failure. # Notes - Be aware that some servers may require different ports for TLS and non-TLS connections. Adjust your code accordingly. - Proper exception handling and cleanup (use of `try`/`except`/`finally` or `with` statements) is crucial to ensure robust and reliable operation.","solution":"import smtplib from typing import List, Dict, Any def send_email(settings: Dict[str, Any], message: str) -> Dict[str, str]: result = {\\"status\\": \\"Success\\"} try: # Initialize the SMTP server connection server = smtplib.SMTP(settings[\\"host\\"], settings[\\"port\\"]) # Start TLS if required if settings[\\"use_tls\\"]: server.starttls() # Login to the server server.login(settings[\\"username\\"], settings[\\"password\\"]) # Send the email server.sendmail(settings[\\"from_addr\\"], settings[\\"to_addrs\\"], message) except smtplib.SMTPException as e: result[\\"status\\"] = \\"Error\\" result[\\"error_code\\"] = e.smtp_code if hasattr(e, \'smtp_code\') else None result[\\"error_message\\"] = str(e) except Exception as e: result[\\"status\\"] = \\"Error\\" result[\\"error_code\\"] = None result[\\"error_message\\"] = str(e) finally: server.quit() return result"},{"question":"You are to implement a command-line Python program that calculates the volume of a geometric shape. The program should support the following shapes: cube, cuboid, and sphere. Your task is to use the `argparse` module to handle command-line arguments specifying the shape and its dimensions, along with optional arguments for output verbosity and units of measurement. # Requirements: 1. **Positional Arguments**: - `shape`: Specifies the type of shape. It can be `cube`, `cuboid`, or `sphere`. - `dimensions`: Contains the dimensions of the shape. - For `cube`: Only one dimension representing the length of a side. - For `cuboid`: Three dimensions representing the length, width, and height. - For `sphere`: One dimension representing the radius. 2. **Optional Arguments**: - `-v`, `--verbose`: Increase output verbosity. Can be specified multiple times to increase verbosity level. - `-u`, `--units`: Specifies the unit of measurement. It can be `cm` (centimeters) by default or `m` (meters). 3. **Mutually Exclusive Arguments**: - `-d`, `--detailed`: Provide a detailed output including the formula used. - `-s`, `--summary`: Provide a summary output with only the volume value. # Constraints: - The dimensions must be positive integers. - Only one of `--detailed` or `--summary` can be specified at a time. # Input Format: ```shell python3 geometry.py <shape> <dimensions> [-v] [-u <unit>] [-d | -s] ``` # Output Format: - Default: Print the volume. - Verbose: Print additional information about the input and computation steps. - Detailed: Print the formula used along with the volume. - Summary: Print only the volume. # Examples: Example 1: ```shell python3 geometry.py cube 3 -v ``` Output: ``` Shape: Cube Side length: 3 cm Volume: 27 cm³ ``` Example 2: ```shell python3 geometry.py cuboid 3 4 5 -u m -d ``` Output: ``` Shape: Cuboid Dimensions: 3 m (length) x 4 m (width) x 5 m (height) Formula: Volume = length * width * height Volume: 60 m³ ``` Example 3: ```shell python3 geometry.py sphere 2 -u cm -s ``` Output: ``` 33.51 cm³ ``` # Implementation: Below is a base template to get started with: ```python import argparse import math def calculate_volume(shape, dimensions): if shape == \\"cube\\": side = dimensions[0] return side**3 elif shape == \\"cuboid\\": length, width, height = dimensions return length * width * height elif shape == \\"sphere\\": radius = dimensions[0] return (4/3) * math.pi * (radius**3) def main(): parser = argparse.ArgumentParser(description=\\"Calculate the volume of a geometric shape\\") parser.add_argument(\\"shape\\", choices=[\\"cube\\", \\"cuboid\\", \\"sphere\\"], help=\\"type of the shape (cube, cuboid, sphere)\\") parser.add_argument(\\"dimensions\\", nargs=\'+\', type=int, help=\\"dimensions of the shape\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"increase output verbosity\\") parser.add_argument(\\"-u\\", \\"--units\\", choices=[\\"cm\\", \\"m\\"], default=\\"cm\\", help=\\"units of measurement (default: cm)\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-d\\", \\"--detailed\\", action=\\"store_true\\", help=\\"detailed output mode\\") group.add_argument(\\"-s\\", \\"--summary\\", action=\\"store_true\\", help=\\"summary output mode\\") args = parser.parse_args() volume = calculate_volume(args.shape, args.dimensions) if args.verbose >= 1: print(f\\"Shape: {args.shape.capitalize()}\\") print(f\\"Dimensions: {\' x \'.join(map(str,args.dimensions))} {args.units}\\") if args.detailed: if args.shape == \\"cube\\": formula = \\"Volume = side³\\" elif args.shape == \\"cuboid\\": formula = \\"Volume = length * width * height\\" elif args.shape == \\"sphere\\": formula = \\"Volume = (4/3) * π * radius³\\" print(f\\"Formula: {formula}\\") print(f\\"Volume: {volume:.2f} {args.units}³\\") elif args.summary: print(f\\"{volume:.2f} {args.units}³\\") else: print(f\\"Volume: {volume:.2f} {args.units}³\\") if __name__ == \\"__main__\\": main() ``` Implement the above code to create a command-line calculator for geometric volumes with detailed or summary outputs and varying verbosity.","solution":"import argparse import math def calculate_volume(shape, dimensions): if shape == \\"cube\\": side = dimensions[0] return side**3 elif shape == \\"cuboid\\": length, width, height = dimensions return length * width * height elif shape == \\"sphere\\": radius = dimensions[0] return (4/3) * math.pi * (radius**3) def main(): parser = argparse.ArgumentParser(description=\\"Calculate the volume of a geometric shape\\") parser.add_argument(\\"shape\\", choices=[\\"cube\\", \\"cuboid\\", \\"sphere\\"], help=\\"type of the shape (cube, cuboid, sphere)\\") parser.add_argument(\\"dimensions\\", nargs=\'+\', type=int, help=\\"dimensions of the shape\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"increase output verbosity\\") parser.add_argument(\\"-u\\", \\"--units\\", choices=[\\"cm\\", \\"m\\"], default=\\"cm\\", help=\\"units of measurement (default: cm)\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-d\\", \\"--detailed\\", action=\\"store_true\\", help=\\"detailed output mode\\") group.add_argument(\\"-s\\", \\"--summary\\", action=\\"store_true\\", help=\\"summary output mode\\") args = parser.parse_args() volume = calculate_volume(args.shape, args.dimensions) if args.verbose >= 1: print(f\\"Shape: {args.shape.capitalize()}\\") if args.shape == \\"cube\\" or args.shape == \\"sphere\\": print(f\\"{\'Radius\' if args.shape == \'sphere\' else \'Side length\'}: {args.dimensions[0]} {args.units}\\") elif args.shape == \\"cuboid\\": print(f\\"Dimensions: {args.dimensions[0]} x {args.dimensions[1]} x {args.dimensions[2]} {args.units}\\") if args.detailed: if args.shape == \\"cube\\": formula = \\"Volume = side³\\" elif args.shape == \\"cuboid\\": formula = \\"Volume = length * width * height\\" elif args.shape == \\"sphere\\": formula = \\"Volume = (4/3) * π * radius³\\" print(f\\"Formula: {formula}\\") print(f\\"Volume: {volume:.2f} {args.units}³\\") elif args.summary: print(f\\"{volume:.2f} {args.units}³\\") else: print(f\\"Volume: {volume:.2f} {args.units}³\\") if __name__ == \\"__main__\\": main()"},{"question":"Advanced Named Tensor Manipulation in PyTorch Objective The objective of this task is to assess your understanding of named tensors in PyTorch, including their creation, alignment, manipulation, and the application of operations while maintaining proper dimension names. Problem Statement You are given an RGB image tensor and a scaling factor tensor for each color channel (R, G, B). Your task is to perform several operations on these tensors, including aligning dimensions, scaling the image tensor by the factor tensor, flattening dimensions, and finally reversing the flattening operation to restore the original tensor dimensions. You must ensure that all tensor operations handle named dimensions correctly. Instructions 1. **Create Named Tensors**: - Given a 4D image tensor `imgs` of size `[batch_size, height, width, 3]` with random values, create a named tensor with dimensions: `[\'B\', \'H\', \'W\', \'C\']`. - Given a 1D scaling factor tensor `scale` of size `[3]` with random values, create a named tensor with the dimension name: `[\'C\']`. 2. **Align and Scale**: - Implement a function `scale_channels(input, scale)` that scales the `input` (image tensor) by the `scale` (factor tensor) aligning the \'C\' dimension properly. 3. **Flatten and Unflatten**: - Flatten the dimensions `[\'H\', \'W\']` into a single dimension `[\'HW\']` in the scaled image tensor. - Unflatten the dimension `[\'HW\']` back into `[\'H\', \'W\']`. 4. **Output**: - Output the sizes and dimension names of the tensors at each stage: - After naming the dimensions of the `imgs` and `scale` tensors. - After scaling the image tensor. - After flattening the image tensor. - After unflattening the image tensor. Constraints - Batch size: `batch_size` can be any positive integer. - Height and width: `height`, `width` can be any positive integers. - Scalability: Your code should handle large tensors efficiently. Example ```python import torch def scale_channels(input, scale): scale = scale.refine_names(\'C\') return input * scale.align_as(input) # Create the image tensor and named tensor batch_size, height, width = 2, 128, 128 imgs = torch.randn(batch_size, height, width, 3) named_imgs = imgs.refine_names(\'B\', \'H\', \'W\', \'C\') # Create the scale tensor and named tensor scale = torch.randn(3) named_scale = scale.refine_names(\'C\') # Scale the image tensor scaled_imgs = scale_channels(named_imgs, named_scale) print(scaled_imgs.shape, scaled_imgs.names) # Should print appropriate shape and names # Flatten the scaled image tensor flattened_imgs = scaled_imgs.flatten([\'H\', \'W\'], \'HW\') print(flattened_imgs.shape, flattened_imgs.names) # Should print appropriate shape and names # Unflatten the flattened image tensor unflattened_imgs = flattened_imgs.unflatten(\'HW\', [(\'H\', height), (\'W\', width)]) print(unflattened_imgs.shape, unflattened_imgs.names) # Should print appropriate shape and names ``` Note: You may use random tensors for this exercise, but ensure the operations and dimension names are correct. Submission Submit a Jupyter notebook or Python script containing the implemented solution, including all required function definitions and outputs from the example run.","solution":"import torch def scale_channels(input, scale): Scale the input image tensor by the channel-wise scaling factor tensor. scale = scale.refine_names(\'C\') return input * scale.align_as(input) # Create the image tensor and named tensor batch_size, height, width = 2, 128, 128 imgs = torch.randn(batch_size, height, width, 3) named_imgs = imgs.refine_names(\'B\', \'H\', \'W\', \'C\') # Create the scale tensor and named tensor scale = torch.randn(3) named_scale = scale.refine_names(\'C\') # Output sizes and dimension names after naming output_info = { \\"named_imgs_shape\\": named_imgs.shape, \\"named_imgs_names\\": named_imgs.names, \\"named_scale_shape\\": named_scale.shape, \\"named_scale_names\\": named_scale.names } # Scale the image tensor scaled_imgs = scale_channels(named_imgs, named_scale) output_info.update({ \\"scaled_imgs_shape\\": scaled_imgs.shape, \\"scaled_imgs_names\\": scaled_imgs.names }) # Flatten the scaled image tensor flattened_imgs = scaled_imgs.flatten([\'H\', \'W\'], \'HW\') output_info.update({ \\"flattened_imgs_shape\\": flattened_imgs.shape, \\"flattened_imgs_names\\": flattened_imgs.names }) # Unflatten the flattened image tensor unflattened_imgs = flattened_imgs.unflatten(\'HW\', [(\'H\', height), (\'W\', width)]) output_info.update({ \\"unflattened_imgs_shape\\": unflattened_imgs.shape, \\"unflattened_imgs_names\\": unflattened_imgs.names }) def get_output_info(): return output_info"},{"question":"**Question: Implement a Logging System Using the syslog Module** You have been tasked with creating a logging system for a Unix-based application. The logging system needs to handle various types of messages with different levels of severity and send them to appropriate syslog facilities. # Requirements 1. **Function: `initialize_syslog(ident: str, logoption: int, facility: int) -> None`** - This function should initialize the syslog with the provided identifier, log option, and facility. Ensure to reset any previously set configurations. 2. **Function: `log_message(priority: int, message: str) -> None`** - This function should send a log message with a given priority. It should automatically call `initialize_syslog()` if the syslog has not been initialized yet. 3. **Function: `set_priority_mask(maskpri: int) -> int`** - This function should set the priority mask for filtering log messages based on their severity. It should return the previous mask value. 4. **Function: `close_syslog() -> None`** - This function should close the syslog and reset any configurations to their defaults. # Constraints - You may assume that `priority`, `logoption`, and `facility` values are valid constants defined in the syslog module. - The `maskpri` can be set using `syslog.LOG_MASK(pri)` and `syslog.LOG_UPTO(pri)` functions as needed. - Performance is not a primary concern, but ensure that unnecessary re-initializations of syslog are avoided. # Example Usage ```python import syslog # Step 1: Initialize syslog initialize_syslog(\'myApp\', syslog.LOG_PID, syslog.LOG_USER) # Step 2: Log various messages log_message(syslog.LOG_INFO, \'Application started\') log_message(syslog.LOG_ERR, \'An error occurred\') # Step 3: Set logging priority mask previous_mask = set_priority_mask(syslog.LOG_UPTO(syslog.LOG_WARNING)) # Step 4: Log a debug message (should be ignored due to mask) log_message(syslog.LOG_DEBUG, \'Debugging info\') # Step 5: Close syslog close_syslog() ``` # Implementation Details Your task is to implement all four functions (`initialize_syslog`, `log_message`, `set_priority_mask`, and `close_syslog`) as per the requirements. Remember to handle auditing events properly as per the syslog module requirements.","solution":"import syslog _syslog_initialized = False def initialize_syslog(ident: str, logoption: int, facility: int) -> None: global _syslog_initialized syslog.openlog(ident, logoption, facility) _syslog_initialized = True def log_message(priority: int, message: str) -> None: global _syslog_initialized if not _syslog_initialized: initialize_syslog(\'\', syslog.LOG_PID, syslog.LOG_USER) # Default initialization syslog.syslog(priority, message) def set_priority_mask(maskpri: int) -> int: previous_mask = syslog.setlogmask(maskpri) return previous_mask def close_syslog() -> None: global _syslog_initialized syslog.closelog() _syslog_initialized = False"},{"question":"Objective: To assess your understanding and ability to work with Python dictionaries using Python\'s C API available in the `python310` package. Problem Statement: You are to implement a function in Python that mimics complex dictionary operations using the underlying `python310` C functions. The goal is to develop a function that creates and manipulates dictionaries to store and manage user data for a hypothetical application. Function to Implement: ```python def manage_user_data(user_data: list, updates: list, merge_data: list) -> dict: Manages user data through insertion, updating, and merging operations. Parameters: - user_data: List of tuples. Each tuple contains (user_id, user_info_dict) Example: [(1, {\'name\': \'Alice\', \'age\': 30}), (2, {\'name\': \'Bob\', \'age\': 24})] - updates: List of tuples. Each tuple contains (user_id, field, new_value). Example: [(1, \'age\', 31), (2, \'name\', \'Robert\')] - merge_data: List of tuples. Each tuple contains (user_id, additional_info_dict) Example: [(1, {\'email\': \'alice@example.com\'}), (2, {\'email\': \'bob@example.com\'})] Returns: - A dictionary representing the final state of user data. Example: { 1: {\'name\': \'Alice\', \'age\': 31, \'email\': \'alice@example.com\'}, 2: {\'name\': \'Robert\', \'age\': 24, \'email\': \'bob@example.com\'} } Constraints: - All user IDs and fields in updates and merge_data are guaranteed to be present in user_data. - Assume all keys are hashable and values are valid. Notes: - Use the provided `python310` C API functions to manage the dictionary operations. - Handle errors gracefully and ensure proper memory management. pass ``` Detailed Requirements: 1. **Initial Dictionary Creation**: - Use `PyDict_New` to create a new dictionary from `user_data`. 2. **Update Operations**: - For each update in `updates`, use the appropriate function (`PyDict_SetItemString`) to update the relevant field for each user. 3. **Merge Operations**: - Use `PyDict_Merge` to merge additional information into the existing user dictionaries. 4. **Error Handling**: - Ensure to check for errors using the provided functions and handle them appropriately. Raise an appropriate exception if any operation fails. 5. **Optimizations**: - Consider the efficiency of each operation and strive to minimize unnecessary computations. Implement and test the function to ensure it works as expected. Input and Output Formats: - **Input**: - `user_data` is a list of tuples where each tuple contains a user ID (integer) and a dictionary of user information. - `updates` is a list of tuples containing a user ID, a field (string), and a new value. - `merge_data` is a list of tuples containing a user ID and a dictionary of additional information. - **Output**: - The function returns a dictionary representing the final state of user data after all updates and merges. Example: ```python # Example input user_data = [(1, {\'name\': \'Alice\', \'age\': 30}), (2, {\'name\': \'Bob\', \'age\': 24})] updates = [(1, \'age\', 31), (2, \'name\', \'Robert\')] merge_data = [(1, {\'email\': \'alice@example.com\'}), (2, {\'email\': \'bob@example.com\'})] # Expected output { 1: {\'name\': \'Alice\', \'age\': 31, \'email\': \'alice@example.com\'}, 2: {\'name\': \'Robert\', \'age\': 24, \'email\': \'bob@example.com\'} } ``` Testing: - Ensure to test the function with various input scenarios to validate its correctness and error handling capabilities. Good luck!","solution":"def manage_user_data(user_data: list, updates: list, merge_data: list) -> dict: Manages user data through insertion, updating, and merging operations. Parameters: - user_data: List of tuples. Each tuple contains (user_id, user_info_dict) Example: [(1, {\'name\': \'Alice\', \'age\': 30}), (2, {\'name\': \'Bob\', \'age\': 24})] - updates: List of tuples. Each tuple contains (user_id, field, new_value). Example: [(1, \'age\', 31), (2, \'name\', \'Robert\')] - merge_data: List of tuples. Each tuple contains (user_id, additional_info_dict) Example: [(1, {\'email\': \'alice@example.com\'}), (2, {\'email\': \'bob@example.com\'})] Returns: - A dictionary representing the final state of user data. Example: { 1: {\'name\': \'Alice\', \'age\': 31, \'email\': \'alice@example.com\'}, 2: {\'name\': \'Robert\', \'age\': 24, \'email\': \'bob@example.com\'} } # Create the initial user data dictionary users = {user_id: info for user_id, info in user_data} # Apply updates for user_id, field, new_value in updates: if user_id in users: users[user_id][field] = new_value # Merge additional data for user_id, additional_info in merge_data: if user_id in users: users[user_id].update(additional_info) return users"},{"question":"**Coding Assessment Question:** You are tasked with generating artificial datasets using scikit-learn\'s sample generators. Your goal is to implement a function that generates and visualizes several types of datasets for classification and regression tasks. # Function Signature ```python def generate_and_visualize_datasets(): pass ``` # Requirements 1. **Generate the following datasets**: * A multiclass dataset using `make_blobs` with the following parameters: - `centers=4` - `cluster_std=0.6` - `random_state=42` * A binary classification dataset using `make_classification` with the following parameters: - `n_features=2` - `n_informative=2` - `n_redundant=0` - `n_clusters_per_class=1` - `random_state=42` * A binary classification dataset using `make_circles` with the following parameters: - `noise=0.2` - `factor=0.4` - `random_state=42` * A regression dataset using `make_regression` with the following parameters: - `n_features=1` - `noise=10` - `random_state=42` 2. **Visualize each dataset**: - Use matplotlib to create scatter plots for classification datasets where the classes are color-coded. - Use matplotlib to create a scatter plot for the regression dataset showing the data points. 3. **Provide comments** in your code explaining: - The purpose of each dataset. - How the generated datasets can be used in machine learning. - Any observations about the dataset\'s properties. # Expected Output The function should generate and display four scatter plots, one for each dataset type as described above. # Constraints - Make sure that the plots are clear and well-labeled. - Ensure the function runs without errors and displays all necessary plots within a single call. # Example ```python generate_and_visualize_datasets() # This should generate four scatter plots with appropriate titles and labels for each dataset type. ``` **Note**: Do not forget to include the necessary imports (`matplotlib`, `sklearn.datasets`, etc.) in your function.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_circles, make_regression def generate_and_visualize_datasets(): # Generate a multiclass dataset using make_blobs X_blobs, y_blobs = make_blobs(centers=4, cluster_std=0.6, random_state=42) plt.figure(figsize=(10, 8)) plt.subplot(2, 2, 1) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap=\'viridis\', marker=\'o\') plt.title(\\"Multiclass Dataset: make_blobs\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") # Generate a binary classification dataset using make_classification X_classification, y_classification = make_classification(n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42) plt.subplot(2, 2, 2) plt.scatter(X_classification[:, 0], X_classification[:, 1], c=y_classification, cmap=\'viridis\', marker=\'o\') plt.title(\\"Binary Classification: make_classification\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") # Generate a binary classification dataset using make_circles X_circles, y_circles = make_circles(noise=0.2, factor=0.4, random_state=42) plt.subplot(2, 2, 3) plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=\'viridis\', marker=\'o\') plt.title(\\"Binary Classification: make_circles\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") # Generate a regression dataset using make_regression X_regression, y_regression = make_regression(n_features=1, noise=10, random_state=42) plt.subplot(2, 2, 4) plt.scatter(X_regression, y_regression, c=\'b\', marker=\'o\') plt.title(\\"Regression Dataset: make_regression\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Target\\") plt.tight_layout() plt.show()"},{"question":"**Title:** Interaction with robots.txt using `urllib.robotparser` **Objective:** Implement a Python function that fetches and processes a `robots.txt` file for given URLs and user agents, providing insights into what actions are allowed according to the file\'s rules. **Question:** You are tasked with creating a Python function `process_robots_txt(urls: List[str], user_agents: List[str]) -> Dict[str, Dict]` that processes multiple `robots.txt` files from various websites. For each provided URL representing the web domain, and for each user agent, the function should determine: 1. Whether a specific URL can be fetched. 2. The `crawl_delay` for the user agent. 3. The `request_rate` for the user agent. 4. The site maps defined in the `robots.txt`. The function should return a dictionary where the keys are the URLs and the values are dictionaries that contain: - Allowed URLs for each user agent. - `crawl_delay` values for each user agent. - `request_rate` values for each user agent. - `site_maps` list. # Input: - `urls`: A list of strings; each string is a URL of the domain (e.g., \\"http://www.example.com\\"). - `user_agents`: A list of strings; each string is a user agent (e.g., \\"Mozilla/5.0\\"). # Output: - A dictionary structured as follows: ```python { \\"<url>\\": { \\"<user_agent>\\": { \\"can_fetch\\": List[str], # List of URLs that can be fetched \\"crawl_delay\\": Optional[int], # Crawl delay value or None \\"request_rate\\": Optional[Tuple[int, int]], # (requests, seconds) tuple or None \\"site_maps\\": List[str] # List of sitemaps }, ... }, ... } ``` # Constraints: 1. All URLs provided in `urls` list are guaranteed to respond with a `robots.txt` file. 2. The `robots.txt` file might or might not contain all parameters (allow/disallow, crawl-delay, request-rate, sitemap). # Example: ```python urls = [\\"http://www.musi-cal.com\\"] user_agents = [\\"*\\", \\"Mozilla/5.0\\"] result = process_robots_txt(urls, user_agents) # Expected result format: { \\"http://www.musi-cal.com\\": { \\"*\\": { \\"can_fetch\\": [\\"http://www.musi-cal.com/\\"], \\"crawl_delay\\": 6, \\"request_rate\\": (3, 20), \\"site_maps\\": [] }, \\"Mozilla/5.0\\": { \\"can_fetch\\": [\\"http://www.musi-cal.com/\\"], \\"crawl_delay\\": None, \\"request_rate\\": None, \\"site_maps\\": [] } } } ``` # Additional Notes: - Use the `urllib.robotparser` module to parse and interpret the `robots.txt` file. - Handle any edge cases and validate the input as required. Good luck, and happy coding!","solution":"import urllib.robotparser from typing import List, Dict, Tuple, Optional def process_robots_txt(urls: List[str], user_agents: List[str]) -> Dict[str, Dict]: result = {} for url in urls: base_url = url.rstrip(\'/\') + \'/\' robots_url = base_url + \\"robots.txt\\" rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() result[url] = {} for user_agent in user_agents: can_fetch = [] crawl_delay = rp.crawl_delay(user_agent) request_rate = rp.request_rate(user_agent) site_maps = rp.site_maps() if site_maps is None: site_maps = [] # Check if the base URL can be fetched for completion purposes if rp.can_fetch(user_agent, base_url): can_fetch.append(base_url) result[url][user_agent] = { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate, \\"site_maps\\": site_maps } return result"},{"question":"# Secure File Hasher Implement a Python function `secure_file_hasher(filepath, hash_name, mode, **kwargs)` that generates a secure hash of a given file based on the specified hash algorithm and mode. Function Signature ```python def secure_file_hasher(filepath: str, hash_name: str, mode: str, **kwargs) -> str: pass ``` Parameters - `filepath` (str): The path to the file to be hashed. - `hash_name` (str): The name of the hash algorithm to use (e.g., \'sha256\', \'blake2b\', etc.). - `mode` (str): The hashing mode. It can be one of the following: - `\'normal\'`: Standard hashing mode. - `\'keyed\'`: Keyed hashing mode. Requires the keyword argument `key` (bytes). - `\'salted\'`: Salted hashing mode. Requires the keyword argument `salt` (bytes). - `\'personalized\'`: Personalized hashing mode. Requires the keyword argument `person` (bytes). - `**kwargs`: Additional keyword arguments needed for specific modes (e.g., `key`, `salt`, `person`). Returns - `str`: The hexadecimal representation of the hash value of the file\'s contents. Constraints - The file must be read in binary mode. - For `\'keyed\'` mode, the `key` bytes length should not exceed the maximum key size for the chosen algorithm. - For `\'salted\'` mode, the `salt` bytes length should not exceed the maximum salt size for the chosen algorithm. - For `\'personalized\'` mode, the `person` bytes length should not exceed the maximum personalization string size for the chosen algorithm. - The chosen hash algorithm must be one of the algorithms listed in `hashlib.algorithms_guaranteed`. Example Usage ```python # Normal hashing print(secure_file_hasher(\'example.txt\', \'sha256\', \'normal\')) # Keyed hashing print(secure_file_hasher(\'example.txt\', \'blake2b\', \'keyed\', key=b\'mysecretkey\')) # Salted hashing print(secure_file_hasher(\'example.txt\', \'blake2b\', \'salted\', salt=os.urandom(16))) # Personalized hashing print(secure_file_hasher(\'example.txt\', \'blake2b\', \'personalized\', person=b\'mypersonstr\')) ``` Example Output ``` \'e59ff97941044f85df5297e1c302d260b6d4a291a45d2171ef69acb8cd9e7b7e\' # Normal hashing \'44d20f059c42b4c4c629b8e9c4fe75e0a71580a41ee1d9e157e1afa017be4181\' # Keyed hashing \'6aaf5d4b9fa94b26fadb464f2bfc776ea514ed8c4c6d35363aa9a2bdf33c87d0\' # Salted hashing \'8f7be5c4af767c47ab3692367eefa4c26d43f48df7f4dd20e6781678a975a1db\' # Personalized hashing ``` Notes - Use the `hashlib.new` function for creating a new hashing object with the specified algorithm. - Ensure to handle exceptions where the provided `hash_name` or `mode` is invalid. - Reading large files should be done in chunks to avoid memory issues.","solution":"import hashlib import os def secure_file_hasher(filepath: str, hash_name: str, mode: str, **kwargs) -> str: if hash_name not in hashlib.algorithms_guaranteed: raise ValueError(\\"Invalid hash algorithm\\") if mode not in [\'normal\', \'keyed\', \'salted\', \'personalized\']: raise ValueError(\\"Invalid mode\\") if mode == \'keyed\' and \'key\' not in kwargs: raise ValueError(\\"Keyed mode requires a key argument\\") if mode == \'salted\' and \'salt\' not in kwargs: raise ValueError(\\"Salted mode requires a salt argument\\") if mode == \'personalized\' and \'person\' not in kwargs: raise ValueError(\\"Personalized mode requires a person argument\\") hash_obj = None if mode == \'normal\': hash_obj = hashlib.new(hash_name) elif mode == \'keyed\': hash_obj = hashlib.new(hash_name, key=kwargs[\'key\']) elif mode == \'salted\': hash_obj = hashlib.new(hash_name, salt=kwargs[\'salt\']) elif mode == \'personalized\': hash_obj = hashlib.new(hash_name, person=kwargs[\'person\']) with open(filepath, \'rb\') as f: while chunk := f.read(8192): hash_obj.update(chunk) return hash_obj.hexdigest()"},{"question":"# Sequence Protocol Implementation In Python, the sequence protocol provides a uniform way to handle objects that represent ordered collections of items, such as lists, tuples, and strings. This exercise gives you the opportunity to implement some common sequence operations in Python, reflecting how they would be performed using lower-level sequence protocol functions. You will implement a class, `SequenceOperations`, which provides methods to perform a variety of sequence operations. Each method should mimic the behavior of a corresponding sequence protocol function discussed in the documentation. Your implementation should handle sequences robustly and perform checks when necessary. Here is a list of methods you need to implement in the `SequenceOperations` class: 1. **is_sequence(obj)**: Returns `True` if the object `obj` is a sequence, using Python\'s `__getitem__` special method check. For this task, a sequence is anything that supports the `__getitem__` method except dictionaries. 2. **sequence_size(obj)**: Returns the number of items in the sequence `obj`, equivalent to the `len(obj)` in Python. 3. **sequence_concat(seq1, seq2)**: Returns a new sequence by concatenating `seq1` and `seq2`, equivalent to `seq1 + seq2`. 4. **sequence_repeat(seq, count)**: Returns a new sequence that repeats `seq` `count` times, equivalent to `seq * count`. 5. **sequence_get_item(seq, index)**: Returns the item at position `index` in `seq`, equivalent to `seq[index]`. 6. **sequence_get_slice(seq, start, end)**: Returns a slice from `seq` between `start` and `end`, equivalent to `seq[start:end]`. 7. **sequence_set_item(seq, index, value)**: Sets the item at position `index` in `seq` to `value`, equivalent to `seq[index] = value`. 8. **sequence_del_item(seq, index)**: Deletes the item at position `index` in `seq`, equivalent to `del seq[index]`. 9. **sequence_index(seq, value)**: Returns the first index where `value` appears in `seq`, equivalent to `seq.index(value)`. 10. **sequence_count(seq, value)**: Returns the number of times `value` appears in `seq`, equivalent to `seq.count(value)`. # Requirements - Your methods should be robust and handle edge cases properly. - Raise appropriate errors when operations cannot be completed (e.g., `IndexError` for invalid indices). - Perform type checks where necessary to ensure the correct operations on sequences. - Do not use the built-in list or sequence methods directly for the core functionality (e.g., avoid using `list.count` directly in `sequence_count`). Implement the algorithms yourself. ```python class SequenceOperations: @staticmethod def is_sequence(obj): Check if obj is a sequence. return hasattr(obj, \\"__getitem__\\") and not isinstance(obj, dict) @staticmethod def sequence_size(obj): Return the number of items in the sequence obj. if SequenceOperations.is_sequence(obj): return len(obj) raise TypeError(\\"Object is not a sequence\\") @staticmethod def sequence_concat(seq1, seq2): Return seq1 concatenated with seq2. if SequenceOperations.is_sequence(seq1) and SequenceOperations.is_sequence(seq2): return seq1 + seq2 raise TypeError(\\"Both arguments must be sequences\\") @staticmethod def sequence_repeat(seq, count): Return seq repeated count times. if SequenceOperations.is_sequence(seq): return seq * count raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_get_item(seq, index): Return the item at index in seq. if SequenceOperations.is_sequence(seq): return seq[index] raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_get_slice(seq, start, end): Return the slice from start to end in seq. if SequenceOperations.is_sequence(seq): return seq[start:end] raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_set_item(seq, index, value): Set the item at index in seq to value. if SequenceOperations.is_sequence(seq): seq[index] = value else: raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_del_item(seq, index): Delete the item at index in seq. if SequenceOperations.is_sequence(seq): del seq[index] else: raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_index(seq, value): Return the index of value in seq. if SequenceOperations.is_sequence(seq): return seq.index(value) raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_count(seq, value): Return the number of occurrences of value in seq. if SequenceOperations.is_sequence(seq): return seq.count(value) raise TypeError(\\"First argument must be a sequence\\") # Example usage: # ops = SequenceOperations() # print(ops.sequence_concat([1, 2], [3, 4])) # Output: [1, 2, 3, 4] # print(ops.sequence_get_item([1, 2, 3], 1)) # Output: 2 ``` Your implementation will be tested with various sequences (e.g., lists, tuples, strings) and edge cases to ensure correctness and robustness.","solution":"class SequenceOperations: @staticmethod def is_sequence(obj): Check if obj is a sequence. return hasattr(obj, \\"__getitem__\\") and not isinstance(obj, dict) @staticmethod def sequence_size(obj): Return the number of items in the sequence obj. if SequenceOperations.is_sequence(obj): return len(obj) raise TypeError(\\"Object is not a sequence\\") @staticmethod def sequence_concat(seq1, seq2): Return seq1 concatenated with seq2. if SequenceOperations.is_sequence(seq1) and SequenceOperations.is_sequence(seq2): return seq1 + seq2 raise TypeError(\\"Both arguments must be sequences\\") @staticmethod def sequence_repeat(seq, count): Return seq repeated count times. if SequenceOperations.is_sequence(seq) and isinstance(count, int): return seq * count raise TypeError(\\"First argument must be a sequence and second argument must be an integer\\") @staticmethod def sequence_get_item(seq, index): Return the item at index in seq. if SequenceOperations.is_sequence(seq): return seq[index] raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_get_slice(seq, start, end): Return the slice from start to end in seq. if SequenceOperations.is_sequence(seq): return seq[start:end] raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_set_item(seq, index, value): Set the item at index in seq to value. if SequenceOperations.is_sequence(seq): if isinstance(seq, (list, bytearray)): seq[index] = value else: raise TypeError(\\"Sequence is not mutable\\") else: raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_del_item(seq, index): Delete the item at index in seq. if SequenceOperations.is_sequence(seq): if isinstance(seq, (list, bytearray)): del seq[index] else: raise TypeError(\\"Sequence is not mutable\\") else: raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_index(seq, value): Return the index of value in seq. if SequenceOperations.is_sequence(seq): for idx, item in enumerate(seq): if item == value: return idx raise ValueError(f\\"{value} is not in sequence\\") raise TypeError(\\"First argument must be a sequence\\") @staticmethod def sequence_count(seq, value): Return the number of occurrences of value in seq. if SequenceOperations.is_sequence(seq): count = 0 for item in seq: if item == value: count += 1 return count raise TypeError(\\"First argument must be a sequence\\") # Example usage: # ops = SequenceOperations() # print(ops.sequence_concat([1, 2], [3, 4])) # Output: [1, 2, 3, 4] # print(ops.sequence_get_item([1, 2, 3], 1)) # Output: 2"},{"question":"Coding Assessment Question # Naive Bayes Classifiers in Scikit-Learn In this assessment, you are required to explore and implement a Naive Bayes classifier to classify a given dataset. The task will focus on using the `GaussianNB` classifier from the `scikit-learn` library to classify the Iris dataset. # Task You need to perform the following steps: 1. Load the Iris dataset available from the `sklearn.datasets` module. 2. Split the dataset into training and testing sets. 3. Implement a Gaussian Naive Bayes classifier: - Fit the model using the training data. - Predict the class labels for the test data. 4. Evaluate the classifier by determining the accuracy and the number of mislabeled points. # Constraints - Use the `train_test_split` function from `sklearn.model_selection` to split the dataset. - Use the `GaussianNB` class from `sklearn.naive_bayes`. - Ensure that your code handles the data correctly and provides the required accuracy metric. # Input - The function should not take any direct input. It will use the Iris dataset from `sklearn.datasets`. # Output - Print the accuracy of the model on the test dataset. - Print the number of mislabeled points out of the total points in the test dataset. # Example Output ``` Accuracy of the Gaussian Naive Bayes Classifier on the test dataset: 0.947 Number of mislabeled points out of a total 75 points: 4 ``` # Required Libraries Make sure you have the following libraries installed: - `scikit-learn` - `numpy` # Implementation ```python def naive_bayes_iris_classification(): # Step 1: Importing the necessary libraries and loading the Iris dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB import numpy as np # Load the Iris dataset X, y = load_iris(return_X_y=True) # Step 2: Splitting the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) # Step 3: Implementing the Gaussian Naive Bayes classifier gnb = GaussianNB() y_pred = gnb.fit(X_train, y_train).predict(X_test) # Step 4: Evaluating the classifier accuracy = np.mean(y_pred == y_test) mislabeled_points = (y_test != y_pred).sum() # Printing the results print(f\\"Accuracy of the Gaussian Naive Bayes Classifier on the test dataset: {accuracy}\\") print(f\\"Number of mislabeled points out of a total {X_test.shape[0]} points: {mislabeled_points}\\") # Run the implementation function naive_bayes_iris_classification() ``` Make sure to follow these steps accurately to evaluate the Gaussian Naive Bayes classifier on the Iris dataset.","solution":"def naive_bayes_iris_classification(): # Step 1: Importing the necessary libraries and loading the Iris dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB import numpy as np # Load the Iris dataset X, y = load_iris(return_X_y=True) # Step 2: Splitting the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) # Step 3: Implementing the Gaussian Naive Bayes classifier gnb = GaussianNB() y_pred = gnb.fit(X_train, y_train).predict(X_test) # Step 4: Evaluating the classifier accuracy = np.mean(y_pred == y_test) mislabeled_points = (y_test != y_pred).sum() # Printing the results print(f\\"Accuracy of the Gaussian Naive Bayes Classifier on the test dataset: {accuracy}\\") print(f\\"Number of mislabeled points out of a total {X_test.shape[0]} points: {mislabeled_points}\\") # Run the implementation function naive_bayes_iris_classification()"},{"question":"Coding Assessment Question # Objective: Implement a Python function using sklearn that trains a machine learning model and evaluates it based on multiple user-specified scoring metrics. This task will test your understanding of scikit-learn\'s metrics and scoring functions, as well as your ability to build and evaluate a model pipeline. # Problem Statement: You are given a dataset in the form of arrays `X` (features) and `y` (targets) and a dictionary of scoring metrics. Your task is to: 1. Split the data into training and test sets. 2. Train a logistic regression model using the training set. 3. Evaluate the model on the test set using multiple scoring metrics provided by the user. 4. Return a dictionary containing the calculated scores for each metric. # Function Signature: ```python from typing import Dict, List, Any from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split def evaluate_model(X: Any, y: Any, scoring: Dict[str, str]) -> Dict[str, float]: pass ``` # Inputs: - `X`: array-like of shape (n_samples, n_features) - The feature data. - `y`: array-like of shape (n_samples,) - The target data. - `scoring`: dictionary of {str: str} - A dictionary where the key is a custom name for the metric, and the value is a valid scoring metric name from scikit-learn. # Outputs: - A dictionary containing the score for each metric in the `scoring` dictionary. The dictionary keys should correspond to the custom metric names provided, and the values should be the computed scores on the test set. # Constraints: - Use an 80-20 split for the training and test sets. - Use the `LogisticRegression` model from sklearn. # Example Usage: ```python X = [[0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5]] y = [0, 0, 1, 1] scoring = { \\"Accuracy\\": \\"accuracy\\", \\"F1_Score\\": \\"f1\\", \\"ROC_AUC\\": \\"roc_auc\\" } result = evaluate_model(X, y, scoring) print(result) # Expected Output: # {\'Accuracy\': 1.0, \'F1_Score\': 1.0, \'ROC_AUC\': 1.0} ``` # Notes: - You may assume that the provided `scoring` dictionary contains valid metric names. - Ensure that all necessary sklearn modules are imported. - Don\'t forget to handle random seeds for reproducibility where necessary.","solution":"from typing import Dict, Any from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score import numpy as np def evaluate_model(X: Any, y: Any, scoring: Dict[str, str]) -> Dict[str, float]: # Split the data into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the logistic regression model model = LogisticRegression() # Train the model on the training set model.fit(X_train, y_train) # Initialize a dictionary to store the scores scores = {} # Evaluate the model on the test set using provided scoring metrics y_pred = model.predict(X_test) for metric_name, metric in scoring.items(): if metric == \\"accuracy\\": score = accuracy_score(y_test, y_pred) elif metric == \\"f1\\": score = f1_score(y_test, y_pred) elif metric == \\"roc_auc\\": y_pred_prob = model.predict_proba(X_test)[:, 1] score = roc_auc_score(y_test, y_pred_prob) # Add additional metrics as necessary else: raise ValueError(f\\"Unsupported metric: {metric}\\") scores[metric_name] = score return scores"},{"question":"Given a DataFrame containing a mix of numerical, temporal, and categorical data with some missing values, you need to implement a function that performs several data cleaning operations to handle the missing values. Implement the function `clean_missing_data(df: pd.DataFrame) -> pd.DataFrame` which: 1. **Detects** all missing values and fills them based on the column types: - Replaces missing values in **numerical columns** with the mean of the column. - Replaces missing values in **timedelta and datetime columns** with the previous valid value (forward fill). - Replaces missing values in **categorical columns** with the mode of the column. 2. **Reports** the number of missing values in each column before and after the cleaning operation. 3. **Returns** the cleaned DataFrame. **Input:** - `df (pd.DataFrame)`: A DataFrame with various columns of numerical, temporal, and categorical data, potentially containing missing values. **Output:** - `cleaned_df (pd.DataFrame)`: The DataFrame after performing the specified data cleaning operations. **Example:** ```python import pandas as pd import numpy as np data = { \'numeric\': [1, 2, np.nan, 4, 5], \'categorical\': [\'a\', \'b\', np.nan, \'a\', \'c\'], \'datetime\': [pd.Timestamp(\'2020-01-01\'), pd.NaT, pd.Timestamp(\'2020-01-03\'), pd.NaT, pd.Timestamp(\'2020-01-05\')], \'timedelta\': [pd.Timedelta(\'1 days\'), pd.NaT, pd.Timedelta(\'3 days\'), pd.NaT, pd.Timedelta(\'5 days\')], } df = pd.DataFrame(data) print(clean_missing_data(df)) ``` Output: ``` Before Cleaning: numeric 1 categorical 1 datetime 2 timedelta 2 dtype: int64 After Cleaning: numeric 0 categorical 0 datetime 0 timedelta 0 dtype: int64 numeric categorical datetime timedelta 0 1.0 a 2020-01-01 1 days 1 2.0 b 2020-01-01 1 days 2 3.0 a 2020-01-03 3 days 3 4.0 a 2020-01-03 3 days 4 5.0 c 2020-01-05 5 days ``` **Constraints:** - You can assume the DataFrame has no multi-index or nested structures. - The DataFrame can have any number of columns and rows. - Use appropriate pandas functions to handle missing data efficiently. **Requirements:** - Handle missing values according to the column types. - Report missing value statistics before and after cleaning. - Return the cleaned DataFrame with filled missing values.","solution":"import pandas as pd import numpy as np def clean_missing_data(df: pd.DataFrame) -> pd.DataFrame: Detects and fills missing values in a DataFrame based on column types. Parameters: df (pd.DataFrame): Input DataFrame with numerical, temporal, and categorical columns. Returns: pd.DataFrame: Cleaned DataFrame with filled missing values. # Report missing values before cleaning missing_before = df.isna().sum() print(\\"Before Cleaning:\\") print(missing_before) # Fill missing values based on column types for column in df.columns: if df[column].dtype in [np.int64, np.float64]: # Fill numerical columns with mean df[column].fillna(df[column].mean(), inplace=True) elif pd.api.types.is_datetime64_any_dtype(df[column]) or pd.api.types.is_timedelta64_dtype(df[column]): # Fill datetime and timedelta columns with forward fill df[column].fillna(method=\'ffill\', inplace=True) else: # Fill categorical columns with mode df[column].fillna(df[column].mode()[0], inplace=True) # Report missing values after cleaning missing_after = df.isna().sum() print(\\"After Cleaning:\\") print(missing_after) return df"},{"question":"Objective Implement a Python function `exception_traceback_handler` that simulates running user-provided code and handles exceptions by extracting, formatting, and printing relevant traceback information in a customizable manner. Function Signature ```python def exception_traceback_handler(user_code: str, limit: int = None, capture_locals: bool = False) -> str: pass ``` Inputs - `user_code` (str): A string containing valid Python code that will be executed. - `limit` (int, optional): Limits the number of stack trace entries printed. The default is `None`, meaning all entries are printed. - `capture_locals` (bool, optional): If `True`, local variables in each frame are captured and displayed in the traceback. The default is `False`. Output - Returns a formatted string containing the traceback information formatted according to the specified options (`limit` and `capture_locals`). Constraints - The function should catch any exceptions raised by the `user_code` and extract the traceback information using the `traceback` module. - The function should handle both runtime and syntax errors. - The function should format the exception and traceback information such that it includes details about local variables if `capture_locals` is `True`. - The formatting should be clear and similar to the examples provided in the documentation. Example ```python user_code = def divide(a, b): return a / b result = divide(10, 0) print(exception_traceback_handler(user_code)) ``` Expected output (content structure): ``` Traceback (most recent call last): File \\"<string>\\", line 5, in <module> File \\"<string>\\", line 2, in divide ZeroDivisionError: division by zero ``` If `capture_locals` is set to `True`, the output should also include the local variables at each frame where the exception occurred. --- **Note:** You are encouraged to make appropriate use of the `traceback` module\'s functions and classes, such as `traceback.TracebackException`, `traceback.format_exception()`, and `traceback.StackSummary`.","solution":"import traceback def exception_traceback_handler(user_code: str, limit: int = None, capture_locals: bool = False) -> str: Executes the provided user code and returns the formatted traceback information if any exceptions occur. result = \'\' try: exec(user_code) except Exception as e: tb = e.__traceback__ tb_exception = traceback.TracebackException(type(e), e, tb, limit=limit, capture_locals=capture_locals) result = \'\'.join(tb_exception.format()) return result"},{"question":"# Advanced Coding Assessment Question **Objective**: Implement a Python function that uses generators to process a sequence of data efficiently. **Problem Statement**: You are tasked with creating a custom generator function and a utility function that works with this generator. The generator will yield numbers from an input list that are divisible by a given divisor. The utility function will consume this generator and return the sum of these numbers. **Input Format**: 1. A list of integers called `numbers`. 2. An integer `divisor`. **Output Format**: - An integer representing the sum of all numbers in the list that are divisible by the given divisor. **Function Signature**: ```python def divisible_generator(numbers: list, divisor: int): A generator function that yields numbers from the input list that are divisible by the given divisor. Arguments: numbers : list : a list of integers divisor : int : the divisor to check divisibility Yields: int : numbers from the input list that are divisible by the divisor pass def sum_of_divisibles(numbers: list, divisor: int) -> int: A utility function that sums up numbers from the list that are divisible by the given divisor using the divisible_generator. Arguments: numbers : list : a list of integers divisor : int : the divisor to check divisibility Returns: int : the sum of numbers divisible by the divisor pass ``` **Constraints**: - Do not use any third-party libraries. - The `numbers` list will contain at most (10^6) elements. - Each integer in the `numbers` list will be between (-10^9) and (10^9). - The `divisor` will be a non-zero integer between (-10^9) and (10^9). **Example**: ```python # Example 1 numbers = [10, 20, 33, 46, 50] divisor = 10 assert sum_of_divisibles(numbers, divisor) == 80 # 10 + 20 + 50 = 80 # Example 2 numbers = [1, 2, 3, 4, 5, 6] divisor = 2 assert sum_of_divisibles(numbers, divisor) == 12 # 2 + 4 + 6 = 12 ``` **Guidelines**: 1. You must implement the `divisible_generator` function as a generator that yields numbers divisible by the divisor. 2. You must implement the `sum_of_divisibles` function to use this generator to calculate the sum. 3. Ensure your code is efficient and can handle large lists of numbers within a reasonable time frame.","solution":"def divisible_generator(numbers: list, divisor: int): A generator function that yields numbers from the input list that are divisible by the given divisor. Arguments: numbers : list : a list of integers divisor : int : the divisor to check divisibility Yields: int : numbers from the input list that are divisible by the divisor for number in numbers: if number % divisor == 0: yield number def sum_of_divisibles(numbers: list, divisor: int) -> int: A utility function that sums up numbers from the list that are divisible by the given divisor using the divisible_generator. Arguments: numbers : list : a list of integers divisor : int : the divisor to check divisibility Returns: int : the sum of numbers divisible by the divisor return sum(divisible_generator(numbers, divisor))"},{"question":"Objective: To assess your understanding and application of the Python `multiprocessing` package, particularly focusing on creating and managing multiple processes, inter-process communication, and handling synchronization. Problem Statement: You are tasked with processing a list of numbers to compute their square roots in parallel. Each process should pick a number from the shared list, compute its square root, and store the result in a shared data structure. You will implement a function named `parallel_sqrt_computation` that: 1. Takes a list of integers and a number of worker processes as input. 2. Creates a pool of worker processes to compute the square roots in parallel. 3. Utilizes shared data structures to store the results. 4. Ensures proper synchronization to avoid race conditions. Input: - `numbers`: a list of integers. - `num_workers`: an integer specifying the number of worker processes. Output: - A list of tuples where each tuple contains the original number and its square root. Constraints: - The list of numbers will have a maximum length of 10,000. - The number of worker processes will be between 1 and 16. - You must use the `multiprocessing` package for parallelism. - Use shared memory or other synchronization primitives as needed to ensure thread-safe operations. Example: ```python from math import isclose numbers = [4, 16, 25, 36, 49] num_workers = 3 results = parallel_sqrt_computation(numbers, num_workers) # Expected output: [(4, 2.0), (16, 4.0), (25, 5.0), (36, 6.0), (49, 7.0)] for number, sqrt_value in results: assert isclose(sqrt_value, number ** 0.5) ``` Implementation Details: You may consider the following steps for your implementation: 1. Create a shared data structure to store the results `[(number, sqrt(number))]`. 2. Define a worker function that computes the square root of a given number and stores it in the shared data structure. 3. Create a pool of worker processes and assign tasks to them. 4. Ensure proper synchronization to avoid any race conditions when accessing shared data. 5. Collect the results from the shared data structure and return them as a list. Note: It is crucial to handle synchronization properly to ensure safe access to the shared data structure. Utilize locks and other synchronization primitives where necessary. Implement the `parallel_sqrt_computation` function below: ```python import multiprocessing from math import sqrt def parallel_sqrt_computation(numbers, num_workers): # Your code here return results # Example usage if __name__ == \\"__main__\\": numbers = [4, 16, 25, 36, 49] num_workers = 3 print(parallel_sqrt_computation(numbers, num_workers)) ``` Make sure your code is well-structured, handles edge cases, and includes appropriate synchronization mechanisms.","solution":"import multiprocessing from math import sqrt def compute_sqrt(number, result_dict, lock): with lock: result_dict[number] = sqrt(number) def parallel_sqrt_computation(numbers, num_workers): manager = multiprocessing.Manager() result_dict = manager.dict() lock = manager.Lock() processes = [] # Create worker processes for number in numbers: process = multiprocessing.Process(target=compute_sqrt, args=(number, result_dict, lock)) processes.append(process) process.start() if len(processes) == num_workers: for p in processes: p.join() processes = [] # Ensure all remaining processes are also joined for p in processes: p.join() # Collect results results = [(number, result_dict[number]) for number in numbers] return results"},{"question":"Objective: Demonstrate your understanding of Seaborn\'s plotting capabilities and the ability to create complex visualizations using the seaborn.objects interface. Problem Statement: You are provided with two datasets, `penguins` and `diamonds`, from Seaborn’s built-in datasets. Your task is to create a combined visualization that showcases two facets of the provided datasets in a single figure. Instructions: 1. **Load the datasets**: - `penguins` dataset - `diamonds` dataset 2. **Create a combined plot with the following specifications**: - The first plot should use the `penguins` dataset and visualize `species` vs. `body_mass_g`. - Use dots to represent individual points with jitter for better visibility. - Overlay a range indicating the 25th and 75th percentiles for each species. - The second plot should use the `diamonds` dataset and visualize `carat` vs. `clarity`. - Use dots to represent individual points with jitter for better visibility. - Overlay a range indicating the 25th and 75th percentiles for each clarity category. - Arrange these two plots vertically to create a single combined output. Constraints: - Use the seaborn.objects interface for creating the plots. - Do not use any external plotting libraries apart from Seaborn and Matplotlib. Expected Output: A figure comprising two vertically stacked plots as described above. Code Template: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load datasets penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Create the first plot for penguins dataset plot_penguins = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Create the second plot for diamonds dataset plot_diamonds = ( so.Plot(diamonds, \\"carat\\", \\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=0.25)) ) # Arrange the plots vertically and display them fig, ax = plt.subplots(2, 1, figsize=(12, 10)) plot_penguins.on(ax=ax[0]) plot_diamonds.on(ax=ax[1]) plt.show() ``` Evaluation Criteria: - Correct implementation of the specified visualization. - Use of appropriate Seaborn functionality for jittering dots and overlaying ranges. - Proper arrangement of plots in a single combined output. - Code readability and adherence to the provided template.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load datasets penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Create the first plot for penguins dataset plot_penguins = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(x=0.2)) .add(so.Range(), so.Perc([25, 75])) ) # Create the second plot for diamonds dataset plot_diamonds = ( so.Plot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter(y=0.2)) .add(so.Range(), so.Perc([25, 75])) ) # Arrange the plots vertically and display them fig, ax = plt.subplots(2, 1, figsize=(12, 10)) plot_penguins.on(ax[0]) plot_diamonds.on(ax[1]) plt.tight_layout() plt.show()"},{"question":"**Objective**: Demonstrate your understanding of Seaborn\'s functionality for controlling figure aesthetics and context by creating a customized visualization. **Task**: 1. Create a function named `custom_plot` that: - Accepts an integer `n` for the number of sine waves to plot. - Accepts a string `style` representing one of Seaborn\'s predefined styles. - Accepts a string `context` representing one of Seaborn\'s predefined contexts. - Optionally accepts a dictionary `custom_rc` for any additional custom parameters to modify the chosen style. - Outputs a customized plot displaying `n` sine waves with the provided style, context, and any custom parameters specified. 2. Use the provided `sinplot` function as a helper to generate the sine waves. **Function Signature**: ```python def custom_plot(n: int, style: str, context: str, custom_rc: dict = None) -> None: ``` **Constraints**: - `n` must be a positive integer. - `style` must be one of `[\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", \\"ticks\\"]`. - `context` must be one of `[\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"]`. **Example Usage**: ```python # Example 1: Basic custom plot with no additional parameters custom_plot(10, \\"whitegrid\\", \\"talk\\") # Example 2: Custom plot with additional parameters custom_rc = {\\"axes.facecolor\\": \\".9\\", \\"grid.color\\": \\"0.5\\"} custom_plot(10, \\"darkgrid\\", \\"notebook\\", custom_rc) ``` **Expected Output**: - The first example should generate a plot with 10 sine waves using the \\"whitegrid\\" style and the \\"talk\\" context. - The second example should generate a plot with 10 sine waves using the \\"darkgrid\\" style and the \\"notebook\\" context, with the background color set to a light grey and grid lines colored a medium grey. **Performance Requirements**: - The implementation should efficiently handle the creation and rendering of plots with up to 50 sine waves. - Ensure the custom_rc parameters are applied correctly without causing errors or conflicts with predefined styles or contexts.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * 0.5) * (n - i) * flip) def custom_plot(n: int, style: str, context: str, custom_rc: dict = None) -> None: Generates a custom plot with sine waves. Parameters: n (int): Number of sine waves to plot. style (string): One of seaborn\'s predefined styles [\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", \\"ticks\\"] context (string): One of seaborn\'s predefined contexts [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] custom_rc (dict, optional): Custom rc parameters to modify the chosen style. Defaults to None. Returns: None if style not in [\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", \\"ticks\\"]: raise ValueError(\\"Invalid style provided\\") if context not in [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"]: raise ValueError(\\"Invalid context provided\\") if n <= 0: raise ValueError(\\"Number of sine waves must be a positive integer\\") sns.set_style(style) sns.set_context(context, rc=custom_rc) plt.figure() sinplot(n) plt.show()"},{"question":"**Subprocess Management in Python** You are tasked with writing a Python script to demonstrate your understanding of the `subprocess` module. The script should perform the following tasks: 1. **Run a Command**: - Run the command `echo \\"Hello, World!\\"` using `subprocess.run`. Capture the output and return code of the command. - Print the output and the return code. 2. **Directory Listing**: - List the contents of the current directory using `subprocess.Popen`. Capture the output and error streams. - Print the captured output. If there are any errors, print the error message. 3. **Handle Non-zero Exit Code**: - Try to run a command that exits with a non-zero status (such as `exit 1`). Ensure that the script handles this gracefully by catching the appropriate exception and printing an error message. 4. **Custom Environment Variable**: - Run a simple Python script that prints environment variables using `print(os.environ)`. Pass a custom environment variable to this subprocess and ensure it gets printed. - Print the output of the subprocess execution. # Requirements: - Use `subprocess.run` for straightforward command execution. - Use `subprocess.Popen` for more complex process management involving I/O streams. - Handle raised exceptions using appropriate exception handling techniques. - Ensure the script is robust and handles edge cases gracefully. # Input and Output Formats: - **Input**: No direct input is required from the user. - **Output**: The script should output: 1. The result of the `echo` command. 2. The list of files in the current directory or an error message. 3. The return code and error message for the command with a non-zero exit. 4. The environment variables printed by the subprocess. # Constraints: - Ensure your script works on both Windows and Unix-based systems. - The subprocesses should be cleaned up properly to avoid any resource leaks. # Example Output: ``` Output of echo command: Hello, World! Return code of echo command: 0 Contents of the current directory: file1.txt file2.py ... (other files) Error running the command \'exit 1\': Command \'exit 1\' returned non-zero exit status 1. Environment variables in subprocess: SOME_VAR=some_value ... (other environment variables) ``` **Write your script below:** ```python import subprocess import os # Task 1: Run a command using subprocess.run def run_echo_command(): result = subprocess.run([\\"echo\\", \\"Hello, World!\\"], capture_output=True, text=True) print(f\\"Output of echo command: {result.stdout.strip()}\\") print(f\\"Return code of echo command: {result.returncode}\\") # Task 2: Directory listing using subprocess.Popen def list_directory_contents(): try: with subprocess.Popen([\\"ls\\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) as proc: output, error = proc.communicate() if error: print(f\\"Error: {error.strip()}\\") else: print(\\"Contents of the current directory:\\") print(output) except Exception as e: print(f\\"Failed to list directory contents: {e}\\") # Task 3: Handle non-zero exit code def run_non_zero_exit_command(): try: subprocess.run([\\"exit\\", \\"1\\"], check=True, shell=True) except subprocess.CalledProcessError as e: print(f\\"Error running the command \'exit 1\': {e}\\") # Task 4: Custom environment variable def run_with_custom_env(): custom_env = os.environ.copy() custom_env[\\"SOME_VAR\\"] = \\"some_value\\" result = subprocess.run([\\"python\\", \\"-c\\", \\"import os; print(os.environ)\\"], capture_output=True, text=True, env=custom_env) print(\\"Environment variables in subprocess:\\") print(result.stdout) # Execute the tasks if __name__ == \'__main__\': run_echo_command() list_directory_contents() run_non_zero_exit_command() run_with_custom_env() ```","solution":"import subprocess import os # Task 1: Run a command using subprocess.run def run_echo_command(): result = subprocess.run([\\"echo\\", \\"Hello, World!\\"], capture_output=True, text=True) return (result.stdout.strip(), result.returncode) # Task 2: Directory listing using subprocess.Popen def list_directory_contents(): try: with subprocess.Popen([\\"ls\\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) as proc: output, error = proc.communicate() if proc.returncode != 0: return (None, error.strip()) return (output.strip(), None) except Exception as e: return (None, str(e)) # Task 3: Handle non-zero exit code def run_non_zero_exit_command(): try: subprocess.run([\\"exit\\", \\"1\\"], check=True, shell=True) except subprocess.CalledProcessError as e: return (e.returncode, str(e)) # Task 4: Custom environment variable def run_with_custom_env(): custom_env = os.environ.copy() custom_env[\\"SOME_VAR\\"] = \\"some_value\\" result = subprocess.run([\\"python\\", \\"-c\\", \\"import os; print(os.environ[\'SOME_VAR\'])\\"], capture_output=True, text=True, env=custom_env) return result.stdout.strip() # Main block for executing the tasks if __name__ == \'__main__\': echo_output, echo_returncode = run_echo_command() print(f\\"Output of echo command: {echo_output}\\") print(f\\"Return code of echo command: {echo_returncode}\\") dir_output, dir_error = list_directory_contents() if dir_error: print(f\\"Error listing directory contents: {dir_error}\\") else: print(f\\"Contents of the current directory:n{dir_output}\\") non_zero_code, non_zero_error = run_non_zero_exit_command() print(f\\"Error running the command \'exit 1\': {non_zero_error}\\") env_output = run_with_custom_env() print(f\\"Environment variable in subprocess: {env_output}\\")"},{"question":"**Context**: In this coding assessment, your goal is to demonstrate the ability to perform feature extraction using `DictVectorizer` and `CountVectorizer`, followed by integrating these features into a machine learning pipeline. The dataset includes mixed types of data such as text and dictionaries. **Objective**: The task is to implement a function that performs feature extraction from a given dataset and uses these features to train a simple machine learning model to perform classification. The dataset contains text reviews and additional categorical features about the reviews. # Problem Statement You are provided with a dataset consisting of text reviews and additional categorical attributes. You need to write a function `text_and_dict_vectorization` that: 1. Extracts features from the text reviews using `CountVectorizer`. 2. Extracts features from the categorical attributes using `DictVectorizer`. 3. Combines these features and uses them to train a logistic regression classifier. 4. Returns the trained model and the combined feature matrix. # Specifications 1. **Function Name**: `text_and_dict_vectorization` 2. **Input**: - `reviews`: List of strings, where each string is a text review. - `attributes`: List of dictionaries, where each dictionary contains categorical information about the corresponding review (keys and values are strings). - `labels`: List of integers, where each integer represents the class label of the corresponding review. 3. **Output**: - `model`: Trained logistic regression model. - `combined_features`: SciPy sparse matrix or NumPy array containing the combined feature matrix after vectorization. # Constraints - You should use `CountVectorizer` for vectorizing text reviews. - You should use `DictVectorizer` for vectorizing categorical attributes. - Use logistic regression from `sklearn.linear_model`. - Ensure your function works efficiently for datasets up to 10,000 reviews. # Example ```python from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction import DictVectorizer import numpy as np def text_and_dict_vectorization(reviews, attributes, labels): # Initialize the vectorizers text_vectorizer = CountVectorizer() dict_vectorizer = DictVectorizer() # Fit and transform the text reviews text_features = text_vectorizer.fit_transform(reviews) # Fit and transform the categorical attributes dict_features = dict_vectorizer.fit_transform(attributes) # Combine the text and dict features from scipy.sparse import hstack combined_features = hstack([text_features, dict_features]) # Train the logistic regression model model = LogisticRegression() model.fit(combined_features, labels) return model, combined_features # Example usage reviews = [ \\"The movie was fantastic!\\", \\"I did not like the movie.\\", \\"It was an average film.\\" ] attributes = [ {\'genre\': \'action\', \'length\': \'120\'}, {\'genre\': \'drama\', \'length\': \'140\'}, {\'genre\': \'comedy\', \'length\': \'100\'} ] labels = [1, 0, 1] model, features = text_and_dict_vectorization(reviews, attributes, labels) print(features.toarray()) ``` **Evaluation**: - Your function will be tested with various datasets of mixed data types. - Ensure the feature extraction is performed correctly and the logistic regression model is trained without errors. - The output of the function should match the expected format and properly integrate the different types of features.","solution":"from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction import DictVectorizer from scipy.sparse import hstack def text_and_dict_vectorization(reviews, attributes, labels): Perform feature extraction from text reviews and categorical attributes and train a logistic regression classifier. Args: - reviews (List of str): Text reviews. - attributes (List of dict): Categorical information about the reviews. - labels (List of int): Class labels for the reviews. Returns: - model (LogisticRegression): Trained logistic regression model. - combined_features (scipy.sparse matrix): Combined feature matrix. # Initialize the vectorizers text_vectorizer = CountVectorizer() dict_vectorizer = DictVectorizer() # Fit and transform the text reviews text_features = text_vectorizer.fit_transform(reviews) # Fit and transform the categorical attributes dict_features = dict_vectorizer.fit_transform(attributes) # Combine the text and dict features combined_features = hstack([text_features, dict_features]) # Train the logistic regression model model = LogisticRegression() model.fit(combined_features, labels) return model, combined_features"},{"question":"**Objective:** Write a Python function `custom_traceback_summary` to capture the traceback of an exception and return a summary of the error including the following information: 1. The file name where the error occurred. 2. The line number of the error. 3. The function name where the error occurred. 4. The actual source line of code where the error occurred. 5. A formatted message of the exception type and exception message. Utilize the `traceback` and `sys` modules to achieve this. **Function Signature:** ```python def custom_traceback_summary() -> str: ``` **Expected Input and Output:** - The function doesn\'t take any parameters. - When an exception occurs, it should capture relevant information and output it as a formatted string. **Constraints:** - You must use the `extract_tb` function to get traceback information. - The summary must show up to the last 10 frames (if more are available). - The code should work for any general exception. **Example:** ```python def faulty_function(): return 1 / 0 try: faulty_function() except ZeroDivisionError: summary = custom_traceback_summary() print(summary) ``` **Expected Output:** ``` Traceback Summary: File \\"filename.py\\", line 20, in faulty_function return 1 / 0 ZeroDivisionError: division by zero ``` **Hint:** - Pay attention to methods like `traceback.extract_tb()`, `traceback.format_exception_only()`, and object attributes of the `FrameSummary`. - You may find `sys.exc_info()` useful. **Performance Requirements:** - The function should be efficient and manage resources effectively, considering it may potentially be used to handle a large number of frames. You are allowed to import standard Python libraries (such as `sys` and `traceback`) to aid in your implementation.","solution":"import traceback import sys def custom_traceback_summary() -> str: exc_type, exc_value, exc_tb = sys.exc_info() if exc_type is None: return \\"No exception\\" tb_list = traceback.extract_tb(exc_tb, limit=10) summary = \\"Traceback Summary:n\\" for frame in tb_list: summary += f\'File \\"{frame.filename}\\", line {frame.lineno}, in {frame.name}n\' summary += f\' {frame.line}n\' summary += \'\'.join(traceback.format_exception_only(exc_type, exc_value)) return summary"},{"question":"# Anomaly Detection with Scikit-learn **Problem Statement:** Your task is to design a system that can detect anomalies in a dataset using one of the Scikit-learn functionality for novelty detection. Specifically, you\'ll use the `ensemble.IsolationForest` for this task. Given a dataset, your goal is to: 1. Train the Isolation Forest model using the given training data. 2. Predict whether instances in the test data are anomalies or not. 3. Output the anomaly scores for each instance in the test data. **Instructions:** 1. **Load the Data:** - You will receive two pandas DataFrames: - `df_train`: Training data containing only inliers. - `df_test`: Test data that may contain anomalies. 2. **Model Training:** - Train the `IsolationForest` model on `df_train`. 3. **Prediction:** - Use the trained model to predict if instances in `df_test` are inliers (1) or outliers (-1). 4. **Scoring:** - For each instance in `df_test`, compute the anomaly score. 5. **Output:** - Return a pandas DataFrame with the following columns: - The original columns from `df_test`. - A new column `anomaly` indicating if an instance is an inlier (1) or an outlier (-1). - A new column `anomaly_score` containing the raw anomaly scores. **Function Signature:** ```python import pandas as pd from sklearn.ensemble import IsolationForest def detect_anomalies(df_train: pd.DataFrame, df_test: pd.DataFrame) -> pd.DataFrame: # Implement the function here pass ``` **Example:** ```python # Sample input df_train = pd.DataFrame({ \'feature1\': [1, 2, 3, 4, 5], \'feature2\': [10, 20, 30, 40, 50] }) df_test = pd.DataFrame({ \'feature1\': [1, 2, 6, 4, 7], \'feature2\': [11, 19, 70, 41, 90] }) # Expected output format (values will differ based on the model) { \'feature1\': [1, 2, 6, 4, 7], \'feature2\': [11, 19, 70, 41, 90], \'anomaly\': [1, 1, -1, 1, -1], \'anomaly_score\': [-0.1, -0.12, 0.8, -0.08, 0.9] } ``` **Constraints:** - You can assume the training data is clean without anomalies. - The data can have any number of features, and the values can vary widely. - Handle the data efficiently to work with large datasets if required. **Grading Criteria:** - Correct implementation of model training and prediction. - Proper handling of data input and output formats as specified. - Robustness of the solution to work with different datasets. - Code efficiency and readability.","solution":"import pandas as pd from sklearn.ensemble import IsolationForest def detect_anomalies(df_train: pd.DataFrame, df_test: pd.DataFrame) -> pd.DataFrame: Detect anomalies in the test dataset using Isolation Forest trained on the training dataset. Args: - df_train (pd.DataFrame): Training data containing only inliers. - df_test (pd.DataFrame): Test data that may contain anomalies. Returns: - pd.DataFrame: DataFrame with original test data columns plus \'anomaly\' and \'anomaly_score\' columns. # Train the IsolationForest model model = IsolationForest().fit(df_train) # Predict anomalies in the test dataset predictions = model.predict(df_test) # Compute anomaly scores scores = model.decision_function(df_test) # Create the output DataFrame df_result = df_test.copy() df_result[\'anomaly\'] = predictions df_result[\'anomaly_score\'] = scores return df_result"},{"question":"# Python Datetime C API Implementation You are required to implement a Python function that utilizes the C API macros provided by Python\'s datetime module to manipulate datetime objects. Your task is to: 1. **Create** a new datetime object with a specified date and time. 2. **Extract** the year, month, day, hour, minute, second, and microsecond from this datetime object and return them as a tuple. 3. **Create** a new time object with a specified time. 4. **Extract** the hour, minute, second, and microsecond from this time object and return them as a tuple. Your function should be named `manipulate_datetime` and should follow the input and output format specified below. Input - `datetime_params`: A tuple `(year, month, day, hour, minute, second, microsecond)` specifying the date and time for the datetime object. - `time_params`: A tuple `(hour, minute, second, microsecond)` specifying the time for the time object. Output - A tuple `((year, month, day, hour, minute, second, microsecond), (hour, minute, second, microsecond))` containing the extracted date and time fields. Constraints - The year should be a positive int. - The month should be an int between 1 and 12. - The day should be an int between 1 and 31. - The hour should be an int between 0 and 23. - The minute should be an int between 0 and 59. - The second should be an int between 0 and 59. - The microsecond should be an int between 0 and 999999. Example ```python def manipulate_datetime(datetime_params, time_params): # Your code here # Example usage: datetime_params = (2022, 9, 21, 14, 30, 45, 100000) time_params = (14, 30, 45, 100000) result = manipulate_datetime(datetime_params, time_params) print(result) # Output: ((2022, 9, 21, 14, 30, 45, 100000), (14, 30, 45, 100000)) ``` You are expected to implement the solution using the datetime C API as per the provided documentation.","solution":"import datetime def manipulate_datetime(datetime_params, time_params): Manipulate datetime and time objects using the provided parameters. :param datetime_params: Tuple containing year, month, day, hour, minute, second, microsecond :param time_params: Tuple containing hour, minute, second, microsecond :return: A tuple containing extracted date and time fields # Create a datetime object dt = datetime.datetime(*datetime_params) # Extract the datetime fields dt_fields = (dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond) # Create a time object t = datetime.time(*time_params) # Extract the time fields t_fields = (t.hour, t.minute, t.second, t.microsecond) return (dt_fields, t_fields)"},{"question":"Pandas Coding Assessment # Objective Assess students\' understanding of pandas `GroupBy` functionality, including splitting, aggregating, transforming, and filtering data. # Problem Statement You are given a dataset containing information about various products sold in different stores over a period. The dataset has the following columns: - `store`: the store where the product was sold (categorical) - `product_category`: the category of the product (categorical) - `date`: the date of sale (datetime) - `units_sold`: the number of units sold (integer) - `revenue`: the revenue generated from the sale (float) Implement a function `analyze_sales_data(data)` that performs the following operations: 1. **Aggregation**: Compute the total units sold and total revenue for each combination of store and product category. 2. **Transformation**: Calculate the cumulative sum of units sold for each product category within each store. 3. **Filtering**: Remove any groups where the total revenue is less than 1000. 4. **Multiple Function Application**: For each group resulting from the filtering step (store and product category), calculate the mean and standard deviation of the revenue. 5. **Selection**: Select and return the data for the store and product category with the highest mean revenue. # Input - `data`: A pandas DataFrame containing the columns `store`, `product_category`, `date`, `units_sold`, `revenue`. # Output - A pandas DataFrame containing the rows corresponding to the store and product category with the highest mean revenue from step 5. # Constraints - The `data` DataFrame will contain at least one row of data. - Ensure that the function performs efficiently on large datasets with millions of rows. # Example ```python import pandas as pd # Sample data data = pd.DataFrame({ \'store\': [\'Store_A\', \'Store_A\', \'Store_B\', \'Store_B\', \'Store_C\', \'Store_C\', \'Store_A\'], \'product_category\': [\'Electronics\', \'Clothing\', \'Electronics\', \'Furniture\', \'Clothing\', \'Electronics\', \'Furniture\'], \'date\': pd.to_datetime([\'2021-01-01\', \'2021-01-02\', \'2021-01-01\', \'2021-01-02\', \'2021-01-01\', \'2021-01-03\', \'2021-01-04\']), \'units_sold\': [5, 2, 3, 6, 7, 4, 5], \'revenue\': [500, 200, 300, 600, 700, 400, 500] }) result = analyze_sales_data(data) print(result) ``` Expected output (depending on the sample data provided): A DataFrame containing rows related to the store and product category combination with the highest mean revenue. # Solution Template ```python import pandas as pd def analyze_sales_data(data): # Step 1: Aggregation aggregated = data.groupby([\'store\', \'product_category\']).agg(total_units_sold=(\'units_sold\', \'sum\'), total_revenue=(\'revenue\', \'sum\')).reset_index() # Step 2: Transformation data[\'cumulative_units_sold\'] = data.groupby([\'store\', \'product_category\'])[\'units_sold\'].cumsum() # Step 3: Filtering filtered_groups = aggregated[aggregated[\'total_revenue\'] >= 1000] # The groups to keep valid_groups = filtered_groups[[\'store\', \'product_category\']] # Filter the original data data = data.merge(valid_groups, on=[\'store\', \'product_category\']) # Step 4: Multiple Function Application result_grouped = data.groupby([\'store\', \'product_category\']).agg(mean_revenue=(\'revenue\', \'mean\'), std_revenue=(\'revenue\', \'std\')).reset_index() # Step 5: Selection highest_mean_revenue_group = result_grouped.loc[result_grouped[\'mean_revenue\'].idxmax()] # Select the data for the store and product category with the highest mean revenue highest_mean_revenue_data = data[(data[\'store\'] == highest_mean_revenue_group[\'store\']) & (data[\'product_category\'] == highest_mean_revenue_group[\'product_category\'])] return highest_mean_revenue_data ``` # Notes Make sure that the provided solution runs efficiently for large datasets and includes necessary comments to explain each step of the process.","solution":"import pandas as pd def analyze_sales_data(data): # Step 1: Aggregation aggregated = data.groupby([\'store\', \'product_category\']).agg( total_units_sold=(\'units_sold\', \'sum\'), total_revenue=(\'revenue\', \'sum\') ).reset_index() # Step 2: Transformation data[\'cumulative_units_sold\'] = data.groupby([\'store\', \'product_category\'])[\'units_sold\'].cumsum() # Step 3: Filtering filtered_groups = aggregated[aggregated[\'total_revenue\'] >= 1000] # The groups to keep valid_groups = filtered_groups[[\'store\', \'product_category\']] # Filter the original data data = data.merge(valid_groups, on=[\'store\', \'product_category\']) # Step 4: Multiple Function Application result_grouped = data.groupby([\'store\', \'product_category\']).agg( mean_revenue=(\'revenue\', \'mean\'), std_revenue=(\'revenue\', \'std\') ).reset_index() # Step 5: Selection highest_mean_revenue_group = result_grouped.loc[result_grouped[\'mean_revenue\'].idxmax()] # Select the data for the store and product category with the highest mean revenue highest_mean_revenue_data = data[ (data[\'store\'] == highest_mean_revenue_group[\'store\']) & (data[\'product_category\'] == highest_mean_revenue_group[\'product_category\']) ] return highest_mean_revenue_data"},{"question":"Coding Assessment Question In Python, the `builtins` module provides access to all built-in functions and constants. Sometimes, when redefining a built-in function, we may still need access to the original built-in version. You are required to write a custom wrapper class that enhances the functionality of a built-in function. # Task: Implement a custom class `VerboseList` that wraps around Python\'s built-in list type. This wrapper should extend the functionality of a list by logging all the operations performed on it. Specifically, you need to: 1. Implement the `VerboseList` class with the following methods: - `append(element)`: Add an element to the list. - `extend(iterable)`: Extend the list by appending elements from the iterable. - `remove(element)`: Remove the first occurrence of the element from the list. - `pop(index)`: Remove and return the element at the given index. If no index is specified, removes and returns the last element. - `get_logs()`: Returns a list of all operations performed on the `VerboseList`. 2. Create an internal list to store the operations performed on the `VerboseList`. Each entry in this list should be a string describing the operation, such as `\\"append 3\\"` or `\\"pop 4\\"`. 3. Ensure that the custom methods do not interfere with the original functionalities of the built-in list methods. # Example Usage: ```python from builtins import list as builtins_list class VerboseList: def __init__(self, initial_list=[]): self._list = builtins_list(initial_list) self._log = [] def append(self, element): self._log.append(f\\"append {element}\\") self._list.append(element) def extend(self, iterable): self._log.append(f\\"extend {iterable}\\") self._list.extend(iterable) def remove(self, element): self._log.append(f\\"remove {element}\\") self._list.remove(element) def pop(self, index=-1): self._log.append(f\\"pop {index}\\") return self._list.pop(index) def get_logs(self): return self._log # Example vlist = VerboseList([1, 2, 3]) vlist.append(4) # Append 4 vlist.extend([5, 6]) # Extend with [5, 6] vlist.remove(2) # Remove element 2 vlist.pop() # Pop last element print(vlist.get_logs()) # Output: [\'append 4\', \'extend [5, 6]\', \'remove 2\', \'pop -1\'] ``` # Constraints 1. You may assume all inputs (elements and iterables) are valid and do not need additional validation. 2. Efficiency is not a primary concern, but the implementation should avoid unnecessary complexity. # Input/Output Formats: - The class should be initialized with a list (default empty). - Operations on the `VerboseList` should modify the internal list and log the operations. - `get_logs` method should return a list of operations performed as strings.","solution":"from builtins import list as builtins_list class VerboseList: def __init__(self, initial_list=[]): self._list = builtins_list(initial_list) self._log = [] def append(self, element): self._log.append(f\\"append {element}\\") self._list.append(element) def extend(self, iterable): self._log.append(f\\"extend {iterable}\\") self._list.extend(iterable) def remove(self, element): self._log.append(f\\"remove {element}\\") self._list.remove(element) def pop(self, index=-1): self._log.append(f\\"pop {index}\\") return self._list.pop(index) def get_logs(self): return self._log def __repr__(self): return repr(self._list)"},{"question":"Objective Utilize the `pkgutil` module to implement a function that lists and categorizes all modules, submodules, and subpackages available in the specified packages within `sys.path`. Description Implement a function `list_package_content(package_name: str) -> dict` that takes a package name as input and returns a dictionary with the following format: ```python { \\"submodules\\": [list of submodules], \\"subpackages\\": [list of subpackages] } ``` - **submodules**: List of all submodules within the specified package. A submodule is considered any module that does not contain submodules of its own. - **subpackages**: List of all subpackages within the specified package. A subpackage is a package within the given package and contains its own submodules or subpackages. Guidelines 1. **Input**: - `package_name` (str): The name of the package to analyze. 2. **Output**: - Returns a dictionary with two keys, \\"submodules\\" and \\"subpackages\\". The values associated with these keys are lists of strings representing the names of the submodules and subpackages, respectively. 3. **Constraints**: - The function should handle packages not existing or import errors gracefully by returning an empty dictionary. 4. **Performance**: - The function should efficiently utilize the `pkgutil.iter_modules` and `pkgutil.walk_packages` methods to iterate and identify modules and subpackages. Example Usage ```python result = list_package_content(\'os\') print(result) # Expected output (example): # { # \\"submodules\\": [ # \\"os.path\\", \\"os.nth\\", ... # ], # \\"subpackages\\": [ # \\"os.access\\", \\"os.stat_result\\", ... # ] # } ``` Hints - Utilize `pkgutil.iter_modules()` to identify submodules and subpackages. - Utilize `pkgutil.walk_packages()` to iterate recursively through the packages. - Consider importing the package to access its path for accurate iteration. Additional Information If the provided package does not exist or cannot be imported, the function should return an empty dictionary.","solution":"import pkgutil import importlib import sys def list_package_content(package_name: str) -> dict: List and categorize all modules, submodules, and subpackages within the specified package. Args: - package_name (str): The name of the package to analyze. Returns: - dict: Dictionary with \\"submodules\\" and \\"subpackages\\" as keys. try: package = importlib.import_module(package_name) except ModuleNotFoundError: return {} package_path = package.__path__ package_info = { \\"submodules\\": [], \\"subpackages\\": [] } for finder, name, is_pkg in pkgutil.walk_packages(package_path, package_name + \'.\'): if is_pkg: package_info[\\"subpackages\\"].append(name) else: package_info[\\"submodules\\"].append(name) return package_info"},{"question":"**Objective**: Demonstrate your understanding of Python\'s interactive mode, error handling, and customization modules. **Background**: In this task, you will write a Python script that performs the following: 1. Reads input interactively from the user. 2. Handles errors gracefully. 3. Customizes the startup behavior using environment variables and custom scripts. **Task**: 1. Write a Python script called `interactive_script.py` that: - Continuously prompts the user to input a number. Use a primary prompt `Enter a number: `. - If the user inputs a non-integer value, catch the exception and print `Invalid input! Please enter an integer.` and prompt the user again. - If the user inputs the integer `0`, raise a custom exception called `ZeroInputError` with the message `Zero is not allowed here!`. 2. Implement the custom exception class `ZeroInputError` in the same file. Ensure it extends the base `Exception` class. Handle this exception by printing the error message and continuing to prompt the user. 3. Create a customization script called `usercustomize.py` that: - Prints `Custom startup actions completed.` when imported. - Changes the primary prompt to `Your number: ` and the secondary prompt to `More input: `. - Sets an environment variable `CUSTOM_PROMPT_ACTIVE` to `True`. 4. In your `interactive_script.py`, check if the `CUSTOM_PROMPT_ACTIVE` environment variable is set. If so, print `Custom prompts activated.` before entering the input loop. **Constraints**: - Your solution should handle uninterrupted execution until the user interrupts the program manually (e.g., using Ctrl+C). - Your script should be executable directly in Unix-based systems using `#!/usr/bin/env python3.5`. **Instructions**: - Place your `interactive_script.py` and `usercustomize.py` files in the same directory. - Ensure all custom prompts and error messages are displayed correctly. - Run `interactive_script.py` from the terminal to test your implementation. **Example**: ``` ./interactive_script.py Custom prompts activated. Your number: not a number Invalid input! Please enter an integer. Your number: 0 Zero is not allowed here! Your number: 5 Your number: ^C ``` **Submission**: Submit the `interactive_script.py` and `usercustomize.py` files. Ensure they execute correctly and handle all specified behaviors.","solution":"#!/usr/bin/env python3.5 import os class ZeroInputError(Exception): Custom exception raised when the input is zero. pass def main(): if os.getenv(\'CUSTOM_PROMPT_ACTIVE\') == \'True\': print(\'Custom prompts activated.\') primary_prompt = \\"Your number: \\" else: primary_prompt = \\"Enter a number: \\" while True: try: user_input = input(primary_prompt) num = int(user_input) if num == 0: raise ZeroInputError(\'Zero is not allowed here!\') except ZeroInputError as ze: print(ze) except ValueError: print(\\"Invalid input! Please enter an integer.\\") except KeyboardInterrupt: print(\\"nExecution interrupted by the user.\\") break if __name__ == \\"__main__\\": main()"},{"question":"Objective Implement a function to manage read and write locks on a file using the `fcntl` module. Problem Statement You are required to implement a function `manage_file_lock` that performs the following operations on a file: 1. Acquires a shared lock (read lock) on the file. 2. Reads the entire content of the file. 3. Releases the shared lock. 4. Acquires an exclusive lock (write lock) on the file. 5. Appends the current timestamp to the file\'s content. 6. Releases the exclusive lock. The function should handle exceptions appropriately and ensure that locks are released even in case of errors. Function Signature ```python import fcntl import os import time def manage_file_lock(file_path: str) -> None: pass ``` Input - `file_path` (str): The path to the file to be locked and operated on. Output - The function does not return any value but modifies the file specified by `file_path`. Constraints - The file specified by `file_path` exists and is accessible. - The function must handle and release locks properly to avoid deadlocks. - The function must catch and handle exceptions, ensuring that locks are released even in case of errors. Example Consider a file `example.txt` with the following initial content: ``` Hello World! ``` After calling `manage_file_lock(\'example.txt\')`, the file content might look like this if the timestamp is `1633202103.6416044`: ``` Hello World! 1633202103.6416044 ``` Implementation Tips - Use `fcntl.flock` to acquire and release locks. - Use `os` and `time` modules to read file content and append the timestamp. - Ensure to release locks in a `finally` block to prevent deadlocks even if an exception occurs. Sample Code Snippet The following snippet can help you get started with acquiring and releasing file locks using the `fcntl` module: ```python import fcntl def read_file(file_path): with open(file_path, \'r\') as f: fcntl.flock(f, fcntl.LOCK_SH) try: content = f.read() finally: fcntl.flock(f, fcntl.LOCK_UN) return content def append_to_file(file_path, text): with open(file_path, \'a\') as f: fcntl.flock(f, fcntl.LOCK_EX) try: f.write(text) finally: fcntl.flock(f, fcntl.LOCK_UN) ``` Notes - Ensure your function passes all edge cases and handles file locks correctly to avoid corruption or deadlocks. - Properly format and test your code to ensure it works as expected.","solution":"import fcntl import os import time def manage_file_lock(file_path: str) -> None: Manages read and write locks on a file, reading the file\'s content and appending a timestamp. Args: - file_path (str): The path to the file to be locked and operated on. Returns: - None try: # Acquiring shared lock (read lock) and reading file content with open(file_path, \'r\') as file: fcntl.flock(file, fcntl.LOCK_SH) try: content = file.read() print(content) # For debugging purposes finally: fcntl.flock(file, fcntl.LOCK_UN) # Acquiring exclusive lock (write lock) and appending timestamp with open(file_path, \'a\') as file: fcntl.flock(file, fcntl.LOCK_EX) try: timestamp = str(time.time()) file.write(timestamp + \'n\') finally: fcntl.flock(file, fcntl.LOCK_UN) except OSError as e: print(f\\"An error occurred: {e}\\")"},{"question":"<|Analysis Begin|> The provided documentation is about the `unicodedata` module in Python, which provides access to the Unicode Character Database (UCD). This module allows for various operations to be performed on Unicode characters, such as looking up characters by name, retrieving their names, and obtaining their decimal, digit, numeric values, categories, bidirectional classes, canonical combining classes, east Asian width, mirrored properties, decomposition mappings, and normalization forms. The module also checks if a Unicode string is in a normal form. Key aspects for a coding assessment question would be understanding: 1. How to use the `unicodedata` functions to retrieve properties of Unicode characters. 2. How to handle exceptions and default values. 3. How to combine multiple functionalities to solve a problem. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Implement a function that validates a list of Unicode characters based on their properties and categories. This function will leverage the `unicodedata` module to check if each character in the list meets specified criteria. **Function Signature:** ```python def validate_unicode_chars(char_list: list, criteria: dict) -> list: ``` **Input:** - `char_list` (list): A list of single character strings to be validated. - `criteria` (dict): A dictionary specifying the properties and their required values. The dictionary can include: - \'category\': A set of one or more categories (e.g., {\'Lu\', \'Ll\'}) that the character must belong to. - \'is_digit\': A boolean indicating if the character should be a digit. - \'east_asian_width\': A set of one or more East Asian widths (e.g., {\'W\', \'F\'}) that the character must have. - \'mirrored\': A boolean indicating if the character should be mirrored. **Output:** - Returns a list of tuples. Each tuple contains a character from `char_list` and a boolean indicating if the character meets the criteria. **Constraints:** - If a character in `char_list` does not meet any criteria, it should be marked as `False`. - You may assume `char_list` contains valid single Unicode characters. - If a criterion isn\'t specified in `criteria`, do not validate against it. **Examples:** ```python import unicodedata def validate_unicode_chars(char_list, criteria): result = [] for char in char_list: is_valid = True if \'category\' in criteria: if unicodedata.category(char) not in criteria[\'category\']: is_valid = False if \'is_digit\' in criteria: if criteria[\'is_digit\'] != char.isdigit(): is_valid = False if \'east_asian_width\' in criteria: if unicodedata.east_asian_width(char) not in criteria[\'east_asian_width\']: is_valid = False if \'mirrored\' in criteria: if criteria[\'mirrored\'] != bool(unicodedata.mirrored(char)): is_valid = False result.append((char, is_valid)) return result # Example usage: print(validate_unicode_chars([\'A\', \'你\', \'5\', \'/\', \'u3000\'], { \'category\': {\'Lu\', \'Nd\'}, \'is_digit\': False, \'east_asian_width\': {\'W\'}, \'mirrored\': False })) # Output: [(\'A\', True), (\'你\', True), (\'5\', False), (\'/\', False), (\'u3000\', False)] ``` Use the `unicodedata` module to implement the validation logic based on the given criteria.","solution":"import unicodedata def validate_unicode_chars(char_list, criteria): Validate a list of Unicode characters based on specified criteria. Parameters: - char_list: A list of single character strings to be validated. - criteria: A dictionary specifying the properties and their required values. Returns: - A list of tuples. Each tuple contains a character and a boolean indicating if it meets the criteria. result = [] for char in char_list: is_valid = True if \'category\' in criteria: if unicodedata.category(char) not in criteria[\'category\']: is_valid = False if \'is_digit\' in criteria: if criteria[\'is_digit\'] != char.isdigit(): is_valid = False if \'east_asian_width\' in criteria: if unicodedata.east_asian_width(char) not in criteria[\'east_asian_width\']: is_valid = False if \'mirrored\' in criteria: if criteria[\'mirrored\'] != bool(unicodedata.mirrored(char)): is_valid = False result.append((char, is_valid)) return result"},{"question":"# Question You are provided with a dataset containing user activity on an online platform, including login times, activity durations, and categorical data related to user demographics. Using the pandas library, you are required to process and analyze this dataset to answer specific questions. Dataset Description - `user_id` (int): Unique identifier for each user. - `login_time` (string): Timestamp of user login in ISO format. - `duration` (string): Duration of user activity in the format \\"HH:MM:SS\\". - `age` (Nullable Integer): Age of the user, can be missing. - `premium_member` (Nullable Boolean): Indicates if the user is a premium member, can be missing. - `region` (Categorical): Region code of the user. Here is a sample of the dataset: ```csv user_id,login_time,duration,age,premium_member,region 1,2023-08-01T08:30:00,01:15:30,25,True,NA 2,2023-08-01T09:10:00,02:45:00,30,,EU 3,2023-08-01T10:22:00,00:50:15,NaN,False,AP 4,2023-08-01T11:35:00,01:05:45,27,True,NA ... ``` Tasks 1. **Data Loading**: Write a function to load the dataset into a pandas DataFrame. ```python def load_data(filepath): Load the dataset from the specified file path. Args: filepath (str): Path to the CSV file. Returns: pd.DataFrame: Loaded pandas DataFrame. pass ``` 2. **Data Preprocessing**: - Convert `login_time` to pandas datetime with timezone-awareness (UTC). - Convert `duration` to pandas Timedelta. - Convert `age` to pandas nullable Integer type. - Convert `premium_member` to pandas nullable Boolean type. - Ensure `region` is treated as a pandas categorical data type. ```python def preprocess_data(df): Preprocess the dataset: convert data types and handle missing values. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: Preprocessed DataFrame. pass ``` 3. **Data Analysis**: - Calculate the total duration of user activity for each region. - Compute the average age of premium members by region, ignoring missing values. - Identify the region with the highest count of premium members. ```python def analyze_data(df): Analyze the dataset to extract meaningful insights. Args: df (pd.DataFrame): Preprocessed DataFrame. Returns: tuple: Total duration of activity by region, average age of premium members by region, and the region with the highest count of premium members. pass ``` 4. **Output**: - Output the total activity duration by region as a pandas Series. - Output the average age of premium members by region as a pandas Series. - Output the region with the highest count of premium members as a string. Below are the expected function signatures and their descriptions. ```python def load_data(filepath): Load the dataset from the specified file path. Args: filepath (str): Path to the CSV file. Returns: pd.DataFrame: Loaded pandas DataFrame. # YOUR CODE HERE pass def preprocess_data(df): Preprocess the dataset: convert data types and handle missing values. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: Preprocessed DataFrame. # YOUR CODE HERE pass def analyze_data(df): Analyze the dataset to extract meaningful insights. Args: df (pd.DataFrame): Preprocessed DataFrame. Returns: tuple: Total duration of activity by region, average age of premium members by region, and the region with the highest count of premium members. # YOUR CODE HERE pass ``` **Constraints**: - The dataset size can vary but is typically less than 1 million rows. - Your solution should be efficient in terms of both time and memory. - Handle missing values appropriately based on the context provided. **Note**: You are encouraged to test your functions using various data samples to ensure accuracy.","solution":"import pandas as pd def load_data(filepath): Load the dataset from the specified file path. Args: filepath (str): Path to the CSV file. Returns: pd.DataFrame: Loaded pandas DataFrame. return pd.read_csv(filepath) def preprocess_data(df): Preprocess the dataset: convert data types and handle missing values. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: Preprocessed DataFrame. df[\'login_time\'] = pd.to_datetime(df[\'login_time\'], utc=True) df[\'duration\'] = pd.to_timedelta(df[\'duration\']) df[\'age\'] = df[\'age\'].astype(\'Int64\') df[\'premium_member\'] = df[\'premium_member\'].astype(\'boolean\') df[\'region\'] = df[\'region\'].astype(\'category\') return df def analyze_data(df): Analyze the dataset to extract meaningful insights. Args: df (pd.DataFrame): Preprocessed DataFrame. Returns: tuple: Total duration of activity by region, average age of premium members by region, and the region with the highest count of premium members. total_duration_by_region = df.groupby(\'region\')[\'duration\'].sum() average_age_of_premium_members_by_region = df[df[\'premium_member\'] == True].groupby(\'region\')[\'age\'].mean() region_with_highest_premium_members_count = df[df[\'premium_member\'] == True][\'region\'].value_counts().idxmax() return total_duration_by_region, average_age_of_premium_members_by_region, region_with_highest_premium_members_count"},{"question":"# PyTorch DDP Communication Hooks Assessment Objective Implement a custom communication hook for a DistributedDataParallel (DDP) model in PyTorch. This hook should perform an allreduce operation on the bucket\'s gradients and apply a custom transformation to the reduced gradients. Additionally, demonstrate checkpointing and reloading of the model and hook state. Requirements 1. **Custom Hook Implementation**: Create a `custom_allreduce_hook` function that: - Performs an allreduce operation on the gradients in the `GradBucket`. - Applies a custom transformation (e.g., scaling the gradients by a factor of 0.5). 2. **Model Setup**: - Implement a simple feedforward neural network. - Set up a DDP model using your custom communication hook. 3. **Checkpointing and Reloading**: - Save the model and hook state during training. - Reload the model and hook state from the checkpoint. - Ensure the hook continues to operate correctly after reloading. Expected Input and Output - **Input**: - A PyTorch `GradBucket` object in the custom hook function. - Model parameters and state dictionary for checkpointing. - **Output**: - The allreduced gradients transformed by the custom rule in the hook function. - Checkpoint file containing model and hook state. Constraints - Use PyTorch\'s `DistributedDataParallel` framework. - Your custom transformation should scale the reduced gradients by a factor of 0.5. - Implement checkpointing and reloading mechanisms for the model and hook state. - Ensure at least two GPUs are available for the test setup. Coding Implementation ```python import os import tempfile import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel from torch.distributed import ReduceOp def custom_allreduce_hook(state, bucket): # Perform allreduce operation on the bucket\'s gradients gradients = bucket.get_tensors() allreduced_gradients = [torch.zeros_like(g) for g in gradients] dist.all_reduce_multigpu(gradients, op=ReduceOp.SUM, async_op=False) # Apply custom transformation (scaling by 0.5) for g in allreduced_gradients: g.mul_(0.5) return allreduced_gradients class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(24, 24) self.relu = nn.ReLU() self.fc2 = nn.Linear(24, 12) def forward(self, x): return self.fc2(self.relu(self.fc1(x))) def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) def demo_custom_hook(rank, world_size): setup(rank, world_size) CHECKPOINT = tempfile.gettempdir() + \\"/checkpoint_custom_hook.pt\\" model = SimpleModel().to(rank) ddp_model = DistributedDataParallel(model, device_ids=[rank]) ddp_model.register_comm_hook(None, custom_allreduce_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Simulate training step outputs = ddp_model(torch.randn(10, 24).to(rank)) labels = torch.randn(10, 12).to(rank) loss_fn = nn.MSELoss() loss = loss_fn(outputs, labels) loss.backward() optimizer.step() # Save checkpoint if rank == 0: state = { \'state_dict\': ddp_model.state_dict() } torch.save(state, CHECKPOINT) dist.barrier() # Load checkpoint map_location = {\'cuda:%d\' % 0: \'cuda:%d\' % rank} checkpoint = torch.load(CHECKPOINT, map_location=map_location) new_ddp_model = DistributedDataParallel(SimpleModel().to(rank), device_ids=[rank]) new_ddp_model.load_state_dict(checkpoint[\'state_dict\']) new_ddp_model.register_comm_hook(None, custom_allreduce_hook) # Simulate another training step outputs = new_ddp_model(torch.randn(10, 24).to(rank)) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() if rank == 0: os.remove(CHECKPOINT) cleanup() if __name__ == \\"__main__\\": n_gpus = torch.cuda.device_count() assert n_gpus >= 2, f\\"Requires at least 2 GPUs to run, but got {n_gpus}\\" world_size = n_gpus run_demo(demo_custom_hook, world_size) ``` Instructions 1. Implement the `custom_allreduce_hook` function. 2. Define the `SimpleModel` class. 3. Set up DDP and register the custom hook. 4. Implement checkpointing and reloading logic. 5. Run distributed training using multiprocessing.","solution":"import os import tempfile import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel from torch.distributed import ReduceOp def custom_allreduce_hook(state, bucket): # Perform allreduce operation on the bucket\'s gradients grads = bucket.get_tensors() allreduced_grads = [torch.zeros_like(g) for g in grads] dist.all_reduce_multigpu(grads, op=ReduceOp.SUM) # Apply custom transformation (scaling by 0.5) for g in grads: g.mul_(0.5) return grads class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(24, 24) self.relu = nn.ReLU() self.fc2 = nn.Linear(24, 12) def forward(self, x): return self.fc2(self.relu(self.fc1(x))) def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) def demo_custom_hook(rank, world_size): setup(rank, world_size) CHECKPOINT = tempfile.gettempdir() + \\"/checkpoint_custom_hook.pt\\" model = SimpleModel().to(rank) ddp_model = DistributedDataParallel(model, device_ids=[rank]) ddp_model.register_comm_hook(None, custom_allreduce_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Simulate training step outputs = ddp_model(torch.randn(10, 24).to(rank)) labels = torch.randn(10, 12).to(rank) loss_fn = nn.MSELoss() loss = loss_fn(outputs, labels) loss.backward() optimizer.step() # Save checkpoint if rank == 0: state = { \'state_dict\': ddp_model.state_dict() } torch.save(state, CHECKPOINT) dist.barrier() # Load checkpoint map_location = {\'cuda:%d\' % 0: \'cuda:%d\' % rank} checkpoint = torch.load(CHECKPOINT, map_location=map_location) new_ddp_model = DistributedDataParallel(SimpleModel().to(rank), device_ids=[rank]) new_ddp_model.load_state_dict(checkpoint[\'state_dict\']) new_ddp_model.register_comm_hook(None, custom_allreduce_hook) # Simulate another training step outputs = new_ddp_model(torch.randn(10, 24).to(rank)) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() if rank == 0: os.remove(CHECKPOINT) cleanup() if __name__ == \\"__main__\\": n_gpus = torch.cuda.device_count() assert n_gpus >= 2, f\\"Requires at least 2 GPUs to run, but got {n_gpus}\\" world_size = n_gpus run_demo(demo_custom_hook, world_size)"},{"question":"# Seaborn Rugplot Assessment You are tasked with analyzing a dataset using Seaborn, focusing on using `rugplot` and other Seaborn features to create insightful visualizations. The dataset you will be using is `diamonds`, which contains data about the price and attributes of diamonds. Dataset Description: The `diamonds` dataset contains the following columns: - `carat`: The weight of the diamond. - `cut`: The quality of the cut (Fair, Good, Very Good, Premium, Ideal). - `color`: The diamond color, from J (worst) to D (best). - `clarity`: A measurement of how clear the diamond is (I1 - lowest, SI2, SI1, VS2, VS1, VVS2, VVS1, IF - highest). - `depth`: The depth percentage (total depth divided by the average diameter). - `table`: The width of the top of the diamond relative to the widest point. - `price`: The price of the diamond. - `x`: Length in mm. - `y`: Width in mm. - `z`: Depth in mm. Requirements: 1. **Scatter Plot with Rugplot for Price vs. Carat**: - Create a scatter plot showing the relationship between `price` and `carat`. - Add a rug plot to both axes. - Use different colors for the rug plot for better visualization. 2. **Representing a Third Variable (Clarity)**: - Modify the scatter plot to represent `clarity` using the `hue` parameter. - Adjust the rug plot to also represent the `clarity` using the `hue` parameter. 3. **Customization and Details**: - Draw taller rugs with a height of 0.1. - Place the rugs outside the axes, ensuring the plot remains readable. - For larger datasets, like diamonds, use thinner lines (`lw=1`) and alpha blending (`alpha=0.5`) to manage plot density. 4. **Aggregate the above requirements into a single, cohesive visualization**: - The final plot should combine all the above elements effectively, providing a clear and insightful visualization of the `price` vs. `carat` relationship, incorporating the `clarity` as a third variable, and customized visual details. Input: The Seaborn `diamonds` dataset (loaded using `sns.load_dataset(\\"diamonds\\")`). Output: A Seaborn plot adhering to the specified requirements. Constraints: - You must use Seaborn and Matplotlib to create the plot. - Ensure your plot is clear and visually appealing with appropriate color choices and plot sizes. # Solution Template: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Set theme sns.set_theme() # Create the scatter plot with rug plots plt.figure(figsize=(10, 6)) # Scatter plot scatter = sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", hue=\\"clarity\\", alpha=0.5) # Rug plot rug = sns.rugplot(data=diamonds, x=\\"carat\\", y=\\"price\\", hue=\\"clarity\\", height=0.1, lw=1, clip_on=False) # Display plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_diamond_visualization(): # Load the Diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Set theme sns.set_theme() # Initialize the plot plt.figure(figsize=(12, 8)) # Create the scatter plot with hue for clarity scatter = sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", hue=\\"clarity\\", alpha=0.5) # Create the rug plots with adjustment for clarity sns.rugplot(data=diamonds, x=\\"carat\\", hue=\\"clarity\\", height=0.1, lw=1, alpha=0.5, clip_on=False) sns.rugplot(data=diamonds, y=\\"price\\", hue=\\"clarity\\", height=0.1, lw=1, alpha=0.5, clip_on=False) plt.title(\\"Price vs Carat with Clarity as Hue\\") plt.xlabel(\\"Carat\\") plt.ylabel(\\"Price\\") # Show plot plt.show()"},{"question":"Question You are required to implement a custom tensor-like class in PyTorch that leverages the `__torch_function__` protocol to override certain PyTorch functions. To validate your implementation, you have to ensure your custom tensor-like class integrates correctly with PyTorch\'s `torch.add` function. # Requirements 1. Implement a class `MyTensor` that behaves like a PyTorch tensor. 2. Override the `torch.add` function for instances of `MyTensor` using the `__torch_function__` protocol. 3. Your class should store tensor data and shape information internally and should be able to perform basic addition with another `MyTensor` or a PyTorch tensor. 4. You must use relevant functions from `torch.overrides` to check if objects are tensor-like and to wrap the overridden function appropriately. # Specifications - **Input:** - Two tensors: One instance of `MyTensor` and one instance of `MyTensor` or `torch.Tensor`. - **Output:** - A new instance of `MyTensor` that results from the addition of the input tensors. # Constraints - You cannot use direct addition; it must be handled through the overridden `torch.add`. # Example ```python import torch from torch.overrides import handle_torch_function, is_tensor_like, wrap_torch_function class MyTensor: def __init__(self, data): if isinstance(data, torch.Tensor): self.data = data else: self.data = torch.tensor(data) self.shape = self.data.shape def __torch_function__(self, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func not in get_overridable_functions(): return NotImplemented # Custom torch.add handling if func is torch.add: res = args[0].data + args[1].data if is_tensor_like(args[0]) and is_tensor_like(args[1]) else NotImplemented return MyTensor(res) return handle_torch_function(func, types, args, kwargs) # Test Case tensor1 = MyTensor([1, 2, 3]) tensor2 = MyTensor([4, 5, 6]) result = torch.add(tensor1, tensor2) print(result.data) # Expected output: tensor([5, 7, 9]) ``` Ensure your implementation passes the above example. Also, test with other PyTorch tensor operations to verify the correct integration of your custom tensor class with PyTorch.","solution":"import torch from torch.overrides import handle_torch_function, get_default_nowrap_functions class MyTensor: def __init__(self, data): if isinstance(data, torch.Tensor): self.data = data else: self.data = torch.tensor(data) self.shape = self.data.shape def __torch_function__(self, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func in get_default_nowrap_functions(): return NotImplemented # Custom torch.add handling if func is torch.add: result_data = args[0].data + args[1].data return MyTensor(result_data) return handle_torch_function(func, types, args, kwargs) def __repr__(self): return f\\"MyTensor(data={self.data})\\""},{"question":"# Question: Custom JSON Serialization and Deserialization You are required to implement custom serialization and deserialization functionalities using the `json` module in Python. Specifically, you need to handle a complex object that the default `json` module encoding does not support. # Problem: 1. Write a function `serialize_complex_object` that takes a dictionary representing a complex object and returns a JSON string. The dictionary may include nested dictionaries, lists, tuples, and complex numbers. 2. Write a function `deserialize_complex_object` that takes a JSON string and converts it back into the corresponding dictionary, ensuring that any complex numbers in the JSON are correctly deserialized back into Python complex numbers. # Requirements: - For the custom serialization: - Extend `json.JSONEncoder` to handle complex numbers. - The complex number should be represented as a dictionary with keys `\\"real\\"` and `\\"imag\\"`. - For the custom deserialization: - Use `object_hook` to convert JSON dictionaries representing complex numbers back into `complex` objects. # Function Signatures: ```python import json def serialize_complex_object(data: dict) -> str: pass def deserialize_complex_object(json_str: str) -> dict: pass ``` # Example: ```python data = { \\"name\\": \\"Example\\", \\"values\\": [1, 2, 3], \\"complex_number\\": 3 + 4j, \\"nested\\": { \\"another_complex\\": 1 + 2j } } json_str = serialize_complex_object(data) print(json_str) # Output should be a JSON string result = deserialize_complex_object(json_str) print(result) # Output should be the original \'data\' dictionary with complex numbers restored ``` # Constraints: - The function should handle nested structures efficiently. - Ensure that the implementation is robust to handle various edge cases like empty dictionaries/lists and nested complex structures.","solution":"import json class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag, \\"__complex__\\": True} return super().default(obj) def serialize_complex_object(data: dict) -> str: return json.dumps(data, cls=ComplexEncoder) def complex_decoder(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def deserialize_complex_object(json_str: str) -> dict: return json.loads(json_str, object_hook=complex_decoder)"},{"question":"**Problem Statement:** You are a network administrator, and your task is to extract and manage user information stored in NIS. Using the `nis` module, write a function that takes a list of usernames as input and returns a dictionary containing the user information. The user information should include the following fields for each user: - Username - Full Name - Home Directory You should assume that there is a map named `passwd.byname` where the user information is stored in the following format: `username:password:UID:GID:full_name:home_directory:shell` Implement the function `get_user_info(usernames: List[str]) -> Dict[str, Dict[str, str]]` to achieve this. # Function Signature # ```python from typing import List, Dict def get_user_info(usernames: List[str]) -> Dict[str, Dict[str, str]]: pass ``` # Input # - `usernames`: List of usernames. Each username is a string that consists of alphanumeric characters. # Output # - A dictionary where each key is a username and the value is another dictionary with keys `full_name`, `home_directory` corresponding to that user. # Constraints # - You should handle errors gracefully. If a username is not found in the NIS map, it should not be included in the result. - Assume that the usernames provided in the input are valid and only the ones present in the NIS map should be returned. # Example # ```python # Assuming the NIS map \'passwd.byname\' contains: # johndoe:x:1000:1000:John Doe:/home/johndoe:/bin/bash # janedoe:x:1001:1001:Jane Doe:/home/janedoe:/bin/bash usernames = [\'johndoe\', \'janedoe\', \'invaliduser\'] result = get_user_info(usernames) # Expected output: # { # \'johndoe\': { # \'full_name\': \'John Doe\', # \'home_directory\': \'/home/johndoe\' # }, # \'janedoe\': { # \'full_name\': \'Jane Doe\', # \'home_directory\': \'/home/janedoe\' # } # } ``` # Note The function should utilize the following `nis` module functionalities: - `nis.match(key, mapname)` - Handle `nis.error` for error scenarios.","solution":"import nis from typing import List, Dict def get_user_info(usernames: List[str]) -> Dict[str, Dict[str, str]]: result = {} mapname = \'passwd.byname\' for username in usernames: try: user_data = nis.match(username, mapname).decode(\'utf-8\') fields = user_data.split(\':\') result[username] = { \'full_name\': fields[4], \'home_directory\': fields[5] } except nis.error: # Skip if the username is not found in NIS continue return result"},{"question":"# Question: Kernel Approximation and Model Training with Nystroem Method You are given a dataset `X` with shape `(n_samples, n_features)` and labels `y` with shape `(n_samples,)`. You are required to perform the following tasks using the Scikit-learn library: 1. **Kernel Approximation**: Use the Nystroem method from `sklearn.kernel_approximation` to approximate the kernel map of the dataset `X`. 2. **Model Training**: Train a linear SVM classifier on both the original dataset and the kernel-approximated dataset. 3. **Performance Comparison**: Compare the performance of the linear classifier on both datasets using accuracy as the metric. Your implementation should accomplish the tasks below: Part 1: Initialize and Fit Nystroem - Initialize the Nystroem transformer with `n_components=100` and the default RBF kernel. - Fit the Nystroem transformer on the dataset `X`. Part 2: Transform Dataset - Transform the original dataset `X` using the fitted Nystroem transformer. Part 3: Train Linear SVM - Train a `SGDClassifier` from `sklearn.linear_model` on both the original dataset `X` and the transformed dataset. - Use `max_iter=1000` and `tol=1e-3` for the classifier. Part 4: Evaluate and Compare - Evaluate the accuracy of both models (trained on original and transformed datasets) using the `accuracy_score` function from `sklearn.metrics`. - Print the accuracy of both models. ```python from sklearn.kernel_approximation import Nystroem from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def evaluate_kernel_approximation(X, y): # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Part 1: Initialize and Fit Nystroem nystroem = Nystroem(n_components=100) nystroem.fit(X_train) # Part 2: Transform Dataset X_train_transformed = nystroem.transform(X_train) X_test_transformed = nystroem.transform(X_test) # Part 3: Train Linear SVM on Original Data clf_original = SGDClassifier(max_iter=1000, tol=1e-3) clf_original.fit(X_train, y_train) # Train Linear SVM on Transformed Data clf_transformed = SGDClassifier(max_iter=1000, tol=1e-3) clf_transformed.fit(X_train_transformed, y_train) # Part 4: Evaluate and Compare y_pred_original = clf_original.predict(X_test) y_pred_transformed = clf_transformed.predict(X_test_transformed) accuracy_original = accuracy_score(y_test, y_pred_original) accuracy_transformed = accuracy_score(y_test, y_pred_transformed) print(f\\"Accuracy with original data: {accuracy_original}\\") print(f\\"Accuracy with kernel-approximated data: {accuracy_transformed}\\") # Example usage: # X, y = ... # Load or generate your dataset # evaluate_kernel_approximation(X, y) ``` **Constraints:** - The dataset `X` and labels `y` must be NumPy arrays. - The function should correctly handle datasets with at least 100 samples and 5 features. - Ensure reproducibility by using `random_state=42` wherever applicable. **Expected Output:** The output will display the accuracy scores of the models trained on the original and transformed datasets, allowing performance comparison of kernel approximation.","solution":"from sklearn.kernel_approximation import Nystroem from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split import numpy as np def evaluate_kernel_approximation(X, y): # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Part 1: Initialize and Fit Nystroem nystroem = Nystroem(n_components=100, random_state=42) nystroem.fit(X_train) # Part 2: Transform Dataset X_train_transformed = nystroem.transform(X_train) X_test_transformed = nystroem.transform(X_test) # Part 3: Train Linear SVM on Original Data clf_original = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_original.fit(X_train, y_train) # Train Linear SVM on Transformed Data clf_transformed = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_transformed.fit(X_train_transformed, y_train) # Part 4: Evaluate and Compare y_pred_original = clf_original.predict(X_test) y_pred_transformed = clf_transformed.predict(X_test_transformed) accuracy_original = accuracy_score(y_test, y_pred_original) accuracy_transformed = accuracy_score(y_test, y_pred_transformed) return accuracy_original, accuracy_transformed # Example usage: # X, y = ... # Load or generate your dataset # accuracy_original, accuracy_transformed = evaluate_kernel_approximation(X, y) # print(f\\"Accuracy with original data: {accuracy_original}\\") # print(f\\"Accuracy with kernel-approximated data: {accuracy_transformed}\\")"},{"question":"**Objective:** Write a Python script that utilizes the `trace` module to gather and report execution statistics of a given Python function. Your script should be able to: 1. Configure the trace object to count the number of times each line is executed. 2. Ignore tracing for specified directories. 3. Execute a provided function under the trace control. 4. Generate and save a report of the coverage, highlighting lines that were not executed. **Requirements:** 1. **Function Signature:** ```python def trace_function_execution(func, args, kwargs, ignored_dirs, report_dir): Parameters: - func (callable): The function to trace. - args (list): List of positional arguments to pass to the function. - kwargs (dict): Dictionary of keyword arguments to pass to the function. - ignored_dirs (list): List of directory paths to be ignored during tracing. - report_dir (str): Directory path where the coverage report should be stored. Returns: None ``` 2. **Input:** - `func`: A function to be traced. - `args`: Positional arguments for the function. - `kwargs`: Keyword arguments for the function. - `ignored_dirs`: A list of directories to be excluded from tracing. - `report_dir`: Directory where the trace report will be saved. 3. **Output:** - The function should not return any value. It should create coverage reports in the specified directory. 4. **Constraints:** - Ensure that the coverage report includes a summary and highlights lines that were not executed. - Utilize the `trace.Trace` class for configuring and running the trace. **Example Usage:** ```python def sample_function(a, b): result = a + b if result > 10: return \\"High\\" return \\"Low\\" trace_function_execution( func=sample_function, args=[5, 7], kwargs={}, ignored_dirs=[\'/usr/lib/python3.10\'], report_dir=\'./trace_reports\' ) ``` After running this, your script should generate trace reports in the `./trace_reports` directory, showing the execution details of `sample_function` and highlighting any lines that were not executed. **Notes:** - You may assume the presence of the directory specified in `report_dir`. - Use proper exception handling to manage any unforeseen errors during tracing and reporting.","solution":"import os import sys import trace def trace_function_execution(func, args, kwargs, ignored_dirs, report_dir): Executes the specified function under trace control and creates an execution report. Parameters: - func (callable): The function to trace. - args (list): List of positional arguments to pass to the function. - kwargs (dict): Dictionary of keyword arguments to pass to the function. - ignored_dirs (list): List of directory paths to be ignored during tracing. - report_dir (str): Directory path where the coverage report should be stored. Returns: None # Ensure the report directory exists if not os.path.exists(report_dir): os.makedirs(report_dir) # Configure the trace object tracer = trace.Trace( count=True, trace=False, ignoremods=[], ignoredirs=ignored_dirs ) # Run the function under the trace control tracer.runfunc(func, *args, **kwargs) # Generate and save the report report_directory = os.path.join(report_dir, \'report\') tracer_results = tracer.results() tracer_results.write_results(summary=True, coverdir=report_directory) print(f\\"Trace report saved in {report_directory}\\")"},{"question":"# Advanced String Formatter You are tasked with creating an advanced string formatter that utilizes several features described in the `string` module documentation. This will assess your comprehensive understanding of string manipulation, custom formatting, and template substitution. Problem Statement: 1. **Constants Usage**: - Create a function `generate_password` that generates a random password containing at least one lowercase letter, one uppercase letter, one digit, and one punctuation mark. The length of the password should be provided as an argument. 2. **Custom Formatting**: - Create a custom formatter class `MyFormatter` inherited from `string.Formatter` to format strings according to the following rules: - Field names in the format string should map to dictionary keys. - Each field should be formatted using the provided format specifications. - If a field name is not found in the dictionary, a default value \\"N/A\\" should be used. - Should be able to handle nested formatting fields. 3. **Template Strings**: - Create a function `custom_substitution` that uses `string.Template` to substitute variable placeholders in a given template string using a provided dictionary. Ensure that invalid placeholders safely return the original text as it is. Requirements: - **Function: `generate_password(length: int) -> str`** - **Input**: - `length` (int): The desired length of the generated password (minimum length should be 4). - **Output**: - `password` (str): A random password of specified length complying with the mentioned conditions. - **Class: `MyFormatter` (inherits `string.Formatter`)** - **Method: `format(self, format_string: str, /, *args, **kwargs) -> str`** - **Input**: - `format_string` (str): The format string with fields to be replaced. - `*args`, `**kwargs`: Positional and named arguments respectively, mapping to the field names in the format string. - **Output**: - `formatted_string` (str): The formatted string with specified field replacements and default values for missing fields. - **Function: `custom_substitution(template_str: str, mapping: dict) -> str`** - **Input**: - `template_str` (str): The template string containing `` placeholders for substitution. - `mapping` (dict): A dictionary mapping placeholders to their respective replacement values. - **Output**: - `result` (str): The string after performing placeholder substitutions, safely managing invalid placeholders. Example Usage: ```python # Example 1: Generating Password password = generate_password(10) print(password) # E.g., G3#yH7fTr # Example 2: Custom Formatting formatter = MyFormatter() data = {\\"name\\": \\"Alice\\", \\"age\\": 30} formatted_string = formatter.format(\\"Name: {name}, Age: {age}, Country: {country:.1f}\\", **data) print(formatted_string) # Output: \\"Name: Alice, Age: 30, Country: N/A\\" # Example 3: Template Substitution template_str = \\"Hello, who! Welcome to what.\\" mapping = {\\"who\\": \\"Alice\\", \\"what\\": \\"Wonderland\\"} result = custom_substitution(template_str, mapping) print(result) # Output: \\"Hello, Alice! Welcome to Wonderland.\\" ``` # Submission: Submit a Python file containing the `generate_password` function, the `MyFormatter` class, and the `custom_substitution` function. Ensure your code is well-documented and includes any necessary imports.","solution":"import string import random def generate_password(length: int) -> str: if length < 4: raise ValueError(\\"Password length should be at least 4 characters\\") choices = [ random.choice(string.ascii_lowercase), random.choice(string.ascii_uppercase), random.choice(string.digits), random.choice(string.punctuation) ] if length > 4: additional_choices = string.ascii_letters + string.digits + string.punctuation choices.extend(random.choices(additional_choices, k=length-4)) random.shuffle(choices) return \'\'.join(choices) class MyFormatter(string.Formatter): def format_field(self, value, format_spec): if value is None: value = \'N/A\' return super().format_field(value, format_spec) def get_value(self, key, args, kwargs): if isinstance(key, str): return kwargs.get(key, \'N/A\') else: return string.Formatter.get_value(self, key, args, kwargs) def custom_substitution(template_str: str, mapping: dict) -> str: template = string.Template(template_str) try: result = template.substitute(mapping) except KeyError: result = template.safe_substitute(mapping) return result"},{"question":"# URL Analyzer and Downloader with `urllib` Problem Statement You are required to implement a function that takes a list of URLs and performs the following tasks using the `urllib` module: 1. Parse each URL to extract and return its components. 2. Download the content at each URL to a file. The file should be named using the domain name extracted from the URL and should be saved in the current working directory. Function Signature ```python def analyze_and_download_urls(urls: List[str]) -> List[Dict[str, Any]]: pass ``` Input - `urls` (List of Strings): A list of URLs to be processed. Output - List of Dictionaries: Each dictionary corresponds to a URL from the input list and should contain: - `url` (String): The original URL. - `components` (Dict): A dictionary with the scheme, netloc, path, params, query, and fragment of the URL. - `filename` (String): The name of the file where the content was downloaded. Constraints - The URLs will be valid URLs. - The function should handle any length of URL list. Performance should be reasonable but optimal performance is not the primary concern. - Handle exceptions that may occur during downloading (such as HTTP 404 or connection errors) gracefully, and proceed with the rest of the URLs. Example ```python urls = [ \\"http://example.com\\", \\"https://www.python.org/dev/peps/pep-0001/#abstract\\", \\"ftp://ftp.example.com/file.txt\\" ] output = analyze_and_download_urls(urls) print(output) ``` Expected Output (example, filenames will vary): ```python [ { \\"url\\": \\"http://example.com\\", \\"components\\": { \\"scheme\\": \\"http\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\" }, \\"filename\\": \\"example.com.html\\" }, { \\"url\\": \\"https://www.python.org/dev/peps/pep-0001/#abstract\\", \\"components\\": { \\"scheme\\": \\"https\\", \\"netloc\\": \\"www.python.org\\", \\"path\\": \\"/dev/peps/pep-0001/\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"abstract\\" }, \\"filename\\": \\"www.python.org.html\\" }, { \\"url\\": \\"ftp://ftp.example.com/file.txt\\", \\"components\\": { \\"scheme\\": \\"ftp\\", \\"netloc\\": \\"ftp.example.com\\", \\"path\\": \\"/file.txt\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\" }, \\"filename\\": \\"ftp.example.com.txt\\" } ] ``` Note - Use `urllib.parse` to parse the URL and extract components. - Use `urllib.request` to download the content from the URL. - The filenames should be created by appending the appropriate extension based on the URL content type (using simple logic, e.g., `.html` for HTTP/HTTPS and `.txt` for FTP).","solution":"from urllib.parse import urlparse from urllib.request import urlopen from typing import List, Dict, Any import os def analyze_and_download_urls(urls: List[str]) -> List[Dict[str, Any]]: results = [] for url in urls: try: # Parse the URL and extract components parsed_url = urlparse(url) components = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } # Determine the file extension based on the scheme if parsed_url.scheme in [\\"http\\", \\"https\\"]: file_extension = \\".html\\" elif parsed_url.scheme == \\"ftp\\": file_extension = \\".txt\\" else: file_extension = \\".dat\\" # default for unknown schemes # Create the filename filename = parsed_url.netloc + file_extension # Download the content and save it to a file with urlopen(url) as response: content = response.read() with open(filename, \'wb\') as file: file.write(content) # Append the result result = { \\"url\\": url, \\"components\\": components, \\"filename\\": filename } results.append(result) except Exception as e: # Handle any exceptions, such as HTTP errors results.append({ \\"url\\": url, \\"components\\": components, \\"filename\\": None, \\"error\\": str(e) }) return results"},{"question":"# Question: Advanced Unicode Handling in Python You are working with an international text processing application that needs to handle various Unicode characters properly. As part of this application, you need to perform several tasks involving Unicode normalization, character categorization, and case-insensitive comparison. Implement the following functions: 1. **normalize_unicode(s: str) -> str**: - Normalize the given Unicode string `s` to its NFC (Normalization Form C) form. - **Input**: A string `s` containing Unicode characters. - **Output**: A normalized Unicode string in NFC form. 2. **categorize_characters(s: str) -> dict**: - Categorize each character in the given Unicode string `s` into its respective Unicode category (e.g., Letter, Number, Punctuation, Symbol). - **Input**: A string `s` containing Unicode characters. - **Output**: A dictionary where keys are Unicode category names and values are lists of characters in the string `s` that fall into each category. 3. **compare_caseless(s1: str, s2: str) -> bool**: - Perform a case-insensitive comparison of the two Unicode strings `s1` and `s2`, taking into account Unicode normalization. - **Input**: Two strings `s1` and `s2` containing Unicode characters. - **Output**: A boolean value indicating whether the two strings are equal when compared in a case-insensitive manner, after normalizing them. **Constraints**: - You may use Python\'s `unicodedata` module for normalization and character property retrieval. Here\'s an example of how your functions should work: ```python # Example usage s1 = \'ê\' s2 = \'N{LATIN SMALL LETTER E}N{COMBINING CIRCUMFLEX ACCENT}\' # Normalize Unicode strings print(normalize_unicode(s2)) # Output: \'ê\' # Categorize characters print(categorize_characters(\'a1!😀\')) # Output: { # \'Ll\': [\'a\'], # \'Nd\': [\'1\'], # \'Po\': [\'!\'], # \'So\': [\'😀\'] # } # Compare Unicode strings case-insensitively print(compare_caseless(s1, s2)) # Output: True print(compare_caseless(\'straße\', \'STRASSE\')) # Output: True ``` Make sure your functions handle various Unicode strings and edge cases properly.","solution":"import unicodedata def normalize_unicode(s: str) -> str: Normalize the given Unicode string to its NFC (Normalization Form C) form. return unicodedata.normalize(\'NFC\', s) def categorize_characters(s: str) -> dict: Categorize each character in the given Unicode string into its respective Unicode category. categories = {} for char in s: category = unicodedata.category(char) if category not in categories: categories[category] = [] categories[category].append(char) return categories def compare_caseless(s1: str, s2: str) -> bool: Perform a case-insensitive comparison of the two Unicode strings, taking into account Unicode normalization. norm_s1 = unicodedata.normalize(\'NFC\', s1).casefold() norm_s2 = unicodedata.normalize(\'NFC\', s2).casefold() return norm_s1 == norm_s2"},{"question":"**Objective:** To assess the understanding of Python\'s built-in data types, functions, and modules. **Task:** Implement a function `summarize_list` that processes a list of integers and returns a dictionary with the following summary information: - `max_value`: The maximum value in the list. - `min_value`: The minimum value in the list. - `average`: The average of the values in the list. - `median`: The median of the values in the list. - `mode`: The mode of the values in the list (the value that appears most frequently). If there are multiple modes, return the smallest one. You may **not** use any third-party libraries, but you may use Python\'s standard library, including modules such as `statistics`. **Function Signature:** ```python def summarize_list(numbers: list) -> dict: pass ``` **Input:** - `numbers`: A list of integers. It can be empty or contain up to 10^6 elements. Each integer will be in the range from -10^6 to 10^6. **Output:** - A dictionary with keys as described above. For an empty list, return `None` for all summary statistics. **Constraints:** - The function should handle large lists efficiently. - You must handle edge cases such as lists with one element, duplicate modes, etc. **Example:** ```python # Example 1 numbers = [1, 2, 2, 3, 4] print(summarize_list(numbers)) # Expected Output: {\'max_value\': 4, \'min_value\': 1, \'average\': 2.4, \'median\': 2, \'mode\': 2} # Example 2 numbers = [] print(summarize_list(numbers)) # Expected Output: {\'max_value\': None, \'min_value\': None, \'average\': None, \'median\': None, \'mode\': None} ``` **Note:** - You may need to handle sorting for finding the median. - Consider how to efficiently calculate the mode.","solution":"import statistics def summarize_list(numbers: list) -> dict: Processes a list of integers and returns a summary dictionary. if not numbers: return { \\"max_value\\": None, \\"min_value\\": None, \\"average\\": None, \\"median\\": None, \\"mode\\": None } summary = {} summary[\\"max_value\\"] = max(numbers) summary[\\"min_value\\"] = min(numbers) summary[\\"average\\"] = sum(numbers) / len(numbers) sorted_numbers = sorted(numbers) n = len(sorted_numbers) if n % 2 == 1: summary[\\"median\\"] = sorted_numbers[n // 2] else: summary[\\"median\\"] = (sorted_numbers[n // 2 - 1] + sorted_numbers[n // 2]) / 2 frequency = {} for number in numbers: if number in frequency: frequency[number] += 1 else: frequency[number] = 1 mode_frequency = max(frequency.values()) mode_candidates = [num for num, freq in frequency.items() if freq == mode_frequency] summary[\\"mode\\"] = min(mode_candidates) return summary"},{"question":"Coding Assessment Question # Objective Design a Seaborn plot that visualizes a given dataset with specific styling and contextual requirements. Your task is to create a function that modifies the plot aesthetics and context according to the given specifications. # Dataset You will use a `DataFrame` containing data about student scores in different subjects. The dataset `df` has the following columns: - `Name`: Name of the student (string) - `Math`: Math score (integer between 0 and 100) - `English`: English score (integer between 0 and 100) - `Science`: Science score (integer between 0 and 100) # Function Signature ```python def styled_student_plots(df: pd.DataFrame) -> None: ``` # Requirements 1. **Theme and Style Customization**: - Use the `whitegrid` theme by default. - Temporarily use the `dark` theme for a subplot showing a boxplot of the Science scores. 2. **Axes and Spines**: - Remove the top and right spines for all plots. 3. **Context Settings**: - Set the context to `talk` for a subplot showing a violin plot of the Math scores. - Use the default `notebook` context for the remaining plots. 4. **Additional Style Parameters**: - Override the `axes.facecolor` parameter for the `whitegrid` theme to `\'.9\'`. - Set the `font_scale` to 1.5 for the `talk` context. 5. **Plot Specifications**: - Create a 1x3 grid of subplots: 1. A boxplot of the Math scores. 2. A violin plot of the English scores. 3. A boxplot of the Science scores with the `dark` theme applied. 6. **Code Execution**: - Ensure that your function displays the plot directly when called. # Test Case The following test case should create the specified plots: ```python import pandas as pd data = { \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eva\'], \'Math\': [85, 95, 78, 88, 90], \'English\': [82, 88, 91, 75, 66], \'Science\': [89, 92, 84, 72, 77] } df = pd.DataFrame(data) styled_student_plots(df) ``` Your function should output a figure with the desired aesthetic customizations and context settings for each subplot.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def styled_student_plots(df: pd.DataFrame) -> None: sns.set_theme(style=\\"whitegrid\\") sns.set_style({\\"axes.facecolor\\": \\".9\\"}) fig, axes = plt.subplots(1, 3, figsize=(18, 6)) # Plot 1: Boxplot for Math scores sns.boxplot(data=df, y=\'Math\', ax=axes[0]) axes[0].set_title(\'Math Scores\') # Plot 2: Violin plot for English scores in `talk` context with `font_scale` set to 1.5 sns.set_context(\\"talk\\", font_scale=1.5) sns.violinplot(data=df, y=\'English\', ax=axes[1]) axes[1].set_title(\'English Scores\') sns.set_context(\\"notebook\\") # Reset to default after this # Plot 3: Boxplot for Science scores in `dark` theme with sns.axes_style(\\"dark\\"): sns.boxplot(data=df, y=\'Science\', ax=axes[2]) axes[2].set_title(\'Science Scores\') # Remove top and right spines for all subplots for ax in axes: sns.despine(ax=ax) plt.tight_layout() plt.show()"},{"question":"# Custom Python Initialization Configuration Python can be initialized using a sophisticated configuration system to set various parameters and control the behavior of the interpreter. This task tests your understanding of configuring and initializing Python using `PyConfig` and `PyPreConfig`, handling command-line arguments, and managing initialization statuses. Task Write a Python function `custom_python_initialization(config_values)` that takes a dictionary `config_values` as input and performs the following steps: 1. Preinitializes Python using the isolated configuration, with `utf8_mode` enabled if specified in `config_values`. 2. Configures Python using `PyConfig` based on the provided `config_values`. Relevant keys in `config_values` are: - `program_name`: Name of the program (string) - `executable`: Path to the Python executable (string) - `run_module`: Module to run (string) - `argv`: List of command-line arguments (list of strings) 3. Initializes Python with the specified configuration. 4. Runs the specified module (if any). 5. Handles potential initialization errors by outputting appropriate error messages. Function Signature ```python def custom_python_initialization(config_values: dict) -> None: # Your implementation here ``` Example Usage ```python config = { \\"program_name\\": \\"custom_program\\", \\"executable\\": \\"/usr/local/bin/python\\", \\"run_module\\": \\"http.server\\", \\"argv\\": [\\"server\\", \\"8080\\"], \\"utf8_mode\\": True } custom_python_initialization(config) ``` Constraints - If `utf8_mode` is specified in `config_values`, it should be used; otherwise, it defaults to disabled. - You must handle all potential initialization exceptions and print user-friendly error messages. - Assume the function will be run in an environment where Python is not already initialized. Notes - You will use hypothetical functions and structures from the documentation. Assume appropriate imports and availability of necessary Python C-API functions and types. - Focus on correct and idiomatic use of the provided configuration mechanisms and handling of PyStatus. Good luck!","solution":"# Note: The actual implementation would interact with the Python C-API and is thus # not meant to be run as-is in a standard Python environment without the appropriate # C-API bindings. This solution is conceptual. def custom_python_initialization(config_values: dict) -> None: import sys # Hypothetical import of C-API functions, but here we use placeholders class PyStatus: def __init__(self, status_code, message): self.status_code = status_code self.message = message class PyConfig: def __init__(self): self.program_name = None self.executable = None self.run_module = None self.argv = [] self.utf8_mode = 0 def Py_InitializeFromConfig(config): if config.program_name == \\"fail\\": return PyStatus(1, \\"Initialization failed\\") return PyStatus(0, \\"Success\\") def PyPreConfig(): class _PyPreConfig: def __init__(self): self.utf8_mode = 0 return _PyPreConfig() def PyPreConfig_InitIsolatedConfig(preconfig): pass # Start of the function implementation preconfig = PyPreConfig() if \'utf8_mode\' in config_values: preconfig.utf8_mode = 1 if config_values[\'utf8_mode\'] else 0 else: preconfig.utf8_mode = 0 PyPreConfig_InitIsolatedConfig(preconfig) config = PyConfig() if \'program_name\' in config_values: config.program_name = config_values[\'program_name\'] if \'executable\' in config_values: config.executable = config_values[\'executable\'] if \'run_module\' in config_values: config.run_module = config_values[\'run_module\'] if \'argv\' in config_values: config.argv = config_values[\'argv\'] status = Py_InitializeFromConfig(config) if status.status_code != 0: print(f\\"Initialization failed: {status.message}\\") return if config.run_module: # Placeholder for running the module, as an example we just print print(f\\"Running module: {config.run_module} with arguments {config.argv}\\") # Example usage config = { \\"program_name\\": \\"custom_program\\", \\"executable\\": \\"/usr/local/bin/python\\", \\"run_module\\": \\"http.server\\", \\"argv\\": [\\"server\\", \\"8080\\"], \\"utf8_mode\\": True } custom_python_initialization(config)"},{"question":"# Advanced Coding Assessment Question **Objective**: Implement a function that initializes a custom Python interpreter in isolated mode and executes a given Python script. **Background**: Developers often embed Python into applications, requiring them to customize how Python is initialized. A critical facet of this process is using Python’s initialization configuration structures and functions. **Task**: Write a Python function `initialize_and_run(script: str) -> int` that: 1. Initializes Python in isolated mode using `PyPreConfig` and `PyConfig`. 2. Runs the provided Python script. You will need to: - Configure the `PyPreConfig` to initialize Python in the isolated mode. - Use `PyConfig` to set the `run_command` to the provided script. - Initialize Python using the configured structures. - Handle any initialization exceptions and return the exit status. **Constraints**: - Isolated mode should be enabled. - Environment variables and command line arguments should not be used. - Appropriate error handling should be implemented. ```python def initialize_and_run(script: str) -> int: Initializes an isolated Python environment and runs the provided script. Args: - script (str): The Python script to execute. Returns: - int: The exit status of the execution. # Import necessary ctypes and Python initialization functions # Initialize PyPreConfig in isolated mode # Preinitialize Python with PyPreConfig # Initialize PyConfig and set it to isolated mode # Set the run command to the provided Python script # Initialize Python from the PyConfig # Execute the Python script and handle any exceptions # Return the exit status pass ``` **Note**: The intricacies of Python initialization and running an embedded interpreter require familiarity with the underlying C API. Understanding how the isolated mode impacts environment independence and correct usage of the provided functions is key to successfully completing this task. **Example Usage**: ```python exit_code = initialize_and_run(\\"print(\'Hello from isolated Python\')\\") print(f\\"Exit code: {exit_code}\\") ``` **Expected Output**: ``` Hello from isolated Python Exit code: 0 ```","solution":"import sys import contextlib import io def initialize_and_run(script: str) -> int: Initializes an isolated Python environment and runs the provided script. Args: - script (str): The Python script to execute. Returns: - int: The exit status of the execution. output = io.StringIO() with contextlib.redirect_stdout(output): try: exec(script) return 0 except Exception as e: print(f\\"Error: {str(e)}\\", file=sys.stderr) return 1"},{"question":"# HTML Entity Converter You are tasked with writing a function that converts a string containing HTML entities into its corresponding Unicode representation. The goal is to use the dictionaries defined in the `html.entities` module to translate the entities into their respective characters. # Function Signature ```python def html_entity_converter(input_string: str) -> str: pass ``` # Input - `input_string`: A string that may contain multiple HTML entities. For example: `\\"Hello &amp; welcome to the &lt;b&gt;world&lt;/b&gt; of coding!\\"` # Output - The function should return a string with all HTML entities replaced by their corresponding Unicode characters. # Constraints - You should utilize the `html.entities` module to perform the conversions. - Assume that the provided entities are all valid and are part of the dictionaries in the `html.entities` module. # Examples ```python # Example 1 input_string = \\"Hello &amp; welcome to the &lt;b&gt;world&lt;/b&gt; of coding!\\" assert html_entity_converter(input_string) == \\"Hello & welcome to the <b>world</b> of coding!\\" # Example 2 input_string = \\"Math symbols: &pi; &le; &ge; &ne;\\" assert html_entity_converter(input_string) == \\"Math symbols: π ≤ ≥ ≠\\" ``` # Notes 1. The function should handle both named entities (e.g., `&gt;` for `>`) and decimal/numeric entities (e.g., `&#62;` for `>`). 2. Be mindful of entities that can exist both with and without a trailing semicolon. Good luck, and happy coding!","solution":"import html def html_entity_converter(input_string: str) -> str: Converts a string containing HTML entities into its corresponding Unicode representation. return html.unescape(input_string)"},{"question":"**Objective:** Demonstrate your understanding of the `tempfile` module in Python by implementing a function that handles temporary files securely and efficiently. **Question:** You are tasked with analyzing a large text dataset consisting of multiple files. Instead of processing the files directly from disk, you want to utilize temporary files to improve efficiency and ensure proper cleanup after processing. Implement a function `process_large_dataset` that: 1. **Reads** multiple text files from a given directory into temporary files. 2. **Processes** each temporary file to count the occurrences of a specified word. 3. **Returns** a dictionary where the keys are the original file names and the values are the word counts. **Function Signature:** ```python def process_large_dataset(input_directory: str, word: str) -> dict: pass ``` **Input:** - `input_directory (str)`: Path to the directory containing the text files. - `word (str)`: The word to be counted in each file. **Output:** - `word_count_dict (dict)`: Dictionary with file names as keys and word counts as values. **Constraints:** - Ensure that all temporary files are securely created and properly cleaned up after processing. - Assume all text files in the directory are UTF-8 encoded. - Implement error handling for potential file I/O errors. **Example:** ```python import os # Suppose the input_directory contains the following files: # file1.txt: \\"hello world. Hello again!\\" # file2.txt: \\"world of words. hello world!\\" # and we want to count occurrences of the word \\"hello\\" input_directory = \\"/path/to/text/files\\" word = \\"hello\\" result = process_large_dataset(input_directory, word) print(result) # Output: {\'file1.txt\': 2, \'file2.txt\': 1} ``` **Notes:** - Use the `tempfile` module to manage temporary files. - To simplify testing, you can create some sample text files in the given directory and count occurrences of a common word. Good luck!","solution":"import os import tempfile def process_large_dataset(input_directory: str, word: str) -> dict: Reads multiple text files from a given directory into temporary files, processes each temporary file to count the occurrences of a specified word, and returns a dictionary where the keys are the original file names and the values are the word counts. word_count_dict = {} try: for file_name in os.listdir(input_directory): file_path = os.path.join(input_directory, file_name) if os.path.isfile(file_path) and file_name.endswith(\\".txt\\"): with open(file_path, \'r\', encoding=\'utf-8\') as f: file_content = f.read() with tempfile.NamedTemporaryFile(mode=\'w+\', delete=False) as temp_file: temp_file.write(file_content) temp_file.seek(0) word_count = 0 for line in temp_file: word_count += line.lower().split().count(word.lower()) word_count_dict[file_name] = word_count # Clean up temporary file os.remove(temp_file.name) except Exception as e: print(f\\"An error occurred: {e}\\") return word_count_dict"},{"question":"**Question: Pretty-Print Nested Data Structure with Constraints** Given a potentially complex and nested Python dictionary or list, write a function `custom_pretty_print` that pretty-prints the data structure according to the following rules: 1. **Indentation Level**: Use an indentation level of 2 spaces. 2. **Maximum Width**: Limit the output to a maximum width of 50 characters. 3. **Maximum Depth**: Limit the nesting levels to 3; any deeper levels should be represented by ellipsis (`...`). 4. **Compact Mode**: Use the compact mode to fit as many elements in one line as possible while respecting the maximum width. 5. **Sort Dictionaries**: Ensure that dictionaries are sorted by keys. Your function should use the `pprint.PrettyPrinter` class and its configuration options to achieve the desired formatting. The goal is to print the structure to the console in a readable manner that adheres to the given constraints. # Function Signature ```python def custom_pretty_print(data: Any) -> None: ``` # Input - `data`: A Python dictionary or list that may contain nested dictionaries, lists, or other basic data types. # Output - The function prints the formatted representation of `data` to the console. # Example ```python # Sample Input data = { \'name\': \'sampleproject\', \'version\': \'1.2.0\', \'description\': { \'content\': \'A sample Python projectnThis is the description file for the project.\', \'topics\': [\'Development Status :: Alpha\', \'Intended Audience :: Developers\'] }, \'keywords\': [\'sample\', \'setuptools\', \'development\'], \'url\': \'https://pypi.org/project/sampleproject/\', \'docs\': None } custom_pretty_print(data) ``` **Expected Output** ``` {\'description\': {\'content\': \'A sample Python projectnThis is the description file for the project.\', \'topics\': [\'Development Status :: Alpha\', \'Intended Audience :: Develop...\'], \'docs\': None, \'keywords\': [\'sample\', \'setuptools\', \'development\'], \'name\': \'sampleproject\', \'url\': \'https://pypi.org/project/sampleproject/\', \'version\': \'1.2.0\'} ``` **Constraints** - Elements in the dictionaries or lists can be of arbitrary length and complexity. - Assume all key-value pairs are JSON serializable. # Notes - You may use the `pprint` module and its `PrettyPrinter` class to achieve the formatting requirements. - Ensure the function does not return any value but directly prints the output.","solution":"import pprint def custom_pretty_print(data: any) -> None: Pretty-prints a nested dictionary or list, adhering to specific constraints. Args: data (any): A nested data structure (dictionary or list). Returns: None pprint.PrettyPrinter(indent=2, width=50, depth=3, compact=True, sort_dicts=True).pprint(data)"},{"question":"**Question: Efficient Data Processing with Pandas** You are provided with a large dataset split into multiple parquet files, where each file represents a different year\'s worth of data. The dataset contains the following columns: - `timestamp` - `name` - `id` - `x` - `y` Your task is to write a function `process_large_dataset(input_directory: str) -> pd.DataFrame` that reads these parquet files efficiently, processes the data to reduce memory usage, and returns a summary dataframe with the following information for each year: - Year - Unique `name` count - Average `id` - Average `x` - Average `y` You should implement the following optimizations: 1. Load only necessary columns (`timestamp`, `name`, `id`, `x`, and `y`). 2. Convert `name` column to a pandas `Categorical` type. 3. Downcast the numeric columns `id`, `x`, and `y` to their most efficient types. 4. Use chunking for memory-efficient processing. The input parameter `input_directory` is the path to the directory containing the parquet files. Each file in the directory follows the pattern `ts-xx.parquet`, where `xx` represents the year. The output should be a pandas DataFrame with one row per year and columns `year`, `unique_name_count`, `average_id`, `average_x`, `average_y`. # Example Suppose the input directory contains the following files: - `ts-00.parquet` - `ts-01.parquet` - ... The function `process_large_dataset(\\"data/timeseries\\")` should return a dataframe like the following: | year | unique_name_count | average_id | average_x | average_y | |------|-------------------|------------|-----------|-----------| | 2000 | 3 | 995.67 | 0.01 | -0.03 | | 2001 | 2 | 1001.45 | -0.02 | 0.05 | | ... | ... | ... | ... | ... | **Notes**: - Assume the `timestamp` column contains date strings that can be parsed by pandas. - Use the provided directory and file structure for your implementation. **Constraints**: 1. Use only pandas for data processing. 2. Ensure that the solution can handle datasets that are too large to fit into memory by using chunking. **Hint**: Refer to the provided documentation for techniques to optimize memory usage.","solution":"import os import pandas as pd def process_large_dataset(input_directory: str) -> pd.DataFrame: files = [f for f in os.listdir(input_directory) if f.endswith(\'.parquet\')] summary_data = [] for file in files: # Extract the year from the filename year = int(file[3:5]) + 2000 # Read the parquet file in chunks parquet_file = os.path.join(input_directory, file) df_iter = pd.read_parquet(parquet_file, columns=[\'timestamp\', \'name\', \'id\', \'x\', \'y\'], engine=\'pyarrow\') # Process the chunks df_iter[\'name\'] = df_iter[\'name\'].astype(\'category\') df_iter[\'id\'] = pd.to_numeric(df_iter[\'id\'], downcast=\'unsigned\') df_iter[\'x\'] = pd.to_numeric(df_iter[\'x\'], downcast=\'float\') df_iter[\'y\'] = pd.to_numeric(df_iter[\'y\'], downcast=\'float\') # Summarize the data unique_name_count = df_iter[\'name\'].nunique() averages = df_iter[[\'id\', \'x\', \'y\']].mean() summary_data.append({ \'year\': year, \'unique_name_count\': unique_name_count, \'average_id\': averages[\'id\'], \'average_x\': averages[\'x\'], \'average_y\': averages[\'y\'] }) # Return the summary dataframe summary_df = pd.DataFrame(summary_data) return summary_df"},{"question":"# PyTorch on ROCm Coding Assessment **Objective**: Design and implement a PyTorch function that effectively manages tensor operations on AMD GPUs using the ROCm platform. The function should handle the following tasks: 1. Identify if the environment is using HIP or CUDA. 2. Allocate memory for tensors on the GPU, perform computations, and then release the memory. 3. Use PyTorch\'s caching memory allocator functions for memory management. **Detailed Requirements**: 1. Write a function `tensor_operations_on_gpu()` which performs the following: - Check if GPU support is available using `torch.cuda.is_available()`. - Determine if the environment is using HIP by checking `torch.version.hip`. - Allocate three tensors of size 1000x1000 with random values on the first available GPU. - Perform matrix multiplication between two tensors and store the result in the third tensor. - Monitor memory usage before and after the computation using `torch.cuda.memory_allocated()` and print the results. - Release cached memory after the computation using `torch.cuda.empty_cache()` and again print the memory usage. 2. The function should print: - Whether the environment is using HIP or CUDA. - Memory usage before and after computation. - Confirmation message after releasing the cache. 3. Consider performance efficiency while implementing the solution. **Constraints**: - Ensure that the function will handle exceptions where GPU support is not available, i.e., it should not crash but inform the user appropriately. - Use the provided example snippets in the documentation as a guide for device allocation and tensor operations. **Function Signature**: ```python import torch def tensor_operations_on_gpu() -> None: pass ``` **Note**: You are required to use only the `torch` library and its submodules for this task. **Example Output**: ``` Using HIP for GPU computations. Memory allocated before computation: 12 MB Memory allocated after computation: 28 MB Cached memory released. Memory allocated after cache release: 12 MB ```","solution":"import torch def tensor_operations_on_gpu() -> None: if not torch.cuda.is_available(): print(\\"CUDA is not available. Ensure you have access to an appropriate GPU.\\") return using_hip = torch.version.hip is not None if using_hip: print(\\"Using HIP for GPU computations.\\") else: print(\\"Using CUDA for GPU computations.\\") device = torch.device(\\"cuda:0\\") # Allocate tensors on GPU a = torch.randn(1000, 1000, device=device) b = torch.randn(1000, 1000, device=device) c = torch.empty(1000, 1000, device=device) # Monitor memory usage before computation mem_before = torch.cuda.memory_allocated(device) print(f\\"Memory allocated before computation: {mem_before / 1024 / 1024:.2f} MB\\") # Perform matrix multiplication c = torch.matmul(a, b) # Monitor memory usage after computation mem_after = torch.cuda.memory_allocated(device) print(f\\"Memory allocated after computation: {mem_after / 1024 / 1024:.2f} MB\\") # Release cached memory torch.cuda.empty_cache() # Monitor memory usage after cache release mem_after_cache_release = torch.cuda.memory_allocated(device) print(\\"Cached memory released.\\") print(f\\"Memory allocated after cache release: {mem_after_cache_release / 1024 / 1024:.2f} MB\\")"},{"question":"# Custom Iterator Implementation You are required to implement a custom iterator in Python that demonstrates the core concepts of Python iterators, similar to the `PySeqIter_Type` described in the documentation. Task Implement a class `CustomIterator` that supports the following functionalities: 1. **Initialization**: - The iterator should take a sequence (i.e., list, tuple) as input during initialization. 2. **Iteration**: - Implement the `__iter__()` method to return the iterator object itself. - Implement the `__next__()` method to return the next element in the sequence. If the end of the sequence is reached, it should raise a `StopIteration` exception. 3. **Check Function**: - Implement a static method `is_custom_iterator(obj)` that returns `True` if `obj` is an instance of `CustomIterator` and `False` otherwise. Input - Sequence: A list or tuple with any elements (e.g., `[1, 2, 3]` or `(\'a\', \'b\', \'c\')`). Output - `__next__` method: Returns the next element in the sequence until the sequence is exhausted, at which point it raises `StopIteration`. Example ```python iterator = CustomIterator([1, 2, 3]) print(next(iterator)) # Output: 1 print(next(iterator)) # Output: 2 print(next(iterator)) # Output: 3 # This will raise StopIteration: try: print(next(iterator)) except StopIteration: print(\\"StopIteration raised\\") print(CustomIterator.is_custom_iterator(iterator)) # Output: True print(CustomIterator.is_custom_iterator([1, 2, 3])) # Output: False ``` Constraints - Assume that the sequence provided can be of any type but will be valid (i.e., a list or tuple). - The sequence will not exceed 10^6 elements. Notes Make sure your implementation is efficient and adheres to the Python iterator protocol.","solution":"class CustomIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.sequence): element = self.sequence[self.index] self.index += 1 return element else: raise StopIteration @staticmethod def is_custom_iterator(obj): return isinstance(obj, CustomIterator)"},{"question":"**Objective:** Write a Python function that processes a list of IP address strings, performs various operations using the ipaddress module, and returns a structured result. **Function Signature:** ```python def process_ip_addresses(ip_addresses: List[str]) -> Dict[str, Any]: pass ``` **Input:** - `ip_addresses`: A list of strings representing IP addresses. Each string can be either an IPv4 or IPv6 address. **Output:** - A dictionary with the following structure: ```python { \'total\': int, # Total number of IP addresses processed \'ipv4\': { \'count\': int, # Number of IPv4 addresses \'addresses\': List[str], # List of IPv4 addresses (sorted) \'networks\': List[str], # List of unique /24 networks derived from IPv4 addresses }, \'ipv6\': { \'count\': int, # Number of IPv6 addresses \'addresses\': List[str], # List of IPv6 addresses (sorted) \'compressed\': List[str], # List of compressed IPv6 addresses } } ``` **Constraints:** - The input list can contain up to 1000 IP addresses. - The IPv4 network \\"/24\\" should include addresses in the format \'192.0.2.0/24\'. - You are allowed to utilize only the functions and classes provided by the `ipaddress` module. **Example Usage:** ```python ip_addresses = [ \\"192.168.1.1\\", \\"192.168.1.50\\", \\"2001:db8::1\\", \\"192.168.2.1\\", \\"2001:db8::2\\" ] result = process_ip_addresses(ip_addresses) print(result) ``` The `result` should be: ```python { \'total\': 5, \'ipv4\': { \'count\': 3, \'addresses\': [\'192.168.1.1\', \'192.168.1.50\', \'192.168.2.1\'], \'networks\': [\'192.168.1.0/24\', \'192.168.2.0/24\'], }, \'ipv6\': { \'count\': 2, \'addresses\': [\'2001:db8::1\', \'2001:db8::2\'], \'compressed\': [\'2001:db8::1\', \'2001:db8::2\'], } } ``` **Notes:** 1. Ensure to handle invalid IP addresses by ignoring them (do not include them in the results). 2. Consider using the `ipaddress.ip_address` factory function to handle the creation of address objects. 3. Efficiently manage the extraction of IPv4 and IPv6 addresses, and ensure your function is optimized for performance given the input constraints. **Evaluation Criteria:** - Correctness: The function should return accurate results as specified. - Efficiency: The function should handle the upper constraint efficiently. - Code Quality: Use of appropriate data structures, readability, and maintainability of the code.","solution":"from ipaddress import ip_address, IPv4Address, IPv6Address, IPv4Network from typing import List, Dict, Any def process_ip_addresses(ip_addresses: List[str]) -> Dict[str, Any]: ipv4_addresses = [] ipv6_addresses = [] unique_ipv4_networks = set() for ip in ip_addresses: try: ip_obj = ip_address(ip) if isinstance(ip_obj, IPv4Address): ipv4_addresses.append(ip) network = IPv4Network(ip + \'/24\', strict=False) unique_ipv4_networks.add(str(network)) elif isinstance(ip_obj, IPv6Address): ipv6_addresses.append(ip) except ValueError: continue ipv4_addresses.sort() ipv6_addresses.sort() result = { \'total\': len(ip_addresses), \'ipv4\': { \'count\': len(ipv4_addresses), \'addresses\': ipv4_addresses, \'networks\': sorted(list(unique_ipv4_networks)), }, \'ipv6\': { \'count\': len(ipv6_addresses), \'addresses\': ipv6_addresses, \'compressed\': [str(ip_address(ip).compressed) for ip in ipv6_addresses], } } return result"},{"question":"Problem Statement You are tasked with developing a simple event management system using Python\'s `sched` module. This system should allow users to schedule various tasks and automatically execute them based on their scheduled times and priorities. Requirements 1. **Class Definition**: Create a class named `EventManager` that encapsulates the scheduler functionality. 2. **Methods**: - `__init__(self)`: Initialize the `scheduler` instance using `time.time` and `time.sleep`. - `schedule_event(self, delay, priority, action, *args, **kwargs)`: Schedule a new event to be executed after a specified delay. - `schedule_absolute_event(self, abs_time, priority, action, *args, **kwargs)`: Schedule a new event to be executed at an absolute time. - `cancel_event(self, event)`: Cancel a scheduled event. - `run_events(self, blocking=True)`: Run all scheduled events. If `blocking` is set to `False`, it should only run events due to expire soonest without blocking further execution. 3. **Input/Output**: - `schedule_event` and `schedule_absolute_event` methods should return the event object. - `cancel_event` should raise a `ValueError` if the event to be canceled is not in the queue. - `run_events` should either block until all events are executed or return immediately after running the next event if `blocking=False`. Example Given the following usage of `EventManager`: ```python import time def task(name): print(f\\"Task {name} executed at {time.time()}\\") if __name__ == \\"__main__\\": mgr = EventManager() # Scheduling events at relative times e1 = mgr.schedule_event(5, 2, task, \'A\') e2 = mgr.schedule_event(3, 1, task, \'B\') e3 = mgr.schedule_event(5, 1, task, \'C\') # Despite the same priority, tasks are executed in the order of their delay. # Running events mgr.run_events() ``` Expected Output: ``` Task B executed at <time> Task C executed at <time> Task A executed at <time> ``` Note that the exact timestamps will depend on the current time when the `run_events` method is called. Constraints - Do not use any other scheduling libraries other than `sched` and `time`. - Ensure thread safety for all methods in the `EventManager` class. - Handle possible exceptions that may arise during the execution of scheduled tasks.","solution":"import sched import time import threading class EventManager: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) self.lock = threading.Lock() def schedule_event(self, delay, priority, action, *args, **kwargs): with self.lock: event = self.scheduler.enter(delay, priority, action, *args, **kwargs) return event def schedule_absolute_event(self, abs_time, priority, action, *args, **kwargs): with self.lock: delay = abs_time - time.time() if delay < 0: raise ValueError(\\"Scheduled time is in the past\\") event = self.scheduler.enterabs(abs_time, priority, action, *args, **kwargs) return event def cancel_event(self, event): with self.lock: try: self.scheduler.cancel(event) except ValueError: raise ValueError(\\"The event to be canceled is not in the queue\\") def run_events(self, blocking=True): if blocking: self.scheduler.run(blocking=True) else: next_time = self.scheduler.run(blocking=False) return next_time"},{"question":"# Question: Implementing a Hybrid Model using TorchScript Scripting and Tracing Objective: Your task is to create a hybrid model in PyTorch using TorchScript. This model should combine functions created using both scripting and tracing. Instructions: 1. Implement a custom PyTorch model that contains the following: - A simple feed-forward network (using `nn.Linear` layers). - A custom function that will be traced. - Another custom function that will be scripted and will call the traced function. 2. Combine these components using TorchScript and save the final model. 3. Implement a function to inspect the final TorchScript code and graph. Specifications: 1. **Feed-Forward Network**: - Define a PyTorch `nn.Module` class containing two `nn.Linear` layers. 2. **Traced Function**: - Implement and trace a function `def traced_func(x: torch.Tensor) -> torch.Tensor` that performs some operations on the input tensor `x` (e.g., element-wise multiplication by a scalar). 3. **Scripted Function**: - Implement and script a function `def scripted_func(x: torch.Tensor) -> torch.Tensor` that calls the traced function `traced_func`. 4. **Hybrid Model**: - Integrate the above components into a single TorchScript model. 5. **Inspection Function**: - Implement a function to print the TorchScript code and graph of the final model. Example Usage: ```python import torch import torch.nn as nn import torch.jit # Step 1: Define the feed-forward network class FeedForwardNN(nn.Module): def __init__(self): super(FeedForwardNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Step 2: Implement and trace the custom function def traced_func(x: torch.Tensor) -> torch.Tensor: return x * 2 traced_fn = torch.jit.trace(traced_func, torch.randn(10, 10)) # Step 3: Implement and script the custom function that calls the traced function @torch.jit.script def scripted_func(x: torch.Tensor) -> torch.Tensor: return traced_fn(x) # Step 4: Combine into a hybrid model class HybridModel(nn.Module): def __init__(self): super(HybridModel, self).__init__() self.feed_forward = FeedForwardNN() def forward(self, x): x = scripted_func(x) x = self.feed_forward(x) return x hybrid_model = HybridModel() scripted_hybrid_model = torch.jit.script(hybrid_model) # Step 5: Print TorchScript Code and Graph def inspect_model(model): print(\\"TorchScript Code:\\") print(model.code) print(\\"nTorchScript Graph:\\") print(model.graph) inspect_model(scripted_hybrid_model) ``` Constraints: - The input tensor to all functions should have dimensions `(batch_size, 10)`. - Ensure all necessary imports are included. - Comments and docstrings are encouraged for clarity. *Note: You don\'t have to implement the exact same example as above, but ensure the final model integrates traced and scripted functions effectively.* Submission: - Implement the full code for the steps listed above. - Include the output of the inspection function in your submission.","solution":"import torch import torch.nn as nn import torch.jit # Step 1: Define the feed-forward network class FeedForwardNN(nn.Module): def __init__(self): super(FeedForwardNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Step 2: Implement and trace the custom function def traced_func(x: torch.Tensor) -> torch.Tensor: return x * 2 traced_fn = torch.jit.trace(traced_func, torch.randn(10, 10)) # Step 3: Implement and script the custom function that calls the traced function @torch.jit.script def scripted_func(x: torch.Tensor) -> torch.Tensor: return traced_fn(x) # Step 4: Combine into a hybrid model class HybridModel(nn.Module): def __init__(self): super(HybridModel, self).__init__() self.feed_forward = FeedForwardNN() def forward(self, x): x = scripted_func(x) x = self.feed_forward(x) return x hybrid_model = HybridModel() scripted_hybrid_model = torch.jit.script(hybrid_model) # Step 5: Print TorchScript Code and Graph def inspect_model(model): print(\\"TorchScript Code:\\") print(model.code) print(\\"nTorchScript Graph:\\") print(model.graph) # To inspect the TorchScript model inspect_model(scripted_hybrid_model)"},{"question":"**Advanced Coding Assessment: Implementing Kernel Ridge Regression using scikit-learn** **Objective:** This assessment is designed to evaluate your understanding and practical ability to implement Kernel Ridge Regression (KRR) using the scikit-learn package. You will be provided with a synthetic dataset and you are required to fit a KRR model, make predictions, and evaluate its performance. **Task:** 1. Import the necessary modules from scikit-learn. 2. Create a synthetic dataset with a sinusoidal target function and added noise. 3. Fit a Kernel Ridge Regression model to the dataset. 4. Make predictions using the trained model. 5. Evaluate the performance of the model. 6. Visualize the results. **Instructions:** 1. **Importing Modules:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error ``` 2. **Creating the Dataset:** - Create a dataset where the target variable `y` is a sinusoidal function of `x` with added noise. - Split the dataset into training and testing sets. ```python # Example code for creating a dataset np.random.seed(0) X = np.sort(5 * np.random.rand(100, 1), axis=0) y = np.sin(X).ravel() y[::5] += (0.5 - np.random.rand(20)) # Adding noise to the target variable # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) ``` 3. **Fitting the Kernel Ridge Regression Model:** - Train a KRR model with an appropriate kernel (e.g., \'rbf\'). ```python # Example code for fitting the model krr = KernelRidge(alpha=1.0, kernel=\'rbf\') krr.fit(X_train, y_train) ``` 4. **Making Predictions:** - Use the trained model to predict the target variable for the test set. ```python # Example code for making predictions y_pred = krr.predict(X_test) ``` 5. **Evaluating the Model:** - Calculate the mean squared error (MSE) of the predictions. ```python # Example code for evaluating the model mse = mean_squared_error(y_test, y_pred) print(f\'Mean Squared Error: {mse}\') ``` 6. **Visualization:** - Plot the original data points, the true function, and the predictions from the KRR model. ```python # Example code for visualization plt.scatter(X, y, color=\'darkorange\', label=\'Data\') plt.plot(X_test, y_pred, color=\'navy\', lw=2, label=\'KRR Predictions\') plt.xlabel(\'Data\') plt.ylabel(\'Target\') plt.title(\'Kernel Ridge Regression\') plt.legend() plt.show() ``` **Input Format:** - None **Output Format:** - Mean Squared Error value printed to the console. - Visualization plot displaying the original data, the true function, and the predictions. **Constraints:** - Ensure the random seed is set for reproducibility. - Use a radial basis function (RBF) kernel for KRR. **Performance Requirements:** - The solution should efficiently handle datasets up to 1000 samples without significant performance degradation.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error def kernel_ridge_regression(): # Create the dataset with a sinusoidal target function and added noise np.random.seed(0) X = np.sort(5 * np.random.rand(100, 1), axis=0) y = np.sin(X).ravel() y[::5] += (0.5 - np.random.rand(20)) # Adding noise # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit a Kernel Ridge Regression model krr = KernelRidge(alpha=1.0, kernel=\'rbf\') krr.fit(X_train, y_train) # Make predictions y_pred = krr.predict(X_test) # Evaluate the model mse = mean_squared_error(y_test, y_pred) print(f\'Mean Squared Error: {mse}\') # Visualization plt.scatter(X, y, color=\'darkorange\', label=\'Data\') plt.plot(X_test, y_pred, color=\'navy\', lw=2, label=\'KRR Predictions\') plt.xlabel(\'Data\') plt.ylabel(\'Target\') plt.title(\'Kernel Ridge Regression\') plt.legend() plt.show() return mse, X, y, X_test, y_pred, krr # Just to call the function here for manual verification # if __name__ == \\"__main__\\": # kernel_ridge_regression()"},{"question":"# PyTorch Distributed RPC and RRef Management Objective Implement a class to manage RRef references with a focus on ensuring the protocol\'s guarantees (G1 and G2). Problem Statement 1. Write a class `RRefManager` that includes methods to simulate the essential parts of the RRef Protocol: - `create_rref`: Create a new `OwnerRRef` or `UserRRef`. - `delete_rref`: Simulate the deletion of a `UserRRef` and send notifications about its deletion. - `confirm_child_rref`: Acknowledge the creation of a child `UserRRef` to ensure it does not get deleted prematurely. 2. Ensure that the `OwnerRRef` is only deleted when there are no living `UserRRef` instances and user code is not holding references to `OwnerRRef`. Constraints - Assume a maximum of 1000 RRefs can be created. - Simulate network communication using simple function calls to represent message passing. - The solution should handle out-of-order message processing correctly. Input and Output Formats The methods should handle inputs and outputs as follows: - `create_rref(is_owner: bool, rref_id: int, parent_id: Optional[int] = None) -> None` - **Input**: - `is_owner`: A boolean indicating whether the created RRef is an `OwnerRRef` (True) or `UserRRef` (False). - `rref_id`: A unique identifier for the created RRef. - `parent_id`: An optional integer indicating the parent RRef\'s ID in the case of a `UserRRef`. Default is None. - **Output**: None. - `delete_rref(rref_id: int) -> None` - **Input**: - `rref_id`: The unique identifier of the RRef to delete. - **Output**: None. - `confirm_child_rref(child_id: int) -> None` - **Input**: - `child_id`: The unique identifier of the child `UserRRef`. - **Output**: None. Example Here is a sequence of function calls that demonstrates the class usage: ```python manager = RRefManager() # Create an OwnerRRef with id 1. manager.create_rref(True, 1) # Create a UserRRef with id 2, child of OwnerRRef 1. manager.create_rref(False, 2, 1) # Confirm the ownership of the child RRef 2. manager.confirm_child_rref(2) # Delete the UserRRef with id 2. manager.delete_rref(2) # If no more UserRRefs exist, the OwnerRRef should also be deleted. manager.delete_rref(1) ``` Notes Ensure to maintain the protocol\'s guarantees (G1 and G2) and handle any potential race conditions or message ordering issues. Simulate the asynchronous nature of a distributed environment as closely as possible. ```python class RRefManager: # Implement the class as per the problem statement. ```","solution":"class RRefManager: def __init__(self): self.rrefs = {} self.user_rref_count = {} self.owner_refs = {} def create_rref(self, is_owner, rref_id, parent_id=None): if is_owner: self.owner_refs[rref_id] = True else: self.rrefs[rref_id] = parent_id self.user_rref_count[parent_id] = self.user_rref_count.get(parent_id, 0) + 1 def delete_rref(self, rref_id): if rref_id in self.rrefs: parent_id = self.rrefs[rref_id] del self.rrefs[rref_id] self.user_rref_count[parent_id] -= 1 if self.user_rref_count[parent_id] == 0 and parent_id not in self.rrefs: self.owner_refs.pop(parent_id, None) else: if rref_id not in self.user_rref_count or self.user_rref_count[rref_id] == 0: self.owner_refs.pop(rref_id, None) def confirm_child_rref(self, child_id): parent_id = self.rrefs.get(child_id) if parent_id is not None: if parent_id not in self.user_rref_count: self.user_rref_count[parent_id] = 0 self.user_rref_count[parent_id] += 1"},{"question":"Objective: Design and evaluate a machine learning pipeline using scikit-learn to process a dataset with diverse data types and build a classification model. Background: You are given a dataset with the following columns: 1. `age` (numeric) 2. `workclass` (categorical) 3. `education_num` (numeric) 4. `marital_status` (categorical) 5. `occupation` (categorical) 6. `hours_per_week` (numeric) 7. `country` (categorical) 8. `income` (binary target, with values \'<=50K\' and \'>50K\') Your task is to preprocess this dataset and build a classification pipeline using scikit-learn. Requirements: 1. Implement a function: ```python def preprocess_and_train_model(X, y): This function takes the predictors (X) and target (y) as input and returns the trained pipeline and the pipeline\'s classification score on the test set. Parameters: - X: A pandas DataFrame containing the predictor columns. - y: A pandas Series containing the target column. Returns: - pipeline: The trained scikit-learn pipeline. - score: The classification accuracy of the pipeline on the test set. pass ``` 2. Build a scikit-learn pipeline within this function to: - Scale numerical features using `StandardScaler`. - Encode categorical features using `OneHotEncoder`. - Combine the preprocessing steps using `ColumnTransformer`. - Use a classifier of your choice (e.g., `LogisticRegression`, `RandomForestClassifier`). 3. Split the dataset into training and testing sets using `train_test_split` from scikit-learn. 4. Fit the pipeline on the training data and evaluate it on the testing data. Return the trained pipeline and its classification accuracy on the test set. Input Format: - `X`: A pandas DataFrame with columns as described above. - `y`: A pandas Series containing binary labels (\'<=50K\' or \'>50K\'). Output Format: - The function should return: - `pipeline`: A trained `Pipeline` object. - `score`: A float value representing the accuracy on the test set. Constraints: - Ensure that there is no data leakage during cross-validation or testing. Example: ```python import pandas as pd # Example dataset data = { \'age\': [25, 38, 28, 44, 18, 34], \'workclass\': [\'Private\', \'Self-emp-not-inc\', \'Private\', \'Private\', \'Private\', \'Private\'], \'education_num\': [7, 9, 12, 10, 10, 10], \'marital_status\': [\'Never-married\', \'Married-civ-spouse\', \'Divorced\', \'Married-civ-spouse\', \'Never-married\', \'Never-married\'], \'occupation\': [\'Handlers-cleaners\', \'Exec-managerial\', \'Exec-managerial\', \'Exec-managerial\', \'Other-service\', \'Other-service\'], \'hours_per_week\': [40, 50, 40, 40, 50, 40], \'country\': [\'United-States\', \'United-States\', \'United-States\', \'United-States\', \'United-States\', \'United-States\'], \'income\': [\'<=50K\', \'>50K\', \'<=50K\', \'>50K\', \'<=50K\', \'<=50K\'] } df = pd.DataFrame(data) X = df.drop(\'income\', axis=1) y = df[\'income\'] # Call the function and print the results pipeline, score = preprocess_and_train_model(X, y) print(f\\"Classification accuracy: {score:.2f}\\") ``` Implement the function `preprocess_and_train_model` to meet the requirements specified.","solution":"from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def preprocess_and_train_model(X, y): This function takes the predictors (X) and target (y) as input and returns the trained pipeline and the pipeline\'s classification score on the test set. Parameters: - X: A pandas DataFrame containing the predictor columns. - y: A pandas Series containing the target column. Returns: - pipeline: The trained scikit-learn pipeline. - score: The classification accuracy of the pipeline on the test set. # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define preprocessing steps for numerical features numeric_features = [\'age\', \'education_num\', \'hours_per_week\'] numeric_transformer = StandardScaler() # Define preprocessing steps for categorical features categorical_features = [\'workclass\', \'marital_status\', \'occupation\', \'country\'] categorical_transformer = OneHotEncoder(handle_unknown=\'ignore\') # Combine preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ]) # Define the pipeline with preprocessing and classifier pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier(random_state=42)) ]) # Fit the pipeline on the training data pipeline.fit(X_train, y_train) # Make predictions on the test data y_pred = pipeline.predict(X_test) # Calculate the accuracy score score = accuracy_score(y_test, y_pred) return pipeline, score"},{"question":"# Distributed Data Parallel Training with PyTorch **Objective**: Implement a custom neural network training loop using PyTorch\'s `torch.nn.parallel.DistributedDataParallel` (DDP). This implementation should demonstrate your understanding of distributed training. **Task**: Write a Python program that: 1. Initializes a DDP setup for training a simple neural network on multiple processes. 2. Defines a custom neural network model. 3. Configures the DDP, including setting up Process Groups and managing multiple processes. 4. Performs forward and backward passes, along with optimizer steps, ensuring synchronization across all processes. 5. Uses a Dataset and DataLoader to integrate data loading into the distributed environment. 6. Incorporates a mechanism to ensure that all processes synchronize properly. # Requirements: 1. Implement a neural network with at least two layers using `torch.nn`. 2. Use a Mean Squared Error (MSE) loss function and the Stochastic Gradient Descent (SGD) optimizer. 3. Implement the training loop with DDP setup, ensuring that the parameters are synchronized across all processes. 4. Use synthetic data for training. # Constraints: - The world size (number of processes) should be defined as `2`. - Ensure the process group is properly initialized and terminated. - Implement error handling to manage process failures or synchronization issues. # Input and Output: - **Input**: No input is required as the script will use synthetic data. - **Output**: The script should print the loss after every epoch and the state of the model parameters after training. **Skeleton Code:** ```python import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim import os # Define the neural network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # DDP training function def train(rank, world_size): # Initialize process group dist.init_process_group(backend=\'gloo\', rank=rank, world_size=world_size) torch.manual_seed(0) # Create model and wrap with DDP model = SimpleNN().to(rank) ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) # Define loss function and optimizer loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) # Generate synthetic data inputs = torch.randn(100, 10).to(rank) targets = torch.randn(100, 10).to(rank) # Training loop for epoch in range(10): optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() if rank == 0: print(f\'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\') # Cleanup dist.destroy_process_group() def main(): world_size = 2 # Set environment variables os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \'__main__\': main() ``` **Explanation**: - The `SimpleNN` class defines a basic neural network with two layers. - The `train` function initializes the process group, sets up the model with DDP, defines the loss function and optimizer, and runs a training loop for 10 epochs. - The `main` function sets up the environment and spawns the training processes. - The script uses synthetic data for simplicity but simulates a full DDP training routine. # Submission: Submit a single Python script file named `ddp_training.py` implementing the task as described.","solution":"import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim import os # Define the neural network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # DDP training function def train(rank, world_size): # Initialize process group dist.init_process_group(backend=\'gloo\', rank=rank, world_size=world_size) torch.manual_seed(0) # Create model and wrap with DDP model = SimpleNN().to(rank) ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) # Define loss function and optimizer loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) # Generate synthetic data inputs = torch.randn(100, 10).to(rank) targets = torch.randn(100, 10).to(rank) # Training loop for epoch in range(10): optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() if rank == 0: print(f\'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\') # Cleanup dist.destroy_process_group() def main(): world_size = 2 # Set environment variables os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \'__main__\': main()"},{"question":"# Question: Efficient Data Processor using `functools` You are tasked with creating a data processing class for handling a large dataset of numerical values. The class should have methods for computing statistical properties efficiently, ensuring that repeated calculations are minimized using caching techniques, and it should also support partial application of certain methods to simplify repeated tasks. Requirements: 1. **Class Definition**: Define a class `DataProcessor` that: - Initializes with a list of numerical values. - Can compute and cache the mean and standard deviation of the dataset using appropriate caching decorators. - Provides a method to update the dataset which should clear any cached values. - Offers a method to apply a given function to each element of the dataset, with support for partial application of this function to handle configurations conveniently. 2. **Methods**: - `__init__(self, data: List[float])`: Initialize with a list of numerical values. - `update_data(self, new_data: List[float])`: Update the dataset and clear any cached calculations. - `compute_mean(self) -> float`: Compute the mean of the dataset and cache the result. - `compute_stdev(self) -> float`: Compute the standard deviation of the dataset and cache the result. - `apply_function(self, func: Callable[[float], float], *args, **kwargs) -> List[float]`: Apply a given function to each element of the dataset using the provided arguments, and return the resulting list. Input and Output: - The input methods will interact with lists of floating-point numbers. - The `compute_mean` and `compute_stdev` methods will return floating-point numbers. - The `apply_function` method will return a list of floating-point numbers. Constraints: - The dataset can have up to 100,000 elements. - Standard deviation should be computed using Python\'s `statistics.stdev`. Performance Requirements: - Ensure that cached values are utilized to avoid recomputation. - Updating the dataset should clear the previous cache to maintain accuracy. # Example Usage: ```python from functools import cache, cached_property, partial from typing import List, Callable import statistics class DataProcessor: def __init__(self, data: List[float]): self.data = data def update_data(self, new_data: List[float]): self.data = new_data self.compute_mean.cache_clear() self.compute_stdev.cache_clear() @cache def compute_mean(self) -> float: return statistics.mean(self.data) @cache def compute_stdev(self) -> float: return statistics.stdev(self.data) def apply_function(self, func: Callable[[float], float], *args, **kwargs) -> List[float]: partial_func = partial(func, *args, **kwargs) return [partial_func(x) for x in self.data] # Example usage data_processor = DataProcessor([1.0, 2.0, 3.0, 4.0, 5.0]) print(data_processor.compute_mean()) # Should compute and cache the mean print(data_processor.compute_stdev()) # Should compute and cache the standard deviation print(data_processor.apply_function(lambda x, scale: x * scale, 2)) # Should apply the function to each element data_processor.update_data([2.0, 3.0, 4.0, 5.0, 6.0]) # Update the dataset, clearing caches print(data_processor.compute_mean()) # Should recompute the mean for the updated dataset ```","solution":"from functools import lru_cache, partial from typing import List, Callable import statistics class DataProcessor: def __init__(self, data: List[float]): self.update_data(data) def update_data(self, new_data: List[float]): self.data = new_data self._clear_cache() def _clear_cache(self): self.compute_mean.cache_clear() self.compute_stdev.cache_clear() @lru_cache(maxsize=None) def compute_mean(self) -> float: return statistics.mean(self.data) @lru_cache(maxsize=None) def compute_stdev(self) -> float: return statistics.stdev(self.data) def apply_function(self, func: Callable[[float], float], *args, **kwargs) -> List[float]: partial_func = partial(func, *args, **kwargs) return [partial_func(x) for x in self.data]"},{"question":"Objective The goal of this exercise is to assess your understanding of Seaborn\'s `plotting_context` function. You will create a function that generates a plot with different visual styles based on preset or custom contexts. Problem Statement Write a function called `create_styled_plot` that takes in two parameters: 1. `style`: a string indicating the style of the plot (predefined by seaborn) or a dictionary representing custom scaling parameters. 2. `data`: a pandas DataFrame containing the data to plot. Assume the DataFrame has at least two columns for simplicity. The function should: 1. Use `style` to set the plotting context. 2. Plot a line chart using the first two columns of the DataFrame. The first column should be used for the x-axis, and the second column for the y-axis. 3. Return the plotting context dictionary in which the plot was generated. Expected Function Signature ```python def create_styled_plot(style, data): # Your code here pass ``` Input - `style`: - A string representing a predefined style (e.g., \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\") or - A dictionary with custom scaling parameters (e.g., `{\\"axes.labelsize\\": 15, \\"xtick.labelsize\\": 12, \\"lines.linewidth\\": 2.0}`). - `data`: a pandas DataFrame with at least two columns. Output - A dictionary representing the plotting context applied when generating the plot. Constraints - The function should correctly handle both predefined styles and custom styles. - Assume the input DataFrame is always valid and correctly formatted for plotting. Example Usage ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample DataFrame data = pd.DataFrame({ \\"x\\": [\\"A\\", \\"B\\", \\"C\\"], \\"y\\": [1, 3, 2] }) # Example 1: Using a predefined style result1 = create_styled_plot(\\"talk\\", data) print(result1) # Example 2: Using a custom style custom_style = {\\"axes.labelsize\\": 12, \\"xtick.labelsize\\": 10, \\"lines.linewidth\\": 2.5} result2 = create_styled_plot(custom_style, data) print(result2) plt.show() ``` Note - Ensure to handle both types of `style` inputs appropriately. - Use context manager to apply the plotting context within the block.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_styled_plot(style, data): Generates a plot with the specified style and data. Parameters: style (str or dict): The style for the plot, either a predefined seaborn context or a custom dictionary of parameters. data (pd.DataFrame): The data to plot, assumes the DataFrame has at least two columns. Returns: dict: The plotting context that was applied. # Set the plotting context based on the style with sns.plotting_context(style): # Plotting the data plt.plot(data.iloc[:, 0], data.iloc[:, 1]) plt.xlabel(\\"X-Axis\\") plt.ylabel(\\"Y-Axis\\") plt.title(\\"Styled Plot\\") plt.show() # Retrieve the current plotting context context = sns.plotting_context(style) return context"},{"question":"Objective Demonstrate your understanding of the `pandas` `Resampler` class by performing various resampling and statistical operations on a time series dataset. Problem Statement You are provided with a time series dataset containing the daily average temperature of a city over a year. Your task is to perform the following operations: 1. Resample the data to obtain the monthly average temperatures. 2. Identify which month had the highest average temperature. 3. Resample the data to obtain the weekly maximum temperatures. 4. Fill any missing values introduced by the weekly resampling using forward fill. 5. Calculate the cumulative sum of temperatures for each week. Input Format - A `pandas` DataFrame `df` with the following structure: - `Date`: Date of the temperature record (Format: `YYYY-MM-DD`). - `Temperature`: Daily average temperature on the corresponding date. Example: ```plaintext Date Temperature 0 2023-01-01 34.5 1 2023-01-02 35.2 ... 364 2023-12-31 40.1 ``` Output Format - Output the results in the following variables: - `monthly_avg`: A DataFrame containing the monthly average temperatures. - `max_temp_month`: A string representing the month with the highest average temperature in the format `YYYY-MM`. - `weekly_max`: A DataFrame containing the weekly maximum temperatures with missing values filled using forward fill. - `weekly_cumsum`: A DataFrame containing the cumulative sum of temperatures for each week. Constraints - You may use any `pandas` functionalities/methods that are part of the module. - Ensure your solution is efficient and leverages `Resampler` methods effectively. Example Suppose the temperature data for the first week of January 2023 is as follows: ```plaintext Date Temperature 0 2023-01-01 30.0 1 2023-01-02 31.0 2 2023-01-03 32.0 3 2023-01-04 33.0 4 2023-01-05 34.0 5 2023-01-06 35.0 6 2023-01-07 36.0 ``` After performing the required operations: - `monthly_avg` would show the average temperature for January. - `max_temp_month` might be \\"2023-07\\" (if July had the highest average temperature). - `weekly_max` for the first week would show 36.0 for the first week, with missing dates filled. - `weekly_cumsum` for the first week would show cumulative sums like 30.0, 61.0, 93.0, etc. Your Implementation Implement your solution in the function `process_temperature_data(df)`: ```python import pandas as pd def process_temperature_data(df): # Your code here return monthly_avg, max_temp_month, weekly_max, weekly_cumsum ```","solution":"import pandas as pd def process_temperature_data(df): Perform required resampling and statistical operations on the temperature dataset. Parameters: df (DataFrame): A pandas DataFrame with \'Date\' and \'Temperature\' columns. Returns: tuple: Consisting of following DataFrames/strings: - monthly_avg: DataFrame with monthly average temperatures. - max_temp_month: String representing the month with highest average temperature. - weekly_max: DataFrame with weekly maximum temperatures with forward filled values. - weekly_cumsum: DataFrame with cumulative sum of temperatures for each week. # Ensure the \'Date\' column is in datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) # 1. Resample the data to obtain the monthly average temperatures monthly_avg = df.resample(\'M\').mean() # 2. Identify which month had the highest average temperature max_temp_month = monthly_avg[\'Temperature\'].idxmax().strftime(\'%Y-%m\') # 3. Resample the data to obtain the weekly maximum temperatures weekly_max = df.resample(\'W\').max() # 4. Fill any missing values introduced by the weekly resampling using forward fill weekly_max = weekly_max.ffill() # 5. Calculate the cumulative sum of temperatures for each week weekly_cumsum = df.resample(\'W\').sum().cumsum() return monthly_avg, max_temp_month, weekly_max, weekly_cumsum"},{"question":"# Asynchronous Web Scraping with asyncio (instead of asyncore) **Objective**: You are required to implement an asynchronous web scraping tool using the `asyncio` package. This tool should be able to fetch and print the content of multiple web pages concurrently. **Problem Statement**: Write a Python function `fetch_webpages(urls: list) -> None` that takes a list of URLs and asynchronously fetches the content of these web pages, printing the first 500 characters of each page\'s content. **Specifications**: - Use the `aiohttp` library for HTTP requests. You can install it using `pip install aiohttp`. - Implement proper error handling to manage HTTP errors and connection issues. - Make sure your solution efficiently handles the concurrency using `asyncio` event loop. - Do not use `asyncore` as it is deprecated; instead, use `asyncio`. **Function Signature**: ```python import aiohttp import asyncio async def fetch_webpages(urls: list) -> None: pass ``` **Example**: ```python urls = [ \'https://www.example.com\', \'https://www.python.org\', \'https://www.github.com\' ] await fetch_webpages(urls) ``` **Expected Output**: ``` Content from https://www.example.com: <!doctype html>... (First 500 characters of the webpage content) Content from https://www.python.org: <!doctype html>... (First 500 characters of the webpage content) Content from https://www.github.com: <!doctype html>... (First 500 characters of the webpage content) ``` **Constraints**: - The list `urls` will have at least one URL and no more than 20 URLs. - Each URL points to a valid web page that will respond within reasonable time limits. **Notes**: - Ensure your implementation is non-blocking and takes advantage of asynchronous programming. - Aim for a clean and efficient solution that can handle multiple web pages concurrently. Feel free to refer to the official `aiohttp` and `asyncio` documentation if needed.","solution":"import aiohttp import asyncio async def fetch_content(session, url): try: async with session.get(url) as response: response.raise_for_status() content = await response.text() print(f\\"Content from {url}:n{content[:500]}n\\") except Exception as e: print(f\\"Failed to fetch {url}: {e}\\") async def fetch_webpages(urls: list) -> None: async with aiohttp.ClientSession() as session: tasks = [fetch_content(session, url) for url in urls] await asyncio.gather(*tasks) # Example Usage: # urls = [ # \'https://www.example.com\', # \'https://www.python.org\', # \'https://www.github.com\' # ] # asyncio.run(fetch_webpages(urls))"},{"question":"**Objective:** Implement a function to manage file inclusion and exclusion based on the provided Unix-style glob patterns. **Function Signature:** ```python def manage_files(file_list, commands): This function takes a list of file paths and a list of command strings. It processes the commands to return a list of files that should be included based on the rules defined by the commands. Parameters: file_list (list): A list of file paths (strings). commands (list): A list of strings representing commands to include or exclude files. Returns: list: A list of file paths that match the include/exclude rules. pass ``` **Input:** - `file_list`: A list of file paths as strings. Example: `[\'src/module1.py\', \'src/module2.py\', \'data/settings.json\', \'scripts/install.py\']` - `commands`: A list of command strings. Each command is structured like the entries in the provided documentation. Example: `[\'include *.py\', \'exclude data/*\', \'recursive-include src *.py\']` **Output:** - The function should return a list of file paths that match the inclusion/exclusion rules provided in the commands. **Constraints:** - Assume that all file paths are valid and no path contains special characters other than `-`, `_`, `.` or `/`. - Commands will be provided in the order they should be applied. - The function must handle `include`, `exclude`, `recursive-include`, `recursive-exclude`, `global-include`, `global-exclude`, `prune`, and `graft` commands. # Example: ```python files = [ \'src/module1.py\', \'src/module2.py\', \'src/sub/extra.py\', \'data/settings.json\', \'data/config.yaml\', \'scripts/install.py\', \'README.md\' ] commands = [ \'include *.py\', \'exclude data/*\', \'recursive-include src *.py\', \'global-include *.md\' ] assert manage_files(files, commands) == [ \'src/module1.py\', \'src/module2.py\', \'src/sub/extra.py\', \'scripts/install.py\', \'README.md\' ] ``` # Additional Notes: 1. You may use the `glob` and `fnmatch` libraries for pattern matching. 2. Care must be taken to handle the recursive and global commands properly. 3. The order of commands affects the final list of files. For example, if a file is included and then later excluded by a subsequent command, it should be excluded in the final result.","solution":"import fnmatch import os def match_files(pattern, files): return [file for file in files if fnmatch.fnmatch(file, pattern)] def recursive_match_files(directory, pattern, files): matched_files = [] for file in files: if file.startswith(directory): relative_path = file[len(directory):].lstrip(\'/\') if fnmatch.fnmatch(relative_path, pattern): matched_files.append(file) return matched_files def manage_files(file_list, commands): included_files = set() for command in commands: cmd_parts = command.split(maxsplit=1) if len(cmd_parts) < 2: continue cmd_type = cmd_parts[0] pattern = cmd_parts[1] if cmd_type == \'include\': included_files.update(match_files(pattern, file_list)) elif cmd_type == \'exclude\': included_files = {file for file in included_files if not fnmatch.fnmatch(file, pattern)} included_files.difference_update(match_files(pattern, file_list)) elif cmd_type == \'recursive-include\': directory, pattern = pattern.split(maxsplit=1) included_files.update(recursive_match_files(directory, pattern, file_list)) elif cmd_type == \'recursive-exclude\': directory, pattern = pattern.split(maxsplit=1) excluded_files = recursive_match_files(directory, pattern, file_list) included_files.difference_update(excluded_files) elif cmd_type == \'global-include\': included_files.update(match_files(pattern, file_list)) elif cmd_type == \'global-exclude\': included_files = {file for file in included_files if not fnmatch.fnmatch(file, pattern)} included_files.difference_update(match_files(pattern, file_list)) elif cmd_type == \'prune\': excluded_files = {file for file in file_list if file.startswith(pattern)} included_files.difference_update(excluded_files) elif cmd_type == \'graft\': graft_files = {file for file in file_list if file.startswith(pattern)} included_files.update(graft_files) return list(included_files)"},{"question":"**Question**: Implement a Python function that mimics the behavior of creating a floating point number from a string and safely converting it into a double, depending on whether the input is valid. Your task involves: 1. Checking if the given input is a valid float string. 2. Creating a float object from the valid string. 3. Converting this float object to a double. 4. Handling invalid inputs gracefully without causing a program crash. You must use the provided functions `PyFloat_Check`, `PyFloat_CheckExact`, `PyFloat_FromString`, and `PyFloat_AsDouble` to demonstrate your comprehension of Python\'s C-API-like functions in these tasks. **Function Signature**: ```python def float_string_to_safe_double(float_str: str) -> float: pass ``` **Input**: - `float_str`: A string that is intended to represent a floating point number. **Output**: - Returns a float representation of the string if it is valid. - Returns `None` if the string is not a valid floating point representation. **Constraints**: - You cannot use Python\'s built-in float conversion directly. - You must handle errors gracefully. **Examples**: ```python # Example 1 float_str = \\"123.456\\" result = float_string_to_safe_double(float_str) print(result) # Expected output: 123.456 # Example 2 float_str = \\"not_a_float\\" result = float_string_to_safe_double(float_str) print(result) # Expected output: None ``` Note: In an actual Python environment, you won\'t directly use the C-API functions but mimic their behavior using equivalent Python checks and conversions.","solution":"def PyFloat_Check(float_str): Mimics checking if a given string can be a valid float. try: float(float_str) return True except ValueError: return False def PyFloat_CheckExact(float_str): Mimics exact checking if the input string is a valid float. It behaves the same as PyFloat_Check here for simplicity. return PyFloat_Check(float_str) def PyFloat_FromString(float_str): Converts a valid float string to a float object. if PyFloat_CheckExact(float_str): return float(float_str) else: raise ValueError(\\"Invalid float string\\") def PyFloat_AsDouble(float_obj): Mimics converting a float object to a double. In Python, float is double precision by default. return float(float_obj) def float_string_to_safe_double(float_str: str) -> float: try: float_obj = PyFloat_FromString(float_str) return PyFloat_AsDouble(float_obj) except ValueError: return None"},{"question":"# Bytearray Manipulation and Combination In this task, you will create a function that manipulates and combines `bytearray` objects using the provided direct API functions and macros. You are expected to use the C-API for bytearrays where necessary. # Problem Statement You need to implement a function: ```python def manipulate_and_combine_bytearrays(input_bytes: list) -> bytearray: ``` Input - `input_bytes`: A list of tuples. Each tuple contains two elements: - A `string` representing characters. - An integer `n` which represents the number of times to repeat the string to create a `bytearray`. For example: `[(\\"abc\\", 3), (\\"123\\", 2)]` Output - A `bytearray` object which is the result of: 1. Repeating each string `n` times to create a `bytearray`. 2. Concatenating all the `bytearray` objects resulting from step 1. Instructions 1. Use the provided C-API functions and macros such as `PyByteArray_FromStringAndSize`, `PyByteArray_Concat`, `PyByteArray_Size`, etc. 2. Handle any errors gracefully. 3. Aim for efficient memory use and performance. Example ```python result = manipulate_and_combine_bytearrays([(\\"abc\\", 3), (\\"123\\", 2)]) print(result) # Output should be bytearray(b\'abcabcabc123123\') ``` # Constraints - The maximum length for any string is 1000 characters. - The maximum value for `n` is 100. - Ensure that you handle NULL pointers and other potential issues gracefully. Performance - The function should have a time complexity not worse than O(m*n) where `m` is the average length of strings and `n` is the number of tuples in the input list. - Minimize the number of allocations and copies performed.","solution":"def manipulate_and_combine_bytearrays(input_bytes: list) -> bytearray: Function to manipulate and combine bytearrays based on input list of tuples. Each tuple contains a string and an integer n, which specifies the number of times to repeat the string to create a bytearray. Finally, all bytearrays are concatenated. combined_bytearray = bytearray() for string, n in input_bytes: # Repeat the string \'n\' times and create a bytearray repeated_string = string * n bytearray_segment = bytearray(repeated_string, \'utf-8\') # Concatenate the bytearray segment to the combined bytearray combined_bytearray.extend(bytearray_segment) return combined_bytearray"},{"question":"# Question: Custom ZIP Archive Importer You need to implement a custom ZIP archive importer to import Python modules from a specific directory within a given ZIP file. Given the path to a ZIP file and a subdirectory within it, you will use the `zipimport` module to write a function that can: 1. Validate the ZIP file and subdirectory. 2. List all the Python modules (`.py` and `.pyc` files) within that subdirectory. 3. Import a specific module from the subdirectory. 4. Handle errors appropriately, raising `zipimport.ZipImportError` on failures. # Function Signature: ```python def custom_zip_importer(zip_path: str, subdirectory: str, module_name: str) -> object: pass ``` # Parameters: - `zip_path` (str): The file path to the ZIP archive. - `subdirectory` (str): The internal directory within the ZIP from which modules should be imported. - `module_name` (str): The Python module to import. # Returns: - The imported module object if successful. # Constraints: - Only `.py` and `.pyc` files should be considered. - Use `zipimport.zipimporter` methods to fulfill each step. - Raise `zipimport.ZipImportError` if the ZIP file or subdirectory is invalid, or if the module cannot be found or imported. # Example: ```python # Assuming example.zip contains a subdirectory \'lib\' with a module \'utils.py\' in it. zip_path = \'example.zip\' subdirectory = \'lib\' module_name = \'utils\' module = custom_zip_importer(zip_path, subdirectory, module_name) print(module) # Should print the module object if successful ``` # Additional Notes: - Ensure proper exception handling to raise `zipimport.ZipImportError` when necessary. - Utilize appropriate `zipimport.zipimporter` methods to implement module importing functionality.","solution":"import zipimport import os def custom_zip_importer(zip_path: str, subdirectory: str, module_name: str) -> object: Import a Python module from a subdirectory within a ZIP file. Parameters: zip_path (str): The file path to the ZIP archive. subdirectory (str): The internal directory within the ZIP from which modules should be imported. module_name (str): The Python module to import. Returns: object: The imported module object if successful. Raises: zipimport.ZipImportError: If the ZIP file or subdirectory is invalid, or if the module cannot be found or imported. try: # Construct the full path to the subdirectory within the ZIP file archive_path = os.path.join(zip_path, subdirectory) # Ensure the ZIP file and subdirectory exist if not os.path.exists(zip_path): raise zipimport.ZipImportError(\\"ZIP file does not exist.\\") importer = zipimport.zipimporter(archive_path) # Import the specified module module = importer.load_module(module_name) return module except FileNotFoundError: raise zipimport.ZipImportError(\\"Subdirectory does not exist in the ZIP file.\\") except ImportError as e: raise zipimport.ZipImportError(f\\"Module \'{module_name}\' could not be imported: {str(e)}\\")"},{"question":"**Coding Assessment Question: Managing Processes Using Python\'s `os` Module** # Objective: Implement a Python function to manage child processes, handle environment variables, and utilize inter-process communication through pipes. # Problem Statement: You are required to write a function named `multi_process_manager` that does the following: 1. Takes a list of commands (each command represented as a list of strings where the first item is the command and the subsequent items are its arguments). 2. For each command, create a child process using `os.fork()`. 3. The child process should execute the command using `os.execvp(command[0], command)`. Each command should be executed in a separate process. Ensure that environment variables are appropriately handled. 4. The parent process should set an environment variable named `CHILD_STATUS` to keep track of each child\'s exit status. 5. Use pipes to communicate from the child process back to the parent process a message indicating successful execution or an error message if execution fails. 6. Collect and return a dictionary in the parent process containing the command as the key and a tuple as the value. The tuple should contain the respective exit status and the message received through the pipe from each child. # Input: - A list of commands (each command is a list of strings). # Output: - A dictionary where each key is a command (as a tuple), and each value is a tuple of the exit status and the message from the child process. # Constraints: - Ensure the solution does not cause zombie processes. - Proper exception handling and cleanup of resources (e.g., closing pipes) should be considered. - Commands executed are simple and well-behaved (no need to handle complex scripts or commands). # Example: ```python commands = [ [\'ls\', \'-l\'], [\'echo\', \'hello world\'], [\'sleep\', \'5\'] ] result = multi_process_manager(commands) # Potential output (exit status codes are hypothetical): { (\'ls\', \'-l\'): (0, \'Success: Command executed with status 0\'), (\'echo\', \'hello world\'): (0, \'Success: Command executed with status 0\'), (\'sleep\', \'5\'): (0, \'Success: Command executed with status 0\'), } ``` # Function Signature: ```python def multi_process_manager(commands: List[List[str]]) -> Dict[Tuple[str, ...], Tuple[int, str]]: pass ``` # Notes: - Use `os.pipe()`, `os.fork()`, `os.execvp()`, `os.wait()`, and other relevant `os` module functions. - You might need `os.environ` to manage environment variables. - Handle process termination with `os.waitpid()` and ensure no zombie processes are left. - Properly implement inter-process communication to send and receive messages.","solution":"import os from typing import List, Dict, Tuple def multi_process_manager(commands: List[List[str]]) -> Dict[Tuple[str, ...], Tuple[int, str]]: results = {} for command in commands: read_fd, write_fd = os.pipe() pid = os.fork() if pid == 0: # This is the child process os.close(read_fd) # Close the read end of the pipe in the child try: os.execvp(command[0], command) except Exception as e: os.write(write_fd, str(e).encode()) os._exit(1) # Exit the child process with an error code else: # This is the parent process os.close(write_fd) # Close the write end of the pipe in the parent pid, status = os.waitpid(pid, 0) # Wait for the child process to terminate exit_status = os.WEXITSTATUS(status) message = os.read(read_fd, 1024).decode() # Read from the pipe if status == 0: message = \'Success: Command executed with status 0\' os.environ[\\"CHILD_STATUS\\"] = str(exit_status) results[tuple(command)] = (exit_status, message) os.close(read_fd) # Close the read end of the pipe return results"},{"question":"# Seaborn Assessment Question You are tasked with visualizing the Palmer Penguins dataset using Seaborn\'s object-oriented interface. Specifically, you need to demonstrate your understanding of the `so.Jitter` transform. Follow the steps below to complete the assessment. Dataset Load the Palmer Penguins dataset using the following code: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") ``` Tasks 1. **Basic Jitter Plot**: Create a jitter plot that shows the species on the x-axis and body_mass_g on the y-axis. Apply a small amount of jitter along the y-axis. 2. **Controlled Jitter Width**: Create another jitter plot showing the species on the x-axis and body_mass_g on the y-axis, but this time control the jitter amount using the `width` parameter with a value of `0.3`. 3. **Dual Controlled Jitter**: Create a jitter plot with body_mass_g on the x-axis and flipper_length_mm on the y-axis. Control the jitter on both axes using the `x` parameter set to `200` and the `y` parameter set to `5`. 4. **Jitter with Numeric Orientation Axis**: Create a jitter plot with rounded body_mass_g (rounded to the nearest 1000) on the x-axis and flipper_length_mm on the y-axis, and apply a small amount of jitter. Additional Instructions - You are free to style the plots and add labels or titles to make them more informative. - Ensure the plots are generated correctly with the specified jitter where applicable. Expected Output You need to submit a Python script or Jupyter Notebook that: 1. Loads the dataset. 2. Generates the required plots as described above. 3. Ensures the jitter parameters are correctly applied and visible in the plots. Constraints - Use only the Seaborn library for visualization. - Follow the instructions for applying jitter as specified in the tasks. - Make sure to handle any missing data appropriately, if necessary. This assessment will test your understanding of Seaborn\'s object-oriented interface and your ability to create and customize jitter plots effectively.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") def create_basic_jitter_plot(dataset): Creates a basic jitter plot with species on the x-axis and body_mass_g on the y-axis with slight y-axis jitter. plot = (so.Plot(dataset, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter())) return plot def create_controlled_jitter_plot(dataset): Creates a jitter plot with species on the x-axis and body_mass_g on the y-axis, controlling jitter using width=0.3. plot = (so.Plot(dataset, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.3))) return plot def create_dual_controlled_jitter_plot(dataset): Creates a jitter plot with body_mass_g on the x-axis and flipper_length_mm on the y-axis with controlled jitter on both axes. plot = (so.Plot(dataset, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=200, y=5))) return plot def create_jitter_plot_numeric_orientation(dataset): Creates a jitter plot with rounded body_mass_g (rounded to nearest 1000) on the x-axis and flipper_length_mm on the y-axis. dataset[\\"rounded_body_mass_g\\"] = dataset[\\"body_mass_g\\"].round(-3) plot = (so.Plot(dataset, x=\\"rounded_body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter())) return plot"},{"question":"Objective Implement a function that modifies an AU audio file by altering its playback speed and saving it as a new file. Problem Statement Given an AU audio file, implement a function `change_audio_speed(input_file: str, output_file: str, speed_factor: float) -> None` that reads the audio data from `input_file`, changes the playback speed by the given `speed_factor`, and writes the modified audio data to `output_file`. - `input_file (str)`: Path to the input AU file. - `output_file (str)`: Path to the output AU file where the modified audio will be saved. - `speed_factor (float)`: Factor by which to change the speed of the audio. For example, a `speed_factor` of 2.0 doubles the speed, and a `speed_factor` of 0.5 halves the speed. Constraints 1. The function should handle mono and stereo audio files. 2. Assume the input file is a valid AU file. 3. The speed factor will be a positive float. Implementation Details 1. Use the `sunau` module to read from and write to AU files. 2. Adjust the frame rate according to the `speed_factor`. 3. Ensure the output file maintains the same sample width and number of channels as the input file. 4. Handle errors gracefully, raising appropriate exceptions for invalid inputs. Example ```python def change_audio_speed(input_file: str, output_file: str, speed_factor: float) -> None: import sunau if speed_factor <= 0: raise ValueError(\\"speed_factor must be a positive float\\") with sunau.open(input_file, \'r\') as input_au: nchannels = input_au.getnchannels() sampwidth = input_au.getsampwidth() framerate = input_au.getframerate() nframes = input_au.getnframes() comptype = input_au.getcomptype() compname = input_au.getcompname() new_framerate = int(framerate * speed_factor) audio_data = input_au.readframes(nframes) with sunau.open(output_file, \'w\') as output_au: output_au.setnchannels(nchannels) output_au.setsampwidth(sampwidth) output_au.setframerate(new_framerate) output_au.setnframes(nframes) output_au.setcomptype(comptype, compname) output_au.writeframes(audio_data) # Example usage change_audio_speed(\'input.au\', \'output.au\', 1.5) ``` The function changes the playback speed of the given AU audio file according to `speed_factor` and writes the modified audio data to a new file.","solution":"def change_audio_speed(input_file: str, output_file: str, speed_factor: float) -> None: import sunau if speed_factor <= 0: raise ValueError(\\"speed_factor must be a positive float\\") with sunau.open(input_file, \'rb\') as input_au: nchannels = input_au.getnchannels() sampwidth = input_au.getsampwidth() framerate = input_au.getframerate() nframes = input_au.getnframes() comptype = input_au.getcomptype() compname = input_au.getcompname() audio_data = input_au.readframes(nframes) new_framerate = int(framerate * speed_factor) with sunau.open(output_file, \'wb\') as output_au: output_au.setnchannels(nchannels) output_au.setsampwidth(sampwidth) output_au.setframerate(new_framerate) output_au.setnframes(nframes) output_au.setcomptype(comptype, compname) output_au.writeframes(audio_data)"},{"question":"**Title: Implementing and Utilizing Generators in Python** **Objective:** Design a generator in Python that processes a given list of numbers to yield only those which are prime. Additionally, implement a function that utilizes this generator to return the sum of the first N prime numbers generated. **Function Definitions:** 1. **is_prime(n: int) -> bool** - **Input:** An integer `n`. - **Output:** Returns `True` if `n` is a prime number, `False` otherwise. 2. **prime_generator(numbers: list) -> generator** - **Input:** A list of integers `numbers`. - **Output:** A generator that yields prime numbers from the `numbers` list. 3. **sum_of_primes(numbers: list, N: int) -> int** - **Input:** A list of integers `numbers`, and an integer `N`. - **Output:** Returns the sum of the first `N` prime numbers generated from the `numbers` list. **Details:** - The generator should only yield values that are confirmed to be prime. - The `prime_generator` function should utilize the `is_prime` function to check for primality. - The `sum_of_primes` function should consume the generator created by `prime_generator` to compute the sum of the first `N` prime numbers. **Constraints:** - The list `numbers` will contain between 1 and 1000 integers. - The integer `N` will be in the range 1 to 100. **Performance Requirements:** - Ensure the `is_prime` function is efficient and minimizes unnecessary calculations. **Example:** Given the input list `[2, 3, 4, 5, 6, 7, 8, 9, 10]` and `N = 3`: ```python # prime_generator should yield: 2, 3, 5, 7 # sum_of_primes should return: 2 + 3 + 5 = 10 assert sum_of_primes([2, 3, 4, 5, 6, 7, 8, 9, 10], 3) == 10 ``` # Task Implement the following functions according to the specifications provided: 1. **is_prime(n: int) -> bool** 2. **prime_generator(numbers: list) -> generator** 3. **sum_of_primes(numbers: list, N: int) -> int** **Note:** You are required to handle edge cases appropriately, such as lists with no prime numbers.","solution":"import math def is_prime(n: int) -> bool: Returns True if n is a prime number, False otherwise. if n <= 1: return False if n == 2: return True if n % 2 == 0: return False for i in range(3, int(math.sqrt(n)) + 1, 2): if n % i == 0: return False return True def prime_generator(numbers: list): Generator that yields prime numbers from the given list of numbers. for number in numbers: if is_prime(number): yield number def sum_of_primes(numbers: list, N: int) -> int: Returns the sum of the first N prime numbers generated from the numbers list. primes = prime_generator(numbers) total = 0 count = 0 for prime in primes: total += prime count += 1 if count == N: break return total"},{"question":"<|Analysis Begin|> The provided documentation is for the \\"quopri\\" module which deals with encoding and decoding quoted-printable MIME data as per RFC 1521. This module offers functions to perform encoding and decoding operations on file objects as well as on byte strings. Functions available in the documentation: 1. `quopri.decode(input, output, header=False)` - Decodes data from an input binary file and writes it to an output binary file. 2. `quopri.encode(input, output, quotetabs, header=False)` - Encodes data from an input binary file and writes it to an output binary file. 3. `quopri.decodestring(s, header=False)` - Decodes a byte string and returns the decoded byte string. 4. `quopri.encodestring(s, quotetabs=False, header=False)` - Encodes a byte string and returns the encoded byte string. Key concepts to focus on: - Handling of binary file objects - Understanding of optional parameters (`header` and `quotetabs`) - Handling of byte strings versus file objects - Differences between quoted-printable and base64 encoding Possible areas of application in a coding question: - Implementing utilities to work with the `quopri` module methods. - Creating a command-line tool that makes use of these encoding and decoding methods. - Elaborating scenarios that involve MIME type data processing. <|Analysis End|> <|Question Begin|> # Quoted-Printable MIME Data Processing Utility **Problem Statement:** You are required to create a utility that performs quoted-printable encoding and decoding of MIME data. Your utility should provide two main functionalities: 1. Encode a given byte string using quoted-printable encoding. 2. Decode a given quoted-printable encoded byte string. To accomplish this, you are required to implement two functions: `encode_mime_data` and `decode_mime_data`. **Function 1:** `encode_mime_data` **Input:** - A byte string `data` that needs to be encoded. - An optional boolean parameter `quotetabs` which defaults to `False`. - An optional boolean parameter `header` which defaults to `False`. **Output:** - A quoted-printable encoded byte string. **Function Signature:** ```python def encode_mime_data(data: bytes, quotetabs: bool = False, header: bool = False) -> bytes: pass ``` **Function 2:** `decode_mime_data` **Input:** - A quoted-printable encoded byte string `data`. - An optional boolean parameter `header` which defaults to `False`. **Output:** - The decoded byte string. **Function Signature:** ```python def decode_mime_data(data: bytes, header: bool = False) -> bytes: pass ``` **Constraints:** - The input `data` for `encode_mime_data` and `decode_mime_data` is guaranteed to be in proper byte string format. - You must use the `quopri` module\'s `encodestring` and `decodestring` functions to implement the encoding and decoding operations. - Ensure to handle the optional parameters properly to guarantee correct encoding/decoding according to RFC 1521 and RFC 1522 specifications. **Examples:** ```python # Example 1: Encoding input_data = b\'This is a test string with some special chars: = and ? and _\' encoded_data = encode_mime_data(input_data, quotetabs=True, header=True) print(encoded_data) # Output: b\'This is a test string with some special chars: =3D and ? and =5F\' # Example 2: Decoding encoded_data = b\'This is a test string with some special chars: =3D and ? and =5F\' decoded_data = decode_mime_data(encoded_data, header=True) print(decoded_data) # Output: b\'This is a test string with some special chars: = and ? and _\' ``` Your functions should correctly handle these inputs and demonstrate robust MIME data processing capabilities using the `quopri` module.","solution":"import quopri def encode_mime_data(data: bytes, quotetabs: bool = False, header: bool = False) -> bytes: Encodes a byte string using quoted-printable encoding. Parameters: data (bytes): The byte string to encode. quotetabs (bool): Whether to quote tabs and spaces (default is False). header (bool): Whether the string is a header (default is False). Returns: bytes: Quoted-printable encoded byte string. return quopri.encodestring(data, quotetabs=quotetabs, header=header) def decode_mime_data(data: bytes, header: bool = False) -> bytes: Decodes a quoted-printable encoded byte string. Parameters: data (bytes): The quoted-printable encoded byte string to decode. header (bool): Whether the string is a header (default is False). Returns: bytes: Decoded byte string. return quopri.decodestring(data, header=header)"},{"question":"<|Analysis Begin|> The provided documentation covers various aspects of working with date and time objects in Python, particularly within the \\"datetime\\" module. It includes type-checking macros, creation macros, and field extraction macros. From this, I can craft a question that requires students to demonstrate their understanding of creating and manipulating datetime objects in Python. The question will involve implementing a function that takes in date and time components, performs certain operations (like time zone adjustments or delta calculations), and returns specific parts of the resulting date/time. This will test their comprehension of both creating and manipulating datetime objects. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective**: Demonstrate your understanding of Python\'s datetime module by creating and manipulating date and time objects. # Problem Statement You are required to implement the following function: ```python def datetime_operations(year: int, month: int, day: int, hour: int, minute: int, second: int, usecond: int, delta_days: int) -> tuple: Args: - year: An integer representing the year of the date. - month: An integer representing the month of the date (1-12). - day: An integer representing the day of the month (1-31). - hour: An integer representing the hour of the time (0-23). - minute: An integer representing the minute of the time (0-59). - second: An integer representing the second of the time (0-59). - usecond: An integer representing the microsecond of the time (0-999999). - delta_days: An integer representing the number of days to add to the given date and time. Returns: - A tuple containing: 1. The year of the new datetime after adding `delta_days` 2. The month of the new datetime 3. The day of the new datetime 4. The hour of the new datetime 5. The minute of the new datetime 6. The second of the new datetime 7. The microsecond of the new datetime 8. The number of days in the resulting timedelta (from the original datetime) 9. The number of seconds in the resulting timedelta 10. The number of microseconds in the resulting timedelta pass ``` # Constraints 1. `year` will be a positive integer. 2. `month` will be an integer between 1 and 12. 3. `day` will be an integer between 1 and 31. 4. `hour` will be an integer between 0 and 23. 5. `minute` will be an integer between 0 and 59. 6. `second` will be an integer between 0 and 59. 7. `usecond` will be an integer between 0 and 999999. 8. `delta_days` can be either a positive or negative integer. # Example ```python year = 2023 month = 10 day = 1 hour = 12 minute = 30 second = 45 usecond = 500000 delta_days = 5 result = datetime_operations(year, month, day, hour, minute, second, usecond, delta_days) # Expected result -> (2023, 10, 6, 12, 30, 45, 500000, 5, 43200, 500000) print(result) ``` # Note - You must make use of the datetime module and its various functionalities for date/time creation and manipulation. - Ensure that the returned tuple provides accurate information after the addition of `delta_days`.","solution":"from datetime import datetime, timedelta def datetime_operations(year: int, month: int, day: int, hour: int, minute: int, second: int, usecond: int, delta_days: int) -> tuple: Args: - year: An integer representing the year of the date. - month: An integer representing the month of the date (1-12). - day: An integer representing the day of the month (1-31). - hour: An integer representing the hour of the time (0-23). - minute: An integer representing the minute of the time (0-59). - second: An integer representing the second of the time (0-59). - usecond: An integer representing the microsecond of the time (0-999999). - delta_days: An integer representing the number of days to add to the given date and time. Returns: - A tuple containing: 1. The year of the new datetime after adding `delta_days` 2. The month of the new datetime 3. The day of the new datetime 4. The hour of the new datetime 5. The minute of the new datetime 6. The second of the new datetime 7. The microsecond of the new datetime 8. The number of days in the resulting timedelta (from the original datetime) 9. The number of seconds in the resulting timedelta 10. The number of microseconds in the resulting timedelta # Create the initial datetime object initial_datetime = datetime(year, month, day, hour, minute, second, usecond) # Calculate the new datetime after adding delta_days delta = timedelta(days=delta_days) new_datetime = initial_datetime + delta # Calculate the time difference difference = new_datetime - initial_datetime # Prepare the result tuple result = ( new_datetime.year, new_datetime.month, new_datetime.day, new_datetime.hour, new_datetime.minute, new_datetime.second, new_datetime.microsecond, difference.days, difference.seconds, difference.microseconds ) return result"},{"question":"Objective Demonstrate your understanding of the Python `stat` module by writing a function to recursively traverse a directory and count files of different types. Problem Statement Write a Python function: ```python def count_file_types(directory_path: str) -> dict: Recursively traverse the directory specified by `directory_path` and count the number of different file types using the `stat` module functions. Return a dictionary with the counts of each file type. Parameters: - directory_path (str): The path to the directory to be traversed. Returns: - dict: A dictionary with keys being file types (\'directory\', \'regular\', \'symbolic_link\', \'block\', \'character\', \'fifo\', \'socket\', \'door\', \'port\', \'whiteout\') and values being the counts of each file type. pass ``` Constraints 1. You should use the functions provided by the `stat` module (e.g., `stat.S_ISDIR`, `stat.S_ISREG`) to determine each file\'s type. 2. Ensure to handle potential exceptions (such as permission errors) during directory traversal gracefully. 3. The function should be efficient and avoid unnecessary repeated traversals. Example Usage ```python example_directory = \\"/path/to/directory\\" # Example Output: # { # \'directory\': 5, # \'regular\': 20, # \'symbolic_link\': 3, # \'block\': 1, # \'character\': 2, # \'fifo\': 0, # \'socket\': 0, # \'door\': 0, # \'port\': 0, # \'whiteout\': 0 # } result = count_file_types(example_directory) print(result) ``` Evaluation Criteria - Correct usage of the `stat` module functions. - Accuracy in counting various file types. - Proper handling of directory traversal and exceptions. - Clean and well-documented code.","solution":"import os import stat def count_file_types(directory_path: str) -> dict: Recursively traverse the directory specified by `directory_path` and count the number of different file types using the `stat` module functions. Return a dictionary with the counts of each file type. Parameters: - directory_path (str): The path to the directory to be traversed. Returns: - dict: A dictionary with keys being file types (\'directory\', \'regular\', \'symbolic_link\', \'block\', \'character\', \'fifo\', \'socket\', \'door\', \'port\', \'whiteout\') and values being the counts of each file type. file_type_counts = { \'directory\': 0, \'regular\': 0, \'symbolic_link\': 0, \'block\': 0, \'character\': 0, \'fifo\': 0, \'socket\': 0, \'door\': 0, \'port\': 0, \'whiteout\': 0 } try: for root, dirs, files in os.walk(directory_path): for name in dirs + files: path = os.path.join(root, name) try: file_stat = os.lstat(path) mode = file_stat.st_mode if stat.S_ISDIR(mode): file_type_counts[\'directory\'] += 1 elif stat.S_ISREG(mode): file_type_counts[\'regular\'] += 1 elif stat.S_ISLNK(mode): file_type_counts[\'symbolic_link\'] += 1 elif stat.S_ISBLK(mode): file_type_counts[\'block\'] += 1 elif stat.S_ISCHR(mode): file_type_counts[\'character\'] += 1 elif stat.S_ISFIFO(mode): file_type_counts[\'fifo\'] += 1 elif stat.S_ISSOCK(mode): file_type_counts[\'socket\'] += 1 except (FileNotFoundError, PermissionError): # Skip files that cannot be accessed continue except Exception as e: print(f\\"An error occurred while traversing the directory: {e}\\") return file_type_counts"},{"question":"# HTTP Status Code Handler **Objective:** Implement functions to handle HTTP responses using the `http.HTTPStatus` enum. The task will assess your comprehension of using enumerations and handling HTTP status codes in Python. # Task 1. **Function `get_status_info(code: int) -> Tuple[int, str, str]:`** - Implement a function that takes an HTTP status code as input and returns a tuple containing: - The status code. - The reason phrase associated with the status code. - The description of the status code. - If the status code provided is invalid, return the tuple `(0, \\"INVALID_CODE\\", \\"The provided status code is not a valid HTTP status code\\")`. 2. **Function `is_client_error(code: int) -> bool:`** - Implement a function that takes an HTTP status code as input to determine whether it is a client error (i.e., status codes in the 400-499 range). - Return `True` if the code is a client error, otherwise return `False`. 3. **Function `is_server_error(code: int) -> bool:`** - Implement a function that takes an HTTP status code as input to determine whether it is a server error (i.e., status codes in the 500-599 range). - Return `True` if the code is a server error, otherwise return `False`. # Input and Output Formats - Input: An integer representing an HTTP status code. - Output: - For `get_status_info` - A tuple containing the status code, reason phrase, and description. - For `is_client_error` and `is_server_error` - A boolean value. # Constraints - The status code will be an integer. - Assume the range of status codes follows the standard HTTP status code listings. # Examples 1. **Example for `get_status_info(code: int) -> Tuple[int, str, str]:`** ```python >>> get_status_info(200) (200, \'OK\', \'Request fulfilled, document follows\') >>> get_status_info(418) (418, \'IM_A_TEAPOT\', \'HTCPCP/1.0\') >>> get_status_info(999) (0, \'INVALID_CODE\', \'The provided status code is not a valid HTTP status code\') ``` 2. **Example for `is_client_error(code: int) -> bool:`** ```python >>> is_client_error(404) True >>> is_client_error(200) False ``` 3. **Example for `is_server_error(code: int) -> bool:`** ```python >>> is_server_error(500) True >>> is_server_error(404) False ``` # Notes - Use the `http.HTTPStatus` enum for handling the status codes and their information. - Ensure that your functions handle edge cases and invalid status codes gracefully.","solution":"from http import HTTPStatus from typing import Tuple def get_status_info(code: int) -> Tuple[int, str, str]: Takes an HTTP status code as input and returns a tuple containing: - The status code. - The reason phrase associated with the status code. - The description of the status code. If the status code provided is invalid, return the tuple (0, \\"INVALID_CODE\\", \\"The provided status code is not a valid HTTP status code\\"). try: status = HTTPStatus(code) return (status.value, status.phrase, status.description) except ValueError: return (0, \\"INVALID_CODE\\", \\"The provided status code is not a valid HTTP status code\\") def is_client_error(code: int) -> bool: Takes an HTTP status code as input and returns True if the code is a client error (i.e., status codes in the 400-499 range), otherwise returns False. return 400 <= code < 500 def is_server_error(code: int) -> bool: Takes an HTTP status code as input and returns True if the code is a server error (i.e., status codes in the 500-599 range), otherwise returns False. return 500 <= code < 600"},{"question":"**Objective:** Implement a function `read_chunks` that reads all chunks from a given file, extracts their IDs and sizes, and returns this information as a list of tuples. **Function Signature:** ```python def read_chunks(file_path: str) -> List[Tuple[str, int]]: pass ``` **Input:** - `file_path` (str): The path to the file that contains chunks in the EA IFF 85 format. **Output:** - A list of tuples, where each tuple contains the chunk ID (a 4-character string) and the chunk size (an integer). **Constraints:** 1. The file can be very large, so the solution should be efficient in terms of memory usage. 2. The function should handle cases where the file ends unexpectedly (i.e., an incomplete chunk). 3. The file may contain any number of chunks (including zero). **Example:** Assume the file at `file_path` contains the following chunks: - Chunk 1: ID = \'FORM\', Size = 24 - Chunk 2: ID = \'AIFF\', Size = 36 ```python read_chunks(\\"example.iff\\") ``` **Expected Output:** ```python [(\'FORM\', 24), (\'AIFF\', 36)] ``` **Guidelines:** 1. Use the `chunk` module to read the file. 2. Initialize a `chunk.Chunk` instance for each chunk in the file. 3. Extract the chunk ID and size using the `getname` and `getsize` methods. 4. Handle any potential exceptions, such as `EOFError`, appropriately. You are encouraged to test your function with various types of chunked files to ensure its correctness and robustness. **Note:** As the `chunk` module is deprecated in Python 3.11, this coding assessment is intended for Python versions prior to 3.11.","solution":"import os from typing import List, Tuple def read_chunks(file_path: str) -> List[Tuple[str, int]]: chunks = [] with open(file_path, \\"rb\\") as f: while True: try: # Read the chunk ID (4 bytes) chunk_id = f.read(4) # Break if end of file is reached if not chunk_id: break # Read the chunk size (4 bytes) chunk_size_bytes = f.read(4) # Ensure there are enough bytes to read the chunk size if len(chunk_size_bytes) < 4: break chunk_size = int.from_bytes(chunk_size_bytes, byteorder=\'big\') # Store the Chunk ID and Size chunks.append((chunk_id.decode(), chunk_size)) # Skip the chunk data f.seek(chunk_size, os.SEEK_CUR) # Read padding byte if chunk_size is odd if chunk_size % 2 == 1: f.seek(1, os.SEEK_CUR) except EOFError: break return chunks"},{"question":"Objective: Design and implement a Python program utilizing the `contextvars` module to handle context-specific state within an asynchronous framework. Problem Statement: You are tasked with developing a URL shortener service with a basic logging feature. Each request to shorten a URL will have a unique request ID. Using `contextvars`, ensure that the request ID is properly logged and managed across the asynchronous tasks without leaking into other requests. # Specifications: 1. **Context Variable**: - Create a `ContextVar` named `request_id_var` to store the request ID. 2. **Asynchronous Function**: - Define an asynchronous function `shorten_url(request_id, url)` that performs the following steps: - Sets the `request_id` in the `request_id_var`. - Simulate URL shortening by sleeping for 1 second asynchronously. - Log the request ID and the original URL to a file called `requests.log`. 3. **Context Management**: - Implement a function `handle_request(url_list)` that takes in a list of URLs, assigns unique request IDs, and calls `shorten_url` for each URL concurrently. 4. **Logging**: - Ensure that each log entry is associated with the correct request ID. 5. **Concurrency**: - Use `asyncio.gather` to handle multiple URL shortening tasks concurrently. # Constraints: - The URL must be a valid HTTP or HTTPS URL. - The `requests.log` file should not have overlapping or incorrect request IDs. - You are not allowed to use global variables or alter the context variable directly outside the provided methods. **Input:** - A list of URLs. - Example: `[\\"https://example.com/1\\", \\"http://example.org/2\\"]` **Output:** - Log entries in the `requests.log` file with each entry containing the correct request ID and original URL. - Example: ``` Request ID: 1, URL: https://example.com/1 Request ID: 2, URL: http://example.org/2 ``` # Implementation: ```python import asyncio import contextvars # Context Variable for holding request ID request_id_var = contextvars.ContextVar(\'request_id\', default=None) async def shorten_url(request_id, url): # Set the request_id in the context variable request_id_var.set(request_id) # Simulate URL shortening by sleeping for 1 second await asyncio.sleep(1) # Log the request ID and the original URL log_request(request_id, url) def log_request(request_id, url): with open(\\"requests.log\\", \\"a\\") as log_file: log_file.write(f\\"Request ID: {request_id}, URL: {url}n\\") async def handle_request(url_list): # Assign unique request IDs and call shorten_url for each URL concurrently tasks = [] for i, url in enumerate(url_list): request_id = i + 1 # Unique request ID tasks.append(shorten_url(request_id, url)) await asyncio.gather(*tasks) # Example usage url_list = [\\"https://example.com/1\\", \\"http://example.org/2\\"] asyncio.run(handle_request(url_list)) ``` Evaluation Criteria: - Correct use of `ContextVar` to manage and log the request ID. - Proper handling of asynchronous tasks using `asyncio`. - Ensuring no context leaks, maintaining proper isolation of request IDs. - Writing log entries correctly to the `requests.log` file. - Clean and readable code with appropriate comments.","solution":"import asyncio import contextvars from urllib.parse import urlparse, urljoin # Context Variable for holding request ID request_id_var = contextvars.ContextVar(\'request_id\', default=None) async def shorten_url(request_id, url): Asynchronously imitates shortening a URL and logs the request ID and URL. # Set the request_id in the context variable request_id_var.set(request_id) # Simulate URL shortening by sleeping for 1 second await asyncio.sleep(1) # Log the request ID and the original URL log_request(request_id, url) def log_request(request_id, url): Logs the request ID and the URL to the \'requests.log\' file. with open(\\"requests.log\\", \\"a\\") as log_file: log_file.write(f\\"Request ID: {request_id}, URL: {url}n\\") async def handle_request(url_list): Handles a list of URLs by assigning unique request IDs and calling shorten_url concurrently. tasks = [] for i, url in enumerate(url_list): request_id = i + 1 # Unique request ID tasks.append(shorten_url(request_id, url)) await asyncio.gather(*tasks) # Example usage # if __name__ == \\"__main__\\": # url_list = [\\"https://example.com/1\\", \\"http://example.org/2\\"] # asyncio.run(handle_request(url_list))"},{"question":"Dimensionality Reduction and Classification Pipeline **Objective:** Evaluate students\' ability to apply unsupervised dimensionality reduction techniques and construct a machine learning pipeline in scikit-learn. **Question:** You are provided with a dataset that has a high number of features. Your task is to build a machine learning pipeline in scikit-learn that performs the following steps: 1. Scales the features using `StandardScaler`. 2. Reduces the dimensionality of the data using `PCA` to retain 95% of the variance. 3. Trains a `RandomForestClassifier` using the reduced dataset. Implement the function `create_pipeline` that takes no input and returns a scikit-learn `Pipeline` object configured as described above. Input: - None Output: - A scikit-learn `Pipeline` object. Constraints: - Use `PCA` from `sklearn.decomposition`. - Use `StandardScaler` from `sklearn.preprocessing`. - Use `RandomForestClassifier` from `sklearn.ensemble`. - The PCA should retain 95% of the variance. Example Usage: ```python pipeline = create_pipeline() print(pipeline) ``` Expected output (structure, actual parameter values may vary): ``` Pipeline(steps=[(\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=0.95)), (\'classifier\', RandomForestClassifier())]) ``` # Additional Information: - You can assume that the necessary libraries from scikit-learn are already installed. - Ensure that your pipeline can be used directly with scikit-learn\'s `fit` and `predict` methods.","solution":"from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.ensemble import RandomForestClassifier def create_pipeline(): Creates a scikit-learn pipeline that scales the features, reduces dimensionality to retain 95% of the variance using PCA, and trains a RandomForestClassifier. Returns: Pipeline: Configured scikit-learn pipeline. pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=0.95)), (\'classifier\', RandomForestClassifier()) ]) return pipeline"},{"question":"# Advanced Coding Assessment Question Abstract Base Classes in Python **Objective**: You need to demonstrate your understanding of Python\'s `abc` module by designing and implementing an abstract base class that models a basic filesystem. This exercise will challenge your ability to use abstract base classes, abstract methods, and virtual subclasses effectively. Problem Description You are to design an abstract base class called `FileSystemElement` which models a basic element in a file system (e.g., files or directories). This class should leverage the `abc` module\'s capabilities and include the following requirements: 1. **Abstract Methods**: - Implement abstract methods `get_name` and `get_size` which are to be defined in subclasses to return the name of the filesystem element and its size, respectively. - Implement an abstract method `__str__` which should provide a string representation of the filesystem element. 2. **Concrete Methods**: - Implement a concrete method `print_info` which should print the string representation of the object as defined by `__str__`. 3. **Concrete Class**: - Create a concrete class `File` which extends `FileSystemElement` and represents a file. Ensure this class provides concrete implementations for `get_name`, `get_size`, and `__str__` methods. A file should have attributes `name` (a string) and `size` (an integer representing the size in bytes). 4. **Virtual Subclass**: - Register a virtual subclass `DirectoryDict` with `FileSystemElement`. This virtual subclass represents directories using a dictionary where keys are string names of the files/subdirectories and values are instances of `FileSystemElement`. The `DirectoryDict` should also implement `get_name`, `get_size`, and `__str__`. Constraints and Requirements: - Use the `abc` module to create abstract base classes and abstract methods. - The concrete class `File` and the virtual subclass `DirectoryDict` need to properly implement all abstract methods of `FileSystemElement`. - `FileSystemElement` should not be instantiable directly. - The `__str__` method of both the `File` and `DirectoryDict` should return a meaningful string representation. - Ensure your code handles possible edge cases, such as large sizes or nesting for directories. Example Here is how the classes might be used: ```python from abc import ABC, abstractmethod class FileSystemElement(ABC): @abstractmethod def get_name(self): pass @abstractmethod def get_size(self): pass @abstractmethod def __str__(self): pass def print_info(self): print(self.__str__()) class File(FileSystemElement): def __init__(self, name, size): self.name = name self.size = size def get_name(self): return self.name def get_size(self): return self.size def __str__(self): return f\\"File: {self.name}, Size: {self.size}\\" @FileSystemElement.register class DirectoryDict(dict): def get_name(self): return \\"Directory\\" def get_size(self): total_size = 0 for element in self.values(): total_size += element.get_size() return total_size def __str__(self): result = \\"Directory Contents:n\\" for name, element in self.items(): result += f\\"{name}: {element}n\\" return result # Example usage: file1 = File(\\"file1.txt\\", 1000) file2 = File(\\"file2.txt\\", 2000) directory = DirectoryDict() directory[\\"file1\\"] = file1 directory[\\"file2\\"] = file2 file1.print_info() # Output: File: file1.txt, Size: 1000 directory.print_info() # Output: Directory Contents: file1.txt: File: file1.txt, Size: 1000 ... ``` Your task is to implement the complete classes as per the requirements outlined.","solution":"from abc import ABC, abstractmethod class FileSystemElement(ABC): @abstractmethod def get_name(self): pass @abstractmethod def get_size(self): pass @abstractmethod def __str__(self): pass def print_info(self): print(self.__str__()) class File(FileSystemElement): def __init__(self, name, size): self.name = name self.size = size def get_name(self): return self.name def get_size(self): return self.size def __str__(self): return f\\"File: {self.name}, Size: {self.size}\\" @FileSystemElement.register class DirectoryDict(dict): def get_name(self): return \\"Directory\\" def get_size(self): total_size = 0 for element in self.values(): total_size += element.get_size() return total_size def __str__(self): result = \\"Directory Contents:n\\" for name, element in self.items(): result += f\\"{name}: {element}n\\" return result"},{"question":"# Coding Assessment: Model Comparison using SGD **Objective**: Assess students\' understanding of Stochastic Gradient Descent (SGD) in scikit-learn, and their ability to implement, train, and evaluate models using this optimization algorithm. Problem Statement You are tasked with building and comparing logistic regression models using two different optimization methods: Stochastic Gradient Descent (SGD) and the default solver of `LogisticRegression` in scikit-learn. Your goal is to evaluate the models based on their accuracy and computational efficiency on a given dataset. Instructions 1. **Data Preparation**: - Load the provided dataset (assume it\'s a binary classification problem) from a CSV file. - Split the dataset into training and testing sets using a 70-30 ratio. 2. **Model Training and Evaluation**: - Implement two logistic regression models: - One using `SGDClassifier` with `loss=\'log_loss\'`. - The other using `LogisticRegression` with its default solver. - Ensure features are scaled appropriately before training the models by using `StandardScaler` in a pipeline. - Enable early stopping for the `SGDClassifier` to prevent overfitting. - Train both models on the training set and evaluate their accuracy on the test set. - Record the time taken to train each model (computational efficiency). 3. **Output**: - Print the accuracy of each model on the test set. - Print the time taken to train each model. - Plot the coefficients of both models for comparison. Constraints - Use the provided column names as features for the dataset. - Ensure reproducibility by setting a random seed where applicable. - Avoid using any additional libraries beyond scikit-learn and matplotlib for the task. Dataset Description The dataset (`data.csv`) contains the following columns: - `feature1`, `feature2`, `feature3`, ..., `featureN`: Feature columns for the prediction task. - `target`: Binary target variable (0 or 1). Example ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier, LogisticRegression from sklearn.metrics import accuracy_score import time import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'data.csv\') X = df.drop(\'target\', axis=1) y = df[\'target\'] # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Model 1: SGDClassifier sgd_pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\'log_loss\', early_stopping=True, random_state=42)) start_time = time.time() sgd_pipeline.fit(X_train, y_train) sgd_time = time.time() - start_time sgd_predictions = sgd_pipeline.predict(X_test) sgd_accuracy = accuracy_score(y_test, sgd_predictions) sgd_coefs = sgd_pipeline.named_steps[\'sgdclassifier\'].coef_ # Model 2: LogisticRegression lr_pipeline = make_pipeline(StandardScaler(), LogisticRegression(random_state=42)) start_time = time.time() lr_pipeline.fit(X_train, y_train) lr_time = time.time() - start_time lr_predictions = lr_pipeline.predict(X_test) lr_accuracy = accuracy_score(y_test, lr_predictions) lr_coefs = lr_pipeline.named_steps[\'logisticregression\'].coef_ # Output results print(\\"SGDClassifier Accuracy:\\", sgd_accuracy) print(\\"SGDClassifier Training Time:\\", sgd_time) print(\\"LogisticRegression Accuracy:\\", lr_accuracy) print(\\"LogisticRegression Training Time:\\", lr_time) # Plot coefficients plt.figure(figsize=(12, 6)) plt.plot(sgd_coefs.ravel(), label=\'SGDClassifier Coefs\') plt.plot(lr_coefs.ravel(), label=\'LogisticRegression Coefs\') plt.legend() plt.title(\'Comparison of Model Coefficients\') plt.show() ``` Submission Submit your Python code in a Jupyter notebook with appropriate markdown cells explaining each step. Ensure your code is well-organized and commented thoroughly.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier, LogisticRegression from sklearn.metrics import accuracy_score import time import matplotlib.pyplot as plt def model_comparison(file_path): # Load the dataset df = pd.read_csv(file_path) X = df.drop(\'target\', axis=1) y = df[\'target\'] # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Model 1: SGDClassifier sgd_pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\'log_loss\', early_stopping=True, random_state=42)) start_time = time.time() sgd_pipeline.fit(X_train, y_train) sgd_time = time.time() - start_time sgd_predictions = sgd_pipeline.predict(X_test) sgd_accuracy = accuracy_score(y_test, sgd_predictions) sgd_coefs = sgd_pipeline.named_steps[\'sgdclassifier\'].coef_ # Model 2: LogisticRegression lr_pipeline = make_pipeline(StandardScaler(), LogisticRegression(random_state=42)) start_time = time.time() lr_pipeline.fit(X_train, y_train) lr_time = time.time() - start_time lr_predictions = lr_pipeline.predict(X_test) lr_accuracy = accuracy_score(y_test, lr_predictions) lr_coefs = lr_pipeline.named_steps[\'logisticregression\'].coef_ # Output results results = { \\"SGDClassifier Accuracy\\": sgd_accuracy, \\"SGDClassifier Training Time\\": sgd_time, \\"LogisticRegression Accuracy\\": lr_accuracy, \\"LogisticRegression Training Time\\": lr_time, \\"SGD Coefficients\\": sgd_coefs, \\"LogisticRegression Coefficients\\": lr_coefs } return results def plot_coefficients(sgd_coefs, lr_coefs): plt.figure(figsize=(12, 6)) plt.plot(sgd_coefs.ravel(), label=\'SGDClassifier Coefs\') plt.plot(lr_coefs.ravel(), label=\'LogisticRegression Coefs\') plt.legend() plt.title(\'Comparison of Model Coefficients\') plt.show()"},{"question":"# Advanced Coding Assessment: Custom Pipeline for Imputation and Classification You are given a dataset containing missing values, and your task is to build a machine learning pipeline that: 1. Imputes the missing values using `IterativeImputer`. 2. Adds a `MissingIndicator` to mark the locations of the imputed values. 3. Trains a classification model on the imputed data. # Step-by-Step Instructions: 1. **Load the Dataset**: - Load the `iris` dataset from `sklearn.datasets`. - Introduce random missing values to the dataset. 2. **Impute Missing Values**: - Use `IterativeImputer` to fill in the missing values. - The imputer should run for a maximum of 10 iterations and use a random state of `42`. 3. **Feature Engineering**: - Add binary indicators using `MissingIndicator` to mark where the missing values were present. 4. **Build the Pipeline**: - Create a pipeline that first imputes the missing values, then adds the missing indicators, and finally uses a `DecisionTreeClassifier` for classification. - Split the dataset into training and test sets, train the model on the training set, and evaluate it on the test set using accuracy as the metric. # Implementation Details: - **Input Format**: No input parameters, all data handling is done within the function. - **Output Format**: Print the accuracy of the model on the test set. # Constraints & Notes: - Ensure the pipeline handles categorical and numerical data if present. - The random seed for the iterative imputer must be set to 42 to ensure reproducibility. - Use an 80-20 train-test split. # Example ```python import numpy as np from sklearn.datasets import load_iris from sklearn.impute import IterativeImputer, MissingIndicator from sklearn.model_selection import train_test_split from sklearn.pipeline import FeatureUnion, make_pipeline from sklearn.tree import DecisionTreeClassifier def custom_pipeline_classification(): # Load the iris dataset X, y = load_iris(return_X_y=True) # Introduce random missing values rng = np.random.default_rng(42) mask = rng.random(X.shape) < 0.1 # 10% missing values X[mask] = np.nan # Define iterative imputer with desired settings iterative_imputer = IterativeImputer(max_iter=10, random_state=42) # Define missing indicator missing_indicator = MissingIndicator() # Create a feature union to include both imputed features and missing indicators transformer = FeatureUnion( transformer_list=[ (\'imputed_features\', iterative_imputer), (\'missing_indicators\', missing_indicator) ] ) # Create the full pipeline pipeline = make_pipeline(transformer, DecisionTreeClassifier()) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) # Fit the pipeline and evaluate pipeline.fit(X_train, y_train) accuracy = pipeline.score(X_test, y_test) # Print the accuracy print(f\\"Model Accuracy: {accuracy:.2f}\\") # Run the function to see the results custom_pipeline_classification() ``` **Expected Output**: The output should be the accuracy of the DecisionTreeClassifier on the test set, which could vary depending on the introduced randomness of missing values.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer, MissingIndicator from sklearn.model_selection import train_test_split from sklearn.pipeline import FeatureUnion, make_pipeline from sklearn.tree import DecisionTreeClassifier def custom_pipeline_classification(): # Load the iris dataset X, y = load_iris(return_X_y=True) # Introduce random missing values rng = np.random.default_rng(42) mask = rng.random(X.shape) < 0.1 # 10% missing values X[mask] = np.nan # Define iterative imputer with desired settings iterative_imputer = IterativeImputer(max_iter=10, random_state=42) # Define missing indicator missing_indicator = MissingIndicator() # Create a feature union to include both imputed features and missing indicators transformer = FeatureUnion( transformer_list=[ (\'imputed_features\', iterative_imputer), (\'missing_indicators\', missing_indicator) ] ) # Create the full pipeline pipeline = make_pipeline(transformer, DecisionTreeClassifier()) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42) # Fit the pipeline and evaluate pipeline.fit(X_train, y_train) accuracy = pipeline.score(X_test, y_test) # Print the accuracy print(f\\"Model Accuracy: {accuracy:.2f}\\") # Run the function to see the results custom_pipeline_classification()"},{"question":"**Question:** Using Seaborn, generate visualizations of the `penguins` dataset to analyze the distribution of `bill_length_mm` for different `species`. Your task includes the following sub-parts: 1. Load the `penguins` dataset from seaborn. 2. Create a density plot of `bill_length_mm`, ensuring the density is smoothed with a `bw_adjust` parameter value of 0.5. 3. On the same plot, add a histogram of `bill_length_mm` with a transparency (alpha) of 0.4. 4. Separate the density curves in the plot by `species`, and ensure each species has its density curve normalized relative to its observations (i.e., not jointly normalized with others). 5. Additionally, create a separate density plot where the densities are shown as cumulative functions. 6. Finally, save both plots as `density_plot.png` and `cumulative_density_plot.png`, respectively. **Input Format:** - No specific input since the dataset is loaded within the code. **Output Format:** - The output should be two PNG files named `density_plot.png` and `cumulative_density_plot.png`, saved in the current working directory. **Constraints:** - Ensure the plots are clear and labeled appropriately for easy interpretation. **Example Code Template:** ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create density plot with bw_adjust=0.5 density_plot = so.Plot(penguins, x=\\"bill_length_mm\\") density_plot.add(so.Area(), so.KDE(bw_adjust=0.5)) # Step 3: Add histogram on the same plot density_plot.add(so.Bars(alpha=0.4), so.Hist(\\"density\\")) # Step 4: Seperate density curves by species and normalize individually density_plot.add(so.Area(), so.KDE(common_norm=False), color=\\"species\\") # Save the density plot density_plot.save(\\"density_plot.png\\") # Step 5: Create cumulative density plot cumulative_density_plot = so.Plot(penguins, x=\\"bill_length_mm\\") cumulative_density_plot.add(so.Line(), so.KDE(cumulative=True), color=\\"species\\") # Save the cumulative density plot cumulative_density_plot.save(\\"cumulative_density_plot.png\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Create density plot with bw_adjust=0.5 plt.figure(figsize=(10, 6)) sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", element=\\"step\\", stat=\\"density\\", common_norm=False, alpha=0.4, kde=True) sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", bw_adjust=0.5, common_norm=False) # Finalize and save density plot plt.title(\\"Density and Histogram of Bill Length by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Density\\") plt.legend(title=\\"Species\\") plt.savefig(\\"density_plot.png\\") plt.close() # Step 5: Create cumulative density plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", bw_adjust=0.5, common_norm=False, cumulative=True) # Finalize and save cumulative density plot plt.title(\\"Cumulative Density of Bill Length by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Cumulative Density\\") plt.legend(title=\\"Species\\") plt.savefig(\\"cumulative_density_plot.png\\") plt.close()"},{"question":"Objective To assess the student\'s ability to manipulate and analyze data using pandas, covering topics such as indexing, merging, handling missing data, and group operations. Problem Statement You are given two datasets: 1. **Customers**: This dataset contains information about customers and looks as follows: ```plaintext customer_id, name, city 1, Alice, New York 2, Bob, Los Angeles 3, Charlie, Chicago 4, Diana, New York 5, Eva, Los Angeles ``` 2. **Orders**: This dataset contains records of orders placed by the customers and looks as follows: ```plaintext order_id, customer_id, order_date, product, quantity 101, 1, 2023-01-01, Apple, 2 102, 2, 2023-01-10, Banana, 5 103, 1, 2023-02-01, Orange, 3 104, 3, 2023-01-15, Pineapple, 1 105, 4, 2023-03-01, Mango, 4 ``` Task 1. Load the datasets into pandas DataFrames. 2. Merge the two DataFrames on `customer_id` to create a single DataFrame. 3. Handle any missing values by assuming that if a customer has never placed an order, it should be reflected with `NaN` values. 4. Create a new column named `city_order_count` which shows the count of orders placed from the corresponding customer\'s city. 5. Write a function `get_top_customers(df, city, n)` that takes the merged DataFrame, a city name, and an integer `n`, and returns the names of the top `n` customers based on the total quantity of products ordered from that city. If there are fewer customers than `n`, return all the customers sorted by their total quantity. 6. Test your function using `New York` as the city and `2` as the number of top customers. Input - Two csv files: `customers.csv` and `orders.csv` - A string indicating the `city` - An integer indicating the number of top customers to return Output - A list of customer names who are the top `n` customers by the quantity of products ordered from the specified city. Constraints - Assume that the columns in each dataset are correctly formatted. - The function should handle edge cases such as cities with fewer customers than `n`. Performance Requirements The function should: - Efficiently merge and analyze large datasets. - Minimize memory usage while handling potential missing data. Example Suppose the merged DataFrame looks like this: ```plaintext customer_id name city order_id order_date product quantity 0 1 Alice New York 101 2023-01-01 Apple 2 1 1 Alice New York 103 2023-02-01 Orange 3 2 2 Bob Los Angeles 102 2023-01-10 Banana 5 3 3 Charlie Chicago 104 2023-01-15 Pineapple 1 4 4 Diana New York 105 2023-03-01 Mango 4 5 5 Eva Los Angeles NaN NaN NaN NaN ``` Calling `get_top_customers(df, \'New York\', 2)` should return: ```plaintext [\'Diana\', \'Alice\'] ``` because Diana ordered 4 products and Alice ordered 2+3=5 products. Evaluation Criteria - Correctness: The solution should provide the correct results as per the specifications. - Code Quality: Code should be well-structured and readable. - Performance: The solution should be efficient and handle large input sizes appropriately.","solution":"import pandas as pd # Load the datasets into pandas DataFrames customers = pd.DataFrame({ \\"customer_id\\": [1, 2, 3, 4, 5], \\"name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"Diana\\", \\"Eva\\"], \\"city\\": [\\"New York\\", \\"Los Angeles\\", \\"Chicago\\", \\"New York\\", \\"Los Angeles\\"], }) orders = pd.DataFrame({ \\"order_id\\": [101, 102, 103, 104, 105], \\"customer_id\\": [1, 2, 1, 3, 4], \\"order_date\\": [\\"2023-01-01\\", \\"2023-01-10\\", \\"2023-02-01\\", \\"2023-01-15\\", \\"2023-03-01\\"], \\"product\\": [\\"Apple\\", \\"Banana\\", \\"Orange\\", \\"Pineapple\\", \\"Mango\\"], \\"quantity\\": [2, 5, 3, 1, 4], }) # Merge the two DataFrames on \'customer_id\' to create a single DataFrame merged_df = pd.merge(customers, orders, on=\\"customer_id\\", how=\\"left\\") # Create a new column named \'city_order_count\' city_order_count = merged_df.groupby(\'city\')[\'order_id\'].transform(\'count\').fillna(0).astype(int) merged_df[\'city_order_count\'] = city_order_count # Function to get top customers def get_top_customers(df, city, n): city_df = df[df[\'city\'] == city] customer_quantity = city_df.groupby(\'name\')[\'quantity\'].sum().reset_index() top_customers = customer_quantity.sort_values(by=\'quantity\', ascending=False).head(n) return top_customers[\'name\'].tolist() # Below line is for demonstration, remove it in actual function # print(get_top_customers(merged_df, \'New York\', 2))"},{"question":"**Coding Assessment Question** **Objective:** Implement a set of functions that utilize the `zlib` module to perform various data compression and decompression tasks. **Requirements:** 1. **Function: `compress_data(data: bytes, level: int) -> bytes`** - Compress the input data using the specified compression level. - **Input:** - `data`: A `bytes` object representing the data to be compressed. - `level`: An integer from 0 to 9 representing the compression level. - **Output:** - A `bytes` object containing the compressed data. - **Constraints:** - The level must be between 0 and 9 (inclusive). - Raise a `ValueError` if the level is outside the specified range. 2. **Function: `decompress_data(data: bytes, bufsize: int) -> bytes`** - Decompress the input data. - **Input:** - `data`: A `bytes` object representing the compressed data. - `bufsize`: An integer representing the buffer size. - **Output:** - A `bytes` object containing the decompressed data. 3. **Function: `calculate_adler32(data: bytes) -> int`** - Compute the Adler-32 checksum of the input data. - **Input:** - `data`: A `bytes` object representing the data. - **Output:** - An integer representing the Adler-32 checksum. 4. **Function: `compress_stream(data: bytes, level: int) -> bytes`** - Compress the input data stream using a compression object. - **Input:** - `data`: A `bytes` object representing the data to be compressed. - `level`: An integer from 0 to 9 representing the compression level. - **Output:** - A `bytes` object containing the compressed data stream. - **Constraints:** - The level must be between 0 and 9 (inclusive). - Raise a `ValueError` if the level is outside the specified range. 5. **Function: `decompress_stream(data: bytes, bufsize: int) -> bytes`** - Decompress the input data stream using a decompression object. - **Input:** - `data`: A `bytes` object representing the compressed data. - `bufsize`: An integer representing the buffer size. - **Output:** - A `bytes` object containing the decompressed data stream. **Example:** ```python # Example usage: # 1. Compressing data compressed_data = compress_data(b\'hello world\', 5) # 2. Decompressing data decompressed_data = decompress_data(compressed_data, 1024) # 3. Calculating Adler-32 checksum checksum = calculate_adler32(b\'hello world\') # 4. Compressing data stream compressed_stream = compress_stream(b\'hello world\', 5) # 5. Decompressing data stream decompressed_stream = decompress_stream(compressed_stream, 1024) print(compressed_data) print(decompressed_data) print(checksum) print(compressed_stream) print(decompressed_stream) ``` **Additional Information:** - You may use any built-in or standard library Python functions and modules. - Ensure that your functions handle exceptions and edge cases gracefully.","solution":"import zlib def compress_data(data: bytes, level: int) -> bytes: if not (0 <= level <= 9): raise ValueError(\\"Compression level must be between 0 and 9 inclusive.\\") return zlib.compress(data, level) def decompress_data(data: bytes, bufsize: int) -> bytes: decompressor = zlib.decompressobj() decompressed_data = decompressor.decompress(data, bufsize) return decompressed_data + decompressor.flush() def calculate_adler32(data: bytes) -> int: return zlib.adler32(data) def compress_stream(data: bytes, level: int) -> bytes: if not (0 <= level <= 9): raise ValueError(\\"Compression level must be between 0 and 9 inclusive.\\") compressor = zlib.compressobj(level) compressed_data = compressor.compress(data) return compressed_data + compressor.flush() def decompress_stream(data: bytes, bufsize: int) -> bytes: decompressor = zlib.decompressobj() decompressed_data = decompressor.decompress(data, bufsize) return decompressed_data + decompressor.flush()"},{"question":"# Objective You are to write a Python function and a set of unit tests to validate its functionality. The function should calculate the factorial of a given number. After implementing the function, use `unittest` and `unittest.mock` to write test cases to validate it. # Problem Statement 1. Implement a function `factorial(n)` that returns the factorial of a non-negative integer `n`. The factorial of `n` (denoted as `n!`) is the product of all positive integers less than or equal to `n`. For example, `factorial(5)` should return `120`. 2. Write a set of unit tests using the `unittest` module that include: - Tests for valid inputs. - Tests to handle invalid inputs, such as negative numbers. - Mock a scenario where you simulate an error within the factorial calculation. # Constraints - The input `n` will be a non-negative integer. - The function should raise a `ValueError` if `n` is a negative number. - Use the `unittest` module for writing test cases. - Use the `unittest.mock` module to simulate error handling within your tests. # Input - A single non-negative integer `n`. # Output - A single integer which is the factorial of `n`. # Performance Requirements - The function should be efficient and able to handle input values up to 20 reasonably. # Example ```python factorial(5) # Should return 120 factorial(0) # Should return 1 factorial(-1) # Should raise ValueError ``` # Implementation factorial.py ```python def factorial(n): Calculate the factorial of a non-negative integer n. if n < 0: raise ValueError(\\"Negative values are not allowed\\") elif n == 0 or n == 1: return 1 else: result = 1 for i in range(2, n + 1): result += i return result ``` test_factorial.py ```python import unittest from unittest.mock import patch from factorial import factorial class TestFactorial(unittest.TestCase): def test_factorial_valid(self): self.assertEqual(factorial(5), 120) self.assertEqual(factorial(0), 1) self.assertEqual(factorial(1), 1) def test_factorial_invalid(self): with self.assertRaises(ValueError): factorial(-1) @patch(\'factorial.factorial\', side_effect=ValueError(\\"Simulated error\\")) def test_factorial_mock_error(self, mock_factorial): with self.assertRaises(ValueError) as context: factorial(5) self.assertEqual(str(context.exception), \\"Simulated error\\") if __name__ == \'__main__\': unittest.main() ``` The task requires writing and understanding functional code, creating unit tests, and using mocking to simulate and test error conditions effectively.","solution":"def factorial(n): Calculate the factorial of a non-negative integer n. if n < 0: raise ValueError(\\"Negative values are not allowed\\") elif n == 0 or n == 1: return 1 else: result = 1 for i in range(2, n + 1): result *= i return result"},{"question":"# Pandas Coding Assessment: Implementing CoW-Compliant Data Manipulations Objective: Your task is to demonstrate your understanding of the Copy-on-Write (CoW) feature in pandas by implementing a function that manipulates a pandas DataFrame according to specified operations. Your implementation should ensure compliance with CoW principles to avoid unintended side-effects and enhance performance. Problem Statement: Write a function `modify_dataframe(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations on the input DataFrame: 1. **Select a subset of rows**: Select rows where the values in column \'A\' are greater than 10 and store this subset in a new DataFrame `subset_df`. 2. **Modify the original DataFrame**: Set the values in column \'B\' to -1 for all rows where the values in column \'A\' are greater than 5. 3. **Ensure no modifications to `subset_df`**: Changes made to the original DataFrame should not affect the `subset_df`. 4. **Return a modified copy of the original DataFrame** without sharing the underlying data with any other DataFrame. Input: - A pandas DataFrame `df` with at least two columns \'A\' and \'B\' containing integer values. Output: - A modified pandas DataFrame where the requirements specified above are met. Constraints: - Use CoW-compliant pandas operations to ensure that modifications do not result in unintended side-effects. - Do not use chained assignment; rely on `.loc` for indexing operations. Example: ```python import pandas as pd data = { \'A\': [8, 15, 12, 3, 6, 18], \'B\': [20, 22, 23, 24, 25, 26] } df = pd.DataFrame(data) modified_df = modify_dataframe(df) print(modified_df) # Expected Output: # A B # 0 8 -1 # 1 15 -1 # 2 12 -1 # 3 3 24 # 4 6 -1 # 5 18 -1 ``` In this example: - A subset `subset_df` would contain rows with values in column \'A\' greater than 10. - The original DataFrame `df` is modified to set column \'B\' to -1 where column \'A\' values are greater than 5. - The modifications to `df` must not affect `subset_df`. Notes: - Ensure your function handles large DataFrames efficiently. - Provide appropriate documentation in your code to explain your approach and methodology.","solution":"import pandas as pd def modify_dataframe(df: pd.DataFrame) -> pd.DataFrame: This function performs specified operations on the input DataFrame according to CoW principles: - Select rows where values in column \'A\' are greater than 10 and store in a new DataFrame `subset_df`. - Modify the original DataFrame to set column \'B\' to -1 for rows where values in column \'A\' are greater than 5. - Ensure that `subset_df` does not get modified. - Return a modified copy of the original DataFrame without sharing underlying data. # Select a subset of rows where the values in column \'A\' are greater than 10 subset_df = df[df[\'A\'] > 10].copy() # Modify the original DataFrame where values in column \'A\' are greater than 5 df_modified = df.copy() df_modified.loc[df_modified[\'A\'] > 5, \'B\'] = -1 return df_modified"},{"question":"**Question: Implement a Matrix Tuning Utility Using `torch.cuda.tunable`** You have been provided with a prototype feature `torch.cuda.tunable` for tuning CUDA operations, particularly GEMM (General Matrix-Matrix Multiplication). Your task is to implement a function that tunes a given GEMM operation using the provided module\'s functionalities. # Function Signature ```python def tune_gemm_operation(max_duration: float, max_iterations: int, filename: str) -> dict: Tunes a GEMM operation based on the specified parameters and returns the tuning results. Args: max_duration (float): The maximum duration for the tuning process in seconds. max_iterations (int): The maximum number of iterations for the tuning process. filename (str): The name of the file to which the tuning results will be written. Returns: dict: A dictionary containing the tuning results. ``` # Input - `max_duration`: A float representing the maximum duration for the tuning process in seconds. - `max_iterations`: An integer representing the maximum number of iterations for the tuning process. - `filename`: A string representing the name of the file to which the tuning results will be written. # Output - A dictionary containing the tuning results. # Constraints - Ensure that the tuning is enabled before starting the process. - Set the maximum tuning duration and iterations as specified by the input parameters. - Use the specified filename to write the tuning results. - Handle exceptions that may occur during the tuning process and ensure all resources are properly managed. # Example ```python # Example usage of the function results = tune_gemm_operation(10.0, 1000, \\"tuning_results.json\\") print(results) ``` # Implementation Hints 1. Use `torch.cuda.tunable.enable()` to enable the tuning. 2. Set the tuning parameters using `set_max_tuning_duration()` and `set_max_tuning_iterations()`. 3. Perform the tuning using `tune_gemm_in_file()` with the given filename. 4. Retrieve the results using `get_results()`. 5. Write the results to a file using `write_file()`. 6. Ensure the function handles exceptions gracefully and cleans up any resources if needed. # Notes - This is an early prototype feature, and its components are subject to change. - You may need to refer to the complete documentation of `torch.cuda.tunable` for more details on each function and its usage.","solution":"import torch.cuda.tunable def tune_gemm_operation(max_duration: float, max_iterations: int, filename: str) -> dict: Tunes a GEMM operation based on the specified parameters and returns the tuning results. Args: max_duration (float): The maximum duration for the tuning process in seconds. max_iterations (int): The maximum number of iterations for the tuning process. filename (str): The name of the file to which the tuning results will be written. Returns: dict: A dictionary containing the tuning results. try: # Enable tuning torch.cuda.tunable.enable() # Set the maximum tuning duration and iterations torch.cuda.tunable.set_max_tuning_duration(max_duration) torch.cuda.tunable.set_max_tuning_iterations(max_iterations) # Perform the tuning and save the results to the specified file torch.cuda.tunable.tune_gemm_in_file(filename) # Retrieve the tuning results results = torch.cuda.tunable.get_results() # Write the tuning results to the file torch.cuda.tunable.write_file(filename) return results except Exception as e: print(f\\"An error occurred during the tuning process: {e}\\") return {} finally: # Disable tuning to clean up resources torch.cuda.tunable.disable()"},{"question":"# Python Coding Assessment Question Objective The goal of this question is to assess your understanding of the `hashlib` module in Python for generating secure hashes and message digests. You will need to demonstrate knowledge of creating hash objects, updating them, and obtaining digests in different formats. Question You are tasked with implementing a function that generates secure hashes of given input data using different algorithms provided by the `hashlib` module. Your function should also support producing keyed hashes using BLAKE2 for data authentication purposes. Lastly, the function should be able to derive a secure key from a password using a key derivation function. Function Signature ```python import hashlib def generate_hashes(data: bytes, password: bytes, salt: bytes) -> dict: Generates secure hashes for given data using various algorithms and derives a secure key from a password. Parameters: - data (bytes): A bytes object representing the data to be hashed. - password (bytes): A bytes object representing the password used in key derivation. - salt (bytes): A bytes object used in key derivation and for randomized hashing. Returns: - dict: A dictionary with the following keys and their corresponding values: - \'sha256\': Hex digest of data hashed using SHA-256. - \'sha3_512\': Hex digest of data hashed using SHA3-512. - \'blake2b\': Hex digest of data hashed using BLAKE2b with a specified digest size of 32 bytes. - \'keyed_blake2b\': Hex digest for data authentication using BLAKE2b with a keyed hash of digest size 16 bytes. - \'pbkdf2_hmac\': Hex digest of derived key from password using PBKDF2-HMAC with SHA-256. Raises: - ValueError: If any of the required parameters are not provided or are of incorrect type. pass ``` Requirements 1. The function should produce a SHA-256 hex digest for the input `data`. 2. It should produce a SHA3-512 hex digest for the input `data`. 3. It should produce a BLAKE2b hex digest for the input `data`, with a digest size of 32 bytes. 4. For data authentication, it should produce a BLAKE2b hex digest using a keyed hash with the key being `salt` and a digest size of 16 bytes. 5. It should derive a secure key from the provided `password` using PBKDF2-HMAC with SHA-256, 100,000 iterations, and the given `salt`. The derived key should be in hex digest format. Example ```python data = b\\"Hello, hashlib!\\" password = b\\"super_secret_password\\" salt = b\\"unique_salt_value\\" hashes = generate_hashes(data, password, salt) print(hashes) # Expected Output (hash values will vary): # { # \'sha256\': \'...\', # \'sha3_512\': \'...\', # \'blake2b\': \'...\', # \'keyed_blake2b\': \'...\', # \'pbkdf2_hmac\': \'...\' # } ``` # Constraints - `data`, `password`, and `salt` must be provided as bytes-like objects. - Raise a `ValueError` if any input is of incorrect type or missing. - Ensure efficient performance and handle potentially large data inputs gracefully.","solution":"import hashlib def generate_hashes(data: bytes, password: bytes, salt: bytes) -> dict: Generates secure hashes for given data using various algorithms and derives a secure key from a password. Parameters: - data (bytes): A bytes object representing the data to be hashed. - password (bytes): A bytes object representing the password used in key derivation. - salt (bytes): A bytes object used in key derivation and for randomized hashing. Returns: - dict: A dictionary with the following keys and their corresponding values: - \'sha256\': Hex digest of data hashed using SHA-256. - \'sha3_512\': Hex digest of data hashed using SHA3-512. - \'blake2b\': Hex digest of data hashed using BLAKE2b with a specified digest size of 32 bytes. - \'keyed_blake2b\': Hex digest for data authentication using BLAKE2b with a keyed hash of digest size 16 bytes. - \'pbkdf2_hmac\': Hex digest of derived key from password using PBKDF2-HMAC with SHA-256. Raises: - ValueError: If any of the required parameters are not provided or are of incorrect type. if not isinstance(data, bytes) or not isinstance(password, bytes) or not isinstance(salt, bytes): raise ValueError(\\"Invalid input type. data, password, and salt should be bytes.\\") # SHA-256 sha256_hash = hashlib.sha256(data).hexdigest() # SHA3-512 sha3_512_hash = hashlib.sha3_512(data).hexdigest() # BLAKE2b with a digest size of 32 bytes blake2b_hash = hashlib.blake2b(data, digest_size=32).hexdigest() # BLAKE2b with a keyed hash for data authentication keyed_blake2b_hash = hashlib.blake2b(data, digest_size=16, key=salt).hexdigest() # PBKDF2-HMAC with SHA-256 pbkdf2_hmac_hash = hashlib.pbkdf2_hmac(\'sha256\', password, salt, 100000).hex() return { \'sha256\': sha256_hash, \'sha3_512\': sha3_512_hash, \'blake2b\': blake2b_hash, \'keyed_blake2b\': keyed_blake2b_hash, \'pbkdf2_hmac\': pbkdf2_hmac_hash }"},{"question":"# Advanced Turtle Graphics Challenge Objective: Write a Python function that uses the `turtle` module to draw a pattern of concentric polygons. The function should demonstrate proficiency in utilizing turtle methods, managing turtle state, and event handling within the `turtle` graphics framework. Function Signature: ```python def draw_concentric_polygons(sides: int, distance: int, number_of_polygons: int) -> None: ``` Input Parameters: - `sides` (int): The number of sides for each polygon (e.g., 3 for a triangle, 4 for a square). - `distance` (int): The distance between the consecutive polygons. - `number_of_polygons` (int): The number of concentric polygons to draw. Output: - This function should not return anything. Instead, it should display a `turtle` graphics window with the drawn pattern. Constraints: 1. The `sides` parameter should be greater than or equal to 3 and less than or equal to 10. 2. The `distance` parameter should be a positive integer. 3. The `number_of_polygons` parameter should be a positive integer, but not more than 20 to prevent excessive drawing time. Requirements: - Use appropriate `turtle` methods to move and draw shapes. - The turtle should start at a default position and follow the default orientation. - Each polygon should be centered in the same position but with increasing size. - Utilize `turtle` methods for setting colors and pen states to differentiate between each polygon (optional but encouraged for enhancing visibility). Example: ```python # Expected behavior: this will draw 5 concentric triangles with increasing sizes. draw_concentric_polygons(sides=3, distance=20, number_of_polygons=5) ``` Additional Information: - Ensure that the `turtle` window remains open until the user closes it manually, enabling a proper inspection of the drawn pattern. - Provide comments and docstrings to explain the key steps in your implementation. Good luck!","solution":"import turtle def draw_concentric_polygons(sides: int, distance: int, number_of_polygons: int) -> None: Draws concentric polygons using the turtle graphics module. Parameters: sides (int): The number of sides for each polygon (3 <= sides <= 10). distance (int): The distance between the sides of consecutive polygons. number_of_polygons (int): The number of concentric polygons to draw (1 <= number_of_polygons <= 20). if sides < 3 or sides > 10: raise ValueError(\\"The number of sides must be between 3 and 10.\\") if distance <= 0: raise ValueError(\\"Distance must be a positive integer.\\") if number_of_polygons <= 0 or number_of_polygons > 20: raise ValueError(\\"The number of polygons must be between 1 and 20.\\") angle = 360 / sides turtle.speed(\'fastest\') for i in range(number_of_polygons): for _ in range(sides): turtle.forward(distance * (i + 1)) turtle.left(angle) turtle.penup() turtle.goto(-distance * (i + 1) // 2, -distance * (i + 1) // 2) turtle.pendown() turtle.done()"},{"question":"<|Analysis Begin|> The provided documentation covers several advanced topics related to extending PyTorch, including creating custom operators, extending `torch.autograd` with new functions, creating custom modules in `torch.nn`, and extending the PyTorch Python API with custom tensor-like types. From this documentation, the fundamental concepts and skills a student needs to understand include: 1. Implementing and using custom autograd functions by subclassing `torch.autograd.Function`. 2. Creating and using custom modules by extending `torch.nn.Module`. 3. Implementing custom tensor behaviors using the `__torch_function__` or `__torch_dispatch__` methods. To create an assessment question that requires students to demonstrate their comprehension of both fundamental and advanced concepts in PyTorch, a good challenge would involve the implementation of a custom autograd function and then using it within a custom `torch.nn.Module`. This way, students can show their understanding of both extending autograd functionality and properly integrating it within the module system. <|Analysis End|> <|Question Begin|> You are tasked with implementing a custom PyTorch operation and using it within a neural network module. Your goal is to create a custom autograd function and then integrate it into a custom `torch.nn.Module` for a linear transformation that includes an additional custom operation. Follow the steps below to complete the task: # Step 1: Implement Custom Autograd Function Create a custom autograd function that performs an element-wise square of the input tensor during the forward pass and computes the corresponding gradients during the backward pass. # Step 2: Implement Custom Module Create a custom module `CustomLinear` that extends `torch.nn.Module`. The module should perform a linear transformation followed by the custom element-wise square operation from Step 1. # Step 3: Testing and Validation Write a test script to validate your implementation, including: - Creating instances of the custom module. - Performing a forward pass. - Computing gradients with respect to the inputs. # Implementation Details Custom Autograd Function Implement a custom autograd function `SquareFunction` by subclassing `torch.autograd.Function`. Implement the required `forward` and `backward` methods. Custom Module Define a class `CustomLinear` extending `torch.nn.Module`. Define `__init__` and `forward` methods. Integrate the custom autograd function into the `forward` method following a linear transformation. Use `torch.nn.Linear` for the linear transformation part. Expected Function Signatures: ```python import torch from torch.autograd import Function import torch.nn as nn class SquareFunction(Function): @staticmethod def forward(ctx, input): # Perform the forward pass (element-wise square) ctx.save_for_backward(input) return input ** 2 @staticmethod def backward(ctx, grad_output): # Compute the gradient of the input input, = ctx.saved_tensors grad_input = 2 * input * grad_output return grad_input class CustomLinear(nn.Module): def __init__(self, in_features, out_features, bias=True): super(CustomLinear, self).__init__() self.linear = nn.Linear(in_features, out_features, bias=bias) def forward(self, input): # Apply linear transformation followed by custom square operation linear_output = self.linear(input) squared_output = SquareFunction.apply(linear_output) return squared_output # Testing and validation script def test_custom_module(): batch_size, in_features, out_features = 4, 5, 3 model = CustomLinear(in_features, out_features) input_tensor = torch.randn(batch_size, in_features, requires_grad=True) # Forward pass output = model(input_tensor) print(\\"Output: \\", output) # Backward pass output.sum().backward() print(\\"Input gradients: \\", input_tensor.grad) if __name__ == \\"__main__\\": test_custom_module() ``` # Constraints and Limitations: - You should not use any external libraries other than PyTorch. - Ensure your `SquareFunction` properly saves any necessary tensors for the backward pass. - Verify the correctness of gradients using the `.backward()` method. # Performance Requirements: - Your implementation should handle inputs of shape `(batch_size, in_features)` efficiently. - Ensure that the implementation is tested and correctly propagates gradients through the network.","solution":"import torch from torch.autograd import Function import torch.nn as nn class SquareFunction(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input ** 2 @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = 2 * input * grad_output return grad_input class CustomLinear(nn.Module): def __init__(self, in_features, out_features, bias=True): super(CustomLinear, self).__init__() self.linear = nn.Linear(in_features, out_features, bias=bias) def forward(self, input): linear_output = self.linear(input) squared_output = SquareFunction.apply(linear_output) return squared_output"},{"question":"Terminal Text Input Mode Manipulator You are tasked with developing a command-line utility to toggle a Unix terminal between normal, raw, and cbreak modes. Your program should provide a menu to the user to choose the mode they want to switch to. Implement a function that takes user input to set the terminal mode using the `tty` module functions. Requirements: 1. **Function Name**: `set_terminal_mode` 2. **Input**: The function should accept a string input from the user indicating the desired mode: - `\\"normal\\"`: Reverts the terminal to its default mode. - `\\"raw\\"`: Puts the terminal in raw mode using `tty.setraw(fd)`. - `\\"cbreak\\"`: Puts the terminal in cbreak mode using `tty.setcbreak(fd)`. 3. **Output**: The function should print a message indicating the new mode of the terminal. 4. **Constraints**: - Assume the terminal file descriptor is standard input (file descriptor 0). - If the input is not one of the specified modes, print an error message. 5. **Performance**: The function should handle user input and errors gracefully. Example Usage: ```python def set_terminal_mode(mode: str): import sys import tty import termios fd = sys.stdin.fileno() original_mode = termios.tcgetattr(fd) if mode == \\"normal\\": termios.tcsetattr(fd, termios.TCSAFLUSH, original_mode) print(\\"Terminal set to normal mode.\\") elif mode == \\"raw\\": tty.setraw(fd) print(\\"Terminal set to raw mode.\\") elif mode == \\"cbreak\\": tty.setcbreak(fd) print(\\"Terminal set to cbreak mode.\\") else: print(\\"Error: Invalid mode. Choose \'normal\', \'raw\', or \'cbreak\'.\\") # Example interaction set_terminal_mode(\\"cbreak\\") set_terminal_mode(\\"raw\\") set_terminal_mode(\\"normal\\") set_terminal_mode(\\"invalid\\") ``` Notes: - Make sure to handle exceptions, ensuring the terminal is reset to its original mode if an error occurs. - You may need to test and run this script in a Unix environment due to the dependency on the `termios` module.","solution":"import sys import tty import termios def set_terminal_mode(mode: str): Set the terminal mode to \'normal\', \'raw\', or \'cbreak\'. fd = sys.stdin.fileno() original_mode = termios.tcgetattr(fd) try: if mode == \\"normal\\": termios.tcsetattr(fd, termios.TCSAFLUSH, original_mode) print(\\"Terminal set to normal mode.\\") elif mode == \\"raw\\": tty.setraw(fd) print(\\"Terminal set to raw mode.\\") elif mode == \\"cbreak\\": tty.setcbreak(fd) print(\\"Terminal set to cbreak mode.\\") else: print(\\"Error: Invalid mode. Choose \'normal\', \'raw\', or \'cbreak\'.\\") except Exception as e: termios.tcsetattr(fd, termios.TCSAFLUSH, original_mode) print(f\\"Error setting terminal mode: {e}\\")"},{"question":"# Question: Automating Compilation of Conditional Python Libraries Problem Statement: You are developing a Python project wherein multiple libraries need to be installed, compiled, and managed across different environments. Your task is to create a Python script `conditional_compile.py` using the `compileall` module. This script should: 1. **Accept multiple directories** to compile corresponding Python files. 2. **Optionally force recompilation** even if timestamps are up-to-date. 3. **Exclude specific paths** or files by matching the provided regex patterns. 4. **Compile files using multiple worker threads** for parallel processing. 5. **Provide adjustable verbosity levels** for output. Requirements: - Implement a function `conditional_compile(dirs, force=False, regex=None, workers=1, verbosity=0)`, where: * `dirs` is a list of directories to compile. * `force` is a boolean, when `True` it forces re-compilation; default is `False`. * `regex` is a string pattern; files matching this pattern are skipped during compilation. * `workers` is an integer defining the number of worker threads for parallel compilation. * `verbosity` is an integer where `0` means no output, `1` means error messages only, and `2` means all output is displayed. Function Implementation: - Validate the inputs. - Utilize the `compileall.compile_dir` function to compile the provided directories. - Ensure the function accommodates the specified parameters for force, regex filtering, and using multiple workers. Example Usage: ```python import re from compileall import compile_dir def conditional_compile(dirs, force=False, regex=None, workers=1, verbosity=0): if not all(map(lambda d: isinstance(d, str), dirs)): raise ValueError(\\"All directories must be specified as strings.\\") if regex: try: rx = re.compile(regex) except re.error: raise ValueError(\\"Invalid regex pattern provided.\\") else: rx = None for dir in dirs: compile_dir( dir, force=force, quiet=verbosity, workers=workers, rx=rx ) # Example of how to call the function conditional_compile([\'lib1\', \'lib2\'], force=True, regex=r\'[/]test_.*\', workers=4, verbosity=1) ``` Constraints: - The script should not manually create `.pyc` or other byte-code files, and should exclusively use the `compileall` module for this task. - Ensure the regular expressions are compiled only once per invocation to optimize performance. Explanation: Your task demonstrates understanding of file system operations, using regular expressions for filtering, parallel processing techniques, and the flexibility to handle verbose outputs. The `compileall` module\'s documentation and this exercise highlight your ability to synthesize and apply the advanced features provided by Python’s standard library.","solution":"import re import compileall def conditional_compile(dirs, force=False, regex=None, workers=1, verbosity=0): Compiles Python files in the specified directories. Args: dirs (list): List of directories to compile. force (bool): Force recompilation even if timestamps are up-to-date. Default is False. regex (str): Regex pattern to exclude files. workers (int): Number of worker threads for parallel compilation. Default is 1. verbosity (int): Verbosity level for output. 0 = no output, 1 = errors only, 2 = all output. Raises: ValueError: If directories are not strings or if regex pattern is invalid. if not all(isinstance(d, str) for d in dirs): raise ValueError(\\"All directories must be specified as strings.\\") rx = None if regex: try: rx = re.compile(regex) except re.error as e: raise ValueError(f\\"Invalid regex pattern provided: {e}\\") for dir_path in dirs: compileall.compile_dir( dir_path, force=force, quiet=verbosity < 2, workers=workers, rx=rx )"},{"question":"Objective The objective of this question is to assess your understanding of assignment statements in Python. You are required to implement a function that demonstrates various types of assignment statements, including simple assignments, augmented assignments, and annotated assignments. Problem Description You are to write a function `process_data` that takes a list of tuples as input. Each tuple contains a command and a value. The function should process each command based on the assignment rules described below. 1. **Simple Assignment**: The command will be in the form `(\\"assign\\", target, value)`. Perform a simple assignment where the target is set to the value. 2. **Augmented Assignment**: The command will be in the form `(\\"aug_assign\\", target, operation, value)`. Perform an augmented assignment operation (+=, -=, *=, /=). 3. **Annotated Assignment**: The command will be in the form `(\\"annotate\\", target, annotation)`. Annotate the target variable with the specified annotation. The function should return a dictionary representing the final state of all variables after the commands have been processed. Input Format - A list of commands where each command is a tuple. - Simple Assignment: `(\\"assign\\", target, value)` - Augmented Assignment: `(\\"aug_assign\\", target, operation, value)` - Annotated Assignment: `(\\"annotate\\", target, annotation)` Output Format - A dictionary representing the final state of all variables, including their values and annotations. Example ```python def process_data(commands): # implementation here commands = [ (\\"assign\\", \\"x\\", 10), (\\"assign\\", \\"y\\", 20), (\\"aug_assign\\", \\"x\\", \\"+=\\", 5), (\\"annotate\\", \\"y\\", \\"int\\") ] output = process_data(commands) print(output) ``` Expected Output: ```python { \\"x\\": 15, \\"y\\": 20, \\"__annotations__\\": { \\"y\\": \\"int\\" } } ``` Constraints - Assume the `target` in any command will be a valid variable name (a string). - Supported operations for augmented assignments are `+=`, `-=`, `*=`, `/=`. - The value for assignments and augmented assignments will be integers. - All annotations will be strings. Implementation Notes - Use dictionaries to maintain the values and annotations of the variables. - Make sure to handle cases where augmented operations are performed on variables that have not been assigned yet. # Function Signature ```python def process_data(commands: list) -> dict: pass ```","solution":"def process_data(commands): variables = {} annotations = {} for command in commands: if command[0] == \\"assign\\": _, target, value = command variables[target] = value elif command[0] == \\"aug_assign\\": _, target, operation, value = command if target not in variables: variables[target] = 0 if operation == \\"+=\\": variables[target] += value elif operation == \\"-=\\": variables[target] -= value elif operation == \\"*=\\": variables[target] *= value elif operation == \\"/=\\": variables[target] /= value elif command[0] == \\"annotate\\": _, target, annotation = command annotations[target] = annotation if annotations: variables[\\"__annotations__\\"] = annotations return variables"},{"question":"# PyTorch Assessment: Meta Device and Tensor Operations **Objective**: You are required to demonstrate your understanding of PyTorch\'s meta device and its usage for managing tensors that only store metadata. The task involves: 1. Loading a model onto the meta device. 2. Performing an operation on the meta tensor. 3. Converting the model back from the meta device to a real device (CPU) post data reinitialization. **Problem Statement**: Implement the following steps in your solution: 1. Define a simple neural network model using `torch.nn.Module`. You can use a predefined architecture like a two-layer linear network with ReLU activation between them. 2. Save the model state to a file named `model.pt`. 3. Load the model from the file onto the meta device. 4. Create a meta tensor with specific dimensions (e.g., [50, 50]). 5. Perform some tensor operations on this meta tensor (e.g., matrix multiplication with itself). 6. Convert the neural network model back to the CPU device, ensuring the parameters are reinitialized with ones. **Expected Input/Output**: - No direct inputs; you would be defining the model and performing operations within the code. - Output should include: - The meta tensor with its device and size displayed. - The results of the tensor operations. - The summary of the reinitialized model parameters after moving back to CPU. **Constraints**: - Do not use real data for tensor operations beyond what is allowed for meta tensors. - Ensure that all tensor operations are valid on meta tensors. **Performance Requirements**: - Efficient handling of model and tensor loading, conversion, and operations. **Sample Solution Outline**: ```python import torch import torch.nn as nn # 1. Define a simple neural network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(100, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Initialize and save the model model = SimpleNN() torch.save(model.state_dict(), \'model.pt\') # 2. Load the model on the meta device model_meta = SimpleNN() model_meta.load_state_dict(torch.load(\'model.pt\', map_location=\'meta\')) # 3. Create a meta tensor and perform operations with torch.device(\'meta\'): meta_tensor = torch.randn(50, 50) print(f\'Meta tensor: device={meta_tensor.device}, size={meta_tensor.size()}\') # Perform tensor operations result_tensor = meta_tensor @ meta_tensor print(f\'Result tensor: device={result_tensor.device}, size={result_tensor.size()}\') # 4. Convert the model back to CPU and reinitialize parameters model_cpu = model_meta.to_empty(device=\'cpu\') for param in model_cpu.parameters(): param.data.fill_(1) print(model_cpu) ``` **Notes**: - Ensure the solution is clean and includes necessary comments for clarity. - You are free to add any helper functions or additional steps if required.","solution":"import torch import torch.nn as nn # 1. Define a simple neural network model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(100, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Initialize and save the model model = SimpleNN() torch.save(model.state_dict(), \'model.pt\') # 2. Load the model on the meta device meta_device = torch.device(\'meta\') model_meta = SimpleNN().to(meta_device) model_meta.load_state_dict(torch.load(\'model.pt\', map_location=meta_device)) # 3. Create a meta tensor and perform operations meta_tensor = torch.empty(50, 50, device=\'meta\') print(f\'Meta tensor: device={meta_tensor.device}, size={meta_tensor.size()}\') # Perform tensor operations result_tensor = meta_tensor @ meta_tensor print(f\'Result tensor: device={result_tensor.device}, size={result_tensor.size()}\') # 4. Convert the model back to CPU and reinitialize parameters model_cpu = SimpleNN() model_cpu.load_state_dict(torch.load(\'model.pt\')) for param in model_cpu.parameters(): param.data.fill_(1) print(model_cpu)"},{"question":"# Advanced Python Module Importing **Objective:** To assess your understanding of advanced module importing techniques in Python, including importing from zip archives, programmatically checking for module availability, and dynamically importing modules using `importlib`. **Problem Statement:** You are tasked with creating a Python program that dynamically imports and checks for the availability of certain modules from specified locations (including zip files). You must implement the following functions: 1. **`import_from_zip(zip_path: str, module_name: str) -> module`**: - **Input:** - `zip_path` (str): The file path to the zip archive. - `module_name` (str): The name of the module to import from the zip archive. - **Output:** - Returns the imported module object if successful, otherwise raises an appropriate exception (e.g., `ModuleNotFoundError`). 2. **`is_module_available(module_name: str) -> bool`**: - **Input:** - `module_name` (str): The name of the module to check for availability. - **Output:** - Returns `True` if the module is available for import, otherwise `False`. 3. **`import_module_dynamic(module_name: str) -> module`**: - **Input:** - `module_name` (str): The name of the module to import dynamically. - **Output:** - Returns the imported module object if successful, otherwise raises an appropriate exception (e.g., `ModuleNotFoundError`). **Constraints:** - You are not allowed to use the regular `import` statement directly within the functions (except for importing necessary utilities). - You must handle potential errors and exceptions gracefully. **Examples:** 1. **import_from_zip** ```python # Assuming `myarchive.zip` contains a module named `mymodule.py` module = import_from_zip(\\"path/to/myarchive.zip\\", \\"mymodule\\") print(module) ``` 2. **is_module_available** ```python print(is_module_available(\\"math\\")) # Should return True print(is_module_available(\\"non_existing_module\\")) # Should return False ``` 3. **import_module_dynamic** ```python module = import_module_dynamic(\\"datetime\\") print(module) ``` **Notes:** - Ensure your code handles edge cases, such as invalid file paths or module names. - The function `import_module_dynamic` can be used to import any module available in the Python environment. - The function `import_from_zip` should leverage `zipimport` and handle the extraction and importing appropriately. Implement these functions in a `module_importer.py` file and provide comprehensive test cases to validate your solutions.","solution":"import zipimport import importlib def import_from_zip(zip_path, module_name): Imports a module from a given zip archive. Args: zip_path (str): The file path to the zip archive. module_name (str): The name of the module to import from the zip archive. Returns: module: The imported module object if successful. Raises: ModuleNotFoundError: If the module cannot be found or imported. try: importer = zipimport.zipimporter(zip_path) module = importer.load_module(module_name) return module except Exception as e: raise ModuleNotFoundError(f\\"Module {module_name} not found in {zip_path}\\") from e def is_module_available(module_name): Checks if a module is available for import. Args: module_name (str): The name of the module to check for availability. Returns: bool: True if the module is available for import, otherwise False. spec = importlib.util.find_spec(module_name) return spec is not None def import_module_dynamic(module_name): Dynamically imports a module. Args: module_name (str): The name of the module to import dynamically. Returns: module: The imported module object if successful. Raises: ModuleNotFoundError: If the module cannot be found or imported. try: module = importlib.import_module(module_name) return module except ModuleNotFoundError as e: raise ModuleNotFoundError(f\\"Module {module_name} not found\\") from e"},{"question":"# Coding Assessment: Functional Programming with Python Problem Statement: Using the `itertools`, `functools`, and `operator` modules, implement a function that processes a list of integers to perform the following steps: 1. **Filter** the list to keep only even numbers. 2. **Square** each of the filtered even numbers. 3. **Sum** the squared numbers. 4. **Partially apply** this transformation to allow the processing of multiple lists with the same function configuration. The function `process_numbers` should take the following inputs: - `numbers`: A list of integers. The output should be an integer representing the sum of squares of even numbers in the list. Additionally, implement a function `config_process_numbers` that generates a partially applied version of `process_numbers` based on a given filter and transformation sequence. Function Signature: ```python def process_numbers(numbers: list) -> int: pass def config_process_numbers(filter_func, transform_func) -> callable: pass ``` Example: ```python from operator import mod, pow import functools # Example usage of process_numbers numbers = [1, 2, 3, 4, 5, 6, 7, 8] print(process_numbers(numbers)) # Output: 120 # Example usage of config_process_numbers custom_filter = lambda x: mod(x, 2) == 1 # Filter odd numbers custom_transform = lambda x: pow(x, 3) # Cube the numbers custom_process = config_process_numbers(custom_filter, custom_transform) custom_numbers = [1, 2, 3, 4, 5] print(custom_process(custom_numbers)) # Output: 153 (since 1^3 + 3^3 + 5^3 = 1 + 27 + 125 = 153) ``` Constraints: - The input list can have up to 10^6 elements. - Elements in the input list can range from -10^6 to 10^6. Notes: - Use `itertools` for efficient looping and filtering. - Use `functools.partial` to create partially applied functions. - Use `operator` functions for transformations and conditions.","solution":"from itertools import filterfalse from functools import reduce, partial from operator import mod, pow def process_numbers(numbers: list) -> int: Process a list of numbers by filtering even numbers, squaring them, and summing the results. # Step 1: Filter even numbers evens = filter(lambda x: x % 2 == 0, numbers) # Step 2: Square the filtered even numbers squared_evens = map(lambda x: x ** 2, evens) # Step 3: Sum the squared numbers result = reduce(lambda x, y: x + y, squared_evens, 0) return result def config_process_numbers(filter_func, transform_func) -> callable: Generate a partially applied version of `process_numbers` based on provided filter and transformation functions. def custom_process(numbers): # Apply the provided filter function filtered_numbers = filter(filter_func, numbers) # Apply the provided transformation function transformed_numbers = map(transform_func, filtered_numbers) # Sum the transformed numbers result = reduce(lambda x, y: x + y, transformed_numbers, 0) return result return custom_process"},{"question":"# Password Generator with Security Constraints **Objective:** Design and implement a function `generate_secure_password` in Python that utilizes the `secrets` module to create a secure password that meets specific complexity requirements and length constraints. **Function Signature:** ```python def generate_secure_password(length: int) -> str: pass ``` **Input:** - `length` (int): The length of the desired password. Must be an integer greater than or equal to 12. **Output:** - A string representing a randomly generated secure password. **Complexity Requirements:** The generated password must satisfy the following criteria: 1. It must be exactly `length` characters long. 2. It must contain at least: - 1 uppercase letter (A-Z), - 1 lowercase letter (a-z), - 1 digit (0-9), - 1 special character from the set `!@#%^&*()-_=+[]{}|;:,.<>?/`. 3. The entire password must be generated using functions from the `secrets` module to ensure cryptographic security. **Constraints:** - The function should raise a `ValueError` if the `length` is less than 12. - The password must be generated in such a way that the characters are randomly distributed to avoid predictable placement. **Example Usage:** ```python password = generate_secure_password(16) print(password) # Output: A randomly generated secure password of length 16 that meets the complexity requirements. ``` **Performance Requirements:** - Your solution should efficiently handle typical password lengths (e.g., 12 to 64 characters) within a reasonable execution time.","solution":"import secrets import string def generate_secure_password(length: int) -> str: Generates a secure password with the given length that meets certain complexity requirements. if length < 12: raise ValueError(\\"Password length must be at least 12 characters.\\") # Character sets upper = string.ascii_uppercase lower = string.ascii_lowercase digits = string.digits special = \\"!@#%^&*()-_=+[]{}|;:,.<>?/\\" # Ensure one character from each set is in the password password_chars = [ secrets.choice(upper), secrets.choice(lower), secrets.choice(digits), secrets.choice(special), ] # Fill the remaining length with a mix of all character sets if length > 4: all_chars = upper + lower + digits + special password_chars.extend(secrets.choice(all_chars) for _ in range(length - 4)) # Shuffle password characters to avoid predictable placement secrets.SystemRandom().shuffle(password_chars) return \'\'.join(password_chars)"},{"question":"<|Analysis Begin|> The documentation provided explains various advanced features of pandas, focusing heavily on MultiIndex (hierarchical indexing). Key concepts covered include: - Creation of MultiIndexes using arrays, tuples, products, and DataFrames. - Hierarchical indexing and its integration with pandas indexing. - Methods related to MultiIndex such as `get_level_values`, `swaplevel`, `reorder_levels`, and `remove_unused_levels`. - Indexing operations and advanced indexing with MultiIndex using `.loc`, `IndexSlice`, boolean indexers, and slicers. - Index types along with advanced reindexing and alignment. The documentation provides comprehensive examples, making it possible to design an assessment question that involves creating and manipulating a MultiIndex, performing advanced indexing and selection, and ensuring efficient data alignment. <|Analysis End|> <|Question Begin|> # Advanced Pandas Data Manipulation with MultiIndex **Problem Statement:** You have been provided a dataset containing information about sales transactions in a multi-level format. Your task is to create and manipulate a `MultiIndex` DataFrame to answer specific queries about the data. The objective is to assess your understanding of hierarchical indexing and advanced indexing features in pandas. **Dataset:** ```python import pandas as pd import numpy as np data = { \'Product\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\', \'D\', \'D\'], \'Region\': [\'North\', \'South\', \'North\', \'South\', \'North\', \'South\', \'North\', \'South\'], \'Sales\': [250, 300, 150, 230, 100, 220, 400, 170], \'Profit\': [50, 70, 40, 60, 30, 80, 100, 55] } df = pd.DataFrame(data) ``` **Tasks:** 1. **Create a MultiIndex DataFrame:** - Convert the DataFrame `df` into a MultiIndex DataFrame with `Product` and `Region` as hierarchical indexes (in that order). - Name the indexes appropriately. 2. **Indexing and Selection:** - Select the data for product \'A\' across all regions. - Select the sales data for \'North\' regions across all products. - Use a list of tuples to select sales data for (\'A\', \'North\') and (\'C\', \'South\'). 3. **Advanced Indexing:** - Swap the levels of the MultiIndex so that `Region` is the first level and `Product` is the second level. - Perform a partial indexing to select all products in the \'South\' region. - Use `IndexSlice` to slice the data for the \'North\' region, but only for products \'B\' and \'D\'. 4. **Reindexing and Data Alignment:** - Reindex the MultiIndex DataFrame to add regions \'East\' and \'West\' for each product. Note that there may be missing data. - Align the original dataset with a new dataset containing overall sales averages per product (ignoring the region). - Ensure that the final aligned DataFrame has data filled appropriately for missing entries. **Function Signature:** ```python def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Input:** - `df`: A DataFrame containing columns [\'Product\', \'Region\', \'Sales\', \'Profit\'] **Output:** - A processed DataFrame based on the tasks above. **Constraints:** - Assume the dataset size will not exceed 1000 rows. - Use appropriate pandas methods/functions to achieve the tasks efficiently. **Example Usage:** ```python result_df = process_sales_data(df) print(result_df) ``` Ensure your solution produces correct and optimal results for the outlined tasks.","solution":"import pandas as pd import numpy as np def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: # Task 1: Create a MultiIndex DataFrame df = df.set_index([\'Product\', \'Region\']) df.index.names = [\'Product\', \'Region\'] # Task 2: Indexing and Selection # Select the data for product \'A\' across all regions product_a_data = df.loc[\'A\'] # Select the sales data for \'North\' regions across all products north_sales = df.loc[(slice(None), \'North\'), \'Sales\'] # Use a list of tuples to select sales data for (\'A\', \'North\') and (\'C\', \'South\') selected_sales = df.loc[[(\'A\', \'North\'), (\'C\', \'South\')], \'Sales\'] # Task 3: Advanced Indexing # Swap the levels of the MultiIndex df_swapped = df.swaplevel() # Perform a partial indexing to select all products in the \'South\' region south_data = df_swapped.loc[\'South\'] # Use IndexSlice to slice data for \'North\' region but only for products \'B\' and \'D\' idx = pd.IndexSlice north_bd_data = df.loc[idx[[\'B\', \'D\'], \'North\'], :] # Task 4: Reindexing and Data Alignment # Reindex to add regions \'East\' and \'West\' for each product new_index = pd.MultiIndex.from_product( [df.index.get_level_values(\'Product\').unique(), [\'North\', \'South\', \'East\', \'West\']], names=[\'Product\', \'Region\'] ) df_reindexed = df.reindex(new_index) # Create dataset with overall sales averages per product df_averages = df.groupby(\'Product\')[[\'Sales\', \'Profit\']].mean() # Align the original dataset with the new dataset aligned_df = df_reindexed.join(df_averages, rsuffix=\'_avg\', how=\'outer\') # Fill missing entries with the average values where possible and NaN otherwise aligned_df[\'Sales\'].fillna(aligned_df[\'Sales_avg\'], inplace=True) aligned_df[\'Profit\'].fillna(aligned_df[\'Profit_avg\'], inplace=True) aligned_df = aligned_df.drop(columns=[\'Sales_avg\', \'Profit_avg\']) # Return the final processed DataFrame return aligned_df"},{"question":"# PyTorch MPS Device Management and Profiling You are tasked with creating a function that utilizes PyTorch\'s MPS (Metal Performance Shaders) backend. Your goal is to perform a specific computation on tensors using MPS devices and profile the operation to analyze performance. You will need to implement the following steps: 1. **Check MPS Device Availability**: Ensure there are MPS devices available. If none are available, the function should raise an appropriate exception. 2. **Initialize Random Seed**: Use a user-provided random seed to ensure reproducibility. 3. **Create and Perform Tensor Operations**: Create two random tensors of a specified shape and perform a specified operation (e.g., addition, multiplication) on them using MPS as the device. 4. **Profile the Operation**: Use the MPS profiler to measure the time taken for the tensor operation. 5. **Memory Management**: After the operation, retrieve the current allocated memory and empty the cache. # Function Specification ```python def mps_tensor_operation(shape: tuple, operation: str, seed: int) -> dict: Perform a tensor operation using PyTorch MPS devices and profile the performance. Args: shape (tuple): Shape of the tensors to be created. operation (str): Operation to perform on the tensors (\'add\' or \'mul\'). seed (int): Random seed for reproducibility. Returns: dict: A dictionary containing: \'operation_time\': Time taken for the operation (in seconds). \'allocated_memory\': Memory allocated before emptying the cache (in bytes). Raises: ValueError: If no MPS devices are available or if an invalid operation is specified. ``` # Implementation Details 1. **Check MPS Device Availability**: Use the `torch.mps.device_count` function to check the number of available devices. 2. **Initialize Random Seed**: Utilize `torch.mps.manual_seed` or `torch.mps.seed` to set the random seed. 3. **Create Tensors and Perform Operation**: Use `torch.randn` to create random tensors and perform the specified operation on an MPS device. 4. **Profile the Operation**: - Start the profiler using `torch.mps.profiler.start()`. - Perform the operation. - Stop the profiler using `torch.mps.profiler.stop()`. 5. **Memory Management**: - Use `torch.mps.current_allocated_memory` to get the allocated memory. - Use `torch.mps.empty_cache` to clear the cache. # Example Usage ```python result = mps_tensor_operation((1000, 1000), \'add\', 42) print(result) ``` This should output a dictionary with the time taken for the addition operation and the allocated memory before clearing the cache. **Constraints**: - Ensure the input operation is either \'add\' or \'mul\'. - Handle any exceptions where MPS devices are not available or other errors.","solution":"import torch import time def mps_tensor_operation(shape: tuple, operation: str, seed: int) -> dict: Perform a tensor operation using PyTorch MPS devices and profile the performance. Args: shape (tuple): Shape of the tensors to be created. operation (str): Operation to perform on the tensors (\'add\' or \'mul\'). seed (int): Random seed for reproducibility. Returns: dict: A dictionary containing: \'operation_time\': Time taken for the operation (in seconds). \'allocated_memory\': Memory allocated before emptying the cache (in bytes). Raises: ValueError: If no MPS devices are available or if an invalid operation is specified. if not torch.backends.mps.is_available(): raise ValueError(\\"No MPS devices are available.\\") if operation not in [\'add\', \'mul\']: raise ValueError(\\"Invalid operation specified. Use \'add\' or \'mul\'.\\") torch.manual_seed(seed) device = torch.device(\\"mps\\") tensor1 = torch.randn(shape, device=device) tensor2 = torch.randn(shape, device=device) start_time = time.time() if operation == \'add\': result = tensor1 + tensor2 elif operation == \'mul\': result = tensor1 * tensor2 end_time = time.time() operation_time = end_time - start_time allocated_memory = torch.cuda.memory_allocated(device) torch.cuda.empty_cache() return { \'operation_time\': operation_time, \'allocated_memory\': allocated_memory }"},{"question":"You are required to demonstrate your understanding of Gaussian Mixture Models (GMM) and Bayesian Gaussian Mixture Models (BGMM) using scikit-learn. Follow the tasks below to design and analyze a dataset using these models. # Tasks: 1. **Generate a Synthetic Dataset**: - Use `numpy` to generate a synthetic dataset with three distinct clusters. Each cluster should follow a Gaussian distribution with different means and covariance matrices. - The dataset should have at least 300 points (100 per cluster). 2. **Fit a Gaussian Mixture Model**: - Fit a `GaussianMixture` model to the dataset. Experiment with different numbers of components and covariance types. - Use BIC to determine the optimal number of components. - Output the BIC values for 1 to 6 components and plot them. 3. **Analyze the Gaussian Mixture Model**: - Predict the cluster for each data point and visualize the results using a scatter plot. Use different colors for different clusters. - Draw the confidence ellipsoids for each cluster using the model\'s parameters. 4. **Fit a Bayesian Gaussian Mixture Model**: - Fit a `BayesianGaussianMixture` model to the dataset. - Experiment with different values of `weight_concentration_prior` and analyze the number of components used by the model. - Plot the data points colored by their predicted cluster, similar to the previous GMM scatter plot. 5. **Discussion**: - Compare the performance and results of the `GaussianMixture` and `BayesianGaussianMixture` models. Discuss the influence of the `weight_concentration_prior` in BGMM and the BIC in GMM for selecting the number of components. # Constraints: - Use only the `sklearn.mixture`, `numpy`, `matplotlib` libraries for the implementation. - Ensure reproducibility by setting random seeds where applicable. # Input and Output Formats: **Input:** - Synthetic dataset generated within the code. **Output:** - BIC values for different numbers of components. - Plots showing cluster assignments and confidence ellipsoids for both GMM and BGMM. - Written comparison of GMM and BGMM performance. # Example Code Structure: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture # Task 1: Generate Dataset np.random.seed(0) # Code to generate dataset... # Task 2: Fit GMM bic_scores = [] # Code to fit GaussianMixture, calculate BIC and plot... # Task 3: Analyze GMM # Code to predict using GMM and plot results... # Task 4: Fit BGMM # Code to fit BayesianGaussianMixture and plot results... # Task 5: Discussion # Write comparison discussion here... ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture # Task 1: Generate Dataset np.random.seed(42) def generate_synthetic_data(): # Means of the clusters mean1 = [0, 0] mean2 = [5, 5] mean3 = [10, 0] # Covariance matrices of the clusters cov1 = [[1, 0.5], [0.5, 1]] cov2 = [[1, -0.5], [-0.5, 1]] cov3 = [[1, 0.5], [0.5, 1]] # Generate 100 points from each cluster cluster1 = np.random.multivariate_normal(mean1, cov1, 100) cluster2 = np.random.multivariate_normal(mean2, cov2, 100) cluster3 = np.random.multivariate_normal(mean3, cov3, 100) # Combine the clusters to create the dataset X = np.vstack((cluster1, cluster2, cluster3)) return X # Create synthetic dataset X = generate_synthetic_data() # Task 2: Fit GMM bic_scores = [] n_components_range = range(1, 7) for n_components in n_components_range: gmm = GaussianMixture(n_components=n_components, random_state=42) gmm.fit(X) bic_scores.append(gmm.bic(X)) # Plot BIC scores plt.figure() plt.plot(n_components_range, bic_scores, marker=\'o\') plt.xlabel(\'Number of components\') plt.ylabel(\'BIC score\') plt.title(\'BIC scores for GaussianMixture model\') plt.show() # Determine the optimal number of components optimal_n_components = n_components_range[np.argmin(bic_scores)] # Fit the GMM with the optimal number of components best_gmm = GaussianMixture(n_components=optimal_n_components, random_state=42) best_gmm.fit(X) labels = best_gmm.predict(X) def plot_gmm_results(X, labels, gmm): plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] for i, color in enumerate(colors): data = X[labels == i] plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color, label=f\'Cluster {i}\') plt.title(\'GMM clustering results\') plt.legend() for mean, cov in zip(gmm.means_, gmm.covariances_): draw_ellipse(plt, mean, cov) plt.show() def draw_ellipse(plt, position, covariance, ax=None, **kwargs): from matplotlib.patches import Ellipse if ax is None: ax = plt.gca() if covariance.shape == (2, 2): eigvals, eigvecs = np.linalg.eigh(covariance) order = eigvals.argsort()[::-1] eigvals = eigvals[order] eigvecs = eigvecs[:, order] else: raise ValueError(\\"Covariance matrix shape is not 2x2\\") angle = np.degrees(np.arctan2(*eigvecs[:, 0][::-1])) width, height = 2 * np.sqrt(eigvals) ellip = Ellipse(xy=position, width=width, height=height, angle=angle, **kwargs) ax.add_artist(ellip) return ellip # Plot GMM results plot_gmm_results(X, labels, best_gmm) # Task 4: Fit BGMM bgmm = BayesianGaussianMixture(n_components=10, random_state=42) bgmm.fit(X) bgmm_labels = bgmm.predict(X) # Plot BGMM results plot_gmm_results(X, bgmm_labels, bgmm) # Task 5: Discussion discussion = The Gaussian Mixture Model (GMM) with BIC for model selection identified the optimal number of clusters as 3. The BIC criterion helps penalize the complexity of the model, leading to the selection of an appropriate number of components. The Bayesian Gaussian Mixture Model (BGMM), even though initialized with a higher number of components (10), tends to automatically determine the actual number of components needed based on the data and the specified weight concentration prior. This allows for more flexibility and often avoids overfitting as the weight concentration prior encourages sparsity. The GMM provides a clear model selection criterion through BIC, while BGMM offers flexibility through the Bayesian framework to estimate the number of effective components. Adjusting the `weight_concentration_prior` in BGMM controls the prior probability of each component and affects the sparsity of the model. print(discussion)"},{"question":"**Question**: Implement a Python function called `manage_dbm_database` that manages a database using the `dbm` module. The function should: 1. Create a new database file or open an existing one and provide options to: - Insert a key-value pair. - Retrieve a value for a given key. - Delete a key-value pair. - Traverse and list all keys in the database in an ordered manner. - Reorganize the database if there have been multiple deletions. - Close the database properly. **Function Specification:** ```python def manage_dbm_database(filename: str, operations: list[tuple]): Manage a dbm database with given operations. Parameters: filename (str): The name of the database file. operations (list[tuple]): A list of operations to perform on the database. Each tuple contains: - A string indicating the operation (\'insert\', \'retrieve\', \'delete\', \'list_keys\', \'reorganize\'). - For \'insert\': A second element with (key, value). - For \'retrieve\': A second element with the key. - For \'delete\': A second element with the key. - For \'list_keys\' and \'reorganize\': No additional elements. Returns: list: A list of results for each operation in order. For \'retrieve\' return value, \'list_keys\' return the list of keys, and \'reorganize\' and \'delete\' return a success message. pass ``` **Example Usage:** ```python operations = [ (\'insert\', (\'example_key\', \'example_value\')), (\'retrieve\', \'example_key\'), (\'insert\', (\'another_key\', \'another_value\')), (\'list_keys\',), (\'delete\', \'example_key\'), (\'list_keys\',), (\'reorganize\',), (\'retrieve\', \'nonexistent_key\') ] results = manage_dbm_database(\'testdb\', operations) print(results) ``` **Expected Output:** ``` [\'example_value\', [b\'another_key\', b\'example_key\'], [b\'another_key\'], \'Database reorganized\', \'Key not found\'] ``` **Constraints:** - All keys and values should be stored and retrieved as bytes. - Handle cases where the specified key does not exist without crashing the program. - Ensure that the database file is properly closed after all operations. **Evaluation Criteria:** - Correct implementation of database operations. - Proper handling of bytes for keys and values. - Handling edge cases like non-existent keys. - Clean and efficient code structure.","solution":"import dbm def manage_dbm_database(filename: str, operations: list): Manage a dbm database with given operations. Parameters: filename (str): The name of the database file. operations (list[tuple]): A list of operations to perform on the database. Each tuple contains: - A string indicating the operation (\'insert\', \'retrieve\', \'delete\', \'list_keys\', \'reorganize\'). - For \'insert\': A second element with (key, value). - For \'retrieve\': A second element with the key. - For \'delete\': A second element with the key. - For \'list_keys\' and \'reorganize\': No additional elements. Returns: list: A list of results for each operation in order. For \'retrieve\' return value, \'list_keys\' return the list of keys, and \'reorganize\' and \'delete\' return a success message. results = [] with dbm.open(filename, \'c\') as db: for op in operations: if op[0] == \'insert\': key, value = op[1] db[key.encode()] = value.encode() results.append(\'Inserted\') elif op[0] == \'retrieve\': key = op[1].encode() value = db.get(key, b\'Key not found\') results.append(value.decode()) elif op[0] == \'delete\': key = op[1].encode() if key in db: del db[key] results.append(\'Deleted\') else: results.append(\'Key not found\') elif op[0] == \'list_keys\': keys = sorted(db.keys()) results.append(keys) elif op[0] == \'reorganize\': # specific reorganization function is not available in dbm.dumb, so we acknowledge request results.append(\'Database reorganized\') else: results.append(\'Unknown operation\') return results"},{"question":"Your task is to analyze a dataset using Seaborn by creating various regression plots. The dataset `tips` is available in Seaborn and contains information about the total bill, tip amount, and other related details for customers in a restaurant. Part 1: Simple Linear Regression Plot 1. Load the `tips` dataset from Seaborn. 2. Create a simple scatter plot with a linear regression line for the variables `total_bill` (x-axis) and `tip` (y-axis) using `regplot`. Display this plot. Part 2: Polynomial Regression Plot 1. Create a second plot using `lmplot` that fits a second-order polynomial regression for the variables `total_bill` (x-axis) and `tip` (y-axis). Display this plot. Part 3: Logistic Regression Plot 1. Add a new column to the `tips` dataset called `big_tip` which is `True` if the tip is more than 15% of the total bill, and `False` otherwise. 2. Create a logistic regression plot using `lmplot` with `total_bill` as the x-axis and `big_tip` as the y-axis. Use `y_jitter` for better visualization. Display this plot. Part 4: Conditional Regression Plots 1. Using `lmplot`, create a conditional regression plot showing the relationship between `total_bill` and `tip`, separately for smokers and non-smokers. Use different colors for each category. 2. Extend the above plot to also separate the data by the time of day (Dinner or Lunch) in separate columns. Display this plot. Part 5: Residual Plot 1. Create a residual plot using `residplot` for the variables `total_bill` and `tip`. Display this plot. # Input - You don\'t need to take any external input; use the `tips` dataset from Seaborn. # Expected Outputs You need to generate and display six plots: 1. A simple linear regression plot. 2. A second-order polynomial regression plot. 3. A logistic regression plot. 4. A conditional regression plot by smoker status. 5. A conditional regression plot by smoker status and time. 6. A residual plot. ```python # Sample Skeleton Code import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Part 1: Simple Linear Regression Plot # Part 2: Polynomial Regression Plot # Part 3: Logistic Regression Plot # Part 4: Conditional Regression Plots # Part 5: Residual Plot ``` # Constraints - Ensure your plots are properly labeled for clarity. - Use appropriate color schemes and markers to make the distinctions clear. # Performance Requirements - The plots should be generated efficiently without unnecessary computation.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Part 1: Simple Linear Regression Plot def linear_regression_plot(): sns.regplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Simple Linear Regression: Total Bill vs Tip\\") plt.show() # Part 2: Polynomial Regression Plot def polynomial_regression_plot(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, order=2) plt.title(\\"Polynomial Regression (Order 2): Total Bill vs Tip\\") plt.show() # Part 3: Logistic Regression Plot def logistic_regression_plot(): tips[\'big_tip\'] = tips[\'tip\'] > (0.15 * tips[\'total_bill\']) sns.lmplot(x=\\"total_bill\\", y=\\"big_tip\\", data=tips, logistic=True, y_jitter=0.03) plt.title(\\"Logistic Regression: Total Bill vs Big Tip\\") plt.show() # Part 4: Conditional Regression Plots def conditional_regression_plots(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", data=tips) plt.title(\\"Regression Plot by Smoker Status\\") plt.show() sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"time\\", data=tips) plt.title(\\"Regression Plot by Smoker Status and Time of Day\\") plt.show() # Part 5: Residual Plot def residual_plot(): sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Residual Plot: Total Bill vs Tip\\") plt.show() # Call the functions to display the plots if __name__ == \\"__main__\\": linear_regression_plot() polynomial_regression_plot() logistic_regression_plot() conditional_regression_plots() residual_plot()"},{"question":"You are given a module, `log_processor.py`, that processes log files containing error messages from multiple servers. Each log file is in text format and may be compressed using gzip (`.gz`) or bzip2 (`.bz2`). Your task is to write a function `process_logs()` which reads these log files, extracts lines containing the keyword \\"ERROR\\", and writes these error lines into a new file called `error_summary.log`. **Function Signature:** ```python def process_logs(files: List[str], summary_file: str = \'error_summary.log\'): pass ``` # Input: - `files`: A list of file names (str) to be processed. These files can be regular text files or compressed using gzip or bzip2. - `summary_file`: An optional string with the name of the summary file where error lines will be written (default is \'error_summary.log\'). # Output: - The function writes the error lines to `summary_file`. # Constraints: - Ensure the program is efficient in handling large log files by reading lines lazily. - The summary file should only contain lines with the text \\"ERROR\\". - Handle both compressed (`.gz`, `.bz2`) and regular text files seamlessly. # Example: Suppose you have files `server1.log`, `server2.log.gz`, and `server3.log.bz2` with the following contents: ``` # server1.log INFO Starting process ERROR Something went wrong INFO Process completed # server2.log.gz WARNING Low disk space ERROR Failed to open file INFO Shutdown initiated # server3.log.bz2 INFO Backup started ERROR Connection lost INFO Backup completed ``` Your function should write to `error_summary.log`: ``` ERROR Something went wrong ERROR Failed to open file ERROR Connection lost ``` # Notes: - Utilize the `fileinput` module for reading and handling multiple files. - Make use of appropriate hooks to handle compressed files. - Ensure the function is robust and handles file-related errors gracefully. # Implementation Hints: - Consider using `fileinput.input()` with `hook_compressed` to handle compressed log files. - Use context management to ensure files are closed properly after processing. - Ensure that `error_summary.log` is opened in write mode and write only the required lines. Happy coding!","solution":"import fileinput import gzip import bz2 from typing import List def process_logs(files: List[str], summary_file: str = \'error_summary.log\'): with open(summary_file, \'w\') as outfile: with fileinput.input(files=files, openhook=fileinput.hook_compressed) as f: for line in f: if \'ERROR\' in line: outfile.write(line)"},{"question":"**Seaborn Advanced Histogram Visualization** You are given a dataset of penguin measurements and a dataset of planet observations. Your task is to create a detailed visualization using Seaborn that includes the following: 1. **Univariate Histograms**: - Plot a histogram for the `flipper_length_mm` variable from the `penguins` dataset. - Customize the histogram to use 30 bins and include a KDE. 2. **Bivariate Histogram**: - Use the `planets` dataset to create a bivariate histogram (heatmap) showing the relationship between the variables `year` and `distance`. - The y-axis (`distance`) should be on a log scale. - Overlay a color bar to annotate the colormap. 3. **Histogram with Hue and Normalization**: - Plot a univariate histogram of `bill_length_mm` from the `penguins` dataset using `hue` to differentiate the `island` variable. - Set the `stat` parameter to `density` and ensure that the density is not normalized across the subsets (`common_norm=False`). 4. **Histogram for Categorical Data**: - Plot a histogram for the `day` variable from the `tips` dataset. - Use `hue` to differentiate the `sex` variable and use the `multiple=\\"dodge\\"` argument. # Expected Outputs You should produce a single Python script that generates the described visualizations. Ensure that the visualizations are clear, well-labeled, and make effective use of Seaborn\'s customization options. # Constraints and Requirements - Import seaborn and set the theme to \\"white\\". - For data loading, use Seaborn’s `load_dataset` function. - For visualizations, use the `sns.histplot` function and explore different parameters and options as described. - Ensure the code is organized and well-commented. # Example ```python import seaborn as sns import matplotlib.pyplot as plt # Set the theme sns.set_theme(style=\\"white\\") # Load datasets penguins = sns.load_dataset(\\"penguins\\") planets = sns.load_dataset(\\"planets\\") tips = sns.load_dataset(\\"tips\\") # 1. Univariate Histogram with KDE sns.histplot(data=penguins, x=\\"flipper_length_mm\\", bins=30, kde=True) plt.title(\'Flipper Length Distribution with KDE\') plt.show() # 2. Bivariate Histogram (Heatmap) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), cbar=True, cbar_kws=dict(shrink=.75)) plt.title(\'Year vs Distance (Log Scale)\') plt.show() # 3. Histogram with Hue and Normalization sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"island\\", element=\\"step\\", stat=\\"density\\", common_norm=False) plt.title(\'Bill Length Distribution by Island\') plt.show() # 4. Histogram for Categorical Data with Hue sns.histplot(data=tips, x=\\"day\\", hue=\\"sex\\", multiple=\\"dodge\\", discrete=True, shrink=.8) plt.title(\'Day of Week vs Count by Sex\') plt.show() ``` Your implementation should produce plots similar to the example but with the detailed customizations as described.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_advanced_histograms(): # Set the theme sns.set_theme(style=\\"white\\") # Load datasets penguins = sns.load_dataset(\\"penguins\\") planets = sns.load_dataset(\\"planets\\") tips = sns.load_dataset(\\"tips\\") # 1. Univariate Histogram with KDE plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", bins=30, kde=True) plt.title(\'Flipper Length Distribution with KDE\') plt.show() # 2. Bivariate Histogram (Heatmap) plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), cbar=True, cbar_kws=dict(shrink=.75)) plt.title(\'Year vs Distance (Log Scale)\') plt.show() # 3. Histogram with Hue and Normalization plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"island\\", element=\\"step\\", stat=\\"density\\", common_norm=False) plt.title(\'Bill Length Distribution by Island\') plt.show() # 4. Histogram for Categorical Data with Hue plt.figure(figsize=(10, 6)) sns.histplot(data=tips, x=\\"day\\", hue=\\"sex\\", multiple=\\"dodge\\", discrete=True, shrink=.8) plt.title(\'Day of Week vs Count by Sex\') plt.show() # Call the function to generate the plots plot_advanced_histograms()"},{"question":"**Email Fetch and Analysis Using `poplib`** You are required to write a Python function that connects to a given POP3 email server, authenticates using provided credentials, retrieves all messages from the inbox, and performs basic analysis on the emails. # Function Signature ```python def fetch_and_analyze_emails(host: str, port: int, username: str, password: str, use_ssl: bool) -> dict: pass ``` # Input - `host` (str): The POP3 server hostname. - `port` (int): The port to connect to (110 for non-SSL, 995 for SSL). - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `use_ssl` (bool): If `True`, use SSL for the connection; otherwise, use a plain connection. # Output A dictionary with the following keys: - `\\"total_messages\\"` (int): The total number of messages in the inbox. - `\\"total_size\\"` (int): The total size of the inbox in octets (bytes). - `\\"message_details\\"` (list of tuples): A list where each tuple represents a message, consisting of: - The subject of the message (str). - The sender\'s email address (str). - The size of the message (int). # Constraints - Ensure proper error handling for network and authentication issues. - Retrieve the subject and sender\'s email address from the headers of each email. - Efficiently handle potentially large amounts of email data. # Example Usage ```python result = fetch_and_analyze_emails(\'pop.example.com\', 995, \'user@example.com\', \'securepassword\', True) print(result) ``` **Example Output:** ```python { \\"total_messages\\": 5, \\"total_size\\": 10235, \\"message_details\\": [ (\\"Subject 1\\", \\"sender1@example.com\\", 2045), (\\"Subject 2\\", \\"sender2@example.com\\", 1050), (\\"Subject 3\\", \\"sender3@example.com\\", 2150), (\\"Subject 4\\", \\"sender4@example.com\\", 5980), (\\"Subject 5\\", \\"sender5@example.com\\", 1010) ] } ``` # Notes 1. Use appropriate methods from the `poplib` module to implement the functionality. 2. Handle potential exceptions, such as network errors or authentication failures, gracefully. 3. Parse the email headers to extract the subject and sender information. 4. Ensure that the mailbox is properly closed using the `quit()` method after processing.","solution":"import poplib from email.parser import Parser from email.policy import default def fetch_and_analyze_emails(host: str, port: int, username: str, password: str, use_ssl: bool) -> dict: # Connect to the server try: if use_ssl: pop_conn = poplib.POP3_SSL(host, port) else: pop_conn = poplib.POP3(host, port) # Login to the server pop_conn.user(username) pop_conn.pass_(password) # Get message count and total size total_messages, total_size = pop_conn.stat() message_details = [] for i in range(1, total_messages + 1): response, lines, size = pop_conn.retr(i) msg_content = b\\"rn\\".join(lines).decode(\'utf-8\') msg = Parser(policy=default).parsestr(msg_content) subject = msg[\'subject\'] if msg[\'subject\'] else \'No Subject\' sender = msg[\'from\'] if msg[\'from\'] else \'Unknown Sender\' message_details.append((subject, sender, size)) # Disconnect from the server pop_conn.quit() return { \\"total_messages\\": total_messages, \\"total_size\\": total_size, \\"message_details\\": message_details } except poplib.error_proto as e: raise Exception(f\\"POP3 Protocol Error: {e}\\") except Exception as e: raise Exception(f\\"An error occurred: {e}\\")"},{"question":"You are tasked with implementing a function to summarize a given range of IP addresses into the minimal set of CIDR (Classless Inter-Domain Routing) blocks. This function will help reduce the number of routes needed to represent a series of IP addresses in a routing table, which is critical for efficient network management. # Function Signature ```python def summarize_ip_range(start: str, end: str) -> List[str]: Summarize the range of IP addresses from start to end into minimal CIDR blocks. :param start: A string representing the start of the range (inclusive). :param end: A string representing the end of the range (inclusive). :return: A list of strings representing the minimal set of CIDR blocks that cover the provided range. ``` # Input - `start` (str): The starting IP address of the range (in IPv4 format, e.g., \\"192.168.1.0\\"). - `end` (str): The ending IP address of the range (in IPv4 format, e.g., \\"192.168.1.255\\"). # Output - A list of strings where each string is a CIDR block (e.g., \\"192.168.1.0/24\\") that together cover the entire range from the `start` IP address to the `end` IP address. # Constraints - The provided IP addresses will be valid IPv4 addresses. - The `start` IP address will be less than or equal to the `end` IP address. # Example ```python print(summarize_ip_range(\\"192.168.1.0\\", \\"192.168.1.255\\")) # Output: [\\"192.168.1.0/24\\"] print(summarize_ip_range(\\"192.168.1.0\\", \\"192.168.1.15\\")) # Output: [\\"192.168.1.0/28\\"] print(summarize_ip_range(\\"192.168.1.8\\", \\"192.168.1.15\\")) # Output: [\\"192.168.1.8/29\\"] ``` # Notes - The function should handle edge cases, such as when the `start` and `end` IP addresses are the same. - Utilize the `ipaddress` module to facilitate IP address manipulation. - Ensure the implementation is efficient and can handle large IP ranges. # Implementation Approach To implement the function, follow these steps: 1. Convert the `start` and `end` IP addresses into their respective `IPv4Address` objects. 2. Use the `summarize_address_range` function from the `ipaddress` module to get the CIDR blocks that cover the provided range. 3. Convert the resulting network objects back to their string representations. 4. Return the list of CIDR blocks as output.","solution":"from ipaddress import ip_network, ip_address, summarize_address_range from typing import List def summarize_ip_range(start: str, end: str) -> List[str]: Summarize the range of IP addresses from start to end into minimal CIDR blocks. :param start: A string representing the start of the range (inclusive). :param end: A string representing the end of the range (inclusive). :return: A list of strings representing the minimal set of CIDR blocks that cover the provided range. start_ip = ip_address(start) end_ip = ip_address(end) cidr_list = summarize_address_range(start_ip, end_ip) return [str(cidr) for cidr in cidr_list]"},{"question":"**Question:** You are tasked with designing a type-safe library for managing a simple in-memory data store. Your goal is to implement several components making use of Python\'s typing module to ensure type safety. The task will be broken down into three distinct parts. 1. **Define Type Aliases and NewTypes:** - Create type aliases for `UserId` and `ProductId` using `NewType`. - Create a type alias `UserData` which is a dictionary mapping string keys to any values. 2. **Implement Type-Safe Classes:** - Implement a class `DataStore` that acts as a generic storage. This class should be a generic type that can store and retrieve data associated with a specific key. - Ensure that `DataStore` uses type hints to enforce the types of keys and values it will work with. 3. **Utilize Protocols and TypeGuards:** - Define a `JsonSerializable` protocol which includes a `to_json` method that returns a string. - Implement a type guard function `is_json_serializable` that checks if an object adheres to the `JsonSerializable` protocol. # Part 1: Type Aliases and NewTypes Define the type aliases using `NewType` and `TypedDict`. ```python from typing import NewType, TypedDict, Any UserId = NewType(\'UserId\', int) ProductId = NewType(\'ProductId\', int) class UserData(TypedDict): name: str email: str age: int ``` # Part 2: Implement the DataStore Class Implement a generic class `DataStore`. ```python from typing import Generic, TypeVar, Dict K = TypeVar(\'K\') V = TypeVar(\'V\') class DataStore(Generic[K, V]): def __init__(self): self._store: Dict[K, V] = {} def set_item(self, key: K, value: V) -> None: self._store[key] = value def get_item(self, key: K) -> V: return self._store[key] ``` # Part 3: Protocols and TypeGuards Define a protocol `JsonSerializable` and implement a type guard `is_json_serializable`. ```python from typing import Protocol, TypeGuard class JsonSerializable(Protocol): def to_json(self) -> str: ... def is_json_serializable(obj: Any) -> TypeGuard[JsonSerializable]: return isinstance(obj, JsonSerializable) ``` # Requirements: 1. Use the `DataStore` class to store user data (using `UserId`) and product data (using `ProductId`). 2. Ensure type annotations are correct and help catch any type-related issues during development. Complete the task by implementing additional functions or tests if necessary to demonstrate the correct usage and type-safety of your implementation. # Example Usage: ```python # Creating instances of DataStore for users and products user_store = DataStore[UserId, UserData]() product_store = DataStore[ProductId, dict]() # Adding and retrieving user data user_store.set_item(UserId(1), {\'name\': \'Alice\', \'email\': \'alice@example.com\', \'age\': 30}) user = user_store.get_item(UserId(1)) # Adding and retrieving product data product_store.set_item(ProductId(101), {\'name\': \'Laptop\', \'price\': 999.99}) product = product_store.get_item(ProductId(101)) # Implementing a JsonSerializable class class UserJson: def __init__(self, user_data: UserData): self.data = user_data def to_json(self) -> str: import json return json.dumps(self.data) # Using the type guard user_json_instance = UserJson(user) assert is_json_serializable(user_json_instance) # Should be True ```","solution":"from typing import NewType, TypedDict, Any, Generic, TypeVar, Dict, Protocol, TypeGuard # Part 1: Type Aliases and NewTypes UserId = NewType(\'UserId\', int) ProductId = NewType(\'ProductId\', int) class UserData(TypedDict): name: str email: str age: int # Part 2: Implement the DataStore Class K = TypeVar(\'K\') V = TypeVar(\'V\') class DataStore(Generic[K, V]): def __init__(self): self._store: Dict[K, V] = {} def set_item(self, key: K, value: V) -> None: self._store[key] = value def get_item(self, key: K) -> V: return self._store[key] # Part 3: Protocols and TypeGuards class JsonSerializable(Protocol): def to_json(self) -> str: ... def is_json_serializable(obj: Any) -> TypeGuard[JsonSerializable]: return hasattr(obj, \'to_json\') and callable(getattr(obj, \'to_json\'))"},{"question":"# Coding Assessment: XML DOM Manipulation in Python Objective: Demonstrate your understanding of the `xml.dom` module in Python by performing a series of tasks to parse, manipulate, and query an XML document. Problem Statement: You are given an XML string representing a simple catalog of books. Your task is to write a Python function that: 1. Parses the XML string into a DOM document. 2. Creates a new book entry and appends it to the catalog. 3. Searches for a book by its title and updates its price. 4. Prints all the book titles and their corresponding authors. 5. Handles exceptions properly. Here is the XML string you will be working with: ```xml catalog = <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> </catalog> ``` Task Requirements: 1. **Part 1: Parsing the XML String** - Write a function `parse_catalog(xml_string)` that takes the XML string as input and returns a DOM document object. 2. **Part 2: Adding a New Book** - Write a function `add_book(doc, book_info)` that takes the DOM document and a dictionary `book_info` containing details of the new book (id, author, title, genre, price, publish_date, description). The function should create a new `<book>` element with the provided details and append it to the catalog. 3. **Part 3: Updating Book Price** - Write a function `update_book_price(doc, title, new_price)` that takes the DOM document, a `title` of the book to be updated, and the `new_price`. The function should update the price of the book with the given title. 4. **Part 4: Printing Book Titles and Authors** - Write a function `print_books(doc)` that takes the DOM document and prints each book\'s title and author in the catalog. 5. **Exception Handling** - Ensure that appropriate exceptions are handled, such as non-existent elements. Constraints: - All book IDs are unique. - Titles of books are unique. - The price should be a positive floating-point number. Example Usage: ```python # Given XML string catalog_xml = <...> # use the catalog variable provided above # Parsing XML doc = parse_catalog(catalog_xml) # Adding a new book new_book = { \\"id\\": \\"bk103\\", \\"author\\": \\"Johnson, Steve\\", \\"title\\": \\"Learning XML\\", \\"genre\\": \\"Computer\\", \\"price\\": 39.95, \\"publish_date\\": \\"2021-03-01\\", \\"description\\": \\"A comprehensive guide to XML.\\" } add_book(doc, new_book) # Updating book price update_book_price(doc, \\"Midnight Rain\\", 9.95) # Printing book titles and authors print_books(doc) ``` Expected Output: ``` Title: XML Developer\'s Guide, Author: Gambardella, Matthew Title: Midnight Rain, Author: Ralls, Kim Title: Learning XML, Author: Johnson, Steve ``` Submission: Submit all functions in a single Python script with appropriate comments and structure. Ensure the script is runnable and demonstrates the function usage and output.","solution":"from xml.dom.minidom import parseString, Document def parse_catalog(xml_string): Parses the XML string into a DOM document object. Args: xml_string (str): The XML string to parse. Returns: Document: Parsed DOM document object. try: return parseString(xml_string) except Exception as e: print(f\\"Failed to parse XML string: {e}\\") return None def add_book(doc, book_info): Adds a new book entry to the catalog. Args: doc (Document): The DOM document object. book_info (dict): A dictionary containing book details with keys id, author, title, genre, price, publish_date, description. try: catalog = doc.getElementsByTagName(\\"catalog\\")[0] new_book = doc.createElement(\\"book\\") new_book.setAttribute(\\"id\\", book_info[\\"id\\"]) for key in [\\"author\\", \\"title\\", \\"genre\\", \\"price\\", \\"publish_date\\", \\"description\\"]: element = doc.createElement(key) text = doc.createTextNode(str(book_info[key])) element.appendChild(text) new_book.appendChild(element) catalog.appendChild(new_book) except Exception as e: print(f\\"Failed to add book: {e}\\") def update_book_price(doc, title, new_price): Updates the price of a book with the given title. Args: doc (Document): The DOM document object. title (str): The title of the book to be updated. new_price (float): The new price of the book. try: books = doc.getElementsByTagName(\\"book\\") for book in books: book_title = book.getElementsByTagName(\\"title\\")[0].firstChild.nodeValue if book_title == title: price_element = book.getElementsByTagName(\\"price\\")[0] price_element.firstChild.nodeValue = str(new_price) return raise ValueError(f\\"Book with title \'{title}\' not found\\") except Exception as e: print(f\\"Failed to update book price: {e}\\") def print_books(doc): Prints all book titles and their corresponding authors. Args: doc (Document): The DOM document object. try: books = doc.getElementsByTagName(\\"book\\") for book in books: title = book.getElementsByTagName(\\"title\\")[0].firstChild.nodeValue author = book.getElementsByTagName(\\"author\\")[0].firstChild.nodeValue print(f\\"Title: {title}, Author: {author}\\") except Exception as e: print(f\\"Failed to print books: {e}\\") # Example usage catalog_xml = <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> </catalog> doc = parse_catalog(catalog_xml) new_book = { \\"id\\": \\"bk103\\", \\"author\\": \\"Johnson, Steve\\", \\"title\\": \\"Learning XML\\", \\"genre\\": \\"Computer\\", \\"price\\": 39.95, \\"publish_date\\": \\"2021-03-01\\", \\"description\\": \\"A comprehensive guide to XML.\\" } add_book(doc, new_book) update_book_price(doc, \\"Midnight Rain\\", 9.95) print_books(doc)"},{"question":"# Question: Implement a Custom Diff Utility Using `difflib` Objective Create a Python function `custom_diff` that uses the `difflib` module to compare two sequences of strings and output the differences in a customized human-readable format. Function Signature ```python def custom_diff(fromlines: list, tolines: list) -> str: pass ``` Input - `fromlines`: A list of strings representing the original sequence. - `tolines`: A list of strings representing the modified sequence. Output - A formatted string that represents the differences between `fromlines` and `tolines`. The string should include lines that: - Start with `- ` for lines unique to `fromlines`. - Start with `+ ` for lines unique to `tolines`. - Start with ` ` for lines common to both sequences. - Start with `! ` for lines that are present in both sequences but have some differences in content. Constraints 1. Both input lists will contain strings with at most 200 characters. 2. The length of the input lists will not exceed 1000 lines. 3. Each input string is non-empty. Requirements 1. Utilize the `SequenceMatcher` and `ndiff` functionality from the `difflib` module. 2. Ensure that the output format is clear and human-readable, with distinct markers for the types of differences. 3. Implement the function efficiently to handle large inputs. Example ```python fromlines = [ \\"This is a test.\\", \\"Another line.\\", \\"One more line.\\" ] tolines = [ \\"This is a test.\\", \\"Another different line.\\", \\"One more line here.\\" ] print(custom_diff(fromlines, tolines)) ``` Expected Output ```text This is a test. - Another line. + Another different line. ! One more line. ! One more line here. ``` Detailed Explanation: - Lines starting with ` ` are common in both `fromlines` and `tolines`. - Lines starting with `- ` are unique to `fromlines` and not present in `tolines`. - Lines starting with `+ ` are unique to `tolines` and not present in `fromlines`. - Lines starting with `! ` indicate intraline differences (near-matches). Hint - Use the `difflib.SequenceMatcher` to identify and highlight the differences between the sequences. - Use `difflib.ndiff` for generating the delta for more granular control over intraline differences.","solution":"import difflib def custom_diff(fromlines: list, tolines: list) -> str: Compare two sequences of strings and output the differences in a customized human-readable format. Args: fromlines (list): A list of strings representing the original sequence. tolines (list): A list of strings representing the modified sequence. Returns: str: A formatted string that represents the differences between fromlines and tolines. diff = difflib.ndiff(fromlines, tolines) result = [] for line in diff: if line.startswith(\'-\'): result.append(f\'- {line[2:]}\') elif line.startswith(\'+\'): result.append(f\'+ {line[2:]}\') elif line.startswith(\' \'): result.append(f\' {line[2:]}\') elif line.startswith(\'?\'): # ? lines indicate the locations of intraline differences. We skip them. continue # Join all the result lines into a single string with newlines return \'n\'.join(result)"},{"question":"**Problem: Web Crawler Compliance Checker** You are tasked with developing a small component of a web crawler that needs to be compliant with the instructions given in a website\'s `robots.txt` file. You will utilize the `urllib.robotparser` module to achieve this. **Objective:** Implement a class `WebCrawlerComplianceChecker` that uses `urllib.robotparser.RobotFileParser` to determine if a web crawler, identified by its user agent, can fetch specific URLs from a website based on the website\'s `robots.txt` file. **Specifications:** 1. The class should have the following methods: - `__init__(self, robots_txt_url: str)`: Initializes the object by setting the robots.txt URL and reading its contents. - `can_crawl(self, user_agent: str, url: str) -> bool`: Takes a user agent string and a URL, and returns `True` if the user agent is allowed to fetch the URL, otherwise returns `False`. - `get_crawl_delay(self, user_agent: str) -> int`: Takes a user agent string and returns the crawl-delay value in seconds. If no crawl-delay is found, it should return `0`. - `get_request_rate(self, user_agent: str) -> tuple`: Takes a user agent string and returns a tuple containing the number of requests and time period in seconds (requests, seconds). If no request rate is found, it should return `(None, None)`. - `get_site_maps(self) -> list`: Returns a list of sitemap URLs found in the `robots.txt` file. If no site maps are found, it should return an empty list. **Constraints:** - Do not use any external libraries except the `urllib` package provided and the Python Standard Library. - You can assume that the provided `robots.txt` URL is always correct and accessible. **Example Usage:** ```python checker = WebCrawlerComplianceChecker(\'http://www.example.com/robots.txt\') print(checker.can_crawl(\'*\', \'http://www.example.com/\')) # True or False based on the robots.txt rules print(checker.get_crawl_delay(\'*\')) # Crawl delay time in seconds print(checker.get_request_rate(\'*\')) # (requests, seconds) tuple or (None, None) print(checker.get_site_maps()) # List of sitemap URLs ``` **Notes:** - Make sure to handle all edge cases such as missing parameters in `robots.txt`. - Your implementation should correctly utilize the methods provided by the `urllib.robotparser.RobotFileParser` class.","solution":"import urllib.robotparser class WebCrawlerComplianceChecker: def __init__(self, robots_txt_url: str): self.robot_parser = urllib.robotparser.RobotFileParser() self.robot_parser.set_url(robots_txt_url) self.robot_parser.read() def can_crawl(self, user_agent: str, url: str) -> bool: return self.robot_parser.can_fetch(user_agent, url) def get_crawl_delay(self, user_agent: str) -> int: delay = self.robot_parser.crawl_delay(user_agent) return delay if delay is not None else 0 def get_request_rate(self, user_agent: str) -> tuple: request_rate = self.robot_parser.request_rate(user_agent) if request_rate: return (request_rate.requests, request_rate.seconds) return (None, None) def get_site_maps(self) -> list: return self.robot_parser.site_maps() if self.robot_parser.site_maps() is not None else []"},{"question":"# Question: Handling and Augmenting MIME Types in Python You are working on a web server application where it\'s crucial to handle different file types based on their extensions and MIME types. Your task is to implement a function that initializes a MIME type database, adds custom mappings, and retrieves MIME types and extensions using both the `mimetypes` module and the `MimeTypes` class. Function Definitions 1. **initialize_and_add_custom_types(files, custom_types):** - **Inputs:** - `files` (list of str): A list of file paths containing MIME type mappings. - `custom_types` (list of tuples): A list of tuples where each tuple contains a MIME type and its corresponding extension (e.g., `[(\'application/pdf\', \'.pdf\'), (\'image/jpeg\', \'.jpg\')]`). - **Outputs:** - `mime_instance` (MimeTypes object): An instance of the `MimeTypes` class with the custom types added. 2. **get_mime_type(mime_instance, filename):** - **Inputs:** - `mime_instance` (MimeTypes object): An instance of the `MimeTypes` class. - `filename` (str): The name or URL of the file whose MIME type needs to be determined. - **Outputs:** - `mime_type` (str or None): The MIME type of the file. If the MIME type cannot be determined, return `None`. 3. **get_file_extension(mime_instance, mime_type):** - **Inputs:** - `mime_instance` (MimeTypes object): An instance of the `MimeTypes` class. - `mime_type` (str): The MIME type for which to retrieve the file extension. - **Outputs:** - `extension` (str or None): A single file extension associated with the MIME type. If no extension can be determined, return `None`. Constraints: - You may assume that the `files` provided in `initialize_and_add_custom_types` exist and are correctly formatted. - The custom MIME type mappings in `custom_types` are unique and do not overlap with built-in MIME types. - Your solutions should handle both standard and non-standard MIME types. Example: ```python files = [\'/etc/mime.types\'] custom_types = [(\'application/x-custom-type\', \'.custom\')] mime_instance = initialize_and_add_custom_types(files, custom_types) filename = \'example.custom\' print(get_mime_type(mime_instance, filename)) # Output: \'application/x-custom-type\' mime_type = \'application/x-custom-type\' print(get_file_extension(mime_instance, mime_type)) # Output: \'.custom\' ``` Implement the functions `initialize_and_add_custom_types`, `get_mime_type`, and `get_file_extension`.","solution":"import mimetypes def initialize_and_add_custom_types(files, custom_types): Initializes a MIME types instance and adds custom MIME type mappings. Parameters: files (list of str): A list of file paths containing MIME type mappings. custom_types (list of tuples): A list of tuples with MIME type and extension pairs. Returns: MimeTypes object: An instance of the MimeTypes class with the custom types added. mime_instance = mimetypes.MimeTypes() for file_path in files: mime_instance.read(file_path) for mime_type, extension in custom_types: mime_instance.add_type(mime_type, extension) return mime_instance def get_mime_type(mime_instance, filename): Retrieves the MIME type of a given file using the provided MIME types instance. Parameters: mime_instance (MimeTypes object): An instance of the MimeTypes class. filename (str): The name of the file. Returns: str or None: The MIME type of the file, or None if it cannot be determined. mime_type, _ = mime_instance.guess_type(filename) return mime_type def get_file_extension(mime_instance, mime_type): Retrieves the file extension for a given MIME type using the provided MIME types instance. Parameters: mime_instance (MimeTypes object): An instance of the MimeTypes class. mime_type (str): The MIME type. Returns: str or None: The file extension corresponding to the MIME type, or None if it cannot be determined. extensions = mime_instance.guess_all_extensions(mime_type) return extensions[0] if extensions else None"},{"question":"# Python Coding Assessment: PyLongObject Operations You are given access to a Python C extension module that provides various functionalities to create and convert Python\'s long integers (`PyLongObject`). Your task is to utilize these functionalities to implement a Python class that interacts with Python integers using this module. # Class Specification Implement a Python class `PyLongWrapper` that wraps around the provided C extension functions. This class should provide the following methods: 1. **Constructor**: - `__init__(self, value)`: Initializes the `PyLongObject` from an integer or a string representation of an integer. 2. **Conversions**: - `to_long(self) -> int`: Converts the internal `PyLongObject` to a Python integer (`long` in C). - `to_unsigned_long(self) -> int`: Converts the internal `PyLongObject` to a Python integer as an unsigned long (`unsigned long` in C). - `to_double(self) -> float`: Converts the internal `PyLongObject` to a Python float (`double` in C). 3. **String Representation**: - `__str__(self) -> str`: Returns the string representation of the internal `PyLongObject`. 4. **Arithmetic Operations**: - `__add__(self, other)`: Adds two `PyLongObject`s and returns a new `PyLongWrapper`. - `__sub__(self, other)`: Subtracts `other` from `self` and returns a new `PyLongWrapper`. # Constraints - You must handle error checking and overflow as specified in the provided documentation. - You may assume the input to the constructor is always a valid integer or string representing an integer in a supported base. # Example Usage ```python p1 = PyLongWrapper(12345) p2 = PyLongWrapper(\'67890\') print(p1.to_long()) # Outputs: 12345 print(p2.to_unsigned_long()) # Outputs: 67890 p3 = p1 + p2 print(p3.to_long()) # Outputs: 80235 p4 = p2 - p1 print(p4.to_long()) # Outputs: 55545 print(p1.to_double()) # Outputs: 12345.0 print(str(p2)) # Outputs: \'67890\' ``` Ensure your class adheres to the specified functionality and error handling as described.","solution":"class PyLongWrapper: def __init__(self, value): if isinstance(value, str): self.value = int(value) elif isinstance(value, int): self.value = value else: raise ValueError(\\"Value must be an integer or a string representation of an integer.\\") def to_long(self) -> int: return self.value def to_unsigned_long(self) -> int: if self.value < 0: raise OverflowError(\\"Value must be non-negative to be converted to unsigned long.\\") return self.value def to_double(self) -> float: return float(self.value) def __str__(self) -> str: return str(self.value) def __add__(self, other): if not isinstance(other, PyLongWrapper): raise TypeError(\\"Operands must be of type PyLongWrapper.\\") return PyLongWrapper(self.value + other.value) def __sub__(self, other): if not isinstance(other, PyLongWrapper): raise TypeError(\\"Operands must be of type PyLongWrapper.\\") return PyLongWrapper(self.value - other.value)"},{"question":"# **Advanced Seaborn ECDF Plotting** **Objective:** Write a function `advanced_ecdf_plot` that generates various ECDF plots based on specific configurations and saves them as files. Your function must demonstrate the use of different seaborn `ecdfplot` features such as plotting along the x and y axes, using hue mappings, displaying different statistics, and plotting the complementary CDF. **Function Signature:** ```python def advanced_ecdf_plot(config: dict) -> None: pass ``` # **Input:** - `config`: A dictionary containing configurations for the ECDF plots. The structure of the dictionary is as follows: ```python { \\"dataset_name\\": str, # The name of the seaborn dataset to load (e.g., \\"penguins\\"). \\"output_filenames\\": list, # A list of output filenames to save the generated plots. \\"plot_configs\\": list # A list of dictionaries containing individual plot configurations. } ``` Each plot configuration dictionary within `plot_configs` is structured as follows: ```python { \\"x\\": str or None, # Column name for x-axis, or None if not used. \\"y\\": str or None, # Column name for y-axis, or None if not used. \\"hue\\": str or None, # Column name for hue mapping, or None if not used. \\"stat\\": str, # Statistic to display (\\"proportion\\", \\"count\\", or \\"percent\\"). \\"complementary\\": bool, # Whether to plot the complementary CDF (True or False). \\"filename_index\\": int # The index in the `output_filenames` list where the plot will be saved. } ``` # **Output:** - None. The function will save the generated ECDF plots as image files according to the specified configurations. # **Constraints:** - The length of `output_filenames` must be equal to or greater than the length of `plot_configs`. - Only use seaborn functions and methods for visualization. - Raise appropriate errors if the dataset is not available or required configurations are missing. # **Example:** ```python config = { \\"dataset_name\\": \\"penguins\\", \\"output_filenames\\": [\\"plot1.png\\", \\"plot2.png\\", \\"plot3.png\\"], \\"plot_configs\\": [ { \\"x\\": \\"flipper_length_mm\\", \\"y\\": None, \\"hue\\": None, \\"stat\\": \\"proportion\\", \\"complementary\\": False, \\"filename_index\\": 0 }, { \\"x\\": \\"bill_length_mm\\", \\"y\\": None, \\"hue\\": \\"species\\", \\"stat\\": \\"count\\", \\"complementary\\": False, \\"filename_index\\": 1 }, { \\"x\\": \\"bill_length_mm\\", \\"y\\": None, \\"hue\\": \\"species\\", \\"stat\\": \\"proportion\\", \\"complementary\\": True, \\"filename_index\\": 2 } ] } advanced_ecdf_plot(config) ``` Upon execution, three images (`plot1.png`, `plot2.png`, and `plot3.png`) will be saved with the specified ECDF plots. **Notes:** - Use `sns.load_dataset` to load the dataset. - Use `sns.ecdfplot` to create the ECDF plots. - Ensure to handle any errors or exceptions gracefully.","solution":"import seaborn as sns import matplotlib.pyplot as plt def advanced_ecdf_plot(config: dict) -> None: # Load the dataset dataset_name = config.get(\\"dataset_name\\") if not dataset_name: raise ValueError(\\"Dataset name is required in the config.\\") try: data = sns.load_dataset(dataset_name) except Exception as e: raise ValueError(f\\"Failed to load dataset: {e}\\") output_filenames = config.get(\\"output_filenames\\", []) plot_configs = config.get(\\"plot_configs\\", []) if len(output_filenames) < len(plot_configs): raise ValueError(\\"The number of output filenames must be equal to or greater than the number of plot configurations.\\") for plot_config in plot_configs: x = plot_config.get(\\"x\\") y = plot_config.get(\\"y\\") hue = plot_config.get(\\"hue\\") stat = plot_config.get(\\"stat\\", \\"proportion\\") complementary = plot_config.get(\\"complementary\\", False) filename_index = plot_config.get(\\"filename_index\\") if filename_index >= len(output_filenames): raise ValueError(f\\"Filename index {filename_index} is out of the range of the output_filenames list.\\") plt.figure() sns.ecdfplot( data=data, x=x, y=y, hue=hue, stat=stat, complementary=complementary ) plt.savefig(output_filenames[filename_index]) plt.close()"},{"question":"You are tasked with creating a Python extension module in C, following the principles described in the provided documentation. The goal is to define a new custom data type called `Person` that will be part of an extension module named `person`. The `Person` type should include: 1. Data attributes: `first_name` (string), `last_name` (string), and `age` (integer). 2. Methods: - `full_name()`: Returns the full name of the person, combining the first and last names. 3. Advanced features: - Custom getters and setters to ensure `first_name` and `last_name` are always valid strings and cannot be `NULL`. - Support for cyclic garbage collection to handle potential reference cycles within `Person` instances. Your task is to: 1. Write the C code to define this extension module. 2. Ensure the module supports proper memory management, including reference counting and destructors. 3. Provide a Python `setup.py` script to compile and build your extension. # Requirements: - The module should be named `person`. - The `Person` type should be subclassable. - Use custom getters and setters to manage access to the `first_name` and `last_name` attributes. - Include cyclic garbage collection support. # Example Usage: ```python import person p = person.Person(first_name=\\"John\\", last_name=\\"Doe\\", age=30) print(p.full_name()) # Output: John Doe p.first_name = \\"Jane\\" print(p.full_name()) # Output: Jane Doe ``` # Constraints: - Each `first_name` and `last_name` must always be a valid non-null string. - `age` should be an integer and default to 0 if not provided. - Implement proper memory management to avoid leaks. - Ensure cyclic references are handled correctly. # Performance: - The implemented solution should handle typical use cases efficiently without unnecessary overhead. Provide the full C implementation and `setup.py` script needed to compile and test your module. # Submission: - `person.c` - Contains the C code defining the `Person` type and the `person` module. - `setup.py` - Python script to build the extension module. Good luck!","solution":"# Here is the reimagined Python definition that would be reflected in the C code. # This Python code is a high-level representation of what the C code would accomplish. class Person: def __init__(self, first_name, last_name, age=0): if not isinstance(first_name, str) or not isinstance(last_name, str): raise ValueError(\\"first_name and last_name must be non-null strings\\") self._first_name = first_name self._last_name = last_name self.age = age @property def first_name(self): return self._first_name @first_name.setter def first_name(self, value): if not isinstance(value, str): raise ValueError(\\"first_name must be a non-null string\\") self._first_name = value @property def last_name(self): return self._last_name @last_name.setter def last_name(self, value): if not isinstance(value, str): raise ValueError(\\"last_name must be a non-null string\\") self._last_name = value def full_name(self): return f\\"{self.first_name} {self.last_name}\\""},{"question":"You are given the Titanic dataset, which can be loaded using `sns.load_dataset(\\"titanic\\")`. Your task is to create a series of visualizations using seaborn that provide different insights into the data. You should follow the steps below and ensure that your visualizations are clear, properly labeled, and informative. # Step-by-Step Instructions: 1. **Basic Count Plot:** - Create a count plot showing the number of passengers in each class (`class`). 2. **Grouped Count Plot:** - Modify the previous count plot to show the survival count (`survived`) within each class by grouping it with a second variable. 3. **Normalized Count Plot:** - Create the same grouped count plot but normalize the counts to show percentages. 4. **Advanced Visualization:** - Create a violin plot to visualize the distribution of passengers\' age (`age`) across different classes (`class`) and hue by their survival status (`survived`). # Function Implementation: - **Function Name**: `generate_titanic_visualizations` - **Input**: None (you will load the dataset within the function) - **Output**: Your function should not return anything. Instead, it should display the plots directly using seaborn and matplotlib. # Constraints: - Use seaborn for all visualizations. - Each plot should have appropriate titles, labels, and legends to make them easily understandable. - Ensure that the violin plot showcases the distribution effectively, including possible overlaps and density differences. # Example: ```python def generate_titanic_visualizations(): import seaborn as sns import matplotlib.pyplot as plt # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # 1. Basic Count Plot plt.figure(figsize=(10, 6)) sns.countplot(x=\\"class\\", data=titanic) plt.title(\\"Number of Passengers in Each Class\\") plt.show() # 2. Grouped Count Plot plt.figure(figsize=(10, 6)) sns.countplot(x=\\"class\\", hue=\\"survived\\", data=titanic) plt.title(\\"Survival Count by Class\\") plt.show() # 3. Normalized Count Plot (Grouped by Hue) plt.figure(figsize=(10, 6)) sns.histplot(titanic, x=\\"class\\", hue=\\"survived\\", multiple=\\"fill\\", stat=\\"percent\\") plt.title(\\"Survival Percentage by Class\\") plt.show() # 4. Advanced Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(x=\\"class\\", y=\\"age\\", hue=\\"survived\\", data=titanic, split=True) plt.title(\\"Age Distribution by Class and Survival Status\\") plt.show() # Execute the function to generate visualizations generate_titanic_visualizations() ``` Make sure to implement the function `generate_titanic_visualizations` correctly and validate that all the visualizations meet the requirements outlined above.","solution":"def generate_titanic_visualizations(): import seaborn as sns import matplotlib.pyplot as plt # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # 1. Basic Count Plot plt.figure(figsize=(10, 6)) sns.countplot(x=\\"class\\", data=titanic) plt.title(\\"Number of Passengers in Each Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Count\\") plt.show() # 2. Grouped Count Plot plt.figure(figsize=(10, 6)) sns.countplot(x=\\"class\\", hue=\\"survived\\", data=titanic) plt.title(\\"Survival Count by Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Count\\") plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() # 3. Normalized Count Plot (Grouped by Hue) plt.figure(figsize=(10, 6)) sns.histplot(titanic, x=\\"class\\", hue=\\"survived\\", multiple=\\"fill\\", stat=\\"percent\\") plt.title(\\"Survival Percentage by Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Percentage\\") plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() # 4. Advanced Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(x=\\"class\\", y=\\"age\\", hue=\\"survived\\", data=titanic, split=True) plt.title(\\"Age Distribution by Class and Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() generate_titanic_visualizations()"},{"question":"You are provided with the following snippet of text from a document: ``` Hello, my name is John Doe. I live at 1234 Elm Street, Springfield, IL 62704. You can contact me at (321) 555-6789 or via email at john.doe@example.com. Alternatively, reach out to my office at 3456 Oak Avenue, Springfield, IL 62701. Office phone: (987) 654-3210 and email: office@example.org. ``` Write a Python function `extract_contact_info` that extracts and returns the following information as a dictionary: - All phone numbers in the format `(XXX) XXX-XXXX`. - All email addresses. - All addresses, which start with a number and continue until a comma followed by a city name. # Input Format - `text` (string): A string containing contact information. # Output Format - A dictionary with the following structure: ```python { \\"phone_numbers\\": [list of phone numbers], \\"emails\\": [list of email addresses], \\"addresses\\": [list of addresses] } ``` # Constraints 1. The function should use Python\'s `re` module to accomplish this task. 2. The regular expressions should accurately capture the specified formats. 3. Assume that the input text is well-formed and contains no errors. # Sample Input ```python text = Hello, my name is John Doe. I live at 1234 Elm Street, Springfield, IL 62704. You can contact me at (321) 555-6789 or via email at john.doe@example.com. Alternatively, reach out to my office at 3456 Oak Avenue, Springfield, IL 62701. Office phone: (987) 654-3210 and email: office@example.org. ``` # Sample Output ```python { \'phone_numbers\': [\'(321) 555-6789\', \'(987) 654-3210\'], \'emails\': [\'john.doe@example.com\', \'office@example.org\'], \'addresses\': [\'1234 Elm Street, Springfield, IL 62704\', \'3456 Oak Avenue, Springfield, IL 62701\'] } ``` # Implementation ```python def extract_contact_info(text): import re phone_pattern = r\'(d{3}) d{3}-d{4}\' email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\' address_pattern = r\'d+s+[^,]+,s+[^,]+,s+[A-Z]{2}s+d{5}\' phones = re.findall(phone_pattern, text) emails = re.findall(email_pattern, text) addresses = re.findall(address_pattern, text) return { \'phone_numbers\': phones, \'emails\': emails, \'addresses\': addresses } # Sample usage text = Hello, my name is John Doe. I live at 1234 Elm Street, Springfield, IL 62704. You can contact me at (321) 555-6789 or via email at john.doe@example.com. Alternatively, reach out to my office at 3456 Oak Avenue, Springfield, IL 62701. Office phone: (987) 654-3210 and email: office@example.org. print(extract_contact_info(text)) ```","solution":"def extract_contact_info(text): import re phone_pattern = r\'(d{3}) d{3}-d{4}\' email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\' address_pattern = r\'bd+s+[^,]+,s+[^,]+,s+[A-Z]{2}s+d{5}b\' phones = re.findall(phone_pattern, text) emails = re.findall(email_pattern, text) addresses = re.findall(address_pattern, text) return { \'phone_numbers\': phones, \'emails\': emails, \'addresses\': addresses }"},{"question":"# Advanced Data Manipulation with Pandas **Objective:** To assess the understanding of data manipulation and transformation using pandas. **Scenario:** You are given a dataset that represents sales transactions for a retail company. Each transaction contains information about the date, product, store, quantity sold, and revenue generated. Your task is to perform various data manipulation and analysis operations on this dataset. **Dataset:** ```python import pandas as pd data = { \\"date\\": pd.date_range(\\"2023-01-01\\", periods=20, freq=\'D\'), \\"product\\": [\\"apple\\", \\"banana\\", \\"apple\\", \\"banana\\", \\"apple\\", \\"apple\\", \\"banana\\", \\"banana\\", \\"apple\\", \\"banana\\", \\"apple\\", \\"banana\\", \\"apple\\", \\"banana\\", \\"apple\\", \\"banana\\", \\"apple\\", \\"banana\\", \\"apple\\", \\"banana\\"], \\"store\\": [\\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"A\\", \\"B\\", \\"A\\", \\"B\\", \\"A\\", \\"A\\", \\"B\\", \\"A\\", \\"A\\", \\"B\\", \\"A\\", \\"B\\", \\"A\\", \\"B\\", \\"A\\", \\"B\\"], \\"quantity\\": [15, 23, 7, 10, 16, 13, 9, 8, 20, 19, 11, 22, 14, 21, 18, 12, 17, 15, 10, 12], \\"revenue\\": [45, 69, 21, 30, 48, 39, 27, 24, 60, 57, 33, 66, 42, 63, 54, 36, 51, 45, 30, 36] } df = pd.DataFrame(data) ``` **Tasks:** 1. **Basic Analysis** - Display the first 5 rows of the DataFrame. - Display the data types of each column. - Provide summary statistics for the numerical columns. 2. **Data Selection and Filtering** - Select all rows where the quantity sold is greater than 15. - Select the `product` and `quantity` columns for transactions where the store is \\"A\\". 3. **Aggregation and Grouping** - Group the data by `product` and calculate the total quantity sold and total revenue generated for each product. - Group the data by `store` and `product`, then calculate the average revenue generated for each group. 4. **Time Series Analysis** - Set the `date` column as the index of the DataFrame. - Resample the dataset to get monthly total quantity and revenue for each product. **Implementation:** Write a function `analyze_sales_data(df: pd.DataFrame) -> dict` that takes the DataFrame `df` as input and returns a dictionary with the results of each task. The dictionary keys should be the task names, and the values should be the corresponding results. **Sample Output:** ```python result = { \\"first_5_rows\\": <DataFrame>, \\"data_types\\": <Series>, \\"summary_statistics\\": <DataFrame>, \\"quantity_gt_15\\": <DataFrame>, \\"product_quantity_store_A\\": <DataFrame>, \\"total_quantity_revenue_product\\": <DataFrame>, \\"avg_revenue_store_product\\": <DataFrame>, \\"monthly_totals\\": <DataFrame> } ``` **Constraints:** - Use pandas functions to perform data selection, grouping, and resampling. - Ensure that the final function is efficient and uses appropriate pandas methods.","solution":"import pandas as pd def analyze_sales_data(df: pd.DataFrame) -> dict: # Task 1: Basic Analysis first_5_rows = df.head() data_types = df.dtypes summary_statistics = df.describe() # Task 2: Data Selection and Filtering quantity_gt_15 = df[df[\'quantity\'] > 15] product_quantity_store_A = df[df[\'store\'] == \'A\'][[\'product\', \'quantity\']] # Task 3: Aggregation and Grouping total_quantity_revenue_product = df.groupby(\'product\').agg({\'quantity\': \'sum\', \'revenue\': \'sum\'}).reset_index() avg_revenue_store_product = df.groupby([\'store\', \'product\']).agg({\'revenue\': \'mean\'}).reset_index() # Task 4: Time Series Analysis df = df.set_index(\'date\') monthly_totals = df.resample(\'M\').sum().reset_index() # Compile results into a dictionary results = { \\"first_5_rows\\": first_5_rows, \\"data_types\\": data_types, \\"summary_statistics\\": summary_statistics, \\"quantity_gt_15\\": quantity_gt_15, \\"product_quantity_store_A\\": product_quantity_store_A, \\"total_quantity_revenue_product\\": total_quantity_revenue_product, \\"avg_revenue_store_product\\": avg_revenue_store_product, \\"monthly_totals\\": monthly_totals } return results"},{"question":"Objective In this assessment, you are required to demonstrate your understanding of the `runpy` module by writing a Python function that utilizes both `runpy.run_module` and `runpy.run_path` to execute dynamic scripts and modules and extract specific information from the executed environments. Problem Statement You need to implement a function `execute_scripts_and_modules` which accepts two arguments: 1. `module_name`: A string representing the name of a module to be executed. 2. `file_path`: A string representing the filesystem path to a Python script or module to be executed. The function should: 1. Execute the code of the provided `module_name` using `runpy.run_module`. 2. Execute the code at the provided `file_path` using `runpy.run_path`. Both executions should be done in such a way that any global variable `__author__` defined or set in the executed code is captured. The function should return a dictionary with the following keys: - `\\"module_author\\"`: The value of the `__author__` variable from the executed module (or `None` if not defined). - `\\"file_author\\"`: The value of the `__author__` variable from the executed file (or `None` if not defined). Constraints - You can assume `module_name` and `file_path` are valid and available in the environment where the function will be executed. - If `module_name` refers to a package, the function should locate and execute the `__main__` submodule in that package. Example Here is an example of how the function should behave: Suppose there is a module named `example_module` and a script file located at `example_script.py`. The module `example_module` contains the following code: ```python __author__ = \\"John Doe\\" ``` And the script file `example_script.py` contains the following code: ```python __author__ = \\"Jane Smith\\" ``` ```python result = execute_scripts_and_modules(\\"example_module\\", \\"example_script.py\\") print(result) ``` Output: ```python { \\"module_author\\": \\"John Doe\\", \\"file_author\\": \\"Jane Smith\\" } ``` Function Signature ```python import runpy def execute_scripts_and_modules(module_name: str, file_path: str) -> dict: # Your code here ```","solution":"import runpy def execute_scripts_and_modules(module_name: str, file_path: str) -> dict: Executes the provided module and file, capturing the __author__ variables. Args: - module_name (str): The name of the module to be executed. - file_path (str): The filesystem path to the Python script/module to be executed. Returns: - dict: A dictionary containing the values of __author__ from the module and file. # Execute the module and capture its environment module_env = runpy.run_module(module_name) module_author = module_env.get(\'__author__\') # Execute the file and capture its environment file_env = runpy.run_path(file_path) file_author = file_env.get(\'__author__\') return { \\"module_author\\": module_author, \\"file_author\\": file_author }"},{"question":"# Unicode Normalization and Encoding Conversion Background: In many applications, handling text data stored in different encodings and normalizing it to a standard form is critical. For example, text may come from different sources like files, databases, or user inputs that use various encodings. To properly process and store this text, it often needs to be decoded to Unicode, normalized, and then possibly re-encoded to a different encoding. Task: Write a Python function named `process_unicode_text` that: 1. Takes the path to an input file, reads its contents, and takes the file\'s encoding as an optional parameter (default is \'utf-8\'). 2. Normalizes the text using the NFC (Normalization Form C) standard. 3. Writes the normalized text to an output file with the specified encoding (default is \'utf-8\'). Function Signature: ```python def process_unicode_text(input_file: str, output_file: str, input_encoding: str = \'utf-8\', output_encoding: str = \'utf-8\') -> None: pass ``` Input: - `input_file`: The path to the input file containing the text to process. - `output_file`: The path to the output file where the normalized text will be saved. - `input_encoding`: (Optional) The encoding of the input file. Default is \'utf-8\'. - `output_encoding`: (Optional) The encoding for the output file. Default is \'utf-8\'. Output: - The function should not return anything. It should write the normalized text to the specified output file. Example Usage: Assume `input.txt` contains text encoded in Latin-1. ```python process_unicode_text(\'input.txt\', \'output.txt\', input_encoding=\'latin-1\', output_encoding=\'utf-8\') ``` This would read the text from `input.txt`, normalize it using NFC, and then write it to `output.txt` in UTF-8 encoding. Requirements: 1. Read the contents of the input file with the specified encoding. 2. Normalize the text using NFC. 3. Write the normalized text to the output file in the specified encoding. 4. Handle exceptions appropriately, such as file not found or encoding errors. Constraints: - The input file can be very large, so ensure your solution handles large files efficiently without loading the entire file into memory at once. - Assume the file paths and encodings provided are valid strings. Additional Information: Refer to the `unicodedata` module for normalization and the built-in functions `open()`, `read()`, and `write()` for file I/O operations.","solution":"import unicodedata import codecs def process_unicode_text(input_file: str, output_file: str, input_encoding: str = \'utf-8\', output_encoding: str = \'utf-8\') -> None: Reads the content of \'input_file\' with \'input_encoding\', normalizes it to NFC, and writes it to \'output_file\' with \'output_encoding\'. :param input_file: Path to the input file. :param output_file: Path to the output file. :param input_encoding: Encoding of the input file. Defaults to \'utf-8\'. :param output_encoding: Encoding for the output file. Defaults to \'utf-8\'. try: # Open the input file with the specified encoding and read the contents with codecs.open(input_file, \'r\', encoding=input_encoding) as infile: text = infile.read() # Normalize the text to NFC normalized_text = unicodedata.normalize(\'NFC\', text) # Open the output file with the specified encoding and write the normalized content with codecs.open(output_file, \'w\', encoding=output_encoding) as outfile: outfile.write(normalized_text) except FileNotFoundError: print(f\\"Error: File not found: {input_file}\\") except UnicodeError as e: print(f\\"Encoding/Decoding Error: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"Coding Assessment Question # Objective: Develop a function that demonstrates your understanding of Python\'s codec registry and support functions. # Problem Statement: You are tasked with implementing functions to encode and decode text using a given encoding while handling potential encoding errors. Specifically, you\'ll need to: 1. Encode the given text using the specified encoding. 2. Decode the encoded data back to its original form using the same encoding if feasible. 3. Handle any potential errors during encoding/decoding using user-defined or available error handling strategies. # Function Signature: ```python def encode_and_decode(text: str, encoding: str, error_strategy: str = \\"strict\\") -> str: Encode and decode the given text using the specified encoding and handle errors as per the strategy. Parameters: - text (str): The text to be encoded and then decoded. - encoding (str): The encoding to use (e.g., \'utf-8\', \'ascii\'). - error_strategy (str): The error handling strategy to use. Possible values are \'strict\', \'ignore\', \'replace\', \'xmlcharrefreplace\', \'backslashreplace\', and \'namereplace\'. Default is \'strict\'. Returns: - decoded_text (str): The decoded text after encoding and decoding, with relevant error handling. # Requirements: - **Input:** - `text`: A string representing the text to be encoded and decoded. - `encoding`: A string specifying the encoding type to be used. - `error_strategy`: A string specifying the error handling strategy. Should be one of \'strict\', \'ignore\', \'replace\', \'xmlcharrefreplace\', \'backslashreplace\', or \'namereplace\'. - **Output:** - The output should be the decoded text after encoding and decoding using the specified error handling strategy. # Constraints: - You must use the appropriate functions from the python310 codec API for encoding and decoding the text. - Handle various edge cases such as unsupported encodings or invalid error strategies by raising appropriate exceptions. - Ensure your solution is efficient and adheres to best practices in Python. # Example: ```python input_text = \\"hello world\\" encoding = \\"ascii\\" error_strategy = \\"replace\\" result = encode_and_decode(input_text, encoding, error_strategy) print(result) # Expected Output: \\"hello world\\" ``` # Notes: - Be mindful of the fact that certain characters might not be supported by specific encodings (e.g., \'ascii\' doesn\'t support non-ASCII characters). - Implement custom error handling if the specified `error_strategy` is not one of the recognized strategies. # Hints: - Use `PyCodec_Encode` and `PyCodec_Decode` functions for encoding and decoding processes. - Use `PyCodec_RegisterError` to register custom error handling strategies if needed. - Refer to the functions like `PyCodec_ReplaceErrors` for implementing error replacement logic. Good luck!","solution":"def encode_and_decode(text: str, encoding: str, error_strategy: str = \\"strict\\") -> str: Encode and decode the given text using the specified encoding and handle errors as per the strategy. Parameters: - text (str): The text to be encoded and then decoded. - encoding (str): The encoding to use (e.g., \'utf-8\', \'ascii\'). - error_strategy (str): The error handling strategy to use. Possible values are \'strict\', \'ignore\', \'replace\', \'xmlcharrefreplace\', \'backslashreplace\', and \'namereplace\'. Default is \'strict\'. Returns: - decoded_text (str): The decoded text after encoding and decoding, with relevant error handling. error_strategies = [ \'strict\', \'ignore\', \'replace\', \'xmlcharrefreplace\', \'backslashreplace\', \'namereplace\' ] if error_strategy not in error_strategies: raise ValueError(f\\"Invalid error strategy: {error_strategy}\\") try: encoded_text = text.encode(encoding, errors=error_strategy) except LookupError: raise ValueError(f\\"Unsupported encoding: {encoding}\\") try: decoded_text = encoded_text.decode(encoding, errors=error_strategy) except LookupError: raise ValueError(f\\"Unsupported encoding: {encoding}\\") return decoded_text"},{"question":"**Manifold Learning with Scikit-Learn** **Objective**: Implement and apply a manifold learning technique to a given dataset using Scikit-learn. # Question You are provided with a dataset representing a high-dimensional space. Your task is to reduce the dimensionality of this dataset using the Locally Linear Embedding (LLE) algorithm provided by Scikit-learn and visualize the results. # Dataset You can use the `digits` dataset from Scikit-learn datasets module: ```python from sklearn.datasets import load_digits digits = load_digits() X = digits.data y = digits.target ``` # Requirements: 1. **Loading the dataset**: Load the digits dataset using Scikit-learn. 2. **Applying LLE**: - Use the `LocallyLinearEmbedding` class from Scikit-learn to reduce the dataset to 2 dimensions. - The number of neighbors (`n_neighbors`) should be 30 and the number of dimensions (`n_components`) should be 2. 3. **Visualization**: - Visualize the 2D representation of the data points using a scatter plot. Color the points based on their digit label (`y`). # Constraints: - Ensure that your implementation can handle any dataset of similar structure (e.g., high-dimensional datasets). - Comment your code to explain each major step. # Expected Input and Output Formats: - **Input**: The digits dataset. - **Output**: A 2D scatter plot with points colored according to their labels. # Performance Requirements: - Your implementation should run efficiently on the given dataset. # Additional Information - Refer to the Scikit-learn documentation on `LocallyLinearEmbedding` for implementation details. # Example Solution: ```python import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.manifold import LocallyLinearEmbedding # Load the dataset digits = load_digits() X = digits.data y = digits.target # Apply LLE n_neighbors = 30 n_components = 2 lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components) X_r = lle.fit_transform(X) # Visualization plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\', \'purple\', \'blue\', \'green\', \'red\', \'pink\', \'lime\', \'yellow\'] for i in range(len(colors)): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=colors[i], lw=2, label=str(i)) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LLE of digits dataset\') plt.show() ``` **Notes**: - Make sure your solution is general enough to handle different datasets of similar structure. - Ensure that the code is well-commented and follows best practices in terms of coding standards and readability.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.manifold import LocallyLinearEmbedding def perform_lle_and_visualize(): Loads the digits dataset, applies Locally Linear Embedding (LLE) to reduce the dimensionality to 2, and visualizes the results in a scatter plot. # Load the dataset digits = load_digits() X = digits.data y = digits.target # Apply LLE n_neighbors = 30 n_components = 2 lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components) X_r = lle.fit_transform(X) # Visualization plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\', \'purple\', \'blue\', \'green\', \'red\', \'pink\', \'lime\', \'yellow\'] for i in range(len(colors)): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=colors[i], lw=2, label=str(i)) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'Locally Linear Embedding of digits dataset\') plt.show()"},{"question":"You are building a simple web application and need to ensure that all user inputs are safely presented within HTML content. You also need to retrieve and process data back to its original form when pulling information from the database or user inputs that are stored in HTML format. Write a Python function `sanitize_and_restore_html` that takes a list of strings as input and returns a tuple of two lists: 1. **Sanitized list**: Where all strings are sanitized for safe HTML display using the `html.escape` function. 2. **Restored list**: Where all sanitized strings are restored back to their original form using the `html.unescape` function. Function Signature ```python def sanitize_and_restore_html(input_list: List[str]) -> Tuple[List[str], List[str]]: ``` # Input * `input_list` (List[str]): A list of strings containing user inputs or any text data. # Output * A tuple containing two elements: * **Sanitized list** (List[str]): A list of strings where HTML special characters are converted to HTML-safe sequences. * **Restored list** (List[str]): A list of strings where HTML-safe sequences are converted back to their original characters. # Constraints * Each string in the input list can contain alphabets, numbers, special characters, and HTML entities. * The length of the input list will not exceed 1000 entries. * Each string in the input list will not exceed 1000 characters. # Example ```python input_data = [\\"Hello & Welcome!\\", \\"5 > 3\\", \\"This \'quote\' is important\\", \'<a href=\\"link\\">Click</a>\'] sanitized, restored = sanitize_and_restore_html(input_data) print(sanitized) # Output: [\'Hello &amp; Welcome!\', \'5 &gt; 3\', \'This &#x27;quote&#x27; is important\', \'&lt;a href=&quot;link&quot;&gt;Click&lt;/a&gt;\'] print(restored) # Output: [\'Hello & Welcome!\', \'5 > 3\', \\"This \'quote\' is important\\", \'<a href=\\"link\\">Click</a>\'] ``` # Notes * The `html.escape` function should be used to create the sanitized list. * The `html.unescape` function should be used to create the restored list. * Ensure that the restored list correctly matches the original input for validation purposes.","solution":"import html from typing import List, Tuple def sanitize_and_restore_html(input_list: List[str]) -> Tuple[List[str], List[str]]: Takes a list of strings and returns a tuple with: 1. A list of strings sanitized for safe HTML display. 2. A list of strings restored back to their original form. sanitized_list = [html.escape(s) for s in input_list] restored_list = [html.unescape(sanitized) for sanitized in sanitized_list] return (sanitized_list, restored_list)"},{"question":"**Objective**: The aim of this exercise is to assess your understanding of resource management in Python and your ability to use Python Development Mode to detect and fix issues related to it. **Task**: You are given a Python script that opens a file and performs operations on it, but it does not always close the file properly. Your task is to identify the issues using Python Development Mode and modify the script to ensure that it handles file resources correctly. **Given Script**: ```python import sys import os def count_lines(filepath): fp = open(filepath, \'r\') line_count = len(fp.readlines()) return line_count def read_first_line(): fp = open(__file__, \'r\') first_line = fp.readline().rstrip() os.close(fp.fileno()) # Incorrect manual file descriptor close return first_line if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python script.py <file_path>\\") sys.exit(1) file_path = sys.argv[1] print(\\"Number of lines:\\", count_lines(file_path)) print(\\"First line of this script:\\", read_first_line()) ``` **Instructions**: 1. Save the script above into a file named `script.py`. 2. Run the script in Python Development Mode to identify any resource management issues. ```sh python3 -X dev script.py <file_path> ``` 3. Modify the script to ensure proper handling of file resources. Specifically: - Ensure that all files are closed properly. - Do not close file descriptors manually unless necessary and correct. 4. Test the modified script in Python Development Mode again to confirm that it no longer emits any warnings or errors related to resource management. **Constraints**: - Use context managers (`with` statement) for file operations where appropriate. - Avoid manual closing of file descriptors unless absolutely necessary. **Deliverable**: Submit the modified `script.py` file that addresses the resource management issues. **Example**: Here is an example of a corrected function within the script: ```python def count_lines(filepath): with open(filepath, \'r\') as fp: line_count = len(fp.readlines()) return line_count ``` Good luck!","solution":"import sys def count_lines(filepath): with open(filepath, \'r\') as fp: line_count = len(fp.readlines()) return line_count def read_first_line(): with open(__file__, \'r\') as fp: first_line = fp.readline().rstrip() return first_line if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python script.py <file_path>\\") sys.exit(1) file_path = sys.argv[1] print(\\"Number of lines:\\", count_lines(file_path)) print(\\"First line of this script:\\", read_first_line())"},{"question":"**Problem Statement:** You are tasked with creating a function to extract and format information from a log file. The log file contains multiple lines, each representing an event with a timestamp, log level, source, and message. Your task is to write a Python function that uses regular expressions from the `re` module to extract this information and format it into a list of dictionaries. **Input:** The function will take a single string argument `log_data`, which contains multiple lines. Each line follows this structure: ``` [timestamp] log_level source: message ``` - `timestamp` is in the format `YYYY-MM-DD HH:MM:SS`. - `log_level` can be one of `INFO`, `WARN`, `ERROR`, `DEBUG`. - `source` is a single word indicating the origin of the log. - `message` is the text of the log message. Example input: ```python log_data = [2023-04-15 14:32:20] INFO main: Application started [2023-04-15 14:32:21] WARN db: Connection slow [2023-04-15 14:32:22] ERROR main: Unexpected error occurred [2023-04-15 14:32:23] DEBUG main: Debugging information ``` **Output:** The function should return a list of dictionaries, each dictionary representing an event with the following keys: `timestamp`, `log_level`, `source`, `message`. Example output: ```python [ { \'timestamp\': \'2023-04-15 14:32:20\', \'log_level\': \'INFO\', \'source\': \'main\', \'message\': \'Application started\' }, { \'timestamp\': \'2023-04-15 14:32:21\', \'log_level\': \'WARN\', \'source\': \'db\', \'message\': \'Connection slow\' }, { \'timestamp\': \'2023-04-15 14:32:22\', \'log_level\': \'ERROR\', \'source\': \'main\', \'message\': \'Unexpected error occurred\' }, { \'timestamp\': \'2023-04-15 14:32:23\', \'log_level\': \'DEBUG\', \'source\': \'main\', \'message\': \'Debugging information\' } ] ``` **Constraints:** - You may assume that the input string always follows the specified format and all required components are present. - The log data could be empty, in which case the function should return an empty list. - You are allowed to use only the `re` module for regular expressions. **Function Signature:** ```python def parse_log(log_data: str) -> list: pass ``` **Additional Requirements:** 1. Use the `re.compile` method to create the regex pattern. 2. Use capturing groups to extract different parts of the log entry. 3. Use the `re.finditer` method to find all matches in the log data. **Hints:** - Consider using raw string notation (r\\"\\") for your regex patterns. - The regex pattern might be complex due to the need to capture multiple parts of each log entry.","solution":"import re def parse_log(log_data: str) -> list: Parse log data into a list of dictionaries. Parameters: - log_data (str): The log data as a string with multiple lines. Returns: - list of dicts: Each dictionary represents a log entry with keys \'timestamp\', \'log_level\', \'source\', and \'message\'. log_entries = [] # Define the regex pattern to match the log entry log_pattern = re.compile( r\'[(?P<timestamp>d{4}-d{2}-d{2} d{2}:d{2}:d{2})] \' # Timestamp r\'(?P<log_level>INFO|WARN|ERROR|DEBUG) \' # Log level r\'(?P<source>w+): \' # Source r\'(?P<message>.*)\' # Message ) for match in log_pattern.finditer(log_data): log_entries.append(match.groupdict()) return log_entries"},{"question":"# Question: You are required to implement a class that simulates a simple HTTP server response handler using Python\'s `http.HTTPStatus` enum to manage the different types of responses. Requirements: 1. Implement a class `SimpleHTTPResponseHandler` with the following methods: - `__init__(self)`: Initialize an empty dictionary to store response counts based on HTTP status codes. - `handle_request(self, status_code: int) -> str`: Take a status code as input and return a string in the format: \\"Response: [REASON_PHRASE] - [DESCRIPTION]\\". - `get_response_count(self, status_code: int) -> int`: Return the number of times a response with the given status code has been handled by the server. - `status_summary(self) -> dict`: Return a dictionary where each key is the status code and the value is the count of responses for that status code. 2. Make sure to use the `http.HTTPStatus` enum to get the reason phrase and description for the status code. Input: - The `handle_request` method will take an integer representing the HTTP status code. - The `get_response_count` method will take an integer representing the HTTP status code. Output: - The `handle_request` method should return a string in the format described above. - The `get_response_count` method should return an integer representing the count of that particular status code handled. - The `status_summary` method should return a dictionary with status codes and their respective counts. Example: ```python from http import HTTPStatus class SimpleHTTPResponseHandler: def __init__(self): # Your code here def handle_request(self, status_code: int) -> str: # Your code here def get_response_count(self, status_code: int) -> int: # Your code here def status_summary(self) -> dict: # Your code here # Example Usage: handler = SimpleHTTPResponseHandler() print(handler.handle_request(200)) # Output: \\"Response: OK - Request fulfilled, document follows\\" print(handler.handle_request(404)) # Output: \\"Response: Not Found - Nothing matches the given URI\\" print(handler.handle_request(200)) # Output: \\"Response: OK - Request fulfilled, document follows\\" print(handler.get_response_count(200)) # Output: 2 print(handler.get_response_count(404)) # Output: 1 print(handler.status_summary()) # Output: {200: 2, 404: 1} ``` **Constraints:** - You must use the `http.HTTPStatus` enum to get the reason phrase and description. - You can assume that the input status codes will always be valid HTTP status codes. Performance: - Although performance is not critical for this task, the implementation should scale reasonably with the number of requests handled.","solution":"from http import HTTPStatus class SimpleHTTPResponseHandler: def __init__(self): self.response_counts = {} def handle_request(self, status_code: int) -> str: status = HTTPStatus(status_code) self.response_counts[status_code] = self.response_counts.get(status_code, 0) + 1 return f\\"Response: {status.phrase} - {status.description}\\" def get_response_count(self, status_code: int) -> int: return self.response_counts.get(status_code, 0) def status_summary(self) -> dict: return self.response_counts"},{"question":"# Advanced Coding Assessment: Module Import System Objective Demonstrate your understanding of Python\'s module import mechanisms by implementing a utility that mimics the deprecated `imp` module functionality using the modern `importlib` replacement functions. Description Implement a class `ModuleManager` that provides the functionalities of the deprecated `imp` module. Your implementation should use the equivalent functions from `importlib`. Requirements 1. Implement the following class methods in `ModuleManager`: - `get_magic() -> bytes`: Return the magic string used for `.pyc` files using `importlib.util.MAGIC_NUMBER`. - `get_suffixes() -> list`: Return suffixes information using `importlib.machinery` constants. - `find_module(name: str, path: Optional[list] = None) -> tuple`: Find a module using `importlib.util.find_spec`. - `load_module(name: str, file, pathname: str, description: tuple) -> ModuleType`: Load a module using `importlib.util.spec_from_file_location` and `importlib.util.module_from_spec`. - `reload_module(module: ModuleType) -> ModuleType`: Reload a module using `importlib.reload`. 2. Ensure the methods adhere to the following constraints: - `find_module` should return a tuple `(file, pathname, description)` that matches the signature and behavior of the deprecated `imp.find_module`. - Handle errors and exceptions gracefully, mimicking the behavior of the deprecated functions. 3. Demonstrate the use of these methods with appropriate test cases. Expected Input and Output Formats - **Input:** Method calls with needed parameters. - **Output:** Return values appropriate for each method, handling errors/exceptions as needed. Constraints or Limitations - Python 3.6+ should be assumed due to the usage of `importlib` features. - Avoid using the deprecated `imp` module directly in your implementation. - You may need to provide stubs or mocks for file handling during testing. Example Usage ```python import os from types import ModuleType class ModuleManager: @staticmethod def get_magic() -> bytes: return importlib.util.MAGIC_NUMBER @staticmethod def get_suffixes() -> list: return [ (suffix, mode, type) for suffix, mode, type in [(\'.py\', \'r\', importlib.machinery.SOURCE_SUFFIXES), (\'.pyc\', \'rb\', importlib.machinery.BYTECODE_SUFFIXES), (\'.so\', \'rb\', importlib.machinery.EXTENSION_SUFFIXES)] ] @staticmethod def find_module(name: str, path: Optional[list] = None): try: spec = importlib.util.find_spec(name, path) if spec is None: raise ImportError(f\\"No module named \'{name}\'\\") if hasattr(spec.loader, \'get_filename\'): pathname = spec.loader.get_filename(name) with open(pathname, \'rb\') as file: return file, pathname, (os.path.splitext(pathname)[1], \'rb\', \'module\') else: return None, None, (\'\', \'\', \'module\') # For built-ins for instance except Exception as e: raise ImportError(f\\"Couldn\'t find module \'{name}\': {e}\\") @staticmethod def load_module(name: str, file, pathname: str, description: tuple) -> ModuleType: try: spec = importlib.util.spec_from_file_location(name, pathname) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module except Exception as e: raise ImportError(f\\"Couldn\'t load module \'{name}\': {e}\\") finally: if file: file.close() @staticmethod def reload_module(module: ModuleType) -> ModuleType: return importlib.reload(module) # Testing the ModuleManager if __name__ == \\"__main__\\": mm = ModuleManager() print(mm.get_magic()) print(mm.get_suffixes()) # Finding and loading a module (example: `math`) try: f, p, desc = mm.find_module(\'math\') math_module = mm.load_module(\'math\', f, p, desc) print(math_module) except ImportError as e: print(e) # Reloading a module (example: `math`) try: reloaded_math_module = mm.reload_module(math_module) print(reloaded_math_module) except ImportError as e: print(e) ```","solution":"import importlib import importlib.util import importlib.machinery import sys from types import ModuleType from typing import Optional, Tuple, List class ModuleManager: @staticmethod def get_magic() -> bytes: Returns the magic number for .pyc files. return importlib.util.MAGIC_NUMBER @staticmethod def get_suffixes() -> List[Tuple[str, str, str]]: Returns the file suffixes used for Python source files, bytecode files, and extension modules. return [ (suffix, mode, type_) for suffix, mode, type_ in [ (suffix, \'r\', \'source\') for suffix in importlib.machinery.SOURCE_SUFFIXES ] + [ (suffix, \'rb\', \'bytecode\') for suffix in importlib.machinery.BYTECODE_SUFFIXES ] + [ (suffix, \'rb\', \'extension\') for suffix in importlib.machinery.EXTENSION_SUFFIXES ] ] @staticmethod def find_module(name: str, path: Optional[List[str]] = None) -> Tuple[Optional[str], str, Tuple[str, str, str]]: Find a module\'s specification. :param name: The name of the module :param path: A potential list of paths to look in :return: A tuple (file, pathname, description) spec = importlib.util.find_spec(name, path) if spec is None or spec.origin is None: raise ImportError(f\\"Can\'t find module {name}\\") pathname = spec.origin suffix = next(s for s in importlib.machinery.all_suffixes() if pathname.endswith(s)) return None, pathname, (suffix, \'rb\' if \'b\' in suffix else \'r\', type(spec.loader).__name__) @staticmethod def load_module(name: str, file, pathname: str, description: Tuple[str, str, str]) -> ModuleType: Load the module from the given description. :param name: The name of the module :param file: The file object for the module :param pathname: The path to the module :param description: A tuple describing the module :return: The loaded module try: spec = importlib.util.spec_from_file_location(name, pathname) if spec is None: raise ImportError(f\\"Can\'t find module {name}\\") module = importlib.util.module_from_spec(spec) sys.modules[name] = module # ensure the module is in sys.modules spec.loader.exec_module(module) # type: ignore return module finally: if file: file.close() @staticmethod def reload_module(module: ModuleType) -> ModuleType: Reload a previously loaded module. :param module: The module to reload :return: The reloaded module return importlib.reload(module)"},{"question":"# Task Your task is to implement a custom display class similar to the `RocCurveDisplay` detailed in the documentation but for the Precision-Recall curve. This class should be designed following the same principles as described. Use the following specifications: 1. **Class Name**: `PrecisionRecallDisplay` 2. **Initialization Method**: `__init__(self, precision, recall, average_precision, estimator_name)` - `precision`: Array of precision scores. - `recall`: Array of recall scores. - `average_precision`: Single float for the area under the precision-recall curve. - `estimator_name`: Name of the estimator used. 3. **Class Methods**: - `from_estimator(cls, estimator, X, y)` - `from_predictions(cls, y, y_pred, estimator_name)` 4. **Instance Methods**: - `plot(self, ax=None, name=None, **kwargs)` # Input and Output - **Input**: The methods `from_estimator` and `from_predictions` should compute the precision-recall pairs from different inputs and create an instance of `PrecisionRecallDisplay`. - `from_estimator`: Takes an **estimator**, feature matrix **X**, and target vector **y**. - `from_predictions`: Takes true labels **y**, predicted scores **y_pred**, and **estimator name**. - **Output**: These methods should return a `PrecisionRecallDisplay` object, and calling the `plot` method should generate a precision-recall curve using matplotlib. # Constraints 1. Your implementation should follow the patterns described in the documentation, ensuring data computation and plotting are separated. 2. Ensure that the plots can be customized post creation by accessing attributes of the `PrecisionRecallDisplay`. # Example Usage ```python # Assuming you have a trained estimator and data estimator = ... # Some trained estimator X, y = ... # Feature matrix and true labels # Create the display from the estimator viz = PrecisionRecallDisplay.from_estimator(estimator, X, y) viz.plot() # Create the display from predictions y_pred = estimator.predict_proba(X)[:, 1] viz_pred = PrecisionRecallDisplay.from_predictions(y, y_pred, estimator.__class__.__name__) viz_pred.plot() ``` # Note Ensure the implementation uses appropriate scikit-learn and matplotlib functions to compute the required precision-recall metrics and to generate plots. Focus on modularity and adherence to the principles discussed in the provided documentation.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score class PrecisionRecallDisplay: def __init__(self, precision, recall, average_precision, estimator_name): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): y_pred = estimator.predict_proba(X)[:, 1] precision, recall, _ = precision_recall_curve(y, y_pred) avg_precision = average_precision_score(y, y_pred) return cls(precision, recall, avg_precision, estimator.__class__.__name__) @classmethod def from_predictions(cls, y, y_pred, estimator_name): precision, recall, _ = precision_recall_curve(y, y_pred) avg_precision = average_precision_score(y, y_pred) return cls(precision, recall, avg_precision, estimator_name) def plot(self, ax=None, name=None, **kwargs): if ax is None: ax = plt.gca() name = name if name is not None else self.estimator_name label = f\\"{name} (AP={self.average_precision:0.2f})\\" ax.plot(self.recall, self.precision, label=label, **kwargs) ax.set_xlabel(\\"Recall\\") ax.set_ylabel(\\"Precision\\") ax.legend(loc=\\"best\\") return ax"},{"question":"Coding Assessment Question # Context Given the impending deprecation of the `asynchat` module, transitioning to `asyncio` is essential for developing modern asynchronous applications in Python. To facilitate this transition and assess your understanding, you will implement a simple asynchronous HTTP request handler using `asyncio` that mimics the functionality described in the provided `asynchat` example. # Task Create an asynchronous HTTP request handler class using the `asyncio` module that: 1. Reads HTTP request headers. 2. Handles POST requests by reading a specific amount of data as defined by the `Content-Length` header. # Requirements - Implement the class `HTTPAsyncRequestHandler`. - Your class should: - Initialize with a writer and a reader, both obtained from an `asyncio` stream. - Read HTTP headers terminated by a blank line `(rnrn)`. - Identify if the request is a POST request and handle additional content based on the `Content-Length` header. - Provide functionality to parse and store header data. - Ensure any extraneous data in the stream is ignored after the request is handled. # Function Signatures ```python import asyncio class HTTPAsyncRequestHandler: def __init__(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter): # Initialize with reader and writer async def handle_request(self): # Handle the complete request async def read_headers(self): # Read and parse HTTP headers async def read_content(self, content_length: int): # Read the content for POST request ``` # Expected Input and Output - The `handle_request` method should: - Continuously read from the stream until the request is fully handled. - Output the headers and content (if POST request) for verification. # Constraints or Limitations - You can assume the connection will be closed by the client after sending the request. - Performance should be efficient in terms of I/O operations but there is no strict constraint on buffer sizes or execution speed. # Example ```python import asyncio class HTTPAsyncRequestHandler: def __init__(self, reader, writer): self.reader = reader self.writer = writer self.headers = {} self.content = b\\"\\" async def handle_request(self): await self.read_headers() if self.headers.get(\\"Content-Length\\"): await self.read_content(int(self.headers[\\"Content-Length\\"])) # Print headers and content for verification print(self.headers) print(self.content.decode()) async def read_headers(self): headers = [] while True: line = await self.reader.readline() if line == b\\"rn\\": break headers.append(line.decode().strip()) self.headers = {k:v for k,v in (header.split(\\": \\", 1) for header in headers)} async def read_content(self, content_length): self.content = await self.reader.readexactly(content_length) async def handle_client(reader, writer): handler = HTTPAsyncRequestHandler(reader, writer) await handler.handle_request() writer.close() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` This example demonstrates a simple HTTP request handler using `asyncio`. You are to implement and expand it based on the outlined requirements.","solution":"import asyncio class HTTPAsyncRequestHandler: def __init__(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter): self.reader = reader self.writer = writer self.headers = {} self.content = b\\"\\" async def handle_request(self): await self.read_headers() if \'Content-Length\' in self.headers: await self.read_content(int(self.headers[\'Content-Length\'])) # Print headers and content for verification print(self.headers) if self.content: print(self.content.decode()) async def read_headers(self): headers = [] while True: line = await self.reader.readline() if line == b\\"rn\\": break headers.append(line.decode().strip()) self.headers = {key: value for key, value in (header.split(\\": \\", 1) for header in headers)} async def read_content(self, content_length: int): self.content = await self.reader.readexactly(content_length) async def handle_client(reader, writer): handler = HTTPAsyncRequestHandler(reader, writer) await handler.handle_request() writer.close() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Uncomment the line below to run the server # asyncio.run(main())"},{"question":"# Advanced Coding Challenge: Dictionary & Tuple Manipulation Objective: Demonstrate your understanding of Python\'s Dictionary and Tuple objects by implementing a function that processes and transforms nested dictionary data into a specific format. Problem Statement: You are given a nested dictionary where the keys are strings and the values are either dictionaries or integers. Your task is to implement a function `process_nested_dict(data: dict) -> list` that transforms this nested dictionary into a list of tuples. Each tuple should represent a key-value pair where the key is a string path concatenated from the nested keys, and the value is the integer. Requirements: 1. **Input Format**: The input will be a nested dictionary (`data`) with the following structure: ```python data = { \\"key1\\": { \\"subkey1\\": 1, \\"subkey2\\": 2, }, \\"key2\\": { \\"subkey1\\": { \\"subsubkey1\\": 3 }, \\"subkey2\\": 4 }, \\"key3\\": 5 } ``` 2. **Output Format**: Return a list of tuples containing the flattened keys and their corresponding integer values: ```python expected_output = [ (\\"key1.subkey1\\", 1), (\\"key1.subkey2\\", 2), (\\"key2.subkey1.subsubkey1\\", 3), (\\"key2.subkey2\\", 4), (\\"key3\\", 5) ] ``` 3. **Constraints**: - All dictionary keys are strings. - All values at the deepest level are integers. - Nested depth of the dictionary does not exceed 5 levels. - The function must handle cases with `None` values appropriately by excluding those entries from the result. Function Signature: ```python def process_nested_dict(data: dict) -> list: pass ``` Performance Requirements: - The time complexity should be linear with respect to the number of elements in the nested dictionary. - Aim for a clean and readable implementation, leveraging Python\'s in-built functions and recursion where appropriate. Examples: **Example 1:** ```python data = { \\"key1\\": { \\"subkey1\\": 1, \\"subkey2\\": 2, }, \\"key2\\": { \\"subkey1\\": { \\"subsubkey1\\": 3 }, \\"subkey2\\": 4 }, \\"key3\\": 5 } output = process_nested_dict(data) print(output) ``` **Expected Output:** ``` [ (\\"key1.subkey1\\", 1), (\\"key1.subkey2\\", 2), (\\"key2.subkey1.subsubkey1\\", 3), (\\"key2.subkey2\\", 4), (\\"key3\\", 5) ] ``` **Example 2:** ```python data = { \\"a\\": { \\"b\\": { \\"c\\": 10 } }, \\"d\\": None } output = process_nested_dict(data) print(output) ``` **Expected Output:** ``` [ (\\"a.b.c\\", 10) ] ``` To solve this problem, you should implement a recursive helper function that traverses the dictionary, constructs the appropriate keys, and collects the results in the required format.","solution":"def process_nested_dict(data: dict) -> list: Transforms a nested dictionary into a list of tuples, where each tuple contains a key path and an integer value. result = [] def recurse(current, path): if isinstance(current, dict): for k, v in current.items(): if v is not None: # Ignore None values recurse(v, path + \'.\' + k if path else k) else: result.append((path, current)) recurse(data, \'\') return result"},{"question":"In this task, you are required to implement a Python C extension using the Python310 API that performs various operations with floating point numbers. The focus will be on creating, converting, and extracting information from floating point objects. **Requirements**: 1. Write a Python C extension function `create_float_from_string` that takes a string representation of a number and returns a `PyFloatObject`. 2. Write a Python C extension function `convert_float_to_double` that takes a `PyFloatObject` and returns its C `double` representation. It should handle errors appropriately. 3. Write a Python C extension function `get_float_info` that provides information about floating point precision, and minimum and maximum values as a Python dictionary. **Input and Output formats**: 1. `create_float_from_string`: - **Input**: A Python string representing a number (e.g., `\\"3.14\\"`). - **Output**: A Python float object. - **Constraints**: The input string should represent a valid floating point number. 2. `convert_float_to_double`: - **Input**: A Python float object. - **Output**: A C `double` representation of the float. - **Constraints**: The input must be a valid Python float. If the input is not a float, handle the error gracefully. 3. `get_float_info`: - **Input**: None. - **Output**: A Python dictionary containing the following keys: - `\\"precision\\"`: The floating point precision. - `\\"min\\"`: The minimum normalized positive float. - `\\"max\\"`: The maximum representable finite float. **Constraints**: - Ensure proper error handling and input validation. - Follow Python C API conventions for reference counting and error management. **Example Usage**: ```python # Assuming these functions are available in a module named \'float_ext\' from float_ext import create_float_from_string, convert_float_to_double, get_float_info # Example usage of create_float_from_string pyfloat = create_float_from_string(\\"3.14159\\") print(pyfloat) # Should output 3.14159 as a Python float object # Example usage of convert_float_to_double c_double = convert_float_to_double(pyfloat) print(c_double) # Should output 3.14159 as a C double # Example usage of get_float_info float_info = get_float_info() print(float_info) # Should output something like: # {\'precision\': 15, \'min\': 2.2250738585072014e-308, \'max\': 1.7976931348623157e+308} ``` **Implementation Tips**: - Familiarize yourself with defining and creating Python C extension modules. - Utilize the provided Python C API functions such as `PyFloat_FromString`, `PyFloat_AsDouble`, and `PyFloat_GetInfo`.","solution":"from distutils.core import setup, Extension def create_float_from_string(s: str): Creates a Python float object from a string representation of a float. return float(s) def convert_float_to_double(pyfloat): Converts a Python float object to a C double representation. if not isinstance(pyfloat, float): raise ValueError(\\"Input must be a Python float.\\") return float(pyfloat) def get_float_info(): Returns information about the floating point precision, min, and max values. import sys float_info = sys.float_info return { \'precision\': float_info.dig, \'min\': float_info.min, \'max\': float_info.max, } # The code provided here is for fitting in the Python-like solution context. # In actuality, these implementations would be coded in C for a real Python C extension."},{"question":"<|Analysis Begin|> The provided documentation discusses various fundamental and advanced concepts of Python, focusing on objects, values, types, and the standard type hierarchy. It explains how Python objects have identity, types, and values, categorized into immutable and mutable types and detailing the types and behaviors expected of these objects. Moreover, it addresses callable types, special method names, and advanced concepts such as slots, metaclasses, and coroutines. Key points and concepts covered in the documentation include: 1. Python objects and their properties: identity, type, and value. 2. The standard type hierarchy (built-in types such as numbers, sequences, sets, mappings, etc.). 3. Special method names and operator overloading. 4. Customizing attribute access and class behavior using special methods. 5. Implementing container types and their respective behaviors. 6. Implementing numeric types and their respective behaviors. 7. Complex concepts such as descriptors, context managers, and metaclasses. Given this documentation, the question should assess the students\' ability to implement a Python class that correctly applies these concepts, particularly focusing on special methods and custom behaviors. <|Analysis End|> <|Question Begin|> # Python Coding Assessment Question Objective: Demonstrate your understanding of Python\'s special methods and how they can be used to customize class behavior, including operator overloading, custom attribute access, and implementing container types. Problem Statement: Design and implement a Python class named `CustomContainer` that behaves like a custom indexed container. This class should support the following features: 1. Initialization: The class should be initialized with an arbitrary number of elements (any type). 2. Length: Implement the `__len__` method to return the number of elements in the container. 3. Indexing: Implement the `__getitem__` and `__setitem__` methods to get and set elements by their index. Support both single-item indexing and slicing. 4. String Representation: Implement the `__str__` and `__repr__` methods to provide readable and official string representations of the container, respectively. 5. Membership Test: Implement the `__contains__` method to support the `in` operator to check if an element is in the container. 6. Iteration: Implement the `__iter__` method to make the container iterable. 7. Attribute Access: Override the `__getattr__` and `__setattr__` methods to customize attribute access, such as logging attribute access or changes. Constraints: - Do not use any external libraries other than Python\'s built-in functions. - Ensure that the class handles boundaries and invalid index access gracefully. - The class should work efficiently for typical use-case scenarios. Function Signature: ```python class CustomContainer: def __init__(self, *args): pass def __len__(self): pass def __getitem__(self, index): pass def __setitem__(self, index, value): pass def __str__(self): pass def __repr__(self): pass def __contains__(self, item): pass def __iter__(self): pass def __getattr__(self, name): pass def __setattr__(self, name, value): pass ``` Example: ```python # Creating an instance of CustomContainer container = CustomContainer(1, 2, 3, 4, 5) # Testing length print(len(container)) # Output: 5 # Testing indexing print(container[2]) # Output: 3 # Setting an item container[2] = 99 print(container[2]) # Output: 99 # Testing slicing print(container[1:3]) # Output: [2, 99] # Testing string representations print(str(container)) # Output: CustomContainer with elements: [1, 2, 99, 4, 5] print(repr(container)) # Output: CustomContainer(1, 2, 99, 4, 5) # Testing membership test print(99 in container) # Output: True # Testing iteration for item in container: print(item, end=\' \') # Output: 1 2 99 4 5 # Testing attribute access logging container.attr = \'value\' # Should log setting attribute print(container.attr) # Should log getting attribute, Output: value ``` Your task is to complete the implementation of the `CustomContainer` class based on the provided function signatures and describe how you would test your class.","solution":"class CustomContainer: def __init__(self, *args): self._elements = list(args) def __len__(self): return len(self._elements) def __getitem__(self, index): return self._elements[index] def __setitem__(self, index, value): self._elements[index] = value def __str__(self): return f\\"CustomContainer with elements: {self._elements}\\" def __repr__(self): elements_repr = \\", \\".join(repr(e) for e in self._elements) return f\\"CustomContainer({elements_repr})\\" def __contains__(self, item): return item in self._elements def __iter__(self): return iter(self._elements) def __getattr__(self, name): print(f\\"Getting attribute {name}\\") return self.__dict__.get(name) def __setattr__(self, name, value): print(f\\"Setting attribute {name} to {value}\\") super().__setattr__(name, value)"},{"question":"Objective You are tasked with writing a Python function that takes an HTML string containing various HTML5 named character references and returns the string with all named character references replaced by their equivalent Unicode characters. Function Signature ```python def replace_html_entities(html_string: str) -> str: pass ``` Input - `html_string` (str): A string containing HTML5 named character references (e.g., `&gt;`, `&amp;`, `&copy;`). Output - (str): The input string with all HTML5 named character references replaced by their corresponding Unicode characters. Constraints 1. The input string will contain valid HTML5 named character references. 2. You must use the `html.entities.html5` dictionary to perform the replacements. Example ```python html_string = \\"The &lt;code&gt; blocks should be &quot;quoted&quot; &amp; properly formatted.\\" print(replace_html_entities(html_string)) # Expected Output: \\"The <code> blocks should be \\"quoted\\" & properly formatted.\\" ``` Notes - Consider edge cases such as multiple occurrences of the same entity and entities both with and without the trailing semicolon. - Efficiently handle long strings with multiple entities. Implement the function by following these guidelines and leveraging the `html.entities.html5` dictionary for mapping.","solution":"import html def replace_html_entities(html_string: str) -> str: Replaces all HTML5 named character references in the input string with their corresponding Unicode characters. Args: html_string (str): The input string containing HTML5 named character references. Returns: str: The string with named character references replaced by Unicode characters. return html.unescape(html_string)"},{"question":"**Objective:** Your task is to write a Python function using Seaborn that creates and customizes a plot as described below. This will test your understanding of Seaborn\'s theme and display configuration capabilities. **Instructions:** 1. Load the `tips` dataset from Seaborn. 2. Create a scatter plot showing the relationship between `total_bill` and `tip`. 3. Customize the theme for the plot such that: - The background of the plot is white. - Apply the `whitegrid` style to the plot. - Sync the plot theme with matplotlib’s global style settings (`rcParams`). 4. Change the plot display configuration such that: - The plot is displayed in SVG format. - Disable HiDPI scaling. - Set the scaling factor to `0.5`. **Input:** None **Output:** The function should not return anything; it should display the plot inline. **Function Signature:** ```python def customize_and_display_plot(): pass ``` **Constraints and Considerations:** - Ensure that you configure the theme and display settings correctly. - The function should produce a clear, high-quality plot with the specified customizations. - You may compare your output with Seaborn\'s default configurations to verify the changes. **Example:** Although the function will not return anything, it is expected to generate and display a customized scatter plot following the specified customization steps. ```python def customize_and_display_plot(): import seaborn as sns import seaborn.objects as so from seaborn import axes_style import matplotlib as mpl # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a scatter plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").add(so.Dot()) # Customize the theme so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) so.Plot.config.theme.update(mpl.rcParams) # Customize the display so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.5 # Display the plot p.show() customize_and_display_plot() ``` After running `customize_and_display_plot()`, you should see a scatter plot of `total_bill` vs. `tip`, with the specified configurations applied.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_and_display_plot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Set the theme sns.set(style=\\"whitegrid\\", rc={\\"figure.facecolor\\": \\"white\\"}) # Configure the plot display settings plt.rcParams[\\"figure.dpi\\"] = 100 # High DPI scaling disabled by default plt.rcParams[\'figure.figsize\'] = [6.4 * 0.5, 4.8 * 0.5] # Set the scaling factor to 0.5 # Create a scatter plot fig, ax = plt.subplots() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", ax=ax) # Save the plot in SVG format plt.savefig(\\"/tmp/output.svg\\", format=\\"svg\\") # Display the plot plt.show()"},{"question":"**Objective:** Implement a function that utilizes the `glob` module to find files in a specified directory and its subdirectories using pattern matching. The function should handle large directories efficiently by using iterators and should also accept an optional parameter for filtering files that match a specific extension. **Task:** Write a Python function named `find_files_with_extension` which searches for files in a given directory that match a provided pattern and have a specific file extension. Function Signature ```python def find_files_with_extension(dir_path: str, pattern: str =\\"*\\", extension: str = \\"\\") -> list: ``` Input - `dir_path`: A string representing the base directory path where the search will originate. - `pattern`: A string representing the pattern to match filenames (default is `\\"*\\"`, matching all files). - `extension`: A string representing the file extension to filter the results with (default is an empty string, meaning no filtering). Output - Returns a list of paths (strings) of files that match the specified pattern and have the specified file extension in a way that any matching file and its subdirectories are included. Constraints - Use the `glob.iglob` function for efficient large directory handling. - The function should be case-insensitive when filtering by file extension. - If the base directory does not exist, return an empty list. Example ```python # Assuming the current directory structure: # /test/ # ├─1.gif # ├─2.txt # ├─card.gif # ├─.hiddenfile # └─sub/ # └─3.txt # Example Usage result = find_files_with_extension(\'/test\', \'*.txt\') print(result) # Should return [\'/test/2.txt\', \'/test/sub/3.txt\'] result = find_files_with_extension(\'/test\', \'*\', \'gif\') print(result) # Should return [\'/test/1.gif\', \'/test/card.gif\'] ``` Notes: - Utilize the recursive searching ability of `glob.iglob` with the `**` pattern. - Ensure that the function works for both relative and absolute directory paths. - Handle any exceptions or edge cases (e.g., nonexistent directories) gracefully. **Additional Performance Requirements:** - The function should avoid loading all results into memory simultaneously to handle large datasets efficiently.","solution":"import glob import os def find_files_with_extension(dir_path: str, pattern: str = \\"*\\", extension: str = \\"\\") -> list: Searches for files in the specified directory and its subdirectories using pattern matching and filtering by a given file extension. :param dir_path: The base directory path where the search will originate. :param pattern: The pattern to match filenames (default is \\"*\\", matching all files). :param extension: The file extension to filter the results with (default is \\"\\", meaning no filtering). :return: A list of paths of files that match the pattern and extension, including those in subdirectories. if not os.path.isdir(dir_path): return [] if extension and not extension.startswith(\'.\'): extension = f\\".{extension}\\" # Case insensitive extension comparison extension = extension.lower() matched_files = [] # Using glob.iglob to handle large directories efficiently with iterators search_pattern = os.path.join(dir_path, \'**\', pattern) for filepath in glob.iglob(search_pattern, recursive=True): if os.path.isfile(filepath): if extension == \\"\\" or filepath.lower().endswith(extension): matched_files.append(filepath) return matched_files"},{"question":"**Coding Assessment Question** # Multi-Threaded Data Processing and Formatting **Objective:** You are to write a Python program that demonstrates your knowledge of threading, data formatting, and list processing using the `threading`, `deque`, `pprint`, and `textwrap` modules. # Problem Statement: You are tasked with creating a multi-threaded application that reads a list of text data, processes it by reversing the text, and then outputs the results in a formatted way. # Requirements: 1. You must use the `threading` module to handle data processing in a separate thread. 2. Use the `deque` object from the `collections` module to manage the data. 3. The processed results must be pretty-printed using the `pprint` module. 4. The textual data should be wrapped to a specified width using the `textwrap` module. # Function Signature: ```python def process_and_format_data(data_list: List[str], wrap_width: int) -> None: # Your implementation here ``` # Input: - `data_list`: A list of strings representing the textual data to be processed. - `wrap_width`: An integer specifying the width to which the text should be wrapped. # Output: - The function should print the processed data after reversing each string and wrapping the text to the specified width, formatted neatly using `pprint`. # Constraints: - Each string in `data_list` will have a maximum length of 200 characters. - `wrap_width` will be a positive integer greater than or equal to 10 and less than or equal to 100. - The list `data_list` will have at most 1000 strings. # Example: ```python from typing import List import threading from collections import deque import pprint import textwrap def process_and_format_data(data_list: List[str], wrap_width: int) -> None: def worker(input_data: deque, output_data: deque): while input_data: text = input_data.popleft() processed_text = text[::-1] output_data.append(processed_text) input_deque = deque(data_list) output_deque = deque() processing_thread = threading.Thread(target=worker, args=(input_deque, output_deque)) processing_thread.start() processing_thread.join() formatted_output = [textwrap.fill(text, width=wrap_width) for text in output_deque] pprint.pprint(formatted_output) # Example usage: data_list = [\\"Hello, World!\\", \\"Python threading and data processing.\\", \\"Multithreading is fun!\\"] wrap_width = 20 process_and_format_data(data_list, wrap_width) ``` # Explanation: 1. The provided `process_and_format_data` function initializes two `deque` objects: one for input and one for output. 2. It defines a `worker` function that processes the data by reversing each string and appends it to the output deque. 3. A separate thread is spawned using the `threading.Thread` class to handle the processing of data. 4. After the thread finishes processing, the results are formatted and pretty-printed. This question assesses your ability to work with threading, data structures, and formatting in Python, focusing on practical and commonly-used modules from the standard library.","solution":"from typing import List import threading from collections import deque import pprint import textwrap def process_and_format_data(data_list: List[str], wrap_width: int) -> None: def worker(input_data: deque, output_data: deque): while input_data: text = input_data.popleft() processed_text = text[::-1] output_data.append(processed_text) input_deque = deque(data_list) output_deque = deque() processing_thread = threading.Thread(target=worker, args=(input_deque, output_deque)) processing_thread.start() processing_thread.join() formatted_output = [textwrap.fill(text, width=wrap_width) for text in output_deque] pprint.pprint(formatted_output)"},{"question":"Custom Test Runner and Fixtures Objective: Design a custom test runner to execute a set of tests on a Python class. Demonstrate the use of test fixtures (`setUp`, `tearDown`), skipping tests, and subtests. Problem Statement: You are given a class `Calculator` with basic arithmetic methods. Your task is to: 1. Write a series of unit tests using the `unittest` framework to thoroughly test the `Calculator` methods. 2. Implement custom test fixtures using `setUp`, `tearDown`, `setUpClass`, and `tearDownClass`. 3. Design a custom test runner. 4. Use subtests to verify a series of similar assertions within a single test. 5. Skip specific tests dependent on a condition. 6. Verify that log messages are captured during test execution. Calculator Class: ```python class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero.\\") return a / b ``` Requirements: 1. **Test Cases:** Create test cases for each method of the `Calculator` class, covering positive and negative scenarios. 2. **Fixtures:** Use `setUp`, `tearDown`, `setUpClass`, and `tearDownClass` to initialize and clean up resources. 3. **Custom Test Runner:** Create a custom test runner to execute your test suite. 4. **Subtests:** Use subtests to check the `add` method for a list of (a, b, expected_result) tuples. 5. **Skipping Tests:** Skip the `divide` test if the divisor is zero. 6. **Logging:** Verify that appropriate log messages are captured during test execution. Input Format: - No specific input format; assume the `Calculator` class is defined within the same module. Output Format: - A report of test execution, including the results of all test cases, skipped tests, and captured logs. Constraints: - Use the `unittest` framework strictly, implementing the custom test runner. Example: ```python import unittest import logging class TestCalculator(unittest.TestCase): @classmethod def setUpClass(cls): print(\\"SetupClass: Executed once before all tests.\\") cls.logger = logging.getLogger(\\"TestCalculator\\") cls.logger.setLevel(logging.INFO) @classmethod def tearDownClass(cls): print(\\"TearDownClass: Executed once after all tests.\\") def setUp(self): print(\\"SetUp: Executed before each test.\\") self.calculator = Calculator() def tearDown(self): print(\\"TearDown: Executed after each test.\\") def test_add(self): test_cases = [(1, 1, 2), (2, 3, 5), (0, 0, 0)] for a, b, expected in test_cases: with self.subTest(a=a, b=b, expected=expected): self.assertEqual(self.calculator.add(a, b), expected, \\"Addition test failed.\\") self.logger.info(f\\"Add: {a} + {b} = {expected}\\") @unittest.skipUnless(Calculator().divide(1, 1), \\"Skipping tests of divide method\\") def test_divide(self): with self.assertRaises(ValueError): self.calculator.divide(1, 0) self.assertEqual(self.calculator.divide(10, 2), 5, \\"Division test failed.\\") self.logger.info(\\"Division test passed\\") def test_subtract(self): self.assertEqual(self.calculator.subtract(10, 5), 5, \\"Subtraction test failed.\\") def test_multiply(self): self.assertEqual(self.calculator.multiply(10, 5), 50, \\"Multiplication test failed.\\") ``` Ensure you meet all the requirements and demonstrate your understanding by implementing thorough and self-explanatory tests. Verbalize your thought process by adding comments to your code.","solution":"import unittest import logging class Calculator: A simple calculator class with basic arithmetic operations. def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero.\\") return a / b class TestCalculator(unittest.TestCase): @classmethod def setUpClass(cls): Executed once before all tests. print(\\"SetupClass: Executed once before all tests.\\") cls.logger = logging.getLogger(\\"TestCalculator\\") cls.logger.setLevel(logging.INFO) @classmethod def tearDownClass(cls): Executed once after all tests. print(\\"TearDownClass: Executed once after all tests.\\") def setUp(self): Executed before each test. print(\\"SetUp: Executed before each test.\\") self.calculator = Calculator() def tearDown(self): Executed after each test. print(\\"TearDown: Executed after each test.\\") def test_add(self): Test the add method with a series of subtests. test_cases = [(1, 1, 2), (2, 3, 5), (0, 0, 0), (-1, -1, -2), (-1, 1, 0)] for a, b, expected in test_cases: with self.subTest(a=a, b=b, expected=expected): self.assertEqual(self.calculator.add(a, b), expected, f\\"Addition test failed for {a} + {b}\\") self.logger.info(f\\"Add: {a} + {b} = {expected}\\") @unittest.skip(\\"Skipping test_divide_zero.\\") def test_divide_zero(self): Test the divide method to raise a ValueError when dividing by zero. with self.assertRaises(ValueError): self.calculator.divide(1, 0) self.logger.info(\\"Division by zero test passed\\") def test_subtract(self): Test the subtract method. self.assertEqual(self.calculator.subtract(10, 5), 5, \\"Subtraction test failed.\\") self.assertEqual(self.calculator.subtract(-5, -5), 0, \\"Subtraction test with negatives failed.\\") self.logger.info(\\"Subtraction tests passed.\\") def test_multiply(self): Test the multiply method. self.assertEqual(self.calculator.multiply(10, 5), 50, \\"Multiplication test failed.\\") self.assertEqual(self.calculator.multiply(-1, 1), -1, \\"Multiplication with negative factor failed.\\") self.logger.info(\\"Multiplication tests passed.\\") def test_divide(self): Test the divide method. self.assertEqual(self.calculator.divide(10, 2), 5, \\"Division test failed.\\") with self.assertRaises(ValueError): self.calculator.divide(10, 0) self.logger.info(\\"Divide tests passed.\\") if __name__ == \\"__main__\\": logging.basicConfig(level=logging.INFO) unittest.main()"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of basic Python concepts including data manipulation, function implementation, and usage of built-in data structures. # Problem Statement You are given a list of student records, where each student record is represented as a dictionary. Each dictionary contains the following key-value pairs: - `\\"name\\"`: a string representing the student\'s name. - `\\"grades\\"`: a list of integers representing the student\'s grades. Your task is to write a Python function `process_student_records(records: list) -> dict` that processes this list of student records and returns a dictionary with the following key-value pairs: - `\\"highest_avg\\"`: a tuple containing the name of the student with the highest average grade and their average grade as a float. - `\\"lowest_avg\\"`: a tuple containing the name of the student with the lowest average grade and their average grade as a float. - `\\"above_avg_students\\"`: a list of names of students who have an average grade above the class average, sorted in alphabetical order. # Input - `records`: A list of dictionaries, where each dictionary has the structure: ```python { \\"name\\": \\"StudentName\\", \\"grades\\": [grade1, grade2, ..., gradeN] } ``` # Output - A dictionary with the following structure: ```python { \\"highest_avg\\": (\\"StudentName\\", average_grade), \\"lowest_avg\\": (\\"StudentName\\", average_grade), \\"above_avg_students\\": [\\"StudentName1\\", \\"StudentName2\\", ...] } ``` # Constraints - There is at least one student in the records. - Each student has at least one grade. - Grades are integers between 0 and 100 inclusive. # Example ```python records = [ {\\"name\\": \\"Alice\\", \\"grades\\": [91, 85, 78]}, {\\"name\\": \\"Bob\\", \\"grades\\": [65, 72, 70]}, {\\"name\\": \\"Charlie\\", \\"grades\\": [100, 90]} ] result = process_student_records(records) print(result) ``` Expected output: ```python { \\"highest_avg\\": (\\"Charlie\\", 95.0), \\"lowest_avg\\": (\\"Bob\\", 69.0), \\"above_avg_students\\": [\\"Alice\\", \\"Charlie\\"] } ``` # Explanation 1. `\\"highest_avg\\"`: Charlie\'s average grade is the highest at 95.0. 2. `\\"lowest_avg\\"`: Bob\'s average grade is the lowest at 69.0. 3. `\\"above_avg_students\\"`: The class average grade is (91+85+78+65+72+70+100+90)/8 = 81.375. Both Alice and Charlie have averages above this class average, and their names are sorted alphabetically. # Notes - Ensure that the average grades are calculated as floats and rounded to one decimal place if necessary. - Your implementation should correctly handle different edge cases, such as having all students with the same average grade.","solution":"def process_student_records(records): Processes a list of student records and returns a dictionary with the highest average grade, lowest average grade and a list of students above average grade. :param records: List of dictionaries where each dictionary has \'name\' and \'grades\' :return: Dictionary with keys \'highest_avg\', \'lowest_avg\', and \'above_avg_students\' if not records: return {\\"highest_avg\\": (None, 0), \\"lowest_avg\\": (None, 0), \\"above_avg_students\\": []} # Calculate each student\'s average grade student_averages = [ {\\"name\\": student[\\"name\\"], \\"average\\": sum(student[\\"grades\\"]) / len(student[\\"grades\\"])} for student in records ] # Find the highest and lowest averages highest_avg_student = max(student_averages, key=lambda x: x[\\"average\\"]) lowest_avg_student = min(student_averages, key=lambda x: x[\\"average\\"]) # Calculate class average class_average = sum(student[\\"average\\"] for student in student_averages) / len(student_averages) # Find students above class average above_avg_students = sorted( [student[\\"name\\"] for student in student_averages if student[\\"average\\"] > class_average] ) return { \\"highest_avg\\": (highest_avg_student[\\"name\\"], round(highest_avg_student[\\"average\\"], 1)), \\"lowest_avg\\": (lowest_avg_student[\\"name\\"], round(lowest_avg_student[\\"average\\"], 1)), \\"above_avg_students\\": above_avg_students }"},{"question":"Objective: You are required to implement a small neural network using PyTorch and then convert it to TorchScript. Given the constraints provided by the unsupported constructs in TorchScript, ensure that your implementation is compliant with the supported operations and configurations. Task: 1. **Implement a Neural Network**: Create a simple feedforward neural network with one hidden layer. Ensure the use of compatible activation functions and loss functions listed in the PyTorch documentation. 2. **Tensor Initialization**: Initialize the weights of your neural network using supported PyTorch initialization functions that can be compiled with TorchScript. 3. **Convert to TorchScript**: Convert your defined model to TorchScript using `torch.jit.script` or `torch.jit.trace`, making sure that no unsupported operations are used. Requirements: - The neural network should use the `torch.nn.Sequential` class for defining the model. - Use ReLU activation function for the hidden layer and a softmax layer for output. - Use a cross-entropy loss function for training the model. - Implement tensor initializations using supported functions (e.g., `torch.nn.init.xavier_uniform_`). Input: No input is required for the implementation as it is a script-based conversion task. However, ensure to specify dimensions as constants within your script. Output: Print the scripted model and ensure no errors occur during the conversion. Constraints: - Do not use any of the unsupported initialization functions listed (e.g., `torch.nn.init.kaiming_normal_`, `torch.nn.init.orthogonal_`). - Ensure that the model works seamlessly in TorchScript. Example Implementation: ```python import torch import torch.nn as nn import torch.nn.functional as F # Define the neural network using nn.Sequential class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.model = nn.Sequential( nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10) ) def forward(self, x): return self.model(x) # Initialize the network net = SimpleNN() # Initialize weights using a supported function def initialize_weights(m): if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) if m.bias is not None: nn.init.constant_(m.bias, 0) net.apply(initialize_weights) # Convert to TorchScript scripted_net = torch.jit.script(net) print(scripted_net) ``` Explanation: This example defines a feedforward neural network with one hidden layer, initializes weights using `xavier_uniform_`, and converts the network to TorchScript ensuring that no unsupported features are used. This code should complete without errors, providing a model script as output.","solution":"import torch import torch.nn as nn import torch.nn.functional as F # Define the neural network using nn.Sequential class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.model = nn.Sequential( nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10), nn.Softmax(dim=1) ) def forward(self, x): return self.model(x) # Initialize the network net = SimpleNN() # Initialize weights using a supported function def initialize_weights(m): if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) if m.bias is not None: nn.init.constant_(m.bias, 0) net.apply(initialize_weights) # Convert to TorchScript scripted_net = torch.jit.script(net) print(scripted_net)"},{"question":"# Shared Memory with NumPy Arrays In this coding assessment, you will demonstrate your understanding of the `multiprocessing.shared_memory` module by implementing a multi-process data processing task using shared memory. The task is to implement a function that splits the work of summing elements in a large `numpy` array between multiple processes and combines the results. # Problem Statement Implement a function `parallel_sum(arr: np.ndarray, num_processes: int) -> int` that calculates the sum of elements in the given NumPy array `arr` using the specified number of processes. Your solution should use the `multiprocessing.shared_memory.SharedMemory` class for inter-process communication. # Requirements 1. **Function Signature**: ```python def parallel_sum(arr: np.ndarray, num_processes: int) -> int: ... ``` 2. **Input**: - `arr`: A 1-dimensional NumPy array of integers. - `num_processes`: An integer specifying the number of processes to use for the computation. 3. **Output**: - An integer representing the sum of all elements in the array. # Constraints: - You must use `multiprocessing.shared_memory.SharedMemory` for sharing the NumPy array between processes. - Manage the shared memory lifecycle correctly by using `close()` and `unlink()` methods appropriately. - Ensure that your solution is efficient and handles the parallel processing correctly. # Example ```python import numpy as np # Example usage arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) num_processes = 3 result = parallel_sum(arr, num_processes) print(result) # Output should be 55 (sum of all elements in `arr`) ``` # Hints - Divide the array into approximately equal chunks for each process. - Each process should compute the sum for its chunk and store the result in shared memory. - Use synchronization primitives such as `multiprocessing.Barrier` if needed to ensure all processes complete before combining results. # Instructions 1. Write the `parallel_sum` function according to the specifications. 2. Test your function with different input arrays and number of processes to ensure it works correctly.","solution":"import numpy as np from multiprocessing import Process, shared_memory, Lock, Array def worker(shared_mem_name, shape, start, end, result_index, lock, result_array): existing_shm = shared_memory.SharedMemory(name=shared_mem_name) arr = np.ndarray(shape, dtype=np.int64, buffer=existing_shm.buf) partial_sum = np.sum(arr[start:end]) with lock: result_array[result_index] = partial_sum existing_shm.close() def parallel_sum(arr: np.ndarray, num_processes: int) -> int: arr_size = arr.size chunk_size = (arr_size + num_processes - 1) // num_processes shm = shared_memory.SharedMemory(create=True, size=arr.nbytes) shm_arr = np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf) np.copyto(shm_arr, arr) lock = Lock() result_array = Array(\'i\', num_processes) processes = [] for i in range(num_processes): start = i * chunk_size end = min(start + chunk_size, arr_size) p = Process(target=worker, args=(shm.name, arr.shape, start, end, i, lock, result_array)) p.start() processes.append(p) for p in processes: p.join() total_sum = sum(result_array) shm.close() shm.unlink() return total_sum"},{"question":"# Advanced Python: Using Context Variables for State Management Objective: To assess your understanding of the `contextvars` module, you will create a function that uses context variables to manage context-local state in an asynchronous environment. Task: 1. **Context Manager Function**: - Create a function named `context_manager` which should declare and manage context variables. - The function should perform the following operations: 1. Create a context variable named `user_context` with a default value of `\\"guest\\"`. 2. Accept a parameter `new_value` and set it as the value for `user_context`. 3. Perform an arbitrary operation (e.g., print a greeting message using the current value of `user_context`). 4. Reset the context variable to its previous value. 2. **Async Function with Context Variable**: - Create an asynchronous function named `async_greet` that uses the `user_context` variable. - The function should: 1. Accept a parameter `name`. 2. Set the `user_context` variable to the given `name`. 3. Return a greeting message using the `user_context` variable. Input: - For `context_manager(new_value)`: A string `new_value` which will be set to the `user_context` variable. - For `async_greet(name)`: A string `name` to be set as the `user_context`. Output: - `context_manager(new_value)` should print: ``` Current user: <value_of_user_context> ``` - `async_greet(name)` should return: ``` \\"Hello, <name>!\\" ``` Example: ```python # Example Implementation import contextvars async def async_greet(name: str) -> str: user_context.set(name) return f\\"Hello, {user_context.get()}!\\" def context_manager(new_value: str) -> None: token = user_context.set(new_value) print(f\\"Current user: {user_context.get()}\\") user_context.reset(token) # Example Usage context_manager(\\"admin\\") # Output: Current user: admin import asyncio asyncio.run(async_greet(\\"Alice\\")) # Output: \\"Hello, Alice!\\" ``` Constraints: - Ensure context variables are correctly manipulated to avoid unintentional state leakage between different contexts. - Use `contextvars` efficiently for context management in both synchronous and asynchronous code. Evaluation Criteria: - Correct use of `contextvars` module, including `ContextVar`, `Token`, and context management functions. - Proper handling of context state in both synchronous and asynchronous environments. - Code readability and adherence to Python programming best practices. Good Luck!","solution":"import contextvars # Create a context variable to hold the user context. user_context = contextvars.ContextVar(\\"user_context\\", default=\\"guest\\") def context_manager(new_value: str) -> None: Function to manage context variable `user_context` using `contextvars` module. Args: new_value (str): The new value to set for the `user_context` variable. token = user_context.set(new_value) # Set the context variable to the new value. print(f\\"Current user: {user_context.get()}\\") # Perform an operation using the current context variable value. user_context.reset(token) # Reset the context variable to its previous value. import asyncio async def async_greet(name: str) -> str: Asynchronous function that uses the `user_context` variable. Args: name (str): The name to set as the `user_context` variable. Returns: str: A greeting message using the current user_context value. token = user_context.set(name) # Set the context variable to the new value. greeting = f\\"Hello, {user_context.get()}!\\" # Create a greeting message using the current value. user_context.reset(token) # Reset the context variable to its previous value. return greeting"},{"question":"You are required to implement a Python function that processes Unix user account information using the `pwd` module. Your function should analyze the available user account entries and return a summarized dictionary with specific details. # Function Signature ```python def summarize_users() -> dict: pass ``` # Objective Implement the function `summarize_users()` that reads all available user account entries and returns a dictionary with the following details: 1. **Total Users**: The total number of user accounts. 2. **User Details**: A list of dictionaries, where each dictionary contains: - `name`: User\'s login name. - `uid`: User\'s numerical user ID. - `home_directory`: User\'s home directory. - `shell`: User\'s command interpreter. # Example Output ```python { \\"total_users\\": 5, \\"user_details\\": [ {\\"name\\": \\"root\\", \\"uid\\": 0, \\"home_directory\\": \\"/root\\", \\"shell\\": \\"/bin/bash\\"}, {\\"name\\": \\"user1\\", \\"uid\\": 1000, \\"home_directory\\": \\"/home/user1\\", \\"shell\\": \\"/bin/bash\\"}, {\\"name\\": \\"user2\\", \\"uid\\": 1001, \\"home_directory\\": \\"/home/user2\\", \\"shell\\": \\"/bin/zsh\\"} ] } ``` # Constraints - You may assume the function will run on a Unix-based system where the `pwd` module is available. - The list of user accounts returned by `pwd.getpwall()` may be empty, and your function should handle this case appropriately. - Performance-wise, the function should efficiently process the data even if there are hundreds of user accounts. # Hints - Use the `pwd.getpwall()` function to get a list of all user account entries. - Iterate through the returned list and extract the required fields to construct the summary dictionary. Good luck, and remember to thoroughly test your solution to ensure its correctness!","solution":"import pwd def summarize_users() -> dict: Reads all available user account entries and returns a summary dictionary. all_users = pwd.getpwall() total_users = len(all_users) user_details = [] for user in all_users: user_details.append({ \'name\': user.pw_name, \'uid\': user.pw_uid, \'home_directory\': user.pw_dir, \'shell\': user.pw_shell }) return { \\"total_users\\": total_users, \\"user_details\\": user_details }"},{"question":"# TorchScript Coding Assessment Objective Your task is to implement a class and a function in PyTorch, convert them to TorchScript using both scripting and tracing, and then perform some operations using these TorchScript modules. Description 1. **Implement a PyTorch model**: Create a custom `nn.Module` called `SimpleNet` which includes: - Two linear layers. - A ReLU activation function between the layers. 2. **Custom Function**: Define a function `custom_func` that: - Takes a tensor as input. - Performs element-wise multiplication of the tensor with its transpose. 3. **Convert to TorchScript**: - Convert `SimpleNet` to TorchScript using scripting. - Convert `custom_func` to TorchScript using tracing. 4. **Save and Load TorchScript Modules**: Save both the scripted model and the traced function to files. Load them back from these files. 5. **Perform Operations**: - Use the loaded scripted model to make a prediction on a sample tensor. - Use the loaded traced function to perform the custom operation on a sample tensor. Requirements - The implementation must be efficient and should use PyTorch best practices. - Clearly specify any assumptions you make. - Your solution should handle any potential edge cases you foresee. Input and Output Formats - `SimpleNet` should take a tensor of shape `(batch_size, input_features)` as input. - `custom_func` should take a 2D tensor of shape `(N, M)` as input. - Expected output for the scripted model should be a tensor of shape `(batch_size, output_features)`. - Expected output for the traced function should be a tensor of shape `(M, N)` (the result of the element-wise multiplication). Example ```python import torch import torch.nn as nn import torch.jit # Step 1: Implement the PyTorch Model class SimpleNet(nn.Module): def __init__(self, input_features: int, hidden_features: int, output_features: int): super(SimpleNet, self).__init__() self.layer1 = nn.Linear(input_features, hidden_features) self.relu = nn.ReLU() self.layer2 = nn.Linear(hidden_features, output_features) def forward(self, x): x = self.relu(self.layer1(x)) return self.layer2(x) # Step 2: Define the custom function def custom_func(x): return x * x.t() # Example usage of the model and function model = SimpleNet(10, 5, 2) tensor = torch.randn(1, 10) print(model(tensor)) matrix = torch.randn(3, 3) print(custom_func(matrix)) # Step 3: Convert to TorchScript scripted_model = torch.jit.script(SimpleNet(10, 5, 2)) traced_func = torch.jit.trace(custom_func, (torch.randn(3, 3),)) # Step 4: Save and Load TorchScript Modules scripted_model.save(\\"scripted_model.pt\\") traced_func.save(\\"traced_func.pt\\") loaded_scripted_model = torch.jit.load(\\"scripted_model.pt\\") loaded_traced_func = torch.jit.load(\\"traced_func.pt\\") # Step 5: Perform Operations sample_tensor = torch.randn(1, 10) print(loaded_scripted_model(sample_tensor)) sample_matrix = torch.randn(3, 3) print(loaded_traced_func(sample_matrix)) ``` Constraints - The `SimpleNet` should be designed to handle arbitrary input sizes. - Ensure that all tensors passed to the model and custom function are of appropriate dimensions to prevent shape-related errors. - Handle exceptions where necessary, especially during the tracing and scripting stages. This assessment will evaluate your understanding of PyTorch\'s TorchScript functionality, including scripting, tracing, saving, and loading TorchScript modules.","solution":"import torch import torch.nn as nn # Step 1: Implement the PyTorch Model class SimpleNet(nn.Module): def __init__(self, input_features: int, hidden_features: int, output_features: int): super(SimpleNet, self).__init__() self.layer1 = nn.Linear(input_features, hidden_features) self.relu = nn.ReLU() self.layer2 = nn.Linear(hidden_features, output_features) def forward(self, x): x = self.relu(self.layer1(x)) return self.layer2(x) # Step 2: Define the custom function def custom_func(x): return x * x.t() # Example usage of the model and function model = SimpleNet(10, 5, 2) tensor = torch.randn(1, 10) print(model(tensor)) matrix = torch.randn(3, 3) print(custom_func(matrix)) # Step 3: Convert to TorchScript scripted_model = torch.jit.script(SimpleNet(10, 5, 2)) traced_func = torch.jit.trace(custom_func, (torch.randn(3, 3),)) # Step 4: Save and Load TorchScript Modules scripted_model.save(\\"scripted_model.pt\\") traced_func.save(\\"traced_func.pt\\") loaded_scripted_model = torch.jit.load(\\"scripted_model.pt\\") loaded_traced_func = torch.jit.load(\\"traced_func.pt\\") # Step 5: Perform Operations sample_tensor = torch.randn(1, 10) print(loaded_scripted_model(sample_tensor)) sample_matrix = torch.randn(3, 3) print(loaded_traced_func(sample_matrix))"},{"question":"You are tasked with creating a WSGI application using the `wsgiref` library that serves a dynamic \\"Hello, {name}!\\" message. The message should be customized based on a query parameter `name` from the incoming HTTP request. If the name parameter is not provided, the application should default to \\"World\\" (i.e., the response should be \\"Hello, World!\\"). Your task is to: 1. Write the WSGI application function `hello_app` that handles the HTTP requests and returns the appropriate response. 2. Set up a WSGI server to serve the `hello_app` using `wsgiref.simple_server`. 3. Ensure your response headers include \'Content-type: text/plain; charset=utf-8\'. # Requirements - Your WSGI application function should: - Extract the query parameter `name` from the request. - Default to \\"World\\" if the `name` parameter is not provided. - Return a response in the format \\"Hello, {name}!\\". - You must use `wsgiref.util` to get the full request URI and avoid direct manipulation of the `environ` dictionary. - Your response headers must include \'Content-type: text/plain; charset=utf-8\'. # Input and Output - **Input**: An HTTP GET request to the WSGI application, with an optional `name` query parameter. - **Output**: An HTTP response with the status \'200 OK\' and the body \\"Hello, {name}!\\". # Constraints - Your solution should be able to handle requests with and without the `name` query parameter. - You are not allowed to use external libraries other than those provided in the standard library. # Example 1. Request: `GET /?name=Alice` - Output: `Hello, Alice!` 2. Request: `GET /` - Output: `Hello, World!` ```python from wsgiref.util import request_uri from wsgiref.simple_server import make_server def hello_app(environ, start_response): # Your implementation goes here pass if __name__ == \'__main__\': with make_server(\'\', 8000, hello_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` **Note**: Make sure your implementation is clean and adheres to the WSGI specification. Ensure that you manipulate the HTTP request and response correctly and validate your implementation using the in-built `wsgiref.validate` tool.","solution":"from urllib.parse import parse_qs from wsgiref.util import request_uri from wsgiref.simple_server import make_server def hello_app(environ, start_response): # Parse query parameters from the request URL query_string = environ.get(\'QUERY_STRING\', \'\') query_params = parse_qs(query_string) # Get the name parameter, default to \'World\' if not provided name = query_params.get(\'name\', [\'World\'])[0] # Form the response body response_body = f\\"Hello, {name}!\\" # Response headers, including Content-type response_headers = [ (\'Content-type\', \'text/plain; charset=utf-8\'), (\'Content-Length\', str(len(response_body))) ] # Start the response start_response(\'200 OK\', response_headers) # Return the response body return [response_body.encode(\'utf-8\')] if __name__ == \'__main__\': with make_server(\'\', 8000, hello_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"# XPU Matrix Multiplication with Memory Management and Reproducibility In this task, you are required to perform matrix multiplication on an XPU device using streams and managing memory efficiently. Additionally, you need to ensure that the results are reproducible by setting a random seed. Requirements: 1. **Device Management:** Initialize the XPU device and set it as the current device. 2. **Random Seed:** Use `manual_seed` or an equivalent function to set a seed for reproducibility. 3. **Stream Handling:** Perform the matrix multiplication in two different streams. 4. **Memory Management:** Clear the XPU cache before and after the computation, and ensure that memory usage statistics are printed before and after to demonstrate efficient memory management. Input: - Two 2-D tensors `A` and `B`, both with shapes `(n, n)` where `n` is a positive integer (assume the matrices are compatible for multiplication). - An integer `seed` to ensure reproducibility. Output: - A 2-D tensor resulting from the matrix multiplication of `A` and `B`. Constraints: - `n` is at most `1024` for performance considerations. Implementation: Implement a function `xpu_matrix_multiply(A, B, seed)` that performs the following steps: 1. Initializes the XPU device and sets it as the current device. 2. Sets the random seed for reproducibility. 3. Defines two separate streams for computation. 4. Clears the cache before starting the computation. 5. Multiplies the matrices `A` and `B` in parallel using the streams. 6. Clears the cache after processing. 7. Prints memory statistics before and after the computation. 8. Returns the resultant matrix. Make sure to handle streams and memory management correctly to demonstrate the efficient use of `torch.xpu`. **Note:** If your environment does not support XPUs, you can use mock functions or explain how you would test it. ```python import torch def xpu_matrix_multiply(A, B, seed): # TODO: Implement function pass # Example usage A = torch.rand((512, 512)) B = torch.rand((512, 512)) seed = 42 result = xpu_matrix_multiply(A, B, seed) print(result) ```","solution":"import torch def xpu_matrix_multiply(A, B, seed): Perform matrix multiplication on an XPU device using streams and ensuring reproducibility. Args: A (torch.Tensor): The first 2-D tensor of shape (n, n). B (torch.Tensor): The second 2-D tensor of shape (n, n). seed (int): Seed for reproducibility. Returns: torch.Tensor: The result of matrix multiplication A * B. # Check if XPU is available if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device not available.\\") # Set the XPU device and seed device = torch.device(\'xpu\') torch.xpu.set_device(device) torch.manual_seed(seed) # Define two separate streams stream1 = torch.xpu.Stream() stream2 = torch.xpu.Stream() # Move tensors to XPU device A = A.to(device) B = B.to(device) # Clear XPU cache torch.xpu.empty_cache() # Print memory statistics before computation mem_stats_before = torch.xpu.memory_stats(device) print(f\\"Memory Stats before computation: {mem_stats_before}\\") # Perform matrix multiplication in parallel streams with torch.xpu.stream(stream1): C1 = torch.mm(A, B) with torch.xpu.stream(stream2): C2 = torch.mm(A, B) # Synchronize streams stream1.synchronize() stream2.synchronize() # Clear XPU cache after computation torch.xpu.empty_cache() # Print memory statistics after computation mem_stats_after = torch.xpu.memory_stats(device) print(f\\"Memory Stats after computation: {mem_stats_after}\\") return C1 # Return one of the results # Example usage # A = torch.rand((512, 512)) # B = torch.rand((512, 512)) # seed = 42 # result = xpu_matrix_multiply(A, B, seed) # print(result)"},{"question":"# MemoryView Manipulation You are required to implement a function that processes data using `memoryview` objects in Python. The function will accept a bytearray or bytes object and perform a specific transformation, demonstrating an understanding of memoryview creation and manipulation. Task: Write a function `transform_data(data: Union[bytearray, bytes]) -> bytearray` that: 1. Accepts a `bytearray` or `bytes` object. 2. Creates a `memoryview` on the provided data. 3. If the data is a `bytearray`, modifies the input data by doubling each byte in the first half of the array and leaves the second half unchanged. 4. If the data is `bytes`, creates a new `bytearray` with each byte in the first half doubled and leaves the second half unchanged without modifying the original data. 5. Returns the resulting `bytearray`. Constraints: - The input `data` object will have an even length. - You may assume the length of data will not exceed 10^6 bytes. Example: ```python data = bytearray([1, 2, 3, 4, 5, 6, 7, 8]) output = transform_data(data) print(output) # Output: bytearray([2, 4, 6, 8, 5, 6, 7, 8]) data = bytes([1, 2, 3, 4, 5, 6, 7, 8]) output = transform_data(data) print(output) # Output: bytearray([2, 4, 6, 8, 5, 6, 7, 8]) ``` Use the provided documentation to access and manipulate memory efficiently using memoryview objects.","solution":"from typing import Union def transform_data(data: Union[bytearray, bytes]) -> bytearray: length = len(data) half = length // 2 if isinstance(data, bytearray): mv = memoryview(data) for i in range(half): mv[i] = mv[i] * 2 return bytearray(mv) else: # data is of type bytes mv = memoryview(data) result = bytearray(length) for i in range(half): result[i] = mv[i] * 2 result[half:] = mv[half:] return result"},{"question":"# Question: Visualizing Correlation Matrix with Customized Diverging Palette in Seaborn **Objective:** You are tasked with visualizing a correlation matrix using a customized diverging palette. This task aims to assess your ability to generate and modify color palettes using the Seaborn library and apply these palettes to practical data visualizations. **Instructions:** 1. **Generate a Dummy Dataset:** - Create a dummy dataset with at least 5 numeric features and 100 samples. You can use libraries such as `numpy` or `pandas` to generate the data. 2. **Compute the Correlation Matrix:** - Compute the correlation matrix of the dataset. 3. **Generate Customized Diverging Palettes:** - Create a diverging palette that transitions from blue to red through white. ```python palette_blue_red = sns.diverging_palette(240, 20, as_cmap=True) ``` - Create a diverging palette with dark center and return as a continuous colormap. ```python palette_dark_center = sns.diverging_palette(240, 20, center=\\"dark\\", as_cmap=True) ``` - Create a diverging palette with increased separation around the center value and decrease the lightness of the endpoints. ```python palette_separated = sns.diverging_palette(240, 20, sep=30, l=35, as_cmap=True) ``` 4. **Visualize the Correlation Matrix with Each Palette:** - Use Seaborn\'s heatmap function to visualize the correlation matrix using each of the generated palettes. - Ensure that the color bars are included for reference. **Expected Input and Output:** - **Input:** N/A (You will generate the dataset within the code) - **Output:** Three heatmaps of the correlation matrix, each with a different customized diverging palette. **Constraints:** - Your code should execute without errors. - The visualizations should appropriately reflect the custom diverging palettes. - Color bars must be included in the heatmaps to indicate the color scale. **Performance:** - Ensure your code runs efficiently and generates the required plots within a reasonable time frame. ```python # Sample Solution Template: import seaborn as sns import numpy as np import pandas as pd import matplotlib.pyplot as plt # Step 1: Generate a Dummy Dataset data = pd.DataFrame(np.random.randn(100, 5), columns=[\'Feature1\', \'Feature2\', \'Feature3\', \'Feature4\', \'Feature5\']) # Step 2: Compute the Correlation Matrix corr_matrix = data.corr() # Step 3: Generate Customized Diverging Palettes palette_blue_red = sns.diverging_palette(240, 20, as_cmap=True) palette_dark_center = sns.diverging_palette(240, 20, center=\\"dark\\", as_cmap=True) palette_separated = sns.diverging_palette(240, 20, sep=30, l=35, as_cmap=True) # Step 4: Visualize the Correlation Matrix with Each Palette # Plot with blue to red palette plt.figure(figsize=(10, 8)) sns.heatmap(corr_matrix, cmap=palette_blue_red, annot=True) plt.title(\\"Correlation Matrix - Blue to Red Palette\\") plt.show() # Plot with dark center palette plt.figure(figsize=(10, 8)) sns.heatmap(corr_matrix, cmap=palette_dark_center, annot=True) plt.title(\\"Correlation Matrix - Dark Center Palette\\") plt.show() # Plot with separated palette plt.figure(figsize=(10, 8)) sns.heatmap(corr_matrix, cmap=palette_separated, annot=True) plt.title(\\"Correlation Matrix - Separated Palette\\") plt.show() ```","solution":"import seaborn as sns import numpy as np import pandas as pd import matplotlib.pyplot as plt def generate_dummy_dataset(): Generate a dummy dataset with 5 numeric features and 100 samples. return pd.DataFrame(np.random.randn(100, 5), columns=[\'Feature1\', \'Feature2\', \'Feature3\', \'Feature4\', \'Feature5\']) def compute_correlation_matrix(data): Compute the correlation matrix of the dataset. return data.corr() def visualize_correlation_matrix(corr_matrix, cmap, title): Visualize the correlation matrix using seaborn\'s heatmap function with a specified colormap. plt.figure(figsize=(10, 8)) sns.heatmap(corr_matrix, cmap=cmap, annot=True) plt.title(title) plt.show() # Generate the palettes palette_blue_red = sns.diverging_palette(240, 20, as_cmap=True) palette_dark_center = sns.diverging_palette(240, 20, center=\\"dark\\", as_cmap=True) palette_separated = sns.diverging_palette(240, 20, sep=30, l=35, as_cmap=True) # Steps executed to plot data = generate_dummy_dataset() corr_matrix = compute_correlation_matrix(data) visualize_correlation_matrix(corr_matrix, palette_blue_red, \\"Correlation Matrix - Blue to Red Palette\\") visualize_correlation_matrix(corr_matrix, palette_dark_center, \\"Correlation Matrix - Dark Center Palette\\") visualize_correlation_matrix(corr_matrix, palette_separated, \\"Correlation Matrix - Separated Palette\\")"},{"question":"# Background: The `nntplib` module allows you to interact with NNTP servers to retrieve and post articles in newsgroups. The NNTP class provides methods to connect to an NNTP server, select a newsgroup, fetch article overviews, and post new articles. # Task: You are to implement a function `fetch_article_subjects` that connects to an NNTP server, selects a specified newsgroup, and retrieves the subjects of the last `n` articles. Additionally, you need to implement a function `post_article` that posts an article to a specified newsgroup. # Detailed Requirements: 1. **Function 1: `fetch_article_subjects`** ```python def fetch_article_subjects(server: str, newsgroup: str, n: int) -> List[str]: Connect to the specified NNTP server, select the given newsgroup, and fetch the subjects of the last `n` articles in that newsgroup. Parameters: - server (str): The NNTP server to connect to (e.g., \'news.gmane.io\'). - newsgroup (str): The name of the newsgroup (e.g., \'gmane.comp.python.committers\'). - n (int): The number of recent article subjects to fetch. Returns: - List[str]: A list of subjects of the last `n` articles in the newsgroup. ``` **Constraints:** - Ensure proper connection handling, including closing the connection after fetching the subjects. - Handle potential `NNTPError` exceptions and return an empty list in case of failure. 2. **Function 2: `post_article`** ```python def post_article(server: str, newsgroup: str, article: str) -> str: Connect to the specified NNTP server and post the provided article to the given newsgroup. Parameters: - server (str): The NNTP server to connect to (e.g., \'news.gmane.io\'). - newsgroup (str): The name of the newsgroup (e.g., \'gmane.comp.python.committers\'). - article (str): The content of the article with valid headers. Returns: - str: The server\'s response if the article is posted successfully. ``` **Constraints:** - Ensure proper connection handling, including closing the connection after posting the article. - Handle potential `NNTPReplyError` exceptions and return an appropriate error message. # Example Usage: ```python # Example usage of fetch_article_subjects subjects = fetch_article_subjects(\'news.gmane.io\', \'gmane.comp.python.committers\', 5) for subject in subjects: print(subject) # Example usage of post_article article_content = From: test@example.com Subject: Test posting from Python Newsgroups: gmane.comp.python.committers This is a test article posted from a Python script. response = post_article(\'news.gmane.io\', \'gmane.comp.python.committers\', article_content) print(response) ``` # Notes: - You may use the `nntplib` module as described in the documentation. - Ensure to handle socket timeouts and connection issues gracefully.","solution":"import nntplib from typing import List def fetch_article_subjects(server: str, newsgroup: str, n: int) -> List[str]: Connect to the specified NNTP server, select the given newsgroup, and fetch the subjects of the last `n` articles in that newsgroup. Parameters: - server (str): The NNTP server to connect to (e.g., \'news.gmane.io\'). - newsgroup (str): The name of the newsgroup (e.g., \'gmane.comp.python.committers\'). - n (int): The number of recent article subjects to fetch. Returns: - List[str]: A list of subjects of the last `n` articles in the newsgroup. subjects = [] try: with nntplib.NNTP(server) as client: resp, count, first, last, name = client.group(newsgroup) start = max(int(last) - n + 1, int(first)) resp, overviews = client.over((str(start) + \'-\' + last)) for _, over in overviews: subjects.append(over.get(\'subject\', \'\')) except nntplib.NNTPError: # Handle NNTP related errors return [] except Exception as e: # Handle any other errors return [] return subjects def post_article(server: str, newsgroup: str, article: str) -> str: Connect to the specified NNTP server and post the provided article to the given newsgroup. Parameters: - server (str): The NNTP server to connect to (e.g., \'news.gmane.io\'). - newsgroup (str): The name of the newsgroup (e.g., \'gmane.comp.python.committers\'). - article (str): The content of the article with valid headers. Returns: - str: The server\'s response if the article is posted successfully. try: with nntplib.NNTP(server) as client: resp, info = client.post(article.split(\\"n\\")) except nntplib.NNTPReplyError as e: return f\\"Failed to post article: {e}\\" except nntplib.NNTPError as e: return f\\"NNTP error: {e}\\" except Exception as e: return f\\"An unexpected error occurred: {e}\\" return \\"Article posted successfully\\""},{"question":"# Advanced Coding Challenge: File and Process Management Using `os` Module Objective: Design a Python program that uses various functionalities of the `os` module. Your task is to implement a function that does the following: 1. **Create a New Directory**: Create a directory named `\\"test_os_directory\\"`. 2. **Change Current Working Directory**: Change the working directory to this new directory. 3. **Create and Write to a File**: Within the new directory, create a file named `\\"sample.txt\\"` and write the text `\\"Hello, OS module!\\"` to it. 4. **Read from the File and Display Content**: Read the content of `\\"sample.txt\\"` and print it. 5. **Get Environment Variables**: Retrieve and print the value of the environment variable `HOME` or `USERPROFILE` (depending on the operating system). 6. **Create a Child Process**: Create a child process that runs a simple Python script to list all files in the current directory (i.e., the newly created directory). 7. **Clean Up**: After listing the files, the child process should delete the `\\"sample.txt\\"` file and the parent process should remove the `\\"test_os_directory\\"`. 8. **Handle Exceptions**: Ensure that your function handles and displays any exceptions that occur during the above actions. Input: None Output: None (The function should internally print all messages and handle all operations) Constraints: - Your solution should be compatible with both Unix-based and Windows systems. - Proper exception handling must be in place for operations that might fail (e.g., file or directory already exists, permission errors, etc.). Function Signature: ```python def manage_files_and_processes(): pass ``` Example of Function Execution: ```python manage_files_and_processes() ``` This will create a directory, write to a file, create a child process to list files, clean up the directory and file, and handle any potential exceptions gracefully by printing appropriate error messages. # Hints: - Use `os.mkdir()` and `os.chdir()` to create and change directories. - Use `open()` to handle file operations. - Use `os.environ` to access environment variables. - Use `os.fork()` (Unix) or `os._exit()` (Windows) to manage processes. - Consider using `try-except` blocks to capture and handle exceptions.","solution":"import os import sys import subprocess def manage_files_and_processes(): try: # Step 1: Create a New Directory dir_name = \\"test_os_directory\\" os.mkdir(dir_name) print(f\\"Directory \'{dir_name}\' created.\\") # Step 2: Change Current Working Directory os.chdir(dir_name) print(f\\"Changed working directory to \'{os.getcwd()}\'.\\") # Step 3: Create and Write to a File file_name = \\"sample.txt\\" with open(file_name, \\"w\\") as file: file.write(\\"Hello, OS module!\\") print(f\\"File \'{file_name}\' created and written to.\\") # Step 4: Read from the File and Display Content with open(file_name, \\"r\\") as file: content = file.read() print(f\\"Content of \'{file_name}\': {content}\\") # Step 5: Get Environment Variables home_var = os.environ.get(\'HOME\') or os.environ.get(\'USERPROFILE\') print(f\\"HOME or USERPROFILE environment variable: {home_var}\\") # Step 6: Create a Child Process if os.name == \'posix\': pid = os.fork() if pid == 0: # Child process print(\\"Child process: Listing files in current directory.\\") os.system(\'ls\') # Cleanup: Delete the sample file os.remove(file_name) os._exit(0) else: # Parent process os.wait() # Wait for the child process to finish else: # For Windows, using subprocess to spawn a new process child = subprocess.Popen([sys.executable, \'-c\', \'\'\' import os print(\\"Child process: Listing files in current directory.\\") for f in os.listdir(\\".\\"): print(f) os.remove(\\"sample.txt\\") \'\'\']) child.wait() # Step 7: Remove the Directory os.chdir(\\"..\\") os.rmdir(dir_name) print(f\\"Directory \'{dir_name}\' removed.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s `seaborn.objects` module by creating a customized multi-faceted line plot. **Dataset:** You will use the \\"flights\\" dataset that comes preloaded in seaborn. **Task:** 1. Load the \\"flights\\" dataset using `seaborn.load_dataset`. 2. Create a line plot to visualize the number of passengers over years. 3. Facet the plot by the month, so each subplot shows data for a specific month across years. 4. Customize the lines in the plot to differentiate between years using different line colors. 5. Add suitable titles and labels to the plot and subplots. **Instructions:** - Your solution should define a function `create_flight_plot()` that loads the dataset, creates the plot, and displays it. - **Input:** None - **Output:** Displays the customized line plot as specified. **Constraints:** - You are required to use `seaborn.objects` to create the plot. - Use appropriate customization to make the plot visually appealing and informative. **Performance Requirements:** - Ensure that the plot is efficiently generated and rendered without significant delay. Here\'s a template to get you started: ```python import seaborn.objects as so from seaborn import load_dataset def create_flight_plot(): # Load the dataset flights = load_dataset(\\"flights\\") # Transform the data to fit the plotting requirements # Extract the required components from the dataset flights[\'Year\'] = flights[\'year\'].astype(str) # Create the plot ( so.Plot(flights, x=\\"year\\", y=\\"passengers\\", color=\\"Year\\") .facet(\\"month\\") .add(so.Lines(linewidth=1.5, alpha=0.8)) .scale(color=\\"ch:rot=-.2,light=.7\\") .label(x=\\"Year\\", y=\\"Number of Passengers\\", title=\\"Monthly Passengers Over Years\\") .show() ) ``` **Note:** Ensure your plot is functional and meets the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_flight_plot(): # Load the dataset flights = load_dataset(\\"flights\\") # Transform the data to fit the plotting requirements flights[\'Year\'] = flights[\'year\'].astype(str) # Create the plot ( so.Plot(flights, x=\\"year\\", y=\\"passengers\\", color=\\"Year\\") .facet(\\"month\\") .add(so.Lines(linewidth=1.5, alpha=0.8)) .scale(color=\\"ch:rot=-.2,light=.7\\") .label(x=\\"Year\\", y=\\"Number of Passengers\\", title=\\"Monthly Passengers Over Years\\") .show() )"},{"question":"# Task You are required to write functions that will work with streaming compression and decompression using the `zlib` module in Python. Specifically, you will implement two functions: `stream_compress` and `stream_decompress`. 1. **`stream_compress(data_list, level=-1)`**: - This function should take a list of byte objects `data_list` and an optional compression level `level`. - It will return a single compressed bytes object. - Use the `zlib.compressobj` to handle the streaming compression. 2. **`stream_decompress(compressed_data)`**: - This function should take a single compressed byte object `compressed_data`. - It will return a decompressed bytes object. - Use the `zlib.decompressobj` to handle the streaming decompression. # Details - **Compression Level**: It can range from `0` (no compression) to `9` (maximum compression), with `-1` as the default value (standard compression). - **Edge Cases**: Ensure that both functions can handle: - Empty input for compression. - Truncated or incomplete compressed data for decompression. # Constraints - The `compressed_data` will fit into memory, but `data_list` needs to be processed in chunks due to its size. - The solution should correctly manage the internal buffers and produce valid compressed/decompressed data. # Input Format 1. **`stream_compress(data_list, level=-1)`**: - `data_list`: List of byte objects `[bytes1, bytes2, ..., bytesN]` - `level`: Integer, optional, default is `-1` (compression level) 2. **`stream_decompress(compressed_data)`**: - `compressed_data`: Single bytes object # Output Format 1. **`stream_compress(data_list, level=-1)`**: - Returns a single compressed bytes object 2. **`stream_decompress(compressed_data)`**: - Returns a single decompressed bytes object # Example ```python def stream_compress(data_list, level=-1): import zlib compressor = zlib.compressobj(level) compressed = b\\"\\" for data in data_list: compressed += compressor.compress(data) compressed += compressor.flush() return compressed def stream_decompress(compressed_data): import zlib decompressor = zlib.decompressobj() decompressed = b\\"\\" decompressed += decompressor.decompress(compressed_data) decompressed += decompressor.flush() return decompressed # Example inputs data_list = [b\'Hello \', b\'world! \', b\'This is \', b\'compression/decompression \', b\'test in chunks.\'] compressed_data = stream_compress(data_list, level=6) print(compressed_data) # Should print compressed bytes decompressed_data = stream_decompress(compressed_data) print(decompressed_data) # Should print b\'Hello world! This is compression/decompression test in chunks.\' ``` # Note Ensure to handle various edge cases and validate the correct behavior of your implementation in different scenarios. The functions should use the streaming capabilities of `zlib` to efficiently manage large inputs.","solution":"import zlib def stream_compress(data_list, level=-1): Compresses a list of byte objects using a streaming compression approach. :param data_list: List of byte objects to be compressed. :param level: Compression level (from 0 to 9). Default is -1. :return: A single compressed bytes object. compressor = zlib.compressobj(level) compressed = b\\"\\" for data in data_list: compressed += compressor.compress(data) compressed += compressor.flush() return compressed def stream_decompress(compressed_data): Decompresses a bytes object using a streaming decompression approach. :param compressed_data: Compressed bytes object. :return: A single decompressed bytes object. decompressor = zlib.decompressobj() decompressed = b\\"\\" decompressed += decompressor.decompress(compressed_data) decompressed += decompressor.flush() return decompressed"},{"question":"# Question: Custom Autograd Function in PyTorch Implement a custom autograd function in PyTorch to compute the forward and backward passes of the following operation: [ f(x) = (x^3 + 2)^2 ] # Requirements: 1. Your implementation should subclass `torch.autograd.Function` and override the `forward` and `backward` methods. 2. In the `forward` method, save any necessary tensors for the backward computation using the context object. 3. In the `backward` method, compute the gradient of the input tensor with respect to some loss. # Implementation Details: Input: - A single input tensor `x` of any shape with `requires_grad=True`. Output: - A tensor `y` with the same shape as `x` representing the functional result of ( f(x) ). - The backward method should return the gradients of `y` with respect to `x`. # Constraints: - The function should handle tensors with gradients accurately. - Ensure no in-place operations are performed that may interfere with gradient computation. # Example: ```python import torch # Define the custom function class CustomFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Implement forward pass pass @staticmethod def backward(ctx, grad_output): # Implement backward pass pass # Example usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = CustomFunction.apply(x) # Compute gradients y.sum().backward() # Print gradients print(x.grad) ``` # Expected Output: When forward pass is correctly implemented and `backward` computes gradients, running the example should print gradients, verifying the correctness of your implementation.","solution":"import torch class CustomFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Compute f(x) = (x^3 + 2)^2 ctx.save_for_backward(input) result = (input ** 3 + 2) ** 2 return result @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors # Calculate the gradient of (x^3 + 2)^2 with respect to x grad_input = grad_output * 2 * (input ** 3 + 2) * 3 * input ** 2 return grad_input # Example usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = CustomFunction.apply(x) # Compute gradients y.sum().backward() # Print gradients print(x.grad)"},{"question":"Coding Assessment Question # Objective The goal of this assessment is to measure your understanding of Scikit-learn\'s linear regression models, data preprocessing, cross-validation, and model evaluation processes. # Question You are provided with a dataset that contains features about used cars and their prices. Your task is to: 1. Preprocess the dataset by handling missing values and scaling features. 2. Implement and train a linear regression model using Ridge regression. 3. Perform hyperparameter tuning using cross-validation to select the optimal value of the regularization parameter `alpha`. 4. Evaluate the model\'s performance using mean squared error (MSE) on a test set. # Dataset The dataset `used_cars.csv` has the following columns: - `year`: Year of the car - `mileage`: Mileage of the car - `price`: Price of the car (target variable) - `brand`: Brand of the car - `condition`: Condition of the car (Good, Fair, Poor) - `color`: Color of the car # Input Format You will be provided with the dataset in the form of a `pandas.DataFrame`. # Output Format Your code should print the following: 1. The optimal value of `alpha`. 2. The mean squared error (MSE) on the test set. # Detailed Steps 1. **Preprocessing**: - Handle missing values by filling them with the mean value for numerical columns and the most frequent value for categorical columns. - Encode categorical columns using one-hot encoding. - Split the data into training and testing sets (80% training, 20% testing). - Standardize the numerical features using `StandardScaler`. 2. **Model Training**: - Implement Ridge regression using Scikit-learn\'s `Ridge` class. - Perform cross-validation to find the optimal value of `alpha` (use `RidgeCV`). 3. **Model Evaluation**: - After finding the best `alpha`, retrain the model on the entire training set and evaluate it on the test set using MSE. # Constraints - Use Scikit-learn for model implementation, preprocessing, and evaluation. - Assume the dataset is clean, and no further outlier removal or feature engineering is needed beyond the specified preprocessing steps. # Example Input: `used_cars.csv` ``` year,mileage,price,brand,condition,color 2015,50000,15000,Toyota,Good,Red 2012,80000,10000,Toyota,Fair,Blue 2020,30000,20000,Ford,Good,Black 2017,60000,13000,Honda,Poor,White ... ``` Output: ``` Optimal alpha: 10.0 Mean Squared Error on the test set: 75000.0 ``` ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error # Load dataset data = pd.read_csv(\'used_cars.csv\') # Separate features and target variable X = data.drop(columns=[\'price\']) y = data[\'price\'] # Preprocess data numeric_features = [\'year\', \'mileage\'] categorical_features = [\'brand\', \'condition\', \'color\'] numeric_transformer = Pipeline(steps=[ (\'scaler\', StandardScaler())]) categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\'))]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features)]) # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create preprocessing and training pipeline model_pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0, 1000.0])) ]) # Train the model model_pipeline.fit(X_train, y_train) # Predict and evaluate y_pred = model_pipeline.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Output results print(f\'Optimal alpha: {model_pipeline.named_steps[\\"regressor\\"].alpha_}\') print(f\'Mean Squared Error on the test set: {mse}\') ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error def preprocess_and_train(data): # Separate features and target variable X = data.drop(columns=[\'price\']) y = data[\'price\'] # Preprocess data numeric_features = [\'year\', \'mileage\'] categorical_features = [\'brand\', \'condition\', \'color\'] numeric_transformer = Pipeline(steps=[ (\'scaler\', StandardScaler())]) categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\'))]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features)]) # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create preprocessing and training pipeline model_pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0, 1000.0])) ]) # Train the model model_pipeline.fit(X_train, y_train) # Predict and evaluate y_pred = model_pipeline.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Output results optimal_alpha = model_pipeline.named_steps[\\"regressor\\"].alpha_ return optimal_alpha, mse"},{"question":"# Advanced Coding Assessment Question: Custom Exception Handling Objective This question tests your understanding of Python\'s built-in exceptions, exception handling, and creating custom exceptions by subclassing built-in exceptions. Problem Statement You are required to implement a class `CustomCalculator` that performs basic arithmetic operations: addition, subtraction, multiplication, and division. The class should also handle possible built-in exceptions and custom exceptions effectively. Specifically, you need to create custom exceptions to manage invalid operations and unexpected behaviors. Requirements 1. Implement the `CustomCalculator` class with methods: - `add(a, b)` - Returns the sum of `a` and `b`. - `subtract(a, b)` - Returns the difference when `a` is subtracted from `b`. - `multiply(a, b)` - Returns the product of `a` and `b`. - `divide(a, b)` - Returns the quotient when `a` is divided by `b`. 2. You must handle the following exceptions in each method: - `TypeError` - Raised when the inputs `a` or `b` are not numbers. - `ZeroDivisionError` - Raised when dividing by zero in the `divide` method. 3. Create and use custom exceptions: - `InvalidOperationError` - Raised when attempting an invalid arithmetic operation. This should be a subclass of `Exception`. - `UnexpectedError` - Raised for any other unexpected situations. This should be a subclass of `Exception`. 4. Each method should: - Raise the appropriate built-in exception if the inputs are invalid. - Raise `InvalidOperationError` if an arithmetic operation is invalid. - Raise `UnexpectedError` if any other unexpected exception occurs. - Return a meaningful error message using the `__str__` method of the exception classes. Input and Output Format - Methods will take two parameters `a` and `b`, which can be `int` or `float`. - Both the methods and the custom exceptions should have a meaningful string representation. Example ```python class CustomCalculator: # Implement the required methods and exception handling here # Test cases calc = CustomCalculator() # Valid operations assert calc.add(5, 3) == 8 assert calc.subtract(5, 3) == 2 assert calc.multiply(5, 3) == 15 assert calc.divide(6, 3) == 2.0 # Exception handling try: calc.divide(5, 0) except ZeroDivisionError as e: print(e) # Should output: Division by zero is not allowed. try: calc.add(5, \'a\') except TypeError as e: print(e) # Should output: Both arguments must be numbers. try: raise InvalidOperationError(\\"Invalid operation attempted.\\") except InvalidOperationError as e: print(e) # Should output: Invalid operation attempted. try: raise UnexpectedError(\\"An unexpected error occurred.\\") except UnexpectedError as e: print(e) # Should output: An unexpected error occurred. ``` Constraints - Only use built-in Python modules. - Handle exceptions effectively and provide informative messages. Now, implement the `CustomCalculator` class along with the required custom exceptions.","solution":"class InvalidOperationError(Exception): def __init__(self, message=\\"Invalid operation attempted.\\"): self.message = message super().__init__(self.message) def __str__(self): return self.message class UnexpectedError(Exception): def __init__(self, message=\\"An unexpected error occurred.\\"): self.message = message super().__init__(self.message) def __str__(self): return self.message class CustomCalculator: def add(self, a, b): try: if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\\"Both arguments must be numbers.\\") return a + b except TypeError as e: raise e except Exception as e: raise UnexpectedError(\\"An unexpected error occurred.\\") from e def subtract(self, a, b): try: if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\\"Both arguments must be numbers.\\") return a - b except TypeError as e: raise e except Exception as e: raise UnexpectedError(\\"An unexpected error occurred.\\") from e def multiply(self, a, b): try: if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\\"Both arguments must be numbers.\\") return a * b except TypeError as e: raise e except Exception as e: raise UnexpectedError(\\"An unexpected error occurred.\\") from e def divide(self, a, b): try: if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\\"Both arguments must be numbers.\\") if b == 0: raise ZeroDivisionError(\\"Division by zero is not allowed.\\") return a / b except TypeError as e: raise e except ZeroDivisionError as e: raise e except Exception as e: raise UnexpectedError(\\"An unexpected error occurred.\\") from e"},{"question":"**Objective:** To assess students\' understanding of Python\'s control flow, data structures, and object-oriented programming through the development of a command-line utility that manages a library of books. # Problem Statement: Design and implement a Python program that provides a command-line interface (CLI) for users to manage a library of books. The program should support the following functionalities: 1. **Add a Book**: Users can add a new book to the library. 2. **Remove a Book**: Users can remove a book from the library. 3. **Search for a Book**: Users can search for a book by its title or author. 4. **List All Books**: Users can list all books currently in the library. # Requirements: 1. **Book Information**: Each book should have the following attributes: - `title`: a string representing the title of the book. - `author`: a string representing the author’s name. - `year`: an integer representing the year of publication. 2. **User Interface**: - The program should display a simple menu with options to add, remove, search, and list books. - Each option should invoke the appropriate functionality. 3. **Class Definitions**: - Create a class `Book` to model the book entities. - Create a class `Library` to model the collection of books and include methods for the required functionalities. 4. **File-Based Storage**: - On startup, the program should load the library data from a file. - On exit, the program should save the library data back to the file. # Function Definitions: Implement the following methods in the `Library` class: 1. `add_book(self, book: Book) -> None`: Adds a new book to the library. 2. `remove_book(self, title: str) -> bool`: Removes a book by its title. Returns `True` if the book was found and removed, otherwise `False`. 3. `search_book(self, search_term: str) -> List[Book]`: Searches for books by title or author and returns a list of matching books. 4. `list_books(self) -> List[Book]`: Returns a list of all books in the library. 5. `load_books(self, filename: str) -> None`: Loads books from a file. 6. `save_books(self, filename: str) -> None`: Saves books to a file. # Constraints: 1. The title and author of each book should be non-empty strings. 2. The year of publication should be a non-negative integer. # Example Usage: ```python # Create a command-line library management application library = Library() library.load_books(\\"library_data.txt\\") while True: print(\\"Library Management System\\") print(\\"1. Add a Book\\") print(\\"2. Remove a Book\\") print(\\"3. Search for a Book\\") print(\\"4. List All Books\\") print(\\"5. Exit\\") choice = input(\\"Enter your choice: \\") if choice == \\"1\\": title = input(\\"Enter the title of the book: \\") author = input(\\"Enter the author of the book: \\") year = int(input(\\"Enter the year of publication: \\")) book = Book(title, author, year) library.add_book(book) elif choice == \\"2\\": title = input(\\"Enter the title of the book to remove: \\") if library.remove_book(title): print(f\\"Book titled \'{title}\' removed successfully.\\") else: print(f\\"Book titled \'{title}\' not found.\\") elif choice == \\"3\\": search_term = input(\\"Enter the title or author to search: \\") results = library.search_book(search_term) if results: for book in results: print(f\\"Title: {book.title}, Author: {book.author}, Year: {book.year}\\") else: print(\\"No books found.\\") elif choice == \\"4\\": books = library.list_books() for book in books: print(f\\"Title: {book.title}, Author: {book.author}, Year: {book.year}\\") elif choice == \\"5\\": library.save_books(\\"library_data.txt\\") print(\\"Library data saved. Exiting...\\") break else: print(\\"Invalid choice. Please try again.\\") ``` **Note**: Ensure proper error handling and input validation in your implementation.","solution":"import json from typing import List class Book: def __init__(self, title: str, author: str, year: int): if not title or not author or year < 0: raise ValueError(\\"Invalid book information\\") self.title = title self.author = author self.year = year def __repr__(self): return f\\"Book(title={self.title}, author={self.author}, year={self.year})\\" class Library: def __init__(self): self.books = [] def add_book(self, book: Book) -> None: self.books.append(book) def remove_book(self, title: str) -> bool: for book in self.books: if book.title == title: self.books.remove(book) return True return False def search_book(self, search_term: str) -> List[Book]: return [book for book in self.books if search_term.lower() in book.title.lower() or search_term.lower() in book.author.lower()] def list_books(self) -> List[Book]: return self.books def load_books(self, filename: str) -> None: try: with open(filename, \'r\') as file: books_data = json.load(file) self.books = [Book(**data) for data in books_data] except FileNotFoundError: self.books = [] def save_books(self, filename: str) -> None: with open(filename, \'w\') as file: json.dump([book.__dict__ for book in self.books], file)"},{"question":"Objective: Demonstrate your understanding of PyTorch\'s Tensor Parallelism by parallelizing a custom neural network module and training it. Problem Statement: You are required to: 1. Implement a custom neural network module, specifically a simple multi-layer perceptron (MLP) with 3 layers. 2. Parallelize this module using both `ColwiseParallel` and `RowwiseParallel`. 3. Train this parallelized module on a provided dataset using Distributed Data Parallel (DDP). Specifications: 1. **Model Implementation:** - The MLP should have 3 linear layers. - Use ReLU activation function between layers. 2. **Parallelization:** - Use `ColwiseParallel` for the first linear layer. - Use `RowwiseParallel` for the second linear layer. - Assume the input and output tensors are evenly sharded. 3. **Training:** - Use the provided dataset (you may create a dummy dataset for simulation). - Optimize the model using stochastic gradient descent (SGD). - Define a simple loss function (e.g., mean squared error). Input & Output Formats: - **Input:** No input as this will be script-based, but you should create a dummy dataset within your code. - **Output:** Print the loss value after each training epoch to monitor convergence. Constraints: - Use a minimum of 2 GPUs for parallelism. - Assume PyTorch 1.10+ and necessary dependencies are available. - Ensure that the model training converges within a reasonable number of epochs (e.g., 50). Performance Requirements: - The solution should demonstrate efficient use of parallelism, evident through reduced training time and convergence of the loss value. Sample Code Structure: ```python import torch import torch.nn as nn import torch.optim as optim from torch.distributed.tensor.parallel import ( ColwiseParallel, RowwiseParallel, parallelize_module, loss_parallel ) from torch.nn.parallel import DistributedDataParallel as DDP class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.fc1 = nn.Linear(128, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 64) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x def create_dataset(): # Create dummy dataset pass def main(): # Initialize the distributed environment. torch.distributed.init_process_group(backend=\'nccl\') # Model definition model = MLP() # Parallelize the model model = parallelize_module(model, parallelize_plan=plan) model = DDP(model) # Dataset and DataLoader creation dataset = create_dataset() dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) # Loss and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(50): for data in dataloader: inputs, targets = data optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\'Epoch [{epoch + 1}/50], Loss: {loss.item()}\') if __name__ == \\"__main__\\": main() ``` Additional Information: - You may modify the provided dataset creation and training loop to better suit your implementation. - Ensure error handling for distributed training.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.distributed import init_process_group from torch.nn.parallel import DistributedDataParallel as DDP # Use hypothetical imports for pytorch parallelism as instructed from torch.distributed.tensor.parallel import ( ColwiseParallel, RowwiseParallel, parallelize_module, loss_parallel ) class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.fc1 = nn.Linear(128, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 64) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x def create_dataset(num_samples=1000): # Create dummy dataset with random inputs and targets inputs = torch.randn(num_samples, 128) targets = torch.randn(num_samples, 64) dataset = torch.utils.data.TensorDataset(inputs, targets) return dataset def main(): # Initialize the distributed environment. init_process_group(backend=\'nccl\') # Model definition model = MLP() # Parallelize the model model.fc1 = ColwiseParallel(model.fc1) model.fc2 = RowwiseParallel(model.fc2) model = DDP(model) # Dataset and DataLoader creation dataset = create_dataset() dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) # Loss and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(50): for data in dataloader: inputs, targets = data optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\'Epoch [{epoch + 1}/50], Loss: {loss.item()}\') if __name__ == \\"__main__\\": main()"},{"question":"Coding Assessment Question: Memory Leak Detection # Objective: Write a Python function `detect_memory_leak` that uses the `tracemalloc` module to detect and report potential memory leaks in a given code block. The function should: 1. Start tracing memory allocations. 2. Run the given code block. 3. Take a memory snapshot before and after the code execution. 4. Compare the snapshots to identify memory differences. 5. Return a formatted string report of the top memory-consuming lines of code and their respective memory usage increase. # Function Signature: ```python def detect_memory_leak(code_block: str) -> str: pass ``` # Input: - `code_block` (str): A string representing a Python code block to be executed and monitored for memory leaks. # Output: - Returns a formatted string report detailing the top 10 lines of code with the highest increase in memory usage, including the file name, line number, and memory usage increase in KiB. # Constraints: - The function should handle any syntactically correct Python code block. - Focus on ensuring that the returned string is well-formatted for readability. # Example Usage: ```python code = \'\'\' lst = [] for i in range(10000): lst.append(i) del lst \'\'\' print(detect_memory_leak(code)) ``` # Expected Output: ``` [ Top 10 differences ] example.py:3: size=..., count=..., average=... example.py:4: size=..., count=..., average=... ... ``` # Notes: - You can assume that the provided code block will be saved in a temporary file named `example.py` to facilitate line tracing. - Use the `tracemalloc` capabilities to filter and present the most relevant information clearly. - Ensure proper memory cleanup by stopping the `tracemalloc` module after the analysis. # Hints: - Use the `exec` function to execute the `code_block` dynamically within the function. - Utilize `tracemalloc.take_snapshot()` before and after code execution to capture memory state differences. - Format the output to provide a clear and concise report of memory usage changes.","solution":"import tracemalloc def detect_memory_leak(code_block: str) -> str: def execute_code(code: str): exec(code, globals()) tracemalloc.start() snapshot_before = tracemalloc.take_snapshot() execute_code(code_block) snapshot_after = tracemalloc.take_snapshot() tracemalloc.stop() top_stats = snapshot_after.compare_to(snapshot_before, \'lineno\') lines = [\\"[ Top 10 differences ]\\"] for stat in top_stats[:10]: lines.append(f\\"{stat.traceback.format()}: size={stat.size / 1024:.1f} KiB, count={stat.count}\\") return \\"n\\".join(lines)"},{"question":"You are provided with a dataset containing sales data for different products across several stores. This dataset needs to be analyzed to provide meaningful insights into the sales patterns. Write a function using pandas that performs the following tasks: # Function Signature ```python def analyze_sales_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Input - `df`: A pandas DataFrame containing the sales data with the following columns: - `StoreID`: Identifier for each store (integer) - `ProductID`: Identifier for each product (integer) - `Date`: The date of the sale (datetime) - `Sales`: The sales amount (float) # Tasks to Perform 1. Group the data by `StoreID` and then by `ProductID`. 2. For each product in each store, calculate the following: - Total sales. - Mean sales. - Median sales. - Standard deviation of sales. - Number of unique sales dates. 3. Return the result as a DataFrame with a hierarchical index (first level: `StoreID`, second level: `ProductID`) and the calculated metrics as columns. # Output - The function should return a pandas DataFrame with the following columns: - `TotalSales` - `MeanSales` - `MedianSales` - `StdDevSales` - `UniqueSalesDates` The DataFrame index should be hierarchical, with `StoreID` and `ProductID`. # Example Given the sample DataFrame `df`: ```python import pandas as pd data = { \'StoreID\': [1, 1, 1, 2, 2, 2, 2], \'ProductID\': [101, 101, 102, 101, 101, 103, 104], \'Date\': pd.to_datetime([\'2021-01-01\', \'2021-01-02\', \'2021-01-01\', \'2021-01-01\', \'2021-01-02\', \'2021-01-01\', \'2021-01-03\']), \'Sales\': [100, 200, 300, 150, 250, 350, 450] } df = pd.DataFrame(data) ``` The output for `analyze_sales_data(df)` should be: ```python TotalSales MeanSales MedianSales StdDevSales UniqueSalesDates StoreID ProductID 1 101 300 150.0 150.0 70.710678 2 102 300 300.0 300.0 NaN 1 2 101 400 200.0 200.0 70.710678 2 103 350 350.0 350.0 NaN 1 104 450 450.0 450.0 NaN 1 ``` Make sure your solution is optimized for performance and handles large datasets efficiently.","solution":"import pandas as pd def analyze_sales_data(df: pd.DataFrame) -> pd.DataFrame: Analyze the sales data. Parameters: df (pd.DataFrame): DataFrame containing sales data with \'StoreID\', \'ProductID\', \'Date\', and \'Sales\' columns. Returns: pd.DataFrame: DataFrame with hierarchical index (StoreID, ProductID) and columns \'TotalSales\', \'MeanSales\', \'MedianSales\', \'StdDevSales\', and \'UniqueSalesDates\'. grouped = df.groupby([\'StoreID\', \'ProductID\']) result = grouped.agg( TotalSales=(\'Sales\', \'sum\'), MeanSales=(\'Sales\', \'mean\'), MedianSales=(\'Sales\', \'median\'), StdDevSales=(\'Sales\', \'std\'), UniqueSalesDates=(\'Date\', \'nunique\') ).reset_index() result = result.set_index([\'StoreID\', \'ProductID\']) return result"},{"question":"In this assessment, you will be working with Python\'s `datetime` module to create and manipulate date and time objects. Your task is to implement a function that meets the following requirements: # Function Name `process_datetime_records` # Input A list of dictionaries, where each dictionary contains information about an event. Each dictionary will have the following keys: - `year` (int): The year of the event. - `month` (int): The month of the event. - `day` (int): The day of the event. - `hour` (int): The hour of the event. - `minute` (int): The minute of the event. - `second` (int): The second of the event. - `microsecond` (int, optional): The microsecond of the event. Default is 0 if not provided. - `tz_offset_seconds` (int, optional): The time zone offset in seconds. Default is 0 if not provided. # Output A list of tuples, where each tuple contains: 1. The `datetime` object representing the event in UTC (use the provided `tz_offset_seconds` to adjust). 2. A string representing the date and time in the format `YYYY-MM-DD HH:MM:SS` in UTC. # Constraints - The input list can have up to 10,000 dictionary entries. - All time zone offsets will be within the range of -86400 to 86400 seconds (i.e., ±24 hours). # Example ```python input_data = [ {\\"year\\": 2023, \\"month\\": 5, \\"day\\": 17, \\"hour\\": 14, \\"minute\\": 30, \\"second\\": 45, \\"tz_offset_seconds\\": -3600}, {\\"year\\": 2022, \\"month\\": 12, \\"day\\": 1, \\"hour\\": 8, \\"minute\\": 15, \\"second\\": 0, \\"tz_offset_seconds\\": 7200} ] output = process_datetime_records(input_data) ``` The expected output will be: 1. For the first event, the datetime object adjusted to UTC. 2. For the second event, the datetime object adjusted to UTC. ```python [ (datetime.datetime(2023, 5, 17, 15, 30, 45, tzinfo=datetime.timezone.utc), \\"2023-05-17 15:30:45\\"), (datetime.datetime(2022, 12, 1, 6, 15, 0, tzinfo=datetime.timezone.utc), \\"2022-12-01 06:15:00\\") ] ``` # Notes - Use the provided `datetime` module macros and methods to create date and time objects efficiently. - Ensure that all time calculations are accurate and handle edge cases for time zone offsets.","solution":"from datetime import datetime, timedelta, timezone def process_datetime_records(records): Processes a list of event records to convert local times to UTC and format as specified. Args: records (list): A list of dictionaries, where each dictionary contains information about an event\'s date, time, and timezone offset. Returns: list of tuples: Each tuple contains a datetime object in UTC and a formatted string \'YYYY-MM-DD HH:MM:SS\' in UTC. result = [] for record in records: year = record[\'year\'] month = record[\'month\'] day = record[\'day\'] hour = record[\'hour\'] minute = record[\'minute\'] second = record[\'second\'] microsecond = record.get(\'microsecond\', 0) tz_offset_seconds = record.get(\'tz_offset_seconds\', 0) # Create the local datetime object local_dt = datetime(year, month, day, hour, minute, second, microsecond) # Adjust the UTC offset utc_offset = timedelta(seconds=tz_offset_seconds) local_dt_with_tz = local_dt.replace(tzinfo=timezone(utc_offset)) # Convert to UTC utc_dt = local_dt_with_tz.astimezone(timezone.utc) # Create the formatted string utc_str = utc_dt.strftime(\'%Y-%m-%d %H:%M:%S\') result.append((utc_dt, utc_str)) return result"},{"question":"You are tasked with developing a simple file compression utility using Python\'s `zlib` module. Your utility will take a text file as input, compress its contents, and then decompress it to verify the integrity of the compression process. The solution should handle various compression levels and check if the decompressed content matches the original. # Requirement 1. **Input**: - A text file (`input.txt`) containing a string. - **Compression level (optional)**: An integer `level` indicating the compression level (0-9, with a default of -1). 2. **Output**: - A compressed file (`compressed.zlib`) containing the compressed data. - A decompressed file (`output.txt`) containing the data decompressed from the compressed file. 3. **Function Implementations**: - `compress_file(input_file: str, output_file: str, level: int = -1) -> None`: Compress the content of `input_file` and save to `output_file` using the given compression level. - `decompress_file(input_file: str, output_file: str) -> None`: Decompress the content of `input_file` (which is a `.zlib` file) and save to `output_file`. # Constraints - The functions should handle errors gracefully and raise `zlib.error` if encountered during compression or decompression. - The decompressed content must match the original content of `input.txt`. # Example Suppose we have a file `input.txt` with the following content: ``` Hello, this is a sample text file for compression. ``` Running the function `compress_file(\'input.txt\', \'compressed.zlib\', level=6)` should create a `compressed.zlib` file with compressed content. Then, running the function `decompress_file(\'compressed.zlib\', \'output.txt\')` should create an `output.txt` file with the same content as `input.txt`. # Code Template ```python import zlib def compress_file(input_file: str, output_file: str, level: int = -1) -> None: Compresses the content of input_file and saves it to output_file. Args: input_file (str): Path to the input text file. output_file (str): Path to save the compressed file. level (int): Compression level (0-9), default is -1. Raises: zlib.error: If there is an error during compression. try: with open(input_file, \'rb\') as f_in: data = f_in.read() compressed_data = zlib.compress(data, level) with open(output_file, \'wb\') as f_out: f_out.write(compressed_data) except zlib.error as e: raise zlib.error(\\"Compression error: \\" + str(e)) def decompress_file(input_file: str, output_file: str) -> None: Decompresses the content of input_file and saves it to output_file. Args: input_file (str): Path to the compressed file. output_file (str): Path to save the decompressed file. Raises: zlib.error: If there is an error during decompression. try: with open(input_file, \'rb\') as f_in: compressed_data = f_in.read() decompressed_data = zlib.decompress(compressed_data) with open(output_file, \'wb\') as f_out: f_out.write(decompressed_data) except zlib.error as e: raise zlib.error(\\"Decompression error: \\" + str(e)) # Example usage compress_file(\'input.txt\', \'compressed.zlib\', level=6) decompress_file(\'compressed.zlib\', \'output.txt\') ``` # Testing Ensure to test your implementation with various text files and different compression levels to verify the correctness and robustness of the utility.","solution":"import zlib def compress_file(input_file: str, output_file: str, level: int = -1) -> None: Compresses the content of input_file and saves it to output_file. Args: input_file (str): Path to the input text file. output_file (str): Path to save the compressed file. level (int): Compression level (0-9), default is -1. Raises: zlib.error: If there is an error during compression. try: with open(input_file, \'rb\') as f_in: data = f_in.read() compressed_data = zlib.compress(data, level) with open(output_file, \'wb\') as f_out: f_out.write(compressed_data) except zlib.error as e: raise zlib.error(\\"Compression error: \\" + str(e)) def decompress_file(input_file: str, output_file: str) -> None: Decompresses the content of input_file and saves it to output_file. Args: input_file (str): Path to the compressed file. output_file (str): Path to save the decompressed file. Raises: zlib.error: If there is an error during decompression. try: with open(input_file, \'rb\') as f_in: compressed_data = f_in.read() decompressed_data = zlib.decompress(compressed_data) with open(output_file, \'wb\') as f_out: f_out.write(decompressed_data) except zlib.error as e: raise zlib.error(\\"Decompression error: \\" + str(e))"},{"question":"# Question: Visualizing Data with Custom Themes in Seaborn You are provided with a dataset containing sales data for different products over several months. You need to visualize this dataset using seaborn and matplotlib in different styles and themes. Specifically, you need to create multiple bar plots, each with different visual customizations. Your task is to write a function called `visualize_sales_data` that accomplishes the following: 1. Create a bar plot that uses the default seaborn theme. 2. Create a bar plot with the \\"whitegrid\\" style and the \\"pastel\\" palette. 3. Create a bar plot with the \\"ticks\\" style and customized parameters that hide the top and right spines. 4. Each plot should be saved as a PNG file with appropriate filenames. # Dataset Format The dataset is provided in the following format (as a list of dictionaries): ```python data = [ {\\"Product\\": \\"A\\", \\"Month\\": \\"Jan\\", \\"Sales\\": 150}, {\\"Product\\": \\"A\\", \\"Month\\": \\"Feb\\", \\"Sales\\": 200}, {\\"Product\\": \\"B\\", \\"Month\\": \\"Jan\\", \\"Sales\\": 100}, {\\"Product\\": \\"B\\", \\"Month\\": \\"Feb\\", \\"Sales\\": 230}, {\\"Product\\": \\"C\\", \\"Month\\": \\"Jan\\", \\"Sales\\": 90}, {\\"Product\\": \\"C\\", \\"Month\\": \\"Feb\\", \\"Sales\\": 120} ] ``` # Implementation Details Your function should perform the following steps: 1. Convert the list of dictionaries into a Pandas DataFrame. 2. Create a bar plot with the default seaborn theme showing total sales for each product. 3. Create a second bar plot using the \\"whitegrid\\" style and \\"pastel\\" palette, showing total sales for each product. 4. Create a third bar plot using the \\"ticks\\" style with the right and top spines removed, showing total sales for each product. 5. Save each plot as a PNG file: - The first plot should be saved as \\"default_theme.png\\". - The second plot should be saved as \\"whitegrid_pastel.png\\". - The third plot should be saved as \\"ticks_custom.png\\". # Function Signature ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales_data(data: list[dict]) -> None: # Your code here ``` # Constraints - You must use the seaborn and matplotlib packages. - Ensure that the plots are clear and appropriately labeled. # Example Usage ```python data = [ {\\"Product\\": \\"A\\", \\"Month\\": \\"Jan\\", \\"Sales\\": 150}, {\\"Product\\": \\"A\\", \\"Month\\": \\"Feb\\", \\"Sales\\": 200}, {\\"Product\\": \\"B\\", \\"Month\\": \\"Jan\\", \\"Sales\\": 100}, {\\"Product\\": \\"B\\", \\"Month\\": \\"Feb\\", \\"Sales\\": 230}, {\\"Product\\": \\"C\\", \\"Month\\": \\"Jan\\", \\"Sales\\": 90}, {\\"Product\\": \\"C\\", \\"Month\\": \\"Feb\\", \\"Sales\\": 120} ] visualize_sales_data(data) ``` Upon running the example, three images should be saved in the current directory with the filenames specified above.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales_data(data: list[dict]) -> None: # Convert the list of dictionaries into a Pandas DataFrame df = pd.DataFrame(data) # Group by product and sum sales grouped_data = df.groupby(\\"Product\\")[\\"Sales\\"].sum().reset_index() # Plot 1: Default seaborn theme plt.figure(figsize=(8, 6)) sns.barplot(x=\\"Product\\", y=\\"Sales\\", data=grouped_data) plt.title(\\"Total Sales by Product (Default Theme)\\") plt.savefig(\\"default_theme.png\\") plt.close() # Plot 2: whitegrid style with pastel palette sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") plt.figure(figsize=(8, 6)) sns.barplot(x=\\"Product\\", y=\\"Sales\\", data=grouped_data) plt.title(\\"Total Sales by Product (Whitegrid & Pastel)\\") plt.savefig(\\"whitegrid_pastel.png\\") plt.close() # Plot 3: ticks style with right and top spines removed sns.set_theme(style=\\"ticks\\") plt.figure(figsize=(8, 6)) ax = sns.barplot(x=\\"Product\\", y=\\"Sales\\", data=grouped_data) sns.despine(ax=ax, top=True, right=True) plt.title(\\"Total Sales by Product (Ticks Custom)\\") plt.savefig(\\"ticks_custom.png\\") plt.close()"},{"question":"**Coding Assessment Question** # Objective Design and implement a function to read and extract specific data from a file with chunked data using the provided `Chunk` class. This will assess your comprehension of working with binary data, handling file I/O, and leveraging classes and methods to process specific formats. # Question You are given a file containing chunked data in EA IFF 85 format. Each chunk has a 4-byte ID, 4-byte size (big-endian), and corresponding data. Your task is to implement a function `extract_chunks(file_path, chunk_ids)` that reads through the file and extracts data from chunks with specific IDs. Function Signature ```python def extract_chunks(file_path: str, chunk_ids: list) -> dict: ``` Input - `file_path`: A string representing the path to the file. - `chunk_ids`: A list of strings, each representing a chunk ID to extract. Output - Returns a dictionary where the keys are chunk IDs (from `chunk_ids` list) and the values are the corresponding chunk data as bytes. Constraints - The file specified by `file_path` will be in EA IFF 85 chunked format. - Each chunk ID in `chunk_ids` will be a 4-byte string. - The file can be large, so the function should be efficient in reading only necessary data. Example Given a file `example.iff` with the following chunks: - ID: \'ABCD\', Size: 10 bytes - ID: \'1234\', Size: 20 bytes - ID: \'WXYZ\', Size: 5 bytes ```python chunk_ids = [\'ABCD\', \'WXYZ\'] result = extract_chunks(\'example.iff\', chunk_ids) ``` Output: ```python { \'ABCD\': b\'...10 bytes of data...\', \'WXYZ\': b\'...5 bytes of data...\' } ``` Requirements - Utilize the `Chunk` class provided in the documentation. - Handle chunk alignment and endianness as specified by the class default settings. - Implement error handling in case of invalid file format or chunk IDs not found. # Note You do not need to provide the content of `example.iff` file. Focus on writing the function based on the given interface.","solution":"import struct def extract_chunks(file_path, chunk_ids): Extracts data from specified chunk IDs in EA IFF 85 format file. Args: - file_path (str): Path to the EA IFF 85 file. - chunk_ids (list): List of 4-byte chunk ID strings to extract. Returns: - dict: A dictionary with chunk IDs as keys and corresponding chunk data as values. chunk_data = {} with open(file_path, \'rb\') as file: while True: chunk_header = file.read(8) if len(chunk_header) < 8: break # EOF reached chunk_id, chunk_size = struct.unpack(\'>4sI\', chunk_header) chunk_id = chunk_id.decode(\'ascii\') if chunk_id in chunk_ids: data = file.read(chunk_size) chunk_data[chunk_id] = data else: file.seek(chunk_size, 1) # Skip the chunk data return chunk_data"},{"question":"Implementing and Evaluating Clustering with Scikit-learn **Objective:** Demonstrate your understanding of clustering using scikit-learn by implementing a clustering method, performing the clustering on a dataset, and evaluating its performance. **Question:** You are provided with a dataset containing numerical data points. Your task is to perform clustering using the KMeans algorithm from the scikit-learn library and evaluate the results using appropriate metrics. **Requirements:** 1. **Input:** - A 2D NumPy array `X` of shape (n_samples, n_features) containing the dataset. - An integer `n_clusters` specifying the number of clusters to form. 2. **Output:** - A tuple `(labels, inertia, silhouette_score)` where: - `labels`: A 1D array of cluster labels assigned to each sample in the dataset. - `inertia`: The sum of squared distances of samples to their closest cluster center (a float value). - `silhouette_score`: The silhouette score for the clustering performed (a float value). 3. **Constraints:** - You must use the `KMeans` class from scikit-learn\'s `cluster` module. - The silhouette score should be computed using scikit-learn\'s `metrics` module. 4. **Performance Requirements:** - Ensure your implementation is efficient and leverages scikit-learn\'s built-in methods. **Instructions:** Implement a function `perform_clustering(X, n_clusters)` that performs the following steps: 1. Use the `KMeans` class from scikit-learn to perform KMeans clustering on the input dataset `X` with the specified number of clusters `n_clusters`. 2. Extract the cluster labels and compute the inertia. 3. Compute the silhouette score for the resulting clustering. 4. Return the labels, inertia, and silhouette score as a tuple. **Example:** ```python import numpy as np # Example dataset X = np.array([[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0], [8.0, 2.0], [10.0, 2.0], [9.0, 3.0]]) n_clusters = 3 # Expected function usage labels, inertia, silhouette = perform_clustering(X, n_clusters) print(f\\"Cluster labels: {labels}\\") print(f\\"Inertia: {inertia}\\") print(f\\"Silhouette Score: {silhouette:.2f}\\") ``` Make sure your code runs efficiently and accurately computes the required metrics. *Note:* Your implementation must be able to handle various sizes and shapes of input data.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score def perform_clustering(X, n_clusters): Perform KMeans clustering on dataset X with n_clusters clusters. Parameters: X (numpy.ndarray): 2D array with shape (n_samples, n_features). n_clusters (int): Number of clusters to form. Returns: tuple: A tuple (labels, inertia, silhouette_score) where labels is a 1D array of cluster labels assigned to each sample, inertia is the sum of squared distances of samples to their closest cluster center, silhouette_score is the silhouette score for the clustering performed. # Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans.fit(X) # Extract labels labels = kmeans.labels_ # Compute inertia inertia = kmeans.inertia_ # Compute silhouette score silhouette = silhouette_score(X, labels) return labels, inertia, silhouette"},{"question":"# Question: Advanced Color Space Conversion You are tasked with implementing a function that takes in RGB coordinates and determines whether they denote a grayscale color in any of the YIQ, HLS, or HSV color spaces. A color is considered grayscale if it has no hue and minimal saturation. Function Signature ```python def is_grayscale(r: float, g: float, b: float) -> dict: Determine if the given RGB coordinates denote a grayscale color in any of the YIQ, HLS, or HSV color spaces. Args: r (float): Red coordinate in the RGB color space, between 0 and 1. g (float): Green coordinate in the RGB color space, between 0 and 1. b (float): Blue coordinate in the RGB color space, between 0 and 1. Returns: dict: A dictionary with boolean values for each color space, indicating whether the color is considered grayscale in that color space. The keys should be \'yiq\', \'hls\', and \'hsv\'. Example: >>> is_grayscale(0.5, 0.5, 0.5) {\'yiq\': True, \'hls\': True, \'hsv\': True} >>> is_grayscale(0.2, 0.5, 0.8) {\'yiq\': False, \'hls\': False, \'hsv\': False} pass ``` Detailed Requirements 1. **RGB to Grayscale Decision**: - For the YIQ color space, a color is considered grayscale if the I and Q components are close to zero. - For the HLS color space, a color is considered grayscale if the saturation (S) component is close to zero. - For the HSV color space, a color is considered grayscale if the saturation (S) component is close to zero. 2. **Functionality**: - You must use the provided `colorsys` module functions to perform the color space conversions. - The function should return a dictionary indicating whether the color is grayscale for each color space with keys: `yiq`, `hls`, and `hsv`. 3. **Constraints**: - The input RGB values are guaranteed to be between 0 and 1 inclusive. - Assume a color is grayscale if the I and Q components (for YIQ) or the S component (for HLS and HSV) are within an absolute tolerance of `1e-5`. 4. **Performance**: - The solution should be efficient and make use of in-built `colorsys` module methods directly for conversion. Example ```python >>> is_grayscale(0.5, 0.5, 0.5) {\'yiq\': True, \'hls\': True, \'hsv\': True} >>> is_grayscale(0.2, 0.5, 0.8) {\'yiq\': False, \'hls\': False, \'hsv\': False} ```","solution":"import colorsys def is_grayscale(r: float, g: float, b: float) -> dict: Determine if the given RGB coordinates denote a grayscale color in any of the YIQ, HLS, or HSV color spaces. Args: r (float): Red coordinate in the RGB color space, between 0 and 1. g (float): Green coordinate in the RGB color space, between 0 and 1. b (float): Blue coordinate in the RGB color space, between 0 and 1. Returns: dict: A dictionary with boolean values for each color space, indicating whether the color is considered grayscale in that color space. The keys should be \'yiq\', \'hls\', and \'hsv\'. Example: >>> is_grayscale(0.5, 0.5, 0.5) {\'yiq\': True, \'hls\': True, \'hsv\': True} >>> is_grayscale(0.2, 0.5, 0.8) {\'yiq\': False, \'hls\': False, \'hsv\': False} tolerance = 1e-5 # Convert RGB to YIQ yiq = colorsys.rgb_to_yiq(r, g, b) yiq_is_grayscale = abs(yiq[1]) < tolerance and abs(yiq[2]) < tolerance # Convert RGB to HLS hls = colorsys.rgb_to_hls(r, g, b) hls_is_grayscale = hls[2] < tolerance # Convert RGB to HSV hsv = colorsys.rgb_to_hsv(r, g, b) hsv_is_grayscale = hsv[1] < tolerance return { \'yiq\': yiq_is_grayscale, \'hls\': hls_is_grayscale, \'hsv\': hsv_is_grayscale, }"},{"question":"**Question: Optimizing Prediction Latency and Throughput with scikit-learn** **Problem Statement:** You are provided with a dataset and a requirement to build a machine learning pipeline in scikit-learn, focusing on optimizing prediction latency and throughput. Your task is to implement functions to evaluate and compare the prediction performance of different scikit-learn models. You need to demonstrate understanding of the factors affecting computational performance such as model complexity, input data representation, and bulk versus atomic predictions. **Dataset:** Download the dataset from the following link: [link_to_dataset] (Ensure to download and load it correctly in your implementation) **Requirements:** 1. Load and preprocess the dataset. 2. Implement functions to train and evaluate the following models: - Linear Regression (`sklearn.linear_model.LinearRegression`) - Support Vector Machine (`sklearn.svm.SVR` with a non-linear kernel) - Random Forest Regressor (`sklearn.ensemble.RandomForestRegressor`) 3. For each model, measure the prediction latency and throughput. 4. Evaluate the effect of input data representation (dense vs sparse) on the prediction performance. 5. Evaluate the effect of model complexity on prediction latency and throughput. 6. Implement a function to compare the performance of all models based on the measured metrics and print a summary of the results. 7. Optimize the model configurations to achieve the best trade-off between prediction accuracy and computational performance. **Function Signatures:** ```python def load_preprocess_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]: Load and preprocess the dataset from the given file path. Args: - file_path (str): Path to the dataset file. Returns: - Tuple[np.ndarray, np.ndarray]: Features and target arrays. pass def train_evaluate_model(model: BaseEstimator, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]: Train and evaluate the model, measuring prediction latency and throughput. Args: - model (BaseEstimator): The scikit-learn model to be trained. - X (np.ndarray): Feature matrix. - y (np.ndarray): Target array. Returns: - Dict[str, Any]: Dictionary containing metrics such as latency, throughput, and accuracy. pass def compare_models(models: List[BaseEstimator], X: np.ndarray, y: np.ndarray) -> None: Compare the performance of given models and print a summary of the results. Args: - models (List[BaseEstimator]): List of scikit-learn models to be compared. - X (np.ndarray): Feature matrix. - y (np.ndarray): Target array. pass def optimize_model_performance(X: np.ndarray, y: np.ndarray) -> None: Optimize model configurations for best trade-off between prediction accuracy and computational performance. Args: - X (np.ndarray): Feature matrix. - y (np.ndarray): Target array. pass ``` **Constraints:** - The dataset should contain a sufficient number of features to evaluate the impact on latency. - You should use appropriate metrics and visualization to report the computational performance. - Make sure to handle model training and evaluation efficiently to avoid excessive computation times. **Notes:** - Ensure to follow best practices for data preprocessing and model evaluation. - Provide detailed comments and documentation for your functions. - Consider using Python\'s built-in `time` library to measure latency and throughput. **Performance Requirements:** - Provide a detailed report on the performance of each model, including the impact of model complexity, input data representation, and bulk versus atomic prediction. - Comment on the trade-offs between prediction accuracy and computational performance for each model. Good luck!","solution":"import numpy as np import pandas as pd import time from typing import Tuple, Dict, Any, List from sklearn.linear_model import LinearRegression from sklearn.svm import SVR from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score from sklearn.base import BaseEstimator def load_preprocess_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]: Load and preprocess the dataset from the given file path. Args: - file_path (str): Path to the dataset file. Returns: - Tuple[np.ndarray, np.ndarray]: Features and target arrays. # For the sake of this example, assume the dataset is a CSV file. data = pd.read_csv(file_path) # Assume the target column is named \'target\' and remaining are features. X = data.drop(columns=[\'target\']).values y = data[\'target\'].values return X, y def train_evaluate_model(model: BaseEstimator, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]: Train and evaluate the model, measuring prediction latency and throughput. Args: - model (BaseEstimator): The scikit-learn model to be trained. - X (np.ndarray): Feature matrix. - y (np.ndarray): Target array. Returns: - Dict[str, Any]: Dictionary containing metrics such as latency, throughput, and accuracy. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model model.fit(X_train, y_train) # Measure prediction latency and throughput start_time = time.time() y_pred = model.predict(X_test) end_time = time.time() latency = end_time - start_time throughput = len(y_test) / latency # Calculate performance metrics mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) return { \\"latency\\": latency, \\"throughput\\": throughput, \\"mse\\": mse, \\"r2\\": r2 } def compare_models(models: List[BaseEstimator], X: np.ndarray, y: np.ndarray) -> None: Compare the performance of given models and print a summary of the results. Args: - models (List[BaseEstimator]): List of scikit-learn models to be compared. - X (np.ndarray): Feature matrix. - y (np.ndarray): Target array. results = [] for model in models: result = train_evaluate_model(model, X, y) results.append(result) for i, model in enumerate(models): print(f\\"Model: {model.__class__.__name__}\\") print(f\\"Latency: {results[i][\'latency\']:.4f} seconds\\") print(f\\"Throughput: {results[i][\'throughput\']:.4f} predictions/second\\") print(f\\"MSE: {results[i][\'mse\']:.4f}\\") print(f\\"R2: {results[i][\'r2\']:.4f}\\") print(\\"\\") def optimize_model_performance(X: np.ndarray, y: np.ndarray) -> None: Optimize model configurations for best trade-off between prediction accuracy and computational performance. Args: - X (np.ndarray): Feature matrix. - y (np.ndarray): Target array. models = [ LinearRegression(), SVR(kernel=\'rbf\', C=100, gamma=0.1, epsilon=.1), RandomForestRegressor(n_estimators=100, max_depth=10) ] compare_models(models, X, y)"},{"question":"**Objective:** Optimize the computational performance of a machine learning model using scikit-learn, focusing on prediction latency and model complexity. **Problem Statement:** You are provided with a dataset and a machine learning model trained for a classification task using scikit-learn. Your task is to optimize the performance of the model by applying several techniques discussed in the scikit-learn documentation. **Requirements:** 1. **Prediction Latency:** - Implement the prediction using both atomic mode and bulk (batch) mode. - Measure and compare the prediction latency for both modes. 2. **Model Complexity:** - Train multiple models with varying complexity using different values of the regularization parameter (`alpha`) and sparsity parameter (`l1_ratio`). - Implement the `sparsify()` method to create a sparse model. 3. **Input Data Representation:** - Convert the input data to a sparse representation if it meets the criteria for high sparsity. - Measure and compare the prediction latency for both dense and sparse input representations. 4. **Optimization:** - Perform limited validation by configuring scikit-learn to assume finite input data. - Optimize the linear algebra libraries used by scikit-learn. 5. **Evaluation:** - Use a performance metric (e.g., accuracy) to evaluate the trade-off between model complexity and prediction latency. **Input:** - `X_train` (numpy array): Training data features. - `y_train` (numpy array): Training data labels. - `X_test` (numpy array): Testing data features. - `y_test` (numpy array): Testing data labels. - `model` (scikit-learn estimator): Pre-trained machine learning model. **Output:** - Print the prediction latency for atomic and bulk modes. - Print the prediction latency for dense and sparse input representations. - Print the evaluation metric for models with different complexities. - Print the optimized linear algebra library used by scikit-learn. **Code Template:** ```python import numpy as np import time from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn import config_context # Helper function to measure prediction latency def measure_latency(model, X, mode=\'atomic\'): start_time = time.time() if mode == \'bulk\': model.predict(X) else: for x in X: model.predict([x]) end_time = time.time() return end_time - start_time # Load dataset X_train, X_test, y_train, y_test = # your code here # Train model with different complexities alpha_values = [1e-2, 1e-3] # Regularization parameter l1_ratios = [0.15, 0.5, 0.85] # Sparsity parameter models = [] for alpha in alpha_values: for l1_ratio in l1_ratios: clf = SGDClassifier(penalty=\'elasticnet\', alpha=alpha, l1_ratio=l1_ratio) clf.fit(X_train, y_train) clf.sparsify() models.append(clf) # Measure prediction latency for atomic and bulk modes clf = models[0] atomic_latency = measure_latency(clf, X_test, mode=\'atomic\') bulk_latency = measure_latency(clf, X_test, mode=\'bulk\') print(f\\"Atomic prediction latency: {atomic_latency}\\") print(f\\"Bulk prediction latency: {bulk_latency}\\") # Convert input data to sparse representation and measure latency from scipy.sparse import csr_matrix def sparsity_ratio(X): return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1]) if sparsity_ratio(X_train) > 0.9: X_train_sparse = csr_matrix(X_train) X_test_sparse = csr_matrix(X_test) else: X_train_sparse, X_test_sparse = X_train, X_test dense_latency = measure_latency(clf, X_test) sparse_latency = measure_latency(clf, X_test_sparse) print(f\\"Dense input prediction latency: {dense_latency}\\") print(f\\"Sparse input prediction latency: {sparse_latency}\\") # Perform limited validation with config_context(assume_finite=True): _ = clf.predict(X_test) # Display linear algebra library import sklearn print(sklearn.show_versions()) # Evaluate models with different complexities for model in models: accuracy = model.score(X_test, y_test) print(f\\"Model with alpha={model.alpha}, l1_ratio={model.l1_ratio}: accuracy={accuracy}\\") ``` **Constraints:** 1. Assume the dataset has a high number of features. 2. Ensure the code can handle large datasets efficiently. **Performance Requirements:** The optimized code should achieve a reduced prediction latency while maintaining acceptable accuracy levels.","solution":"import numpy as np import time from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn import config_context from sklearn.datasets import make_classification from scipy.sparse import csr_matrix # Helper function to measure prediction latency def measure_latency(model, X, mode=\'atomic\'): start_time = time.time() if mode == \'bulk\': model.predict(X) else: for x in X: model.predict([x]) end_time = time.time() return end_time - start_time # Generate synthetic data for illustration purposes X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42, flip_y=0.03) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train model with different complexities alpha_values = [1e-2, 1e-3] # Regularization parameter l1_ratios = [0.15, 0.5, 0.85] # Sparsity parameter models = [] for alpha in alpha_values: for l1_ratio in l1_ratios: clf = SGDClassifier(penalty=\'elasticnet\', alpha=alpha, l1_ratio=l1_ratio, random_state=42) clf.fit(X_train, y_train) clf.sparsify() models.append(clf) # Measure prediction latency for atomic and bulk modes clf = models[0] atomic_latency = measure_latency(clf, X_test, mode=\'atomic\') bulk_latency = measure_latency(clf, X_test, mode=\'bulk\') print(f\\"Atomic prediction latency: {atomic_latency}\\") print(f\\"Bulk prediction latency: {bulk_latency}\\") # Convert input data to sparse representation and measure latency def sparsity_ratio(X): return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1]) X_train_sparse, X_test_sparse = X_train, X_test if sparsity_ratio(X_train) > 0.9: X_train_sparse = csr_matrix(X_train) X_test_sparse = csr_matrix(X_test) dense_latency = measure_latency(clf, X_test, mode=\'bulk\') sparse_latency = measure_latency(clf, X_test_sparse, mode=\'bulk\') print(f\\"Dense input prediction latency: {dense_latency}\\") print(f\\"Sparse input prediction latency: {sparse_latency}\\") # Perform limited validation with config_context(assume_finite=True): _ = clf.predict(X_test) # Display optimized linear algebra library used by scikit-learn import sklearn print(sklearn.show_versions()) # Evaluate models with different complexities for model in models: accuracy = model.score(X_test, y_test) print(f\\"Model with alpha={model.alpha}, l1_ratio={model.l1_ratio}: accuracy={accuracy}\\")"},{"question":"Objective Implement a Python class `CustomSet` that mimics a subset of the functionality of Python\'s built-in set, specifically leveraging the Python C API principles as described in the documentation. Task You need to implement the following methods for the `CustomSet` class: 1. **`__init__(self, iterable=None)`**: Initializes a new set object with elements from the optional iterable. If no iterable is provided, initialize an empty set. 2. **`add(self, element)`**: Adds an element to the set. If the element is already in the set, do nothing. 3. **`discard(self, element)`**: Removes the specified element from the set if present. If the element is not present, do nothing. 4. **`contains(self, element)`**: Returns `True` if the set contains the specified element, otherwise returns `False`. 5. **`size(self)`**: Returns the number of elements in the set. 6. **`clear(self)`**: Removes all elements from the set. Input and Output Formats - `__init__(self, iterable=None)` - **Input**: An optional iterable. Default is `None`, which creates an empty set. - **Output**: None - `add(self, element)` - **Input**: `element` (any hashable object) - **Output**: None - `discard(self, element)` - **Input**: `element` (any hashable object) - **Output**: None - `contains(self, element)` - **Input**: `element` (any hashable object) - **Output**: `True` if `element` is in the set, otherwise `False` - `size(self)` - **Output**: Integer representing the number of elements in the set - `clear(self)` - **Output**: None Constraints 1. Elements added to the set must be hashable. 2. You are not allowed to use Python\'s built-in set methods such as `add`, `discard`, `__contains__`, `__len__`, and `clear`. Example ```python # Example usage custom_set = CustomSet([1, 2, 3]) assert custom_set.size() == 3 assert custom_set.contains(2) == True custom_set.add(4) assert custom_set.size() == 4 custom_set.discard(3) assert custom_set.size() == 3 assert custom_set.contains(3) == False custom_set.clear() assert custom_set.size() == 0 ``` Demonstrate an understanding of underlying set mechanics by implementing your own data structure, rather than relying on Python\'s built-in set operations.","solution":"class CustomSet: def __init__(self, iterable=None): self._data = {} if iterable: for item in iterable: self._data[item] = True def add(self, element): self._data[element] = True def discard(self, element): if element in self._data: del self._data[element] def contains(self, element): return element in self._data def size(self): return len(self._data) def clear(self): self._data = {}"},{"question":"# Question: Implement Custom Boolean-like Class in Python Your task is to create a custom class that mimics the functionality of Python\'s built-in Boolean type but includes additional logging for educational and debugging purposes. Requirements: 1. **Class Name:** `CustomBool` 2. **Methods to Implement:** - `__new__(cls, value)`: This method should handle instance creation. Depending on the truthiness of the input `value`, return either the singleton instance of `CustomBoolTrue` or `CustomBoolFalse`, similar to `PyBool_FromLong`. - `__bool__(self)`: This method should return the Boolean value of the instance. - `__repr__(self)`: This method should provide a clear string representation of the instance for debugging purposes. - `__eq__(self, other)`: This method should implement equality comparison. 3. **Additional Logging:** - Each time an instance is created or a method is called, a log entry should be printed to the console with the format: `\\"[DEBUG] Method <method_name> called with arguments: <args>\\"`. Constraints: - You are not allowed to use the built-in `bool` type anywhere in your `CustomBool` class. - The input to the `__new__` method can be any Python object. Example: ```python >>> a = CustomBool(0) [DEBUG] Method __new__ called with arguments: (0,) >>> b = CustomBool(42) [DEBUG] Method __new__ called with arguments: (42,) >>> a [DEBUG] Method __repr__ called CustomBoolFalse >>> b [DEBUG] Method __repr__ called CustomBoolTrue >>> bool(a) [DEBUG] Method __bool__ called False >>> bool(b) [DEBUG] Method __bool__ called True >>> a == b [DEBUG] Method __eq__ called with arguments: (<CustomBoolTrue object at 0x...>,) False ``` # Solution Template: Use the following template to get started with your solution: ```python class CustomBool: _true_instance = None _false_instance = None def __new__(cls, value): # Implement the custom instance creation logic with logging. pass def __bool__(self): # Implement the Boolean conversion logic with logging. pass def __repr__(self): # Implement the string representation logic with logging. pass def __eq__(self, other): # Implement the equality comparison logic with logging. pass class CustomBoolTrue(CustomBool): pass class CustomBoolFalse(CustomBool): pass # You can add any additional helper functions or classes if necessary. ```","solution":"class CustomBool: _true_instance = None _false_instance = None def __new__(cls, value): print(f\\"[DEBUG] Method __new__ called with arguments: ({value},)\\") if value: if cls._true_instance is None: cls._true_instance = super().__new__(CustomBoolTrue) return cls._true_instance else: if cls._false_instance is None: cls._false_instance = super().__new__(CustomBoolFalse) return cls._false_instance def __bool__(self): print(f\\"[DEBUG] Method __bool__ called\\") if isinstance(self, CustomBoolTrue): return True elif isinstance(self, CustomBoolFalse): return False def __repr__(self): print(f\\"[DEBUG] Method __repr__ called\\") if isinstance(self, CustomBoolTrue): return \\"CustomBoolTrue\\" elif isinstance(self, CustomBoolFalse): return \\"CustomBoolFalse\\" def __eq__(self, other): print(f\\"[DEBUG] Method __eq__ called with arguments: ({other},)\\") if isinstance(other, CustomBool): return bool(self) == bool(other) return False class CustomBoolTrue(CustomBool): pass class CustomBoolFalse(CustomBool): pass"},{"question":"# Problem: Working with Time Deltas in Pandas You are given a dataset containing timestamps for the start and end of different events. Your task is to compute the duration of each event, analyze these durations, and perform some operations using pandas `Timedelta`. Input: You have two lists of strings representing the start and end times of events in ISO 8601 format: ```python start_times = [ \\"2023-01-01T00:00:00\\", \\"2023-01-02T13:45:23\\", \\"2023-01-05T08:21:05\\", \\"2023-01-07T09:15:00\\" ] end_times = [ \\"2023-01-01T12:34:56\\", \\"2023-01-03T15:32:21\\", \\"2023-01-06T14:45:10\\", \\"2023-01-07T10:30:45\\" ] ``` Tasks: 1. **Compute Durations**: - Create a pandas DataFrame to store the start_times and end_times. - Calculate the duration for each event and store it in a new column called `duration`, using pandas `Timedelta`. 2. **Analyze Durations**: - Compute the total duration of all events combined. - Compute the average duration of events. 3. **Manipulating Durations**: - Add 1 hour to each event\'s duration and store the results in a new column called `extended_duration`. - Convert the `extended_duration` to seconds and store it in a column called `duration_seconds`. 4. **Information Extraction**: - Extract the days and hours components of the `extended_duration` and store them in separate columns called `days` and `hours`. Expected Output: Your function should print: - The DataFrame showing all columns. - The total duration of all events combined. - The average duration of events. Constraints: - You must use pandas for this task. - Assume the input lists are of the same length and contain valid ISO 8601 datetime strings. - Be mindful of performance considerations given typical dataset sizes. ```python import pandas as pd def analyze_event_durations(start_times, end_times): # Your code here # Example usage: start_times = [ \\"2023-01-01T00:00:00\\", \\"2023-01-02T13:45:23\\", \\"2023-01-05T08:21:05\\", \\"2023-01-07T09:15:00\\" ] end_times = [ \\"2023-01-01T12:34:56\\", \\"2023-01-03T15:32:21\\", \\"2023-01-06T14:45:10\\", \\"2023-01-07T10:30:45\\" ] analyze_event_durations(start_times, end_times) ```","solution":"import pandas as pd def analyze_event_durations(start_times, end_times): # Create DataFrame df = pd.DataFrame({ \'start_time\': pd.to_datetime(start_times), \'end_time\': pd.to_datetime(end_times) }) # Compute duration for each event df[\'duration\'] = df[\'end_time\'] - df[\'start_time\'] # Total duration of all events combined total_duration = df[\'duration\'].sum() # Average duration of events average_duration = df[\'duration\'].mean() # Add 1 hour to each event\'s duration df[\'extended_duration\'] = df[\'duration\'] + pd.Timedelta(hours=1) # Convert extended_duration to seconds df[\'duration_seconds\'] = df[\'extended_duration\'].dt.total_seconds() # Extract days and hours components of extended_duration df[\'days\'] = df[\'extended_duration\'].dt.days df[\'hours\'] = df[\'extended_duration\'].dt.components[\'hours\'] # Printing the results print(\\"DataFrame with all columns:\\") print(df) print(\\"nTotal duration of all events combined:\\", total_duration) print(\\"Average duration of events:\\", average_duration) return df, total_duration, average_duration # Example usage: start_times = [ \\"2023-01-01T00:00:00\\", \\"2023-01-02T13:45:23\\", \\"2023-01-05T08:21:05\\", \\"2023-01-07T09:15:00\\" ] end_times = [ \\"2023-01-01T12:34:56\\", \\"2023-01-03T15:32:21\\", \\"2023-01-06T14:45:10\\", \\"2023-01-07T10:30:45\\" ] # Run function to analyze durations analyze_event_durations(start_times, end_times)"},{"question":"# Advanced Seaborn Plotting Objective: You are required to demonstrate your understanding of seaborn\'s `seaborn.objects` module by creating a complex faceted plot using the `penguins` dataset from seaborn. Instructions: 1. Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. Create a faceted plot using `seaborn.objects.Plot` that visualizes the `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. 3. The facets should include: - Columns representing the `species` of penguins. - Rows representing the `island` that the penguins inhabit. 4. Ensure that the x-axis is shared only across columns, and the y-axis is shared only across rows. 5. Use dots (`so.Dots()`) as the plot type for visualizing the data points. # Example: ```python # Sample code block structure import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Define the plot plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"island\\") .add(so.Dots()) .share(x=\\"col\\", y=\\"row\\") ) # Display the plot plot ``` Notes: - The data points should display clearly within their respective facets. - Pay attention to axis labels and ranges to ensure they are correctly shared among the appropriate facets. - Any additional customization for better visualization is encouraged. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Define the plot plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"island\\") .add(so.Dots()) .share(x=\\"col\\", y=\\"row\\") ) # Display the plot plot.show()"},{"question":"# Asynchronous Chat Server with asyncio Objective: Design and implement an asynchronous chat server using the `asyncio` library. The server should be able to handle multiple clients, broadcast messages to all connected clients, and properly manage network connections and exceptions. Requirements: 1. **Server Functionality**: - The server should listen on a specified host and port. - It should handle multiple client connections simultaneously. - When a client sends a message, it should be broadcasted to all other connected clients. - The server should handle client disconnections gracefully. 2. **Client Functionality**: - The clients should connect to the server and be able to send messages. - Each client should display messages received from the server. Input Format: 1. Server\'s host address and port number. 2. Clients connecting to the server and sending text messages. Output Format: 1. The server\'s console should log client connections, disconnections, and messages. 2. The client\'s console should display received messages. Constraints: 1. The server must be implemented using the `asyncio` package and its event loop. 2. Each client\'s connection and messaging should be handled asynchronously. Performance Requirements: 1. The server should efficiently handle a moderate number of concurrent connections (e.g., up to 100 clients). Step-by-Step Tasks: 1. **Server Implementation**: - Create an `asyncio` server that listens on a specified host and port. - Implement a coroutine to accept new client connections. - Implement another coroutine to handle communication with each connected client. - Ensure messages from one client are broadcasted to all other clients. 2. **Client Implementation**: - Implement a client that connects to the server. - The client should send messages to the server. - The client should receive messages from the server and display them. Example Usage: **Server startup**: ```bash python chat_server.py --host 127.0.0.1 --port 8888 ``` **Client startup**: ```bash python chat_client.py --host 127.0.0.1 --port 8888 ``` **Sample console log for server**: ```text Connected by (\'127.0.0.1\', 52314) Connected by (\'127.0.0.1\', 52315) Received message from (\'127.0.0.1\', 52314): \\"Hello, World!\\" Broadcasting message: \\"Hello, World!\\" ``` **Sample console log for client**: ```text Connected to server at 127.0.0.1:8888 Received message: \\"Hello from Client 1\\" Received message: \\"Hello from Client 2\\" ... Use the following template for your implementation: ```python # chat_server.py import asyncio async def handle_client(reader, writer): # Handle communication with a connected client pass async def main(host, port): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() if __name__ == \'__main__\': import argparse parser = argparse.ArgumentParser(description=\'Asyncio Chat Server\') parser.add_argument(\'--host\', type=str, default=\'127.0.0.1\') parser.add_argument(\'--port\', type=int, default=8888) args = parser.parse_args() asyncio.run(main(args.host, args.port)) ``` ```python # chat_client.py import asyncio async def handle_input(writer): loop = asyncio.get_running_loop() while True: message = await loop.run_in_executor(None, input, \\"Message: \\") writer.write(message.encode()) await writer.drain() async def handle_output(reader): while True: data = await reader.read(100) if not data: break print(f\\"Received message: {data.decode()}\\") async def main(host, port): reader, writer = await asyncio.open_connection(host, port) input_task = asyncio.create_task(handle_input(writer)) output_task = asyncio.create_task(handle_output(reader)) await asyncio.gather(input_task, output_task) if __name__ == \'__main__\': import argparse parser = argparse.ArgumentParser(description=\'Asyncio Chat Client\') parser.add_argument(\'--host\', type=str, default=\'127.0.0.1\') parser.add_argument(\'--port\', type=int, default=8888) args = parser.parse_args() asyncio.run(main(args.host, args.port)) ``` Complete the code for both the server and client by implementing the communication handling parts. Your solution should demonstrate the use of `asyncio` for asynchronous network communication and handling tasks concurrently.","solution":"# chat_server.py import asyncio clients = [] async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') clients.append(writer) print(f\\"Connected by {addr}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received message from {addr}: {message}\\") broadcast_message(message, writer) finally: print(f\\"Connection closed by {addr}\\") clients.remove(writer) writer.close() await writer.wait_closed() def broadcast_message(message, current_writer): for client in clients: if client != current_writer: client.write(message.encode()) asyncio.create_task(client.drain()) async def main(host, port): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() if __name__ == \'__main__\': import argparse parser = argparse.ArgumentParser(description=\'Asyncio Chat Server\') parser.add_argument(\'--host\', type=str, default=\'127.0.0.1\') parser.add_argument(\'--port\', type=int, default=8888) args = parser.parse_args() asyncio.run(main(args.host, args.port)) # chat_client.py import asyncio async def handle_input(writer): loop = asyncio.get_running_loop() while True: message = await loop.run_in_executor(None, input, \\"Message: \\") writer.write(message.encode()) await writer.drain() async def handle_output(reader): while True: data = await reader.read(100) if not data: break print(f\\"Received message: {data.decode()}\\") async def main(host, port): reader, writer = await asyncio.open_connection(host, port) print(f\\"Connected to server at {host}:{port}\\") input_task = asyncio.create_task(handle_input(writer)) output_task = asyncio.create_task(handle_output(reader)) await asyncio.gather(input_task, output_task) if __name__ == \'__main__\': import argparse parser = argparse.ArgumentParser(description=\'Asyncio Chat Client\') parser.add_argument(\'--host\', type=str, default=\'127.0.0.1\') parser.add_argument(\'--port\', type=int, default=8888) args = parser.parse_args() asyncio.run(main(args.host, args.port))"},{"question":"**Incremental Learning with Scikit-learn** **Objective:** Design and implement a pipeline for handling and learning from a large dataset that does not fit into memory using scikit-learn. This will involve: 1. Streaming data from a file or generated data. 2. Extracting features using a suitable method. 3. Training an incremental learning model. **Task:** 1. **Streaming Data:** - Implement a function `stream_data(file_path, batch_size)` that reads data from a file in smaller chunks (mini-batches) and returns the data in these chunks. 2. **Feature Extraction:** - Implement a function `extract_features(data_chunk)` that processes each mini-batch of data and extracts features using `HashingVectorizer` for text data. 3. **Incremental Learning:** - Implement a function `incremental_learning(stream, n_classes)` that takes the data stream and performs incremental learning using `SGDClassifier`. **Requirements:** - The incremental learning model should handle multi-class classification. - Track and print the model\'s accuracy after each mini-batch. **Constraints:** - Assume the data is a text classification problem with multiple classes. - Use `SGDClassifier` for incremental learning. - Batch size should be configurable. **Function Definitions:** 1. `stream_data(file_path, batch_size) -> Generator` - **Input:** - `file_path`: str, path to the input text file. - `batch_size`: int, number of lines to read per batch. - **Output:** - Generator that yields data in batches of specified size. 2. `extract_features(data_chunk) -> scipy.sparse matrix` - **Input:** - `data_chunk`: list of str, each string represents a line of text. - **Output:** - Feature matrix for the data chunk. 3. `incremental_learning(stream, n_classes)` - **Input:** - `stream`: Generator, data stream. - `n_classes`: int, number of target classes. - **Output:** None, but print accuracy after processing each batch. **Example usage:** ```python def stream_data(file_path, batch_size): # Your implementation here pass def extract_features(data_chunk): # Your implementation here pass def incremental_learning(stream, n_classes): # Your implementation here pass # Example data_stream = stream_data(\'large_text_file.txt\', 1000) incremental_learning(data_stream, 5) ``` Please ensure your solution handles memory efficiently and prints useful performance metrics.","solution":"import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path, batch_size): Streams data from a file in mini-batches. Parameters: file_path (str): Path to the input text file. batch_size (int): Number of lines to read per batch. Yields: List[str]: A batch of lines from the file. with open(file_path, \'r\') as file: while True: lines = [] for _ in range(batch_size): line = file.readline().strip() if not line: break lines.append(line) if not lines: break yield lines def extract_features(data_chunk): Extracts features from a chunk of data using HashingVectorizer. Parameters: data_chunk (List[str]): A batch of lines from the file. Returns: scipy.sparse matrix: Feature matrix for the data chunk. vectorizer = HashingVectorizer(n_features=2**20) return vectorizer.transform(data_chunk) def incremental_learning(stream, n_classes): Performs incremental learning using SGDClassifier. Parameters: stream (Generator): Data stream generator. n_classes (int): Number of target classes. Returns: None: Prints accuracy after processing each batch. classifier = SGDClassifier(max_iter=1000, tol=1e-3) all_labels = [] all_predictions = [] for batch_idx, data_chunk in enumerate(stream): X = extract_features(data_chunk) y = np.random.randint(0, n_classes, X.shape[0]) # Random labels for demonstration if batch_idx == 0: classifier.partial_fit(X, y, classes=np.arange(n_classes)) else: classifier.partial_fit(X, y) predictions = classifier.predict(X) all_labels.extend(y) all_predictions.extend(predictions) accuracy = accuracy_score(all_labels, all_predictions) print(f\\"Batch {batch_idx+1}, Accuracy: {accuracy:.4f}\\")"},{"question":"**Seaborn Advanced Plotting Task** **Objective:** Demonstrate your understanding of Seaborn\'s `so.Plot` class by replicating and extending its functionality to create various bar plots with different aggregation functions and transformations. **Dataset:** You will use Seaborn\'s built-in `diamonds` dataset for this task. **Task:** 1. **Basic Bar Plot:** Create a basic bar plot to visualize the mean carat weight of diamonds grouped by the \'clarity\' column. 2. **Median Aggregation Bar Plot:** Create a bar plot to visualize the median carat weight of diamonds grouped by the \'color\' column with a different color for each bar segment based on the cut variable. 3. **Custom Aggregation Function:** Create a bar plot to visualize the range (75th percentile - 25th percentile) of carat weight of diamonds grouped by the \'clarity\' column. 4. **Transformed Bar Plot:** Create a bar plot that shows the mean carat weight of diamonds grouped by the \'cut\' column and split by the \'color\' column using the `Dodge` transform. **Requirements:** - Use appropriate Seaborn plotting methods and aggregation functions. - Ensure that each plot has titles, labeled axes, and a legend where applicable. - The plots should be displayed in a single output for easier evaluation. **Expected Output:** Save and submit your notebook file which includes the following plots: 1. Basic bar plot of mean carat weight by clarity. 2. Median aggregation bar plot of carat weight by color, colored by cut. 3. Custom aggregation bar plot of the 75th-25th percentile range of carat weight by clarity. 4. Transformed bar plot of mean carat weight by cut, split by color using Dodge. **Constraints:** 1. Use Python 3.10.6 or above. 2. Use Seaborn version 0.11.2 or above. 3. Ensure code is optimized and runs efficiently with the provided dataset. Submit your solution in a Jupyter Notebook file format (.ipynb).","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") def plot_diamonds_bar_plots(): # Basic bar plot for mean carat weight by clarity mean_carat_by_clarity = ( so.Plot(diamonds, x=\'clarity\', y=\'carat\') .add(so.Bar(), so.Agg(), func=\'mean\') ) # Median carat weight by color, colored by cut median_carat_by_color_cut = ( so.Plot(diamonds, x=\'color\', y=\'carat\', color=\'cut\') .add(so.Bar(), so.Agg(), func=\'median\') ) # Range (75th - 25th percentile) of carat weight by clarity range_carat_by_clarity = ( so.Plot(diamonds, x=\'clarity\', y=\'carat\') .add(so.Bar(), so.Agg(), func=lambda x: x.quantile(0.75) - x.quantile(0.25)) ) # Mean carat weight by cut split by color using Dodge mean_carat_by_cut_color = ( so.Plot(diamonds, x=\'cut\', y=\'carat\', color=\'color\') .add(so.Bar(), so.Agg(), so.Dodge(), func=\'mean\') ) # Plot all the subplots fig, axs = plt.subplots(2, 2, figsize=(18, 12)) mean_carat_by_clarity.on(axs[0, 0]) axs[0, 0].set_title(\\"Mean Carat Weight by Clarity\\") median_carat_by_color_cut.on(axs[0, 1]) axs[0, 1].set_title(\\"Median Carat Weight by Color, colored by Cut\\") range_carat_by_clarity.on(axs[1, 0]) axs[1, 0].set_title(\\"Range (75th - 25th Percentile) Carat Weight by Clarity\\") mean_carat_by_cut_color.on(axs[1, 1]) axs[1, 1].set_title(\\"Mean Carat Weight by Cut, split by Color using Dodge\\") plt.tight_layout() plt.show() # Call the plotting function to generate the plots plot_diamonds_bar_plots()"},{"question":"# Hashlib Module Usage and Custom Password Hashing Scheme **Problem Statement:** You are tasked with designing a secure password hashing system for a new web application using the `hashlib` module in Python. Password hashing is critical for protecting user data against potential breaches. You need to implement a custom password hashing function that uses multiple features of the `hashlib` module, including key-based hashing and salt. Your system will follow these steps: 1. Generate a random salt for each password. 2. Use the Blake2b algorithm to hash the password with the salt and a secret key. 3. Provide a function to verify a password against the stored hash. **Requirements:** 1. Implement a function `hash_password(password: str, secret_key: bytes) -> str` that: - Takes a plaintext password and a secret key as input. - Generates a random 16-byte salt. - Uses the Blake2b algorithm with `digest_size=32` and the generated salt. - Returns a base64-encoded string combining the salt and the hash. 2. Implement a function `verify_password(stored_hash: str, password: str, secret_key: bytes) -> bool` that: - Takes the stored hash, the plaintext password, and the secret key as input. - Extracts the salt from the stored hash. - Hashes the password with the extracted salt and the same secret key using Blake2b. - Returns `True` if the hashes match, otherwise returns `False`. **Input and Output Formats:** - `hash_password(password: str, secret_key: bytes) -> str` - Input: A plaintext password (e.g., \\"mysecretpassword\\") and a secret key in bytes (e.g., `b\'secret1234\'`). - Output: A base64-encoded string containing the salt and the hash (e.g., \\"c2FsdEdlbmVyYXRvcnNLZXlzSGFzaEdvZXNIYXNo\\"). - `verify_password(stored_hash: str, password: str, secret_key: bytes) -> bool` - Input: The previously generated stored hash, a plaintext password, and the same secret key used in `hash_password`. - Output: A boolean value indicating if the password is correct. **Constraints:** - The salt should be 16 bytes long. - The Blake2b digest size should be 32 bytes. - Secret keys should be treated securely and must be kept confidential. **Performance Requirements:** - The functions must efficiently handle typical password lengths (8-64 characters) and run within acceptable time limits for a web application environment. **Example:** ```python import base64 import os from hashlib import blake2b def hash_password(password: str, secret_key: bytes) -> str: salt = os.urandom(16) h = blake2b(key=secret_key, salt=salt, digest_size=32) h.update(password.encode(\'utf-8\')) return base64.b64encode(salt + h.digest()).decode(\'utf-8\') def verify_password(stored_hash: str, password: str, secret_key: bytes) -> bool: decoded = base64.b64decode(stored_hash) salt = decoded[:16] stored_digest = decoded[16:] h = blake2b(key=secret_key, salt=salt, digest_size=32) h.update(password.encode(\'utf-8\')) return h.digest() == stored_digest # Example usage: secret_key = b\'secret1234\' pwd = \\"mysecretpassword\\" hashed_pwd = hash_password(pwd, secret_key) print(hashed_pwd) # Output: a base64 encoded hash with salt is_valid = verify_password(hashed_pwd, \\"mysecretpassword\\", secret_key) print(is_valid) # Output: True is_invalid = verify_password(hashed_pwd, \\"wrongpassword\\", secret_key) print(is_invalid) # Output: False ``` Note: Ensure the `secret_key` is stored and managed securely in a real application.","solution":"import base64 import os from hashlib import blake2b def hash_password(password: str, secret_key: bytes) -> str: Generates a hashed password with a random salt using the Blake2b algorithm. :param password: The plaintext password to hash. :param secret_key: The secret key used for hashing. :return: A base64-encoded string combining the salt and the hash. salt = os.urandom(16) h = blake2b(key=secret_key, salt=salt, digest_size=32) h.update(password.encode(\'utf-8\')) return base64.b64encode(salt + h.digest()).decode(\'utf-8\') def verify_password(stored_hash: str, password: str, secret_key: bytes) -> bool: Verifies a password against a stored hash value. :param stored_hash: The stored hash containing the salt and the hash. :param password: The plaintext password to verify. :param secret_key: The secret key used for hashing. :return: True if the password matches the hash, False otherwise. decoded = base64.b64decode(stored_hash) salt = decoded[:16] stored_digest = decoded[16:] h = blake2b(key=secret_key, salt=salt, digest_size=32) h.update(password.encode(\'utf-8\')) return h.digest() == stored_digest"},{"question":"# String Utilities Implementation Challenge Objective Implement a set of utility functions that manipulate and format strings in various customized ways. Task You need to implement the following three functions: 1. **mask_sensitive_info** 2. **custom_format_string** 3. **generate_greeting** Each of these functions must adhere to the specified input and output formats, constraints, and performance requirements. Function 1: mask_sensitive_info This function takes a string and masks any digits in the string, replacing them with the character `\'*\'`. **Input:** - `text` (str): A string containing alphanumeric characters. **Output:** - (str): A string where all digits `0-9` are replaced with `\'*\'`. Example: ```python mask_sensitive_info(\\"Your PIN is 1234\\") # Output: \\"Your PIN is ****\\" ``` Function 2: custom_format_string This function formats a dictionary of named fields into a given template string. If the template contains placeholders not present in the dictionary, raise a `KeyError`. **Input:** - `template` (str): A format string containing placeholders e.g., \\"Hello {name}, welcome to {location}.\\" - `values` (dict): A dictionary with keys corresponding to placeholder names in the template string. **Output:** - (str): A formatted string with placeholders replaced by corresponding values from the dictionary. **Constraints:** - If the template contains placeholders not present in the dictionary, the function must raise a `KeyError`. Example: ```python custom_format_string(\\"Hello {name}, welcome to {location}.\\", {\\"name\\": \\"Alice\\", \\"location\\": \\"Wonderland\\"}) # Output: \\"Hello Alice, welcome to Wonderland.\\" ``` Function 3: generate_greeting This function generates a customized greeting using template strings. It accepts a name, a time of day, and an optional message. **Input:** - `name` (str): The name of the person being greeted. - `time_of_day` (str): The time of day, e.g., \\"morning\\", \\"afternoon\\", \\"evening\\". - `message` (str, optional): An optional message to include in the greeting. Defaults to an empty string. **Output:** - (str): A customized greeting message. Example: ```python generate_greeting(\\"Alice\\", \\"morning\\", \\"Hope you have a great day!\\") # Output: \\"Good morning, Alice! Hope you have a great day!\\" ``` ```python generate_greeting(\\"Bob\\", \\"evening\\") # Output: \\"Good evening, Bob!\\" ``` Implementation Notes - For `mask_sensitive_info`, use string methods and constants from the `string` module. - For `custom_format_string`, use the `Formatter` class or `str.format`. - For `generate_greeting`, use the `Template` class from the `string` module. Write your implementation below: ```python def mask_sensitive_info(text): # Your code here pass def custom_format_string(template, values): # Your code here pass def generate_greeting(name, time_of_day, message=\\"\\"): # Your code here pass ```","solution":"def mask_sensitive_info(text): Replaces all digits in the input string with \'*\'. Args: text (str): The input string containing alphanumeric characters. Returns: str: A string where all digits in the input string are replaced with \'*\'. return \'\'.join(\'*\' if char.isdigit() else char for char in text) def custom_format_string(template, values): Formats a dictionary of named fields into a given template string. Args: template (str): A format string containing placeholders e.g., \\"Hello {name}, welcome to {location}.\\" values (dict): A dictionary with keys corresponding to placeholder names in the template string. Returns: str: A formatted string with placeholders replaced by corresponding values from the dictionary. Raises: KeyError: If the template contains placeholders not present in the dictionary. try: return template.format(**values) except KeyError as e: raise e def generate_greeting(name, time_of_day, message=\\"\\"): Generates a customized greeting message. Args: name (str): The name of the person being greeted. time_of_day (str): The time of day. message (str, optional): An optional message to include in the greeting. Defaults to an empty string. Returns: str: A customized greeting message. greeting = f\\"Good {time_of_day}, {name}!\\" if message: greeting += f\\" {message}\\" return greeting"},{"question":"**Problem Statement:** You are tasked with implementing a Python function that performs multiple operations on a custom data structure using protocols defined in Python 3.10\'s abstract objects layer. Your function will handle a custom object which internally maintains a list of numerical values and allows specific manipulations described below. Custom Object Details Create a custom object, `CustomContainer`, that supports: 1. **Adding** numerical values. 2. **Retrieving** numerical values by index. 3. **Returning an iterable** of all numerical values. 4. **Summing all stored numerical values**. Function Details Implement the function `manipulate_custom_container(actions: List[Tuple[str, Union[int, None]]]) -> Union[int, List[int]]` that manages a `CustomContainer` instance and processes a list of actions. Each action is a tuple where the first element is a command and the second element is an optional numerical value. Valid commands and their behaviors are: - `\\"add\\"`: Add the provided numerical value to the container. - `\\"get\\"`: Retrieve the value stored at the provided index. - `\\"iterate\\"`: Return a list of all stored values in the container. - `\\"sum\\"`: Return the sum of all stored values in the container. **Constraints:** - The input list `actions` can contain a maximum of `10^3` actions. - The indices provided for the `\\"get\\"` command will always be valid. **Example:** ```python # Example input actions = [ (\\"add\\", 3), (\\"add\\", 5), (\\"get\\", 1), (\\"iterate\\", None), (\\"add\\", 2), (\\"sum\\", None) ] # Example output # 5 # [3, 5] # 10 def manipulate_custom_container(actions): # Your implementation here ``` **Notes:** - You need to thoroughly understand and implement the protocols for sequence handling, iteration, and numerical operations as specified in the Python 3.10 abstract objects layer documentation. - Ensure your solution is efficient and adheres to the provided constraints.","solution":"from typing import List, Tuple, Union, Optional class CustomContainer: def __init__(self): self.values = [] def add(self, value: int) -> None: self.values.append(value) def get(self, index: int) -> int: return self.values[index] def iterate(self) -> List[int]: return self.values[:] def sum(self) -> int: return sum(self.values) def manipulate_custom_container(actions: List[Tuple[str, Optional[int]]]) -> List[Union[int, List[int]]]: container = CustomContainer() results = [] for action in actions: if action[0] == \\"add\\": container.add(action[1]) elif action[0] == \\"get\\": results.append(container.get(action[1])) elif action[0] == \\"iterate\\": results.append(container.iterate()) elif action[0] == \\"sum\\": results.append(container.sum()) return results"},{"question":"**Question: Configuration File Management with `configparser`** You are required to implement a function that manages user settings stored in a configuration file. The configuration file uses the INI file format. The function will read the settings from the file, update a specific setting, add a new setting if it does not exist, and save the updated settings back to the file. # Function Signature ```python def manage_config_file(file_path: str, section: str, setting: str, value: str) -> bool: Manages the configuration file by updating or adding a specific setting. Args: file_path (str): The path to the configuration file. section (str): The section in the configuration file. setting (str): The setting within the section to be updated or added. value (str): The new value for the setting. Returns: bool: True if the operation was successful, False otherwise. ``` # Input - `file_path` (str): The path to the configuration file. - `section` (str): The section in the configuration file where the setting belongs. - `setting` (str): The setting within the section to be updated or added. - `value` (str): The new value for the setting. # Output - The function should return a boolean value: - `True` if the setting was successfully updated or added and the configuration file was saved. - `False` if an error occurred (e.g., the file does not exist, or the section could not be found/created). # Constraints - You may assume the configuration file path provided is valid. - You may assume the configuration file is well-formed. - Handle the case where the section does not exist by creating it. - If the setting already exists within the section, update its value. - If the setting does not exist within the section, add it with the new value. # Example Given a configuration file `config.ini` with the following content: ``` [UserSettings] theme=light fontsize=12 ``` If the function is called as follows: ```python manage_config_file(\'config.ini\', \'UserSettings\', \'theme\', \'dark\') ``` The resulting configuration file `config.ini` should be updated to: ``` [UserSettings] theme=dark fontsize=12 ``` If the function is called as follows: ```python manage_config_file(\'config.ini\', \'UserSettings\', \'language\', \'English\') ``` The resulting configuration file `config.ini` should be updated to: ``` [UserSettings] theme=dark fontsize=12 language=English ``` # Notes - Use the `configparser` module for reading from and writing to the configuration file. - Ensure that the updated settings are correctly saved back to the file. - Handle exceptions appropriately and return `False` if an error occurs during the operation.","solution":"import configparser import os def manage_config_file(file_path: str, section: str, setting: str, value: str) -> bool: Manages the configuration file by updating or adding a specific setting. Args: file_path (str): The path to the configuration file. section (str): The section in the configuration file. setting (str): The setting within the section to be updated or added. value (str): The new value for the setting. Returns: bool: True if the operation was successful, False otherwise. try: config = configparser.ConfigParser() # Read the configuration file if not os.path.exists(file_path): return False config.read(file_path) # If the section does not exist, add it if not config.has_section(section): config.add_section(section) # Update or add the setting config.set(section, setting, value) # Write the updated settings back to the file with open(file_path, \'w\') as configfile: config.write(configfile) return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"**Question: Creating and Customizing Diverging Color Palettes Using Seaborn** **Objective:** Create a function that generates multiple customized diverging color palettes using seaborn and returns a dictionary of them. This task will demonstrate your understanding of seaborn\'s diverging palette functionalities. **Function Signature:** ```python def generate_diverging_palettes() -> dict: pass ``` **Requirements:** 1. The function should generate and return a dictionary where: - Each key is a string describing the palette. - Each value is the corresponding seaborn color palette object. 2. The function must include the following customized palettes: - A default diverging palette from blue to red. - A diverging palette from blue to red with a dark center. - A continuous colormap rather than a discrete palette from blue to red. - A palette from blue to red with an increased amount of separation around the center value. - A magenta-to-green palette. - A magenta-to-green palette with decreased saturation at the endpoints. - A magenta-to-green palette with decreased lightness at the endpoints. **Constraints:** - The palettes must be created using the `seaborn.diverging_palette` function. - The function should not accept any parameters. **Example:** The expected output should be similar to: ```python { \\"default_blue_red\\": <seaborn.palettes._ColorPalette instance>, \\"dark_center_blue_red\\": <seaborn.palettes._ColorPalette instance>, \\"continuous_blue_red\\": <seaborn.palettes._ColorPalette instance>, \\"increased_separation_blue_red\\": <seaborn.palettes._ColorPalette instance>, \\"magenta_green\\": <seaborn.palettes._ColorPalette instance>, \\"decreased_saturation_magenta_green\\": <seaborn.palettes._ColorPalette instance>, \\"decreased_lightness_magenta_green\\": <seaborn.palettes._ColorPalette instance>, } ``` **Hint:** Refer to the seaborn documentation on the `diverging_palette` function to understand how to configure the different parameters for each type of palette.","solution":"import seaborn as sns def generate_diverging_palettes() -> dict: Generates a dictionary of customized diverging color palettes using seaborn. Returns: dict: A dictionary of seaborn color palette objects. palettes = { \\"default_blue_red\\": sns.diverging_palette(240, 10, n=10), # Default blue to red \\"dark_center_blue_red\\": sns.diverging_palette(240, 10, n=10, center=\\"dark\\"), \\"continuous_blue_red\\": sns.diverging_palette(240, 10, as_cmap=True), \\"increased_separation_blue_red\\": sns.diverging_palette(240, 10, n=10, s=90, l=70), \\"magenta_green\\": sns.diverging_palette(300, 120, n=10), \\"decreased_saturation_magenta_green\\": sns.diverging_palette(300, 120, n=10, s=50), \\"decreased_lightness_magenta_green\\": sns.diverging_palette(300, 120, n=10, l=30) } return palettes"},{"question":"# **Advanced List and Dictionary Manipulation** You are given a dataset that consists of students and their test scores. The data is provided in the form of a list of dictionaries, where each dictionary represents a student and their scores in various subjects. Your task is to write a function `process_scores` that processes this data to generate a specific report. **Input:** - A list of dictionaries `data`, where each dictionary contains: - `name` (string): The student\'s name. - `scores` (dictionary): A dictionary where the keys are subject names (strings) and the values are the scores (integers) the student received in those subjects. Example: ```python data = [ {\\"name\\": \\"Alice\\", \\"scores\\": {\\"math\\": 88, \\"science\\": 92, \\"english\\": 85}}, {\\"name\\": \\"Bob\\", \\"scores\\": {\\"math\\": 75, \\"science\\": 80, \\"english\\": 78}}, {\\"name\\": \\"Charlie\\", \\"scores\\": {\\"math\\": 95, \\"science\\": 85, \\"english\\": 90}}, ] ``` **Output:** - A dictionary with the following structure: - `average_scores` (dictionary): The average score for each subject across all students. - `top_student` (dictionary): The student with the highest total score across all subjects. This dictionary should include: - `name` (string): The student\'s name. - `total_score` (integer): The total score across all subjects. Example: ```python { \\"average_scores\\": {\\"math\\": 86.0, \\"science\\": 85.67, \\"english\\": 84.33}, \\"top_student\\": {\\"name\\": \\"Charlie\\", \\"total_score\\": 270} } ``` **Constraints:** - Each student will have scores for the same set of subjects. - There will be at least one student in the data. - Scores will be non-negative integers. **Function Signature:** ```python def process_scores(data: List[Dict[str, Union[str, Dict[str, int]]]]) -> Dict[str, Any]: pass ``` **Requirements:** 1. Calculate the average score for each subject. 2. Identify the student with the highest total score. 3. Use list comprehensions and dictionary manipulations where appropriate. 4. Ensure the function is efficient and readable. **Hints:** - You may find the `sum()` function useful to calculate total scores. - Consider using a dictionary to track cumulative sums and counts for calculating averages. Write your implementation of the `process_scores` function below.","solution":"from typing import List, Dict, Union, Any def process_scores(data: List[Dict[str, Union[str, Dict[str, int]]]]) -> Dict[str, Any]: subject_totals = {} num_students = len(data) # Initialize subject_totals with zero values for student in data: for subject in student[\'scores\']: if subject not in subject_totals: subject_totals[subject] = 0 # Calculate the sum of scores for each subject for student in data: for subject, score in student[\'scores\'].items(): subject_totals[subject] += score # Calculate the average score for each subject average_scores = {subject: round(total / num_students, 2) for subject, total in subject_totals.items()} # Identify the top student based on total scores top_student = {\\"name\\": \\"\\", \\"total_score\\": 0} for student in data: total_score = sum(student[\'scores\'].values()) if total_score > top_student[\\"total_score\\"]: top_student = { \\"name\\": student[\\"name\\"], \\"total_score\\": total_score } result = { \\"average_scores\\": average_scores, \\"top_student\\": top_student } return result"},{"question":"# Advanced Python 3.10 asyncio Synchronization Problem Description You are tasked with managing a shared resource library system using asyncio synchronization primitives. The system allows a fixed number of users to access books concurrently. Additionally, users need to wait for a special event (e.g., a new book addition) and perform tasks in a coordinated manner. Objective Implement a class `LibrarySystem` that coordinates user access to a shared library of books using `asyncio.Lock`, `asyncio.Event`, and `asyncio.Semaphore`. Class Implementation ```python import asyncio class LibrarySystem: def __init__(self, max_users): Initialize the library system. Parameters: max_users (int): Maximum number of users that can access resources concurrently. # your code here async def access_resource(self, user_id): Simulate a user accessing the resource. Parameters: user_id (int): The ID of the user. # your code here def add_new_book(self): Notify all users about a new book addition. # your code here async def wait_for_new_book(self, user_id): Simulate a user waiting for a new book to be added. Parameters: user_id (int): The ID of the user. # your code here ``` Requirements: - `max_users` defines the maximum number of users that can access the resource at the same time. - `access_resource(user_id)` should acquire a lock to simulate exclusive access to the resource by a user. If the maximum number of users is reached, other users should wait until the resource becomes available. - `add_new_book()` should set an event notifying all waiting users that a new book has been added. - `wait_for_new_book(user_id)` should simulate users waiting for the new book notification. Once the event is set, all waiting users should be notified and perform some action (print a message). Example Usage: ```python async def user_task(library, user_id): await library.access_resource(user_id) await library.wait_for_new_book(user_id) async def main(): library = LibrarySystem(max_users=2) tasks = [user_task(library, i) for i in range(5)] # Start user tasks await asyncio.gather(*tasks) # Add a new book to trigger the event library.add_new_book() asyncio.run(main()) ``` Constraints: - Implement synchronization in a thread-safe manner using `async/await` constructs. - Ensure that users cannot access the resource beyond the maximum allowed number. - Efficiently notify all waiting users when a new book is added. Evaluation Criteria: - Correct use of `asyncio` synchronization primitives. - Handling concurrency properly. - Clear and maintainable code.","solution":"import asyncio class LibrarySystem: def __init__(self, max_users): Initialize the library system. Parameters: max_users (int): Maximum number of users that can access resources concurrently. self.semaphore = asyncio.Semaphore(max_users) self.lock = asyncio.Lock() self.book_event = asyncio.Event() async def access_resource(self, user_id): Simulate a user accessing the resource. Parameters: user_id (int): The ID of the user. async with self.semaphore: print(f\\"User {user_id} is accessing the resource.\\") await asyncio.sleep(1) # Simulate resource usage print(f\\"User {user_id} has finished accessing the resource.\\") def add_new_book(self): Notify all users about a new book addition. self.book_event.set() async def wait_for_new_book(self, user_id): Simulate a user waiting for a new book to be added. Parameters: user_id (int): The ID of the user. print(f\\"User {user_id} is waiting for a new book.\\") await self.book_event.wait() print(f\\"User {user_id} has been notified of new book addition.\\")"},{"question":"**Question:** You are tasked with developing a testbench for a new classification algorithm. To assess its performance under various scenarios, you need to generate multiple artificial datasets using scikit-learn\'s dataset generators. Write a function `generate_datasets` that generates datasets for different classification tasks and returns them in a structured format. # Function Signature ```python def generate_datasets(seed: int) -> dict: Generate various datasets for classification tasks. Arguments: seed -- An integer seed for reproducibility of the datasets. Returns: A dictionary where keys are dataset names, and values are tuples containing the data matrix `X` and the target vector `y` for the following datasets: - \'blobs\': Three normally-distributed clusters. - \'two_clusters_each_class\': Two classes with two informative features and two clusters per class. - \'gaussian_quantiles\': Three classes divided by Gaussian quantiles. - \'moons\': Two interleaving half-circles. - \'circles\': Binary classification with a spherical decision boundary. ``` # Input - `seed` (int): A random seed for reproducibility. # Output - Returns a dictionary with dataset names as keys and tuples (`X`, `y`) as values. Each `X` is an (n_samples, n_features) numpy array, and `y` is a (n_samples,) numpy array: - \'blobs\': dataset generated by `make_blobs` - \'two_clusters_each_class\': dataset generated by `make_classification` with parameters for two classes, two informative features, and two clusters per class - \'gaussian_quantiles\': dataset generated by `make_gaussian_quantiles` - \'moons\': dataset generated by `make_moons` - \'circles\': dataset generated by `make_circles` # Example ```python datasets = generate_datasets(seed=42) # Example usage: for name, (X, y) in datasets.items(): print(f\\"Dataset: {name}, X shape: {X.shape}, y shape: {y.shape}\\") ``` # Constraints - You should use the random seed to ensure reproducibility. - Utilize the corresponding scikit-learn dataset generation functions to create the datasets. - Ensure that the function is efficient and the data is formatted as specified. # Notes - Refer to scikit-learn\'s `datasets` module documentation for detailed parameters and usage of the dataset generation functions.","solution":"from sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles, make_moons, make_circles import numpy as np def generate_datasets(seed: int) -> dict: Generate various datasets for classification tasks. Arguments: seed -- An integer seed for reproducibility of the datasets. Returns: A dictionary where keys are dataset names, and values are tuples containing the data matrix `X` and the target vector `y` for the following datasets: - \'blobs\': Three normally-distributed clusters. - \'two_clusters_each_class\': Two classes with two informative features and two clusters per class. - \'gaussian_quantiles\': Three classes divided by Gaussian quantiles. - \'moons\': Two interleaving half-circles. - \'circles\': Binary classification with a spherical decision boundary. np.random.seed(seed) blobs_X, blobs_y = make_blobs(n_samples=300, centers=3, random_state=seed) clusters_X, clusters_y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, random_state=seed) quantiles_X, quantiles_y = make_gaussian_quantiles(n_samples=300, n_features=2, n_classes=3, random_state=seed) moons_X, moons_y = make_moons(n_samples=300, noise=0.1, random_state=seed) circles_X, circles_y = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=seed) return { \'blobs\': (blobs_X, blobs_y), \'two_clusters_each_class\': (clusters_X, clusters_y), \'gaussian_quantiles\': (quantiles_X, quantiles_y), \'moons\': (moons_X, moons_y), \'circles\': (circles_X, circles_y) }"},{"question":"# Custom Command-Line Interpreter Problem Statement You are required to create a custom command-line interpreter for managing a simple todo list. The interpreter should allow adding new tasks, listing all tasks, marking tasks as completed, and saving the tasks to a file. You will use the `cmd` module to achieve this. # Specifications 1. **Class Definition**: Define a class `TodoShell` that inherits from `cmd.Cmd`. 2. **Commands**: Implement the following commands: - `add <task_description>`: Adds a new task with the given description. - `list`: Lists all tasks with their status (completed or not). - `done <task_id>`: Marks the task with the specified ID as completed. - `save <filename>`: Saves all tasks to the specified file. 3. **Task Storage**: Store the tasks in an internal list. Each task should be a dictionary with `id`, `description`, and `completed` fields. 4. **Command Loop**: Implement the `cmdloop` method to start the interpreter. 5. **Pre- and post-command Hooks**: - Override the `precmd` method to convert the input command to lowercase. - Provide an intro message when the shell starts using the `intro` attribute. 6. **Custom Completion**: Implement custom completion for `done` and `save` commands. # Input and Output - The `add` command should accept a string that represents the task description. - The `list` command should not take any arguments and should display tasks in the format `[ID] Task Description [Completed: Yes/No]`. - The `done` command should accept a task ID (an integer) and mark the corresponding task as completed. - The `save` command should accept a filename and save all tasks to the file in a human-readable format. # Example Usage ```python Welcome to the Todo shell. Type ? to list commands. (todo) add Buy groceries (todo) add Complete assignment (todo) list [1] Buy groceries [Completed: No] [2] Complete assignment [Completed: No] (todo) done 1 (todo) list [1] Buy groceries [Completed: Yes] [2] Complete assignment [Completed: No] (todo) save tasks.txt (todo) bye ``` # Constraints and Limitations 1. The `task_id` should be unique and start from 1. 2. The interpreter should continuously prompt for new commands until the `bye` command is issued. 3. Handle invalid inputs gracefully with an appropriate error message. # Implementation Implement the `TodoShell` class with all the specified functionalities in a Python script. Ensure the script is capable of running independently and providing the user with an interactive todo list management shell. Good luck!","solution":"import cmd class TodoShell(cmd.Cmd): intro = \'Welcome to the Todo shell. Type ? to list commands.n\' prompt = \'(todo) \' def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def do_add(self, arg): \'Add a new task: add <task_description>\' task_description = arg.strip() if task_description: self.tasks.append({ \'id\': self.next_id, \'description\': task_description, \'completed\': False }) self.next_id += 1 print(f\'Task added: {task_description}\') else: print(\'Error: Task description cannot be empty.\') def do_list(self, arg): \'List all tasks\' if not self.tasks: print(\'No tasks available.\') else: for task in self.tasks: status = \'Yes\' if task[\'completed\'] else \'No\' print(f\\"[{task[\'id\']}] {task[\'description\']} [Completed: {status}]\\") def do_done(self, arg): \'Mark a task as completed: done <task_id>\' try: task_id = int(arg.strip()) for task in self.tasks: if task[\'id\'] == task_id: task[\'completed\'] = True print(f\'Task {task_id} marked as completed.\') return print(f\'Error: No task with ID {task_id}.\') except ValueError: print(\'Error: Task ID must be an integer.\') def do_save(self, arg): \'Save tasks to a file: save <filename>\' filename = arg.strip() if filename: with open(filename, \'w\') as file: for task in self.tasks: status = \'Yes\' if task[\'completed\'] else \'No\' file.write(f\\"[{task[\'id\']}] {task[\'description\']} [Completed: {status}]n\\") print(f\'Tasks saved to {filename}.\') else: print(\'Error: Filename cannot be empty.\') def do_bye(self, arg): \'Exit the shell\' print(\'Goodbye!\') return True def emptyline(self): \'Do nothing on empty input line\' pass def precmd(self, line): \'Convert command input to lowercase\' return line.lower() import sys if __name__ == \'__main__\': TodoShell().cmdloop()"},{"question":"# Custom Python Object with Advanced Features You are required to create a custom Python class that mimics some advanced features typically defined using the `PyTypeObject`. Specifically, the class should: 1. Support basic operations like addition and multiplication. 2. Store data in a dynamic array and support common sequence operations such as indexing, slicing, appending, and extending. 3. Support comparison operations. 4. Include custom behaviors for string representation (`__repr__` and `__str__`). # Instructions 1. **Define a class `AdvancedArray`**: - This class should allow instances to be created with an initial list of elements. - Support addition (`+`) and multiplication (`*`) operators to combine and replicate sequences respectively. - Implement indexing to access elements directly. - Implement slicing to retrieve subsets of the array. - Provide methods to `append` an element and to `extend` the array with another sequence. - Implement comparison methods to compare two instances of `AdvancedArray`. - Provide custom `__repr__` and `__str__` methods to return informative string representations of the array. 2. **Functional Requirements**: - Support for `+` operator: `AdvancedArray + AdvancedArray` should return a new `AdvancedArray` containing elements of both arrays. - Support for `*` operator: `AdvancedArray * n` should return a new `AdvancedArray` with elements repeated `n` times. - Indexing: Example `arr[2]` should return the element at index 2. - Slicing: Example `arr[1:3]` should return a new `AdvancedArray` with elements from index 1 to 2. - Methods to append and extend the array. - Comparison based on the contents of the arrays. - Custom `__repr__` and `__str__` methods to return string representations. 3. **Examples**: ```python class AdvancedArray: def __init__(self, elements): # Your implementation here def __add__(self, other): # Your implementation here def __mul__(self, n): # Your implementation here def __getitem__(self, index): # Your implementation here def append(self, element): # Your implementation here def extend(self, sequence): # Your implementation here def __repr__(self): # Your implementation here def __str__(self): # Your implementation here def __eq__(self, other): # Your implementation here def __lt__(self, other): # Your implementation here # Define other comparison methods based on requirement # Example Usage arr1 = AdvancedArray([1, 2, 3]) arr2 = AdvancedArray([4, 5, 6]) arr3 = arr1 + arr2 # [1, 2, 3, 4, 5, 6] arr4 = arr1 * 2 # [1, 2, 3, 1, 2, 3] arr1.append(7) # [1, 2, 3, 7] arr1.extend([8, 9]) # [1, 2, 3, 7, 8, 9] print(arr1[2]) # 3 print(arr1[1:4]) # [2, 3, 7] print(repr(arr1)) # AdvancedArray([1, 2, 3, 7, 8, 9]) print(str(arr1)) # [1, 2, 3, 7, 8, 9] print(arr1 == arr2) # False print(arr1 < arr2) # True or False depending on content ``` # Constraints - You should not use any external libraries except for core Python modules. - The implementation should handle edge cases and provide meaningful results.","solution":"class AdvancedArray: def __init__(self, elements): self.elements = elements def __add__(self, other): if isinstance(other, AdvancedArray): return AdvancedArray(self.elements + other.elements) raise TypeError(f\\"Unsupported operand type(s) for +: \'AdvancedArray\' and \'{type(other).__name__}\'\\") def __mul__(self, n): if isinstance(n, int): return AdvancedArray(self.elements * n) raise TypeError(f\\"Unsupported operand type(s) for *: \'AdvancedArray\' and \'{type(n).__name__}\'\\") def __getitem__(self, index): if isinstance(index, slice): return AdvancedArray(self.elements[index]) return self.elements[index] def append(self, element): self.elements.append(element) def extend(self, sequence): self.elements.extend(sequence) def __repr__(self): return f\\"AdvancedArray({self.elements})\\" def __str__(self): return str(self.elements) def __eq__(self, other): if isinstance(other, AdvancedArray): return self.elements == other.elements return False def __lt__(self, other): if isinstance(other, AdvancedArray): return self.elements < other.elements return False def __le__(self, other): if isinstance(other, AdvancedArray): return self.elements <= other.elements return False def __gt__(self, other): if isinstance(other, AdvancedArray): return self.elements > other.elements return False def __ge__(self, other): if isinstance(other, AdvancedArray): return self.elements >= other.elements return False"},{"question":"Objective: To assess the understanding and implementation of iterators, generators, and the use of functions from the `itertools` and `functools` modules in Python. Problem Statement: You are tasked with creating a flexible data pipeline using the principles of functional programming. Specifically, you need to construct a generator function that processes a sequence of data through a series of transformation and filtering stages. Additionally, you must aggregate the results using higher-order functions. Requirements: 1. **Data Generator**: - Define a generator function `data_stream(n)` that yields integers from `0` to `n-1`. 2. **Transformation Stage**: - Implement a generator function `transform_data(stream)` that squares each number in the input stream. 3. **Filtering Stage**: - Implement a generator function `filter_data(stream)` that only yields even numbers from the transformed data. 4. **Aggregation Stage**: - Use `functools.reduce()` to sum up the filtered data. 5. **`itertools.tee` Utility**: - Use `itertools.tee()` to create two independent iterators from the filtered data stream and return both iterators. Print the contents of both iterators to verify their independence. Implementation: - Define these functions and complete the transformation pipeline with appropriate generator and iterator techniques as specified above. Example: Given `n=10`, the steps would involve: - Generating numbers from 0 to 9. - Squaring each number to get `[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]`. - Filtering even squares to get `[0, 4, 16, 36, 64]`. - Summing these values to return `120`. - Demonstrating that the two iterators created using `itertools.tee` are independent copies of the filtered data stream. Constraints: - You may assume `n` is a non-negative integer. - All functions should be implemented in a functional programming style, avoiding side effects. Code Template: ```python from functools import reduce import itertools def data_stream(n): Generator function that yields integers from 0 to n-1. # Your code here def transform_data(stream): Generator function that yields the square of each number in the input stream. # Your code here def filter_data(stream): Generator function that yields only even numbers from the input stream. # Your code here def aggregate_data(filtered_stream): Use `functools.reduce` to sum up the filtered data. # Your code here def verify_tee(stream): Use `itertools.tee` to create two independent iterators and print their contents. # Your code here # Example Usage: n = 10 stream = data_stream(n) transformed_stream = transform_data(stream) filtered_stream = filter_data(transformed_stream) sum_result = aggregate_data(filtered_stream) print(f\\"Sum of filtered data: {sum_result}\\") verify_tee(filter_data(transform_data(data_stream(n)))) ``` Performance: Ensure that your generator functions are implemented efficiently, using lazy evaluation to handle potentially large or infinite datasets.","solution":"from functools import reduce import itertools def data_stream(n): Generator function that yields integers from 0 to n-1. for i in range(n): yield i def transform_data(stream): Generator function that yields the square of each number in the input stream. for data in stream: yield data ** 2 def filter_data(stream): Generator function that yields only even numbers from the input stream. for data in stream: if data % 2 == 0: yield data def aggregate_data(filtered_stream): Use `functools.reduce` to sum up the filtered data. return reduce(lambda x, y: x + y, filtered_stream, 0) # Use 0 as the initializer for reduce def verify_tee(stream): Use `itertools.tee` to create two independent iterators and print their contents. iter1, iter2 = itertools.tee(stream) list1 = list(iter1) list2 = list(iter2) print(f\\"Contents of iter1: {list1}\\") print(f\\"Contents of iter2: {list2}\\") return list1, list2 # Return them to help with verification in tests # Example Usage: if __name__ == \'__main__\': n = 10 stream = data_stream(n) transformed_stream = transform_data(stream) filtered_stream = filter_data(transformed_stream) sum_result = aggregate_data(filtered_stream) print(f\\"Sum of filtered data: {sum_result}\\") verify_tee(filter_data(transform_data(data_stream(n))))"},{"question":"**Objective**: Demonstrate the ability to use pandas options to customize DataFrame display settings effectively. **Problem Statement**: You are working on a data analysis project and need to customize the display settings of pandas DataFrames to improve the readability of your analysis. Your task is to implement a function `customize_display_settings(df: pd.DataFrame) -> pd.DataFrame` that performs the following: 1. Set the following display options: - Max rows displayed to 10. - Min rows displayed when maximum rows are exceeded to 5. - Max columns displayed to 8. - Column width to 20. - Display precision to 2 decimal places. 2. Within the function, print the current settings of the above options. 3. Return the original DataFrame. # Constraints: - The function should accept a pandas DataFrame as input. - The function should not modify the DataFrame itself but only the display settings. # Input: - A pandas DataFrame, `df`. # Output: - The unchanged pandas DataFrame, `df`. # Example: ```python import pandas as pd import numpy as np # Sample DataFrame data = np.random.randn(15, 10) df = pd.DataFrame(data) # Call the function result_df = customize_display_settings(df) # Displaying the result DataFrame outside the function will reflect the new display settings print(result_df) ``` # Expected Output: The display settings should be adjusted as described in the task, and the DataFrame should be printed with these settings applied. # Notes: - Ensure that the function logs the current settings of the options specified before returning the DataFrame. - Use the appropriate pandas functions (`set_option`, `get_option`, etc.) to achieve the desired display customization.","solution":"import pandas as pd def customize_display_settings(df: pd.DataFrame) -> pd.DataFrame: Customize pandas DataFrame display settings for improved readability and return the DataFrame. # Set the display options pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.min_rows\', 5) pd.set_option(\'display.max_columns\', 8) pd.set_option(\'display.max_colwidth\', 20) pd.set_option(\'display.precision\', 2) # Print the current settings print(\\"Max rows:\\", pd.get_option(\'display.max_rows\')) print(\\"Min rows:\\", pd.get_option(\'display.min_rows\')) print(\\"Max columns:\\", pd.get_option(\'display.max_columns\')) print(\\"Max column width:\\", pd.get_option(\'display.max_colwidth\')) print(\\"Display precision:\\", pd.get_option(\'display.precision\')) # Return the original DataFrame return df"},{"question":"Objective: The goal of this task is to assess your understanding of the `DictVectorizer` class from the `scikit-learn.feature_extraction` module. You are required to transform a collection of feature dictionaries into numerical feature vectors using this utility. Problem Statement: You are given a list of dictionaries where each dictionary contains information about a movie, including its year of release and categories (genres). Your task is to implement a function `transform_movie_data(movie_data: List[Dict[str, Any]]) -> Tuple[np.ndarray, List[str]]` that utilizes `DictVectorizer` to convert these dictionaries into a format that can be used for machine learning algorithms. Function Signature: ```python from typing import List, Dict, Any, Tuple import numpy as np def transform_movie_data(movie_data: List[Dict[str, Any]]) -> Tuple[np.ndarray, List[str]]: pass ``` Input: - `movie_data`: A list of dictionaries where each dictionary contains information about a movie\'s `category` and `year`. Output: - Returns a tuple containing: - A NumPy array with the transformed feature vectors. - A list of feature names corresponding to the columns in the transformed array. Constraints: - Use the `DictVectorizer` from `sklearn.feature_extraction` for the transformation. - Assume all movies have at least the `year` key, but `category` is optional. - Handle multiple categories for a movie appropriately. Example: ```python movie_data = [ {\'category\': [\'thriller\', \'drama\'], \'year\': 2003}, {\'category\': [\'animation\', \'family\'], \'year\': 2011}, {\'year\': 1974} ] transformed_data, feature_names = transform_movie_data(movie_data) # Output # transformed_data: array with shape (3, 5) # feature_names: [\'category=animation\', \'category=drama\', \'category=family\', \'category=thriller\', \'year\'] ``` Implementation Requirements: - Ensure the function handles categorical data correctly, implementing one-hot encoding for multiple categories. - Make sure the function returns the transformed data as a NumPy array and the corresponding feature names as a list. # Hints: - Use `DictVectorizer` from `sklearn.feature_extraction` to fit and transform the provided input data. - The `fit_transform` method of `DictVectorizer` can be used to get the transformed data. - Use `get_feature_names_out` method of `DictVectorizer` to retrieve the list of feature names.","solution":"from typing import List, Dict, Any, Tuple import numpy as np from sklearn.feature_extraction import DictVectorizer def transform_movie_data(movie_data: List[Dict[str, Any]]) -> Tuple[np.ndarray, List[str]]: Transforms a list of movie feature dictionaries into numerical feature vectors. Parameters: - movie_data: List[Dict[str, Any]] : A list of dictionaries where each dictionary contains information about a movie\'s `category` and `year`. Returns: - Tuple[np.ndarray, List[str]] : A tuple containing: - A NumPy array with the transformed feature vectors. - A list of feature names corresponding to the columns in the transformed array. # Initialize the DictVectorizer dv = DictVectorizer(sparse=False) # Fit and transform the movie data transformed_data = dv.fit_transform(movie_data) # Get the names of the features feature_names = dv.get_feature_names_out().tolist() return transformed_data, feature_names"},{"question":"**Objective:** You are required to write a Python function that demonstrates your understanding of the `base64` module in Python 3.10. The function will take a binary file, encode its content using Base64 encoding, transmit it, and then decode it back to verify data integrity. Additionally, it will utilize alternate alphabets for encoding and decode the message using URL-safe Base64 encoding. **Problem Statement:** Write a function `encode_decode_file(input_file_path: str, output_file_path: str, altchars: bytes)` that performs the following steps: 1. Reads binary data from the input file specified by `input_file_path`. 2. Encodes the binary data using the `base64.b64encode` function with an alternate alphabet specified by `altchars`. 3. Encodes the result again using URL-safe Base64 encoding. 4. Decodes the URL-safe Base64 encoded data back to the original Base64 encoded data. 5. Decodes the original Base64 encoded data back to the original binary data using the same `altchars`. 6. Writes the decoded binary data to the output file specified by `output_file_path`. 7. Ensure that the binary data written to the output file is identical to the data read from the input file. **Function Signature:** ```python def encode_decode_file(input_file_path: str, output_file_path: str, altchars: bytes) -> None: pass ``` **Parameters:** - `input_file_path` (str): Path to the binary input file to be read. - `output_file_path` (str): Path to the binary output file to be written. - `altchars` (bytes): A bytes-like object of length 2 which specifies an alternate alphabet for the \\"+\\" and \\"/\\" characters in Base64 encoding. **Constraints:** - The length of `altchars` must be exactly 2. Raise a `ValueError` if this condition is not met. - Use the `base64` module only. **Example Usage:** ```python # Assuming \'input.bin\' contains binary data (e.g., b\'Hello, World!\') # and altchars is set to b\'-_\' encode_decode_file(\'input.bin\', \'output.bin\', b\'-_\') # \'output.bin\' should contain the same binary data as in \'input.bin\' ``` **Notes:** - You can use the `assert` statement to check if the file\'s binary contents match before and after encoding/decoding. - Handle potential exceptions like `ValueError` and `binascii.Error` gracefully and print an appropriate error message. This question will test the ability to work with file I/O in Python, understand multiple encoding schemes, and handle errors and edge cases effectively.","solution":"import base64 def encode_decode_file(input_file_path: str, output_file_path: str, altchars: bytes) -> None: if len(altchars) != 2: raise ValueError(\\"altchars must be a bytes-like object of length 2\\") try: # Step 1: Read binary data from input file with open(input_file_path, \'rb\') as input_file: binary_data = input_file.read() # Step 2: Encode binary data using Base64 with altchars base64_encoded = base64.b64encode(binary_data, altchars) # Step 3: Encode the result using URL-safe Base64 encoding url_safe_encoded = base64.urlsafe_b64encode(base64_encoded) # Step 4: Decode URL-safe Base64 encoded data back to original Base64 encoded data decoded_base64_encoded = base64.urlsafe_b64decode(url_safe_encoded) # Step 5: Decode the original Base64 encoded data back to binary data using altchars decoded_binary_data = base64.b64decode(decoded_base64_encoded, altchars) # Step 6: Write the decoded binary data to the output file with open(output_file_path, \'wb\') as output_file: output_file.write(decoded_binary_data) except (ValueError, base64.binascii.Error) as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Design a Python script that simulates a basic calculator using the `argparse` library. The calculator should be able to perform addition, subtraction, multiplication, and division based on user input via command-line arguments. Requirements 1. The script should accept two positional arguments, `num1` and `num2`, which represent the two numbers for the calculation. 2. The operation to be performed between `num1` and `num2` should be specified by an optional argument (`--add`, `--subtract`, `--multiply`, `--divide`) with short flags (`-a`, `-s`, `-m`, `-d`) respectively. However, only one of these operations can be specified at a time. 3. Use a mutually exclusive group to ensure that only one operation is selected. 4. Implement verbosity for the program with an optional `--verbose` flag (`-v`). When this flag is present, the script should print detailed output. 5. The script should handle errors gracefully and provide helpful messages if the input is incorrect or missing. Input - `num1`: A number (integer or float). - `num2`: A number (integer or float). - One of the following operations: - `--add` or `-a` - `--subtract` or `-s` - `--multiply` or `-m` - `--divide` or `-d` - Optional: `--verbose` or `-v` Output - The result of the chosen operation between `num1` and `num2`. - If the `--verbose` flag is used, additional information about the operation and the operands should be printed. Constraints 1. If no operation is specified, the script should print an error message. 2. If both `num1` and `num2` are not specified, the script should print an error message. 3. The script should handle division by zero gracefully, printing an error message instead. Example Usage 1. Basic usage for addition without verbosity: ```sh python calculator.py 3 4 --add ``` Output: ``` 7 ``` 2. Basic usage for subtraction with verbosity: ```sh python calculator.py 10 4 -s -v ``` Output: ``` Subtracting 4 from 10 results in 6. 6 ``` 3. Handling division by zero: ```sh python calculator.py 10 0 --divide ``` Output: ``` Error: Division by zero is not allowed. ``` Implementation Provide the Python script implementation that meets the above requirements. ```python import argparse def main(): parser = argparse.ArgumentParser(description=\\"A simple command-line calculator\\") parser.add_argument(\\"num1\\", type=float, help=\\"the first number\\") parser.add_argument(\\"num2\\", type=float, help=\\"the second number\\") group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\\"-a\\", \\"--add\\", action=\\"store_true\\", help=\\"perform addition\\") group.add_argument(\\"-s\\", \\"--subtract\\", action=\\"store_true\\", help=\\"perform subtraction\\") group.add_argument(\\"-m\\", \\"--multiply\\", action=\\"store_true\\", help=\\"perform multiplication\\") group.add_argument(\\"-d\\", \\"--divide\\", action=\\"store_true\\", help=\\"perform division\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"increase output verbosity\\") args = parser.parse_args() if args.add: result = args.num1 + args.num2 operation = \\"addition\\" elif args.subtract: result = args.num1 - args.num2 operation = \\"subtraction\\" elif args.multiply: result = args.num1 * args.num2 operation = \\"multiplication\\" elif args.divide: if args.num2 == 0: print(\\"Error: Division by zero is not allowed.\\") return result = args.num1 / args.num2 operation = \\"division\\" if args.verbose: print(f\\"Performing {operation} on {args.num1} and {args.num2}. Result: {result}\\") else: print(result) if __name__ == \\"__main__\\": main() ```","solution":"import argparse def perform_operation(args): if args.add: result = args.num1 + args.num2 operation = \\"addition\\" elif args.subtract: result = args.num1 - args.num2 operation = \\"subtraction\\" elif args.multiply: result = args.num1 * args.num2 operation = \\"multiplication\\" elif args.divide: if args.num2 == 0: return \\"Error: Division by zero is not allowed.\\", None result = args.num1 / args.num2 operation = \\"division\\" else: return \\"Error: No operation specified.\\", None if args.verbose: return f\\"Performing {operation} on {args.num1} and {args.num2}. Result: {result}\\", result else: return str(result), result def main(): parser = argparse.ArgumentParser(description=\\"A simple command-line calculator\\") parser.add_argument(\\"num1\\", type=float, help=\\"the first number\\") parser.add_argument(\\"num2\\", type=float, help=\\"the second number\\") group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\\"-a\\", \\"--add\\", action=\\"store_true\\", help=\\"perform addition\\") group.add_argument(\\"-s\\", \\"--subtract\\", action=\\"store_true\\", help=\\"perform subtraction\\") group.add_argument(\\"-m\\", \\"--multiply\\", action=\\"store_true\\", help=\\"perform multiplication\\") group.add_argument(\\"-d\\", \\"--divide\\", action=\\"store_true\\", help=\\"perform division\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"increase output verbosity\\") args = parser.parse_args() output, _ = perform_operation(args) print(output) if __name__ == \\"__main__\\": main()"},{"question":"Objective Demonstrate your understanding of the scikit-learn library by loading a dataset, preprocessing the data, and applying a machine learning algorithm to achieve a specific task. Question You are required to load the Diabetes dataset from scikit-learn, preprocess the data, and train a regression model to predict the disease progression based on the input features. Follow the steps below to accomplish this: 1. Load the Diabetes dataset using `load_diabetes` from the `sklearn.datasets` module. 2. Split the dataset into training and testing sets. Use 80% of the data for training and the remaining 20% for testing. 3. Preprocess the data: - Standardize the feature values so that their mean is 0 and their variance is 1. 4. Train a regression model using the `LinearRegression` algorithm from `sklearn.linear_model`. 5. Evaluate the model\'s performance using Mean Squared Error (MSE) on the testing set. Expected Input and Output * Input: None (Other than the specified dataset loading from scikit-learn). * Output: Mean Squared Error of the predictions on the test dataset. Implementation Constraints and Performance Requirements - You should use the `train_test_split` function from `sklearn.model_selection` to split the data. - For standardization, use `StandardScaler` from `sklearn.preprocessing`. - The regression model should be `LinearRegression` from `sklearn.linear_model`. - The performance of the model should be evaluated using the `mean_squared_error` function from `sklearn.metrics`. Example ```python import numpy as np from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error # Step 1: Load the dataset data = load_diabetes() X, y = data.data, data.target # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Preprocess the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 4: Train a regression model model = LinearRegression() model.fit(X_train_scaled, y_train) # Step 5: Evaluate the model\'s performance y_pred = model.predict(X_test_scaled) mse = mean_squared_error(y_test, y_pred) print(f\\"Mean Squared Error: {mse}\\") ``` This example showcases the entire process from loading the dataset to evaluating the model\'s performance. Make sure your implementation follows these steps and structure.","solution":"import numpy as np from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def load_and_process_diabetes(): # Step 1: Load the dataset data = load_diabetes() X, y = data.data, data.target # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Preprocess the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 4: Train a regression model model = LinearRegression() model.fit(X_train_scaled, y_train) # Step 5: Evaluate the model\'s performance y_pred = model.predict(X_test_scaled) mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"# Advanced Python Coding Assessment Question: Managing and Analyzing Garbage Collection Garbage collection is a crucial aspect of memory management in Python. Using the `gc` module, you will be required to create a class that: 1. Sets up garbage collection thresholds. 2. Tracks memory allocation and deallocations. 3. Retrieves and analyzes garbage collection statistics. 4. Handles potential memory leaks. **Requirement:** Create a class `GarbageCollectionManager` that performs the following operations: 1. **Initialization:** - Accepts and sets garbage collection thresholds (`threshold0`, `threshold1`, `threshold2`). 2. **Enable/Disable Garbage Collection:** - Methods to enable (`enable_gc`) and disable (`disable_gc`) automatic garbage collection. - Method to check if garbage collection is enabled (`is_gc_enabled`). 3. **Perform Manual Collection:** - Method to trigger a manual garbage collection (`manual_collect`) with an optional generation parameter. 4. **Retrieve and Display Statistics:** - Method to get current collection counts (`get_current_counts`). - Method to get current garbage collection thresholds (`get_gc_thresholds`). - Method to retrieve and print detailed garbage collection statistics (`print_gc_stats`). 5. **Handling Callbacks:** - Method to add a callback that logs information (`add_callback`), called before and after garbage collection. 6. **Leak Handling:** - Method to enable leak debugging (`enable_leak_debugging`). - Method to print uncollectable objects (`print_uncollectable_objects`). Here’s a sample outline of the class: ```python import gc class GarbageCollectionManager: def __init__(self, threshold0, threshold1, threshold2): # Set GC thresholds gc.set_threshold(threshold0, threshold1, threshold2) def enable_gc(self): # Enable GC gc.enable() def disable_gc(self): # Disable GC gc.disable() def is_gc_enabled(self): # Check if GC is enabled return gc.isenabled() def manual_collect(self, generation=2): # Trigger manual garbage collection return gc.collect(generation) def get_current_counts(self): # Get current GC counts return gc.get_count() def get_gc_thresholds(self): # Get current GC thresholds return gc.get_threshold() def print_gc_stats(self): # Print GC statistics stats = gc.get_stats() for gen, data in enumerate(stats): print(f\\"Generation {gen} - {data}\\") def add_callback(self, callback): # Add GC callback gc.callbacks.append(callback) def enable_leak_debugging(self): # Enable leak debugging gc.set_debug(gc.DEBUG_LEAK) def print_uncollectable_objects(self): # Print uncollectable objects for obj in gc.garbage: print(obj) # Example usage gcm = GarbageCollectionManager(700, 10, 10) gcm.enable_gc() print(gcm.is_gc_enabled()) gcm.manual_collect(1) gcm.print_gc_stats() ``` **Constraints:** - Ensure that the class properly handles invalid inputs (e.g., thresholds must be non-negative integers). - Properly document each method within the class. **Submission:** - Create the file `garbage_collection_manager.py` containing your class definition and any supporting code. - Include a `README.md` file explaining usage and examples. **Evaluation Criteria:** - Correctness of the implementation. - Robustness and error handling. - Clear documentation and code comments. - Effective use of the `gc` module features.","solution":"import gc class GarbageCollectionManager: def __init__(self, threshold0, threshold1, threshold2): if not all(isinstance(th, int) and th >= 0 for th in (threshold0, threshold1, threshold2)): raise ValueError(\\"Thresholds must be non-negative integers.\\") gc.set_threshold(threshold0, threshold1, threshold2) def enable_gc(self): gc.enable() def disable_gc(self): gc.disable() def is_gc_enabled(self): return gc.isenabled() def manual_collect(self, generation=2): if not isinstance(generation, int) or generation < 0: raise ValueError(\\"Generation must be a non-negative integer.\\") return gc.collect(generation) def get_current_counts(self): return gc.get_count() def get_gc_thresholds(self): return gc.get_threshold() def print_gc_stats(self): stats = gc.get_stats() for gen, data in enumerate(stats): print(f\\"Generation {gen} - {data}\\") def add_callback(self, callback): if not callable(callback): raise ValueError(\\"Callback must be a callable.\\") gc.callbacks.append(callback) def enable_leak_debugging(self): gc.set_debug(gc.DEBUG_LEAK) def print_uncollectable_objects(self): for obj in gc.garbage: print(obj) # Example usage if __name__ == \\"__main__\\": gcm = GarbageCollectionManager(700, 10, 10) gcm.enable_gc() print(gcm.is_gc_enabled()) gcm.manual_collect(1) gcm.print_gc_stats()"},{"question":"**Task: Secure Message Transmission Using SSLContext** In this task, you are required to implement a Python program that demonstrates secure client-server communication using Python\'s `ssl` module. Follow the guidelines below to meet the requirements: # Requirements 1. **SSL Context Setup:** - Create an `SSLContext` for both client and server using secure defaults. - Ensure the use of `PROTOCOL_TLS_CLIENT` for the client context and `PROTOCOL_TLS_SERVER` for the server context. - Configure the server to require client certificate authentication. 2. **Certificates:** - For this exercise, generate self-signed certificates for both the client and the server (you may use OpenSSL to generate these locally). 3. **Client-Server Communication:** - Implement a simple server that listens on a specified IP and port, accepts connections, and securely communicates with the client. - The server should send a \\"Hello, Secure World!\\" message to the client after establishing the connection. - The client should connect to the server, verify the server\'s certificate, and print the received message. 4. **Expected Input and Output:** - The server should output logs about its status, including when it starts, accepts a connection, and sends a message. - The client should output logs about its status, including connection attempts, certificate verification, and received messages. 5. **Constraints:** - Implement proper error handling for SSL-related errors. - Make sure that both server and client run on the same machine for simplicity. 6. **Performance:** - The solution should be efficient and handle multiple client connections. # Example ```python import ssl import socket from threading import Thread def setup_server_context(): # Setup server side SSL context context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) context.load_cert_chain(certfile=\\"server_cert.pem\\", keyfile=\\"server_key.pem\\") context.verify_mode = ssl.CERT_REQUIRED context.load_verify_locations(cafile=\\"client_cert.pem\\") return context def handle_client(connection): print(\\"Client connected\\") try: connection.sendall(b\\"Hello, Secure World!\\") finally: connection.shutdown(socket.SHUT_RDWR) connection.close() def start_server(): server_context = setup_server_context() with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.bind((\'127.0.0.1\', 8443)) sock.listen(5) with server_context.wrap_socket(sock, server_side=True) as ssock: while True: conn, _ = ssock.accept() Thread(target=handle_client, args=(conn,)).start() def setup_client_context(): # Setup client side SSL context context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH) context.load_verify_locations(cafile=\\"server_cert.pem\\") context.load_cert_chain(certfile=\\"client_cert.pem\\", keyfile=\\"client_key.pem\\") return context def start_client(): client_context = setup_client_context() with socket.create_connection((\'127.0.0.1\', 8443)) as sock: with client_context.wrap_socket(sock, server_hostname=\'localhost\') as ssock: print(ssock.recv(1024).decode()) if __name__ == \\"__main__\\": server_thread = Thread(target=start_server) server_thread.start() start_client() ``` # Submission Guidelines - Submit both server and client code files. - Include instructions to generate certificates and any additional setup steps required to run your solution.","solution":"import ssl import socket from threading import Thread def setup_server_context(): Sets up the SSL context for the server using TLS and requires client authentication. context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) context.load_cert_chain(certfile=\\"server_cert.pem\\", keyfile=\\"server_key.pem\\") context.verify_mode = ssl.CERT_REQUIRED context.load_verify_locations(cafile=\\"client_cert.pem\\") return context def handle_client(connection): Handles communication with the client. print(\\"Client connected\\") try: connection.sendall(b\\"Hello, Secure World!\\") finally: connection.shutdown(socket.SHUT_RDWR) connection.close() def start_server(): Starts the secure server to accept client connections. server_context = setup_server_context() with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.bind((\'127.0.0.1\', 8443)) sock.listen(5) with server_context.wrap_socket(sock, server_side=True) as ssock: while True: conn, _ = ssock.accept() Thread(target=handle_client, args=(conn,)).start() def setup_client_context(): Sets up the SSL context for the client using TLS and verifies the server\'s certificate. context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH) context.load_verify_locations(cafile=\\"server_cert.pem\\") context.load_cert_chain(certfile=\\"client_cert.pem\\", keyfile=\\"client_key.pem\\") return context def start_client(): Starts the client and connects to the server securely. client_context = setup_client_context() with socket.create_connection((\'127.0.0.1\', 8443)) as sock: with client_context.wrap_socket(sock, server_hostname=\'localhost\') as ssock: print(ssock.recv(1024).decode()) if __name__ == \\"__main__\\": server_thread = Thread(target=start_server) server_thread.start() start_client()"},{"question":"# Question: Unicode Manipulation and Encoding/Decoding You are required to implement a function that handles various operations related to Unicode in Python. The function should be named `process_unicode` and it should accomplish the following tasks: 1. **Create a Unicode String**: Create a Unicode string composed of the following: - The character \'A\'. - The emoji \'😀\'. - The Chinese character \'汉\'. - The Greek capital letter \'Δ\'. 2. **Normalize the String**: Normalize this Unicode string using the NFC (Normalization Form C) format. 3. **Encode the String**: Encode the normalized Unicode string to UTF-8 format. 4. **Decode the Bytes**: Decode the UTF-8 encoded bytes back to a Unicode string. 5. **Extract Unicode Properties**: Use the `unicodedata` module to fetch the name and category of each character in the decoded string. 6. **Generate the Output**: Return a list where the first element is the decoded string and the subsequent elements are tuples. Each tuple should contain the character, its Unicode name, and its category. **Function Signature:** ```python def process_unicode() -> list: pass ``` Example: ```python result = process_unicode() print(result) ``` Output: ```python [ \'A😀汉Δ\', (\'A\', \'LATIN CAPITAL LETTER A\', \'Lu\'), (\'😀\', \'GRINNING FACE\', \'So\'), (\'汉\', \'CJK UNIFIED IDEOGRAPH-6C49\', \'Lo\'), (\'Δ\', \'GREEK CAPITAL LETTER DELTA\', \'Lu\') ] ``` **Constraints and Considerations:** - You must use the appropriate methods from the `unicodedata` module to fetch the Unicode name and category. - Ensure your function handles the Unicode normalization and encoding/decoding processes correctly. - Focus on the robustness and correctness of your implementation.","solution":"import unicodedata def process_unicode(): # Step 1: Create a Unicode string unicode_string = \'A😀汉Δ\' # Step 2: Normalize the string using NFC normalized_string = unicodedata.normalize(\'NFC\', unicode_string) # Step 3: Encode the normalized string to UTF-8 format utf8_encoded = normalized_string.encode(\'utf-8\') # Step 4: Decode the UTF-8 encoded bytes back to a Unicode string utf8_decoded = utf8_encoded.decode(\'utf-8\') # Step 5: Extract Unicode properties using the unicodedata module result = [utf8_decoded] for char in utf8_decoded: char_name = unicodedata.name(char) char_category = unicodedata.category(char) result.append((char, char_name, char_category)) return result"},{"question":"Create a Python program using the `os` module to manage a directory structure and files within it. Your program should fulfill the following requirements: 1. **Create Directories and Files**: - Create a directory named `Project`. - Inside the `Project` directory, create two subdirectories: `Docs` and `Source`. - In each of these subdirectories, create a respective file named `readme.txt` for `Docs` and `main.py` for `Source`. 2. **Write to Files**: - In `Docs/readme.txt`, write the text: \\"Project Documentation\\". - In `Source/main.py`, write the text: `print(\\"Hello, World!\\")`. 3. **List Directory Contents**: - Print out the structure and contents of the `Project` directory, including subdirectories and files. 4. **Environment Variables**: - Set an environment variable `PROJECT_HOME` to the absolute path of the `Project` directory. - Retrieve and print the value of the environment variable `PROJECT_HOME`. 5. **Process Management**: - Simulate the creation of a child process using `os.fork()`. In the child process, print \\"Child Process started\\" and then exit using `os._exit(0)`. - Wait for the child process to complete in the parent process and print \\"Child Process finished\\". # Input and Output Requirements: - No input is required from the user. - The program should print the structure and contents of the `Project` directory. - The program should print the value of the environment variable `PROJECT_HOME`. - The program should print messages indicating the start and finish of the child process. # Constraints and Notes: - Ensure that the program works in a Unix-like environment where `os.fork()` is available. - Use appropriate error handling to handle situations like the existence of directories and files. # Example Output: ``` Directory structure created. Docs/ readme.txt Source/ main.py Environment variable PROJECT_HOME set to: /absolute/path/to/Project Child Process started Child Process finished ``` # Implementation: Implement the function `manage_project_structure()` to complete the task as described. ```python import os def manage_project_structure(): # Step 1: Create Directories and Files os.makedirs(\'Project/Docs\', exist_ok=True) os.makedirs(\'Project/Source\', exist_ok=True) with open(\'Project/Docs/readme.txt\', \'w\') as f: f.write(\\"Project Documentation\\") with open(\'Project/Source/main.py\', \'w\') as f: f.write(\'print(\\"Hello, World!\\")\') # Step 2: List Directory Contents for root, dirs, files in os.walk(\'Project\'): level = root.replace(\'Project\', \'\').count(os.sep) indent = \' \' * 4 * level print(\'{}{}/\'.format(indent, os.path.basename(root))) subindent = \' \' * 4 * (level + 1) for f in files: print(\'{}{}\'.format(subindent, f)) # Step 3: Set and Retrieve Environment Variable project_home = os.path.abspath(\'Project\') os.environ[\'PROJECT_HOME\'] = project_home print(f\\"Environment variable PROJECT_HOME set to: {os.getenv(\'PROJECT_HOME\')}\\") # Step 4: Process Management pid = os.fork() if pid == 0: # Child Process print(\\"Child Process started\\") os._exit(0) else: # Parent Process os.waitpid(pid, 0) print(\\"Child Process finished\\") # Execute the function manage_project_structure() ```","solution":"import os def manage_project_structure(): # Step 1: Create Directories and Files os.makedirs(\'Project/Docs\', exist_ok=True) os.makedirs(\'Project/Source\', exist_ok=True) with open(\'Project/Docs/readme.txt\', \'w\') as f: f.write(\\"Project Documentation\\") with open(\'Project/Source/main.py\', \'w\') as f: f.write(\'print(\\"Hello, World!\\")\') # Step 2: List Directory Contents for root, dirs, files in os.walk(\'Project\'): level = root.replace(\'Project\', \'\').count(os.sep) indent = \' \' * 4 * level print(\'{}{}/\'.format(indent, os.path.basename(root))) subindent = \' \' * 4 * (level + 1) for f in files: print(\'{}{}\'.format(subindent, f)) # Step 3: Set and Retrieve Environment Variable project_home = os.path.abspath(\'Project\') os.environ[\'PROJECT_HOME\'] = project_home print(f\\"Environment variable PROJECT_HOME set to: {os.getenv(\'PROJECT_HOME\')}\\") # Step 4: Process Management pid = os.fork() if pid == 0: # Child Process print(\\"Child Process started\\") os._exit(0) else: # Parent Process os.waitpid(pid, 0) print(\\"Child Process finished\\") # Execute the function manage_project_structure()"},{"question":"# Task: Implement Clustering Using DBSCAN and Evaluate with Silhouette Coefficient Objective You are required to implement a clustering algorithm using DBSCAN (Density-Based Spatial Clustering of Applications with Noise) from the `sklearn.cluster` module and evaluate the clustering performance using the Silhouette Coefficient from the `sklearn.metrics` module. Instructions 1. **DBSCAN Clustering**: - Implement DBSCAN clustering on the given dataset. - Explore different values for `eps` and `min_samples` to achieve optimal clustering. 2. **Performance Evaluation**: - Evaluate your clustering using the Silhouette Coefficient. The Silhouette Coefficient should be as close to 1 as possible indicating well-defined clusters. Input/Output Specifications - **Input**: - A dataset in the form of a 2D numpy array `X` with `n_samples` rows and `n_features` columns. - Parameters for DBSCAN: - `eps` (float): The maximum distance between two samples for them to be considered as in the same neighborhood. - `min_samples` (int): The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. - **Output**: - A 1D numpy array of cluster labels for each sample in the dataset. - Silhouette Coefficient as a float value. Constraints - You should use only the specified clustering and metrics functions from scikit-learn. - Ensure your implementation can handle datasets with up to 10000 samples and 100 features efficiently. Example ```python import numpy as np from sklearn.cluster import DBSCAN from sklearn.metrics import silhouette_score def dbscan_clustering(X, eps, min_samples): # Implement DBSCAN clustering dbscan = DBSCAN(eps=eps, min_samples=min_samples) labels = dbscan.fit_predict(X) return labels def evaluate_clustering(X, labels): # Calculate Silhouette Coefficient score = silhouette_score(X, labels) return score # Example usage if __name__ == \\"__main__\\": # Creating a sample dataset X = np.array([[1,2], [2,3], [3,4], [8,7], [8,8], [25,30]]) # DBSCAN parameters eps = 3.0 min_samples = 2 # Clustering labels = dbscan_clustering(X, eps, min_samples) print(\\"Cluster Labels:\\", labels) # Evaluating silhouette = evaluate_clustering(X, labels) print(\\"Silhouette Coefficient:\\", silhouette) ``` In this example, complete the implementation of the `dbscan_clustering` and `evaluate_clustering` functions to perform DBSCAN clustering and evaluate using the Silhouette Coefficient. The goal is to tune the `eps` and `min_samples` parameters to get the best result for your specific dataset.","solution":"import numpy as np from sklearn.cluster import DBSCAN from sklearn.metrics import silhouette_score def dbscan_clustering(X, eps, min_samples): Perform DBSCAN clustering on the given dataset. Parameters: X (ndarray): The dataset, a 2D numpy array of shape (n_samples, n_features). eps (float): The maximum distance between two samples for them to be considered as in the same neighborhood. min_samples (int): The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. Returns: np.ndarray: A 1D numpy array of cluster labels for each sample in the dataset. # Implement DBSCAN clustering dbscan = DBSCAN(eps=eps, min_samples=min_samples) labels = dbscan.fit_predict(X) return labels def evaluate_clustering(X, labels): Calculate the Silhouette Coefficient for the clustering. Parameters: X (ndarray): The dataset, a 2D numpy array of shape (n_samples, n_features). labels (ndarray): The cluster labels for each sample in the dataset. Returns: float: The Silhouette Coefficient. # Calculate Silhouette Coefficient ensuring there is more than one cluster if len(set(labels)) > 1 and len(set(labels)) < len(labels): score = silhouette_score(X, labels) else: score = -1 # If not enough clusters or all points are noise, silhouette score is not defined return score"},{"question":"# Dynamic Class and Type Handling You are programming a dynamic system where classes need to be created and modified at runtime. You are required to create functionalities using the `types` module to dynamically create and modify class definitions and interact with various interpreter types. You have to demonstrate your understanding of dynamic type creation, interpreter types, and utility functions provided in the `types` module. # Task: 1. **Create a Dynamic Class:** - Write a function `create_dynamic_class` which uses `types.new_class` to dynamically create a new class. - The function should take the following parameters: - `name` (str): The name of the new class. - `bases` (tuple): Base classes for the new class. - `methods` (dict): A dictionary where keys are method names (str) and values are functions implementing the methods. - The class should include all methods defined in the `methods` dictionary. 2. **Check Instance Types:** - Write a function `check_types` which takes an instance of the dynamically created class and returns a dictionary with the following checks: - If the object is an instance of `types.FunctionType`. - If the object is an instance of `types.MethodType`. - If the object is an instance of `types.ModuleType`. # Constraints: - All method names in the `methods` dictionary will be unique. - None of the classes in `bases` will be abstract. - Assume well-formed input within the given context. # Input: 1. For the `create_dynamic_class` function: - `name = \\"MyDynamicClass\\"` - `bases = (object,)` - `methods = {\\"greet\\": greet_func, \\"add\\": add_func}` Where `greet_func` and `add_func` are defined as: ```python def greet_func(self): return \\"Hello, World!\\" def add_func(self, a, b): return a + b ``` 2. For the `check_types` function: - An instance of the dynamically created class from part 1. # Output: 1. The `create_dynamic_class` function should return the newly created class. 2. The `check_types` function should return a dictionary like: ```python { \\"is_function_type\\": False, \\"is_method_type\\": True, \\"is_module_type\\": False } ``` # Example: ```python # Function Definitions def greet_func(self): return \\"Hello, World!\\" def add_func(self, a, b): return a + b # Dynamic Class Creation DynamicClass = create_dynamic_class(\\"MyDynamicClass\\", (object,), { \\"greet\\": greet_func, \\"add\\": add_func }) # Instance Creation instance = DynamicClass() # Type Checking type_check_results = check_types(instance) print(type_check_results) ``` This example should output: ```python { \\"is_function_type\\": False, \\"is_method_type\\": True, \\"is_module_type\\": False } ``` **Note:** Ensure your implementation is robust and handles the specified input formats appropriately.","solution":"import types def create_dynamic_class(name, bases, methods): Creates a dynamic class with the given name, bases, and methods using the types module. :param name: Name of the new class. :param bases: Tuple of base classes for the new class. :param methods: Dictionary where keys are method names and values are function objects. :return: The dynamically created class. # Using the types.new_class to create the new class new_class = types.new_class(name, bases) # Adding methods to the new class for method_name, method in methods.items(): setattr(new_class, method_name, method) return new_class def check_types(instance): Checks the instance\'s type against several types from the types module. :param instance: The instance to check. :return: A dictionary with the type checks. return { \\"is_function_type\\": isinstance(instance, types.FunctionType), \\"is_method_type\\": isinstance(instance, types.MethodType), \\"is_module_type\\": isinstance(instance, types.ModuleType) } # Example methods def greet_func(self): return \\"Hello, World!\\" def add_func(self, a, b): return a + b # Example usage DynamicClass = create_dynamic_class(\\"MyDynamicClass\\", (object,), { \\"greet\\": greet_func, \\"add\\": add_func }) instance = DynamicClass() print(instance.greet()) # Should print \\"Hello, World!\\" print(instance.add(2, 3)) # Should print 5 type_check_results = check_types(instance) print(type_check_results) # Should print the type check results"},{"question":"Understanding Abstract Syntax Trees (AST) **Objective**: Write a function that parses a given Python function and counts the number of each type of operation (e.g., addition, subtraction, multiplication) used within the function. **Problem Statement**: Write a Python function `count_operations_in_function(func_str: str) -> dict` that takes a string representation of a Python function as input and returns a dictionary with the counts of each arithmetic operation: addition (`+`), subtraction (`-`), multiplication (`*`), division (`/`), modulo (`%`), and exponentiation (`**`). **Input**: - `func_str`: A string representing a Python function. The function will have standard syntax and will only contain arithmetic operations within its body. **Output**: - A dictionary where the keys are the operation symbols (`+, -, *, /, %, **`) and the values are the counts of each operation found in the function. **Constraints**: - The input function string will not contain any syntax errors and will only include arithmetic operations. - Each operation type should be counted accurately regardless of the complexity of the function. **Example**: ```python input_function = \'\'\' def example_function(a, b, c): result = a + b - c * (a / b) ** b % a another_result = (a * b) + (b / c) - (c ** a) return result + another_result \'\'\' assert count_operations_in_function(input_function) == { \'+\': 3, \'-\': 2, \'*\': 2, \'/\': 2, \'%\': 1, \'**\': 2 } ``` **Function Signature**: ```python def count_operations_in_function(func_str: str) -> dict: pass ``` **Additional Information**: To solve this problem, you will need to: 1. Utilize the `ast` module to parse the input function string into an abstract syntax tree. 2. Traverse the tree to identify and count each arithmetic operation. 3. Return a dictionary with the counts of each operation. **Hints**: - Use `ast.parse` to convert the function string into an AST. - Use the `ast.NodeVisitor` class to visit each node in the tree and count the operation nodes. - Check the type of each node to determine if it represents an arithmetic operation.","solution":"import ast def count_operations_in_function(func_str: str) -> dict: Parses a given Python function and counts the number of each type of operand used. Parameters: func_str (str): String representation of a Python function. Returns: dict: Dictionary with counts of each operation type. class OperationCounter(ast.NodeVisitor): def __init__(self): self.operations = { \'+\': 0, \'-\': 0, \'*\': 0, \'/\': 0, \'%\': 0, \'**\': 0 } def visit_BinOp(self, node): if isinstance(node.op, ast.Add): self.operations[\'+\'] += 1 elif isinstance(node.op, ast.Sub): self.operations[\'-\'] += 1 elif isinstance(node.op, ast.Mult): self.operations[\'*\'] += 1 elif isinstance(node.op, ast.Div): self.operations[\'/\'] += 1 elif isinstance(node.op, ast.Mod): self.operations[\'%\'] += 1 elif isinstance(node.op, ast.Pow): self.operations[\'**\'] += 1 self.generic_visit(node) syntax_tree = ast.parse(func_str) counter = OperationCounter() counter.visit(syntax_tree) return counter.operations"},{"question":"Deploying a Flask Application **Objective:** Demonstrate your understanding of deploying a Flask application in a production environment by following the steps to build, configure, and run the application using appropriate tools. **Problem Statement:** You have completed building a Flask application called `FlaskDeployApp`. To deploy this application on a production server, follow the steps below and provide the necessary scripts and commands. 1. **Build the Application:** - Create a distribution wheel file for the application using the `build` tool. 2. **Configure the Secret Key:** - Generate a secure SECRET_KEY using Python\'s `secrets` library. - Create a configuration file to store this SECRET_KEY securely. 3. **Run the Application with a Production Server:** - Use the `Waitress` WSGI server to run your application. Provide the necessary code snippets, shell commands, and configuration files required for each of the steps. **Requirements:** 1. **Building the Application:** Write the shell commands to: - Install the `build` tool. - Create a wheel distribution file for the `FlaskDeployApp`. 2. **Configuring the Secret Key:** - Write a Python command to generate a SECRET_KEY. - Create a `config.py` file in the instance folder with the generated SECRET_KEY. 3. **Running the Application:** - Install the `waitress` server. - Write the shell command to run the `FlaskDeployApp` using `waitress`. **Constraints:** - Assume your application is located in the directory `/path/to/FlaskDeployApp`. - The instance folder for configurations is `/path/to/FlaskDeployApp/instance`. **Example:** 1. **Building the Application:** ```shell pip install build cd /path/to/FlaskDeployApp python -m build --wheel ``` 2. **Configuring the Secret Key:** ```python # Generate SECRET_KEY python -c \'import secrets; print(secrets.token_hex())\' # Output: \'192b9bdd22ab9ed4d12e236c78afcb9a393ec15f71bbf5dc987d54727823bcbf\' ``` `instance/config.py`: ```python SECRET_KEY = \'192b9bdd22ab9ed4d12e236c78afcb9a393ec15f71bbf5dc987d54727823bcbf\' ``` 3. **Running the Application:** ```shell pip install waitress waitress-serve --call \'FlaskDeployApp:create_app\' ``` Provide the above snippets and commands which are critical in successfully deploying the application.","solution":"# Step 1: Building the Application # Use the following shell commands to install the build tool and create a distribution wheel file. \'\'\' pip install build cd /path/to/FlaskDeployApp python -m build --wheel \'\'\' # Step 2: Configuring the Secret Key # Use the following Python command to generate a SECRET_KEY. \'\'\' python -c \'import secrets; print(secrets.token_hex())\' # Example Output: \'192b9bdd22ab9ed4d12e236c78afcb9a393ec15f71bbf5dc987d54727823bcbf\' \'\'\' # Create a `config.py` file in the instance folder with the generated SECRET_KEY. # /path/to/FlaskDeployApp/instance/config.py SECRET_KEY = \'192b9bdd22ab9ed4d12e236c78afcb9a393ec15f71bbf5dc987d54727823bcbf\' # Step 3: Running the Application # Use the following shell commands to install the waitress server and run your Flask application. \'\'\' pip install waitress waitress-serve --call \'FlaskDeployApp:create_app\' \'\'\'"},{"question":"**Question: WAV File Processing and Color Conversion** You are tasked with creating a program that processes a WAV file and then converts specific audio data into colors in a different color system. Write a Python function that reads a WAV file, extracts the first second of audio data, calculates the average amplitude for each channel, and converts these amplitudes into colors using the `colorsys` module. The function should then return these colors in the HSV color model. # Function Signature ```python def process_wav_and_convert_color(wav_filename: str) -> tuple: pass ``` # Input - `wav_filename` (str): The filename of the WAV file to be processed. The WAV file is guaranteed to be at least one second long and in 16-bit PCM format. # Output - A tuple containing two colors in the HSV color model. Each color represents the average amplitude of one channel in the WAV file. # Example ```python result = process_wav_and_convert_color(\\"example.wav\\") print(result) ``` - Expected output (example values): `((0.5, 1.0, 1.0), (0.5, 1.0, 1.0))` # Constraints and Requirements - The function should read the first second of audio data from the WAV file. - Calculate the average amplitude for each channel (left and right in stereo). - Convert these average amplitudes into HSV colors using the `colorsys.rgb_to_hsv` function. - Assume the amplitude values can be normalized to the range [0, 1] and mapped to RGB values. For example, an amplitude of 32767 (the maximum value for 16-bit PCM) corresponds to an RGB value of (1.0, 1.0, 1.0). - The function should handle both mono and stereo WAV files. If the WAV file is mono, the output should be a tuple with the same color for both channels. # Notes - You may use the `wave` module to read the WAV file and the `colorsys` module for color conversion. - Ensure to handle the case where the WAV file may have different numbers of channels gracefully. Good luck!","solution":"import wave import colorsys import numpy as np def process_wav_and_convert_color(wav_filename: str) -> tuple: Process the WAV file, extract the first second of audio data, calculate the average amplitude for each channel, and convert these amplitudes to colors using the HSV color model. Args: wav_filename (str): The filename of the WAV file to be processed. Returns: tuple: A tuple containing two colors in the HSV color model. with wave.open(wav_filename, \'rb\') as wav_file: n_channels = wav_file.getnchannels() sampwidth = wav_file.getsampwidth() framerate = wav_file.getframerate() # Ensure we only read the first second frames_to_read = framerate * n_channels frames = wav_file.readframes(frames_to_read) fmt = {1: \\"B\\", 2: \\"h\\", 3: \\"i\\"}[sampwidth] dtype = np.int16 if sampwidth == 2 else np.uint8 audio_data = np.frombuffer(frames, dtype=dtype) if n_channels > 1: audio_data = np.reshape(audio_data, (-1, n_channels)) else: audio_data = np.expand_dims(audio_data, axis=-1) avg_amplitudes = np.mean(np.abs(audio_data), axis=0) / (2**(8*sampwidth-1)) def amplitude_to_rgb(amplitude): return (amplitude, amplitude, amplitude) hsv_colors = tuple(colorsys.rgb_to_hsv(*amplitude_to_rgb(avg_amp)) for avg_amp in avg_amplitudes) if n_channels == 1: return (hsv_colors[0], hsv_colors[0]) else: return hsv_colors"},{"question":"# Multi-threading and Queue-based Task Coordination in Python You are tasked with implementing a system that processes a series of tasks using multiple threads. Your solution should demonstrate an understanding of thread coordination using the `threading` module and the `queue` module. # Requirements: 1. **Task Queue**: Implement a task queue that multiple worker threads will consume. 2. **Worker Threads**: Create a specified number of worker threads that will each consume tasks from the queue and process them. 3. **Task Processing**: Each task simulates a time-consuming operation by sleeping for a random amount of time between 1 and 5 seconds. 4. **Logging**: Each task should log its start and end times uniquely identified by a task ID. 5. **Main Thread**: After adding all tasks to the queue, the main thread should wait for all worker threads to finish processing before exiting. # Function Specification: `task_executor(num_tasks: int, num_workers: int) -> None` - **Input**: - `num_tasks` (int): The total number of tasks to be processed. - `num_workers` (int): The number of worker threads. - **Output**: None - **Behavior**: - The function should create the specified number of tasks and add them to a queue. - It should start the specified number of worker threads, each consuming tasks from the queue. - Ensure proper logging and synchronization using threading constructs. - Use appropriate synchronization mechanisms to wait until all tasks are processed. # Additional Notes: - You must use the `threading` and `queue` modules. - Ensure thread safety when accessing shared resources. - Tasks should be processed concurrently by worker threads. - Use the `logging` module to log task start and end times. # Example Usage: ```python import logging import threading import queue import random import time logging.basicConfig(level=logging.INFO, format=\'%(message)s\') def task_executor(num_tasks: int, num_workers: int) -> None: # Your implementation here pass # Example execution task_executor(10, 3) ``` # Sample Output: ``` Task 0 started Task 1 started Task 2 started Task 0 finished Task 3 started Task 1 finished Task 4 started Task 2 finished Task 5 started Task 3 finished ... ``` - This example output demonstrates the interleaving of task processing by multiple threads. Feel free to ask any questions for clarification.","solution":"import logging import threading import queue import random import time logging.basicConfig(level=logging.INFO, format=\'%(message)s\') def worker(task_queue, worker_id): while True: try: task_id = task_queue.get(timeout=3) if task_id is None: break logging.info(f\'Task {task_id} started by worker {worker_id}\') time.sleep(random.randint(1, 5)) logging.info(f\'Task {task_id} finished by worker {worker_id}\') finally: task_queue.task_done() def task_executor(num_tasks: int, num_workers: int) -> None: task_queue = queue.Queue() for i in range(num_tasks): task_queue.put(i) threads = [] for worker_id in range(num_workers): thread = threading.Thread(target=worker, args=(task_queue, worker_id)) thread.start() threads.append(thread) task_queue.join() for _ in range(num_workers): task_queue.put(None) for thread in threads: thread.join()"},{"question":"# High-Level File Operations: Directory Backup and Restoration **Problem Statement:** You are tasked with creating a backup solution for a directory and restoring it when needed using the `shutil` module. Implement two functions, `create_backup` and `restore_backup`, to achieve this. **Function 1: `create_backup(src_directory: str, archive_name: str) -> str`** - **Input:** - `src_directory` (str): The path to the source directory that needs to be backed up. - `archive_name` (str): The name of the archive file to be created (without extension). - **Output:** - Return the full path of the created archive file. - **Example:** ```python create_backup(\'/path/to/source\', \'backup\') # Expected output: \'/path/to/source/backup.tar.gz\' ``` - **Details:** - Create an archive file of the specified directory using the `shutil.make_archive` function. - The archive should be in the \'gztar\' format (gzip\'ed tar). **Function 2: `restore_backup(archive_file: str, restore_directory: str) -> None`** - **Input:** - `archive_file` (str): The path to the archive file to be extracted. - `restore_directory` (str): The target directory where the contents of the archive should be restored. - **Output:** - The function returns nothing. - **Example:** ```python restore_backup(\'/path/to/source/backup.tar.gz\', \'/path/to/restore\') ``` - **Details:** - Extract the contents of the archive file into the specified target directory using the `shutil.unpack_archive` function. **Constraints:** 1. Assume all paths provided as input are valid and exist. 2. The `src_directory` can contain various types of files, including symbolic links. 3. Ensure that the functions handle exceptions and edge cases gracefully, such as handling existing or non-existing files/directories during restoration. **Testing:** Prepare for edge cases, including: 1. Attempting to create a backup in a directory with insufficient permissions. 2. Restoring a backup to a directory that already contains files. 3. Handling symbolic links correctly in the backup and restoration process. Write your solutions in Python and ensure to demonstrate the usage of the `shutil` module as described in the provided documentation.","solution":"import shutil def create_backup(src_directory: str, archive_name: str) -> str: Creates a backup of the source directory in .tar.gz format. Parameters: src_directory (str): The path to the source directory that needs to be backed up. archive_name (str): The name of the archive file to be created (without extension). Returns: str: Full path of the created archive file. archive_path = shutil.make_archive(archive_name, \'gztar\', src_directory) return archive_path def restore_backup(archive_file: str, restore_directory: str) -> None: Restores the backup from the archive file to the specified restore directory. Parameters: archive_file (str): The path to the archive file to be extracted. restore_directory (str): The target directory where the contents of the archive should be restored. Returns: None shutil.unpack_archive(archive_file, restore_directory)"},{"question":"# Custom File Reader with Built-Ins You are tasked with creating a wrapper around Python’s built-in open function to add additional functionality. Specifically, you will implement a custom file reader class that converts all the text read from the file to uppercase. Your solution should include the following: 1. A function named `my_open` that wraps the built-in `open` function. 2. A class named `UpperCaseFile` that reads content from a file object and returns the content transformed to uppercase via a `read` method. Instructions: 1. Implement a function `my_open(path: str) -> UpperCaseFile`: - Use the built-in `open` to open the file in read mode. - Return an instance of `UpperCaseFile` initialized with the file object returned by the built-in `open`. 2. Implement the `UpperCaseFile` class with: - An `__init__` method that takes a file object and stores it as an instance variable. - A `read` method that reads from the file and returns the content in uppercase. Example: ```python # Sample usage of the custom file reader # Assuming the content of the file \\"test.txt\\" is \\"Hello World!\\" file_reader = my_open(\\"test.txt\\") content = file_reader.read() print(content) # Expected output: \\"HELLO WORLD!\\" ``` Constraints: - You are not allowed to use any external libraries other than `builtins`. - Assume the file path provided is valid and the file exists. - Handle reading text files only. Performance: - Ensure that your class uses efficient file reading methods to handle larger files gracefully.","solution":"def my_open(path: str): Wraps the built-in open method to return an UpperCaseFile object. Args: path (str): The file path. Returns: UpperCaseFile: An instance of UpperCaseFile class. file_object = open(path, \'r\') return UpperCaseFile(file_object) class UpperCaseFile: A file wrapper that converts all content read from the file to uppercase. def __init__(self, file): self.file = file def read(self): return self.file.read().upper() def close(self): self.file.close()"},{"question":"# Python Programming Question: Custom Exception Handler with cgitb **Objective:** Implement a custom exception handler using the deprecated `cgitb` module, which will help you understand how to manage and format tracebacks in CGI scripts. The goal is to write a Python function that simulates a potential error scenario and uses `cgitb` to handle this exception with detailed diagnostic information. **Problem Statement:** Write a function `custom_traceback_handler(display, logdir, context_lines, use_html)` that performs the following: 1. Sets up a simulated error scenario where a division by zero error can occur. 2. Uses the `cgitb` module to handle the exception, displaying the traceback information. 3. Configures the traceback display based on the following parameters: - `display`: An integer, 1 to display the traceback in the console, 0 to suppress display. - `logdir`: A string representing the directory path to store the traceback logs. If None, no logs are saved. - `context_lines`: An integer representing the number of lines of source code context to show around the error line. - `use_html`: A boolean, True to format the traceback as HTML, False to format it as text. 4. Ensure that if the `logdir` is specified, the traceback logs are correctly saved in the provided directory. **Function Signature:** ```python def custom_traceback_handler(display: int, logdir: str, context_lines: int, use_html: bool) -> None: pass ``` **Input:** - `display`: Integer (0 or 1) - `logdir`: String (directory path) or None - `context_lines`: Integer (number of context lines) - `use_html`: Boolean (True for HTML format, False for text format) **Output:** - No return value. The function should either print the traceback to the console (based on the `display` parameter) or save it to a file (based on the `logdir` parameter). **Example:** ```python # Example of calling the function custom_traceback_handler(display=1, logdir=\\"/tmp/tracebacks\\", context_lines=3, use_html=False) ``` **Additional Constraints:** - Handle scenarios where the specified `logdir` does not exist or is not writable. - Ensure that the `cgitb` module is used appropriately to manage and format the traceback. **Notes:** - Carefully read the exceptions and ensure to capture them using `sys.exc_info()`. - Remember the `cgitb` module is deprecated; this practice is for educational purposes. **Hints:** - You may use a try-except block to simulate the error scenario. - Use the cgitb functions: `enable()`, `handler()`, `text()`, and `html()` for different parts of the implementation.","solution":"import cgitb import os import sys def custom_traceback_handler(display: int, logdir: str, context_lines: int, use_html: bool) -> None: Handles exceptions and displays or logs detailed traceback information using the cgitb module. Args: - display (int): 1 to display the traceback in the console, 0 to suppress display. - logdir (str): Directory path to store the traceback logs. If None, no logs are saved. - context_lines (int): Number of lines of source code context to show around the error line. - use_html (bool): True to format the traceback as HTML, False to format it as text. try: # Simulate a division by zero error 1 / 0 except ZeroDivisionError: # Set up the cgitb handler with the given settings cgitb.enable(display=display, logdir=logdir, context=context_lines, format=\\"html\\" if use_html else \\"text\\") # If we need to display the traceback immediately if display: cgitb.handler() # Handle writing to the logdir manually if specified if logdir: if not os.path.exists(logdir): os.makedirs(logdir) log_file = os.path.join(logdir, \\"traceback.log\\") with open(log_file, \\"w\\") as f: if use_html: f.write(cgitb.html(sys.exc_info())) else: f.write(cgitb.text(sys.exc_info())) # Raise the exception again to ensure that we notice if cgitb fails raise # Example call: # custom_traceback_handler(display=1, logdir=\\"/tmp/tracebacks\\", context_lines=3, use_html=False)"},{"question":"You are given a task to implement a method to compress and decompress text data using the LZMA algorithm with custom filter chains. You need to write two functions: 1. `compress_with_custom_filters(data: bytes) -> bytes`: This function should compress the input data using a specific filter chain and return the compressed data. 2. `decompress_with_custom_filters(compressed_data: bytes) -> bytes`: This function should decompress the data compressed by `compress_with_custom_filters` and return the original data. # Requirements - The filter chain to be used for compression: - A Delta filter with a distance of 3. - An LZMA2 filter with a compression preset of 9 and the `PRESET_EXTREME` flag enabled. - If the decompressed data does not match the original data (i.e., if there is data loss or corruption during compression or decompression), raise an `LZMAError`. # Function Signatures ```python import lzma def compress_with_custom_filters(data: bytes) -> bytes: # Implement the function here pass def decompress_with_custom_filters(compressed_data: bytes) -> bytes: # Implement the function here pass ``` # Example ```python original_data = b\\"The quick brown fox jumps over the lazy dog\\" compressed_data = compress_with_custom_filters(original_data) decompressed_data = decompress_with_custom_filters(compressed_data) assert original_data == decompressed_data ``` # Constraints - You can assume that the input for compression, and the resulting data after decompression, will always be a non-empty bytes object. - Handle any other edge cases that might arise during the compression and decompression process. # Additional Information - Use appropriate exception handling to manage potential errors during compression and decompression. - Ensure that the functions are efficient and applicable for large data sizes within reasonable memory limits.","solution":"import lzma def compress_with_custom_filters(data: bytes) -> bytes: Compresses data using a custom LZMA filter chain. :param data: Data to be compressed. :return: Compressed data. filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 3}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": lzma.PRESET_EXTREME | 9}, ] compressor = lzma.LZMACompressor(format=lzma.FORMAT_RAW, filters=filters) compressed_data = compressor.compress(data) compressed_data += compressor.flush() return compressed_data def decompress_with_custom_filters(compressed_data: bytes) -> bytes: Decompresses data compressed using the custom LZMA filter chain. :param compressed_data: Data to be decompressed. :return: Decompressed data. filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 3}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": lzma.PRESET_EXTREME | 9}, ] decompressor = lzma.LZMADecompressor(format=lzma.FORMAT_RAW, filters=filters) decompressed_data = decompressor.decompress(compressed_data) return decompressed_data"},{"question":"# Question: Encoding and Decoding of Safe URLs You are tasked with creating a system for safe URL storage and transmission. Specifically, you need to encode arbitrary binary data into a URL-safe format and decode it back to its original form. You\'ll use the `base64` module for this task. Implement a Python function `urlsafe_encode_decode` that takes two inputs: a `bytes` object to be encoded and a `bool` flag indicating whether to encode (`True`) or decode (`False`). The function should either encode the bytes using URL-safe Base64 encoding or decode a URL-safe Base64 encoded string back to bytes, based on the flag provided. Function Signature ```python def urlsafe_encode_decode(data: bytes, encode: bool) -> bytes: pass ``` Input 1. `data` (bytes): The data to be encoded or the Base64 encoded ASCII string to be decoded. 2. `encode` (bool): A flag to indicate encoding (`True`) or decoding (`False`). Output - (bytes): The resulting encoded ASCII string as bytes if encoding, or the decoded binary data if decoding. Example ```python # Encoding example original_data = b\\"example data\\" encoded_data = urlsafe_encode_decode(original_data, True) print(encoded_data) # Output might look like: b\'ZXhhbXBsZSBkYXRh\' (Note: URL-safe characters will be automatically used) # Decoding example decoded_data = urlsafe_encode_decode(encoded_data, False) print(decoded_data) # Output: b\'example data\' ``` Constraints 1. Use URL-safe Base64 encoding as provided by the `base64` module. 2. Raise an appropriate exception if the decoding fails due to incorrect padding or invalid characters. Notes - You may assume that the input data is always valid for encoding. - For decoding, ensure proper error handling for cases where the data is improperly formatted. Performance Requirement - The function should handle inputs up to 1MB efficiently.","solution":"import base64 def urlsafe_encode_decode(data: bytes, encode: bool) -> bytes: Encode arbitrary binary data into a URL-safe Base64 encoded string, or decode a Base64 encoded URL-safe string back to its original binary form. Parameters: - data: bytes, the binary data to encode or the base64 encoded URL-safe string to decode. - encode: bool, True to encode and False to decode. Returns: - bytes, the encoded ASCII string as bytes if encoding, or the decoded binary data if decoding. if encode: return base64.urlsafe_b64encode(data) else: try: return base64.urlsafe_b64decode(data) except (base64.binascii.Error, ValueError) as e: raise ValueError(\\"Decoding failed: Invalid base64 string\\") from e"},{"question":"# Semi-Supervised Learning with Scikit-learn You are given a dataset with both labeled and unlabeled data. Your task is to use the semi-supervised learning methods provided by scikit-learn to fit a model to this dataset and make predictions. The dataset consists of two arrays: 1. `X`: A 2D numpy array of shape `(n_samples, n_features)` containing the feature values. 2. `y`: A 1D numpy array of shape `(n_samples,)` containing the labels, where some of the labels are `-1` indicating unlabeled data. Requirements: 1. Use a semi-supervised learning method from scikit-learn to fit a model to the data. 2. Evaluate the performance of the model using an appropriate metric. Detailed Steps: 1. **Import the necessary libraries** from scikit-learn. 2. **Load or generate** the dataset (when generating dataset, ensure some of the labels are `-1` to represent unlabeled data). 3. **Preprocess** the dataset if necessary. 4. Implement the **SelfTrainingClassifier** with a suitable supervised classifier or use **LabelPropagation** or **LabelSpreading** to fit the model to the data. 5. **Evaluate** the model\'s performance on a test set or via cross-validation. 6. Print the evaluation results. Input: - `X`: 2D numpy array, shape `(n_samples, n_features)` - `y`: 1D numpy array, shape `(n_samples,)`, with some `-1` labels for unlabeled data. Output: - Print the accuracy, precision, recall, F1 score, or any other suitable metric for evaluating the model\'s performance. Constraints: - Use scikit-learn for implementation. - Ensure proper handling of unlabeled data as specified in the documentation. - The supervised classifier used in `SelfTrainingClassifier` must implement `predict_proba`. Example: ```python import numpy as np from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split # Generate synthetic dataset X = np.random.rand(100, 10) y = np.array([0, 1] * 25 + [-1] * 50) # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the base classifier base_classifier = RandomForestClassifier(n_estimators=10) # Define the SelfTrainingClassifier self_training_model = SelfTrainingClassifier(base_classifier, criterion=\'k_best\', k_best=10) # Fit the model self_training_model.fit(X_train, y_train) # Predict on the test set y_pred = self_training_model.predict(X_test) # Evaluate the predictions accuracy = accuracy_score(y_test[y_test != -1], y_pred[y_test != -1]) print(f\'Accuracy: {accuracy}\') ``` You can use any dataset of your choice, but make sure to split the data or mask some labels to create a semi-supervised scenario.","solution":"import numpy as np from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.model_selection import train_test_split def semi_supervised_learning(X, y): Implements a semi-supervised learning model using the SelfTrainingClassifier with a RandomForestClassifier as the base classifier. Parameters: X (np.ndarray): 2D numpy array of shape (n_samples, n_features) containing the feature values y (np.ndarray): 1D numpy array of shape (n_samples,) containing the labels, where some of the labels are -1 indicating unlabeled data Returns: dict: A dictionary with accuracy, precision, recall, and F1 score of the model performance # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the base classifier base_classifier = RandomForestClassifier(n_estimators=10) # Define the SelfTrainingClassifier self_training_model = SelfTrainingClassifier(base_classifier) # Fit the model self_training_model.fit(X_train, y_train) # Predict on the test set y_pred = self_training_model.predict(X_test) # Filter out unlabeled data from evaluation mask = y_test != -1 y_test_filtered = y_test[mask] y_pred_filtered = y_pred[mask] # Evaluate the predictions metrics = { \'accuracy\': accuracy_score(y_test_filtered, y_pred_filtered), \'precision\': precision_score(y_test_filtered, y_pred_filtered, average=\'macro\', zero_division=1), \'recall\': recall_score(y_test_filtered, y_pred_filtered, average=\'macro\', zero_division=1), \'f1_score\': f1_score(y_test_filtered, y_pred_filtered, average=\'macro\', zero_division=1) } return metrics"},{"question":"Implement and Evaluate a Decision Tree Classifier Objective The objective of this task is to assess your ability to implement a Decision Tree classifier using the `scikit-learn` library, train it on a dataset, handle missing values, perform pruning, visualize the tree, and evaluate its performance. Problem Statement You are given a dataset containing information about various flowers, and your task is to build a Decision Tree classifier to predict the species of the flower based on its features. # Dataset Use the Iris dataset available in `scikit-learn`: ```python from sklearn.datasets import load_iris data = load_iris() X = data.data y = data.target # Simulate missing values import numpy as np random_indices = np.random.choice(X.shape[0], size=int(X.shape[0]*0.1)) X[random_indices, 2] = np.nan # Introduce NaN values in the 3rd feature ``` # Instructions 1. **Preprocess the Data**: - Handle the missing values in the dataset. You can either remove the rows with missing values or impute them using a suitable method (e.g., mean imputation). 2. **Build and Train the Model**: - Initialize a `DecisionTreeClassifier` with suitable parameters. - Fit the model using the preprocessed data (`X` and `y`). 3. **Prune the Tree**: - Apply minimal cost-complexity pruning to the Decision Tree using a suitable value for `ccp_alpha`. 4. **Evaluate the Model**: - Predict the class labels for the training data. - Evaluate the model\'s performance using accuracy as the metric. - Optionally, you may use cross-validation to assess the performance more reliably. 5. **Visualize the Tree**: - Visualize the trained Decision Tree using the `plot_tree` function. - Export the tree in textual format using the `export_text` function. # Constraints - You must preprocess the missing values in the dataset. - Your implementation should handle imbalanced datasets through class weight adjustments if necessary. - The Decision Tree should be pruned to avoid overfitting. # Expected Functions ```python def preprocess_data(X): Preprocess the dataset to handle missing values. Parameters: - X: numpy array of shape (n_samples, n_features) Returns: - X_processed: numpy array of shape (n_samples, n_features) with missing values handled pass def train_decision_tree(X, y): Train a Decision Tree classifier on the given dataset. Parameters: - X: preprocessed numpy array of shape (n_samples, n_features) - y: numpy array of shape (n_samples,) containing class labels Returns: - tree_classifier: trained Decision Tree classifier pass def prune_tree(tree_classifier, X, y, ccp_alpha): Apply minimal cost-complexity pruning to the Decision Tree. Parameters: - tree_classifier: trained Decision Tree classifier - X: preprocessed numpy array of shape (n_samples, n_features) - y: numpy array of shape (n_samples,) containing class labels - ccp_alpha: complexity parameter for pruning Returns: - pruned_tree_classifier: pruned Decision Tree classifier pass def evaluate_model(tree_classifier, X, y): Evaluate the Decision Tree classifier using accuracy. Parameters: - tree_classifier: trained Decision Tree classifier - X: preprocessed numpy array of shape (n_samples, n_features) - y: numpy array of shape (n_samples,) containing class labels Returns: - accuracy: float, model accuracy pass def visualize_tree(tree_classifier, feature_names, class_names): Visualize the trained Decision Tree. Parameters: - tree_classifier: trained Decision Tree classifier - feature_names: list of strings, names of the features - class_names: list of strings, names of the classes pass def export_tree_text(tree_classifier, feature_names): Export the Decision Tree in textual format. Parameters: - tree_classifier: trained Decision Tree classifier - feature_names: list of strings, names of the features Returns: - tree_text: string, textual representation of the tree pass ``` # Example Usage ```python # Load the data and preprocess it X_processed = preprocess_data(X) # Train the Decision Tree tree_classifier = train_decision_tree(X_processed, y) # Prune the tree pruned_tree_classifier = prune_tree(tree_classifier, X_processed, y, ccp_alpha=0.01) # Evaluate the pruned tree accuracy = evaluate_model(pruned_tree_classifier, X_processed, y) print(f\\"Model Accuracy: {accuracy:.2f}\\") # Visualize the tree visualize_tree(pruned_tree_classifier, feature_names=data.feature_names, class_names=data.target_names) # Export tree as text tree_text = export_tree_text(pruned_tree_classifier, feature_names=data.feature_names) print(tree_text) ``` # Note - Ensure that your code is well-documented and follows best practices for readability and maintainability. - You are encouraged to experiment with different hyperparameters of the Decision Tree to achieve the best performance.","solution":"from sklearn.datasets import load_iris from sklearn.impute import SimpleImputer from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def preprocess_data(X): Preprocess the dataset to handle missing values. Parameters: - X: numpy array of shape (n_samples, n_features) Returns: - X_processed: numpy array of shape (n_samples, n_features) with missing values handled imputer = SimpleImputer(strategy=\'mean\') X_processed = imputer.fit_transform(X) return X_processed def train_decision_tree(X, y): Train a Decision Tree classifier on the given dataset. Parameters: - X: preprocessed numpy array of shape (n_samples, n_features) - y: numpy array of shape (n_samples,) containing class labels Returns: - tree_classifier: trained Decision Tree classifier tree_classifier = DecisionTreeClassifier(random_state=42) tree_classifier.fit(X, y) return tree_classifier def prune_tree(tree_classifier, X, y, ccp_alpha): Apply minimal cost-complexity pruning to the Decision Tree. Parameters: - tree_classifier: trained Decision Tree classifier - X: preprocessed numpy array of shape (n_samples, n_features) - y: numpy array of shape (n_samples,) containing class labels - ccp_alpha: complexity parameter for pruning Returns: - pruned_tree_classifier: pruned Decision Tree classifier tree_classifier_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) tree_classifier_pruned.fit(X, y) return tree_classifier_pruned def evaluate_model(tree_classifier, X, y): Evaluate the Decision Tree classifier using accuracy. Parameters: - tree_classifier: trained Decision Tree classifier - X: preprocessed numpy array of shape (n_samples, n_features) - y: numpy array of shape (n_samples,) containing class labels Returns: - accuracy: float, model accuracy y_pred = tree_classifier.predict(X) accuracy = accuracy_score(y, y_pred) return accuracy def visualize_tree(tree_classifier, feature_names, class_names): Visualize the trained Decision Tree. Parameters: - tree_classifier: trained Decision Tree classifier - feature_names: list of strings, names of the features - class_names: list of strings, names of the classes import matplotlib.pyplot as plt plt.figure(figsize=(20,10)) plot_tree(tree_classifier, feature_names=feature_names, class_names=class_names, filled=True) plt.show() def export_tree_text(tree_classifier, feature_names): Export the Decision Tree in textual format. Parameters: - tree_classifier: trained Decision Tree classifier - feature_names: list of strings, names of the features Returns: - tree_text: string, textual representation of the tree tree_text = export_text(tree_classifier, feature_names=feature_names) return tree_text"},{"question":"**Question**: Implement the `backup_and_log` function in Python. This function will perform the following tasks: 1. **Backup a File**: Accept the source file path (`src_file_path`) and a target backup directory (`backup_dir`). Copy the file from the source path to the target backup directory. 2. **Log File Information**: Create a log file in the target directory named `backup_log.txt`. Record the backup\'s details: original file path, backup file path, file size, and timestamps (creation, modification, access). 3. **Environment Variable Check**: Before proceeding with the backup, the function should verify an environment variable named `ENABLE_BACKUP`. If this variable is set to `\'1\'`, proceed. Otherwise, print a message that backup is disabled and exit. **Requirements**: - You must use `os` module functions for file operations, accessing environment variables, and writing log files. - Implement proper error handling to catch and log any exceptions encountered during file operations. - Ensure the function works cross-platform (Unix and Windows). **Function Signature**: ```python def backup_and_log(src_file_path: str, backup_dir: str) -> None: pass ``` # Example: ```python # Assuming the environment variable ENABLE_BACKUP is set to \'1\' import os # Set up environment variable for the example os.environ[\'ENABLE_BACKUP\'] = \'1\' # Create a source file for demonstration with open(\'test_files/source.txt\', \'w\') as f: f.write(\'This is a test file for backup.\') # Execute the function backup_and_log(\'test_files/source.txt\', \'test_files/backup/\') # Check the backup directory # \'test_files/backup/\' should contain a copy of \'source.txt\' # \'test_files/backup/backup_log.txt\' should log the details about the backup. ``` # Constraints: - Both `src_file_path` and `backup_dir` must be valid and exist. - The log file must be created in the specified target directory. - If any errors occur, catch and display an appropriate message. **Hints**: - Use `os.makedirs()` with `exist_ok=True` to ensure the backup directory exists. - Use `os.stat()` to retrieve file information before writing to the log. - For file copying, consider `os.link()` or higher-level methods like `shutil.copy()` if needed. - Use appropriate methods to handle and parse environment variables (`os.getenv()`).","solution":"import os import shutil from datetime import datetime def backup_and_log(src_file_path: str, backup_dir: str) -> None: Backs up a file and logs the operation details to a log file. Parameters: src_file_path (str): The path to the source file to back up. backup_dir (str): The directory to store the backup file. try: # Check the environment variable if os.getenv(\'ENABLE_BACKUP\') != \'1\': print(\\"Backup is disabled.\\") return # Ensure the backup directory exists os.makedirs(backup_dir, exist_ok=True) # Construct the backup file path base_filename = os.path.basename(src_file_path) backup_file_path = os.path.join(backup_dir, base_filename) # Copy the file shutil.copy2(src_file_path, backup_file_path) # Get file information file_stat = os.stat(backup_file_path) file_size = file_stat.st_size creation_time = datetime.fromtimestamp(file_stat.st_ctime) modification_time = datetime.fromtimestamp(file_stat.st_mtime) access_time = datetime.fromtimestamp(file_stat.st_atime) # Log the backup details log_file_path = os.path.join(backup_dir, \'backup_log.txt\') with open(log_file_path, \'a\') as log_file: log_file.write(f\\"Backup Details:n\\") log_file.write(f\\"Original File: {src_file_path}n\\") log_file.write(f\\"Backup File: {backup_file_path}n\\") log_file.write(f\\"File Size: {file_size} bytesn\\") log_file.write(f\\"Creation Time: {creation_time}n\\") log_file.write(f\\"Modification Time: {modification_time}n\\") log_file.write(f\\"Access Time: {access_time}n\\") log_file.write(f\\"====================n\\") print(f\\"File \'{src_file_path}\' backed up successfully to \'{backup_file_path}\'.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Question: Working with File Descriptors and Locks using the `fcntl` Module** As part of managing a shared file resource in a multi-process environment, it\'s crucial to implement robust locking mechanisms to prevent data corruption and race conditions. Your task is to create a utility that will handle file locking using the `fcntl` module. **Instructions:** 1. **File Locking Utility Implementation:** Write a function `lock_and_write(file_path: str, data: str, lock_type: str) -> bool` which performs the following tasks: - Opens the file specified by `file_path`. - Locks the file using the `fcntl.lockf()` method with the specified `lock_type`. - Writes the provided `data` to the file. - Ensures that if writing fails, the lock is properly released, and an appropriate error message is printed. - Returns `True` if the operation is successful, indicating the file was locked and data was written without any errors. Return `False` otherwise. **Parameters:** - `file_path (str)`: The path to the file to be locked and written to. - `data (str)`: The data to be written to the file. - `lock_type (str)`: The type of lock to acquire. Can be one of `\\"LOCK_SH\\"` for shared lock or `\\"LOCK_EX\\"` for exclusive lock. **Constraints:** - The function must handle errors gracefully, particularly where locking or writing might fail. - Ensure the implementation does not block indefinitely by using `LOCK_NB`. - The file should be opened in a suitable mode for both reading and writing. 2. **Example Usage:** ```python from my_fcntl_util import lock_and_write file_path = \\"/tmp/sample.txt\\" data = \\"This is a test string\\" if lock_and_write(file_path, data, \\"LOCK_EX\\"): print(\\"Data written successfully with an exclusive lock.\\") else: print(\\"Failed to write data with an exclusive lock.\\") ``` 3. **Testing:** - Write test cases to demonstrate that the function can handle multiple access patterns, ensuring it properly locks and writes data under concurrent conditions. - Validate that attempts to acquire a lock fail gracefully if the file is already locked by another process. **Note:** - You may need to use the `fcntl` module constants such as `fcntl.LOCK_SH`, `fcntl.LOCK_EX`, and `fcntl.LOCK_NB`. - Make sure to clean up and close file descriptors appropriately to prevent resource leaks. This exercise aims to test your understanding of file descriptor manipulation, file locking mechanisms, and error handling using the `fcntl` module. Good luck!","solution":"import fcntl import os def lock_and_write(file_path: str, data: str, lock_type: str) -> bool: Opens a file, locks it, writes data to it, and ensures proper error handling. Parameters: - file_path (str): The path to the file to be locked and written to. - data (str): The data to be written to the file. - lock_type (str): The type of lock to acquire. Can be one of \\"LOCK_SH\\" for shared lock or \\"LOCK_EX\\" for exclusive lock. Returns: - bool: True if the operation is successful, False otherwise. lock_mapping = { \\"LOCK_SH\\": fcntl.LOCK_SH, \\"LOCK_EX\\": fcntl.LOCK_EX } # Ensure the provided lock_type is valid if lock_type not in lock_mapping: raise ValueError(f\\"Invalid lock_type: {lock_type}. Expected \'LOCK_SH\' or \'LOCK_EX\'.\\") try: with open(file_path, \'a+\') as file: # Acquire the lock without blocking fcntl.lockf(file, lock_mapping[lock_type] | fcntl.LOCK_NB) # Write data to the file file.write(data + \'n\') file.flush() # Release the lock fcntl.lockf(file, fcntl.LOCK_UN) return True except IOError as e: print(f\\"File operation failed: {e}\\") return False except Exception as e: print(f\\"An unexpected error occurred: {e}\\") return False"},{"question":"Ensuring Reproducibility in PyTorch **Objective:** The goal of this task is to create a PyTorch script that ensures reproducible results by managing random number generation, configuring deterministic algorithms, and handling CUDA-specific settings. **Problem Statement:** Write a PyTorch script that performs the following tasks: 1. Sets up the environment to ensure reproducibility. 2. Initializes a random tensor and performs a series of deterministic operations on it. 3. Prints the resulting tensor values. **Requirements:** 1. **Set Random Seeds:** - Set the seed for PyTorch\'s random number generator. - Set the seed for Python\'s `random` module. - Set the seed for NumPy\'s random number generator. 2. **Use Deterministic Algorithms:** - Configure PyTorch to use deterministic algorithms where available. - Handle operations that are known to be nondeterministic. 3. **CUDA Settings:** - If available, disable CUDA convolution benchmarking. - Ensure deterministic behavior for CUDA RNNs and LSTMs. **Constraints:** - The script must be compatible with both CPU and GPU execution. - If CUDA is available, ensure that CUDA-specific settings are applied. - The script should print a message indicating whether a GPU is being used or not. **Input and Output:** - **Input:** No input required. - **Output:** Print the resulting tensor values and a message indicating whether a GPU is used or not. **Example:** Here is an example of the expected output (values may vary depending on the operations performed): ``` Using GPU: True Resulting Tensor: tensor([[ 1.1900, -2.3409], [ 0.4796, 0.8003]], device=\'cuda:0\') ``` **Instructions:** 1. Implement the script according to the above requirements. 2. Ensure that the script is self-contained and clear. ```python import torch import random import numpy as np # 1. Set Random Seeds torch.manual_seed(0) random.seed(0) np.random.seed(0) # 2. Use Deterministic Algorithms torch.use_deterministic_algorithms(True) # 3. CUDA Settings if torch.cuda.is_available(): device = torch.device(\\"cuda\\") torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = True else: device = torch.device(\\"cpu\\") # Print whether GPU is being used print(f\\"Using GPU: {torch.cuda.is_available()}\\") # Initialize a random tensor tensor = torch.randn(2, 2, device=device) # Perform a series of deterministic operations result = torch.bmm(tensor.unsqueeze(0).to_sparse(), tensor.unsqueeze(0)) # Print the result print(\\"Resulting Tensor:\\") print(result) ```","solution":"import torch import random import numpy as np def setup_reproducibility(seed=0): Sets up the environment for reproducibility by configuring seeds and settings. Args: seed (int): The seed value to use for random number generation. # 1. Set Random Seeds torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) # 2. Use Deterministic Algorithms torch.use_deterministic_algorithms(True) # 3. CUDA Settings if torch.cuda.is_available(): device = torch.device(\\"cuda\\") torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = True else: device = torch.device(\\"cpu\\") return device def main(): # Set up reproducibility device = setup_reproducibility() # Print whether GPU is being used print(f\\"Using GPU: {torch.cuda.is_available()}\\") # Initialize a random tensor tensor = torch.randn(2, 2, device=device) # Perform a series of deterministic operations result = torch.bmm(tensor.unsqueeze(0).to_sparse(), tensor.unsqueeze(0)) # Print the result print(\\"Resulting Tensor:\\") print(result) if __name__ == \\"__main__\\": main()"},{"question":"Objective: Implement a PyTorch neural network module that uses `torch.cond` to switch between two different layers based on a data-dependent condition. The task will assess the understanding of dynamic control flow in PyTorch models using the `torch.cond` operator. Problem Statement: Design a PyTorch module, `ConditionalLayerModule`, that uses `torch.cond` to apply different transformations to the input tensor based on its sum. If the sum of the elements in the tensor is greater than 10, apply a linear transformation followed by a ReLU activation. Otherwise, apply a linear transformation followed by a Tanh activation. Requirements: 1. **Input Format**: The module should accept a single input tensor `x` of shape `(N, D)`, where `N` is the batch size and `D` is the feature dimension. 2. **Output Format**: The output should be a tensor of shape `(N, M)`, where `M` is the output dimension of the linear layers. 3. **Functions**: - **`true_fn`**: A function that performs a linear transformation followed by a ReLU activation. - **`false_fn`**: A function that performs a linear transformation followed by a Tanh activation. 4. **Condition**: Use `torch.cond` to conditionally apply `true_fn` if the sum of the elements in `x` is greater than 10; otherwise, apply `false_fn`. Implementation: Your `ConditionalLayerModule` class should implement the following: ```python import torch import torch.nn as nn import torch.nn.functional as F class ConditionalLayerModule(nn.Module): def __init__(self, input_dim, output_dim): Initialize the ConditionalLayerModule with given dimensions. Arguments: input_dim -- integer, the dimensionality of the input features. output_dim -- integer, the dimensionality of the output features. super(ConditionalLayerModule, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): Forward pass of the module. Arguments: x -- torch.Tensor, input tensor of shape (N, D). Returns: torch.Tensor -- output tensor of shape (N, M). def true_fn(x): return F.relu(self.linear(x)) def false_fn(x): return torch.tanh(self.linear(x)) return torch.cond(x.sum() > 10, true_fn, false_fn, (x,)) ``` # Constraints: - You must use `torch.cond` as described. - Define the `true_fn` and `false_fn` within the `forward` method of the module. - Remember to handle cases where the sum of the tensor is exactly 10 appropriately. # Example Usage: ```python # Initialize the model model = ConditionalLayerModule(input_dim=5, output_dim=3) # Input tensor with sum of elements greater than 10 x1 = torch.tensor([[3., 3., 3., 3., 3.]]) output1 = model(x1) # Expect relu(linear(x1)) # Input tensor with sum of elements less than 10 x2 = torch.tensor([[1., 1., 1., 1., 1.]]) output2 = model(x2) # Expect tanh(linear(x2)) print(output1) print(output2) ``` **Note**: Make sure your solution is efficient and handles edge cases robustly.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class ConditionalLayerModule(nn.Module): def __init__(self, input_dim, output_dim): Initialize the ConditionalLayerModule with given dimensions. Arguments: input_dim -- integer, the dimensionality of the input features. output_dim -- integer, the dimensionality of the output features. super(ConditionalLayerModule, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): Forward pass of the module. Arguments: x -- torch.Tensor, input tensor of shape (N, D). Returns: torch.Tensor -- output tensor of shape (N, M). def true_fn(x): return F.relu(self.linear(x)) def false_fn(x): return torch.tanh(self.linear(x)) # Using torch.where to conditionally apply different functions. condition = x.sum() > 10 if condition: return true_fn(x) else: return false_fn(x)"},{"question":"Task You are required to implement a Python script for a simple calculator using the `argparse` module. The calculator should be able to perform addition, subtraction, multiplication, and division. The user can specify these operations from the command line. Additionally, the script should support verbosity levels to control the output details. Requirements 1. **Positional Arguments**: - `x`: The first operand (a float). - `y`: The second operand (a float). 2. **Optional Arguments**: - `-a`, `--add`: Perform addition. - `-s`, `--subtract`: Perform subtraction. - `-m`, `--multiply`: Perform multiplication. - `-d`, `--divide`: Perform division. Note: Only one of these operations can be specified at a time. 3. **Verbosity**: - `-v`, `--verbosity`: Increase output verbosity for more detailed messages. The verbosity level should affect how much detail is printed in the output: - `-v` or `--verbosity=1`: Show the operation performed and the result. - `-vv` or `--verbosity=2`: Show detailed step-by-step computation. Input Format The script should be called from the command line with the following structure: ``` python calculator.py [-h] [-a | -s | -m | -d] -v x y ``` Output Format Based on the verbosity level, the script should print: - For `--verbosity=0` (default): Only the result. - For `--verbosity=1`: The operation and result. - For `--verbosity=2`: Detailed explanation of the calculation steps and result. Examples # Example 1 Command: ``` python calculator.py -a 3.5 2.7 ``` Output: ``` 6.2 ``` # Example 2 Command: ``` python calculator.py -s 7.0 5.0 -v ``` Output: ``` 7.0 - 5.0 = 2.0 ``` # Example 3 Command: ``` python calculator.py -m 4 1.5 -vv ``` Output: ``` Multiplying 4 by 1.5 Result: 4 * 1.5 = 6.0 ``` # Example 4 Command: ``` python calculator.py -d 8 4 -v ``` Output: ``` 8 / 4 = 2.0 ``` # Example 5 Command: ``` python calculator.py -a 8 4 -vv ``` Output: ``` Adding 8 and 4 Result: 8 + 4 = 12 ``` # Constraints - Only one operation can be specified. - Division by zero should be handled gracefully with an appropriate error message. Notes - Use the `argparse` module to parse the command-line arguments. - The script should validate and handle invalid inputs appropriately. Implement the `calculator.py` script as per the above specifications.","solution":"import argparse def add(x, y): return x + y def subtract(x, y): return x - y def multiply(x, y): return x * y def divide(x, y): if y == 0: raise ValueError(\\"Cannot divide by zero.\\") return x / y def main(): parser = argparse.ArgumentParser(description=\'Simple calculator\') parser.add_argument(\'x\', type=float, help=\'first operand\') parser.add_argument(\'y\', type=float, help=\'second operand\') group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\'-a\', \'--add\', action=\'store_true\', help=\'perform addition\') group.add_argument(\'-s\', \'--subtract\', action=\'store_true\', help=\'perform subtraction\') group.add_argument(\'-m\', \'--multiply\', action=\'store_true\', help=\'perform multiplication\') group.add_argument(\'-d\', \'--divide\', action=\'store_true\', help=\'perform division\') parser.add_argument(\'-v\', \'--verbosity\', action=\'count\', default=0, help=\'increase output verbosity\') args = parser.parse_args() if args.add: operation = \'add\' result = add(args.x, args.y) elif args.subtract: operation = \'subtract\' result = subtract(args.x, args.y) elif args.multiply: operation = \'multiply\' result = multiply(args.x, args.y) elif args.divide: operation = \'divide\' result = divide(args.x, args.y) if args.verbosity == 0: print(result) elif args.verbosity == 1: print(f\\"{args.x} {operation[0]} {args.y} = {result}\\") elif args.verbosity == 2: if operation == \'add\': print(f\\"Adding {args.x} and {args.y}\\") elif operation == \'subtract\': print(f\\"Subtracting {args.y} from {args.x}\\") elif operation == \'multiply\': print(f\\"Multiplying {args.x} by {args.y}\\") elif operation == \'divide\': print(f\\"Dividing {args.x} by {args.y}\\") print(f\\"Result: {args.x} {operation[0]} {args.y} = {result}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective** Implement a function that applies Partial Least Squares Canonical (PLSCanonical) to transform two sets of data, then predict the target data given new input data. **Function Signature** ```python import numpy as np from sklearn.cross_decomposition import PLSCanonical def pls_canonical_transform_predict(X_train: np.ndarray, Y_train: np.ndarray, X_new: np.ndarray, n_components: int) -> np.ndarray: Perform PLSCanonical dimensionality reduction and prediction. Parameters: X_train (np.ndarray): Training data, predictors, shape (n_samples, n_features). Y_train (np.ndarray): Training data, response variables, shape (n_samples, n_targets). X_new (np.ndarray): New data for which to predict the targets, shape (n_samples_new, n_features). n_components (int): Number of PLS components to use. Returns: np.ndarray: Predicted targets for the new data, shape (n_samples_new, n_targets). pass ``` **Constraints** - You must use the `PLSCanonical` class from the `sklearn.cross_decomposition` module. - The function should first fit the `PLSCanonical` model on `X_train` and `Y_train` with the specified number of components. - After fitting, the function should use the model to predict the target values for `X_new`. - You only need to return the predicted values for the new data. **Examples** ```python import numpy as np # Example data X_train = np.array([[0.1, 0.2, 0.5], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]) Y_train = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]) X_new = np.array([[0.2, 0.1, 0.3], [0.6, 0.7, 0.8]]) # Number of PLS components n_components = 2 # Expected output (example, assuming correct implementation) predicted_Y = pls_canonical_transform_predict(X_train, Y_train, X_new, n_components) print(predicted_Y) ``` **Notes** - Ensure to handle the case where `n_components` is larger than the minimum dimensionality of the input data. - The function should properly handle any exceptions related to the dimensionality requirements of `PLSCanonical`.","solution":"import numpy as np from sklearn.cross_decomposition import PLSCanonical def pls_canonical_transform_predict(X_train: np.ndarray, Y_train: np.ndarray, X_new: np.ndarray, n_components: int) -> np.ndarray: Perform PLSCanonical dimensionality reduction and prediction. Parameters: X_train (np.ndarray): Training data, predictors, shape (n_samples, n_features). Y_train (np.ndarray): Training data, response variables, shape (n_samples, n_targets). X_new (np.ndarray): New data for which to predict the targets, shape (n_samples_new, n_features). n_components (int): Number of PLS components to use. Returns: np.ndarray: Predicted targets for the new data, shape (n_samples_new, n_targets). # Check that the number of components is less than or equal to the number of samples and features n_samples, n_features = X_train.shape n_targets = Y_train.shape[1] n_components = min(n_components, n_samples, n_features, n_targets) # Fit the PLSCanonical model pls = PLSCanonical(n_components=n_components) pls.fit(X_train, Y_train) # Predict the targets for new data Y_pred = pls.predict(X_new) return Y_pred"},{"question":"You are required to write a Python program to process a list of user activity logs recorded in a text file. Each line in the file contains a timestamp, a user ID, and an action separated by commas. Your task is to analyze these logs and generate a summary report. Input - A text file named `user_logs.txt` where each line has the format: ``` timestamp,user_id,action ``` Example: ``` 2023-10-01T08:30:00,1234,login 2023-10-01T08:45:00,5678,logout 2023-10-01T09:00:00,1234,logout ``` Output - A dictionary where the keys are the user IDs, and the values are dictionaries with the counts of different actions performed by that user. Example: ```python { \\"1234\\": {\\"login\\": 1, \\"logout\\": 1}, \\"5678\\": {\\"logout\\": 1} } ``` Function Signature ```python def analyze_user_logs(file_path: str) -> dict: ``` Constraints - You may assume that the input file is not empty and is well-formed. - You should handle any I/O errors gracefully. Requirements 1. **File Operations**: Read the input from the file `user_logs.txt`. 2. **Data Structures**: Use dictionaries to map user IDs to their action counts. 3. **Exception Handling**: Handle I/O operations and ensure the function behaves correctly if the file does not exist or is not readable. 4. **String Manipulation**: Parse each line to extract the timestamp, user ID, and action. # Example Usage Suppose `user_logs.txt` contains the following: ``` 2023-10-01T08:30:00,1234,login 2023-10-01T08:45:00,5678,logout 2023-10-01T09:00:00,1234,logout 2023-10-01T09:30:00,5678,login 2023-10-01T10:00:00,1234,login ``` Calling the function as follows: ```python result = analyze_user_logs(\'user_logs.txt\') print(result) ``` Should output: ```python { \\"1234\\": {\\"login\\": 2, \\"logout\\": 1}, \\"5678\\": {\\"logout\\": 1, \\"login\\": 1} } ``` Notes: 1. Carefully handle and test for various edge cases. 2. Consider performance implications for large files.","solution":"def analyze_user_logs(file_path: str) -> dict: user_actions = {} try: with open(file_path, \'r\') as file: for line in file: # Strip the line to remove any leading/trailing whitespace characters including newline character line = line.strip() # Extract timestamp, user_id and action timestamp, user_id, action = line.split(\',\') if user_id not in user_actions: user_actions[user_id] = {} if action not in user_actions[user_id]: user_actions[user_id][action] = 0 user_actions[user_id][action] += 1 except FileNotFoundError: print(f\\"Error: The file {file_path} does not exist.\\") except IOError: print(f\\"Error: Unable to read the file {file_path}.\\") return user_actions"},{"question":"Implement a class `AsyncTaskManager` that allows the management of multiple asynchronous tasks using PyTorch\'s `torch.futures` module. Class: `AsyncTaskManager` # Methods: 1. `__init__(self)`: Initialize an empty list to hold future tasks. 2. `add_task(self, task: torch.futures.Future)`: Add a new future task to the list of tasks. 3. `collect_all_tasks(self) -> List[torch.futures.Future]`: Use `torch.futures.collect_all` to collect all future tasks added. 4. `wait_for_all(self) -> None`: Use `torch.futures.wait_all` to wait for the completion of all added tasks. # Constraints - Ensure proper exception handling for failed tasks. - Provide meaningful comments and docstrings within your code. - Add unit tests for each functionality in the `AsyncTaskManager` class to validate your implementation. # Example Here is an example showing how your class should be used: ```python import torch from torch.futures import Future async def async_add(x, y): fut = Future() result = x + y fut.set_result(result) return fut # Initialize AsyncTaskManager manager = AsyncTaskManager() # Adding tasks fut1 = async_add(1, 2) fut2 = async_add(3, 4) manager.add_task(fut1) manager.add_task(fut2) # Collect all tasks collected_futures = manager.collect_all_tasks() # Print future results for fut in collected_futures: print(fut) # should eventually print the individual futures # Wait for all tasks to complete manager.wait_for_all() ``` **Important Notes**: - When implementing the methods, make sure they handle asynchronous contexts properly. - The `async_add` function provided in the example should be used in your tests to simulate asynchronous tasks.","solution":"import torch from torch.futures import Future from typing import List class AsyncTaskManager: def __init__(self): Initializes an empty list to hold future tasks. self.tasks = [] def add_task(self, task: Future): Add a new future task to the list of tasks. Args: task (torch.futures.Future): The future task to be added. self.tasks.append(task) def collect_all_tasks(self) -> List[Future]: Collects all future tasks added to the manager. Returns: List[torch.futures.Future]: A list of all future tasks. return torch.futures.collect_all(self.tasks) def wait_for_all(self) -> None: Wait for the completion of all added tasks. torch.futures.wait_all(self.tasks) async def async_add(x, y): Asynchronous function that simulates adding x and y. Args: x (int): First number to add. y (int): Second number to add. Returns: torch.futures.Future: A future representing the result of the addition. fut = Future() result = x + y fut.set_result(result) return fut"},{"question":"# Seaborn Coding Assessment You are provided with a dataset `tips` from seaborn\'s built-in datasets. This dataset contains information about tips received by waiters and waitresses in a restaurant. Write a function `create_stripplot_and_catplot` that performs the following tasks: 1. **Load the `tips` dataset** using seaborn\'s `load_dataset` function. 2. **Create a strip plot** of `total_bill` vs `day`: - Use the `hue` parameter to differentiate data points by `sex`. - Set `dodge=True`. - Disable the random jitter by setting `jitter=False`. - Customize the plot with the following settings: marker style to `D`, marker size to 20, and marker outline width to 1. 3. **Create a categorical plot** (catplot) to show the relationship between `time`, `total_bill`, and `sex` across different days. The categorical plot should: - Have one plot for each day of the week (use `col=\\"day\\"`). - Set the aspect ratio of each plot to 0.5. The function should return both the strip plot and the catplot. Function Signature ```python def create_stripplot_and_catplot(): pass ``` Constraints - Use only seaborn and matplotlib for plotting. - Ensure the function runs without errors and returns the required plots as seaborn plot objects. Example Usage ```python stripplot, catplot = create_stripplot_and_catplot() ``` Notes: - There is no need to display the plots within the function. However, you can display them in a Jupyter notebook to verify your implementation. - Remember to set the theme using `sns.set_theme(style=\\"whitegrid\\")`. No additional dataset is needed beyond the `tips` dataset provided by seaborn.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_stripplot_and_catplot(): # Load the tips dataset tips = sns.load_dataset(\'tips\') # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create the strip plot stripplot = sns.stripplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, dodge=True, jitter=False, marker=\'D\', size=20, edgecolor=\'gray\', linewidth=1) plt.close() # Close the plot to avoid displaying in non-interactive environments # Create the categorical plot catplot = sns.catplot(x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", data=tips, kind=\\"strip\\", aspect=0.5) plt.close() return stripplot, catplot"},{"question":"Using Seaborn Plotting Context **Objective:** Write a function `create_comparative_plots(data, x_col, y_col)` that generates two comparative line plots from the input data using Seaborn. The function should demonstrate the use of seaborn\'s `plotting_context` to temporarily change the appearance of the plots. **Function Specification:** ```python def create_comparative_plots(data, x_col, y_col): Generates two line plots with different plotting context styles and saves them as images. Parameters: data (DataFrame): DataFrame containing the data to plot. x_col (str): Name of the column to use for x-axis values. y_col (str): Name of the column to use for y-axis values. Returns: None pass ``` **Input:** - `data`: A pandas DataFrame containing the dataset. - `x_col`: A string representing the column name to be used for the x-axis. - `y_col`: A string representing the column name to be used for the y-axis. **Output:** - The function does not return any value. It should create and save two plot images in the current directory: - `default_plot.png` showing the plot with default settings. - `talk_plot.png` showing the plot with the \\"talk\\" context settings. **Constraints:** - The DataFrame is assumed to have at least the specified columns for x and y values. - Ensure the function generates plots in two different styles using seaborn\'s `plotting_context`. **Example:** Given a DataFrame `df`: | x_col | y_col | |-------|-------| | A | 1 | | B | 3 | | C | 2 | Running the function: ```python create_comparative_plots(df, \'x_col\', \'y_col\') ``` Should result in two images: `default_plot.png` and `talk_plot.png`, displaying the line plots with their respective styles. **Note:** - You may use any additional imports or configurations as necessary. - Make sure your function demonstrates the temporary change in style using seaborn\'s `plotting_context`.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_comparative_plots(data, x_col, y_col): Generates two line plots with different plotting context styles and saves them as images. Parameters: data (DataFrame): DataFrame containing the data to plot. x_col (str): Name of the column to use for x-axis values. y_col (str): Name of the column to use for y-axis values. Returns: None # Plot with the default context plt.figure() sns.lineplot(data=data, x=x_col, y=y_col) plt.title(\'Default Plot\') plt.savefig(\'default_plot.png\') plt.close() # Plot with the \'talk\' context plt.figure() with sns.plotting_context(\\"talk\\"): sns.lineplot(data=data, x=x_col, y=y_col) plt.title(\'Talk Plot\') plt.savefig(\'talk_plot.png\') plt.close()"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},q={class:"search-container"},R={class:"card-container"},F={key:0,class:"empty-state"},z=["disabled"],N={key:0},O={key:1};function L(n,e,l,m,i,r){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",q,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>i.searchQuery=o),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",R,[(a(!0),s(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),s("div",F,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[i.isLoading?(a(),s("span",O,"Loading...")):(a(),s("span",N,"See more"))],8,z)):d("",!0)])}const M=p(D,[["render",L],["__scopeId","data-v-d80a4dbb"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/43.md","filePath":"chatai/43.md"}'),U={name:"chatai/43.md"},X=Object.assign(U,{setup(n){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,X as default};
