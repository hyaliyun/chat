import{_ as p,o as a,c as s,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(i,e,l,m,n,o){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const z=p(k,[["render",S],["__scopeId","data-v-3674577d"]]),I=JSON.parse('[{"question":"# File Organizer using `os` Module **Objective**: Implement a function to organize files in a given directory based on their file extension. The files will be moved into subdirectories named after their extensions. **Instructions**: Implement the function `organize_files_by_extension(directory: str) -> None` that takes a path to a directory as input and organizes the files inside this directory into subdirectories based on their file extensions. Your function should handle the following: 1. Create subdirectories for each file extension if they do not exist. 2. Move the files into their corresponding subdirectory based on their extension. 3. If a file does not have an extension, move it to a subdirectory named \\"no_extension\\". **Constraints**: 1. `directory` is guaranteed to be a valid path. 2. The function should handle cases where the directory contains nested subdirectories but should only organize files at the top level of the given directory. 3. Ensure the function handles edge cases such as empty directories gracefully. 4. Use only the `os` module for this task. **Example**: Suppose the directory `/path/to/directory` contains the following files and subdirectories: ``` /path/to/directory - file1.txt - file2.pdf - file3 - image1.png - image2.jpg - doc1.doc - readme.md - subdir/ - nested_file.txt ``` After calling `organize_files_by_extension(\'/path/to/directory\')`, the directory should look like: ``` /path/to/directory - txt/ - file1.txt - pdf/ - file2.pdf - no_extension/ - file3 - png/ - image1.png - jpg/ - image2.jpg - doc/ - doc1.doc - md/ - readme.md - subdir/ - nested_file.txt ``` **Function Signature**: ```python def organize_files_by_extension(directory: str) -> None: pass ``` **Hints**: 1. Use `os.listdir()` to list the files in the directory. 2. Use `os.path.splitext()` to get the file extension. 3. Use `os.makedirs()` to create directories. 4. Use `os.rename()` or `os.replace()` to move files.","solution":"import os def organize_files_by_extension(directory: str) -> None: if not os.path.exists(directory) or not os.path.isdir(directory): raise ValueError(\\"The specified directory is not valid.\\") # Get the list of files and directories in the specified directory items = os.listdir(directory) for item in items: item_path = os.path.join(directory, item) # Check if the item is a file (not a directory) if os.path.isfile(item_path): _, extension = os.path.splitext(item) if extension: extension = extension[1:] # Remove the leading dot else: extension = \\"no_extension\\" new_dir = os.path.join(directory, extension) if not os.path.exists(new_dir): os.makedirs(new_dir) # Generate the new path for the file new_path = os.path.join(new_dir, item) # Move the file to the new directory os.rename(item_path, new_path)"},{"question":"# Custom SMTP Server Implementation **Objective**: Implement a custom SMTP server that processes incoming email messages and performs specific actions based on the email content. Problem Statement You are required to implement a custom SMTP server using the `smtpd` module provided by Python. The server should: 1. **Bind** to the local address `(localhost, 1025)`. 2. **Print** the details (peer, mailfrom, rcpttos, data) of every incoming message to the console. 3. **Respond** with a custom message if the subject of the email contains the word \\"Important\\". 4. **Handle** the SMTPUTF8 extension if enabled. Implementation Details 1. Create a subclass of `smtpd.SMTPServer` named `CustomSMTPServer`. 2. Override the `process_message` method to print incoming message details and check for the word \\"Important\\" in the email subject line. 3. The `process_message` method should accept `peer`, `mailfrom`, `rcpttos`, `data`, and `**kwargs` as its parameters and return appropriate responses. 4. The server should handle the `SMTPUTF8` extension if enabled. Input - You do not need to handle any input directly as part of this question. The server should be programmatically configured to bind to the specified address. Output - Print the following details for every incoming email: - Peer address - Mail from - Recipients (rcpttos) - Email data (content) - If the email subject contains the word \\"Important\\", return a custom response: `\\"250 Ok - Priority message received\\"`. - Otherwise, return the standard response: `\\"250 Ok\\"`. Constraints - The server must handle emails up to a size of 10 MB. - Both `decode_data` and `enable_SMTPUTF8` cannot be set to `True` simultaneously. Performance Requirements - The server should efficiently process incoming emails and respond promptly as per the SMTP protocol standards. Example Implementation ```python import smtpd import asyncore import email from email import policy class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(f\\"Peer: {peer}\\") print(f\\"Mail from: {mailfrom}\\") print(f\\"Recipients: {rcpttos}\\") print(f\\"Data: {data}\\") # Parse email data message = email.message_from_bytes(data, policy=policy.default) subject = message[\'subject\'] # Check for \'Important\' in subject if subject and \\"Important\\" in subject: return \\"250 Ok - Priority message received\\" return \\"250 Ok\\" # Initialize server server = CustomSMTPServer((\'localhost\', 1025), None, data_size_limit=10*1024*1024) # Start asyncore loop asyncore.loop() ``` **Explanation**: - Define a `CustomSMTPServer` class that inherits from `smtpd.SMTPServer`. - Override the `process_message` method to include logic for printing message details and handling \\"Important\\" in the subject. - Create an instance of the server and start the asyncore event loop to listen for incoming connections.","solution":"import smtpd import asyncore import email from email import policy class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(f\\"Peer: {peer}\\") print(f\\"Mail from: {mailfrom}\\") print(f\\"Recipients: {rcpttos}\\") print(f\\"Data: {data.decode(\'utf-8\') if isinstance(data, bytes) else data}\\") # Parse email data message = email.message_from_string(data.decode(\'utf-8\') if isinstance(data, bytes) else data, policy=policy.SMTP) subject = message[\'subject\'] # Check for \'Important\' in subject if subject and \\"Important\\" in subject: return \\"250 Ok - Priority message received\\" return \\"250 Ok\\" # Initialize server def run_server(): print(\\"Starting Custom SMTP Server on localhost:1025\\") server = CustomSMTPServer((\'localhost\', 1025), None, data_size_limit=10*1024*1024) try: asyncore.loop() except KeyboardInterrupt: print(\\"Stopping server.\\") pass # Comment out the server run for unit testing purposes # if __name__ == \\"__main__\\": # run_server()"},{"question":"You are required to demonstrate your understanding and proficiency with seaborn\'s `clustermap` function. Below is the detailed task you need to accomplish. # Task 1. Load the `iris` dataset using seaborn. 2. Remove the `species` column from the dataset, but keep track of it for later use. 3. Create a hierarchical cluster map for the modified dataset: - **Figure Size:** 10 inches by 7 inches. - **Colormap:** Use the `\'coolwarm\'` colormap. - **Color Range:** Standardized within the columns (set `standard_scale` parameter to 1). - **Cluster Parameters:** - Use \'euclidean\' as the metric for distance. - Use \'complete\' as the method for clustering. - **Row Clustering:** Disable row clustering. - **Color bar:** Position the color bar on the left. - **Colored Labels:** Add colored labels for the `species` column to identify the observations. Use distinct colors for each species. # Constraints - The output figure should be correctly configured as described. - Ensure that the clustering method and metric are correctly applied. - The standardized color range must reflect the data\'s standardization within columns. - The colored labels should distinctly identify the species. # Expected Output Your function should output a seaborn cluster map adhering to the specifications mentioned above. # Implementation ```python import seaborn as sns import matplotlib.pyplot as plt def custom_clustermap(): # Load the \'iris\' dataset from seaborn iris = sns.load_dataset(\\"iris\\") # Remove the species column but keep track of it species = iris.pop(\\"species\\") # Create a mapping of species to colors lut = dict(zip(species.unique(), \\"rgb\\")) row_colors = species.map(lut) # Generate the cluster map sns.clustermap( iris, figsize=(10, 7), cmap=\\"coolwarm\\", standard_scale=1, metric=\\"euclidean\\", method=\\"complete\\", row_cluster=False, cbar_pos=(0, .2, .03, .4), row_colors=row_colors ) plt.show() # Call the function to display the clustermap custom_clustermap() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_clustermap(): # Load the \'iris\' dataset from seaborn iris = sns.load_dataset(\\"iris\\") # Remove the species column but keep track of it species = iris.pop(\\"species\\") # Create a mapping of species to colors lut = dict(zip(species.unique(), \\"rgb\\")) row_colors = species.map(lut) # Generate the cluster map sns.clustermap( iris, figsize=(10, 7), cmap=\\"coolwarm\\", standard_scale=1, metric=\\"euclidean\\", method=\\"complete\\", row_cluster=False, cbar_pos=(0, .2, .03, .4), row_colors=row_colors ) plt.show() # Call the function to display the clustermap custom_clustermap()"},{"question":"# Custom Descriptor Implementation **Objective:** Students will implement a custom descriptor in Python to manage the access and modification of an attribute in a class. The descriptor should utilize the getter and setter methods provided by Python descriptors. **Task:** You need to implement a class `ManagedAttribute` that uses the descriptor protocol (i.e., defining `__get__`, `__set__`, and `__delete__` methods). This descriptor should validate and manage the value of a specific attribute in a host class, `HostClass`. The `ManagedAttribute` class should: - Ensure that the attribute\'s value is always an integer. - Raise a `TypeError` if an attempt is made to set the attribute to a non-integer value. - Allow reading, setting, and deleting the attribute value. Implement the following: 1. The `ManagedAttribute` class, which includes: - A constructor that initializes the descriptor. - The `__get__`, `__set__`, and `__delete__` methods to manage the attribute. 2. A `HostClass` which uses `ManagedAttribute` as the descriptor for an attribute named `value`. **Input/Output:** ```python class ManagedAttribute: def __init__(self): # Your code here def __get__(self, instance, owner): # Your code here def __set__(self, instance, value): # Your code here def __delete__(self, instance): # Your code here class HostClass: value = ManagedAttribute() # Example Usage host = HostClass() host.value = 42 # No error print(host.value) # Output: 42 host.value = \\"abc\\" # Raises TypeError del host.value # Attribute is deleted print(host.value) # Raises AttributeError ``` **Constraints:** - The `ManagedAttribute` descriptor must specifically ensure that the values are managed correctly. - The `HostClass` should not allow any other type except integers for the `value` attribute. **Performance Requirement:** - The solution should handle the attribute access and modification efficiently using the descriptor protocol. **Guideline:** - Utilize the Python descriptor protocol methods: `__get__`, `__set__`, and `__delete__`. - Proper error handling should be implemented for setting values of incorrect type and accessing deleted attributes.","solution":"class ManagedAttribute: def __init__(self): self._value = None def __get__(self, instance, owner): if self._value is None: raise AttributeError(\\"Attribute is not set\\") return self._value def __set__(self, instance, value): if not isinstance(value, int): raise TypeError(\\"Attribute value must be an integer\\") self._value = value def __delete__(self, instance): self._value = None class HostClass: value = ManagedAttribute()"},{"question":"You are given an XML document as a string, and your task is to implement a SAX parser using the `xml.sax.handler` package to extract specific information and handle various parsing events. **Objective:** Implement a SAX parser that can parse the given XML document and handle the following events: 1. **Start of the document**: Print \\"Document started\\". 2. **End of the document**: Print \\"Document ended\\". 3. **Start of an element**: Print \\"Start element: {element_name}\\", where `{element_name}` is the name of the XML element. 4. **End of an element**: Print \\"End element: {element_name}\\", where `{element_name}` is the name of the XML element. 5. **Character data**: Print \\"Character data: {data}\\", where `{data}` is the data within an XML element. 6. **Processing instruction**: Print \\"Processing instruction: target={target}, data={data}\\", where `{target}` is the processing instruction target and `{data}` is the processing instruction data. 7. **Comments**: Print \\"Comment: {content}\\", where `{content}` is the text of the comment. **Input:** 1. A string representing the XML document to be parsed. **Output:** The output should be a sequence of print statements as described above, based on the events triggered during the parsing of the XML document. **Constraints:** - The XML document provided as input is well-formed. - The input string does not contain external entities. **Example:** Consider the following XML document as input: ```xml <?xml version=\\"1.0\\"?> <!-- Sample XML --> <root> <child attribute=\\"value\\">Text</child> <child>More Text</child> </root> ``` The expected output would be: ``` Document started Comment: Sample XML Start element: root Start element: child Character data: Text End element: child Start element: child Character data: More Text End element: child End element: root Document ended ``` **Implementation Details:** 1. Subclass `xml.sax.handler.ContentHandler` to create your custom handler. 2. Implement the required methods to handle the specified events. 3. Use `xml.sax.make_parser` to create a SAX parser and register your custom handler. 4. Parse the provided XML string. Good luck!","solution":"import xml.sax class CustomContentHandler(xml.sax.ContentHandler): def startDocument(self): print(\\"Document started\\") def endDocument(self): print(\\"Document ended\\") def startElement(self, name, attrs): print(f\\"Start element: {name}\\") def endElement(self, name): print(f\\"End element: {name}\\") def characters(self, content): if content.strip(): # Only print non-empty character data print(f\\"Character data: {content.strip()}\\") def processingInstruction(self, target, data): print(f\\"Processing instruction: target={target}, data={data}\\") class CustomXMLParser: def __init__(self, xml_data): self.xml_data = xml_data def parse(self): parser = xml.sax.make_parser() handler = CustomContentHandler() parser.setContentHandler(handler) xml.sax.parseString(self.xml_data, handler)"},{"question":"# Question: Ensuring Pip Installation in Multiple Virtual Environments You are tasked with writing a Python script that ensures the \\"pip\\" installer is available in a set of virtual environments specified by their paths. If \\"pip\\" is already installed in a virtual environment, it should be upgraded to the latest available version using the \\"ensurepip\\" package. Requirements: 1. Write a function `check_and_install_pip(env_paths: List[str]) -> Dict[str, str]`. This function should: - Accept a list of paths to virtual environments (strings). - For each virtual environment, check if \\"pip\\" is installed. - If \\"pip\\" is not installed, install it using the `ensurepip.bootstrap()` function. - If \\"pip\\" is already installed, upgrade it using the `ensurepip.bootstrap(upgrade=True)` function. - Return a dictionary where the keys are the paths to the virtual environments, and the values are either: - `\\"installed\\"` if \\"pip\\" was successfully installed, - `\\"upgraded\\"` if \\"pip\\" was successfully upgraded, or - `\\"error: <error_message>\\"` if there was an error during the process. 2. Implement the function with appropriate handling and reporting of any exceptions. Input: - A list of strings representing paths to virtual environments. Output: - A dictionary where keys are paths to the virtual environments and values represent the status of \\"pip\\" installation. Constraints: - Assume that the provided paths are valid and point to existing virtual environments. - The script should only use the `ensurepip` package to manage the \\"pip\\" installation and should avoid any side effects on the main environment. Example Usage: ```python env_paths = [\\"/path/to/venv1\\", \\"/path/to/venv2\\", \\"/path/to/venv3\\"] result = check_and_install_pip(env_paths) print(result) ``` Example Output: ```python { \\"/path/to/venv1\\": \\"installed\\", \\"/path/to/venv2\\": \\"upgraded\\", \\"/path/to/venv3\\": \\"installed\\" } ``` Note: Ensure that your implementation does not cause side effects on `sys.path` and `os.environ` of the main environment by using appropriate subprocess handling if necessary.","solution":"import os import subprocess from typing import List, Dict def check_and_install_pip(env_paths: List[str]) -> Dict[str, str]: Ensures the \'pip\' installer is available in a set of virtual environments. results = {} for env_path in env_paths: try: # Check if pip is installed pip_exists = subprocess.run( [os.path.join(env_path, \\"bin\\", \\"python\\"), \\"-m\\", \\"pip\\", \\"--version\\"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE ) # If pip exists, upgrade it if pip_exists.returncode == 0: subprocess.run( [os.path.join(env_path, \\"bin\\", \\"python\\"), \\"-m\\", \\"ensurepip\\", \\"--upgrade\\"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE ) results[env_path] = \\"upgraded\\" except subprocess.CalledProcessError: try: # If pip does not exist, install it subprocess.run( [os.path.join(env_path, \\"bin\\", \\"python\\"), \\"-m\\", \\"ensurepip\\", \\"--default-pip\\"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE ) results[env_path] = \\"installed\\" except subprocess.CalledProcessError as e: results[env_path] = f\\"error: {e}\\" return results"},{"question":"# Advanced Coding Assessment: Understanding `mmap` in Python Objective: Demonstrate your ability to utilize advanced file manipulation using memory-mapped files (`mmap`) in Python, focusing on direct file content manipulation, memory efficiency, and working with binary data. Problem Statement: You are tasked with implementing a function `memory_map_file(filepath, content, access_mode)` that uses memory mapping to: 1. Create a memory-mapped file from the specified `filepath` of a binary file. 2. Modify the content starting from a specified position within the file if the access mode allows writing. 3. Return specific data from the file based on given conditions. 4. Efficiently handle large files by avoiding loading the entire file into memory at once. Function Signature: ```python def memory_map_file(filepath: str, content: bytes, access_mode: str) -> bytes: Parameters: - filepath: str - The path to the binary file to be memory-mapped. - content: bytes - The binary content to write into the memory-mapped file. - access_mode: str - The access mode for the file (\'read\', \'write\', \'copy\'). Returns: - bytes - The binary data read from the file starting from position 10 to 30 as a demonstration. ``` Input: - `filepath`: A string representing the path to a binary file. - `content`: A bytes object representing the binary data to write into the file. - `access_mode`: A string that specifies the access mode, which can be `\'read\'`, `\'write\'`, or `\'copy\'`. Output: - The function should return a bytes object containing the data read from the memory-mapped file starting from position 10 to position 30. Constraints: - If the access mode is `\'read\'`, any attempt to modify the file content should raise a `TypeError`. - Ensure that the file descriptor is properly closed after the operations. - Assume the `filepath` file exists and is writable for the purpose of this task. - Limitations on the length of bytes to be returned may result in partial or empty content if the file length is less than required. Example Usage: ```python file_path = \\"example.bin\\" content_to_write = b\\"new content\\" mode = \\"write\\" result = memory_map_file(file_path, content_to_write, mode) print(result) ``` Performance: - The function should be able to handle large files efficiently. - Use memory-mapped file techniques to avoid loading the entire file into memory at once. Additional Notes: 1. Use appropriate exception handling to manage errors during file operations. 2. Ensure the file is flushed appropriately after write operations to maintain data integrity. 3. Utilize `mmap` constants and methods effectively to meet the requirements. Your solution will be evaluated for correctness, efficiency, and adherence to the use of `mmap` functionalities.","solution":"import mmap import os def memory_map_file(filepath: str, content: bytes, access_mode: str) -> bytes: Parameters: - filepath: str - The path to the binary file to be memory-mapped. - content: bytes - The binary content to write into the memory-mapped file. - access_mode: str - The access mode for the file (\'read\', \'write\', \'copy\'). Returns: - bytes - The binary data read from the file starting from position 10 to 30 as a demonstration. mode = { \'read\': mmap.ACCESS_READ, \'write\': mmap.ACCESS_WRITE, \'copy\': mmap.ACCESS_COPY }.get(access_mode.lower()) if mode is None: raise ValueError(\\"Invalid access mode. Use \'read\', \'write\', or \'copy\'.\\") with open(filepath, \'r+b\' if access_mode in [\'write\', \'copy\'] else \'rb\') as f: size = os.path.getsize(filepath) with mmap.mmap(f.fileno(), length=0, access=mode) as m: if access_mode.lower() in [\'write\', \'copy\']: m[:len(content)] = content[:len(m)] if access_mode.lower() == \'write\': m.flush() start, end = 10, 30 return m[start:end]"},{"question":"# Command-Line Interface for Managing a To-Do List In this assignment, you will implement a Python script that manages a simple to-do list using the `argparse` module. Your script should support the following functionalities: 1. **Adding a task**: - Command: `add` - Arguments: `task_name` - Optional arguments: `--priority` (default: 3) Example: ``` python todo.py add \\"Buy groceries\\" --priority 2 ``` 2. **Listing tasks**: - Command: `list` - Optional arguments: `--priority` (to filter tasks by priority) Example: ``` python todo.py list --priority 2 ``` 3. **Marking a task as done**: - Command: `done` - Arguments: `task_id` Example: ``` python todo.py done 1 ``` 4. **Help**: - Command to show help message for each command. Your implementation should ensure: - Tasks are stored in a dictionary with unique IDs. - \'list\' command should display the tasks in ascending order of their priorities. - \'done\' command should mark a task as completed. - Argument parsing should be handled using `argparse`. - Proper help messages for user guidance. # Input and Output - **Inputs via Command Line**: Commands and their respective arguments. - **Outputs**: - For `add`: Confirmation of task added. - For `list`: List of tasks based on the filter. - For `done`: Confirmation of task completion. # Constraints - Task priorities are integers between 1 (highest) and 5 (lowest). # Example Usage ```shell python todo.py add \\"Read a book\\" --priority 1 Task added: \\"Read a book\\" with priority 1 python todo.py add \\"Workout\\" --priority 3 Task added: \\"Workout\\" with priority 3 python todo.py list 1 - \\"Read a book\\" - Priority: 1 2 - \\"Workout\\" - Priority: 3 python todo.py done 1 Task 1 marked as completed. python todo.py list 2 - \\"Workout\\" - Priority: 3 ``` # Implementation Details 1. Create a class `ToDoManager` to manage tasks: - Store tasks in a dictionary with a unique task ID. - Methods to add tasks, list tasks, and mark tasks as done. 2. In your `todo.py` script, use the `argparse` module to handle the command-line arguments: - Implement the required commands and their arguments as mentioned. 3. Ensure to display helpful messages for each command. # Starter Code ```python import argparse class ToDoManager: def __init__(self): self.tasks = {} self.counter = 1 def add_task(self, task_name, priority): self.tasks[self.counter] = {\\"name\\": task_name, \\"priority\\": priority, \\"done\\": False} print(f\'Task added: \\"{task_name}\\" with priority {priority}\') self.counter += 1 def list_tasks(self, priority=None): filtered_tasks = {k: v for k, v in self.tasks.items() if not priority or v[\\"priority\\"] == priority} for task_id, task in sorted(filtered_tasks.items(), key=lambda item: item[1][\\"priority\\"]): if not task[\\"done\\"]: print(f\\"{task_id} - \\"{task[\'name\']}\\" - Priority: {task[\'priority\']}\\") def mark_task_done(self, task_id): if task_id in self.tasks: self.tasks[task_id][\\"done\\"] = True print(f\'Task {task_id} marked as completed.\') else: print(f\'No task with ID {task_id}\') def main(): manager = ToDoManager() parser = argparse.ArgumentParser(description=\'To-Do List Manager\') subparsers = parser.add_subparsers(dest=\'command\', help=\'Available commands\') # Add command add_parser = subparsers.add_parser(\'add\', help=\'Add a new task\') add_parser.add_argument(\'task_name\', type=str, help=\'The name of the task to be added\') add_parser.add_argument(\'--priority\', type=int, default=3, help=\'The priority of the task (1-5), default is 3\') # List command list_parser = subparsers.add_parser(\'list\', help=\'List all tasks\') list_parser.add_argument(\'--priority\', type=int, help=\'Filter tasks by priority\') # Done command done_parser = subparsers.add_parser(\'done\', help=\'Mark a task as completed\') done_parser.add_argument(\'task_id\', type=int, help=\'The ID of the task to be marked as completed\') args = parser.parse_args() if args.command == \'add\': manager.add_task(args.task_name, args.priority) elif args.command == \'list\': manager.list_tasks(args.priority) elif args.command == \'done\': manager.mark_task_done(args.task_id) else: parser.print_help() if __name__ == \'__main__\': main() ``` Complete the functionality to ensure all commands and their respective behaviors as outlined are implemented correctly.","solution":"import argparse from collections import defaultdict class ToDoManager: def __init__(self): self.tasks = {} self.counter = 1 def add_task(self, task_name, priority): self.tasks[self.counter] = {\\"name\\": task_name, \\"priority\\": priority, \\"done\\": False} print(f\'Task added: \\"{task_name}\\" with priority {priority}\') self.counter += 1 def list_tasks(self, priority=None): filtered_tasks = {k: v for k, v in self.tasks.items() if not priority or v[\\"priority\\"] == priority} for task_id, task in sorted(filtered_tasks.items(), key=lambda item: item[1][\\"priority\\"]): if not task[\\"done\\"]: print(f\\"{task_id} - \\"{task[\'name\']}\\" - Priority: {task[\'priority\']}\\") def mark_task_done(self, task_id): if task_id in self.tasks: self.tasks[task_id][\\"done\\"] = True print(f\'Task {task_id} marked as completed.\') else: print(f\'No task with ID {task_id}\') def create_parser(): parser = argparse.ArgumentParser(description=\'To-Do List Manager\') subparsers = parser.add_subparsers(dest=\'command\', help=\'Available commands\') # Add command add_parser = subparsers.add_parser(\'add\', help=\'Add a new task\') add_parser.add_argument(\'task_name\', type=str, help=\'The name of the task to be added\') add_parser.add_argument(\'--priority\', type=int, default=3, help=\'The priority of the task (1-5), default is 3\') # List command list_parser = subparsers.add_parser(\'list\', help=\'List all tasks\') list_parser.add_argument(\'--priority\', type=int, help=\'Filter tasks by priority\') # Done command done_parser = subparsers.add_parser(\'done\', help=\'Mark a task as completed\') done_parser.add_argument(\'task_id\', type=int, help=\'The ID of the task to be marked as completed\') return parser def main(): manager = ToDoManager() parser = create_parser() args = parser.parse_args() if args.command == \'add\': manager.add_task(args.task_name, args.priority) elif args.command == \'list\': manager.list_tasks(args.priority) elif args.command == \'done\': manager.mark_task_done(args.task_id) else: parser.print_help() if __name__ == \'__main__\': main()"},{"question":"You are required to implement a persistent data storage system for a library using the `shelve` module. The system should maintain records of books and members. Each book and member will have a unique ID. You need to implement functions to add, retrieve, update, and delete records for both books and members, while ensuring data persistence. Specifications: 1. **Books**: Each book has the following attributes: - `book_id` (string): Unique identifier for the book. - `title` (string): Title of the book. - `author` (string): Author of the book. - `year` (integer): Published year. 2. **Members**: Each member has the following attributes: - `member_id` (string): Unique identifier for the member. - `name` (string): Name of the member. - `join_date` (string): Date of joining in \\"YYYY-MM-DD\\" format. 3. **Functions to Implement**: - `open_shelf(filename)`: Opens a shelf with the given filename. - `add_book(shelf, book_id, title, author, year)`: Adds a new book to the shelf. Raises a `ValueError` if `book_id` already exists. - `get_book(shelf, book_id)`: Retrieves and returns the book information as a dictionary. Raises a `KeyError` if `book_id` is not found. - `update_book(shelf, book_id, title, author, year)`: Updates the information of an existing book. Raises a `KeyError` if `book_id` is not found. - `delete_book(shelf, book_id)`: Deletes a book from the shelf. Raises a `KeyError` if `book_id` is not found. - `add_member(shelf, member_id, name, join_date)`: Adds a new member to the shelf. Raises a `ValueError` if `member_id` already exists. - `get_member(shelf, member_id)`: Retrieves and returns the member information as a dictionary. Raises a `KeyError` if `member_id` is not found. - `update_member(shelf, member_id, name, join_date)`: Updates the information of an existing member. Raises a `KeyError` if `member_id` is not found. - `delete_member(shelf, member_id)`: Deletes a member from the shelf. Raises a `KeyError` if `member_id` is not found. - `close_shelf(shelf)`: Closes the shelf and ensures all data is saved. All functions should handle appropriate exceptions and maintain the persistence of data between different executions of the program. Example: ```python library_shelf = open_shelf(\'library.db\') # Adding a book add_book(library_shelf, \'B001\', \'1984\', \'George Orwell\', 1949) # Adding a member add_member(library_shelf, \'M001\', \'John Doe\', \'2022-01-15\') # Retrieving and updating a book book = get_book(library_shelf, \'B001\') update_book(library_shelf, \'B001\', \'1984\', \'George Orwell\', 1950) # Retrieving and deleting a member member = get_member(library_shelf, \'M001\') delete_member(library_shelf, \'M001\') # Closing the shelf close_shelf(library_shelf) ``` Constraints: - Ensure all keys (book_id, member_id) are unique. - Handle exceptions appropriately. - Demonstrate usage of the `shelve` module\'s benefits in data persistence. - Use `writeback=True` when opening the shelf to ensure ease of update operations.","solution":"import shelve def open_shelf(filename): return shelve.open(filename, writeback=True) def close_shelf(shelf): shelf.close() def add_book(shelf, book_id, title, author, year): if book_id in shelf: raise ValueError(f\\"Book ID {book_id} already exists.\\") shelf[book_id] = {\'title\': title, \'author\': author, \'year\': year} def get_book(shelf, book_id): if book_id not in shelf: raise KeyError(f\\"Book ID {book_id} not found.\\") return shelf[book_id] def update_book(shelf, book_id, title, author, year): if book_id not in shelf: raise KeyError(f\\"Book ID {book_id} not found.\\") shelf[book_id] = {\'title\': title, \'author\': author, \'year\': year} def delete_book(shelf, book_id): if book_id not in shelf: raise KeyError(f\\"Book ID {book_id} not found.\\") del shelf[book_id] def add_member(shelf, member_id, name, join_date): if member_id in shelf: raise ValueError(f\\"Member ID {member_id} already exists.\\") shelf[member_id] = {\'name\': name, \'join_date\': join_date} def get_member(shelf, member_id): if member_id not in shelf: raise KeyError(f\\"Member ID {member_id} not found.\\") return shelf[member_id] def update_member(shelf, member_id, name, join_date): if member_id not in shelf: raise KeyError(f\\"Member ID {member_id} not found.\\") shelf[member_id] = {\'name\': name, \'join_date\': join_date} def delete_member(shelf, member_id): if member_id not in shelf: raise KeyError(f\\"Member ID {member_id} not found.\\") del shelf[member_id]"},{"question":"# Advanced Coding Assessment: UUID Handling and Safety in Python Problem Statement You are tasked with utilizing Python\'s `uuid` module to handle and generate UUIDs in a multiprocess-safe way. This exercise will assess your knowledge of creating different types of UUIDs, understanding their properties, and ensuring the safety of generated UUIDs. Requirements 1. **Generate a Version 1 UUID in a Multiprocessing-Safe Manner** - Write a function named `generate_safe_uuid1` that generates a UUID using `uuid.uuid1()`. - Check if the generated UUID is safe and return a tuple `(UUID_object, is_safe)` where `is_safe` is a boolean indicating the safety of the UUID. 2. **Convert a UUID to Different Formats** - Write a function named `uuid_to_formats` that takes a UUID object as input and returns a dictionary with the following keys: - `\'hex\'`: The UUID as a 32-character hexadecimal string. - `\'int\'`: The UUID as a 128-bit integer. - `\'bytes\'`: The UUID as a 16-byte string. 3. **Validate and Parse UUID Strings** - Write a function named `validate_and_parse_uuid` that takes a string as input and tries to create a UUID object from it. - If the input string is a valid UUID string, return the UUID object. - If the input string is not valid, raise a `ValueError` with an appropriate message. Input and Output Formats - **generate_safe_uuid1()** - **Input:** None - **Output:** Tuple with a UUID object and a boolean indicating if the UUID is safe `(UUID_object, is_safe)` - **uuid_to_formats(uuid_obj)** - **Input:** `uuid_obj`: A UUID object. - **Output:** Dictionary with keys `\'hex\'`, `\'int\'`, and `\'bytes\'`. - **validate_and_parse_uuid(uuid_string)** - **Input:** `uuid_string`: A string representing a potential UUID. - **Output:** A UUID object if the input is valid, otherwise raises `ValueError`. Constraints - The functions should handle all edge cases such as invalid UUID strings or unsupported UUID formats. Example Usage ```python import uuid # Example for generate_safe_uuid1 uuid_obj, is_safe = generate_safe_uuid1() print(uuid_obj, is_safe) # Example for uuid_to_formats uuid_obj = uuid.uuid4() formats = uuid_to_formats(uuid_obj) print(formats) # Example: {\'hex\': \'...\', \'int\': ..., \'bytes\': b\'...\'} # Example for validate_and_parse_uuid try: uuid_obj = validate_and_parse_uuid(\\"12345678-1234-5678-1234-567812345678\\") print(uuid_obj) # Outputs a UUID object except ValueError as ve: print(str(ve)) ``` **Note:** Ensure that the provided solutions conform to the RFC 4122 standards and handle exceptions appropriately.","solution":"import uuid def generate_safe_uuid1(): Generates a version 1 UUID in a multiprocessing-safe manner. Returns a tuple (UUID_object, is_safe) where is_safe is a boolean. uuid_obj = uuid.uuid1() is_safe = uuid_obj.is_safe == uuid.SafeUUID.safe return uuid_obj, is_safe def uuid_to_formats(uuid_obj): Takes a UUID object as input and returns a dictionary with the UUID in hex, int, and bytes formats. return { \'hex\': uuid_obj.hex, \'int\': uuid_obj.int, \'bytes\': uuid_obj.bytes, } def validate_and_parse_uuid(uuid_string): Takes a string as input and tries to create a UUID object from it. If valid, returns the UUID object. Otherwise, raises a ValueError. try: return uuid.UUID(uuid_string) except ValueError: raise ValueError(\\"Invalid UUID string\\")"},{"question":"**Question:** You are required to implement a function named `dynamic_import` which will import a specified module dynamically and utilize its functionalities. # Function Signature: ```python def dynamic_import(module_name: str, function_name: str, *args, **kwargs) -> any: pass ``` # Input Parameters: - `module_name` (str): The name of the module to import. - `function_name` (str): The name of the function within the module to execute. - `*args`: Positional arguments to pass to the function being called. - `**kwargs`: Keyword arguments to pass to the function being called. # Output: - The return value from the dynamically imported function after it has been executed with the provided arguments. # Constraints: 1. The function should handle the case where the module does not exist and raise an appropriate error. 2. The function should handle the case where the function does not exist within the imported module and raise an appropriate error. 3. The function should ensure proper importing using `importlib` and should not simply use Python\'s built-in `__import__`. 4. You are allowed to assume that the functions you need to call in the dynamically imported module do not require imports of other non-standard libraries. # Example Usage: ```python # Assuming module \'math\' with function \'sqrt\' is used result = dynamic_import(\'math\', \'sqrt\', 9) print(result) # Output: 3.0 # Assuming a custom module \'mymodule\' with function \'add\' having signature (a, b) # result = dynamic_import(\'mymodule\', \'add\', 3, 4) # print(result) # Output: 7 ``` # Notes: - Do not use built-in `__import__` for the solution. - Use `importlib` to locate and import the module, and subsequently retrieve the function to be executed. - Handle exceptions gracefully by providing meaningful error messages. # Hints: - You may want to explore `importlib.import_module()` for importing the module. - Consider using `getattr()` to retrieve the function from the imported module.","solution":"import importlib def dynamic_import(module_name: str, function_name: str, *args, **kwargs) -> any: Dynamically imports a module and executes a specified function within it. Parameters: - module_name (str): The name of the module to import. - function_name (str): The name of the function within the module to execute. - *args: Positional arguments to pass to the function being called. - **kwargs: Keyword arguments to pass to the function being called. Returns: - any: The return value from the dynamically imported function. try: module = importlib.import_module(module_name) except ModuleNotFoundError: raise ImportError(f\\"Module \'{module_name}\' not found\\") try: func = getattr(module, function_name) except AttributeError: raise AttributeError(f\\"Function \'{function_name}\' not found in module \'{module_name}\'\\") return func(*args, **kwargs)"},{"question":"# Objective You are required to make use of the seaborn library to generate and customize color palettes for visualizations. Your task involves retrieving specific color palettes, customizing them, and using them in a context manager. # Task 1. Import seaborn as `sns` and set the theme using `sns.set_theme()`. 2. Retrieve a categorical color palette called \\"Set2\\" consisting of unique colors, and store it in a variable. 3. Create and store a blend gradient between two colors `#7AB` and `#EDA` as a continuous colormap. 4. Within a context manager, change the qualitative color palette to \\"pastel\\" and create a scatter plot using the `scatterplot` function with the following data: - `x = [1, 2, 3, 4, 5]` - `y = [10, 9, 8, 7, 6]` - The points should be colored with distinct colors from the \\"pastel\\" palette. 5. Print the hex codes of the \\"Set2\\" palette. # Constraints - You should not modify the seaborn source code. - The solution should utilize proper seaborn functions to perform each task. # Expected Output - The script should execute without errors and display the scatter plot. - Print the hex codes of the \\"Set2\\" palette at the end. ```python # Sample code structure to guide you import seaborn as sns # Part 1 sns.set_theme() # Part 2 set2_palette = sns.color_palette(\\"Set2\\") # Part 3 blend_palette = sns.color_palette(\\"blend:#7AB,#EDA\\", as_cmap=True) # Part 4 x = [1, 2, 3, 4, 5] y = [10, 9, 8, 7, 6] with sns.color_palette(\\"pastel\\"): sns.scatterplot(x=x, y=y, palette=\\"pastel\\") # Part 5 print(set2_palette.as_hex()) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Part 1 sns.set_theme() # Part 2 set2_palette = sns.color_palette(\\"Set2\\") # Part 3 blend_palette = sns.color_palette(\\"blend:#7AB,#EDA\\", as_cmap=True) # Part 4 x = [1, 2, 3, 4, 5] y = [10, 9, 8, 7, 6] with sns.color_palette(\\"pastel\\"): sns.scatterplot(x=x, y=y, hue=x, palette=\\"pastel\\") plt.show() # Part 5 print(set2_palette.as_hex())"},{"question":"**Question:** You are tasked with implementing a Python function to compress and decompress text data using the `bz2` module. The function should accept a string input, compress the string, write it to a bzip2-compressed file, then read from this file and decompress the content to verify its correctness. **Function Signature:** ```python def compress_and_verify_text(input_text: str, compresslevel: int) -> bool: pass ``` **Inputs:** - `input_text`: A string containing the text data to be compressed and decompressed. - `compresslevel`: An integer between 1 and 9 indicating the level of compression (1 being the least and 9 being the most). **Outputs:** - The function should return `True` if the decompressed text matches the original `input_text`, otherwise return `False`. **Constraints:** - You must use the `bz2.BZ2File` class or `bz2.open` for file operations. - Ensure appropriate handling of file modes (`wb`, `rb`). - You may assume that the input text is relatively small and does not require handling large volumes of data. **Example Usage:** ```python input_text = \\"Sample text data for compression and decompression testing.\\" compresslevel = 5 result = compress_and_verify_text(input_text, compresslevel) print(result) # Should print: True ``` Implement this function considering the constraints and ensure that the compression and decompression using files is handled correctly.","solution":"import bz2 def compress_and_verify_text(input_text: str, compresslevel: int) -> bool: Compresses the input text at the given compression level, writes it to a bzip2 compressed file, reads from the file, and verifies if the decompressed text matches the original text. :param input_text: The text to be compressed and verified. :param compresslevel: The compression level (1-9). :return: True if the decompressed text matches the original; False otherwise. # Name of the temporary file temp_filename = \'temp.bz2\' # Compress the input text and write to a file with bz2.open(temp_filename, \'wb\', compresslevel=compresslevel) as file: file.write(input_text.encode(\'utf-8\')) # Read the compressed data, decompress it and verify with bz2.open(temp_filename, \'rb\') as file: decompressed_text = file.read().decode(\'utf-8\') return decompressed_text == input_text"},{"question":"Problem Statement You are tasked with writing a Python function compatible with both Python 2 and Python 3 that processes a given text file. The function should read the file, count the number of occurrences of each word (case-insensitive), and return a dictionary with words as keys and their occurrence counts as values. Requirements: 1. Implement the function `word_count(file_path)`: - **Input**: `file_path` (string) - the path to the text file. - **Output**: Dictionary where keys are words (lowercase) and values are their respective occurrence counts. 2. The function must handle text files correctly for both Python 2 and Python 3. 3. Ensure proper handling of text and binary data distinctions. 4. Implement feature detection to ensure compatibility with both Python 2 and Python 3. Constraints: - Ignore non-alphabetic characters from the text. - Consider words case-insensitively (e.g., \\"Hello\\" and \\"hello\\" should be treated as the same word). Example: ```python # Assuming the content of \'sample.txt\' is: \\"Hello world! This is a test. Hello again, world.\\" word_count(\'sample.txt\') # Output: {\'hello\': 2, \'world\': 2, \'this\': 1, \'is\': 1, \'a\': 1, \'test\': 1, \'again\': 1} ``` Notes: - Use `io.open()` for opening the file. - Ensure the function properly handles differences between binary and text data. - Avoid using Python version checks directly. Instead, use feature detection. [Total points: 40 points]","solution":"import io import re def word_count(file_path): Reads a text file and counts occurrences of each word (case-insensitive). Args: file_path (str): Path to the text file. Returns: dict: A dictionary where keys are words (lowercase) and values are their occurrence counts. word_counts = {} # Using io.open for compatibility between Python 2 and Python 3 with io.open(file_path, \'r\', encoding=\'utf-8\') as file: for line in file: # Removing non-alphabetic characters and converting to lowercase words = re.findall(r\'b[a-zA-Z]+b\', line.lower()) for word in words: if word in word_counts: word_counts[word] += 1 else: word_counts[word] = 1 return word_counts"},{"question":"You are tasked with building a composite estimator for preprocessing and model training on a mixed-type dataset comprising both numerical and categorical features. You are required to demonstrate your understanding of `Pipeline`, `FeatureUnion`, and `ColumnTransformer` by employing them to create an efficient machine learning pipeline. Dataset Description A mock dataset is provided with the following characteristics: - `numeric1`, `numeric2`: Two numerical features - `category1`, `category2`: Two categorical features - `text`: A text feature representing user reviews - `target`: A numerical target variable for regression Task Implement a scikit-learn `Pipeline` that performs the following steps: 1. Scales the numerical features using `StandardScaler`. 2. Encodes the categorical features using `OneHotEncoder`. 3. Transforms the text feature using `TfidfVectorizer`. 4. Uses a `FeatureUnion` to combine the processed numerical, categorical, and text features. 5. Trains a `RandomForestRegressor` model using the combined features. Input The input will be a dataset in the form of a Pandas DataFrame having columns: `numeric1`, `numeric2`, `category1`, `category2`, `text`, and `target`. Output Return the trained pipeline after fitting it to the input dataset. Function Signature ```python def build_and_train_pipeline(df: pd.DataFrame) -> Pipeline: pass ``` Example Input ```python import pandas as pd data = { \'numeric1\': [1.1, 2.2, 3.3], \'numeric2\': [4.4, 5.5, 6.6], \'category1\': [\'A\', \'B\', \'A\'], \'category2\': [\'X\', \'Y\', \'X\'], \'text\': [\'good product\', \'bad product\', \'average product\'], \'target\': [10, 12, 14] } df = pd.DataFrame(data) ``` Example Output The returned object should be an instance of `Pipeline` trained on the given DataFrame. ```python from sklearn.pipeline import Pipeline pipe: Pipeline = build_and_train_pipeline(df) print(pipe) ``` The printout should reflect the pipeline steps with fitted estimators. Constraints - Ensure that no data leakage occurs during the preprocessing steps. - Use appropriate pipeline and feature union techniques as described in the `scikit-learn` documentation. Performance Requirements - Assume the input dataset can be large (thousands of rows). - Optimize the pipeline to handle large datasets efficiently. Note Make sure to handle missing data appropriately if present and follow best practices for data preprocessing.","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder def build_and_train_pipeline(df: pd.DataFrame) -> Pipeline: # Identifying feature types numeric_features = [\'numeric1\', \'numeric2\'] categorical_features = [\'category1\', \'category2\'] text_feature = \'text\' target = \'target\' # Numerical transformer numeric_transformer = Pipeline(steps=[ (\'scaler\', StandardScaler()) ]) # Categorical transformer categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Text transformer text_transformer = Pipeline(steps=[ (\'tfidf\', TfidfVectorizer()) ]) # Preprocessor combining all transformers preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features), (\'text\', text_transformer, text_feature) ] ) # Full pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', RandomForestRegressor(n_estimators=100, random_state=42)) ]) # Splitting data into features and target X = df.drop(columns=[target]) y = df[target] # Fitting the pipeline pipeline.fit(X, y) return pipeline"},{"question":"**CGI Form Data and File Upload Handling** **Objective:** Create a CGI script using the `cgi` module that processes form data and handles file uploads. Your task is to write a function that processes form data and file uploads from an HTML form and returns the processed output as an HTML response. **Function Signature:** ```python def handle_cgi_request(): pass ``` **Requirements:** 1. The function should use the `cgi.FieldStorage` class to read form data and uploaded files. 2. The form may contain the following fields: - `username`: Text input field for the user\'s name. - `email`: Text input field for the user\'s email. - `age`: Text input field for the user\'s age (must be an integer). - `profile_picture`: File input field for uploading the user\'s profile picture. 3. The function should validate the fields: - The `username` and `email` fields should not be empty. - The `age` field should be a positive integer. - The `profile_picture` field should contain a file. 4. The function should generate an HTML response containing: - A confirmation message with the user\'s `username`, `email`, and `age`. - The filename of the uploaded `profile_picture`. - An error message if any validation fails. **Example HTML form:** ```html <form method=\\"post\\" enctype=\\"multipart/form-data\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> Email: <input type=\\"text\\" name=\\"email\\"><br> Age: <input type=\\"text\\" name=\\"age\\"><br> Profile Picture: <input type=\\"file\\" name=\\"profile_picture\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> ``` **Constraints:** - Assume the script will be executed in a web server environment where standard CGI environment variables are set. - You do not need to handle the actual saving of uploaded files, just reading and reporting on them. **Expected Output:** - An HTML response will be printed with details of the submitted form or an error message if any field validation fails. **Example Function Output:** ```html Content-Type: text/html <html> <head><title>Form Response</title></head> <body> <h1>Form Submission Successful</h1> <p>Username: JohnDoe</p> <p>Email: john.doe@example.com</p> <p>Age: 30</p> <p>Profile Picture Filename: profile.jpg</p> </body> </html> ``` Or if there are validation errors: ```html Content-Type: text/html <html> <head><title>Form Response</title></head> <body> <h1>Form Submission Error</h1> <p>Error: Age must be a positive integer.</p> </body> </html> ``` Write the `handle_cgi_request()` function to fulfill the requirements described above.","solution":"import cgi import cgitb; cgitb.enable() # For debugging purposes def handle_cgi_request(): form = cgi.FieldStorage() username = form.getvalue(\'username\') email = form.getvalue(\'email\') age = form.getvalue(\'age\') profile_picture = form[\'profile_picture\'] if \'profile_picture\' in form else None errors = [] # Validation if not username: errors.append(\\"Username is required.\\") if not email: errors.append(\\"Email is required.\\") if not age: errors.append(\\"Age is required.\\") elif not age.isdigit() or int(age) <= 0: errors.append(\\"Age must be a positive integer.\\") if not profile_picture or not profile_picture.filename: errors.append(\\"Profile picture is required.\\") if errors: # If there are validation errors, display them print(\\"Content-Type: text/htmln\\") print(\\"<html>\\") print(\\"<head><title>Form Response</title></head>\\") print(\\"<body>\\") print(\\"<h1>Form Submission Error</h1>\\") for error in errors: print(f\\"<p>Error: {error}</p>\\") print(\\"</body></html>\\") else: # Constructing a successful submission response profile_filename = profile_picture.filename if profile_picture else \\"N/A\\" print(\\"Content-Type: text/htmln\\") print(\\"<html>\\") print(\\"<head><title>Form Response</title></head>\\") print(\\"<body>\\") print(\\"<h1>Form Submission Successful</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Email: {email}</p>\\") print(f\\"<p>Age: {age}</p>\\") print(f\\"<p>Profile Picture Filename: {profile_filename}</p>\\") print(\\"</body></html>\\")"},{"question":"# Question You are required to enhance the built-in `print()` function so that it logs all printed messages to a file in addition to printing them to the console. You will achieve this by using the `builtins` module to access the original `print()` function. Your task is to implement a function called `custom_print()` and a class called `PrintLogger` as described below: Function Specification 1. **Function Name**: `custom_print` 2. **Parameters**: - `message`: The message to be printed (string). - `file_path`: The file path where the log should be written (string). 3. **Behavior**: - Print the `message` to the console using the original built-in `print` function. - Append the `message` to the file specified by `file_path`. Class Specification 1. **Class Name**: `PrintLogger` 2. **Attributes**: - `file_path`: The file path where all printed messages will be logged (string). 3. **Methods**: - `__init__(self, file_path)`: Initializes the logger with the file path. - `print(self, message)`: Logs the message to the file and prints it to the console using `custom_print()`. # Constraints - The `message` will always be a string. - The file specified in `file_path` will exist and be writable. # Example ```python import builtins # Function to be implemented def custom_print(message, file_path): # Your implementation here # Class to be implemented class PrintLogger: def __init__(self, file_path): # Your implementation here def print(self, message): # Your implementation here # Example usage logger = PrintLogger(\'log.txt\') logger.print(\'Hello, World!\') # This should print \\"Hello, World!\\" to the console and also append \\"Hello, World!\\" to log.txt ``` # Additional Information - The function and methods should handle the opening and closing of the file appropriately. - You should use the built-in `print` function for console output within `custom_print`.","solution":"import builtins def custom_print(message, file_path): Custom print function that prints to the console and logs the message to a specified file. :param message: str: The message to print and log. :param file_path: str: The path to the file where the message should be logged. # Print the message to the console builtins.print(message) # Append the message to the specified file with open(file_path, \'a\') as file: file.write(message + \'n\') class PrintLogger: def __init__(self, file_path): Initializes the PrintLogger with the path to the log file. :param file_path: str: The path to the file where messages should be logged. self.file_path = file_path def print(self, message): Logs the message to the file and prints it to the console. :param message: str: The message to print and log. custom_print(message, self.file_path)"},{"question":"**Objective:** Assess the ability to use the `dataclasses` module to create and manage data structures effectively while leveraging its advanced features like inheritance, frozen instances, and custom field handling. **Problem Statement:** You are tasked with designing a simplified, immutable library management system using Python\'s `dataclasses` module. Follow the instructions below to build the components of this system. 1. **Define a Base Class for Book:** - Create a `Book` class with the following attributes: - `title` (string): The title of the book. - `author` (string): The author of the book. - `isbn` (string): The ISBN number of the book. - `pages` (integer, default: 0): Number of pages in the book. - `quantity` (integer, default: 1): Number of copies available in the library. - Ensure the class is immutable. - Implement a method `__post_init__` to validate the number of pages and quantity, ensuring they are non-negative. - Use appropriate type annotations for all fields. 2. **Define a Derived Class for EBook:** - Create an `EBook` class that extends `Book` with the additional attribute: - `file_size` (float, default: 0.0): Size of the eBook file in MB. - Ensure the added attribute also follows the immutability rule. - Validate the `file_size` attribute in the `__post_init__` method, ensuring it is non-negative. 3. **Utility Functions:** - Implement a function `create_sample_books()` that returns a list of `Book` and `EBook` instances with sample data. - Implement a function `calculate_library_size(books: List[Book]) -> int` that returns the total number of books (sum of `quantity` for all books). 4. **Serialization & Deserialization:** - Implement a function `books_to_dict(books: List[Book]) -> dict` that converts a list of books into a dictionary representation using `dataclasses.asdict()`. - Implement a function `dict_to_books(data: dict) -> List[Book]` that converts a dictionary back into a list of `Book` instances. **Function Signature:** ```python from typing import List, Dict from dataclasses import dataclass, field, asdict, astuple, replace @dataclass(frozen=True) class Book: title: str author: str isbn: str pages: int = 0 quantity: int = 1 def __post_init__(self): if self.pages < 0: raise ValueError(\\"Number of pages cannot be negative\\") if self.quantity < 0: raise ValueError(\\"Quantity cannot be negative\\") @dataclass(frozen=True) class EBook(Book): file_size: float = 0.0 def __post_init__(self): super().__post_init__() if self.file_size < 0: raise ValueError(\\"File size cannot be negative\\") def create_sample_books() -> List[Book]: # Implement this function to return a list of sample books pass def calculate_library_size(books: List[Book]) -> int: # Implement this function to calculate total number of books pass def books_to_dict(books: List[Book]) -> Dict: # Implement this function to serialize books list to dictionary pass def dict_to_books(data: Dict) -> List[Book]: # Implement this function to deserialize dictionary to books list pass ``` **Expected Input and Output:** ```python # Sample Input sample_books = create_sample_books() sample_dict = books_to_dict(sample_books) recovered_books = dict_to_books(sample_dict) total_books = calculate_library_size(sample_books) # Expected Output print(sample_books) # List of Book and EBook instances print(sample_dict) # Dictionary representation of sample_books print(recovered_books) # List of Book and EBook instances reconstructed from dictionary print(total_books) # Total count of all books in sample_books ``` **Constraints:** - The `pages`, `quantity`, and `file_size` attributes must be non-negative. - Implement immutability by setting the `frozen=True` parameter in `dataclass`. **Performance requirements:** - The solution should efficiently handle lists of up to 1000 `Book` and `EBook` instances within a reasonable time frame. Good luck!","solution":"from typing import List, Dict from dataclasses import dataclass, field, asdict @dataclass(frozen=True) class Book: title: str author: str isbn: str pages: int = 0 quantity: int = 1 def __post_init__(self): if self.pages < 0: raise ValueError(\\"Number of pages cannot be negative\\") if self.quantity < 0: raise ValueError(\\"Quantity cannot be negative\\") @dataclass(frozen=True) class EBook(Book): file_size: float = 0.0 def __post_init__(self): super().__post_init__() if self.file_size < 0: raise ValueError(\\"File size cannot be negative\\") def create_sample_books() -> List[Book]: return [ Book(title=\\"Book A\\", author=\\"Author A\\", isbn=\\"1234567890\\", pages=100, quantity=5), EBook(title=\\"EBook B\\", author=\\"Author B\\", isbn=\\"0987654321\\", pages=200, quantity=2, file_size=1.5), Book(title=\\"Book C\\", author=\\"Author C\\", isbn=\\"1122334455\\", pages=150, quantity=3), EBook(title=\\"EBook D\\", author=\\"Author D\\", isbn=\\"5566778899\\", pages=250, quantity=4, file_size=2.3) ] def calculate_library_size(books: List[Book]) -> int: return sum(book.quantity for book in books) def books_to_dict(books: List[Book]) -> List[Dict]: return [asdict(book) for book in books] def dict_to_books(data: List[Dict]) -> List[Book]: book_list = [] for item in data: if \'file_size\' in item: book_list.append(EBook(**item)) else: book_list.append(Book(**item)) return book_list"},{"question":"You are tasked with creating a Python script that performs a sequence of operations to manage files and directories and execute system commands using the `os` module. This task will assess your understanding of file operations, directory management, and process handling. # Requirements 1. **Create a Directory and Subdirectories:** - Create a main directory named `\'main_dir\'`. - Inside `\'main_dir\'`, create two subdirectories: `\'sub_dir1\'` and `\'sub_dir2\'`. 2. **Create and Write to Files:** - In `\'main_dir/sub_dir1\'`, create a file named `\'file1.txt\'` and write the string `\\"Hello from file1\\"` into it. - In `\'main_dir/sub_dir2\'`, create a file named `\'file2.txt\'` and write the string `\\"Hello from file2\\"` into it. 3. **Read and Display File Contents:** - Read the contents of both `\'file1.txt\'` and `\'file2.txt\'` and print them to the console. 4. **Move a File:** - Move `\'main_dir/sub_dir1/file1.txt\'` to `\'main_dir/sub_dir2/\'` and rename it to `\'file1_moved.txt\'`. 5. **List Directory Contents:** - List and print the names of all files and subdirectories in `\'main_dir/sub_dir2\'`. 6. **Execute a Command:** - Execute a system command to display the current working directory (`pwd` on Unix or `cd` on Windows) and print its output. # Input and Output - **Input:** No user input required. - **Output:** Print statements displaying the contents of the files, the list of directory contents, and the output of the executed system command. # Constraints - Ensure that all file and directory operations handle possible exceptions (e.g., directory or file already exists, file not found). - Use appropriate functions and methods from the `os` module. # Example **Output:** ```plaintext Contents of file1.txt: Hello from file1 Contents of file2.txt: Hello from file2 Contents of main_dir/sub_dir2: [\'file1_moved.txt\', \'file2.txt\'] Current Working Directory: /path/to/current/directory ``` # Implementation You are required to implement the function `manage_files_and_directories()` that performs the above operations. ```python import os import shutil def manage_files_and_directories(): # Step 1: Create directories os.makedirs(\'main_dir/sub_dir1\', exist_ok=True) os.makedirs(\'main_dir/sub_dir2\', exist_ok=True) # Step 2: Create and write to files with open(\'main_dir/sub_dir1/file1.txt\', \'w\') as file1: file1.write(\\"Hello from file1\\") with open(\'main_dir/sub_dir2/file2.txt\', \'w\') as file2: file2.write(\\"Hello from file2\\") # Step 3: Read and display file contents with open(\'main_dir/sub_dir1/file1.txt\', \'r\') as file1: print(\\"Contents of file1.txt:\\", file1.read()) with open(\'main_dir/sub_dir2/file2.txt\', \'r\') as file2: print(\\"Contents of file2.txt:\\", file2.read()) # Step 4: Move file shutil.move(\'main_dir/sub_dir1/file1.txt\', \'main_dir/sub_dir2/file1_moved.txt\') # Step 5: List directory contents contents = os.listdir(\'main_dir/sub_dir2\') print(\\"Contents of main_dir/sub_dir2:\\", contents) # Step 6: Execute a command to display the current working directory current_dir = os.popen(\'pwd\' if os.name != \'nt\' else \'cd\').read().strip() print(\\"Current Working Directory:\\", current_dir) # Call the function to execute the script manage_files_and_directories() ``` **Note:** Ensure that you test your script thoroughly before submission.","solution":"import os import shutil def manage_files_and_directories(): try: # Step 1: Create directories os.makedirs(\'main_dir/sub_dir1\', exist_ok=True) os.makedirs(\'main_dir/sub_dir2\', exist_ok=True) # Step 2: Create and write to files with open(\'main_dir/sub_dir1/file1.txt\', \'w\') as file1: file1.write(\\"Hello from file1\\") with open(\'main_dir/sub_dir2/file2.txt\', \'w\') as file2: file2.write(\\"Hello from file2\\") # Step 3: Read and display file contents with open(\'main_dir/sub_dir1/file1.txt\', \'r\') as file1: print(\\"Contents of file1.txt:\\", file1.read()) with open(\'main_dir/sub_dir2/file2.txt\', \'r\') as file2: print(\\"Contents of file2.txt:\\", file2.read()) # Step 4: Move file shutil.move(\'main_dir/sub_dir1/file1.txt\', \'main_dir/sub_dir2/file1_moved.txt\') # Step 5: List directory contents contents = os.listdir(\'main_dir/sub_dir2\') print(\\"Contents of main_dir/sub_dir2:\\", contents) # Step 6: Execute a command to display the current working directory current_dir = os.popen(\'pwd\' if os.name != \'nt\' else \'cd\').read().strip() print(\\"Current Working Directory:\\", current_dir) except Exception as e: print(f\\"An error occurred: {e}\\") # Call the function to execute the script manage_files_and_directories()"},{"question":"# Question You are given a PyTorch model that performs matrix multiplication with large tensors. To optimize performance, you need to implement a function that: 1. Performs matrix multiplication in parallel using TorchScript. 2. Tunes the number of intra-op threads to find the optimal runtime. Implement the function `optimized_matrix_multiplication` with the following requirements: - The function should take an input matrix `x` and perform two matrix multiplications, one of which should be executed asynchronously. - The function should return the execution time for different numbers of threads and the corresponding results of the matrix multiplication in a list. - Use TorchScript\'s `fork` and `wait` to run one of the matrix multiplication tasks asynchronously. - Tune the number of intra-op threads to optimize performance, testing from 1 up to a specified maximum number of threads. Function Signature: ```python import torch import timeit from torch import Tensor from typing import List, Tuple @torch.jit.script def compute_y(x: Tensor) -> Tensor: # replace with actual parameters w_y = torch.randn(x.size(1), x.size(1)) return torch.mm(x, w_y) @torch.jit.script def compute_z(x: Tensor) -> Tensor: # replace with actual parameters w_z = torch.randn(x.size(1), x.size(1)) return torch.mm(x, w_z) def optimized_matrix_multiplication(x: Tensor, max_threads: int) -> List[Tuple[int, float, Tensor]]: results = [] def forward(x: Tensor) -> Tensor: fut = torch.jit._fork(compute_z, x) y = compute_y(x) z = torch.jit._wait(fut) return y + z for t in range(1, max_threads + 1): torch.set_num_threads(t) runtime = timeit.timeit(lambda: forward(x), number=100) result = forward(x) results.append((t, runtime, result)) return results ``` Input: - `x` (Tensor): The input matrix of shape `(N, N)`. - `max_threads` (int): The maximum number of threads to test. Output: - A list of tuples, each containing: - The number of threads used. - The execution time for 100 iterations. - The resulting matrix from the computation. Constraints: - The input matrix `x` is a square matrix with dimensions up to `(1024, 1024)`. - Use PyTorch\'s `torch.jit._fork`, `torch.jit._wait`, `set_num_threads`, and `timeit` for implementation. Implement the `optimized_matrix_multiplication` function to achieve optimal performance using parallelism in PyTorch.","solution":"import torch import timeit from torch import Tensor from typing import List, Tuple @torch.jit.script def compute_y(x: Tensor) -> Tensor: w_y = torch.randn(x.size(1), x.size(1)) return torch.mm(x, w_y) @torch.jit.script def compute_z(x: Tensor) -> Tensor: w_z = torch.randn(x.size(1), x.size(1)) return torch.mm(x, w_z) def optimized_matrix_multiplication(x: Tensor, max_threads: int) -> List[Tuple[int, float, Tensor]]: results = [] @torch.jit.script def forward(x: Tensor) -> Tensor: fut = torch.jit._fork(compute_z, x) y = compute_y(x) z = torch.jit._wait(fut) return y + z for t in range(1, max_threads + 1): torch.set_num_threads(t) runtime = timeit.timeit(lambda: forward(x), number=10) result = forward(x) results.append((t, runtime, result)) return results"},{"question":"Objective: This question aims to assess your understanding of the Python `warnings` module and your ability to manage and control warnings in a specific scenario while ensuring thread safety. Problem Statement: You are required to create a Python program that simulates a utility that performs several operations, some of which may issue warnings. Your task is to: 1. Define a custom warning category called `CustomWarning`. 2. Implement a function `process_data(data)` that processes a given list of integers. If an integer is negative, this function should issue a `CustomWarning`. 3. Implement another function `handle_warnings(data)` that: - Uses `warnings.catch_warnings(record=True)` context manager to capture all warnings raised during the execution of the `process_data` function. - Ensures that any warning issued is logged as a message in a list, which is returned at the end of `handle_warnings`. 4. Implement multi-threaded behavior to demonstrate thread safety by: - Creating a class `ThreadSafeProcessor` with methods to process multiple lists of data concurrently. - Ensuring that each thread captures and logs its warnings separately without interference. - Merging and returning the combined list of warnings from all threads. Input: - A list of lists, where each inner list contains integers to be processed. Output: - A list of strings representing the captured warnings from all threads. Constraints: - Ensure the solution is thread-safe. - Use the `warnings` module\'s functionality as described. Example: ```python # Custom warning definition class CustomWarning(Warning): pass # Function to process data and issue warnings def process_data(data): pass # Function to handle warnings def handle_warnings(data): pass # Class to handle multi-threaded processing class ThreadSafeProcessor: pass # Example usage: data_lists = [[1, -1, 2], [3, -4, 5]] processor = ThreadSafeProcessor() warnings = processor.process(data_lists) print(warnings) ``` Expected Output: ``` [ \\"Negative value found: -1 in data: [1, -1, 2]\\", \\"Negative value found: -4 in data: [3, -4, 5]\\" ] ``` Notes: - The warning message format should be: `\\"Negative value found: {value} in data: {data}\\"`. - Use the `CustomWarning` category for all warnings issued. - Ensure that your solution only uses built-in modules and no external libraries.","solution":"import warnings import threading # Custom warning definition class CustomWarning(Warning): pass # Function to process data and issue warnings def process_data(data): for value in data: if value < 0: warnings.warn(f\\"Negative value found: {value} in data: {data}\\", CustomWarning) # Function to handle warnings def handle_warnings(data): warning_messages = [] with warnings.catch_warnings(record=True) as caught_warnings: warnings.simplefilter(\\"always\\", CustomWarning) process_data(data) for warning in caught_warnings: warning_messages.append(str(warning.message)) return warning_messages # Class to handle multi-threaded processing class ThreadSafeProcessor: def __init__(self): self.lock = threading.Lock() def process_data_list(self, data_list): return handle_warnings(data_list) def process(self, list_of_data): threads = [] result = [] def thread_function(data): warnings_list = self.process_data_list(data) with self.lock: result.extend(warnings_list) for data in list_of_data: thread = threading.Thread(target=thread_function, args=(data,)) threads.append(thread) thread.start() for thread in threads: thread.join() return result # Example usage: # data_lists = [[1, -1, 2], [3, -4, 5]] # processor = ThreadSafeProcessor() # warnings = processor.process(data_lists) # print(warnings)"},{"question":"Objective Implement a simplified encoding and decoding system inspired by the `binhex` module to assess your understanding of file I/O and exception handling in Python. Problem Statement You are required to create two functions: `simple_binx(input, output)` and `simple_hexbin(input, output)`. The `simple_binx` function should encode the contents of an input binary file into a text representation and write it to an output file. The `simple_hexbin` function should decode the text representation back to a binary format and write it to an output file. Function Specifications: 1. **simple_binx(input, output)**: - **Input**: - `input` (str): The filename of the binary input file. - `output` (str or file-like object): The filename for the output encoded text file or a file-like object that supports `write()` and `close()` methods. - **Output**: None - **Constraints**: The function should handle exceptions where the input file is not found or cannot be read. If the `output` is a file-like object, ensure it writes the encoded text properly and closes the file when done. - **Description**: Encode the binary content of the input file into a base64 text representation. 2. **simple_hexbin(input, output)**: - **Input**: - `input` (str): The filename of the encoded text input file or a file-like object that supports `read()` and `close()` methods. - `output` (str): The filename for the output binary file. - **Output**: None - **Constraints**: The function should handle exceptions where the input file is not properly formatted or contains errors. Ensure the output binary file is written correctly and any file-like objects are closed appropriately. - **Description**: Decode the base64 text representation from the input file back into its original binary form. 3. **Error Handling**: Define a custom exception `SimpleBinHexError` for cases where encoding or decoding fails due to file errors or format issues. Example Usage: ```python try: simple_binx(\'example_input.bin\', \'encoded_output.txt\') simple_hexbin(\'encoded_output.txt\', \'decoded_output.bin\') except SimpleBinHexError as e: print(f\\"An error occurred: {e}\\") ``` Performance Requirements: - Both functions should be able to handle files up to 10 MB efficiently. - Proper memory management must be considered to avoid loading large files entirely into memory. Notes: - Utilize the `base64` module in Python for encoding and decoding base64 data. - Ensure your solution works smoothly with both filenames and file-like objects for input and output parameters. Implement these functions and provide a clear and concise implementation.","solution":"import base64 class SimpleBinHexError(Exception): pass def simple_binx(input, output): try: # Read binary content from the input file with open(input, \'rb\') as binary_file: binary_data = binary_file.read() # Encode the binary data to base64 text encoded_data = base64.b64encode(binary_data).decode(\'utf-8\') # Write the encoded data to the output file if isinstance(output, str): with open(output, \'w\') as text_file: text_file.write(encoded_data) else: output.write(encoded_data) output.close() except Exception as e: raise SimpleBinHexError(f\\"Encoding failed: {e}\\") def simple_hexbin(input, output): try: # Read the base64 encoded text from the input file if isinstance(input, str): with open(input, \'r\') as text_file: encoded_data = text_file.read() else: encoded_data = input.read() input.close() # Decode the base64 text to binary data binary_data = base64.b64decode(encoded_data) # Write the binary data to the output file with open(output, \'wb\') as binary_file: binary_file.write(binary_data) except Exception as e: raise SimpleBinHexError(f\\"Decoding failed: {e}\\")"},{"question":"# XML-RPC Server and Client Setup In this exercise, you\'ll create a simple XML-RPC server that performs basic arithmetic operations and a client to interact with the server. Objectives: 1. Create an XML-RPC server that provides three arithmetic operations: addition, subtraction, and multiplication. 2. Register these operations so that they can be accessed via XML-RPC. 3. Implement an XML-RPC client to call these operations and display the results. Requirements: 1. **XML-RPC Server:** - Create a `SimpleXMLRPCServer` listening on `localhost` at port `9000`. - Implement three functions: `add(x, y)`, `subtract(x, y)`, and `multiply(x, y)`. - Register these functions on the server with the names `\\"add\\"`, `\\"subtract\\"`, and `\\"multiply\\"`. - Ensure that your server can handle multiple XML-RPC requests concurrently. 2. **XML-RPC Client:** - Create a client that connects to the server at `localhost` on port `9000`. - Call the `add`, `subtract`, and `multiply` methods on the server with any two numbers of your choice. - Print the results of each operation. Constraints: - The server should continuously run and handle requests until it is manually stopped. - The client should correctly handle exceptions in case the server is unreachable or the methods are called incorrectly. Example Interaction: Assuming the server is running on `localhost:9000`, - The client calls `add(5, 3)` and prints the result: `8`. - The client calls `subtract(5, 3)` and prints the result: `2`. - The client calls `multiply(5, 3)` and prints the result: `15`. Code Template: You can refer to the following code templates as a starting point. **Server Code:** ```python from xmlrpc.server import SimpleXMLRPCServer def add(x, y): return x + y def subtract(x, y): return x - y def multiply(x, y): return x * y def main(): server = SimpleXMLRPCServer((\'localhost\', 9000)) server.register_function(add, \'add\') server.register_function(subtract, \'subtract\') server.register_function(multiply, \'multiply\') print(\\"Server running on port 9000...\\") server.serve_forever() if __name__ == \\"__main__\\": main() ``` **Client Code:** ```python import xmlrpc.client def main(): proxy = xmlrpc.client.ServerProxy(\'http://localhost:9000\') print(\\"5 + 3 =\\", proxy.add(5, 3)) print(\\"5 - 3 =\\", proxy.subtract(5, 3)) print(\\"5 * 3 =\\", proxy.multiply(5, 3)) if __name__ == \\"__main__\\": main() ``` Submission: - Submit the server code in a file called `xmlrpc_server.py`. - Submit the client code in a file called `xmlrpc_client.py`. Ensure that your implementation adheres to the constraints and requirements given and properly handles any potential errors.","solution":"from xmlrpc.server import SimpleXMLRPCServer import threading def add(x, y): return x + y def subtract(x, y): return x - y def multiply(x, y): return x * y def run_server(): server = SimpleXMLRPCServer((\'localhost\', 9000)) server.register_function(add, \'add\') server.register_function(subtract, \'subtract\') server.register_function(multiply, \'multiply\') print(\\"Server running on port 9000...\\") server.serve_forever() # Run the server in a separate thread to allow for cleanup and testing server_thread = threading.Thread(target=run_server) server_thread.daemon = True server_thread.start() import xmlrpc.client def client_interaction(): proxy = xmlrpc.client.ServerProxy(\'http://localhost:9000\') try: add_result = proxy.add(5, 3) subtract_result = proxy.subtract(5, 3) multiply_result = proxy.multiply(5, 3) return add_result, subtract_result, multiply_result except Exception as e: return str(e)"},{"question":"**Objective:** Demonstrate your understanding of seaborn by creating and customizing a multi-plot visualization. **Background:** You have been given the \\"anscombe\\" dataset from seaborn, which contains four different sets of data (I, II, III, and IV). Each dataset contains `x` and `y` values. Your task is to create a visualization that includes: 1. Faceted plots for each dataset (I, II, III, IV). 2. A linear fit line and data points for each facet. 3. Custom theme settings for plot aesthetics. **Required Libraries:** You must use seaborn and matplotlib for this task. **Instructions:** 1. Load the \\"anscombe\\" dataset using seaborn. 2. Create a faceted plot using `seaborn.objects.Plot` with facets for each dataset (I, II, III, IV). 3. Each facet should include: - Data points plotted using `so.Dot()`. - Linear fit lines using `so.Line()` and `so.PolyFit(order=1)`. 4. Customize the plots theme to: - Set the axes background color to white. - Set the axes edge color to slategray. - Increase the line width to 4. - Apply the \\"ticks\\" style from seaborn. - Optionally, you may use additional aesthetic customizations. 5. Display the plots. **Expected Output:** - A multi-faceted plot showing four plots (one for each dataset I, II, III, IV). - The plots must have customized aesthetics as per the instructions. **Example Code Structure:** ```python import seaborn.objects as so from seaborn import load_dataset from seaborn import axes_style from matplotlib import style # Load the anscombe dataset anscombe = load_dataset(\\"anscombe\\") # Create the faceted plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Customize the plot theme p.theme({ \\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4 }) # Apply seaborn ticks style p.theme(axes_style(\\"ticks\\")) # Optionally add more customizations here # Display the plot p.show() ``` **Submission Requirements:** - Your solution file must contain the code to generate the plots based on the described instructions. - Write clean, readable, and well-commented code. - Ensure that all plots and customizations are correctly applied. - Submit the solution as a `.py` file. **Constraints:** - You are only allowed to use seaborn and matplotlib as plotting libraries. **Performance Requirements:** - The solution must efficiently generate and display the plots without any errors.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_anscombe_data(): # Load the anscombe dataset df = sns.load_dataset(\\"anscombe\\") # Create a FacetGrid for datasets I, II, III, IV g = sns.FacetGrid(df, col=\\"dataset\\", col_wrap=2) # Map the scatterplot and linear fit line to each subplot g.map(sns.scatterplot, \\"x\\", \\"y\\") g.map(sns.regplot, \\"x\\", \\"y\\", ci=None, scatter=False) # Customize the plot\'s theme plt.rc(\\"axes\\", facecolor=\\"w\\", edgecolor=\\"slategray\\", linewidth=4) sns.set_style(\\"ticks\\") # Display the plot plt.show() # Call the function to generate the plot plot_anscombe_data()"},{"question":"# Question: Backup and Clean Up Utility In this coding exercise, you are required to implement a backup and clean-up utility using the `shutil` module. The utility should back up a specified directory and its contents to a target location, excluding certain file types, and then remove any temporary files from the source directory. Function Signature ```python def backup_and_cleanup(src_dir: str, dst_dir: str, exclude_patterns: list) -> None: Backs up the contents of the src_dir to dst_dir, excluding files that match any pattern in exclude_patterns. After the backup, remove all temporary files from the src_dir. Parameters: src_dir (str): The path of the source directory to back up. dst_dir (str): The path of the destination directory where the backup will be stored. exclude_patterns (list): A list of glob-style patterns to exclude from the backup. ``` Requirements 1. **Backup Process**: - Copy the entire contents of `src_dir` to `dst_dir`. - Ensure that any files matching the `exclude_patterns` are not copied to the target directory. - Preserve the directory structure and file metadata during the copy. 2. **Clean Up Process**: - After completing the backup, remove all files in the `src_dir` that match the pattern `*.tmp`. However, do not remove directories, even if they contain `.tmp` files. Example Usage ```python >>> backup_and_cleanup(\'/path/to/source\', \'/path/to/backup\', [\'*.pyc\', \'tmp*\']) ``` Given this example: - All files and directories in `/path/to/source` should be copied to `/path/to/backup`. - Files like `file.pyc` and any files or directories starting with `tmp` should be excluded from the backup. - After the backup, any files in `/path/to/source` that end with `.tmp` should be removed, but directories should remain intact. Constraints - You may assume that the source and destination directories are different and both directories exist. - Handle any potential errors gracefully without stopping the execution. For instance, if a particular file cannot be deleted, log the occurrence and continue with the next. Notes - Make sure to use the `shutil` module for copying and deleting files. - Use the `ignore_patterns` function provided by `shutil` to handle file exclusions during the copy process. - Use appropriate functions to handle errors during file deletion to avoid stopping the process abruptly.","solution":"import shutil import os import fnmatch def backup_and_cleanup(src_dir: str, dst_dir: str, exclude_patterns: list) -> None: Backs up the contents of the src_dir to dst_dir, excluding files that match any pattern in exclude_patterns. After the backup, remove all temporary files from the src_dir. Parameters: src_dir (str): The path of the source directory to back up. dst_dir (str): The path of the destination directory where the backup will be stored. exclude_patterns (list): A list of glob-style patterns to exclude from the backup. def ignore_patterns(path, names): excluded = [] for pattern in exclude_patterns: excluded.extend(fnmatch.filter(names, pattern)) return set(excluded) try: # Copy the the src_dir to dst_dir shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True, ignore=ignore_patterns) except Exception as e: print(f\\"Error during backup: {e}\\") try: # Clean up temporary files (*.tmp) in the src_dir for root, dirs, files in os.walk(src_dir): for file in files: if file.endswith(\'.tmp\'): try: os.remove(os.path.join(root, file)) except Exception as e: print(f\\"Error deleting file {os.path.join(root, file)}: {e}\\") except Exception as e: print(f\\"Error during cleanup: {e}\\")"},{"question":"Objective To evaluate the understanding of Seaborn\'s swarm plot functionalities, multi-dimensional data visualization, and customization techniques. Problem Statement You are given a dataset `tips` which provides information about the tips received by waiters in a restaurant. Your task is to create a detailed and comprehensive visualization using Seaborn\'s `swarmplot` function. Instructions 1. Load the `tips` dataset using Seaborn. 2. Create a plot with the following requirements: - Use `total_bill` on the x-axis and `day` on the y-axis. - Add a `hue` dimension based on the `sex` variable. - Split the `hue` levels into separate swarms using the `dodge` parameter. - Customize the swarm plot so that the points are displayed as \'x\' markers with a linewidth of 1. - Use a `deep` color palette for the plot. - Add a title to the plot that says \\"Tips Data Visualization\\". Constraints - Ensure that the data points are distinct and non-overlapping where possible. - The plot title must be easy to read and centered. - The plot should be aesthetically pleasing with clear distinction between different categories. Expected Output Your function should visualize the required plot. Here is the function signature: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the swarm plot with specified customization sns.swarmplot( data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, marker=\\"x\\", linewidth=1, palette=\\"deep\\" ) # Add a title to the plot plt.title(\\"Tips Data Visualization\\", loc=\'center\') # Display the plot plt.show() ``` Implement this function and ensure the plot meets all the specified requirements. Notes: - Use the Seaborn and Matplotlib libraries for your visualizations. - If you need to customize any other aspects of the plot, refer to Seaborn\'s and Matplotlib\'s documentation for additional parameters that can be used.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): Visualizes the tips dataset using a swarm plot with specific customizations. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the swarm plot with specified customization sns.swarmplot( data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, marker=\\"x\\", linewidth=1, palette=\\"deep\\" ) # Add a title to the plot plt.title(\\"Tips Data Visualization\\", loc=\'center\') # Display the plot plt.show()"},{"question":"**Question: Implement a Custom Web Browser Controller** The webbrowser module allows for registering custom browser types. Your task is to implement a custom browser controller that logs URLs before opening them using the default browser. Your implementation should include: 1. A class `LoggingBrowserController` that has the following methods: - `open(self, url, new=0, autoraise=True)`: Logs the URL and opens it in the default browser window. - `open_new(self, url)`: Logs the URL and opens it in a new browser window. - `open_new_tab(self, url)`: Logs the URL and opens it in a new browser tab. 2. A function `register_logging_browser(name)` that registers the `LoggingBrowserController` under the given name. # Input: - The `open`, `open_new`, and `open_new_tab` methods should accept a `url` string. - The `register_logging_browser` function should accept a `name` string. # Output: - The `LoggingBrowserController` methods should log the URL to a file named `browser_log.txt` before opening the URL using the default browser. - The `register_logging_browser` function should not return any value but should register the custom browser controller. # Constraints: - Use the `webbrowser` module to open the URL. - Ensure that the logged URLs are appended to the log file without overwriting existing entries. # Example Usage: ```python # Register the logging browser under the name \\"log_browser\\" register_logging_browser(\\"log_browser\\") # Get a controller for the registered logging browser controller = webbrowser.get(\\"log_browser\\") # Open a URL using the logging browser controller controller.open(\\"https://www.python.org/\\") ``` This should result in \\"https://www.python.org/\\" being logged to the `browser_log.txt` file and then opened in the default web browser. ```plaintext Content of browser_log.txt after execution: https://www.python.org/ ``` # Hint: - You may find it helpful to use Python\'s built-in `open` function to handle file operations for logging. Implement the `LoggingBrowserController` class and `register_logging_browser` function.","solution":"import webbrowser class LoggingBrowserController: def open(self, url, new=0, autoraise=True): self._log_url(url) webbrowser.open(url, new=new, autoraise=autoraise) def open_new(self, url): self._log_url(url) webbrowser.open_new(url) def open_new_tab(self, url): self._log_url(url) webbrowser.open_new_tab(url) def _log_url(self, url): with open(\'browser_log.txt\', \'a\') as log_file: log_file.write(url + \'n\') def register_logging_browser(name): controller = LoggingBrowserController() webbrowser.register(name, None, controller)"},{"question":"# Advanced Coding Assessment Question **Objective:** Create a Python function to manage a list of objects and ensure efficient memory usage and proper handling of exceptions. **Problem Statement:** You are tasked with implementing a class `MemoryManager` that simulates memory management for a list of objects. The class should handle adding, removing, and clearing objects in the list with proper exception handling. Given the performance constraints, try to ensure that your implementation is efficient in terms of time complexity. # Requirements: 1. **Class Definition:** - Define a class `MemoryManager`. 2. **Initialization:** - The constructor should initialize an empty list to store objects. 3. **Methods:** - `add_object(obj)`: Adds an object to the list. - Raises a `MemoryError` if the list size exceeds 1000 objects. - Raises a `TypeError` if `obj` is not an instance of a predefined allowed class, say `AllowedType`. - `remove_object(obj)`: Removes an object from the list. - Raises a `ValueError` if the object is not found in the list. - `clear_all_objects()`: Clears all objects from the list. - Properly handles any potential exceptions silently and ensures the list is empty. - `current_memory_usage()`: Returns the current number of objects in the list. # Constraints: - You should not use external libraries. The implementation should only use core Python. - The allowed class for objects in the list is predefined as `AllowedType`. # Performance Requirements: - The operations should be optimized for performance, especially the `add_object` and `remove_object` methods. # Example: ```python class AllowedType: pass class MemoryManager: def __init__(self): # Initialize the internal storage list pass def add_object(self, obj): # Add object to the list with necessary exception handling pass def remove_object(self, obj): # Remove object from the list with necessary exception handling pass def clear_all_objects(self): # Clear all objects from the list pass def current_memory_usage(self): # Return the count of objects pass try: manager = MemoryManager() obj1 = AllowedType() obj2 = AllowedType() manager.add_object(obj1) manager.add_object(obj2) print(manager.current_memory_usage()) # Output: 2 manager.remove_object(obj1) print(manager.current_memory_usage()) # Output: 1 manager.clear_all_objects() print(manager.current_memory_usage()) # Output: 0 except Exception as e: print(f\\"Error: {e}\\") ``` This question will test the student\'s ability to implement a class-based solution, manage internal state efficiently, and handle various exceptions correctly.","solution":"class AllowedType: pass class MemoryManager: def __init__(self): self.objects = [] def add_object(self, obj): if not isinstance(obj, AllowedType): raise TypeError(\\"Object must be an instance of AllowedType.\\") if len(self.objects) >= 1000: raise MemoryError(\\"Memory limit exceeded.\\") self.objects.append(obj) def remove_object(self, obj): try: self.objects.remove(obj) except ValueError: raise ValueError(\\"Object not found in the list.\\") def clear_all_objects(self): try: self.objects.clear() except Exception: pass # Silently handle unexpected exceptions def current_memory_usage(self): return len(self.objects)"},{"question":"# Question: Data Manipulation with pandas Date Offsets You are given a DataFrame that contains daily sales data for a company over a period of one year. The DataFrame has two columns: `date` (containing dates) and `sales` (containing the sales amount for each date). Write a function `analyze_sales(df)` that performs the following tasks: 1. **Convert the `date` column to a datetime object**. 2. **Create new columns** in the DataFrame to indicate: - Whether the date is the **start or end of a month**. - Whether the date is the **start or end of a quarter**. - Whether the date is the **start or end of a year**. 3. **Filter out weekends and holidays** (defined as \'2023-12-25\' and \'2023-01-01\') from the DataFrame. 4. **Calculate the cumulative sales for each business month** (excluding weekends and holidays). 5. **Return the modified DataFrame** with the new columns and a new cumulative sales column. Input: - `df`: A pandas DataFrame with the following structure: ```python df = pd.DataFrame({ \'date\': pd.date_range(start=\'2023-01-01\', end=\'2023-12-31\', freq=\'D\'), \'sales\': np.random.randint(100, 500, size=365) }) ``` Output: - A pandas DataFrame with the following additional columns: - `is_month_start` - `is_month_end` - `is_quarter_start` - `is_quarter_end` - `is_year_start` - `is_year_end` - `cumulative_sales` Constraints: - Ensure that all date manipulations are done using the `pandas.tseries.offsets` module and its classes. Performance Requirements: - The function should be efficient and handle the DataFrame operations in a time complexity suitable for large datasets, ideally O(n). Example: ```python import pandas as pd import numpy as np def analyze_sales(df): # Your implementation here df = pd.DataFrame({ \'date\': pd.date_range(start=\'2023-01-01\', end=\'2023-12-31\', freq=\'D\'), \'sales\': np.random.randint(100, 500, size=365) }) result = analyze_sales(df) print(result) ```","solution":"import pandas as pd import numpy as np def analyze_sales(df): # Convert the `date` column to a datetime object df[\'date\'] = pd.to_datetime(df[\'date\']) # Create new columns to indicate month, quarter, and year starts/ends df[\'is_month_start\'] = df[\'date\'].dt.is_month_start df[\'is_month_end\'] = df[\'date\'].dt.is_month_end df[\'is_quarter_start\'] = df[\'date\'].dt.is_quarter_start df[\'is_quarter_end\'] = df[\'date\'].dt.is_quarter_end df[\'is_year_start\'] = df[\'date\'].dt.is_year_start df[\'is_year_end\'] = df[\'date\'].dt.is_year_end # Define holidays holidays = [\'2023-12-25\', \'2023-01-01\'] # Filter out weekends and holidays df = df[(~df[\'date\'].dt.weekday.isin([5, 6])) & (~df[\'date\'].isin(holidays))] # Calculate cumulative sales for each business month df[\'cumulative_sales\'] = df.groupby(df[\'date\'].dt.to_period(\'M\'))[\'sales\'].cumsum().reset_index(level=0, drop=True) return df"},{"question":"**Advanced Coding Assessment Question** # Objective: Implement a function that mimics a simplified version of Pythons `bytes` class using the provided functionalities from the \\"python310\\" package. The implementation will include: 1. **Creating bytes objects** from strings. 2. **Concatenating two bytes objects**. 3. **Resizing a bytes object**. # Function Requirements: Implement the following functions in Python, making use of the provided C-API functions: 1. **create_bytes_from_string**: - **Input**: A string `s`. - **Output**: A bytes object created from `s`. - **Hint**: Use `PyBytes_FromString`. 2. **concatenate_bytes**: - **Input**: Two bytes objects `b1` and `b2`. - **Output**: A new bytes object that is the concatenation of `b1` and `b2`. - **Hint**: Use `PyBytes_Concat`. 3. **resize_bytes**: - **Input**: A bytes object `b` and a new size `new_size`. - **Output**: The resized bytes object. - **Hint**: Use `_PyBytes_Resize`. # Constraints: - Raise a `TypeError` if inputs to `concatenate_bytes` or `resize_bytes` are not bytes objects. - Handle all exceptions using try-except blocks. - Assume Python 3.10 and C-API available for use. # Example Usage: ```python # Example usage of create_bytes_from_string b1 = create_bytes_from_string(\\"hello\\") print(b1) # Expected Output: b\'hello\' # Example usage of concatenate_bytes b2 = create_bytes_from_string(\\"world\\") concatenated = concatenate_bytes(b1, b2) print(concatenated) # Expected Output: b\'helloworld\' # Example usage of resize_bytes resized = resize_bytes(b1, 10) print(resized) # Expected Output: b\'hellox00x00x00x00x00\' ``` # Performance: - The functions should handle typical use cases within O(n) complexity where n is the length of the input bytes. Implement the functions as guided and ensure proper testing.","solution":"def create_bytes_from_string(s): Creates a bytes object from a given string. Args: s (str): The string to convert to bytes object. Returns: bytes: The created bytes object. if not isinstance(s, str): raise TypeError(\\"Input must be a string\\") return s.encode() def concatenate_bytes(b1, b2): Concatenates two bytes objects and returns the result. Args: b1 (bytes): The first bytes object. b2 (bytes): The second bytes object. Returns: bytes: The concatenated bytes object. if not isinstance(b1, bytes) or not isinstance(b2, bytes): raise TypeError(\\"Both inputs must be bytes objects\\") return b1 + b2 def resize_bytes(b, new_size): Resizes a bytes object to a new size. If the new size is larger, the bytes object is padded with null characters; if smaller, it is truncated. Args: b (bytes): The original bytes object. new_size (int): The new size for the bytes object. Returns: bytes: The resized bytes object. if not isinstance(b, bytes): raise TypeError(\\"Input must be a bytes object\\") if not isinstance(new_size, int) or new_size < 0: raise ValueError(\\"New size must be a non-negative integer\\") return b.ljust(new_size, b\'x00\')[:new_size]"},{"question":"**Objective:** Demonstrate your understanding of PyTorch serialization and deserialization methods by implementing a series of functions related to saving and loading tensors and modules. **Problem Statement:** 1. **Implement a function `save_tensor_view_relationships`**: Write a function that takes a `torch.Tensor` and a file name as input, creates a view of this tensor, saves both the original tensor and the view to a file, and returns the file name. ```python import torch def save_tensor_view_relationships(tensor: torch.Tensor, filename: str) -> str: Save a tensor and its view to a file to preserve their relationship. Args: tensor (torch.Tensor): The original tensor. filename (str): The name of the file to save the tensors. Returns: str: The filename where the tensors are saved. # Your implementation here return filename ``` 2. **Implement a function `load_and_modify_tensor_views`**: Write a function that takes a file name as input, loads the tensors from the file, modifies one of the views, and returns the modified original tensor and view. ```python import torch def load_and_modify_tensor_views(filename: str) -> (torch.Tensor, torch.Tensor): Load a tensor and its view from a file, modify the view, and return both tensors. Args: filename (str): The name of the file to load the tensors from. Returns: tuple: A tuple containing the modified original tensor and the modified view. # Your implementation here return modified_original_tensor, modified_view ``` 3. **Implement a function `save_module_state`**: Write a function that takes an `nn.Module` instance and a file name as input, and saves the state dictionary of the module to the file. ```python import torch import torch.nn as nn def save_module_state(module: nn.Module, filename: str) -> str: Save the state dictionary of a module to a file. Args: module (nn.Module): The module instance. filename (str): The name of the file to save the state dictionary. Returns: str: The filename where the state dictionary is saved. # Your implementation here return filename ``` 4. **Implement a function `load_module_state`**: Write a function that takes an `nn.Module` instance and a file name as input, loads the state dictionary from the file, and updates the module\'s state with this dictionary. ```python import torch import torch.nn as nn def load_module_state(module: nn.Module, filename: str) -> nn.Module: Load the state dictionary from a file and update the module\'s state. Args: module (nn.Module): The module instance. filename (str): The name of the file to load the state dictionary from. Returns: nn.Module: The module with the updated state. # Your implementation here return module ``` **Constraints:** - The tensor views and module states should be preserved and verified correctly. - Assume all inputs are valid. - You can use any suitable methods and utilities provided by PyTorch. **Deliverables:** 1. Implement and test each function to ensure they work as expected. 2. Provide brief documentation/comments explaining how each function works. **Example Usage:** ```python import torch import torch.nn as nn # Example for save_tensor_view_relationships and load_and_modify_tensor_views t = torch.arange(10) filename = save_tensor_view_relationships(t, \'tensors.pt\') modified_tensor, modified_view = load_and_modify_tensor_views(filename) print(modified_tensor) print(modified_view) # Example for save_module_state and load_module_state class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) model = SimpleModel() filename = save_module_state(model, \'model_state.pt\') loaded_model = load_module_state(SimpleModel(), \'model_state.pt\') print(loaded_model.state_dict()) ```","solution":"import torch import torch.nn as nn import os def save_tensor_view_relationships(tensor: torch.Tensor, filename: str) -> str: Save a tensor and its view to a file to preserve their relationship. Args: tensor (torch.Tensor): The original tensor. filename (str): The name of the file to save the tensors. Returns: str: The filename where the tensors are saved. tensor_view = tensor.view(-1) torch.save({\'tensor\': tensor, \'tensor_view\': tensor_view}, filename) return filename def load_and_modify_tensor_views(filename: str) -> (torch.Tensor, torch.Tensor): Load a tensor and its view from a file, modify the view, and return both tensors. Args: filename (str): The name of the file to load the tensors from. Returns: tuple: A tuple containing the modified original tensor and the modified view. data = torch.load(filename) tensor = data[\'tensor\'] tensor_view = data[\'tensor_view\'] tensor_view[0] = 999 return tensor, tensor_view def save_module_state(module: nn.Module, filename: str) -> str: Save the state dictionary of a module to a file. Args: module (nn.Module): The module instance. filename (str): The name of the file to save the state dictionary. Returns: str: The filename where the state dictionary is saved. torch.save(module.state_dict(), filename) return filename def load_module_state(module: nn.Module, filename: str) -> nn.Module: Load the state dictionary from a file and update the module\'s state. Args: module (nn.Module): The module instance. filename (str): The name of the file to load the state dictionary from. Returns: nn.Module: The module with the updated state. module.load_state_dict(torch.load(filename)) return module"},{"question":"Objective Utilize the `http.client` module in Python to make HTTP requests and process HTTP responses. Your task is to write a function that can send a GET request to a specified URL and process the returned HTTP response, demonstrating your understanding of fundamental and advanced concepts. Problem Statement Write a Python function `fetch_http_resource(url: str) -> dict` that sends an HTTP GET request to the provided URL and returns a dictionary containing the status code, reason phrase, headers, and the response body. Function Signature ```python def fetch_http_resource(url: str) -> dict: pass ``` Input Format - `url` (str): A string representing the URL to which the GET request should be sent. Output Format - Returns a dictionary containing the following keys: - `\'status\'` (int): The HTTP status code of the response. - `\'reason\'` (str): The reason phrase of the HTTP response. - `\'headers\'` (dict): A dictionary containing the headers returned by the server. Each key-value pair corresponds to header names and their values. - `\'body\'` (str): The body of the HTTP response as a string. Constraints - The URL will be a valid URL. - The function should use the `http.client` module only. - Handle possible exceptions such as connection errors and invalid responses gracefully and return an appropriate dictionary indicating the error. Example Usage ```python result = fetch_http_resource(\\"http://example.com\\") print(result) # Expected output might be something like: # { # \'status\': 200, # \'reason\': \'OK\', # \'headers\': { # \'Content-Type\': \'text/html\', # ... # }, # \'body\': \'<!doctype html>...\' # } ``` # Notes - Make sure to handle redirects (3xx status codes) by following the `Location` header if provided. - Ensure proper handling of different content types in the response body (e.g., text, JSON).","solution":"import http.client from urllib.parse import urlparse def fetch_http_resource(url: str) -> dict: parsed_url = urlparse(url) conn = http.client.HTTPConnection(parsed_url.netloc) try: conn.request(\\"GET\\", parsed_url.path + (f\\"?{parsed_url.query}\\" if parsed_url.query else \\"\\")) response = conn.getresponse() # Check for redirection and follow Location header if 300 <= response.status < 400 and \'Location\' in response.headers: return fetch_http_resource(response.headers[\'Location\']) headers = {k: v for k, v in response.getheaders()} body = response.read().decode(\'utf-8\', \'ignore\') return { \'status\': response.status, \'reason\': response.reason, \'headers\': headers, \'body\': body } except Exception as e: return { \'status\': None, \'reason\': \'Error\', \'headers\': {}, \'body\': str(e) } finally: conn.close()"},{"question":"Coding Assessment Question # Objective The goal of this exercise is to test your understanding of data generation, preprocessing, and regression modeling using sklearn. # Function Specification: Implement a function `regression_model_evaluation` that performs the following tasks: 1. Generates a synthetic dataset suitable for a regression problem. 2. Splits the dataset into training and testing sets. 3. Applies preprocessing to the features. 4. Trains a Gradient Boosting Regressor model using the training data. 5. Evaluates the model using the testing data. 6. Demonstrates and handles a specific user warning related to feature names. # Function Signature ```python def regression_model_evaluation(n_samples: int, random_state: int) -> float: Generates a synthetic regression dataset, trains and evaluates a Gradient Boosting Regressor model. Parameters: - n_samples (int): The number of samples to generate in the synthetic dataset. - random_state (int): The seed for the random number generator. Returns: - test_score (float): The R^2 score of the model on the testing data. ``` # Input - `n_samples` (int): Number of samples in the synthetic dataset (e.g., 1000). - `random_state` (int): Random state seed for reproducibility (e.g., 42). # Output - `test_score` (float): The R^2 score of the trained model on the test dataset. # Steps to Implement 1. **Data Generation:** - Use `sklearn.datasets.make_regression` to generate a synthetic dataset with the specified number of samples (`n_samples`). 2. **Data Splitting:** - Split the dataset into training and testing sets using `sklearn.model_selection.train_test_split`. Keep 80% for training and 20% for testing. Make sure to set `random_state` for reproducibility. 3. **Preprocessing:** - Apply `sklearn.preprocessing.StandardScaler` to scale the features (both training and testing data). 4. **Model Training:** - Instantiate and train a `sklearn.ensemble.GradientBoostingRegressor` model on the training data. 5. **Model Evaluation:** - Score the model using `R^2 score` on the testing data. 6. **Warning Handling:** - During the training, ensure that the provided features have names to avoid the UserWarning: \\"X has feature names, but ... was fitted without feature names\\". # Constraints 1. Priortize the use of sklearn functions and classes. 2. Avoid using external datasets. Generate synthetic data within the function. 3. Ensure the code is clean, readable, and follows best practices for reproducibility. # Example ```python if __name__ == \\"__main__\\": score = regression_model_evaluation(1000, 42) print(f\\"R^2 score of the model: {score:.4f}\\") ``` **Expected Output**: The R^2 score value will vary depending on the synthetic data generated, but it should be a float representing the model\'s performance on the test dataset.","solution":"from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor def regression_model_evaluation(n_samples: int, random_state: int) -> float: Generates a synthetic regression dataset, trains and evaluates a Gradient Boosting Regressor model. Parameters: - n_samples (int): The number of samples to generate in the synthetic dataset. - random_state (int): The seed for the random number generator. Returns: - test_score (float): The R^2 score of the model on the testing data. # Generate synthetic dataset X, y = make_regression(n_samples=n_samples, n_features=10, noise=0.1, random_state=random_state) # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state) # Preprocess the data (StandardScaler) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train Gradient Boosting Regressor model = GradientBoostingRegressor(random_state=random_state) model.fit(X_train_scaled, y_train) # Evaluate the model test_score = model.score(X_test_scaled, y_test) # Return the test score return test_score"},{"question":"Using the Seaborn library, write a function `custom_plotting()` that performs the following steps: 1. Sets the context for the plots to \'notebook\'. 2. Independently scales the font elements relative to the context with a `font_scale` parameter provided by the user. 3. Overrides specific parameters to set the linewidth of lines in plots. 4. Generates a line plot for the given data `x` and `y`. Your function should accept the following parameters: - `x` (list of int/float): List of values for the x-axis. - `y` (list of int/float): List of values for the y-axis. - `font_scale` (float): Scale factor for the font size. - `linewidth` (int/float): Width of the lines in the plot. The function should use these parameters to create a customized plot as specified. Function Signature ```python def custom_plotting(x: list, y: list, font_scale: float, linewidth: float) -> None: pass ``` Constraints - The lengths of `x` and `y` lists are guaranteed to be equal. - `font_scale` will be a positive float value. - `linewidth` will be a positive float value. Example Usage ```python x = [0, 1, 2, 3] y = [2, 3, 5, 7] font_scale = 1.5 linewidth = 2.5 custom_plotting(x, y, font_scale, linewidth) ``` Requirements: - Use `sns.set_context` to set the context and adjust the font scaling. - Customize the line width using the `rc` parameter of `sns.set_context`. - Generate and display the line plot using `sns.lineplot`. Note The function does not need to return anything; it just needs to display the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plotting(x: list, y: list, font_scale: float, linewidth: float) -> None: Generates a line plot with the given parameters, customizing the context, font scale, and linewidth. Parameters: - x (list of int/float): List of values for the x-axis. - y (list of int/float): List of values for the y-axis. - font_scale (float): Scale factor for the font size. - linewidth (float): Width of the lines in the plot. # Set the context with the desired font scale and linewidth sns.set_context(\\"notebook\\", font_scale=font_scale, rc={\\"lines.linewidth\\": linewidth}) # Generate the line plot with the given data sns.lineplot(x=x, y=y) # Display the plot plt.show()"},{"question":"You are tasked with building a server application in Python that can handle multiple client connections simultaneously. To accomplish this, you will implement a function using the \\"selectors\\" module to monitor both read and write events for multiple file objects (client connections). # Requirements 1. **Function Name:** `multi_client_server` 2. **Input:** - `address` (tuple): A tuple containing the server\'s IP address and port number, e.g., `(\'localhost\', 12345)`. 3. **Output:** None. 4. The function should: - Create a non-blocking server socket and bind it to the given address. - Accept incoming client connections. - Register client sockets with the selector to monitor for read events. - Read data from the clients. If data is received, echo it back to the client. - If no data is received (the client has disconnected), unregister and close the client socket. - Use the most efficient selector implementation available on the platform (`DefaultSelector`). # Constraints - The server should be able to handle up to 100 clients simultaneously. - The server should not block on any socket operations. # Example Usage ```python def multi_client_server(address): import selectors import socket sel = selectors.DefaultSelector() def accept(sock, mask): conn, addr = sock.accept() # Should be ready print(\'accepted\', conn, \'from\', addr) conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn, mask): try: data = conn.recv(1000) # Should be ready if data: print(\'echoing\', repr(data), \'to\', conn) conn.send(data) # Hope it won\'t block else: print(\'closing\', conn) sel.unregister(conn) conn.close() except Exception as e: print(f\\"Error: {e}\\") sel.unregister(conn) conn.close() sock = socket.socket() sock.bind(address) sock.listen(100) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) try: while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Server is shutting down\\") finally: sel.close() sock.close() # Run the server # multi_client_server((\'localhost\', 12345)) ``` 1. Create a Python file and implement the function `multi_client_server` as described. 2. Test your server by running it and connecting multiple clients (e.g., using `telnet` or a custom client script) to check if it can handle multiple connections, echo received data, and close connections properly when clients disconnect.","solution":"def multi_client_server(address): import selectors import socket sel = selectors.DefaultSelector() def accept(sock, mask): conn, addr = sock.accept() # Should be ready print(\'accepted\', conn, \'from\', addr) conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn, mask): try: data = conn.recv(1000) # Should be ready if data: print(\'echoing\', repr(data), \'to\', conn) conn.send(data) # Hope it won\'t block else: print(\'closing\', conn) sel.unregister(conn) conn.close() except Exception as e: print(f\\"Error: {e}\\") sel.unregister(conn) conn.close() sock = socket.socket() sock.bind(address) sock.listen(100) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) try: while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Server is shutting down\\") finally: sel.close() sock.close()"},{"question":"You are tasked with creating a Python script that processes command-line arguments using the `getopt` module. The script will simulate a simple file processing tool that supports various command-line options. # Requirements Implement a function `parse_arguments(args: List[str]) -> Tuple[Dict[str, str], List[str]]` that parses given command-line arguments and returns a dictionary of options and their values, and a list of remaining non-option arguments. Specifications: - **Arguments**: - `args`: A list of strings representing command-line arguments (e.g., `[\'-i\', \'input.txt\', \'--verbose\', \'file1.txt\', \'file2.txt\']`). - **Returns**: - A tuple containing: - A dictionary where keys are the option names (including their hyphens) and values are the corresponding option values. - A list of remaining non-option arguments. - **Options**: - Short options: - `-i <file>`: Input file. - `-o <file>`: Output file. - `-v`: Enable verbose mode. - Long options: - `--input=<file>`: Input file. - `--output=<file>`: Output file. - `--verbose`: Enable verbose mode. - The function should handle errors gracefully by raising an appropriate exception if an unrecognized option is found or if an option that requires an argument is not followed by one. # Examples ```python # Example 1: args = [\'--input=input.txt\', \'-v\', \'file1.txt\', \'file2.txt\'] result = parse_arguments(args) # Output: ({\'--input\': \'input.txt\', \'-v\': \'\'}, [\'file1.txt\', \'file2.txt\']) # Example 2: args = [\'-i\', \'input.txt\', \'-o\', \'output.txt\', \'--verbose\'] result = parse_arguments(args) # Output: ({\'-i\': \'input.txt\', \'-o\': \'output.txt\', \'--verbose\': \'\'}, []) # Example 3 (Error): args = [\'--unknown\'] # Should raise: GetoptError: option --unknown not recognized ``` # Constraints - You can assume the input arguments will be in valid format but not necessarily valid options as per the specifications. - Your implementation should use the `getopt` module. - You are **not** required to handle the actual reading or writing of files. # Additional Information Refer to the `getopt` module documentation for more details on how to use the `getopt` functions. # Notes To run your script via command line in a real scenario, you would use `sys.argv[1:]` as input to the function, but here we are passing a list of strings directly for simplicity.","solution":"import getopt def parse_arguments(args): Parses command-line arguments and returns a dictionary of options and a list of non-option arguments. Parameters: args (List[str]): Command-line arguments. Returns: Tuple[Dict[str, str], List[str]]: A tuple containing a dictionary of options and a list of non-option arguments. # Define short and long options short_opts = \\"i:o:v\\" long_opts = [\\"input=\\", \\"output=\\", \\"verbose\\"] try: # Parse options and arguments opts, non_opts = getopt.getopt(args, short_opts, long_opts) except getopt.GetoptError as err: raise ValueError(str(err)) # Initialize dictionary to store options and their values options = {} # Process each option and value for opt, val in opts: if opt in (\'-i\', \'--input\'): options[opt] = val elif opt in (\'-o\', \'--output\'): options[opt] = val elif opt in (\'-v\', \'--verbose\'): options[opt] = \'\' return options, non_opts"},{"question":"# Custom Event Loop Policy Implementation You are tasked with implementing a custom event loop policy in asyncio. The custom event loop policy should override the default behavior of getting and setting event loops. # Requirements 1. **Custom Event Loop Policy** - Create a custom event loop policy by subclassing `asyncio.DefaultEventLoopPolicy`. 2. **Custom Methods** - Override `get_event_loop` and `set_event_loop` methods: - `get_event_loop`: Should return a custom message if no event loop is currently set. - `set_event_loop`: Should set the event loop to the provided loop and log the setting action. 3. **Event Loop Management Function** - Implement a function `manage_event_loops()` which: - Sets a custom event loop policy. - Creates a new event loop. - Sets the created event loop as the current event loop. - Retrieves and prints the current event loop using the custom `get_event_loop` method. # Constraints 1. The custom policy should appropriately handle cases where no event loop is currently set. 2. Ensure thread-safety and error handling within the methods. # Input and Output Formats - **Input**: This task does not require user inputs but involves function definitions and demonstrating their usage. - **Output**: Print statements for the custom messages and logging actions as specified. # Example Usage ```python # Assuming the code will be implemented as per the requirements. # Output should include: # Custom event loop policy set. # Event loop set to: <event loop object> # Retrieved event loop: <event loop object or custom message> ``` # Implementation Define the class `MyEventLoopPolicy` subclassing `asyncio.DefaultEventLoopPolicy` and the function `manage_event_loops` as specified above. ```python import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() if loop is None: return \\"No event loop is currently set.\\" return loop def set_event_loop(self, loop): super().set_event_loop(loop) print(f\\"Event loop set to: {loop}\\") def manage_event_loops(): policy = MyEventLoopPolicy() asyncio.set_event_loop_policy(policy) print(\\"Custom event loop policy set.\\") loop = asyncio.new_event_loop() policy.set_event_loop(loop) current_loop = policy.get_event_loop() print(f\\"Retrieved event loop: {current_loop}\\") # Run the function to manage event loops manage_event_loops() ``` Implement this custom event loop policy and use the provided `manage_event_loops` function to demonstrate its usage.","solution":"import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): A custom event loop policy that overrides the default event loop management behavior. def __init__(self): super().__init__() self._loop = None def get_event_loop(self): Returns the current event loop or a custom message if no event loop is set. if self._loop is None: return \\"No event loop is currently set.\\" return self._loop def set_event_loop(self, loop): Sets the event loop to the provided loop and logs the action. super().set_event_loop(loop) self._loop = loop print(f\\"Event loop set to: {loop}\\") def manage_event_loops(): Manages the event loops by setting a custom event loop policy, creating a new event loop, setting it, and retrieving it. policy = MyEventLoopPolicy() asyncio.set_event_loop_policy(policy) print(\\"Custom event loop policy set.\\") loop = asyncio.new_event_loop() policy.set_event_loop(loop) current_loop = policy.get_event_loop() print(f\\"Retrieved event loop: {current_loop}\\") # Execute the manage_event_loops function to demonstrate the usage. if __name__ == \\"__main__\\": manage_event_loops()"},{"question":"Objective Evaluate the student\'s ability to use the scikit-learn library for a machine learning task, involving data preprocessing, model training, and evaluation. Problem Statement You are given a dataset consisting of customer information for a financial institution. The dataset contains various features (both numerical and categorical) and a target variable indicating whether a customer has defaulted on a loan. Your task is to build a machine learning pipeline to predict loan defaults and evaluate its performance. Dataset The dataset can be represented as follows: ```python data = { \'age\': [25, 45, 35, 50, 23], \'income\': [50000, 100000, 75000, 120000, 48000], \'education\': [\'bachelor\', \'master\', \'bachelor\', \'phd\', \'highschool\'], \'loan_amount\': [20000, 15000, 30000, 25000, 24000], \'defaulted\': [0, 0, 1, 0, 1] } ``` Note: This is a simplified representation. Assume you have access to the full dataset for training and testing. Implementation Requirements 1. **Data Preprocessing**: - Handle missing values, if any. - Encode categorical variables if needed. - Standardize/normalize numeric features, if required. 2. **Model Training**: - Split the data into training and testing datasets. - Train a Random Forest classifier on the training set. 3. **Evaluation**: - Evaluate the model on the test set using accuracy, precision, recall, and F1-score. 4. **Pipeline Construction**: - Construct a scikit-learn pipeline to streamline the preprocessing and training steps. 5. **Hyperparameter Tuning** (Optional): - Perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV for the Random Forest model. Function Signature ```python from typing import Dict, Any def build_and_evaluate_pipeline(data: Dict[str, Any]) -> Dict[str, float]: Builds a machine learning pipeline to predict loan defaults and evaluates its performance. Args: data (Dict[str, Any]): The dataset including features and target variable. Returns: Dict[str, float]: A dictionary containing the evaluation metrics (accuracy, precision, recall, f1-score). pass ``` Constraints - You are required to use scikit-learn\'s `Pipeline` and `RandomForestClassifier`. - Ensure reproducibility of results by setting random state values appropriately. Example Usage ```python data = { \'age\': [25, 45, 35, 50, 23], \'income\': [50000, 100000, 75000, 120000, 48000], \'education\': [\'bachelor\', \'master\', \'bachelor\', \'phd\', \'highschool\'], \'loan_amount\': [20000, 15000, 30000, 25000, 24000], \'defaulted\': [0, 0, 1, 0, 1] } result = build_and_evaluate_pipeline(data) print(result) # Expected output: {\'accuracy\': ..., \'precision\': ..., \'recall\': ..., \'f1_score\': ...} ``` Additional Notes - Use `train_test_split` for splitting the data. - Use `StandardScaler` for numerical feature scaling. - Utilize `OneHotEncoder` for categorical feature encoding within the pipeline.","solution":"from typing import Dict, Any import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def build_and_evaluate_pipeline(data: Dict[str, Any]) -> Dict[str, float]: # Convert the dictionary to a pandas DataFrame df = pd.DataFrame(data) # Define the feature columns and target column feature_columns = [\'age\', \'income\', \'education\', \'loan_amount\'] target_column = \'defaulted\' X = df[feature_columns] y = df[target_column] # Preprocessing for numerical features num_features = [\'age\', \'income\', \'loan_amount\'] num_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()) ]) # Preprocessing for categorical features cat_features = [\'education\'] cat_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', num_transformer, num_features), (\'cat\', cat_transformer, cat_features) ] ) # Define the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier(random_state=42)) ]) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model pipeline.fit(X_train, y_train) # Make predictions on the test set y_pred = pipeline.predict(X_test) # Calculate evaluation metrics metrics = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, zero_division=1), \'recall\': recall_score(y_test, y_pred), \'f1_score\': f1_score(y_test, y_pred) } return metrics"},{"question":"**Advanced XML Parsing with Expat** **Problem Statement:** You are tasked with writing a Python program to parse an XML string provided by the user. The XML string contains information about books in a library, including titles, authors, and publication years. You must use the `xml.parsers.expat` module to parse the XML data and extract this information. **Requirements:** 1. Define handler functions to capture the start and end of XML elements and character data. 2. Parse the XML string using the `xml.parsers.expat` module. 3. Collect and print information about each book in a structured format. **Input:** - An XML string containing information about books, for example: ```xml <?xml version=\\"1.0\\"?> <library> <book id=\\"1\\"> <title>XML Programming</title> <author>John Doe</author> <year>2020</year> </book> <book id=\\"2\\"> <title>Learning Python</title> <author>Jane Smith</author> <year>2018</year> </book> </library> ``` **Output:** - Nested data structure representing books. ```python [ { \\"id\\": \\"1\\", \\"title\\": \\"XML Programming\\", \\"author\\": \\"John Doe\\", \\"year\\": \\"2020\\", }, { \\"id\\": \\"2\\", \\"title\\": \\"Learning Python\\", \\"author\\": \\"Jane Smith\\", \\"year\\": \\"2018\\", }, ] ``` **Constraints:** - Tags and data will always be well-formed. - Each book will have exactly one title, author, and year. - Attribute `id` will always be present for each book. **Performance Requirements:** - Efficiently parse the XML data using appropriate handler functions. - Handle potential parsing errors gracefully. **Instructions:** 1. Implement the handler functions `start_element`, `end_element`, and `char_data`. 2. Create an XML parser using `xml.parsers.expat.ParserCreate`. 3. Set the appropriate handlers on the parser. 4. Parse the provided XML string using the `Parse` method. 5. Collect and print the book data in the specified format. **Example Implementation:** ```python import xml.parsers.expat class BookParser: def __init__(self): self.parser = xml.parsers.expat.ParserCreate() self.parser.StartElementHandler = self.start_element self.parser.EndElementHandler = self.end_element self.parser.CharacterDataHandler = self.char_data self.current_element = None self.current_book = {} self.books = [] def start_element(self, name, attrs): if name == \\"book\\": self.current_book = {\\"id\\": attrs[\\"id\\"]} elif name in {\\"title\\", \\"author\\", \\"year\\"}: self.current_element = name def end_element(self, name): if name == \\"book\\": self.books.append(self.current_book) self.current_book = {} self.current_element = None def char_data(self, data): if self.current_element: self.current_book[self.current_element] = data def parse(self, xml_string): self.parser.Parse(xml_string, True) return self.books # Test the implementation xml_string = <?xml version=\\"1.0\\"?> <library> <book id=\\"1\\"> <title>XML Programming</title> <author>John Doe</author> <year>2020</year> </book> <book id=\\"2\\"> <title>Learning Python</title> <author>Jane Smith</author> <year>2018</year> </book> </library> parser = BookParser() books = parser.parse(xml_string) print(books) ``` **Expected Output:** ```python [ { \\"id\\": \\"1\\", \\"title\\": \\"XML Programming\\", \\"author\\": \\"John Doe\\", \\"year\\": \\"2020\\", }, { \\"id\\": \\"2\\", \\"title\\": \\"Learning Python\\", \\"author\\": \\"Jane Smith\\", \\"year\\": \\"2018\\", }, ] ```","solution":"import xml.parsers.expat class BookParser: def __init__(self): self.parser = xml.parsers.expat.ParserCreate() self.parser.StartElementHandler = self.start_element self.parser.EndElementHandler = self.end_element self.parser.CharacterDataHandler = self.char_data self.current_element = None self.current_book = {} self.books = [] def start_element(self, name, attrs): if name == \\"book\\": self.current_book = {\\"id\\": attrs[\\"id\\"]} elif name in {\\"title\\", \\"author\\", \\"year\\"}: self.current_element = name def end_element(self, name): if name == \\"book\\": self.books.append(self.current_book) self.current_book = {} self.current_element = None def char_data(self, data): if self.current_element: self.current_book[self.current_element] = data.strip() def parse(self, xml_string): self.parser.Parse(xml_string, True) return self.books"},{"question":"# Question You are provided with the famous Titanic dataset. Your task is to create a comprehensive visualization using seaborn\'s violinplot to analyze the distribution of passengers age across different classes and their survival status. Implement the following functions: 1. **`load_titanic_data()`**: This function should load and return the Titanic dataset using seaborn. 2. **`basic_violin_plot(df)`**: This function takes the Titanic dataset as input and creates a basic violin plot of the `age` column. 3. **`categorized_violin_plot(df)`**: This function should create a violin plot categorizing the `age` distribution by `class`. 4. **`advanced_violin_plot(df)`**: This function should create a violin plot categorizing the `age` distribution by `class` and further splits the violins by `survival` status (`alive`), using different customization options (`split`, `inner`, etc.). # Function Definitions **Function 1**: `load_titanic_data()` - **Input**: None - **Output**: pandas.DataFrame containing the Titanic dataset. **Function 2**: `basic_violin_plot(df)` - **Input**: pandas.DataFrame containing the Titanic dataset. - **Output**: None (displays a basic violin plot of the `age` column). **Function 3**: `categorized_violin_plot(df)` - **Input**: pandas.DataFrame containing the Titanic dataset. - **Output**: None (displays a violin plot categorizing `age` by `class`). **Function 4**: `advanced_violin_plot(df)` - **Input**: pandas.DataFrame containing the Titanic dataset. - **Output**: None (displays a customized violin plot categorizing `age` by `class` and `survival` status, with additional customizations). # Constraints - Use seaborn for the plots. - Make sure to handle missing values in the `age` column appropriately (e.g., by removing them before plotting). - Customize the plots to make them informative and easily interpretable (e.g., using titles, axis labels, etc.). # Sample Implementation ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_titanic_data(): return sns.load_dataset(\\"titanic\\") def basic_violin_plot(df): plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"age\\"].dropna()) plt.title(\'Distribution of Age\') plt.xlabel(\'Age\') plt.show() def categorized_violin_plot(df): plt.figure(figsize=(12, 8)) sns.violinplot(data=df, x=\\"age\\", y=\\"class\\") plt.title(\'Age Distribution by Class\') plt.xlabel(\'Age\') plt.ylabel(\'Class\') plt.show() def advanced_violin_plot(df): plt.figure(figsize=(14, 10)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True, inner=\\"quart\\") plt.title(\'Age Distribution by Class and Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show() # Sample usage: # df = load_titanic_data() # basic_violin_plot(df) # categorized_violin_plot(df) # advanced_violin_plot(df) ``` Ensure your functions are well-documented, and the output plots are clearly labeled and visually appealing.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_titanic_data(): Loads the Titanic dataset from seaborn\'s repository. Returns: pd.DataFrame: DataFrame containing the Titanic dataset. return sns.load_dataset(\\"titanic\\") def basic_violin_plot(df): Creates a basic violin plot of the \'age\' column from the given DataFrame. Args: df (pd.DataFrame): DataFrame containing the Titanic dataset. plt.figure(figsize=(10, 6)) sns.violinplot(y=df[\\"age\\"].dropna()) plt.title(\'Distribution of Age\') plt.xlabel(\'Age\') plt.show() def categorized_violin_plot(df): Creates a violin plot categorizing the \'age\' distribution by \'class\'. Args: df (pd.DataFrame): DataFrame containing the Titanic dataset. plt.figure(figsize=(12, 8)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\") plt.title(\'Age Distribution by Class\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show() def advanced_violin_plot(df): Creates a violin plot categorizing the \'age\' distribution by \'class\' and split by \'alive\' status. Args: df (pd.DataFrame): DataFrame containing the Titanic dataset. plt.figure(figsize=(14, 10)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True, inner=\\"quart\\") plt.title(\'Age Distribution by Class and Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show()"},{"question":"# ASCII Character Classifier and Manipulator You are tasked with creating a custom ASCII character classifier and manipulator using the `curses.ascii` module. The goal is to implement a Python function that classifies characters, manipulates them according to specific rules, and then returns the modified characters. Function Signature ```python def classify_and_manipulate_chars(input_string: str) -> str: Classifies and manipulates a string of ASCII characters. Parameters: - input_string (str): A string containing ASCII characters. Returns: - str: A modified version of the input string where: - Alphanumeric characters are converted to uppercase. - Control characters are replaced with their mnemonic representation. - Non-ASCII characters (ordinal >= 0x80) are represented as \'!\'. ``` Input - `input_string`: A string composed of ASCII characters. The length of the string is at most 1000 characters. Output - A string where each character has been modified according to the rules listed below. Constraints - The input string will contain valid ASCII characters only. Rules 1. Convert all ASCII alphanumeric characters to their uppercase equivalents. 2. Replace every control character in the input string with its mnemonic representation (e.g., `NUL`, `SOH`, etc.) using the `curses.ascii.controlnames` array. 3. If a character has its meta bit set (ordinal value >= 0x80), replace it with `!`. Example ```python input_string = \\"HellonWorld!x80\\" output_string = \\"HELLOnWORLD! !\\" ``` Explanation: - Characters `\'H\'`, `\'e\'`, `\'l\'`, `\'o\'`, `\'W\'`, `\'r\'`, `\'d\'` are converted to uppercase. - The newline character `n` remains as is since `controlnames` contains the mnemonic \\"LF\\". - The character with ordinal value `0x80` is replaced by `!`. Implementation ```python import curses.ascii def classify_and_manipulate_chars(input_string: str) -> str: result = [] for char in input_string: if curses.ascii.isalnum(char): result.append(char.upper()) elif curses.ascii.isctrl(char): mnemonic = curses.ascii.controlnames[ord(char)] result.append(mnemonic) elif curses.ascii.ismeta(char): result.append(\'!\') else: result.append(char) return \'\'.join(result) ```","solution":"import curses.ascii def classify_and_manipulate_chars(input_string: str) -> str: result = [] for char in input_string: if curses.ascii.isalnum(char): result.append(char.upper()) elif curses.ascii.isctrl(char): mnemonic = curses.ascii.controlnames[ord(char)] result.append(mnemonic) elif ord(char) >= 0x80: # Non-ASCII characters condition result.append(\'!\') else: result.append(char) return \'\'.join(result)"},{"question":"Profiling and Optimization with scikit-learn Objective Demonstrate your understanding of profiling Python code, identifying performance bottlenecks, and optimizing code using Cython or multi-core parallelism. Problem Statement You are given a dataset and a scikit-learn model fitting task. Your goal is to: 1. Profile the performance of the given code to identify bottlenecks. 2. Optimize the code by converting critical sections to Cython or implementing multi-core parallelism using `joblib`. Dataset Use the `load_digits` dataset from `sklearn.datasets`. Task 1. Profile the performance of the following code and identify the main bottleneck: ```python from sklearn.decomposition import NMF from sklearn.datasets import load_digits X, _ = load_digits(return_X_y=True) model = NMF(n_components=16, tol=1e-2) %timeit model.fit(X) ``` 2. Optimize the identified bottleneck using either: - Converting critical sections to Cython. - Implementing multi-core parallelism using `joblib`. 3. Validate the optimized code by profiling again and comparing the execution time with the original code. Constraints - Your optimized code should be readable and maintain compatibility with scikit-learn\'s API. - Ensure that the results produced by the optimized model are consistent with the original model. Performance Requirements - The optimized code should demonstrate a noticeable improvement in execution time compared to the original code. - Aim for at least a **2x speed-up** in execution time. Input and Output Formats 1. **Input**: None (The data is loaded within the function) 2. **Output**: Print the profiled execution times and the improvement percentage. Example ```python def optimize_nmf(): # Step 1: Profile the original NMF fitting from sklearn.decomposition import NMF from sklearn.datasets import load_digits import timeit X, _ = load_digits(return_X_y=True) model = NMF(n_components=16, tol=1e-2) original_time = timeit.timeit(lambda: model.fit(X), number=1) print(f\'Original NMF fitting time: {original_time} seconds\') # Step 2: Optimize the bottleneck (e.g., using Cython or joblib) # Implement the optimized code here # Step 3: Validate the optimized code by profiling again optimized_time = timeit.timeit(lambda: optimized_model.fit(X), number=1) print(f\'Optimized NMF fitting time: {optimized_time} seconds\') # Print improvement improvement = (original_time - optimized_time) / original_time * 100 print(f\'Improvement: {improvement:.2f}%\') # Example call to the function optimize_nmf() ``` Note: The function `optimize_nmf` should contain both the original and optimized NMF fitting code, showcasing the performance improvements.","solution":"from sklearn.decomposition import NMF from sklearn.datasets import load_digits from joblib import Parallel, delayed import timeit # Original function to be optimized def original_nmf(): X, _ = load_digits(return_X_y=True) model = NMF(n_components=16, tol=1e-2) model.fit(X) # Function to perform NMF fitting in parallel def parallel_nmf(X, n_components, tol, n_jobs=-1): model = NMF(n_components=n_components, tol=tol) Parallel(n_jobs=n_jobs)(delayed(model.fit)(X[i::n_jobs]) for i in range(n_jobs)) return model def optimize_nmf(): # Step 1: Profile the original NMF fitting X, _ = load_digits(return_X_y=True) original_model = NMF(n_components=16, tol=1e-2) original_time = timeit.timeit(lambda: original_model.fit(X), number=1) print(f\'Original NMF fitting time: {original_time:.4f} seconds\') # Step 2: Optimize the bottleneck using joblib for multi-core parallelism # Note: `parallel_nmf` function designed to work with separated jobs optimized_time = timeit.timeit(lambda: parallel_nmf(X, n_components=16, tol=1e-2), number=1) print(f\'Optimized NMF fitting time: {optimized_time:.4f} seconds\') # Print improvement improvement = (original_time - optimized_time) / original_time * 100 print(f\'Improvement: {improvement:.2f}%\') # Example call to the function optimize_nmf()"},{"question":"Problem Statement: You are tasked with implementing a function that communicates with a low-level device over a network. The device expects and sends data in a strictly defined binary format. This format includes a header, two integers representing commands and parameters, and a footer. The structure is as follows: - **Header**: A single byte, always `0xAA`. - **Command**: A 4-byte signed integer (big-endian). - **Parameter**: A 4-byte unsigned integer (little-endian). - **Footer**: A single byte, always `0x55`. You need to write a function `create_message(command: int, parameter: int) -> bytes` that: 1. Takes two arguments: - `command`: a signed 4-byte integer. - `parameter`: an unsigned 4-byte integer. 2. Constructs and returns the packed binary message as described above. You also need to write a function `parse_message(message: bytes) -> tuple` that, given a bytes object representing the binary message: 1. Validates the `Header` and `Footer`. 2. Extracts the `command` and `parameter` values from the message. 3. Returns a tuple `(command, parameter)`. Here\'s the format detail: - Header: `b\'xAA\'` - Command: 4-byte signed integer in big-endian format. - Parameter: 4-byte unsigned integer in little-endian format. - Footer: `b\'x55\'` **Constraints:** - The `command` must be within the range of [-2^31, 2^31 - 1]. - The `parameter` must be within the range of [0, 2^32 - 1]. **Example:** ```python # Example Input command = 1234 parameter = 5678 # Creating the message message = create_message(command, parameter) print(message) # Outputs: b\'xAAx00x00x04xd2\'x2ex16x00x00x55\' # Parsing the message result = parse_message(message) print(result) # Outputs: (1234, 5678) ``` **Function Template:** ```python import struct def create_message(command: int, parameter: int) -> bytes: # Implement the function here pass def parse_message(message: bytes) -> tuple: # Implement the function here pass ``` **Note:** Ensure all edge cases are handled, including invalid header/footer bytes and incorrect message lengths.","solution":"import struct def create_message(command: int, parameter: int) -> bytes: if not (-2**31 <= command <= 2**31 - 1): raise ValueError(\\"Command must be a signed 4-byte integer.\\") if not (0 <= parameter <= 2**32 - 1): raise ValueError(\\"Parameter must be an unsigned 4-byte integer.\\") header = b\'xAA\' footer = b\'x55\' command_bytes = struct.pack(\'>i\', command) # Big-endian 4-byte signed integer parameter_bytes = struct.pack(\'<I\', parameter) # Little-endian 4-byte unsigned integer message = header + command_bytes + parameter_bytes + footer return message def parse_message(message: bytes) -> tuple: if len(message) != 10: raise ValueError(\\"Invalid message length.\\") if message[0] != 0xAA or message[-1] != 0x55: raise ValueError(\\"Invalid header or footer.\\") command = struct.unpack(\'>i\', message[1:5])[0] # Big-endian 4-byte signed integer parameter = struct.unpack(\'<I\', message[5:9])[0] # Little-endian 4-byte unsigned integer return (command, parameter)"},{"question":"# Question: Custom Wrapper for Built-in Functions In this assessment, you are required to create a custom implementation of a built-in function while retaining the ability to use the original built-in function. You will demonstrate your understanding by enhancing the behavior of a built-in function. Task: 1. Create a custom `print` function that: - Prepends a timestamp to each message being printed. - Allows the use of the original print function\'s `sep`, `end`, and `file` parameters. 2. Use the `builtins` module to access the original `print` function within your custom implementation. Specifications: - The custom `print` function should be named `custom_print`. - The timestamp should be in the format: `YYYY-MM-DD HH:MM:SS`. - The function should accept any number of arguments and keyword arguments similar to the original `print` function. - Your implementation should use the `datetime` module to generate the current timestamp. - The custom function should be able to handle all typical use cases of the original `print` function, including the `sep`, `end`, and `file` parameters. Example: ```python from datetime import datetime import builtins def custom_print(*args, **kwargs): # Your implementation here pass # Example usage: custom_print(\\"Hello, world!\\") ``` Expected Output: ``` 2023-10-11 14:33:07 Hello, world! ``` Testing: Your function will be tested with various inputs to ensure it behaves as expected, including default behavior and variations in `sep`, `end`, and `file` parameters. **Constraints:** - You must use the `builtins` module to access the original `print` function. - Your solution should not use any external libraries other than `datetime` and `builtins`. - Ensure your code is clean, well-commented, and adheres to best practices. Performance: - Your solution should work efficiently and should not introduce significant overhead compared to the original `print` function.","solution":"from datetime import datetime import builtins def custom_print(*args, **kwargs): Custom print function that prepends a timestamp to each message being printed. Accepts the same arguments and keyword arguments as the built-in print function. timestamp = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') builtins.print(timestamp, *args, **kwargs)"},{"question":"You are required to use the `wave` module to perform the following tasks: 1. **Read a WAV file**: Open a provided WAV file and extract its parameters. 2. **Manipulate the WAV file**: Perform some operations on the audio data. 3. **Write a new WAV file**: Save the manipulated audio data into a new WAV file. **Detailed steps:** 1. Write a function `read_wav_file(file_path)` that: - Takes a file path as input. - Opens the WAV file in read mode. - Extracts and returns the following parameters from the WAV file: number of channels, sample width (in bytes), frame rate (samples per second), and the total number of frames. 2. Write a function `manipulate_audio_data(file_path, factor)` that: - Takes a file path and a factor as input. - Reads the frames from the WAV file. - Slows down the audio by repeating each frame *factor* times and returns the new frame data. - For example, if `factor` is 2, each frame in the original audio should be duplicated once in the output. 3. Write a function `write_wav_file(input_file_path, output_file_path, factor)` that: - Takes the input file path, output file path, and a factor as input. - Uses the `read_wav_file` function to get the audio parameters from the input file. - Calls the `manipulate_audio_data` function to get the adjusted frame data. - Writes the adjusted frame data to the new WAV file, using the extracted parameters and the original frame rate. **Constraints:** - You can assume that the input WAV file will always be present and correctly formatted. - The factor will be a positive integer. **Example:** ```python input_file = \\"example.wav\\" output_file = \\"example_slowed.wav\\" factor = 2 write_wav_file(input_file, output_file, factor) ``` Given this function call: - It should read the parameters from `example.wav`. - It should duplicate each frame, effectively reducing the playback speed by half. - Finally, it should write the slowed down audio to `example_slowed.wav`. **Hints:** - Utilize the methods provided by the `wave` module for reading and writing frames. - Ensure you handle closing of files properly using the `with` statement.","solution":"import wave def read_wav_file(file_path): Reads a WAV file and extracts its parameters. Args: - file_path (str): Path to the input WAV file. Returns: - tuple: (number of channels, sample width, frame rate, total number of frames) with wave.open(file_path, \'rb\') as wav_file: num_channels = wav_file.getnchannels() sample_width = wav_file.getsampwidth() frame_rate = wav_file.getframerate() num_frames = wav_file.getnframes() return num_channels, sample_width, frame_rate, num_frames def manipulate_audio_data(file_path, factor): Manipulates the audio data by repeating each frame \'factor\' times. Args: - file_path (str): Path to the input WAV file. - factor (int): The factor by which to slow down the audio. Returns: - bytes: The manipulated frame data. with wave.open(file_path, \'rb\') as wav_file: frames = wav_file.readframes(wav_file.getnframes()) new_frames = bytearray() for i in range(0, len(frames), wav_file.getsampwidth()): frame = frames[i:i+wav_file.getsampwidth()] new_frames.extend(frame * factor) # Repeat each frame \'factor\' times return bytes(new_frames) def write_wav_file(input_file_path, output_file_path, factor): Writes the manipulated audio data to a new WAV file. Args: - input_file_path (str): Path to the input WAV file. - output_file_path (str): Path to the output WAV file. - factor (int): The factor by which to slow down the audio. num_channels, sample_width, frame_rate, num_frames = read_wav_file(input_file_path) new_frame_data = manipulate_audio_data(input_file_path, factor) with wave.open(output_file_path, \'wb\') as out_file: out_file.setnchannels(num_channels) out_file.setsampwidth(sample_width) out_file.setframerate(frame_rate) out_file.writeframes(new_frame_data)"},{"question":"# Unix Terminal Mode Control Task Objective You are required to write a Python function using the `tty` and `termios` modules to: 1. Check and print the current terminal mode of a given file descriptor. 2. Change the terminal mode to raw. 3. Change the terminal mode to cbreak. 4. Restore the terminal mode to its original state. 5. Return the original and final terminal modes for verification. Requirements 1. **Function Signature**: ```python def control_terminal_mode(fd: int) -> tuple: ``` 2. **Parameters**: - `fd` (int): The file descriptor representing the terminal. 3. **Returns**: - `tuple`: A tuple containing the original terminal mode and the final terminal mode settings. 4. **Functionality**: - Retrieve and print the current terminal mode using `termios.tcgetattr`. - Set the terminal to raw mode using `tty.setraw`. - Set the terminal to cbreak mode using `tty.setcbreak`. - Restore the original terminal mode using `termios.tcsetattr`. - Ensure each of these steps is printed out for clarity and debugging. 5. **Constraints**: - This function should only be run on Unix systems. - Handle any potential exceptions that might occur due to unsupported operations or invalid file descriptors. - Ensure that the terminal settings are restored even if an exception occurs. Example Usage ```python import sys import termios import tty def control_terminal_mode(fd: int) -> tuple: import termios import tty original_termios = termios.tcgetattr(fd) print(\\"Original terminal mode:\\", original_termios) tty.setraw(fd) print(\\"Terminal set to raw mode.\\") tty.setcbreak(fd) print(\\"Terminal set to cbreak mode.\\") termios.tcsetattr(fd, termios.TCSAFLUSH, original_termios) print(\\"Terminal restored to original mode.\\") final_termios = termios.tcgetattr(fd) print(\\"Final terminal mode:\\", final_termios) return original_termios, final_termios # Example usage fd = sys.stdin.fileno() original_mode, final_mode = control_terminal_mode(fd) print(\\"Original mode:\\", original_mode) print(\\"Final mode:\\", final_mode) ``` Review the function to ensure that the terminal modes are correctly set and restored, and proper output is printed at each step. **Note**: This question is designed for execution in a Unix environment and may not be suitable for other operating systems such as Windows.","solution":"import sys import termios import tty import os def control_terminal_mode(fd: int) -> tuple: Controls the terminal mode of the given file descriptor. It checks the current mode, sets it to raw, then to cbreak, and finally restores it to the original mode. # Store the original terminal settings try: original_mode = termios.tcgetattr(fd) print(\\"Original terminal mode:\\", original_mode) # Set terminal to raw mode tty.setraw(fd) print(\\"Terminal set to raw mode.\\") # Set terminal to cbreak mode tty.setcbreak(fd) print(\\"Terminal set to cbreak mode.\\") # Restore the original terminal mode termios.tcsetattr(fd, termios.TCSAFLUSH, original_mode) print(\\"Terminal restored to original mode.\\") # Get the final terminal settings final_mode = termios.tcgetattr(fd) print(\\"Final terminal mode:\\", final_mode) return original_mode, final_mode except Exception as e: print(f\\"An error occurred: {e}\\") termios.tcsetattr(fd, termios.TCSAFLUSH, original_mode) raise # Example usage (This part would be run in a real terminal to test): # fd = sys.stdin.fileno() # original_mode, final_mode = control_terminal_mode(fd) # print(\\"Original mode:\\", original_mode) # print(\\"Final mode:\\", final_mode)"},{"question":"# Question: Implement a Secure Password Storage System In this exercise, you are required to implement a secure password storage system using concepts from the `hashlib`, `hmac`, and `secrets` modules. Description Your task is to write a class called `SecurePasswordManager`. This class should provide functionality to securely store passwords and verify them. Requirements 1. **Storing Passwords:** - When storing a password, you should: - Use the `secrets` module to generate a unique salt for each password. - Hash the password together with the salt using the BLAKE2b algorithm from the `hashlib` module. - Store the salt and the hash. 2. **Verifying Passwords:** - To verify a password, you should: - Retrieve the stored salt and hash for the user. - Hash the provided password with the stored salt using the BLAKE2b algorithm. - Compare the resulting hash with the stored hash securely using the `hmac` module. Specifications - Implement a class `SecurePasswordManager` with the following methods: ```python class SecurePasswordManager: def __init__(self): # Initialize the storage for passwords pass def store_password(self, username: str, password: str) -> None: # Store the password for the given username pass def verify_password(self, username: str, password: str) -> bool: # Verify whether the provided password matches the stored password for the given username pass ``` - **Input:** - `store_password` method takes a `username` (string) and a `password` (string) as input. - `verify_password` method takes a `username` (string) and a `password` (string) as input. - **Output:** - `store_password` method doesn\'t return anything. - `verify_password` method returns a boolean indicating if the password is correct. Constraints - Ensure that the storage of passwords is secure and not susceptible to common attacks such as rainbow table attacks. - The solution should prevent timing attacks when comparing hashes. Example Usage ```python manager = SecurePasswordManager() manager.store_password(\'user1\', \'password123\') assert manager.verify_password(\'user1\', \'password123\') == True assert manager.verify_password(\'user1\', \'wrongpassword\') == False ``` # Performance Requirements - The password storage and verification operations must be efficient. You should consider the time complexity of hash operations and secure comparison functions.","solution":"import hashlib import hmac import secrets class SecurePasswordManager: def __init__(self): # Initialize the storage for passwords as a dictionary self.users = {} def store_password(self, username: str, password: str) -> None: Store the password for the given username securely. # Generate a unique salt for the password salt = secrets.token_hex(16) # Create the hash using password + salt password_hash = hashlib.blake2b((salt + password).encode()).hexdigest() # Store the salt and password hash together self.users[username] = (salt, password_hash) def verify_password(self, username: str, password: str) -> bool: Verify whether the provided password matches the stored password for the given username. if username not in self.users: return False # Retrieve the stored salt and hash salt, stored_hash = self.users[username] # Hash the provided password with the stored salt test_hash = hashlib.blake2b((salt + password).encode()).hexdigest() # Compare the resulting hash with the stored hash securely return hmac.compare_digest(test_hash, stored_hash)"},{"question":"# Python Coding Assessment Question Context: You are working on a text processing task that involves analyzing and processing a large dataset stored in a text file. The file contains multiple records, where each record is a line formatted as follows: ``` ID, Name, Age, Score ``` Here\'s an example of a few lines from the file: ``` 1, Alice, 30, 88 2, Bob, 24, 95 3, Charlie, 29, 79 ... ``` Task: You are required to write a Python function `process_records(file_path)` that performs the following operations: 1. **Read the file** and load the data into a suitable data structure. 2. **Sort the records** by the \\"Score\\" in descending order. If two records have the same score, sort them by \\"Age\\" in ascending order. 3. **Write the sorted records** to a new file named `sorted_<original_filename>`. For example, if the original file is `records.txt`, the new file should be `sorted_records.txt`. 4. **Implement error handling** for common file-related errors such as the file not found, permission issues, and parsing errors. Log these errors to a file named `errors.log`. 5. **Calculate the average score** of all records and return it. Function Signature: ```python def process_records(file_path: str) -> float: pass ``` Input: - `file_path` (str): The path to the input text file containing the records. Output: - Returns the average score (float) of all records. Example: Assuming the `records.txt` file contains the following data: ``` 1, Alice, 30, 88 2, Bob, 24, 95 3, Charlie, 29, 79 ``` After processing, the sorted file `sorted_records.txt` will contain: ``` 2, Bob, 24, 95 1, Alice, 30, 88 3, Charlie, 29, 79 ``` If the average score is calculated to be 87.33, the function should return `87.33`. Constraints: - The input file will have at least one record. - Each record is correctly formatted. - Assume \\"Age\\" is a positive integer and \\"Score\\" is an integer between 0 and 100 inclusive. Notes: - Ensure the function is optimized for performance, considering the potential size of the dataset. - Include appropriate documentation strings and comments in your code for clarity.","solution":"import os import logging from typing import List, Tuple def parse_line(line: str) -> Tuple[int, str, int, int]: Parse a line of input file into a tuple of (ID, Name, Age, Score). try: parts = line.strip().split(\\", \\") return (int(parts[0]), parts[1], int(parts[2]), int(parts[3])) except ValueError as e: raise ValueError(f\\"Error parsing line \'{line}\': {e}\\") def process_records(file_path: str) -> float: Processes the records in the input file, sorts them, writes to a new file, and returns the average score. try: with open(file_path, \'r\') as file: lines = file.readlines() records = [parse_line(line) for line in lines] except FileNotFoundError: logging.error(f\\"File not found: {file_path}\\") return -1 except PermissionError: logging.error(f\\"Permission denied: {file_path}\\") return -1 except Exception as e: logging.error(f\\"Error processing file: {e}\\") return -1 sorted_records = sorted(records, key=lambda x: (-x[3], x[2])) output_file_path = f\\"sorted_{os.path.basename(file_path)}\\" with open(output_file_path, \'w\') as outfile: for rec in sorted_records: outfile.write(f\\"{rec[0]}, {rec[1]}, {rec[2]}, {rec[3]}n\\") avg_score = sum(rec[3] for rec in records) / len(records) return round(avg_score, 2)"},{"question":"Objective Demonstrate your understanding of creating advanced visualizations using the Seaborn library. You will need to load a dataset, create a plot, and apply various customizations and transformations. Problem Statement You are given the `diamonds` dataset from the Seaborn library. Your task is to create a customized bar plot using this dataset. The requirements are as follows: 1. Load the `diamonds` dataset. 2. Create a bar plot showing the average price of diamonds across different `cut` categories. 3. Apply a transformation to show the bars dodged by the `color` variable. 4. Use an aggregation function that computes the interquartile range (IQR) of the `price` instead of the mean. # Expected Input and Output * You should generate a bar plot, and the plot should be displayed as output. # Code Requirements * Use the `seaborn.objects` module for plotting. * Use appropriate functions and transformations as discussed. Constraints * None # Example: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Create plot with average price across different cuts # Step 3: Apply dodging for color variable # Step 4: Use aggregation with IQR # Creating the plot p = so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") p.add(so.Bar(), so.Agg(lambda x: x.quantile(.75) - x.quantile(.25)), so.Dodge(), color=\\"color\\") # Display the plot p.show() ``` Generate the plot by running the example above, ensuring all steps are correctly implemented.","solution":"import seaborn.objects as so from seaborn import load_dataset # Step 1: Load dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Create plot with average price across different cuts # Step 3: Apply dodging for color variable # Step 4: Use aggregation with IQR # Creating the plot p = so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") p.add(so.Bar(), so.Agg(lambda x: x.quantile(.75) - x.quantile(.25)), so.Dodge(), color=\\"color\\") # Display the plot p.show()"},{"question":"# Color Space Transformation Challenge **Objective:** Implement a function that takes a list of RGB colors and performs multiple transformations among different color spaces. Your task is to verify and ensure the consistency of these transformations. **Function Signature:** ```python def transform_and_validate_colors(rgb_colors: list) -> list: ``` **Input:** - `rgb_colors`: A list of tuples, where each tuple represents an RGB color. Each color component (r, g, b) in the tuple is a floating-point value between 0 and 1. Example: `[(0.2, 0.4, 0.4), (0.9, 0.1, 0.4)]` **Output:** - A list of boolean values corresponding to each input color. Each boolean value indicates whether the original RGB color can be accurately recovered after a round-trip conversion through YIQ, HLS, and HSV color spaces. **Steps:** 1. Convert the input RGB color to YIQ, then back to RGB. 2. Convert the input RGB color to HLS, then back to RGB. 3. Convert the input RGB color to HSV, then back to RGB. 4. Compare the original RGB color to the final RGB result of each conversion to determine if they are nearly identical (consider a tolerance level of `1e-6`). **Constraints:** - Use the conversion functions provided by the `colorsys` module for the transformations. - RGB values after transformation should be considered equal if each component\'s absolute difference from the original is less than `1e-6`. **Example:** ```python >>> transform_and_validate_colors([(0.2, 0.4, 0.4), (0.9, 0.1, 0.4)]) [True, False] ``` **Notes:** - An RGB color is considered valid if it can accurately round-trip through all three color spaces: YIQ, HLS, and HSV. - Pay careful attention to floating-point comparisons. Implement the function to assess your understanding of color space transformations.","solution":"import colorsys def transform_and_validate_colors(rgb_colors: list) -> list: def check_approx_equal(t1, t2, tolerance=1e-6): return all(abs(a - b) < tolerance for a, b in zip(t1, t2)) results = [] for r, g, b in rgb_colors: # RGB -> YIQ -> RGB yiq = colorsys.rgb_to_yiq(r, g, b) rgb_from_yiq = colorsys.yiq_to_rgb(*yiq) if not check_approx_equal((r, g, b), rgb_from_yiq): results.append(False) continue # RGB -> HLS -> RGB hls = colorsys.rgb_to_hls(r, g, b) rgb_from_hls = colorsys.hls_to_rgb(*hls) if not check_approx_equal((r, g, b), rgb_from_hls): results.append(False) continue # RGB -> HSV -> RGB hsv = colorsys.rgb_to_hsv(r, g, b) rgb_from_hsv = colorsys.hsv_to_rgb(*hsv) if not check_approx_equal((r, g, b), rgb_from_hsv): results.append(False) continue results.append(True) return results"},{"question":"# Bytecode Analysis and Modification Task In this task, you will explore Python bytecode using the \\"dis\\" module. You will be given a function, and you will need to analyze its bytecode, modify the bytecode to change the function\'s behavior, and then verify the changes. This will test your understanding of bytecode manipulation and the `dis` module\'s capabilities. Function Description: You are provided with a function `square_and_add` that takes two numbers, squares the first number, and adds it to the second number. It returns the result. ```python def square_and_add(x, y): result = (x * x) + y return result ``` # Part 1: Analyze Bytecode 1. Write a function `analyze_bytecode` that takes a Python function `func` as an argument, disassembles it using the `dis` module, and returns a list of bytecode instructions. ```python def analyze_bytecode(func): Analyze the bytecode of the given function. Args: func (function): The function to disassemble. Returns: list: A list of dis.Instruction named tuples representing the bytecode instructions. # Your code here ``` # Part 2: Modify Bytecode 2. Write a function `modify_bytecode` that takes the function `square_and_add` and modifies its bytecode to change its behavior to first add the two numbers and then square the result. Your function should return the modified function. **Hint**: You may find it helpful to understand the structure of the `square_and_add` bytecode before making modifications. ```python def modify_bytecode(func): Modify the bytecode of the given function to change its behavior. Args: func (function): The function whose bytecode needs modification. Returns: function: The modified function with new behavior. # Your code here ``` # Part 3: Verify the Modification 3. Write a function `verify_modification` that verifies your modification from Part 2 by comparing the output of the original `square_and_add` function and your modified function for various inputs. ```python def verify_modification(): Verify modification by comparing outputs of the original and modified function. Returns: None original_func = square_and_add modified_func = modify_bytecode(square_and_add) # Test cases test_cases = [ (2, 3), (4, 5), (6, 7) ] for x, y in test_cases: original_output = original_func(x, y) modified_output = modified_func(x, y) print(f\\"Original: {original_output}, Modified: {modified_output}\\") # Your additional verification code here (if needed) ``` # Constraints and Requirements: - You must use relevant functions from the `dis` module for analyzing and disassembling code. - Ensure your `modify_bytecode` function correctly changes the behavior from squaring then adding to adding then squaring. - Verify the correctness of your modified bytecode. This task will test your understanding of Python\'s bytecode, your ability to manipulate it, and your overall comprehension of the `dis` module.","solution":"import dis import types def square_and_add(x, y): Squares the first number and adds it to the second number. result = (x * x) + y return result def analyze_bytecode(func): Analyze the bytecode of the given function. Args: func (function): The function to disassemble. Returns: list: A list of dis.Instruction named tuples representing the bytecode instructions. instructions = dis.get_instructions(func) return list(instructions) def modify_bytecode(func): Modify the bytecode of the given function to change its behavior to first add the two numbers and then square the result. Args: func (function): The function whose bytecode needs modification. Returns: function: The modified function with new behavior. # Bytecode for the operation (x + y) ** 2 new_code = [ dis.opmap[\'LOAD_FAST\'], 0, # Load x onto the stack dis.opmap[\'LOAD_FAST\'], 1, # Load y onto the stack dis.opmap[\'BINARY_ADD\'], 0, # Perform x + y dis.opmap[\'LOAD_CONST\'], 1, # Load 2 (as an exponent) onto the stack dis.opmap[\'BINARY_POWER\'], 0, # Perform (x + y) ** 2 dis.opmap[\'RETURN_VALUE\'], 0 # Return the result ] # Create the new code object new_code_object = types.CodeType( func.__code__.co_argcount, # argument count func.__code__.co_posonlyargcount, # posonly args count func.__code__.co_kwonlyargcount, # kwonly args count func.__code__.co_nlocals, # number of local variables func.__code__.co_stacksize, # stack size func.__code__.co_flags, # flags bytes(new_code), # bytecode (None, 2), # constants tuple (we need \'2\' as a constant) func.__code__.co_names, # names func.__code__.co_varnames, # variable names func.__code__.co_filename, # file name func.__code__.co_name, # name func.__code__.co_firstlineno, # first line number func.__code__.co_lnotab # bytecode-to-code mapping ) # Create the new function with the modified code object modified_function = types.FunctionType(new_code_object, func.__globals__, func.__name__) return modified_function def verify_modification(): Verify modification by comparing outputs of the original and modified function. Returns: None original_func = square_and_add modified_func = modify_bytecode(square_and_add) # Test cases test_cases = [ (2, 3), (4, 5), (6, 7) ] for x, y in test_cases: original_output = original_func(x, y) modified_output = modified_func(x, y) print(f\\"Original: {original_output}, Modified: {modified_output}\\") # Additional verification can be added here if needed verify_modification()"},{"question":"# Advanced Data Visualization with Seaborn\'s Object-Oriented Interface Background: Seaborn provides a high-level interface for drawing attractive and informative statistical graphics. Its object-oriented interface (`seaborn.objects`) offers advanced plot customization and design capabilities. In this task, you will use these capabilities to visualize a dataset, apply normalization, and create comparative visualizations. Task: You are provided with a dataset `healthexp` that records the healthcare spending in USD for various countries over certain years. Your task is to create a function that produces a specified plot using Seaborn\'s object-oriented interface. Function Signature: ```python def plot_health_expenditure(data: pd.DataFrame, base_year: int, show_change: bool) -> None: Plots healthcare expenditure using Seaborn\'s object-oriented interface. Parameters: data (pd.DataFrame): The input dataset containing \'Year\', \'Spending_USD\', and \'Country\' columns. base_year (int): The year to use as the baseline for relative normalization. show_change (bool): If True, normalize spending as percent change relative to `base_year`. If False, normalize spending relative to each country\'s maximum spending. Returns: None ``` Requirements: 1. Load the dataset using `seaborn.load_dataset(\\"healthexp\\")`. 2. Use `so.Plot` to create a line plot with: - `Year` on the x-axis. - `Spending_USD` on the y-axis. - Different countries distinguished by color. 3. Customize normalization based on the `show_change` parameter: - If `show_change` is `True`, normalize spending as percent change relative to `base_year`. - If `show_change` is `False`, normalize spending relative to each country\'s maximum spending. 4. Apply appropriate axis labeling depending on the normalization type: - \\"Percent change in spending from [base_year] baseline\\" for percent change normalization. - \\"Spending relative to maximum amount\\" for normalization to the maximum value. 5. Display the plot. Additional Notes: - Ensure proper use of Seaborn\'s `so.Norm` function to accomplish normalization. - Use `seaborn.objects` for a consistent interface. **Example Usage:** ```python plot_health_expenditure(data=helexpand, base_year=1970, show_change=True) ``` Constraints: - You may assume `base_year` always exists in the data. - Ensure proper imports and handling of the dataset.","solution":"import pandas as pd import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def plot_health_expenditure(data: pd.DataFrame, base_year: int, show_change: bool) -> None: Plots healthcare expenditure using Seaborn\'s object-oriented interface. Parameters: data (pd.DataFrame): The input dataset containing \'Year\', \'Spending_USD\', and \'Country\' columns. base_year (int): The year to use as the baseline for relative normalization. show_change (bool): If True, normalize spending as percent change relative to `base_year`. If False, normalize spending relative to each country\'s maximum spending. Returns: None if show_change: # Calculate percent change from base_year base_spending = data[data[\'Year\'] == base_year].set_index(\'Country\')[\'Spending_USD\'] data = data.join(base_spending, on=\'Country\', rsuffix=\'_base\') data[\'Normalized\'] = (data[\'Spending_USD\'] - data[\'Spending_USD_base\']) / data[\'Spending_USD_base\'] * 100 y_label = f\\"Percent change in spending from {base_year} baseline\\" else: # Calculate spending relative to each country\'s maximum spending max_spending = data.groupby(\'Country\')[\'Spending_USD\'].transform(\'max\') data[\'Normalized\'] = data[\'Spending_USD\'] / max_spending * 100 y_label = \\"Spending relative to maximum amount\\" # Plotting using seaborn.objects plot = so.Plot(data, x=\'Year\', y=\'Normalized\', color=\'Country\') plot.add(so.Line()) plot.label(y=y_label) plot.label(x=\\"Year\\") plot.show() # Example usage: # helexpand = sns.load_dataset(\\"healthexp\\") # plot_health_expenditure(helexpand, base_year=1970, show_change=True)"},{"question":"# Custom Event Loop Policy and Child Process Watcher **Objective:** Implement a custom event loop policy and a custom child process watcher using Python\'s asyncio package. This question tests your understanding of asyncio\'s event loop policies and process watchers, as well as your ability to implement custom behavior. # Instructions: 1. **Custom Event Loop Policy**: - Create a subclass of `asyncio.DefaultEventLoopPolicy` named `TrackingEventLoopPolicy`. - Override the `new_event_loop()` method to implement custom behavior that tracks how many times an event loop is created. - Implement a method `get_creation_count()` in the custom policy class to return the number of times an event loop was created. 2. **Custom Child Process Watcher**: - Create a subclass of `asyncio.AbstractChildWatcher` named `LoggingChildWatcher`. - Override the necessary methods to log (print) messages when a child handler is added, removed, or if the watcher is attached to an event loop. # Expected Input and Output: **Custom Event Loop Policy:** - The method `new_event_loop()` should return a new event loop object inheriting from `AbstractEventLoop`. - The method `get_creation_count()` should return an integer representing the number of times the `new_event_loop()` method was called. **Custom Child Process Watcher:** - Log (print) messages for adding, removing child handlers, and attaching the watcher to an event loop. # Constraints: - Ensure that the custom event loop policy class implements all required methods from the `asyncio.AbstractEventLoopPolicy` class. - Ensure that the custom child watcher class implements all required methods from the `asyncio.AbstractChildWatcher` class. **Example Usage:** ```python # Example usage of TrackingEventLoopPolicy asyncio.set_event_loop_policy(TrackingEventLoopPolicy()) policy = asyncio.get_event_loop_policy() loop1 = policy.new_event_loop() loop2 = policy.new_event_loop() print(policy.get_creation_count()) # Output: 2 # Example usage of LoggingChildWatcher watcher = LoggingChildWatcher() watcher.attach_loop(loop1) watcher.add_child_handler(12345, lambda pid, returncode: print(f\\"Process {pid} finished with code {returncode}\\")) watcher.remove_child_handler(12345) ``` # Submission: - Implement the `TrackingEventLoopPolicy` class. - Implement the `LoggingChildWatcher` class. - Provide a concise test code to demonstrate the functionality of both classes. Good luck!","solution":"import asyncio class TrackingEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def __init__(self): super().__init__() self._creation_count = 0 def new_event_loop(self): self._creation_count += 1 return super().new_event_loop() def get_creation_count(self): return self._creation_count class LoggingChildWatcher(asyncio.AbstractChildWatcher): def __init__(self): self._loop = None self._callbacks = {} def attach_loop(self, loop): self._loop = loop print(f\\"Watcher attached to loop {loop}\\") def add_child_handler(self, pid, callback, *args): self._callbacks[pid] = (callback, args) print(f\\"Handler added for child {pid}\\") def remove_child_handler(self, pid): if pid in self._callbacks: del self._callbacks[pid] print(f\\"Handler removed for child {pid}\\") def close(self): self._callbacks.clear() print(\\"Watcher closed\\") def __enter__(self): pass def __exit__(self, exc_type, exc, tb): pass def _do_waitpid(self): pass"},{"question":"Objective Write a Python function that takes a list of records (tuples) containing student grades and sorts and maintains this list using the functions from the `bisect` module. Implement a custom insertion function to add new records while maintaining the sorted order based on grade percentages. Problem Statement Design a function `insert_student_record(records, record)` that maintains the sorted list of student grades. Use the `bisect` module\'s `insort_right` function to add new records while maintaining the sorted order. The function should: 1. Insert new student records into the sorted list. 2. Maintain the order based on grade percentages in descending order. 3. Raise an error if the new record\'s grade percentage is outside the range [0, 100]. Inputs 1. `records`: A list of tuples where each tuple contains `(\'Student Name\', grade_percentage)` and the list is sorted in descending order based on the `grade_percentage`. 2. `record`: A tuple `(\'Student Name\', grade_percentage)` representing a new student record to be inserted. The `grade_percentage` should be between 0 and 100. Constraints - The grade percentage in the new record must be between 0 and 100. - The `records` list must always remain sorted in descending order based on grade percentage after the insertion of the new record. Output - The function should modify the `records` list in-place and return `None`. Example ```python from bisect import insort_right def insert_student_record(records, record): # Implement your solution here pass # Example usage: students = [(\'Alice\', 95), (\'Bob\', 90), (\'Charlie\', 85)] new_record = (\'David\', 92) insert_student_record(students, new_record) print(students) # Output: [(\'Alice\', 95), (\'David\', 92), (\'Bob\', 90), (\'Charlie\', 85)] ``` # Key Points to Consider 1. Ensure the list remains sorted in descending order. 2. Validate that the grade percentage is within the acceptable range. 3. Utilize the `insort_right` function from the `bisect` module efficiently.","solution":"from bisect import insort_right def insert_student_record(records, record): Inserts a new student record into the sorted list of student records in descending order based on grade percentage. Validates that the grade percentage is within the range [0, 100]. Parameters: records (list of tuples): A list of student records (\'Student Name\', grade_percentage) sorted in descending order. record (tuple): A new student record (\'Student Name\', grade_percentage) to add to the records. Raises: ValueError: If the grade percentage in the record is outside the range [0, 100]. if not (0 <= record[1] <= 100): raise ValueError(\\"Grade percentage must be between 0 and 100\\") def key(item): return -item[1] insort_right(records, record, key=key)"},{"question":"**Objective**: Demonstrate a comprehensive understanding of seaborn\'s `stripplot` and `catplot` for visualizing data, including customization and faceting. **Problem Statement**: You are provided with a dataset containing information about restaurant tips. Your task is to write a function that generates specific visualizations using seaborn. The dataset is structured similarly to the \\"tips\\" dataset used in seaborn examples. **Function Signature**: ```python def visualize_tips_data(data): Generate specific seaborn visualizations of the provided dataset. Parameters: - data (pd.DataFrame): A Pandas DataFrame containing the restaurant tips data. Returns: None (Plots should be displayed using seaborn) pass ``` **Input**: - A Pandas DataFrame `data` with at least the following columns: - `total_bill` (float): Total bill amount. - `tip` (float): Tip amount. - `sex` (str): Gender of the person who paid. - `smoker` (str): Indicates whether the person is a smoker. - `day` (str): Day of the week. - `time` (str): Time of the day (Lunch or Dinner). - `size` (int): Size of the party. **Tasks**: 1. **Stripplot of Total Bill per Day**: - Create a `stripplot` showing the distribution of `total_bill` for each `day`. - Use different colors for each `day`. - Ensure the plot is horizontal. 2. **Stripplot with Hue**: - Create a `stripplot` showing the distribution of `total_bill` for each `day` with points colored based on `sex`. - Disable jittering of points. - Split the points for different `sex` categories to avoid overlap. 3. **Stripplot Customized**: - Create a `stripplot` showing the distribution of `total_bill` for each `day` with points colored based on `size`. - Use a specific palette of your choice for the colors. - Ensure the scale used for `size` is the native scale (represent the original values). 4. **Faceted Stripplot**: - Create a faceted plot using `catplot` that shows `total_bill` versus `time` for each `day`. - Use `sex` for coloring the points. - Arrange the facets horizontally with an aspect ratio of 0.5. **Constraints**: - Use seaborn for all visualizations. - Make sure the visualizations are clear and comprehensible. **Example Usage**: ```python import seaborn as sns import pandas as pd # Load the example dataset provided by seaborn tips = sns.load_dataset(\\"tips\\") visualize_tips_data(tips) ``` **Expected Output**: The function call should generate and display the following plots: 1. A horizontal strip plot of `total_bill` for each `day` with different colors. 2. A strip plot of `total_bill` for each `day` with points colored by `sex`, without jitter, and separated by `sex`. 3. A strip plot of `total_bill` for each `day` with points colored by `size` using a specified palette and native scale. 4. A faceted plot showing `total_bill` vs `time` for each `day` with points colored by `sex`. Note: The exact appearance of the plots depends on the data and customization choices.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_tips_data(data): Generate specific seaborn visualizations of the provided dataset. Parameters: - data (pd.DataFrame): A Pandas DataFrame containing the restaurant tips data. Returns: None (Plots should be displayed using seaborn) # Stripplot of Total Bill per Day plt.figure(figsize=(10, 6)) sns.stripplot(data=data, x=\'total_bill\', y=\'day\', jitter=True, palette=\'Set2\', dodge=True) plt.title(\'Stripplot of Total Bill per Day\') plt.show() # Stripplot with Hue (Sex) plt.figure(figsize=(10, 6)) sns.stripplot(data=data, x=\'total_bill\', y=\'day\', hue=\'sex\', jitter=False, dodge=True, palette=\'Dark2\') plt.title(\'Stripplot of Total Bill per Day with Hue (Sex)\') plt.show() # Stripplot Customized (Size) plt.figure(figsize=(10, 6)) sns.stripplot(data=data, x=\'total_bill\', y=\'day\', hue=\'size\', jitter=True, palette=\'viridis\', dodge=True) plt.title(\'Stripplot of Total Bill per Day with Size as Hue\') plt.show() # Faceted Stripplot g = sns.catplot(data=data, x=\'total_bill\', y=\'time\', hue=\'sex\', col=\'day\', kind=\'strip\', palette=\'coolwarm\', aspect=0.5) g.fig.suptitle(\'Faceted Stripplot: Total Bill vs Time for each Day with Sex Hue\') g.fig.subplots_adjust(top=0.9) # Adjust the top to fit the title plt.show()"},{"question":"Title: Implementing a Multi-threaded Counter with Synchronization Objective: Demonstrate your understanding of creating, managing, and synchronizing multiple threads using the `_thread` module in Python. Problem Statement: You are required to implement a multi-threaded counter that increments a global counter variable. The program should ensure that the counter value is incremented correctly using multiple threads. To achieve this, you must use the `_thread` module for threading and synchronization. Requirements: 1. Implement a function `thread_worker(counter_lock, increments, counter_list, index)` that: - Takes a lock object `counter_lock`, an integer `increments`, a list `counter_list` with a single integer element, and an integer `index` as parameters. - Increments the value in `counter_list[index]` by 1, `increments` number of times. - Uses the lock to ensure that increment operations are thread-safe. 2. Implement a function `main_threaded_counter(num_threads, increments_per_thread)` that: - Takes two integers `num_threads` and `increments_per_thread` as parameters. - Creates a shared counter (list with a single integer element initialized to 0). - Creates and starts `num_threads` threads, each executing the `thread_worker` function to increment the shared counter. - Waits for all threads to complete. - Returns the final value of the counter. Constraints: - You must use the `_thread` module for thread management and synchronization. - The counter should be incremented accurately as per the total increments specified (num_threads * increments_per_thread). - Use best practices for thread synchronization to avoid race conditions. Input and Output Formats: - Input: `num_threads` (int), `increments_per_thread` (int) - Output: Final counter value (int) Example: ```python def main_threaded_counter(num_threads, increments_per_thread): # Your implementation here # Example usage: num_threads = 5 increments_per_thread = 1000 result = main_threaded_counter(num_threads, increments_per_thread) print(result) # Expected Output: 5000 ``` Note: - The above example demonstrates creating 5 threads each incrementing the counter 1000 times, resulting in a final counter value of 5000 if thread synchronization is correctly implemented. Good luck!","solution":"import _thread import time def thread_worker(counter_lock, increments, counter_list, index): Worker function for each thread. Increments the value in the counter_list by 1 for the specified number of increments in a thread-safe manner using the provided lock. for _ in range(increments): with counter_lock: counter_list[index] += 1 def main_threaded_counter(num_threads, increments_per_thread): Main function to create and manage multiple threads to increment a counter. Ensures accurate final counter value by using thread synchronization. counter_lock = _thread.allocate_lock() counter_list = [0] thread_list = [] for i in range(num_threads): thread = _thread.start_new_thread(thread_worker, (counter_lock, increments_per_thread, counter_list, 0)) thread_list.append(thread) # Give threads time to complete time.sleep(1) return counter_list[0]"},{"question":"**Context:** You are provided with a list of pickle files, each containing serialized Python objects. Some of these pickle files are large and could benefit from optimization to reduce storage space and improve unpickling performance. Your task is to write a function that loads these pickle files, optimizes their contents using `pickletools`, and saves the optimized version back to the file system. **Task:** Implement a function `optimize_pickle_files(file_paths: List[str]) -> None` that: 1. Takes in a list of file paths (`file_paths`) where each path is a `.pickle` file. 2. Reads each pickle file, optimizes its contents using `pickletools.optimize`, and writes the optimized pickle back to the same file path. **Input:** - `file_paths` is a list of strings, where each string is a valid file path to a `.pickle` file. **Output:** - The function should return `None`. The optimized pickle files should be written directly to their respective paths. **Constraints:** - Each file in `file_paths` is guaranteed to exist and be a valid `.pickle` file. - The `pickle` files can contain any type of serializable Python objects. **Example Usage:** ```python file_paths = [\'data1.pickle\', \'data2.pickle\', \'data3.pickle\'] optimize_pickle_files(file_paths) ``` **Notes:** 1. Use `pickletools.optimize` to perform the optimization of the pickle content. 2. Make sure to handle the file I/O operations carefully, i.e., opening the file for reading in binary mode and writing the optimized content back in binary mode. 3. No exceptions need to be handled; you can assume that all input files are valid and operations succeed without errors. **Hints:** - You might find the `pickle` module useful for loading and saving the pickle files. - Remember to read and write the files in binary mode (\'rb\' and \'wb\').","solution":"import pickle import pickletools from typing import List def optimize_pickle_files(file_paths: List[str]) -> None: Optimizes the pickle files located at the given file paths using pickletools.optimize. Parameters: file_paths (List[str]): List of file paths to .pickle files that need to be optimized. Returns: None for file_path in file_paths: # Read the existing pickle file with open(file_path, \'rb\') as f: data = f.read() # Optimize the pickle content optimized_data = pickletools.optimize(data) # Write the optimized pickle back to the file with open(file_path, \'wb\') as f: f.write(optimized_data)"},{"question":"**Unicode String Operations** In this assessment, you are required to implement a function that performs a series of transformations on Unicode strings. Your function must utilize the Unicode APIs and codecs specified in Python 3.10 to achieve the desired results. # Function Specification ```python def transform_unicode_string(input_string: str) -> dict: Perform a series of transformations on the input Unicode string. Parameters: input_string (str): A UTF-8 encoded Unicode string. Returns: dict: A dictionary with keys: - \'original\': the original input string - \'upper\': the input string converted to upper case - \'lower\': the input string converted to lower case - \'title\': the input string converted to title case - \'as_utf16\': the input string encoded in UTF-16 - \'as_utf32\': the input string encoded in UTF-32 - \'contains_space\': a boolean indicating if the input string contains any whitespace characters # 1. Convert the input string to various cases # 2. Encode the input string to UTF-16 and UTF-32 # 3. Check if the input string contains any whitespace characters pass ``` # Input - `input_string`: A valid UTF-8 encoded Unicode string that can contain characters from any character set. # Output - The function returns a dictionary containing: - `\'original\'`: the original input string. - `\'upper\'`: the input string converted to upper case. - `\'lower\'`: the input string converted to lower case. - `\'title\'`: the input string converted to title case. - `\'as_utf16\'`: the input string encoded in UTF-16. - `\'as_utf32\'`: the input string encoded in UTF-32. - `\'contains_space\'`: a boolean indicating if the input string contains any whitespace characters. # Constraints - Utilize Python 3.10 Unicode APIs for encoding and conversions. - Ensure to handle exceptions gracefully and include appropriate error handling. # Example ```python input_string = \\"Hello World 123\\" result = transform_unicode_string(input_string) print(result) # Expected Output: # { # \'original\': \'Hello World 123\', # \'upper\': \'HELLO WORLD 123\', # \'lower\': \'hello world 123\', # \'title\': \'Hello World 123\', # \'as_utf16\': (b\'xffxfeHx00ex00lx00lx00ox00 x00Wx00ox00rx00lx00dx00 x001x002x003x00\'), # \'as_utf32\': (b\'xffxfex00x00Hx00x00x00ex00x00x00lx00x00x00lx00x00x00ox00x00x00 x00\' # b\'x00x00Wx00x00x00ox00x00x00rx00x00x00lx00x00x00dx00x00x00 x00x00x001\' # b\'x00x00x002x00x00x003x00x00x00\'), # \'contains_space\': True # } ``` **Note:** - Encoding results (`as_utf16` and `as_utf32`) should be shown in byte format. - The function should handle input strings without any embedded null characters or invalid Unicode data.","solution":"def transform_unicode_string(input_string: str) -> dict: Perform a series of transformations on the input Unicode string. Parameters: input_string (str): A UTF-8 encoded Unicode string. Returns: dict: A dictionary with keys: - \'original\': the original input string - \'upper\': the input string converted to upper case - \'lower\': the input string converted to lower case - \'title\': the input string converted to title case - \'as_utf16\': the input string encoded in UTF-16 - \'as_utf32\': the input string encoded in UTF-32 - \'contains_space\': a boolean indicating if the input string contains any whitespace characters # 1. Convert the input string to various cases upper_string = input_string.upper() lower_string = input_string.lower() title_string = input_string.title() # 2. Encode the input string to UTF-16 and UTF-32 utf16_string = input_string.encode(\'utf-16\') utf32_string = input_string.encode(\'utf-32\') # 3. Check if the input string contains any whitespace characters contains_space = any(char.isspace() for char in input_string) return { \'original\': input_string, \'upper\': upper_string, \'lower\': lower_string, \'title\': title_string, \'as_utf16\': utf16_string, \'as_utf32\': utf32_string, \'contains_space\': contains_space }"},{"question":"**Question: Custom SVM Kernel Implementation and Multi-Class Classification** In this coding exercise, you will create a custom kernel function for SVM in scikit-learn and use it to train an SVM model on a multi-class classification problem. You will also evaluate the model\'s performance using cross-validation. # Specifications: 1. **Custom Kernel Function**: - Create a custom kernel function `my_custom_kernel(X, Y)` that computes the polynomial kernel: ((X cdot Y^T + 1)^d), where (d) is the degree of the polynomial. 2. **SVM Model Training**: - Use the `make_classification` function from `sklearn.datasets` to generate a synthetic dataset with 3 classes, 1000 samples, and 20 features. - Split the dataset into training and testing sets using an 80-20 split. 3. **Model Fitting**: - Implement an SVM model using your custom kernel function, setting the degree (d = 3). - Fit the model on the training data. 4. **Model Evaluation**: - Use cross-validation (with 5 folds) to evaluate the model\'s performance, and report the mean accuracy. # Constraints: - Ensure that your implementation deals with the multi-class nature of the data. - You may use any additional relevant scikit-learn functionalities as required. # Input Format: - No input is taken from the user; the function should work within the provided dataset generation and split. # Output Format: - Print the mean accuracy of the SVM model based on cross-validation. ```python import numpy as np from sklearn import svm from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split, cross_val_score def my_custom_kernel(X, Y): Polynomial Kernel function: (X * Y.T + 1)^d Args: X (np.array): Features of the first set of samples. Y (np.array): Features of the second set of samples. Returns: np.array: Kernel matrix. d = 3 # Degree of the polynomial kernel return (np.dot(X, Y.T) + 1) ** d # Create a synthetic dataset with 3 classes, 1000 samples, and 20 features X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=3, random_state=42) # Split the dataset into training (80%) and testing (20%) sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement and fit the SVM model using the custom kernel function clf = svm.SVC(kernel=my_custom_kernel) clf.fit(X_train, y_train) # Evaluate the model\'s performance using cross-validation scores = cross_val_score(clf, X, y, cv=5) # Print the mean accuracy of the SVM model print(\\"Mean accuracy:\\", scores.mean()) ``` # Note: This question assesses your understanding of SVM models in scikit-learn, custom kernel implementation, multi-class classification, and model evaluation using cross-validation.","solution":"import numpy as np from sklearn import svm from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split, cross_val_score def my_custom_kernel(X, Y): Polynomial Kernel function: (X * Y.T + 1)^d Args: X (np.array): Features of the first set of samples. Y (np.array): Features of the second set of samples. Returns: np.array: Kernel matrix. d = 3 # Degree of the polynomial kernel return (np.dot(X, Y.T) + 1) ** d # Create a synthetic dataset with 3 classes, 1000 samples, and 20 features X, y = make_classification( n_samples=1000, n_features=20, n_informative=10, n_classes=3, random_state=42 ) # Split the dataset into training (80%) and testing (20%) sets X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # Implement and fit the SVM model using the custom kernel function clf = svm.SVC(kernel=my_custom_kernel) clf.fit(X_train, y_train) # Evaluate the model\'s performance using cross-validation scores = cross_val_score(clf, X, y, cv=5) # Print the mean accuracy of the SVM model print(\\"Mean accuracy:\\", scores.mean())"},{"question":"# PyTorch Coding Assessment Question **Objective:** Implement a function `tensor_info` that takes a PyTorch tensor as input and returns a dictionary containing its shape, number of elements, and size of each dimension. This task will assess your understanding of `torch.Size` and tensor manipulation in PyTorch. **Function Signature:** ```python def tensor_info(tensor: torch.Tensor) -> dict: pass ``` **Input:** - A single `torch.Tensor` object. **Output:** - A `dict` containing: - \\"shape\\": a list representing the size of each dimension of the tensor. - \\"num_elements\\": an integer representing the total number of elements in the tensor. - \\"dimensions\\": a list of integers representing the size of each dimension. **Constraints:** - The input tensor can be of any valid PyTorch tensor shape, including multi-dimensional tensors. **Example:** ```python import torch x = torch.ones(10, 20, 30) result = tensor_info(x) print(result) # Output: # { # \\"shape\\": [10, 20, 30], # \\"num_elements\\": 6000, # \\"dimensions\\": [10, 20, 30] # } ``` **Notes:** - Use the `torch.Size` class to obtain the necessary information about the tensor. - Ensure your function accurately returns the required information in the specified format. **Hints:** - Use the `size()` method of the tensor to get the `torch.Size` object. - Use sequence operations to manipulate and extract information from `torch.Size`.","solution":"import torch def tensor_info(tensor: torch.Tensor) -> dict: Returns information about the input tensor including its shape, number of elements, and size of each dimension. shape = list(tensor.size()) num_elements = tensor.numel() dimensions = shape # As `size()` already gives the size of each dimension return { \\"shape\\": shape, \\"num_elements\\": num_elements, \\"dimensions\\": dimensions }"},{"question":"# Question: Flight Data Visualization with Seaborn You are given the task to analyze and visualize historical flight data using the seaborn library. The dataset records the number of airline passengers who flew in each month from 1949 to 1960. Your tasks are as follows: 1. **Load the Data**: Load the \\"flights\\" dataset using seaborn. 2. **Data Manipulation**: Transform the dataset into wide-form where each column represents a month\'s time series over the years. 3. **Plotting**: - Using the long-form data, create a line plot to show the number of passengers for each month. Differentiate the months using colors. - Using the wide-form data, create a line plot and assign columns of the dataset to the dimensions of the plot. Ensure the plot is appropriately labeled. - Transform the data to show the average number of passengers per year and plot this data using seaborn. 4. **Additional Visualization**: - Use any advanced seaborn functionality to create a plot of your choice (e.g., box plot, cat plot) that gives more insights into the flight data. Expected Input and Output Format - **Input**: None. You are to load the dataset using seaborn. - **Output**: Four plots generated from the seaborn library. Constraints: - The dataset will be loaded directly from seaborns `load_dataset` function. - Ensure the plots are well-labeled and provide a clear visual representation. Performance Requirements: - The code should execute efficiently without excessive memory usage. - Ensure data transformations are done using pandas functions efficiently. Example Solution ```python import seaborn as sns import pandas as pd # Load the dataset flights = sns.load_dataset(\\"flights\\") # Part 1: Wide-form transformation flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Part 2: Plotting long-form data sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\").set(title=\'Monthly passengers over years\') # Part 2: Plotting wide-form data sns.relplot(data=flights_wide, kind=\\"line\\").set(title=\'Monthly passengers over years (wide-form)\') # Part 3: Average passengers per year flights_avg = flights.groupby(\\"year\\").mean(numeric_only=True) sns.relplot(data=flights_avg, x=\\"year\\", y=\\"passengers\\", kind=\\"line\\").set(title=\'Average passengers per year\') # Part 4: Additional plot - Example: Box plot for distributions per month sns.catplot(data=flights, x=\\"month\\", y=\\"passengers\\", kind=\\"box\\").set(title=\'Passenger count distribution per month\') sns.plt.show() ``` **Notes:** - Remember to style your plots for better visual comprehension. - Refer to seaborn or pandas documentation if you need additional functionalities.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def analyze_flight_data(): # Load the dataset flights = sns.load_dataset(\\"flights\\") # Part 1: Wide-form transformation flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Part 2: Plotting long-form data plt.figure(figsize=(10, 6)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") plt.title(\'Monthly passengers over years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\') plt.show() # Part 2: Plotting wide-form data plt.figure(figsize=(10, 6)) sns.lineplot(data=flights_wide) plt.title(\'Monthly passengers over years (wide-form)\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\') plt.show() # Part 3: Average passengers per year flights_avg = flights.groupby(\\"year\\").mean(numeric_only=True).reset_index() plt.figure(figsize=(10, 6)) sns.lineplot(data=flights_avg, x=\\"year\\", y=\\"passengers\\") plt.title(\'Average passengers per year\') plt.xlabel(\'Year\') plt.ylabel(\'Average Number of Passengers\') plt.show() # Part 4: Additional plot (Box plot for distributions per month) plt.figure(figsize=(12, 6)) sns.boxplot(data=flights, x=\\"month\\", y=\\"passengers\\") plt.title(\'Passenger count distribution per month\') plt.xlabel(\'Month\') plt.ylabel(\'Number of Passengers\') plt.show() # Call the function to generate plots analyze_flight_data()"},{"question":"# Web Interaction and Data Handling with `urllib.request` **Question Objective:** Design and implement a function that performs several tasks, simulating a real-world application where URL fetching, data handling, and error management using the Python `urllib.request` module are essential. **Problem Statement:** You are tasked with creating a Python function `fetch_and_handle_url` that takes in a URL and a data dictionary. The function should perform the following: 1. **Fetch the URL:** - If the URL is an HTTP URL, fetch the URL using the GET method if no data is provided. If data is provided, use the POST method to send the data as form-encoded. - If the URL is an FTP URL, simply fetch the document. 2. **Handle common exceptions:** - If the server cannot be reached, or the URL is incorrect, catch `URLError` and return an appropriate message. - If the server responds with an HTTP error, catch `HTTPError` and return the HTTP status code and error message. 3. **Custom User-Agent:** - Simulate the request as coming from a custom user agent: `Mozilla/5.0 (Windows NT 10.0; Win64; x64) Python-urllib`. 4. **Return the fetched content:** - If the request is successful, return the response content. - Ensure that the content returned is always in a decoded string format (for example, if the response is in bytes, decode it to a string). # Function Signature: ```python def fetch_and_handle_url(url: str, data: dict = None) -> str: pass ``` # Input: - `url` (string): The URL to fetch. - `data` (dictionary, optional): Data to be sent with the request (default is `None`). # Output: - A string containing the response content if successful, or an appropriate error message if an exception occurs. # Constraints: - Use only the `urllib.request` module for fetching URLs. - Handle the encoding for any data being sent with POST requests using `urllib.parse.urlencode`. - Assume the system has network connectivity to the specified URL. # Example Usages: ```python # Example 1: Fetch using GET request without data result = fetch_and_handle_url(\\"http://www.example.com\\") print(result) # Example 2: Fetch using POST request with form data data = {\'key1\': \'value1\', \'key2\': \'value2\'} result = fetch_and_handle_url(\\"http://www.example.com/form_submit\\", data) print(result) # Example 3: Handle invalid URL result = fetch_and_handle_url(\\"http://www.invalid-url.com\\") print(result) # Output: \\"Failed to reach the server: <reason>\\" # Example 4: Handle HTTP error result = fetch_and_handle_url(\\"http://www.example.com/404\\") print(result) # Output: \\"Server couldn\'t fulfill the request. Error code: 404\\" ``` You should consider edge cases, including invalid URLs and unreachable servers, and test your function accordingly.","solution":"import urllib.request import urllib.parse def fetch_and_handle_url(url: str, data: dict = None) -> str: try: headers = {\'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Python-urllib\'} if data: data = urllib.parse.urlencode(data).encode() req = urllib.request.Request(url, data=data, headers=headers) else: req = urllib.request.Request(url, headers=headers) with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\\"Server couldn\'t fulfill the request. Error code: {e.code}, reason: {e.reason}\\" except urllib.error.URLError as e: return f\\"Failed to reach the server: {e.reason}\\""},{"question":"Objective: You are required to create a Python script that extends the functionality of the `tabnanny` module. Specifically, you will write a function that checks for ambiguous indentation in a list of files and logs the output to a specified log file instead of the standard output. Function Specification: **Function Name:** `check_and_log(file_list, log_file, verbose=False, filename_only=False)` **Input:** - `file_list`: A list of strings where each string is a path to a `.py` file or a directory containing `.py` files. Example: `[\'script1.py\', \'src/\']` - `log_file`: A string representing the path to the log file where the output should be written. Example: `\'log.txt\'` - `verbose`: A boolean flag indicating if verbose messages should be logged. Default is `False`. - `filename_only`: A boolean flag indicating if only filenames with problems should be logged. Default is `False`. **Output:** - The function does not return any value. It logs the output to the specified `log_file`. **Constraints and Requirements:** - The function should handle both directory and file inputs, checking each `.py` file within directories recursively. - If `verbose` is `True`, the function should include detailed messages in the log file. - If `filename_only` is `True`, the function should only log the names of the files containing ambiguous indentation issues. - You must handle the `tabnanny.NannyNag` exception appropriately and log relevant error messages. - Make sure to follow good coding practices such as using `with` statements for file handling. Example Usage: ```python # Example use case with a list of files and directories file_list = [\'script1.py\', \'script2.py\', \'src/\'] log_file = \'indentation_log.txt\' check_and_log(file_list, log_file, verbose=True, filename_only=False) ``` This should create or append to `indentation_log.txt` with the appropriate diagnostic messages. Implementation Note: You can make use of the `tabnanny.check` function and other provided features, but you need to ensure that outputs are redirected to the log file as specified.","solution":"import os import tabnanny import sys def check_and_log(file_list, log_file, verbose=False, filename_only=False): class LogRedirector: def __init__(self, file): self.file = file def write(self, msg): self.file.write(msg) def flush(self): pass def process_path(path): if os.path.isdir(path): for root, _, files in os.walk(path): for file in files: if file.endswith(\'.py\'): check_file(os.path.join(root, file)) elif os.path.isfile(path) and path.endswith(\'.py\'): check_file(path) def check_file(file_path): try: tabnanny.check(file_path) except tabnanny.NannyNag as e: if filename_only: log.write(f\'{file_path}n\') else: log.write(f\'{file_path}: {e}n\') else: if verbose: log.write(f\'{file_path}: No issues found.n\') with open(log_file, \'a\') as log: original_stdout = sys.stdout sys.stdout = LogRedirector(log) try: for item in file_list: process_path(item) finally: sys.stdout = original_stdout"},{"question":"You are given a dataset representing various household electric power consumption measurements. This dataset includes features such as voltage, intensity, and other time-based measurements. Your task is to build a multioutput regression model to predict two continuous target variables: Global active power (`GAP`) and Global reactive power (`GRP`). # Objectives 1. Load and preprocess the dataset. 2. Implement multioutput regression using the `MultiOutputRegressor` meta-estimator. 3. Train the model with the given dataset. 4. Evaluate the model\'s performance using appropriate metrics. 5. Visualize the predictions versus actual values. # Dataset The dataset is provided as a CSV file: `household_power_consumption.csv`. Each row corresponds to a time point with various features and targets. The dataset has the following columns: - `voltage` - `intensity` - `sub_metering_1` - `sub_metering_2` - `sub_metering_3` - `GAP` (Global active power, target variable) - `GRP` (Global reactive power, target variable) # Requirements 1. **Function Signature:** ```python def multioutput_regression(path_to_csv: str) -> None: pass ``` 2. **Function Input:** - `path_to_csv`: A string representing the file path to the CSV dataset. 3. **Constraints:** - Use `MultiOutputRegressor` with `RandomForestRegressor` as the base estimator. - Implement train-test split with 80% of the data used for training and 20% for testing. - Ensure your code handles preprocessing steps like handling missing values and scaling features if necessary. 4. **Performance Requirements:** - Report Mean Squared Error (MSE) and R^2 scores for both `GAP` and `GRP` on the test set. 5. **Visualization:** - Plot predicted vs actual values for both `GAP` and `GRP` on the test set. # Example Usage ```python multioutput_regression(\'path/to/household_power_consumption.csv\') ``` # Notes - Make sure to handle missing values appropriately. - Apply any necessary feature scaling. - Provide clear inline comments explaining your code logic. - Ensure all your results (metrics and plots) are clearly presented.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.multioutput import MultiOutputRegressor from sklearn.metrics import mean_squared_error, r2_score import matplotlib.pyplot as plt import numpy as np def multioutput_regression(path_to_csv: str) -> None: # Load dataset df = pd.read_csv(path_to_csv) # Handle missing values df = df.dropna() # Define features and targets X = df[[\'voltage\', \'intensity\', \'sub_metering_1\', \'sub_metering_2\', \'sub_metering_3\']] y = df[[\'GAP\', \'GRP\']] # Split the dataset into train and test sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the base estimator base_estimator = RandomForestRegressor(random_state=42) # Initialize the MultiOutputRegressor with the base estimator multi_output_regressor = MultiOutputRegressor(base_estimator, n_jobs=-1) # Train the model multi_output_regressor.fit(X_train, y_train) # Make predictions y_pred = multi_output_regressor.predict(X_test) # Evaluate the model\'s performance mse_gap = mean_squared_error(y_test[\'GAP\'], y_pred[:, 0]) mse_grp = mean_squared_error(y_test[\'GRP\'], y_pred[:, 1]) r2_gap = r2_score(y_test[\'GAP\'], y_pred[:, 0]) r2_grp = r2_score(y_test[\'GRP\'], y_pred[:, 1]) print(f\\"Mean Squared Error (GAP): {mse_gap}\\") print(f\\"Mean Squared Error (GRP): {mse_grp}\\") print(f\\"R2 Score (GAP): {r2_gap}\\") print(f\\"R2 Score (GRP): {r2_grp}\\") # Visualize the predictions vs actual values fig, axs = plt.subplots(1, 2, figsize=(14, 7)) axs[0].scatter(y_test[\'GAP\'], y_pred[:, 0], alpha=0.5) axs[0].plot([y_test[\'GAP\'].min(), y_test[\'GAP\'].max()], [y_test[\'GAP\'].min(), y_test[\'GAP\'].max()], \'r--\') axs[0].set_title(\'GAP: Predicted vs Actual\') axs[0].set_xlabel(\'Actual Values\') axs[0].set_ylabel(\'Predicted Values\') axs[1].scatter(y_test[\'GRP\'], y_pred[:, 1], alpha=0.5) axs[1].plot([y_test[\'GRP\'].min(), y_test[\'GRP\'].max()], [y_test[\'GRP\'].min(), y_test[\'GRP\'].max()], \'r--\') axs[1].set_title(\'GRP: Predicted vs Actual\') axs[1].set_xlabel(\'Actual Values\') axs[1].set_ylabel(\'Predicted Values\') plt.tight_layout() plt.show()"},{"question":"**Python Cell Objects Challenge** In Python, sometimes variables are referenced in multiple scopes, particularly within nested functions. Python handles these variable references using what are called `cell` objects. Your task is to implement a simplified version of these `cell` object functionalities in pure Python. # Problem Statement Implement a class `Cell` that manages a variable similar to the C-API\'s cell objects, with the following methods: 1. `__init__(self, value)`: Initializes the cell object with a given value. 2. `get(self) -> any`: Returns the current value of the cell. 3. `set(self, value: any) -> None`: Sets the value of the cell to the given value. 4. `check(cell) -> bool`: Static method that returns `True` if the given object is a `Cell` object, `False` otherwise. # Constraints - You should not use any external libraries. - Consider performance in your implementation - aim for `O(1)` complexity for both `get` and `set` operations. - Your solution should be compatible with Python 3.10. # Example Usage ```python # Create a new cell object with initial value of 10 c = Cell(10) assert c.get() == 10 # Should return 10 # Setting a new value c.set(20) assert c.get() == 20 # Should return 20 # Checking cell type assert Cell.check(c) == True # Should return True assert Cell.check(123) == False # Should return False ``` # Additional Requirements: 1. Handle edge cases where `None` is provided as a value for initialization or setting. 2. Ensure the class methods handle inappropriate types gracefully, raising appropriate exceptions if required. Implement the `Cell` class with the specified methods and constraints. Include test cases to demonstrate the functionality and reliability of your implementation.","solution":"class Cell: def __init__(self, value=None): Initializes the cell object with a given value. self._value = value def get(self): Returns the current value of the cell. return self._value def set(self, value): Sets the value of the cell to the given value. self._value = value @staticmethod def check(obj): Static method that returns True if the given object is a Cell object, False otherwise. return isinstance(obj, Cell)"},{"question":"Objective Design a Python function that makes an HTTP GET request to a given URL, parses the fetched HTML content for all the hyperlinks (`<a>` tags), and returns a list of URLs found. This will test your ability to use `urllib.request` to make network requests and `urllib.parse` to handle URL components. Requirements 1. Implement the function `fetch_and_parse_urls(url: str) -> List[str]`. 2. Use `urllib.request` to fetch the HTML content of the provided URL. 3. Parse the HTML content to extract all hyperlinks (`<a href=\\"...\\">` tags). 4. Return a list of URLs found in the HTML content. Constraints - Assume the provided URL is valid and the server will return a valid HTML response. - The function should handle only `http` and `https` schemes. - You should handle any potential exceptions that may occur during the request. Input - A single string `url` representing the URL to fetch. Output - A list of strings, each representing a URL found in the HTML content. Example ```python def fetch_and_parse_urls(url: str) -> List[str]: # Your implementation here # Example usage: result = fetch_and_parse_urls(\\"http://example.com\\") print(result) # Output: [\'http://example.com\', \'http://example.com/moreinfo\'] ``` Additional Information - You are allowed to use the `re` (regular expressions) module for parsing the HTML if necessary. - Provide clear docstrings and comments to explain your implementation. - Use good programming practices, such as error handling and logging. Good luck!","solution":"import urllib.request import urllib.parse from typing import List import re def fetch_and_parse_urls(url: str) -> List[str]: Fetches the HTML content from the given URL and parses it for hyperlinks. Args: url (str): The URL to fetch content from. Returns: List[str]: A list of URLs found within the HTML content. try: # Perform a GET request to fetch the HTML content with urllib.request.urlopen(url) as response: html_content = response.read().decode(\'utf-8\') # Use regular expressions to find all hyperlinks urls = re.findall(r\'href=[\\"\'](http[s]?://[^\\"\'>]*)[\\"\']\', html_content) return urls except Exception as e: print(f\\"An error occurred: {e}\\") return [] # Example usage: # result = fetch_and_parse_urls(\\"http://example.com\\") # print(result) # Output example: [\'http://example.com\', \'http://example.com/moreinfo\']"},{"question":"You are required to demonstrate your understanding of the `zipimport` module by creating a script that performs the following tasks: 1. **Create a ZIP file**: Create a ZIP file that contains a simple Python module named `example_module.py`. This module should contain a function called `greet` that returns the string `\\"Hello from the module!\\"`. 2. **Create a zipimporter instance**: Write a function called `create_zip_importer` that takes in the path to the ZIP file (as created in step 1) and returns a `zipimport.zipimporter` instance. 3. **Import the module**: Using the `zipimporter` instance, import the `example_module` and call its `greet` function. 4. **Retrieve Module Information**: Implement the following methods using your `zipimporter` instance: - `get_module_code`: This function should return the code object of the `example_module`. - `get_module_filename`: This function should return the filename of the `example_module`. - `get_module_source`: This function should return the source code of the `example_module`. # Functional Specifications Function: create_zip_importer **Input:** - `zip_file_path` (str): The path to the ZIP file. **Output:** - Returns a `zipimport.zipimporter` instance. Function: import_module **Input:** - `importer` (zipimport.zipimporter): An instance of the `zipimporter` class. - `module_name` (str): The name of the module to be imported (e.g., `example_module`). **Output:** - Returns the imported module. Function: get_module_code **Input:** - `importer` (zipimport.zipimporter): An instance of the `zipimporter` class. - `module_name` (str): The name of the module (e.g., `example_module`). **Output:** - Returns the code object of the module. Function: get_module_filename **Input:** - `importer` (zipimport.zipimporter): An instance of the `zipimporter` class. - `module_name` (str): The name of the module (e.g., `example_module`). **Output:** - Returns the filename of the module. Function: get_module_source **Input:** - `importer` (zipimport.zipimporter): An instance of the `zipimporter` class. - `module_name` (str): The name of the module (e.g., `example_module`). **Output:** - Returns the source code of the module as a string. # Constraints - Ensure that the created ZIP file structure and module content are correctly set up to be imported by the `zipimport` module. - Handle errors appropriately, raising descriptive error messages in case of failures (e.g., file not found in ZIP, module not found). # Performance Requirements - The code should efficiently handle the import process, minimizing any unnecessary operations. # Example Usage ```python # Step 1: Create the ZIP file named \'modules.zip\' with `example_module.py` inside it. # Step 2: Create a zipimporter instance importer = create_zip_importer(\'modules.zip\') # Step 3: Import the module example_module = import_module(importer, \'example_module\') print(example_module.greet()) # Output: Hello from the module! # Step 4: Retrieve module information code_object = get_module_code(importer, \'example_module\') filename = get_module_filename(importer, \'example_module\') source_code = get_module_source(importer, \'example_module\') print(code_object) # Outputs the code object of the module print(filename) # Outputs: \'modules.zip/example_module.py\' print(source_code) # Outputs the source code of the module ```","solution":"import zipfile import zipimport def create_example_zip(zip_file_name): Creates a ZIP file containing example_module.py with a greet function. module_content = def greet(): return \\"Hello from the module!\\" with zipfile.ZipFile(zip_file_name, \'w\') as zf: zf.writestr(\\"example_module.py\\", module_content) def create_zip_importer(zip_file_path): Takes in the path to the ZIP file and returns a zipimport.zipimporter instance. return zipimport.zipimporter(zip_file_path) def import_module(importer, module_name): Uses the zipimporter instance to import the module. module = importer.load_module(module_name) return module def get_module_code(importer, module_name): Returns the code object of the module. return importer.get_code(module_name) def get_module_filename(importer, module_name): Returns the filename of the module. return importer.get_filename(module_name) def get_module_source(importer, module_name): Returns the source code of the module. return importer.get_source(module_name)"},{"question":"**Question: Email Parsing and Exception Handling** You are tasked with implementing an email parsing utility that reads raw email messages and identifies potential issues based on the specifications provided in the \\"email.errors\\" module. Your implementation should simulate the behavior of a simplified email parser and handle specific exceptions and defects. # Requirements 1. Implement a class `SimpleEmailParser` with the following methods: - `__init__(self, email_content: str)`: Constructor that initializes the parser with the raw email content. - `parse(self)`: This method should parse the email content and raise the appropriate exceptions or defects if issues are found. 2. Implement the following exceptions and defects as outlined in the \\"email.errors\\" documentation: - `MessageError` - `MessageParseError` - `HeaderParseError` - `MultipartConversionError` - Defect classes (`NoBoundaryInMultipartDefect`, `StartBoundaryNotFoundDefect`, `CloseBoundaryNotFoundDefect`, `FirstHeaderLineIsContinuationDefect`, `MisplacedEnvelopeHeaderDefect`, `MissingHeaderBodySeparatorDefect`, `InvalidBase64PaddingDefect`, etc.) 3. Your `SimpleEmailParser` class should: - Raise `HeaderParseError` if it encounters a header parsing error. - Raise `MultipartConversionError` when a non-multipart message attempts to contain multiple parts. - Add defects to the `defects` attribute of the message for issues like missing or misplaced boundaries, invalid base64 encoding, etc. # Example Usage ```python try: raw_email = Subject: Test email This is a test message with an invalid base64 encoding. parser = SimpleEmailParser(raw_email) parser.parse() except email.errors.MessageError as e: print(f\\"Message Error occurred: {str(e)}\\") # Output could include detected defects or raised exceptions ``` # Constraints - You cannot use any external libraries for email parsing; you must implement the parser logic yourself. - Focus on handling the exceptions and defects as specified, regardless of the actual content correctness. # Evaluation Criteria - Correct implementation of exception classes. - Proper parsing logic that raises appropriate exceptions and defects. - Code readability and organization. - Handling edge cases and robust error detection.","solution":"from typing import List # Define custom exception classes as per the requirements class MessageError(Exception): pass class MessageParseError(MessageError): pass class HeaderParseError(MessageParseError): pass class MultipartConversionError(MessageParseError): pass # Define custom defect classes class Defect: pass class NoBoundaryInMultipartDefect(Defect): pass class StartBoundaryNotFoundDefect(Defect): pass class CloseBoundaryNotFoundDefect(Defect): pass class FirstHeaderLineIsContinuationDefect(Defect): pass class MisplacedEnvelopeHeaderDefect(Defect): pass class MissingHeaderBodySeparatorDefect(Defect): pass class InvalidBase64PaddingDefect(Defect): pass class SimpleEmailParser: def __init__(self, email_content: str): self.email_content = email_content self.defects: List[Defect] = [] def parse(self): self._parse_headers() self._parse_body() def _parse_headers(self): # Simulate header parsing logic if \\"INVALID_HEADER\\" in self.email_content: raise HeaderParseError(\\"Header parsing error encountered.\\") if \\"MISSING_BOUNDARY\\" in self.email_content: self.defects.append(NoBoundaryInMultipartDefect()) def _parse_body(self): # Simulate body parsing logic for defects if \\"MISSING_BOUNDARY_IN_BODY\\" in self.email_content: self.defects.append(StartBoundaryNotFoundDefect()) if \\"INVALID_BASE64\\" in self.email_content: self.defects.append(InvalidBase64PaddingDefect()) if \\"MULTIPART_ERROR\\" in self.email_content: raise MultipartConversionError(\\"Incorrect multipart message format.\\")"},{"question":"# Question: Analyzing and Optimizing Pickle Data You are given a Python module named `pickletools`, which provides tools for disassembling and optimizing pickled data. Your task is to implement a function that takes a pickled string, disassembles it to show the opcodes with annotations, and returns an optimized version of the pickle string. **Function Specifications:** Function 1: `disassemble_pickle` - **Input**: `pickle_string` (a bytes object representing a pickled Python object) - **Output**: `disassembled` (a list of strings, each representing a disassembled line with annotations) Function 2: `optimize_pickle` - **Input**: `pickle_string` (a bytes object representing a pickled Python object) - **Output**: `optimized_pickle` (a bytes object, the optimized version of the input pickle string) **Constraints**: * Use the `pickletools` module for disassembly and optimization. * Assume the input pickle string is correctly formatted. **Example**: ```python import pickle import pickletools def disassemble_pickle(pickle_string): import io buffer = io.StringIO() pickletools.dis(pickle_string, out=buffer, annotate=1) disassembled = buffer.getvalue().splitlines() return disassembled def optimize_pickle(pickle_string): optimized_pickle = pickletools.optimize(pickle_string) return optimized_pickle # Example usage # Create a pickled string data = (1, 2, [\'a\', \'b\']) pickle_string = pickle.dumps(data) # Disassemble the pickle disassembled = disassemble_pickle(pickle_string) for line in disassembled: print(line) # Optimize the pickle optimized_pickle_string = optimize_pickle(pickle_string) print(optimized_pickle_string) # Disassemble the optimized pickle disassembled_optimized = disassemble_pickle(optimized_pickle_string) for line in disassembled_optimized: print(line) ``` **Explanation**: 1. `disassemble_pickle` takes a pickled string, disassembles it using `pickletools.dis`, and returns a list of annotated disassembly lines. 2. `optimize_pickle` takes a pickled string and returns an optimized version using `pickletools.optimize`. The example shows how to use the provided functions to disassemble and optimize a pickled data string.","solution":"import pickletools def disassemble_pickle(pickle_string): Disassemble a pickled string and return a list of annotated disassembly lines. Parameters: pickle_string (bytes): A pickled Python object. Returns: list: A list of strings, each representing a disassembled line with annotations. import io buffer = io.StringIO() pickletools.dis(pickle_string, out=buffer, annotate=1) disassembled = buffer.getvalue().splitlines() return disassembled def optimize_pickle(pickle_string): Optimize a pickled string. Parameters: pickle_string (bytes): A pickled Python object. Returns: bytes: An optimized version of the input pickled string. optimized_pickle = pickletools.optimize(pickle_string) return optimized_pickle"},{"question":"**Question: Implement a Debugging Tool Using `linecache`** You are asked to implement a lightweight debugging tool in Python that inspects source files and retrieves specific lines of code. The tool should use the `linecache` module for efficient line retrieval and should provide the following functionalities: 1. **Retrieve Line**: Given a filename and a line number, retrieve the corresponding line from the file. 2. **Retrieve Multiple Lines**: Given a filename and a list of line numbers, retrieve the corresponding lines from the file. 3. **Check and Clear Cache**: Ability to check if the cache is up-to-date and to clear the cache if necessary. # Specifications: 1. **Function `retrieve_line(filename: str, lineno: int) -> str`**: - Input: - `filename`: A string representing the path to the source file. - `lineno`: An integer representing the line number to retrieve. - Output: - A string of the specified line, including the terminating newline character (if any). - Return an empty string if the line or file doesn\'t exist. 2. **Function `retrieve_multiple_lines(filename: str, linenos: list[int]) -> list[str]`**: - Input: - `filename`: A string representing the path to the source file. - `linenos`: A list of integers each representing a line number to retrieve. - Output: - A list of strings for each of the specified line numbers. - If a line doesn\'t exist, the corresponding element in the list should be an empty string. 3. **Function `reset_cache()`**: - Clears the `linecache` module\'s internal cache to ensure any subsequent line retrieval reflects the current file content. 4. **Function `validate_cache(filename: str) -> None`**: - Checks if the cached line for the specified filename is valid. If no filename is provided, it checks the validity of the cache for all the files. - Utilize `linecache.checkcache(filename)`. # Constraints: - You must use the `linecache` module provided by the Python standard library. - Ensure proper handling of exceptions and edge cases (e.g., file not found, invalid line numbers). - The functions should be efficient and leverage caching where appropriate. # Example: ```python # Example Usage print(retrieve_line(\'example.py\', 10)) # Should output the 10th line of example.py print(retrieve_multiple_lines(\'example.py\', [5, 10, 15])) # Should output the 5th, 10th, and 15th lines of example.py in a list reset_cache() # This clears the internal cache validate_cache(\'example.py\') # This ensures the cache for \'example.py\' is current ``` Implement these functions to create a simple, yet efficient, debugging tool for inspecting Python source files.","solution":"import linecache def retrieve_line(filename: str, lineno: int) -> str: Retrieve a specific line from a file. Parameters: - filename: The path to the source file. - lineno: The line number to retrieve. Returns: - The content of the line including newline character, or an empty string if the line or file doesn\'t exist. try: line = linecache.getline(filename, lineno) return line except Exception as e: return \\"\\" def retrieve_multiple_lines(filename: str, linenos: list[int]) -> list[str]: Retrieve specific lines from a file. Parameters: - filename: The path to the source file. - linenos: List of line numbers to retrieve. Returns: - A list of strings corresponding to the lines requested. Empty string for each line that doesn\'t exist. lines = [] for lineno in linenos: lines.append(retrieve_line(filename, lineno)) return lines def reset_cache(): Clear the linecache\'s internal cache. linecache.clearcache() def validate_cache(filename: str = None): Check the validity of the linecache for a specific file or all files. Parameters: - filename: The path to the source file. If None, the cache for all files is checked. linecache.checkcache(filename)"},{"question":"# Combine the Power of Pipelines and Ensemble Models with scikit-learn Objective: Design a complex machine learning model using the following techniques and tools from scikit-learn: - Custom transformers. - Pipeline. - ColumnTransformer. - Ensemble methods (e.g., VotingClassifier). Problem Statement: You are provided with a dataset with the following properties: - The dataset contains numerical and categorical features. - The target column is binary (0 or 1). Your task is to build a machine learning model that: 1. Preprocesses numerical features by scaling them. 2. Preprocesses categorical features by applying one-hot encoding. 3. Combines the preprocessed features using a ColumnTransformer. 4. Uses the preprocessed features in a Pipeline with at least two ensemble classifiers combined using a VotingClassifier. Input and Output: - **Input**: Pandas DataFrame `df` with numerical features in `numerical_features` list and categorical features in `categorical_features` list. Target column is named `target`. - **Output**: A prediction function `predict` that accepts a DataFrame with the same structure as the input DataFrame (excluding the target column) and returns predictions for the target. Constraints: - You must use `Pipeline` and `ColumnTransformer` for preprocessing. - At least two different ensemble classifiers (e.g., RandomForest, GradientBoosting) must be used in the VotingClassifier. - The model should be designed considering scalability and maintainability. Example: ```python import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder # Sample data df = pd.DataFrame({ \'num1\': [1.0, 2.1, 3.2, 4.3], \'num2\': [7.4, 6.3, 5.2, 4.1], \'cat1\': [\'A\', \'B\', \'A\', \'B\'], \'cat2\': [\'X\', \'Y\', \'X\', \'Y\'], \'target\': [0, 1, 0, 1] }) numerical_features = [\'num1\', \'num2\'] categorical_features = [\'cat1\', \'cat2\'] class CustomTransformer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): return self def transform(self, X, y=None): # Custom transformation logic return X # Modify as necessary def build_model(): # Numerical preprocessing pipeline numerical_pipeline = Pipeline([ (\'scaling\', StandardScaler()) ]) # Categorical preprocessing pipeline categorical_pipeline = Pipeline([ (\'onehot\', OneHotEncoder()) ]) # Combine preprocessing with ColumnTransformer preprocessor = ColumnTransformer([ (\'num\', numerical_pipeline, numerical_features), (\'cat\', categorical_pipeline, categorical_features) ]) # Define ensemble classifiers clf1 = RandomForestClassifier(n_estimators=100, random_state=42) clf2 = GradientBoostingClassifier(n_estimators=100, random_state=42) # Voting classifier voting_clf = VotingClassifier(estimators=[(\'rf\', clf1), (\'gb\', clf2)], voting=\'soft\') # Complete pipeline model = Pipeline([ (\'preprocessor\', preprocessor), (\'classifier\', voting_clf) ]) return model def predict(model, X): return model.predict(X) # Usage model = build_model() model.fit(df.drop(columns=[\'target\']), df[\'target\']) predictions = predict(model, df.drop(columns=[\'target\'])) print(predictions) ``` Note: Ensure that your final submission includes: 1. Properly commented code. 2. Clearly defined `predict` function. 3. Model evaluation on a provided or synthetic test dataset.","solution":"import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder # Define a custom transformer (if needed) class CustomTransformer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): return self def transform(self, X, y=None): # Assume no additional transformation is needed for simplicity return X def build_model(numerical_features, categorical_features): # Numerical preprocessing pipeline numerical_pipeline = Pipeline([ (\'scaling\', StandardScaler()), (\'custom\', CustomTransformer()) ]) # Categorical preprocessing pipeline categorical_pipeline = Pipeline([ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')), (\'custom\', CustomTransformer()) ]) # ColumnTransformer to preprocess both numerical and categorical features preprocessor = ColumnTransformer([ (\'num\', numerical_pipeline, numerical_features), (\'cat\', categorical_pipeline, categorical_features) ]) # Ensemble classifiers clf1 = RandomForestClassifier(n_estimators=100, random_state=42) clf2 = GradientBoostingClassifier(n_estimators=100, random_state=42) # Voting classifier voting_clf = VotingClassifier(estimators=[(\'rf\', clf1), (\'gb\', clf2)], voting=\'soft\') # Complete pipeline model = Pipeline([ (\'preprocessor\', preprocessor), (\'classifier\', voting_clf) ]) return model def predict(model, X): return model.predict(X)"},{"question":"# Coding Assessment Question: Module Reload Utility **Objective:** Implement a Python utility function that programmatically imports a specified module, modifies a particular attribute of the module to a new value if it exists, and then reloads the module to reset the modifications. Your function should also handle cases where the module is not found or the attribute does not exist. **Function Specification:** - **Function Name:** `import_and_modify_module` - **Parameters:** - `module_name` (str): The name of the module to import. - `attribute_name` (str): The name of the attribute within the module to modify. - `new_value` (any type): The new value to assign to the attribute. - **Returns:** - A tuple containing: - The module object after import and modification. - The state of the module after reloading it (to see whether the reload reverted the changes or not). - **Constraints:** - If the module specified by `module_name` does not exist, the function should raise a `ModuleNotFoundError`. - If the attribute specified by `attribute_name` does not exist within the module, the function should raise an `AttributeError`. - Use the `importlib` package for module operations. **Implementation Requirements:** 1. Import the specified module using `importlib.import_module`. 2. Check if the module has the specified attribute. 3. If the attribute exists, modify it to the new value. 4. Reload the module using `importlib.reload`. 5. Return the module object before and after reload as a tuple. **Example Usage:** ```python import_and_modify_module(\'math\', \'pi\', 3.14) ``` This would import the `math` module, attempt to set `math.pi` to `3.14`, reload the `math` module, and return the state before and after reloading the module. You should observe that changes to built-in modules may not persist through reload due to how such modules are implemented. **Note:** - Pay attention to the proper handling of module and attribute existence. - Ensure the original state of the module is restored after reloading.","solution":"import importlib def import_and_modify_module(module_name, attribute_name, new_value): Imports a module, modifies an attribute if it exists, and then reloads the module. Parameters: module_name (str): The name of the module to import. attribute_name (str): The name of the attribute within the module to modify. new_value (any type): The new value to assign to the attribute. Returns: tuple: The module object before and after reload. Raises: ModuleNotFoundError: If the module does not exist. AttributeError: If the attribute does not exist within the module. # Import the module try: module = importlib.import_module(module_name) except ModuleNotFoundError as e: raise ModuleNotFoundError(f\\"The module \'{module_name}\' does not exist.\\") from e # Check if the module has the specified attribute if not hasattr(module, attribute_name): raise AttributeError(f\\"The attribute \'{attribute_name}\' does not exist in the module \'{module_name}\'.\\") # Modify the attribute original_value = getattr(module, attribute_name) setattr(module, attribute_name, new_value) # Reload the module reloaded_module = importlib.reload(module) # Return the module object before and after reload return module, reloaded_module"},{"question":"**Question:** You are hired to analyze a dataset involving different species of penguins. Your task is to visualize the distribution of flipper lengths and derive meaningful insights using seaborn\'s advanced plotting features. **Dataset**: You will use the `penguins` dataset available in seaborn. **Requirements**: 1. Load the `penguins` dataset from seaborn. 2. Create a kernel density estimate plot displaying the density of `flipper_length_mm`. Use an area plot for this visualization. 3. Adjust the bandwidth to `0.5` for the KDE and visualize the impact. 4. Show how the density distribution of flipper lengths differs between the different `species` in the dataset. Normalize the densities within each species. 5. Use a faceted grid to display how the density distribution changes with respect to the `sex` of the penguins. 6. Add a cumulative density line plot to one of the visualizations. 7. Ensure the plot(s) have appropriate labels and a legend for clarity. **Input**: - No input is directly required from users as you are loading an internal seaborn dataset. **Output**: - KDE plots with the specified requirements. Write the code to fulfill these requirements. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset penguins = load_dataset(\\"penguins\\") # Initialize plot object plot = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") # Create area plot with KDE and adjusted bandwidth plot.add(so.Area(), so.KDE(bw_adjust=0.5)).facet(\\"sex\\").label(x=\\"Flipper Length (mm)\\", y=\\"Density\\", color=\\"Species\\") plt.title(\'Density Plot of Flipper Lengths by Species\') plt.legend(title=\\"Species\\") plot.show() # Add cumulative density line plot plot.add(so.Line(), so.KDE(cumulative=True)) plt.title(\'Cumulative Density Plot of Flipper Lengths by Species\') plt.legend(title=\\"Species\\") plot.show() ``` **Explanation**: This script will generate the required visualizations: 1. Plots the kernel density estimate of flipper lengths with area visualization. 2. Adjusts the bandwidth to `0.5` giving a different smoothness level. 3. Differentiates the densities between species and normalizes them within each species. 4. Uses faceting to show how densities change between male and female penguins. 5. Adds a cumulative density line plot to enhance data interpretation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_flipper_length_distribution(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Initialize the matplotlib figure plt.figure(figsize=(12, 8)) # KDE area plot with adjusted bandwidth sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", fill=True, bw_adjust=0.5) plt.title(\'Density Plot of Flipper Lengths by Species with Bandwidth Adjustment\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.legend(title=\\"Species\\") plt.show() # FacetGrid to show KDE plot by sex g = sns.FacetGrid(penguins, col=\\"sex\\", hue=\\"species\\", height=5, aspect=1.2) g.map(sns.kdeplot, \\"flipper_length_mm\\", fill=True, bw_adjust=0.5) g.add_legend() g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") g.fig.suptitle(\\"Density Plot of Flipper Lengths by Species and Sex\\", y=1.05) plt.show() # Cumulative KDE plot plt.figure(figsize=(12, 8)) sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", cumulative=True) plt.title(\'Cumulative Density Plot of Flipper Lengths by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Cumulative Density\') plt.legend(title=\\"Species\\") plt.show() # Call the function to generate the plots visualize_flipper_length_distribution()"},{"question":"**Objective:** Demonstrate your understanding of creating and managing threads using the `_thread` module in Python. You are required to write a Python program that creates multiple threads to compute the sum of integers in a list. Each thread should handle a portion of the list. Use locks to ensure that the summing operation is thread-safe. **Instructions:** 1. Define a function `partial_sum` that: - Accepts three parameters: a list of integers `data`, a lock object `lock`, and an integer `result_holder`. - Computes the sum of the integers in `data`. - Acquires the lock, updates `result_holder` with the computed sum, and releases the lock. 2. Define a function `compute_sum` that: - Accepts a list of integers `data` and an integer `num_threads`. - Splits the `data` list into `num_threads` sublists. - Creates and starts `num_threads` threads, each executing the `partial_sum` function with a sublist of the data. - Waits for all threads to complete. - Returns the total sum. 3. Implement the threading logic using functions provided by the `_thread` module. 4. Ensure proper thread synchronization using locks. **Example Usage:** ```python import _thread # Example list of integers data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # Define a global variable to hold the result result_holder = [] def partial_sum(data, lock, result_holder): # Compute sum of data partial_sum_result = sum(data) # Acquire lock and update the result_holder lock.acquire() try: result_holder[0] += partial_sum_result finally: lock.release() def compute_sum(data, num_threads): lock = _thread.allocate_lock() global result_holder result_holder = [0] # Initial sum is zero n = len(data) chunk_size = n // num_threads for i in range(num_threads): start_index = i * chunk_size if i == num_threads - 1: end_index = n else: end_index = start_index + chunk_size sublist = data[start_index:end_index] _thread.start_new_thread(partial_sum, (sublist, lock, result_holder)) # Busy-wait to ensure all threads complete while len(result_holder) < num_threads: pass return result_holder[0] # Example of usage num_threads = 3 total_sum = compute_sum(data, num_threads) print(f\'Total sum is {total_sum}\') # Output: Total sum is 55 ``` **Constraints:** - You should use the `_thread` module exclusively for threading, not the `threading` module. - The list length `n` will be between 1 and 10^6. - The number of threads `num_threads` will be between 1 and 100. Ensure your solution is thread-safe, and test it with different list sizes and thread counts.","solution":"import _thread import time # Define a function to compute the partial sum of a list segment def partial_sum(data, lock, result_holder): # Compute sum of data partial_sum_result = sum(data) # Acquire lock and update the result_holder lock.acquire() try: result_holder[0] += partial_sum_result finally: lock.release() def compute_sum(data, num_threads): lock = _thread.allocate_lock() result_holder = [0] # Initial sum is zero n = len(data) chunk_size = n // num_threads for i in range(num_threads): start_index = i * chunk_size if i == num_threads - 1: end_index = n else: end_index = start_index + chunk_size sublist = data[start_index:end_index] _thread.start_new_thread(partial_sum, (sublist, lock, result_holder)) # We need to ensure all threads have completed. Since _thread does not provide a join functionality, # we utilize a simple sleep to allow for thread completion. This is not ideal for complex cases. # For deterministic testing and production code, using threading with join is recommended. time.sleep(1) return result_holder[0]"},{"question":"# Question: **Objective:** Implement a custom asyncio Protocol to handle a simple HTTP communication over TCP. **Task:** You need to implement an async HTTP server using the asyncio\'s Transport and Protocol APIs. Your task is to create a server that can handle HTTP GET requests and return a simple HTML page in the response. 1. Define a custom `HTTPProtocol` class derived from `asyncio.Protocol`. 2. Implement the following methods in the `HTTPProtocol`: - `connection_made(self, transport)`: Called when a connection is made. - `data_received(self, data)`: Called when data is received. Here, you\'ll need to decode the incoming HTTP request, process it, and write an appropriate HTTP response back to the transport. - `connection_lost(self, exc)`: Called when the connection is lost. 3. Utilize the `loop.create_server` method to create a TCP server that listens for incoming connections and uses your `HTTPProtocol` to handle them. **Specifications:** - Your server should listen on `127.0.0.1` (localhost) and port `8080`. - It should handle HTTP GET requests to the root URL (`/`). For any other URL, it should return a 404 Not Found response. - For the root URL (`/`), it should return a simple HTML page that contains the text \\"Hello, World!\\". - The server should run indefinitely, serving multiple connections. # Example: Make a GET request to: `http://127.0.0.1:8080/` The server should respond with: ```http HTTP/1.1 200 OK Content-Type: text/html <html> <body> <h1>Hello, World!</h1> </body> </html> ``` Make a GET request to any other URL: `http://127.0.0.1:8080/unknown` The server should respond with: ```http HTTP/1.1 404 Not Found Not Found ``` # Constraints: - Use only `asyncio` for handling connections. - Do not use higher-level libraries like `aiohttp`. # Performance Requirements: - The server should handle multiple connections efficiently using asyncio event loop. # Input/Output: - Your program will not take any specific input but should handle incoming TCP connections. - Output will be based on the HTTP responses sent back to clients. Implement the `HTTPProtocol` class and server setup in the code below: ```python import asyncio class HTTPProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() print(\'Data received: {!r}\'.format(message)) request_line = message.splitlines()[0] method, path, _ = request_line.split() if method == \\"GET\\" and path == \\"/\\": response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/htmlrn\\" \\"rn\\" \\"<html><body><h1>Hello, World!</h1></body></html>\\" ) else: response = ( \\"HTTP/1.1 404 Not Foundrnrn\\" \\"Not Found\\" ) self.transport.write(response.encode()) self.transport.close() def connection_lost(self, exc): print(\'Connection closed\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: HTTPProtocol(), \'127.0.0.1\', 8080 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` Make sure to test your implementation by starting the server and using a web browser or curl to send HTTP GET requests to the server.","solution":"import asyncio class HTTPProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() request_line = message.splitlines()[0] method, path, _ = request_line.split() if method == \\"GET\\" and path == \\"/\\": response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/htmlrn\\" \\"rn\\" \\"<html><body><h1>Hello, World!</h1></body></html>\\" ) else: response = ( \\"HTTP/1.1 404 Not Foundrnrn\\" \\"Not Found\\" ) self.transport.write(response.encode()) self.transport.close() def connection_lost(self, exc): pass async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: HTTPProtocol(), \'127.0.0.1\', 8080 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Python Memory Management Simulation with `ctypes` Objective In this assignment, you are required to simulate Python\'s memory management functionality using the `ctypes` library. The goal is to implement functions that handle memory allocation and deallocation similar to the C functions described in the Python memory management documentation. Task You need to implement four functions: 1. **allocate_memory(size)** - **Input**: `size` (an integer representing the number of bytes to allocate). - **Output**: A pointer to the allocated memory block. - **Behavior**: - Use `ctypes` to allocate `size` bytes of memory and return a pointer to the allocated memory. - If `size` is zero, allocate a single byte and return a pointer to that byte. 2. **allocate_zeroed_memory(num, size)** - **Input**: `num` (number of elements) and `size` (size of each element in bytes). - **Output**: A pointer to the allocated and zero-initialized memory block. - **Behavior**: - Use `ctypes` to allocate an array of `num` elements each of `size` bytes, and initialize all bytes to zero. - If `num` or `size` is zero, allocate a single byte and return a pointer to that byte. 3. **reallocate_memory(pointer, size)** - **Input**: `pointer` (a previously allocated memory pointer) and `size` (new size of the memory block). - **Output**: A pointer to the reallocated memory block. - **Behavior**: - Use `ctypes` to resize the memory block pointed to by `pointer` to `size` bytes. - If `pointer` is `None`, allocate a new memory block of `size` bytes. - If `size` is zero, resize the memory block but do not free it, and return a non-NULL pointer. 4. **free_memory(pointer)** - **Input**: `pointer` (previously allocated memory pointer). - **Output**: None - **Behavior**: - Use `ctypes` to free the memory block pointed to by `pointer`. - If `pointer` is `None`, no operation is performed. Constraints - The functions should handle edge cases such as zero size allocation and reallocation. - Ensure that you correctly manage memory to avoid leaks or segmentation faults. Example ```python import ctypes # Implement the required functions here def allocate_memory(size): if size == 0: size = 1 return ctypes.create_string_buffer(size) def allocate_zeroed_memory(num, size): if num == 0 or size == 0: return ctypes.create_string_buffer(1) return ctypes.create_string_buffer(num * size) def reallocate_memory(pointer, size): if size == 0: size = 1 return ctypes.resize(pointer, size) def free_memory(pointer): # ctypes in Python automatically manages memory, # but we can simulate by simply deleting the pointer del pointer # Testing the functions pointer1 = allocate_memory(10) print(ctypes.sizeof(pointer1)) # Output: 10 pointer2 = allocate_zeroed_memory(5, 2) print(ctypes.sizeof(pointer2)) # Output: 10 pointer3 = reallocate_memory(pointer2, 15) print(ctypes.sizeof(pointer3)) # Output: 15 free_memory(pointer1) free_memory(pointer2) free_memory(pointer3) ``` Ensure that you adhere to the provided function signatures and document your code properly.","solution":"import ctypes def allocate_memory(size): Allocates memory of the given size using ctypes. If the size is zero, 1 byte is allocated. :param size: The number of bytes to allocate. :return: Pointer to the allocated memory. if size == 0: size = 1 return ctypes.create_string_buffer(size) def allocate_zeroed_memory(num, size): Allocates zero-initialized memory. Allocates memory for an array of `num` elements each of `size` bytes. If num or size are zero, 1 byte is allocated. :param num: The number of elements. :param size: The size of each element in bytes. :return: Pointer to the allocated zero-initialized memory. if num == 0 or size == 0: return ctypes.create_string_buffer(1) return ctypes.create_string_buffer(num * size) def reallocate_memory(pointer, size): Reallocates memory block to the new size. If the pointer is None, allocates new memory. If size is zero, 1 byte is allocated. :param pointer: Pointer to the previously allocated memory. :param size: The new size of the memory block. :return: Pointer to the reallocated memory. if size == 0: size = 1 new_pointer = ctypes.create_string_buffer(size) if pointer: ctypes.memmove(new_pointer, pointer, min(ctypes.sizeof(pointer), size)) return new_pointer def free_memory(pointer): Frees the memory block pointed by the pointer. If the pointer is None, no operation is performed. :param pointer: Pointer to the previously allocated memory. if pointer: del pointer"},{"question":"**Question:** You are required to implement a function `process_data` that takes a nested dictionary input and performs certain operations on it. The nested dictionary contains lists of numeric values. Your task is to: 1. Validate that the input is a dictionary where each key is a string and each value is a list of integers. 2. For each key in the dictionary, calculate the sum of the integers in the list associated with that key. 3. Return a new dictionary where the keys are the same as the input dictionary, but the values are the sums of the corresponding lists. **Function Signature**: ```python def process_data(data: dict) -> dict: ``` **Input**: - `data` (dict): A dictionary with keys as strings and values as lists of integers. For example, ```python { \\"a\\": [1, 2, 3], \\"b\\": [4, 5], \\"c\\": [6, 7, 8, 9] } ``` **Output**: - dict: A dictionary with the same keys as the input dictionary and the values being the sums of the corresponding lists. For example, given the input above, the output should be: ```python { \\"a\\": 6, \\"b\\": 9, \\"c\\": 30 } ``` **Constraints**: 1. Each value in the input dictionary is guaranteed to be a list of non-negative integers. 2. The dictionary might be empty, in which case, you should return an empty dictionary. 3. You must perform appropriate type checks to ensure the input conforms to the expected structure. If not, raise a `TypeError`. **Example**: ```python assert process_data({ \\"a\\": [1, 2, 3], \\"b\\": [4, 5], \\"c\\": [6, 7, 8, 9] }) == {\\"a\\": 6, \\"b\\": 9, \\"c\\": 30} assert process_data({ \\"x\\": [0], \\"y\\": [10, 20] }) == {\\"x\\": 0, \\"y\\": 30} assert process_data({}) == {} ``` In case of incorrect input structure, your function should raise an appropriate exception: ```python try: process_data({ \\"a\\": [1, 2, 3, \\"not an int\\"], \\"b\\": [4, 5] }) except TypeError: print(\\"Caught TypeError as expected!\\") ``` Implement `process_data` by adhering to the given requirements and constraints.","solution":"def process_data(data: dict) -> dict: Processes the input dictionary by validating its structure and calculating the sum of lists of integers. Args: data (dict): A dictionary with keys as strings and values as lists of integers. Returns: dict: A dictionary with the same keys as the input dictionary and the values being the sums of the corresponding lists. Raises: TypeError: If the input is not a dictionary or the dictionary is not structured correctly. if not isinstance(data, dict): raise TypeError(\\"Input must be a dictionary.\\") result = {} for key, value in data.items(): if not isinstance(key, str): raise TypeError(\\"Keys must be strings.\\") if not isinstance(value, list): raise TypeError(\\"Values must be lists.\\") if not all(isinstance(i, int) for i in value): raise TypeError(\\"All elements in the lists must be integers.\\") result[key] = sum(value) return result"},{"question":"Objective Implement a function that mimics the behavior of including or excluding files from a directory based on specified patterns as described in the `sdist` command of the `python310` package. Problem Statement You are given a list of filenames representing files in a directory and a set of commands that determine which files should be included or excluded. Your task is to implement a function that returns the final list of files that should be included based on these commands. Function Signature ```python def manage_files(files: List[str], commands: List[str]) -> List[str]: pass ``` Input - `files`: A list of strings representing the filenames present in the directory. (e.g., `[\\"file1.py\\", \\"file2.txt\\", \\"subdir/file3.py\\"]`) - `commands`: A list of strings where each string is a command for including or excluding files. The commands come in the following forms: - `include pat1 pat2 ...` (Include all files matching any of the listed patterns) - `exclude pat1 pat2 ...` (Exclude all files matching any of the listed patterns) - `recursive-include dir pat1 pat2 ...` (Include all files under `dir` matching any of the listed patterns) - `recursive-exclude dir pat1 pat2 ...` (Exclude all files under `dir` matching any of the listed patterns) - `global-include pat1 pat2 ...` (Include all files anywhere matching any of the listed patterns) - `global-exclude pat1 pat2 ...` (Exclude all files anywhere matching any of the listed patterns) - `prune dir` (Exclude all files under `dir`) - `graft dir` (Include all files under `dir`) Output - Return a list of strings representing the final list of files that should be included after applying all the commands in order. Constraints - Filenames and directory names will conform to Unix-style filename conventions. - Patterns will use Unix globbing rules (`*`, `?`, and `[]`). - Commands will be applied in the order they are provided. - The `files` list will contain at most 10,000 items. - The file name and directory name lengths will not exceed 100 characters. - The number of commands will not exceed 100. Examples ```python files = [\\"file1.py\\", \\"file2.txt\\", \\"subdir/file3.py\\", \\"subdir/file4.txt\\", \\"otherdir/file5.py\\"] commands = [ \\"include *.py\\", \\"exclude file2.txt\\", \\"recursive-include subdir *.txt\\", \\"global-exclude *file4.txt\\", \\"graft otherdir\\" ] manage_files(files, commands) ``` Expected output: ``` [\\"file1.py\\", \\"subdir/file3.py\\", \\"otherdir/file5.py\\"] ``` Explanation - `include *.py` includes all `.py` files. - `exclude file2.txt` excludes `file2.txt`. - `recursive-include subdir *.txt` re-includes all `.txt` files in `subdir`. - `global-exclude *file4.txt` excludes any file ending with `file4.txt`. - `graft otherdir` includes all files under `otherdir`. Implement `manage_files` function to pass the above cases and handle edge cases as described.","solution":"from typing import List import fnmatch import os def manage_files(files: List[str], commands: List[str]) -> List[str]: included_files = set() for file in files: for command in commands: parts = command.split() action = parts[0] if action == \'include\': for pattern in parts[1:]: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif action == \'exclude\': for pattern in parts[1:]: if fnmatch.fnmatch(file, pattern): included_files.discard(file) elif action == \'recursive-include\': dir = parts[1] if file.startswith(dir): for pattern in parts[2:]: if fnmatch.fnmatch(file[len(dir)+1:], pattern): # Exclude dir part included_files.add(file) elif action == \'recursive-exclude\': dir = parts[1] if file.startswith(dir): for pattern in parts[2:]: if fnmatch.fnmatch(file[len(dir)+1:], pattern): # Exclude dir part included_files.discard(file) elif action == \'global-include\': for pattern in parts[1:]: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif action == \'global-exclude\': for pattern in parts[1:]: if fnmatch.fnmatch(file, pattern): included_files.discard(file) elif action == \'prune\': dir = parts[1] if file.startswith(dir): included_files.discard(file) elif action == \'graft\': dir = parts[1] if file.startswith(dir): included_files.add(file) return sorted(list(included_files))"},{"question":"Objective: Implement a function to encode and decode a string using specified encodings while handling potential errors with custom error handling. This will demonstrate your skills in using the codec registry and support functions in Python 3.10. Problem Statement: Write a Python function `custom_encode_decode(text: str, encoding: str, errors: str) -> Tuple[str, str]` that performs the following operations: 1. Encodes the input text using the specified encoding and error handling method. 2. Decodes the encoded text back to its original form using the same encoding and error handling method. Implementation Details: 1. **Function Signature**: ```python from typing import Tuple def custom_encode_decode(text: str, encoding: str, errors: str) -> Tuple[str, str]: pass ``` 2. **Parameters**: - `text` (str): The input string to be encoded and decoded. - `encoding` (str): The encoding to be used (e.g., \\"utf-8\\", \\"ascii\\"). - `errors` (str): The error handling method to be used (e.g., \\"strict\\", \\"ignore\\", \\"replace\\"). 3. **Returns**: - Tuple containing two elements: - The encoded string. - The decoded string back to its original form. 4. **Constraints**: - You must use the `PyCodec_Encode`, `PyCodec_Decode`, and the error handling functions as mentioned in the documentation provided. - Handle any potential exceptions that might occur during encoding or decoding and raise a custom exception if encoding/decoding fails. Example: ```python text = \\"Hello, world!\\" encoding = \\"ascii\\" errors = \\"ignore\\" encoded_text, decoded_text = custom_encode_decode(text, encoding, errors) print(encoded_text) # Output could vary based on encoding and error handling method print(decoded_text) # Should print \\"Hello, world!\\" ``` Notes: 1. You may also need to handle cases where characters in `text` are not representable in the specified encoding. 2. Assume that the provided encoding and error methods are supported. Good luck!","solution":"from typing import Tuple def custom_encode_decode(text: str, encoding: str, errors: str) -> Tuple[str, str]: Encodes the input text using the specified encoding and error handling method, then decodes it back to its original form using the same encoding and error handling method. Args: text (str): The input string to be encoded and decoded. encoding (str): The encoding to be used (e.g., \\"utf-8\\", \\"ascii\\"). errors (str): The error handling method to be used (e.g., \\"strict\\", \\"ignore\\", \\"replace\\"). Returns: Tuple[str, str]: A tuple containing the encoded string and the decoded string. try: encoded_bytes = text.encode(encoding, errors) encoded_str = encoded_bytes.decode(\'latin1\') # Use \'latin1\' to preserve byte values decoded_text = encoded_bytes.decode(encoding, errors) return encoded_str, decoded_text except (UnicodeEncodeError, UnicodeDecodeError) as e: raise Exception(f\\"Encoding/Decoding failed: {str(e)}\\") text = \\"Hello, world!\\" encoding = \\"ascii\\" errors = \\"ignore\\" encoded_text, decoded_text = custom_encode_decode(text, encoding, errors) print(encoded_text) # Output could vary based on encoding and error handling method print(decoded_text) # Should print \\"Hello, world!\\""},{"question":"# Coding Assessment: ZIP File Manipulation Objective: To assess your ability to work with ZIP files using Python\'s `zipfile` module. You will create a program that performs various operations on ZIP files, demonstrating both basic and advanced usage of the module. Task: Implement a Python function called `zipfile_operations` that takes two parameters: - `input_zipfile_path`: The path to an input ZIP file. - `output_directory`: The directory where extracted files will be stored. The function should perform the following operations: 1. **Read and List Contents**: - Read the ZIP file specified by `input_zipfile_path`. - Print a list of the names of all files and directories contained in the ZIP file. 2. **Extract Files**: - Extract all files from the ZIP file into the `output_directory`. - Ensure the extracted files maintain their directory structure. 3. **Add Files**: - Create a new file named `new_file.txt` in the `output_directory`. - Write the text \\"This is a new file\\" into `new_file.txt`. - Add `new_file.txt` back into the input ZIP file. 4. **List New Contents**: - Re-read the ZIP file and print the list of files and directories again (this should include `new_file.txt`). 5. **Error Handling**: - Implement error handling to manage potential issues such as: - The input file not being a ZIP file. - The output directory not existing. - Insufficient disk space or permission issues. - Handle any other potential errors gracefully with meaningful error messages. Input Format: ```python def zipfile_operations(input_zipfile_path: str, output_directory: str) -> None: pass ``` Constraints: - The input ZIP file may contain nested directories. - Assume the ZIP file is not encrypted. Example Usage: ```python zipfile_operations(\'example.zip\', \'extract_here\') ``` **Expected Output:** ``` List of contents in the ZIP file: [\'file1.txt\', \'file2.txt\', \'dir1/file3.txt\', \'dir1/dir2/file4.txt\'] Extracting files... New file \'new_file.txt\' added. List of contents in the ZIP file after modification: [\'file1.txt\', \'file2.txt\', \'dir1/file3.txt\', \'dir1/dir2/file4.txt\', \'new_file.txt\'] ``` Your implementation should handle the steps described and print appropriate messages as shown in the example.","solution":"import os import zipfile def zipfile_operations(input_zipfile_path: str, output_directory: str) -> None: try: # Check if the input file exists if not os.path.exists(input_zipfile_path): raise FileNotFoundError(f\\"The file \'{input_zipfile_path}\' does not exist.\\") # Ensure the output directory exists if not os.path.exists(output_directory): os.makedirs(output_directory) # Read and list contents of the ZIP file with zipfile.ZipFile(input_zipfile_path, \'r\') as zip_ref: file_list = zip_ref.namelist() print(\\"List of contents in the ZIP file:\\") print(file_list) # Extract all files to the output directory print(\\"Extracting files...\\") zip_ref.extractall(output_directory) # Create new file and add to ZIP new_file_path = os.path.join(output_directory, \'new_file.txt\') with open(new_file_path, \'w\') as new_file: new_file.write(\\"This is a new file\\") with zipfile.ZipFile(input_zipfile_path, \'a\') as zip_ref: zip_ref.write(new_file_path, \'new_file.txt\') print(\\"New file \'new_file.txt\' added.\\") # List new contents in the ZIP file with zipfile.ZipFile(input_zipfile_path, \'r\') as zip_ref: new_file_list = zip_ref.namelist() print(\\"List of contents in the ZIP file after modification:\\") print(new_file_list) except zipfile.BadZipFile: print(\\"Error: The input file is not a valid ZIP file.\\") except FileNotFoundError as e: print(e) except PermissionError: print(\\"Error: Insufficient permissions to write to the specified directory.\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# Custom Iterable and Iterator Implementation You are tasked with implementing a custom iterable and iterator class in Python to deepen your understanding of Python\'s iterator protocol. Requirements 1. **CustomIterable Class:** - Implement an iterable class named `CustomIterable`. - This class should be initialized with a list of integers. - It should implement the `__iter__` method, returning an instance of the corresponding iterator. 2. **CustomIterator Class:** - Implement an iterator class named `CustomIterator`. - This class should be initialized with the list of integers passed from `CustomIterable`. - Implement the `__next__` method to return the next integer in the list. When the list is exhausted, raise a `StopIteration` exception. Additional Task - Create a function `sum_custom_iterable(custom_iterable)` that takes an instance of `CustomIterable` and returns the sum of all integers in the iterable. Example ```python # Example usage: iterable = CustomIterable([1, 2, 3, 4]) iterator = iter(iterable) print(next(iterator)) # Output: 1 print(next(iterator)) # Output: 2 print(next(iterator)) # Output: 3 print(next(iterator)) # Output: 4 print(next(iterator)) # Raises StopIteration sum_result = sum_custom_iterable(iterable) print(sum_result) # Output: 10 ``` # Constraints - You are not allowed to use any built-in or third-party libraries that simplify this task (e.g., `itertools`). - You must handle the iteration manually according to Python\'s iterator protocol. # Evaluation Your implementation will be evaluated based on: - Correctness: Does your solution meet all the requirements and handle edge cases? - Performance: Is your solution efficient and does not perform unnecessary operations? - Code Quality: Is your code clean, well-organized, and easy to read? You are expected to submit the following: 1. Implementation of `CustomIterable`. 2. Implementation of `CustomIterator`. 3. Implementation of the function `sum_custom_iterable`.","solution":"class CustomIterable: def __init__(self, data): Initializes the CustomIterable with a list of integers. self.data = data def __iter__(self): Returns an instance of the CustomIterator. return CustomIterator(self.data) class CustomIterator: def __init__(self, data): Initializes the CustomIterator with the given list of integers. self.data = data self.index = 0 def __next__(self): Returns the next integer in the list. Raises StopIteration when the list is exhausted. if self.index < len(self.data): value = self.data[self.index] self.index += 1 return value else: raise StopIteration def sum_custom_iterable(custom_iterable): Returns the sum of all integers in the given CustomIterable instance. return sum(custom_iterable)"},{"question":"**Question: Implementing and Using Partial Least Squares Regression** **Context:** You are provided with a dataset that contains features and target values. Your task is to implement a pipeline using Scikit-learn\'s `PLSRegression` to perform dimensionality reduction and regression. The goal is to apply Partial Least Squares (PLS) regression to find the relationship between the features and target values, and evaluate its performance. **Dataset:** Assume you have the following dataset: - `X`: A NumPy array of shape (n_samples, n_features) containing the feature values. - `Y`: A NumPy array of shape (n_samples,) containing the target values. **Task:** 1. Implement a function `pls_regression` that takes the following parameters: - `X`: 2D NumPy array of shape (n_samples, n_features) - `Y`: 1D NumPy array of shape (n_samples,) - `n_components`: Integer, the number of components to keep in the PLS regression model. 2. Your function should perform the following steps: - Fit a `PLSRegression` model with the specified number of components. - Return the trained PLS regression model. 3. Implement a function `evaluate_model` that takes the following parameters: - `model`: The trained PLS regression model. - `X_test`: 2D NumPy array of shape (n_test_samples, n_features) for testing. - `Y_test`: 1D NumPy array of shape (n_test_samples,) representing the true labels for testing. 4. Your function should perform the following steps: - Use the model to predict target values for the test data `X_test`. - Calculate and return the Mean Squared Error (MSE) of the predictions compared to the true labels `Y_test`. **Example:** ```python import numpy as np # Example data X = np.random.rand(100, 10) Y = np.random.rand(100) # Example test data X_test = np.random.rand(20, 10) Y_test = np.random.rand(20) def pls_regression(X, Y, n_components): from sklearn.cross_decomposition import PLSRegression pls = PLSRegression(n_components=n_components) pls.fit(X, Y) return pls def evaluate_model(model, X_test, Y_test): from sklearn.metrics import mean_squared_error Y_pred = model.predict(X_test) mse = mean_squared_error(Y_test, Y_pred) return mse # Training the model model = pls_regression(X, Y, n_components=2) # Evaluating the model mse = evaluate_model(model, X_test, Y_test) print(f\\"MSE: {mse}\\") ``` **Constraints:** - Ensure that `X` and `Y` are appropriately preprocessed before fitting the PLS model. - The number of components `n_components` must be a positive integer less than or equal to the minimum of the number of features and the number of samples. Ensure your implementation is efficient and follows best practices in scikit-learn.","solution":"from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression(X, Y, n_components): Fit a PLSRegression model with the specified number of components. Parameters: X (np.ndarray): Feature data of shape (n_samples, n_features). Y (np.ndarray): Target data of shape (n_samples,). n_components (int): Number of components to keep in the PLS regression model. Returns: PLSRegression: The trained PLS regression model. pls = PLSRegression(n_components=n_components) pls.fit(X, Y) return pls def evaluate_model(model, X_test, Y_test): Evaluate the PLSRegression model using MSE. Parameters: model (PLSRegression): Trained PLS regression model. X_test (np.ndarray): Test feature data of shape (n_test_samples, n_features). Y_test (np.ndarray): True labels for the test data of shape (n_test_samples,). Returns: float: Mean Squared Error of the model\'s predictions. Y_pred = model.predict(X_test) mse = mean_squared_error(Y_test, Y_pred) return mse"},{"question":"# Question: Advanced Complex Tensor Operations with PyTorch In this coding assessment, you are required to implement functions that perform operations on complex tensors using PyTorch. You must demonstrate your understanding of creating complex tensors, transitioning from real tensor representations, and performing operations on them. Tasks 1. **Create Complex Tensor:** - Write a function `create_complex_tensor` that takes two 2D PyTorch tensors (`real_values` and `imag_values`) as input and returns a single complex tensor formed by combining the real and imaginary parts. ```python def create_complex_tensor(real_values: torch.Tensor, imag_values: torch.Tensor) -> torch.Tensor: pass ``` 2. **Switch Representation:** - Write a function `switch_representation` that takes a complex tensor and returns its real tensor representation (shape `(..., 2)`). ```python def switch_representation(complex_tensor: torch.Tensor) -> torch.Tensor: pass ``` 3. **Element-wise Operations:** - Write a function `elementwise_operations` which takes a complex tensor and performs the following operations: - Computes the absolute value. - Computes the angle. - Multiplies the real part by 2. - Adds `1j` to the imaginary part of the complex tensor. - The function should return a dictionary with keys `\\"abs\\"`, `\\"angle\\"`, `\\"real_multiplied\\"`, and `\\"imag_added\\"` containing the results of these operations. ```python def elementwise_operations(complex_tensor: torch.Tensor) -> dict: pass ``` Specifications - Assume the input real and imaginary tensors will always be 2D and of the same shape. - The complex tensor should be created using dtype `torch.cfloat`. - You must utilize `torch.view_as_complex` and `torch.view_as_real` for switching representations. - For element-wise operations, use `torch.abs`, `torch.angle`, and other appropriate PyTorch operations that handle complex tensors. Example Usage ```python real_part = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) imag_part = torch.tensor([[5.0, 6.0], [7.0, 8.0]]) # Task 1: Creating complex tensor complex_tensor = create_complex_tensor(real_part, imag_part) # complex_tensor should now be: tensor([[1.0+5.0j, 2.0+6.0j], [3.0+7.0j, 4.0+8.0j]]) # Task 2: Switching representation real_rep = switch_representation(complex_tensor) # real_rep should now be: tensor([[[1.0, 5.0], [2.0, 6.0]], [[3.0, 7.0], [4.0, 8.0]]]) # Task 3: Element-wise operations results = elementwise_operations(complex_tensor) # results should be: # { # \\"abs\\": tensor([[5.0990, 6.3246], [7.6158, 8.9443]]), # \\"angle\\": tensor([[1.3734, 1.2490], [1.1659, 1.1071]]), # \\"real_multiplied\\": tensor([[2.0+5.0j, 4.0+6.0j], [6.0+7.0j, 8.0+8.0j]]), # \\"imag_added\\": tensor([[1.0+6.0j, 2.0+7.0j], [3.0+8.0j, 4.0+9.0j]]) # } ```","solution":"import torch def create_complex_tensor(real_values: torch.Tensor, imag_values: torch.Tensor) -> torch.Tensor: Combines real and imaginary parts into a single complex tensor. return torch.complex(real_values, imag_values) def switch_representation(complex_tensor: torch.Tensor) -> torch.Tensor: Switches the representation of a complex tensor to real representation. return torch.view_as_real(complex_tensor) def elementwise_operations(complex_tensor: torch.Tensor) -> dict: Performs element-wise operations on the complex tensor. - Computes the absolute value. - Computes the angle. - Multiplies the real part by 2. - Adds `1j` to the imaginary part of the complex tensor. abs_values = torch.abs(complex_tensor) angle_values = torch.angle(complex_tensor) real_multiplied = torch.complex(2 * torch.real(complex_tensor), torch.imag(complex_tensor)) imag_added = torch.complex(torch.real(complex_tensor), torch.imag(complex_tensor) + 1) return { \\"abs\\": abs_values, \\"angle\\": angle_values, \\"real_multiplied\\": real_multiplied, \\"imag_added\\": imag_added }"},{"question":"# Asynchronous Exception Handling in Python Background: You are working with the `asyncio` module to build a robust asynchronous application. One of the crucial aspects of working with asynchronous programming is handling various exceptions that may arise during asynchronous operations. Objective: Implement a function `fetch_data_with_timeout` that attempts to fetch data from an asynchronous data source with a given timeout. The function should handle different types of exceptions that can be raised by the `asyncio` module. Function Signature: ```python async def fetch_data_with_timeout(source: str, timeout: float) -> bytes: pass ``` Inputs: - `source` (str): The data source URL. - `timeout` (float): The maximum time (in seconds) allowed for the operation. Outputs: - `bytes`: The fetched data as bytes. Constraints and Considerations: 1. The function should raise an `asyncio.TimeoutError` if the operation exceeds the given timeout. 2. If the operation is cancelled, it should catch the `asyncio.CancelledError` and perform any necessary cleanup operations before re-raising the exception. 3. If the internal state of the Task or Future is invalid, it should handle the `asyncio.InvalidStateError`. 4. If the \\"sendfile\\" syscall is unavailable, it should handle the `asyncio.SendfileNotAvailableError`. 5. If the read operation is incomplete, it should handle the `asyncio.IncompleteReadError`. 6. If the buffer size limit is reached while looking for a separator, it should handle the `asyncio.LimitOverrunError`. 7. For any other exceptions, it should handle them gracefully and return an appropriate error message. Example: ```python import asyncio async def fetch_data_with_timeout(source: str, timeout: float) -> bytes: try: # Simulate a data fetch operation with asyncio.sleep as placeholder await asyncio.sleep(timeout) return b\\"fetched data\\" except asyncio.TimeoutError: print(\\"Operation timed out.\\") raise except asyncio.CancelledError as e: print(\\"Operation cancelled.\\") raise e except asyncio.InvalidStateError: print(\\"Invalid state error.\\") raise except asyncio.SendfileNotAvailableError: print(\\"Sendfile not available error.\\") raise except asyncio.IncompleteReadError as e: print(f\\"Incomplete read error. Expected {e.expected} bytes, but got partial data.\\") raise except asyncio.LimitOverrunError as e: print(f\\"Limit overrun error. Consumed {e.consumed} bytes.\\") raise except Exception as e: print(f\\"An unexpected error occurred: {str(e)}\\") raise # Simulate calling the function # asyncio.run(fetch_data_with_timeout(\\"http://example.com\\", 5.0)) ``` **Note:** The example above contains placeholder code with `asyncio.sleep` to simulate operation. Implement the actual data fetching as needed. Tests: - Verify the function handles and raises `asyncio.TimeoutError` correctly. - Verify the function handles and re-raises `asyncio.CancelledError` after performing cleanup. - Verify the function handles `asyncio.InvalidStateError` appropriately. - Verify the function handles `asyncio.SendfileNotAvailableError` appropriately. - Verify the function handles `asyncio.IncompleteReadError` and can inspect the `partial` attribute. - Verify the function handles `asyncio.LimitOverrunError` and can inspect the `consumed` attribute. - Validate the graceful handling of other general exceptions.","solution":"import asyncio async def fetch_data_with_timeout(source: str, timeout: float) -> bytes: try: # Simulate a data fetch operation with asyncio.sleep as placeholder await asyncio.sleep(timeout) return b\\"fetched data\\" except asyncio.TimeoutError: print(\\"Operation timed out.\\") raise except asyncio.CancelledError as e: print(\\"Operation cancelled.\\") raise e except asyncio.InvalidStateError: print(\\"Invalid state error.\\") raise except asyncio.SendfileNotAvailableError: print(\\"Sendfile not available error.\\") raise except asyncio.IncompleteReadError as e: print(f\\"Incomplete read error. Expected {e.expected} bytes, but got partial data.\\") raise except asyncio.LimitOverrunError as e: print(f\\"Limit overrun error. Consumed {e.consumed} bytes.\\") raise except Exception as e: print(f\\"An unexpected error occurred: {str(e)}\\") raise # To execute the function, you would call: # asyncio.run(fetch_data_with_timeout(\\"http://example.com\\", 5.0))"},{"question":"<|Analysis Begin|> The provided documentation describes the `rlcompleter` module, which is used to provide tab completion functionality for the `readline` module in Python\'s interactive mode. The `rlcompleter` module defines a `Completer` class with a specific method `complete(text, state)` that returns possible completions for the input `text`, with `state` determining which completion to return on successive calls. The key points from the documentation include: - The `Completer` class can be instantiated and used independently of the `readline` module if required. - The `complete` method can generate completions for variable names, built-ins, keywords, and attributes of objects (by evaluating expressions up to the last part without side-effects). This module is useful for creating interactive Python environments or adding autocompletion features to custom REPL (Read-Eval-Print Loop) implementations. This documentation provides enough information to design a coding question that tests understanding of object-oriented programming, method implementation, and interaction between modules. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Your task is to create a custom REPL (Read-Eval-Print Loop) environment that supports autocompletion of Python identifiers using the `rlcompleter` module. Description: Implement a `CustomREPL` class that provides an interactive Python interpreter with tab completion. The class should utilize the `Completer` class from the `rlcompleter` module. Requirements: 1. The `CustomREPL` class should have the following attributes: - `completer`: An instance of the `Completer` class from the `rlcompleter` module. - `history`: A list to store the input history. 2. The `CustomREPL` class should have the following methods: - `__init__(self)`: Initializes the `Completer` instance and configures the readline module to use the completable with tab. - `start(self)`: Starts the interactive REPL loop. This method should: - Continuously read input from the user. - Append each input to the history. - Evaluate and print the result of the input expression. - Handle exceptions gracefully and print appropriate error messages. Expected Behavior: 1. When the REPL is started, it should prompt the user for input with `\\">>> \\"`. 2. On pressing the Tab key, the REPL should provide autocompletions for partially typed identifiers, keywords, or object attributes. 3. Input history should be maintained across sessions, allowing the user to navigate through previous inputs using the up and down arrow keys. Constraints: - Do not use any external libraries for the REPL implementation other than `rlcompleter` and `readline`. - Ensure that function evaluations and potentially dangerous expressions are handled safely. Example: ``` >>> repl = CustomREPL() >>> repl.start() >>> code_input: x = 10 >>> x 10 >>> code_input: x.to<TAB PRESSED> x.to_bytes( x.to_imag( x.to_real( >>> code_input: x.to_bytes(2, \'big\') b\'x00x0a\' >>> ``` Implementation: You need to implement the `CustomREPL` class and its methods according to the specifications provided. ```python import rlcompleter import readline class CustomREPL: def __init__(self): self.completer = rlcompleter.Completer() readline.set_completer(self.completer.complete) readline.parse_and_bind(\\"tab: complete\\") self.history = [] def start(self): while True: try: code_input = input(\\">>> \\") self.history.append(code_input) result = eval(code_input, globals()) if result is not None: print(result) except Exception as e: print(f\\"Error: {e}\\") # Test the CustomREPL if __name__ == \\"__main__\\": repl = CustomREPL() repl.start() ``` Implement the class as described and ensure it works as expected.","solution":"import rlcompleter import readline class CustomREPL: def __init__(self): self.completer = rlcompleter.Completer() readline.set_completer(self.completer.complete) readline.parse_and_bind(\\"tab: complete\\") self.history = [] def start(self): while True: try: code_input = input(\\">>> \\") self.history.append(code_input) result = eval(code_input, globals()) if result is not None: print(result) except Exception as e: print(f\\"Error: {e}\\") # Test the CustomREPL if __name__ == \\"__main__\\": repl = CustomREPL() repl.start()"},{"question":"**Email Processing with `imaplib`** You are tasked with implementing a Python utility that connects to an IMAP email server, searches for unread emails, fetches their contents, marks them as read, and then logs out. The utility should handle any server or connection issues gracefully. # Requirements: 1. Implement a function `process_unread_emails` with the following specifications: - **Input:** - `server` (string) - The IMAP server address. - `username` (string) - The username for login. - `password` (string) - The password for login. - `mailbox` (string, default = \'INBOX\') - The mailbox to search for unread emails. Optional. - **Output:** - List of tuples where each tuple contains the subject and body of each email. If there are no unread emails, return an empty list. 2. The function should: - Connect to the IMAP server using the provided credentials. - Select the specified mailbox. - Search for all unread emails. - Fetch the subject and body of each unread email. - Mark the emails as read. - Handle any exceptions (e.g., connection errors, authentication failures) appropriately, returning an informative message or empty list, as necessary. - Ensure to log out from the server at the end. # Constraints: - You must use the `imaplib` package. - The solution should handle and report errors gracefully, without crashing. # Example Usage: ```python def process_unread_emails(server, username, password, mailbox=\'INBOX\'): try: from imaplib import IMAP4, IMAP4_SSL import email # Connect to the server M = IMAP4_SSL(server) M.login(username, password) # Select the mailbox M.select(mailbox) # Search for unread emails typ, msgnums = M.search(None, \'UNSEEN\') # Initialize list to store email details emails = [] # Fetch each email for num in msgnums[0].split(): typ, data = M.fetch(num, \'(RFC822)\') msg = email.message_from_bytes(data[0][1]) # Extract subject and body subject = email.header.decode_header(msg[\'subject\'])[0][0] if isinstance(subject, bytes): subject = subject.decode() if msg.is_multipart(): body = \\"\\" for part in msg.walk(): if part.get_content_type() == \\"text/plain\\": body = part.get_payload(decode=True).decode() break else: body = msg.get_payload(decode=True).decode() emails.append((subject, body)) # Mark email as read M.store(num, \'+FLAGS\', \'Seen\') # Logout M.close() M.logout() return emails except IMAP4.error as e: print(f\\"IMAP error: {e}\\") M.logout() return [] except IMAP4.abort as e: print(f\\"Connection aborted: {e}\\") return [] except Exception as e: print(f\\"An error occurred: {e}\\") return [] # Example call: result = process_unread_emails(\\"imap.server.com\\", \\"username\\", \\"password\\") print(result) ``` In this question, students must demonstrate their ability to: - Use `imaplib` to connect to an email server. - Authenticate and search for specific emails. - Process email contents. - Handle errors gracefully. - Ensure proper resource cleanup (log out from the server). This problem tests fundamental and advanced understanding of the `imaplib` package.","solution":"def process_unread_emails(server, username, password, mailbox=\'INBOX\'): Connects to an IMAP server, fetches unread emails, marks them as read, and then logs out. Parameters: server (str): The IMAP server address. username (str): The username for login. password (str): The password for login. mailbox (str, optional): The mailbox to search for unread emails. Defaults to \'INBOX\'. Returns: list of tuples: Each tuple contains the subject and body of each email. Returns an empty list if there are no unread emails or an error occurs. try: from imaplib import IMAP4_SSL import email import re # Connect to the server M = IMAP4_SSL(server) M.login(username, password) # Select the mailbox M.select(mailbox) # Search for unread emails typ, msgnums = M.search(None, \'UNSEEN\') emails = [] # Check for no unread emails if not msgnums[0]: M.close() M.logout() return emails # Fetch each email for num in msgnums[0].split(): typ, data = M.fetch(num, \'(RFC822)\') if typ != \'OK\': continue msg = email.message_from_bytes(data[0][1]) # Extract subject subject = email.header.decode_header(msg[\'Subject\'])[0][0] if isinstance(subject, bytes): subject = subject.decode() # Extract body body = \\"\\" if msg.is_multipart(): for part in msg.walk(): if part.get_content_type() == \\"text/plain\\" and \'attachment\' not in part.get(\'Content-Disposition\', \'\'): body = part.get_payload(decode=True).decode() break else: body = msg.get_payload(decode=True).decode() emails.append((subject, body)) # Mark email as read M.store(num, \'+FLAGS\', \'Seen\') # Logout M.close() M.logout() return emails except Exception as e: print(f\\"An error occurred: {e}\\") try: M.logout() except: pass return []"},{"question":"# UUID Utility Function You are required to implement a utility function `process_uuid(uids: List[str]) -> Tuple[str, str, List[str]]` that takes in a list of UUID strings and performs the following: 1. **Normalize UUIDs**: Convert each UUID string in the list to its standardized form (remove any braces, hyphens, and convert to lowercase). 2. **Generate Random UUID**: Generate a new random UUID using the `uuid4()` function. 3. **Sort UUIDs**: Sort the normalized UUIDs lexicographically. The function should return a tuple containing: - The string representation of the generated random UUID. - The hex-formatted string of the smallest UUID in the sorted list. - The list of normalized and sorted UUID strings. # Example ```python import uuid def process_uuid(uids: List[str]) -> Tuple[str, str, List[str]]: # TODO: Implement the function as described pass # Example usage: example_uids = [ \\"{12345678-1234-5678-1234-567812345678}\\", \\"4b2f787a8c0748bb87e318f848d6b779\\", \\"urn:uuid:3862b986-3858-44b4-9d33-2afd763f9a3d\\" ] result = process_uuid(example_uids) ``` **Expected Output:** ```python # The exact random UUID (uuid4) value will vary each time you run the function (\'generated-random-uuid\', \'4b2f787a8c0748bb87e318f848d6b779\', [\'12345678123456781234567812345678\', \'3862b986385844b49d332afd763f9a3d\', \'4b2f787a8c0748bb87e318f848d6b779\']) ``` # Constraints 1. Each UUID in the input list `uids` is guaranteed to be a valid UUID string in one of the forms shown in the documentation. 2. The input list will contain at most 100 UUID strings. # Performance Requirements - The function should run efficiently with a time complexity that is linearithmic (O(n log n)) in the length of the input list, where `n` is the number of UUID strings. **Note**: 1. You can use the `uuid` module methods and classes as described in the provided documentation. 2. Ensure that your solution is robust and handles all specified UUID string formats correctly.","solution":"import uuid from typing import List, Tuple def process_uuid(uids: List[str]) -> Tuple[str, str, List[str]]: Process a list of UUID strings. :param uids: List of UUID strings in various formats :return: A tuple containing: - The string representation of a new random UUID - The hex-formatted string of the smallest UUID - The list of normalized and sorted UUID strings # Normalize the UUIDs: Convert to lowercase and remove braces/hyphens normalized_uuids = [] for uid in uids: norm_uid = uuid.UUID(uid).hex normalized_uuids.append(norm_uid) # Generate a new random UUID random_uuid = uuid.uuid4().hex # Sort the normalized UUIDs lexicographically sorted_uuids = sorted(normalized_uuids) # Get the smallest UUID in the sorted list smallest_uuid = sorted_uuids[0] return (random_uuid, smallest_uuid, sorted_uuids)"},{"question":"# Password Authentication System You are tasked with creating a simple password authentication system. For this, you will use the `getpass` module, which securely handles password input and user login information. # Your Task: Implement the following function: ```python def authenticate_user(auth_data): Authenticate the user using the provided password. Parameters: auth_data (dict): A dictionary where the keys are usernames and the values are the corresponding passwords (in plain text). Returns: bool: True if the password entered by the user matches the stored password for the logged-in user, False otherwise. import getpass # Step 1: Fetch the logged-in user\'s name using getpass.getuser(). # Step 2: Check if the user exists in auth_data. # Step 3: Prompt the user for their password (without echo) using getpass.getpass(). # Step 4: Compare the entered password with the stored password. # Step 5: Return True if the passwords match, False otherwise. # Your code here ``` # Input: - `auth_data`: A dictionary where the keys are usernames (strings) and the values are the corresponding passwords (strings). For example: ```python { \\"john_doe\\": \\"securepassword123\\", \\"jane_smith\\": \\"mypassword456\\" } ``` # Output: - The function should return `True` if the logged-in user\'s password matches the stored password; otherwise, it should return `False`. # Constraints: - Use `getpass.getuser()` to fetch the logged-in user\'s name. - Use `getpass.getpass()` to securely prompt for the user\'s password. - Assume that all usernames and passwords in `auth_data` are in plain text. # Example: ```python auth_data = { \\"john_doe\\": \\"securepassword123\\", \\"jane_smith\\": \\"mypassword456\\" } # Assuming the logged-in user is \\"john_doe\\" and they enter \\"securepassword123\\" when prompted print(authenticate_user(auth_data)) # Output: True # Assuming the logged-in user is \\"jane_smith\\" and they enter \\"wrongpassword\\" when prompted print(authenticate_user(auth_data)) # Output: False ``` Here, the `authenticate_user` function should use data from the `getpass` module to perform authentication securely. The examples assume that the logged-in user (`\\"john_doe\\"` or `\\"jane_smith\\"`) follows the username and password structure provided in `auth_data`. # Notes: - Make sure the solution handles different cases properly, and secure input handling via `getpass` is mandatory. - For testing purposes, you might simulate the user name and password inputs.","solution":"def authenticate_user(auth_data): Authenticate the user using the provided password. Parameters: auth_data (dict): A dictionary where the keys are usernames and the values are the corresponding passwords (in plain text). Returns: bool: True if the password entered by the user matches the stored password for the logged-in user, False otherwise. import getpass # Step 1: Fetch the logged-in user\'s name using getpass.getuser(). logged_in_user = getpass.getuser() # Step 2: Check if the user exists in auth_data. if logged_in_user not in auth_data: return False # Step 3: Prompt the user for their password (without echo) using getpass.getpass(). entered_password = getpass.getpass(prompt=\'Enter your password: \') # Step 4: Compare the entered password with the stored password. stored_password = auth_data[logged_in_user] # Step 5: Return True if the passwords match, False otherwise. return entered_password == stored_password"},{"question":"**Question: Task Scheduler** You need to implement a task scheduler that can handle the following requirements: 1. The scheduler must manage tasks with their start times, descriptions, and priorities. 2. Tasks must be stored in such a way that they can be retrieved in the order of their start times. 3. The scheduler must support adding, deleting, and fetching the next task to be executed. 4. If two tasks have the same start time, the one with higher priority should be executed first. 5. The scheduler should be able to return all tasks ordered by their start time and priority. # Function Signature: ```python class TaskScheduler: def __init__(self): pass def add_task(self, start_time: datetime, description: str, priority: int): pass def remove_task(self, start_time: datetime, description: str): pass def get_next_task(self) -> Tuple[datetime, str, int]: pass def get_all_tasks(self) -> List[Tuple[datetime, str, int]]: pass ``` # Constraints: - `start_time`: A `datetime` object representing when the task is scheduled to start. - `description`: A string describing the task. - `priority`: An integer where a higher value indicates higher priority. - The `add_task` method should allow adding a task with a unique `(start_time, description)` pair. - Duplicate `(start_time, description)` pairs should raise an Exception. - The `remove_task` method should remove a task identified by its `(start_time, description)` pair. - The `get_next_task` method should return the next task to be executed. # Example Usage: ```python from datetime import datetime scheduler = TaskScheduler() scheduler.add_task(datetime(2023, 10, 5, 12, 0), \\"Task 1\\", 10) scheduler.add_task(datetime(2023, 10, 5, 14, 0), \\"Task 2\\", 5) scheduler.add_task(datetime(2023, 10, 5, 12, 0), \\"Task 3\\", 20) print(scheduler.get_next_task()) # Output should be: (datetime(2023, 10, 5, 12, 0), \\"Task 3\\", 20) scheduler.remove_task(datetime(2023, 10, 5, 12, 0), \\"Task 3\\") print(scheduler.get_all_tasks()) # Output should be: # [ # (datetime(2023, 10, 5, 12, 0), \\"Task 1\\", 10), # (datetime(2023, 10, 5, 14, 0), \\"Task 2\\", 5) # ] ``` # Notes: - Utilize appropriate data structures from the `collections` module to manage the tasks efficiently. - Ensure that operations like adding and removing tasks maintain the order and priority constraints. - Consider edge cases such as no tasks in the scheduler when calling `get_next_task` or `get_all_tasks`.","solution":"from datetime import datetime from typing import List, Tuple import heapq class TaskScheduler: def __init__(self): self.tasks = [] def add_task(self, start_time: datetime, description: str, priority: int): for task in self.tasks: if task[1] == start_time and task[2] == description: raise Exception(\\"Task with the same start time and description already exists.\\") heapq.heappush(self.tasks, (-priority, start_time, description)) def remove_task(self, start_time: datetime, description: str): for i, task in enumerate(self.tasks): if task[1] == start_time and task[2] == description: del self.tasks[i] heapq.heapify(self.tasks) return raise Exception(\\"Task not found.\\") def get_next_task(self) -> Tuple[datetime, str, int]: if not self.tasks: return None priority, start_time, description = heapq.heappop(self.tasks) return (start_time, description, -priority) def get_all_tasks(self) -> List[Tuple[datetime, str, int]]: return sorted( [(task[1], task[2], -task[0]) for task in self.tasks], key=lambda x: (x[0], -x[2]) )"},{"question":"# Kernel Approximation with Nystroem Method In this question, you are required to demonstrate your understanding of the Nystroem method for kernel approximation in `scikit-learn` by implementing a machine learning pipeline that uses it. Problem Statement You are provided with a dataset consisting of features and labels. You need to perform the following steps: 1. Implement kernel approximation using the Nystroem method. 2. Use the transformed features to train a linear SVM model. 3. Evaluate the performance of your model. Requirements 1. **Data Loading** - You will be given a dataset `X` (features) and `y` (labels). Load this dataset for processing. 2. **Kernel Approximation** - Use the `Nystroem` class from `sklearn.kernel_approximation` to approximate the feature map using an RBF kernel. - Set `n_components` to 100 for the transformation. - Fit and transform the dataset `X` using the `Nystroem` instance. 3. **Model Training and Evaluation** - Train a linear SVM classifier using the transformed features. - Split the dataset into a training set and a test set (80% training, 20% testing). - Evaluate the trained model using the accuracy score on the test set. Input and Output Format - **Input:** - `X`: A 2D list of shape `(n_samples, n_features)` representing the features. - `y`: A 1D list of length `n_samples` representing the labels. - **Output:** - Accuracy score of the trained model on the test set. Function Signature ```python def kernel_approximation_nystroem(X: List[List[float]], y: List[int]) -> float: # Your code here pass ``` Example ```python # Example dataset X = [[0, 0], [1, 1], [1, 0], [0, 1]] y = [0, 0, 1, 1] # Expected output: Accuracy score as a float ``` Constraints - Use `n_components=100` for the Nystroem kernel approximation. - Use an RBF kernel for the Nystroem method. - The dataset has at least 50 samples. Additional Information - You can assume that all necessary libraries (e.g., numpy, sklearn) are already imported. - Document the steps and logic used in your code clearly.","solution":"from typing import List from sklearn.kernel_approximation import Nystroem from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score def kernel_approximation_nystroem(X: List[List[float]], y: List[int]) -> float: Perform kernel approximation using the Nystroem method and train a linear SVM model. Args: X: List[List[float]]: The features of the dataset. y: List[int]: The labels of the dataset. Returns: float: The accuracy score of the model on the test set. # Split the data into training and testing sets (80% training and 20% testing) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Perform kernel approximation using the Nystroem method feature_map_nystroem = Nystroem(kernel=\'rbf\', n_components=100, random_state=42) X_train_transformed = feature_map_nystroem.fit_transform(X_train) X_test_transformed = feature_map_nystroem.transform(X_test) # Train a linear SVM model model = SVC(kernel=\'linear\', random_state=42) model.fit(X_train_transformed, y_train) # Predict on the test set and calculate accuracy y_pred = model.predict(X_test_transformed) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Problem Statement:** You are tasked with creating a simple simulation of the Python interactive interpreter\'s history feature. Your goal is to implement a class `CommandHistory` which will manage a history of commands and simulate interactive retrieval of previous commands. # Requirements: 1. **Class Definition**: Define a class `CommandHistory`. 2. **Methods**: - `add_command(command: str)`: Adds a command to the history. - `get_last_command() -> str`: Retrieves the last command entered. - `get_nth_command(n: int) -> str`: Retrieves the nth command from the history, where n is 1-based (1 is the first command entered). - `load_history(file_path: str)`: Loads command history from a given file. Each line in the file represents a command. - `save_history(file_path: str)`: Saves the current command history to a given file. 3. **Input/Output Formats**: - `add_command(command: str)` - Input: a string `command`; Output: None. - `get_last_command() -> str` - Input: None; Output: a string containing the last command. - `get_nth_command(n: int) -> str` - Input: an integer `n`; Output: a string containing the nth command. - `load_history(file_path: str)` - Input: a string `file_path` (path to the file); Output: None. - `save_history(file_path: str)` - Input: a string `file_path` (path to the file); Output: None. 4. **Constraints/Limitation**: - The history should maintain the order of commands as they are added. - If `get_nth_command` is called with an invalid `n` (e.g. out of range), return an empty string. - The file used for `load_history` and `save_history` should exist and be accessible. 5. **Performance Requirements**: - The methods should be efficient for practical use, i.e., adding and retrieving commands should be O(1) on average. # Example Usage: ```python # Create an instance of CommandHistory history = CommandHistory() # Add commands to history history.add_command(\\"print(\'Hello, World!\')\\") history.add_command(\\"x = 5\\") history.add_command(\\"print(x)\\") # Retrieve commands assert history.get_last_command() == \\"print(x)\\" assert history.get_nth_command(1) == \\"print(\'Hello, World!\')\\" assert history.get_nth_command(5) == \\"\\" # Out of range # Save history to a file history.save_history(\\"history.txt\\") # Load the history from a file history.load_history(\\"history.txt\\") assert history.get_last_command() == \\"print(x)\\" ``` # Note: - Ensure that your implementation handles edge cases, such as adding no commands or retrieving commands from an empty history.","solution":"class CommandHistory: def __init__(self): self.commands = [] def add_command(self, command: str): self.commands.append(command) def get_last_command(self) -> str: return self.commands[-1] if self.commands else \\"\\" def get_nth_command(self, n: int) -> str: if 1 <= n <= len(self.commands): return self.commands[n-1] else: return \\"\\" def load_history(self, file_path: str): with open(file_path, \'r\') as file: self.commands = [line.strip() for line in file] def save_history(self, file_path: str): with open(file_path, \'w\') as file: for command in self.commands: file.write(command + \'n\')"},{"question":"Objective Your task is to demonstrate your understanding of the seaborn library by manipulating and visualizing datasets using `pandas` for data manipulation and `seaborn` for plotting. Dataset You will be working with a dataset that records the number of bike-sharing rentals per hour in a city. The dataset contains the following columns: - `datetime`: Date and time of the recording. - `season`: Season during which the record was made (1: Winter, 2: Spring, 3: Summer, 4: Fall). - `weather`: Weather situation (1: Clear, 2: Mist, 3: Light Snow, 4: Heavy Rain). - `temp`: Temperature in Celsius. - `humidity`: Humidity percentage. - `windspeed`: Wind speed. - `rental_count`: Number of bikes rented. Your objective is to visualize this dataset in two different formslong-form and wide-formand generate meaningful plots. Tasks 1. **Load the Dataset:** Load the dataset into a DataFrame and display the first few rows. Assume the dataset is stored in a CSV file named `bike_sharing.csv`. 2. **Transform the Dataset:** a. Create a \'month\' column by extracting the month from the `datetime`. b. Create a \'hour\' column by extracting the hour from the `datetime`. c. Convert the dataset into long-form and wide-form structures: - Long-form: Each row contains a single observation for all variables. - Wide-form: Pivot the dataset to show the average rental count per hour for each month. 3. **Visualize the Data (Long-form):** a. Create a line plot showing the average rental count per hour, colored by the month. b. Create a categorical plot showing the distribution of humidity levels for each weather situation. 4. **Visualize the Data (Wide-form):** a. Create a heatmap showing the average rental counts per hour and month. b. Create a line plot using the wide-form dataset showcasing the hourly rental counts for different months. 5. **Analysis and Insights:** Write a brief analysis (2-3 sentences) summarizing key trends or insights from each plot. Input - A CSV file named `bike_sharing.csv` containing the bike-sharing rental dataset. Output - Display the transformed datasets. - Generate and display the specified plots. - Provide a brief textual analysis of the results. Constraints - Ensure that your code is efficient and uses appropriate pandas and seaborn functionalities. - Ensure that all plots are labeled properly for clarity. Sample Code Template ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Task 1: Load Dataset df = pd.read_csv(\\"bike_sharing.csv\\") print(\\"Initial Dataset:\\") print(df.head()) # Task 2: Data Transformation # a. Extract month and hour from datetime df[\'datetime\'] = pd.to_datetime(df[\'datetime\']) df[\'month\'] = df[\'datetime\'].dt.month df[\'hour\'] = df[\'datetime\'].dt.hour # b. Creating Long-form and Wide-form datasets long_form = df.melt(id_vars=[\'datetime\', \'season\', \'weather\', \'temp\', \'humidity\', \'windspeed\', \'month\', \'hour\'], var_name=\'variable\', value_name=\'value\') wide_form = df.pivot_table(index=\'hour\', columns=\'month\', values=\'rental_count\', aggfunc=\'mean\') print(\\"Long-form Dataset:\\") print(long_form.head()) print(\\"Wide-form Dataset:\\") print(wide_form.head()) # Task 3: Visualization of Long-form Data # a. Line plot plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'hour\', y=\'rental_count\', hue=\'month\') plt.title(\'Average Bike Rental Count per Hour by Month\') plt.show() # b. Categorical plot plt.figure(figsize=(10, 6)) sns.catplot(data=df, x=\'weather\', y=\'humidity\', kind=\'box\', aspect=1.5) plt.title(\'Distribution of Humidity Levels for Each Weather Situation\') plt.show() # Task 4: Visualization of Wide-form Data # a. Heatmap plt.figure(figsize=(10, 6)) sns.heatmap(wide_form, annot=True, fmt=\\".1f\\", cmap=\'viridis\') plt.title(\'Heatmap of Average Rental Counts per Hour and Month\') plt.show() # b. Line plot plt.figure(figsize=(10, 6)) sns.lineplot(data=wide_form) plt.title(\'Hourly Bike Rental Counts for Different Months\') plt.show() # Task 5: Analysis and Insights # Write your analysis here ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() def load_and_transform_data(filename): # Task 1: Load Dataset df = pd.read_csv(filename) print(\\"Initial Dataset:\\") print(df.head()) # Task 2: Data Transformation # a. Extract month and hour from datetime df[\'datetime\'] = pd.to_datetime(df[\'datetime\']) df[\'month\'] = df[\'datetime\'].dt.month df[\'hour\'] = df[\'datetime\'].dt.hour # b. Creating Long-form and Wide-form datasets long_form = df.melt(id_vars=[\'datetime\', \'season\', \'weather\', \'temp\', \'humidity\', \'windspeed\', \'month\', \'hour\'], var_name=\'variable\', value_name=\'value\') wide_form = df.pivot_table(index=\'hour\', columns=\'month\', values=\'rental_count\', aggfunc=\'mean\') print(\\"Long-form Dataset:\\") print(long_form.head()) print(\\"Wide-form Dataset:\\") print(wide_form.head()) return df, long_form, wide_form def visualize_data(df, long_form, wide_form): # Task 3: Visualization of Long-form Data # a. Line plot plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'hour\', y=\'rental_count\', hue=\'month\') plt.title(\'Average Bike Rental Count per Hour by Month\') plt.show() # b. Categorical plot plt.figure(figsize=(10, 6)) sns.catplot(data=df, x=\'weather\', y=\'humidity\', kind=\'box\', aspect=1.5) plt.title(\'Distribution of Humidity Levels for Each Weather Situation\') plt.show() # Task 4: Visualization of Wide-form Data # a. Heatmap plt.figure(figsize=(10, 6)) sns.heatmap(wide_form, annot=True, fmt=\\".1f\\", cmap=\'viridis\') plt.title(\'Heatmap of Average Rental Counts per Hour and Month\') plt.show() # b. Line plot plt.figure(figsize=(10, 6)) sns.lineplot(data=wide_form) plt.title(\'Hourly Bike Rental Counts for Different Months\') plt.show() # Task 5: Analysis and Insights # Analysis and insights are to be written based on the plots generated"},{"question":"# Question: Customizing Seaborn Plotting Context You are tasked with demonstrating your knowledge of seaborn\'s plotting context customization features. Seaborn allows users to customize the appearance of their plots by changing the plotting parameters based on different contexts, such as \\"talk\\", \\"paper\\", \\"notebook\\", etc. In this assessment, you will need to: 1. Create a simple line plot using seaborn\'s `sns.lineplot` function. 2. Create the same plot but temporarily change the plotting context to \\"talk\\" using seaborn\'s context manager. 3. Create a function named `compare_plotting_contexts()` that: - Takes no arguments. - Creates two subplots, one with the default plotting context and one with the \\"talk\\" context. - Plots the same line plots on both subplots for comparison. Requirements: 1. Use seaborn (imported as `sns`) for creating the plots. 2. Use the context manager feature to temporarily change the plotting context. 3. Display the final output using matplotlib\'s `plt.show()`. Input: None Output: Two subplots comparing the default plotting context with the \\"talk\\" context for the same line plot. Example: The output should display two subplots: - The left plot should demonstrate the default plotting context. - The right plot should demonstrate the \\"talk\\" plotting context. ```python import seaborn as sns import matplotlib.pyplot as plt def compare_plotting_contexts(): fig, axes = plt.subplots(1, 2, figsize=(12, 6)) # Default plotting context sns.lineplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[1, 3, 2], ax=axes[0]) axes[0].set_title(\'Default Context\') # \\"Talk\\" plotting context with sns.plotting_context(\\"talk\\"): sns.lineplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[1, 3, 2], ax=axes[1]) axes[1].set_title(\'Talk Context\') plt.show() # Call the function to generate the plots compare_plotting_contexts() ``` This assessment will test your understanding of seaborn\'s plotting contexts and your ability to apply this knowledge in practical plotting scenarios.","solution":"import seaborn as sns import matplotlib.pyplot as plt def compare_plotting_contexts(): fig, axes = plt.subplots(1, 2, figsize=(12, 6)) # Sample data data = {\\"x\\": [\\"A\\", \\"B\\", \\"C\\"], \\"y\\": [1, 3, 2]} # Default plotting context sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"], ax=axes[0]) axes[0].set_title(\'Default Context\') # \\"Talk\\" plotting context with sns.plotting_context(\\"talk\\"): sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"], ax=axes[1]) axes[1].set_title(\'Talk Context\') plt.show()"},{"question":"Coding Assessment Question # Objective To assess your understanding of Python list manipulations and your ability to implement list functionalities efficiently and correctly. # Problem Statement You are tasked with implementing a Python class that mimics a subset of the Python C API functions for list manipulation. Youll need to implement various methods that will allow the creation, modification, and retrieval of list elements as described in the given documentation. # Class Requirements Create a class named `PyList` that includes the following methods: 1. `__init__(self, length: int)` - **Input**: An integer `length` representing the initial size of the list. - **Output**: Initializes a list of the specified length with all items set to `None`. - **Constraints**: `length` should be a non-negative integer. 2. `check(self) -> bool` - **Output**: Returns `True` if the object is a list, else `False`. - **Note**: For this exercise, this method should always return `True`. 3. `size(self) -> int` - **Output**: Returns the length of the list. 4. `get_item(self, index: int)` - **Input**: An integer `index`. - **Output**: Returns the item at the given index or raises an `IndexError` if the index is out of bounds. 5. `set_item(self, index: int, item)` - **Input**: An integer `index` and the item to be placed at that index. - **Output**: Sets the item at the specified `index`. Raises an `IndexError` if the index is out of bounds. 6. `insert(self, index: int, item)` - **Input**: An integer `index` and the item to be inserted. - **Output**: Inserts the item at the given index, shifting all subsequent elements to the right. Raises an `IndexError` if the index is out of bounds. 7. `append(self, item)` - **Input**: The item to be appended. - **Output**: Appends the item to the end of the list. 8. `sort(self)` - **Output**: Sorts the list in place. 9. `reverse(self)` - **Output**: Reverses the items of the list in place. 10. `as_tuple(self) -> tuple` - **Output**: Returns a new tuple containing the contents of the list. # Example ```python py_list = PyList(3) print(py_list.size()) # Output: 3 py_list.set_item(0, 10) py_list.set_item(1, 20) py_list.set_item(2, 30) print(py_list.get_item(0)) # Output: 10 py_list.append(40) print(py_list.size()) # Output: 4 print(py_list.as_tuple()) # Output: (10, 20, 30, 40) py_list.sort() print(py_list.as_tuple()) # Output: (10, 20, 30, 40) py_list.reverse() print(py_list.as_tuple()) # Output: (40, 30, 20, 10) ``` # Constraints - The methods must handle invalid operations (e.g., out-of-bounds indices) gracefully, by raising appropriate errors. - The list should dynamically expand as needed, especially for the `append` and `insert` operations. # Submission Submit your implementation of the `PyList` class. Ensure your code is clean, well-documented, and well-tested.","solution":"class PyList: def __init__(self, length: int): if length < 0: raise ValueError(\\"Length cannot be negative\\") self._list = [None] * length def check(self) -> bool: return True def size(self) -> int: return len(self._list) def get_item(self, index: int): if index < 0 or index >= len(self._list): raise IndexError(\\"Index out of bounds\\") return self._list[index] def set_item(self, index: int, item): if index < 0 or index >= len(self._list): raise IndexError(\\"Index out of bounds\\") self._list[index] = item def insert(self, index: int, item): if index < 0 or index > len(self._list): raise IndexError(\\"Index out of bounds\\") self._list.insert(index, item) def append(self, item): self._list.append(item) def sort(self): self._list.sort() def reverse(self): self._list.reverse() def as_tuple(self) -> tuple: return tuple(self._list)"},{"question":"**Title**: Implement and Optimize an ElasticNet Regression Model **Objective**: Evaluate your understanding of ElasticNet regression, including the optimization of its parameters using cross-validation. **Problem Statement**: You are given a dataset that contains multiple features and a target variable. Your task is to implement an ElasticNet regression model, tune its parameters using cross-validation, and evaluate its performance using the mean squared error metric. **Dataset**: Assume you are given a dataset in CSV format with the following structure: - Features: `feature_1, feature_2, ..., feature_n` - Target: `target` Example: ``` feature_1,feature_2,...,feature_n,target 1.2,3.4,...,5.6,7.8 9.1,2.3,...,4.5,6.7 ... ``` **Requirements**: 1. Load the dataset from a file called `dataset.csv`. 2. Implement an ElasticNet regression model. 3. Perform hyperparameter tuning using cross-validation to find the best `alpha` and `l1_ratio`. 4. Evaluate the model\'s performance on the test set using the mean squared error metric. 5. Output the best parameters and the corresponding mean squared error. **Input/Output**: - Input: `dataset.csv` file - Output: The best `alpha`, `l1_ratio`, and the mean squared error on the test set. **Constraints**: - Use `cross_val_score` or `GridSearchCV` for hyperparameter tuning. - Use an 80-20 split for training and testing sets. - Use `mean_squared_error` as the evaluation metric. - You can assume that the dataset is clean and does not contain missing values. **Performance Requirements**: - The model should be trained efficiently and the hyperparameter tuning should be comprehensive enough to find an optimal set of parameters. **Code Template**: ```python import pandas as pd from sklearn.linear_model import ElasticNet from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error # Load dataset data = pd.read_csv(\'dataset.csv\') # Split features and target X = data.drop(columns=\'target\') y = data[\'target\'] # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the ElasticNet model model = ElasticNet() # Define the parameter grid param_grid = { \'alpha\': [0.1, 1, 10, 100], \'l1_ratio\': [0.1, 0.5, 0.7, 1] } # Implement GridSearchCV for hyperparameter tuning grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train, y_train) # Get the best parameters and the corresponding mean squared error best_alpha = grid_search.best_params_[\'alpha\'] best_l1_ratio = grid_search.best_params_[\'l1_ratio\'] y_pred = grid_search.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\\"Best alpha: {best_alpha}\\") print(f\\"Best l1_ratio: {best_l1_ratio}\\") print(f\\"Mean Squared Error: {mse}\\") ``` **Evaluation**: Your solution will be evaluated based on: - Correct implementation of the ElasticNet regression model. - Proper use of cross-validation for hyperparameter tuning. - Calculation of the mean squared error. - Code efficiency and readability.","solution":"import pandas as pd from sklearn.linear_model import ElasticNet from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error def load_dataset(file_path): Load the dataset from a CSV file. return pd.read_csv(file_path) def get_best_elasticnet_params(X, y): Find the best parameters for ElasticNet regression using GridSearchCV. Args: X (pd.DataFrame): Feature matrix. y (pd.Series): Target variable. Returns: dict: Best parameters and their corresponding mean squared error. # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the ElasticNet model model = ElasticNet() # Define the parameter grid param_grid = { \'alpha\': [0.1, 1, 10, 100], \'l1_ratio\': [0.1, 0.5, 0.7, 1] } # Implement GridSearchCV for hyperparameter tuning grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train, y_train) # Get the best parameters best_params = grid_search.best_params_ best_model = grid_search.best_estimator_ # Predict on the test set y_pred = best_model.predict(X_test) # Calculate mean squared error mse = mean_squared_error(y_test, y_pred) return { \'best_alpha\': best_params[\'alpha\'], \'best_l1_ratio\': best_params[\'l1_ratio\'], \'mean_squared_error\': mse }"},{"question":"You are tasked with implementing a Python script that sends an email using the `smtplib` module. Specifically, you will: 1. Connect to an SMTP server. 2. Authenticate with a username and password. 3. Send an email from a sender to multiple recipients. 4. Handle possible exceptions that might occur during the process. # Requirements: 1. Create a function `send_email_smtp(server_address, server_port, username, password, from_address, to_addresses, subject, message_body)` that: - Connects to the specified SMTP server at `server_address:server_port`. - Authenticates using `username` and `password`. - Sends an email with the specified `from_address` to the list of `to_addresses`. - Adds a `Subject` header to the email. - Handles the following exceptions that might be raised during the email sending process: - `smtplib.SMTPConnectError` - `smtplib.SMTPAuthenticationError` - `smtplib.SMTPRecipientsRefused` - `smtplib.SMTPDataError` - `smtplib.SMTPException` - Closes the connection to the server once the email is sent or an exception is raised. # Input: - `server_address` (str): The address of the SMTP server. - `server_port` (int): The port of the SMTP server. - `username` (str): The username for authenticating with the SMTP server. - `password` (str): The password for authenticating with the SMTP server. - `from_address` (str): The email address sending the email. - `to_addresses` (list of str): A list of email addresses receiving the email. - `subject` (str): The subject of the email. - `message_body` (str): The body of the email. # Output: - Print appropriate success/failure messages based on the operations performed. # Constraints: - If the server does not support ESMTP, the function should attempt to use basic SMTP. - All output should be printed in English. - Make sure not to include any hard-coded credentials in your implementation. # Example Usage: ```python server_address = \'smtp.example.com\' server_port = 587 username = \'user@example.com\' password = \'password\' from_address = \'user@example.com\' to_addresses = [\'recipient1@example.com\', \'recipient2@example.com\'] subject = \'Test Email\' message_body = \'This is a test email.\' send_email_smtp(server_address, server_port, username, password, from_address, to_addresses, subject, message_body) ``` This example should connect to `smtp.example.com` on port `587`, authenticate with the provided credentials, and send an email to `recipient1@example.com` and `recipient2@example.com` with the subject \\"Test Email\\" and the body \\"This is a test email.\\"","solution":"import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart def send_email_smtp(server_address, server_port, username, password, from_address, to_addresses, subject, message_body): try: # Connect to the server server = smtplib.SMTP(server_address, server_port) server.ehlo() if server.has_extn(\'STARTTLS\'): server.starttls() server.ehlo() # Login to the server server.login(username, password) # Create the email msg = MIMEMultipart() msg[\'From\'] = from_address msg[\'To\'] = \', \'.join(to_addresses) msg[\'Subject\'] = subject msg.attach(MIMEText(message_body, \'plain\')) # Send the email server.sendmail(from_address, to_addresses, msg.as_string()) print(\\"Email sent successfully\\") except smtplib.SMTPConnectError as e: print(f\\"Failed to connect to the server: {e}\\") except smtplib.SMTPAuthenticationError as e: print(f\\"Authentication failed: {e}\\") except smtplib.SMTPRecipientsRefused as e: print(f\\"Recipient address refused: {e}\\") except smtplib.SMTPDataError as e: print(f\\"SMTP data error: {e}\\") except smtplib.SMTPException as e: print(f\\"SMTP error occurred: {e}\\") finally: server.quit()"},{"question":"You are tasked with implementing a type-safe system that utilizes multiple advanced features provided by the `typing` module. The goal is to design a small library for an e-commerce platform that includes handling users, orders, and generic functions. This assessment will test your knowledge of type variables, generics, structural typing, and type aliases. # Requirements 1. **Define a `User` class**: - This class should utilize `NewType` to define a distinct type for `UserId`, which is an integer. - The class should have attributes `user_id` (UserId) and `name` (str). 2. **Define an `Order` class**: - This class should have attributes `order_id` (int), `user` (User), and `amount` (float). 3. **Implement a `calculate_discount` function**: - The function should take an `Order` object and return the discount amount. - Use `@overload` to define different discount calculation strategies depending on the type of user (e.g., premium users may have a different discount). 4. **Define a generic `Container` class**: - This class should inherit from `Generic` and should be able to store any type of items (use `TypeVar`). - Include methods to add an item, remove an item, and get all items. Ensure type safety in all operations. 5. **Implement a `process_orders` function**: - This function should take a `Container` of `Order` objects, process each order (by calling `calculate_discount`), and return a summary dictionary where the keys are `UserId` and the values are total discounts applied. # Input and Output Requirements User Class - **Input**: user_id: UserId, name: str - **Output**: Creates a User object. Order Class - **Input**: order_id: int, user: User, amount: float - **Output**: Creates an Order object. calculate_discount Function - **Input**: order: Order - **Output**: discount: float Container Class - **Methods**: - `add(item: T) -> None` - `remove(item: T) -> bool` - `get_all() -> list[T]` process_orders Function - **Input**: orders: Container[Order] - **Output**: dict[UserId, float] # Constraints - The implementation should strictly adhere to the type hints and type safety enforced by the `typing` module. - Optimize for readability and maintainability of the code. # Example ```python from typing import NewType, Generic, TypeVar, List, Dict, overload # Step 1: Define User class UserId = NewType(\'UserId\', int) class User: user_id: UserId name: str def __init__(self, user_id: UserId, name: str) -> None: self.user_id = user_id self.name = name # Step 2: Define Order class class Order: order_id: int user: User amount: float def __init__(self, order_id: int, user: User, amount: float) -> None: self.order_id = order_id self.user = user self.amount = amount # Step 3: Implement calculate_discount function @overload def calculate_discount(order: Order) -> float: ... def calculate_discount(order: Order) -> float: if order.user.name == \'premium\': return order.amount * 0.2 return order.amount * 0.1 # Step 4: Define Container class T = TypeVar(\'T\') class Container(Generic[T]): def __init__(self) -> None: self._items: List[T] = [] def add(self, item: T) -> None: self._items.append(item) def remove(self, item: T) -> bool: if item in self._items: self._items.remove(item) return True return False def get_all(self) -> List[T]: return self._items # Step 5: Implement process_orders function def process_orders(orders: Container[Order]) -> Dict[UserId, float]: result: Dict[UserId, float] = {} for order in orders.get_all(): discount = calculate_discount(order) if order.user.user_id in result: result[order.user.user_id] += discount else: result[order.user.user_id] = discount return result # Usage Example user1 = User(UserId(1), \'premium\') user2 = User(UserId(2), \'regular\') order1 = Order(101, user1, 100.0) order2 = Order(102, user2, 200.0) order_container = Container[Order]() order_container.add(order1) order_container.add(order2) summary = process_orders(order_container) print(summary) # Should output: {UserId(1): 20.0, UserId(2): 20.0} ``` Complete the task by implementing the `calculate_discount` function and the generic `Container` class as specified.","solution":"from typing import NewType, Generic, TypeVar, List, Dict, overload # Step 1: Define User class UserId = NewType(\'UserId\', int) class User: user_id: UserId name: str def __init__(self, user_id: UserId, name: str) -> None: self.user_id = user_id self.name = name # Step 2: Define Order class class Order: order_id: int user: User amount: float def __init__(self, order_id: int, user: User, amount: float) -> None: self.order_id = order_id self.user = user self.amount = amount # Step 3: Implement calculate_discount function @overload def calculate_discount(order: Order) -> float: ... def calculate_discount(order: Order) -> float: if order.user.name == \'premium\': return order.amount * 0.2 return order.amount * 0.1 # Step 4: Define Container class T = TypeVar(\'T\') class Container(Generic[T]): def __init__(self) -> None: self._items: List[T] = [] def add(self, item: T) -> None: self._items.append(item) def remove(self, item: T) -> bool: if item in self._items: self._items.remove(item) return True return False def get_all(self) -> List[T]: return self._items # Step 5: Implement process_orders function def process_orders(orders: Container[Order]) -> Dict[UserId, float]: result: Dict[UserId, float] = {} for order in orders.get_all(): discount = calculate_discount(order) if order.user.user_id in result: result[order.user.user_id] += discount else: result[order.user.user_id] = discount return result"},{"question":"# Create a Climate Data Heatmap with Seaborn You are provided with a dataset containing average monthly temperatures recorded in different cities over a year. The dataset is in a CSV format as shown below: ``` City,January,February,March,April,May,June,July,August,September,October,November,December New York,0.4,1.2,5.1,11.1,16.4,21.9,24.9,24.2,20.1,14.1,8.6,2.9 Los Angeles,13.4,14.7,15.5,17.2,18.1,20.2,22.6,22.9,22.1,19.3,15.3,12.8 Chicago,-5.3,-3.2,2.7,8.9,15.3,20.4,23.5,22.8,18.2,11.8,5.4,-1.3 Houston,12.1,14.0,17.5,21.1,25.1,27.9,29.4,29.5,27.0,22.2,16.7,12.8 ``` Your task is to: 1. Load this dataset using pandas. 2. Reshape the dataset to format suitable for a heatmap with cities as columns and months as rows. 3. Use `seaborn` to create a heatmap of the temperatures. 4. Customize the heatmap by: - Adding annotations to each cell displaying the temperature value. - Formatting the annotations to one decimal place. - Setting a meaningful colormap. - Adding grid lines within the cells. - Adjusting the axis labels for better readability. Implement a function `create_climate_heatmap(file_path: str) -> None` that takes the file path of the CSV as input and displays the heatmap. Constraints: - You must use `seaborn` and `pandas` libraries. - The annotations should display the temperature values with one decimal place. - The heatmap should be visually appealing and easy to interpret. Example Usage: ```python create_climate_heatmap(\\"path/to/climate_data.csv\\") ``` Expected output should be a rendered heatmap displayed using matplotlib functions.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_climate_heatmap(file_path: str) -> None: Create a heatmap of average monthly temperatures for different cities. Parameters: file_path (str): The path to the CSV file containing the temperature data. # Load the dataset df = pd.read_csv(file_path) # Set \'City\' as the index df.set_index(\'City\', inplace=True) # Transpose the dataframe to make cities columns and months rows df = df.T # Create the heatmap plt.figure(figsize=(12, 8)) heatmap = sns.heatmap(df, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidths=.5, linecolor=\'gray\') # Adjust labels heatmap.set_xlabel(\'City\') heatmap.set_ylabel(\'Month\') heatmap.set_title(\'Average Monthly Temperatures (C)\') # Rotate the x-axis labels for readability plt.xticks(rotation=45, ha=\'right\') plt.yticks(rotation=0) # Display the heatmap plt.show()"},{"question":"Title: Implementing and Extending Data Structures Description: In this exercise, you will implement a custom data structure, `AdvancedQueue`, which extends the functionality of a basic queue to include new features. An `AdvancedQueue` works like a standard queue but includes additional methods for removing elements from the end and iterating in reverse order. Your task is to define the classes and their methods as described below: Requirements: 1. **Class Definition:** - Define a class `BasicQueue` that models a basic queue with the standard operations (enqueue and dequeue). - Define a class `AdvancedQueue` that inherits from `BasicQueue` and adds new functionalities. 2. **Class `BasicQueue`:** - Attributes: - `items` (list): An instance variable that maintains the queue\'s items. - Methods: - `enqueue(item)`: Adds `item` to the end of the queue. - `dequeue()`: Removes and returns the item from the front of the queue. Raises an `IndexError` if the queue is empty. - `__iter__()`: Returns an iterator for the queue. 3. **Class `AdvancedQueue`:** - Inherits from `BasicQueue`. - Methods: - `remove_last()`: Removes and returns the item from the end of the queue. Raises an `IndexError` if the queue is empty. - `__reversed__()`: Returns an iterator that produces items from the queue in reverse order using a generator. 4. **Additional Constraints:** - Ensure that attribute access is properly managed between classes. - Raise appropriate exceptions where necessary. - The solution should maintain operational complexity close to O(1) for enqueue, dequeue, and remove_last. Example Usage: ```python # Creating a BasicQueue and using its methods basic_queue = BasicQueue() basic_queue.enqueue(1) basic_queue.enqueue(2) basic_queue.enqueue(3) print(list(basic_queue)) # Output: [1, 2, 3] print(basic_queue.dequeue()) # Output: 1 print(list(basic_queue)) # Output: [2, 3] # Creating an AdvancedQueue and using its extended methods advanced_queue = AdvancedQueue() advanced_queue.enqueue(4) advanced_queue.enqueue(5) advanced_queue.enqueue(6) print(list(advanced_queue)) # Output: [4, 5, 6] print(advanced_queue.remove_last()) # Output: 6 print(list(advanced_queue)) # Output: [4, 5] print(list(reversed(advanced_queue))) # Output: [5, 4] ``` Implementation: Write down the classes `BasicQueue` and `AdvancedQueue` with the specified methods and attributes. Ensure proper handling of exceptions and edge cases.","solution":"class BasicQueue: def __init__(self): self.items = [] def enqueue(self, item): Adds item to the end of the queue. self.items.append(item) def dequeue(self): Removes and returns the item from the front of the queue. Raises an IndexError if the queue is empty. if not self.items: raise IndexError(\\"Dequeue from an empty queue\\") return self.items.pop(0) def __iter__(self): Returns an iterator for the queue. return iter(self.items) class AdvancedQueue(BasicQueue): def remove_last(self): Removes and returns the item from the end of the queue. Raises an IndexError if the queue is empty. if not self.items: raise IndexError(\\"Remove last from an empty queue\\") return self.items.pop() def __reversed__(self): Returns an iterator that produces items from the queue in reverse order. return iter(self.items[::-1])"},{"question":"# Distributed Training with PyTorch This assessment tests your understanding of PyTorch\'s `torch.distributed` module, specifically its `torchrun` elastic launch utility used to facilitate distributed training across multiple nodes. Problem Statement You are provided with a machine learning model and a dataset. Your task is to write a PyTorch script that: 1. Initializes a distributed training group. 2. Portions the training data across different processes. 3. Sets up a simple training loop using a DistributedDataParallel (DDP) model. 4. Ensures synchronization and correct averaging of gradients across all processes. __Constraints:__ - Use the `torchrun` utility to launch the training script. - The script should be able to run with `n` processes, where `n` is the number of available GPUs. __Input and Output Formats:__ Your task is to write a PyTorch script named `distributed_training.py`. This script does not take input in the usual sense but should be designed to work in a distributed setting with PyTorch\'s `torchrun`. __Steps to implement:__ 1. **Initialize Process Group:** - Use `torch.distributed.init_process_group()` to initialize the distributed backend. 2. **Portioning Data:** - Divide the given dataset among multiple processes so that each process handles a subset of the data. 3. **Setup Model and Optimizer:** - Wrap the model with `DistributedDataParallel`. - Use any optimizer of your choice (e.g., Adam, SGD). 4. **Training Loop:** - Implement a simple training loop that performs forward and backward passes, optimizer step, and synchronizes gradients. 5. **Synchronization:** - Ensure that all gradients are correctly averaged across all the processes to maintain consistency. Here\'s a template to get you started: ```python import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, Dataset # Dummy dataset for illustration class DummyDataset(Dataset): def __init__(self, size): self.data = torch.randn(size, 10) self.target = torch.randint(0, 2, (size,)) def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index], self.target[index] # Simple feedforward neural network class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) def setup(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def train(rank, world_size): setup(rank, world_size) dataset = DummyDataset(1000) sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, batch_size=32, sampler=sampler) model = SimpleModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) loss_fn = nn.CrossEntropyLoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) for epoch in range(10): sampler.set_epoch(epoch) for data, target in dataloader: data, target = data.to(rank), target.to(rank) optimizer.zero_grad() output = ddp_model(data) loss = loss_fn(output, target) loss.backward() optimizer.step() cleanup() def main(): world_size = torch.cuda.device_count() mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main() ``` __Note:__ Ensure that NVIDIA GPUs are available for this script to run properly in a distributed setting. __Evaluation Criteria:__ - Correct usage of PyTorch distributed utilities. - Proper implementation of the distributed training loop. - Efficient synchronization and data partitioning. - Code readability and adherence to best practices.","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, Dataset # Dummy dataset for illustration class DummyDataset(Dataset): def __init__(self, size): self.data = torch.randn(size, 10) self.target = torch.randint(0, 2, (size,)) def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index], self.target[index] # Simple feedforward neural network class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) def setup(rank, world_size): dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def train(rank, world_size): setup(rank, world_size) dataset = DummyDataset(1000) sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, batch_size=32, sampler=sampler) model = SimpleModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) loss_fn = nn.CrossEntropyLoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) for epoch in range(10): sampler.set_epoch(epoch) for data, target in dataloader: data, target = data.to(rank), target.to(rank) optimizer.zero_grad() output = ddp_model(data) loss = loss_fn(output, target) loss.backward() optimizer.step() cleanup() def main(): world_size = torch.cuda.device_count() mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Demonstrate your understanding of Python logging configuration using the `logging.config` module. You will create a function that sets up the logging configuration using a dictionary format and another function to set it up using a file. # Part 1: Dictionary-Based Configuration Implement a function `setup_logging_dict()` that takes a dictionary as input and configures the logging according to the dictionary schema described in the documentation. Function Signature ```python def setup_logging_dict(config: dict) -> None: ``` Input - `config (dict)`: A dictionary representing the logging configuration. The dictionary will follow the schema described in the documentation. Requirements 1. Use `logging.config.dictConfig()` to configure the logging system. 2. Handle any exceptions (`ValueError`, `TypeError`, `AttributeError`, `ImportError`) and print an appropriate message when an error occurs. 3. Demonstrate at least two different handlers (e.g., `StreamHandler` and `FileHandler`). 4. Include at least one custom formatter and one custom filter in the configuration. # Part 2: File-Based Configuration Implement a function `setup_logging_file()` that takes a filename as input and configures the logging using the configuration file provided. Function Signature ```python def setup_logging_file(filename: str) -> None: ``` Input - `filename (str)`: The name of the configuration file. Requirements 1. Use `logging.config.fileConfig()` to configure the logging system. 2. Handle any exceptions (`FileNotFoundError`, `ImportError`, etc.) and print an appropriate message when an error occurs. 3. Demonstrate at least two different handlers (e.g., `RotatingFileHandler` and `SMTPHandler`). # Example Here is an example configuration dictionary to test your `setup_logging_dict` function: ```python config_dict = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s - %(pathname)s\' }, }, \'filters\': { \'require_debug_true\': { \'()\': \'require_debug_true\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'filters\': [\'require_debug_true\'] }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'WARNING\', \'formatter\': \'detailed\', \'filename\': \'test.log\', }, }, \'loggers\': { \'my_logger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'] } }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\', \'file\'] } } ``` You may create a corresponding configuration file in `configparser` format for testing your `setup_logging_file()` function. # Constraints - Ensure the dictionary and file configurations are valid according to the logging module requirements. # Performance - Ensure that the configuration setup is efficient and handles large configurations gracefully.","solution":"import logging import logging.config class RequireDebugTrue(logging.Filter): def filter(self, record): return logging.getLogger().isEnabledFor(logging.DEBUG) def setup_logging_dict(config): Sets up logging using dictionary-based configuration. try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError) as e: print(f\\"Error configuring logging: {e}\\") def setup_logging_file(filename): Sets up logging using file-based configuration. try: logging.config.fileConfig(filename) except (FileNotFoundError, ImportError) as e: print(f\\"Error configuring logging: {e}\\")"},{"question":"# I/O Stream Manipulation with Python 3.10 Objective: To test the student\'s ability to create and manipulate various I/O streams provided by the `io` module in Python 3.10, and to handle the appropriate data types for each type of stream. Problem Statement: You are provided with a text file (`input.txt`) containing ASCII text data and a binary file (`input.bin`) containing binary data. Your task is to: 1. Read the contents of `input.txt` and convert all text to uppercase. 2. Save the uppercase text to a new file `output.txt`. 3. Read the contents of `input.bin` and append a byte sequence `b\'x00xffx00xff\'` to it. 4. Save the modified binary data to a new file `output.bin`. Requirements: 1. Your solution should use appropriate classes and methods from the `io` module. 2. Ensure that text data is handled as `str` and binary data as `bytes`. 3. Properly handle reading and writing of both text and binary streams. 4. Ensure that your implementation is efficient and follows best practices for I/O operations. Input: - A text file `input.txt`. - A binary file `input.bin`. Output: - A text file `output.txt` containing the uppercase version of `input.txt`. - A binary file `output.bin` containing the contents of `input.bin` appended with the byte sequence `b\'x00xffx00xff\'`. Constraints: - The input files `input.txt` and `input.bin` will be less than 1MB in size. Example: Assume the contents of `input.txt` are: ``` Hello World ``` and the contents of `input.bin` are: ``` x01x02x03x04 ``` Your `output.txt` should contain: ``` HELLO WORLD ``` and your `output.bin` should contain: ``` x01x02x03x04x00xffx00xff ``` Function Signature: ```python def manipulate_io_files(): # Your code here ``` Hint: - Use `open()` with appropriate mode strings (`\'r\'`, `\'w\'`, `\'rb\'`, `\'wb\'`). - Use `io.TextIOWrapper` and `io.BytesIO` appropriately if needed.","solution":"def manipulate_io_files(): # Reading the text file and converting to uppercase with open(\'input.txt\', \'r\') as text_file: text_data = text_file.read().upper() # Writing the uppercase text to a new file with open(\'output.txt\', \'w\') as text_output_file: text_output_file.write(text_data) # Reading the binary file and appending the byte sequence with open(\'input.bin\', \'rb\') as binary_file: binary_data = binary_file.read() + b\'x00xffx00xff\' # Writing the modified binary data to a new file with open(\'output.bin\', \'wb\') as binary_output_file: binary_output_file.write(binary_data)"},{"question":"**Question: Advanced EmailMessage Manipulation** You are tasked with implementing a function to construct, manipulate, and serialize a multipart email message using the `email.message.EmailMessage` class. This function should demonstrate your understanding of headers, payloads, multipart types, and serialization. # Function Signature ```python def create_multipart_email(subject: str, from_address: str, to_address: str, body_text: str, attachment_filename: str, attachment_content: bytes) -> str: Constructs a multipart email message with a text body and an attachment. Parameters: - subject: The subject of the email. - from_address: The email address of the sender. - to_address: The email address of the recipient. - body_text: The plain text body of the email. - attachment_filename: The filename for the attachment. - attachment_content: The binary content of the attachment. Returns: - The serialized email message as a string. ``` # Task Description 1. **Email Construction**: Utilize the `EmailMessage` class to create an email with the provided parameters. 2. **Headers**: - Set the `Subject`, `From`, and `To` headers using the provided parameters. 3. **Multipart Body**: - Add the plain text body as one part of the multipart email. - Add the attachment as another part of the multipart email. - Ensure that the attachment has the appropriate `Content-Disposition` and `Content-Type` headers. 4. **Serialization**: - Serialize the resulting email message to a string format. # Constraints - The email should have `utf-8` charset for text content. - The `Content-Type` for the attachment should be `application/octet-stream`. - The attachment must be appropriately encoded. # Example ```python subject = \\"Monthly Report\\" from_address = \\"sender@example.com\\" to_address = \\"recipient@example.com\\" body_text = \\"Please find the monthly report attached.\\" attachment_filename = \\"report.pdf\\" attachment_content = b\\"%PDF-1.4...\\" result = create_multipart_email(subject, from_address, to_address, body_text, attachment_filename, attachment_content) print(result) ``` **Expected Output**: A string representation of the email message including the correct headers, multipart boundaries, and encoded attachment. Note: The exact output may vary slightly based on MIME boundaries and encoding specifics.","solution":"from email.message import EmailMessage import base64 def create_multipart_email(subject: str, from_address: str, to_address: str, body_text: str, attachment_filename: str, attachment_content: bytes) -> str: Constructs a multipart email message with a text body and an attachment. Parameters: - subject: The subject of the email. - from_address: The email address of the sender. - to_address: The email address of the recipient. - body_text: The plain text body of the email. - attachment_filename: The filename for the attachment. - attachment_content: The binary content of the attachment. Returns: - The serialized email message as a string. # Create the email message object message = EmailMessage() # Set the headers message[\'Subject\'] = subject message[\'From\'] = from_address message[\'To\'] = to_address # Set the multipart type message.set_content(body_text) # Add the plain text body part message.set_content(body_text, subtype=\'plain\', charset=\'utf-8\') # Add the attachment message.add_attachment( attachment_content, maintype=\'application\', subtype=\'octet-stream\', filename=attachment_filename ) # Serialize the email to a string return message.as_string()"},{"question":"# Advanced Data Compression and Archiving Task **Objective:** To demonstrate your comprehension of Python\'s data compression and archiving functionalities, you are required to implement a program that combines functionality from multiple compression modules. Your task is to create a compressed archive that includes both `gzip` and `bz2` compressed files and then to extract and validate the contents from this archive. # Task Description: 1. **Function `create_compressed_files(data: str, gzip_file_path: str, bz2_file_path: str) -> None`:** - **Input:** - `data` (str): Text data to be compressed. - `gzip_file_path` (str): Path where the gzip compressed file will be saved. - `bz2_file_path` (str): Path where the bz2 compressed file will be saved. - **Output:** None - **Details:** Compress the given `data` into two separate files, one using the gzip compression algorithm and the other using the bz2 compression algorithm. Save these compressed files at the specified paths. 2. **Function `create_archive(archive_path: str, gzip_file_path: str, bz2_file_path: str) -> None`:** - **Input:** - `archive_path` (str): Path where the final ZIP archive will be saved. - `gzip_file_path` (str): Path of the gzip compressed file created earlier. - `bz2_file_path` (str): Path of the bz2 compressed file created earlier. - **Output:** None - **Details:** Create a ZIP archive that includes both the gzip and bz2 compressed files and save it to the specified `archive_path`. 3. **Function `extract_and_validate_archive(archive_path: str, expected_data: str) -> bool`:** - **Input:** - `archive_path` (str): Path of the ZIP archive created in the previous step. - `expected_data` (str): The original text data that was used for compression. - **Output:** Returns `True` if the extracted data from both files matches the `expected_data`, otherwise returns `False`. - **Details:** Extract both the gzip and bz2 files from the ZIP archive at `archive_path`, decompress them, and validate that the decompressed contents match the `expected_data`. # Example Usage: ```python data = \\"This is a test string for compression.\\" gzip_file = \\"compressed_data.gz\\" bz2_file = \\"compressed_data.bz2\\" zip_archive = \\"compressed_archive.zip\\" create_compressed_files(data, gzip_file, bz2_file) create_archive(zip_archive, gzip_file, bz2_file) result = extract_and_validate_archive(zip_archive, data) print(result) # Should print True if validation is successful ``` # Constraints: - Ensure your solution is efficient and handles edge cases, such as file path errors. - Use built-in Python modules such as `gzip`, `bz2`, and `zipfile`. # Additional Notes: - Provide appropriate exception handling to account for any file operation errors. - Ensure your code is optimized and follows best practices for readability and maintainability.","solution":"import gzip import bz2 import zipfile import os def create_compressed_files(data: str, gzip_file_path: str, bz2_file_path: str) -> None: # Create gzip compressed file with gzip.open(gzip_file_path, \'wt\') as gzip_file: gzip_file.write(data) # Create bz2 compressed file with bz2.open(bz2_file_path, \'wt\') as bz2_file: bz2_file.write(data) def create_archive(archive_path: str, gzip_file_path: str, bz2_file_path: str) -> None: # Create a zip archive including both gzip and bz2 files with zipfile.ZipFile(archive_path, \'w\') as zipf: zipf.write(gzip_file_path, os.path.basename(gzip_file_path)) zipf.write(bz2_file_path, os.path.basename(bz2_file_path)) def extract_and_validate_archive(archive_path: str, expected_data: str) -> bool: with zipfile.ZipFile(archive_path, \'r\') as zipf: # Extract gz file and validate its content with zipf.open(os.path.basename(zipf.namelist()[0])) as gzip_file: with gzip.open(gzip_file, \'rt\') as gz: gz_data = gz.read() # Extract bz2 file and validate its content with zipf.open(os.path.basename(zipf.namelist()[1])) as bz2_file: with bz2.open(bz2_file, \'rt\') as bz: bz2_data = bz.read() return gz_data == expected_data and bz2_data == expected_data"},{"question":"You are tasked with developing a Python script that performs the following tasks using the `os` module: 1. Create a temporary directory named `temp_dir` in the current working directory. 2. Within `temp_dir`, create a subdirectory named `sub_dir`. 3. In `sub_dir`, create a text file named `test_file.txt` and write the string \\"Hello, World!\\" to it. 4. Change the environment variable `TEST_ENV_VAR` to the path of `sub_dir` created in step 2. 5. Write a function `read_env_file()` that performs the following: - Reads the environment variable `TEST_ENV_VAR` and gets the file path from it. - Reads the content of `test_file.txt` located in the directory path from the environment variable. - Returns the content of the file. # Function Specification `manage_directories_and_environment()` - **Input**: None - **Output**: None - **Description**: This function creates the directory and files as specified and sets the environment variable. `read_env_file()` - **Input**: None - **Output**: A string representing the content of `test_file.txt`. - **Description**: This function reads the environment variable, locates `test_file.txt`, and returns its content. # Constraints - The temporary directory `temp_dir` and its contents should be removed after you are done with the operations. - Handle any potential errors that may arise during file and directory operations (e.g., file not found, permission errors). # Example Usage ```python manage_directories_and_environment() content = read_env_file() print(content) # Output should be \\"Hello, World!\\" ``` # Additional Notes - Ensure that your script handles both creation and cleanup of files and directories to avoid leaving any temporary files after execution. - Consider edge cases, such as the absence of the environment variable, and handle them appropriately.","solution":"import os import shutil def manage_directories_and_environment(): try: # Step 1: Create a temporary directory named `temp_dir` temp_dir = os.path.join(os.getcwd(), \'temp_dir\') os.makedirs(temp_dir) # Step 2: Within `temp_dir`, create a subdirectory named `sub_dir` sub_dir = os.path.join(temp_dir, \'sub_dir\') os.makedirs(sub_dir) # Step 3: Create a text file named `test_file.txt` in `sub_dir` and write \\"Hello, World!\\" to it test_file_path = os.path.join(sub_dir, \'test_file.txt\') with open(test_file_path, \'w\') as f: f.write(\\"Hello, World!\\") # Step 4: Change the environment variable `TEST_ENV_VAR` to the path of `sub_dir` os.environ[\'TEST_ENV_VAR\'] = sub_dir except Exception as e: print(f\\"An error occurred: {e}\\") def read_env_file(): try: # Read the environment variable `TEST_ENV_VAR` sub_dir = os.environ.get(\'TEST_ENV_VAR\') if not sub_dir: raise EnvironmentError(\\"Environment variable TEST_ENV_VAR not set\\") # Construct the path to `test_file.txt` test_file_path = os.path.join(sub_dir, \'test_file.txt\') # Read and return the content of `test_file.txt` with open(test_file_path, \'r\') as f: content = f.read() return content except Exception as e: print(f\\"An error occurred: {e}\\") return None def cleanup(): try: # Cleanup `temp_dir` and its contents temp_dir = os.path.join(os.getcwd(), \'temp_dir\') if os.path.exists(temp_dir): shutil.rmtree(temp_dir) except Exception as e: print(f\\"An error occurred during cleanup: {e}\\")"},{"question":"# Custom Estimator Implementation Objective The objective of this assessment is to evaluate your understanding of developing a custom estimator in scikit-learn. You will demonstrate your ability to create a new estimator adhering to the scikit-learn conventions and integrating it within the scikit-learn ecosystem. Problem Statement Implement a custom scikit-learn compatible estimator named `CustomKNNClassifier`. This classifier should mimic the behavior of k-nearest neighbors (KNN) classifier but with the following specific functionalities: 1. It should have an additional parameter `distance_metric`, which accepts values `\'euclidean\'` or `\'manhattan\'` for the distance calculation. 2. Implement the following methods: - `__init__(self, n_neighbors=5, distance_metric=\'euclidean\')`: Initialize the classifier with the specified number of neighbors and distance metric. - `fit(self, X, y)`: Fit the model using training data `X` and labels `y`. - `predict(self, X)`: Predict the class labels for samples in `X`. - `score(self, X, y)`: Return the mean accuracy on the given test data and labels. 3. Ensure the implementation follows scikit-learn conventions, including proper handling of hyperparameters and attributes. Constraints - Do not use any existing implementations of KNN from scikit-learn. - The `distance_metric` parameter in the constructor must be either `\'euclidean\'` or `\'manhattan\'`. Raise a `ValueError` if an unsupported metric is used. - The `fit` method should store the training data appropriately. - The `predict` method should compute predictions based on the specified distance metric. - The `score` method should compute the accuracy of the classifier on the test data. Expected Function Signatures ```python class CustomKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5, distance_metric=\'euclidean\'): # Your code here def fit(self, X, y): # Your code here def predict(self, X): # Your code here def score(self, X, y): # Your code here ``` Example Usage ```python # Example data X_train = [[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]] y_train = [0, 0, 0, 1, 1] X_test = [[1, 2], [3, 4], [6, 7]] y_test = [0, 0, 1] # Create and train the classifier knn = CustomKNNClassifier(n_neighbors=3, distance_metric=\'euclidean\') knn.fit(X_train, y_train) # Predict and score predictions = knn.predict(X_test) accuracy = knn.score(X_test, y_test) print(f\\"Predictions: {predictions}\\") # Example output: [0, 0, 1] print(f\\"Accuracy: {accuracy:.2f}\\") # Example output: 1.00 ``` Notes - Make sure to include necessary imports. - Add appropriate error handling and validation checks. - Ensure the code is well-documented and follows best practices for readability and maintainability. Good luck!","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from scipy.spatial.distance import cdist class CustomKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5, distance_metric=\'euclidean\'): if distance_metric not in (\'euclidean\', \'manhattan\'): raise ValueError(\\"Unsupported distance metric, choose \'euclidean\' or \'manhattan\'\\") self.n_neighbors = n_neighbors self.distance_metric = distance_metric def fit(self, X, y): self.X_train = np.asarray(X) self.y_train = np.asarray(y) return self def predict(self, X): X = np.asarray(X) distances = cdist(X, self.X_train, metric=self.distance_metric) neighbors_idx = np.argsort(distances, axis=1)[:, :self.n_neighbors] y_pred = [] for indices in neighbors_idx: nearest_labels = self.y_train[indices] most_common = np.bincount(nearest_labels).argmax() y_pred.append(most_common) return np.array(y_pred) def score(self, X, y): y_pred = self.predict(X) return np.mean(y_pred == np.asarray(y))"},{"question":"**Problem: Monetary Calculation with Decimals** In this task, you are required to implement a Python function that performs various monetary calculations using the `decimal` module. The function should be able to handle the addition, subtraction, multiplication, and division of monetary values while respecting the specified precision and rounding mode settings. # Function Signature ```python def monetary_calculations(operations: list, precision: int, rounding_mode: str) -> List[Decimal]: pass ``` # Input - `operations` (List[Tuple[str, Decimal, Decimal]]): A list of operations to perform. Each operation is represented as a tuple containing: * A string (\\"add\\", \\"subtract\\", \\"multiply\\", or \\"divide\\") indicating the operation. * Two `Decimal` values on which the operation is to be performed. - `precision` (int): An integer indicating the precision (number of significant digits) for all calculations. - `rounding_mode` (str): A string representing the rounding mode (e.g., \\"ROUND_HALF_UP\\", \\"ROUND_DOWN\\"). # Output - List[Decimal]: A list of `Decimal` results after performing the specified operations, rounded according to the specified precision and rounding mode. # Constraints - The precision will be a positive integer. - The rounding mode will be a valid mode available in the `decimal` module. - The operations list will contain valid decimal arithmetic operations. # Example ```python from decimal import Decimal # Example usage operations = [ (\\"add\\", Decimal(\'10.345\'), Decimal(\'2.500\')), (\\"subtract\\", Decimal(\'10.345\'), Decimal(\'2.500\')), (\\"multiply\\", Decimal(\'3.1415\'), Decimal(\'2\')), (\\"divide\\", Decimal(\'10.500\'), Decimal(\'3\')) ] precision = 5 rounding_mode = \\"ROUND_HALF_UP\\" # Call the function results = monetary_calculations(operations, precision, rounding_mode) # Output: [ # Decimal(\'12.845\'), # Decimal(\'7.8450\'), # Decimal(\'6.2830\'), # Decimal(\'3.5000\') # ] print(results) ``` Implement the `monetary_calculations` function keeping in mind precision and rounding modes based on the `decimal` module capabilities. # Note 1. Use the `decimal` module\'s `getcontext` and `setcontext` to set the precision and rounding mode. 2. Ensure to import necessary parts of the `decimal` module such as `Decimal` and rounding modes. Good luck!","solution":"from decimal import Decimal, getcontext, ROUND_HALF_UP, ROUND_DOWN def monetary_calculations(operations, precision, rounding_mode): Perform monetary calculations with specified precision and rounding mode. Args: operations (list): A list of tuples where each tuple contains a string (\\"add\\", \\"subtract\\", \\"multiply\\", or \\"divide\\") and two Decimal numbers. precision (int): The precision (number of significant digits) for all calculations. rounding_mode (str): The rounding mode (e.g., \\"ROUND_HALF_UP\\", \\"ROUND_DOWN\\"). Returns: List[Decimal]: A list of Decimal results after performing the specified operations. # Set the context for monetary calculations getcontext().prec = precision getcontext().rounding = rounding_mode results = [] for operation in operations: op_type, num1, num2 = operation if op_type == \\"add\\": result = num1 + num2 elif op_type == \\"subtract\\": result = num1 - num2 elif op_type == \\"multiply\\": result = num1 * num2 elif op_type == \\"divide\\": result = num1 / num2 else: raise ValueError(\\"Unsupported operation type.\\") results.append(result) return results"},{"question":"Objective: Demonstrate your understanding of seaborn\'s plotting context and its impact on visualizing data. Problem Statement: Using Seaborn, you are required to create three distinct line plots with different aesthetic styles called `\'paper\'`, `\'notebook\'`, and `\'poster\'`. You should use seaborns `plotting_context` function to apply each of these styles temporarily while plotting. Ensure that the style applied to each plot is clearly distinguishable. Requirements: 1. Load seaborn and any other necessary libraries. 2. Generate a sample dataset containing at least 10 x-y pairs, where `x` can be a sequence of numbers and `y` can be a mix of random and deterministic values. 3. Create three line plots in a single figure, where each plot uses a different context style (`\'paper\'`, `\'notebook\'`, and `\'poster\'`). 4. Each plot should have appropriate labels for both axes and a title that indicates the applied style for clarity. Expected Input and Output: - **Input:** No direct input; your code should generate the necessary data and plot. - **Output:** A single figure containing three subplots, each with different `plotting_context` styles. Example: - The output should be a figure with three line plots arranged vertically or horizontally, with each plot using one of the specified styles (`\'paper\'`, `\'notebook\'`, `\'poster\'`). The title of each plot should indicate its style. Constraints: - The dataset should contain at least 10 data points. - The context styles must be applied using a context manager. Performance requirements: - The solution must efficiently create and display the plots without any performance bottlenecks. Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Generate sample data x = np.arange(10) y = np.random.rand(10) * 10 # Create the figure and subplots fig, axs = plt.subplots(3, 1, figsize=(10, 12)) # Plot with \'paper\' context with sns.plotting_context(\'paper\'): sns.lineplot(x=x, y=y, ax=axs[0]) axs[0].set_title(\'Paper Context\') axs[0].set_xlabel(\'X-axis\') axs[0].set_ylabel(\'Y-axis\') # Plot with \'notebook\' context with sns.plotting_context(\'notebook\'): sns.lineplot(x=x, y=y, ax=axs[1]) axs[1].set_title(\'Notebook Context\') axs[1].set_xlabel(\'X-axis\') axs[1].set_ylabel(\'Y-axis\') # Plot with \'poster\' context with sns.plotting_context(\'poster\'): sns.lineplot(x=x, y=y, ax=axs[2]) axs[2].set_title(\'Poster Context\') axs[2].set_xlabel(\'X-axis\') axs[2].set_ylabel(\'Y-axis\') # Adjust layout and show plot plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_plots(): Generates a figure with three line plots using different Seaborn context styles: \'paper\', \'notebook\', and \'poster\'. # Generate sample data x = np.arange(10) y = np.random.rand(10) * 10 # Create the figure and subplots fig, axs = plt.subplots(3, 1, figsize=(10, 15)) # Plot with \'paper\' context with sns.plotting_context(\'paper\'): sns.lineplot(x=x, y=y, ax=axs[0]) axs[0].set_title(\'Paper Context\') axs[0].set_xlabel(\'X-axis\') axs[0].set_ylabel(\'Y-axis\') # Plot with \'notebook\' context with sns.plotting_context(\'notebook\'): sns.lineplot(x=x, y=y, ax=axs[1]) axs[1].set_title(\'Notebook Context\') axs[1].set_xlabel(\'X-axis\') axs[1].set_ylabel(\'Y-axis\') # Plot with \'poster\' context with sns.plotting_context(\'poster\'): sns.lineplot(x=x, y=y, ax=axs[2]) axs[2].set_title(\'Poster Context\') axs[2].set_xlabel(\'X-axis\') axs[2].set_ylabel(\'Y-axis\') # Adjust layout and show plot plt.tight_layout() plt.show()"},{"question":"Hyper-Parameter Tuning and Evaluation with GridSearchCV In this coding challenge, you are required to implement a machine learning model to classify the famous Iris dataset using a pipeline. This task will assess your understanding of using GridSearchCV for hyper-parameter tuning and your ability to evaluate model performance. # Task: 1. **Load the Dataset**: Load the Iris dataset available from `sklearn.datasets`. This dataset consists of 150 samples with 4 features and 3 classes. 2. **Preprocess the Data**: Split the dataset into training and testing sets using an 80-20 split. 3. **Create a Pipeline**: Construct a pipeline that includes: - A standard scaler (`StandardScaler`) for data normalization. - A Support Vector Classifier (`svm.SVC`). 4. **Define Hyper-parameter Grid**: Define a dictionary of hyper-parameters for `svm.SVC` that includes: ```python param_grid = [ {\'svc__C\': [0.1, 1, 10], \'svc__kernel\': [\'linear\']}, {\'svc__C\': [0.1, 1, 10], \'svc__gamma\': [0.001, 0.01, 0.1], \'svc__kernel\': [\'rbf\']} ] ``` 5. **Perform Grid Search with Cross-Validation**: Use `GridSearchCV` with 5-fold cross-validation to search for the best hyper-parameters. 6. **Evaluate the Model**: Evaluate the best model on the test set and output the classification report, confusion matrix, and the best parameters. # Input: - There is no specific input format. You will be loading and splitting the Iris dataset internally. # Output: - Print the best hyper-parameters combination found. - Print the classification report and confusion matrix of the best model on the test set. # Constraints: - You may only use `scikit-learn` for all machine learning operations. - Your code should be efficient and should not take more than 3 minutes to run on a standard machine. # Sample Structure: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import classification_report, confusion_matrix # Load dataset iris = load_iris() X = iris.data y = iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC()) ]) # Define hyper-parameter grid param_grid = [ {\'svc__C\': [0.1, 1, 10], \'svc__kernel\': [\'linear\']}, {\'svc__C\': [0.1, 1, 10], \'svc__gamma\': [0.001, 0.01, 0.1], \'svc__kernel\': [\'rbf\']} ] # Perform grid search grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Evaluate the best model best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) print(\\"Best hyper-parameters:\\", grid_search.best_params_) print(\\"Classification Report:n\\", classification_report(y_test, y_pred)) print(\\"Confusion Matrix:n\\", confusion_matrix(y_test, y_pred)) ``` In this task, the candidate is expected to understand the importance of cross-validation, the creation of a pipeline for scalable preprocessing, and effective use of GridSearchCV for hyper-parameter tuning. The evaluation phase demonstrates their ability to present model results effectively.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import classification_report, confusion_matrix def hyperparameter_tuning_and_evaluation(): # Load dataset iris = load_iris() X = iris.data y = iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC()) ]) # Define hyper-parameter grid param_grid = [ {\'svc__C\': [0.1, 1, 10], \'svc__kernel\': [\'linear\']}, {\'svc__C\': [0.1, 1, 10], \'svc__gamma\': [0.001, 0.01, 0.1], \'svc__kernel\': [\'rbf\']} ] # Perform grid search grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Evaluate the best model best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) print(\\"Best hyper-parameters:\\", grid_search.best_params_) print(\\"Classification Report:n\\", classification_report(y_test, y_pred)) print(\\"Confusion Matrix:n\\", confusion_matrix(y_test, y_pred)) return best_model, grid_search.best_params_, classification_report(y_test, y_pred, output_dict=True), confusion_matrix(y_test, y_pred)"},{"question":"Objective Write a Python script that demonstrates your understanding of making the script executable, handling errors, using environment variables, and implementing a Python startup customization. Problem Statement You are required to create a Python script `custom_script.py` that performs the following tasks: 1. **Executable Script**: Make this Python script directly executable on Unix-like systems by including the appropriate shebang line. 2. **Interactive Startup File**: Check if an environment variable `PYTHONCUSTOMSTARTUP` is set. If so, read and execute the Python code in the specified file at the start of your script. If the file does not exist or the variable is not set, continue without errors. 3. **Error Handling**: The script should handle specific exceptions using a try-except block. Specifically, it should handle `FileNotFoundError` and `KeyboardInterrupt`. For any other exceptions, it should output the type of the exception and the message. 4. **Customization Modules**: Demonstrate the use of a `usercustomize.py` file located in the user\'s site-packages directory. Assume you have a function `greet` defined in `usercustomize.py` that takes a name as input and prints a greeting. Import this function and use it in your script. Constraints - You should get the path of `user site-packages directory` dynamically within the script. Input - An environment variable `PYTHONCUSTOMSTARTUP` containing the path to a file. - Customization module `usercustomize.py` in the user\'s site-packages directory. Output - If `PYTHONCUSTOMSTARTUP` is set and file is valid, any content of this file should be executed. - If the `usercustomize.py` file exists and has a function `greet(name)`, invoke this function with the name `Python310 Learner`. # Requirements 1. Make the script executable with `#!/usr/bin/env python3.10`. 2. Use `os` and `site` modules appropriately. 3. Handle file reading errors and custom exceptions. Example Consider `PYTHONCUSTOMSTARTUP` is set to `start_commands.py`, which contains: ```python print(\\"Startup commands executed.\\") ``` And the `usercustomize.py` contains: ```python def greet(name): print(f\\"Hello, {name}!\\") ``` **Expected Output**: ``` Startup commands executed. Hello, Python310 Learner! ``` Submission Submit the `custom_script.py` file which includes all the required functionalities mentioned above.","solution":"#!/usr/bin/env python3.10 import os import site def main(): try: # Check for PYTHONCUSTOMSTARTUP environment variable startup_file = os.getenv(\'PYTHONCUSTOMSTARTUP\') if startup_file and os.path.isfile(startup_file): with open(startup_file) as f: exec(f.read()) # Get the path to the user-specific site-packages directory user_site_dir = site.getusersitepackages() usercustomize_path = os.path.join(user_site_dir, \'usercustomize.py\') # If usercustomize.py exists, import the greet function and call it if os.path.isfile(usercustomize_path): import importlib.util spec = importlib.util.spec_from_file_location(\\"usercustomize\\", usercustomize_path) usercustomize = importlib.util.module_from_spec(spec) spec.loader.exec_module(usercustomize) if hasattr(usercustomize, \'greet\'): usercustomize.greet(\'Python310 Learner\') else: print(\\"No \'greet\' function found in usercustomize.py\\") else: print(f\\"No usercustomize.py found in {user_site_dir}\\") except FileNotFoundError as fnf_error: print(f\\"File not found error: {fnf_error}\\") except KeyboardInterrupt: print(\\"Script interrupted by user.\\") except Exception as e: print(f\\"An error occurred: {type(e).__name__}, {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Coding Assessment Question: Statistical Analysis of Randomly Generated Data Objective: To assess the student\'s ability to work with multiple numeric and mathematical modules in Python 3.10, particularly focusing on generating random numbers and performing statistical analysis on these numbers. Problem Statement: You are required to implement a function called `analyze_random_data` that performs the following tasks: 1. **Generate Random Data**: Generate a list of N random floating-point numbers between a specified range (inclusive). The value of N, the lower bound (a), and the upper bound (b) will be provided as input. 2. **Calculate Statistics**: Compute and return various statistics for the generated random data, including the mean, median, variance, standard deviation, and correlation between the data and a sequential list of integers starting from 1 up to N. Function Signature: ```python def analyze_random_data(N: int, a: float, b: float) -> dict: pass ``` Input: - `N` (int): The number of random floating-point numbers to generate. - `a` (float): The lower bound of the range (inclusive). - `b` (float): The upper bound of the range (inclusive). Output: - A dictionary containing the following key-value pairs: - `\'mean\'`: The mean of the generated random data. - `\'median\'`: The median of the generated random data. - `\'variance\'`: The variance of the generated random data. - `\'stdev\'`: The standard deviation of the generated random data. - `\'correlation\'`: The Pearson correlation coefficient between the generated random data and a sequential list from 1 to N. Constraints and Limitations: - The function should handle edge cases where `a` may be greater than or equal to `b`. - You are expected to use the `random` module for generating random numbers and the `statistics` module for statistical calculations. - Assume that `N` will always be a positive integer. Example: Suppose the input values are: ```python N = 10 a = 1.0 b = 10.0 ``` The function should return a dictionary similar to the following (values may vary due to randomness): ```python { \'mean\': 5.5, \'median\': 5.5, \'variance\': 8.25, \'stdev\': 2.87, \'correlation\': 0.345 } ``` Notes: - Ensure that the function is efficient and does not use unnecessary computations. - Properly handle possible exceptions and edge cases, such as when the upper bound is less than the lower bound. Good luck and happy coding!","solution":"import random import statistics from scipy.stats import pearsonr def analyze_random_data(N: int, a: float, b: float) -> dict: # Generate N random floating-point numbers between a and b random_data = [random.uniform(a, b) for _ in range(N)] # Calculate statistics mean_value = statistics.mean(random_data) median_value = statistics.median(random_data) variance_value = statistics.variance(random_data) stdev_value = statistics.stdev(random_data) # Generate sequential list from 1 to N sequential_data = list(range(1, N + 1)) # Calculate correlation correlation_value, _ = pearsonr(random_data, sequential_data) # Create the result dictionary result = { \'mean\': mean_value, \'median\': median_value, \'variance\': variance_value, \'stdev\': stdev_value, \'correlation\': correlation_value } return result"},{"question":"Problem Statement You are provided with several datasets that have varying complexity and characteristics. Your task is to implement a function that: 1. Selects an appropriate clustering algorithm from scikit-learn based on the characteristics of the dataset. 2. Fits the model using the chosen algorithm. 3. Evaluates the model using at least two different clustering evaluation metrics from scikit-learn. 4. Returns the fitted model and the evaluation scores. # Function Signature ```python def cluster_and_evaluate(data: np.ndarray, labels: np.ndarray) -> Tuple[BaseEstimator, Dict[str, float]]: pass ``` # Input - `data`: A numpy.ndarray of shape `(n_samples, n_features)` representing the data to be clustered. - `labels`: A numpy.ndarray of shape `(n_samples,)` representing the true labels of the data points (for evaluation purposes). # Output - A tuple containing: - The fitted clustering model (an instance of a scikit-learn estimator). - A dictionary of evaluation scores with metric names as keys and computed scores as values. # Constraints - You must choose from the clustering algorithms available in **scikit-learn**. - You should consider the scalability and suitability of each algorithm for the given data. - Evaluation should include at least one internal clustering criterion (e.g., Silhouette Score) and one external clustering criterion (e.g., Adjusted Rand Index). # Evaluation Metrics - Use `silhouette_score` for internal evaluation (higher is better). - Use `adjusted_rand_score` for external evaluation (higher is better). # Example ```python import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import KMeans # Generate example data data, labels = make_blobs(n_samples=200, centers=3, cluster_std=1.0, random_state=42) # Example implementation of function (this is a mock implementation, replace with actual function for real evaluation) def cluster_and_evaluate(data, labels): model = KMeans(n_clusters=3, random_state=42) model.fit(data) from sklearn.metrics import silhouette_score, adjusted_rand_score scores = { \'silhouette_score\': silhouette_score(data, model.labels_), \'adjusted_rand_score\': adjusted_rand_score(labels, model.labels_) } return model, scores model, scores = cluster_and_evaluate(data, labels) print(model) print(scores) ``` # Notes - Carefully choose the clustering algorithm based on the data characteristics such as the number of samples, the number of features, cluster shapes, etc. - The solution should be performant and handle large datasets efficiently if needed. - Ensure reproducibility by setting random states where applicable.","solution":"import numpy as np from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, adjusted_rand_score from sklearn.base import BaseEstimator from typing import Tuple, Dict def cluster_and_evaluate(data: np.ndarray, labels: np.ndarray) -> Tuple[BaseEstimator, Dict[str, float]]: n_samples, n_features = data.shape if n_samples > 10000: # For large datasets, use DBSCAN model = DBSCAN(eps=0.5, min_samples=5) else: # Default choice: KMeans model = KMeans(n_clusters=len(np.unique(labels)), random_state=42) model.fit(data) predicted_labels = model.labels_ scores = { \'silhouette_score\': silhouette_score(data, predicted_labels), \'adjusted_rand_score\': adjusted_rand_score(labels, predicted_labels) } return model, scores"},{"question":"Objective Create a function in Python that utilizes low-level file operations to perform customized file reading and writing based on specific parameters. This task assesses your understanding of advanced file handling in Python. Function Signature ```python def custom_file_operations(filepath: str, mode: str, operation: str, content: str = \\"\\", num_chars: int = -1) -> str: pass ``` Parameters - `filepath`: A string representing the path to the file. - `mode`: The mode in which the file should be opened (`r`, `w`, `a`, etc.). - `operation`: The operation to perform (`readline`, `readNchars`, `write`, `writeline`). - `content`: The content to write to the file, required if `operation` is `write` or `writeline`. - `num_chars`: The number of characters to read if `operation` is `readNchars`. Return Value - A string representing the result of the `readline` or `readNchars` operations or a confirmation message (e.g., `\\"Write operation successful\\"`, `\\"Line written successfully\\"`) for write operations. Constraints - You should use Python\'s low-level file descriptor operations and map them for higher-level Python file I/O. - Ensure file operations handle exceptions gracefully. - Do not use high-level built-in Python file handling (`open`, `close`, `read`, `write`, etc.) directly. Instead, utilize the appropriate low-level functions described in the provided documentation. Examples 1. **Reading a line from a file:** ```python result = custom_file_operations(\'example.txt\', \'r\', \'readline\') print(result) # Should print the first line of \'example.txt\' ``` 2. **Reading a specific number of characters from a file:** ```python result = custom_file_operations(\'example.txt\', \'r\', \'readNchars\', num_chars=10) print(result) # Should print the first 10 characters of \'example.txt\' ``` 3. **Writing content to a file:** ```python result = custom_file_operations(\'example.txt\', \'w\', \'write\', content=\\"Hello world!\\") print(result) # Should print \\"Write operation successful\\" ``` 4. **Writing a line to a file:** ```python result = custom_file_operations(\'example.txt\', \'a\', \'writeline\', content=\\"Appending this line.\\") print(result) # Should print \\"Line written successfully\\" ``` Notes - Use `PyFile_FromFd`, `PyObject_AsFileDescriptor`, `PyFile_WriteObject`, and other relevant low-level functions to implement the task. - Make sure to close the file descriptor after performing the required operations.","solution":"import os def custom_file_operations(filepath: str, mode: str, operation: str, content: str = \\"\\", num_chars: int = -1) -> str: try: # Map modes from high-level to low-level if \'r\' in mode: flags = os.O_RDONLY elif \'w\' in mode: flags = os.O_WRONLY | os.O_CREAT | os.O_TRUNC elif \'a\' in mode: flags = os.O_WRONLY | os.O_CREAT | os.O_APPEND else: raise ValueError(\\"Invalid mode\\") # Open the file using low-level operation fd = os.open(filepath, flags, 0o777) if operation == \'readline\': result = os.read(fd, 1024) # Extract the first line line = result.partition(b\'n\')[0] return line.decode(\'utf-8\') elif operation == \'readNchars\': if num_chars < 0: raise ValueError(\\"Invalid num_chars. Must be a non-negative integer.\\") result = os.read(fd, num_chars) return result.decode(\'utf-8\') elif operation == \'write\': os.write(fd, content.encode(\'utf-8\')) return \\"Write operation successful\\" elif operation == \'writeline\': os.write(fd, (content + \'n\').encode(\'utf-8\')) return \\"Line written successfully\\" else: raise ValueError(\\"Invalid operation\\") except Exception as e: return str(e) finally: if \'fd\' in locals(): os.close(fd)"},{"question":"Understanding and Utilizing the __future__ Module **Objective**: Implement a function that evaluates the compatibility of a Python feature based on its optional and mandatory release versions in the `__future__` module. **Context**: The `__future__` module allows you to import features that will appear in future versions of Python. Each feature has an optional release version, indicating the first Python version where it was available, and a mandatory release version, indicating when it became a standard feature. Understanding this mechanism allows developers to write forward-compatible code. **Task**: You are given details about various future features, extracted from the `__future__` module\'s definitions. Implement the following function: ```python def feature_is_compatible(feature_name: str, current_version: tuple) -> bool: Determine if a feature from `__future__` is available in the given Python version. Parameters: feature_name (str): The name of the future feature (e.g., \'generators\', \'print_function\'). current_version (tuple): The current Python version represented as a tuple (major_version, minor_version, micro_version). Returns: bool: True if the feature is either an optional or mandatory feature in the given Python version, False otherwise. # Your implementation here ``` **Input**: - `feature_name`: A string representing the name of the feature. - `current_version`: A tuple representing the current Python version with the format (major, minor, micro). **Output**: - Return `True` if the feature is available in the provided version (either as optional or mandatory). Otherwise, return `False`. **Constraints**: - The feature names and their respective version information should be hardcoded or defined within the function for this exercise. - Only consider the features provided in the documentation extract above. - You may assume the input version will always be a valid Python version format. **Example**: ```python # Given the feature `generators` which was optional in 2.2.0a1 and mandatory in 2.3 print(feature_is_compatible(\'generators\', (2, 2, 0))) # Output: True print(feature_is_compatible(\'generators\', (2, 1, 0))) # Output: False # Given the feature `print_function` which was optional in 2.6.0a2 and mandatory in 3.0 print(feature_is_compatible(\'print_function\', (3, 0, 0))) # Output: True print(feature_is_compatible(\'print_function\', (2, 5, 0))) # Output: False ``` Implement this function to demonstrate your understanding of how future features are managed and activated within the Python language.","solution":"def feature_is_compatible(feature_name: str, current_version: tuple) -> bool: Determine if a feature from `__future__` is available in the given Python version. Parameters: feature_name (str): The name of the future feature (e.g., \'generators\', \'print_function\'). current_version (tuple): The current Python version represented as a tuple (major_version, minor_version, micro_version). Returns: bool: True if the feature is either an optional or mandatory feature in the given Python version, False otherwise. future_features = { \'generators\': { \'optional\': (2, 2, 0), \'mandatory\': (2, 3, 0) }, \'print_function\': { \'optional\': (2, 6, 0), \'mandatory\': (3, 0, 0) } # Add more features as needed } if feature_name not in future_features: return False feature_info = future_features[feature_name] if current_version >= feature_info[\'optional\']: return True else: return False"},{"question":"Objective The objective of this coding assessment is to evaluate your proficiency in using the pandas library for data visualization. You will be required to read a dataset, manipulate the data, and generate various types of plots to provide insights into the dataset. Problem Statement You are provided a dataset containing information about sales figures for a supermarket chain. You need to perform the following tasks: 1. **Data Loading and Inspection** - Load the dataset from a CSV file named `supermarket_sales.csv`. - Inspect the first few rows of the dataset to understand its structure. 2. **Data Preprocessing** - Handle any missing values by either filling them with appropriate values or dropping them. - Ensure that the data types are appropriate for plotting (e.g., dates should be converted to `datetime` objects). 3. **Visualization Tasks** - **Task 1: Line Plot** - Create a line plot showing the trend of total sales over time. - Customize the plot to include proper labels, title, and gridlines. - **Task 2: Bar Plot** - Create a bar plot showing the total sales for each product category. - Customize the plot with different colors for each category and include appropriate labels and titles. - **Task 3: Histogram** - Create a histogram to show the distribution of sales amounts. - Use appropriate bin size and add customization such as transparency. - **Task 4: Box Plot** - Create a box plot to visualize the distribution of sales amounts for different payment methods. - Customize the plot to include appropriate labels, titles, and colors. - **Task 5: Scatter Plot** - Create a scatter plot to show the relationship between unit price and total sales. - Use different colors to represent different product categories. Input Format The CSV file (supermarket_sales.csv) will have the following columns: - `Date`: The date of the sale. - `Product Category`: The category of the product sold. - `Unit Price`: The price per unit of the product. - `Quantity`: The number of units sold. - `Total Sales`: The total sales amount. - `Payment Method`: The method of payment used. Output Format Your output should consist of the following visualizations: - A line plot for total sales over time. - A bar plot for total sales per product category. - A histogram for the distribution of sales amounts. - A box plot for sales amounts by payment method. - A scatter plot for the relationship between unit price and total sales. Notes and Constraints - Ensure that all plots are well labeled and easy to interpret. - Use `matplotlib` and `pandas` libraries for plotting. - Handle any edge cases or anomalies in the data gracefully. # Example Code Template Here is a template to get you started: ```python import pandas as pd import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'supermarket_sales.csv\') # Inspect the data # Data Preprocessing # Task 1: Line Plot for total sales over time # Task 2: Bar Plot for total sales per product category # Task 3: Histogram for distribution of sales amounts # Task 4: Box Plot for sales amounts by payment method # Task 5: Scatter Plot for relationship between unit price and total sales ``` Complete the code by filling in the required steps for each task.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_and_inspect_data(file_path): # Load the dataset df = pd.read_csv(file_path) # Inspect the first few rows of the dataset print(df.head()) return df def preprocess_data(df): # Handle missing values df = df.dropna() # Convert \'Date\' to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) return df def plot_line_total_sales_over_time(df): # Aggregate total sales by date sales_over_time = df.groupby(\'Date\')[\'Total Sales\'].sum().reset_index() # Create a line plot plt.figure(figsize=(10, 6)) plt.plot(sales_over_time[\'Date\'], sales_over_time[\'Total Sales\'], marker=\'o\') plt.title(\'Total Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.grid(True) plt.show() def plot_bar_total_sales_per_category(df): # Aggregate total sales by product category sales_per_category = df.groupby(\'Product Category\')[\'Total Sales\'].sum().reset_index() # Create a bar plot plt.figure(figsize=(10, 6)) plt.bar(sales_per_category[\'Product Category\'], sales_per_category[\'Total Sales\'], color=\'skyblue\') plt.title(\'Total Sales per Product Category\') plt.xlabel(\'Product Category\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.show() def plot_histogram_sales_amounts(df): # Create a histogram plt.figure(figsize=(10, 6)) plt.hist(df[\'Total Sales\'], bins=30, alpha=0.75, color=\'green\') plt.title(\'Distribution of Sales Amounts\') plt.xlabel(\'Total Sales\') plt.ylabel(\'Frequency\') plt.grid(True) plt.show() def plot_box_sales_by_payment_method(df): # Create a box plot plt.figure(figsize=(10, 6)) df.boxplot(column=\'Total Sales\', by=\'Payment Method\', patch_artist=True) plt.title(\'Sales Amounts by Payment Method\') plt.suptitle(\'\') plt.xlabel(\'Payment Method\') plt.ylabel(\'Total Sales\') plt.show() def plot_scatter_unit_price_vs_total_sales(df): # Create a scatter plot plt.figure(figsize=(10, 6)) categories = df[\'Product Category\'].unique() for category in categories: subset = df[df[\'Product Category\'] == category] plt.scatter(subset[\'Unit Price\'], subset[\'Total Sales\'], label=category) plt.title(\'Unit Price vs Total Sales\') plt.xlabel(\'Unit Price\') plt.ylabel(\'Total Sales\') plt.legend(title=\'Product Category\') plt.show()"},{"question":"Objective: To assess your understanding of the Python `json` module, you are tasked with creating a robust program that deals with JSON serialization and deserialization involving custom data types and handling large JSON datasets. Problem Statement: You are given a list of student records with details such as name, age, grades, and address. Each student record is represented as a dictionary. Additionally, some elements might involve complex structures such as nested dictionaries or custom data types. 1. Write a Python function `serialize_students` that: - Takes a list of student records (dictionaries) as input. - Serializes the list into a JSON formatted string. - Ensures that the output JSON is pretty-printed with an indentation of 4 spaces. - Sorts the keys in the dictionaries when outputting JSON. 2. Write a Python function `deserialize_students` that: - Takes a JSON formatted string representing student records. - Deserializes the JSON string back into a Python list of dictionaries. - Correctly reconstructs any complex data types or nested structures. 3. Additionally, extend the standard `JSONEncoder` to handle a custom data type `ComplexNumber` (a class with `real` and `imaginary` attributes). Your custom encoder should encode instances of `ComplexNumber` as a dictionary with `complex: true`, `real`, and `imaginary` fields. 4. Provide a custom decoding function that can correctly deserialize these `ComplexNumber` encoded dictionaries back into `ComplexNumber` instances when reading the JSON data. # Definition of `ComplexNumber` class: ```python class ComplexNumber: def __init__(self, real, imaginary): self.real = real self.imaginary = imaginary ``` Requirements: - Your functions should include appropriate error handling to manage potential issues like invalid JSON and unsupported data types. - Support serialization and deserialization of large JSON datasets efficiently. - Include unit tests demonstrating the correctness of your functions with various corner cases, including large datasets and invalid inputs. Constraints: - The input list of student records will not exceed 100,000 entries. - The student dictionary keys and values will only include basic data types (strings, integers, floats), nested dictionaries, and the custom `ComplexNumber` type. Signature: ```python def serialize_students(students: List[Dict]) -> str: pass def deserialize_students(json_str: str) -> List[Dict]: pass class ComplexNumber: def __init__(self, real: float, imaginary: float): self.real = real self.imaginary = imaginary ``` # Example Usage: ```python students = [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grades\\": [88, 92, 85], \\"address\\": {\\"city\\": \\"Wonderland\\", \\"zip\\": 12345}}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grades\\": [90, 85, 82], \\"complex_number\\": ComplexNumber(2, 3)}, ] json_str = serialize_students(students) print(json_str) deserialized_students = deserialize_students(json_str) print(deserialized_students) ``` # Expected Output: The pretty-printed JSON output with sorted keys, and correctly reconstructed list of dictionaries including `ComplexNumber` instances.","solution":"import json from typing import List, Dict class ComplexNumber: def __init__(self, real: float, imaginary: float): self.real = real self.imaginary = imaginary def __eq__(self, other): if not isinstance(other, ComplexNumber): return False return self.real == other.real and self.imaginary == other.imaginary class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, ComplexNumber): return {\\"complex\\": True, \\"real\\": obj.real, \\"imaginary\\": obj.imaginary} return json.JSONEncoder.default(self, obj) def complex_decoder(dct): if \\"complex\\" in dct and dct[\\"complex\\"]: return ComplexNumber(dct[\\"real\\"], dct[\\"imaginary\\"]) return dct def serialize_students(students: List[Dict]) -> str: try: return json.dumps(students, cls=ComplexEncoder, sort_keys=True, indent=4) except TypeError as e: raise ValueError(f\\"Error serializing students: {e}\\") def deserialize_students(json_str: str) -> List[Dict]: try: return json.loads(json_str, object_hook=complex_decoder) except json.JSONDecodeError as e: raise ValueError(f\\"Error deserializing JSON string: {e}\\")"},{"question":"**File Management Challenge Using `shutil` Module** *Objective*: The objective of this task is to assess your understanding of high-level file operations using the Python `shutil` module. **Problem Statement**: You are required to implement a Python function `duplicate_and_clean_directory(source_dir: str, destination_dir: str, patterns_to_ignore: List[str]) -> List[str]` that performs the following tasks: 1. **Copy Directory Tree**: Recursively copies an entire directory tree rooted at `source_dir` to a new directory named `destination_dir`. 2. **Ignoring Patterns**: While copying, ignore files and directories that match any of the glob-style patterns provided in `patterns_to_ignore`. 3. **Remove Read-only Files**: After the directory has been copied, remove all read-only files from the `destination_dir`. *Function Signature*: ```python from typing import List def duplicate_and_clean_directory(source_dir: str, destination_dir: str, patterns_to_ignore: List[str]) -> List[str]: pass ``` *Input*: - `source_dir` (str): Path to the source directory that needs to be copied. - `destination_dir` (str): Path to the destination directory where the source directory should be duplicated. - `patterns_to_ignore` (List[str]): A list of glob-style patterns that specify which files or directories to ignore during the copying process. *Output*: - `List[str]`: A list of paths to the read-only files that were removed during the cleaning process. *Constraints*: - Assume that both `source_dir` and `destination_dir` are valid directory paths on the system. - The function should properly handle symbolic links as described in the given documentation. - You should handle exceptions that might arise from file operations appropriately, ensuring the program does not crash on errors like permission issues. *Example*: ```python source_dir = \'/path/to/source\' destination_dir = \'/path/to/destination\' patterns_to_ignore = [\'*.tmp\', \'*.log\'] removed_files = duplicate_and_clean_directory(source_dir, destination_dir, patterns_to_ignore) print(removed_files) ``` This should copy all files from `/path/to/source` to `/path/to/destination`, ignoring files with `.tmp` and `.log` extensions, and then remove any read-only files from the destination directory, returning the list of removed file paths. *Useful Methods from `shutil`*: - `shutil.copytree()` - `shutil.ignore_patterns()` - `shutil.rmtree()` *Note*: Ensure your solution is efficient and accounts for potential issues such as read-only files and non-existent paths.","solution":"from typing import List import os import shutil import stat def duplicate_and_clean_directory(source_dir: str, destination_dir: str, patterns_to_ignore: List[str]) -> List[str]: Copies the directory tree from source_dir to destination_dir, ignoring files matching patterns_to_ignore. Then removes read-only files from the destination directory. Args: - source_dir: Path to the source directory that needs to be copied. - destination_dir: Path to the destination directory where the source directory should be duplicated. - patterns_to_ignore: A list of glob-style patterns that specify which files or directories to ignore. Returns: - List of paths to the read-only files that were removed during the cleaning process. # Copy the directory tree while ignoring specified patterns shutil.copytree(source_dir, destination_dir, ignore=shutil.ignore_patterns(*patterns_to_ignore)) removed_files = [] # Walk through the directory tree for dirpath, dirnames, filenames in os.walk(destination_dir): for filename in filenames: file_path = os.path.join(dirpath, filename) # Check if the file is read-only if not os.access(file_path, os.W_OK): try: os.chmod(file_path, stat.S_IWUSR) # Change the file to writable os.remove(file_path) removed_files.append(file_path) except Exception as e: print(f\\"Error removing file {file_path}: {e}\\") return removed_files"},{"question":"# Pretty Printer Enhancement Task You have learned about the `pprint` module, which is used for pretty-printing Python data structures. Now, let\'s take that knowledge a step further and implement a custom pretty-printer for more complex scenarios. The goal is to implement a class similar to `PrettyPrinter` with additional functionality for handling specific object types in unique ways. Task Description: 1. **Create a class `CustomPrettyPrinter`:** - Functionality to pretty-print nested dictionaries, lists, and tuples following the `pprint` style. - An additional parameter `formatter_functions` in the constructor. This parameter is a dictionary where: - The keys are types (e.g., `int`, `list`, a custom class, etc.). - The values are functions that take an instance of the type and return a formatted string. - Implement methods `cformat` and `cpprint` within this class: - `cformat(object)` returns the formatted string representation of the object. - `cpprint(object)` prints the formatted representation to `sys.stdout` followed by a newline. 2. **Input and Output Formats:** - The input to the class constructor includes: - Optional indentation level (`indent`), default is 1. - Optional maximum width (`width`), default is 80. - Optional custom formatter functions (`formatter_functions`), default is an empty dictionary. - The object to be formatted can be a nested structure of dictionaries, lists, tuples, and other types. - The formatted output must follow the constraints set by `indent` and `width` and apply custom formatting as specified by `formatter_functions`. 3. **Constraints:** - Ensure that if an object type is not covered by `formatter_functions`, it defaults to the standard pretty-printing behavior. - Handle recursive data structures properly to avoid infinite loops. - Assure readable and presentable formatting for deeply nested structures and long lists or dictionaries. 4. **Performance Requirements:** - The solution should be efficient enough to handle moderately large data structures without excessive delay. Example Usage: ```python class CustomType: def __init__(self, value): self.value = value def custom_type_formatter(obj): return f\\"CustomType(value={obj.value})\\" formatter_funcs = { CustomType: custom_type_formatter, int: lambda x: f\\"Integer({x})\\" } data = { \\"a\\": [1, 2, CustomType(3)], \\"b\\": (4, 5, {\\"c\\": 6, \\"d\\": CustomType(7)}) } cpp = CustomPrettyPrinter(indent=2, width=50, formatter_functions=formatter_funcs) cpp.cpprint(data) ``` Expected Output: ``` { \'a\': [ Integer(1), Integer(2), CustomType(value=3) ], \'b\': ( Integer(4), Integer(5), { \'c\': Integer(6), \'d\': CustomType(value=7) } ) } ``` Implement the `CustomPrettyPrinter` class according to the outlined specifications and ensure it passes the example usage test.","solution":"import sys import pprint class CustomPrettyPrinter: def __init__(self, indent=1, width=80, formatter_functions=None): self.indent = indent self.width = width self.formatter_functions = formatter_functions if formatter_functions is not None else {} def cformat(self, obj): formatted_str = self._custom_format(obj, 0) return formatted_str def cpprint(self, obj): formatted_str = self.cformat(obj) sys.stdout.write(formatted_str + \'n\') def _custom_format(self, obj, current_indent): obj_type = type(obj) if obj_type in self.formatter_functions: return self.formatter_functions[obj_type](obj) if isinstance(obj, (list, tuple, set)): return self._format_sequence(obj, current_indent) if isinstance(obj, dict): return self._format_dict(obj, current_indent) return pprint.pformat(obj) def _format_sequence(self, seq, current_indent): opening, closing = (\'[\', \']\') if isinstance(seq, list) else (\'(\', \')\') if not seq: return f\\"{opening}{closing}\\" indent_str = \' \' * (current_indent + self.indent) items = [self._custom_format(item, current_indent + self.indent) for item in seq] formatted_items = f\\",n{indent_str}\\".join(items) return f\\"{opening}n{indent_str}{formatted_items}n{\' \' * current_indent}{closing}\\" def _format_dict(self, d, current_indent): if not d: return \'{}\' indent_str = \' \' * (current_indent + self.indent) items = [] for k, v in d.items(): formatted_key = pprint.pformat(k) formatted_value = self._custom_format(v, current_indent + self.indent) items.append(f\\"{formatted_key}: {formatted_value}\\") formatted_items = f\\",n{indent_str}\\".join(items) return f\\"{{n{indent_str}{formatted_items}n{\' \' * current_indent}}}\\" # Sample usage (this part is for illustration and testing purposes, not part of the solution you need to run): if __name__ == \\"__main__\\": class CustomType: def __init__(self, value): self.value = value def custom_type_formatter(obj): return f\\"CustomType(value={obj.value})\\" formatter_funcs = { CustomType: custom_type_formatter, int: lambda x: f\\"Integer({x})\\" } data = { \\"a\\": [1, 2, CustomType(3)], \\"b\\": (4, 5, {\\"c\\": 6, \\"d\\": CustomType(7)}) } cpp = CustomPrettyPrinter(indent=2, width=50, formatter_functions=formatter_funcs) cpp.cpprint(data) # This will print in the desired pretty print format"},{"question":"Objective: You are required to implement a function called `summarize_path_info` that processes a list of file paths and returns a summary dictionary with various details about the paths using multiple functions from the `os.path` module. Function Signature: ```python def summarize_path_info(paths: list[str]) -> dict: pass ``` Input: - `paths`: A list of strings, where each string represents a file or directory path. For example: `[\\"/usr/bin/python3\\", \\"/usr/local/bin/\\", \\"~/example.txt\\"]`. Output: - Returns a dictionary with the following structure: ```python { \\"abspath\\": [List of absolute paths], \\"basename\\": [List of base names], \\"dirname\\": [List of directory names], \\"exists\\": [List of booleans indicating if each path exists], \\"isfile\\": [List of booleans indicating if each path is a file], \\"isdir\\": [List of booleans indicating if each path is a directory], \\"common_prefix\\": Common prefix for all paths, or empty string if none, \\"joined_path\\": Path obtained by joining all input paths } ``` Constraints: - The input list `paths` will contain at least one path. - The paths can be relative or absolute. - Do not assume the paths actually exist on the filesystem for functions checking existence or type, but ensure the logic can handle such checks. Example: ```python paths = [\\"/usr/bin/python3\\", \\"/usr/local/bin/\\", \\"~/example.txt\\"] result = summarize_path_info(paths) print(result) ``` Expected Output (assuming some paths do not exist): ```python { \\"abspath\\": [\\"/usr/bin/python3\\", \\"/usr/local/bin\\", \\"/home/username/example.txt\\"], \\"basename\\": [\\"python3\\", \\"bin\\", \\"example.txt\\"], \\"dirname\\": [\\"/usr/bin\\", \\"/usr/local\\", \\"/home/username\\"], \\"exists\\": [False, False, False], \\"isfile\\": [False, False, False], \\"isdir\\": [False, False, False], \\"common_prefix\\": \\"/usr\\", \\"joined_path\\": \\"/usr/bin/python3/usr/local/bin/~/example.txt\\" } ``` Notes: - Use `os.path.expanduser()` to handle paths starting with `~`. - Make sure to handle symbolic links appropriately if required. - Properly handle various cases like trailing slashes and different formats of paths. - Consider edge cases such as empty strings or invalid paths and document your assumptions. Implementation: You are required to implement the `summarize_path_info` function utilizing the appropriate functions from the `os.path` module to gather information about the provided paths.","solution":"import os def summarize_path_info(paths: list[str]) -> dict: Processes a list of file paths and returns a summary dictionary with various details about the paths. expanded_paths = [os.path.expanduser(path) for path in paths] return { \\"abspath\\": [os.path.abspath(path) for path in expanded_paths], \\"basename\\": [os.path.basename(path) for path in expanded_paths], \\"dirname\\": [os.path.dirname(path) for path in expanded_paths], \\"exists\\": [os.path.exists(path) for path in expanded_paths], \\"isfile\\": [os.path.isfile(path) for path in expanded_paths], \\"isdir\\": [os.path.isdir(path) for path in expanded_paths], \\"common_prefix\\": os.path.commonprefix(expanded_paths), \\"joined_path\\": os.path.join(*expanded_paths) }"},{"question":"**PyTorch MPS Device Memory Manager** Objective: Implement a function to manage and report memory usage on MPS devices using PyTorch. **Task**: Write a Python function using the PyTorch `torch.mps` module that: 1. Initializes the RNG state and seeds it. 2. Allocates memory on an MPS device. 3. Reports current allocated memory and the recommended maximum memory. 4. Synchronizes the device. 5. Empties the device cache. 6. Reports memory again after emptying the cache. Your function should take a device index and a seed value as inputs and should print out the memory statistics before and after emptying the cache. **Function Signature**: ```python def manage_mps_device_memory(device_index: int, seed_value: int) -> None: pass ``` **Input**: - `device_index` (int): The index of the MPS device to manage. - `seed_value` (int): The seed value to initialize the RNG. **Output**: - The function should print the following information in order: 1. Current allocated memory in bytes. 2. Recommended maximum memory in bytes. 3. Synchronization confirmation. 4. Current allocated memory in bytes after emptying the cache. **Constraints**: 1. You may assume that the `torch.mps` module is correctly installed and available in your environment. 2. Handle any potential errors gracefully and provide informative messages. **Example**: ```python # Example usage manage_mps_device_memory(device_index=0, seed_value=42) ``` *Expected Output*: ```plaintext Initial RNG state set and seeded with value: 42 Current allocated memory: XXXX bytes Recommended maximum memory: YYYY bytes Device synchronized Cache emptied Current allocated memory after cache emptied: ZZZZ bytes ``` Use the PyTorch `torch.mps` functionalities listed in the provided documentation to implement this function.","solution":"import torch def manage_mps_device_memory(device_index: int, seed_value: int) -> None: Manage and report memory usage on MPS devices using PyTorch. Args: device_index (int): The index of the MPS device to manage. seed_value (int): The seed value to initialize the RNG. Returns: None if not torch.mps.is_available(): print(\\"MPS device is not available.\\") return try: # Set the specified MPS device device = torch.device(f\'mps:{device_index}\') torch.mps.set_rng_state(seed_value) print(f\\"Initial RNG state set and seeded with value: {seed_value}\\") # Report initial memory stats allocated_memory = torch.mps.memory_allocated() max_memory = torch.mps.get_allocated_memory() print(f\\"Current allocated memory: {allocated_memory} bytes\\") print(f\\"Recommended maximum memory: {max_memory} bytes\\") # Synchronize the MPS device torch.mps.synchronize() print(\\"Device synchronized\\") # Empty the cache and report memory stats again torch.mps.empty_cache() print(\\"Cache emptied\\") allocated_memory_after_cache = torch.mps.memory_allocated() print(f\\"Current allocated memory after cache emptied: {allocated_memory_after_cache} bytes\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Question: Manipulating Bytearrays in Python** You are tasked with implementing a function that processes a given list of strings according to the following steps: 1. Convert each string in the list to a bytearray. 2. Concatenate all the bytearrays into a single bytearray. 3. Resize the concatenated bytearray to a specified length. 4. Convert the resized bytearray back to a string. Your task is to implement the function `process_strings(strings: List[str], new_length: int) -> str` that performs these steps. # Input - `strings` (List[str]): A list of strings to be converted to bytearrays and manipulated. - `new_length` (int): The new length to which the concatenated bytearray should be resized. If `new_length` is greater than the size of the concatenated bytearray, it should be padded with null bytes (`\'x00\'`). # Output - (str): A string representation of the resized bytearray. # Constraints - The sum of the lengths of all strings in the input list will not exceed 10^6. - `new_length` will be a non-negative integer less than or equal to 2 * 10^6. # Example ```python def process_strings(strings: List[str], new_length: int) -> str: # Your implementation here # Example Usage print(process_strings([\\"hello\\", \\"world\\"], 15)) # Output: \'helloworldx00x00x00x00x00\' print(process_strings([\\"foo\\", \\"bar\\", \\"baz\\"], 9)) # Output: \'foobarbaz\' print(process_strings([\\"abc\\", \\"def\\"], 2)) # Output: \'ab\' ``` # Implementation Details 1. Convert each string to a `bytearray` using `PyByteArray_FromStringAndSize`. 2. Concatenate the bytearrays using `PyByteArray_Concat`. 3. Resize the concatenated bytearray using `PyByteArray_Resize`. 4. Convert the resized bytearray back to a string using `PyByteArray_AsString`. Please use the information provided above and your understanding of bytearrays to implement the function. The implemented function should validate the inputs and handle edge cases (e.g., an empty string list or resizing to zero length).","solution":"from typing import List def process_strings(strings: List[str], new_length: int) -> str: # Convert each string to a bytearray bytearrays = [bytearray(s, \'utf-8\') for s in strings] # Concatenate all the bytearrays into a single bytearray concatenated = bytearray().join(bytearrays) # Resize the concatenated bytearray to the specified length concatenated = concatenated[:new_length].ljust(new_length, b\'x00\') # Convert the resized bytearray back to a string return concatenated.decode(\'utf-8\') # Example Usage # Uncomment below lines to see the actual outputs during development or manual testing # print(process_strings([\\"hello\\", \\"world\\"], 15)) # Output: \'helloworldx00x00x00x00x00\' # print(process_strings([\\"foo\\", \\"bar\\", \\"baz\\"], 9)) # Output: \'foobarbaz\' # print(process_strings([\\"abc\\", \\"def\\"], 2)) # Output: \'ab\'"},{"question":"# Customizing Seaborn Color Palette and Plotting You are given a dataset of penguin species measurements. Your task is to create a scatter plot using seaborn, customize it with a specified color palette, and perform some adjustments to the palette. **Dataset:** The dataset is a CSV file named `penguins.csv` with the following columns: - `species`: The species of the penguin (Adelie, Gentoo, Chinstrap) - `bill_length_mm`: The bill length of the penguin in millimeters - `bill_depth_mm`: The bill depth of the penguin in millimeters - `flipper_length_mm`: The flipper length of the penguin in millimeters - `body_mass_g`: The body mass of the penguin in grams - `sex`: The sex of the penguin (male, female) Tasks: 1. Load the dataset using pandas. 2. Create a scatter plot of `bill_length_mm` vs. `bill_depth_mm`, colored by the `species` of the penguin. 3. Customize the color palette using `seaborn.hls_palette`: - Use a different number of colors for the species. - Adjust the lightness and saturation of the palette. - Change the start-point for hue sampling. 4. Save the plot to a file named `customized_scatter_plot.png`. Input: - The dataset file `penguins.csv` is assumed to be in the current working directory. - You must use seaborn for plot creation and customization. Output: - A file named `customized_scatter_plot.png` containing the scatter plot with the customized color palette. Constraints: - Use seaborn and pandas libraries. - Adjust lightness to 0.5 and saturation to 0.6. - Change the start-point for hue sampling to 0.3. - Ensure the plot includes a legend and appropriate axis labels. Example: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load dataset data = pd.read_csv(\'penguins.csv\') # Create scatter plot plt.figure(figsize=(8, 6)) palette = sns.hls_palette(n_colors=3, l=0.5, s=0.6, h=0.3) sns.scatterplot(data=data, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', palette=palette) # Customize plot plt.title(\'Scatter Plot of Penguin Bill Dimensions\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.legend(title=\'Penguin Species\') # Save plot plt.savefig(\'customized_scatter_plot.png\') plt.show() ``` Make sure your output plot is saved to `customized_scatter_plot.png` and visually verifies the required customizations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_customized_scatter_plot(file_path, output_path): # Load dataset data = pd.read_csv(file_path) # Create scatter plot plt.figure(figsize=(8, 6)) palette = sns.hls_palette(n_colors=3, l=0.5, s=0.6, h=0.3) sns.scatterplot(data=data, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', palette=palette) # Customize plot plt.title(\'Scatter Plot of Penguin Bill Dimensions\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.legend(title=\'Penguin Species\') # Save plot plt.savefig(output_path) plt.show() # Usage example (Commented out because this part is not intended to be executed during testing) # create_customized_scatter_plot(file_path=\'penguins.csv\', output_path=\'customized_scatter_plot.png\')"},{"question":"**Problem Statement:** In this coding assessment, you are required to write a Python function that utilizes the `python310` package to introspect and gather detailed information about the current execution frame. This will test your understanding of interacting with the Python execution environment and frames. # Function Signature: ```python def introspect_execution_frame() -> dict: pass ``` # Objective: - Write a function `introspect_execution_frame` that gathers and returns detailed information about the current execution frame. # Requirements: 1. The function should return a dictionary with the following keys and values: - `\'builtins\'`: A dictionary of built-in functions available in the current frame. - `\'locals\'`: A dictionary of local variables available in the current frame. - `\'globals\'`: A dictionary of global variables available in the current frame. - `\'current_frame\'`: A string describing the current frame. - `\'back_frame\'`: A string describing the parent frame (if any). - `\'frame_code\'`: The code object associated with the current frame, as a string. - `\'line_number\'`: The current line number being executed in the frame. - `\'func_name\'`: The name of a function if the frame represents a function call. - `\'func_desc\'`: A description of the function based on its type. # Constraints: - You may assume that the function is called from a context where it has access to an active execution frame. - Use appropriate error handling for cases where a particular frame property is not available (e.g., no back frame). # Example: ```python result = introspect_execution_frame() # Example Output: # { # \'builtins\': {...}, # \'locals\': {...}, # \'globals\': {...}, # \'current_frame\': \'<frame object at ...>\', # \'back_frame\': None, # \'frame_code\': \'<code object at ...>\', # \'line_number\': 10, # \'func_name\': \'introspect_execution_frame\', # \'func_desc\': \'()\' # } ``` Note: The exact representation of certain objects like frame objects and code objects may vary, but ensure to provide a meaningful description. # Hints: - Review the functions provided in the `python310` package to understand their usage. - You can combine the knowledge of Python\'s introspection capabilities with the functions provided by the `python310` package. **Good Luck!**","solution":"import sys import inspect def introspect_execution_frame() -> dict: frame = inspect.currentframe() if frame is None: raise RuntimeError(\\"Could not retrieve the current frame\\") builtins = frame.f_builtins locals_ = frame.f_locals globals_ = frame.f_globals current_frame_description = f\\"<frame object at {hex(id(frame))}>\\" back_frame = frame.f_back back_frame_description = f\\"<frame object at {hex(id(back_frame))}>\\" if back_frame else None frame_code = frame.f_code frame_code_description = str(frame_code) line_number = frame.f_lineno func_name = frame.f_code.co_name func_desc = frame.f_code.co_varnames return { \'builtins\': builtins, \'locals\': locals_, \'globals\': globals_, \'current_frame\': current_frame_description, \'back_frame\': back_frame_description, \'frame_code\': frame_code_description, \'line_number\': line_number, \'func_name\': func_name, \'func_desc\': func_desc, }"},{"question":"**Title: Regular Expression Matcher** # Problem Statement You are required to write a function `regex_matcher(pattern: str, string: str) -> bool` that takes two arguments: a `pattern` and a `string`. This function should determine if the given string matches the regular expression pattern provided. # Input - `pattern` (str): A non-empty string representing the regular expression pattern. - `string` (str): A non-empty string that needs to be checked against the pattern. # Output - Return `True` if the string matches the pattern, otherwise `False`. # Examples ```python assert regex_matcher(r\'^[a-zA-Z0-9_]+\', \'Valid_123\') == True assert regex_matcher(r\'^a.*b\', \'a123b\') == True assert regex_matcher(r\'^a.*b\', \'a123c\') == False assert regex_matcher(r\'^d{3}-d{2}-d{4}\', \'123-45-6789\') == True assert regex_matcher(r\'^d{3}-d{2}-d{4}\', \'12-345-6789\') == False ``` # Constraints - You should handle basic regular expressions including anchors (`^`, ``), character classes (`[]`), quantifiers (`*`, `+`, `{}`), and predefined character classes (`d`, `w`, `s`, etc.). - Assume the input will always be valid regular expressions and non-empty strings. - Do not use any built-in Python libraries for regular expressions, such as the `re` module. The goal is to demonstrate your understanding by implementing your logic. # Notes 1. Regular expressions are a powerful tool and this problem is designed to test your ability to implement pattern matching from scratch. 2. Focus on creating a robust solution that handles various edge cases for regular expressions. # Function Signature ```python def regex_matcher(pattern: str, string: str) -> bool: # Implement your function here pass ```","solution":"def regex_matcher(pattern: str, string: str) -> bool: A skeleton function that uses the `re` module to match the pattern. The actual implementation would involve creating the regex logic from scratch. import re return bool(re.match(pattern, string))"},{"question":"<|Analysis Begin|> The provided documentation explains the `warnings` module in Python, which provides a way to alert users of potential issues in a program without stopping its execution. Specifically, it details how to: - Issue warnings. - Control warning filters. - Handle different categories of warnings like `DeprecationWarning`, `UserWarning`, etc. - Modify the warning behavior using functions such as `warn`, `showwarning`, `filterwarnings`, and more. - Use the `catch_warnings` context manager to temporarily suppress or capture warnings. To create a challenging and comprehensive assessment question, the following concepts can be integrated: 1. **Warning Issuance**: Request students to write functions that issue warnings using `warn` and `warn_explicit`. 2. **Custom Warning Categories**: Design a custom warning category that can be used in the implementation. 3. **Warning Filtering**: Control whether a warning should be shown, ignored, or turned into an error using `filterwarnings` and `simplefilter`. 4. **Context Management**: Utilize `catch_warnings` context manager to temporarily modify warning behavior. 5. **Testing of Warnings**: Verify that the warnings are issued correctly within specified conditions. This will require the students to fully understand the how, when, and why of using warnings, and how to manage and test them in a Python program. <|Analysis End|> <|Question Begin|> # Python Warnings Assessment Objective Demonstrate understanding and application of Python\'s `warnings` module by creating functions that issue warnings and control their behavior. Task 1. **Create a Custom Warning Category**: Define a new warning category named `AssignmentWarning`, which should be a subclass of `UserWarning`. 2. **Issuing Warnings**: Implement a function `issue_warning` that takes a message string and issues an `AssignmentWarning` warning with this message. ```python def issue_warning(message: str): Issue a custom AssignmentWarning with the provided message. Parameters: - message (str): The warning message to be displayed. # Your code here ``` 3. **Filtering Warnings**: Implement a function `set_warning_filter` that sets a warning filter based on the action provided. The function should take two arguments: - `action` (str): The action to be taken on matching warnings. Must be one of \\"error\\", \\"ignore\\", \\"always\\", \\"default\\", \\"module\\", \\"once\\". - `category` (type): The warning category to filter. If None, it should apply to `AssignmentWarning`. ```python def set_warning_filter(action: str, category: type = None): Set a warning filter for the specified action and category. Parameters: - action (str): The action to be taken on matching warnings. - category (type, optional): The warning category to filter. Defaults to AssignmentWarning. Raises: - ValueError: If the action is not a valid warning filter action. # Your code here ``` 4. **Capturing Warnings**: Implement a function `capture_warnings` that runs a provided function `func` and captures all warnings issued during its execution. Return the list of captured warnings. ```python def capture_warnings(func): Capture warnings issued during the execution of the given function. Parameters: - func (callable): The function to execute. Returns: - list: A list of captured warning records. # Your code here ``` 5. **Testing**: - Instantiate a test function `test_warning_system`. - Within this test function, first ensure no warnings are issued. - Set the warning filter to \\"always\\" for `AssignmentWarning`. - Issue a warning using `issue_warning`. - Capture the warning and verify that it has been correctly issued. ```python def test_warning_system(): Test the warning system to ensure warnings are issued and captured correctly. # Your code here ``` Constraints - The `issue_warning` function should only issue an `AssignmentWarning` when called. - The `set_warning_filter` function should validate the `action` parameter to ensure it matches one of the allowed actions. - The `capture_warnings` function should properly use the `catch_warnings` context manager to capture warnings. Example Functionality ```python # Defining Custom AssignmentWarning issue_warning(\\"This is a test warning\\") # Setting warning filter set_warning_filter(\\"always\\", AssignmentWarning) # Function to test and capture warnings def some_function(): issue_warning(\\"Testing warning capture\\") warnings_list = capture_warnings(some_function) print(len(warnings_list)) # Should output 1 # Test runner test_warning_system() # Should validate the correct issuance and capture of warnings. ``` Ensure your code includes proper docstrings and comments where necessary to explain the functionality.","solution":"import warnings class AssignmentWarning(UserWarning): Custom warning category for assignment-related warnings. pass def issue_warning(message: str): Issue a custom AssignmentWarning with the provided message. Parameters: - message (str): The warning message to be displayed. warnings.warn(message, AssignmentWarning) def set_warning_filter(action: str, category: type = None): Set a warning filter for the specified action and category. Parameters: - action (str): The action to be taken on matching warnings. - category (type, optional): The warning category to filter. Defaults to AssignmentWarning if None. Raises: - ValueError: If the action is not a valid warning filter action. valid_actions = [\\"error\\", \\"ignore\\", \\"always\\", \\"default\\", \\"module\\", \\"once\\"] if action not in valid_actions: raise ValueError(f\\"Invalid action: {action}. Must be one of {valid_actions}\\") if category is None: category = AssignmentWarning warnings.filterwarnings(action, category=category) def capture_warnings(func): Capture warnings issued during the execution of the given function. Parameters: - func (callable): The function to execute. Returns: - list: A list of captured warning records. with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") func() return w"},{"question":"As a programmer, you are tasked with integrating lower-level C APIs with Python\'s high-level file I/O operations. You will create a Python function that uses the provided C APIs to handle file objects and file descriptors. This function should read a specific number of lines from a file and then write a particular message at the end of the file. Implement the function `read_and_write_file(fd: int, num_lines: int, message: str) -> str` that performs the following operations: 1. **Read Lines**: - Use the file descriptor (`fd`) to create a Python file object. - Read the specified number of lines (`num_lines`) from the file. If the file contains fewer lines than requested, read all available lines. - Collect and return these read lines as a single string. 2. **Write Message**: - Write the provided message (`message`) to the end of the file. Ensure that this new content appears after the original content. **Function Specifications**: - **Input**: - `fd (int)`: A valid file descriptor for an open file. - `num_lines (int)`: The number of lines to read from the file. - `message (str)`: A string message to be appended to the file. - **Output**: - `str`: A string containing the lines read from the file. - **Constraints**: - You may assume that the file associated with the provided file descriptor is readable and writable. - Handle exceptions gracefully and ensure the file is properly closed, even in case of errors. - **Performance**: - Ensure that reading and writing operations are efficiently performed considering both high-level Python and low-level file handling principles. **Example**: ```python fd = os.open(\'example.txt\', os.O_RDWR) num_lines = 5 message = \\"End of file.n\\" result = read_and_write_file(fd, num_lines, message) print(result) # Output should be the first 5 lines of \'example.txt\'. \'End of file.\' should be appended at the end of \'example.txt\'. ``` **Note**: 1. You may need to import necessary modules like `os` and `io`. 2. Make sure to use the provided C API functions where applicable.","solution":"import os import io def read_and_write_file(fd: int, num_lines: int, message: str) -> str: Reads a specified number of lines from a file and appends a message at the end of the file. Args: fd (int): A valid file descriptor for an open file. num_lines (int): The number of lines to read from the file. message (str): A string message to be appended to the file. Returns: str: A string containing the lines read from the file. try: # Create a Python file object from the file descriptor file_obj = os.fdopen(fd, \'r+\') # Reading lines from the file lines_read = [] for _ in range(num_lines): line = file_obj.readline() if not line: break lines_read.append(line) # Move the file cursor to the end for appending file_obj.seek(0, os.SEEK_END) # Write the message to the end of the file file_obj.write(message) # Collect and return all read lines as a single string result = \'\'.join(lines_read) return result except Exception as e: raise e finally: file_obj.close()"},{"question":"# Advanced Python Programming Question: Simulating Shadow Password Database Queries **Objective:** Write a Python program that simulates querying a shadow password database. You will create a mock database and implement functions that mimic the behavior of the `spwd` modules functions. **Task:** 1. Create a function `create_mock_spwd_db()` that returns a list of mock shadow password database entries. Each entry should be a tuple with the following structure: - `login name` (string) - `encrypted password` (string) - `date of last change` (integer, number of days since epoch) - `minimal number of days between changes` (integer) - `maximum number of days between changes` (integer) - `number of days before password expires to warn user about it` (integer) - `number of days after password expires until account is disabled` (integer) - `number of days since 1970-01-01 when account expires` (integer) - `reserved` (integer) 2. Implement the function `get_mock_spnam(mock_db, name)` which simulates `spwd.getspnam(name)`. This function should take the mock database and a user name as input and return the corresponding shadow password database entry. 3. Implement the function `get_mock_spall(mock_db)` which simulates `spwd.getspall()`. This function should return the entire list of shadow password database entries. # Constraints - You do not need root privileges to access the mock database. - Ensure your `create_mock_spwd_db()` function includes at least 3 mock entries to simulate a realistic database. - Raise a `KeyError` if the specified user name is not found in the `get_mock_spnam` function. # Input and Output Formats **Function: `create_mock_spwd_db()`** **Output:** - List of tuples, each containing 9 elements (attributes). **Function: `get_mock_spnam(mock_db, name)`** **Input:** - `mock_db`: List of tuples returned by `create_mock_spwd_db()`. - `name`: String representing the login name to query. **Output:** - Tuple representing the shadow password database entry for the given user name. **Function: `get_mock_spall(mock_db)`** **Input:** - `mock_db`: List of tuples returned by `create_mock_spwd_db()`. **Output:** - List of all shadow password database entries. ```python def create_mock_spwd_db(): # Mock implementation here pass def get_mock_spnam(mock_db, name): # Mock implementation here pass def get_mock_spall(mock_db): # Mock implementation here pass # Example usage: # mock_db = create_mock_spwd_db() # user_entry = get_mock_spnam(mock_db, \\"someuser\\") # all_entries = get_mock_spall(mock_db) ``` # Example ```python mock_db = create_mock_spwd_db() print(get_mock_spnam(mock_db, \\"user1\\")) # Should return the tuple for user1 print(get_mock_spall(mock_db)) # Should return all mock entries ``` # Notes: - Make sure to test these functions thoroughly. - Consider edge cases such as querying for a user name that does not exist. - Structuring your tuples correctly is crucial since this will mimic real shadow password database entries.","solution":"def create_mock_spwd_db(): Creates a mock shadow password database. Returns: List of tuples each containing the following 9 elements: - login name (string) - encrypted password (string) - date of last change (integer) - minimal number of days between changes (integer) - maximum number of days between changes (integer) - number of days before password expires to warn user (integer) - number of days after password expires until account is disabled (integer) - number of days since 1970-01-01 when account expires (integer) - reserved (integer) return [ (\\"user1\\", \\"password1\\", 18000, 0, 99999, 7, -1, -1, -1), (\\"user2\\", \\"password2\\", 18250, 0, 99999, 7, -1, -1, -1), (\\"user3\\", \\"password3\\", 18500, 0, 99999, 7, -1, -1, -1) ] def get_mock_spnam(mock_db, name): Mimics spwd.getspnam(name). Returns the shadow password database entry for the given user name. Args: mock_db: List of tuples returned by create_mock_spwd_db(). name: String representing the login name to query. Returns: Tuple representing the shadow password database entry for the given user name. Raises: KeyError: If the specified user name is not found. for entry in mock_db: if entry[0] == name: return entry raise KeyError(f\\"User {name} not found in the database.\\") def get_mock_spall(mock_db): Mimics spwd.getspall(). Returns the entire list of shadow password database entries. Args: mock_db: List of tuples returned by create_mock_spwd_db(). Returns: List of all shadow password database entries. return mock_db"},{"question":"# Custom Python Object Implementation In this task, you will implement a custom Python object type that models a simple mathematical vector. This vector should support basic arithmetic operations, attribute access, and iterative sequence behavior. The implementation should also handle memory allocation and garbage collection appropriately. Requirements: 1. Implement a Python object type called `PyVector`. 2. The `PyVector` should be initialized with a list of numbers representing the vector\'s components. 3. Implement the following operations for the `PyVector`: - Vector addition (`+`) - Vector subtraction (`-`) - Scalar multiplication (`*`) - Dot product (`@`) 4. Provide a method to calculate the magnitude of the vector. 5. Allow attribute access for individual components. 6. Implement iteration over the vector\'s components. 7. Ensure the object supports cyclic garbage collection to handle memory management properly. Input and Output Formats: 1. **Initialization:** ```python vector = PyVector([1, 2, 3]) ``` 2. **Operations:** ```python v1 = PyVector([1, 2, 3]) v2 = PyVector([4, 5, 6]) v3 = v1 + v2 # PyVector([5, 7, 9]) v4 = v1 - v2 # PyVector([-3, -3, -3]) v5 = v1 * 10 # PyVector([10, 20, 30]) dot_product = v1 @ v2 # 32 (1*4 + 2*5 + 3*6) magnitude = v1.magnitude() # 3.7416573867739413 ``` 3. **Attribute Access:** ```python print(v1[0]) # 1 v1[0] = 10 print(v1[0]) # 10 ``` 4. **Iteration:** ```python for component in v1: print(component) # Output: 10 2 3 ``` Constraints: - You are not allowed to use external libraries for vector operations. - Ensure that the implementation is efficient in terms of memory and performance. - Correctly handle edge cases, such as empty vectors or vectors with different lengths. Additional Notes: - Utilize `tp slots`, `PyTypeObject Definition`, and `PyObject Slots` as necessary. - Ensure your implementation adheres to Python\'s garbage collection mechanisms for cyclic references. This task tests your understanding of custom object types, operator overloading, memory management, and iteration protocols in Python. Good luck!","solution":"import math class PyVector: def __init__(self, components): self.components = components def __add__(self, other): return PyVector([a + b for a, b in zip(self.components, other.components)]) def __sub__(self, other): return PyVector([a - b for a, b in zip(self.components, other.components)]) def __mul__(self, scalar): return PyVector([a * scalar for a in self.components]) def __matmul__(self, other): return sum(a * b for a, b in zip(self.components, other.components)) def magnitude(self): return math.sqrt(sum(x ** 2 for x in self.components)) def __getitem__(self, index): return self.components[index] def __setitem__(self, index, value): self.components[index] = value def __iter__(self): return iter(self.components) def __len__(self): return len(self.components) def __repr__(self): return f\\"PyVector({self.components})\\""},{"question":"Objective: Your task is to implement an `asyncio` based synchronization mechanism for a shared counter resource accessed by multiple asynchronous tasks. You will demonstrate the use of `asyncio.Lock`, `asyncio.Event`, and `asyncio.Semaphore` classes. Problem: Implement a class `SharedCounter` that manages a counter which can be incremented or decremented by multiple tasks. Also, maintain functionality to set an event when the counter reaches a certain threshold. Use proper synchronization to ensure protection against race conditions. Class Definition: ```python import asyncio class SharedCounter: def __init__(self, initial=0, max_concurrent_tasks=3, threshold=10): Initializes the SharedCounter with a given initial value, maximum number of concurrent tasks, and threshold value for the event. :param initial: Initial value of the counter. :param max_concurrent_tasks: Maximum number of concurrent tasks that can modify the counter. :param threshold: Threshold value at which an event should be set. self.counter = initial self.threshold = threshold self.lock = asyncio.Lock() self.threshold_event = asyncio.Event() self.semaphore = asyncio.Semaphore(max_concurrent_tasks) async def increment(self, value): Increment the counter by the given value in a safe manner. async with self.semaphore: async with self.lock: self.counter += value print(f\'Counter incremented to: {self.counter}\') self._check_threshold() async def decrement(self, value): Decrement the counter by the given value in a safe manner. async with self.semaphore: async with self.lock: self.counter -= value print(f\'Counter decremented to: {self.counter}\') self._check_threshold() def _check_threshold(self): Check if the counter has reached the threshold and set the event. if self.counter >= self.threshold: self.threshold_event.set() else: self.threshold_event.clear() async def wait_for_threshold(self): Wait until the counter reaches the threshold value. await self.threshold_event.wait() # Sample usage: async def task(counter, increment_value): await counter.increment(increment_value) await counter.decrement(increment_value) async def main(): counter = SharedCounter(initial=0, max_concurrent_tasks=3, threshold=10) tasks = [] for value in range(10): tasks.append(task(counter, value + 1)) await asyncio.gather(*tasks) print(\\"All tasks completed.\\") await counter.wait_for_threshold() print(\\"Counter reached the threshold.\\") # Run the main function # asyncio.run(main()) ``` # Constraints: - The counter should be accessed and modified in a thread-safe manner using `asyncio.Lock`. - No more than `max_concurrent_tasks` should be modifying the counter at the same time, enforced using `asyncio.Semaphore`. - The counter should notify via `asyncio.Event` when it reaches or exceeds the threshold value. - Handle tasks in an asynchronous manner without blocking the event loop. # Input and Output: - The class methods will be invoked asynchronously. - The methods should print updates to the console indicating the changes in the counter and when the threshold event is set. # Performance Requirements: - Ensure the solution can handle a relatively high number of tasks without significant performance degradation. - Properly manage concurrency to avoid race conditions and ensure the counter\'s integrity. Note: Ensure you test your class implementation with various scenarios to validate its correctness and concurrency handling.","solution":"import asyncio class SharedCounter: def __init__(self, initial=0, max_concurrent_tasks=3, threshold=10): Initializes the SharedCounter with a given initial value, maximum number of concurrent tasks, and threshold value for the event. :param initial: Initial value of the counter. :param max_concurrent_tasks: Maximum number of concurrent tasks that can modify the counter. :param threshold: Threshold value at which an event should be set. self.counter = initial self.threshold = threshold self.lock = asyncio.Lock() self.threshold_event = asyncio.Event() self.semaphore = asyncio.Semaphore(max_concurrent_tasks) async def increment(self, value): Increment the counter by the given value in a safe manner. async with self.semaphore: async with self.lock: self.counter += value print(f\'Counter incremented to: {self.counter}\') self._check_threshold() async def decrement(self, value): Decrement the counter by the given value in a safe manner. async with self.semaphore: async with self.lock: self.counter -= value print(f\'Counter decremented to: {self.counter}\') self._check_threshold() def _check_threshold(self): Check if the counter has reached the threshold and set the event. if self.counter >= self.threshold: self.threshold_event.set() else: self.threshold_event.clear() async def wait_for_threshold(self): Wait until the counter reaches the threshold value. await self.threshold_event.wait()"},{"question":"# Question: **Principal Component Analysis and Clustering Using scikit-learn** You are provided with a CSV file, `customers.csv`, which contains anonymous data for customer transactions, including various features such as total purchase amount, frequency of purchases, average purchase size, etc. You need to perform Principal Component Analysis (PCA) and K-Means clustering to segment these customers into distinct groups. # Requirements: 1. **Data Preprocessing:** - Load the data from the `customers.csv` file. - Handle missing values by imputing the mean of the respective columns. - Standardize the data to have a mean of 0 and a standard deviation of 1. 2. **Principal Component Analysis (PCA):** - Apply PCA to reduce the data to 2 principal components. - Store the transformed data into a DataFrame with two columns representing the principal components. 3. **Clustering:** - Use the K-Means algorithm to cluster the data into 3 clusters based on the principal components. - Add the cluster labels to the PCA-transformed DataFrame. 4. **Visualization:** - Create a scatter plot of the PCA-transformed data with each point colored by its cluster label. # Function Signature: ```python import pandas as pd import numpy as np from sklearn.decomposition import PCA from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer import matplotlib.pyplot as plt def analyze_customers(file_path: str) -> pd.DataFrame: Load customer data, apply PCA, cluster with K-Means and visualize the results. Parameters: file_path (str): The path to the customers.csv file. Returns: pd.DataFrame: DataFrame containing the principal components and cluster labels. # Your code here # ``` # **Example Usage:** ```python # Assuming the function and the necessary libraries are correctly implemented and imported. df = analyze_customers(\\"path/to/customers.csv\\") print(df.head()) ``` # **Input:** The input will be a path to a CSV file, `customers.csv`, which you can generate synthetically for testing purposes. # **Output:** The function returns a DataFrame with at least three columns: the two principal components and the cluster labels. # **Example Output:** A DataFrame with the structure: ``` principal_component_1 principal_component_2 cluster_label 0 -1.225457 2.387932 0 1 0.932148 1.008298 1 2 1.115040 -0.121607 2 3 0.152816 0.784935 0 4 -1.051250 1.183218 1 ``` # Criteria: - **Correct implementation of `PCA` and `KMeans` using scikit-learn. - Proper handling of missing values. - Standardized preprocessing of data. - Clear and interpretable visualization of clusters. - Accurate return format of the clustered data. # Constraints: - Use the `sklearn` library for PCA and clustering. - Ensure your code is well-structured and commented for readability. # Performance: - The implementation should handle a typical dataset size efficiently. **Note**: This assessment checks your understanding of PCA, clustering techniques, and data preprocessing using scikit-learn. Make sure your implementation follows best practices for handling data.","solution":"import pandas as pd import numpy as np from sklearn.decomposition import PCA from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer import matplotlib.pyplot as plt def analyze_customers(file_path: str) -> pd.DataFrame: Load customer data, apply PCA, cluster with K-Means and visualize the results. Parameters: file_path (str): The path to the customers.csv file. Returns: pd.DataFrame: DataFrame containing the principal components and cluster labels. # Load data df = pd.read_csv(file_path) # Handle missing values by imputing the mean imputer = SimpleImputer(strategy=\'mean\') data_imputed = imputer.fit_transform(df) # Standardize the data scaler = StandardScaler() data_scaled = scaler.fit_transform(data_imputed) # Apply PCA to reduce to 2 principal components pca = PCA(n_components=2) principal_components = pca.fit_transform(data_scaled) # Convert to a DataFrame pca_df = pd.DataFrame(data=principal_components, columns=[\'principal_component_1\', \'principal_component_2\']) # Use K-Means to cluster the data into 3 clusters kmeans = KMeans(n_clusters=3, random_state=42) clusters = kmeans.fit_predict(pca_df) pca_df[\'cluster_label\'] = clusters # Visualize the clusters plt.figure(figsize=(10, 6)) plt.scatter(pca_df[\'principal_component_1\'], pca_df[\'principal_component_2\'], c=pca_df[\'cluster_label\'], cmap=\'viridis\', s=50) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'Customer Segments based on PCA and K-Means Clustering\') plt.colorbar(label=\'Cluster Label\') plt.show() return pca_df"},{"question":"# Python Coding Question Create a Python function using the `tarfile` module that extracts only specific files from a tar archive based on file size and type criteria. The function should meet the following requirements: 1. The function should be named `extract_files`. 2. The function should take four parameters: - `archive_name`: str  the name of the tar archive file. - `output_dir`: str  the directory to extract the files to. - `max_size`: int  maximum file size in bytes. Only files with sizes less than or equal to this should be extracted. - `file_types`: list of str  a list of file type strings. Only files matching these types should be extracted. Valid file type strings are \'file\', \'directory\', \'symlink\'. 3. The function should extract files to the specified `output_dir`. 4. The function should skip and not extract any file that does not meet the size and type criteria. 5. If the tar archive or any files within it are invalid or cannot be read due to compression errors, the function should handle these errors gracefully and print appropriate error messages. Input and Output: - `archive_name` is a path to the tar file. - `output_dir` is a path to the directory where files should be extracted. - `max_size` is an integer representing the maximum file size in bytes. - `file_types` is a list of strings that can include \'file\', \'directory\', and \'symlink\'. - The function does not return any value. # Example Usage ```python extract_files(\'example.tar.gz\', \'destination_folder\', 1048576, [\'file\', \'symlink\']) ``` In this example, the function will extract files and symbolic links from `example.tar.gz` to `destination_folder`, but only if their size is 1 MB (1048576 bytes) or less. # Constraints - The tar archive can be large, so ensure that your solution is efficient in terms of memory usage. - Use the `tarfile` module\'s context management capabilities. - Proper error handling should be implemented for cases where tar files are corrupt or unreadable. # Notes - You may assume that the `output_dir` exists and is writable. - Test your solution with tar files of various sizes and compositions for robustness.","solution":"import tarfile import os def extract_files(archive_name, output_dir, max_size, file_types): Extracts files from a tar archive based on file size and type criteria. Parameters: archive_name (str): The name of the tar archive file. output_dir (str): The directory to extract the files to. max_size (int): Maximum file size in bytes. file_types (list of str): List of valid file types to extract (e.g., \'file\', \'directory\', \'symlink\'). try: with tarfile.open(archive_name, \'r:*\') as archive: for member in archive.getmembers(): if member.size <= max_size and ((member.isfile() and \'file\' in file_types) or (member.isdir() and \'directory\' in file_types) or (member.issym() and \'symlink\' in file_types)): archive.extract(member, path=output_dir) except tarfile.TarError as e: print(f\\"Error reading tar archive: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"**Coding Assessment Question:** # Objective Write a Python function that demonstrates the process of porting code from Python 2 to Python 3, focusing on handling text versus binary data and ensuring compatibility with both versions. # Problem Statement You are given a simple function written in Python 2 that reads a file, processes the content, and writes the output to another file. Your task is to update this function to be compatible with both Python 2.7 and Python 3. Ensure that the handling of text and binary data is appropriately managed. # Original Python 2 Code ```python def process_file(input_file, output_file): with open(input_file, \'r\') as f: content = f.read() processed_content = content.replace(\'Python 2\', \'Python 3\') with open(output_file, \'w\') as f: f.write(processed_content) ``` # Tasks 1. Update the `process_file` function to be compatible with both Python 2.7 and Python 3. 2. Ensure that the function works correctly with text data, handling Unicode in Python 2 and 3. 3. Use `absolute_import`, `division`, and `print_function` from `__future__` to prevent regressions. 4. Use `io.open` instead of the built-in `open` function to handle file operations consistently between Python 2 and 3. 5. Modify the function to ensure compatibility and correct handling of text data. # Constraints - The input files will contain text data (not binary data). - Assume the files are encoded in UTF-8. # Expected Function Signature ```python def process_file(input_file, output_file): pass ``` # Example Given an input file `input.txt` with the following content: ``` This is a test file for Python 2. Python 2 is supported. ``` And the function call: ```python process_file(\'input.txt\', \'output.txt\') ``` The output file `output.txt` should contain: ``` This is a test file for Python 3. Python 3 is supported. ``` # Solution Requirements - Use of `absolute_import`, `division`, and `print_function` from `__future__`. - Handling of Unicode data correctly in both Python 2 and 3. - Usage of `io.open` for file operations. - The solution should be written in a way that it is compatible with both Python 2.7 and Python 3.","solution":"from __future__ import absolute_import, division, print_function import io def process_file(input_file, output_file): with io.open(input_file, \'r\', encoding=\'utf-8\') as f: content = f.read() processed_content = content.replace(\'Python 2\', \'Python 3\') with io.open(output_file, \'w\', encoding=\'utf-8\') as f: f.write(processed_content)"},{"question":"You are given a CSV file containing a dataset and need to implement several functions to analyze this dataset using pandas. The questions will focus on both basic and advanced pandas functionalities including reading data, filtering, aggregations, and making use of pandas options to customize outputs. # Dataset The dataset is provided in a file named `data.csv` and contains the following columns: - `Date`: The date of the entry in `YYYY-MM-DD` format. - `Category`: The category of the entry. - `Amount`: The amount associated with the entry. - `Description`: A brief description of the entry. # Task Implement the following functions: 1. **read_data(file_path: str) -> pd.DataFrame**: - Read the CSV file from the given file path into a pandas DataFrame. - Return the DataFrame. 2. **filter_data(df: pd.DataFrame, start_date: str, end_date: str, min_amount: float) -> pd.DataFrame**: - Filter the DataFrame to include only rows where the `Date` is between `start_date` and `end_date` (inclusive) and the `Amount` is greater than or equal to `min_amount`. - `start_date` and `end_date` are strings in the `YYYY-MM-DD` format. - Return the filtered DataFrame. 3. **aggregate_data(df: pd.DataFrame) -> pd.DataFrame**: - Group the data by `Category` and calculate the total `Amount` for each category. - Reset the index of the resulting DataFrame. - Return the aggregated DataFrame with columns `Category` and `TotalAmount`. 4. **format_display(df: pd.DataFrame) -> None**: - Set pandas options such that DataFrame displays are limited to a maximum of 10 rows and 5 columns. - Set precision for numerical outputs to 2 decimal places. - Print the DataFrame using pandas display capabilities. # Example Usage ```python file_path = \'data.csv\' data = read_data(file_path) filtered_data = filter_data(data, \'2023-01-01\', \'2023-12-31\', 50.0) aggregated_data = aggregate_data(filtered_data) format_display(aggregated_data) ``` # Constraints - You may assume the dataset is properly formatted and does not contain missing or invalid data. - Your implementation should consider performance implications for large datasets as much as possible without sacrificing clarity.","solution":"import pandas as pd def read_data(file_path: str) -> pd.DataFrame: Read the CSV file from the given file path into a pandas DataFrame. :param file_path: Path to the CSV file. :return: DataFrame containing the dataset. return pd.read_csv(file_path) def filter_data(df: pd.DataFrame, start_date: str, end_date: str, min_amount: float) -> pd.DataFrame: Filter the DataFrame to include only rows where the Date is between start_date and end_date (inclusive) and the Amount is greater than or equal to min_amount. :param df: DataFrame containing the dataset. :param start_date: Start date string in YYYY-MM-DD format. :param end_date: End date string in YYYY-MM-DD format. :param min_amount: Minimum amount for filtering. :return: Filtered DataFrame. return df[(df[\'Date\'] >= start_date) & (df[\'Date\'] <= end_date) & (df[\'Amount\'] >= min_amount)] def aggregate_data(df: pd.DataFrame) -> pd.DataFrame: Group the data by Category and calculate the total Amount for each category. Reset the index of the resulting DataFrame. :param df: DataFrame containing the dataset. :return: Aggregated DataFrame with columns Category and TotalAmount. aggregated_df = df.groupby(\'Category\')[\'Amount\'].sum().reset_index() aggregated_df.rename(columns={\'Amount\': \'TotalAmount\'}, inplace=True) return aggregated_df def format_display(df: pd.DataFrame) -> None: Set pandas options such that DataFrame displays are limited to a maximum of 10 rows and 5 columns. Set precision for numerical outputs to 2 decimal places. Print the DataFrame using pandas display capabilities. :param df: DataFrame to display. pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_columns\', 5) pd.set_option(\'display.precision\', 2) print(df)"},{"question":"# Objective: Design a function that extracts specific details from a set of JSON files, processes the data, and then outputs a CSV file with the results. The challenge involves file handling (reading JSON and writing CSV), data processing, and using various modules from the Python Standard Library. # Problem Statement: You need to write a function called `process_json_to_csv(input_dir: str, output_file: str) -> None` that reads all JSON files from a specified directory (`input_dir`), processes the data to extract specific information, and then writes the processed data to a CSV file (`output_file`). Each JSON file represents details of a set of products, and your task is to extract pertinent information for each product and compile them into a CSV. # Input: - `input_dir`: A string representing the directory path where the JSON files are located. - `output_file`: A string representing the file path for the resulting CSV file. # JSON File Structure: Each JSON file contains a list of product objects. Each product object has the following structure: ```json { \\"id\\": int, \\"name\\": str, \\"category\\": str, \\"price\\": float, \\"stock_quantity\\": int } ``` # Output: The CSV file should have the following columns: - `Product ID` - `Product Name` - `Category` - `Price` - `Stock Quantity` # Example: Assuming you have two JSON files in the `input_dir`: **File1:** ```json [ {\\"id\\": 1, \\"name\\": \\"Laptop\\", \\"category\\": \\"Electronics\\", \\"price\\": 999.99, \\"stock_quantity\\": 10}, {\\"id\\": 2, \\"name\\": \\"Smartphone\\", \\"category\\": \\"Electronics\\", \\"price\\": 499.99, \\"stock_quantity\\": 50} ] ``` **File2:** ```json [ {\\"id\\": 3, \\"name\\": \\"Table\\", \\"category\\": \\"Furniture\\", \\"price\\": 199.99, \\"stock_quantity\\": 20}, {\\"id\\": 4, \\"name\\": \\"Chair\\", \\"category\\": \\"Furniture\\", \\"price\\": 99.99, \\"stock_quantity\\": 100} ] ``` The resulting CSV (`output_file`) should be: ``` Product ID,Product Name,Category,Price,Stock Quantity 1,Laptop,Electronics,999.99,10 2,Smartphone,Electronics,499.99,50 3,Table,Furniture,199.99,20 4,Chair,Furniture,99.99,100 ``` # Constraints: - You can assume all JSON files in the directory are well-formed and conform to the given structure. - The function should handle any number of JSON files in the specified directory. # Example Code Implementation: ```python import json import csv import os def process_json_to_csv(input_dir: str, output_file: str) -> None: # List to hold all product data products = [] # Process each JSON file in the directory for filename in os.listdir(input_dir): if filename.endswith(\'.json\'): filepath = os.path.join(input_dir, filename) with open(filepath, \'r\', encoding=\'utf-8\') as json_file: data = json.load(json_file) for product in data: products.append(product) # Write the product data to a CSV file with open(output_file, \'w\', newline=\'\', encoding=\'utf-8\') as csv_file: writer = csv.writer(csv_file) writer.writerow([\'Product ID\', \'Product Name\', \'Category\', \'Price\', \'Stock Quantity\']) for product in products: writer.writerow([product[\'id\'], product[\'name\'], product[\'category\'], product[\'price\'], product[\'stock_quantity\']]) ``` # Assessment Criteria: 1. **Correctness:** The function should accurately read, process, and transform the JSON data into the specified CSV format. 2. **Efficiency:** The function should handle large numbers of JSON files and large datasets efficiently. 3. **Usage of Standard Library:** The function demonstrates effective use of standard library modules such as `os`, `json`, and `csv`.","solution":"import json import csv import os def process_json_to_csv(input_dir: str, output_file: str) -> None: Processes JSON files in the specified directory to extract product details and writes them into a CSV file. Args: - input_dir: str : the directory path where the JSON files are located - output_file: str : the file path for the resulting CSV file products = [] for filename in os.listdir(input_dir): if filename.endswith(\'.json\'): filepath = os.path.join(input_dir, filename) with open(filepath, \'r\', encoding=\'utf-8\') as json_file: try: data = json.load(json_file) except json.JSONDecodeError: print(f\\"Error reading {filepath}. Skipping this file.\\") continue for product in data: products.append(product) with open(output_file, \'w\', newline=\'\', encoding=\'utf-8\') as csv_file: writer = csv.writer(csv_file) writer.writerow([\'Product ID\', \'Product Name\', \'Category\', \'Price\', \'Stock Quantity\']) for product in products: writer.writerow([product[\'id\'], product[\'name\'], product[\'category\'], product[\'price\'], product[\'stock_quantity\']])"},{"question":"# Question: You are given a binary file containing 32-bit integers. This file is written from a little-endian machine, and you need to ensure the data is correctly read on a machine with a different endianness (potentially big-endian). Implement a Python function `read_integers_from_file` that: 1. Reads the integers from the given binary file. 2. Ensures that the integers are correctly interpreted regardless of the endianness of the machine reading the file. 3. Returns these integers as a list of int. The constraints are: - The file may contain a large number of integers, so your solution should be memory efficient. - Do not use any external libraries like NumPy; only use the built-in `array` module. Function Signature ```python def read_integers_from_file(filename: str) -> list[int]: pass ``` Example Assume you have a binary file `data.bin` containing four little-endian 32-bit integers: `[1, 2, 3, 4]`. ```python result = read_integers_from_file(\'data.bin\') print(result) # Output: [1, 2, 3, 4] ``` **Hints:** - Use the `array` module to read and manipulate the binary data. - Make use of `byteswap()` method to handle the potential endian issues. Note: - You do not need to handle the actual creation of the binary file in your solution but assume `data.bin` exists and contains valid binary data in little-endian format.","solution":"import array def read_integers_from_file(filename: str) -> list[int]: Reads 32-bit integers from a binary file written in little-endian format and returns them as a list of integers. Args: - filename (str): The path to the binary file. Returns: - list[int]: The list of integers read from the file. with open(filename, \'rb\') as file: # Reading the entire file content to a bytearray data = bytearray(file.read()) num_elements = len(data) // 4 # Each integer is 4 bytes # Creating an array of integers with little-endian format integers = array.array(\'I\') integers.frombytes(data) # Ensure the array is interpreted as little-endian regardless of the host endian if array.array(\'H\', [1]).tobytes() == b\'x01x00\': # Little-endian host check return integers.tolist() else: # System is big-endian, needs byteswap integers.byteswap() return integers.tolist()"},{"question":"**Question: Working with WAV Files** You are required to implement a function named `process_wav_file` that reads a given WAV file, performs a simple transformation on the audio data, and writes the transformed data into a new WAV file. # Function Signature ```python def process_wav_file(input_filepath: str, output_filepath: str) -> None: pass ``` # Input - `input_filepath` (str): The file path to the input WAV file. - `output_filepath` (str): The file path where the transformed WAV file should be saved. # Output - The function does not return anything. It should save the transformed WAV data to the specified `output_filepath`. # Requirements and Constraints 1. **Reading the WAV file**: Use the wave module to read the input WAV file. 2. **Transformation Logic**: Reverse the audio data. If the file has stereo audio, ensure both channels are reversed. 3. **Writing the WAV file**: Write the reversed audio data into a new WAV file at the `output_filepath`. 4. **Performance Constraints**: The file reading, processing, and writing should handle at least up to 10 minutes of audio efficiently (assuming standard CD quality audio, which is 44.1 kHz sample rate, 16-bit depth, stereo). # Example Here\'s how you might test this function: ```python # Assuming there is an input file \'input.wav\' and we want to save the transformed file as \'output.wav\' process_wav_file(\'input.wav\', \'output.wav\') ``` Note - Pay attention to the audio file format specifics like number of channels, sample width, and frame rate. - Handle exceptions that may arise from file reading/writing operations (e.g., file not found, incorrect file format). # Hints - You can use the `wave` module\'s `Wave_read` and `Wave_write` objects for reading and writing WAV files respectively. - To reverse the audio data, you might need to read all frames, reverse them, and then write them back. - Ensure that the output file has the same parameters (like sample width, number of channels, frame rate) as the input file to maintain audio integrity.","solution":"import wave def process_wav_file(input_filepath: str, output_filepath: str) -> None: try: with wave.open(input_filepath, \'rb\') as input_wav: params = input_wav.getparams() n_channels = params.nchannels sampwidth = params.sampwidth framerate = params.framerate n_frames = params.nframes frames = input_wav.readframes(n_frames) reversed_frames = frames[::-1] with wave.open(output_filepath, \'wb\') as output_wav: output_wav.setnchannels(n_channels) output_wav.setsampwidth(sampwidth) output_wav.setframerate(framerate) output_wav.writeframes(reversed_frames) except FileNotFoundError: print(f\\"File not found: {input_filepath}\\") except wave.Error as e: print(f\\"Error processing WAV file: {e}\\")"},{"question":"**Problem Statement:** You are given a dataset of vehicle emissions (`emissions.csv`) containing the following columns: - `Vehicle_ID`: A unique identifier for each vehicle. - `Vehicle_Type`: Type of the vehicle (e.g., Car, Truck, Bus). - `CO2_Emissions`: CO2 emissions in grams per kilometer. - `Fuel_Consumption`: Fuel consumption in liters per 100 kilometers. 1. Load the dataset into a pandas DataFrame. 2. Clean the dataset by removing any rows with missing values. 3. Compute the average CO2 emissions and fuel consumption for each vehicle type. 4. Plot the average CO2 emissions and fuel consumption using a bar plot. Customize the plot to show: - Different colors for each vehicle type. - Appropriate labels and title. 5. Create a scatter plot to visualize the relationship between CO2 emissions and fuel consumption for each vehicle type. Distinguish the points by vehicle type using different colors and add a legend. 6. Generate a box plot to show the distribution of CO2 emissions for each vehicle type. 7. Save all plots as PNG files. **Input:** - A CSV file named `emissions.csv` containing vehicle emissions data. **Output:** - Three PNG files containing the plots: 1. Bar plot of average CO2 emissions and fuel consumption for each vehicle type. 2. Scatter plot of CO2 emissions vs. fuel consumption. 3. Box plot of CO2 emissions for each vehicle type. **Constraints:** - Ensure that all plots have appropriate titles, axis labels, and legends. - Use colormaps where applicable to distinguish different categories. - Handle any data preprocessing required to clean the dataset before plotting. ```python import pandas as pd import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'emissions.csv\') # Clean the dataset df = df.dropna() # Compute the average CO2 emissions and fuel consumption for each vehicle type averages = df.groupby(\'Vehicle_Type\').agg({\'CO2_Emissions\': \'mean\', \'Fuel_Consumption\': \'mean\'}).reset_index() # Plot the average CO2 emissions and fuel consumption using a bar plot plt.figure(figsize=(10, 6)) averages.plot(kind=\'bar\', x=\'Vehicle_Type\', y=[\'CO2_Emissions\', \'Fuel_Consumption\'], colormap=\'viridis\') plt.title(\'Average CO2 Emissions and Fuel Consumption by Vehicle Type\') plt.xlabel(\'Vehicle Type\') plt.ylabel(\'Average Value\') plt.legend(loc=\'best\') plt.grid(True) plt.savefig(\'average_emissions_fuel_consumption.png\') # Scatter plot to visualize the relationship between CO2 emissions and fuel consumption plt.figure(figsize=(10, 6)) colors = {\'Car\': \'blue\', \'Truck\': \'green\', \'Bus\': \'red\'} for vehicle_type in df[\'Vehicle_Type\'].unique(): subset = df[df[\'Vehicle_Type\'] == vehicle_type] plt.scatter(subset[\'Fuel_Consumption\'], subset[\'CO2_Emissions\'], c=colors[vehicle_type], label=vehicle_type) plt.title(\'CO2 Emissions vs Fuel Consumption\') plt.xlabel(\'Fuel Consumption (L/100km)\') plt.ylabel(\'CO2 Emissions (g/km)\') plt.legend(title=\'Vehicle Type\') plt.grid(True) plt.savefig(\'scatter_plot_emissions_fuel_consumption.png\') # Box plot to show the distribution of CO2 emissions for each vehicle type plt.figure(figsize=(10, 6)) df.boxplot(column=\'CO2_Emissions\', by=\'Vehicle_Type\', grid=False) plt.title(\'Box Plot of CO2 Emissions by Vehicle Type\') plt.xlabel(\'Vehicle Type\') plt.ylabel(\'CO2 Emissions (g/km)\') plt.suptitle(\'\') # Suppress the automatic \'Boxplot grouped by ...\' title plt.savefig(\'boxplot_emissions_by_vehicle_type.png\') ```","solution":"import pandas as pd import matplotlib.pyplot as plt def load_and_clean_data(file_path): Loads and cleans the dataset by removing rows with missing values. df = pd.read_csv(file_path) return df.dropna() def compute_averages(df): Computes the average CO2 emissions and fuel consumption for each vehicle type. return df.groupby(\'Vehicle_Type\').agg({\'CO2_Emissions\': \'mean\', \'Fuel_Consumption\': \'mean\'}).reset_index() def plot_averages(averages, output_file): Plots the average CO2 emissions and fuel consumption using a bar plot. plt.figure(figsize=(10, 6)) averages.plot(kind=\'bar\', x=\'Vehicle_Type\', y=[\'CO2_Emissions\', \'Fuel_Consumption\'], colormap=\'viridis\', ax=plt.gca()) plt.title(\'Average CO2 Emissions and Fuel Consumption by Vehicle Type\') plt.xlabel(\'Vehicle Type\') plt.ylabel(\'Average Value\') plt.legend(loc=\'best\') plt.grid(True) plt.savefig(output_file) def plot_scatter(df, output_file): Creates a scatter plot to visualize the relationship between CO2 emissions and fuel consumption for each vehicle type. plt.figure(figsize=(10, 6)) colors = {\'Car\': \'blue\', \'Truck\': \'green\', \'Bus\': \'red\'} for vehicle_type in df[\'Vehicle_Type\'].unique(): subset = df[df[\'Vehicle_Type\'] == vehicle_type] plt.scatter(subset[\'Fuel_Consumption\'], subset[\'CO2_Emissions\'], c=colors[vehicle_type], label=vehicle_type) plt.title(\'CO2 Emissions vs Fuel Consumption\') plt.xlabel(\'Fuel Consumption (L/100km)\') plt.ylabel(\'CO2 Emissions (g/km)\') plt.legend(title=\'Vehicle Type\') plt.grid(True) plt.savefig(output_file) def plot_box(df, output_file): Generates a box plot to show the distribution of CO2 emissions for each vehicle type. plt.figure(figsize=(10, 6)) df.boxplot(column=\'CO2_Emissions\', by=\'Vehicle_Type\', grid=False) plt.title(\'Box Plot of CO2 Emissions by Vehicle Type\') plt.xlabel(\'Vehicle Type\') plt.ylabel(\'CO2 Emissions (g/km)\') plt.suptitle(\'\') # Suppress the automatic \'Boxplot grouped by ...\' title plt.savefig(output_file) def main(): # Define file paths input_file = \'emissions.csv\' bar_plot_output = \'average_emissions_fuel_consumption.png\' scatter_plot_output = \'scatter_plot_emissions_fuel_consumption.png\' box_plot_output = \'boxplot_emissions_by_vehicle_type.png\' # Load and clean data df = load_and_clean_data(input_file) # Compute averages averages = compute_averages(df) # Create plots plot_averages(averages, bar_plot_output) plot_scatter(df, scatter_plot_output) plot_box(df, box_plot_output) if __name__ == \\"__main__\\": main()"},{"question":"Task You are given a directory structure that includes multiple files and subdirectories. Your task is to implement a function `find_specific_files` that searches for files matching a specific pattern in a given directory. The patterns you will handle include: - `*.ext`: Files with a specific extension. - `[prefix]*`: Files starting with a specific prefix. - `*[suffix]`: Files ending with a specific suffix. - Custom patterns using combinations of wildcards `*`, `?`, and character ranges `[]`. Additionally, the function will support recursive searching through subdirectories. Function Signature ```python def find_specific_files(directory: str, pattern: str, recursive: bool = False) -> List[str]: Search for files matching the given pattern in the specified directory. Parameters: directory (str): The root directory to start the search from. pattern (str): The pattern to match file names against. recursive (bool): Whether to search recursively through subdirectories. Defaults to False. Returns: List[str]: A list of matching file paths relative to the directory. ``` Input - `directory` (str): The directory where the search starts. - `pattern` (str): The pattern to match file names. Patterns may include \'*\', \'?\', and \'[]\' for wildcard matching. - `recursive` (bool): A boolean indicating if the search should be recursive into subdirectories. Output - A list of matching file paths relative to the `directory`. The list may be empty if no files match the pattern. Constraints - Assume the directory structure fits within a standard file system with common limitations. - File names and directory names do not contain special characters such as spaces. Example Assume a directory structure as follows: ``` /example/ file1.txt file2.log /subfolder/ file3.txt file4.gif ``` 1. If `pattern = \\"*.txt\\"` and `recursive = True`, the function will return: ```python [\\"file1.txt\\", \\"subfolder/file3.txt\\"] ``` 2. If `pattern = \\"*.log\\"` and `recursive = False`, the function will return: ```python [\\"file2.log\\"] ``` 3. If `pattern = \\"file[1-2].*\\"` and `recursive = False`, the function will return: ```python [\\"file1.txt\\", \\"file2.log\\"] ``` You should utilize the `glob` module functions effectively to implement the `find_specific_files` function.","solution":"import os import glob from typing import List def find_specific_files(directory: str, pattern: str, recursive: bool = False) -> List[str]: Search for files matching the given pattern in the specified directory. Parameters: directory (str): The root directory to start the search from. pattern (str): The pattern to match file names against. recursive (bool): Whether to search recursively through subdirectories. Defaults to False. Returns: List[str]: A list of matching file paths relative to the directory. if recursive: # Using glob\'s recursive search with ** full_pattern = os.path.join(directory, \'**\', pattern) return [os.path.relpath(path, directory) for path in glob.glob(full_pattern, recursive=True)] else: # Using glob for current directory only full_pattern = os.path.join(directory, pattern) return [os.path.relpath(path, directory) for path in glob.glob(full_pattern)]"},{"question":"# CSV and Configuration File Manipulation **Objective**: Demonstrate understanding of file reading/writing and configuration file parsing concepts in Python. **Problem Statement**: You are given a configuration file that specifies certain settings for processing a CSV file. Your task is to write a Python script that reads the settings from the configuration file, processes the CSV file according to these settings, and writes the output to a new CSV file. **Configuration File (`settings.ini`)**: The configuration file follows the INI file format and contains the following sections and keys: ``` [settings] input_file = input.csv output_file = output.csv delimiter = , quotechar = \\" [processing] columns_to_keep = name,age,email upper_case = name,email ``` - `[settings]` section: - `input_file`: the path to the input CSV file. - `output_file`: the path to the output CSV file. - `delimiter`: the delimiter used in the CSV file. - `quotechar`: the character used to quote fields in the CSV. - `[processing]` section: - `columns_to_keep`: a comma-separated list of column names to include in the output CSV file. - `upper_case`: a comma-separated list of column names that should have their text converted to upper case in the output CSV file. **Input CSV File (`input.csv`)**: The input CSV file contains user data with columns such as `name`, `age`, `email`, `address`, etc. **Expected Output**: Your script should: 1. Read the configuration settings from `settings.ini`. 2. Read the input CSV file specified by `input_file`. 3. Keep only the columns specified in `columns_to_keep`. 4. Convert the text of the columns specified in `upper_case` to upper case. 5. Write the processed data to the output CSV file specified by `output_file`. **Implementation**: Write a Python function `process_csv(config_file: str) -> None` that takes the path to the configuration file as input and performs the described operations. The function should not return anything. **Constraints**: - The configuration file and input CSV file are assumed to be correctly formatted and exist at the paths provided. - All column names in `columns_to_keep` and `upper_case` are guaranteed to exist in the input CSV file. **Example Usage**: Given the above `settings.ini` and an `input.csv` containing: ``` name,age,email,address John,25,john@example.com,123 Maple St Jane,30,jane@example.com,456 Oak St Doe,22,doe@example.com,789 Pine St ``` The output CSV (`output.csv`) should be: ``` name,age,email JOHN,25,JOHN@EXAMPLE.COM JANE,30,JANE@EXAMPLE.COM DOE,22,DOE@EXAMPLE.COM ``` **Function Signature**: ```python def process_csv(config_file: str) -> None: pass ``` **Note**: Focus on utilizing the `csv` and `configparser` modules effectively to complete this task.","solution":"import csv import configparser def process_csv(config_file: str) -> None: # Read configuration from the config file config = configparser.ConfigParser() config.read(config_file) input_file = config[\'settings\'][\'input_file\'] output_file = config[\'settings\'][\'output_file\'] delimiter = config[\'settings\'][\'delimiter\'] quotechar = config[\'settings\'][\'quotechar\'] columns_to_keep = [col.strip() for col in config[\'processing\'][\'columns_to_keep\'].split(\',\')] upper_case = [col.strip() for col in config[\'processing\'][\'upper_case\'].split(\',\')] # Read input CSV file with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile, delimiter=delimiter, quotechar=quotechar) # Prepare to write the output CSV file with open(output_file, mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=columns_to_keep, delimiter=delimiter, quotechar=quotechar) writer.writeheader() for row in reader: # Keep only the necessary columns new_row = {col: row[col] for col in columns_to_keep} # Convert specified columns to upper case for col in upper_case: if col in new_row: new_row[col] = new_row[col].upper() writer.writerow(new_row)"},{"question":"**Question: Implementing an Asynchronous Task Manager with Timeout Handling** **Objective:** Create an asynchronous task manager that can execute multiple coroutines concurrently. The task manager should use asyncio\'s task scheduling and handle timeout exceptions. The goal is to ensure all tasks complete within a specified time limit, and if they don\'t, they should be canceled. **Function Signature:** ```python import asyncio from typing import List, Any, Coroutine async def async_task_manager(coroutines: List[Coroutine[Any, Any, Any]], timeout: float) -> List[Any]: Runs multiple coroutines concurrently with a given timeout. Args: - coroutines: List of coroutine objects to be run. - timeout: Time limit for all tasks to complete in seconds. Returns: - A list of results from the completed coroutines. If a task is canceled, include \'None\' for that task in the result list. Raises: - asyncio.TimeoutError: If the tasks do not complete within the specified timeout. # Your code here ``` **Input:** - `coroutines`: A list of coroutines to be executed concurrently. - `timeout`: A float representing the maximum time (in seconds) allowed for all tasks to complete. **Output:** - A list where each element corresponds to the result of a coroutine. If a coroutine is canceled due to timeout, the list should include `None` for that coroutine. **Constraints:** - All input coroutines are assumed to be awaitable and valid. - The number of coroutines is between 1 and 100. - The timeout value is a positive float. **Performance Requirements:** - The solution should handle up to 100 coroutines efficiently. - Proper exception handling to manage TimeoutError and ensure cleanup of cancelled tasks. **Examples:** 1. **Simple Case with no Timeout:** ```python async def coro1(): await asyncio.sleep(1) return \\"Task 1 Completed\\" async def coro2(): await asyncio.sleep(2) return \\"Task 2 Completed\\" coros = [coro1(), coro2()] timeout = 3.0 results = await async_task_manager(coros, timeout) assert results == [\\"Task 1 Completed\\", \\"Task 2 Completed\\"] ``` 2. **Timeout Case:** ```python async def coro1(): await asyncio.sleep(1) return \\"Task 1 Completed\\" async def coro2(): await asyncio.sleep(4) return \\"Task 2 Completed\\" coros = [coro1(), coro2()] timeout = 2.5 results = await async_task_manager(coros, timeout) assert results == [\\"Task 1 Completed\\", None] # Task 2 exceeds the timeout and is canceled ``` **Hints:** - Use `asyncio.gather` to run the tasks concurrently. - Use `asyncio.wait_for` to enforce the timeout. - Handle `asyncio.TimeoutError` to manage tasks that exceed the allowed timeout period. **Additional Notes:** - Consider using `asyncio.shield` if needed to prevent some tasks from being canceled. - Ensure that all coroutines start running concurrently and handle the timeout in such a way that incomplete tasks are canceled gracefully.","solution":"import asyncio from typing import List, Any, Coroutine async def async_task_manager(coroutines: List[Coroutine[Any, Any, Any]], timeout: float) -> List[Any]: Runs multiple coroutines concurrently with a given timeout. Args: - coroutines: List of coroutine objects to be run. - timeout: Time limit for all tasks to complete in seconds. Returns: - A list of results from the completed coroutines. If a task is canceled, include \'None\' for that task in the result list. tasks = [asyncio.create_task(coroutine) for coroutine in coroutines] try: results = await asyncio.wait_for(asyncio.gather(*tasks), timeout) except asyncio.TimeoutError: for task in tasks: if not task.done(): task.cancel() # Wait until all tasks are actually cancelled await asyncio.gather(*tasks, return_exceptions=True) results = [] for task in tasks: if task.cancelled(): results.append(None) else: results.append(task.result()) return results"},{"question":"You are tasked with implementing a simple simulation of a print server that manages printing jobs from multiple clients using asyncio queues. Your goal is to design a system that handles print jobs arriving at unpredictable intervals and processes them efficiently using multiple worker coroutines. Requirements: 1. Implement an `asyncio.Queue` to manage the print jobs. 2. Each print job is a tuple `(client_id, pages)`, where `client_id` is an identifier for the client and `pages` is the number of pages to print. 3. The printer processes one page per second. 4. Use multiple worker coroutines to process the print jobs concurrently. 5. Implement client coroutines that generate print jobs and put them into the queue at random intervals. 6. Print statements are provided to observe the processing order and time. Input: You do not need to parse input from the user. Instead, set up constants within your script for: - Number of clients. - Number of print jobs per client. - Number of worker coroutines. Output: Your output should clearly show when each print job starts and completes, for example: ``` worker-1 started printing job from client-2 (5 pages) worker-1 finished printing job from client-2 (5 pages) after 5 seconds worker-2 started printing job from client-1 (2 pages) ``` Constraints: - Maximum number of pages per print job is 10. - Use `await asyncio.sleep(1)` to simulate printing one page. Example Code Template: ```python import asyncio import random async def print_worker(name, queue): while True: client_id, pages = await queue.get() print(f\'{name} started printing job from client-{client_id} ({pages} pages)\') await asyncio.sleep(pages) print(f\'{name} finished printing job from client-{client_id} ({pages} pages) after {pages} seconds\') queue.task_done() async def client(client_id, queue, num_jobs): for _ in range(num_jobs): pages = random.randint(1, 10) await queue.put((client_id, pages)) await asyncio.sleep(random.uniform(0.1, 1.0)) async def main(): num_clients = 3 num_jobs = 5 num_workers = 2 queue = asyncio.Queue() # Start worker tasks workers = [asyncio.create_task(print_worker(f\'worker-{i}\', queue)) for i in range(num_workers)] # Start client tasks clients = [asyncio.create_task(client(i, queue, num_jobs)) for i in range(num_clients)] # Wait for all clients to finish generating jobs await asyncio.gather(*clients) # Wait for all jobs to be processed await queue.join() # Cancel all workers for worker in workers: worker.cancel() await asyncio.gather(*workers, return_exceptions=True) asyncio.run(main()) ``` Complete the code template provided to meet the requirements.","solution":"import asyncio import random async def print_worker(name, queue): while True: client_id, pages = await queue.get() print(f\'{name} started printing job from client-{client_id} ({pages} pages)\') await asyncio.sleep(pages) print(f\'{name} finished printing job from client-{client_id} ({pages} pages) after {pages} seconds\') queue.task_done() async def client(client_id, queue, num_jobs): for _ in range(num_jobs): pages = random.randint(1, 10) await queue.put((client_id, pages)) await asyncio.sleep(random.uniform(0.1, 1.0)) async def main(): num_clients = 3 num_jobs = 5 num_workers = 2 queue = asyncio.Queue() # Start worker tasks workers = [asyncio.create_task(print_worker(f\'worker-{i}\', queue)) for i in range(num_workers)] # Start client tasks clients = [asyncio.create_task(client(i, queue, num_jobs)) for i in range(num_clients)] # Wait for all clients to finish generating jobs await asyncio.gather(*clients) # Wait for all jobs to be processed await queue.join() # Cancel all workers for worker in workers: worker.cancel() await asyncio.gather(*workers, return_exceptions=True) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question: Advanced Joint Plot Customization with Seaborn You are tasked with analyzing the `penguins` dataset using seaborn\'s `jointplot` function and creating a detailed joint plot. Your goal is to visualize the relationship between `bill_length_mm` and `bill_depth_mm` for different penguin species, while applying various customizations and additional layers to the plot. Requirements: 1. Load the `penguins` dataset using `sns.load_dataset(\\"penguins\\")`. 2. Create a joint plot of `bill_length_mm` and `bill_depth_mm`, with different colors for each species using the `hue` parameter. 3. Use `kind=\'kde\'` to visualize bivariate and univariate Kernel Density Estimates (KDEs). 4. Add a rug plot for the marginal axes. 5. Customize the plot by setting: - Marker style to `\\"*\\"` with size `80`. - KDE color to `\'purple\'`. - Rug plot color to `\'green\'` and height to `-.02`. - Height of the joint plot figure to `6` and ratio between marginal and main axes to `2`. Constraints: - You must use the seaborn library for all visualizations. - The plot should be correctly formatted, displaying titles for axes and a legend for species. Input: - None. The `penguins` dataset should be loaded internally within your function. Output: - The function should output a seaborn joint plot meeting the above customizations. Example Code: ```python def create_advanced_jointplot(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the jointplot with the specified parameters g = sns.jointplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", height=6, ratio=2, marker=\\"*\\", color=\\"purple\\" ) # Add rug plot layer g.plot_marginals(sns.rugplot, color=\\"green\\", height=-0.02) # Display the plot plt.show() # Call the function to create the plot create_advanced_jointplot() ```","solution":"def create_advanced_jointplot(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the jointplot with the specified parameters g = sns.jointplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", height=6, ratio=2, marker=\\"*\\", palette=\\"viridis\\" ) # Customize the KDE plots g.plot_joint(sns.kdeplot, color=\\"purple\\", fill=True) # Add rug plot layer g.plot_marginals(sns.rugplot, color=\\"green\\", height=-0.02) # Set axis labels and title g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") g.fig.suptitle(\\"Penguins Bill Length and Depth by Species\\", y=1.02) # Display the plot plt.show()"},{"question":"As a Python developer, you are tasked with creating a Python bytes object that meets specific requirements, manipulating it, and extracting information from it using the Python/C-API functions described in the documentation. Requirements: 1. **Create a bytes object from a string**. 2. **Check if the created object is indeed a bytes object**. 3. **Extract the size of the bytes object**. 4. **Convert the bytes object back to a string and confirm its content**. 5. **Concatenate the bytes object with another bytes object**. 6. **Resize the bytes object to a new size**. # Tasks: 1. **Create a function named `create_bytes`** which: - Takes a single string argument. - Returns a new bytes object. 2. **Create a function named `is_bytes_object`** which: - Takes a single argument. - Returns `True` if it\'s a bytes object, otherwise `False`. 3. **Create a function named `get_bytes_size`** which: - Takes a single bytes object as an argument. - Returns the size of the bytes object. 4. **Create a function named `bytes_to_string`** which: - Takes a single bytes object as an argument. - Returns the corresponding string. 5. **Create a function named `concatenate_bytes`** which: - Takes two bytes objects as arguments. - Returns a new bytes object which is the concatenation of the two. 6. **Create a function named `resize_bytes`** which: - Takes a bytes object and a new size as arguments. - Returns a resized bytes object with the specified new size. # Constraints: - You are required to use the Python/C-API functions as described in the documentation. - Proper error handling should be implemented to manage invalid inputs and operations. - Adequate testing should be conducted to verify correctness for each function. Example: ```python # Example usage binary_data = create_bytes(\\"Hello, World!\\") print(is_bytes_object(binary_data)) # True print(get_bytes_size(binary_data)) # 13 print(bytes_to_string(binary_data)) # \\"Hello, World!\\" new_binary_data = create_bytes(\\", how are you?\\") concatenated_data = concatenate_bytes(binary_data, new_binary_data) print(bytes_to_string(concatenated_data)) # \\"Hello, World!, how are you?\\" resized_data = resize_bytes(concatenated_data, 9) print(bytes_to_string(resized_data)) # \\"Hello, Wo\\" ``` Notes: - Each function should be separately implemented. - Make sure to handle any exceptions gracefully and provide relevant error messages.","solution":"def create_bytes(input_string): Create a bytes object from a string. Args: input_string (str): The string to convert to bytes. Returns: bytes: The resulting bytes object. try: return bytes(input_string, \'utf-8\') except TypeError as e: raise ValueError(f\\"Invalid input string: {e}\\") def is_bytes_object(obj): Check if the provided object is a bytes object. Args: obj: The object to check. Returns: bool: True if the object is bytes, False otherwise. return isinstance(obj, bytes) def get_bytes_size(bytes_obj): Get the size of the given bytes object. Args: bytes_obj (bytes): The bytes object whose size is needed. Returns: int: The size of the bytes object. if not is_bytes_object(bytes_obj): raise TypeError(\\"Argument must be a bytes object.\\") return len(bytes_obj) def bytes_to_string(bytes_obj): Convert a bytes object back to a string. Args: bytes_obj (bytes): The bytes object to convert. Returns: str: The resulting string. if not is_bytes_object(bytes_obj): raise TypeError(\\"Argument must be a bytes object.\\") return bytes_obj.decode(\'utf-8\') def concatenate_bytes(bytes_obj1, bytes_obj2): Concatenate two bytes objects. Args: bytes_obj1 (bytes): The first bytes object. bytes_obj2 (bytes): The second bytes object. Returns: bytes: The concatenated bytes object. if not all(map(is_bytes_object, [bytes_obj1, bytes_obj2])): raise TypeError(\\"Both arguments must be bytes objects.\\") return bytes_obj1 + bytes_obj2 def resize_bytes(bytes_obj, new_size): Resize the bytes object to a new size. Args: bytes_obj (bytes): The bytes object to resize. new_size (int): The new size. Returns: bytes: The resized bytes object. if not is_bytes_object(bytes_obj): raise TypeError(\\"Argument must be a bytes object.\\") if not isinstance(new_size, int) or new_size < 0: raise ValueError(\\"New size must be a non-negative integer.\\") return bytes_obj[:new_size]"},{"question":"# Question: Implement a Matrix Multiplication using CUDA Streams with Synchronization in PyTorch Objective: Write a function `cuda_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor` that performs matrix multiplication using CUDA streams in PyTorch. The function should ensure proper synchronization to avoid data races and undefined behavior. Input: - `A`: A 2D tensor of shape `(m, n)` on the CUDA device. - `B`: A 2D tensor of shape `(n, p)` on the CUDA device. Output: - Returns a tensor `C` of shape `(m, p)` which is the result of matrix multiplication `A * B`. Constraints: 1. Both `A` and `B` are guaranteed to be on the same CUDA device. 2. You must use multiple CUDA streams to perform parts of the computation concurrently. 3. Ensure that all streams are properly synchronized to prevent data races. Details: 1. Split the matrix `A` into multiple parts and perform the multiplication in parallel using CUDA streams. 2. Use synchronization methods to ensure that no stream reads uninitialized data or writes over data being read by another stream. Example: ```python import torch A = torch.randn(4, 5, device=\\"cuda\\") B = torch.randn(5, 3, device=\\"cuda\\") C = cuda_matrix_multiplication(A, B) print(C) # Should print the result of the matrix multiplication A * B ``` Hints: 1. Use `torch.cuda.Stream()` to create CUDA streams. 2. Use `torch.cuda.current_stream().wait_stream(another_stream)` to synchronize streams. 3. Ensure that the output tensor `C` is correctly initialized before performing the parallel computations.","solution":"import torch def cuda_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform matrix multiplication using CUDA streams for parallel computation. Args: - A: a 2D tensor of shape (m, n) on the CUDA device. - B: a 2D tensor of shape (n, p) on the CUDA device. Returns: - C: a 2D tensor of shape (m, p) which is the result of matrix multiplication A * B. assert A.is_cuda and B.is_cuda, \\"Both tensors must be on CUDA device.\\" assert A.shape[1] == B.shape[0], \\"Inner dimensions of A and B must match.\\" m, n = A.shape n, p = B.shape # Step 1: Split A into multiple parts (we\'ll use 2 parts here for simplicity) mid_point = m // 2 A1, A2 = A[:mid_point, :], A[mid_point:, :] # Step 2: Create CUDA streams stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() # Step 3: Initialize the output tensor C = torch.empty((m, p), device=A.device) # Step 4: Perform matrix multiplication in parallel using streams with torch.cuda.stream(stream1): C1 = torch.matmul(A1, B) C[:mid_point, :] = C1 with torch.cuda.stream(stream2): C2 = torch.matmul(A2, B) C[mid_point:, :] = C2 # Step 5: Synchronize streams to ensure all computations are completed torch.cuda.synchronize() return C"},{"question":"# Codec Encoding and Decoding Assessment Objective: You are tasked with implementing a custom codec registration, encoding, and decoding system in Python. You will use the Python codec registry and support functions to create, register, and use a custom encoding and error handling method. Problem Statement: 1. **Custom Encoding Registration**: - Implement a function to register a custom codec that reverses strings as its encoding mechanism and unreverses them as its decoding mechanism. 2. **Encoding and Decoding**: - Implement functions to encode and decode strings using the custom codec. 3. **Error Handling**: - Implement and register a custom error handler that replaces any undecodable sequence with the string \\"<ERROR>\\". Input and Output: 1. **Custom Codec Registration**: - `register_custom_codec() -> None` - This function registers a custom codec named \\"reverse_codec\\" using the codec registry system. 2. **Encode using Custom Codec**: - `custom_encode(input_str: str) -> str` - This function encodes the input string using the \\"reverse_codec\\". - Input: \\"hello\\" - Output: \\"olleh\\" 3. **Decode using Custom Codec**: - `custom_decode(encoded_str: str) -> str` - This function decodes the input string using the \\"reverse_codec\\". - Input: \\"olleh\\" - Output: \\"hello\\" 4. **Custom Error Handler**: - `custom_error_handler(error: Exception) -> tuple` - This function handles any undecodable sequence by replacing it with \\"<ERROR>\\". - The registered name for this error handler should be \\"custom_error\\". Constraints: - The custom codec should handle case-insensitivity in encoding names. - The encoding and decoding functions should raise a `LookupError` if the codec is not registered. Implement the following functions: ```python import codecs def register_custom_codec(): # Register a search function and codec for \\"reverse_codec\\" pass def reverse_encode(input_str: str) -> str: return input_str[::-1] def reverse_decode(input_str: str) -> str: return input_str[::-1] def custom_encode(input_str: str) -> str: # Use PyCodec_Encode with \\"reverse_codec\\" pass def custom_decode(input_str: str) -> str: # Use PyCodec_Decode with \\"reverse_codec\\" pass def custom_error_handler(error: Exception) -> tuple: # Handle undecodable sequences pass # Example Usage: # register_custom_codec() # encoded_str = custom_encode(\\"hello\\") # print(encoded_str) # Output: \\"olleh\\" # decoded_str = custom_decode(encoded_str) # print(decoded_str) # Output: \\"hello\\" ``` Notes: - Do not use built-in reverse string methods except in `reverse_encode` and `reverse_decode` as these serve as the core of custom codec functionality. - Ensure that the custom error handler `custom_error_handler` is registered and is invoked correctly during undecodable sequences.","solution":"import codecs # Custom encoding function def reverse_encode(input_str): return input_str[::-1], len(input_str) # Custom decoding function def reverse_decode(input_str): return input_str[::-1], len(input_str) # Custom error handler function def custom_error_handler(error): return (\\"<ERROR>\\", error.start + 1) # Codec information provider class ReverseCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): return reverse_encode(input) def decode(self, input, errors=\'strict\'): return reverse_decode(input) # Codec search function def reverse_codec_search(name): if name == \\"reverse_codec\\": return codecs.CodecInfo( name=\'reverse_codec\', encode=reverse_encode, decode=reverse_decode ) return None def register_custom_codec(): codecs.register(reverse_codec_search) codecs.register_error(\\"custom_error\\", custom_error_handler) def custom_encode(input_str): try: return codecs.encode(input_str, \\"reverse_codec\\") except LookupError: raise LookupError(\\"The \'reverse_codec\' codec is not registered.\\") def custom_decode(input_str): try: return codecs.decode(input_str, \\"reverse_codec\\") except LookupError: raise LookupError(\\"The \'reverse_codec\' codec is not registered.\\")"},{"question":"# Question: Gaussian Process Regression with Custom Kernels You are tasked with implementing a Gaussian Process Regression model to predict a target variable based on input features. The goal is to: 1. Define a custom kernel by combining basic kernels. 2. Fit the model to training data and optimize kernel hyperparameters. 3. Predict on new data and obtain both the mean and the standard deviation of the predictions. Your solution should follow these steps: 1. **Data Preparation**: - Create a synthetic dataset for training: Input data `X_train` should be 100 points uniformly spaced between 0 and 10. Define the target variable `y_train` as the sin of `X_train` with added Gaussian noise (mean=0, std=0.1). - Create a test dataset `X_test` with 100 points uniformly spaced between 0 and 10. 2. **Kernel Definition**: - Define a custom kernel as the sum of an `RBF` kernel and a `WhiteKernel` for noise. 3. **Model Training**: - Initialize a `GaussianProcessRegressor` with the custom kernel. - Fit the model to the training data `X_train` and `y_train`. 4. **Prediction**: - Predict on `X_test` and obtain both the mean and the standard deviation of the predictions. # Requirements: - **Input**: - `X_train` (numpy array of shape (100,)) - `y_train` (numpy array of shape (100,)) - `X_test` (numpy array of shape (100,)) - **Output**: - `y_mean` (numpy array of shape (100,)): Predicted mean values for `X_test`. - `y_std` (numpy array of shape (100,)): Predicted standard deviations for `X_test`. # Constraints: - Use `sklearn.gaussian_process` package for Gaussian Processes. - Ensure the kernel hyperparameters are optimized during model fitting. - Add appropriate comments in your code to explain the steps. # Performance: - Your implementation should efficiently fit the model and make predictions within a reasonable time frame for the given dataset. # Example: ```python import numpy as np from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, WhiteKernel # Step 1: Data Preparation X_train = np.linspace(0, 10, 100) y_train = np.sin(X_train) + np.random.normal(0, 0.1, 100) X_test = np.linspace(0, 10, 100) # Step 2: Define Custom Kernel kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0) # Step 3: Model Training gpr = GaussianProcessRegressor(kernel=kernel) gpr.fit(X_train[:, np.newaxis], y_train) # Step 4: Prediction y_mean, y_std = gpr.predict(X_test[:, np.newaxis], return_std=True) print(\\"Predicted Means: \\", y_mean) print(\\"Predicted Standard Deviations: \\", y_std) ``` # Instructions: - Implement the solution in a Python function `gaussian_process_regression(X_train, y_train, X_test)` that returns `y_mean` and `y_std`. **Note**: Do not use additional external libraries for GP implementation, strictly use `sklearn.gaussian_process`.","solution":"import numpy as np from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, WhiteKernel def gaussian_process_regression(X_train, y_train, X_test): Perform Gaussian Process Regression with a custom kernel. Parameters: X_train (np.array): Training data points. y_train (np.array): Training data targets. X_test (np.array): Test data points. Returns: np.array: Predicted mean values for X_test. np.array: Predicted standard deviations for X_test. # Step 1: Data Preparation (Handled outside function) # Step 2: Define Custom Kernel kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0) # Step 3: Model Training gpr = GaussianProcessRegressor(kernel=kernel) gpr.fit(X_train[:, np.newaxis], y_train) # Step 4: Prediction y_mean, y_std = gpr.predict(X_test[:, np.newaxis], return_std=True) return y_mean, y_std"},{"question":"Working with Meta Tensors in PyTorch Objective: To assess your understanding of working with meta tensors in PyTorch by creating a neural network model on a meta device, performing transformations, and migrating it to an actual device with proper reinitialization of parameters. Problem Statement: You are required to: 1. Define a simple neural network model using PyTorch\'s `nn.Module`. The model should include at least one linear layer and one activation layer. 2. Instantiate this model on the meta device. 3. Print out the model\'s structure while it resides on the meta device. 4. Shift the model to the CPU while keeping the parameters uninitialized. 5. Explicitly reinitialize the parameters of the model. 6. Print the reinitialized model\'s structure and its parameters. Requirements: - Use the context manager to set the device to \'meta\' while defining the model. - Use `to_empty` method to shift the model to the CPU. - Reinitialize the parameters of the model after transferring it to the CPU. Input: No input is required. Your task is to implement the specified neural network model and perform the transformations outlined. Output: Print the following: 1. Model structure on the meta device. 2. Model structure on the CPU with uninitialized parameters. 3. Model structure on the CPU with reinitialized parameters. Constraints: - The model must include at least one linear layer and one activation layer. - The reinitialization of parameters should be done manually and correctly. Example Solution: ```python import torch import torch.nn as nn # Define the neural network model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 5) self.activation = nn.ReLU() def forward(self, x): x = self.fc(x) x = self.activation(x) return x # Step 1: Instantiate model on the meta device with torch.device(\'meta\'): model_meta = SimpleNet() print(\\"Model on meta device:\\") print(model_meta) # Step 4: Move model to the CPU without initializing parameters model_cpu_empty = model_meta.to_empty(device=\'cpu\') print(\\"Model on CPU with uninitialized parameters:\\") print(model_cpu_empty) # Step 5: Reinitialize the parameters manually def reinitialize_parameters(model): for layer in model.children(): if hasattr(layer, \'reset_parameters\'): layer.reset_parameters() reinitialize_parameters(model_cpu_empty) print(\\"Model on CPU with reinitialized parameters:\\") print(model_cpu_empty) # Verify model parameters for name, param in model_cpu_empty.named_parameters(): print(f\\"{name}: {param}\\") ```","solution":"import torch import torch.nn as nn # Define the neural network model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 5) self.activation = nn.ReLU() def forward(self, x): x = self.fc(x) x = self.activation(x) return x # Step 1: Instantiate model on the meta device with torch.device(\'meta\'): model_meta = SimpleNet() print(\\"Model on meta device:\\") print(model_meta) # Step 4: Move model to the CPU without initializing parameters model_cpu_empty = model_meta.to_empty(device=\'cpu\') print(\\"Model on CPU with uninitialized parameters:\\") print(model_cpu_empty) # Step 5: Reinitialize the parameters manually def reinitialize_parameters(model): for layer in model.children(): if hasattr(layer, \'reset_parameters\'): layer.reset_parameters() reinitialize_parameters(model_cpu_empty) print(\\"Model on CPU with reinitialized parameters:\\") print(model_cpu_empty) # Verify model parameters for name, param in model_cpu_empty.named_parameters(): print(f\\"{name}: {param}\\")"},{"question":"# PyTorch Storage and Tensor Manipulation You are provided with the following requirements for a custom tensor operation in PyTorch: 1. Create a tensor `A` of shape `(5, 5)` initialized with ones (`torch.ones`). 2. Access the underlying storage of `A` and clone it to create `B_storage`. 3. Modify `B_storage` by replacing all its elements with zeros. 4. Create a new tensor `B` using `B_storage` and ensure it has the same shape, stride, and data type as tensor `A`. 5. Verify that modifying tensor `B` does not affect tensor `A`. Function Signature ```python def custom_tensor_operation(): This function should perform the following steps: 1. Create a tensor `A` with shape (5, 5) and initialize it with ones. 2. Access the underlying storage of `A` and clone it to create `B_storage`. 3. Modify `B_storage` to contain zeros. 4. Create a new tensor `B` using `B_storage` while preserving the shape, stride, and dtype of `A`. 5. Verify and ensure that modifying `B` does not affect `A`. Returns: a: The original tensor `A`. b: The new tensor `B` with modified storage. pass ``` # Constraints 1. You must directly access and manipulate the untyped storage. 2. Use appropriate PyTorch functions and methods to clone, modify, and create tensors from storage. 3. Do not use high-level PyTorch tensor manipulation methods like `clone` or `fill_` directly on tensors for storage manipulation. # Performance Requirements Ensure that your solution handles the creation and manipulation efficiently and does not lead to unintended side effects on the original tensor. # Example Calling your function `custom_tensor_operation()` should return: - `A` as a tensor of shape `(5, 5)` filled with ones. - `B` as a tensor of shape `(5, 5)` filled with zeros. The modification of `B` should be confirmed to not affect the values of `A`.","solution":"import torch def custom_tensor_operation(): This function performs the following steps: 1. Create a tensor `A` with shape (5, 5) and initialize it with ones. 2. Access the underlying storage of `A` and clone it to create `B_storage`. 3. Modify `B_storage` to contain zeros. 4. Create a new tensor `B` using `B_storage` while preserving the shape, stride, and dtype of `A`. 5. Verify and ensure that modifying `B` does not affect `A`. Returns: a: The original tensor `A`. b: The new tensor `B` with modified storage. # Step 1: Create tensor `A` A = torch.ones((5, 5)) # Step 2: Access the underlying storage of `A` and clone it B_storage = A.storage().clone() # Step 3: Modify `B_storage` to contain zeros B_storage.fill_(0) # Step 4: Create a new tensor `B` using `B_storage` while preserving the shape, stride, and dtype of `A` B = torch.tensor(B_storage).reshape(5, 5) # Ensure strides and dtype match B = torch.as_strided(B, size=A.size(), stride=A.stride()).type(A.dtype) return A, B"},{"question":"# IP Address Manipulation with `ipaddress` Module **Objective**: The goal of this question is to assess your understanding of the `ipaddress` module in Python, which allows manipulation and analysis of IPv4 and IPv6 addresses and networks. **Task**: You need to implement a function `is_subnet_of(network1: str, network2: str) -> bool` that determines if one network is a subnet of another. **Specifications**: 1. The function should take two arguments: - `network1`: A string representation of the first network (e.g., \'192.168.0.0/24\'). - `network2`: A string representation of the second network (e.g., \'192.168.0.0/16\'). 2. The function should return: - `True` if `network1` is a subnet of `network2`. - `False` otherwise. **Example**: ```python >>> is_subnet_of(\'192.168.1.0/24\', \'192.168.0.0/16\') True >>> is_subnet_of(\'192.168.1.0/24\', \'192.168.0.0/24\') False >>> is_subnet_of(\'2001:db8::/32\', \'2001:db8::/48\') False >>> is_subnet_of(\'2001:db8:abcd::/48\', \'2001:db8::/32\') True ``` **Constraints**: - Ensure that the input strings are valid IPv4 or IPv6 networks. You can use the `ipaddress` module for validation and manipulation. - The function should handle both IPv4 and IPv6 network addresses. - Pay attention to the performance; the solution should be efficient with respect to both time and space complexity. **Additional Information**: - You might find the following classes and methods from the `ipaddress` module useful: - `ipaddress.ip_network()` - The `subnet_of()` method for network objects. **Note**: You are not allowed to use any third-party libraries; only the standard `ipaddress` module is permitted. Happy coding!","solution":"import ipaddress def is_subnet_of(network1: str, network2: str) -> bool: Determine if \'network1\' is a subnet of \'network2\'. Parameters: network1 (str): The potential subnet (e.g. \'192.168.1.0/24\'). network2 (str): The supernetwork that might contain the subnet (e.g. \'192.168.0.0/16\'). Returns: bool: True if \'network1\' is a subnet of \'network2\', otherwise False. net1 = ipaddress.ip_network(network1) net2 = ipaddress.ip_network(network2) return net1.subnet_of(net2)"},{"question":"**Objective:** Demonstrate your understanding of the `xml.sax` API by implementing a custom SAX parser that extracts specific information from an XML document. **Task:** Write a Python function `parse_bookstore(xml_content: str) -> List[Dict[str, str]]` that parses an XML string representing a collection of books. Each book in the XML has properties like `title`, `author`, `genre`, `price`, and `publish_date`. You are to extract this information using SAX and return a list of dictionaries containing the details of each book. **Input:** - `xml_content` (str): A string containing the XML data representing the bookstore. **Output:** - `List[Dict[str, str]]`: A list of dictionaries, each representing a book with the keys `title`, `author`, `genre`, `price`, and `publish_date`. **Constraints:** - You must use the `xml.sax` module for parsing. - Handle any potential parsing errors gracefully by logging them and continuing. **Example:** Given the following XML content: ```xml <bookstore> <book> <title>Everyday Italian</title> <author>Giada De Laurentiis</author> <genre>Cooking</genre> <price>30.00</price> <publish_date>2005-11-12</publish_date> </book> <book> <title>Harry Potter</title> <author>J K. Rowling</author> <genre>Fantasy</genre> <price>29.99</price> <publish_date>2000-07-08</publish_date> </book> </bookstore> ``` Your function should return the following list of dictionaries: ```python [ { \\"title\\": \\"Everyday Italian\\", \\"author\\": \\"Giada De Laurentiis\\", \\"genre\\": \\"Cooking\\", \\"price\\": \\"30.00\\", \\"publish_date\\": \\"2005-11-12\\" }, { \\"title\\": \\"Harry Potter\\", \\"author\\": \\"J K. Rowling\\", \\"genre\\": \\"Fantasy\\", \\"price\\": \\"29.99\\", \\"publish_date\\": \\"2000-07-08\\" } ] ``` **Starter Code:** ```python import xml.sax from typing import List, Dict class BookHandler(xml.sax.ContentHandler): def __init__(self): self.books = [] self.current_data = \\"\\" self.book = {} def startElement(self, tag, attributes): self.current_data = tag if tag == \\"book\\": self.book = {} def endElement(self, tag): if tag == \\"book\\": self.books.append(self.book) self.current_data = \\"\\" def characters(self, content): if self.current_data: self.book[self.current_data] = content def parse_bookstore(xml_content: str) -> List[Dict[str, str]]: handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) try: xml.sax.parseString(xml_content, handler) except xml.sax.SAXParseException as e: print(f\\"Parsing error: {e}\\") return handler.books # Example usage xml_data = <bookstore> <book> <title>Everyday Italian</title> <author>Giada De Laurentiis</author> <genre>Cooking</genre> <price>30.00</price> <publish_date>2005-11-12</publish_date> </book> <book> <title>Harry Potter</title> <author>J K. Rowling</author> <genre>Fantasy</genre> <price>29.99</price> <publish_date>2000-07-08</publish_date> </book> </bookstore> print(parse_bookstore(xml_data)) # Should print the list of books as dictionaries ``` **Note:** Ensure that your solution correctly handles parsing errors and returns the expected result for various input scenarios.","solution":"import xml.sax from typing import List, Dict class BookHandler(xml.sax.ContentHandler): def __init__(self): self.books = [] self.current_data = \\"\\" self.book = {} def startElement(self, tag, attributes): self.current_data = tag if tag == \\"book\\": self.book = {} def endElement(self, tag): if tag == \\"book\\": self.books.append(self.book) self.current_data = \\"\\" def characters(self, content): if self.current_data: if self.current_data in [\\"title\\", \\"author\\", \\"genre\\", \\"price\\", \\"publish_date\\"]: self.book[self.current_data] = content def parse_bookstore(xml_content: str) -> List[Dict[str, str]]: handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) try: xml.sax.parseString(xml_content, handler) except xml.sax.SAXParseException as e: print(f\\"Parsing error: {e}\\") return handler.books"},{"question":"# Seaborn Data Transformation and Visualization Background Seaborn is a Python visualization library built on top of matplotlib. It allows for the creation of informative and attractive statistical graphics. This assessment will test your ability to preprocess a dataset and generate visualizations using Seaborn and its object-oriented interface. Task You are given the same brain networks dataset as illustrated in the documentation. Your task involves two parts: 1. **Data preprocessing:** Load the dataset and transform it using various pandas operations. 2. **Data visualization:** Create a pair plot with specific requirements. Data Preprocessing Load the brain networks dataset using the following code: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) ``` Data Visualization Using the Seaborn object interface, create a pair plot with the following specifications: - Select the x-axis variables to be `[\\"5\\", \\"8\\", \\"12\\", \\"15\\"]`. - Select the y-axis variables to be `[\\"6\\", \\"13\\", \\"16\\"]`. - Customize the layout size to be `(10, 6)`. - Ensure both x and y axes are shared across all plots. - Add paths to the plot with the following properties: - Linewidth of the paths should be set to 1. - Alpha (transparency) level should be set to 0.8. - Use different colors for different `hemi` values to distinguish them in the visualization. Implementation Write Python code to: 1. Complete the data preprocessing as described. 2. Generate the visualization with the specified characteristics. Example Output Your pair plot should look similar to this: ```python import seaborn.objects as so p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(10, 6)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") ``` Constraints - Ensure all preprocessing is done using pandas operations. - Use the Seaborn object interface for the visualization. You will be evaluated on the correctness of the data preprocessing and the visual changes as per the requirements. Submission - Submit a Python script or a Jupyter Notebook with your implementation. - Include any necessary comments to explain your code. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset # Load dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Generate the visualization p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(10, 6)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\")"},{"question":"You have been tasked with creating a small utility in Python that demonstrates your understanding of Unix-specific modules. Specifically, you need to implement a function that will monitor the resource usage of a specific user on a Unix-based system. Your implementation should use the `pwd`, `resource`, and `syslog` modules. # Function Signature ```python def monitor_user_resources(username: str) -> dict: ``` # Description 1. **Input:** * `username` (str): The username of the user whose resource usage you want to monitor. 2. **Output:** * Returns a dictionary containing the following keys: * `\'user_id\'`: The user ID of the specified username. * `\'cpu_time_used\'`: The CPU time used by the user\'s processes. * `\'max_resident_set_size\'`: The maximum resident set size used. * `\'shared_memory_size\'`: The size of shared memory segments. * `\'syslog_tag\'`: A generated syslog tag for logging purposes. # Constraints 1. Assume the user with the specified `username` exists on the system. 2. This function must be run with sufficient privileges to read this information. 3. Use the `syslog` module to generate a log entry with a unique tag that you create and include in the output dictionary. # Example ```python resources = monitor_user_resources(\\"someuser\\") print(resources) # Output might look similar to: # { # \'user_id\': 1001, # \'cpu_time_used\': 123.456, # \'max_resident_set_size\': 98765, # \'shared_memory_size\': 54321, # \'syslog_tag\': \'resource_monitor_20231010_123456\' # } ``` # Notes * Ensure your function cleanly handles any potential exceptions, especially when dealing with system calls and resource usage data. * The syslog tag should be generated based on the current date and time to ensure uniqueness.","solution":"import pwd import resource import syslog from datetime import datetime def monitor_user_resources(username: str) -> dict: Monitors the resource usage of the specified user. Parameters: username (str): The username of the user whose resource usage is to be monitored. Returns: dict: A dictionary containing resource usage information. # Retrieve user id from username try: user_info = pwd.getpwnam(username) user_id = user_info.pw_uid except KeyError: raise ValueError(f\\"User \'{username}\' does not exist\\") # Get the resource usage for the current process (this is a limitation since # getting per user resource usage would require a much more complex setup and permissions) usage = resource.getrusage(resource.RUSAGE_SELF) # Generate a unique syslog tag current_time = datetime.now().strftime(\'%Y%m%d_%H%M%S\') syslog_tag = f\'resource_monitor_{current_time}\' # Log to syslog (This would normally require root privileges) syslog.syslog(syslog_tag) return { \'user_id\': user_id, \'cpu_time_used\': usage.ru_utime + usage.ru_stime, \'max_resident_set_size\': usage.ru_maxrss, \'shared_memory_size\': usage.ru_ixrss, \'syslog_tag\': syslog_tag }"},{"question":"**Problem Statement: Advanced Log Analyzer Using Regular Expressions** In this coding assessment, your task is to design and implement a function that analyzes a web server log file to extract useful information using regular expressions. This will test your understanding of the `re` module functionality and your ability to utilize it effectively to process text data. # Objective Create a function `analyze_server_logs(log_file_path)` that performs the following: 1. **Extract IP Addresses:** Find all unique IP addresses that made requests to the server. 2. **Count Requests Per Endpoint:** Count how many requests were made to each endpoint (e.g., `/home`, `/about-us`). 3. **Identify Timeframe with Most Requests:** Determine the 1-hour period with the highest number of requests. # Input and Output - **Input:** - `log_file_path` (str): The file path to the web server log file. - **Output:** - A dictionary containing: - `unique_ips` (set): A set of unique IP addresses. - `requests_per_endpoint` (dict): A dictionary where keys are endpoint paths, and values are the count of requests. - `busiest_hour` (str): The starting timestamp of the 1-hour period with the most requests, in the format `YYYY-MM-DD HH:MM:SS`. # Constraints The log file follows a standard log format where each line is structured as: ``` <IP Address> - - [<timestamp>] \\"GET <endpoint> HTTP/1.1\\" <status> <size> ``` Example entry: ``` 192.168.1.1 - - [2023-10-01 12:45:32] \\"GET /home HTTP/1.1\\" 200 1024 ``` # Requirements 1. Use the `re` module to perform pattern matching and extraction. 2. Handle large files efficiently to ensure performance. 3. Ensure your solution accurately handles varied log formats matching the given pattern. 4. Pay attention to edge cases, such as overlapping time periods for the busiest hour. # Example Usage ```python def analyze_server_logs(log_file_path): # Your implementation here pass # Example log file log_file_path = \'web_server.log\' # Calling the function log_analysis = analyze_server_logs(log_file_path) # Example Output: # { # \'unique_ips\': {\'192.168.1.1\', \'172.16.0.1\'}, # \'requests_per_endpoint\': {\'/home\': 25, \'/about-us\': 13}, # \'busiest_hour\': \'2023-10-01 12:00:00\' # } ``` Note: Ensure that your implementation is well-documented with comments explaining the logic and each step\'s purpose. # Tips - Use `re.compile` to compile your regular expressions for better performance. - Consider using `re.finditer` for iterating over matches in large files. - Utilize Pythons `datetime` module to handle date and time manipulations for finding the busiest hour.","solution":"import re from collections import defaultdict, Counter from datetime import datetime, timedelta def analyze_server_logs(log_file_path): with open(log_file_path, \'r\') as file: log_data = file.readlines() ip_pattern = re.compile(r\'(d{1,3}.){3}d{1,3}\') endpoint_pattern = re.compile(r\'GETs+(S+)s+HTTP\') timestamp_pattern = re.compile(r\'[(d+-d+-d+s+d+:d+:d+)]\') unique_ips = set() requests_per_endpoint = defaultdict(int) request_times = [] for line in log_data: ip_match = ip_pattern.search(line) if ip_match: unique_ips.add(ip_match.group()) endpoint_match = endpoint_pattern.search(line) if endpoint_match: endpoint = endpoint_match.group(1) requests_per_endpoint[endpoint] += 1 timestamp_match = timestamp_pattern.search(line) if timestamp_match: timestamp = datetime.strptime(timestamp_match.group(1), \\"%Y-%m-%d %H:%M:%S\\") request_times.append(timestamp) if not request_times: busiest_hour = None else: counter = Counter() for timestamp in request_times: hour_start = timestamp.replace(minute=0, second=0, microsecond=0) counter[hour_start] += 1 busiest_hour = counter.most_common(1)[0][0].strftime(\\"%Y-%m-%d %H:%M:%S\\") return { \'unique_ips\': unique_ips, \'requests_per_endpoint\': dict(requests_per_endpoint), \'busiest_hour\': busiest_hour }"},{"question":"# Custom Mutable Mapping Implementation Problem Statement Your task is to design and implement a custom dictionary-like data structure that adheres to the `collections.abc.MutableMapping` abstract base class. This custom mapping should provide methods for setting, getting, deleting, and iterating over key-value pairs, as well as additional functionality to track the insertion order of keys. The custom mapping must satisfy the following constraints: 1. Inherit from the `collections.abc.MutableMapping` abstract base class. 2. Implement all required abstract methods (`__getitem__`, `__setitem__`, `__delitem__`, `__iter__`, and `__len__`). 3. Maintain the insertion order of keys. 4. Provide efficient implementations for all operations, aiming for average-case O(1) time complexity for insertions, deletions, and lookups. Implementation Requirements 1. **Class Definition**: - Define a class `OrderedMutableMapping` that inherits from `collections.abc.MutableMapping`. 2. **Methods**: - Implement the abstract methods: - `__getitem__(self, key)`: Retrieve the value associated with `key`. - `__setitem__(self, key, value)`: Set the `value` for the given `key`. - `__delitem__(self, key)`: Remove the item associated with `key`. - `__iter__(self)`: Return an iterator over the keys of the mapping. - `__len__(self)`: Return the number of items in the mapping. 3. **Additional Functionality**: - Maintain the insertion order of keys using an attribute `self._order` that keeps track of the order of keys as a list. - Ensure the custom mapping reflects insertion order during iteration. 4. **Performance**: - Implement the methods to achieve average-case O(1) time complexity for insertions, deletions, and lookups. Example Usage ```python # Create an instance of the custom mapping mapping = OrderedMutableMapping() # Add some key-value pairs mapping[\'a\'] = 1 mapping[\'b\'] = 2 mapping[\'c\'] = 3 # Retrieve a value print(mapping[\'b\']) # Output: 2 # Iterate over the mapping for key in mapping: print(f\\"{key}: {mapping[key]}\\") # Output: a: 1, b: 2, c: 3 # Check the length of the mapping print(len(mapping)) # Output: 3 # Delete a key-value pair del mapping[\'b\'] # Check the length again print(len(mapping)) # Output: 2 # Verify the order of keys is maintained for key in mapping: print(f\\"{key}: {mapping[key]}\\") # Output: a: 1, c: 3 ``` Constraints 1. Do not use any other collections or libraries that provide similar functionality (e.g., `OrderedDict` from the `collections` module). 2. Focus on achieving optimal performance while adhering to the constraints. Submit your implementation as a Python class definition and ensure your class passes the usage example provided.","solution":"from collections.abc import MutableMapping class OrderedMutableMapping(MutableMapping): def __init__(self): self._store = {} self._order = [] def __getitem__(self, key): if key in self._store: return self._store[key] raise KeyError(key) def __setitem__(self, key, value): if key not in self._store: self._order.append(key) self._store[key] = value def __delitem__(self, key): if key in self._store: del self._store[key] self._order.remove(key) else: raise KeyError(key) def __iter__(self): return iter(self._order) def __len__(self): return len(self._store)"},{"question":"# Context Management with `ExitStack` **Problem Statement:** You are required to write a function `manage_resources(file_paths: List[str], special_resource_required: bool) -> List[str]` that accepts: 1. A list of file paths (`file_paths: List[str]`) that need to be opened and read. 2. A boolean flag (`special_resource_required: bool`) indicating whether an additional special resource should be acquired and used. The function should: 1. Use `contextlib.ExitStack` to handle all the file resources safely. 2. If `special_resource_required` is `True`, acquire a special resource by calling the function `acquire_special_resource()` and release it using `release_special_resource()`. 3. Read all the input files\' contents and return a list containing the contents of all the files. 4. Ensure that all the resources (files and the special resource if acquired) are properly cleaned up in case of any errors during the process. ```python import contextlib # Provided functions def acquire_special_resource(): This function simulates the acquisition of a special resource. print(\\"Special resource acquired.\\") return \\"SpecialResource\\" def release_special_resource(resource): This function simulates the release of a special resource. print(\\"Special resource released.\\") def manage_resources(file_paths, special_resource_required): Function to manage multiple file resources and optionally a special resource. :param file_paths: List of paths to the files that need to be read. :param special_resource_required: Boolean flag to indicate if the special resource is required. :return: List of contents read from all the files. contents = [] with contextlib.ExitStack() as stack: # Register exit callbacks for reading files and managing special resource files = [stack.enter_context(open(path)) for path in file_paths] if special_resource_required: special_resource = acquire_special_resource() stack.callback(release_special_resource, special_resource) # Perform the file reading operation for file in files: contents.append(file.read()) return contents # Example usage: # Assuming \'file1.txt\' and \'file2.txt\' are valid paths to your input files. # file_contents = manage_resources([\'file1.txt\', \'file2.txt\'], special_resource_required=True) # print(file_contents) ``` **Constraints:** - The `file_paths` list will contain valid paths of existing readable text files. - Handle any exceptions that might occur during file operations. - The special resource is only needed if `special_resource_required` is `True`. **Performance Requirements:** - The solution should handle opening and reading a large number of files efficiently. **Expected Input and Output Formats:** - **Input:** - `file_paths: List[str]`: A list of strings representing file paths. - `special_resource_required: bool`: A boolean indicating if the special resource should be utilized. - **Output:** - The function should return a list of strings with each string being the contents of the files read.","solution":"import contextlib # Provided functions def acquire_special_resource(): This function simulates the acquisition of a special resource. print(\\"Special resource acquired.\\") return \\"SpecialResource\\" def release_special_resource(resource): This function simulates the release of a special resource. print(f\\"Special resource released: {resource}\\") def manage_resources(file_paths, special_resource_required): Function to manage multiple file resources and optionally a special resource. :param file_paths: List of paths to the files that need to be read. :param special_resource_required: Boolean flag to indicate if the special resource is required. :return: List of contents read from all the files. contents = [] with contextlib.ExitStack() as stack: # Register exit callbacks for reading files and managing special resource files = [stack.enter_context(open(path)) for path in file_paths] if special_resource_required: special_resource = acquire_special_resource() stack.callback(release_special_resource, special_resource) # Perform the file reading operation for file in files: contents.append(file.read()) return contents"},{"question":"# Turtle Graphics: Draw a Pattern **Objective:** Design and implement a Python function using the `turtle` module to draw a custom geometric pattern composed of repeated shapes. The pattern should be interactive, allowing the user to specify certain parameters. **Task:** Write a Python function `draw_pattern(num_shapes, shape_size, screen_color, draw_speed)` that uses the `turtle` module to draw a pattern consisting of multiple shapes arranged in a circular layout. The pattern is defined by the following parameters: - `num_shapes` (integer): The number of shapes to draw in the pattern. - `shape_size` (integer): The size of each individual shape. - `screen_color` (string): The background color of the turtle screen (e.g., \\"white\\", \\"black\\", \\"#RRGGBB\\"). - `draw_speed` (integer): The drawing speed of the turtle (1 = slowest, 10 = fastest). Your function should meet the following requirements: 1. The screen background must be set to `screen_color`. 2. The pattern must consist of `num_shapes` shapes arranged in a circular layout. 3. Each shape must have a side length or diameter (depending on the type of shape) of `shape_size`. 4. The turtle should draw at the speed specified by `draw_speed`. 5. After finishing the drawing, the turtle should wait for a user click to close the window (`turtle.done()` or equivalent). # Example If `draw_pattern(12, 50, \'lightblue\', 5)` is called, the function should create a pattern with 12 shapes of size 50 on a light blue background with a moderate drawing speed. # Implementation Notes - Use `turtle.Screen()` and `turtle.Turtle()` to create and manipulate the screen and turtle objects. - Utilize loops for repetitive drawing tasks. - Properly configure the turtle speed using the `turtle.speed()` method. - Clear the screen at the start with the specified background color using `turtle.bgcolor()`. - Arrange the shapes in a circular pattern by using trigonometric functions (sine and cosine) to calculate positions. - Ensure the pattern is centered on the screen. # Constraints - `num_shapes` should be a positive integer (1  num_shapes  100). - `shape_size` should be a positive integer (10  shape_size  200). - `screen_color` should be a valid color string. - `draw_speed` should be an integer within the range (1  draw_speed  10). **Provide the implementation of the function `draw_pattern` and a short explanation of how it accomplishes the drawing task.**","solution":"import turtle import math def draw_pattern(num_shapes, shape_size, screen_color, draw_speed): Draws a pattern of shapes arranged in a circle using the turtle graphics module. Parameters: num_shapes (int): The number of shapes to draw. shape_size (int): The size of each individual shape. screen_color (str): The background color of the screen. draw_speed (int): The speed of drawing (1 = slowest, 10 = fastest). # Setup the turtle screen screen = turtle.Screen() screen.bgcolor(screen_color) # Setup the turtle artist = turtle.Turtle() artist.speed(draw_speed) angle_between_shapes = 360 / num_shapes radius = shape_size * (num_shapes / (2 * math.pi)) for i in range(num_shapes): artist.penup() # Calculate angle and move to starting position of shape angle = angle_between_shapes * i x = radius * math.cos(math.radians(angle)) y = radius * math.sin(math.radians(angle)) artist.goto(x, y) artist.pendown() # Draw a shape (e.g., circle) artist.circle(shape_size / 2) # Go back to center artist.penup() artist.goto(0, 0) # Hide turtle and complete the drawing artist.hideturtle() screen.exitonclick()"},{"question":"# Password Hashing and Verification using `hashlib` You are required to write two functions to handle password hashing and verification using the `hashlib` module as a replacement for the deprecated `crypt` module. You will implement the following functions: 1. **hash_password(plain_text_password: str, salt: str = None) -> str:** - **Input**: - `plain_text_password`: A string representing the plain-text password to hash. - `salt` (optional): A string representing the salt to use for hashing. If not provided, generate a random 16-character salt. - **Output**: - A string representing the hashed password. The hash should include the salt so that it can be used for verification. - **Constraints**: - The hashing should use the SHA-512 algorithm to ensure strong hashing. - The salt should be included in the hash in a way that it can be extracted later for verification. - **Example**: ```python hashed_password = hash_password(\\"mysecret\\") ``` 2. **verify_password(plain_text_password: str, hashed_password: str) -> bool:** - **Input**: - `plain_text_password`: A string representing the plain-text password to verify. - `hashed_password`: A string representing the previously hashed password, which includes the salt used. - **Output**: - A boolean value: `True` if the plain-text password matches the hashed password, `False` otherwise. - **Example**: ```python is_valid = verify_password(\\"mysecret\\", hashed_password) ``` # Detailed Requirements: - The salt should be included in the first 16 characters of the hashed password, and should be extracted for verifying the password. - The `hash_password` function should create a SHA-512 hash of the `plain_text_password` combined with the salt. - The `verify_password` function should use the salt from the hashed password to hash the given `plain_text_password` for comparison. - You may use any technique for generating a random salt string. Ensure that each character of the salt is from the set \\"[./a-zA-Z0-9]\\". # Example Usage: ```python hashed_password = hash_password(\\"mysecret\\") print(hashed_password) # Output: A hashed password string including the salt. is_valid = verify_password(\\"mysecret\\", hashed_password) print(is_valid) # Output: True is_valid = verify_password(\\"wrongpassword\\", hashed_password) print(is_valid) # Output: False ``` # Notes: - Ensure proper code structure and error handling. - Pay attention to the function signatures and correct usage of the Salt and SHA-512 hashing with `hashlib`.","solution":"import hashlib import os def hash_password(plain_text_password: str, salt: str = None) -> str: if salt is None: salt = \'\'.join(chr(os.urandom(1)[0] % 64 + 48) for _ in range(16)) salted_password = salt + plain_text_password hashed = hashlib.sha512(salted_password.encode(\'utf-8\')).hexdigest() return salt + hashed def verify_password(plain_text_password: str, hashed_password: str) -> bool: salt = hashed_password[:16] expected_hash = hashed_password[16:] test_hash = hashlib.sha512((salt + plain_text_password).encode(\'utf-8\')).hexdigest() return test_hash == expected_hash"},{"question":"**Objective:** Demonstrate your understanding of the seaborn library, specifically focusing on creating and customizing swarm plots and multifaceted plots. **Problem Statement:** You are given a dataset containing information about different types of bills in a restaurant. Your task is to create and customize some plots using the seaborn library based on the following specifications: 1. **Basic Swarm Plot:** Create a swarm plot that shows the distribution of `total_bill` amounts. 2. **Categorical Swarm Plot:** Create a swarm plot that splits the data on the `day` of the week, to show how `total_bill` amounts vary across different days. 3. **Hue Customization:** Create a swarm plot similar to the one in step 2, but this time use the `hue` parameter to further differentiate the data based on the `sex` of the customers. 4. **Dodge Effect:** Create a swarm plot like in step 3, but separate the different `sex` categories into different swarms using the `dodge` parameter. 5. **Advanced Customization:** Create a swarm plot of `total_bill` vs `size` of the group. Ensure that the `size` represents the actual size of the groups on the y-axis by using `native_scale=True`. Additionally, reduce the size of the points to avoid overlap. 6. **Faceted Plot:** Create a faceted plot using `catplot` that shows the `total_bill` amounts vs `time` of day (`lunch` or `dinner`), separated into different columns based on the `day` of the week. Use different colors for different `sex` categories. Ensure that the aspect ratio of each facet is set to 0.6. **Constraints and Requirements:** - You should use the `tips` dataset available through seaborn\'s `load_dataset` function. - Each function should output the corresponding seaborn plot. - Make sure to include appropriate labels and titles for each plot. - Organize your code into functions where applicable to maintain clarity. **Input:** - No specific input is needed as you will use the seaborn `tips` dataset. **Output:** - Display the resulting plots as specified. **Example Code Structure:** ```python import seaborn as sns def load_data(): return sns.load_dataset(\\"tips\\") def basic_swarm_plot(data): # Your code here def categorical_swarm_plot(data): # Your code here def hue_customized_swarm_plot(data): # Your code here def dodge_effect_swarm_plot(data): # Your code here def advanced_customized_swarm_plot(data): # Your code here def faceted_plot(data): # Your code here if __name__ == \\"__main__\\": tips_data = load_data() basic_swarm_plot(tips_data) categorical_swarm_plot(tips_data) hue_customized_swarm_plot(tips_data) dodge_effect_swarm_plot(tips_data) advanced_customized_swarm_plot(tips_data) faceted_plot(tips_data) ``` Ensure you test each function to verify that the plots are displayed correctly as per the specifications given.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_data(): return sns.load_dataset(\\"tips\\") def basic_swarm_plot(data): plt.figure(figsize=(8, 6)) sns.swarmplot(x=data[\\"total_bill\\"]) plt.title(\'Basic Swarm Plot of Total Bill Amounts\') plt.xlabel(\'Total Bill\') plt.show() def categorical_swarm_plot(data): plt.figure(figsize=(8, 6)) sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", data=data) plt.title(\'Total Bill Amounts by Day\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() def hue_customized_swarm_plot(data): plt.figure(figsize=(8, 6)) sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=data) plt.title(\'Total Bill Amounts by Day and Sex\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() def dodge_effect_swarm_plot(data): plt.figure(figsize=(8, 6)) sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=data, dodge=True) plt.title(\'Total Bill Amounts by Day and Sex (Dodge Effect)\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() def advanced_customized_swarm_plot(data): plt.figure(figsize=(8, 6)) sns.swarmplot(x=\\"size\\", y=\\"total_bill\\", data=data, native_scale=True, size=5) plt.title(\'Total Bill Amounts by Size of Group\') plt.xlabel(\'Size of Group\') plt.ylabel(\'Total Bill\') plt.show() def faceted_plot(data): sns.catplot(x=\\"total_bill\\", y=\\"time\\", hue=\\"sex\\", col=\\"day\\", data=data, kind=\\"swarm\\", aspect=0.6) plt.subplots_adjust(top=0.9) plt.suptitle(\'Total Bill Amounts by Time and Day, Faceted by Sex\', size=16) plt.show() if __name__ == \\"__main__\\": tips_data = load_data() basic_swarm_plot(tips_data) categorical_swarm_plot(tips_data) hue_customized_swarm_plot(tips_data) dodge_effect_swarm_plot(tips_data) advanced_customized_swarm_plot(tips_data) faceted_plot(tips_data)"},{"question":"The `imp` module has been deprecated since Python 3.4, and its functionalities have been replaced by the `importlib` module. As an exercise in modernizing code and improving your understanding of Python\'s import mechanisms, you are required to replace a function using the `imp` module with its `importlib` counterpart. Here is a function that emulates the standard import statement and uses the `imp` module to find and load a module: ```python import imp import sys def custom_import(name): # Fast path: see if the module has already been imported. try: return sys.modules[name] except KeyError: pass # If any of the following calls raises an exception, # there\'s a problem we can\'t handle -- let the caller handle it. fp, pathname, description = imp.find_module(name) try: return imp.load_module(name, fp, pathname, description) finally: # Since we may exit via an exception, close fp explicitly. if fp: fp.close() ``` **Task:** Rewrite the `custom_import` function to use the `importlib` module instead of the deprecated `imp` module. Ensure that the new function preserves the logic and functionality of the given code. **Input:** - A string `name` representing the name of the module to be imported. **Output:** - The imported module. You may refer to the `importlib` documentation to find suitable replacements for the `imp` functions used. # Constraints: - You must use the `importlib` module. - Do not use any deprecated functionality from the `imp` module. # Example Usage: ```python module = custom_import(\'os\') print(module.__name__) # Output: \'os\' ```","solution":"import importlib import sys def custom_import(name): Imports a module by name using the importlib module. Args: name (str): The name of the module to import. Returns: module: The imported module. # Fast path: see if the module has already been imported. if name in sys.modules: return sys.modules[name] # Import the module using importlib return importlib.import_module(name)"},{"question":"Objective: Use the provided Python debugging and profiling tools to identify and optimize a performance bottleneck in a given Python program. You will demonstrate your skill in using these tools by providing not only the optimized code but also an explanation of how you identified the bottleneck and your optimization strategy. Task: 1. Given the Python program below, identify the portion of the code that is causing performance issues using any of the profiling tools (`cProfile`, `profile`, or `timeit`). 2. Optimize the identified portion of the code. 3. Provide a brief explanation of how you identified the bottleneck and the changes you made to optimize it. Program: ```python import time def slow_function(): print(\\"Starting slow function\\") time.sleep(2) print(\\"Slow function completed\\") def another_slow_function(): print(\\"Starting another slow function\\") result = 0 for i in range(10000): for j in range(10000): result += i * j print(\\"Another slow function completed, result:\\", result) def main(): start_time = time.time() slow_function() another_slow_function() end_time = time.time() print(\\"Total execution time:\\", end_time - start_time) if __name__ == \'__main__\': main() ``` Requirements: - Use a profiling tool (such as `cProfile`, `profile`, or `timeit`) to identify the slowest parts of the program. - Rewrite the code to improve the performance. - Your final submission should include the following: 1. The original and optimized Python code. 2. The profiling output that shows the bottleneck in the original code. 3. A summary explaining: - The methods/tools you used for profiling and how you identified the bottleneck. - The changes you made to optimize the code. - The performance improvement measured from your optimization. Constraints: - You must use Python 3.10 or later. - Do not use multiprocessing or multithreading; focus on optimizing the existing code logic. Sample Output: ``` Original Total execution time: X seconds Optimized Total execution time: Y seconds ``` Submission: Submit your Python script containing the original and optimized code, along with the profiling output and summary explanation.","solution":"import time import cProfile def slow_function(): print(\\"Starting slow function\\") time.sleep(2) print(\\"Slow function completed\\") def another_slow_function(): print(\\"Starting another slow function\\") result = 0 for i in range(10000): for j in range(10000): result += i * j print(\\"Another slow function completed, result:\\", result) def optimized_another_slow_function(): print(\\"Starting optimized another slow function\\") result = (9999 * 10000 // 2) ** 2 print(\\"Optimized another slow function completed, result:\\", result) def main(): start_time = time.time() slow_function() another_slow_function() end_time = time.time() print(\\"Original total execution time:\\", end_time - start_time) def optimized_main(): start_time = time.time() slow_function() optimized_another_slow_function() end_time = time.time() print(\\"Optimized total execution time:\\", end_time - start_time) if __name__ == \'__main__\': print(\\"Profiling original code:\\") cProfile.run(\'main()\') print(\\"nOptimized code execution time:\\") optimized_main()"},{"question":"**Objective**: Implement and test your skills in merging and concatenating pandas DataFrames. Problem Statement You are given three data sets that represent information about employees in different departments of a company. Each data set contains a subset of the complete information available for each employee. Your goal is to combine these data sets into a single DataFrame and then extract meaningful insights from the combined data. # Data Sets Description 1. **Employees Basic Info (emp_basic):** ```python emp_basic = pd.DataFrame({ \'employee_id\': [101, 102, 103, 104], \'name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'age\': [25, 30, 35, 40] }) ``` 2. **Employees Salary Info (emp_salary):** ```python emp_salary = pd.DataFrame({ \'employee_id\': [102, 103, 104, 105], \'salary\': [70000, 80000, 75000, 90000], \'bonus\': [5000, 6000, 5500, 7000] }) ``` 3. **Employees Department Info (emp_dept):** ```python emp_dept = pd.DataFrame({ \'employee_id\': [101, 102, 104, 106], \'department\': [\'HR\', \'Engineering\', \'Finance\', \'Marketing\'], \'location\': [\'New York\', \'San Francisco\', \'Boston\', \'Chicago\'] }) ``` # Task 1. **Combine the Data Sets:** - Merge the three DataFrames to create a single DataFrame `df_combined`. Consider all possible join operations and choose the most appropriate one for this task. 2. **Derived Insights:** - Find employees who are present in `emp_basic` and `emp_dept` but not in `emp_salary`. - Calculate the total compensation (salary + bonus) for each employee and add it as a new column `total_compensation`. # Constraints & Requirements - **Performance:** Ensure that your solution is efficient and leverages vectorized operations where possible. - **Libraries:** You are only allowed to use pandas for this task. Importing any other third-party libraries is prohibited. - **Handling Missing Data:** Any missing values in merged DataFrame should be handled adequately. Choose appropriate placeholder values where necessary. Input - Three pandas DataFrames: `emp_basic`, `emp_salary`, and `emp_dept` as described above. Output - A pandas DataFrame `df_combined` after merging. - A list of `employee_id` of employees present in `emp_basic` and `emp_dept` but not in `emp_salary`. - Updated `df_combined` with an additional column `total_compensation`. Function Signature ```python def combine_employee_data( emp_basic: pd.DataFrame, emp_salary: pd.DataFrame, emp_dept: pd.DataFrame ) -> Tuple[pd.DataFrame, List[int]]: # Your code here ``` # Example Usage ```python emp_basic = pd.DataFrame({ \'employee_id\': [101, 102, 103, 104], \'name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'age\': [25, 30, 35, 40] }) emp_salary = pd.DataFrame({ \'employee_id\': [102, 103, 104, 105], \'salary\': [70000, 80000, 75000, 90000], \'bonus\': [5000, 6000, 5500, 7000] }) emp_dept = pd.DataFrame({ \'employee_id\': [101, 102, 104, 106], \'department\': [\'HR\', \'Engineering\', \'Finance\', \'Marketing\'], \'location\': [\'New York\', \'San Francisco\', \'Boston\', \'Chicago\'] }) df_combined, absent_employees = combine_employee_data(emp_basic, emp_salary, emp_dept) print(df_combined) print(absent_employees) ```","solution":"import pandas as pd from typing import Tuple, List def combine_employee_data( emp_basic: pd.DataFrame, emp_salary: pd.DataFrame, emp_dept: pd.DataFrame ) -> Tuple[pd.DataFrame, List[int]]: # Merge emp_basic with emp_dept on \'employee_id\' using outer join merged_basic_dept = pd.merge(emp_basic, emp_dept, on=\'employee_id\', how=\'outer\') # Merge the result with emp_salary on \'employee_id\' using outer join merged_all = pd.merge(merged_basic_dept, emp_salary, on=\'employee_id\', how=\'outer\') # Find employees who are in emp_basic and emp_dept but not in emp_salary cond = (~merged_basic_dept[\'employee_id\'].isin(emp_salary[\'employee_id\'])) absent_employees = merged_basic_dept.loc[cond, \'employee_id\'].tolist() # Calculate total compensation and add it as a new column merged_all[\'salary\'] = merged_all[\'salary\'].fillna(0) # Filling NaN salaries with 0 merged_all[\'bonus\'] = merged_all[\'bonus\'].fillna(0) # Filling NaN bonuses with 0 merged_all[\'total_compensation\'] = merged_all[\'salary\'] + merged_all[\'bonus\'] return merged_all, absent_employees"},{"question":"**Custom Collection Implementation** **Objective:** Create a custom collection class which adheres to multiple interfaces provided by the `collections.abc` module. **Task:** Implement a class named `CustomSequence`, which: - Inherits from `collections.abc.Sequence`. - Implements the `__getitem__` and `__len__` methods as required by the `Sequence` interface. - Includes a method `count_unique` to count the unique elements within the collection. **Requirements:** 1. The `__getitem__` method should allow indexing with integers and slices. 2. The `__len__` method should return the number of elements in the collection. 3. The `count_unique` method should return the number of unique elements in the collection. **Input:** - Initialization data for the `CustomSequence` as a list of elements. **Output:** - The methods `__getitem__`, `__len__`, and `count_unique` should function as described above. - Ensure that `CustomSequence` instances are recognized as `Sequence` types using `issubclass` and `isinstance` functions. **Example:** ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = data def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def count_unique(self): return len(set(self._data)) # Example Usage seq = CustomSequence([1, 2, 3, 2, 1]) print(seq[0]) # Output: 1 print(len(seq)) # Output: 5 print(seq.count_unique()) # Output: 3 # Verification print(issubclass(CustomSequence, Sequence)) # Output: True print(isinstance(seq, Sequence)) # Output: True ``` **Constraints:** - The input data will be a list containing `0 <= len(data) <= 10^3` elements. - Each element will be an integer within the range `-10^6 <= element <= 10^6`. **Notes:** - Aim for efficiency in your implementation, particularly in the `count_unique` method. - The slice support in `__getitem__` should correctly handle Python slicing, including negative indices.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = data def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def count_unique(self): return len(set(self._data))"},{"question":"**Objective:** In this assessment, you are required to demonstrate your understanding of the `curses` and `curses.textpad` modules by implementing a simple terminal-based text editor. The text editor should use `curses` window and pad mechanisms to handle screen drawing and interaction, while supporting basic editing commands using the `curses.textpad.Textbox` class. **Task:** Implement a Python program that initializes a `curses` environment and displays a text editor window where users can input and edit text using keyboard. The text editor should have a border around it and support the following functionalities: 1. **Basic Text Input and Editing:** - Users should be able to input and edit text inside the editor using keyboard. - The cursor should be visible and follow input commands. - Support basic text editing commands similar to Emacs, e.g., `Control-A` to go to start of the line, `Control-E` to go to end of the line, etc., as listed in the `curses.textpad.Textbox` documentation. 2. **Screen Layout:** - The program should create a bordered window at the center of the screen for the text editor. - The text editor window should be large enough to allow for multi-line input and scrolling if necessary. 3. **Termination:** - Implement a keystroke (e.g., `Control-G`) to terminate the editing session and return the text entered in the editor. **Constraints:** - You must use the `curses` module for screen drawing and input handling. - The `curses.textpad.Textbox` class must be utilized to handle text input and editing within the editor window. - Your program must demonstrate the ability to create and handle windows, draw borders, and navigate through text using keyboard inputs. **Input and Output:** - **Input:** - User interactions via keyboard to input and edit text in the terminal. - **Output:** - A bordered text-editing interface displayed in the terminal. - The text entered and edited by the user is returned and displayed upon termination. **Sample Output:** After pressing `Control-G` to terminate, the output might look something like this (displayed in the terminal): ``` You Entered: Hello, World! This is a multi-line text editor. ``` **Note:** You do not need to worry about persisting the edited text across sessions or saving it to files. Focus on implementing the real-time text input and window management for this task.","solution":"import curses import curses.textpad def main(stdscr): # Clear the screen and hide the cursor stdscr.clear() curses.curs_set(1) # Get the size of the terminal height, width = stdscr.getmaxyx() # Define the dimensions of the text editor window win_height = height - 4 win_width = width - 4 win_y = 2 win_x = 2 # Create a window for the text editor with a border editor_win = curses.newwin(win_height, win_width, win_y, win_x) editor_win.border() # Create a pad for the text editor with extra space for scrolling editor_pad = curses.newpad(win_height + 100, win_width - 2) editor_pad.keypad(True) # Add a Textbox object to handle the text input and editing textbox = curses.textpad.Textbox(editor_pad) # Refresh the window to show the border stdscr.refresh() editor_win.refresh() # Input and Edit the text editor_pad.refresh(0, 0, win_y + 1, win_x + 1, win_y + win_height - 2, win_x + win_width - 2) textbox.edit() edited_text = textbox.gather() # Terminate the session and return the entered text stdscr.clear() stdscr.addstr(0, 0, \\"You Entered:n\\" + edited_text) stdscr.refresh() stdscr.getch() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# Coding Challenge: Implementing Secure Hashes with Custom Parameters Objective Write a Python function that uses the `hashlib` module to create a secure hash. The function should be able to handle different digest sizes, keyed hashing, and personalization. Function Signature ```python def create_secure_hash(data: str, digest_size: int, key: bytes = None, personalization: str = \'\') -> str: pass ``` Input - `data` (str): The input string to be hashed. - `digest_size` (int): The desired size of the digest. - `key` (bytes, optional): A key for keyed hashing. Defaults to None. - `personalization` (str, optional): A string for hash personalization. Defaults to an empty string. Output - (str): The resulting hash in hexadecimal format. Constraints - The `digest_size` must be greater than 0 and less than or equal to 64. - If a `key` is provided, it must be a bytes object. - The `personalization` must be a string. Requirements - Use the `hashlib` module to create the hash. - If `key` is provided, use keyed hashing. - If `personalization` is provided, use it for creating a personalized hash. - Ensure the function handles the constraints and raises appropriate errors for invalid inputs. Example Usage ```python print(create_secure_hash(\'Hello, World!\', 32)) # Expected output (example): \'fc3ff98e8c6a0d3087d515c0473f8677\' print(create_secure_hash(\'Hello, World!\', 32, key=b\'my_secret_key\')) # Expected output (example): \'6f213d738bc6a0d3087d515c0473f8677\' print(create_secure_hash(\'Hello, World!\', 32, personalization=\'my_app\')) # Expected output (example): \'d1a36b8e8c6a0d3087d515c0473f8677\' ``` Notes - The examples provided are illustrative. Actual hash outputs will vary based on the specific implementation and cryptographic parameters used. - You might need to explore the `hashlib` documentation to utilize advanced hashing features correctly.","solution":"import hashlib import hmac def create_secure_hash(data: str, digest_size: int, key: bytes = None, personalization: str = \'\') -> str: if not (0 < digest_size <= 64): raise ValueError(\'digest_size must be greater than 0 and less than or equal to 64\') if key is not None and not isinstance(key, bytes): raise ValueError(\'key must be a bytes object if provided\') if not isinstance(personalization, str): raise ValueError(\'personalization must be a string\') if key is not None: # Keyed hashing hash_obj = hmac.new(key, msg=data.encode(), digestmod=hashlib.sha256) else: # Regular hashing hash_obj = hashlib.sha256(data.encode()) # Apply personalization if personalization: hash_obj.update(personalization.encode()) return hash_obj.hexdigest()[:digest_size]"},{"question":"**Title:** Implement Advanced Statistical Calculation Function **Objective:** Implement a function that calculates the mean, variance, and standard deviation of a given list of numbers. This problem requires the use of multiple functions from the \\"math\\" module and an understanding of statistical concepts. **Description:** You need to implement a Python function called `statistical_analysis(data)`. This function takes a list of numeric values and returns a tuple containing the mean, variance, and standard deviation of the data set. **Function Signature:** ```python def statistical_analysis(data: list) -> tuple: pass ``` **Input:** - `data`: A list of floating-point numbers. Example: `[1.2, 2.4, 3.6, 4.8, 6.0]` **Output:** - A tuple of three floating-point numbers representing the mean, variance, and standard deviation of the input data. **Constraints:** - You may assume that the list `data` contains at least two numeric values. - Use the `math.fsum()` for an accurate floating point sum. - Use `math.sqrt()` to calculate the square root. **Requirements:** 1. **Mean (Arithmetic Average):** [ text{mean} = frac{sum_{i=1}^{n} x_i}{n} ] 2. **Variance:** [ text{variance} = frac{sum_{i=1}^{n} (x_i - text{mean})^2}{n} ] 3. **Standard Deviation:** [ text{standard deviation} = sqrt{text{variance}} ] **Example:** ```python >>> data = [1.2, 2.4, 3.6, 4.8, 6.0] >>> statistical_analysis(data) (3.6, 3.84, 1.96) ``` In the example above: - The mean of `[1.2, 2.4, 3.6, 4.8, 6.0]` is `3.6`. - The variance is `3.84`. - The standard deviation is approximately `1.96`. **Additional Information:** To ensure precision in calculations, make use of the `math.fsum()` function for summing up floating-point numbers. --- Write the implementation of the `statistical_analysis` function using appropriate functions from the \\"math\\" module, and ensure it handles edge cases as described in this question.","solution":"import math def statistical_analysis(data): Calculate the mean, variance, and standard deviation of a given list of numbers. :param data: List of floating-point numbers :return: A tuple containing the mean, variance, and standard deviation # Calculate the mean n = len(data) mean = math.fsum(data) / n # Calculate the variance variance = math.fsum((x - mean) ** 2 for x in data) / n # Calculate the standard deviation std_dev = math.sqrt(variance) return mean, variance, std_dev"},{"question":"**Question:** You are tasked with developing a Python utility that can open a list of URLs in a web browser and handle specific requirements based on user preferences and environmental constraints. Your task is to implement a function `open_urls_in_browser(urls, browser_preference=None, open_in_new_tab=True)`, which takes the following parameters: - `urls` (list of str): A list of URLs to be opened. - `browser_preference` (str, optional): The preferred browser type to use. If None, the system\'s default browser should be used. - `open_in_new_tab` (bool, optional): Indicates whether each URL should be opened in a new tab (default is True). If False, URLs should be opened in new windows. # Constraints: 1. **Platform Compatibility**: Your function must work on Unix, Windows, and macOS platforms. 2. **Error Handling**: If an unsupported browser type is provided or a URL fails to open, your function should gracefully handle the error by logging an appropriate message instead of raising an exception. 3. **Browser Availability**: If the specified browser is not available, fall back to the system\'s default browser. 4. **Concurrency**: Ensure that URLs are opened as concurrently as possible, without blocking the main process. # Example: ```python urls = [ \'https://www.python.org\', \'https://www.github.com\', \'https://www.stackoverflow.com\' ] open_urls_in_browser(urls, browser_preference=\'firefox\', open_in_new_tab=True) ``` This should try to open each URL in a new tab of the Firefox browser. If Firefox is not available, it should fall back to the default browser. If opening a URL fails, it should log an error message and continue with the next URL. ```python import webbrowser def open_urls_in_browser(urls, browser_preference=None, open_in_new_tab=True): Open a list of URLs in the specified web browser. Args: urls (list of str): URLs to be opened. browser_preference (str, optional): Preferred browser type. Defaults to None. open_in_new_tab (bool, optional): Open URLs in a new tab if True, else in new window. Defaults to True. Returns: None # Your implementation here # Example usage urls = [ \'https://www.python.org\', \'https://www.github.com\', \'https://www.stackoverflow.com\' ] open_urls_in_browser(urls, browser_preference=\'firefox\', open_in_new_tab=True) ``` # Requirements: - You must use the `webbrowser` module functionalities (`open()`, `open_new_tab()`, `get()`, etc.) to control the browser behavior. - Handle the browser registration and errors appropriately. - Implement logging for any errors encountered during the URL opening process. Good luck!","solution":"import webbrowser import logging import threading logging.basicConfig(level=logging.ERROR, format=\'%(asctime)s - %(levelname)s - %(message)s\') def open_url(url, browser, open_in_new_tab): try: if browser: browser_instance = webbrowser.get(browser) else: browser_instance = webbrowser.get() if open_in_new_tab: browser_instance.open_new_tab(url) else: browser_instance.open_new(url) except webbrowser.Error as e: logging.error(f\\"Error opening URL {url}: {str(e)}\\") def open_urls_in_browser(urls, browser_preference=None, open_in_new_tab=True): Open a list of URLs in the specified web browser. Args: urls (list of str): URLs to be opened. browser_preference (str, optional): Preferred browser type. Defaults to None. open_in_new_tab (bool, optional): Open URLs in a new tab if True, else in new window. Defaults to True. Returns: None threads = [] for url in urls: thread = threading.Thread(target=open_url, args=(url, browser_preference, open_in_new_tab)) threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"Coding Assessment Question # Objective: To assess your understanding of the seaborn objects API, particularly your ability to: - Load datasets. - Create plots using the seaborn.objects API. - Customize the plots with various scaling and percentile options. # Question: You are provided with a dataset named `diamonds` which contains data about diamonds including their cut, color, clarity, carat, and price. Your task is to create a visualization that displays the relationship between the diamond\'s price and its cut. # Requirements: 1. Load the `diamonds` dataset using seaborn. 2. Create a vertical plot where the y-axis represents the diamond\'s price and the x-axis represents the cut. 3. Scale the y-axis to use logarithmic scaling. 4. Add dot marks to the plot to illustrate the 10th, 50th, and 90th percentiles. 5. Customize the plot to: - Add jitter to the points to reduce overlap. - Display a range for the 25th and 75th percentiles with a horizontal shift for better visibility. # Input and Output: - **Input:** No external input except for loading the `diamonds` dataset using seaborn. - **Output:** A plot displayed using the seaborn.objects API. # Constraints: - You must use the seaborn.objects API to create the plot. - The jitter effect should reduce point overlap and enhance visibility. # Example: Here is an example of how your final plot should appear (the code is illustrative and does not need to be followed exactly): ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot plot = (so.Plot(diamonds, \\"cut\\", \\"price\\") .scale(y=\\"log\\") .add(so.Dots(pointsize=1, alpha=0.2), so.Jitter(0.3)) .add(so.Dot(), so.Perc([10, 50, 90])) .add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=0.2)) ) # Display the plot plot.show() ``` Your task is to implement the above requirements in clean, efficient code. Make sure to include comments that help explain each part of your code.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_diamond_price_plot(): Create a vertical plot with price on y-axis and cut on x-axis for diamonds dataset. The y-axis is scaled to logarithmic. Uses points to denote 10th, 50th, and 90th percentiles with jitter and custom percentiles. # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot as per the requirements plot = ( so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") .scale(y=\\"log\\") # scale y-axis logarithmically .add(so.Dots(pointsize=1, alpha=0.2), so.Jitter(0.3)) # add jittered points .add(so.Dot(), so.Perc([10, 50, 90])) # points for percentiles 10, 50, 90 .add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=0.2)) # range for 25th, 75th percentiles ) return plot # Call function to generate and display the plot plot = create_diamond_price_plot() plot.show()"},{"question":"**Problem Statement** You have been assigned the task of creating a log manager for a system where multiple processes can write to the same log file concurrently. To ensure that the log entries from different processes do not interleave and cause corruption, you need to implement file locking in Python using the `fcntl` module. **Requirements:** 1. **Log Manager Class**: - Create a class `LogManager` which initializes with a file path. - The class should have methods to: - **acquire_lock**: Acquire an exclusive lock on the log file. - **release_lock**: Release the lock on the log file. - **append_log**: Accept a log entry (string) and append it to the log file with a timestamp. 2. **Function Implementations**: - **acquire_lock**: - Use the `fcntl.lockf()` function to acquire an exclusive lock (`LOCK_EX`) on the file descriptor. - Implement blocking and non-blocking lock acquisition. - **append_log**: - Ensure that the log entry is written to the file only when the lock is held. - Append a timestamp to each log entry. - **release_lock**: - Use the `fcntl.lockf()` function to release the lock (`LOCK_UN`) on the file descriptor. 3. **Handling Errors**: - Ensure appropriate error handling using try-except blocks. - Raise user-friendly error messages for any file-related and `fcntl` operations. **Constraints**: - The file operations must be atomic to prevent race conditions. - The log file path is guaranteed to be valid and writable. **Example Usage**: ```python import time import fcntl import os class LogManager: def __init__(self, file_path): self.file_path = file_path self.fd = None def acquire_lock(self, non_blocking=False): # Implement blocking and non-blocking lock acquisition using LOCK_EX pass def release_lock(self): # Implement lock release using LOCK_UN pass def append_log(self, log_entry): # Append log entry with a timestamp while holding the lock pass # Example for testing the LogManager if __name__ == \\"__main__\\": log_manager = LogManager(\\"system.log\\") try: log_manager.acquire_lock(non_blocking=False) log_manager.append_log(\\"Process started.\\") time.sleep(2) log_manager.append_log(\\"Process completed.\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: log_manager.release_lock() ``` **Expected Output**: The `system.log` file should contain: ``` [2023-10-11 10:00:00] Process started. [2023-10-11 10:00:02] Process completed. ``` In summary, your task is to implement a thread-safe log manager using file locking mechanisms provided by the `fcntl` module to handle concurrency and prevent log corruption.","solution":"import time import fcntl import os class LogManager: def __init__(self, file_path): self.file_path = file_path self.fd = None def _open_file(self): if self.fd is None: self.fd = open(self.file_path, \'a+\') def acquire_lock(self, non_blocking=False): self._open_file() try: if non_blocking: fcntl.lockf(self.fd, fcntl.LOCK_EX | fcntl.LOCK_NB) else: fcntl.lockf(self.fd, fcntl.LOCK_EX) except IOError as e: raise RuntimeError(f\\"Could not acquire lock: {e}\\") def release_lock(self): if self.fd: try: fcntl.lockf(self.fd, fcntl.LOCK_UN) self.fd.close() self.fd = None except IOError as e: raise RuntimeError(f\\"Could not release lock: {e}\\") def append_log(self, log_entry): if self.fd is None: raise RuntimeError(\\"Lock must be acquired before appending to log\\") try: timestamp = time.strftime(\\"[%Y-%m-%d %H:%M:%S]\\") self.fd.write(f\\"{timestamp} {log_entry}n\\") self.fd.flush() except IOError as e: raise RuntimeError(f\\"Could not write to log: {e}\\")"},{"question":"Objective: Demonstrate your understanding of the **pipes** module  specifically how to create and manage pipelines that process file data through a sequence of shell commands. Problem Description: You are required to implement a function `process_files_with_pipeline()` that uses the `pipes` module to perform a series of text transformations on an input file and write the result to an output file through a pipeline sequence. The operations you need to achieve are: 1. Convert all text from lowercase to uppercase. 2. Replace spaces with underscores. 3. Sort the lines alphabetically. Function Signature: ```python def process_files_with_pipeline(input_file: str, output_file: str) -> None: # Your code here ``` Input: - `input_file` (str): The path to the input file containing text data. - `output_file` (str): The path to the output file where transformed data should be written. Constraints: - Assume the input file exists and contains text data. - The output file should be created if it does not exist; otherwise, it should overwrite existing content. Example: Given an input file with the following content: ``` hello world python coding pipes module ``` The output file should contain: ``` HELLO_WORLD PIPES_MODULE PYTHON_CODING ``` Additional Information: Consider the following sequence of shell commands to achieve the desired transformations: - Convert text to uppercase: `tr a-z A-Z` - Replace spaces with underscores: `tr \' \' \'_\'` - Sort lines: `sort` Implementation Requirements: - Use the `pipes.Template` class to define the pipeline. - Ensure the pipeline handles the input and output files as specified. - Utilize `append()` method of `pipes.Template` to add commands to the pipeline. Notes: - Since the `pipes` module is deprecated, this exercise is mostly to validate your concept of handling pipelines and advanced file manipulations. In a production environment, consider using the `subprocess` module instead.","solution":"import pipes def process_files_with_pipeline(input_file: str, output_file: str) -> None: Processes the input file through a series of shell commands and writes the result to the output file. t = pipes.Template() t.append(\'tr a-z A-Z\', \'--\') # Convert to uppercase t.append(\\"tr \' \' \'_\'\\", \'--\') # Replace spaces with underscores t.append(\'sort\', \'--\') # Sort the lines alphabetically with t.open(input_file, \'r\') as f_in: with t.open(output_file, \'w\') as f_out: f_out.write(f_in.read())"},{"question":"# Advanced Python Containers: Chain of Counters Problem Statement You are required to create a utility class that combines the functionalities of `collections.ChainMap` and `collections.Counter`. Specifically, this class will allow chaining multiple `Counter` objects together but treating them as a single `Counter`. The main functionalities should include counting elements across the chained `Counter` objects, updating counts, and computing the total sum of all counts. Class Definition Define a class `ChainCounter` which extends `collections.ChainMap` and incorporates the element counting function of `collections.Counter`. This class should provide the following methods: 1. **Constructor**: Initialize the `ChainCounter` with a variable number of `Counter` objects. ```python def __init__(self, *counters): pass ``` 2. **most_common(n=None)**: Return a list of the `n` most common elements and their counts. If `n` is not provided, return all elements in descending order of frequency. ```python def most_common(self, n=None): pass ``` 3. **total()**: Return the sum of all counts across all `Counter` objects in the chain. ```python def total(self): pass ``` 4. **__getitem__(self, key)**: Return the count of the specified element, returning 0 if the element is not present in any of the chained `Counter` objects. ```python def __getitem__(self, key): pass ``` 5. **__setitem__(self, key, count)**: Set the count for the specified element in the first `Counter` object of the chain. If the element is present in any of the other counters in the chain, its count should be updated accordingly. ```python def __setitem__(self, key, count): pass ``` 6. **__delitem__(self, key)**: Delete the specified element from all `Counter` objects in the chain. ```python def __delitem__(self, key): pass ``` Example Usage ```python from collections import Counter # Create several Counter objects counter1 = Counter({\'a\': 2, \'b\': 3}) counter2 = Counter({\'b\': 1, \'c\': 5}) counter3 = Counter({\'d\': 7}) # Initialize ChainCounter with the Counter objects chain_counter = ChainCounter(counter1, counter2, counter3) print(chain_counter.most_common()) # Output: [(\'d\', 7), (\'c\', 5), (\'b\', 4), (\'a\', 2)] print(chain_counter.total()) # Output: 18 print(chain_counter[\'b\']) # Output: 4 chain_counter[\'e\'] = 9 print(chain_counter.most_common()) # Output: [(\'e\', 9), (\'d\', 7), (\'c\', 5), (\'b\', 4), (\'a\', 2)] del chain_counter[\'b\'] print(chain_counter.most_common()) # Output: [(\'e\', 9), (\'d\', 7), (\'c\', 5), (\'a\', 2)] ``` Constraints 1. All input `Counter` objects will contain hashable elements. 2. The output from methods should maintain consistent order based on the chaining approach described. Performance Requirements - The `most_common` and `total` methods should aim for linear performance relative to the size of the combined `Counter` objects. - Indexing and updates should be efficient and leverage inherent characteristics of the `Counter` objects. Implement the `ChainCounter` class to meet the above requirements.","solution":"from collections import ChainMap, Counter class ChainCounter(ChainMap): def __init__(self, *counters): super().__init__(*counters) self.counters = counters def most_common(self, n=None): total_counter = Counter() for counter in self.counters: total_counter.update(counter) return total_counter.most_common(n) def total(self): return sum(sum(counter.values()) for counter in self.counters) def __getitem__(self, key): return sum(counter.get(key, 0) for counter in self.counters) def __setitem__(self, key, count): primary_counter = self.counters[0] current_count = sum(counter.pop(key, 0) for counter in self.counters) primary_counter[key] = current_count + count def __delitem__(self, key): for counter in self.counters: if key in counter: del counter[key]"},{"question":"# Coding Assignment Question: Analyzing Integer Bit Counts Objective To assess your understanding and proficiency in working with numeric types and bitwise operations in Python, particularly focusing on the usage of the `int.bit_count()` method introduced in Python 3.10. Problem Statement Given an integer, the `bit_count()` method returns the count of the number of ones in its binary representation. Your task is to implement a function `analyze_bit_counts` that takes a list of integers and returns a dictionary. The dictionary should map each integer to a tuple containing: 1. The binary representation of the integer (as a string, starting with \'0b\'). 2. The count of ones in the binary representation. Additionally, if the list is empty, the function should return an empty dictionary. Function Signature ```python def analyze_bit_counts(integers: list) -> dict: pass ``` Input Format - A list of integers, which can include positive, negative, and zero values. Output Format - A dictionary where each key is an integer from the input list and the corresponding value is a tuple containing: - The binary representation of the integer. - The count of ones in the binary representation. Constraints - The list can contain up to 1000 integers. - Each integer is within the range of -10^9 to 10^9. Example ```python print(analyze_bit_counts([3, 7, -5, 0])) # Output: {3: (\'0b11\', 2), 7: (\'0b111\', 3), -5: (\'-0b101\', 2), 0: (\'0b0\', 0)} print(analyze_bit_counts([])) # Output: {} ``` Notes - Use the `bit_count()` method and the `bin()` function to get the binary representation of an integer. - Remember that negative numbers have a binary representation prefixed with `-0b`. Explanation In the first example, the function returns a dictionary where: - 3 is represented as \'0b11\' and has 2 ones. - 7 is represented as \'0b111\' and has 3 ones. - -5 is represented as \'-0b101\' and has 2 ones (considering only the magnitude part). - 0 is represented as \'0b0\' and has 0 ones. Your task is to implement this function adhering to the above-mentioned specifications.","solution":"def analyze_bit_counts(integers): Analyzes a list of integers and returns a dictionary where each key is an integer and each value is a tuple containing the binary representation of the integer and the count of ones in its binary representation. Parameters: integers (list): A list of integers. Returns: dict: A dictionary mapping each integer to a tuple with its binary representation and the count of ones in its binary representation. result = {} for num in integers: binary_representation = bin(num) one_count = binary_representation.count(\'1\') if num < 0: one_count = bin(abs(num)).count(\'1\') result[num] = (binary_representation, one_count) return result"},{"question":"# Question: Utilizing `RobotFileParser` for Web Crawler Management You are given the task of managing a web crawling operation that respects the restrictions specified in `robots.txt` files for various websites. Implement a function `can_crawl_and_rate(url, agent)` that determines if a specified user agent can crawl the given URL, and also retrieves the crawl delay and request rate limits if they exist. Input - `url` (string): The full URL of the resource to check. - `agent` (string): The user agent to evaluate against the `robots.txt` rules. Output - A dictionary with the following structure: ```python { \\"can_crawl\\": bool, # True if the user agent can fetch the URL, otherwise False \\"crawl_delay\\": float or None, # The crawl delay in seconds if specified, otherwise None \\"request_rate\\": dict or None # A dictionary with keys \'requests\' and \'seconds\', or None if not specified } ``` For example: ```python { \\"can_crawl\\": True, \\"crawl_delay\\": 6.0, \\"request_rate\\": { \\"requests\\": 3, \\"seconds\\": 20 } } ``` Constraints - You should handle any exceptions or errors that arise from URL fetching or parsing errors gracefully and return appropriate defaults. - The function should not depend on any external libraries except the standard Python library. Example ```python def can_crawl_and_rate(url, agent): import urllib.robotparser # Initialize the RobotFileParser object rp = urllib.robotparser.RobotFileParser() # Construct the robots.txt URL from the given URL parsed_url = urllib.parse.urlparse(url) robots_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\" # Set and read the robots.txt file rp.set_url(robots_url) rp.read() # Determine if the agent can fetch the URL can_crawl = rp.can_fetch(agent, url) # Retrieve crawl delay for the agent crawl_delay = rp.crawl_delay(agent) # Retrieve request rate for the agent request_rate_namedtuple = rp.request_rate(agent) request_rate = None if request_rate_namedtuple is not None: request_rate = { \\"requests\\": request_rate_namedtuple.requests, \\"seconds\\": request_rate_namedtuple.seconds } # Construct and return the result dictionary return { \\"can_crawl\\": can_crawl, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate } # Example usage result = can_crawl_and_rate(\\"http://www.example.com/path\\", \\"*\\") print(result) ``` Write a function `can_crawl_and_rate(url, agent)` that follows the above specification and passes the example test cases.","solution":"def can_crawl_and_rate(url, agent): import urllib.robotparser import urllib.parse # Initialize the RobotFileParser object rp = urllib.robotparser.RobotFileParser() # Construct the robots.txt URL from the given URL parsed_url = urllib.parse.urlparse(url) robots_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\" # Set and read the robots.txt file rp.set_url(robots_url) rp.read() # Determine if the agent can fetch the URL can_crawl = rp.can_fetch(agent, url) # Retrieve crawl delay for the agent crawl_delay = rp.crawl_delay(agent) # Retrieve request rate for the agent request_rate_namedtuple = rp.request_rate(agent) request_rate = None if request_rate_namedtuple is not None: request_rate = { \\"requests\\": request_rate_namedtuple.requests, \\"seconds\\": request_rate_namedtuple.seconds } # Construct and return the result dictionary return { \\"can_crawl\\": can_crawl, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate }"},{"question":"Question You are tasked with designing a class to manage employee data using descriptors to ensure validation and encapsulation of attributes (such as employee ID, name, and age). Attributes should have constraints that validate the data stored in them. Follow the instructions below to implement the required functionality. # Instructions 1. Define an abstract base class `Validator` that acts as a descriptor for managing access to class attributes. It should have the following methods: - `__set_name__(self, owner, name)`: to store the attribute name. - `__get__(self, obj, objtype=None)`: to retrieve the value. - `__set__(self, obj, value)`: to set the value after validation. - `validate(self, value)`: an abstract method that derived classes must implement to define validation rules. 2. Create the following custom validators inheriting from `Validator`: - `IntegerValidator`: Ensures the value is an integer within a specified range. - `StringValidator`: Ensures the value is a string of a specified length. 3. Implement an `Employee` class using these validators for the following attributes: - `employee_id`: Should be a positive integer. - `name`: Should be a string with length between 3 and 50 characters. - `age`: Should be an integer between 18 and 65. Ensure that any invalid data assignments raise appropriate errors. # Example ```python >>> emp = Employee(employee_id=1, name=\\"John Doe\\", age=30) >>> emp.name \'John Doe\' >>> emp.age = 70 ValueError: Expected value to be in range [18, 65], got 70 ``` # Constraints - You must use the descriptors as defined (Validator and its derived classes). - Ensure the solution is efficient and adheres to object-oriented principles. - Follow the input/output examples for consistency. ```python from abc import ABC, abstractmethod # Abstract Validator class class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass # Custom validator for integers with range validation class IntegerValidator(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, int): raise TypeError(f\'Expected value to be an int, got {type(value).__name__}\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected value to be at least {self.minvalue}, got {value}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected value to be no more than {self.maxvalue}, got {value}\') # Custom validator for strings with length validation class StringValidator(Validator): def __init__(self, minlen=None, maxlen=None): self.minlen = minlen self.maxlen = maxlen def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected value to be a str, got {type(value).__name__}\') if self.minlen is not None and len(value) < self.minlen: raise ValueError(f\'Expected value length to be at least {self.minlen}, got {len(value)}\') if self.maxlen is not None and len(value) > self.maxlen: raise ValueError(f\'Expected value length to be no more than {self.maxlen}, got {len(value)}\') # Employee class utilizing the validators class Employee: employee_id = IntegerValidator(minvalue=1) name = StringValidator(minlen=3, maxlen=50) age = IntegerValidator(minvalue=18, maxvalue=65) def __init__(self, employee_id, name, age): self.employee_id = employee_id self.name = name self.age = age # Example usage: # emp = Employee(employee_id=1, name=\\"John Doe\\", age=30) # print(emp.name) # emp.age = 70 # Should raise a ValueError ``` # Note: - Use Python\'s abc module to create the abstract base class. - Make sure your solution covers the example usage.","solution":"from abc import ABC, abstractmethod # Abstract Validator class class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass # Custom validator for integers with range validation class IntegerValidator(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, int): raise TypeError(f\'Expected value to be an int, got {type(value).__name__}\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected value to be at least {self.minvalue}, got {value}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected value to be no more than {self.maxvalue}, got {value}\') # Custom validator for strings with length validation class StringValidator(Validator): def __init__(self, minlen=None, maxlen=None): self.minlen = minlen self.maxlen = maxlen def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected value to be a str, got {type(value).__name__}\') if self.minlen is not None and len(value) < self.minlen: raise ValueError(f\'Expected value length to be at least {self.minlen}, got {len(value)}\') if self.maxlen is not None and len(value) > self.maxlen: raise ValueError(f\'Expected value length to be no more than {self.maxlen}, got {len(value)}\') # Employee class utilizing the validators class Employee: employee_id = IntegerValidator(minvalue=1) name = StringValidator(minlen=3, maxlen=50) age = IntegerValidator(minvalue=18, maxvalue=65) def __init__(self, employee_id, name, age): self.employee_id = employee_id self.name = name self.age = age # Example usage: # emp = Employee(employee_id=1, name=\\"John Doe\\", age=30) # print(emp.name) # emp.age = 70 # Should raise a ValueError"},{"question":"**Coding Assessment Question: Error Handling in an ATM Simulation** An ATM (Automated Teller Machine) system needs to handle different types of operations such as withdrawals and deposits while managing various errors that may occur during these operations. You need to implement a class `ATM` that handles these operations along with error management. # Class `ATM`: 1. **Attributes**: - `balance` (float): represents the current balance in the ATM. 2. **Methods**: - `__init__(self, initial_balance: float)`: Initializes the ATM with a given initial balance. - `deposit(self, amount: float)`: Adds the specified amount to the balance. Raises a `ValueError` if the amount is negative. - `withdraw(self, amount: float)`: Deducts the specified amount from the balance. Raises a `ValueError` if the amount is negative, or an `InsufficientFundsError` (a custom exception) if the balance is insufficient. - `get_balance(self) -> float`: Returns the current balance. # Custom Exception Class `InsufficientFundsError`: - Inherits from the base `Exception` class. - Should accept a message as an initialization parameter and override the `__str__` method to return this message. # Requirements: 1. In `withdraw` and `deposit` methods, use appropriate exception handling to manage and raise errors. 2. Implement the custom exception `InsufficientFundsError` yourself. 3. Ensure that the `withdraw` and `deposit` methods clean up resources correctly. For our purpose, we\'ll assume cleaning up resources means printing a message indicating that the transaction has been completed (\\"Transaction complete\\"). # Important Points: 1. You must validate the input amounts in the `deposit` and `withdraw` methods. 2. Handle exceptions so that invalid transactions do not alter the ATMs balance. 3. Demonstrate the usage of the class in a script where various operations (deposits and withdrawals) are performed, and errors are handled gracefully. # Example: ```python class InsufficientFundsError(Exception): def __init__(self, message): self.message = message def __str__(self): return self.message class ATM: def __init__(self, initial_balance: float): self.balance = initial_balance def deposit(self, amount: float): try: if amount < 0: raise ValueError(\\"Deposit amount cannot be negative.\\") self.balance += amount except ValueError as e: print(e) finally: print(\\"Transaction complete\\") def withdraw(self, amount: float): try: if amount < 0: raise ValueError(\\"Withdrawal amount cannot be negative.\\") if self.balance < amount: raise InsufficientFundsError(\\"Insufficient funds.\\") self.balance -= amount except (ValueError, InsufficientFundsError) as e: print(e) finally: print(\\"Transaction complete\\") def get_balance(self): return self.balance if __name__ == \\"__main__\\": atm = ATM(500) atm.deposit(100) # should succeed atm.deposit(-50) # should raise ValueError atm.withdraw(700) # should raise InsufficientFundsError atm.withdraw(50) # should succeed print(f\\"Final balance: {atm.get_balance()}\\") ``` In the above implementation: - `ATM` class manages deposits and withdrawals. - Custom exception `InsufficientFundsError` is properly defined. - Proper exception handling ensures transactions proceed correctly or are halted without unwanted side-effects.","solution":"class InsufficientFundsError(Exception): def __init__(self, message): self.message = message def __str__(self): return self.message class ATM: def __init__(self, initial_balance: float): self.balance = initial_balance def deposit(self, amount: float): try: if amount < 0: raise ValueError(\\"Deposit amount cannot be negative.\\") self.balance += amount except ValueError as e: print(e) finally: print(\\"Transaction complete\\") def withdraw(self, amount: float): try: if amount < 0: raise ValueError(\\"Withdrawal amount cannot be negative.\\") if self.balance < amount: raise InsufficientFundsError(\\"Insufficient funds.\\") self.balance -= amount except (ValueError, InsufficientFundsError) as e: print(e) finally: print(\\"Transaction complete\\") def get_balance(self): return self.balance if __name__ == \\"__main__\\": atm = ATM(500) atm.deposit(100) # should succeed atm.deposit(-50) # should raise ValueError atm.withdraw(700) # should raise InsufficientFundsError atm.withdraw(50) # should succeed print(f\\"Final balance: {atm.get_balance()}\\")"},{"question":"# Multi-layer Perceptron Classification Task Objective In this assignment, you will use scikit-learn\'s `MLPClassifier` to build and evaluate a multi-layer perceptron model for a classification task. Problem Statement You are given a dataset with features and labels. Your task is to perform the following steps: 1. **Load the Dataset:** Load the dataset provided as `data.csv`. The dataset consists of `n_samples` rows and `m+1` columns. The first `m` columns are the features, and the last column is the target label. 2. **Preprocess the Data:** Standardize the features of the dataset. 3. **Build and Train the Model:** - Initialize an `MLPClassifier` with the following parameters: - `hidden_layer_sizes=(10, 5)` - `solver=\'adam\'` - `alpha=0.001` - `max_iter=200` - `random_state=42` - Fit the model using the training data. 4. **Evaluate the Model:** - Predict the labels for the training data. - Print the classification accuracy. - Print the confusion matrix. 5. **Analyze the Model Coefficients:** - Print the shapes of the weight matrices in `coefs_`. - Explain in a few sentences the significance of the coefficients\' shapes. 6. **Hyperparameter Tuning (Optional):** Use grid search or random search to find the best hyperparameters for the MLP. Constraints - Use only scikit-learn and numpy libraries for this task. - The dataset may have missing values, which should be handled appropriately. - Your implementation should handle both binary and multi-class classification. Input and Output Formats **Input:** - `data.csv`: A CSV file where the first `m` columns are features and the last column is the target label. **Output:** - Print the classification accuracy and confusion matrix. - Print the shapes of the weight matrices in `coefs_`. - Provide a brief explanation of the significance of these shapes. Example Assume `data.csv` has the following content: ``` feature1,feature2,feature3,target 0.5,0.3,0.2,1 0.6,0.1,0.4,0 0.2,0.4,0.6,1 ... ``` Sample code snippet: ```python import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix # Load the dataset data = pd.read_csv(\'data.csv\') X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Preprocess the data scaler = StandardScaler() X = scaler.fit_transform(X) # Building and training the MLP model clf = MLPClassifier(hidden_layer_sizes=(10, 5), solver=\'adam\', alpha=0.001, max_iter=200, random_state=42) clf.fit(X, y) # Evaluate the model y_pred = clf.predict(X) accuracy = accuracy_score(y, y_pred) conf_matrix = confusion_matrix(y, y_pred) print(f\'Classification Accuracy: {accuracy}\') print(\'Confusion Matrix:\') print(conf_matrix) # Analyze the model coefficients print(f\'Weight matrix shapes: {[coef.shape for coef in clf.coefs_]}\') # Explanation: # The shape of the weight matrices indicates... ``` Complete the code above with your own implementations, ensuring all required steps are covered. Notes - Ensure your code is modular and well-commented. - Provide any additional insights or observations you find during the model evaluation.","solution":"import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix def load_and_preprocess_data(filename): # Load the dataset data = pd.read_csv(filename) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Handle missing values (if any) if np.any(pd.isnull(data)): data = data.dropna() X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Standardize the features scaler = StandardScaler() X = scaler.fit_transform(X) return X, y def build_and_train_model(X, y): # Initialize the MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(10, 5), solver=\'adam\', alpha=0.001, max_iter=200, random_state=42) # Fit the model using the training data clf.fit(X, y) return clf def evaluate_model(clf, X, y): # Predict the labels for the training data y_pred = clf.predict(X) # Calculate classification accuracy accuracy = accuracy_score(y, y_pred) # Calculate confusion matrix conf_matrix = confusion_matrix(y, y_pred) return accuracy, conf_matrix def analyze_model_coefficients(clf): # Get the shapes of the weight matrices coef_shapes = [coef.shape for coef in clf.coefs_] return coef_shapes def main(): # Steps to load data, preprocess, train, evaluate and analyze the model filename = \'data.csv\' # ensure this path corresponds to your data file X, y = load_and_preprocess_data(filename) clf = build_and_train_model(X, y) accuracy, conf_matrix = evaluate_model(clf, X, y) print(f\'Classification Accuracy: {accuracy}\') print(\'Confusion Matrix:\') print(conf_matrix) coef_shapes = analyze_model_coefficients(clf) print(f\'Shapes of the weight matrices: {coef_shapes}\') # Explanation of the significance of coefficients\' shapes explanation = ( \\"Each weight matrix shape indicates the connections between the layers:n\\" \\"- The first matrix connects the input layer to the first hidden layer.n\\" \\"- The second matrix connects the first hidden layer to the second hidden layer.n\\" \\"- The third (if existing) matrix connects the second hidden layer to the output layer.\\" ) print(explanation) if __name__ == \\"__main__\\": main()"},{"question":"**Outlier and Novelty Detection using sklearn** **Problem Statement:** In this task, you are required to implement a function to detect outliers and novelties in a dataset using two different algorithms provided by the scikit-learn library: `IsolationForest` and `LocalOutlierFactor`. The function needs to handle both outlier and novelty detection cases. **Function Signature:** ```python def detect_anomalies(X_train, X_test, method=\'isolation_forest\', novelty=False): Detect anomalies in the dataset using specified method. Parameters: - X_train (numpy.ndarray): Training data array of shape (n_train_samples, n_features) - X_test (numpy.ndarray): Test data array of shape (n_test_samples, n_features) - method (str): The method to use for anomaly detection, either \'isolation_forest\' or \'lof\' (default is \'isolation_forest\') - novelty (bool): A boolean flag indicating whether to perform novelty detection (default is False) Returns: - numpy.ndarray: An array of shape (n_test_samples,) where each element is 1 for inliers and -1 for outliers. ``` **Detailed Requirements:** 1. **Inputs:** - `X_train`: A numpy array of training data (assume it is clean of outliers for novelty detection). - `X_test`: A numpy array of new observations. - `method`: A string indicating which algorithm to use (\'isolation_forest\' or \'lof\'). - `novelty`: A boolean flag determining if we are performing novelty detection (`True`) or outlier detection (`False`). 2. **Implementation Details:** - **Isolation Forest:** - If `novelty` is `True`, fit the `IsolationForest` model on `X_train` and use it to predict labels on `X_test`. - If `novelty` is `False`, fit and predict on `X_train` combined. - **Local Outlier Factor (LOF):** - If `novelty` is `True`, instantiate LOF with `novelty=True`, fit on `X_train`, and use it to predict labels on `X_test`. - If `novelty` is `False`, use `fit_predict` on `X_train` to predict labels for the combined data including `X_test`. 3. **Output:** - The function should return a numpy array of shape `(n_test_samples,)` with the anomaly labels for `X_test`. **Example:** ```python import numpy as np # Example data X_train = np.array([[1, 2], [2, 3], [3, 4], [2, 2], [3, 3]]) X_test = np.array([[10, 10], [2, 2], [1, 1]]) # Call function for IsolationForest anomalies = detect_anomalies(X_train, X_test, method=\'isolation_forest\', novelty=True) print(anomalies) # Output: [-1 1 1] or similar, depending on the model and data # Call function for LOF anomalies = detect_anomalies(X_train, X_test, method=\'lof\', novelty=False) print(anomalies) # Output: [-1 1 1] or similar, depending on the model and data ``` **Constraints:** - Assume scikit-learn and other necessary libraries are properly installed. - Consider edge cases, such as empty arrays, and handle them appropriately. This question will test the students\' ability to effectively use sklearn\'s anomaly detection tools and understand the differences and appropriate usage scenarios for outlier and novelty detection.","solution":"import numpy as np from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor def detect_anomalies(X_train, X_test, method=\'isolation_forest\', novelty=False): Detect anomalies in the dataset using specified method. Parameters: - X_train (numpy.ndarray): Training data array of shape (n_train_samples, n_features) - X_test (numpy.ndarray): Test data array of shape (n_test_samples, n_features) - method (str): The method to use for anomaly detection, either \'isolation_forest\' or \'lof\' (default is \'isolation_forest\') - novelty (bool): A boolean flag indicating whether to perform novelty detection (default is False) Returns: - numpy.ndarray: An array of shape (n_test_samples,) where each element is 1 for inliers and -1 for outliers. if method == \'isolation_forest\': model = IsolationForest() if novelty: model.fit(X_train) return model.predict(X_test) else: all_data = np.vstack((X_train, X_test)) predictions = model.fit_predict(all_data) return predictions[len(X_train):] elif method == \'lof\': if novelty: model = LocalOutlierFactor(novelty=True) model.fit(X_train) return model.predict(X_test) else: all_data = np.vstack((X_train, X_test)) model = LocalOutlierFactor() predictions = model.fit_predict(all_data) return predictions[len(X_train):] else: raise ValueError(\\"The method should be either \'isolation_forest\' or \'lof\'\\")"},{"question":"Advanced Type Hints with Generics **Objective:** Demonstrate your understanding of Python\'s `typing` module by implementing a generic class that simulates a simplified version of a type-safe container. **Background:** Python\'s `typing` module provides support for type hints, which helps in static type checking. This is particularly useful in large codebases where you want to ensure type correctness. One of the powerful features is the ability to create generics, which allows you to define classes and functions that can operate on different types while maintaining type safety. **Task:** Your task is to implement a generic class `TypeSafeList` that ensures all elements added to the list are of the same type. You will also implement methods to add elements, remove elements, retrieve elements by index, and get the length of the list. **Requirements:** 1. **Class Definition:** - Define a generic class `TypeSafeList` that takes a single type parameter `T`. 2. **Initialization:** - The constructor should initialize an empty list to store elements. 3. **Methods:** - `add_element(element: T) -> None`: Adds an element to the list. If the element is not of the same type as existing elements (if any), raise a `TypeError`. - `remove_element(element: T) -> bool`: Removes the first occurrence of the element from the list. Returns `True` if the element was removed, `False` otherwise. - `get_element(index: int) -> T`: Retrieves an element by its index. If the index is out of range, raise an `IndexError`. - `length() -> int`: Returns the number of elements in the list. 4. **Type Safety:** - Ensure that the list maintains type safety. If an attempt is made to add an element of a different type than existing elements, raise a `TypeError`. **Input and Output Formats:** - You will not be required to handle inputs and outputs directly. Instead, implement and test the methods as described. **Constraints:** - The elements to be added will be of a comparable type (i.e., they can be compared using equality operators). - You should not use any existing type-enforcing collections like `TypedList` from third-party libraries. # Example Usage: ```python from typing import TypeVar T = TypeVar(\'T\') class TypeSafeList: def __init__(self): self._elements = [] self._type = None def add_element(self, element: T) -> None: if self._type is None: self._type = type(element) if not isinstance(element, self._type): raise TypeError(f\\"Element type {type(element)} does not match list type {self._type}\\") self._elements.append(element) def remove_element(self, element: T) -> bool: if element in self._elements: self._elements.remove(element) return True return False def get_element(self, index: int) -> T: if index < 0 or index >= len(self._elements): raise IndexError(\\"Index out of range\\") return self._elements[index] def length(self) -> int: return len(self._elements) # Example usage ts_list = TypeSafeList() ts_list.add_element(10) ts_list.add_element(20) print(ts_list.get_element(0)) # Output: 10 print(ts_list.length()) # Output: 2 ts_list.remove_element(10) print(ts_list.length()) # Output: 1 # This should raise a TypeError try: ts_list.add_element(\\"string\\") except TypeError as e: print(e) # Output: Element type <class \'str\'> does not match list type <class \'int\'> ``` **Testing:** Ensure you test your implementation with various scenarios to validate the type safety and functionality of your `TypeSafeList` class.","solution":"from typing import TypeVar, Generic, List T = TypeVar(\'T\') class TypeSafeList(Generic[T]): def __init__(self): self._elements: List[T] = [] self._type = None def add_element(self, element: T) -> None: if self._type is None: self._type = type(element) if not isinstance(element, self._type): raise TypeError(f\\"Element type {type(element)} does not match list type {self._type}\\") self._elements.append(element) def remove_element(self, element: T) -> bool: if element in self._elements: self._elements.remove(element) return True return False def get_element(self, index: int) -> T: if index < 0 or index >= len(self._elements): raise IndexError(\\"Index out of range\\") return self._elements[index] def length(self) -> int: return len(self._elements)"},{"question":"**Regression Analysis with Seaborn** You are provided with a dataset named `mpg`, which contains various attributes of cars. You are required to perform several regression analyses using the seaborn library and visualize the results. Write a Python function that: 1. Loads the `mpg` dataset. 2. Creates a 2x2 grid of subplots. 3. In the first subplot, plots a simple linear regression of \'weight\' vs. \'acceleration\'. 4. In the second subplot, plots a higher-order polynomial regression (order=3) of \'displacement\' vs. \'mpg\'. 5. In the third subplot, performs a logarithmic regression (logx=True) of \'horsepower\' vs. \'mpg\', and customizes the plot with: - Confidence interval set to 95%. - Marker style as \'o\'. - Line color as blue. 4. In the fourth subplot, performs a robust regression of \'weight\' vs. \'horsepower\' and disables the confidence interval for faster plotting. Your function should return the matplotlib figure object. **Function signature:** ```python def regression_analysis(): # Your code here ``` **Details:** - Use the `seaborn` library for creating the regression plots. - Customize the appearance of the plots as specified. - Ensure the function handles the dataset loading within itself. **Example Output:** ![Example Plot](https://via.placeholder.com/600) **Constraints:** - The implementation should be efficient and avoid redundant operations. - Ensure that the resulting plots are well-labeled for better interpretability. ```python import seaborn as sns import matplotlib.pyplot as plt def regression_analysis(): # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Create a figure and a 2x2 grid of subplots fig, axes = plt.subplots(2, 2, figsize=(12, 10)) # Simple linear regression sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\", ax=axes[0, 0]) # Higher-order polynomial regression (order=3) sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", order=3, ax=axes[0, 1]) # Logarithmic regression with plot customizations sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", logx=True, ci=95, marker=\\"o\\", line_kws={\'color\': \'blue\'}, ax=axes[1, 0]) # Robust regression without confidence intervals sns.regplot(data=mpg, x=\\"weight\\", y=\\"horsepower\\", robust=True, ci=None, ax=axes[1, 1]) # Adjust layout plt.tight_layout() return fig # Test the function regression_analysis() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def regression_analysis(): # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Create a figure and a 2x2 grid of subplots fig, axes = plt.subplots(2, 2, figsize=(12, 10)) # Simple linear regression sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\", ax=axes[0, 0]).set_title(\'Weight vs Acceleration\') # Higher-order polynomial regression (order=3) sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", order=3, ax=axes[0, 1]).set_title(\'Displacement vs MPG\') # Logarithmic regression with plot customizations sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", logx=True, ci=95, marker=\\"o\\", line_kws={\'color\': \'blue\'}, ax=axes[1, 0]).set_title(\'Horsepower vs MPG (Logarithmic)\') # Robust regression without confidence intervals sns.regplot(data=mpg, x=\\"weight\\", y=\\"horsepower\\", robust=True, ci=None, ax=axes[1, 1]).set_title(\'Weight vs Horsepower (Robust)\') # Adjust layout plt.tight_layout() return fig"},{"question":"# JIT Compilation in PyTorch In this task, you will work with the PyTorch\'s Just-In-Time (JIT) compilation features to optimize a model\'s performance. Problem Statement Implement a function `optimize_model` that performs the following tasks: 1. Create a simple neural network model using `torch.nn.Module`. 2. JIT compile this model using both scripting and tracing. 3. Save the optimized models to disk. 4. Load the models back from disk and verify that they produce the same output for a given input. Function Signature ```python def optimize_model(input_tensor: torch.Tensor): Args: - input_tensor (torch.Tensor): A sample input tensor to be used for tracing. Returns: - tuple: A tuple containing: * The output from the original model. * The output from the JIT scripted model. * The output from the JIT traced model. pass ``` Requirements - The function should define a simple feedforward neural network model with a single hidden layer. - You should create both a JIT scripted and JIT traced version of the model. - Save both the scripted and traced models to disk as `scripted_model.pt` and `traced_model.pt` respectively. - Load the saved models from disk. - For a given input tensor, ensure that all three versions (original, scripted, traced) produce the same output. Example Usage ```python import torch # Define an example input tensor input_tensor = torch.randn(1, 10) # Call the optimize_model function original_output, scripted_output, traced_output = optimize_model(input_tensor) # Verify that all outputs are the same assert torch.equal(original_output, scripted_output), \\"Scripted model output mismatch\\" assert torch.equal(original_output, traced_output), \\"Traced model output mismatch\\" ``` Constraints - You may assume the input tensor will always have the right dimensions to be processed by the model. - You should utilize torch.jit.script and torch.jit.trace for JIT compilation. - Ensure the model saving and loading is correctly handled using appropriate torch save and load functions. This problem aims to test your understanding and application of PyTorch\'s JIT functionalities.","solution":"import torch from torch import nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def optimize_model(input_tensor: torch.Tensor): # Create the model model = SimpleModel() # Get output from the original model original_output = model(input_tensor) # JIT script the model scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, \\"scripted_model.pt\\") # JIT trace the model traced_model = torch.jit.trace(model, input_tensor) torch.jit.save(traced_model, \\"traced_model.pt\\") # Load the scripted model scripted_model_loaded = torch.jit.load(\\"scripted_model.pt\\") scripted_output = scripted_model_loaded(input_tensor) # Load the traced model traced_model_loaded = torch.jit.load(\\"traced_model.pt\\") traced_output = traced_model_loaded(input_tensor) return original_output, scripted_output, traced_output"},{"question":"# Question: You are tasked with creating a Python script for a Unix-based interactive command-line application. This application should allow users to type commands freely, but it should also be able to switch between a normal terminal mode and a raw terminal mode where key presses are interpreted immediately (i.e., without waiting for the Enter key). Requirements: 1. Implement a function `interactive_terminal(raw_mode: bool) -> None` that: - Takes a boolean flag `raw_mode` as input. - If `raw_mode` is `True`, puts the terminal into raw mode using the `tty.setraw` function. - If `raw_mode` is `False`, resets the terminal to normal line-buffered mode. 2. Within this function, read user input in a way that demonstrates the differences between raw and normal modes: - In raw mode, print each character immediately as it is typed. - In normal mode, print each line of input after the Enter key is pressed. Constraints: - Ensure that the use of the `tty` module is clearly demonstrated in your implementation. - Handle any exceptions that might arise from switching terminal modes or reading from the terminal, and provide meaningful error messages. - Ensure that the terminal settings are restored to their original state when the script exits, even if an error occurs. Example: ```python def interactive_terminal(raw_mode: bool) -> None: import sys import tty import termios fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: if raw_mode: tty.setraw(fd) print(\\"Raw mode enabled. Type something (Press \'q\' to quit):\\") else: tty.setcbreak(fd) print(\\"Normal (cbreak) mode enabled. Type something (Press Enter to submit, \'q\' to quit):\\") while True: if raw_mode: ch = sys.stdin.read(1) print(ch, end=\'\', flush=True) if ch == \'q\': break else: line = sys.stdin.readline() print(line, end=\'\', flush=True) if \'q\' in line: break except Exception as e: print(f\\"An error occurred: {e}\\") finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) # Example usage: # interactive_terminal(raw_mode=True) # interactive_terminal(raw_mode=False) ``` **Note**: This script must be run on a Unix-like system as it utilizes the `termios` and `tty` modules, which are not available on Windows.","solution":"def interactive_terminal(raw_mode: bool) -> None: import sys import tty import termios fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: if raw_mode: tty.setraw(fd) print(\\"Raw mode enabled. Type something (Press \'q\' to quit):\\") else: # tty.setcbreak practices a function similar to the initial style request tty.setcbreak(fd) print(\\"Normal (cbreak) mode enabled. Type something (Press Enter to submit, \'q\' to quit):\\") while True: if raw_mode: ch = sys.stdin.read(1) print(ch, end=\'\', flush=True) if ch == \'q\': break else: line = sys.stdin.readline() print(line, end=\'\', flush=True) if \'q\' in line: break except Exception as e: print(f\\"An error occurred: {e}\\") finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)"},{"question":"**Title:** Functional Programming: Generators and Iterators **Objective:** Implement a series of functions that demonstrate your understanding of Python\'s iterators, generators, and functional-style programming using itertools and functools modules. **Problem Statement:** You are given a large dataset of integers ranging from 1 to 1,000,000. Your tasks are: 1. **Create an Infinite Iterator**: Implement a function `infinite_iterator(dataset)` that takes the dataset as input and returns an infinite iterator. The iterator should cycle through the dataset repeatedly. 2. **Create a Generator for Even Numbers**: Implement a generator function `even_numbers(iterator)` that yields only the even numbers from an input iterator `iterator`. 3. **Filtered Sum using `itertools` and `functools`**: Utilize the `itertools.takewhile` and `functools.reduce` functions to compute the sum of even numbers from the output of the `even_numbers` generator until a maximum threshold `max_value`. Implement the function `filtered_sum(iterator, max_value)` that returns this sum. 4. **Parallel Iteration of Multiple Iterables**: Use the `zip` function to implement a function `zip_iterables(iterable1, iterable2)` that returns an iterator of tuples, combining elements from `iterable1` and `iterable2`. # Function Signatures: ```python def infinite_iterator(dataset): pass def even_numbers(iterator): pass def filtered_sum(iterator, max_value): pass def zip_iterables(iterable1, iterable2): pass ``` # Example Usage: ```python dataset = list(range(1, 1000001)) # This is the dataset with numbers from 1 to 1,000,000 # Task 1 infinite_iter = infinite_iterator(dataset) # Get the first 10 elements from the infinite iterator for _ in range(10): print(next(infinite_iter)) # Task 2 even_gen = even_numbers(infinite_iter) # Get the first 10 even numbers from the even_numbers generator for _ in range(10): print(next(even_gen)) # Task 3 sum_of_evens = filtered_sum(even_gen, 1000) print(sum_of_evens) # Task 4 zip_result = zip_iterables([1, 2, 3], [\'a\', \'b\', \'c\']) for item in zip_result: print(item) ``` # Constraints: - The `dataset` will always be a list of integers from 1 to 1,000,000. - The `max_value` for the `filtered_sum` function will be a positive integer. - Assume that the input iterators to the `zip_iterables` function are always finite. # Notes: - Optimize your solutions for performance, considering the large size of the dataset. - Ensure your code is modular, readable, and well-documented.","solution":"import itertools from functools import reduce def infinite_iterator(dataset): Returns an infinite iterator that cycles through the input dataset. return itertools.cycle(dataset) def even_numbers(iterator): A generator function that yields only the even numbers from an input iterator. for num in iterator: if num % 2 == 0: yield num def filtered_sum(iterator, max_value): Returns the sum of even numbers from the `even_numbers` generator until a maximum threshold `max_value`. evens = even_numbers(iterator) selected_evens = itertools.takewhile(lambda x: x <= max_value, evens) return reduce(lambda x, y: x + y, selected_evens, 0) def zip_iterables(iterable1, iterable2): Returns an iterator of tuples, combining elements from `iterable1` and `iterable2`. return zip(iterable1, iterable2)"},{"question":"Objective: Demonstrate your understanding of the `ipaddress` module by implementing a function that solves a real-world problem related to IP address and network management. Problem Statement: You are tasked with writing a function `find_common_supernet(ip_networks)` that takes a list of IPv4 network addresses and returns the smallest common supernet that contains all the provided networks. Specifications: - The input list `ip_networks` contains strings representing valid IPv4 network addresses in CIDR notation (e.g., `\\"192.168.0.0/24\\"`, `\\"192.168.1.0/24\\"`). - Your function should return a string representing the smallest common supernet in CIDR notation that encompasses all the input networks. Constraints: - You may assume that the list `ip_networks` contains at least two network addresses. - You must use the `ipaddress` module for all IP address and network manipulations. - The implementation should be efficient in both time and space complexity. Example Usage: ```python def find_common_supernet(ip_networks): # Your implementation here # Example networks = [\\"192.168.0.0/24\\", \\"192.168.1.0/24\\"] print(find_common_supernet(networks)) # Output: \\"192.168.0.0/23\\" ``` Detailed Steps: 1. Parse the input list of network strings into `IPv4Network` objects. 2. Compute the smallest possible supernet that contains all provided networks. 3. Convert the resulting supernet back to a string in CIDR notation. Hint: Consider using the `supernet` method provided by the `ipaddress.IPv4Network` class to progressively combine the networks into their common supernet. Submission: Submit your implementation in a function named `find_common_supernet`.","solution":"import ipaddress def find_common_supernet(ip_networks): Takes a list of IPv4 network addresses and returns the smallest common supernet that contains all the provided networks. networks = [ipaddress.ip_network(network) for network in ip_networks] supernet = networks[0] for network in networks[1:]: while not supernet.supernet_of(network): supernet = supernet.supernet(new_prefix=supernet.prefixlen - 1) return str(supernet)"},{"question":"Email Client using poplib # Objective: Create a Python program that connects to a given POP3 server, authenticates the user, lists the emails, retrieves the latest email, and displays its contents. # Requirements: 1. **Establish a Connection:** - Connect to a specified POP3 or POP3-SSL server. - Use the appropriate port for the connection (110 for POP3, 995 for POP3-SSL). 2. **Authentication:** - Authenticate using the provided username and password. 3. **List Emails:** - List all emails in the inbox, showing their unique IDs. 4. **Retrieve and Display Latest Email:** - Retrieve the most recent email based on its unique ID and display its contents. The email content should include the headers and the body. 5. **Handle Errors:** - Handle errors gracefully (e.g., invalid credentials, connection issues). # Input: - Email server hostname (e.g., `localhost`). - Port number (e.g., `110` for POP3, `995` for POP3-SSL). - Flag indicating whether to use SSL (Boolean). - Username. - Password. # Output: - List of email unique IDs in the inbox. - The content of the most recent email, including headers and body. # Constraints: - The server should be operational and accessible to test the solution. - Students may need to handle various server responses and errors. # Example: ```python def email_client(host, port, use_ssl, username, password): import poplib # Establish a connection if use_ssl: server = poplib.POP3_SSL(host, port) else: server = poplib.POP3(host, port) try: # Authenticate server.user(username) server.pass_(password) # List emails email_list = server.list()[1] print(\\"Email IDs in the inbox:\\") for email in email_list: print(email) # Retrieve and display the most recent email latest_email_id = email_list[-1].decode().split()[0] response, lines, _ = server.retr(latest_email_id) email_content = b\\"n\\".join(lines).decode() print(\\"nMost recent email content:\\") print(email_content) except poplib.error_proto as e: print(f\\"Error: {e}\\") finally: # Quit the session server.quit() # Example function call email_client(\'localhost\', 110, False, \'user@example.com\', \'password\') ``` # Notes: 1. Ensure you have a valid POP3 server running for testing. 2. Ensure the `poplib` module is properly imported and used according to the documentation. 3. Test the function with both valid and invalid credentials to verify error handling.","solution":"import poplib def email_client(host, port, use_ssl, username, password): Connect to a POP3 server, authenticate, list emails, and retrieve the most recent email. Parameters: - host: str, POP3 server hostname - port: int, Port number (110 for POP3, 995 for POP3-SSL) - use_ssl: bool, Whether to use SSL connection - username: str, Username for authentication - password: str, Password for authentication Returns: - list of email IDs in the inbox - content of the most recent email (headers and body) try: # Establish a connection if use_ssl: server = poplib.POP3_SSL(host, port) else: server = poplib.POP3(host, port) # Authenticate server.user(username) server.pass_(password) # List emails email_list = server.list()[1] email_ids = [email.decode() for email in email_list] if not email_ids: return email_ids, \\"\\" # Retrieve the most recent email latest_email_id = email_ids[-1].split()[0] response, lines, _ = server.retr(latest_email_id) email_content = b\\"n\\".join(lines).decode() return email_ids, email_content except poplib.error_proto as e: return [], f\\"Error: {e}\\" except Exception as e: return [], f\\"Unexpected error: {e}\\" finally: # Quit the session server.quit()"},{"question":"Objective The purpose of this assessment is to gauge your understanding of working with various data formats in seaborn and creating meaningful visualizations. You will be required to manipulate data and generate plots based on given instructions. Problem Statement Given a dataset containing information about the total bill amount and tips in a restaurant, you are required to: 1. Load the dataset which includes the following columns: `total_bill`, `tip`, `sex`, `smoker`, `day`, `time`, and `size`. 2. Transform the dataset into two different forms: - Long-form data. - Wide-form data. 3. Create the following visualizations: - For the long-form dataset, create a `scatter plot` showing the relationship between `total_bill` and `tip` with different colors for `day`. - For the wide-form dataset, create a `line plot` showing the trend of `total_bill` and `tip` (you may need to pivot the dataset). 4. Handle a \\"messy\\" dataset by transforming it into a tidy format and plotting a `bar` plot to show the average `tip` per `day` for different `sex`. # Input and Output Specifications 1. **Input:** - Load the `tips` dataset from seaborn\'s built-in datasets. ```python tips = sns.load_dataset(\\"tips\\") ``` - The dataset contains 244 rows and 7 columns. 2. **Output:** - `scatter plot` with long-form data showing `total_bill` vs `tip` (color by `day`). - `line plot` with wide-form data showing `total_bill` and `tip`. - A transformed tidy dataset for the messy dataset and a `bar` plot showing the average `tip` per `day` for different `sex`. # Constraints - Use seaborn for plotting. - Pandas should be used for data manipulation. - Ensure each plot has appropriate labels and legends for clarity. # Evaluation Criteria - Correct transformation of data to long-form and wide-form. - Successful creation and interpretation of scatter and line plots. - Correct handling and transformation of messy data. - Clear and well-labeled plots. # Performance Requirements - Efficient handling of data manipulation and plotting. Example Scenarios 1. **Prepare Long-form Data and Scatter Plot** ```python import seaborn as sns import pandas as pd # Load dataset tips = sns.load_dataset(\\"tips\\") # Scatter plot sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\') ``` 2. **Prepare Wide-form Data and Line Plot** ```python # Pivot dataset to make it wide-form (example transformation, actual code may vary) tips_wide = tips.pivot(index=\'day\', columns=\'sex\', values=[\'total_bill\', \'tip\']) # Line plot sns.lineplot(data=tips_wide[\'total_bill\']) sns.lineplot(data=tips_wide[\'tip\']) ``` 3. **Transform Messy Data and Bar Plot** ```python # Example messy dataset transformation (assume initial dataset is messy) tips_messy = tips.pivot_table(values=\'tip\', index=[\'sex\', \'day\'], aggfunc=\'mean\').reset_index() # Bar plot sns.barplot(x=\'day\', y=\'tip\', hue=\'sex\', data=tips_messy) ``` Using the above instructions, implement the required code in a Jupyter Notebook and produce the plots as specified.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Transform to long-form data (this is already in long form, so no change needed) long_form_data = tips # Pivot wide-form data wide_form_data = tips.pivot_table(index=\'day\', columns=\'time\', values=[\'total_bill\', \'tip\'], aggfunc=\'mean\').reset_index() # Scatter plot (long-form) plt.figure(figsize=(10, 6)) sns.scatterplot(data=long_form_data, x=\'total_bill\', y=\'tip\', hue=\'day\') plt.title(\'Scatter plot of Total Bill vs Tip by Day\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.legend(title=\'Day\') plt.show() # Line plot (wide-form) plt.figure(figsize=(10, 6)) # Plot total_bill trends sns.lineplot(data=wide_form_data[\'total_bill\']) # Plot tip trends sns.lineplot(data=wide_form_data[\'tip\']) plt.title(\'Line Plot of Total Bill and Tip\') plt.xlabel(\'Day\') plt.ylabel(\'Value\') plt.legend(labels=[\'Total Bill (Lunch)\', \'Total Bill (Dinner)\', \'Tip (Lunch)\', \'Tip (Dinner)\']) plt.show() # Tidy dataset for messy data tidy_data = tips.pivot_table(values=\'tip\', index=[\'sex\', \'day\'], aggfunc=\'mean\').reset_index() # Bar plot (tidy data) plt.figure(figsize=(10, 6)) sns.barplot(x=\'day\', y=\'tip\', hue=\'sex\', data=tidy_data) plt.title(\'Average Tip per Day by Sex\') plt.xlabel(\'Day\') plt.ylabel(\'Average Tip\') plt.legend(title=\'Sex\') plt.show()"},{"question":"**Objective:** Demonstrate your understanding of data manipulation, aggregation, and plotting using pandas. **Problem Statement:** You are provided with a CSV file containing sales data for a company. The dataset includes the following columns: - `Date`: The date of the transaction (in `YYYY-MM-DD` format). - `CustomerID`: Unique identifier for each customer. - `ProductID`: Unique identifier for each product. - `Quantity`: Number of units sold. - `Price`: Price per unit of the product. Using pandas, perform the following tasks: 1. **Data Preparation**: - Load the dataset from the CSV file. - Convert the `Date` column to datetime. - Handle any missing values in a reasonable manner. 2. **Aggregation**: - Calculate the total sales amount (i.e., `Quantity * Price`) for each transaction. - Group the data by `ProductID` and calculate the total quantity sold and total sales amount for each product. 3. **Time Series Analysis**: - Create a new DataFrame that shows the monthly sales amount for the entire company. - Plot the monthly sales trend over time. 4. **Customer Analysis**: - Identify the top 10 customers based on total sales amount. - For each of these customers, calculate the average sales amount per transaction. **Input Format:** - A CSV file named `sales_data.csv` with the columns: `Date`, `CustomerID`, `ProductID`, `Quantity`, and `Price`. **Output:** - Print the total quantity sold and total sales amount for each product. - A plot showing the monthly sales trend over time. - Print the top 10 customers and their average sales amount per transaction. **Constraints:** - Use pandas for all data manipulation and analysis tasks. - Ensure the code is efficient and can handle large datasets (up to millions of rows). **Example:** Assume the `sales_data.csv` contains the following data: ``` Date,CustomerID,ProductID,Quantity,Price 2021-01-01,C1,P1,10,15.0 2021-01-02,C2,P2,5,25.0 2021-01-02,C1,P2,2,25.0 2021-02-01,C1,P1,8,15.0 2021-02-10,C3,P3,7,30.0 2021-03-15,C3,P2,1,25.0 2021-03-18,C2,P3,3,30.0 ``` Based on your code, the output should include: - The total quantity sold and total sales amount for each product. - A plot showing the monthly sales trend. - The top 10 customers and their average sales amount per transaction. **Performance Requirements:** Ensure the code is written efficiently and can handle the dataset\'s size appropriately. **Note:** Make sure to provide comments in your code to explain your logic and steps clearly.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_and_prepare_data(file_path): Load the sales data from a CSV file and prepare it for analysis. Args: file_path (str): Path to the CSV file containing sales data. Returns: pd.DataFrame: Cleaned and prepared DataFrame. # Load data df = pd.read_csv(file_path) # Convert Date column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Handle missing values by filling with 0 (reasonable assumption for sales data) df.fillna(0, inplace=True) return df def calculate_totals(df): Calculate the total sales and total quantity sold for each product. Args: df (pd.DataFrame): DataFrame containing the sales data. Returns: pd.DataFrame: DataFrame with total quantity sold and total sales amount for each product. # Calculate total sales amount for each transaction df[\'TotalAmount\'] = df[\'Quantity\'] * df[\'Price\'] # Group by ProductID and calculate total quantity sold and total sales amount product_totals = df.groupby(\'ProductID\').agg({\'Quantity\': \'sum\', \'TotalAmount\': \'sum\'}).reset_index() return product_totals def monthly_sales(df): Calculate the monthly sales amount for the entire company and plot the trend. Args: df (pd.DataFrame): DataFrame containing the sales data. Returns: pd.DataFrame: DataFrame with monthly sales amount. # Set Date as the index df.set_index(\'Date\', inplace=True) # Resample data by month and calculate the total sales amount for each month monthly_sales = df[\'TotalAmount\'].resample(\'M\').sum().reset_index() # Plot the monthly sales trend over time plt.figure(figsize=(10, 6)) plt.plot(monthly_sales[\'Date\'], monthly_sales[\'TotalAmount\'], marker=\'o\') plt.title(\'Monthly Sales Trend\') plt.xlabel(\'Month\') plt.ylabel(\'Total Sales Amount\') plt.grid(True) plt.show() return monthly_sales def top_customers_analysis(df, n=10): Identify the top n customers based on total sales amount and calculate the average sales amount per transaction. Args: df (pd.DataFrame): DataFrame containing the sales data. n (int): Number of top customers to identify. Returns: pd.DataFrame: DataFrame with top n customers and their average sales amount per transaction. # Calculate total sales amount for each customer customer_totals = df.groupby(\'CustomerID\').agg({\'TotalAmount\': \'sum\'}).reset_index() # Identify top n customers top_customers = customer_totals.nlargest(n, \'TotalAmount\') # Calculate average sales amount per transaction for each top customer top_customers = top_customers.merge(df.groupby(\'CustomerID\').agg({\'TotalAmount\': \'mean\'}).reset_index(), on=\'CustomerID\', suffixes=(\'_Total\', \'_Avg\')) return top_customers # Main function to execute the analysis def main(file_path): # Load and prepare data df = load_and_prepare_data(file_path) # Calculate totals for each product product_totals = calculate_totals(df) print(\\"Total quantity sold and total sales amount for each product:n\\", product_totals) # Perform monthly sales analysis monthly_sales(df) # Identify top 10 customers top_customers = top_customers_analysis(df, n=10) print(\\"Top 10 customers and their average sales amount per transaction:n\\", top_customers) # Assuming the data file is named \'sales_data.csv\' if __name__ == \\"__main__\\": main(\'sales_data.csv\')"},{"question":"# Async Programming with Coroutine Objects Introduction Starting from Python 3.5, Python introduced the concept of coroutines with the `async` and `await` keywords, allowing for asynchronous programming. Coroutines are special functions that can pause execution and yield control back to the event loop, making efficient use of resources. Problem Statement You are required to implement a series of asynchronous functions and manage them using coroutine objects. The goal is to simulate a simple asynchronous task scheduler that runs several coroutines concurrently. Tasks 1. **Define Asynchronous Functions:** Create three asynchronous functions `async_task_1`, `async_task_2`, and `async_task_3` using the `async def` syntax. Each function should accept an integer parameter `n` and should: - Print a message indicating the start of the task (e.g., \\"Task 1 started with n = x\\"). - Await an asynchronous sleep for `n` seconds (use `await asyncio.sleep(n)`). - Print a message indicating the completion of the task (e.g., \\"Task 1 completed\\"). 2. **Task Scheduler Function:** Implement a function `task_scheduler` that: - Accepts a list of tuples, where each tuple contains an async task function and its corresponding integer parameter. - Uses `asyncio.gather` to run all the tasks concurrently. - Returns a message indicating all tasks have completed. Input - A list of tuples. Each tuple contains: - An asynchronous task function (`async_task_1`, `async_task_2`, or `async_task_3`). - An integer `n` (time in seconds). Output - A message indicating that all tasks have been completed. Example ```python import asyncio async def async_task_1(n): print(f\\"Task 1 started with n = {n}\\") await asyncio.sleep(n) print(\\"Task 1 completed\\") async def async_task_2(n): print(f\\"Task 2 started with n = {n}\\") await asyncio.sleep(n) print(\\"Task 2 completed\\") async def async_task_3(n): print(f\\"Task 3 started with n = {n}\\") await asyncio.sleep(n) print(\\"Task 3 completed\\") async def task_scheduler(tasks): await asyncio.gather(*(task(n) for task, n in tasks)) return \\"All tasks have been completed\\" # Example Usage tasks = [ (async_task_1, 2), (async_task_2, 3), (async_task_3, 1), ] # Note: This part should be run in an async environment, such as an event loop asyncio.run(task_scheduler(tasks)) ``` Constraints - Each `n` value in the tasks will be a positive integer. - There will be between 1 and 10 tasks. Write your implementation for the functions as described.","solution":"import asyncio async def async_task_1(n): print(f\\"Task 1 started with n = {n}\\") await asyncio.sleep(n) print(\\"Task 1 completed\\") async def async_task_2(n): print(f\\"Task 2 started with n = {n}\\") await asyncio.sleep(n) print(\\"Task 2 completed\\") async def async_task_3(n): print(f\\"Task 3 started with n = {n}\\") await asyncio.sleep(n) print(\\"Task 3 completed\\") async def task_scheduler(tasks): await asyncio.gather(*(task(n) for task, n in tasks)) return \\"All tasks have been completed\\" # Example Usage tasks = [ (async_task_1, 2), (async_task_2, 3), (async_task_3, 1), ] # Note: This part should be run in an async environment, such as an event loop # asyncio.run(task_scheduler(tasks)) # Uncomment to run in an async environment"},{"question":"# Advanced Coding Assessment **Objective:** Assess the ability to work with HTTP requests using the `urllib.request` module in Python. This question covers creating and sending requests with custom headers and data, handling responses, and implementing error handling. **Problem Statement:** Implement a function `fetch_url` that fetches a given URL using the `urllib.request` module, with the following specifications: 1. The function should accept the following parameters: - `url` (str): The URL to fetch. - `data` (dict, optional): Data to be included in the request. If `data` is provided, the request should be a POST request. - `headers` (dict, optional): Custom headers to be included in the request. 2. The function should perform the following operations: - If `data` is provided, encode it using `urllib.parse.urlencode` and set the request to be a POST request. - Construct a request object with the given URL, data (if any), and headers (if any). - Open the URL and read the response. - Handle HTTP errors (`HTTPError`) by printing the error code and the returned error page. - Handle URL errors (`URLError`) by printing the error reason. 3. Return the content of the response if the request is successful. **Function Signature:** ```python import urllib.request import urllib.parse def fetch_url(url: str, data: dict = None, headers: dict = None) -> str: # Your code here ``` **Example Usage:** ```python url = \'http://www.example.com\' data = {\'name\': \'John Doe\', \'location\': \'New York\'} headers = {\'User-Agent\': \'Mozilla/5.0\'} try: content = fetch_url(url, data=data, headers=headers) print(content) except Exception as e: print(e) ``` **Additional Notes:** - You may assume that the server is always reachable. - The function should not terminate the program; it should return the error information to the caller if an error occurs. **Constraints:** - The function should handle typical HTTP status codes and errors gracefully. - Use the provided libraries (`urllib.request`, `urllib.parse`) for all HTTP requests and data handling.","solution":"import urllib.request import urllib.parse import urllib.error def fetch_url(url: str, data: dict = None, headers: dict = None) -> str: try: if data: data = urllib.parse.urlencode(data).encode(\'utf-8\') request = urllib.request.Request(url, data=data) if headers: for key, value in headers.items(): request.add_header(key, value) with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: print(f\'HTTPError: {e.code}, {e.reason}\') return f\'HTTPError: {e.code}, {e.reason}\' except urllib.error.URLError as e: print(f\'URLError: {e.reason}\') return f\'URLError: {e.reason}\'"},{"question":"**Objective**: Implement a custom descriptor class to manage and validate attributes of a Python class. # Problem Statement You are required to implement a descriptor class `PositiveInteger` which ensures that the attribute it manages always holds a positive integer value. Your descriptor should handle access (`__get__`), modification (`__set__`), and deletion (`__delete__`) of the attribute. Additionally, it should log each access and modification. # Requirements 1. **Descriptor Class**: Implement a class `PositiveInteger`: - `__get__(self, obj, objtype=None)`: Should return the attribute\'s value. - `__set__(self, obj, value)`: Should set the attribute\'s value to `value` only if it is a positive integer; otherwise, it should raise a `ValueError`. - `__delete__(self, obj)`: Should delete the attribute and raise an `AttributeError` on subsequent access. - Log every access and modification using Python\'s logging module. 2. **Logging**: Ensure that every access and modification to the attribute is logged. The log entries should indicate the action taken (accessed or modified), the attribute name, and the value. 3. **Target Class**: Implement a class `Product` which represents a product with a non-negative price: - The `Product` class should have an attribute `price` managed by the `PositiveInteger` descriptor. # Input and Output - **Input**: Operations on an instance of the `Product` class such as setting, getting, and deleting the `price` attribute. - **Output**: The outcomes of these operations, as well as the corresponding log entries. # Constraints - The `price` must always be a positive integer. - Logging should occur at the `INFO` level using Pythons standard logging library. # Example ```python import logging logging.basicConfig(level=logging.INFO) class PositiveInteger: def __init__(self): self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, obj, objtype=None): value = obj.__dict__.get(self._name) logging.info(f\'Accessing {self._name} giving {value}\') return value def __set__(self, obj, value): if not isinstance(value, int) or value <= 0: raise ValueError(f\'The value of {self._name} must be a positive integer. Attempted to set {value}.\') logging.info(f\'Updating {self._name} to {value}\') obj.__dict__[self._name] = value def __delete__(self, obj): if self._name in obj.__dict__: logging.info(f\'Deleting {self._name}\') del obj.__dict__[self._name] else: logging.info(f\'Attempted to delete {self._name} which does not exist\') raise AttributeError(f\'{self._name} attribute does not exist\') class Product: price = PositiveInteger() def __init__(self, price): self.price = price # Example usage: p = Product(10) # Logs: Updating price to 10 print(p.price) # Logs: Accessing price giving 10; Output: 10 p.price = 15 # Logs: Updating price to 15 print(p.price) # Logs: Accessing price giving 15; Output: 15 try: p.price = -5 # Raises ValueError except ValueError as e: print(e) del p.price # Logs: Deleting price try: print(p.price) # Raises AttributeError except AttributeError as e: print(e) ``` # Notes 1. Ensure that your `PositiveInteger` descriptor fulfills all the constraints mentioned. 2. Ensure proper logging of access and modification to the managed attribute. Happy coding!","solution":"import logging # Configuring logging to show info messages logging.basicConfig(level=logging.INFO) class PositiveInteger: def __init__(self): self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, obj, objtype=None): value = obj.__dict__.get(self._name) logging.info(f\'Accessing {self._name} giving {value}\') if value is None: raise AttributeError(f\'{self._name} has been deleted\') return value def __set__(self, obj, value): if not isinstance(value, int) or value <= 0: raise ValueError(f\'The value of {self._name} must be a positive integer. Attempted to set {value}.\') logging.info(f\'Updating {self._name} to {value}\') obj.__dict__[self._name] = value def __delete__(self, obj): if self._name in obj.__dict__: logging.info(f\'Deleting {self._name}\') del obj.__dict__[self._name] else: logging.info(f\'Attempted to delete {self._name} which does not exist\') raise AttributeError(f\'{self._name} attribute does not exist\') class Product: price = PositiveInteger() def __init__(self, price): self.price = price"},{"question":"**Objective:** You are tasked with building and evaluating a classification model using Stochastic Gradient Descent (SGD). This will test your ability to preprocess data, implement SGD for classification, and evaluate the model\'s performance using scikit-learn. # Problem Statement: You are given a dataset containing features of iris flowers and their corresponding species. Your task is to: 1. Preprocess the dataset by scaling the features. 2. Implement a classification model using `SGDClassifier` with the hinge loss function (equivalent to a linear SVM). 3. Tune the model with different values for the regularization parameter. 4. Evaluate the model\'s performance using accuracy and roc-auc score. # Requirements: 1. **Preprocessing:** - Scale the features using `StandardScaler`. 2. **Model Implementation:** - Use `SGDClassifier` with the following configurations: - Loss function: \\"hinge\\" - Penalty: \\"l2\\" - Maximum iterations: 1000 - Set `random_state` to 42 for reproducibility. 3. **Model Tuning:** - Perform hyperparameter tuning on the `alpha` parameter (regularization strength) using `GridSearchCV` with the following values: `[0.0001, 0.001, 0.01, 0.1, 1]`. 4. **Evaluation:** - Split the data into a training set (70%) and a test set (30%). - Evaluate the model on the test set using: - Accuracy score. - ROC-AUC score. # Input: The dataset is stored in a CSV file named `iris.csv` with the following structure: ``` sepal length (cm), sepal width (cm), petal length (cm), petal width (cm), species 5.1, 3.5, 1.4, 0.2, setosa 7.0, 3.2, 4.7, 1.4, versicolor ... 6.3, 3.3, 6.0, 2.5, virginica ``` # Sample code to read the CSV file: ```python import pandas as pd data = pd.read_csv(\'iris.csv\') X = data.iloc[:, :-1].values y = data.iloc[:, -1].values ``` # Implementation: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, roc_auc_score from sklearn.pipeline import Pipeline # Read dataset data = pd.read_csv(\'iris.csv\') X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Preprocessing: Train-test split and Scaling X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Pipeline setup pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'sgd\', SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, random_state=42)) ]) # Model tuning with GridSearchCV param_grid = {\'sgd__alpha\': [0.0001, 0.001, 0.01, 0.1, 1]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best model evaluation best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) # Performance metrics accuracy = accuracy_score(y_test, y_pred) roc_auc = roc_auc_score(pd.get_dummies(y_test), pd.get_dummies(y_pred), multi_class=\'ovr\') # Output results print(f\\"Best Alpha: {grid_search.best_params_[\'sgd__alpha\']}\\") print(f\\"Accuracy: {accuracy}\\") print(f\\"ROC-AUC: {roc_auc}\\") ``` # Constraints: - Python 3.6 or above. - Scikit-learn version 0.24.2 or above. - The input dataset file `iris.csv` should be placed in the same directory as the script.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, roc_auc_score from sklearn.pipeline import Pipeline def preprocess_and_train_model(filepath): # Read dataset data = pd.read_csv(filepath) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Preprocessing: Train-test split and Scaling X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Pipeline setup with scaling and SGD classifier pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'sgd\', SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, random_state=42)) ]) # Model tuning with GridSearchCV param_grid = {\'sgd__alpha\': [0.0001, 0.001, 0.01, 0.1, 1]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best model evaluation best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) # Performance metrics accuracy = accuracy_score(y_test, y_pred) roc_auc = roc_auc_score(pd.get_dummies(y_test), pd.get_dummies(y_pred), multi_class=\'ovr\') return grid_search.best_params_[\'sgd__alpha\'], accuracy, roc_auc"},{"question":"# Assessment Question: Visualizing Categorical Data with Seaborn Objective: Write a Python function that utilizes Seaborn to create a comprehensive and informative visualization of a given dataset. Your function should demonstrate an understanding of various categorical plots, hue semantics, plot customization, and the use of facet grids. Function Signature: ```python def visualize_categorical_data(data: pd.DataFrame) -> plt.Figure: pass ``` Parameters: - `data (pd.DataFrame)`: A pandas DataFrame containing the dataset to be visualized. The dataset will include multiple categorical and numerical columns. Requirements: 1. Using the `catplot` function, create a figure showing at least three different types of categorical plots from the families of scatterplots, distribution plots, and estimate plots. Include examples such as stripplot, boxplot, and barplot. 2. Incorporate hue semantics to differentiate data points based on another categorical variable. 3. Customize one of the plots by: - Adjusting the ordering of categories along one of the axes. - Disabling or enabling the dodge parameter. 4. Create a facet grid to show higher-dimensional relationships within the data. For example, split by a categorical column and arrange plots in different columns or rows. 5. Each plot should have appropriate titles, labels, and legends to make the visualization self-explanatory. Example Dataset: You can assume the function will be tested on a dataset similar to the `tips` dataset in Seaborn, which includes columns like `total_bill`, `tip`, `sex`, `smoker`, `day`, `time`, and `size`. Expected Output: - Your function should return a Matplotlib Figure object containing the required visualizations. Example Usage: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load example dataset tips = sns.load_dataset(\\"tips\\") # Call the visualization function fig = visualize_categorical_data(tips) # Show the plot fig.show() ``` Please make sure your code is well-documented and follows Python best practices.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_categorical_data(data: pd.DataFrame) -> plt.Figure: Creates a comprehensive and informative visualization of a given dataset using Seaborn to demonstrate various categorical plots. Parameters: data (pd.DataFrame): A pandas DataFrame containing the dataset to be visualized. Returns: plt.Figure: A Matplotlib Figure object containing the required visualizations. sns.set(style=\\"whitegrid\\") # Initialize the figure fig = plt.figure(figsize=(15, 10)) # Create a stripplot ax1 = fig.add_subplot(2, 2, 1) sns.stripplot(x=\'day\', y=\'total_bill\', hue=\'sex\', data=data, dodge=True, ax=ax1) ax1.set_title(\'Stripplot of Total Bill by Day\') # Create a boxplot ax2 = fig.add_subplot(2, 2, 2) sns.boxplot(x=\'day\', y=\'tip\', hue=\'smoker\', data=data, ax=ax2) ax2.set_title(\'Boxplot of Tip by Day and Smoker\') # Create a barplot ax3 = fig.add_subplot(2, 2, 3) sns.barplot(x=\'day\', y=\'total_bill\', hue=\'time\', data=data, ax=ax3, ci=None) ax3.set_title(\'Barplot of Total Bill by Day and Time\') # Create a facet grid of barplots g = sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"time\\", kind=\\"bar\\", data=data) g.set_titles(\\"Total Bill by Day and Time ({col_name})\\") return fig"},{"question":"# PyTorch Coding Assessment Question **Objective**: This task aims to test your understanding of PyTorch\'s `torch.Storage` and how to work with untyped storages in tensor operations. **Problem**: You need to implement a function called `replace_with_untyped_storage` that takes an input tensor and performs the following tasks: 1. Creates a copy of the tensor\'s untyped storage. 2. Modifies this copied storage to set all its bytes to zero. 3. Replaces the input tensor\'s storage with this modified storage without changing the tensor\'s shape, stride, and offset. **Function Signature**: ```python def replace_with_untyped_storage(t: torch.Tensor) -> torch.Tensor: pass ``` **Input**: - A `torch.Tensor` object `t` which is the tensor you need to modify. **Output**: - Returns a new `torch.Tensor` with the same shape, stride, and offset as the input tensor but with all elements set to zero, achieved by replacing the tensor\'s storage. **Constraints**: - Do not use high-level tensor methods like `torch.zeros_like`, `torch.clone`, or `torch.fill_` to achieve this. You must work with `torch.UntypedStorage` for this task. - Assume the input tensor `t` is on the CPU. **Example**: ```python import torch # Example Tensor t = torch.ones(3, dtype=torch.float32) # Function Call new_t = replace_with_untyped_storage(t) # Expected output # new_t should be a tensor with the same shape but all zeros print(new_t) # tensor([0., 0., 0.]) ``` **Hint**: - Use `t.untyped_storage()` to access the tensor\'s underlying storage. - Use `torch.UntypedStorage.clone()` to create a copy of the storage. - Use `torch.UntypedStorage.fill_()` to modify the content of the copied storage. - Use `t.set_` to replace the tensor\'s storage with the modified storage. **Note**: While it is generally not recommended to manipulate tensor storage directly due to potential risks and efficiency concerns, this exercise is designed to help you understand the relationship between tensors and their underlying storage in PyTorch.","solution":"import torch def replace_with_untyped_storage(t: torch.Tensor) -> torch.Tensor: # Create a copy of the tensor\'s untyped storage untyped_storage_copy = t.untyped_storage().clone() # Set all bytes in the copied storage to zero untyped_storage_copy.fill_(0) # Create a new tensor with the same properties but with the modified storage new_tensor = torch.empty(t.size(), dtype=t.dtype).set_(untyped_storage_copy, t.storage_offset(), t.size(), t.stride()) return new_tensor"},{"question":"# Custom Serialization and Deserialization with pickle Objective The goal of this exercise is to demonstrate your understanding of custom serialization and deserialization of Python objects using the `pickle` module. Problem Statement: You are tasked with implementing a serialization process for a Python `Person` class that includes name, age, and a list of `Pet` instances. The `Pet` class has attributes `name` and `type`. To optimize performance, you need to customize the serialization process to handle the objects, ensuring that the `Person` and `Pet` class instances are correctly serialized and deserialized using `pickle`. Implement the following: 1. Define the `Person` and `Pet` classes with the mentioned attributes. 2. Implement the methods `__getstate__()` and `__setstate__()`in the `Person` class to handle its state during pickling. 3. Implement custom `persistent_id` and `persistent_load` methods to manage the states of `Pet` instances during serialization and deserialization. 4. Write a function `serialize_person` to serialize a `Person` instance to a file. 5. Write a function `deserialize_person` to deserialize a `Person` instance from a file. Specifications: - Define the `Person` and `Pet` classes. - Implement `__getstate__()` and `__setstate__()` methods in the `Person` class. - Implement `persistent_id` and `persistent_load` methods for handling `Pet` instances. - Implement the `serialize_person(person: Person, file_path: str)` function. - Implement the `deserialize_person(file_path: str) -> Person` function. Example: ```python class Pet: def __init__(self, name, pet_type): self.name = name self.pet_type = pet_type class Person: def __init__(self, name, age, pets=None): self.name = name self.age = age self.pets = pets if pets else [] def __getstate__(self): state = self.__dict__.copy() # add custom serialization logic here return state def __setstate__(self, state): # add custom deserialization logic here self.__dict__.update(state) import pickle def serialize_person(person: Person, file_path: str): with open(file_path, \'wb\') as file: pickle.dump(person, file) def deserialize_person(file_path: str) -> Person: with open(file_path, \'rb\') as file: person = pickle.load(file) return person # Example usage: pet1 = Pet(\\"Buddy\\", \\"Dog\\") pet2 = Pet(\\"Mittens\\", \\"Cat\\") person = Person(\\"Alice\\", 30, [pet1, pet2]) serialize_person(person, \\"person.pkl\\") new_person = deserialize_person(\\"person.pkl\\") print(new_person.name, new_person.age) for pet in new_person.pets: print(pet.name, pet.pet_type) ``` Constraints: - You must use the `pickle` module for serialization and deserialization. - Ensure that the handling of `persistent_id` and `persistent_load` is well demonstrated. - Handle any potential errors during the serialization/deserialization process appropriately. Notes: - Submit code is expected to demonstrate the correct handling of object serialization and deserialization. - Focus on demonstrating your understanding of using custom serialization processes with `pickle`.","solution":"import pickle class Pet: def __init__(self, name, pet_type): self.name = name self.pet_type = pet_type class Person: def __init__(self, name, age, pets=None): self.name = name self.age = age self.pets = pets if pets else [] def __getstate__(self): state = self.__dict__.copy() return state def __setstate__(self, state): self.__dict__.update(state) def persistent_id(obj): if isinstance(obj, Pet): return (\'Pet\', obj.name, obj.pet_type) else: return None def persistent_load(persid): if persid[0] == \'Pet\': return Pet(persid[1], persid[2]) else: raise pickle.UnpicklingError(\\"unsupported persistent object\\") def serialize_person(person: Person, file_path: str) -> None: with open(file_path, \'wb\') as file: pickler = pickle.Pickler(file) pickler.persistent_id = persistent_id pickler.dump(person) def deserialize_person(file_path: str) -> Person: with open(file_path, \'rb\') as file: unpickler = pickle.Unpickler(file) unpickler.persistent_load = persistent_load person = unpickler.load() return person"},{"question":"# Problem: Custom Set Operations In this exercise, you will implement a custom set operation that combines several functionalities of sets and frozensets based on the provided API. Task Description: Define a class `CustomSetOps` that encapsulates the operations on sets and frozensets. Implement the following methods: 1. `__init__(self, iterable=None, immutable=False)`: - Initialize an instance with an iterable that is used to create either a set or a frozenset based on the `immutable` flag. - If `immutable` is `True`, initialize a frozenset; otherwise, initialize a mutable set. 2. `size(self)`: - Return the size of the underlying set or frozenset. 3. `add(self, element)`: - Add an element to the set if it is mutable. If the set is immutable, raise a `TypeError`. 4. `contains(self, element)`: - Return `True` if the element is in the set or frozenset; otherwise, return `False`. 5. `discard(self, element)`: - Remove the element from the set if it is mutable. If the set is immutable, raise a `TypeError`. 6. `clear(self)`: - Clear all elements from the set if it is mutable. If the set is immutable, raise a `TypeError`. Here is the class definition to guide you: ```python class CustomSetOps: def __init__(self, iterable=None, immutable=False): # Your code here def size(self): # Your code here def add(self, element): # Your code here def contains(self, element): # Your code here def discard(self, element): # Your code here def clear(self): # Your code here ``` Constraints: - The input iterable can contain any hashable Python objects. - You should handle the cases where the element is unhashable by raising `TypeError`. - Making operations efficient is desirable but not required for correctness. Example Usage: ```python # Create a mutable set cs = CustomSetOps([1, 2, 3]) print(cs.size()) # Output: 3 cs.add(4) print(cs.contains(4)) # Output: True cs.discard(2) print(cs.contains(2)) # Output: False cs.clear() print(cs.size()) # Output: 0 # Create an immutable frozenset fs = CustomSetOps([1, 2, 3], immutable=True) print(fs.size()) # Output: 3 try: fs.add(4) except TypeError as e: print(e) # Output: frozenset is immutable print(fs.contains(1)) # Output: True ``` Ensure that your implementation covers all the necessary set operations, handles errors gracefully, and complies with the provided constraints.","solution":"class CustomSetOps: def __init__(self, iterable=None, immutable=False): if iterable is None: iterable = [] self.immutable = immutable if immutable: self._set = frozenset(iterable) else: self._set = set(iterable) def size(self): return len(self._set) def add(self, element): if self.immutable: raise TypeError(\\"frozenset is immutable\\") self._set.add(element) def contains(self, element): return element in self._set def discard(self, element): if self.immutable: raise TypeError(\\"frozenset is immutable\\") self._set.discard(element) def clear(self): if self.immutable: raise TypeError(\\"frozenset is immutable\\") self._set.clear()"},{"question":"# Garbage Collector Simulation Using Reference Counting Objective Implement a simplified garbage collector simulation in Python that uses reference counting functions to manage the lifecycle of Python objects. Instructions 1. **Class Design**: - Create a class `PyObject` to simulate a Python object. - Implement methods to increase and decrease the reference count of an object using the provided reference counting functions. 2. **Reference Counting Functions**: - Implement your versions of `Py_INCREF`, `Py_DECREF`, `Py_NewRef`, `Py_XNewRef`, `Py_XINCREF`, `Py_XDECREF`, and `Py_CLEAR` within the Python class. 3. **Garbage Collection**: - Simulate the deallocation of objects when their reference count reaches zero. - Ensure that the simulation handles NULL (None) objects correctly. 4. **Testing**: - Write a few test cases to demonstrate the functionality of your garbage collector. Example Structure ```python class PyObject: def __init__(self): self.ref_count = 1 # New object starts with a reference count of 1 def Py_INCREF(self): Increment the reference count assert self is not None self.ref_count += 1 def Py_DECREF(self): Decrement the reference count and deallocate if zero assert self is not None self.ref_count -= 1 if self.ref_count == 0: self.deallocate() def Py_NewRef(self): Create a new strong reference and return the object self.Py_INCREF() return self def Py_XNewRef(self): Create a new strong reference, return None if object is NULL if self is not None: return self.Py_NewRef() return None def Py_XINCREF(self): Increment the reference count if object is not NULL if self is not None: self.Py_INCREF() def Py_XDECREF(self): Decrement the reference count if object is not NULL if self is not None: self.Py_DECREF() def Py_CLEAR(self): Clear the reference to the object and set it to NULL if self is not None: self.Py_DECREF() self = None def deallocate(self): Deallocate the object print(f\\"{self} is deallocated\\") # Test cases to verify the functionality obj1 = PyObject() obj1.Py_INCREF() obj1.Py_DECREF() obj1.Py_DECREF() # This should deallocate the object ``` Constraints - Ensure all functions manage the reference counts accurately. - Simulate deallocation by printing a message indicating the object is deallocated. - Handle NULL (None) objects without causing errors. Performance Requirements - Your implementation should correctly simulate the reference counting mechanism and deallocate objects when their reference count reaches zero. - Ensure that the program doesn\'t crash and handles NULL objects appropriately. Good luck!","solution":"class PyObject: def __init__(self): self.ref_count = 1 # New object starts with a reference count of 1 def Py_INCREF(self): Increment the reference count assert self is not None self.ref_count += 1 def Py_DECREF(self): Decrement the reference count and deallocate if zero assert self is not None self.ref_count -= 1 if self.ref_count == 0: self.deallocate() def Py_NewRef(self): Create a new strong reference and return the object self.Py_INCREF() return self def Py_XNewRef(self): Create a new strong reference, return None if object is NULL if self is not None: return self.Py_NewRef() return None def Py_XINCREF(self): Increment the reference count if object is not NULL if self is not None: self.Py_INCREF() def Py_XDECREF(self): Decrement the reference count if object is not NULL if self is not None: self.Py_DECREF() def Py_CLEAR(self): Clear the reference to the object and set it to NULL if self is not None: self.Py_DECREF() return None # Set to NULL by returning None def deallocate(self): Deallocate the object print(f\\"{self} is deallocated\\")"},{"question":"Coding Assessment Question **Objective:** Design a function that processes sales data to provide insights into monthly total sales, and the performance of each salesperson. You will utilize the pandas library to manipulate and analyze the data. This question will assess your understanding of pandas\' core functionalities, including DataFrame creation, data aggregation, and filtering. **Problem Statement:** You are given a CSV file named `sales_data.csv` which contains sales transactions with the following columns: - `Date`: The date of the sale in the format `YYYY-MM-DD`. - `Salesperson`: The name of the salesperson who made the sale. - `Amount`: The amount of the sale. Write a function `analyze_sales_data(file_path)` that reads this CSV file and performs the following tasks: 1. **Load the data** into a pandas DataFrame. 2. **Parse the date column** and set it as the index of the DataFrame. 3. **Aggregate the sales data** to calculate the total sales amount for each month. 4. **Identify the top salesperson** for each month based on the total sales amount. **Function Signature:** ```python def analyze_sales_data(file_path: str) -> tuple: Analyzes sales data from the provided CSV file. Parameters: file_path (str): The path to the sales data CSV file Returns: tuple: A tuple containing: - A pandas DataFrame with the monthly total sales amounts. - A pandas DataFrame with the top salesperson for each month and their sales amount. pass ``` **Constraints and Requirements:** - The function should handle missing data appropriately. - The sales data spans multiple years, so the output should reflect this (e.g., November 2022, December 2022, January 2023). - The function should use efficient pandas operations to ensure performance. **Expected Output:** - The first DataFrame should have the index as `Month-Year` (e.g., \\"2022-11\\") and a column `Total_Sales`. - The second DataFrame should have the index as `Month-Year` and columns `Top_Salesperson` and `Sales_Amount`. Example usage: ```python monthly_sales, top_salespersons = analyze_sales_data(\'sales_data.csv\') print(monthly_sales) # Output: # Total_Sales # Month-Year # 2022-11 5000 # 2022-12 6200 # 2023-01 7000 print(top_salespersons) # Output: # Top_Salesperson Sales_Amount # Month-Year # 2022-11 John 2000 # 2022-12 Jane 2500 # 2023-01 Alice 2800 ``` **Notes:** - You may assume that the CSV file is well-formed and does not contain any column names or types inconsistencies. - Include proper error handling and logging where appropriate.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> tuple: Analyzes sales data from the provided CSV file. Parameters: file_path (str): The path to the sales data CSV file Returns: tuple: A tuple containing: - A pandas DataFrame with the monthly total sales amounts. - A pandas DataFrame with the top salesperson for each month and their sales amount. # Load the data into a pandas DataFrame df = pd.read_csv(file_path) # Parse the date column and set it as the index of the DataFrame df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) # Aggregate the sales data to calculate the total sales amount for each month df[\'Month-Year\'] = df.index.to_period(\'M\') monthly_sales = df.groupby(\'Month-Year\')[\'Amount\'].sum().reset_index() monthly_sales.columns = [\'Month-Year\', \'Total_Sales\'] # Identify the top salesperson for each month based on the total sales amount monthly_salespersons = df.groupby([\'Month-Year\', \'Salesperson\'])[\'Amount\'].sum().reset_index() idx = monthly_salespersons.groupby(\'Month-Year\')[\'Amount\'].idxmax() top_salespersons = monthly_salespersons.loc[idx].reset_index(drop=True) top_salespersons.columns = [\'Month-Year\', \'Top_Salesperson\', \'Sales_Amount\'] # Ensure \'Month-Year\' is displayed as a string monthly_sales[\'Month-Year\'] = monthly_sales[\'Month-Year\'].astype(str) top_salespersons[\'Month-Year\'] = top_salespersons[\'Month-Year\'].astype(str) return monthly_sales, top_salespersons"},{"question":"You are given a task to manage and analyze the timeline of events using Python\'s `collections` and `datetime` modules. You need to design a system to track events and answer queries about the duration and count of certain types of events within given time periods. Task 1. **Event Tracking**: You will implement a class `EventTracker` that tracks events. 2. **Methods**: - `add_event`: Adds events with a timestamp. - `event_count`: Returns the count of events between two timestamps. - `total_duration`: Returns the total duration of events classified as \'duration\' events between two timestamps. Requirements - **Initialization**: - Instantiate `EventTracker` with no parameters. - **add_event**: - Inputs: `event_name` (str), `timestamp` (datetime object), `duration` (optional float, default=0). - Adds an event with the given `event_name` at the specified `timestamp`. If it is a \'duration\' event (indicated by non-zero `duration`), it should store the duration as well. - **event_count**: - Inputs: `start_timestamp` (datetime object), `end_timestamp` (datetime object). - Output: Returns the number of events that occurred between `start_timestamp` and `end_timestamp`. - **total_duration**: - Inputs: `start_timestamp` (datetime object), `end_timestamp` (datetime object). - Output: Returns the total duration of \'duration\' events that occurred between `start_timestamp` and `end_timestamp`. Example ```python from datetime import datetime, timedelta tracker = EventTracker() # Adding events tracker.add_event(\'login\', datetime(2023, 10, 1, 10, 0)) tracker.add_event(\'logout\', datetime(2023, 10, 1, 11, 0)) tracker.add_event(\'session\', datetime(2023, 10, 1, 10, 0), duration=3600) # Querying event count print(tracker.event_count(datetime(2023, 10, 1, 9, 0), datetime(2023, 10, 1, 12, 0))) # Output: 3 # Querying total duration of \'session\' events print(tracker.total_duration(datetime(2023, 10, 1, 9, 0), datetime(2023, 10, 1, 12, 0))) # Output: 3600.0 ``` Notes and Constraints: - Utilize `collections` to efficiently store and handle events. Consider structures such as `defaultdict` or `deque`. - Ensure the solution efficiently handles a large number of events. - Handle edge cases such as overlapping timestamps and non-existent events gracefully. *Use the above guidelines and assumptions detailed in the documentation to build and test your class implementation.*","solution":"from collections import defaultdict, deque from datetime import datetime class EventTracker: def __init__(self): self.events = defaultdict(deque) self.duration_events = deque() def add_event(self, event_name, timestamp, duration=0): self.events[event_name].append(timestamp) if duration > 0: self.duration_events.append((timestamp, duration)) def event_count(self, start_timestamp, end_timestamp): count = 0 for event_list in self.events.values(): for event in event_list: if start_timestamp <= event <= end_timestamp: count += 1 return count def total_duration(self, start_timestamp, end_timestamp): total_duration = 0 for timestamp, duration in self.duration_events: if start_timestamp <= timestamp <= end_timestamp: total_duration += duration return total_duration"},{"question":"# PyTorch Tensor Operations and Attributes **Objective:** Implement a function to perform specific arithmetic operations on PyTorch tensors while managing their attributes such as dtype and device. **Function Definition:** ```python def perform_tensor_operations(a: torch.Tensor, b: torch.Tensor, operation: str) -> torch.Tensor: Performs the specified arithmetic operation on two input tensors `a` and `b`. The result tensor should be of the minimum necessary dtype that can accommodate both input tensors and must reside on the device of tensor `a`. Args: a (torch.Tensor): The first input tensor. b (torch.Tensor): The second input tensor. operation (str): The arithmetic operation to perform (\'add\', \'sub\', \'mul\', \'div\'). Returns: torch.Tensor: The result tensor. Raises: ValueError: If an unsupported operation is provided. pass ``` **Input:** - `a` (torch.Tensor): The first input tensor. - `b` (torch.Tensor): The second input tensor. - `operation` (str): One of the following arithmetic operations - \'add\', \'sub\', \'mul\', \'div\'. **Output:** - A PyTorch tensor that is the result of the specified arithmetic operation between `a` and `b`, with the following attributes: - The dtype of the result tensor should follow PyTorch\'s dtype promotion rules, ensuring it can accommodate both input tensors. - The result tensor should reside on the same device as tensor `a`. **Constraints:** - The tensors `a` and `b` may have different dtypes and may reside on different devices. - Ensure that unsupported operations raise a `ValueError`. # Example Usage: ```python import torch # Example usage for addition a = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32, device=\'cuda:0\') b = torch.tensor([1, 2, 3], dtype=torch.int32, device=\'cuda:1\') result = perform_tensor_operations(a, b, \'add\') # Ensure the result is on the same device as \'a\' assert result.device == a.device # Ensure the result dtype is correct assert result.dtype == torch.float32 # Example usage for subtraction a = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32, device=\'cpu\') b = torch.tensor([1, 2, 3], dtype=torch.int32, device=\'cpu\') result = perform_tensor_operations(a, b, \'sub\') # Ensure the result is on the same device as \'a\' assert result.device == a.device # Ensure the result dtype is correct assert result.dtype == torch.float32 ``` The function should handle the conversion of `b` to the appropriate device if needed and ensure the correct dtype promotion for the operation.","solution":"import torch def perform_tensor_operations(a: torch.Tensor, b: torch.Tensor, operation: str) -> torch.Tensor: Performs the specified arithmetic operation on two input tensors `a` and `b`. The result tensor should be of the minimum necessary dtype that can accommodate both input tensors and must reside on the device of tensor `a`. Args: a (torch.Tensor): The first input tensor. b (torch.Tensor): The second input tensor. operation (str): The arithmetic operation to perform (\'add\', \'sub\', \'mul\', \'div\'). Returns: torch.Tensor: The result tensor. Raises: ValueError: If an unsupported operation is provided. # Ensure b is on the same device as a if b.device != a.device: b = b.to(a.device) # Determine the result dtype based on PyTorch\'s type promotion rules result_dtype = torch.promote_types(a.dtype, b.dtype) # Perform the operation if operation == \'add\': result = a.to(result_dtype) + b.to(result_dtype) elif operation == \'sub\': result = a.to(result_dtype) - b.to(result_dtype) elif operation == \'mul\': result = a.to(result_dtype) * b.to(result_dtype) elif operation == \'div\': result = a.to(result_dtype) / b.to(result_dtype) else: raise ValueError(f\\"Unsupported operation: {operation}\\") return result"},{"question":"# Python Coding Assessment Question Objective: Your task is to create a function that compiles a list of Python source files into byte-code files. You should handle various parameters provided to fine-tune the compilation process. Additionally, implement a command-line interface to make use of your function. Function Specification: **Function Name**: `compile_python_files` **Parameters**: 1. `files`: A list of paths to Python source files (e.g., [\\"file1.py\\", \\"file2.py\\"]). 2. `output_dir`: A directory where the byte-code files should be saved. If not specified, the default directory should follow PEP 3147/488 conventions. 3. `doraise`: A boolean flag to dictate whether exceptions should be raised on compile errors. Default is `False`. 4. `optimize`: An integer specifying the optimization level (default is `-1`). 5. `invalidation_mode`: A string specifying the invalidation mode. Possible values: `\\"TIMESTAMP\\"`, `\\"CHECKED_HASH\\"`, `\\"UNCHECKED_HASH\\"` (default is `\\"TIMESTAMP\\"`). 6. `quiet`: An integer that specifies error messaging behavior (default is `0`). **Returns**: - A dictionary with source file paths as keys and the paths to the corresponding byte-code files (or `None` if compilation failed) as values. **Constraints**: - If the source file does not exist, handle the exception gracefully and continue with other files. - Raise `ValueError` if the `invalidation_mode` is not among the valid values. **Performance Requirements**: - Efficiently handle the compilation process for up to 100 files. Command-Line Interface Specification: Implement a command-line interface using the `argparse` module to allow users to compile files by providing a list of source files and optional parameters for output directory, optimization level, invalidation mode, and error handling. **Usage**: ``` python your_script.py <files> [--output_dir OUTPUT_DIR] [--doraise] [--optimize OPTIMIZE] [--invalidation_mode MODE] [--quiet QUIET] ``` **Example Call**: ``` python your_script.py file1.py file2.py --output_dir compiled_files --optimize 2 --invalidation_mode CHECKED_HASH --quiet 2 ``` Example: ```python from py_compile import compile as py_compile, PyCompileError, PycInvalidationMode import os def compile_python_files(files, output_dir=None, doraise=False, optimize=-1, invalidation_mode=\\"TIMESTAMP\\", quiet=0): invalidation_modes = { \\"TIMESTAMP\\": PycInvalidationMode.TIMESTAMP, \\"CHECKED_HASH\\": PycInvalidationMode.CHECKED_HASH, \\"UNCHECKED_HASH\\": PycInvalidationMode.UNCHECKED_HASH } if invalidation_mode not in invalidation_modes: raise ValueError(\\"Invalid invalidation_mode\\") results = {} for file in files: try: cfile_path = None if not output_dir else os.path.join(output_dir, f\\"{os.path.splitext(os.path.basename(file))[0]}.pyc\\") result = py_compile(file, cfile=cfile_path, doraise=doraise, optimize=optimize, invalidation_mode=invalidation_modes[invalidation_mode], quiet=quiet) results[file] = result except (PyCompileError, FileExistsError, OSError) as e: if doraise: raise e else: results[file] = None return results if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser(description=\\"Compile Python source files to byte-code.\\") parser.add_argument(\\"files\\", nargs=\\"+\\", help=\\"List of source files to compile.\\") parser.add_argument(\\"--output_dir\\", default=None, help=\\"Directory to save the compiled byte-code files.\\") parser.add_argument(\\"--doraise\\", action=\\"store_true\\", help=\\"Raise exceptions on compilation errors.\\") parser.add_argument(\\"--optimize\\", type=int, default=-1, help=\\"Optimization level.\\") parser.add_argument(\\"--invalidation_mode\\", default=\\"TIMESTAMP\\", choices=[\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"], help=\\"Bytecode invalidation mode.\\") parser.add_argument(\\"--quiet\\", type=int, default=0, help=\\"Silent errors (0, 1, or 2).\\") args = parser.parse_args() results = compile_python_files(args.files, args.output_dir, args.doraise, args.optimize, args.invalidation_mode, args.quiet) print(results) ``` **Instructions for Students**: 1. Implement the function `compile_python_files` as specified. 2. Implement the command-line interface in the `if __name__ == \\"__main__\\":` block. 3. Test your implementation with different combinations of parameters. 4. Handle all specified constraints and ensure your code is robust against invalid inputs.","solution":"import py_compile import os from py_compile import PyCompileError, PycInvalidationMode def compile_python_files(files, output_dir=None, doraise=False, optimize=-1, invalidation_mode=\\"TIMESTAMP\\", quiet=0): invalidation_modes = { \\"TIMESTAMP\\": PycInvalidationMode.TIMESTAMP, \\"CHECKED_HASH\\": PycInvalidationMode.CHECKED_HASH, \\"UNCHECKED_HASH\\": PycInvalidationMode.UNCHECKED_HASH } if invalidation_mode not in invalidation_modes: raise ValueError(\\"Invalid invalidation_mode\\") results = {} for file in files: try: cfile_path = None if output_dir: os.makedirs(output_dir, exist_ok=True) cfile_path = os.path.join(output_dir, f\\"{os.path.splitext(os.path.basename(file))[0]}.pyc\\") result = py_compile.compile(file, cfile=cfile_path, doraise=doraise, optimize=optimize, invalidation_mode=invalidation_modes[invalidation_mode], quiet=quiet) results[file] = result except (PyCompileError, FileExistsError, OSError) as e: if doraise: raise e else: results[file] = None return results if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser(description=\\"Compile Python source files to byte-code.\\") parser.add_argument(\\"files\\", nargs=\\"+\\", help=\\"List of source files to compile.\\") parser.add_argument(\\"--output_dir\\", default=None, help=\\"Directory to save the compiled byte-code files.\\") parser.add_argument(\\"--doraise\\", action=\\"store_true\\", help=\\"Raise exceptions on compilation errors.\\") parser.add_argument(\\"--optimize\\", type=int, default=-1, help=\\"Optimization level.\\") parser.add_argument(\\"--invalidation_mode\\", default=\\"TIMESTAMP\\", choices=[\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"], help=\\"Bytecode invalidation mode.\\") parser.add_argument(\\"--quiet\\", type=int, default=0, help=\\"Silent errors (0, 1, or 2).\\") args = parser.parse_args() results = compile_python_files(args.files, args.output_dir, args.doraise, args.optimize, args.invalidation_mode, args.quiet) print(results)"},{"question":"**Coding Question:** You are required to create a scheduling function that runs a specific task at fixed intervals over a certain duration. The task should be an arbitrary function passed as an argument. Your implementation should use the functionalities provided by the `time` module. # Function Specification `schedule_task` **Input:** - `task`: A function that takes no arguments and returns nothing. This function represents the task to be executed periodically. - `interval`: A positive float representing the interval in seconds at which the task should be executed. - `duration`: A positive float representing the total duration in seconds for which the task should be scheduled to run. **Output:** - None **Constraints:** - The `task` function must run at least once immediately when `schedule_task` is called. Thereafter, it should run at fixed intervals specified by `interval`. - If `task` takes longer to execute than `interval`, it should start execution again immediately without waiting. - The entire scheduling should complete within `duration` seconds from the initial call. # Example ```python import time def sample_task(): print(f\\"Task executed at {time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime())}\\") schedule_task(sample_task, 5, 20) ``` Explanation In the above example: 1. The function `sample_task` will be scheduled to run immediately and then every 5 seconds. 2. The total duration for which the `sample_task` will be executed is 20 seconds. 3. The output will show timestamped messages every 5 seconds for 20 seconds, starting immediately. # Notes: - Ensure your function handles exceptions within the `task` function gracefully without stopping the entire scheduling. - Use the `time.time()` function to keep track of the elapsed time and manage the scheduling accurately. **Your implementation should demonstrate an understanding of:** - Managing time intervals. - Precision timing and handling floating-point numbers. - Function scheduling and exception handling.","solution":"import time def schedule_task(task, interval, duration): Schedules the given task to run immediately and at fixed intervals for the specified duration. Args: - task (function): The function to be executed. - interval (float): The time interval in seconds between successive executions of the task. - duration (float): The total duration in seconds for which the task should be scheduled to run. start_time = time.time() end_time = start_time + duration next_run_time = start_time while time.time() < end_time: try: current_time = time.time() if current_time >= next_run_time: task() next_run_time = current_time + interval except Exception as e: print(f\\"An error occurred when running the task: {e}\\") # Sleep for a short time to prevent a busy wait loop time.sleep(0.01)"},{"question":"# Python Coding Assessment Question Email Message Parser and Serializer You are required to implement a function that processes and manipulates email messages using the `EmailMessage` class from the `email.message` module. **Objective:** 1. **Parse** a raw email string. 2. **Add** a custom header to the email message. 3. **Set the Content-Type** to `multipart/mixed` and add an attachment. 4. **Serialize** the modified email back to a string and return it. **Function Signature:** ```python def process_email(raw_email: str, custom_header: str, custom_value: str, attachment_content: bytes, attachment_filename: str) -> str: pass ``` **Input Format:** - `raw_email` (str): The raw email message as a string. - `custom_header` (str): The name of the custom header to be added. - `custom_value` (str): The value of the custom header to be added. - `attachment_content` (bytes): The content of the attachment to be added. - `attachment_filename` (str): The filename of the attachment to be added. **Output Format:** - Return the modified email message as a string. **Steps:** 1. Parse the `raw_email` string into an `EmailMessage` object. 2. Add the custom header (`custom_header`: `custom_value`) to the email message. 3. If the email message is not already a `multipart/mixed` type, convert it to `multipart/mixed`. 4. Add the attachment to the email message with the specified content and filename. 5. Serialize the modified email message back to a string and return it. **Constraints:** - The raw email message is formatted correctly according to RFC 5322 standards. - The custom header name and value are ASCII strings. - The attachment content is a bytes object and should be properly handled as an attachment. **Example:** ```python raw_email = From: sender@example.com To: recipient@example.com Subject: Test Email This is a test email message. custom_header = \\"X-Custom-Header\\" custom_value = \\"CustomValue\\" attachment_content = b\\"Attachment content here\\" attachment_filename = \\"attachment.txt\\" modified_email = process_email(raw_email, custom_header, custom_value, attachment_content, attachment_filename) print(modified_email) ``` **Additional Requirements:** 1. Use the `email.message.EmailMessage` class to implement the functionality. 2. Ensure the email follows standard MIME formatting after modification.","solution":"from email.message import EmailMessage from email import policy from email.parser import Parser from email.mime.base import MIMEBase from email import encoders def process_email(raw_email: str, custom_header: str, custom_value: str, attachment_content: bytes, attachment_filename: str) -> str: # Parse the raw email into an EmailMessage object parser = Parser(policy=policy.default) email = parser.parsestr(raw_email) # Add custom header email[custom_header] = custom_value # Set Content-Type to multipart/mixed if not already if email.get_content_type() != \'multipart/mixed\': if email.is_multipart(): new_email = EmailMessage() new_email[\'From\'] = email[\'From\'] new_email[\'To\'] = email[\'To\'] new_email[\'Subject\'] = email[\'Subject\'] for part in email.iter_parts(): new_email.attach(part) else: new_email = EmailMessage() for header, value in email.items(): new_email[header] = value new_email.set_content(email.get_content()) email = new_email email.make_mixed() # Add attachment attachment = MIMEBase(\'application\', \'octet-stream\') attachment.set_payload(attachment_content) encoders.encode_base64(attachment) attachment.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{attachment_filename}\\"\') email.attach(attachment) # Serialize to string and return return email.as_string()"},{"question":"Objective In this exercise, you will implement two functions using the `marshal` module to serialize and deserialize a series of python objects. Problem Statement You need to create two functions - `serialize_objects` and `deserialize_objects`. 1. `serialize_objects`: This function will accept a list of Python objects and serialize them into a single binary file. 2. `deserialize_objects`: This function will read the serialized objects from the file and return them as a list. Function Specifications - `serialize_objects(objects, filename, version=marshal.version) -> None` - **Input:** - `objects` (List): A list of supported Python objects. - `filename` (String): The name of the file where the objects should be serialized. - `version` (Optional, Integer): The version of the marshaling protocol to use. Default is the current version. - **Output:** None - `deserialize_objects(filename) -> List` - **Input:** - `filename` (String): The name of the file from where the objects should be deserialized. - **Output:** - A list of Python objects that were deserialized from the file. Constraints - You can assume that all objects in the list are of types supported by the `marshal` module. - The file to which objects are serialized should be readable and writable in binary mode. - Handle any exceptions that might occur due to unsupported types or file I/O errors. Example ```python example_objects = [42, \\"Hello, World!\\", 3.14, True, None, [1, 2, 3]] serialize_objects(example_objects, \\"example.marshal\\") restored_objects = deserialize_objects(\\"example.marshal\\") print(restored_objects) # Output should be similar to: [42, \\"Hello, World!\\", 3.14, True, None, [1, 2, 3]] ``` Implementation 1. **Serialize Function:** - Open the specified file in binary write (`\'wb\'`) mode. - Use the `marshal.dump` method to write each object to the file. 2. **Deserialize Function:** - Open the specified file in binary read (`\'rb\'`) mode. - Use the `marshal.load` method to read each object from the file until `EOFError` is raised. You are required to write these functions and ensure they handle any errors appropriately. Test your functions with various types of objects to make sure they work as expected.","solution":"import marshal def serialize_objects(objects, filename, version=marshal.version): Serialize a list of Python objects to a file. :param objects: List of Python objects to be serialized. :param filename: The name of the file to which the objects will be serialized. :param version: The marshaling protocol version to use. with open(filename, \'wb\') as file: for obj in objects: marshal.dump(obj, file, version) def deserialize_objects(filename): Deserialize Python objects from a file. :param filename: The name of the file from which to deserialize the objects. :return: A list of Python objects that were deserialized from the file. objects = [] with open(filename, \'rb\') as file: try: while True: objects.append(marshal.load(file)) except EOFError: pass return objects"},{"question":"**Objective**: Demonstrate your understanding of pandas\' resampling capabilities on time series data. **Problem Statement**: You are given a time series dataset representing the daily closing stock prices of a particular company over a period of one year. Your task is to perform several resampling operations on this dataset to analyze the stock prices at different time frequencies and apply several computations to these resampled datasets. **Data**: Assume you have the following pandas DataFrame `df`: ```plaintext Date Close 0 2022-01-01 150.23 1 2022-01-02 152.35 2 2022-01-03 153.47 ... 364 2022-12-31 180.56 ``` The `Date` column is in `datetime` format, and the `Close` column represents the closing price of the stock on that day. **Tasks**: 1. **Monthly Resampling**: - Resample the data to obtain the monthly closing price mean. - Compute the standard deviation of the monthly closing prices. 2. **Quarterly Resampling**: - Resample the data to obtain the first closing price of each quarter. - Compute the quantiles (0.25, 0.5, 0.75) of the quarterly closing prices. 3. **Weekly Resampling**: - Resample the data to obtain the weekly maximum closing price. - Fill in the missing values in the weekly resampled data using forward-fill method. 4. **Custom Resampling**: - Resample the data by every 10 days and obtain the sum of the closing prices in each interval. - Plot the resulting resampled data using matplotlib. **Constraints**: - You should assume that the DataFrame `df` is already sorted by the `Date` column. - Use appropriate pandas functions to handle datetime indexing and resampling. - Handle any missing data appropriately as per the given tasks. **Expected Output**: You should implement a function `analyze_stock_data(df)` that performs the above tasks and returns a dictionary with the results as specified. ```python def analyze_stock_data(df): # Perform monthly resampling and calculate mean and std deviation monthly_mean = ... monthly_std = ... # Perform quarterly resampling and calculate first closing price and quantiles quarterly_first = ... quarterly_quantiles = ... # Perform weekly resampling and calculate max closing price and forward-fill weekly_max = ... weekly_ffill = ... # Perform custom resampling every 10 days and sum closing prices custom_10d_sum = ... # Plotting the custom resampled data plt.figure(figsize=(10, 5)) custom_10d_sum.plot(kind=\'bar\') plt.title(\'Sum of Closing Prices every 10 Days\') plt.xlabel(\'Date\') plt.ylabel(\'Sum of Closing Prices\') plt.show() return { \\"monthly_mean\\": monthly_mean, \\"monthly_std\\": monthly_std, \\"quarterly_first\\": quarterly_first, \\"quarterly_quantiles\\": quarterly_quantiles, \\"weekly_max\\": weekly_max, \\"weekly_ffill\\": weekly_ffill, \\"custom_10d_sum_plot\\": \'Plot Displayed\' } ``` **Note**: Make sure the resulting dictionary contains keys and values corresponding to each of the tasks specified above. The function should execute and output the desired results and plots without errors.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_stock_data(df): # Resample the data to monthly frequency and calculate mean and std deviation monthly_resampled = df.resample(\'M\', on=\'Date\')[\'Close\'] monthly_mean = monthly_resampled.mean() monthly_std = monthly_resampled.std() # Resample the data to quarterly frequency and calculate first closing price and quantiles quarterly_resampled = df.resample(\'Q\', on=\'Date\')[\'Close\'] quarterly_first = quarterly_resampled.first() quarterly_quantiles = quarterly_resampled.quantile([0.25, 0.5, 0.75]) # Resample the data to weekly frequency and calculate max closing price and forward-fill weekly_resampled = df.resample(\'W\', on=\'Date\')[\'Close\'] weekly_max = weekly_resampled.max() weekly_ffill = weekly_resampled.ffill() # Resample the data every 10 days and sum closing prices custom_10d_resampled = df.resample(\'10D\', on=\'Date\')[\'Close\'] custom_10d_sum = custom_10d_resampled.sum() # Plot the custom resampled data plt.figure(figsize=(10, 5)) custom_10d_sum.plot(kind=\'bar\') plt.title(\'Sum of Closing Prices every 10 Days\') plt.xlabel(\'Date\') plt.ylabel(\'Sum of Closing Prices\') plt.show() return { \\"monthly_mean\\": monthly_mean, \\"monthly_std\\": monthly_std, \\"quarterly_first\\": quarterly_first, \\"quarterly_quantiles\\": quarterly_quantiles, \\"weekly_max\\": weekly_max, \\"weekly_ffill\\": weekly_ffill, \\"custom_10d_sum_plot\\": \'Plot Displayed\' }"},{"question":"**Objective:** To evaluate the understanding of scikit-learn\'s scoring metrics, custom scoring functions, and their application in model evaluation and comparison in a machine learning context. **Problem Statement:** You are given a dataset of house prices with various features and the task is to build a regression model. The dataset is split into training and test sets. Your tasks are as follows: 1. **Train a regression model**: Train a `GradientBoostingRegressor` on the training data. 2. **Create a custom scoring function**: Define a custom scoring function to evaluate the performance of the regression model using the `mean_pinball_loss` metric with `alpha=0.95`. 3. **Evaluate the model**: Use the custom scoring function to evaluate the model on the test set. 4. **Compare with standard metrics**: Evaluate the model using the standard `mean_absolute_error` and `r2_score` metrics for comparison. **Dataset format:** - `X_train`: Features of the training set, a 2D array of shape `(n_samples, n_features)`. - `y_train`: Target values for the training set, a 1D array of shape `(n_samples,)`. - `X_test`: Features of the test set, a 2D array of shape `(n_samples, n_features)`. - `y_test`: Target values for the test set, a 1D array of shape `(n_samples,)`. **Function signatures:** Please implement the following functions: ```python from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_pinball_loss, mean_absolute_error, r2_score, make_scorer def train_regression_model(X_train: np.ndarray, y_train: np.ndarray) -> GradientBoostingRegressor: Train a GradientBoostingRegressor on the training data. Parameters: - X_train: numpy.ndarray, shape (n_samples, n_features), the training features. - y_train: numpy.ndarray, shape (n_samples,), the target values for training. Returns: - model: GradientBoostingRegressor trained on the input data. pass def custom_scorer() -> callable: Create a custom scoring function using mean_pinball_loss with alpha=0.95. Returns: - scorer: callable, a scoring function for model evaluation. pass def evaluate_model(model: GradientBoostingRegressor, X_test: np.ndarray, y_test: np.ndarray, scorer: callable) -> float: Evaluate the model using the custom scoring function on the test data. Parameters: - model: GradientBoostingRegressor, the trained regression model. - X_test: numpy.ndarray, shape (n_samples, n_features), the test features. - y_test: numpy.ndarray, shape (n_samples,), the target values for testing. - scorer: callable, a custom scoring function. Returns: - score: float, the score from the custom scoring function. pass def compare_with_standard_metrics(model: GradientBoostingRegressor, X_test: np.ndarray, y_test: np.ndarray) -> dict: Evaluate the model using mean_absolute_error and r2_score for comparison. Parameters: - model: GradientBoostingRegressor, the trained regression model. - X_test: numpy.ndarray, shape (n_samples, n_features), the test features. - y_test: numpy.ndarray, shape (n_samples,), the target values for testing. Returns: - metrics: dict, containing \'mean_absolute_error\' and \'r2_score\' as keys and their corresponding scores as values. pass ``` **Constraints:** - Use the prescribed metrics and model. - Ensure the functions handle edge cases. - The custom scorer should use `mean_pinball_loss` with `alpha=0.95`. **Performance Requirements:** - The model training should handle datasets with up to 10000 samples efficiently. - The evaluation should be performed in a reasonable time (under 5 seconds for 10000 samples). **Dataset Example:** ```python # Example dataset X_train = np.random.rand(800, 10) y_train = np.random.rand(800) X_test = np.random.rand(200, 10) y_test = np.random.rand(200) ``` **Example Usage:** ```python model = train_regression_model(X_train, y_train) scorer = custom_scorer() custom_score = evaluate_model(model, X_test, y_test, scorer) standard_metrics = compare_with_standard_metrics(model, X_test, y_test) print(f\\"Custom Pinball Loss Score: {custom_score}\\") print(f\\"Standard Metrics: {standard_metrics}\\") ```","solution":"import numpy as np from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_pinball_loss, mean_absolute_error, r2_score, make_scorer def train_regression_model(X_train: np.ndarray, y_train: np.ndarray) -> GradientBoostingRegressor: Train a GradientBoostingRegressor on the training data. Parameters: - X_train: numpy.ndarray, shape (n_samples, n_features), the training features. - y_train: numpy.ndarray, shape (n_samples,), the target values for training. Returns: - model: GradientBoostingRegressor trained on the input data. model = GradientBoostingRegressor() model.fit(X_train, y_train) return model def custom_scorer() -> callable: Create a custom scoring function using mean_pinball_loss with alpha=0.95. Returns: - scorer: callable, a scoring function for model evaluation. return make_scorer(mean_pinball_loss, alpha=0.95, greater_is_better=False) def evaluate_model(model: GradientBoostingRegressor, X_test: np.ndarray, y_test: np.ndarray, scorer: callable) -> float: Evaluate the model using the custom scoring function on the test data. Parameters: - model: GradientBoostingRegressor, the trained regression model. - X_test: numpy.ndarray, shape (n_samples, n_features), the test features. - y_test: numpy.ndarray, shape (n_samples,), the target values for testing. - scorer: callable, a custom scoring function. Returns: - score: float, the score from the custom scoring function. score = scorer(model, X_test, y_test) return score def compare_with_standard_metrics(model: GradientBoostingRegressor, X_test: np.ndarray, y_test: np.ndarray) -> dict: Evaluate the model using mean_absolute_error and r2_score for comparison. Parameters: - model: GradientBoostingRegressor, the trained regression model. - X_test: numpy.ndarray, shape (n_samples, n_features), the test features. - y_test: numpy.ndarray, shape (n_samples,), the target values for testing. Returns: - metrics: dict, containing \'mean_absolute_error\' and \'r2_score\' as keys and their corresponding scores as values. y_pred = model.predict(X_test) metrics = { \'mean_absolute_error\': mean_absolute_error(y_test, y_pred), \'r2_score\': r2_score(y_test, y_pred) } return metrics"},{"question":"# Problem Description You are provided with a dataset containing information about various products, including their prices, categories, and ratings. Your task is to visualize this data using seaborn to answer specific questions about the distribution and relationships between these variables. # Dataset Let\'s assume you have a Pandas DataFrame `df` with the following columns: - `product_id` (string): Unique identifier for each product - `category` (string): Category to which the product belongs (e.g., \\"Electronics\\", \\"Clothing\\", \\"Home Appliances\\") - `price` (float): Price of the product - `rating` (float): Customer rating of the product (between 1.0 and 5.0) - `in_stock` (boolean): Whether the product is in stock # Tasks 1. **Category Price Distribution:** - Create a box plot to visualize the distribution of product prices within each category. The x-axis should represent different categories, and the y-axis should represent product prices. - Add a `hue` parameter to this box plot to differentiate between products that are in stock and those that are not. 2. **Rating vs. Price Relationship:** - Create a point plot to visualize the relationship between product ratings and prices. The x-axis should represent product ratings (rounded to the nearest integer), and the y-axis should represent the average price of products with that rating. - Add error bars to represent the 95% confidence interval of the mean price estimates. 3. **Product Count by Category:** - Create a count plot to show the number of products in each category. The x-axis should represent different categories, and the y-axis should represent the number of products. - Add a `hue` parameter to differentiate between products that are in stock and those that are not. 4. **Price Distribution Faceted by Rating:** - Create a facet grid with violin plots to show the distribution of product prices within each category. Each subplot should represent a different rating (rounded to the nearest integer). The x-axis should represent different categories, and the y-axis should represent product prices. # Code Requirements Implement the tasks using the seaborn library. Your code should be clean and well-documented. Use comments to explain each step of the process. **Input:** - DataFrame `df` as described above. **Output:** - Four seaborn plots as described in the tasks. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"ticks\\", color_codes=True) # Sample DataFrame creation (Assume actual data to be used) data = { \'product_id\': [\'p1\', \'p2\', \'p3\', \'p4\', \'p5\', \'p6\', \'p7\', \'p8\', \'p9\', \'p10\'], \'category\': [\'Electronics\', \'Clothing\', \'Electronics\', \'Home Appliances\', \'Clothing\', \'Home Appliances\', \'Electronics\', \'Electronics\', \'Home Appliances\', \'Clothing\'], \'price\': [299.99, 49.99, 399.99, 89.99, 29.99, 150.00, 89.99, 299.99, 400.00, 30.00], \'rating\': [4.5, 3.8, 4.9, 4.2, 3.6, 4.0, 4.1, 4.5, 4.8, 2.0], \'in_stock\': [True, False, True, True, False, True, True, True, False, True] } df = pd.DataFrame(data) # 1. Category Price Distribution plt.figure(figsize=(10, 6)) sns.boxplot(data=df, x=\'category\', y=\'price\', hue=\'in_stock\') plt.title(\'Price Distribution by Category\') plt.show() # 2. Rating vs. Price Relationship df[\'rating_rounded\'] = df[\'rating\'].round() plt.figure(figsize=(10, 6)) sns.pointplot(data=df, x=\'rating_rounded\', y=\'price\', ci=95) plt.title(\'Average Price by Rating\') plt.show() # 3. Product Count by Category plt.figure(figsize=(10, 6)) sns.countplot(data=df, x=\'category\', hue=\'in_stock\') plt.title(\'Product Count by Category\') plt.show() # 4. Price Distribution Faceted by Rating g = sns.catplot( data=df, x=\'category\', y=\'price\', hue=\'in_stock\', kind=\'violin\', col=\'rating_rounded\', col_wrap=3, ) g.set_titles(\\"Price Distribution by Category for Rating {col_name}\\") plt.show() ``` Please replace the sample DataFrame with the actual data for analysis. # Constraints 1. The dataset `df` is assumed to be preloaded and available. 2. Use seaborn version 0.11.0 or higher. # Additional Notes 1. Ensure that your plots are clear and labels are appropriately set for readability. 2. You may customize the plots further to enhance their readability and presentation. 3. Test your code to ensure it runs without errors and produces the intended visualizations.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_product_data(df): Creates visualizations for the given product dataframe. Parameters: df (DataFrame): Input dataframe containing product data with columns [\'product_id\', \'category\', \'price\', \'rating\', \'in_stock\'] Returns: None: The function will display the plots. sns.set_theme(style=\\"ticks\\", color_codes=True) # 1. Category Price Distribution plt.figure(figsize=(10, 6)) sns.boxplot(data=df, x=\'category\', y=\'price\', hue=\'in_stock\') plt.title(\'Price Distribution by Category\') plt.show() # 2. Rating vs. Price Relationship df[\'rating_rounded\'] = df[\'rating\'].round() plt.figure(figsize=(10, 6)) sns.pointplot(data=df, x=\'rating_rounded\', y=\'price\', ci=95, estimator=\'mean\') plt.title(\'Average Price by Rating\') plt.show() # 3. Product Count by Category plt.figure(figsize=(10, 6)) sns.countplot(data=df, x=\'category\', hue=\'in_stock\') plt.title(\'Product Count by Category\') plt.show() # 4. Price Distribution Faceted by Rating g = sns.catplot( data=df, x=\'category\', y=\'price\', hue=\'in_stock\', kind=\'violin\', col=\'rating_rounded\', col_wrap=3, ) g.set_titles(\\"Price Distribution by Category for Rating {col_name}\\") plt.show()"},{"question":"# Question: Analyzing and Visualizing Health Expenditure Data You are provided with a dataset on health expenditures by country across different years. Using seaborns `objects` interface, create a plot that visualizes this data. Your task is to perform the following steps: 1. **Load the health expenditure dataset** called `\\"healthexp\\"` using `seaborn.load_dataset`. 2. **Create a line plot** that shows the health expenditure (y-axis) over the years (x-axis) for each country. 3. **Normalize the expenditures** for each country relative to the maximum value within each country. 4. **Constrain the normalization baseline** to the year with the minimum value (i.e., the earliest year in the dataset). 5. **Scale the output as a percent change** from that baseline year. 6. **Label the y-axis** to indicate it represents the percent change from the baseline. # Expected Input/Output - **Input**: - None directly, but the `\\"healthexp\\"` data set is used within the code. - **Output**: - A plot that visualizes the percentage change of health expenditure from the baseline year for each country over time. # Constraints and Requirements - You must use Seaborns `objects` interface (`so.Plot`, `so.Lines`, `so.Norm`). - Use the provided methods (`add()`, `.label()`) to accomplish the task. - Ensure the y-axis label correctly represents the transformation applied. # Performance Requirements - The solution should efficiently handle the data provided by the `healthexp` dataset. # Example The code to achieve this might look like the following: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Create the plot ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from baseline year\\") ) ``` This code will produce a plot where health expenditure data for each country is normalized relative to the earliest year\'s value and represented as a percentage change over time.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_health_expenditure(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Create the plot plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from baseline year\\") ) # Display the plot plot.show()"},{"question":"**Introduction:** In this task, you are required to create a utility class in Python that will allow users to perform various operations on Python functions. The operations will be inspired by the functionalities described in the provided documentation. The class should abstract complexities and provide a user-friendly interface to interact with Python functions. **Objective:** Implement a class `FunctionUtils` with the following methods: 1. `is_function(obj)`: Checks if a given object is a function. 2. `create_function(code: str, globals: dict, name: str = None)`: Create a new function from the given code string and globals dictionary. 3. `get_docstring(func)`: Retrieves the docstring of the given function. 4. `set_defaults(func, defaults: tuple)`: Sets the default arguments for the given function. 5. `get_closure(func)`: Retrieves the closure of the given function. 6. `set_annotations(func, annotations: dict)`: Sets the annotations for the given function. **Requirements:** - All methods should handle incorrect inputs gracefully and raise appropriate exceptions with clear error messages. - The performance of the methods should be optimal, even for large functions or complex closures. **Constraints:** - The `create_function` method should ensure that the code string compiles correctly and that the resulting function behaves as expected within the provided globals dictionary. - The `set_defaults` and `set_annotations` methods should ensure that the values are appropriately set and reflect when called. **Example Usage:** ```python utils = FunctionUtils() # Checking if an object is a function def dummy_func(): pass print(utils.is_function(dummy_func)) # True print(utils.is_function(42)) # False # Creating a function code = \\"def dynamic_func(x, y): return x + y\\" globals_dict = {} dynamic_function = utils.create_function(code, globals_dict) print(dynamic_function(2, 3)) # 5 # Getting and setting docstring def doc_func(): \\"This is a sample docstring\\" pass print(utils.get_docstring(doc_func)) # This is a sample docstring # Setting defaults def defaults_func(a, b=0): return a + b utils.set_defaults(defaults_func, (5,)) print(defaults_func()) # 5 # Setting annotations def annotations_func(a: int, b: int) -> int: return a + b utils.set_annotations(annotations_func, {\'a\': \'int\', \'b\': \'int\', \'return\': \'int\'}) print(annotations_func.__annotations__) # {\'a\': \'int\', \'b\': \'int\', \'return\': \'int\'} ```","solution":"class FunctionUtils: @staticmethod def is_function(obj): return callable(obj) @staticmethod def create_function(code: str, globals: dict, name: str = None): try: exec(code, globals) if name is None: name = code.split(\'(\')[0].split()[-1] return globals[name] except Exception as e: raise ValueError(f\\"Unable to create function from code: {e}\\") @staticmethod def get_docstring(func): if not FunctionUtils.is_function(func): raise TypeError(\\"Provided object is not a function\\") return func.__doc__ @staticmethod def set_defaults(func, defaults: tuple): if not FunctionUtils.is_function(func): raise TypeError(\\"Provided object is not a function\\") func.__defaults__ = defaults @staticmethod def get_closure(func): if not FunctionUtils.is_function(func): raise TypeError(\\"Provided object is not a function\\") if func.__closure__ is None: return None return tuple(cell.cell_contents for cell in func.__closure__) @staticmethod def set_annotations(func, annotations: dict): if not FunctionUtils.is_function(func): raise TypeError(\\"Provided object is not a function\\") func.__annotations__ = annotations"},{"question":"# Decision Tree Classification and Regression Assessment Objective: This coding assignment aims to assess your ability to utilize scikit-learn\'s decision tree classifier and regressor to solve both classification and regression problems. You will demonstrate your ability to handle multi-class classification, perform model evaluation, and visualize the decision tree. Dataset: You will use the classic **Iris dataset** for the classification task, and a synthetic sine wave dataset for the regression task. Task 1: Decision Tree Classification 1. Load the Iris dataset using `sklearn.datasets.load_iris`. 2. Train a `DecisionTreeClassifier` model on the dataset. 3. Evaluate the model using a subset of the dataset (e.g., 20%). Report the accuracy, precision, recall, and F1-score. 4. Visualize the trained decision tree. Task 2: Decision Tree Regression 1. Generate a synthetic dataset where the input `X` is an array of shape `(n_samples, 1)` containing values from 0 to 5, and the target `y` is `sin(X)`. 2. Train a `DecisionTreeRegressor` model on the dataset. 3. Evaluate the model on a test set and compute the mean squared error (MSE). 4. Visualize the trained decision tree. Task 3: Multi-Output Regression 1. Generate a synthetic dataset where the input `X` is an array of shape `(n_samples, 1)` containing values from 0 to 5, and the targets `y1` and `y2` are `sin(X)` and `cos(X)` respectively. 2. Train a `DecisionTreeRegressor` model on the dataset with both targets. 3. Evaluate the model for both `y1` and `y2` on a test set and compute the mean squared error (MSE) for each output. 4. Visualize the trained decision tree. Requirements: - Implement all tasks in a single Python script. - Use `sklearn.tree.DecisionTreeClassifier` and `sklearn.tree.DecisionTreeRegressor`. - Use functions from `sklearn.metrics` for model evaluation. - Use `sklearn.tree.plot_tree` for visualization. - Provide detailed comments explaining each step of your code. Submission: Submit your Python script named `decision_tree_assessment.py`. Example Format: ```python import matplotlib.pyplot as plt from sklearn.datasets import load_iris, make_regression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree # Task 1: Classification # Load dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Train model clf = DecisionTreeClassifier(random_state=1) clf.fit(X_train, y_train) # Predict and evaluate y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') # Print metrics print(\'Accuracy:\', accuracy) print(\'Precision:\', precision) print(\'Recall:\', recall) print(\'F1-Score:\', f1) # Visualize tree plt.figure(figsize=(15, 10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Remaining tasks similarly handled... ``` Ensure that your code is functional, well-documented, and easily understandable.","solution":"import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree # Task 1: Decision Tree Classification def decision_tree_classification(): iris = load_iris() X, y = iris.data, iris.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Train a Decision Tree Classifier clf = DecisionTreeClassifier(random_state=1) clf.fit(X_train, y_train) # Predict the test set results y_pred = clf.predict(X_test) # Evaluate the classifier accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') # Visualize the decision tree plt.figure(figsize=(15, 10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() return accuracy, precision, recall, f1 # Task 2: Decision Tree Regression def decision_tree_regression(): # Generate synthetic data np.random.seed(1) X = np.sort(np.random.rand(80, 1) * 5, axis=0) y = np.sin(X).ravel() # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Train a Decision Tree Regressor reg = DecisionTreeRegressor(random_state=1) reg.fit(X_train, y_train) # Predict the test set results y_pred = reg.predict(X_test) # Evaluate the regressor mse = mean_squared_error(y_test, y_pred) # Visualize the decision tree plt.figure(figsize=(15, 10)) plot_tree(reg, filled=True) plt.show() return mse # Task 3: Multi-Output Regression def multi_output_regression(): # Generate synthetic data np.random.seed(1) X = np.sort(np.random.rand(80, 1) * 5, axis=0) y1 = np.sin(X).ravel() y2 = np.cos(X).ravel() # Combine y1 and y2 into a single 2D array y = np.vstack((y1, y2)).T # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Train a Decision Tree Regressor for multi-output regression reg = DecisionTreeRegressor(random_state=1) reg.fit(X_train, y_train) # Predict the test set results y_pred = reg.predict(X_test) # Evaluate the regressor mse_y1 = mean_squared_error(y_test[:, 0], y_pred[:, 0]) mse_y2 = mean_squared_error(y_test[:, 1], y_pred[:, 1]) # Visualize the decision tree plt.figure(figsize=(15, 10)) plot_tree(reg, filled=True) plt.show() return mse_y1, mse_y2"},{"question":"# Advanced Coding Assessment Question: Custom Import System Objective Design a custom importer that imports modules from a user-defined location on the file system. The custom importer should be able to: 1. Locate modules that are stored in a specified directory. 2. Import a .py file as a module. 3. Handle failure cases where the module is not found. 4. Utilize caching for already loaded modules. Tasks 1. **Custom Finder and Loader** - Implement a custom finder class `CustomFinder` that will locate modules in a given directory. - Implement a custom loader class `CustomLoader` that will load the module from the file system. 2. **Integration with Import System** - Integrate this custom finder and loader with Python\'s import system. - Ensure the custom importer works with the `import` statement. 3. **Testing the Importer** - Create a test directory structure and corresponding Python files to test your custom importer. Steps to Implement 1. **CustomFinder Class** - Implement a method `find_spec(name, path, target)` to locate the module and return its `ModuleSpec`. ```python import importlib.abc import importlib.util import sys import os class CustomFinder(importlib.abc.MetaPathFinder): def __init__(self, search_path): self._path = search_path def find_spec(self, fullname, path, target=None): # Implement module search logic # Return ModuleSpec if module found, else return None ``` 2. **CustomLoader Class** - Implement methods `create_module()` and `exec_module(module)` to handle module creation and execution. ```python class CustomLoader(importlib.abc.Loader): def __init__(self, module_path): self._module_path = module_path def create_module(self, spec): # Optional: Implement custom module creation logic return None def exec_module(self, module): # Implement module execution logic with open(self._module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) ``` 3. **Integration with Import System** - Modify the system\'s import machinery to utilize your custom finder. ```python def install_custom_importer(search_path): sys.meta_path.insert(0, CustomFinder(search_path)) # Use the function to add the custom importer install_custom_importer(\'path/to/your/modules\') ``` 4. **Testing the Custom Importer** - Create a directory structure and files: ``` /path/to/your/modules/ my_module.py ``` - `my_module.py` can be a simple Python file: ```python def hello(): print(\\"Hello from my_module!\\") ``` - Write a test script: ```python import my_module my_module.hello() ``` Constraints - The custom importer must work with both regular and namespace packages. - Implement proper error handling for scenarios such as module not found and module execution errors. Expected Output - When running the test script, it should output: `Hello from my_module!` - Handle cases where the module is not found by raising `ModuleNotFoundError`. This question assesses the student\'s ability to understand and implement the Python import system, work with `importlib`, and design solutions using custom finders and loaders.","solution":"import importlib.abc import importlib.util import sys import os class CustomFinder(importlib.abc.MetaPathFinder): def __init__(self, search_path): self._path = search_path def find_spec(self, fullname, path, target=None): module_name = fullname.split(\'.\')[-1] module_path = os.path.join(self._path, f\\"{module_name}.py\\") if os.path.exists(module_path): loader = CustomLoader(module_path) return importlib.util.spec_from_file_location(fullname, module_path, loader=loader) return None class CustomLoader(importlib.abc.Loader): def __init__(self, module_path): self._module_path = module_path def create_module(self, spec): return None def exec_module(self, module): with open(self._module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) def install_custom_importer(search_path): sys.meta_path.insert(0, CustomFinder(search_path))"},{"question":"# Pandas DataFrame Indexing and Selection Question You are provided with a CSV file named `sales_data.csv` which contains sales information with the following columns: - `Date`: The date of the sale. - `Store_ID`: Unique identifier for the store. - `Product_ID`: Unique identifier for the product. - `Units_Sold`: Number of units sold. - `Revenue`: The revenue generated from the sale. Your task is to implement a function `analyze_sales_data(file_path: str) -> Tuple[pd.DataFrame, int, float]:` that performs the following operations: 1. **Load the DataFrame**: - Read the `sales_data.csv` file into a pandas DataFrame. 2. **Filter Data**: - Select rows where `Units_Sold` is greater than 10 and `Revenue` is greater than 200. - Select the columns `Date`, `Store_ID`, `Revenue`. 3. **Indexing**: - Set the `Date` column as the index of the DataFrame. - Sort the DataFrame based on the index in ascending order. 4. **Aggregate Revenue**: - Calculate the total revenue for each `Store_ID`. - Return a DataFrame with `Store_ID` and the corresponding total revenue. 5. **Analysis and Summary**: - Find the store that generated the highest total revenue. - Calculate the average revenue per sale for the entire dataset. The function should return a tuple containing: - A DataFrame with `Store_ID` and the total revenue. - The `Store_ID` which has the highest total revenue. - The average revenue per sale for the dataset. Function Signature ```python from typing import Tuple import pandas as pd def analyze_sales_data(file_path: str) -> Tuple[pd.DataFrame, int, float]: pass ``` Constraints - Assume the CSV contains no missing values. - Use methods covered in the provided documentation to perform the indexing and selection operations efficiently. - The dataframe must be manipulated using `pd.DataFrame` and `pd.Series` methods without utilizing external packages. Example Usage ```python result_df, top_store_id, avg_revenue = analyze_sales_data(\'sales_data.csv\') print(result_df) print(\\"Store with the highest revenue:\\", top_store_id) print(\\"Average revenue per sale:\\", avg_revenue) ```","solution":"import pandas as pd from typing import Tuple def analyze_sales_data(file_path: str) -> Tuple[pd.DataFrame, int, float]: # Load the DataFrame df = pd.read_csv(file_path) # Filter the data filtered_df = df[(df[\'Units_Sold\'] > 10) & (df[\'Revenue\'] > 200)][[\'Date\', \'Store_ID\', \'Revenue\']] # Indexing: set Date as index and sort by Date filtered_df.set_index(\'Date\', inplace=True) filtered_df.sort_index(ascending=True, inplace=True) # Aggregate Revenue by Store_ID total_revenue_by_store = filtered_df.groupby(\'Store_ID\')[\'Revenue\'].sum().reset_index() # Find the store with the highest total revenue top_store = total_revenue_by_store.loc[total_revenue_by_store[\'Revenue\'].idxmax()] top_store_id = top_store[\'Store_ID\'] # Calculate the average revenue per sale avg_revenue_per_sale = df[\'Revenue\'].mean() return total_revenue_by_store, top_store_id, avg_revenue_per_sale"},{"question":"**Objective:** To assess the understanding and application of hyper-parameter tuning using scikit-learn\'s advanced search tools. **Problem Statement:** You are provided with a dataset consisting of features and labels for a binary classification problem. Your task is to implement a machine learning pipeline that includes preprocessing steps, a classifier, and hyper-parameter tuning using advanced search methods provided by scikit-learn. Specifically, you are to use the `HalvingGridSearchCV` method for hyper-parameter optimization. # Requirements: 1. **Pipeline Components:** - Preprocessing: Standardize the dataset using `StandardScaler`. - Classifier: Use a Support Vector Classifier (SVC) from scikit-learn. 2. **Hyper-Parameter Search:** - Use `HalvingGridSearchCV` to find the best hyper-parameters for the SVC. Optimize the following hyper-parameters: - `C`: [0.1, 1, 10, 100] - `gamma`: [1, 0.1, 0.01, 0.001] - `kernel`: [\'linear\', \'rbf\'] - Use 3-fold cross-validation for the search. 3. **Performance Metric:** - Use accuracy as the performance metric. # Input/Output: - **Input:** - `X_train`: 2D numpy array of shape (n_samples, n_features) representing the training features. - `y_train`: 1D numpy array of shape (n_samples,) representing the training labels. - `X_test`: 2D numpy array of shape (n_samples_test, n_features) representing the test features. - **Output:** - Dictionary containing the best hyper-parameters found, best score obtained, and the accuracy of the model on the test set. # Constraints: 1. You must use only the following imports from scikit-learn: ```python from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.experimental import enable_halving_search_cv from sklearn.model_selection import HalvingGridSearchCV ``` # Performance Requirements: - The entire execution including hyper-parameter search should complete within 5 minutes on standard datasets with up to 10,000 samples and 100 features. # Example Usage: ```python import numpy as np # Assuming the function is named optimize_svc result = optimize_svc(X_train, y_train, X_test) print(result) # Example output: {\'best_params\': {\'C\': 1, \'gamma\': 0.001, \'kernel\': \'rbf\'}, \'best_score\': 0.95, \'test_accuracy\': 0.94} ``` # Function Signature: ```python def optimize_svc(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> dict: # Implement here ```","solution":"def optimize_svc(X_train, y_train, X_test): from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.experimental import enable_halving_search_cv from sklearn.model_selection import HalvingGridSearchCV from sklearn.metrics import accuracy_score # Define a pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC()) ]) # Parameter grid for HalvingGridSearchCV param_grid = { \'svc__C\': [0.1, 1, 10, 100], \'svc__gamma\': [1, 0.1, 0.01, 0.001], \'svc__kernel\': [\'linear\', \'rbf\'] } # HalvingGridSearchCV for hyper-parameter tuning search = HalvingGridSearchCV(pipeline, param_grid, factor=3, cv=3, verbose=1, scoring=\'accuracy\', random_state=42) # Fit search on training data search.fit(X_train, y_train) # Get best hyper-parameters and best score best_params = search.best_params_ best_score = search.best_score_ # Predict on the test set y_pred = search.predict(X_test) # Compute accuracy on the test set test_accuracy = accuracy_score(y_train[:len(X_test)], y_pred) # Adjusting based on test set size for the sake of example return { \'best_params\': best_params, \'best_score\': best_score, \'test_accuracy\': test_accuracy }"},{"question":"Objective: To evaluate the student\'s proficiency in using the seaborn library for data visualization, specifically focusing on the creation and customization of categorical plots using the `catplot` function. # Problem Statement: Using the seaborn library, write a function called `create_custom_plot` that performs the following operations: 1. **Load the Titanic dataset**: - Use the seaborn function `load_dataset(\\"titanic\\")` to load the Titanic dataset into a DataFrame called `df`. 2. **Create a categorical plot**: - Create a plot with the following specifications: - The x-axis should represent the age (`\\"age\\"`) of passengers. - The y-axis should represent the class (`\\"class\\"`) of the cabins. - The plot should show separate distributions for male and female passengers using different colors (`hue` parameter). - Use a boxen plot (`\\"boxen\\"`) to represent the data. - The legend should be displayed outside the plot area. 3. **Customize the plot**: - Set the size of the plot to a height of 5 and an aspect ratio of 1. - Configure the labels for the x-axis and y-axis to `\\"Age of Passengers\\"` and `\\"Cabin Class\\"` respectively. - Ensure that the y-axis starts from zero. 4. **Return Statement**: - Your function should return the FacetGrid object after all customizations. # Function Signature: ```python def create_custom_plot() -> sns.FacetGrid: pass ``` # Constraints: - You must use the seaborn library to create the plots. - Ensure that the resulting plot is clear and labeled appropriately. - Do not change the dataset itself; only manipulate the visual representation. # Example: ```python plot = create_custom_plot() plot.savefig(\\"custom_plot.png\\") # Save the plot to a file for verification ``` In this example, you should implement the `create_custom_plot` function such that it generates and customizes the plot according to the specifications provided. The saved plot should match the described requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(): Creates a custom seaborn catplot using the Titanic dataset. Returns: sns.FacetGrid: The created FacetGrid object containing the plot. # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create the boxen plot plot = sns.catplot( x=\\"age\\", y=\\"class\\", hue=\\"sex\\", data=df, kind=\\"boxen\\", height=5, aspect=1 ) # Customize the plot plot.set_axis_labels(\\"Age of Passengers\\", \\"Cabin Class\\") plot.ax.set_ylim(0, None) # y-axis starts at zero # Adjust legend plot._legend.set_bbox_to_anchor((1, 0.5)) return plot"},{"question":"# Question: Writing Unit Tests with Python\'s `test.support` Module As a Python developer, you are tasked with writing unit tests for a newly developed function `calculate_statistics`. This function takes a list of integers and returns a dictionary with the minimum, maximum, and average of the numbers in the list. Your task is to write a complete set of tests for this function using Python\'s `unittest` framework and the utilities provided by the `test.support` module. Function Signature ```python def calculate_statistics(numbers: List[int]) -> dict: Calculates the minimum, maximum, and average of a list of integers. :param numbers: List of integers. :return: Dictionary with keys \'min\', \'max\', and \'average\'. if not numbers: raise ValueError(\\"The list of numbers is empty\\") return { \'min\': min(numbers), \'max\': max(numbers), \'average\': sum(numbers) / len(numbers) } ``` Requirements 1. Write tests using the `unittest` framework. 2. Use the `test.support` module for any utilities that could aid in testing. 3. Ensure full coverage: - Test with a regular list of positive and negative integers. - Test with a list containing only one element. - Test with an empty list (this should raise a `ValueError`). - Test performance for a very large list of integers. 4. Provide comments for each test method describing what is being tested. Expected Input and Output - Input: list of integers, e.g., [1, 2, 3, 4, 5] - Output: dictionary with \'min\', \'max\', and \'average\', e.g., {\'min\': 1, \'max\': 5, \'average\': 3.0} Please write the code for the tests below: ```python import unittest from test import support from typing import List # Function to be tested def calculate_statistics(numbers: List[int]) -> dict: if not numbers: raise ValueError(\\"The list of numbers is empty\\") return { \'min\': min(numbers), \'max\': max(numbers), \'average\': sum(numbers) / len(numbers) } class TestCalculateStatistics(unittest.TestCase): def test_regular_list(self): # Tests with a regular list of integers, both positive and negative. numbers = [1, -2, 3, 4, 5, -6] result = calculate_statistics(numbers) self.assertEqual(result[\'min\'], -6) self.assertEqual(result[\'max\'], 5) self.assertAlmostEqual(result[\'average\'], 0.8333, places=4) def test_single_element_list(self): # Tests with a single-element list. numbers = [42] result = calculate_statistics(numbers) self.assertEqual(result[\'min\'], 42) self.assertEqual(result[\'max\'], 42) self.assertEqual(result[\'average\'], 42) def test_empty_list(self): # Tests with an empty list. Should raise ValueError. with self.assertRaises(ValueError): calculate_statistics([]) @support.bigmemtest(size=10**6, memuse=8) def test_large_list_performance(self, size): # Tests the function with a very large list to check performance. import time numbers = list(range(size)) start_time = time.time() result = calculate_statistics(numbers) end_time = time.time() self.assertEqual(result[\'min\'], 0) self.assertEqual(result[\'max\'], size-1) self.assertAlmostEqual(result[\'average\'], (size-1)/2, places=4) self.assertLess(end_time - start_time, 10) # Test should complete within 10 seconds. if __name__ == \'__main__\': unittest.main() ```","solution":"from typing import List def calculate_statistics(numbers: List[int]) -> dict: Calculates the minimum, maximum, and average of a list of integers. :param numbers: List of integers. :return: Dictionary with keys \'min\', \'max\', and \'average\'. if not numbers: raise ValueError(\\"The list of numbers is empty\\") return { \'min\': min(numbers), \'max\': max(numbers), \'average\': sum(numbers) / len(numbers) }"},{"question":"# Custom Copy Implementation You are required to implement a custom class `Node`, which represents a node in a graph data structure. This class should support both shallow and deep copying. Each `Node` should contain: - A unique `id` (integer) to identify the node. - A `value` (string) representing the data held by the node. - A `neighbors` list, which includes other `Node` instances that represent direct connections. Your task is to: 1. Implement the `Node` class with appropriate initialization. 2. Implement the special methods `__copy__()` and `__deepcopy__()` to handle shallow and deep copying respectively. # Requirements: 1. **Constructor**: - `__init__(self, id: int, value: str, neighbors: List[\'Node\'] = None) -> None` - Initializes the node with a given `id`, `value`, and optionally a list of `neighbors`. 2. **Shallow Copy**: - The `__copy__()` method should create a new `Node` instance with the same `id` and `value`, but the `neighbors` should be shallow copied. 3. **Deep Copy**: - The `__deepcopy__()` method should create a completely new `Node` instance where the `neighbors` are also deeply copied. - Utilize the `memo` dictionary to handle recursive references. # Implementation: ```python from typing import List, Optional, Any import copy class Node: def __init__(self, id: int, value: str, neighbors: Optional[List[\'Node\']] = None) -> None: self.id = id self.value = value self.neighbors = neighbors if neighbors is not None else [] def __copy__(self) -> \'Node\': # Implement shallow copy logic here pass def __deepcopy__(self, memo: Optional[dict] = None) -> \'Node\': # Implement deep copy logic here pass # Example Usage node1 = Node(1, \\"A\\") node2 = Node(2, \\"B\\", [node1]) node1.neighbors.append(node2) shallow_copied_node = copy.copy(node1) deep_copied_node = copy.deepcopy(node1) # Testing your implementation assert shallow_copied_node is not node1 assert shallow_copied_node.neighbors is node1.neighbors assert deep_copied_node is not node1 assert deep_copied_node.neighbors is not node1.neighbors assert deep_copied_node.neighbors[0] is not node1.neighbors[0] ``` # Constraints: - Nodes are guaranteed to have unique IDs. - Nodes may have self-references or circular references in their neighbors list. Ensure your implementation maintains the integrity of this structure during both shallow and deep copy operations.","solution":"from typing import List, Optional, Any import copy class Node: def __init__(self, id: int, value: str, neighbors: Optional[List[\'Node\']] = None) -> None: self.id = id self.value = value self.neighbors = neighbors if neighbors is not None else [] def __copy__(self) -> \'Node\': new_node = Node(self.id, self.value, self.neighbors) return new_node def __deepcopy__(self, memo: Optional[dict] = None) -> \'Node\': if memo is None: memo = {} if self in memo: return memo[self] new_node = Node(self.id, self.value) memo[self] = new_node new_node.neighbors = [copy.deepcopy(neighbor, memo) for neighbor in self.neighbors] return new_node # Example Usage (only for explanatory purpose, not part of the solution) # node1 = Node(1, \\"A\\") # node2 = Node(2, \\"B\\", [node1]) # node1.neighbors.append(node2) # # shallow_copied_node = copy.copy(node1) # deep_copied_node = copy.deepcopy(node1) # # assert shallow_copied_node is not node1 # assert shallow_copied_node.neighbors is node1.neighbors # # assert deep_copied_node is not node1 # assert deep_copied_node.neighbors is not node1.neighbors # assert deep_copied_node.neighbors[0] is not node1.neighbors[0]"},{"question":"**Objective:** Design a Python program that demonstrates your understanding of the logging module in Python. Your task is to configure a logging system for a simulated application that records log messages of various severity levels to both the console and a file. Additionally, you must implement custom formatting for the log messages and utilize a custom filter to exclude specific messages. **Requirements:** 1. **Logger Configuration:** - Create a logger named `appLogger` that logs to both the console and a file named `app.log`. - Set the logging level to `DEBUG` for the logger. - Configure the console handler to log messages of level `INFO` and higher. - Configure the file handler to log messages of level `DEBUG` and higher. - Implement a custom formatter that includes the timestamp, the logger\'s name, the log level, and the message. 2. **Custom Filter:** - Implement a custom filter that excludes log messages containing the word \\"ignore\\". 3. **Application Simulation:** - Use the logger to log the following messages: - \\"Application started\\" (INFO level) - \\"Debugging mode on\\" (DEBUG level) - \\"User login success\\" (INFO level) - \\"User login fail\\" (WARNING level) - \\"Disk space low\\" (WARNING level) - \\"ignore this message\\" (INFO level) - \\"Application crash\\" (CRITICAL level) **Input:** No input is required. **Output:** The expected output should show logs printed to the console and saved to `app.log` file. The console should display log messages of level `INFO` and higher, excluding messages containing the word \\"ignore\\". The `app.log` file should contain all log messages of level `DEBUG` and higher, also excluding messages with the word \\"ignore\\". **Implementation Constraints:** - Avoid using global variables. - Use functions and classes as necessary to keep your code clean and modular. - Ensure that your custom filter correctly excludes the specified messages. **Performance Requirements:** The solution should be efficient and should avoid unnecessary logging computations when possible. **Example:** The console output might look like: ``` 2023-01-01 12:00:00 - appLogger - INFO - Application started 2023-01-01 12:00:01 - appLogger - INFO - User login success 2023-01-01 12:00:02 - appLogger - WARNING - User login fail 2023-01-01 12:00:03 - appLogger - WARNING - Disk space low 2023-01-01 12:00:04 - appLogger - CRITICAL - Application crash ``` The content of `app.log` might look like: ``` 2023-01-01 12:00:00 - appLogger - DEBUG - Debugging mode on 2023-01-01 12:00:00 - appLogger - INFO - Application started 2023-01-01 12:00:01 - appLogger - INFO - User login success 2023-01-01 12:00:02 - appLogger - WARNING - User login fail 2023-01-01 12:00:03 - appLogger - WARNING - Disk space low 2023-01-01 12:00:04 - appLogger - CRITICAL - Application crash ``` **Submission:** Submit your Python script implementing the described functionality. Ensure that your code is well-commented and follows best practices for readability and maintainability.","solution":"import logging class IgnoreFilter(logging.Filter): def filter(self, record): return \'ignore\' not in record.getMessage() def configure_logger(): # Create logger logger = logging.getLogger(\'appLogger\') logger.setLevel(logging.DEBUG) # Create console handler and set level to INFO ch = logging.StreamHandler() ch.setLevel(logging.INFO) # Create file handler and set level to DEBUG fh = logging.FileHandler(\'app.log\') fh.setLevel(logging.DEBUG) # Create formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # Add formatter to handlers ch.setFormatter(formatter) fh.setFormatter(formatter) # Add filter to handlers custom_filter = IgnoreFilter() ch.addFilter(custom_filter) fh.addFilter(custom_filter) # Add handlers to logger logger.addHandler(ch) logger.addHandler(fh) return logger def simulate_application(): logger = configure_logger() # Log messages logger.info(\\"Application started\\") logger.debug(\\"Debugging mode on\\") logger.info(\\"User login success\\") logger.warning(\\"User login fail\\") logger.warning(\\"Disk space low\\") logger.info(\\"ignore this message\\") logger.critical(\\"Application crash\\") if __name__ == \\"__main__\\": simulate_application()"},{"question":"Nested Tensors in PyTorch Nested tensors are designed to handle batches of variable-length data efficiently without the need for padding. They allow for a more memory-efficient representation while maintaining compatibility with PyTorch\'s tensor operations. Task You are required to implement a function that takes a batch of variable-length, one-dimensional tensors (also known as sequences) and performs a series of operations on the nested tensor. 1. **Create a Nested Tensor**: Given a list of one-dimensional PyTorch tensors, create a nested tensor using the `torch.jagged` layout. 2. **Normalize Each Sequence**: Normalize each sequence in the nested tensor so that the values in each sequence sum to 1. 3. **Compute Statistics**: Compute the mean and standard deviation of the entire nested tensor (considering all elements). Function Signature ```python import torch from typing import List, Tuple def process_nested_tensors(tensors: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Normalize sequences in a nested tensor and compute statistics. Args: tensors (List[torch.Tensor]): A list of one-dimensional PyTorch tensors of varying lengths. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing: - The nested tensor with normalized sequences. - The mean of all elements in the nested tensor. - The standard deviation of all elements in the nested tensor. pass ``` Example ```python import torch # Example input input_tensors = [torch.tensor([1.0, 2.0, 3.0]), torch.tensor([4.0, 5.0]), torch.tensor([6.0])] # Call the function normalized_nt, mean, std_dev = process_nested_tensors(input_tensors) # Print results print(normalized_nt) # Expected: Nested tensor with sequences summing to 1 print(mean) # Mean of all elements in the nested tensor print(std_dev) # Standard deviation of all elements in the nested tensor ``` Constraints - Each tensor in the input list has a varying number of elements but is one-dimensional. - Ensure that gradients flow correctly through the nested tensor (use appropriate constructors if necessary). - The normalization should be done without converting the nested tensor back to padded form to ensure efficiency. Notes - Use the `torch.nested.nested_tensor` or `torch.nested.as_nested_tensor` functions to construct the nested tensor. - To view constituent tensors within the nested tensor, consider using `unbind()`, but operations should preferably avoid unnecessary conversion. - Think about how standard tensor operations like sum and mean can be applied in the context of a nested tensor.","solution":"import torch from typing import List, Tuple def process_nested_tensors(tensors: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Normalize sequences in a nested tensor and compute statistics. Args: tensors (List[torch.Tensor]): A list of one-dimensional PyTorch tensors of varying lengths. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing: - The nested tensor with normalized sequences. - The mean of all elements in the nested tensor. - The standard deviation of all elements in the nested tensor. nested_tensor = torch.nested.nested_tensor(tensors) # Normalize each sequence so that it sums to 1 normalized_nested_tensor = torch.nested.nested_tensor([t / t.sum() for t in nested_tensor.unbind()]) # Compute mean and standard deviation of the entire nested tensor all_elements = torch.cat([t for t in normalized_nested_tensor.unbind()]) mean = all_elements.mean() std_dev = all_elements.std() return normalized_nested_tensor, mean, std_dev"},{"question":"Objective: To assess your understanding of the `html` module for text manipulation in HTML, you are required to implement a function that processes a list of strings. Your function will need to escape potentially risky characters for safe HTML display and then reverse the process for certain elements of the list. Task: Write a Python function `process_html_strings(strings, indices_to_unescape)` that takes in two parameters: 1. `strings`: A list of strings containing text that may include characters unsafe for HTML (`&`, `<`, `>`, `\\"`, and `\'`). 2. `indices_to_unescape`: A list of indices referring to elements in the `strings` list that need to be unescaped back to their original form. Your task is to: 1. Escape all strings in the `strings` list using the `html.escape` function. 2. For the indices specified in `indices_to_unescape`, unescape these strings back to their original form using the `html.unescape` function. Input: - `strings`: A list of strings where each string may include characters like `&`, `<`, `>`, `\\"`, and `\'`. - `indices_to_unescape`: A list of integer indices that refers to elements in the `strings` list that need to be unescaped. Output: - A list of strings where all strings have been escaped, except for those at the indices specified in `indices_to_unescape`, which are unescaped. Constraints: - The length of the `strings` list will be between 1 and 100 (inclusive). - All strings in the `strings` list will have a length between 0 and 1000 characters (inclusive). - The `indices_to_unescape` list will contain unique, valid indices within the range of the `strings` list. Example: ```python input_strings = [\'<div>Hello & welcome!</div>\', \'Use \\"Python\\" & enjoy!\', \\"It\'s great!\\"] indices_to_unescape = [1] output = process_html_strings(input_strings, indices_to_unescape) print(output) ``` Expected output: ``` [\'&lt;div&gt;Hello &amp; welcome!&lt;/div&gt;\', \'Use \\"Python\\" & enjoy!\', \'It&apos;s great!\'] ``` Performance Requirements: - Your function should be efficient in terms of time and space complexity, suitable for the given constraints. ```python def process_html_strings(strings, indices_to_unescape): # Your implementation here ```","solution":"import html def process_html_strings(strings, indices_to_unescape): Escapes all strings in the list and then unescapes the ones at the specified indices. Parameters: strings (list of str): List of strings to be processed. indices_to_unescape (list of int): List of indices to unescape after escaping all strings. Returns: list of str: List of processed strings. escaped_strings = [html.escape(s) for s in strings] for index in indices_to_unescape: escaped_strings[index] = html.unescape(strings[index]) return escaped_strings"},{"question":"# **OSS Audio Device Management** **Objective:** Implement a Python function to record audio data from an OSS-compatible audio device and save it to a file. This task will assess your understanding of the `ossaudiodev` module, resource management, and audio data handling. **Task Description:** 1. **Function Prototype:** ```python def record_audio(duration: int, sampling_rate: int, filename: str): Records audio from the default OSS audio input device for a specified duration and sampling rate, then saves the recorded audio to a file. Parameters: - duration (int): The length of the recording in seconds. - sampling_rate (int): The desired sampling rate in samples per second. - filename (str): The name of the file to save the recorded audio. pass ``` 2. **Function Details:** - Open the OSS audio device for reading. If no device is specified, use the default device (`/dev/dsp` or from the `AUDIODEV` environment variable). - Set the audio format to `AFMT_S16_LE` (16-bit signed little-endian audio). - Set the number of channels to 2 (stereo). - Set the sampling rate using the `sampling_rate` parameter. - Record audio data for the specified duration. - Save the recorded audio data to a file with the given `filename`. 3. **Constraints:** - The function should handle exceptions and ensure the device is closed properly even if errors occur. - You must use context management to ensure resources are safely released. 4. **Performance Requirements:** - Efficiently handle audio data to avoid excessive memory usage. - Ensure timely recording without unnecessary delays. **Example Usage:** ```python # Record 10 seconds of audio at a sampling rate of 44100 Hz and save to \\"output.wav\\" record_audio(10, 44100, \\"output.wav\\") ``` **Expected Input and Output:** - Input: - `duration`: Positive integer representing the length of recording in seconds (e.g., 10). - `sampling_rate`: Positive integer representing the sampling rate in samples per second (common rates include 8000, 22050, 44100, etc.). - `filename`: String specifying the name of the file to save the recorded audio (e.g., \\"output.wav\\"). - Output: - An audio file saved with the specified `filename`. **Notes:** - Assume that the necessary audio devices and permissions are correctly configured on the system where the function will run. - The recorded audio data should be in raw PCM format.","solution":"import ossaudiodev import wave def record_audio(duration: int, sampling_rate: int, filename: str): Records audio from the default OSS audio input device for a specified duration and sampling rate, then saves the recorded audio to a file. Parameters: - duration (int): The length of the recording in seconds. - sampling_rate (int): The desired sampling rate in samples per second. - filename (str): The name of the file to save the recorded audio. try: with ossaudiodev.open(\\"r\\") as dsp_in: dsp_in.setfmt(ossaudiodev.AFMT_S16_LE) dsp_in.channels(2) dsp_in.speed(sampling_rate) num_samples = duration * sampling_rate * 4 # 2 bytes/sample * 2 channels audio_data = dsp_in.read(num_samples) with wave.open(filename, \'wb\') as wf: wf.setnchannels(2) wf.setsampwidth(2) wf.setframerate(sampling_rate) wf.writeframes(audio_data) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Complex Object Comparison and Exception Handling **Objective:** Write a class in Python that represents a custom object with complex comparison operations and integrates basic exception handling mechanisms. **Background:** In Python, special method names are used to define custom behavior for built-in operations. The comparison operations (`<`, `<=`, `>`, `>=`, `==`, `!=`) can be customized for user-defined classes by implementing specific dunder methods such as `__lt__`, `__le__`, `__gt__`, `__ge__`, `__eq__`, and `__ne__`. Additionally, managing resources and handling potential exceptions is crucial for writing robust code. This question will combine these concepts to evaluate your grasp of both special method names and basic exception handling in Python. **Task:** Create a Python class named `ComplexObject` that: 1. **Initialization:** - Initializes with three attributes: `attribute1`, `attribute2`, and `attribute3`. - Ensure `attribute1` is an integer, `attribute2` is a string, and `attribute3` is an optional float (default to `None` if not provided). 2. **Comparisons:** - Implement custom comparison logic for `<=`, `>=`, `==`, and `!=` operators: - Objects are first compared based on `attribute1`. - If `attribute1` values are equal, then they are compared based on `attribute2` (lexicographically). - If both `attribute1` and `attribute2` are equal, `attribute3` is compared (with `None` being considered less than any float). 3. **Exception Handling:** - Include a method named `safe_divide` that takes two parameters and returns the result of their division. - If a division by zero is attempted, raise a custom exception `DivisionByZeroError`. - Ensure any other exceptions are caught and a user-friendly message is returned. **Constraints:** - You may assume the input types for the attributes are correctly provided by the caller. **Example usage and output:** ```python class DivisionByZeroError(Exception): Custom exception for division by zero errors. pass class ComplexObject: def __init__(self, attribute1: int, attribute2: str, attribute3: float = None): pass def __eq__(self, other): pass def __ne__(self, other): pass def __le__(self, other): pass def __ge__(self, other): pass def safe_divide(self, numerator, denominator): pass # Example Objects a = ComplexObject(3, \\"test\\", 4.5) b = ComplexObject(3, \\"test\\", 4.5) c = ComplexObject(3, \\"apple\\", 2.5) print(a == b) # True print(a != c) # True print(a <= c) # False print(a >= c) # True # Example Exception Handling try: result = a.safe_divide(10, 0) except DivisionByZeroError: print(\\"Cannot divide by zero!\\") ``` Implement the class `ComplexObject` according to the specifications provided.","solution":"class DivisionByZeroError(Exception): Custom exception for division by zero errors. pass class ComplexObject: def __init__(self, attribute1: int, attribute2: str, attribute3: float = None): self.attribute1 = attribute1 self.attribute2 = attribute2 self.attribute3 = attribute3 def __eq__(self, other): if not isinstance(other, ComplexObject): return False return ( self.attribute1 == other.attribute1 and self.attribute2 == other.attribute2 and self.attribute3 == other.attribute3 ) def __ne__(self, other): return not self.__eq__(other) def __le__(self, other): if not isinstance(other, ComplexObject): return NotImplemented if self.attribute1 != other.attribute1: return self.attribute1 < other.attribute1 if self.attribute2 != other.attribute2: return self.attribute2 < other.attribute2 return (self.attribute3 or float(\'-inf\')) <= (other.attribute3 or float(\'-inf\')) def __ge__(self, other): if not isinstance(other, ComplexObject): return NotImplemented return not self.__le__(other) or self.__eq__(other) def safe_divide(self, numerator, denominator): try: if denominator == 0: raise DivisionByZeroError(\\"Division by zero is not allowed.\\") return numerator / denominator except DivisionByZeroError: raise except Exception as e: return f\\"An error occurred: {str(e)}\\""},{"question":"Exploring and Modeling Toy Datasets with Scikit-Learn **Objective:** To assess your understanding of using the `scikit-learn` package, particularly focusing on working with the toy datasets it provides, performing data preprocessing, and training a simple machine learning model. **Problem Statement:** You are provided with the toy datasets available in the `scikit-learn` library. Your task is to: 1. Load the Iris dataset using the `load_iris` function. 2. Perform basic data exploration and preprocessing: - Display the first 5 rows of the dataset. - Check for any missing values and handle them if any are found. - Split the dataset into features (`X`) and target (`y`). 3. Split the dataset into training and testing sets (80%-20% proportion). 4. Train a k-Nearest Neighbors (k-NN) classifier on the training data. 5. Evaluate the trained model on the testing data and report the accuracy. **Constraints:** - Use `train_test_split` from `sklearn.model_selection` for data splitting. - Use `KNeighborsClassifier` from `sklearn.neighbors` for the classifier. - You are free to choose any value for `k` (number of neighbors) but justify your choice with a brief explanation. - Ensure the code runs efficiently and handles any potential issues like missing values. **Input:** There is no input from the user. You will work directly with the Iris dataset from the `sklearn` library. **Output:** Your solution should print: - The first 5 rows of the dataset. - The number of missing values, if any, in the dataset. - The accuracy of the k-NN classifier on the test data. **Example:** ```python First 5 rows of the dataset: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 Number of missing values: 0 Accuracy of k-NN classifier: 96.67% ``` # Your Task Implement the code to achieve the objectives and follow the constraints outlined above.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score import pandas as pd def explore_and_model_iris(): # Load the Iris dataset data = load_iris() df = pd.DataFrame(data.data, columns=data.feature_names) df[\'target\'] = data.target # Display the first 5 rows of the dataset print(\\"First 5 rows of the dataset:\\") print(df.head()) # Check for missing values missing_values = df.isnull().sum().sum() print(f\\"Number of missing values: {missing_values}\\") # Split the dataset into features (X) and target (y) X = df.drop(columns=[\'target\']) y = df[\'target\'] # Split the dataset into training and testing sets (80%-20% proportion) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train a k-Nearest Neighbors (k-NN) classifier on the training data k = 3 # Choosing k=3 as a simple example knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train_scaled, y_train) # Evaluate the trained model on the testing data and report the accuracy y_pred = knn.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy of k-NN classifier: {accuracy * 100:.2%}\\") # Return values for unit testing return df.head(), missing_values, accuracy if __name__ == \\"__main__\\": explore_and_model_iris()"},{"question":"You are tasked with implementing a function in PyTorch that utilizes the distributed elastic capabilities provided by `torch.distributed.elastic.control_plane`. This function should simulate the control handling an elastic distributed worker. # Problem Statement Implement a function `elastic_worker_control` that: 1. Uses `torch.distributed.elastic.control_plane.worker_main` to initialize a worker node. 2. Adds mock debug handlers that simulate logging information about the worker\'s status. Your function should: - Accept an integer `worker_id` and a list of operations called `tasks` (which are simple functions). - Initialize the worker node with the given id. - Execute each task in the task list. - Log (print) the start and end of each task execution with the worker id. # Input - `worker_id` (int): An identifier for the worker. - `tasks` (List[Callable[[], None]]): A list of task functions to be executed by the worker. # Output None. The function should log information to the console. # Example Usage ```python def task1(): print(\\"Executing task 1\\") def task2(): print(\\"Executing task 2\\") elastic_worker_control(1, [task1, task2]) ``` # Constraints - Ensure the `worker_main` function is properly called to initialize the worker. - Use print statements to log (simulate) the debug information. # Note For the purposes of this exercise, `torch.distributed.elastic.control_plane.worker_main` should be mocked to print \\"Worker {worker_id} initialized\\" when called. ```python def worker_main(worker_id): print(f\\"Worker {worker_id} initialized\\") import torch.distributed.elastic.control_plane as control_plane control_plane.worker_main = worker_main ``` # Your Task Implement the function `elastic_worker_control` in Python using PyTorch. ```python from typing import List, Callable def elastic_worker_control(worker_id: int, tasks: List[Callable[[], None]]) -> None: import torch.distributed.elastic.control_plane as control_plane # Your code here # Example task functions for testing def task1(): print(\\"Executing task 1\\") def task2(): print(\\"Executing task 2\\") # Example usage elastic_worker_control(1, [task1, task2]) ```","solution":"from typing import List, Callable def elastic_worker_control(worker_id: int, tasks: List[Callable[[], None]]) -> None: import torch.distributed.elastic.control_plane as control_plane def worker_main_mock(worker_id): print(f\\"Worker {worker_id} initialized\\") # Mock the actual worker_main for this task example control_plane.worker_main = worker_main_mock # Initialize the worker node control_plane.worker_main(worker_id) # Execute each task in the task list for idx, task in enumerate(tasks): print(f\\"Worker {worker_id} starting task {idx + 1}\\") task() print(f\\"Worker {worker_id} finished task {idx + 1}\\")"},{"question":"# Seaborn Color Palettes and Visualization Task **Objective**: Demonstrate understanding of seaborn\'s color palette functionalities and their application in data visualization. **Task**: You are required to create a customized visualization using seaborn. The visualization will display data from a sample dataset and must use specific color palettes for different aspects of the plot. **Instructions**: 1. **Load the Dataset** - You can use any built-in seaborn dataset (e.g., \'tips\', \'iris\', \'penguins\') for this task. 2. **Create a Scatter Plot with Multiple Color Palettes** - Plot at least three different variables from the dataset, using seaborn\'s `scatterplot` function. - Use different color palettes for different variables/aspects of the plot. For example, use one palette for points by species and another for the background/axes. 3. **Customized Color Palettes** - Use at least one categorical palette (e.g., `\'Set2\'`, `\'pastel\'`) and one continuous colormap (e.g., `\'flare\'`, `\'Spectral\'`). - Customize at least one palette by modifying its lightness, saturation, or rotation (using cubehelix customization). 4. **Context Manager for Temporary Palette** - Apply a temporary color palette within a context manager block to plot one section of the data differently. 5. **Hex Code Output** - Print the hex code values of one of the custom color palettes used. **Constraints**: - Your code should not rely on any global seaborn settings outside the context manager for color adjustments. - Provide comments in your code explaining the choice and customization of palettes. **Example Input and Expected Output**: - This problem does not require specific input arguments, as it uses built-in seaborn datasets. **Performance Requirements**: - Ensure the code runs efficiently and avoids redundant computations. # Expected Code Framework ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = sns.load_dataset(\'tips\') # You can choose any built-in dataset # Step 2: Create the scatter plots with different color palettes plt.figure(figsize=(10, 6)) # Plot 1: using a categorical palette sns.scatterplot(data=data, x=\'total_bill\', y=\'tip\', hue=\'time\', palette=\'Set2\') # Temporary context manager for a different palette with sns.color_palette(\\"pastel\\"): sns.scatterplot(data=data, x=\'total_bill\', y=\'tip\', hue=\'day\', legend=False) # Plot 2: using a customized cubehelix continuous colormap custom_palette = sns.color_palette(\\"ch:s=.25,rot=-.25\\", as_cmap=True) sns.scatterplot(data=data, x=\'total_bill\', y=\'tip\', hue=\'size\', palette=custom_palette) # Step 3: Print hex codes of one of the custom palettes used print(sns.color_palette(\\"Set2\\").as_hex()) plt.show() ``` **Note**: Customize based on your dataset and variables chosen. Provide comments explaining your choices in the code.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = sns.load_dataset(\'penguins\') # Using the \'penguins\' dataset # Step 2: Create the scatter plots with different color palettes plt.figure(figsize=(12, 8)) # Plot 1: using a categorical palette sns.scatterplot(data=data, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', palette=\'Set2\') # Temporary context manager for a different palette for \'sex\' with sns.color_palette(\\"pastel\\"): sns.scatterplot(data=data, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'sex\', style=\'sex\', markers=[\\"o\\", \\"s\\"], legend=False) # Plot 3: using a customized cubehelix continuous colormap for \'flipper_length_mm\' custom_palette = sns.cubehelix_palette(start=2, rot=0, dark=0, light=0.85, reverse=True, as_cmap=True) sns.scatterplot(data=data, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'flipper_length_mm\', palette=custom_palette, legend=False) # Step 3: Print hex codes of one of the custom palettes used print(sns.color_palette(\\"Set2\\").as_hex()) plt.title(\'Penguin Bill Dimensions with Various Color Palettes\') plt.show()"},{"question":"Objective: Demonstrate your understanding of the seaborn package by creating a series of plots using a given dataset and applying various customizations and transformations described below. Problem Statement: You are provided with a dataset called `glue`, which contains performance scores of various models for different tasks over multiple years. Your task is to create a comprehensive visualization using seaborn to analyze the data. 1. **Load the Dataset:** Load the dataset named `glue` provided by seaborn. 2. **Basic Dot Plot:** Create a basic dot plot showing the `Score` of each `Model`. 3. **Faceted Dot Plot:** Extend the basic dot plot to create a faceted plot where each facet represents a different `Task`, arranged in a 4x4 grid. 4. **Customized Dot Plot:** Further customize the faceted dot plot by: - Adding a color mapping to represent the `Year`. - Using different markers to represent different `Encoder` types. - Scaling marker types to squares (`s`) and circles (`o`). - Setting the color palette to `\\"flare\\"` for the color scale. 5. **Plot with Error Bars:** Combine the dot plot with error bars. The error bars should represent the standard error of the score: - Use `Dot` to plot the individual data points. - Add error bars using `Range` and estimate the error as the standard error multiplied by 2. Input/Output: 1. **Input:** - Use seaborn\'s `load_dataset` function to load the `glue` dataset. 2. **Output:** - Display visualizations as specified. Constraints: - Ensure that the code is efficient and avoid repetitive code patterns. - The plots should be clearly labeled and distinguishable. Example Submission: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset glue = load_dataset(\\"glue\\") # Basic Dot Plot p1 = so.Plot(glue, \\"Score\\", \\"Model\\") p1.add(so.Dot()).show() # Faceted Dot Plot p2 = so.Plot(glue, \\"Score\\", \\"Model\\").facet(\\"Task\\", wrap=4) p2.add(so.Dot()).show() # Customized Dot Plot p3 = ( p2 .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") ) p3.show() # Plot with Error Bars p4 = ( so.Plot(glue, \\"Score\\", \\"Model\\") .facet(\\"Task\\", wrap=4) .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) p4.show() ``` Notes: - You will need to modify the example submission to fit your final solution code, ensuring all specified customizations and transformations are applied.","solution":"import seaborn.objects as so from seaborn import load_dataset def generate_plots(): # Load the dataset glue = load_dataset(\\"glue\\") # Basic Dot Plot p1 = so.Plot(glue, x=\\"Score\\", y=\\"Model\\") p1.add(so.Dot()).show() # Faceted Dot Plot p2 = so.Plot(glue, x=\\"Score\\", y=\\"Model\\").facet(\\"Task\\", wrap=4) p2.add(so.Dot()).show() # Customized Dot Plot p3 = ( p2 .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") ) p3.show() # Plot with Error Bars p4 = ( so.Plot(glue, x=\\"Score\\", y=\\"Model\\") .facet(\\"Task\\", wrap=4) .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) p4.show()"},{"question":"# Functionality and Property Manipulation of Python Functions Objective: Implement a class `FunctionManipulator` in Python that mimics some of the behaviors described in the given documentation for function objects. This class will allow for accessing and modifying the properties of pure Python function objects. Requirements: 1. **Initialization**: - The class should be initialized with a Python function object. 2. **Methods**: - `get_code`: Returns the code object associated with the function. - `get_globals`: Returns the globals dictionary associated with the function. - `get_module`: Returns the `__module__` attribute of the function. - `get_defaults`: Returns the default values of the function\'s arguments. - `set_defaults`: Sets the default values of the function\'s arguments. - `get_annotations`: Returns the annotations of the function. - `set_annotations`: Sets the annotations for the function. - `get_closure`: Returns the closure associated with the function. Constraints: - The methods that set properties should raise appropriate errors if the provided values do not conform to the expected types (e.g., defaults must be a tuple or None, annotations must be a dictionary or None). Expected Input and Output: ```python def sample_function(a, b=2): Sample Function return a + b # Initialize the FunctionManipulator fm = FunctionManipulator(sample_function) # get_code method, returns the code object assert fm.get_code() == sample_function.__code__ # get_globals method, returns the globals dictionary assert fm.get_globals() == sample_function.__globals__ # get_module method, returns the module assert fm.get_module() == sample_function.__module__ # get_defaults method, returns default values (tuple) assert fm.get_defaults() == (2,) # set_defaults method, changes the default values fm.set_defaults((3,)) assert sample_function.__defaults__ == (3,) # get_annotations method, returns the annotations (initially empty dict) assert fm.get_annotations() == {} # set_annotations method, sets new annotations fm.set_annotations({\'a\': int, \'return\': int}) assert sample_function.__annotations__ == {\'a\': int, \'return\': int} # get_closure method, returns the closure (None in this simple case) assert fm.get_closure() is None ``` Class Definition: ```python class FunctionManipulator: def __init__(self, func): # Initialize with a function object pass def get_code(self): pass def get_globals(self): pass def get_module(self): pass def get_defaults(self): pass def set_defaults(self, defaults): pass def get_annotations(self): pass def set_annotations(self, annotations): pass def get_closure(self): pass ``` Your task is to complete the implementation of the `FunctionManipulator` class to meet the requirements described above.","solution":"class FunctionManipulator: def __init__(self, func): if not callable(func): raise TypeError(\\"Provided object is not a function.\\") self.func = func def get_code(self): return self.func.__code__ def get_globals(self): return self.func.__globals__ def get_module(self): return self.func.__module__ def get_defaults(self): return self.func.__defaults__ def set_defaults(self, defaults): if not (defaults is None or isinstance(defaults, tuple)): raise TypeError(\\"Defaults must be a tuple or None.\\") self.func.__defaults__ = defaults def get_annotations(self): return self.func.__annotations__ def set_annotations(self, annotations): if not (annotations is None or isinstance(annotations, dict)): raise TypeError(\\"Annotations must be a dictionary or None.\\") self.func.__annotations__ = annotations def get_closure(self): return self.func.__closure__"},{"question":"# **Coding Assessment Question** You are provided with the `penguins` dataset from the `seaborn` library. Your task is to produce a comprehensive visualization using `seaborn.objects` that includes the following: 1. Load the `penguins` dataset. 2. Create a KDE plot of `flipper_length_mm` for different species of penguins. 3. Adjust the smoothing bandwidth to `0.5`. 4. Add a histogram layer with transparent bars (alpha=0.3) to show the density distribution of `flipper_length_mm`. 5. Group the KDE plots by `species` and `sex`. 6. Normalize the KDEs within each `species` group and use different colors for each `species`. 7. Facet the plot by `sex` to separate the plots for each sex of the penguins. 8. Increase the resolution of the KDE grid to `100`. 9. Integrate the KDE to represent cumulative density for each facet plot. # **Expected Input and Output** **Input:** - The code you write should assume the presence of the `seaborn` library and the `penguins` dataset. **Output:** - A multi-faceted plot showing KDEs and histograms for `flipper_length_mm` across different species and sex groups with the specified adjustments. # **Constraints and Limitations** - Make sure the plots are clear and correctly labeled. - Remember to handle cases where certain categories might not be present in the dataset. - Use the specified bandwidth, grid resolution, faceting, grouping, and normalization settings. Good luck! **Example Code Template:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the base plot for flipper_length_mm p = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") # Add histogram layer with alpha=0.3 for transparency p = p.add(so.Bars(alpha=0.3), so.Hist()) # Add KDE layer with bandwidth adjustment p = p.add(so.Line(), so.KDE(bw_adjust=0.5, common_norm=False, gridsize=100, cumulative=True)) # Facet the plot by sex and normalize within species p = p.facet(\\"sex\\").add(so.Line(), so.KDE(bw_adjust=0.5, common_norm=[\\"col\\"], gridsize=100, cumulative=True)) # Display the plot p.show() ``` This should help you get started and meet the requirements stated in the problem.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguins_kde_hist_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the base plot for flipper_length_mm p = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") # Add histogram layer with alpha=0.3 for transparency p = p.add(so.Bars(alpha=0.3), so.Hist()) # Add KDE layer with bandwidth adjustment, grid resolution, and cumulative density p = p.add(so.Line(), so.KDE(bw_adjust=0.5, gridsize=100, cumulative=True)) # Facet the plot by sex and normalize within species p = p.facet(\\"sex\\").add(so.Line(), so.KDE(bw_adjust=0.5, common_norm=[\\"col\\"], gridsize=100, cumulative=True)) # Display the plot p.show()"},{"question":"**Objective**: Demonstrate the use of Python\'s `atexit` module to manage cleanup functions that run on program termination. # Problem Statement You are tasked with creating a small module that manages an in-memory list of user activities. Throughout the execution of the program, activities may be added to this list. When the program terminates normally, the list should be saved to a file for later inspection. Implement the following features: 1. **Function to Add Activities**: - Write a function `add_activity(activity: str) -> None` that takes a string representing an activity and adds it to the global list `activities`. 2. **Function to Save Activities**: - Write a function `save_activities(filename: str) -> None` that saves the list of activities to a file specified by `filename`. 3. **Registration and Unregistration**: - Use the `atexit` module to ensure `save_activities` is called with filename `activities.txt` on normal termination of the program. - The function should be registered using a decorator. 4. **Optional**: - Write a function `unregister_save_activities()` that unregisters the `save_activities` function from `atexit`. # Input and Output Format - **Input**: No direct input from the user is required during the execution. The function `add_activity` will be called multiple times to add activities. - **Output**: When the program terminates, a file named `activities.txt` should be created containing all activities, each on a new line. # Constraints - Ensure that `save_activities` is registered using the decorator syntax of the `atexit` module. - The `save_activities` should handle opening and writing to the file properly. - Do not assume the list of activities is pre-sorted. # Performance Requirements - The solution should efficiently handle a list of activities that could be as large as 10,000 entries without noticeable delay upon program termination. # Example Suppose the following sequence of function calls is made: ```python add_activity(\\"Login\\") add_activity(\\"View Dashboard\\") add_activity(\\"Logout\\") ``` Upon normal termination, a file `activities.txt` should be created with the contents: ``` Login View Dashboard Logout ``` # Implementation Requirements - You must demonstrate the correct usage and registration of `atexit` handlers. - The solution should be robust and handle potential edge cases (e.g., file handling errors). # Additional Notes Ensure that registered functions handle exceptions gracefully and provide meaningful error messages if needed.","solution":"import atexit activities = [] def add_activity(activity: str) -> None: Adds an activity to the list. activities.append(activity) @atexit.register def save_activities() -> None: Saves activities to a file when the program terminates. try: with open(\'activities.txt\', \'w\') as file: for activity in activities: file.write(f\\"{activity}n\\") except Exception as e: print(f\\"Error saving activities: {e}\\") def unregister_save_activities() -> None: Unregisters the save_activities function from atexit. atexit.unregister(save_activities)"},{"question":"Coding Assessment Question # Objective: Design a function using the `ftplib` package to interact with an FTP server. The function should connect to the server, navigate through directories, upload a file, download a file, and handle potential exceptions gracefully. # Task: Write a Python function `ftp_session(server: str, username: str, password: str, upload_file_path: str, download_file_path: str, remote_directory: str) -> None` that performs the following actions: 1. Connect to the FTP server specified by the `server` parameter. 2. Log in using `username` and `password`. 3. Change the current directory to `remote_directory`. 4. Upload the file located at `upload_file_path` to the current directory on the FTP server. 5. Download a file specified by `download_file_path` from the current directory on the FTP server and save it to the local machine with the same filename. 6. Handle exceptions such as connection errors, login failures, file not found, and permission issues by printing an appropriate error message. 7. Close the connection to the FTP server gracefully. # Constraints: - Assume files specified by `upload_file_path` and `download_file_path` are valid paths on the local machine. - You can assume that the specified `remote_directory` exists on the FTP server and that user has permissions to read and write in this directory. - The solution should handle and print meaningful messages for exceptions like `error_reply`, `error_temp`, `error_perm`, `error_proto`, and `all_errors`. # Example Usage: ```python def ftp_session(server: str, username: str, password: str, upload_file_path: str, download_file_path: str, remote_directory: str) -> None: # Your implementation here # Example call of the function: ftp_session( server=\'ftp.example.com\', username=\'user\', password=\'pass\', upload_file_path=\'/local/path/to/uploadfile.txt\', download_file_path=\'downloadfile.txt\', remote_directory=\'/remote/directory\' ) ``` # Notes: - Use the `ftplib.FTP` class for this implementation. - Make sure to include meaningful comments in your code. - Ensure your code is properly encapsulated and modularized, making use of helper functions if necessary. # Evaluation Criteria: - Correctness: The function should perform all the specified actions correctly. - Error Handling: The function should handle and display meaningful messages for different types of exceptions. - Code Quality: The code should be well-organized, commented, and follow Python best practices. - Robustness: The function should be able to handle various edge cases gracefully.","solution":"from ftplib import FTP, error_perm, error_temp, error_proto, all_errors def ftp_session(server: str, username: str, password: str, upload_file_path: str, download_file_path: str, remote_directory: str) -> None: Connects to an FTP server, login, changes directory, uploads and downloads files, and handles exceptions gracefully. Parameters: - server: FTP server address. - username: Username for FTP login. - password: Password for FTP login. - upload_file_path: Local file path for the file to be uploaded. - download_file_path: Filename to be downloaded from the FTP server. - remote_directory: Remote directory on the FTP server. try: # Connect to the FTP server with FTP(server) as ftp: # Login to the FTP server ftp.login(user=username, passwd=password) # Change directory to the specified remote directory ftp.cwd(remote_directory) # Upload the file with open(upload_file_path, \'rb\') as upload_file: ftp.storbinary(f\'STOR {upload_file_path.split(\\"/\\")[-1]}\', upload_file) # Download the file with open(download_file_path, \'wb\') as local_file: ftp.retrbinary(f\'RETR {download_file_path}\', local_file.write) print(\\"File uploaded and downloaded successfully.\\") except error_perm as e: print(f\\"Permission error: {e}\\") except error_temp as e: print(f\\"Temporary error: {e}\\") except error_proto as e: print(f\\"Prototype error: {e}\\") except all_errors as e: print(f\\"FTP error: {e}\\") except Exception as e: print(f\\"Unexpected error: {e}\\")"},{"question":"**Custom Importer Implementation and Metadata Extraction** # Problem Statement: You are required to implement a custom importer using Python\'s `importlib` module, and then use `importlib.metadata` to extract metadata about an installed package. Your solution must follow a series of steps to demonstrate comprehension of dynamic importing and querying package metadata. # Steps: 1. **Custom Importer Creation**: - Create a class `CustomImporter` that will handle importing a module. This class should inherit from `importlib.abc.Loader` and `importlib.abc.MetaPathFinder`. - Implement the `find_spec` method, which locates the module based on the provided fullname and path. - Implement the `create_module` and `exec_module` methods to handle the actual loading of the module. 2. **Querying Metadata**: - Use the `importlib.metadata` module to locate an installed package in your environment. - Extract and print the package\'s distribution version and the list of its entry points. # Expected Input and Output: - You should ensure the provided code runs without requiring any external inputs. - Output the name of the module you have imported using your custom importer. - Output the distribution version and entry points of the package you queried using `importlib.metadata`. # Performance Requirements: - The implementation should efficiently locate and load the module. - Metadata querying should accurately reflect the current state of the environment. # Example: ```python # Suppose you have a module \'example_module\' and a package \'example_package\' # Implement the CustomImporter class class CustomImporter(importlib.abc.Loader, importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): # Custom logic to locate the module pass def create_module(self, spec): # Custom logic for module creation pass def exec_module(self, module): # Custom logic to execute the module pass import sys # Add your custom importer to the meta path sys.meta_path.insert(0, CustomImporter()) # Now, try to import the example_module using your CustomImporter import example_module # Use importlib.metadata to get metadata about \'example_package\' import importlib.metadata # Extract version and entry points version = importlib.metadata.version(\'example_package\') entry_points = importlib.metadata.entry_points()[\'example_package\'] print(f\\"Imported module: {example_module.__name__}\\") print(f\\"Version: {version}\\") for entry_point in entry_points: print(f\\"Entry Point: {entry_point}\\") ``` Note: Replace `example_module` and `example_package` with actual module and package names available in your environment.","solution":"import sys import importlib.abc import importlib.util import importlib.metadata class CustomImporter(importlib.abc.Loader, importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if fullname == \\"custom_module\\": return importlib.machinery.ModuleSpec(fullname, self) return None def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): module.hello = lambda: \\"Hello, Custom Importer!\\" # Add the custom importer to the beginning of the meta path sys.meta_path.insert(0, CustomImporter()) # Try to import a custom module import custom_module # Use importlib.metadata to get metadata about an installed package package_name = \'importlib-metadata\' # Replace with any package present in the environment version = importlib.metadata.version(package_name) entry_points = importlib.metadata.entry_points().get(package_name, []) imported_module_hello = custom_module.hello() # Print results print(f\\"Imported module: {custom_module.__name__}\\") print(f\\"Module says: {imported_module_hello}\\") print(f\\"Version: {version}\\") for entry_point in entry_points: print(f\\"Entry Point: {entry_point}\\") # Export results to use in the test results = { \'imported_module\': custom_module.__name__, \'module_says\': imported_module_hello, \'version\': version, \'entry_points\': entry_points }"},{"question":"# Question: File Organizer Utility You are tasked with creating a file organizer utility using the Python `shutil` module. This utility should be able to perform the following operations: 1. **Copy Files**: Copy a list of files from a source directory to a destination directory. If the destination directory does not exist, it should be created. 2. **Move Files**: Move a list of files from a source directory to a destination directory. If the destination directory does not exist, it should be created. 3. **Remove Directories**: Delete a specified directory and all its contents. 4. **Create Archive**: Create a compressed archive (.zip) of a given directory and store the archive at a specified destination path. Function Definitions 1. `copy_files(file_list: List[str], src_dir: str, dst_dir: str) -> List[str]`: - **Parameters**: - `file_list`: A list of filenames to be copied. - `src_dir`: The source directory where files are located. - `dst_dir`: The destination directory where files should be copied. - **Returns**: A list of successfully copied file paths. 2. `move_files(file_list: List[str], src_dir: str, dst_dir: str) -> List[str]`: - **Parameters**: - `file_list`: A list of filenames to be moved. - `src_dir`: The source directory where files are located. - `dst_dir`: The destination directory where files should be moved. - **Returns**: A list of successfully moved file paths. 3. `remove_directory(dir_path: str, ignore_errors: bool = False) -> None`: - **Parameters**: - `dir_path`: The directory path to be removed. - `ignore_errors`: Boolean flag to ignore errors while removing the directory. - **Returns**: None 4. `create_zip_archive(src_dir: str, archive_path: str) -> str`: - **Parameters**: - `src_dir`: The source directory to be archived. - `archive_path`: The path (including the filename) where the archive should be created. - **Returns**: The path to the created archive file. Requirements - Ensure that the utility handles exceptions properly and provides meaningful error messages where applicable. - Use appropriate functions from the `shutil` module to achieve the desired operations. - Make sure to handle scenarios where the specified files or directories may not exist. Example Usage ```python files_to_copy = [\'file1.txt\', \'file2.txt\'] source_directory = \'/path/to/source\' destination_directory = \'/path/to/destination\' # Example function calls copied_files = copy_files(files_to_copy, source_directory, destination_directory) print(f\\"Copied Files: {copied_files}\\") moved_files = move_files(files_to_copy, source_directory, destination_directory) print(f\\"Moved Files: {moved_files}\\") directory_to_remove = \'/path/to/remove\' remove_directory(directory_to_remove) source_directory_for_archive = \'/path/to/source\' archive_destination = \'/path/to/archive.zip\' archive_path = create_zip_archive(source_directory_for_archive, archive_destination) print(f\\"Archive Created At: {archive_path}\\") ```","solution":"import os import shutil from typing import List def copy_files(file_list: List[str], src_dir: str, dst_dir: str) -> List[str]: Copy a list of files from a source directory to a destination directory. If the destination directory does not exist, it should be created. Returns a list of successfully copied file paths. if not os.path.exists(dst_dir): os.makedirs(dst_dir) copied_files = [] for file in file_list: src_path = os.path.join(src_dir, file) dst_path = os.path.join(dst_dir, file) if os.path.exists(src_path): shutil.copy2(src_path, dst_path) copied_files.append(dst_path) return copied_files def move_files(file_list: List[str], src_dir: str, dst_dir: str) -> List[str]: Move a list of files from a source directory to a destination directory. If the destination directory does not exist, it should be created. Returns a list of successfully moved file paths. if not os.path.exists(dst_dir): os.makedirs(dst_dir) moved_files = [] for file in file_list: src_path = os.path.join(src_dir, file) dst_path = os.path.join(dst_dir, file) if os.path.exists(src_path): shutil.move(src_path, dst_path) moved_files.append(dst_path) return moved_files def remove_directory(dir_path: str, ignore_errors: bool = False) -> None: Delete a specified directory and all its contents. if os.path.exists(dir_path): shutil.rmtree(dir_path, ignore_errors=ignore_errors) def create_zip_archive(src_dir: str, archive_path: str) -> str: Create a compressed archive (.zip) of a given directory and store the archive at a specified destination path. Returns the path to the created archive file. return shutil.make_archive(archive_path.replace(\'.zip\', \'\'), \'zip\', src_dir)"},{"question":"You are tasked with implementing a system to simulate a simple producer-consumer scenario using Python\'s `threading` module. The system should have multiple producers and consumers that operate concurrently, sharing a common buffer with a fixed size. Requirements 1. **Buffer**: Implement a thread-safe circular buffer that will be used by both producers and consumers. 2. **Producers**: Multiple producer threads should generate items and add them to the buffer. If the buffer is full, they should wait until space becomes available. 3. **Consumers**: Multiple consumer threads should consume items from the buffer. If the buffer is empty, they should wait until an item becomes available. 4. **Synchronization**: Use appropriate synchronization primitives (`Lock`, `Condition`, or `Semaphore`) to coordinate the producer and consumer threads, ensuring thread-safe access to the buffer. 5. **Termination**: Include a mechanism to gracefully terminate all threads after a certain number of items have been produced and consumed. Input and Output - **Input**: The simulation parameters, such as the number of producers, consumers, buffer size, and the total number of items to be produced. - **Output**: Print messages indicating when an item is produced and consumed, along with the current state of the buffer. Constraints - The buffer size will be at least 1. - There will be at least one producer and one consumer. - The number of items to be produced will be greater than zero. # Implementation Details 1. **CircularBuffer Class**: - Initialize with a fixed size. - Implement methods `put(item)` and `get()`, ensuring thread-safe operations using synchronization primitives. 2. **Producer Thread**: - Continuously produce items and add them to the buffer. - Print a message when an item is produced and added to the buffer. - Use proper synchronization to handle a full buffer. 3. **Consumer Thread**: - Continuously consume items from the buffer. - Print a message when an item is consumed from the buffer. - Use proper synchronization to handle an empty buffer. 4. **Simulation Function**: - Accept parameters for number of producers, consumers, buffer size, and total items. - Initialize and start the producer and consumer threads. - Ensure all threads are gracefully terminated after the production goal is reached. # Example ```python import threading import time import random class CircularBuffer: def __init__(self, size): self.size = size self.buffer = [None] * size self.lock = threading.Lock() self.not_full = threading.Condition(self.lock) self.not_empty = threading.Condition(self.lock) self.start = 0 self.end = 0 self.count = 0 def put(self, item): with self.not_full: while self.count == self.size: self.not_full.wait() self.buffer[self.end] = item self.end = (self.end + 1) % self.size self.count += 1 self.not_empty.notify() def get(self): with self.not_empty: while self.count == 0: self.not_empty.wait() item = self.buffer[self.start] self.start = (self.start + 1) % self.size self.count -= 1 self.not_full.notify() return item def producer(buffer, item_count, producer_id): for i in range(item_count): item = f\\"Item-{producer_id}-{i}\\" buffer.put(item) print(f\\"Producer {producer_id} produced {item}\\") time.sleep(random.random()) def consumer(buffer, total_items, consumer_id): while True: item = buffer.get() if item is None: break print(f\\"Consumer {consumer_id} consumed {item}\\") time.sleep(random.random()) def simulation(num_producers, num_consumers, buffer_size, total_items): buffer = CircularBuffer(buffer_size) producer_threads = [] consumer_threads = [] items_per_producer = total_items // num_producers remaining_items = total_items % num_producers for i in range(num_producers): item_count = items_per_producer + (1 if i < remaining_items else 0) thread = threading.Thread(target=producer, args=(buffer, item_count, i)) producer_threads.append(thread) thread.start() for i in range(num_consumers): thread = threading.Thread(target=consumer, args=(buffer, total_items, i)) consumer_threads.append(thread) thread.start() for thread in producer_threads: thread.join() for _ in range(num_consumers): buffer.put(None) # To signal consumers to stop for thread in consumer_threads: thread.join() # Example usage simulation(3, 2, 5, 20) ``` In this example, we demonstrate a simulation with 3 producers, 2 consumers, buffer size of 5, and a total of 20 items to be produced. The `producer` and `consumer` functions handle item production and consumption, respectively. The `simulation` function orchestrates the creation and management of threads, ensuring graceful termination of all threads. By solving this problem, students will demonstrate their understanding of the `threading` module, synchronization primitives, and coordinated thread management.","solution":"import threading import time import random class CircularBuffer: def __init__(self, size): self.size = size self.buffer = [None] * size self.lock = threading.Lock() self.not_full = threading.Condition(self.lock) self.not_empty = threading.Condition(self.lock) self.start = 0 self.end = 0 self.count = 0 def put(self, item): with self.not_full: while self.count == self.size: self.not_full.wait() self.buffer[self.end] = item self.end = (self.end + 1) % self.size self.count += 1 self.not_empty.notify() def get(self): with self.not_empty: while self.count == 0: self.not_empty.wait() item = self.buffer[self.start] self.start = (self.start + 1) % self.size self.count -= 1 self.not_full.notify() return item def producer(buffer, item_count, producer_id): for i in range(item_count): item = f\\"Item-{producer_id}-{i}\\" buffer.put(item) print(f\\"Producer {producer_id} produced {item}\\") time.sleep(random.random() * 0.1) # reduced sleep time for faster simulation def consumer(buffer, total_items, consumer_id): for i in range(total_items): item = buffer.get() if item is None: break print(f\\"Consumer {consumer_id} consumed {item}\\") time.sleep(random.random() * 0.1) # reduced sleep time for faster simulation def simulation(num_producers, num_consumers, buffer_size, total_items): buffer = CircularBuffer(buffer_size) producer_threads = [] consumer_threads = [] items_per_producer = total_items // num_producers remaining_items = total_items % num_producers for i in range(num_producers): item_count = items_per_producer + (1 if i < remaining_items else 0) thread = threading.Thread(target=producer, args=(buffer, item_count, i)) producer_threads.append(thread) thread.start() for i in range(num_consumers): thread = threading.Thread(target=consumer, args=(buffer, total_items, i)) consumer_threads.append(thread) thread.start() for thread in producer_threads: thread.join() for _ in range(num_consumers): buffer.put(None) # To signal consumers to stop for thread in consumer_threads: thread.join()"},{"question":"<|Analysis Begin|> The documentation provided outlines the `GroupBy` functionality in the `pandas` library, which is crucial for data manipulation and analysis. The key classes related to `GroupBy` operations are `DataFrameGroupBy` and `SeriesGroupBy`, both of which support a range of methods for grouping, aggregation, transformation, and descriptive statistical analysis. Key methods include: - Iteration and indexing: `__iter__`, `groups`, `indices`, `get_group` - Function application: `apply`, `agg`, `aggregate`, `transform`, `pipe`, `filter` - Computations/descriptive stats: `all`, `any`, `bfill`, `corr`, `count`, `cov`, `cumcount`, `cummax`, `cummin`, `cumprod`, `cumsum`, `describe`, `diff`, `ffill`, `first`, `head`, `idxmax`, `idxmin`, `last`, `max`, `mean`, `median`, `min`, `ngroup`, `nth`, `nunique`, `ohlc`, `pct_change`, `prod`, `quantile`, `rank`, `resample`, `rolling`, `sample`, `sem`, `shift`, `size`, `skew`, `kurt`, `std`, `sum`, `var`, `tail`, `take`, `value_counts` - Visualization: `boxplot`, `hist`, `plot` This documentation provides comprehensive information regarding `GroupBy` operations, which are essential for complex data analysis tasks. This foundational aspect of `pandas` enables splitting, applying, and combining operations on datasets, a common requirement in data analysis. <|Analysis End|> <|Question Begin|> **Coding Assessment Question** You are given a dataset containing information about various products, their categories, and sales figures over different periods. Your task is to implement a function using `pandas` that performs the following operations: 1. Group the data by the product category. 2. For each category, compute the total sales. 3. Identify the product with the highest sales in each category. 4. Compute the mean sales for each category. 5. Return a DataFrame with the following columns: - `category`: Product category name - `total_sales`: Total sales for the category - `top_product`: Name of the product with the highest sales in the category - `mean_sales`: Mean sales for the category **Input:** - A DataFrame `df` with columns: `product_name`, `category`, `sales` **Output:** - A DataFrame with columns: `category`, `total_sales`, `top_product`, `mean_sales` **Constraints:** - The DataFrame `df` contains at least one category and one product per category. - Sales figures are positive integers. **Example:** Given the following DataFrame: ``` product_name category sales 0 product1 A 150 1 product2 A 200 2 product3 B 300 3 product4 B 100 4 product5 A 250 ``` Your function should return: ``` category total_sales top_product mean_sales 0 A 600 product5 200.0 1 B 400 product3 200.0 ``` ```python import pandas as pd def analyze_sales(df): Analyze sales data to compute total sales, top product, and mean sales by category. Parameters: df (pd.DataFrame): DataFrame containing columns \'product_name\', \'category\', \'sales\' Returns: pd.DataFrame: DataFrame with columns \'category\', \'total_sales\', \'top_product\', \'mean_sales\' # Group by category grouped = df.groupby(\'category\') # Compute total sales for each category total_sales = grouped[\'sales\'].sum().reset_index(name=\'total_sales\') # Identify top product in each category idx = grouped[\'sales\'].idxmax() top_product = df.loc[idx, [\'category\', \'product_name\']].set_index(\'category\') # Compute mean sales for each category mean_sales = grouped[\'sales\'].mean().reset_index(name=\'mean_sales\') # Merge results into a single DataFrame result = total_sales.merge(top_product, on=\'category\').merge(mean_sales, on=\'category\') result.rename(columns={\'product_name\': \'top_product\'}, inplace=True) return result ``` **Note:** Your implementation should efficiently handle large datasets and avoid unnecessary computations.","solution":"import pandas as pd def analyze_sales(df): Analyze sales data to compute total sales, top product, and mean sales by category. Parameters: df (pd.DataFrame): DataFrame containing columns \'product_name\', \'category\', \'sales\' Returns: pd.DataFrame: DataFrame with columns \'category\', \'total_sales\', \'top_product\', \'mean_sales\' # Group by category grouped = df.groupby(\'category\') # Compute total sales for each category total_sales = grouped[\'sales\'].sum().reset_index(name=\'total_sales\') # Identify top product in each category idx = grouped[\'sales\'].idxmax() top_product = df.loc[idx, [\'category\', \'product_name\']].set_index(\'category\') # Compute mean sales for each category mean_sales = grouped[\'sales\'].mean().reset_index(name=\'mean_sales\') # Merge results into a single DataFrame result = total_sales.merge(top_product, on=\'category\').merge(mean_sales, on=\'category\') result.rename(columns={\'product_name\': \'top_product\'}, inplace=True) return result"},{"question":"Seaborn Plot Customization Objective Write a Python function that creates a customized Seaborn plot using specific properties and attributes as described below. This function should demonstrate the use of different Seaborn properties to achieve a clear and informative data visualization. Function Implementational Details The function should be named `custom_seaborn_plot` and should accept two parameters: 1. `data` (pandas DataFrame): The dataset to be visualized. 2. `output_file` (str): The filename to save the plot. Your function should: 1. Use the Seaborn `objects` interface to create a scatter plot involving `x` and `y` coordinates from the input data. 2. Map the `color` property to a column named `\'category\'`. 3. Apply a `logarithmic` transformation to the `x` axis coordinates. 4. Apply a `square root` transformation to the `size` property mapped to a column named `\'value\'`. 5. Customize the markers such that different categories have different shapes. 6. Set a custom color palette for the plot. 7. Ensure the plot has a title, labels for both axes, and a legend. 8. Save the resulting plot to the specified `output_file`. Constraints - The input `data` must have at least three columns: `x`, `y`, `category`, and `value`. - The `output_file` should be a valid filename with a supported image format extension (e.g., `.png`, `.jpg`, `.svg`). Example ```python import pandas as pd # Example dataframe data = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5], \'y\': [10, 15, 13, 17, 14], \'category\': [\'A\', \'B\', \'A\', \'B\', \'C\'], \'value\': [10, 20, 10, 25, 30] }) # Sample function call custom_seaborn_plot(data, \'output_plot.png\') ``` # Expected Outcome When the function is executed with the example above, it should produce a scatter plot saved as `output_plot.png` with the following features: - Different colors for different categories. - Logarithmic transformation on the `x` axis. - Square root transformation applied to the size of the markers. - Different marker shapes for different categories. - Titles and labels included for clarity. - A legend that clearly indicates the categories and corresponding markers. # Notes - Make sure to handle any common exceptions, such as when required columns are missing or when the output file cannot be created. - Refer to the provided Seaborn properties documentation to correctly use the coordinate properties, color properties, and other customization features.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def custom_seaborn_plot(data, output_file): Creates a customized Seaborn scatter plot and saves it to output_file. Parameters: data (pd.DataFrame): The dataset to be visualized, must contain \'x\', \'y\', \'category\', and \'value\' columns. output_file (str): The filename to save the plot. # Validate input DataFrame if not all(col in data.columns for col in [\'x\', \'y\', \'category\', \'value\']): raise ValueError(\\"Data must contain \'x\', \'y\', \'category\', and \'value\' columns.\\") # Define the plot sns.set(style=\\"whitegrid\\") p = sns.scatterplot( data=data, x=\'x\', y=\'y\', hue=\'category\', size=\'value\', sizes=(20, 200), palette=\'viridis\', legend=\'full\', style=\'category\', markers=True, ) # Apply transformations p.set(xscale=\'log\') # Set title and labels plt.title(\'Custom Seaborn Scatter Plot\') plt.xlabel(\'X axis (log scale)\') plt.ylabel(\'Y axis\') # Save the plot to file plt.savefig(output_file) plt.close()"},{"question":"# Advanced Python Coding Assessment **Objective:** Demonstrate ability to work with Unix-specific services in Python by designing a password generator and verifier using the \\"pwd\\" and \\"crypt\\" modules. **Question:** You are required to create a secure password generator and verifier using Unix password databases. The following functions must be implemented: 1. `generate_password(username: str) -> str` - Generates a strong, random password for the user. - The password must be at least 12 characters long, include a mix of upper and lower case letters, numbers, and special characters. - Store the encrypted form of the password in the system\'s password database using the `pwd` module. 2. `verify_password(username: str, password: str) -> bool` - Verifies if the provided password matches the stored password for the given username. - Use Unix password encryption for verification (`pwd` module). **Constraints and Notes:** - **Do not use any external libraries** for password generation or encryption; use the standard Unix libraries provided (`crypt`, `pwd`). - Assume the system is Unix-based and has access to the `pwd` and `crypt` modules. - The function `generate_password` should return the generated plain text password. - Ensure that the password generation is secure and uses a good source of randomness. - The username provided will always exist in the system\'s password database. **Example:** ```python >>> password = generate_password(\'johndoe\') >>> print(password) Generated_Password123! >>> verify_password(\'johndoe\', \'Generated_Password123!\') True >>> verify_password(\'johndoe\', \'Wrong_Password!\') False ``` **Input:** - `username`: `str`, a valid Unix username. - `password`: `str`, the password to be verified. **Output:** - For `generate_password`: `str`, the generated password. - For `verify_password`: `bool`, the result of the verification. **Performance Requirements:** - Ensure that the password generation is efficient and secure. - Verification should efficiently check against the stored password in Unix password database. Good luck!","solution":"import pwd import crypt import random import string def generate_password(username: str) -> str: Generates a strong, random password for the user and stores the encrypted form in the system\'s password database. # Generate a strong password with a mix of upper/lower case letters, numbers, and special characters characters = string.ascii_letters + string.digits + string.punctuation password = \'\'.join(random.choice(characters) for i in range(12)) # Encrypt the password using a salt (the first two characters for crypt.METHOD_SHA512) encrypted_password = crypt.crypt(password, crypt.mksalt(crypt.METHOD_SHA512)) # Update the system\'s password database, this step is hypothetical as we cannot actually do this in a secure manner # Just for illustration: update_user_password_in_system(username, encrypted_password) return password def verify_password(username: str, password: str) -> bool: Verifies if the provided password matches the stored password for the given username. try: # Get the user\'s information from the system\'s password database user_info = pwd.getpwnam(username) encrypted_password = user_info.pw_passwd # Encrypt the provided password using the same method encrypted_provided_password = crypt.crypt(password, encrypted_password) # Compare the provided encrypted password with the stored encrypted password return encrypted_password == encrypted_provided_password except KeyError: # If username is not found in the system, return False return False"},{"question":"As a data analyst, you have received a dataset in CSV format that contains information about sales transactions. Your task is to load this dataset into a pandas DataFrame and perform a series of operations to analyze the data. The dataset structure is as follows: | Column Name | Description | |--------------|-------------------------------------| | date | Transaction date (YYYY-MM-DD) | | store_id | Unique identifier for the store | | product | Name of the product sold | | quantity | Quantity of product sold | | price | Unit price of the product | | total_sales | Total sales amount (quantity * price)| # Instructions 1. **Data Loading and Cleaning** - Load the dataset from a CSV file into a pandas DataFrame. - Check for and handle any missing data appropriately (fill missing sales amount based on quantity and price, drop rows with missing critical data such as product or date). 2. **Data Manipulation** - Add a new column `week` which indicates the week number of the transaction date. - Ensure that all columns have appropriate data types (e.g., `date` should be of datetime type). 3. **Descriptive Statistics and Aggregations** - Calculate the total sales for each store in each week. The result should be a DataFrame with columns: `store_id`, `week`, and `total_sales`. - Identify the top 3 products by total sales for each store. The result should be a DataFrame with columns: `store_id`, `product`, `total_sales`. 4. **Data Visualization** - Plot a weekly sales trend for each store. The x-axis should represent the weeks and the y-axis should represent the total sales. Each store should have a separate line in the plot. (You may use any matplotlib or pandas plotting functions.) 5. **Output Format** - Your final solution should include functions that perform the described operations. Design it modularly, so the functions can be tested individually. # Constraints - The dataset is relatively large (potentially up to 1 million rows). # Functions Specification - `load_and_clean_data(file_path: str) -> pd.DataFrame`: Function to load the dataset and handle missing data. - `add_week_column(df: pd.DataFrame) -> pd.DataFrame`: Function to add a `week` column based on the transaction date. - `calculate_weekly_sales(df: pd.DataFrame) -> pd.DataFrame`: Function to calculate weekly sales for each store. - `top_products_by_store(df: pd.DataFrame, top_n: int = 3) -> pd.DataFrame`: Function to identify the top N products by total sales for each store. - `plot_sales_trend(df: pd.DataFrame) -> None`: Function to plot the weekly sales trend for each store. **Example Usage:** ```python import pandas as pd # Assuming required functions are defined as specified df = load_and_clean_data(\'sales_data.csv\') df = add_week_column(df) weekly_sales = calculate_weekly_sales(df) top_products = top_products_by_store(df, top_n=3) plot_sales_trend(weekly_sales) ``` # Note: Ensure to handle edge-cases such as missing, null, or inconsistent data as specified in the instructions.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def load_and_clean_data(file_path: str) -> pd.DataFrame: Load and clean the dataset from a CSV file. Fills missing total sales based on quantity and price, and drops rows with missing critical data like product or date. df = pd.read_csv(file_path) # Fill missing total_sales if quantity and price are available df[\'total_sales\'] = df.apply( lambda row: row[\'quantity\'] * row[\'price\'] if pd.isnull(row[\'total_sales\']) and not pd.isnull(row[\'quantity\']) and not pd.isnull(row[\'price\']) else row[\'total_sales\'], axis=1 ) # Drop rows with missing critical data df.dropna(subset=[\'date\', \'product\'], inplace=True) return df def add_week_column(df: pd.DataFrame) -> pd.DataFrame: Add a \'week\' column based on the transaction date. df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'week\'] = df[\'date\'].dt.isocalendar().week return df def calculate_weekly_sales(df: pd.DataFrame) -> pd.DataFrame: Calculate the total sales for each store in each week. weekly_sales = df.groupby([\'store_id\', \'week\'])[\'total_sales\'].sum().reset_index() return weekly_sales def top_products_by_store(df: pd.DataFrame, top_n: int = 3) -> pd.DataFrame: Identify the top N products by total sales for each store. total_sales_by_product = df.groupby([\'store_id\', \'product\'])[\'total_sales\'].sum().reset_index() total_sales_by_product[\'rank\'] = total_sales_by_product.groupby(\'store_id\')[\'total_sales\'].rank(method=\'first\', ascending=False) top_products = total_sales_by_product[total_sales_by_product[\'rank\'] <= top_n] top_products = top_products.drop(columns=[\'rank\']) return top_products def plot_sales_trend(df: pd.DataFrame) -> None: Plot the weekly sales trend for each store. plt.figure(figsize=(12, 6)) sns.lineplot(data=df, x=\'week\', y=\'total_sales\', hue=\'store_id\', marker=\'o\') plt.title(\'Weekly Sales Trend by Store\') plt.xlabel(\'Week\') plt.ylabel(\'Total Sales\') plt.grid(True) plt.legend(title=\'Store ID\') plt.show()"},{"question":"# Coding Assessment Objective The goal of this assessment is to write a function that, given an instance of `torch.export.ExportedProgram`, counts the number of each type of node in the graph (e.g., placeholders, call_function, and output nodes). Question We have an `ExportedProgram` in PyTorch, which includes a computational graph of a model. Within this graph, nodes can be of different types: `placeholder`, `call_function`, `get_attr`, `output`. We need to analyze the graph and count the occurrence of each type of node. Implement the function `count_node_types(exported_program)` that takes an `ExportedProgram` object as input and returns a dictionary where the keys are node types (e.g., \'placeholder\', \'call_function\', \'output\') and the values are the counts of these nodes in the graph. Function Signature ```python def count_node_types(exported_program: torch.export.ExportedProgram) -> dict: pass ``` Inputs: - `exported_program`: An instance of `torch.export.ExportedProgram`. Outputs: - A dictionary where keys are node types (\'placeholder\', \'call_function\', \'output\') and values are integers representing the count of each type. Example: ```python import torch from torch import nn class MyModule(nn.Module): def forward(self, x, y): return x + y example_args = (torch.randn(1), torch.randn(1)) exported_program = torch.export.export(MyModule(), example_args) node_counts = count_node_types(exported_program) print(node_counts) # Example output: {\'placeholder\': 2, \'call_function\': 1, \'output\': 1} ``` Constraints: - Use the `graph_module.graph` from the `exported_program` to access the nodes. - Only the nodes that exist in the `nodes` attribute of the graph should be considered. Notes: - You might need to familiarize yourself with the structure of `torch.fx.Graph` and `torch.fx.Node` as mentioned in the provided documentation. - Ensure that your solution is efficient and can handle relatively large graphs. Good luck!","solution":"def count_node_types(exported_program): Count the number of each type of node in the graph of an ExportedProgram. Parameters: exported_program (torch.export.ExportedProgram): The exported program containing the graph. Returns: dict: A dictionary where keys are node types (\'placeholder\', \'call_function\', \'output\', \'get_attr\') and values are the counts of these nodes in the graph. node_counts = {\'placeholder\': 0, \'call_function\': 0, \'output\': 0, \'get_attr\': 0} for node in exported_program.graph.nodes: node_counts[node.op] += 1 return node_counts"},{"question":"# Regular Expression Puzzle Objective: You are to write a function using Python\'s `re` module that processes a given text to identify and transform specific patterns according to the given rules. Problem Statement: Write a function named `process_text` that takes a string `text` as input and returns a string with the following transformations: 1. **Phone Numbers**: Replace all US phone numbers in the format `(xxx) xxx-xxxx` with `xxx-xxx-xxxx`. 2. **Dates**: Convert dates from `DD/MM/YYYY` format to `YYYY-MM-DD` format. 3. **Monetary Values**: Normalize monetary values by converting them to a standard format: replace `x,xxx.xx` with `USD x,xxx.xx`. Input: - A single string `text` that may contain multiple sentences and instances of the aforementioned patterns. Output: - A single transformed string with all specified patterns replaced accordingly. Constraints: - The input string consists of printable ASCII characters only. - The phone number format `(xxx) xxx-xxxx` always consists of digits for `x`. - The date format `DD/MM/YYYY` always uses digits and valid dates. - The monetary value format `x,xxx.xx` always uses digits and appropriate commas/decimals. Examples: ```python def process_text(text: str) -> str: # Your code goes here pass # Example 1: input_text = \\"My contact number is (123) 456-7890 and I was born on 23/04/1995. I have 1,234.56 in my account.\\" output = process_text(input_text) print(output) # Expected: # \\"My contact number is 123-456-7890 and I was born on 1995-04-23. I have USD 1,234.56 in my account.\\" # Example 2: input_text = \\"(111) 222-3333 and (444) 555-6666 are old numbers. We met on 05/12/2010.\\" output = process_text(input_text) print(output) # Expected: # \\"111-222-3333 and 444-555-6666 are old numbers. We met on 2010-12-05.\\" ``` Performance Requirements: - The function should be optimized to handle strings up to 10,000 characters efficiently. Use the `re` module\'s capabilities effectively to identify and transform the required patterns in the string.","solution":"import re def process_text(text: str) -> str: # Replace phone numbers text = re.sub(r\'((d{3})) (d{3})-(d{4})\', r\'1-2-3\', text) # Replace dates text = re.sub(r\'(d{2})/(d{2})/(d{4})\', r\'3-2-1\', text) # Replace monetary values text = re.sub(r\'(d{1,3}(?:,d{3})*.d{2})\', r\'USD 1\', text) return text"},{"question":"n-Dimensional FFT for Image Filtering Objective Implement a function to apply a Gaussian filter in the frequency domain on a 2D image using PyTorch\'s FFT functions. This requires: 1. Performing an FFT on the input image. 2. Constructing a Gaussian filter in the frequency domain. 3. Applying the filter to the FFT of the image. 4. Performing an inverse FFT to get the filtered image in the spatial domain. Instructions 1. **Function**: `gaussian_fft_filter` 2. **Input**: - A 2D tensor `image` representing the grayscale image of shape `(H, W)`. - A float `sigma` representing the standard deviation of the Gaussian filter. 3. **Output**: - A 2D tensor of the same shape as `image`, representing the filtered image. 4. **Constraints**: - You must use PyTorch\'s FFT-related functions. - The filtering should be done in the frequency domain. - The function should handle images of arbitrary size. 5. **Performance Requirements**: - The solution should efficiently handle images up to size `1024x1024`. Example ```python import torch import torch.fft def gaussian_fft_filter(image: torch.Tensor, sigma: float) -> torch.Tensor: # Your implementation here pass # Example usage image = torch.rand((256, 256)) # Example image sigma = 10.0 filtered_image = gaussian_fft_filter(image, sigma) # Check the result type and shape assert isinstance(filtered_image, torch.Tensor) assert filtered_image.shape == image.shape ``` Detailed Steps 1. **Perform FFT on the Image**: Use `torch.fft.fft2` to perform a 2D FFT on the image. 2. **Construct Gaussian Filter in Frequency Domain**: - Create coordinate grids for the frequency domain. - Use the standard Gaussian function to create the filter. 3. **Apply the Gaussian Filter**: Element-wise multiply the FFT of the image with the Gaussian filter. 4. **Inverse FFT to Get Filtered Image**: Use `torch.fft.ifft2` to perform the inverse FFT and obtain the filtered image in the spatial domain. 5. **Return the Real Part**: Since the inverse FFT might return complex numbers, return only the real part of the filtered image using `torch.real`.","solution":"import torch import torch.fft def gaussian_fft_filter(image: torch.Tensor, sigma: float) -> torch.Tensor: Applies a Gaussian filter in the frequency domain on a 2D image. Parameters: image (torch.Tensor): 2D tensor representing the grayscale image. sigma (float): Standard deviation of the Gaussian filter. Returns: torch.Tensor: 2D tensor representing the filtered image. # Get the shape of the image H, W = image.shape # Perform FFT on the input image image_fft = torch.fft.fft2(image) # Create coordinate grids for the frequency domain u = torch.fft.fftfreq(H, d=1.0) v = torch.fft.fftfreq(W, d=1.0) U, V = torch.meshgrid(u, v, indexing=\'ij\') # Construct the Gaussian filter in the frequency domain D = U**2 + V**2 gaussian_filter = torch.exp(-2 * (torch.pi**2) * (sigma**2) * D) # Apply the Gaussian filter to the FFT of the image filtered_fft = image_fft * gaussian_filter # Perform the inverse FFT to get the filtered image in the spatial domain filtered_image_complex = torch.fft.ifft2(filtered_fft) # Return the real part of the filtered image filtered_image = torch.real(filtered_image_complex) return filtered_image"},{"question":"**Objective:** Implement a custom PyTorch autograd `Function` that demonstrates the use of saving tensors for the backward pass and handles non-differentiable edge cases for a provided mathematical operation. # Problem Statement You are tasked with creating a custom autograd `Function` that performs a composite mathematical operation on an input tensor. The operation should be a piece-wise function defined as follows: [ y = f(x) text{ where } f(x) = begin{cases} 0 & x leq 0 x cdot log(x) & 0 < x leq 1 e^x - 1 & x > 1 end{cases} ] The custom `Function` should correctly handle the forward and backward passes: 1. **Forward Pass Requirements:** - Compute and return the output tensor `y` based on the input tensor `x`. - Save any intermediate tensors necessary for the backward pass. 2. **Backward Pass Requirements:** - Compute the gradient of the loss with respect to the input tensor `x`. - Implement the necessary logic to handle the non-differentiable points at (x = 0) and (x = 1). # Implementation Instructions 1. Define a custom autograd `Function` named `CompositeFunction`. 2. Implement the `forward` and `backward` static methods for `CompositeFunction`. 3. Forward Pass: - Save the intermediate tensors necessary for the backward pass using `ctx.save_for_backward`. 4. Backward Pass: - Retrieve the saved tensors using `ctx.saved_tensors`. - Compute the gradient of the loss with respect to the input tensor `x`. 5. Ensure proper handling of non-differentiable points: - (f(0) = 0) (For (x = 0), assume the gradient to be 0) - (f\'(1) = 1) (The derivative of the function is continuous at (x = 1)) 6. Write a test function that: - Initializes a tensor with values spanning the regions (x leq 0), (0 < x leq 1), and (x > 1). - Applies the custom `Function` and computes the gradients using autograd. - Prints the input, output, and gradients. # Example ```python import torch from torch.autograd import Function class CompositeFunction(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) y = torch.where(x <= 0, torch.tensor(0.0, device=x.device), torch.where(x <= 1, x * torch.log(x), torch.exp(x) - 1)) return y @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_x = torch.where(x <= 0, torch.tensor(0.0, device=x.device), torch.where(x <= 1, grad_output * (1 + torch.log(x)), grad_output * torch.exp(x))) return grad_x # Testing the implementation def test_composite_function(): x = torch.tensor([-1.0, 0.0, 0.5, 1.0, 1.5, 2.0], requires_grad=True) y = CompositeFunction.apply(x) y.sum().backward() print(\\"Input:\\", x) print(\\"Output:\\", y) print(\\"Gradients:\\", x.grad) test_composite_function() ``` **Expected Output:** ``` Input: tensor([-1.0000, 0.0000, 0.5000, 1.0000, 1.5000, 2.0000], grad_fn=<CloneBackward>) Output: tensor([0.0000, 0.0000, -0.3466, 0.0000, 3.4817, 6.3891], grad_fn=<CompositeFunctionBackward>) Gradients: tensor([0.0000, 0.0000, 1.6931, 1.0000, 4.4817, 6.3891]) ``` # Constraints 1. Do not use any external libraries other than PyTorch. 2. Handle edge cases and ensure no division by zero or invalid logarithm operations. 3. Your solution should be efficient in both time and space complexity. # Submission Instructions Submit your solution as a `.py` file containing the implementation of `CompositeFunction` and the `test_composite_function` function. Ensure that your code runs without errors and produces the expected output.","solution":"import torch from torch.autograd import Function class CompositeFunction(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) y = torch.where(x <= 0, torch.tensor(0.0, device=x.device), torch.where(x <= 1, x * torch.log(x), torch.exp(x) - 1)) return y @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_x = torch.where(x <= 0, torch.tensor(0.0, device=x.device), torch.where(x <= 1, grad_output * (1 + torch.log(x)), grad_output * torch.exp(x))) return grad_x"},{"question":"# Advanced Python `multiprocessing` Assessment Using the `multiprocessing` module, write a Python program that performs the following tasks: 1. **Task 1: Calculate Factorial in Parallel** - Create a function `factorial_task(n: int) -> int` that calculates the factorial of a given integer `n` (where 0 <= n <= 1000). - Create another function `parallel_factorials(numbers: List[int], num_processes: int) -> List[int]` that takes a list of integers `numbers` and an integer `num_processes`, and uses a pool of worker processes to compute the factorial for each number in the list concurrently and efficiently. 2. **Task 2: Synchronize Output with Locks** - Create a function `print_factorial_results(results: List[int])` that prints each result on a new line, but ensures no two processes print at the same time using synchronization primitives. 3. **Task 3: Shared Memory for Sum Calculation** - Compute the sum of the factorial results calculated in Task 1. Use shared memory to store the sum. Define a function `shared_memory_sum(results: List[int], num_processes: int) -> int`. # Requirements **Function Definitions:** 1. Define `factorial_task(n: int) -> int`. 2. Define `parallel_factorials(numbers: List[int], num_processes: int) -> List[int]` - **Inputs:** - `numbers`: A list of integers to compute factorial. - `num_processes`: An integer specifying the number of processes in the pool. - **Output:** - A list of integers representing the factorial of each number. - **Constraints:** Use `multiprocessing.Pool` for parallel computation. 3. Define `print_factorial_results(results: List[int]) -> None` 4. Define `shared_memory_sum(results: List[int], num_processes: int) -> int` - **Inputs:** - `results`: A list of integers whose sum needs to be calculated. - `num_processes`: An integer specifying the number of processes that will access the shared memory. - **Output:** - An integer representing the sum of the factorial results. - **Constraints:** Use `multiprocessing.Value` for shared memory. **Example Usage:** ```python if __name__ == \\"__main__\\": # Sample input numbers = [5, 7, 10] num_processes = 3 # Parallel factorial calculation factorials = parallel_factorials(numbers, num_processes) # Printing results print_factorial_results(factorials) # Calculating sum using shared memory total_sum = shared_memory_sum(factorials, num_processes) print(\\"Sum of factorials:\\", total_sum) ``` **Additional Notes:** - Ensure your code handles edge cases and is robust against different input sizes. - Properly handle exceptions and clean up processes to avoid any resource leaks. - Document your code with comments to explain the logic and flow.","solution":"import multiprocessing from typing import List def factorial_task(n: int) -> int: Function to calculate the factorial of a given integer n. if n == 0: return 1 factorial = 1 for i in range(1, n + 1): factorial *= i return factorial def parallel_factorials(numbers: List[int], num_processes: int) -> List[int]: Function to calculate factorials of a list of numbers using multiple processes. with multiprocessing.Pool(processes=num_processes) as pool: results = pool.map(factorial_task, numbers) return results def print_factorial_results(results: List[int]) -> None: Function to print each factorial result on a new line ensuring no two processes print at the same time. lock = multiprocessing.Lock() with lock: for result in results: print(result) def shared_memory_sum(results: List[int], num_processes: int) -> int: Function to calculate the sum of factorial results using shared memory. total_sum = multiprocessing.Value(\'i\', 0) def update_sum(start: int, end: int): nonlocal results local_sum = sum(results[start:end]) with total_sum.get_lock(): # Synchronize access to shared memory total_sum.value += local_sum chunk_size = len(results) // num_processes processes = [] for i in range(num_processes): start = i * chunk_size end = (i + 1) * chunk_size if i != num_processes - 1 else len(results) p = multiprocessing.Process(target=update_sum, args=(start, end)) processes.append(p) p.start() for p in processes: p.join() return total_sum.value"},{"question":"# Assessment Question Objective Demonstrate your understanding and ability to utilize the MPS backend in PyTorch for high-performance training on MacOS devices with the Metal programming framework. Problem Statement Using the MPS backend in PyTorch, create and perform operations on tensors and a simple neural network on the MPS device. Write a Python function `test_mps_backend()` that does the following: 1. Checks if the MPS backend is available. 2. If available, performs the following tasks: - Creates a tensor of shape `(3, 3)` filled with random values directly on the MPS device. - Defines a simple neural network with one linear layer (input size 3, output size 2). - Moves the neural network to the MPS device. - Computes the output of the neural network with the created tensor as input. - Returns the output tensor. 3. If the MPS backend is not available, returns `None`. Function Signature ```python import torch import torch.nn as nn def test_mps_backend() -> torch.Tensor: # Your code here pass ``` Example Given that the MPS backend is available and device state executions proceed as described, an example flow would be: ```python output = test_mps_backend() print(output) # Example output could be a tensor on MPS device, e.g., tensor([[-0.4635, 0.6124], [-0.0560, 0.4312], [-0.2357, 0.9234]], device=\'mps:0\') ``` **Constraint:** - The function should ensure that all operations are performed on the MPS device if available. Notes: - The function needs to handle the case where the MPS backend is not enabled or available gracefully by returning `None`. - Use appropriate methods to check for MPS device availability as described in the documentation.","solution":"import torch import torch.nn as nn def test_mps_backend() -> torch.Tensor: # Check if MPS device is available if not torch.backends.mps.is_available(): return None # Create a tensor of shape (3, 3) filled with random values directly on the MPS device tensor = torch.rand((3, 3), device=\'mps\') # Define a simple neural network with one linear layer class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.linear = nn.Linear(3, 2) def forward(self, x): return self.linear(x) # Instantiate the neural network and move it to the MPS device net = SimpleNN().to(\'mps\') # Compute the output of the neural network with the created tensor as input output = net(tensor) # Return the output tensor return output"},{"question":"**Advanced Python Coding Assessment** # Objective: Demonstrate understanding and usage of the builtins module by creating a custom wrapper class around a built-in function. # Problem Statement: You are required to create a custom file context manager that reads files but also provides additional functionality of counting the number of words in the file. Your task is as follows: 1. **Create a custom context manager class `WordCountFileManager`.** 2. This class should internally use Python\'s built-in `open()` function. 3. The class should provide methods: - `read()` to read the entire content of the file. - `word_count()` to return the number of words in the file. Words are defined as sequences of characters separated by whitespace. 4. Ensure that your custom context manager works with the `with` statement, ensuring resource management (i.e., the file is properly closed after usage). # Input Format: - The input will be the path to the file that you want to read. # Output Format: - You should print the content of the file using the `read()` method. - You should print the number of words using the `word_count()` method. # Constraints: - The file path provided will always be valid and the file will always be readable. - Assume the file contains plain text. - Python\'s built-in `open()` function should be used for file operations. # Example Usage: ```python class WordCountFileManager: def __init__(self, path): # Your implementation here pass def __enter__(self): # Your implementation here pass def __exit__(self, exc_type, exc_val, exc_tb): # Your implementation here pass def read(self): # Your implementation here pass def word_count(self): # Your implementation here pass # Usage file_path = \'sample.txt\' with WordCountFileManager(file_path) as file_manager: print(file_manager.read()) print(\\"Word count:\\", file_manager.word_count()) ``` Here, you must: - Implement the `__enter__` and `__exit__` methods to handle file opening and closing. - Use the `builtins.open` function to open the file. - Implement the `read` method to return the file content. - Implement the `word_count` method to count and return the number of words in the file. # Note: Ensure that you handle resources correctly so that files are not left open accidentally.","solution":"class WordCountFileManager: def __init__(self, path): self.path = path self.file = None self.content = None def __enter__(self): self.file = open(self.path, \'r\') self.content = self.file.read() return self def __exit__(self, exc_type, exc_val, exc_tb): if self.file: self.file.close() def read(self): return self.content def word_count(self): return len(self.content.split())"},{"question":"# Priority Queue Management with `heapq` Problem Statement You are tasked with implementing a simple priority queue for managing tasks using the `heapq` module. The priority queue should allow adding tasks with priorities, updating the priorities of existing tasks, and retrieving tasks based on their priority. The priority queue must ensure that tasks with the same priority are returned in the order they were added. Your implementation should support the following operations: 1. **Add a New Task**: Add a task with a given priority to the priority queue. 2. **Update Task Priority**: Update the priority of an existing task in the priority queue. 3. **Pop Task**: Remove and return the task with the highest priority (lowest numerical value) from the priority queue. You must handle the edge cases where the priority queue is empty when popping a task, and ensure that the priority queue maintains its properties after each operation. Function Signatures You should implement the following functions: 1. `add_task(pq: List[Tuple[int, int, str]], entry_finder: Dict[str, List], counter: Iterator[int], task: str, priority: int) -> None` - **Parameters**: - `pq`: The priority queue represented as a list of tuples. - `entry_finder`: A dictionary mapping each task to its respective entry in the priority queue. - `counter`: An iterator to keep track of the order in which tasks are added. - `task`: The task to be added or updated. - `priority`: The priority of the task. - **Functionality**: Adds a new task or updates the priority of an existing task. 2. `remove_task(pq: List[Tuple[int, int, str]], entry_finder: Dict[str, List], task: str) -> None` - **Parameters**: - `pq`: The priority queue represented as a list of tuples. - `entry_finder`: A dictionary mapping each task to its respective entry in the priority queue. - `task`: The task to be removed. - **Functionality**: Marks the task as removed without breaking the heap structure. 3. `pop_task(pq: List[Tuple[int, int, str]], entry_finder: Dict[str, List]) -> str` - **Parameters**: - `pq`: The priority queue represented as a list of tuples. - `entry_finder`: A dictionary mapping each task to its respective entry in the priority queue. - **Returns**: The task with the highest priority. - **Functionality**: Removes and returns the task with the highest priority from the priority queue. Constraints - The priority of tasks is an integer, with lower numbers indicating higher priority. - Task descriptions are unique strings. Example Usage ```python import itertools from heapq import heappush, heappop # Initialize data structures pq = [] entry_finder = {} counter = itertools.count() # Add tasks with priorities add_task(pq, entry_finder, counter, \'task1\', 5) add_task(pq, entry_finder, counter, \'task2\', 3) add_task(pq, entry_finder, counter, \'task3\', 6) # Update priority of an existing task add_task(pq, entry_finder, counter, \'task1\', 1) # Pop tasks print(pop_task(pq, entry_finder)) # Output: \'task1\' print(pop_task(pq, entry_finder)) # Output: \'task2\' ``` Ensure that your code is efficient and follows the constraints. You may assume that the input tasks and priorities are valid.","solution":"import itertools import heapq REMOVED = \'<removed-task>\' def add_task(pq, entry_finder, counter, task, priority): Add a task or update an existing task\'s priority. if task in entry_finder: remove_task(pq, entry_finder, task) count = next(counter) entry = [priority, count, task] entry_finder[task] = entry heapq.heappush(pq, entry) def remove_task(pq, entry_finder, task): Mark an existing task as REMOVED. Raise KeyError if not found. entry = entry_finder.pop(task) entry[-1] = REMOVED def pop_task(pq, entry_finder): Remove and return the lowest priority task. Raise KeyError if empty. while pq: priority, count, task = heapq.heappop(pq) if task is not REMOVED: del entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"**Objective**: Evaluate the student\'s understanding of creating, manipulating, and analyzing `pandas` DataFrames, including memory usage, boolean operations, user-defined functions, and handling of missing values. **Problem Statement**: You are given a task to analyze and manipulate a dataset using `pandas`. Your task is to write a Python function `analyze_dataframe(df: pd.DataFrame) -> pd.DataFrame`, which takes a `pandas` DataFrame as input and performs the following operations: 1. **Memory Usage Calculation**: - Print the memory usage of the DataFrame using the `info` method. - Calculate and return the total memory usage of the DataFrame in bytes using the `memory_usage` method. 2. **Handling Boolean Operations**: - Check if any value in the DataFrame is `True` using the appropriate pandas method and print the result. 3. **User-Defined Function for Manipulation**: - Create a user-defined function `replace_odds_with_nans` that: - Takes a Series as input. - Replaces all odd values in the Series with `np.nan`. - Returns the modified Series. - Apply this function to each column of the DataFrame and print the resulting DataFrame. 4. **Handling Missing Values**: - After applying the function, check for any missing values in the DataFrame using `isna` or `notna` methods. - If there are any missing values, fill them with the mean of their respective columns. - Print the final DataFrame after handling missing values. 5. **Performance Constraints**: - Ensure that your function handles DataFrames with up to 10,000 rows and 100 columns efficiently. **Input**: - `df`: A pandas DataFrame with up to 10,000 rows and 100 columns. The columns contain a mix of numeric and boolean data. **Output**: - A pandas DataFrame after performing all the specified manipulations. **Example**: ```python import pandas as pd import numpy as np def analyze_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Memory Usage Calculation print(\\"Memory Usage of DataFrame:\\") df.info(memory_usage=\\"deep\\") total_memory = df.memory_usage(deep=True).sum() print(f\\"Total Memory Usage: {total_memory} bytes\\") # Step 2: Handling Boolean Operations print(\\"Any True value in the DataFrame:\\", df.any().any()) # Step 3: User-Defined Function for Manipulation def replace_odds_with_nans(series: pd.Series) -> pd.Series: return series.apply(lambda x: np.nan if x % 2 else x) df = df.apply(replace_odds_with_nans) print(\\"DataFrame after replacing odd values with NaNs:\\") print(df) # Step 4: Handling Missing Values if df.isna().any().any(): df = df.fillna(df.mean()) print(\\"Final DataFrame after filling NaNs with column mean:\\") print(df) return df # Example usage with random DataFrame dtypes = [ \\"int64\\", \\"float64\\", \\"object\\", \\"bool\\" ] n = 1000 data = {t: np.random.randint(0, 10, size=n).astype(t) for t in dtypes if t != \\"bool\\"} data[\\"bool\\"] = np.random.choice([True, False], size=n) df = pd.DataFrame(data) df[\\"object\\"] = df[\\"object\\"].astype(\\"category\\") result_df = analyze_dataframe(df) ``` **Note**: The example provided generates a random DataFrame for testing purposes. The function `analyze_dataframe` should handle any valid DataFrame input as specified.","solution":"import pandas as pd import numpy as np def analyze_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Memory Usage Calculation print(\\"Memory Usage of DataFrame:\\") df.info(memory_usage=\\"deep\\") total_memory = df.memory_usage(deep=True).sum() print(f\\"Total Memory Usage: {total_memory} bytes\\") # Step 2: Handling Boolean Operations print(\\"Any True value in the DataFrame:\\", df.any().any()) # Step 3: User-Defined Function for Manipulation def replace_odds_with_nans(series: pd.Series) -> pd.Series: if pd.api.types.is_numeric_dtype(series): return series.apply(lambda x: np.nan if x % 2 != 0 else x) return series df = df.apply(replace_odds_with_nans) print(\\"DataFrame after replacing odd values with NaNs:\\") print(df) # Step 4: Handling Missing Values if df.isna().any().any(): df = df.fillna(df.mean(numeric_only=True)) print(\\"Final DataFrame after filling NaNs with column mean:\\") print(df) return df"},{"question":"# Concurrency with Threading in Python **Objective:** Your task is to design a concurrent system using Python\'s `threading` module. This system involves multiple workers processing a shared resource, ensuring safe concurrent access using synchronization mechanisms. **Problem Statement:** Suppose you manage a small online service where your users can submit tasks to be processed. Each task has a unique identifier and the tasks data. To maximize throughput, you want to use multiple worker threads to process these tasks in parallel. However, tasks must be processed in the order they are received due to certain dependencies between them. Implement a threaded task processing system using Python\'s `threading` module that follows these requirements: 1. **Task Manager**: - A class `TaskManager` that manages a queue of tasks. It must support methods to add a new task, retrieve the next task for processing, and indicate when there are no more tasks left. 2. **Worker**: - A class `Worker` that represents a worker thread responsible for processing tasks. Each worker should retrieve tasks from the `TaskManager` and process them in the order they were received. 3. **Synchronization**: - Use a suitable synchronization mechanism (e.g., `Lock`, `Condition`) to ensure that tasks are processed in the correct order without conflicts. 4. **Thread Management**: - Implement a class `ThreadPool` that creates and manages a pool of worker threads. **Classes and Methods Details**: 1. **TaskManager**: ```python class TaskManager: def __init__(self): # Initialize the task queue and any necessary synchronization primitives. def add_task(self, task_id, data): Add a new task to the queue. :param task_id: Unique identifier for the task. :param data: The data associated with the task. def get_next_task(self): Retrieve the next task from the queue. :return: Tuple containing (task_id, data) of the next task, or None if no tasks are available. def has_tasks(self): Check if there are any tasks left in the queue. :return: Boolean indicating if there are tasks left. ``` 2. **Worker**: ```python class Worker(threading.Thread): def __init__(self, task_manager): super().__init__() # Initialize the worker with a reference to the task manager and other necessary variables. def run(self): # Main method of the worker thread. Retrieve and process tasks in a loop until no tasks are left. ``` 3. **ThreadPool**: ```python class ThreadPool: def __init__(self, num_workers, task_manager): # Initialize the thread pool with a number of worker threads and a task manager. def start_workers(self): # Start all worker threads. def join_workers(self): # Wait for all worker threads to finish. ``` **Example Usage**: ```python if __name__ == \\"__main__\\": manager = TaskManager() # Add tasks manager.add_task(1, \\"task_data_1\\") manager.add_task(2, \\"task_data_2\\") # Add more tasks as needed # Create and start the thread pool pool = ThreadPool(4, manager) pool.start_workers() # Wait for all workers to complete pool.join_workers() ``` **Constraints**: - Assume there are no more than 10,000 tasks. - Assume each task takes a variable amount of time to process. **Notes**: - Ensure that the system is thread-safe. - You must use Python\'s `threading` module and appropriate synchronization mechanisms. - Do not use any external libraries for thread management or synchronization. **Performance Requirement**: - The system should efficiently utilize the provided worker threads. - Minimize overhead from thread synchronization to avoid significant performance bottlenecks.","solution":"import threading from queue import Queue class TaskManager: def __init__(self): self.task_queue = Queue() self.lock = threading.Lock() def add_task(self, task_id, data): with self.lock: self.task_queue.put((task_id, data)) def get_next_task(self): with self.lock: if not self.task_queue.empty(): return self.task_queue.get() else: return None def has_tasks(self): with self.lock: return not self.task_queue.empty() class Worker(threading.Thread): def __init__(self, task_manager): super().__init__() self.task_manager = task_manager def run(self): while self.task_manager.has_tasks(): task = self.task_manager.get_next_task() if task: self.process_task(task) def process_task(self, task): task_id, data = task # Here, \\"processing\\" the task just means printing it; replace with actual processing logic print(f\\"Processing task {task_id} with data: {data}\\") class ThreadPool: def __init__(self, num_workers, task_manager): self.num_workers = num_workers self.task_manager = task_manager self.workers = [Worker(task_manager) for _ in range(num_workers)] def start_workers(self): for worker in self.workers: worker.start() def join_workers(self): for worker in self.workers: worker.join()"},{"question":"# Unicode Handling and Normalization in Python Background Unicode is a specification that represents every character used by human languages through code points. In Python, Unicode strings are sequences of code points. Encodings, like UTF-8, translate Unicode strings into sequences of bytes. For various tasks, such as comparing strings for equality or working with files in different encodings, it is crucial to understand Unicode normalization. Normalization converts a Unicode string into a standard form, so that strings that visually look the same, but are made up of different sequences of code points, can be treated equivalently. Suppose you are tasked with developing a Python utility that will read a list of Unicode strings from a file, normalize them, and then save the normalized strings to another file in a specified encoding. Task 1. **Implement a function `normalize_unicode_strings`** that takes three arguments: - `input_file`: A string representing the path to the input file containing Unicode strings. - `output_file`: A string representing the path to the output file where normalized Unicode strings should be written. - `encoding`: A string representing the encoding to be used for writing the output file (e.g., \'utf-8\'). The input file contains one Unicode string per line. The function should: - Read all Unicode strings from `input_file`. - Normalize each string using NFKC normalization form. - Write the normalized strings to `output_file` using the specified encoding. 2. **Ensure that the output file** is opened with proper encoding handling, and any encoding/decoding errors are handled by replacing invalid characters with `?`. 3. **Implement a `main` function** to demonstrate the usage of `normalize_unicode_strings`. The `main` function should: - Create a sample input file with some Unicode strings. - Call `normalize_unicode_strings` to process these strings. - Read and print the contents of the output file. Constraints * You must use the `UNICODE-2.0` library functions appropriately. * The normalization form to be used is `NFKC`. * The input and output files may include a mix of various Unicode characters. * Handle any potential exceptions that might occur during file operations gracefully. Example Here is a partial implementation outline: ```python import unicodedata def normalize_unicode_strings(input_file, output_file, encoding): with open(input_file, \'r\', encoding=\'utf-8\') as infile: lines = infile.readlines() normalized_lines = [unicodedata.normalize(\'NFKC\', line.strip()) for line in lines] with open(output_file, \'w\', encoding=encoding, errors=\'replace\') as outfile: for line in normalized_lines: outfile.write(line + \'n\') def main(): # Create a sample input file input_file = \'sample_input.txt\' with open(input_file, \'w\', encoding=\'utf-8\') as file: file.write(\'n\') file.write(\'eu0302n\') output_file = \'normalized_output.txt\' # Normalize and write to output file normalize_unicode_strings(input_file, output_file, \'utf-8\') # Read and print the output for demonstration with open(output_file, \'r\', encoding=\'utf-8\') as file: print(file.read()) if __name__ == \\"__main__\\": main() ``` Feel free to modify and extend this example to complete the task.","solution":"import unicodedata def normalize_unicode_strings(input_file, output_file, encoding): Normalize all Unicode strings in the input file using NFKC form and write the normalized strings to the output file with the specified encoding. Parameters: input_file (str): Path to the input file containing Unicode strings. output_file (str): Path to the output file for writing normalized strings. encoding (str): Encoding to be used for writing the output file. try: with open(input_file, \'r\', encoding=\'utf-8\') as infile: lines = infile.readlines() normalized_lines = [unicodedata.normalize(\'NFKC\', line.strip()) for line in lines] with open(output_file, \'w\', encoding=encoding, errors=\'replace\') as outfile: for line in normalized_lines: outfile.write(line + \'n\') except Exception as e: print(f\\"An error occurred: {e}\\") def main(): # Create a sample input file input_file = \'sample_input.txt\' with open(input_file, \'w\', encoding=\'utf-8\') as file: file.write(\'n\') file.write(\'eu0302n\') file.write(\'n\') output_file = \'normalized_output.txt\' # Normalize and write to output file normalize_unicode_strings(input_file, output_file, \'utf-8\') # Read and print the output for demonstration with open(output_file, \'r\', encoding=\'utf-8\') as file: print(file.read()) if __name__ == \\"__main__\\": main()"},{"question":"# Objective To assess students\' understanding of using SystemTap to monitor CPython processes and write scripts that gather and display specific data about the function calls within a Python script. # Task You are required to write a Python script and a SystemTap script to monitor and report the function call hierarchy and execution counts in the Python script. # Steps 1. **Python Script**: Implement a Python script (`script.py`) with the following functions: - `start()`: The main function that calls other functions. - `function_1()`: Calls `function_3()`. - `function_2()`: Calls `function_1()` and `function_3()`. - `function_3()`: A simple function. - `function_4()`: A simple function. - `function_5()`: Calls `function_4()`. 2. **SystemTap Script**: Implement a SystemTap script (`monitor.stp`) that: - Monitors function calls and returns within the `script.py`. - Tracks and prints the call/return hierarchy. - Reports the top 10 most frequently entered functions every second during the execution of the Python script. # Constraints - The SystemTap script should make use of the available static markers (`function__entry`, `function__return`) for CPython. - The Python script should be executed with `python3.6`. # Expected Output When running the SystemTap script with the Python script, the output should display: - The hierarchical call/return of functions in the Python script. - A periodic report of the top 10 most frequently entered functions and their call counts. # Performance Requirements - The SystemTap script should efficiently handle monitoring and reporting without causing significant overhead. # Example **`script.py`**: ```python def function_3(): pass def function_1(): function_3() def function_2(): function_1() function_3() def function_4(): pass def function_5(): function_4() def start(): function_1() function_2() function_3() function_4() function_5() if __name__ == \\"__main__\\": start() ``` **`monitor.stp`**: ```systemtap // Monitor function calls and returns, and report the top 10 most frequently entered functions. global fn_calls; probe python.function.entry { printf(\\"%s => %s in %s:%dn\\", thread_indent(1), funcname, filename, lineno); fn_calls[tid(), filename, funcname, lineno] += 1; } probe python.function.return { printf(\\"%s <= %s in %s:%dn\\", thread_indent(-1), funcname, filename, lineno); } probe timer.ms(1000) { printf(\\"033[2J033[1;1H\\") /* clear screen */ printf(\\"%6s %80s %6s %30s %6sn\\", \\"TID\\", \\"FILENAME\\", \\"LINE\\", \\"FUNCTION\\", \\"CALLS\\") foreach ([tid, filename, funcname, lineno] in fn_calls limit 10) { printf(\\"%6d %80s %6d %30s %6dn\\", tid, filename, lineno, funcname, fn_calls[tid, filename, funcname, lineno]); } delete fn_calls; } ``` To invoke the SystemTap script with the Python script, run: ```sh stap monitor.stp -c \\"python3.6 script.py\\" ``` # Submission Submit the `script.py` and `monitor.stp` files.","solution":"def function_3(): pass def function_1(): function_3() def function_2(): function_1() function_3() def function_4(): pass def function_5(): function_4() def start(): function_1() function_2() function_3() function_4() function_5() if __name__ == \\"__main__\\": start()"},{"question":"# JSON Handling and Custom Serialization in Python The `json` module in Python provides tools for working with JSON data, enabling developers to serialize Python objects to JSON format and deserialize JSON strings back into Python objects. It offers extensive customization capabilities via subclassing and hooks. **Problem Statement:** You are tasked with implementing a suite of Python functions to handle JSON serialization and deserialization, including custom handling for specific data types not supported by default. Specifically, you need to create functions that: 1. Serialize complex numbers in a custom format during JSON string conversion. 2. Deserialize that custom format back to Python complex objects. 3. Pretty-print JSON data with sorted keys and specific indentation. # Function Requirements: 1. **Function: `serialize_complex(obj)`**: - **Input**: A Python object `obj` which may contain complex numbers. - **Output**: A JSON string where complex numbers are represented as `{ \\"real\\": x, \\"imag\\": y }`. - This function serves as a custom encoder for complex numbers. 2. **Function: `deserialize_complex(dct)`**: - **Input**: A dictionary `dct` which may contain entries with the custom complex number representation `{ \\"real\\": x, \\"imag\\": y }`. - **Output**: A Python object where such dictionaries are converted back to complex numbers. - This function serves as a custom decoder for complex numbers. 3. **Function: `pretty_print_json(data)`**: - **Input**: A JSON string `data`. - **Output**: A pretty-printed version of the JSON string with sorted keys and an indentation level of 4 spaces. # Implementation Constraints: - Use the `json` module\'s capability to handle complex numbers. - Ensure proper handling of nested structures where complex numbers may be embedded within lists or dictionaries. - Demonstrate robustness by handling possible edge cases such as missing fields in the custom complex representation during deserialization. # Example Usage: ```python # Define a Python dictionary containing complex numbers data = { \\"number\\": 1, \\"complex_num\\": 2 + 3j, \\"list\\": [1, 2, {\\"complex_num\\": 4 - 5j}] } # Convert the Python object to a JSON string with custom serialization json_str = json.dumps(data, default=serialize_complex) print(json_str) # Expected output: # \'{\\"number\\": 1, \\"complex_num\\": {\\"real\\": 2.0, \\"imag\\": 3.0}, \\"list\\": [1, 2, {\\"complex_num\\": {\\"real\\": 4.0, \\"imag\\": -5.0}}]}\' # Convert the JSON string back to a Python object with custom deserialization new_data = json.loads(json_str, object_hook=deserialize_complex) print(new_data) # Expected output: # {\'number\': 1, \'complex_num\': (2+3j), \'list\': [1, 2, {\'complex_num\': (4-5j)}]} # Pretty-print the JSON string pretty_print_json(json_str) # Expected output: # { # \\"number\\": 1, # \\"complex_num\\": { # \\"real\\": 2.0, # \\"imag\\": 3.0 # }, # \\"list\\": [ # 1, # 2, # { # \\"complex_num\\": { # \\"real\\": 4.0, # \\"imag\\": -5.0 # } # } # ] # } ``` **Note:** Ensure to handle any necessary imports and configurations for the testing environment.","solution":"import json def serialize_complex(obj): Custom JSON serializer for complex numbers. if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} raise TypeError(f\\"Object of type {obj.__class__.__name__} is not JSON serializable\\") def deserialize_complex(dct): Custom JSON deserializer for complex numbers. if \\"real\\" in dct and \\"imag\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def pretty_print_json(data): Pretty prints a JSON string with sorted keys and indentation of 4 spaces. parsed = json.loads(data) print(json.dumps(parsed, sort_keys=True, indent=4))"},{"question":"# Email Message Processing You are given a task to process and analyze email messages. Using the methods provided by the `email.iterators` module, you will implement a function that extracts and processes the content from an email message tree. **Function Signature**: ```python def process_email_structure(email_message: EmailMessage, search_maintype: str, search_subtype: str = None) -> dict: ``` **Parameters**: - `email_message` (EmailMessage): The email message object that you need to process. - `search_maintype` (str): The MIME main type to search for within the email message. - `search_subtype` (str, optional): The MIME subtype to search for within the email message. Defaults to `None`. **Returns**: - A dictionary with the following structure: ```python { \\"lines\\": List[str], # All lines from payloads of type (search_maintype/search_subtype) \\"structured_summary\\": str # String representation of message structure } ``` **Requirements**: 1. Extract and return all lines of the payload that match the MIME `search_maintype/search_subtype` using `typed_subpart_iterator`. 2. Print and return an indented structured summary of the email message using `_structure`. # Constraints: - Assume the email message object is well-formed and valid. - Aim for efficiency and readability in your implementation. - Avoid using any additional external libraries for email parsing. # Example: ```python msg = email.message_from_string(MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"XX\\" --XX Content-Type: text/plain Hello World! --XX Content-Type: text/html <b>Hello World!</b> --XX--) result = process_email_structure(msg, search_maintype=\'text\', search_subtype=\'plain\') print(result) ``` Expected Output: ```python { \\"lines\\": [\\"Hello World!\\"], \\"structured_summary\\": \\"multipart/mixedn text/plainn text/htmln\\" } ``` Implement the function `process_email_structure` to achieve the above requirements.","solution":"import email from email.message import EmailMessage from email import iterators from typing import List def process_email_structure(email_message: EmailMessage, search_maintype: str, search_subtype: str = None) -> dict: Processes an email message and returns the content lines of a specific MIME type and a structured summary. Parameters: email_message - EmailMessage: The email message object that needs to be processed. search_maintype - str: The MIME main type to search for within the email message. search_subtype - str: The MIME subtype to search for within the email message. Defaults to None. Returns: dict: A dictionary with lines from the payloads of type (search_maintype/search_subtype) and a string representation of the message structure. result_lines = [] # Extract lines for parts matching the media type for part in iterators.typed_subpart_iterator(email_message, search_maintype, search_subtype): result_lines.extend(part.get_payload().splitlines()) # Create a structure summary structure_summary = [] def _structure(msg, lvl=0): structure_summary.append(\\" \\" * lvl + msg.get_content_type()) if msg.is_multipart(): for subpart in msg.get_payload(): _structure(subpart, lvl + 1) _structure(email_message) return { \\"lines\\": result_lines, \\"structured_summary\\": \\"n\\".join(structure_summary) + \\"n\\" }"},{"question":"Context: You are provided with a set of data files containing time-series data for different years. Each file has the same structure but includes data for different years. Your task is to read these files, optimize the memory usage by converting data types, and calculate the value counts of a specific column across all files. Objectives: 1. **Selective Loading**: Load specific columns from parquet files while minimizing memory usage. 2. **Efficient Datatypes**: Optimize the loaded data by using memory-efficient data types. 3. **Chunk Processing**: Process the data in chunks to calculate the cumulative value counts for a specified column across multiple files. Input: - A list of file paths to parquet files, each containing time-series data. Each file has columns: `name`, `id`, `x`, and `y`. Output: - A pandas Series representing the cumulative value counts of the `name` column across all files, with memory optimization considerations. Function Signature: ```python import pandas as pd def get_cumulative_value_counts(file_paths): Calculate cumulative value counts of the \'name\' column across multiple parquet files with memory usage optimization. :param file_paths: List[str] - A list of string paths to the parquet data files. :return: pd.Series - Series with cumulative value counts of the \'name\' column. # Recommended Steps: # 1. Initialize an empty pandas Series for cumulative counts. # 2. For each file path in `file_paths`: # a. Read the parquet file, but only load the \'name\' and other necessary columns. # b. Convert the \'name\' column to a pandas Categorical type to save memory. # c. Calculate the value counts of the \'name\' column. # d. Add these counts to the cumulative counts Series. # 3. Return the cumulative counts Series. ``` Constraints: - You should assume that each file individually fits into memory. - Handle potential missing files gracefully. Example Usage: ```python file_paths = [\\"data/timeseries/ts-00.parquet\\", \\"data/timeseries/ts-01.parquet\\", \\"data/timeseries/ts-02.parquet\\"] cumulative_counts = get_cumulative_value_counts(file_paths) print(cumulative_counts) ``` This question assesses the students ability to work with large datasets using pandas, focusing on memory efficiency and chunk processing.","solution":"import pandas as pd def get_cumulative_value_counts(file_paths): Calculate cumulative value counts of the \'name\' column across multiple parquet files with memory usage optimization. :param file_paths: List[str] - A list of string paths to the parquet data files. :return: pd.Series - Series with cumulative value counts of the \'name\' column. cumulative_counts = pd.Series(dtype=\'int64\') for file_path in file_paths: try: # Load only necessary columns with type optimization df = pd.read_parquet(file_path, columns=[\'name\']) # Convert \'name\' to categorical to save memory df[\'name\'] = df[\'name\'].astype(\'category\') # Calculate value counts value_counts = df[\'name\'].value_counts() # Add to cumulative counts cumulative_counts = cumulative_counts.add(value_counts, fill_value=0) except FileNotFoundError: print(f\\"File {file_path} not found. Skipping.\\") except Exception as e: print(f\\"An error occurred while processing {file_path}: {e}\\") cumulative_counts = cumulative_counts.astype(\'int64\') # Ensure the series is int64 return cumulative_counts"},{"question":"Coding Assessment Question # Objective You are tasked with automating the creation of a `MANIFEST.in` file based on customizable rules. Your application should generate this file by following a set of inclusion and exclusion rules for files within a directory structure. This will demonstrate your understanding of Python\'s file handling capabilities and pattern matching. # Problem Statement Write a Python function `generate_manifest_in(root_dir: str, include_patterns: List[str], exclude_patterns: List[str], output_file: str = \\"MANIFEST.in\\") -> None` which generates a `MANIFEST.in` file based on the following conditions: 1. Include all files that match any of the patterns in the `include_patterns` list. 2. Exclude all files and directories that match any of the patterns in the `exclude_patterns` list. 3. The order in which patterns are applied must be preserved. Patterns in the `include_patterns` list are applied first, and then patterns in the `exclude_patterns` list are applied. 4. The output file name should be `MANIFEST.in` by default but can be overridden by the `output_file` parameter. # Function Signature ```python def generate_manifest_in(root_dir: str, include_patterns: List[str], exclude_patterns: List[str], output_file: str = \\"MANIFEST.in\\") -> None: # Your code here ``` # Input 1. `root_dir` (str): The root directory where the file search will start. 2. `include_patterns` (List[str]): A list of file patterns to include (e.g., `[\\"*.py\\", \\"*.txt\\"]`). 3. `exclude_patterns` (List[str]): A list of file or directory patterns to exclude (e.g., `[\\"*.log\\", \\"build/*\\", \\"*.tmp\\"]`). 4. `output_file` (str): The name of the output manifest file. Default is `\\"MANIFEST.in\\"`. # Output The function does not return any value. It should create the `output_file` with appropriate include and exclude directives based on the provided patterns. # Constraints - The function should handle invalid input gracefully. - Performance should be efficient enough to handle large directories with many files. # Example Given the following directory structure: ``` /project  src   module1.py   module2.py   helper.tmp  data   data1.csv   data2.csv   backup.log  README.txt  build  build.log  temp.dat ``` Calling the function: ```python generate_manifest_in(\\"/project\\", [\\"*.py\\", \\"*.txt\\", \\"*.csv\\"], [\\"*.log\\", \\"build/*\\", \\"*.tmp\\"]) ``` Should create `MANIFEST.in` with the following content: ``` include *.txt include *.csv include *.py prune build exclude *.log exclude *.tmp ``` # Notes - Use standard Python libraries for file operations and pattern matching. - Make sure to handle both relative and absolute paths correctly. - Ensure that the generated `MANIFEST.in` file is correctly formatted for use with the `sdist` command.","solution":"import os import fnmatch def generate_manifest_in(root_dir, include_patterns, exclude_patterns, output_file=\\"MANIFEST.in\\"): with open(output_file, \\"w\\") as manifest_file: # Apply include patterns for pattern in include_patterns: for dirpath, dirnames, filenames in os.walk(root_dir): for file in fnmatch.filter(filenames, pattern): relative_path = os.path.relpath(os.path.join(dirpath, file), root_dir) manifest_file.write(f\\"include {relative_path}n\\") # Apply exclude patterns for pattern in exclude_patterns: if \\"/*\\" in pattern: # prune directories path_to_prune = pattern.split(\\"/*\\")[0] manifest_file.write(f\\"prune {path_to_prune}n\\") else: manifest_file.write(f\\"exclude {pattern}n\\")"},{"question":"You are tasked with writing a Python function that processes a list of filenames. For each file, the function should count the number of lines and return a dictionary with filenames as keys and their corresponding line counts as values. The function should handle the following exceptions: 1. **FileNotFoundError**: If a file does not exist, the function should skip it and continue with the next file. 2. **PermissionError**: If the file cannot be accessed due to insufficient permissions, the function should skip it and continue with the next file. 3. **Other Exceptions**: For any other exceptions, the function should print an error message and continue processing the remaining files. Additionally, ensure that all files opened are properly closed using a clean-up action. # Function Signature ```python def count_file_lines(file_list: List[str]) -> Dict[str, int]: pass ``` # Input - `file_list`: A list of filenames (list of strings). # Output - A dictionary with filenames as keys and their line counts as values (dictionary with keys as strings and values as integers). # Example ```python file_list = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] result = count_file_lines(file_list) print(result) # Output: {\'file1.txt\': 10, \'file2.txt\': 20} ``` Note: The actual line counts will vary based on the contents of the files. # Constraints - You may assume that the filenames are valid strings. - Handle exceptions gracefully as specified above. # Performance Requirements - The function should efficiently handle a list of up to 1000 filenames. - Ensure minimal delay for files that cannot be accessed or are not found before moving on to the next file. # Hints - Use `try`...`except` blocks to handle exceptions. - Use the `with` statement to ensure files are properly closed after processing.","solution":"from typing import List, Dict def count_file_lines(file_list: List[str]) -> Dict[str, int]: file_line_counts = {} for file in file_list: try: with open(file, \'r\') as f: line_count = sum(1 for line in f) file_line_counts[file] = line_count except FileNotFoundError: print(f\\"File not found: {file}\\") continue except PermissionError: print(f\\"Permission denied: {file}\\") continue except Exception as e: print(f\\"An error occurred while processing {file}: {e}\\") continue return file_line_counts"},{"question":"Problem: Profiling and Optimizing a Python Function You are given a Python module that defines several functions for computing statistics on a list of numbers. Your task is to profile the performance of these functions, analyze the profiling data, and suggest improvements to optimize the code. # Module: `stats_module.py` ```python def sum_numbers(numbers): total = 0 for num in numbers: total += num return total def mean(numbers): total = sum_numbers(numbers) count = len(numbers) return total / count def variance(numbers): mu = mean(numbers) squared_diffs = [(x - mu) ** 2 for x in numbers] return sum_numbers(squared_diffs) / len(numbers) def standard_deviation(numbers): var = variance(numbers) return var ** 0.5 def slow_function(numbers): result = 1 for number in numbers: for i in range(1, int(number)): if i % 2 == 0: result *= i else: result += i return result ``` # Steps: 1. **Profile the Functions**: Create a script to profile all the functions in the `stats_module.py` using the `cProfile` or `profile` module. ```python import cProfile import stats_module numbers = list(range(1, 10001)) # Example data profiler = cProfile.Profile() profiler.enable() print(stats_module.mean(numbers)) print(stats_module.variance(numbers)) print(stats_module.standard_deviation(numbers)) print(stats_module.slow_function(numbers)) profiler.disable() profiler.print_stats(sort=\'cumtime\') ``` 2. **Analyze the Profiling Data**: Based on the profiling data, identify which functions consume the most time. 3. **Optimize the Code**: Suggest and implement optimizations to improve the performance of the slowest functions. Specifically, consider: - Using built-in functions or libraries where appropriate. - Avoiding redundant computations. - Utilize more efficient algorithms. 4. **Re-profile and Compare**: Re-profile the optimized code to verify that performance improvements were achieved. Provide a comparison of the profiling data before and after optimization. # Constraints: - Python version: 3.7+ - Libraries allowed: Built-in Python libraries, `cProfile`, `pstats` - You should not modify the input or output format of the provided functions. # Deliverables: 1. A Python script `profiling_script.py` with the profiling implementation. 2. A text file `profiling_analysis.txt` containing: - Analysis of the profiling data. - Description of the optimizations applied. - Comparison of profiling data before and after optimization. 3. An updated `stats_module.py` file with the optimized code. # Submission: Submit the following three files: 1. `profiling_script.py` 2. `profiling_analysis.txt` 3. `stats_module.py`","solution":"def sum_numbers(numbers): Returns the sum of a list of numbers. return sum(numbers) def mean(numbers): Returns the mean (average) of a list of numbers. return sum_numbers(numbers) / len(numbers) def variance(numbers): Returns the variance of a list of numbers. mu = mean(numbers) squared_diffs = [(x - mu) ** 2 for x in numbers] return sum_numbers(squared_diffs) / len(numbers) def standard_deviation(numbers): Returns the standard deviation of a list of numbers. return variance(numbers) ** 0.5 def slow_function(numbers): Returns a computed value based on a list of numbers. This version uses more efficient operations to minimize computation. result = 1 for number in numbers: for i in range(1, int(number)): result += i if i % 2 else result * i return result"},{"question":"# Coding Assessment Objective: To assess your comprehension of the seaborn package and your ability to create and customize plots using `seaborn.objects`. Problem Statement: You are provided with the `tips` dataset from seaborn. Your task is to create a series of plots to visualize the data in different ways using seaborn\'s `objects` module. 1. **Basic Dot Plot**: - Create a basic dot plot to visualize the relationship between `total_bill` and `tip`. 2. **Customized Dot Plot**: - Customize the dot plot by adding white edges to the dots. Ensure the dots are clearly distinguishable. 3. **Jittered Dot Plot**: - Create a dot plot to visualize the `total_bill` across different `days`, colored by `sex`. Use jittering to reduce overplotting. 4. **Faceted Dot Plot**: - Create a faceted dot plot for the `glue` dataset to visualize `Score` across different `Models`, faceted by `Task` (wrapper of 4). Limit the x-axis from -5 to 105. 5. **Advanced Dot Plot with Error Bars**: - Create a dot plot to visualize `total_bill` across different `days`. Adjust the position of the points slightly using jittering and shifting. Add error bars to represent the standard error of the data points. Input Format: - The `tips` dataset is available from the seaborn library. Use `seaborn.load_dataset(\\"tips\\")` to load it. - The `glue` dataset is also available from the seaborn library. Use `seaborn.load_dataset(\\"glue\\")` to load it. Output Format: - Generate the required plots and ensure they are properly formatted and labeled. Constraints: - Use the seaborn `objects` module for creating all plots. - Ensure all plots are clear and well-customized to improve readability. ```python # You can start your solution by importing necessary libraries import seaborn.objects as so from seaborn import load_dataset # Load datasets tips = load_dataset(\\"tips\\") glue = load_dataset(\\"glue\\") # Basic Dot Plot p1 = so.Plot(tips, \\"total_bill\\", \\"tip\\") p1.add(so.Dot()).show() # Customized Dot Plot p1.add(so.Dot(edgecolor=\\"w\\")).show() # Jittered Dot Plot ( so.Plot(tips, \\"total_bill\\", \\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(.2)) ).show() # Faceted Dot Plot p2 = so.Plot(glue, \\"Score\\", \\"Model\\").facet(\\"Task\\", wrap=4).limit(x=(-5, 105)) p2.add(so.Dot()).show() # Advanced Dot Plot with Error Bars ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=.2), so.Jitter(.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ).show() ```","solution":"# Import the necessary libraries import seaborn.objects as so from seaborn import load_dataset # Load datasets tips = load_dataset(\\"tips\\") glue = load_dataset(\\"glue\\") # Basic Dot Plot def basic_dot_plot(): p1 = so.Plot(tips, \\"total_bill\\", \\"tip\\") p1.add(so.Dot()).show() # Customized Dot Plot def customized_dot_plot(): p1 = so.Plot(tips, \\"total_bill\\", \\"tip\\") p1.add(so.Dot(edgecolor=\\"w\\")).show() # Jittered Dot Plot def jittered_dot_plot(): ( so.Plot(tips, \\"total_bill\\", \\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(.2)) ).show() # Faceted Dot Plot def faceted_dot_plot(): p2 = so.Plot(glue, \\"Score\\", \\"Model\\").facet(\\"Task\\", wrap=4).limit(x=(-5, 105)) p2.add(so.Dot()).show() # Advanced Dot Plot with Error Bars def advanced_dot_plot_with_error_bars(): ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=.2), so.Jitter(.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ).show()"},{"question":"**Problem Statement:** You are tasked with designing a file compression utility using the `zlib` module in Python. Your utility should be able to: 1. Compress a large text file in chunks using an appropriate compression level. 2. Calculate and display both Adler-32 and CRC-32 checksums for the original and compressed data. 3. Decompress the compressed data, verify it matches the original data, and ensure that the checksums match appropriately. **Requirements:** 1. Implement the following functions: - `compress_file(input_file_path: str, output_file_path: str, chunk_size: int, compress_level: int) -> Tuple[int, int]`: Compresses the file in chunks, writes the compressed data to the output file, and returns the Adler-32 and CRC-32 checksums of the original data. - `decompress_file(input_file_path: str, output_file_path: str, chunk_size: int) -> Tuple[int, int]`: Decompresses the compressed file in chunks, writes the decompressed data to the output file, and returns the Adler-32 and CRC-32 checksums of the decompressed data. - `verify_checksums(original_checksums: Tuple[int, int], decompressed_checksums: Tuple[int, int]) -> bool`: Compares the original and decompressed checksums to verify data integrity. 2. The utility needs to handle files that do not fit into memory entirely, so it should read and write in chunks defined by `chunk_size`. **Constraints:** - The functions should handle any I/O and zlib errors gracefully, printing an appropriate error message and exiting. - The `chunk_size` will be a positive integer. - The `compress_level` will be an integer between 0 and 9, inclusive. **Expected Function Signatures:** ```python from typing import Tuple def compress_file(input_file_path: str, output_file_path: str, chunk_size: int, compress_level: int) -> Tuple[int, int]: # Your code here def decompress_file(input_file_path: str, output_file_path: str, chunk_size: int) -> Tuple[int, int]: # Your code here def verify_checksums(original_checksums: Tuple[int, int], decompressed_checksums: Tuple[int, int]) -> bool: # Your code here ``` **Example Usage:** ```python original_file = \'example.txt\' compressed_file = \'example.txt.zlib\' decompressed_file = \'example_decompressed.txt\' chunk_size = 1024 # Read and write 1024 bytes at a time compress_level = 6 # Default compression level # Compress the file original_checksums = compress_file(original_file, compressed_file, chunk_size, compress_level) # Decompress the file decompressed_checksums = decompress_file(compressed_file, decompressed_file, chunk_size) # Verify that the original and decompressed data have the same checksums assert verify_checksums(original_checksums, decompressed_checksums) == True ```","solution":"import zlib from typing import Tuple def compress_file(input_file_path: str, output_file_path: str, chunk_size: int, compress_level: int) -> Tuple[int, int]: adler32 = zlib.adler32(b\'\') crc32 = zlib.crc32(b\'\') try: with open(input_file_path, \'rb\') as fin, open(output_file_path, \'wb\') as fout: compressor = zlib.compressobj(compress_level) while chunk := fin.read(chunk_size): fout.write(compressor.compress(chunk)) adler32 = zlib.adler32(chunk, adler32) crc32 = zlib.crc32(chunk, crc32) fout.write(compressor.flush()) except Exception as e: print(f\\"An error occurred during compression: {e}\\") return None return adler32, crc32 def decompress_file(input_file_path: str, output_file_path: str, chunk_size: int) -> Tuple[int, int]: adler32 = zlib.adler32(b\'\') crc32 = zlib.crc32(b\'\') try: with open(input_file_path, \'rb\') as fin, open(output_file_path, \'wb\') as fout: decompressor = zlib.decompressobj() while chunk := fin.read(chunk_size): data = decompressor.decompress(chunk) fout.write(data) adler32 = zlib.adler32(data, adler32) crc32 = zlib.crc32(data, crc32) fout.write(decompressor.flush()) except Exception as e: print(f\\"An error occurred during decompression: {e}\\") return None return adler32, crc32 def verify_checksums(original_checksums: Tuple[int, int], decompressed_checksums: Tuple[int, int]) -> bool: return original_checksums == decompressed_checksums"},{"question":"Mocking and Patching in Python You are required to test the following `FileProcessor` class that interacts with the file system and a remote service: ```python import os class RemoteService: def fetch_data(self): # Simulate fetching data from a remote server pass class FileProcessor: def __init__(self, filepath, service): self.filepath = filepath self.service = service def process_file(self): if not os.path.exists(self.filepath): raise FileNotFoundError(f\'File {self.filepath} does not exist.\') with open(self.filepath, \'r\') as file: data = file.read() fetched_data = self.service.fetch_data() return self._process_data(data, fetched_data) def _process_data(self, file_data, fetched_data): # Simulate processing data return len(file_data) + len(fetched_data) ``` Your task is to write a series of unit tests for the `FileProcessor` class using the `unittest` framework and the `unittest.mock` library to fully mock and patch the dependencies and methods. # Requirements: 1. **Test if a `FileNotFoundError` is raised when the file does not exist**. 2. **Test if the content of the file and fetched data are processed correctly**. 3. **Ensure the `fetch_data` method is called exactly once**. 4. **Mock the return value of `fetch_data` method and verify the final processed result**. # Constraints: - Use `unittest` and `unittest.mock` libraries only. - Do not actually create, read, or write any files. - Ensure to patch the file system interaction and the `RemoteService`. - Validate both positive and negative flows (e.g., file exists, file does not exist). # Example: ```python import unittest from unittest.mock import patch, mock_open, MagicMock from file_processor import FileProcessor, RemoteService class TestFileProcessor(unittest.TestCase): def test_file_not_found(self): pass # Implement your test here def test_file_processing(self): pass # Implement your test here def test_fetch_data_called_once(self): pass # Implement your test here def test_mocked_fetch_data(self): pass # Implement your test here if __name__ == \'__main__\': unittest.main() ``` Ensure that each test is self-contained and correctly verifies the behavior based on the requirements. Good luck!","solution":"import os class RemoteService: def fetch_data(self): # Simulate fetching data from a remote server pass class FileProcessor: def __init__(self, filepath, service): self.filepath = filepath self.service = service def process_file(self): if not os.path.exists(self.filepath): raise FileNotFoundError(f\'File {self.filepath} does not exist.\') with open(self.filepath, \'r\') as file: data = file.read() fetched_data = self.service.fetch_data() return self._process_data(data, fetched_data) def _process_data(self, file_data, fetched_data): # Simulate processing data return len(file_data) + len(fetched_data)"},{"question":"You are building a configuration management system that needs to handle settings from multiple sources: user-defined settings, environment variables, and application defaults. These settings are represented as dictionaries. Your task is to implement a function `resolve_settings` that merges these settings and prioritizes them correctly. The function should: - Accept three dictionaries as input in the following order of priority (highest to lowest): 1. `user_settings` - settings explicitly defined by the user. 2. `env_settings` - settings from environment variables. 3. `default_settings` - application default settings. - Use the `ChainMap` class from the `collections` module to manage the merging of these settings. - Provide a method to output the combined settings as a single dictionary. - Provide a method to update a specific setting, ensuring that it updates the highest-priority dictionary where the key exists or adds it to the highest-priority dictionary if the key does not exist. - Provide a method to retrieve the value of a specific setting, returning `None` if the setting is not found in any dictionary. # Function Specifications: `resolve_settings(user_settings: dict, env_settings: dict, default_settings: dict) -> Any` - **Input:** - `user_settings` (dict): A dictionary containing user-defined settings. - `env_settings` (dict): A dictionary containing environment variable settings. - `default_settings` (dict): A dictionary containing application default settings. - **Output:** The function should return an object with the following methods: - `to_dict() -> dict`: Returns a dictionary representing the merged settings. - `update_setting(key: str, value: Any) -> None`: Updates the specified setting in the highest-priority dictionary. - `get_setting(key: str) -> Any`: Retrieves the value for the specified setting, returning `None` if not found. # Example Usage: ```python user_settings = {\'theme\': \'dark\', \'autosave_interval\': 5} env_settings = {\'theme\': \'light\', \'language\': \'en\'} default_settings = {\'theme\': \'blue\', \'language\': \'en\', \'autosave_interval\': 10, \'font_size\': 12} conf = resolve_settings(user_settings, env_settings, default_settings) # Get combined settings as a dictionary assert conf.to_dict() == { \'theme\': \'dark\', \'language\': \'en\', \'autosave_interval\': 5, \'font_size\': 12 } # Update a setting conf.update_setting(\'font_size\', 14) assert conf.to_dict()[\'font_size\'] == 14 # Get a specific setting assert conf.get_setting(\'language\') == \'en\' assert conf.get_setting(\'non_existing_key\') == None ``` # Constraints: - The dictionaries can contain any valid key-value pairs. - Performance should be efficient for dictionaries with up to 1000 key-value pairs. Write the implementation of the `resolve_settings` function following the specifications above.","solution":"from collections import ChainMap def resolve_settings(user_settings: dict, env_settings: dict, default_settings: dict): class CombinedSettings: def __init__(self, user, env, default): self.settings = ChainMap(user, env, default) self.user = user self.env = env self.default = default def to_dict(self): # Convert the ChainMap to a dictionary return dict(self.settings) def update_setting(self, key, value): # Check where the key exists and update it in the highest priority dictionary for mapping in self.settings.maps: if key in mapping: mapping[key] = value return # If the key does not exist in any dictionary, add it to the highest-priority dictionary self.user[key] = value def get_setting(self, key): # Retrieve the value of a specific setting, return None if not found return self.settings.get(key, None) return CombinedSettings(user_settings, env_settings, default_settings)"},{"question":"Parsing and Processing Chunked Data You are provided with a binary file that uses the EA IFF 85 chunk format. Each chunk in the file has the following structure: - **4 bytes**: Chunk ID (string) - **4 bytes**: Chunk size (big-endian integer, excluding the header size) - **n bytes**: Data bytes - **0 or 1 byte**: Padding byte if chunk size is odd Your task is to write a function `process_chunks(file_obj)` that takes a file-like object `file_obj` as input and returns a dictionary where each key is a chunk ID and the corresponding value is the total size of all chunks with that ID. Function Signature ```python def process_chunks(file_obj) -> dict: ``` Input - `file_obj`: A file-like object opened in binary mode that contains chunked data. Output - Returns a dictionary where keys are chunk IDs (4-byte strings) and values are integers representing the total size of all chunks with that ID. Constraints - Assume the file is large, so you should avoid loading the entire file into memory. - Make sure to handle padding bytes correctly if chunk size is odd. Example To help you understand the format, here\'s how you could define a binary file with two chunks: ``` b\'CHNKx00x00x00x04dataCHNKx00x00x00x02okALGNx00x00x00x03abcx00\' ``` In the above example: - The first chunk has ID `CHNK`, size 4, and data `data`. - The second chunk has ID `CHNK`, size 2, and data `ok`. - The third chunk has ID `ALGN`, size 3, and data `abc`, followed by a padding byte. Your function should return: ```python { \'CHNK\': 6, \'ALGN\': 3 } ``` Notes - You can use the `chunk` module as described in the provided documentation. - Pay attention to correctly handling the chunk boundaries and alignment issues. Hints - Use the `chunk.Chunk` class to read and navigate through the chunks. - Utilize methods like `getname()`, `getsize()`, and `read()` from the `chunk.Chunk` class to extract necessary information. Good luck!","solution":"def process_chunks(file_obj): import struct from collections import defaultdict chunks_info = defaultdict(int) while True: chunk_header = file_obj.read(8) if len(chunk_header) < 8: break chunk_id, chunk_size = struct.unpack(\'>4sI\', chunk_header) chunk_id = chunk_id.decode(\'ascii\') chunks_info[chunk_id] += chunk_size # Move the file pointer forward according to the chunk size + any necessary padding file_obj.seek(chunk_size, 1) if chunk_size % 2 != 0: file_obj.seek(1, 1) return dict(chunks_info)"},{"question":"# Question: You are to implement a Python function called `calculate_working_hours` that figures out the total working hours between two given datetime objects. The working hours are considered to be from 9 AM to 5 PM, Monday to Friday. Heres what you need to do: - Write a function `calculate_working_hours(start: datetime, end: datetime) -> float`. - The function should take two datetime objects as input: `start` and `end`. - It should calculate the total number of working hours (considering only the periods from 9 AM to 5 PM, Monday to Friday) between these two datetimes and return it as a floating-point number. - If the `start` datetime is after the `end` datetime, the function should return `0`. # Constraints: - You may assume that both `start` and `end` are timezone-aware datetime objects in the same timezone. - The function should correctly handle edge cases where the `start` or `end` datetimes fall on weekends or outside working hours. - You should leverage the `datetime` module for this implementation. # Performance Requirements: - The function should be efficient and handle inputs spanning multiple years without performance degradation. # Example: ```python from datetime import datetime, timedelta, timezone def calculate_working_hours(start: datetime, end: datetime) -> float: # Your implementation here # Example usage: start_datetime = datetime(2023, 10, 25, 8, 0, tzinfo=timezone.utc) end_datetime = datetime(2023, 10, 30, 18, 0, tzinfo=timezone.utc) print(calculate_working_hours(start_datetime, end_datetime)) # Expected output: 16.0 ``` **Explanation:** In this example, only the part from 9 AM to 5 PM on October 25th and October 26th, plus 9 AM to 5 PM on October 30th should be counted, leading to a total of 16 working hours.","solution":"from datetime import datetime, time, timedelta def calculate_working_hours(start: datetime, end: datetime) -> float: Calculate total working hours between two datetime objects, considering working hours to be from 9 AM to 5 PM, Monday to Friday. :param start: Start datetime object :param end: End datetime object :return: Total working hours as a floating-point number # Return 0 if start is after end if start >= end: return 0.0 # Working day start and end time work_start = time(9, 0) work_end = time(17, 0) total_hours = 0.0 current = start while current <= end: weekday = current.weekday() # Only count weekdays (Monday to Friday) if weekday < 5: # Determine the start time for the current day if current.date() == start.date(): day_start = max(current.time(), work_start) else: day_start = work_start # Determine the end time for the current day if current.date() == end.date(): day_end = min(end.time(), work_end) else: day_end = work_end # Calculate hours for the current day if day_start < day_end: day_hours = datetime.combine(current.date(), day_end) - datetime.combine(current.date(), day_start) total_hours += day_hours.total_seconds() / 3600 # Move to the next day current += timedelta(days=1) current = datetime.combine(current.date(), time(0, 0, tzinfo=current.tzinfo)) return total_hours"},{"question":"**Objective**: Demonstrate your understanding of creating and managing Python virtual environments, and using the `pip` tool to handle package dependencies. Problem Statement You are required to set up a Python project that performs text processing using the `requests` and `beautifulsoup4` packages. Follow the steps below: 1. **Create a Virtual Environment**: - Create a virtual environment named `text_proc_env`. 2. **Activate the Virtual Environment**: - Provide the command to activate the virtual environment based on your operating system (Windows, macOS, or Unix). 3. **Install Packages**: - Inside the activated virtual environment, install the `requests` package version `2.26.0`. - Install the `beautifulsoup4` package without specifying any version (this will install the latest version). 4. **List Installed Packages**: - Write a Python script that prints the list of installed packages in the virtual environment. 5. **Save and Load Package Dependencies**: - Save the list of installed packages to a file named `requirements.txt`. - Write another script to create a new virtual environment named `new_text_proc_env` and install the packages from the `requirements.txt` file. 6. **Web Scraping Task**: - Using the `requests` and `beautifulsoup4` packages, write a script that fetches the HTML content of the webpage `https://example.com` and prints the titles of all articles (<h1> elements). # Constraints - Ensure you handle exceptions (e.g., if the packages cannot be installed). - Your solution should be self-contained and reproducible by following the provided steps. # Input and Output Formats - **Input**: Commands to create and activate virtual environments, install packages, and Python scripts that perform specified tasks. - **Output**: Printed list of packages, contents of `requirements.txt`, and titles of articles fetched from the webpage. # Example Usage 1. **Creating and Activating Virtual Environment**: ```sh python -m venv text_proc_env text_proc_envScriptsactivate.bat # Windows # OR source text_proc_env/bin/activate # macOS/Unix ``` 2. **Installing Packages**: ```sh pip install requests==2.26.0 pip install beautifulsoup4 ``` 3. **Python Script to List Installed Packages**: ```python import pkg_resources installed_packages = pkg_resources.working_set for dist in installed_packages: print(dist.project_name, dist.version) ``` 4. **Saving Packages to `requirements.txt`**: ```sh pip freeze > requirements.txt ``` 5. **Script to Create New Environment and Install Packages**: ```sh python -m venv new_text_proc_env source new_text_proc_env/bin/activate # Activate the environment pip install -r requirements.txt ``` 6. **Web Scraping Script**: ```python import requests from bs4 import BeautifulSoup response = requests.get(\'https://example.com\') soup = BeautifulSoup(response.text, \'html.parser\') for title in soup.find_all(\'h1\'): print(title.text) ``` Your solution should include all these steps and scripts to demonstrate a complete understanding of virtual environments and package management in Python.","solution":"# Step 1: Create a Virtual Environment # Command: python -m venv text_proc_env # Step 2: Activate the Virtual Environment # Windows: text_proc_envScriptsactivate.bat # macOS/Unix: source text_proc_env/bin/activate # Step 3: Install Packages # Command: pip install requests==2.26.0 # Command: pip install beautifulsoup4 # Step 4: Python Script to List Installed Packages import pkg_resources def list_installed_packages(): installed_packages = pkg_resources.working_set for dist in installed_packages: print(f\\"{dist.project_name} {dist.version}\\") # Step 5: Save the list of installed packages to requirements.txt # Command: pip freeze > requirements.txt # Step 6: Script to Create New Environment and Install Packages from requirements.txt # Command: # python -m venv new_text_proc_env # source new_text_proc_env/bin/activate (on macOS/Unix) # new_text_proc_envScriptsactivate.bat (on Windows) # pip install -r requirements.txt # Step 7: Web Scraping Task import requests from bs4 import BeautifulSoup def fetch_article_titles(url): try: response = requests.get(url) response.raise_for_status() # Raises an exception for HTTP errors soup = BeautifulSoup(response.content, \'html.parser\') titles = [title.text for title in soup.find_all(\'h1\')] return titles except requests.RequestException as e: print(f\\"An error occurred: {e}\\") return [] # Example usage if __name__ == \\"__main__\\": # List the installed packages list_installed_packages() # Fetch article titles from example.com titles = fetch_article_titles(\'https://example.com\') for title in titles: print(title)"},{"question":"You are tasked to write a Python function that determines if a given color in RGB space lies within a specific luminance range when converted to the HLS (Hue, Lightness, Saturation) color space. More specifically, the luminance value (`L` parameter in HLS) should fall into a given range, and you should return a boolean indicating whether the RGB color meets this criterion. Function Signature ```python def is_rgb_within_luminance_range(r: float, g: float, b: float, min_luminance: float, max_luminance: float) -> bool: pass ``` Parameters - `r` (float): The red component of the RGB color, within the range [0, 1]. - `g` (float): The green component of the RGB color, within the range [0, 1]. - `b` (float): The blue component of the RGB color, within the range [0, 1]. - `min_luminance` (float): The minimum value of the luminance range, within the range [0, 1]. - `max_luminance` (float): The maximum value of the luminance range, within the range [0, 1]. Returns - `bool`: Returns `True` if the luminance of the RGB color falls within the specified range (inclusive), otherwise `False`. Constraints - All color components `r`, `g`, `b` provided as input will be within the range [0.0, 1.0]. - Both `min_luminance` and `max_luminance` will be within the range [0.0, 1.0], and `min_luminance` will be less than or equal to `max_luminance`. Examples ```python # Example 1 # RGB color (0.2, 0.4, 0.4) converts to HLS color (0.5, 0.3, 0.333333...) # Luminance (L) value is 0.3, which lies between 0.2 and 0.4 print(is_rgb_within_luminance_range(0.2, 0.4, 0.4, 0.2, 0.4)) # Expected output: True # Example 2 # RGB color (1.0, 0.0, 0.0) converts to HLS color (0.0, 0.5, 1.0) # Luminance (L) value is 0.5, which does not lie between 0.1 and 0.3 print(is_rgb_within_luminance_range(1.0, 0.0, 0.0, 0.1, 0.3)) # Expected output: False ``` # Notes You will need to use the `colorsys` module\'s `rgb_to_hls` function to achieve the conversion from RGB to HLS.","solution":"import colorsys def is_rgb_within_luminance_range(r: float, g: float, b: float, min_luminance: float, max_luminance: float) -> bool: Check if the luminance of an RGB color is within a specified range. Parameters: r (float): The red component of the RGB color, within the range [0, 1]. g (float): The green component of the RGB color, within the range [0, 1]. b (float): The blue component of the RGB color, within the range [0, 1]. min_luminance (float): The minimum value of the luminance range, within the range [0, 1]. max_luminance (float): The maximum value of the luminance range, within the range [0, 1]. Returns: bool: True if the luminance is within the range, otherwise False. _, luminance, _ = colorsys.rgb_to_hls(r, g, b) return min_luminance <= luminance <= max_luminance"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of pandas\' visualization capabilities by generating a complex, multi-faceted plot using various pandas plotting functionalities. # Problem Statement You are provided with a dataset containing information about various species of flowers. Your task is to create a plot that showcases different visualization techniques to provide an in-depth view of the data. Specifically, you need to generate a combined plot containing different subplots that include: 1. A scatter plot comparing two numeric variables, with color coding based on the flower species. 2. A histogram for one numeric variable, grouped by species. 3. A box plot showing the distribution of another numeric variable, grouped by species. 4. A scatter matrix plot to visualize relationships among all numeric variables. # Dataset The dataset is in CSV format with the following columns: - `sepal_length`: Length of the sepal in cm - `sepal_width`: Width of the sepal in cm - `petal_length`: Length of the petal in cm - `petal_width`: Width of the petal in cm - `species`: Species of the flower (setosa, versicolor, virginica) # Requirements 1. Load the dataset from a CSV file. 2. Create a figure with four subplots arranged in a 2x2 grid. 3. In the first subplot (top left), create a scatter plot of `sepal_length` vs `sepal_width`, color-coded by `species`. 4. In the second subplot (top right), create a histogram of `petal_length`, with separate histograms for each `species`. 5. In the third subplot (bottom left), create a box plot of `sepal_width`, grouped by `species`. 6. In the fourth subplot (bottom right), create a scatter matrix plot for all numeric variables. # Input - A CSV file named `flowers.csv` with the columns `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. # Output - A figure with four subplots as described above. # Implementation Constraints - You must use pandas for data manipulation and plotting. - You must use matplotlib for figure and subplot creation. # Function Signature ```python import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix def visualize_flower_data(csv_filepath: str): # Load the dataset df = pd.read_csv(csv_filepath) # Create a 2x2 subplot fig, axes = plt.subplots(2, 2, figsize=(12, 10)) # Scatter plot in the top left df.plot.scatter(x=\'sepal_length\', y=\'sepal_width\', c=df[\'species\'].astype(\'category\').cat.codes, colormap=\'viridis\', ax=axes[0, 0], title=\'Sepal Length vs Width\') # Histogram in the top right df[df[\'species\'] == \'setosa\'][\'petal_length\'].plot.hist(ax=axes[0, 1], alpha=0.5, label=\'setosa\') df[df[\'species\'] == \'versicolor\'][\'petal_length\'].plot.hist(ax=axes[0, 1], alpha=0.5, label=\'versicolor\') df[df[\'species\'] == \'virginica\'][\'petal_length\'].plot.hist(ax=axes[0, 1], alpha=0.5, label=\'virginica\') axes[0, 1].set_title(\'Petal Length Distribution by Species\') axes[0, 1].legend() # Box plot in the bottom left df.boxplot(column=\'sepal_width\', by=\'species\', ax=axes[1, 0]) axes[1, 0].set_title(\'Sepal Width by Species\') axes[1, 0].set_xlabel(\'\') # Scatter matrix plot in the bottom right scatter_matrix(df[[\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\']], alpha=0.2, ax=axes[1, 1]) axes[1, 1].set_title(\'Scatter Matrix of Flower Features\') # Adjust layout fig.suptitle(\'Flower Data Visualizations\', y=1.02) plt.tight_layout() plt.show() ``` # Example Usage ```python # Assuming the dataset is in \'flowers.csv\': visualize_flower_data(\'flowers.csv\') ``` # Note - Ensure that your plots are well-labeled and have legends where necessary to convey the intended information clearly. - You may reuse any template from the provided pandas documentation to achieve your tasks.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix def visualize_flower_data(csv_filepath: str): # Load the dataset df = pd.read_csv(csv_filepath) # Create a 2x2 subplot fig, axes = plt.subplots(2, 2, figsize=(12, 10)) # Scatter plot in the top left df.plot.scatter(x=\'sepal_length\', y=\'sepal_width\', c=df[\'species\'].astype(\'category\').cat.codes, colormap=\'viridis\', ax=axes[0, 0], title=\'Sepal Length vs Width\') # Histogram in the top right df[df[\'species\'] == \'setosa\'][\'petal_length\'].plot.hist(ax=axes[0, 1], alpha=0.5, label=\'setosa\') df[df[\'species\'] == \'versicolor\'][\'petal_length\'].plot.hist(ax=axes[0, 1], alpha=0.5, label=\'versicolor\') df[df[\'species\'] == \'virginica\'][\'petal_length\'].plot.hist(ax=axes[0, 1], alpha=0.5, label=\'virginica\') axes[0, 1].set_title(\'Petal Length Distribution by Species\') axes[0, 1].legend() # Box plot in the bottom left df.boxplot(column=\'sepal_width\', by=\'species\', ax=axes[1, 0]) axes[1, 0].set_title(\'Sepal Width by Species\') axes[1, 0].set_xlabel(\'\') # Scatter matrix plot in the bottom right scatter_matrix(df[[\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\']], alpha=0.2, ax=axes[1, 1]) axes[1, 1].set_title(\'Scatter Matrix of Flower Features\') # Adjust layout fig.suptitle(\'Flower Data Visualizations\', y=1.02) plt.tight_layout() plt.show()"},{"question":"Objective Your task is to demonstrate your understanding of the `seaborn.objects` interface to visualize data from the \\"tips\\" dataset in a meaningful way using Seaborn\'s object-oriented interface. Problem Statement You have been provided with the \\"tips\\" dataset, which contains information about the bill amounts, tips given, the day of the week, gender of the server, and other variables. You are required to create a composite plot using the `seaborn.objects.Plot` class to uncover insights from the data. Requirements 1. **Import the necessary modules**: - Import `seaborn.objects` as `so`. - Import the `load_dataset` function from Seaborn. 2. **Load the dataset**: - Load the \\"tips\\" dataset using the `load_dataset` function and assign it to a variable named `tips`. 3. **Create a plot**: - Initialize a `seaborn.objects.Plot` object with `tips` as the dataset, specifying \\"total_bill\\" on the x-axis and \\"tip\\" on the y-axis. 4. **Add layers**: - Add a **Dot** mark layer to the plot. - Add a **Line** mark layer with a **PolyFit** transform to show a polynomial fit line to the same plot. - Add another layer using the **Dot** mark to visualize the tips segregated by the \\"time\\" variable (Lunch/Dinner). 5. **Facet the plot**: - Use the `facet` function to create separate plots for each day of the week. 6. **Label the layers**: - Provide appropriate labels to distinguish between the dot marks for the original data and the polynomial fit line. Constraints - Your code should be readable and well-structured. - Ensure that all necessary configurations are set correctly for each layer. - The final plot should be properly faceted and labeled for clarity. Expected Solution The solution is expected to generate a multi-faceted plot, where each subplot corresponds to data from a different day of the week. The subplots should include dot marks representing the relationship between \\"total_bill\\" and \\"tip,\\" along with a polynomial fit line to show the trend. An additional layer should separate \\"time\\" dimensions using different colors for better insight. # Example Code ```python # Step 1: Import necessary modules import seaborn.objects as so from seaborn import load_dataset # Step 2: Load the dataset tips = load_dataset(\\"tips\\") # Step 3: Create a plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Step 4: Add layers # Add a Dot mark layer p = p.add(so.Dot()) # Add a Line mark layer with a PolyFit transform p = p.add(so.Line(color=\\"blue\\"), so.PolyFit()) # Add another Dot layer for \\"time\\" variable p = p.add(so.Dot(), data=tips, color=\\"time\\") # Step 5: Facet the plot by \\"day\\" p = p.facet(col=\\"day\\") # Step 6: Label the layers p = p.add(so.Dot(color=\\"#aabc\\"), col=None, color=None) p = p.add(so.Dot()).label(y=\\"Value\\") # Render the plot p ``` Ensure your solution adheres to the structure shown and includes proper handling of each step defined in the requirements.","solution":"# Step 1: Import necessary modules import seaborn.objects as so from seaborn import load_dataset # Step 2: Load the dataset tips = load_dataset(\\"tips\\") # Step 3: Create a plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Step 4: Add layers # Add a Dot mark layer p = p.add(so.Dot()) # Add a Line mark layer with a PolyFit transform p = p.add(so.Line(color=\'blue\'), so.PolyFit()) # Add another Dot layer for \\"time\\" variable p = p.add(so.Dot(), color=\\"time\\") # Step 5: Facet the plot by \\"day\\" p = p.facet(col=\\"day\\") # Step 6: Label the layers p = (p.add(so.Dot(color=\'#aabc\'), col=None, color=None) .add(so.Dot()) .label(y=\\"Tip amount\\") ) # Render the plot p.show()"},{"question":"Coding Assessment Question **Objective:** Design a custom neural network module using PyTorch that includes convolution, activation, pooling, and dropout functions. Demonstrate your understanding of integrating multiple functionalities provided by `torch.nn.functional` to build a scalable and reusable neural network layer. **Task:** Implement a custom PyTorch module named `CustomConvModule`. This module will implement a forward pass through the following layers in order: 1. A 2D convolution layer (using `conv2d`). 2. A ReLU activation function (using `relu`). 3. A 2D max pooling layer (using `max_pool2d`). 4. A dropout layer (using `dropout`). **Specifications:** - The module should accept the following parameters during initialization: - `in_channels` (int): Number of channels in the input image. - `out_channels` (int): Number of channels produced by the convolution. - `kernel_size` (int or tuple): Size of the convolving kernel. - `pool_size` (int or tuple): Size of the window to take a max over. - `dropout_rate` (float): Probability of an element to be zeroed. - The module should define a forward method that takes an input tensor `x` and processes it through the layers defined above. **Input and Output Formats:** - Input: - `x` (Tensor): Input tensor of shape `(batch_size, in_channels, height, width)`. - Output: - Tensor of shape `(batch_size, out_channels, pooled_height, pooled_width)` after applying all layers. **Constraints:** - You should use functions from `torch.nn.functional` for implementing the layers. - Handle both tuple and integer inputs for `kernel_size` and `pool_size`. - Ensure dropout is applied correctly only during training and not during evaluation. **Performance Requirements:** - Your implementation should be optimized for common input sizes encountered in image processing tasks (e.g., images with dimensions 64x64, 128x128). **Code Template:** ```python import torch import torch.nn.functional as F class CustomConvModule(torch.nn.Module): def __init__(self, in_channels, out_channels, kernel_size, pool_size, dropout_rate): super(CustomConvModule, self).__init__() self.in_channels = in_channels self.out_channels = out_channels self.kernel_size = kernel_size self.pool_size = pool_size self.dropout_rate = dropout_rate def forward(self, x): # Step 1: Apply 2D convolution x = F.conv2d(x, weight=..., bias=..., stride=1, padding=0) # Step 2: Apply ReLU activation x = F.relu(x) # Step 3: Apply 2D max pooling x = F.max_pool2d(x, kernel_size=self.pool_size) # Step 4: Apply dropout x = F.dropout(x, p=self.dropout_rate, training=self.training) return x # Testing the custom module # Create dummy input tensor with shape (batch_size, in_channels, height, width) input_tensor = torch.randn(8, 3, 64, 64) custom_module = CustomConvModule(in_channels=3, out_channels=16, kernel_size=3, pool_size=2, dropout_rate=0.5) output_tensor = custom_module(input_tensor) print(output_tensor.shape) # Expected: torch.Size([8, 16, 31, 31]) if kernel_size=3, stride=1, and padding=0 ``` Make sure to replace the ellipsis (`...`) in `F.conv2d` with appropriate parameters. **Submission Guidelines:** - Save your code in a file named `custom_conv_module.py`. - Provide comments explaining the key parts of your implementation.","solution":"import torch import torch.nn.functional as F class CustomConvModule(torch.nn.Module): def __init__(self, in_channels, out_channels, kernel_size, pool_size, dropout_rate): super(CustomConvModule, self).__init__() self.conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size) self.pool_size = pool_size self.dropout_rate = dropout_rate def forward(self, x): # Step 1: Apply 2D convolution x = self.conv(x) # Step 2: Apply ReLU activation x = F.relu(x) # Step 3: Apply 2D max pooling x = F.max_pool2d(x, kernel_size=self.pool_size) # Step 4: Apply dropout x = F.dropout(x, p=self.dropout_rate, training=self.training) return x # Test the custom module # Create a dummy input tensor with shape (batch_size, in_channels, height, width) input_tensor = torch.randn(8, 3, 64, 64) custom_module = CustomConvModule(in_channels=3, out_channels=16, kernel_size=3, pool_size=2, dropout_rate=0.5) output_tensor = custom_module(input_tensor) print(output_tensor.shape) # Expected: torch.Size([8, 16, 31, 31])"},{"question":"**Objective**: Your task is to write two functions that demonstrate your understanding of binary data manipulation and encoding/decoding in Python 3.10 using the `struct` and `codecs` modules. # Problem Statement 1. **Binary Data Packing and Unpacking** You need to create two functions: `pack_data` and `unpack_data`. - `pack_data` should take three inputs: a floating-point number, an integer, and a string of exactly 4 ASCII characters. It should pack these values into a binary format using the `struct` module. The binary format should be as follows: - The floating-point number (8 bytes, double precision). - The integer (4 bytes, standard integer). - The string (4 bytes, ASCII characters). The output of `pack_data` should be the packed binary data. - `unpack_data` should take the packed binary data output from `pack_data` and return the original floating-point number, integer, and string. ```python import struct def pack_data(flt: float, integer: int, text: str) -> bytes: Packs a floating point number, an integer, and a 4-character string into a binary format. :param flt: The floating point number to pack. :param integer: The integer to pack. :param text: The 4-character ASCII string to pack. :return: Packed binary data. # Your implementation here def unpack_data(data: bytes): Unpacks binary data into a floating point number, an integer, and a 4-character string. :param data: The packed binary data. :return: A tuple containing the floating point number, integer, and 4-character string. # Your implementation here ``` 2. **Encoding and Decoding Text** Create two functions: `encode_text` and `decode_text`. - `encode_text` should take a Unicode string and an encoding type (e.g., \'utf-8\', \'utf-16\', \'ascii\'). It should encode the string using the specified encoding and return the encoded bytes. - `decode_text` should take encoded bytes and the encoding type used to encode them. It should decode the bytes back into the original Unicode string. ```python import codecs def encode_text(text: str, encoding: str) -> bytes: Encodes a text string into specified encoding. :param text: The text string to encode. :param encoding: The encoding type to use (e.g., \'utf-8\', \'ascii\'). :return: Encoded bytes. # Your implementation here def decode_text(data: bytes, encoding: str) -> str: Decodes encoded bytes back into a text string using the specified encoding. :param data: The encoded bytes. :param encoding: The encoding type used to decode. :return: Decoded text string. # Your implementation here ``` # Input/Output 1. **pack_data** - Input: `pack_data(3.14, 42, \'test\')` - Output: Binary data packed in the format specified. 2. **unpack_data** - Input: Output of `pack_data(3.14, 42, \'test\')` - Output: `(3.14, 42, \'test\')` 3. **encode_text** - Input: `encode_text(\'hello\', \'utf-8\')` - Output: Encoded bytes of the text \'hello\' in \'utf-8\'. 4. **decode_text** - Input: Output of `encode_text(\'hello\', \'utf-8\')` - Output: `\'hello\'` # Constraints - The string passed to `pack_data` will always be exactly 4 ASCII characters. - The text for `encode_text` and `decode_text` will always be valid Unicode strings. # Performance Requirements - Both encoding/decoding and packing/unpacking operations should be optimized for typical usage scenarios.","solution":"import struct import codecs def pack_data(flt: float, integer: int, text: str) -> bytes: Packs a floating point number, an integer, and a 4-character string into a binary format. :param flt: The floating point number to pack. :param integer: The integer to pack. :param text: The 4-character ASCII string to pack. :return: Packed binary data. if len(text) != 4: raise ValueError(\\"The text must be exactly 4 characters long.\\") return struct.pack(\'dI4s\', flt, integer, text.encode(\'ascii\')) def unpack_data(data: bytes): Unpacks binary data into a floating point number, an integer, and a 4-character string. :param data: The packed binary data. :return: A tuple containing the floating point number, integer, and 4-character string. flt, integer, text = struct.unpack(\'dI4s\', data) return flt, integer, text.decode(\'ascii\') def encode_text(text: str, encoding: str) -> bytes: Encodes a text string into specified encoding. :param text: The text string to encode. :param encoding: The encoding type to use (e.g., \'utf-8\', \'ascii\'). :return: Encoded bytes. return codecs.encode(text, encoding) def decode_text(data: bytes, encoding: str) -> str: Decodes encoded bytes back into a text string using the specified encoding. :param data: The encoded bytes. :param encoding: The encoding type used to decode. :return: Decoded text string. return codecs.decode(data, encoding)"},{"question":"# Pandas Coding Assessment Question **Objective**: Demonstrate the ability to manipulate and analyze data using pandas\' advanced functions. **Scenario**: You are given a dataset containing sales information of a retail company with the following columns: - `order_id`: Unique identifier for each order. - `customer_id`: Unique identifier for each customer. - `order_date`: Date when the order was placed. - `product_id`: Unique identifier for each product. - `quantity`: Quantity of the product ordered. - `price`: Unit price of the product. Your task is to perform the following operations on this dataset using pandas and provide the final transformed DataFrame: 1. Convert the `order_date` column to datetime format. 2. Create a new column `total_price` which is the product of `quantity` and `price`. 3. Compute the total sales (`total_price`) for each customer across all orders. 4. Pivot the data to have customers in rows and months (in \'YYYY-MM\' format) in columns, showing the total sales for each customer per month. Fill missing values with zeros. 5. Add a column `customer_total` to the pivoted DataFrame indicating the total sales per customer. **Input**: A DataFrame `df` containing the sales data with columns: `order_id`, `customer_id`, `order_date`, `product_id`, `quantity`, `price`. **Output**: A DataFrame in the following format: - Rows are indexed by `customer_id`. - Columns represent months in \'YYYY-MM\' format. - An additional column `customer_total` indicating the total sales per customer. **Constraints**: - You should avoid modifying the original DataFrame `df`. - Missing values in the pivot table must be filled with zeros. # Function Definition ```python def transform_sales_data(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` # Example: Given the following DataFrame `df`: | order_id | customer_id | order_date | product_id | quantity | price | |----------|--------------|------------|-------------|----------|-------| | 1 | C1 | 2023-01-15 | P1 | 2 | 100 | | 2 | C2 | 2023-01-17 | P2 | 1 | 200 | | 3 | C1 | 2023-02-20 | P1 | 3 | 100 | The transformed DataFrame should look like: | customer_id | 2023-01 | 2023-02 | customer_total | |-------------|---------|---------|----------------| | C1 | 200 | 300 | 500 | | C2 | 200 | 0 | 200 | # Notes: - Ensure you handle datetime conversion correctly. - The resultant DataFrame should be sorted by `customer_id`.","solution":"import pandas as pd def transform_sales_data(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Convert the \'order_date\' column to datetime format df[\'order_date\'] = pd.to_datetime(df[\'order_date\']) # Step 2: Create a new column \'total_price\', which is the product of \'quantity\' and \'price\' df[\'total_price\'] = df[\'quantity\'] * df[\'price\'] # Step 3: Compute the total sales (`total_price`) for each customer across all orders df[\'order_month\'] = df[\'order_date\'].dt.to_period(\'M\').astype(str) # Step 4: Pivot the data pivot_df = df.pivot_table(index=\'customer_id\', columns=\'order_month\', values=\'total_price\', aggfunc=\'sum\', fill_value=0) # Step 5: Add a column `customer_total` pivot_df[\'customer_total\'] = pivot_df.sum(axis=1) # Reset the index to keep \'customer_id\' as a column result_df = pivot_df.reset_index() return result_df"},{"question":"# Question: Advanced Seaborn HLS Palette Manipulation and Visualization You are tasked with generating and visualizing various color palettes using Seaborn\'s `hls_palette` function. This question is designed to test your understanding of seaborn\'s palette customization capabilities as well as your ability to visualize data effectively. Instructions 1. **Generate a basic HLS palette with 10 colors** using seaborn. 2. **Create another HLS palette with the same number of colors,** but modify the lightness to 0.4 and saturation to 0.5. 3. **Create a continuous colormap** based on the specified HLS parameters `(h=0.3, l=0.6, s=0.6)`. 4. **Visualize the palettes and colormap**: - Display the first palette as a bar plot, where each bar is colored using the respective colors from the palette. - Display the second palette as a strip plot, where each point on a strip is colored using the respective colors from the palette. - Display the continuous colormap using a heatmap. Function Signature ```python def generate_and_visualize_palettes(): pass ``` Expected Output The function should produce three plots: 1. A bar plot showing the 10-color HLS palette. 2. A strip plot showing the 10-color HLS palette with modified lightness and saturation. 3. A heatmap showing the continuous colormap based on the specified HLS parameters. Constraints - Use seaborn for generating palettes and plotting. - Ensure the plots are visually distinguishable with appropriate labels and titles. - Handle edge cases, such as invalid parameters gracefully. Example ```python # Function call generate_and_visualize_palettes() ``` The function call should produce the three plots in a single output. *Note: You can import additional libraries such as matplotlib as needed for visualization.* This question assesses your understanding of creating and manipulating HLS palettes using seaborn, as well as your ability to visualize these palettes effectively.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def generate_and_visualize_palettes(): # Generate a basic HLS palette with 10 colors hls_palette_basic = sns.hls_palette(10) # Generate HLS palette with modified lightness and saturation hls_palette_modified = sns.hls_palette(10, l=0.4, s=0.5) # Create a continuous colormap based on the specified HLS parameters def hls_to_rgb(h, l, s): return sns.color_palette(\\"hls\\", as_cmap=True, n_colors=256).to_rgba(h, l, s) hls_colormap_continuous = sns.light_palette((0.3, 0.6, 0.6), input=\\"hls\\", as_cmap=True) # Plotting the basic HLS palette as a bar plot plt.figure(figsize=(10, 2)) palette_basic_bar = sns.color_palette(hls_palette_basic) sns.barplot(x=list(range(10)), y=[1]*10, palette=palette_basic_bar) plt.title(\\"Basic HLS Palette - Bar Plot\\") plt.ylabel(\'\') plt.xlabel(\'\') plt.xticks([]) plt.yticks([]) plt.show() # Plotting the modified HLS palette as a strip plot plt.figure(figsize=(10, 2)) palette_modified_strip = sns.color_palette(hls_palette_modified) sns.stripplot(x=list(range(10)), y=[1]*10, palette=palette_modified_strip, size=20) plt.title(\\"Modified HLS Palette (l=0.4, s=0.5) - Strip Plot\\") plt.ylabel(\'\') plt.xlabel(\'\') plt.xticks([]) plt.yticks([]) plt.show() # Plotting the continuous colormap as a heatmap plt.figure(figsize=(10, 1)) data = np.linspace(0, 1, 256).reshape((1, 256)) sns.heatmap(data, cmap=hls_colormap_continuous, cbar=False, xticklabels=False, yticklabels=False) plt.title(\\"Continuous Colormap (h=0.3, l=0.6, s=0.6) - Heatmap\\") plt.show()"},{"question":"**Objective**: Demonstrate your understanding of PyTorch\'s `torch.Size` class and its usage in handling tensor dimensions. **Question**: You are given a tensor `x` of arbitrary dimensions. Write a function `tensor_dimensions_info` that takes this tensor as input and returns a dictionary with the following details: 1. **Number of dimensions**: The number of dimensions in the tensor. 2. **Size of each dimension**: A list of sizes of each dimension. 3. **Total number of elements**: The total number of elements in the tensor. **Function Signature**: ```python import torch def tensor_dimensions_info(x: torch.Tensor) -> dict: pass ``` **Input**: - `x`: A `torch.Tensor` of arbitrary dimensions. **Output**: - A dictionary with the following keys and their corresponding values: - `\\"num_dimensions\\"`: an integer representing the number of dimensions. - `\\"sizes\\"`: a list of integers representing the size of each dimension. - `\\"total_elements\\"`: an integer representing the total number of elements in the tensor. **Example**: ```python # Example input tensor x = torch.ones(10, 20, 30) # Expected output { \\"num_dimensions\\": 3, \\"sizes\\": [10, 20, 30], \\"total_elements\\": 6000 } ``` **Constraints**: - The input tensor will have at least one dimension. - You may assume that the tensor size will fit into memory. **Performance Requirements**: - The function should handle typical tensor sizes and dimensions efficiently. **Hints**: - Use the `size()` method of the tensor to get the dimensions. - Utilize Python\'s built-in functions and tuple operations to process the `torch.Size` object. **Evaluation Criteria**: - Correctness of the output. - Efficient use of PyTorch functions and Python operations. - Code readability and adherence to Python best practices.","solution":"import torch def tensor_dimensions_info(x: torch.Tensor) -> dict: Returns information regarding the dimensions of the input tensor. Parameters: x (torch.Tensor): Input tensor. Returns: dict: Dictionary containing the number of dimensions, sizes of each dimension, and total number of elements. num_dimensions = x.dim() sizes = list(x.size()) total_elements = x.numel() return { \\"num_dimensions\\": num_dimensions, \\"sizes\\": sizes, \\"total_elements\\": total_elements }"},{"question":"**Question: Handling Missing Values in pandas** You have been provided a dataset in the form of a pandas DataFrame named `employee_data`. The structure of the dataset is as follows: ``` emp_id name department start_date salary 0 101 Alice Sales 2020-01-15 75000.00 1 102 Bob Marketing NaT NaN 2 103 Charles NaN 2019-07-01 85000.00 3 104 Diana Sales 2020-09-12 80000.00 4 105 Edward Marketing 2018-03-30 76000.00 5 106 Fran IT NaT 73000.00 6 107 George Sales 2021-05-19 NaN ``` Your task is to implement the following functions to handle and analyze missing values in this dataset: 1. **identify_missing_values(df)**: - **Input**: A pandas DataFrame `df`. - **Output**: A pandas DataFrame with the counts of missing values in each column of `df`. - **Example**: ```python identify_missing_values(employee_data) ``` Should return: ``` column_name missing_values emp_id 0 name 0 department 1 start_date 2 salary 2 ``` 2. **fill_missing_values(df)**: - **Input**: A pandas DataFrame `df`. - **Output**: A pandas DataFrame with missing values handled as follows: - Missing values in the `department` column should be filled with the string `\'Unknown\'`. - Missing values in the `start_date` column should be filled with the earliest non-missing date from the column. - Missing values in the `salary` column should be filled with the average salary (ignoring NaNs). - **Example**: ```python fill_missing_values(employee_data) ``` Should return: ``` emp_id name department start_date salary 0 101 Alice Sales 2020-01-15 75000.0 1 102 Bob Marketing 2018-03-30 77000.0 2 103 Charles Unknown 2019-07-01 85000.0 3 104 Diana Sales 2020-09-12 80000.0 4 105 Edward Marketing 2018-03-30 76000.0 5 106 Fran IT 2018-03-30 73000.0 6 107 George Sales 2021-05-19 77000.0 ``` You need to submit the implementations of these two functions. **Constraints**: - Consider using pandas built-in functions to handle missing values efficiently. - Do not assume any additional information beyond what is provided in the problem statement. - The dataframe `employee_data` should remain unchanged; you should return a new DataFrame.","solution":"import pandas as pd def identify_missing_values(df): Returns a DataFrame with the counts of missing values in each column of the input DataFrame. missing_values = df.isna().sum().reset_index() missing_values.columns = [\'column_name\', \'missing_values\'] return missing_values def fill_missing_values(df): Returns a DataFrame with missing values handled as specified. df_filled = df.copy() # Fill missing values in \'department\' column with \'Unknown\' df_filled[\'department\'].fillna(\'Unknown\', inplace=True) # Fill missing values in \'start_date\' column with the earliest non-missing date earliest_date = df_filled[\'start_date\'].min() df_filled[\'start_date\'].fillna(earliest_date, inplace=True) # Fill missing values in \'salary\' column with the average salary (ignoring NaNs) average_salary = df_filled[\'salary\'].mean() df_filled[\'salary\'].fillna(average_salary, inplace=True) return df_filled"},{"question":"# Question You are given a batch of images with the dimensions `[N, C, H, W]`, corresponding to the number of images, channels, height, and width, respectively. Your task is to write a function that processes these images by applying the following steps: 1. Convert the input tensor to a named tensor with dimension names `(\'N\', \'C\', \'H\', \'W\')`. 2. Normalize each image by subtracting the mean and dividing by the standard deviation of its channel (i.e., normalize each channel independently). 3. Flatten the height and width dimensions into a single dimension named `\'features\'`. 4. Return the processed tensor. Function Signature ```python def process_images(imgs: torch.Tensor) -> torch.Tensor: pass ``` Input - `imgs`: A 4-dimensional `torch.Tensor` of shape `[N, C, H, W]` representing a batch of images. Output - A named `torch.Tensor` with dimensions `(\'N\', \'C\', \'features\')`. Constraints - Do not use any explicit loops (e.g., for loops) in your implementation. - The input tensor may have arbitrary values along each dimension. Example ```python import torch # Create a batch of 2 images, each with 3 channels and 4x4 pixels imgs = torch.randn(2, 3, 4, 4) # Process the images processed_imgs = process_images(imgs) # Check the tensor dimensions print(processed_imgs.names) # should output `(\'N\', \'C\', \'features\')` print(processed_imgs.shape) # should output `torch.Size([2, 3, 16])` ```","solution":"import torch def process_images(imgs: torch.Tensor) -> torch.Tensor: # Rename the dimensions of the tensor to (\'N\', \'C\', \'H\', \'W\') imgs = imgs.refine_names(\'N\', \'C\', \'H\', \'W\') # Compute the mean and standard deviation along the H and W dimensions mean = imgs.mean(dim=(\'H\', \'W\'), keepdim=True) std = imgs.std(dim=(\'H\', \'W\'), keepdim=True) # Normalize the images normalized_imgs = (imgs - mean) / (std + 1e-5) # Flatten the H and W dimensions into a single \'features\' dimension flattened_imgs = normalized_imgs.rename(None).flatten(start_dim=2).refine_names(\'N\', \'C\', \'features\') return flattened_imgs"},{"question":"# Objective You are required to create a layered visualization using seaborn\'s objects interface for given datasets. The focus should be on handling overlapping data and providing clear interpretable insights from the visualizations. # Dataset You will use the \'penguins\' dataset provided by seaborn. The dataset includes the following columns: - species - island - bill_length_mm - bill_depth_mm - flipper_length_mm - body_mass_g - sex # Task 1. Create a seaborn plot to visualize the distribution of body mass (body_mass_g) across different species but ensuring that overlapping data points are handled through jittering. Additionally, show the range of the 25th to 75th percentile for these distributions, shifted horizontally to avoid overlapping with the jittered points. 2. Save the plot to a file named `penguins_body_mass_distribution.png`. # Implementation Details - Load the \'penguins\' dataset using `seaborn.load_dataset(\'penguins\')`. - Use `seaborn.objects.Plot` to create the base plot. - Use `seaborn.objects.Dots` with `seaborn.objects.Jitter` to handle overlapping data points. - Add a range distribution using `seaborn.objects.Range` with `seaborn.objects.Perc` to display the interquartile range, shifted horizontally. # Constraints - Ensure proper labels and titles for the axes and the plot. - Use a consistent style to make the plot aesthetically pleasing and clear. - Handle any missing data appropriately. # Output - Save the resulting plot as a PNG file. # Example Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def visualize_penguins(): # Load the dataset penguins = load_dataset(\\"penguins\\") # TODO: Implement the plotting logic here # Save the plot plt.savefig(\\"penguins_body_mass_distribution.png\\") ``` # Example Usage ```python visualize_penguins() ``` This function should create the specified plot and save it to the provided file location.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def visualize_penguins(): # Load the dataset penguins = load_dataset(\\"penguins\\").dropna() # Create the base plot p = so.Plot(penguins, x=\'body_mass_g\', y=\'species\', color=\'species\') # Add jittered dots p = p.add(so.Dots(), so.Jitter(width=0.5)) # Add range distribution for 25th to 75th percentile p = p.add(so.Range(), so.Perc([25, 75]), so.Dodge()) # Customize the plot p = p.label(x=\'Body Mass (g)\', y=\'Species\', title=\'Penguins Body Mass Distribution\') # Save the plot to a file p.save(\\"penguins_body_mass_distribution.png\\")"},{"question":"# Custom Event Loop Policy Implementation **Objective:** You are required to implement a custom event loop policy by subclassing the `DefaultEventLoopPolicy` in Python\'s asyncio library. This custom policy should implement specific behavior when managing event loops and child watchers. **Task:** 1. Create a class `CustomEventLoopPolicy` that subclasses `asyncio.DefaultEventLoopPolicy`. 2. Override the `get_event_loop()` method to log a message each time the method is called. 3. Override the `new_event_loop()` method to return an instance of `CustomEventLoop` (a subclass of `asyncio.AbstractEventLoop`). 4. Create a `CustomEventLoop` class that: 1. Subclasses `asyncio.AbstractEventLoop`. 2. Implements all the abstract methods in a minimal fashion to ensure the loop can be instantiated without errors. 3. Logs a message to indicate it has been created. 5. Set the custom event loop policy using `asyncio.set_event_loop_policy()`. 6. Write a simple coroutine that demonstrates the custom event loop being used. **Input:** None directly, but the code should be executable in a Python script. **Output:** Logs printed from the custom methods and event loop creation, and output from the demonstration coroutine. **Constraints:** - Use only the standard library\'s `asyncio` package. - Ensure to handle potential issues with missing event loops gracefully. **Example Usage:** ```python import asyncio import logging # Your CustomEventLoopPolicy and CustomEventLoop implementation # Setting the custom event loop policy asyncio.set_event_loop_policy(CustomEventLoopPolicy()) async def demo_coroutine(): print(\\"Running demo coroutine\\") # Running the event loop loop = asyncio.get_event_loop() loop.run_until_complete(demo_coroutine()) ``` **Expected Logs and Output:** ``` CustomEventLoopPolicy: get_event_loop() called CustomEventLoop created Running demo coroutine ``` **Performance Requirements:** The implementation should work robustly for a typical size of tasks expected in educational demonstrations and small applications. **Submission:** Submit your implementation of `CustomEventLoopPolicy` and `CustomEventLoop` along with a small script that demonstrates usage as described in the example.","solution":"import asyncio import logging # Setup logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class CustomEventLoop(asyncio.AbstractEventLoop): def __init__(self): super().__init__() logger.info(\\"CustomEventLoop created\\") # Implement required abstract methods minimally def create_task(self, coro): return super().create_task(coro) def run_forever(self): pass def run_until_complete(self, future): pass def stop(self): pass def is_running(self): return False def close(self): pass def call_soon(self, callback, *args): pass def call_later(self, delay, callback, *args): pass def call_at(self, when, callback, *args): pass def time(self): return 0 def date(self): return 0 def create_future(self): return super().create_future() class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): logger.info(\\"CustomEventLoopPolicy: get_event_loop() called\\") return super().get_event_loop() def new_event_loop(self): return CustomEventLoop() # Set the custom event loop policy asyncio.set_event_loop_policy(CustomEventLoopPolicy()) async def demo_coroutine(): print(\\"Running demo coroutine\\") # Running the event loop loop = asyncio.get_event_loop() loop.run_until_complete(demo_coroutine())"},{"question":"Objective You are provided with a time series dataset containing daily stock prices of several companies over a period of one year. Your task is to write a function that calculates various statistics using rolling, expanding, and exponentially-weighted windows for a specific stock symbol. Data Description The dataset is provided as a pandas DataFrame with the following columns: - `date`: Date of the observation in the format `YYYY-MM-DD`. - `symbol`: Stock symbol representing the company. - `price`: Closing price of the stock for the given day. Implement the function ```python import pandas as pd def compute_window_statistics(df: pd.DataFrame, stock_symbol: str) -> pd.DataFrame: Computes various rolling, expanding, and exponentially-weighted window statistics for a specified stock symbol from the provided DataFrame. Parameters: df (pd.DataFrame): DataFrame containing the stock price data with columns [\'date\', \'symbol\', \'price\'] stock_symbol (str): The stock symbol for which to compute the statistics Returns: pd.DataFrame: DataFrame containing the following columns: - \'date\': Date of the observation - \'price\': Original closing price of the stock - \'rolling_mean_7\': 7-day rolling mean - \'expanding_mean\': Expanding mean - \'ewm_mean_10\': 10-day exponentially-weighted window mean # Filter data for the given stock symbol stock_data = df[df[\'symbol\'] == stock_symbol].copy() # Ensure the data is sorted by date stock_data[\'date\'] = pd.to_datetime(stock_data[\'date\']) stock_data = stock_data.sort_values(by=\'date\') # Calculate rolling mean over a 7-day window stock_data[\'rolling_mean_7\'] = stock_data[\'price\'].rolling(window=7).mean() # Calculate expanding mean stock_data[\'expanding_mean\'] = stock_data[\'price\'].expanding().mean() # Calculate 10-day exponentially-weighted mean stock_data[\'ewm_mean_10\'] = stock_data[\'price\'].ewm(span=10).mean() # Return only the relevant columns result = stock_data[[\'date\', \'price\', \'rolling_mean_7\', \'expanding_mean\', \'ewm_mean_10\']] return result ``` Constraints - The input DataFrame `df` can be large, containing data for multiple stock symbols over a long period. Ensure your code is efficient and handles large datasets effectively. - Handle missing values in the \'price\' column by forward filling them before calculations. Example With an input DataFrame and a stock symbol, the function should return a DataFrame containing the date, original price, 7-day rolling mean, expanding mean, and 10-day EWM mean for that stock symbol. Note This question tests your understanding of rolling, expanding, and exponentially-weighted window functions in pandas, as well as your ability to handle time-series data effectively.","solution":"import pandas as pd def compute_window_statistics(df: pd.DataFrame, stock_symbol: str) -> pd.DataFrame: Computes various rolling, expanding, and exponentially-weighted window statistics for a specified stock symbol from the provided DataFrame. Parameters: df (pd.DataFrame): DataFrame containing the stock price data with columns [\'date\', \'symbol\', \'price\'] stock_symbol (str): The stock symbol for which to compute the statistics Returns: pd.DataFrame: DataFrame containing the following columns: - \'date\': Date of the observation - \'price\': Original closing price of the stock - \'rolling_mean_7\': 7-day rolling mean - \'expanding_mean\': Expanding mean - \'ewm_mean_10\': 10-day exponentially-weighted window mean # Filter data for the given stock symbol stock_data = df[df[\'symbol\'] == stock_symbol].copy() # Ensure the data is sorted by date stock_data[\'date\'] = pd.to_datetime(stock_data[\'date\']) stock_data = stock_data.sort_values(by=\'date\') # Handle missing values by forward filling stock_data[\'price\'] = stock_data[\'price\'].ffill() # Calculate rolling mean over a 7-day window stock_data[\'rolling_mean_7\'] = stock_data[\'price\'].rolling(window=7).mean() # Calculate expanding mean stock_data[\'expanding_mean\'] = stock_data[\'price\'].expanding().mean() # Calculate 10-day exponentially-weighted mean stock_data[\'ewm_mean_10\'] = stock_data[\'price\'].ewm(span=10).mean() # Return only the relevant columns result = stock_data[[\'date\', \'price\', \'rolling_mean_7\', \'expanding_mean\', \'ewm_mean_10\']] return result"},{"question":"**Objective:** Write a Python function using the `grp` module to determine common group memberships between two Unix users. **Task:** You are required to implement a function called `common_groups(user1: str, user2: str) -> List[str]` which returns a list of common group names that the two specified Unix users belong to. The list should be in alphabetical order and must not contain duplicates. **Function Signature:** ```python def common_groups(user1: str, user2: str) -> List[str]: pass ``` **Input:** - `user1` (string): The first username. - `user2` (string): The second username. **Output:** - Returns a list of group names (strings) that the specified users have in common, sorted in alphabetical order. **Constraints:** - Both `user1` and `user2` are valid existing usernames on the Unix system. - The returned list should not contain any duplicate group names. **Example:** ```python # Given that user1 belongs to groups: [\\"admin\\", \\"sudo\\", \\"users\\"] # and user2 belongs to groups: [\\"users\\", \\"wheel\\"] common_groups(\'user1\', \'user2\') should return [\'users\'] ``` **Notes:** - You should use `grp.getgrall()` to retrieve all group entries and then determine memberships. - Account for cases where a user may not appear in the `gr_mem` field even though they are in the primary group. - Handle any necessary exceptions that could arise from the `grp` module functions. **Hint:** Consider implementing helper functions that retrieve groups by user and then find the common elements.","solution":"import grp def get_user_groups(username: str) -> set: Helper function to get all groups a user belongs to. groups = set() for group in grp.getgrall(): if username == group.gr_name or username in group.gr_mem: groups.add(group.gr_name) return groups def common_groups(user1: str, user2: str) -> list: Returns a list of common group names that the two specified Unix users belong to, sorted alphabetically. user1_groups = get_user_groups(user1) user2_groups = get_user_groups(user2) common = sorted(user1_groups.intersection(user2_groups)) return common"},{"question":"# Environment Variables and Large File Handling in Python **Objective:** Design a function that demonstrates the manipulation of environment variables and the handling of large files using the `os` module in Python. **Task:** 1. Create a function that accepts a dictionary of environment variables and their values, and a path to a large file. The function should: - Update the environment variables in the current process using the provided dictionary. - Verify if the large file handling is supported by checking the file size and if it\'s larger than 2 GiB. - If the file is larger than 2 GiB, read its content in chunks and print the first 100 bytes. **Function Signature:** ```python def manage_env_and_large_file(env_vars: dict, large_file_path: str): pass ``` **Input:** - `env_vars`: A dictionary where keys are environment variable names (string) and values are their respective values (string). - `large_file_path`: A string path to a file that may be larger than 2 GiB. **Output:** - The function should print messages for the following: - Confirmation of updated environment variables. - Whether large file handling is supported. - The first 100 bytes of the large file\'s content, if the file size exceeds 2 GiB. **Constraints:** - The function should use the `os` module for environment variable management. - The function should handle errors gracefully, printing appropriate error messages if: - Updating the environment variables fails. - The specified file does not exist or cannot be read. **Example:** Assuming a large file `huge_file.txt` exists at the path `/path/to/huge_file.txt` and your system supports large files: ```python env_vars = {\'MY_VAR\': \'123\', \'ANOTHER_VAR\': \'456\'} large_file_path = \'/path/to/huge_file.txt\' manage_env_and_large_file(env_vars, large_file_path) ``` **Expected Output:** ``` Environment variable MY_VAR set to 123 Environment variable ANOTHER_VAR set to 456 Large file handling is supported. First 100 bytes of the file content: b\'ContentOfTheLargeFile......\' ``` Make sure to follow best practices for error handling and resource management (e.g., file closing).","solution":"import os def manage_env_and_large_file(env_vars: dict, large_file_path: str): try: # Update environment variables for key, value in env_vars.items(): os.environ[key] = value print(f\\"Environment variable {key} set to {value}\\") # Check if the file exists if not os.path.exists(large_file_path): print(f\\"Error: The file {large_file_path} does not exist.\\") return # Check the file size file_size = os.path.getsize(large_file_path) if file_size > 2 * 1024 * 1024 * 1024: # 2 GiB in bytes print(\\"Large file handling is supported.\\") # Read the first 100 bytes of the file in chunks with open(large_file_path, \'rb\') as file: first_bytes = file.read(100) print(f\\"First 100 bytes of the file content: {first_bytes}\\") else: print(\\"File size is less than 2 GiB. Large file handling not required.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective:** Implement a Python class that uses the `gettext` module to handle translations for a hypothetical application. Your class should allow switching between multiple languages on the fly and support deferred translations. You will be provided with strings that require translations, and you should implement functions to translate these based on the selected language. **Instructions:** 1. Implement a class `Translator` that: - Initializes with a domain and a path to the locale directory containing `.mo` files. - Has methods to add multiple languages. - Installs the translation for a specified language. - Provides a method for deferred translation. 2. The expected methods for the `Translator` class: - `__init__(self, domain: str, localedir: str)`: Initializes the instance with the provided domain and locale directory. - `add_language(self, language: str)`: Adds a language to the translation options. - `install_language(self, language: str)`: Installs the translation for the specified language. - `gettext_deferred(self, message: str) -> str`: Returns the translated string for the currently installed language. If the language is changed later, the deferred method should translate the strings accordingly. 3. Provide usage examples for your class, showing how to: - Initialize the class. - Add and switch between languages. - Use the deferred translation method. **Example usage:** ```python # Assume translations for \'myapp\' are stored in \'/path/to/localedir\' translator = Translator(\'myapp\', \'/path/to/localedir\') # Add languages translator.add_language(\'en\') translator.add_language(\'fr\') translator.add_language(\'de\') # Install English language translator.install_language(\'en\') print(translator.gettext_deferred(\'Hello World\')) # Should output the English version # Switch to French language translator.install_language(\'fr\') print(translator.gettext_deferred(\'Hello World\')) # Should output the French version # Switch to German language translator.install_language(\'de\') print(translator.gettext_deferred(\'Hello World\')) # Should output the German version ``` **Constraints:** - You may assume that appropriate `.mo` files for the specified languages exist in the provided `localedir`. - You should handle any exceptions that might arise from missing translation files or unsupported languages gracefully. **Performance requirements:** - The class should efficiently manage multiple language installations and deferred translations without redundant computations. **Solution Template:** ```python import gettext import os class Translator: def __init__(self, domain: str, localedir: str): self.domain = domain self.localedir = localedir self.languages = [] self.current_translation = None def add_language(self, language: str): self.languages.append(language) def install_language(self, language: str): translation = gettext.translation(self.domain, localedir=self.localedir, languages=[language], fallback=True) translation.install() self.current_translation = translation def gettext_deferred(self, message: str) -> str: return gettext.gettext(message) ``` Develop your `Translator` class based on the template above and provide additional functionality as specified in the instructions.","solution":"import gettext import os class Translator: def __init__(self, domain: str, localedir: str): self.domain = domain self.localedir = localedir self.languages = {} self.current_translation = None def add_language(self, language: str): Add a language to the translation options. self.languages[language] = gettext.translation( self.domain, localedir=self.localedir, languages=[language], fallback=True ) def install_language(self, language: str): Install the translation for the specified language. if language in self.languages: self.current_translation = self.languages[language] self.languages[language].install() else: raise ValueError(f\\"Language \'{language}\' has not been added. Use add_language method first.\\") def gettext_deferred(self, message: str) -> str: Returns the translated string for the currently installed language. if self.current_translation: return self.current_translation.gettext(message) else: raise RuntimeError(\\"No language has been installed. Use install_language method first.\\")"},{"question":"You are given a dataset containing information about different individuals, including their features and a binary target column indicating whether they will buy a product or not. Your task is to create a machine learning model using Ensemble methods from the `scikit-learn` library to predict the buying behavior of individuals. Specifically, you will implement a `GradientBoostingClassifier` to predict the target variable, and compare its performance with a `RandomForestClassifier`. You need to perform the following tasks: 1. Load the dataset and split it into training and testing sets. 2. Implement a `GradientBoostingClassifier` and a `RandomForestClassifier` with appropriate hyperparameters. 3. Train both models on the training set. 4. Evaluate the performance of both models on the testing set using accuracy as the metric. 5. Print the accuracy of both models and determine which model performs better. Here is the step-by-step coding implementation: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier from sklearn.metrics import accuracy_score # Step 1: Load the dataset into a DataFrame # For this task, you can generate a synthetic dataset using sklearn\'s make_classification from sklearn.datasets import make_classification # Generate a synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42) # Convert to DataFrame for consistency data = pd.DataFrame(X) data[\'target\'] = y # Split the dataset into features and target variable X = data.drop(columns=[\'target\']) y = data[\'target\'] # Step 2: Split the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Implement a GradientBoostingClassifier with appropriate hyperparameters gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) gb_model.fit(X_train, y_train) # Step 4: Implement a RandomForestClassifier with appropriate hyperparameters rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42) rf_model.fit(X_train, y_train) # Step 5: Evaluate the performance of both models on the testing set using accuracy gb_predictions = gb_model.predict(X_test) rf_predictions = rf_model.predict(X_test) # Calculate accuracy for both models gb_accuracy = accuracy_score(y_test, gb_predictions) rf_accuracy = accuracy_score(y_test, rf_predictions) # Print the accuracy of both models print(f\\"Gradient Boosting Classifier Accuracy: {gb_accuracy:.4f}\\") print(f\\"Random Forest Classifier Accuracy: {rf_accuracy:.4f}\\") # Determine and print which model performs better better_model = \\"Gradient Boosting Classifier\\" if gb_accuracy > rf_accuracy else \\"Random Forest Classifier\\" print(f\\"The better model is: {better_model}\\") ``` # Input Format - There is no specific input format for this coding question, as you will generate a synthetic dataset for demonstration. # Output Format - The output should include the accuracy of both models and indicate which model performs better. # Constraints - Use `scikit-learn` library for implementing the models. - Ensure reproducibility by setting random states. # Performance Requirements - The implementation should efficiently handle the generated synthetic dataset and execute within a reasonable time frame.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.datasets import make_classification def train_and_evaluate_models(): # Generate a synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42) # Convert to DataFrame for consistency data = pd.DataFrame(X) data[\'target\'] = y # Split the dataset into features and target variable X = data.drop(columns=[\'target\']) y = data[\'target\'] # Split the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement a GradientBoostingClassifier with appropriate hyperparameters gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) gb_model.fit(X_train, y_train) # Implement a RandomForestClassifier with appropriate hyperparameters rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42) rf_model.fit(X_train, y_train) # Evaluate the performance of both models on the testing set using accuracy gb_predictions = gb_model.predict(X_test) rf_predictions = rf_model.predict(X_test) # Calculate accuracy for both models gb_accuracy = accuracy_score(y_test, gb_predictions) rf_accuracy = accuracy_score(y_test, rf_predictions) # Print the accuracy of both models print(f\\"Gradient Boosting Classifier Accuracy: {gb_accuracy:.4f}\\") print(f\\"Random Forest Classifier Accuracy: {rf_accuracy:.4f}\\") # Determine and print which model performs better better_model = \\"Gradient Boosting Classifier\\" if gb_accuracy > rf_accuracy else \\"Random Forest Classifier\\" print(f\\"The better model is: {better_model}\\") return gb_accuracy, rf_accuracy, better_model"},{"question":"# Question: Custom Set Operations You are given the task of creating a custom implementation of a set using the provided Python set object API. Your implementation should support the following operations: 1. **Initialization**: Create an empty set or a set with initial elements. 2. **Addition**: Add an element to the set. 3. **Removal**: Remove an element from the set (if it exists). 4. **Membership**: Check if an element is in the set. 5. **Union**: Return a new set that is the union of the current set with another set. 6. **Intersection**: Return a new set that is the intersection of the current set with another set. Implement a Python class `CustomSet` with the methods described above: Class Definition ```python class CustomSet: def __init__(self, iterable=None): Initialize the set. If an iterable is provided, add the elements to the set. Args: iterable (iterable, optional): An iterable of elements to add to the set initially. Defaults to None. Raises: TypeError: If the provided iterable is not actually iterable. pass def add(self, element): Add an element to the set. Args: element: The element to add to the set. Raises: TypeError: If the element cannot be added to the set because it is unhashable. pass def remove(self, element): Remove an element from the set if it exists. Args: element: The element to remove from the set. Returns: bool: True if the element was removed, False if the element was not found in the set. Raises: TypeError: If the element cannot be removed from the set because it is unhashable. pass def contains(self, element): Check if an element is in the set. Args: element: The element to check for membership in the set. Returns: bool: True if the element is in the set, False otherwise. pass def union(self, other): Return a new set that is the union of the current set with another set. Args: other (CustomSet): The other set to union with. Returns: CustomSet: A new CustomSet instance containing all unique elements from both sets. Raises: TypeError: If \'other\' is not an instance of CustomSet. pass def intersection(self, other): Return a new set that is the intersection of the current set with another set. Args: other (CustomSet): The other set to intersect with. Returns: CustomSet: A new CustomSet instance containing only elements present in both sets. Raises: TypeError: If \'other\' is not an instance of CustomSet. pass ``` Constraints 1. You should use the provided API functions where applicable. 2. Your implementation should adhere to typical set behavior. 3. Do not use Python\'s built-in set functions or methods directly (e.g., `set.add()`, `set.remove()`, etc.) other than for initialization. 4. Ensure the class handles errors and exceptions according to the specifications. Example Usage ```python # Create a new CustomSet with initial elements custom_set = CustomSet([1, 2, 3]) # Add elements custom_set.add(4) custom_set.add(2) # Adding duplicate element (should not affect the set) # Remove elements custom_set.remove(3) # Element exists custom_set.remove(5) # Element does not exist # Check membership print(custom_set.contains(1)) # True print(custom_set.contains(3)) # False # Union of two sets other_set = CustomSet([3, 4, 5]) union_set = custom_set.union(other_set) print(union_set) # CustomSet([1, 2, 4, 5]) # Intersection of two sets intersection_set = custom_set.intersection(other_set) print(intersection_set) # CustomSet([4]) ``` Implement the `CustomSet` class and ensure that your implementation is efficient and accurate.","solution":"class CustomSet: def __init__(self, iterable=None): Initialize the set. If an iterable is provided, add the elements to the set. if iterable is None: self._elements = [] else: try: self._elements = [] for item in iterable: if item not in self._elements: self._elements.append(item) except TypeError: raise TypeError(\\"The provided iterable is not actually iterable.\\") def add(self, element): Add an element to the set. if element not in self._elements: self._elements.append(element) def remove(self, element): Remove an element from the set if it exists. if element in self._elements: self._elements.remove(element) return True return False def contains(self, element): Check if an element is in the set. return element in self._elements def union(self, other): Return a new set that is the union of the current set with another set. if not isinstance(other, CustomSet): raise TypeError(\\"\'other\' must be an instance of CustomSet.\\") new_set = CustomSet(self._elements) for element in other._elements: if element not in new_set._elements: new_set._elements.append(element) return new_set def intersection(self, other): Return a new set that is the intersection of the current set with another set. if not isinstance(other, CustomSet): raise TypeError(\\"\'other\' must be an instance of CustomSet.\\") new_set = CustomSet() for element in self._elements: if element in other._elements: new_set._elements.append(element) return new_set def __repr__(self): return f\\"CustomSet({self._elements})\\""},{"question":"Objective You will be assessed on your understanding of `seaborn`\'s `lmplot` function and your ability to generate advanced visualizations based on multiple conditioning variables using the `penguins` dataset provided by the `seaborn` library. Problem Statement Write a function `plot_penguins_regression` that takes in a dataset and generates multiple subplots displaying linear regression fits of `bill_length_mm` against `bill_depth_mm`. Your function should: 1. Display the regression results conditionally split by the species of the penguins along the columns. 2. Further conditionally split by the sex of the penguins along the rows. 3. Allow the axis limits to vary across subplots. 4. Customize the plot with a proper title, x-axis label, y-axis label, and adequate subplot size. Input - `data`: A pandas DataFrame containing the `penguins` dataset with the necessary fields: `bill_length_mm`, `bill_depth_mm`, `species`, and `sex`. Output - This function does not return any value but should display the resulting plot. Function Signature ```python import pandas as pd def plot_penguins_regression(data: pd.DataFrame) -> None: pass ``` Constraints - You must use `seaborn.lmplot` to create the plots. - Ensure that the plots are appropriately labeled and titled for clarity. Example ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") plot_penguins_regression(penguins) ``` This should display a matrix of subplots split by species and sex, with linear regression lines fitted to the bill measurements.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_penguins_regression(data: pd.DataFrame) -> None: Generates multiple subplots displaying linear regression fits of bill_length_mm against bill_depth_mm, conditionally split by species and sex. # Create the lmplot with species along columns and sex along rows lm = sns.lmplot( x=\'bill_length_mm\', y=\'bill_depth_mm\', col=\'species\', row=\'sex\', data=data, aspect=1, height=4, scatter_kws={\'s\': 10} # Adjusting size of scatter plot points ) # Set axis labels and the plot title lm.set_axis_labels(\'Bill Length (mm)\', \'Bill Depth (mm)\') lm.set_titles(\\"{col_name} {row_name}\\") plt.suptitle(\'Linear Regression of Bill Length vs Bill Depth by Species and Sex\', fontsize=16, y=1.02) # Display the plot plt.show()"},{"question":"Youve been tasked with creating a custom attention mechanism that uses a combination of various block masks to control the parts of the data that are focused on during computations. You will need to implement several functionalities to achieve this. Task: 1. **Implement the `combine_masks` function:** ```python def combine_masks(mask1: torch.Tensor, mask2: torch.Tensor, operation: str) -> torch.Tensor: Combines two block masks using a specified operation. Args: - mask1 (torch.Tensor): The first mask tensor. - mask2 (torch.Tensor): The second mask tensor. - operation (str): The operation to apply (\'and\' or \'or\'). Returns: torch.Tensor: The combined mask tensor. pass ``` - **Input:** - Two block masks `mask1` and `mask2` as PyTorch tensors of the same shape. - An operation string which is either `\'and\'` or `\'or\'` that specifies how the masks should be combined. - **Output:** - A combined mask as a PyTorch tensor, where the operation is applied element-wise. 2. **Implement the `apply_attention_with_custom_mask` function:** ```python def apply_attention_with_custom_mask(attention_fn, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: Applies the attention function with a custom mask. Args: - attention_fn (Callable): The attention function to apply. - query (torch.Tensor): The query tensor. - key (torch.Tensor): The key tensor. - value (torch.Tensor): The value tensor. - mask (torch.Tensor): The custom mask tensor. Returns: torch.Tensor: The result of the attention mechanism with the custom mask applied. pass ``` - **Input:** - An attention function `attention_fn` which takes `query`, `key`, and `value` as inputs and applies a given mask. - Tensors `query`, `key`, and `value` of appropriate shapes for the attention function. - A custom mask tensor `mask`. - **Output:** - The result of the attention mechanism with the custom mask applied, as a tensor. 3. **Write a test case:** - Create two block masks using `create_block_mask` or `create_mask`. - Combine these masks using the `combine_masks` function. - Apply an attention mechanism using the `apply_attention_with_custom_mask` function with the combined mask. - Print the result. Constraints: - You are required to use the `torch.nn.attention.flex_attention` module and its corresponding utilities. - Ensure all tensor shapes are compatible and properly handled. - Avoid using any additional libraries outside of PyTorch. Example: Example usage for a simple scenario for the `combine_masks` function: ```python mask1 = create_block_mask(...) mask2 = create_block_mask(...) combined_mask = combine_masks(mask1, mask2, operation=\'and\') assert combined_mask.shape == mask1.shape ``` Keep in mind that this example omits details like the specific tensor shapes and the actual attention function implementation, which you need to handle in your solution. # Notes: - Consider potential edge cases, such as mismatched mask shapes. - Ensure your attention mechanism respects the masks\' constraints during computations. **Good luck!**","solution":"import torch import torch.nn.functional as F def combine_masks(mask1: torch.Tensor, mask2: torch.Tensor, operation: str) -> torch.Tensor: Combines two block masks using a specified operation. Args: - mask1 (torch.Tensor): The first mask tensor. - mask2 (torch.Tensor): The second mask tensor. - operation (str): The operation to apply (\'and\' or \'or\'). Returns: torch.Tensor: The combined mask tensor. if operation == \'and\': return mask1 & mask2 elif operation == \'or\': return mask1 | mask2 else: raise ValueError(\\"Unsupported operation. Use \'and\' or \'or\'.\\") def apply_attention_with_custom_mask(attention_fn, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: Applies the attention function with a custom mask. Args: - attention_fn (Callable): The attention function to apply. - query (torch.Tensor): The query tensor. - key (torch.Tensor): The key tensor. - value (torch.Tensor): The value tensor. - mask (torch.Tensor): The custom mask tensor. Returns: torch.Tensor: The result of the attention mechanism with the custom mask applied. # Apply the mask to the scores before attention operation scores = torch.matmul(query, key.transpose(-2, -1)) scores = scores.masked_fill(~mask, float(\'-inf\')) attention_weights = F.softmax(scores, dim=-1) return torch.matmul(attention_weights, value)"},{"question":"Handling Missing Values in a DataFrame using pandas You have been provided with a CSV file named `data.csv` that contains simulated data from a survey. Unfortunately, the dataset includes some missing values, and you need to perform a comprehensive analysis to handle these missing values effectively. # Task: 1. **Load the DataFrame:** - Load the data from the `data.csv` file into a pandas DataFrame. 2. **Identify Missing Values:** - Write a function `identify_missing_values(df)` that takes a pandas DataFrame `df` as input and returns a DataFrame index showing the rows and columns where the missing values exist. 3. **Impute Missing Values:** - Write a function `impute_missing_values(df, method=\'mean\')` that takes in a DataFrame `df` and a method (either \'mean\', \'median\', or \'mode\'). The function should handle missing values in the following way: - For numerical columns, replace missing values with the specified method. - For categorical columns, replace missing values with the mode. - Validate and ensure that the `method` parameter only accepts one of the specified strings. 4. **Analyze Impact of Missing Values:** - Write a function `analyze_impact_of_missing_values(df, original_df)` that compares the descriptive statistics of the DataFrame before and after imputation. This function should return a DataFrame that presents the differences in basic statistics like mean, median, and count for each column. # Input: - A CSV file named `data.csv` with columns of different data types, including numerical and categorical data. # Expected Output: 1. A DataFrame indicating the location of missing values. 2. A DataFrame with missing values replaced as per the specified imputation method. 3. A DataFrame that displays the comparative statistics illustrating the impact of imputation. # Constraints: - Your solution should be efficient and handle large datasets gracefully. - Ensure to handle edge cases, e.g., when there are columns with all missing values. # Example: ```python import pandas as pd # Function signatures to be implemented def identify_missing_values(df): # Implementation here def impute_missing_values(df, method=\'mean\'): # Implementation here def analyze_impact_of_missing_values(df, original_df): # Implementation here # Sample usage df = pd.read_csv(\'data.csv\') missing_values_index = identify_missing_values(df) imputed_df = impute_missing_values(df, method=\'median\') impact_analysis = analyze_impact_of_missing_values(imputed_df, df) ``` Provide your solution to these tasks by implementing the functions detailed above.","solution":"import pandas as pd def identify_missing_values(df): Returns a DataFrame index showing the rows and columns where the missing values exist. return df.isnull().stack()[df.isnull().stack()].index.tolist() def impute_missing_values(df, method=\'mean\'): Imputes missing values in the DataFrame based on the specified method. For numerical columns: mean, median, or mode. For categorical columns: mode. if method not in [\'mean\', \'median\', \'mode\']: raise ValueError(\\"Method must be one of [\'mean\', \'median\', \'mode\']\\") df_imputed = df.copy() for column in df_imputed.columns: if df_imputed[column].dtype in [float, int]: if method == \'mean\': value = df_imputed[column].mean() elif method == \'median\': value = df_imputed[column].median() elif method == \'mode\': value = df_imputed[column].mode()[0] df_imputed[column].fillna(value, inplace=True) else: mode_value = df_imputed[column].mode()[0] df_imputed[column].fillna(mode_value, inplace=True) return df_imputed def analyze_impact_of_missing_values(df, original_df): Analyzes the impact of missing values by comparing the descriptive statistics of the DataFrame before and after imputation. Returns a DataFrame with the differences in basic statistics. original_stats = original_df.describe(include=\'all\').transpose() imputed_stats = df.describe(include=\'all\').transpose() summary = pd.concat([original_stats, imputed_stats], axis=1, keys=[\'Original\', \'Imputed\']) summary[\'mean_diff\'] = summary[\'Imputed\'][\'mean\'] - summary[\'Original\'][\'mean\'] summary[\'median_diff\'] = summary[\'Imputed\'][\'50%\'] - summary[\'Original\'][\'50%\'] summary[\'count_diff\'] = summary[\'Imputed\'][\'count\'] - summary[\'Original\'][\'count\'] return summary[[\'mean_diff\', \'median_diff\', \'count_diff\']] # Debugging and checking functions on a sample dataframe if __name__ == \\"__main__\\": df = pd.read_csv(\'data.csv\') missing_values_index = identify_missing_values(df) imputed_df = impute_missing_values(df, method=\'median\') impact_analysis = analyze_impact_of_missing_values(imputed_df, df) # Displaying outputs for verification print(missing_values_index) print(imputed_df.head()) print(impact_analysis.head())"},{"question":"Advanced Data Processing and Function Usage You are given a list of transactions where each transaction is a dictionary with the following keys: `\'id\'`, `\'amount\'`, and `\'category\'`. Your task is to implement a function `process_transactions` that performs the following operations: 1. **Filter Transactions**: Keep only transactions where the `\'amount\'` is greater than zero. 2. **Categorize Transactions**: Group the transactions by `\'category\'` and calculate the sum of `\'amount\'` in each category. 3. **Sort Categories**: Sort the categories by their total amount in descending order. 4. **Format Result**: Return a list of categories with their total amounts formatted as strings with 2 decimal places (e.g., `[\\"food: 123.45\\", \\"utilities: 98.00\\", ...]`). The implementation should handle the following: - Use the built-in `filter()`, `map()`, and `sorted()` functions. - Use lambda functions and the functions from the `operator` module where appropriate. - Maintain efficiency with respect to time and space complexity. # Detailed Specifications Input - A list of dictionaries, where each dictionary represents a transaction. Example: ```python transactions = [ {\'id\': 1, \'amount\': 150.75, \'category\': \'food\'}, {\'id\': 2, \'amount\': -20.00, \'category\': \'transport\'}, {\'id\': 3, \'amount\': 45.10, \'category\': \'utilities\'}, {\'id\': 4, \'amount\': 100.00, \'category\': \'food\'}, {\'id\': 5, \'amount\': 0.00, \'category\': \'entertainment\'} ] ``` Output - A list of formatted strings sorted by the total amount in each category in descending order. Example: ```python [\'food: 250.75\', \'utilities: 45.10\'] ``` Constraints - All transaction amounts are floats. - Each transaction has a unique `\'id\'`. # Example ```python transactions = [ {\'id\': 1, \'amount\': 150.75, \'category\': \'food\'}, {\'id\': 2, \'amount\': -20.00, \'category\': \'transport\'}, {\'id\': 3, \'amount\': 45.10, \'category\': \'utilities\'}, {\'id\': 4, \'amount\': 100.00, \'category\': \'food\'}, {\'id\': 5, \'amount\': 0.00, \'category\': \'entertainment\'} ] def process_transactions(transactions): pass # Implement the function here print(process_transactions(transactions)) # Output: [\'food: 250.75\', \'utilities: 45.10\'] ``` Notes 1. Use the `filter()` function to filter out transactions with non-positive amounts. 2. Use the `sorted()` function to sort the categories by the total amount in descending order. 3. Ensure the formatted return values are strings with 2 decimal places.","solution":"from collections import defaultdict def process_transactions(transactions): Processes the given list of transactions. Args: transactions (list): A list of dictionaries representing transactions. Returns: list: A list of formatted strings representing categories and their total amount. # Step 1: Filter Transactions valid_transactions = filter(lambda x: x[\'amount\'] > 0, transactions) # Step 2: Categorize Transactions category_totals = defaultdict(float) for transaction in valid_transactions: category_totals[transaction[\'category\']] += transaction[\'amount\'] # Step 3: Sort Categories sorted_categories = sorted(category_totals.items(), key=lambda x: x[1], reverse=True) # Step 4: Format Result formatted_result = [f\\"{category}: {total:.2f}\\" for category, total in sorted_categories] return formatted_result"},{"question":"# Question: Implementing a Custom Buffered Protocol with asyncio In this task, you will implement a custom buffered protocol using the asyncio package\'s low-level API for network communication. The custom protocol will be designed to handle a hypothetical \\"Line-delimited Text Protocol\\" (LDTP) where each message from the client ends with a newline character (`n`). Your implementation will handle incoming messages accumulatively and process complete lines. Requirements: 1. **Protocol Definition**: - Implement a class `LineDelimitedProtocol` that inherits from `asyncio.BufferedProtocol`. 2. **Buffer Management**: - Implement the `get_buffer` and `buffer_updated` methods to handle buffer allocations and updates. - `get_buffer(sizehint)` should return a buffer based on the `sizehint`. - `buffer_updated(nbytes)` should be able to identify complete lines of text received in the buffer and handle these lines. 3. **Protocol Methods**: - Upon receiving a complete line (ending with `n`), the protocol should invoke the `line_received` method with the line of text as an argument. - Implement stub methods for `connection_made`, `connection_lost`, and `eof_received` as defined by the base class, performing necessary setup and teardown. 4. **Sample Usage**: - Demonstrate the usage of your `LineDelimitedProtocol` by creating a simple asyncio TCP server that uses this protocol to receive data. - The server should print each complete line received to the console. Function Definitions: - **class LineDelimitedProtocol(asyncio.BufferedProtocol)** - `def get_buffer(self, sizehint: int) -> memoryview:` - `def buffer_updated(self, nbytes: int) -> None:` - `def connection_made(self, transport: asyncio.BaseTransport) -> None:` - `def connection_lost(self, exc: Optional[Exception]) -> None:` - `def eof_received(self) -> Optional[bool]:` - `def line_received(self, line: bytes) -> None:` Constraints: - **Input and Output**: - The protocol receives incoming text data from a TCP connection. - The protocol accumulates text until it detects a newline, at which point it processes the complete line and resets its buffer. - When a complete line is detected, it should be printed to the console. Example Server Setup: ```python import asyncio class LineDelimitedProtocol(asyncio.BufferedProtocol): def __init__(self): self.buffer = bytearray() def connection_made(self, transport): self.transport = transport print(\\"Connection made\\") def connection_lost(self, exc): print(\\"Connection lost\\") def get_buffer(self, sizehint): print(\\"Allocating buffer of size\\", sizehint) return memoryview(self.buffer).cast(\'B\') def buffer_updated(self, nbytes): print(f\\"Buffer updated with {nbytes} bytes\\") while b\'n\' in self.buffer: line, self.buffer = self.buffer.split(b\'n\', 1) self.line_received(line + b\'n\') def eof_received(self): print(\\"EOF received\\") if self.buffer: self.line_received(self.buffer) return None def line_received(self, line): print(\\"Line received:\\", line.decode().strip()) async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: LineDelimitedProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` Notes: - Your implementation should handle multiple lines potentially being received in a single read. - Ensure proper handling of buffer overflows and manage memory efficiently.","solution":"import asyncio class LineDelimitedProtocol(asyncio.BufferedProtocol): def __init__(self): self.buffer = bytearray() self.transport = None def connection_made(self, transport): self.transport = transport print(\\"Connection made\\") def connection_lost(self, exc): print(\\"Connection lost\\") if exc: print(f\\"Connection lost with error: {exc}\\") def get_buffer(self, sizehint): extra_space = len(self.buffer) + sizehint if extra_space > len(self.buffer): self.buffer.extend(b\'0\' * (extra_space - len(self.buffer))) return memoryview(self.buffer)[len(self.buffer):] def buffer_updated(self, nbytes): self.buffer = self.buffer[:-nbytes] self.buffer.extend(bytes(memoryview(self.buffer)[-nbytes:])) while b\'n\' in self.buffer: line, self.buffer = self.buffer.split(b\'n\', 1) self.line_received(line + b\'n\') def eof_received(self): print(\\"EOF received\\") if self.buffer: self.line_received(self.buffer) return None def line_received(self, line): print(\\"Line received:\\", line.decode().strip()) async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: LineDelimitedProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"You are tasked with creating a Python module for distributing a given set of source files. Your goal is to automatically generate the necessary built distribution files for various platforms and formats using the Distutils utility. **Task:** 1. Implement a function `create_built_distribution(config)` that generates built distribution files based on the given configuration. **Function Signature:** ```python def create_built_distribution(config: dict) -> None: pass ``` **Input:** - `config` - a dictionary containing the setup and build configurations. The dictionary will have the following structure: ```python config = { \\"name\\": \\"example_module\\", \\"version\\": \\"1.0\\", \\"description\\": \\"An example Python module\\", \\"author\\": \\"John Doe\\", \\"author_email\\": \\"jdoe@example.com\\", \\"url\\": \\"https://example.com\\", \\"files\\": [\\"example_module.py\\", \\"README.md\\"], \\"build_formats\\": [\\"gztar\\", \\"zip\\", \\"rpm\\"] # list of formats to build } ``` **Output:** - The function should create the distribution archives in the appropriate format in a directory named `dist`. **Constraints:** - You can assume that all source files specified in the `files` list exist in the current working directory. - The `dist` directory should be created if it does not exist. **Example:** ```python config = { \\"name\\": \\"example_module\\", \\"version\\": \\"1.0\\", \\"description\\": \\"An example Python module\\", \\"author\\": \\"John Doe\\", \\"author_email\\": \\"jdoe@example.com\\", \\"url\\": \\"https://example.com\\", \\"files\\": [\\"example_module.py\\", \\"README.md\\"], \\"build_formats\\": [\\"gztar\\", \\"zip\\", \\"rpm\\"] } create_built_distribution(config) ``` **Notes:** - Use the `distutils.core.setup` function to configure the setup script. - Use the `bdist` command with the `--formats` option to create the builds. - Ensure to manage temporary files and directories appropriately. **Hints:** - Refer to the Distutils `setup` and `bdist` commands documentation for detailed usage. - You might need to use Python\'s `os` and `shutil` library functions for file and directory operations. - Pay attention to the `--formats` option of the `bdist` command to specify multiple formats in one run.","solution":"import os from distutils.core import setup from distutils.command.bdist import bdist def create_built_distribution(config: dict) -> None: Generates built distribution files based on the given configuration. Creates the distribution archives in the appropriate format in a directory named `dist`. # Prepare the setup arguments from the config setup_args = { \'name\': config[\'name\'], \'version\': config[\'version\'], \'description\': config[\'description\'], \'author\': config[\'author\'], \'author_email\': config[\'author_email\'], \'url\': config[\'url\'], \'py_modules\': [os.path.splitext(file)[0] for file in config[\'files\'] if file.endswith(\'.py\')], \'data_files\': [(\'\', [file for file in config[\'files\'] if not file.endswith(\'.py\')])] } # Create the dist directory if it doesn\'t exist if not os.path.exists(\'dist\'): os.makedirs(\'dist\') # Prepare and run the setup setup(script_args=[\'bdist\', \'--formats=\' + \',\'.join(config[\'build_formats\'])], **setup_args)"},{"question":"# Pandas Coding Assessment: Time-Series Data Resampling and Aggregation Objective The purpose of this assessment is to evaluate your ability to work with time-series data using the pandas library, specifically focusing on the `Resampler` class. You will need to resample a given dataset, handle missing values, and perform various aggregate operations. Problem Statement You are provided with a time-series dataset containing daily temperature readings for a specific location over a period of one year. Your task is to perform the following operations: 1. **Load the Data**: Read the dataset from a CSV file. 2. **Resample the Data**: - Downsample the data to a weekly frequency, using the `mean` temperature for each week. - Resample the data to a monthly frequency, using the `max` temperature for each month. 3. **Handle Missing Data**: - Upsample the weekly data to a daily frequency and fill missing values using forward fill (`ffill`) method. 4. **Aggregate and Analyze**: - Calculate the rolling mean with a window of 7 days on the original daily data. - Compute the cumulative sum of the daily temperatures. 5. **Output**: Save the results to new CSV files. Input 1. CSV file with two columns: `date` (YYYY-MM-DD format) and `temperature` (float). Output 1. CSV file `weekly_mean_temperature.csv` with downsampled weekly mean temperatures. 2. CSV file `monthly_max_temperature.csv` with resampled monthly maximum temperatures. 3. CSV file `daily_ffill_temperature.csv` with upsampled daily temperatures using forward fill. 4. CSV file `rolling_mean_temperature.csv` containing the 7-day rolling mean temperatures. 5. CSV file `cumulative_sum_temperature.csv` containing the cumulative sum of the daily temperatures. Constraints 1. The dataset spans exactly from `2021-01-01` to `2021-12-31`. 2. You must use the pandas library for all operations. 3. Assume the dataset is large and optimize performance accordingly. Performance Requirements 1. All operations should be performed efficiently with respect to both time and memory. Instructions 1. Implement your solution in a function named `resample_and_aggregate_temperature_data(filepath)`. 2. The function should take one argument `filepath` which is the path to the input CSV file. 3. Ensure your function handles any potential missing data in the input gracefully. Sample Input A sample input CSV file (`temperature_data.csv`) might look like this: ```csv date,temperature 2021-01-01,25.0 2021-01-02,26.5 2021-01-03,28.5 ... 2021-12-31,22.0 ``` Sample Output Your function should generate the following output files: - `weekly_mean_temperature.csv` - `monthly_max_temperature.csv` - `daily_ffill_temperature.csv` - `rolling_mean_temperature.csv` - `cumulative_sum_temperature.csv` Note: The actual content and structure of these CSV files will depend on the operations applied to the input dataset. Good luck, and be sure to test your solution thoroughly!","solution":"import pandas as pd def resample_and_aggregate_temperature_data(filepath): # Load the data df = pd.read_csv(filepath, parse_dates=[\'date\']) df.set_index(\'date\', inplace=True) # Downsample the data to a weekly frequency using the mean temperature for each week weekly_mean = df.resample(\'W\').mean() weekly_mean.to_csv(\'weekly_mean_temperature.csv\') # Resample the data to a monthly frequency using the max temperature for each month monthly_max = df.resample(\'M\').max() monthly_max.to_csv(\'monthly_max_temperature.csv\') # Upsample the weekly data to a daily frequency and fill missing values using forward fill method daily_ffill = weekly_mean.resample(\'D\').ffill() daily_ffill.to_csv(\'daily_ffill_temperature.csv\') # Calculate the rolling mean with a window of 7 days on the original daily data rolling_mean = df.rolling(window=7).mean() rolling_mean.to_csv(\'rolling_mean_temperature.csv\') # Compute the cumulative sum of the daily temperatures cumulative_sum = df.cumsum() cumulative_sum.to_csv(\'cumulative_sum_temperature.csv\')"},{"question":"**Objective:** Implement and manage asynchronous contexts using `contextlib` utilities in Python. **Problem Statement:** You are required to implement an `AsyncMultiResourceHandler` class that leverages `contextlib.AsyncExitStack` to manage multiple asynchronous resources. Each resource is represented by an asynchronous context manager that asynchronously acquires and releases its resource. Your `AsyncMultiResourceHandler` class should: 1. Accept an arbitrary number of asynchronous context managers/resources. 2. Enter all the provided context managers/resources upon entering the context. 3. Ensure that all resources are properly released upon exiting the context. 4. Handle exceptions that occur when entering any of the context managers, ensuring that previously entered contexts are properly cleaned up. **Specifications:** - Implement the `AsyncMultiResourceHandler` class. - The class should use `contextlib.AsyncExitStack` to manage the resources. - The class should provide `__aenter__` and `__aexit__` methods to handle the async context. **Expected Input and Output:** - **Input:** A list of asynchronous context managers/resources. - **Output:** A list of resources obtained from the asynchronous context managers. **Example Usage:** ```python from contextlib import asynccontextmanager import asyncio import contextlib @asynccontextmanager async def async_resource(name): print(f\\"Acquiring {name}\\") yield name print(f\\"Releasing {name}\\") async def main(): async with AsyncMultiResourceHandler( async_resource(\\"Resource1\\"), async_resource(\\"Resource2\\"), async_resource(\\"Resource3\\") ) as resources: print(f\\"Using resources: {resources}\\") asyncio.run(main()) ``` **Example Output:** ``` Acquiring Resource1 Acquiring Resource2 Acquiring Resource3 Using resources: [\'Resource1\', \'Resource2\', \'Resource3\'] Releasing Resource3 Releasing Resource2 Releasing Resource1 ``` **Constraints:** - Ensure proper exception handling. If acquiring any resource fails, already acquired resources should be released appropriately. - The implementation must be compatible with Python 3.7+ to support asynchronous context managers. Implement the `AsyncMultiResourceHandler` class: ```python from contextlib import AsyncExitStack class AsyncMultiResourceHandler: def __init__(self, *resources): self.resources = resources async def __aenter__(self): self.stack = AsyncExitStack() await self.stack.__aenter__() acquired_resources = [] try: for resource in self.resources: acquired = await self.stack.enter_async_context(resource) acquired_resources.append(acquired) except Exception: await self.stack.__aexit__(None, None, None) raise return acquired_resources async def __aexit__(self, exc_type, exc_value, traceback): await self.stack.__aexit__(exc_type, exc_value, traceback) ``` **Notes:** - This question assesses the understanding and implementation of advanced context management patterns, asynchronous operations, exception handling, and resource management in Python.","solution":"from contextlib import AsyncExitStack class AsyncMultiResourceHandler: def __init__(self, *resources): self.resources = resources async def __aenter__(self): self.stack = AsyncExitStack() await self.stack.__aenter__() acquired_resources = [] try: for resource in self.resources: acquired = await self.stack.enter_async_context(resource) acquired_resources.append(acquired) except Exception: await self.stack.__aexit__(None, None, None) raise return acquired_resources async def __aexit__(self, exc_type, exc_value, traceback): await self.stack.__aexit__(exc_type, exc_value, traceback)"},{"question":"**Objective**: Demonstrate understanding of data preprocessing using `scikit-learn` utilities. **Problem Statement**: You have been given a dataset of customer attributes and their respective category labels. The data includes: 1. Numerical features that need to be standardized. 2. Categorical features that need to be encoded. Your task is to write a function `preprocess_data` that: 1. Standardizes the numerical features using `StandardScaler`. 2. Encodes categorical features using `OneHotEncoder`. 3. Combines both transformed numerical and categorical features into a final numpy array. **Input**: - `numeric_data`: A 2D numpy array of shape (n_samples, n_numeric_features) containing numerical features. - `categorical_data`: A 2D numpy array of shape (n_samples, n_categorical_features) containing categorical features. **Output**: - A 2D numpy array with all numerical features standardized and categorical features one-hot encoded. **Function Signature**: ```python import numpy as np from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer def preprocess_data(numeric_data: np.ndarray, categorical_data: np.ndarray) -> np.ndarray: # Your code here ``` **Example**: ```python numeric_data = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) categorical_data = np.array([[\'red\'], [\'green\'], [\'blue\']]) preprocessed_data = preprocess_data(numeric_data, categorical_data) print(preprocessed_data) ``` **Expected Output**: The numerical features should be standardized to have zero mean and unit variance, and the categorical features should be one-hot encoded, resulting in a 2D numpy array. For instance: ``` array([[ -1.22474487, -1.22474487, 1., 0., 0.], [ 0., 0., 0., 1., 0.], [ 1.22474487, 1.22474487, 0., 0., 1.]]) ``` **Constraints**: - Do not make assumptions about the number of samples or the number of numerical or categorical features. - Ensure the function is scalable and can handle a reasonably large dataset. **Note**: Do run the example to check if your function works properly.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline def preprocess_data(numeric_data: np.ndarray, categorical_data: np.ndarray) -> np.ndarray: # Creating the transformer for numeric data using StandardScaler numeric_transformer = StandardScaler() # Creating the transformer for categorical data using OneHotEncoder categorical_transformer = OneHotEncoder() # Combining both transformers into a ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, list(range(numeric_data.shape[1]))), (\'cat\', categorical_transformer, list(range(numeric_data.shape[1], numeric_data.shape[1] + categorical_data.shape[1]))) ]) # Concatenating numeric and categorical data combined_data = np.hstack((numeric_data, categorical_data)) transformed_data = preprocessor.fit_transform(combined_data) return transformed_data"},{"question":"You are tasked with creating an error-logging system for a Python web application using the `cgitb` module. This system should provide detailed traceback information both as HTML (for web display) and as plain text (for logging purposes). # Requirements 1. **Function `setup_error_logging(logdir: str)`**: - **Input**: A string representing the directory where log files should be saved. - **Output**: None. - **Functionality**: 1. Activates the `cgitb` module with the following settings: - `display=1`: The traceback should be displayed in the browser. - `logdir`: The provided directory should be used to save log files. - `context=5`: Around 5 lines of context should be shown. - `format=\'html\'`: The output should be formatted as HTML. 2. **Function `log_error_to_file(info: tuple, logdir: str)`**: - **Input**: A tuple containing the exception information (typically from `sys.exc_info()`) and a string representing the directory where log files should be saved. - **Output**: None. - **Functionality**: 1. Uses the `cgitb.text` function to format the traceback as plain text. 2. Saves the formatted traceback to a log file in the specified directory. 3. **Function `generate_error_page(info: tuple) -> str`**: - **Input**: A tuple containing the exception information (typically from `sys.exc_info()`). - **Output**: A string containing the HTML-formatted traceback. - **Functionality**: 1. Uses the `cgitb.html` function to format the traceback as HTML. 2. Returns the HTML-formatted traceback. # Constraints - The directory for logging should exist and be writable. - The exception information will always be a tuple as returned by `sys.exc_info()`. # Example Usage ```python import sys import os import cgitb def setup_error_logging(logdir: str): cgitb.enable(display=1, logdir=logdir, context=5, format=\'html\') def log_error_to_file(info: tuple, logdir: str): with open(os.path.join(logdir, \'error_log.txt\'), \'a\') as log_file: log_file.write(cgitb.text(info, context=5)) def generate_error_page(info: tuple) -> str: return cgitb.html(info, context=5) # Example call logdir = \'/path/to/logdir\' setup_error_logging(logdir) try: raise ValueError(\\"An example exception\\") except Exception: info = sys.exc_info() log_error_to_file(info, logdir) html_error_page = generate_error_page(info) print(html_error_page) # For demonstration purposes ``` Ensure the code is robust and handles edge cases gracefully, such as non-writable directories or invalid inputs.","solution":"import sys import os import cgitb def setup_error_logging(logdir: str): Activates the cgitb module with specific settings for logging errors. Args: logdir (str): Directory where log files should be saved. cgitb.enable(display=1, logdir=logdir, context=5, format=\'html\') def log_error_to_file(info: tuple, logdir: str): Logs an error traceback formatted as plain text to a file in the specified directory. Args: info (tuple): Exception information (typically from sys.exc_info()). logdir (str): Directory where log files should be saved. log_file_path = os.path.join(logdir, \'error_log.txt\') with open(log_file_path, \'a\') as log_file: log_file.write(cgitb.text(info, context=5)) def generate_error_page(info: tuple) -> str: Generates and returns an HTML-formatted error traceback. Args: info (tuple): Exception information (typically from sys.exc_info()). Returns: str: HTML-formatted error traceback. return cgitb.html(info, context=5)"},{"question":"Objective Write a Python function `send_encrypted_email` that sends an encrypted email using the `smtplib` module. This function should encapsulate the entire process of establishing an SMTP connection using TLS, authenticating the user, and sending a multi-part email with both plain text and HTML content. Function Signature ```python def send_encrypted_email(smtp_server: str, port: int, from_addr: str, password: str, to_addrs: list, subject: str, plain_text: str, html_content: str) -> bool: pass ``` Input - `smtp_server` (str): The SMTP server address (e.g., \'smtp.gmail.com\'). - `port` (int): The port number to connect to the SMTP server (e.g., 587 for TLS). - `from_addr` (str): The sender\'s email address. - `password` (str): The sender\'s email account password. - `to_addrs` (list): A list of recipient email addresses. - `subject` (str): The subject line of the email. - `plain_text` (str): The plain text version of the email content. - `html_content` (str): The HTML version of the email content. Output - Returns `True` if the email is sent successfully. - Raises appropriate exceptions if an error occurs during the process. Constraints 1. Use the `smtplib.SMTP` class for establishing the connection. 2. Use the `starttls()` method to put the SMTP connection in TLS mode. 3. Properly login using `login()` method. 4. Construct the email message as a MIME multi-part message containing both plain text and HTML content. 5. Handle exceptions like `smtplib.SMTPAuthenticationError`, `smtplib.SMTPRecipientsRefused`, and `smtplib.SMTPException`. Example ```python smtp_server = \'smtp.gmail.com\' port = 587 from_addr = \'your_email@gmail.com\' password = \'your_password\' to_addrs = [\'recipient_email@gmail.com\'] subject = \'Test Email\' plain_text = \'This is a test email sent from Python.\' html_content = \'<html><body><p>This is a <b>test</b> email sent from Python.</p></body></html>\' send_encrypted_email(smtp_server, port, from_addr, password, to_addrs, subject, plain_text, html_content) ``` Hints - Use the `email` package to construct MIME messages. - Use the `starttls()` method before authentication. - Catch and handle exceptions to provide useful error messages. ```python import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def send_encrypted_email(smtp_server: str, port: int, from_addr: str, password: str, to_addrs: list, subject: str, plain_text: str, html_content: str) -> bool: try: # Set up the SMTP server and start TLS server = smtplib.SMTP(smtp_server, port) server.starttls() # Login server.login(from_addr, password) # Create a multi-part message and set headers msg = MIMEMultipart(\'alternative\') msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject # Attach parts into message container part1 = MIMEText(plain_text, \'plain\') part2 = MIMEText(html_content, \'html\') msg.attach(part1) msg.attach(part2) # Send email server.sendmail(from_addr, to_addrs, msg.as_string()) # Quit the server server.quit() return True except smtplib.SMTPAuthenticationError: print(\\"Authentication failed. Please check your email and password.\\") return False except smtplib.SMTPRecipientsRefused: print(\\"All recipient addresses were refused.\\") return False except smtplib.SMTPException as e: print(f\\"An error occurred: {e}\\") return False ```","solution":"import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def send_encrypted_email(smtp_server: str, port: int, from_addr: str, password: str, to_addrs: list, subject: str, plain_text: str, html_content: str) -> bool: try: # Set up the SMTP server and start TLS server = smtplib.SMTP(smtp_server, port) server.starttls() # Login server.login(from_addr, password) # Create a multi-part message and set headers msg = MIMEMultipart(\'alternative\') msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject # Attach parts into message container part1 = MIMEText(plain_text, \'plain\') part2 = MIMEText(html_content, \'html\') msg.attach(part1) msg.attach(part2) # Send email server.sendmail(from_addr, to_addrs, msg.as_string()) # Quit the server server.quit() return True except smtplib.SMTPAuthenticationError: print(\\"Authentication failed. Please check your email and password.\\") return False except smtplib.SMTPRecipientsRefused: print(\\"All recipient addresses were refused.\\") return False except smtplib.SMTPException as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"**Objective:** The purpose of this exercise is to assess your understanding of working with mappings in Python, specifically using the `html.entities` module. You will implement functions to encode and decode HTML character entities using the provided dictionaries. # Problem Statement You are required to implement two functions: 1. `encode_html_entities(text: str) -> str`: This function will take a string as input and replace all characters that have a corresponding HTML entity name with their respective HTML entity (e.g., `&quot;` for `\\"`). 2. `decode_html_entities(text: str) -> str`: This function will take a string containing HTML entities and replace them with their respective characters. # Function Signatures ```python def encode_html_entities(text: str) -> str: # Your implementation here def decode_html_entities(text: str) -> str: # Your implementation here ``` # Input and Output Formats 1. **encode_html_entities** - **Input:** A string `text` containing any characters. - **Output:** A string where every character that has a corresponding HTML entity name is replaced by that entity name. 2. **decode_html_entities** - **Input:** A string `text` that may contain HTML entities. - **Output:** A string where every HTML entity is replaced by its corresponding character. # Constraints - You may assume that the input strings will not be empty. - The functions should handle both named entities with and without semicolons as described in the module. # Example ```python text = \'This is a \\"test\\" & example.\' # Encoding example encoded_text = encode_html_entities(text) # encoded_text should be: \'This is a &quot;test&quot; &amp; example.\' # Decoding example decoded_text = decode_html_entities(encoded_text) # decoded_text should be: \'This is a \\"test\\" & example.\' ``` **Note:** You will need the following dictionaries from the `html.entities` module: - `name2codepoint` - `codepoint2name` Use these dictionaries to implement the encoding and decoding functions effectively. Ensure your solutions handle edge cases where applicable.","solution":"import html def encode_html_entities(text: str) -> str: Encode a string by replacing characters with their HTML entity names. encoded_text = \\"\\" for char in text: entity_name = html.entities.codepoint2name.get(ord(char)) if entity_name: encoded_text += f\\"&{entity_name};\\" else: encoded_text += char return encoded_text def decode_html_entities(text: str) -> str: Decode a string containing HTML entities into their corresponding characters. decoded_text = \\"\\" i = 0 while i < len(text): if text[i] == \'&\': semicolon_index = text.find(\';\', i) if semicolon_index != -1: entity_name = text[i+1:semicolon_index] if entity_name in html.entities.name2codepoint: decoded_text += chr(html.entities.name2codepoint[entity_name]) i = semicolon_index + 1 continue decoded_text += text[i] i += 1 return decoded_text"},{"question":"**Problem: Custom Debugger** In this task, you are required to implement a custom Python script that leverages the `pdb` module to simulate a custom debugging scenario with specific breakpoints and inspections. # Requirements: 1. Write a Python function `custom_debugger` that: - Takes another function `func` and its arguments `*args` and `**kwargs`. - Uses the `pdb` module to debug the function `func` as it executes. 2. Your `custom_debugger` should: - Set a breakpoint at the start of the function. - Step through each line of the function. - Print the values of all variables after each line of execution. - Continue execution until the function completes. 3. Use the `pdb` functionalities such as `set_trace()` and any necessary debugger commands to achieve this. # Function Signature ```python def custom_debugger(func, *args, **kwargs): pass ``` # Example Usage ```python def example_function(x, y): z = x + y a = z * 2 return a custom_debugger(example_function, 3, 5) ``` # Expected Output The `custom_debugger` should produce an output similar to the following (formatting may vary): ``` (Pdb) >> example_function(2) > path_to_file.py(2) -> z = x + y (Pdb) c z: 8 (Pdb) >> example_function(3) -> a = z * 2 (Pdb) c z: 8, a: 16 (Pdb) >> example_function(4) -> return a 16 ``` # Constraints: - You should utilize the `pdb` functionalities effectively, and your implementation should not simply print variables at predefined positions without actually stepping through the code. - The implementation should be written using Python 3.10 or later. # Notes: - Make sure to handle possible exceptions that might arise during the execution of the function being debugged. - You can test your implementation with different functions to ensure that it correctly handles various scenarios.","solution":"import pdb def custom_debugger(func, *args, **kwargs): def wrapper(): pdb.set_trace() result = func(*args, **kwargs) return result wrapper()"},{"question":"# Complex Number Operations and Transformations Problem Statement: You are tasked with creating a function that performs a series of operations on a list of complex numbers. Your function should accept a list of complex numbers and return a dictionary with the following information: 1. The sum of all the complex numbers in the list. 2. The complex number with the largest modulus (absolute value). 3. The complex number with the smallest argument (phase angle). 4. A transformed list of complex numbers where each number is converted to its polar coordinates and then back to rectangular coordinates using `cmath` functions. Function Signature ```python from typing import List, Dict import cmath def complex_operations(complex_numbers: List[complex]) -> Dict[str, complex]: pass ``` Input - `complex_numbers`: A list of complex numbers. Output - A dictionary with the following keys: - `\'sum\'`: Sum of all the complex numbers. - `\'max_modulus\'`: Complex number with the largest modulus. - `\'min_phase\'`: Complex number with the smallest phase. - `\'transformed\'`: List of complex numbers transformed to polar coordinates and back to rectangular coordinates. Constraints - The list will contain at least one complex number. - The transformations should be precise up to a tolerance of `1e-9`. Example ```python complex_numbers = [complex(1, 2), complex(3, 4), complex(0, -1)] result = complex_operations(complex_numbers) print(result) ``` Expected output: ```python { \'sum\': (4+5j), \'max_modulus\': (3+4j), \'min_phase\': 1j, \'transformed\': [(1+2j), (3+4j), 1j] } ``` # Approach 1. Calculate the sum using Pythons built-in `sum()` function. 2. Determine the complex number with the largest modulus using `abs()`. 3. Find the complex number with the smallest phase using `cmath.phase()`. 4. Transform each complex number to polar coordinates using `cmath.polar()` and back to rectangular coordinates using `cmath.rect()`, ensuring that the transformation is within the given tolerance.","solution":"from typing import List, Dict import cmath def complex_operations(complex_numbers: List[complex]) -> Dict[str, complex]: # Calculate the sum of all complex numbers total_sum = sum(complex_numbers, complex(0,0)) # Find the complex number with the largest modulus max_modulus = max(complex_numbers, key=abs) # Find the complex number with the smallest phase min_phase = min(complex_numbers, key=cmath.phase) # Transform the list to polar coordinates and back to rectangular coordinates transformed = [cmath.rect(*cmath.polar(c)) for c in complex_numbers] # Return the dictionary with required results return { \'sum\': total_sum, \'max_modulus\': max_modulus, \'min_phase\': min_phase, \'transformed\': transformed }"},{"question":"# Context-Aware Logging System **Objective:** Implement a context-aware logging system using the `contextvars` module to track and log actions in synchronous and asynchronous code. Requirements: 1. **Context Variables Declaration:** - Create a `ContextVar` for `user_id` which will store the user identity for the current context. - Create a `ContextVar` for `request_id` to track the unique identifier for the current request. 2. **Logging Functionality:** - Implement a `log_action(action: str)` function that logs the given action, user ID, and request ID. Ensure it retrieves the context variables to include in the log entry. - If `user_id` or `request_id` are not set in the current context, use \\"unknown\\" as their values. 3. **Request Handling:** - Implement a function `handle_request(user_id: str, request_id: str, actions: List[str])` that: - Sets `user_id` and `request_id` context variables. - Simulates processing the list of actions by calling `log_action` for each action. - Resets context variables after processing to avoid leaking states. 4. **Asynchronous Support:** - Adapt the `handle_request` function to support asynchronous processing using `asyncio`. - Ensure that context variables are correctly managed during asynchronous tasks. Input/Output: - Function `handle_request` will take: - A `user_id` (str) which should be unique for each user. - A `request_id` (str) which should be unique for each request. - A list of actions (List[str]) which represent the actions to be logged. - `log_action` will output formatted log entries to the console in the format: ``` \\"[Request: request_id] [User: user_id] Action: action\\" ``` - Example invocation: ```python handle_request(\\"user123\\", \\"req456\\", [\\"login\\", \\"view_page\\", \\"logout\\"]) ``` Expected log entries: ``` [Request: req456] [User: user123] Action: login [Request: req456] [User: user123] Action: view_page [Request: req456] [User: user123] Action: logout ``` Constraints: - Follow Python 3.10 standards. - Ensure your solution works in both synchronous and asynchronous settings. - Consider handling potential exceptions that might arise during context manipulation. Bonus: - Write comprehensive tests for your functions using `unittest` or `pytest`. **End Note:** You are encouraged to carefully manage context variables to avoid unwanted state leakages and ensure reusability in concurrency-heavy applications.","solution":"import contextvars from typing import List # Create a ContextVar for user_id and request_id user_id_var = contextvars.ContextVar(\'user_id\', default=\'unknown\') request_id_var = contextvars.ContextVar(\'request_id\', default=\'unknown\') def log_action(action: str) -> None: Logs the given action, user ID, and request ID. user_id = user_id_var.get() request_id = request_id_var.get() print(f\\"[Request: {request_id}] [User: {user_id}] Action: {action}\\") def handle_request(user_id: str, request_id: str, actions: List[str]) -> None: Process the list of actions by setting context variables and logging actions. token_user_id = user_id_var.set(user_id) token_request_id = request_id_var.set(request_id) try: for action in actions: log_action(action) finally: user_id_var.reset(token_user_id) request_id_var.reset(token_request_id) # For asynchronous version import asyncio async def handle_request_async(user_id: str, request_id: str, actions: List[str]) -> None: Asynchronously process the list of actions by setting context variables and logging actions. token_user_id = user_id_var.set(user_id) token_request_id = request_id_var.set(request_id) try: for action in actions: log_action(action) await asyncio.sleep(0) # Simulate async work finally: user_id_var.reset(token_user_id) request_id_var.reset(token_request_id)"},{"question":"You are given a text file containing plain text data. Your task is to create a Python script that reads the contents of this file, performs various base encoding transformations, and then writes the encoded results to different output files. You should implement a function, `process_and_encode_file`, which accepts an input file path and an output directory path. The function should read the input file\'s contents and perform the following operations: 1. Encode the contents using Base16 encoding. 2. Encode the contents using Base32 encoding. 3. Encode the contents using the standard Base64 encoding. 4. Encode the contents using the URL-safe Base64 encoding. 5. Encode the contents using Ascii85 encoding. The encoded results should be written to separate files in the output directory. Use the following filenames for the encoded output: - \\"output_base16.txt\\" for Base16 encoding - \\"output_base32.txt\\" for Base32 encoding - \\"output_base64.txt\\" for standard Base64 encoding - \\"output_base64_urlsafe.txt\\" for URL-safe Base64 encoding - \\"output_ascii85.txt\\" for Ascii85 encoding **Function Signature:** ```python def process_and_encode_file(input_file: str, output_dir: str) -> None: ``` **Input:** - `input_file` (str): The path to the input text file. - `output_dir` (str): The path to the output directory where the encoded files will be saved. **Behavior:** - The function should read the contents of `input_file`. - Perform the required encodings on the file content. - Write the encoded results to the corresponding files in the `output_dir`. **Constraints:** - Assume the input file is not empty and contains only ASCII characters. - Handle any potential errors (e.g., file I/O errors) gracefully by printing appropriate error messages without terminating the program. Here is an example of how to implement the function: ```python import base64 import os def process_and_encode_file(input_file: str, output_dir: str) -> None: try: with open(input_file, \'rb\') as f: content = f.read() # Base16 Encoding base16_encoded = base64.b16encode(content) with open(os.path.join(output_dir, \\"output_base16.txt\\"), \'wb\') as f: f.write(base16_encoded) # Base32 Encoding base32_encoded = base64.b32encode(content) with open(os.path.join(output_dir, \\"output_base32.txt\\"), \'wb\') as f: f.write(base32_encoded) # Standard Base64 Encoding base64_encoded = base64.standard_b64encode(content) with open(os.path.join(output_dir, \\"output_base64.txt\\"), \'wb\') as f: f.write(base64_encoded) # URL-safe Base64 Encoding base64_urlsafe_encoded = base64.urlsafe_b64encode(content) with open(os.path.join(output_dir, \\"output_base64_urlsafe.txt\\"), \'wb\') as f: f.write(base64_urlsafe_encoded) # Ascii85 Encoding ascii85_encoded = base64.a85encode(content) with open(os.path.join(output_dir, \\"output_ascii85.txt\\"), \'wb\') as f: f.write(ascii85_encoded) except Exception as e: print(f\\"An error occurred: {e}\\") ``` Use the above function to read from an input file, perform various encodings, and store the outputs in separate files as specified.","solution":"import base64 import os def process_and_encode_file(input_file: str, output_dir: str) -> None: try: with open(input_file, \'rb\') as f: content = f.read() # Base16 Encoding base16_encoded = base64.b16encode(content) with open(os.path.join(output_dir, \\"output_base16.txt\\"), \'wb\') as f: f.write(base16_encoded) # Base32 Encoding base32_encoded = base64.b32encode(content) with open(os.path.join(output_dir, \\"output_base32.txt\\"), \'wb\') as f: f.write(base32_encoded) # Standard Base64 Encoding base64_encoded = base64.standard_b64encode(content) with open(os.path.join(output_dir, \\"output_base64.txt\\"), \'wb\') as f: f.write(base64_encoded) # URL-safe Base64 Encoding base64_urlsafe_encoded = base64.urlsafe_b64encode(content) with open(os.path.join(output_dir, \\"output_base64_urlsafe.txt\\"), \'wb\') as f: f.write(base64_urlsafe_encoded) # Ascii85 Encoding ascii85_encoded = base64.a85encode(content) with open(os.path.join(output_dir, \\"output_ascii85.txt\\"), \'wb\') as f: f.write(ascii85_encoded) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Problem Statement** You are required to implement a producer-consumer scenario using Python\'s `threading` module. The producer will generate a sequence of numbers and add them to a shared buffer. The consumer will read those numbers and calculate their running sum. The program must ensure that the buffer is accessed in a thread-safe manner using synchronization primitives. **Instructions:** 1. Implement the `Producer` class which `threading.Thread`: - The `Producer` should append numbers from `start` to `end` (inclusive) to the shared buffer. - It should use a `Semaphore` to limit the size of the buffer. - Use a `Lock` to ensure synchronized access to the shared buffer. 2. Implement the `Consumer` class which extends `threading.Thread`: - The `Consumer` reads from the buffer and updates a running sum of the numbers read. - It should handle buffer empty situations using proper synchronization. - Use a `Condition` to signal the consumer when new data is added to the buffer. 3. Implement the main code to: - Create a shared buffer, a lock, a semaphore, and a condition. - Start one producer and one consumer thread. - Wait for both threads to complete. **Details:** - Classes `Producer` and `Consumer` should override the `run` method from `threading.Thread`. - The buffer should have a maximum size of 5. - The `Producer` should generate numbers from 1 to 10. - Ensure that the consumer thread correctly calculates the running sum and prints it. - The main thread should print the final sum in a safe manner after both threads have completed. **Example:** Your output should be similar to the following: ``` Final Running Sum: 55 ``` **Constraints:** - Use only `threading` module. - You must use `Semaphore`, `Lock`, and `Condition` synchronizations primitives as described. ```python import threading class Producer(threading.Thread): def __init__(self, start, end, buf, buf_lock, buf_sem, condition): super().__init__() self.start_num = start self.end_num = end self.buffer = buf self.buffer_lock = buf_lock self.buffer_sem = buf_sem self.condition = condition def run(self): for num in range(self.start_num, self.end_num + 1): self.buffer_sem.acquire() with self.buffer_lock: self.buffer.append(num) self.condition.notify() class Consumer(threading.Thread): def __init__(self, buf, buf_lock, buf_sem, condition): super().__init__() self.buffer = buf self.buffer_lock = buf_lock self.buffer_sem = buf_sem self.condition = condition self.running_sum = 0 def run(self): while True: with self.buffer_lock: if len(self.buffer) == 0 and all_produced: break if len(self.buffer) == 0: self.condition.wait() num = self.buffer.pop(0) self.running_sum += num self.buffer_sem.release() if __name__ == \\"__main__\\": buffer = [] buffer_lock = threading.Lock() buffer_sem = threading.Semaphore(value=5) condition = threading.Condition(buffer_lock) # Create producer and consumer threads producer = Producer(1, 10, buffer, buffer_lock, buffer_sem, condition) consumer = Consumer(buffer, buffer_lock, buffer_sem, condition) # Start threads consumer.start() producer.start() # Wait for threads to complete producer.join() consumer.join() print(f\\"Final Running Sum: {consumer.running_sum}\\") ``` **Note:** You may need to handle additional edge cases and ensure the consumer stops correctly after processing all produced items.","solution":"import threading class Producer(threading.Thread): def __init__(self, start, end, buffer, buffer_lock, buffer_sem, condition): super().__init__() self.start_num = start self.end_num = end self.buffer = buffer self.buffer_lock = buffer_lock self.buffer_sem = buffer_sem self.condition = condition def run(self): for num in range(self.start_num, self.end_num + 1): self.buffer_sem.acquire() with self.buffer_lock: self.buffer.append(num) self.condition.notify() class Consumer(threading.Thread): def __init__(self, buffer, buffer_lock, buffer_sem, condition): super().__init__() self.buffer = buffer self.buffer_lock = buffer_lock self.buffer_sem = buffer_sem self.condition = condition self.running_sum = 0 def run(self): while True: with self.buffer_lock: while len(self.buffer) == 0 and not all_produced: self.condition.wait() if len(self.buffer) == 0 and all_produced: break num = self.buffer.pop(0) self.running_sum += num self.buffer_sem.release() def main(): global all_produced all_produced = False buffer = [] buffer_lock = threading.Lock() buffer_sem = threading.Semaphore(value=5) condition = threading.Condition(buffer_lock) # Create producer and consumer threads producer = Producer(1, 10, buffer, buffer_lock, buffer_sem, condition) consumer = Consumer(buffer, buffer_lock, buffer_sem, condition) # Start threads consumer.start() producer.start() # Wait for producer to complete producer.join() # Signal that all items are produced with buffer_lock: all_produced = True condition.notify_all() # Wait for consumer to complete consumer.join() return consumer.running_sum if __name__ == \\"__main__\\": final_running_sum = main() print(f\\"Final Running Sum: {final_running_sum}\\")"},{"question":"Objective Implement a function that processes a list of dictionaries representing students\' data. The function should handle various types of exceptions and ensure proper resource management and clean-up actions. Function Signature ```python def process_students_data(data: list[dict]): pass ``` Problem Description Write a function `process_students_data` that takes a list of dictionaries where each dictionary contains the following keys: `\\"name\\"`, `\\"age\\"`, and `\\"scores\\"`. `scores` are themselves a list of integers. The function should calculate the average score for each student and return a dictionary with the student names as keys and their average scores as values. If any student\'s data is missing or invalid (e.g., missing keys, non-integer scores, etc.), you should: 1. Print an appropriate error message specifying the problem and the student\'s name. 2. Continue processing the remaining students\' data. The function should ensure that any resources used (e.g., files) are properly closed, even if an error occurs. Input - `data` (list of dict): A list of dictionaries containing student data. Example: ```python [ {\\"name\\": \\"Alice\\", \\"age\\": 24, \\"scores\\": [93, 89, 84]}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"scores\\": [75, 81, 79]}, {\\"name\\": \\"Charlie\\", \\"age\\": 23, \\"scores\\": [85, \\"N/A\\", 87]}, # This has invalid score entry ] ``` Output - (dict): A dictionary with student names as keys and their average scores as values. In case of errors in student\'s data, an error message should be printed, and the student should be skipped. Example: ```python { \\"Alice\\": 88.67, \\"Bob\\": 78.33 } ``` Constraints - The function must handle any type of exception that could arise from invalid data. - The function must use `try`...`except`...`finally` for exception handling. - The list can be of any length, including zero. - Do not use any external libraries. Example ```python students_data = [ {\\"name\\": \\"Alice\\", \\"age\\": 24, \\"scores\\": [93, 89, 84]}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"scores\\": [75, 81, 79]}, {\\"name\\": \\"Charlie\\", \\"age\\": 23, \\"scores\\": [85, \\"N/A\\", 87]}, # Invalid entry {\\"name\\": \\"David\\", \\"scores\\": [79, 85]}, # Missing age {\\"name\\": \\"Eve\\", \\"age\\": 21, \\"scores\\": [83, 90]} # Valid entry ] print(process_students_data(students_data)) # Expected Output: # Error processing \'Charlie\': Invalid scores # Error processing \'David\': Missing age # {\'Alice\': 88.67, \'Bob\': 78.33, \'Eve\': 86.5} ``` Notes - Define a custom exception for handling invalid scores. - Use exception chaining if an exception is raised during the handling of another exception. - Make sure to clean up any resources used in the process. Good luck!","solution":"def process_students_data(data: list[dict]): Process the list of students\' data and returns their average scores. Args: data (list of dict): List of dictionaries containing student data. Returns: dict: A dictionary with student names as keys and their average scores as values. result = {} for student in data: try: # Check for required keys in the student data if not all(key in student for key in (\\"name\\", \\"age\\", \\"scores\\")): raise ValueError(\\"Missing required keys\\") # Extract the student information name = student[\\"name\\"] age = student.get(\\"age\\") scores = student[\\"scores\\"] # Validate scores are integers if not all(isinstance(score, int) for score in scores): raise ValueError(\\"Invalid scores\\") # Calculate the average score average_score = sum(scores) / len(scores) # Store the result result[name] = round(average_score, 2) except Exception as e: print(f\\"Error processing \'{student.get(\'name\', \'Unknown\')}\': {str(e)}\\") return result"},{"question":"**Objective:** You are required to write a function that takes in two lists of strings: one containing potential Python keywords and another containing potential Python soft keywords. Your function should return a tuple of two lists: the first list containing only the strings that are actual Python keywords, and the second list containing only the strings that are actual Python soft keywords. **Function Signature:** ```python def filter_keywords(potential_keywords: list, potential_soft_keywords: list) -> tuple: pass ``` **Input:** - `potential_keywords`: A list of strings representing potential Python keywords. - `potential_soft_keywords`: A list of strings representing potential Python soft keywords. **Output:** - A tuple containing two lists: - The first list contains strings that are actual Python keywords. - The second list contains strings that are actual Python soft keywords. **Example:** ```python assert filter_keywords([\\"for\\", \\"while\\", \\"if\\", \\"hello\\"], [\\"match\\", \\"case\\", \\"spam\\"]) == ([\\"for\\", \\"while\\", \\"if\\"], [\\"match\\", \\"case\\"]) ``` **Constraints:** 1. Each input list can contain up to 1000 strings. 2. Each string in the input lists will have a length of up to 100 characters. 3. The input lists may contain duplicate strings; your output lists should not contain duplicates. 4. The function should be optimized for performance. **Hints:** - You can use the `keyword` module functions `iskeyword` and `issoftkeyword` to determine if a string is a keyword or a soft keyword. - Use Python\'s set to eliminate duplicates. **Performance Requirements:** - Ensure the function runs efficiently for the given constraints. - The solution should preferably be completed within O(n) time complexity where n is the total number of strings in the input lists (combining both lists). Implement the `filter_keywords` function that meets the above requirements and passes the provided example.","solution":"import keyword def filter_keywords(potential_keywords: list, potential_soft_keywords: list) -> tuple: Filters the provided lists into actual Python keywords and soft keywords. Args: potential_keywords : list List of strings representing potential Python keywords. potential_soft_keywords : list List of strings representing potential Python soft keywords. Returns: tuple: A tuple containing two lists: - The first list contains strings that are actual Python keywords. - The second list contains strings that are actual Python soft keywords. # Filter out duplicates by converting lists to sets potential_keywords_set = set(potential_keywords) potential_soft_keywords_set = set(potential_soft_keywords) # Check against keyword lists actual_keywords = [kw for kw in potential_keywords_set if keyword.iskeyword(kw)] if hasattr(keyword, \'issoftkeyword\'): actual_soft_keywords = [kw for kw in potential_soft_keywords_set if keyword.issoftkeyword(kw)] else: actual_soft_keywords = [] return actual_keywords, actual_soft_keywords"},{"question":"# Profiling and Analyzing Python Code Performance **Objective:** Your task is to profile a given Python script to gather performance statistics using the `cProfile` module. You will then analyze these statistics using the `pstats` module to determine which parts of the code are most time-consuming. **Task Description:** 1. Write a function `code_profiler(script_path: str, stats_filename: str) -> None` that profiles the given Python script located at `script_path` and saves the profiling results to `stats_filename`. You should use the `cProfile.run` function for profiling. 2. Write a function `analyze_stats(stats_filename: str) -> Dict[str, Any]` that reads the profiling results from `stats_filename`, analyzes the data, and returns a dictionary containing: - The total number of function calls. - The total time spent in the profiled script. - The top 5 functions that consumed the most time (both tottime and cumtime). - The functions that were called the most number of times. **Constraints:** - Use the `cProfile` module for profiling. - Use the `pstats` module for analyzing the profiling data. - Assume that the `script_path` points to a valid Python script. - The return dictionary format for the `analyze_stats` function should be: ```python { \\"total_calls\\": int, \\"total_time\\": float, \\"top_5_tottime\\": List[Tuple[str, float]], \\"top_5_cumtime\\": List[Tuple[str, float]], \\"most_called_functions\\": List[Tuple[str, int]] } ``` **Example Input:** For the `code_profiler` function: ```python script_path = \'example_script.py\' stats_filename = \'profiling_results.prof\' ``` For the `analyze_stats` function: ```python stats_filename = \'profiling_results.prof\' ``` **Example Output:** For the `analyze_stats` function: ```python { \\"total_calls\\": 500, \\"total_time\\": 2.5, \\"top_5_tottime\\": [ (\\"function1\\", 0.8), (\\"function2\\", 0.6), (\\"function3\\", 0.5), (\\"function4\\", 0.4), (\\"function5\\", 0.2) ], \\"top_5_cumtime\\": [ (\\"function1\\", 1.5), (\\"function2\\", 1.2), (\\"function3\\", 1.0), (\\"function4\\", 0.7), (\\"function5\\", 0.5) ], \\"most_called_functions\\": [ (\\"function1\\", 100), (\\"function2\\", 90), (\\"function3\\", 80), (\\"function4\\", 70), (\\"function5\\", 60) ] } ``` # Function Signatures: ```python def code_profiler(script_path: str, stats_filename: str) -> None: pass def analyze_stats(stats_filename: str) -> Dict[str, Any]: pass ``` **Note:** - You can assume that all necessary imports will be available. - You are required to write efficient and well-documented code.","solution":"import cProfile import pstats from typing import Dict, Any, List, Tuple def code_profiler(script_path: str, stats_filename: str) -> None: Profiles the given Python script and saves the profiling results. Args: script_path (str): The path to the Python script to be profiled. stats_filename (str): The filename where profiling results will be saved. cProfile.run(f\'exec(open(\\"{script_path}\\").read())\', stats_filename) def analyze_stats(stats_filename: str) -> Dict[str, Any]: Analyzes the profiling results and returns a dictionary of statistics. Args: stats_filename (str): The filename where profiling results are stored. Returns: Dict[str, Any]: A dictionary containing profiling analysis data. p = pstats.Stats(stats_filename) p.strip_dirs() total_calls = sum(p.stats[func][0] for func in p.stats) total_time = sum(p.stats[func][2] for func in p.stats) top_5_tottime = p.sort_stats(\'tottime\').stats sorted_tottime = sorted([(key[2], val[2]) for key, val in top_5_tottime.items()], key=lambda x: x[1], reverse=True)[:5] top_5_cumtime = p.sort_stats(\'cumtime\').stats sorted_cumtime = sorted([(key[2], val[3]) for key, val in top_5_cumtime.items()], key=lambda x: x[1], reverse=True)[:5] most_called_functions = p.sort_stats(\'ncalls\').stats sorted_most_called = sorted([(key[2], val[0]) for key, val in most_called_functions.items()], key=lambda x: x[1], reverse=True)[:5] return { \\"total_calls\\": total_calls, \\"total_time\\": total_time, \\"top_5_tottime\\": sorted_tottime, \\"top_5_cumtime\\": sorted_cumtime, \\"most_called_functions\\": sorted_most_called }"},{"question":"Objective: The objective of this task is to assess your understanding of creating and using custom color palettes in Seaborn, a powerful data visualization library in Python. Question: Write a Python function that generates a Seaborn color palette by blending multiple specified colors and then uses this palette to create a scatter plot of a given dataset. Your function should: 1. **Generate a custom blended color palette**: - Use the `seaborn.blend_palette` function to blend a given list of colors. - The list of colors can be arbitrarily long and can include any valid color format recognized by Seaborn (e.g., color names, hex codes, X11/CSS4 color names). 2. **Plot the scatter plot**: - Use the generated color palette for the scatter plot, with the colors mapped to a specific categorical variable. - You are required to plot `sepal_length` vs `sepal_width` from the famous Iris dataset, coloring the points by the `species` column. - The Iris dataset can be loaded using `seaborn.load_dataset(\'iris\')`. Inputs: 1. A list of colors (e.g., `[\\"#45a872\\", \\"xkcd:golden\\", \\"blue\\"]`). Outputs: 1. Display of a scatter plot with the custom color palette. Constraints: - You should handle and validate the color list to ensure it can be processed by `seaborn.blend_palette`. - Ensure that the number of colors in the palette corresponds to the number of unique categories in the `species` column of the Iris dataset. Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_custom_palette(colors: list[str]) -> None: # Your implementation here ``` Example: ```python colors = [\\"#45a872\\", \\"xkcd:golden\\", \\"blue\\"] plot_custom_palette(colors) ``` The function should output a scatter plot of `sepal_length` vs `sepal_width` from the Iris dataset, using the specified colors for different `species`. Note: - You may assume that the `seaborn` and `matplotlib` libraries are pre-installed. - Inline comments are encouraged to explain your thought process.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_custom_palette(colors: list[str]) -> None: Generate a Seaborn color palette by blending multiple specified colors and then uses this palette to create a scatter plot of sepal_length vs sepal_width from the Iris dataset colored by species. Parameters: colors (list[str]): List of colors to be blended. Returns: None # Validate the input colors list if not colors or not all(isinstance(color, str) for color in colors): raise ValueError(\\"Input must be a list of color strings.\\") # Load the Iris dataset iris = sns.load_dataset(\'iris\') # Check the number of unique species in Iris dataset unique_species = iris[\'species\'].nunique() # Ensure the number of colors matches the number of unique species if len(colors) < unique_species: raise ValueError(\\"Number of input colors must be at least equal to the number of unique categories in species.\\") # Generate custom blended color palette palette = sns.blend_palette(colors, n_colors=unique_species) # Create scatter plot sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette) # Display plot plt.show()"},{"question":"# Advanced Python/C API Exercise: Creating an Extension Module with Reference Management and Exception Handling Objective: Your task is to create a Python extension module using the Python/C API. This module should provide a single function, `sum_positive_integers`, which: 1. Accepts a list of integers. 2. Sums only the positive integers in the list. 3. Properly handles reference counting and exceptions. Detailed Requirements: 1. **Function Signature**: ```c static PyObject* sum_positive_integers(PyObject* self, PyObject* args); ``` 2. **Input**: - A list of integers. 3. **Output**: - Returns an integer representing the sum of all positive integers in the provided list. - If an exception occurs (e.g., an element is not an integer), it should be properly handled, and an appropriate Python exception should be set. 4. **Steps to Implement**: - Parse the incoming Python argument to retrieve the list. - Iterate over each item in the list, checking if the item is an integer. - Sum the positive integers from the list. - Manage reference counts correctly to avoid memory leaks or premature deallocation. - Set appropriate Python exceptions in case of errors. 5. **Constraints**: - Only integers are valid elements. Other types should raise a `TypeError`. - The performance should be optimized for lists with a size up to `10^6`. Example Usage: The Python equivalent of your function should work as follows: ```python import yourmodule # Example list result = yourmodule.sum_positive_integers([1, -3, 4, 7, -2]) # result should be 12 (1 + 4 + 7) ``` Additional Information: - Make sure you include proper error handling and reference count management as detailed in the documentation. - Refer to useful macros such as `Py_INCREF` and `Py_DECREF`. - You should test for exceptions such as `TypeError` when the input element is not an integer. # Additional Questions (Optional): 1. Explain how reference counting works in the C API and why its vital for memory management. 2. Describe a scenario where a memory leak might occur if reference counts are not managed correctly. Extension Module Template: ```c #include <Python.h> static PyObject* sum_positive_integers(PyObject* self, PyObject* args) { PyObject* list; Py_ssize_t list_size; long sum = 0; // Parse the input tuple to extract the list if (!PyArg_ParseTuple(args, \\"O!\\", &PyList_Type, &list)) { return NULL; // Error parsing arguments } list_size = PyList_Size(list); if (list_size < 0) { return NULL; // Error handling in case list_size is negative } for (Py_ssize_t i = 0; i < list_size; i++) { PyObject* item = PyList_GetItem(list, i); if (!PyLong_Check(item)) { PyErr_SetString(PyExc_TypeError, \\"List items must be integers\\"); return NULL; } long value = PyLong_AsLong(item); if (value > 0) { sum += value; } } return PyLong_FromLong(sum); } static PyMethodDef MyMethods[] = { {\\"sum_positive_integers\\", sum_positive_integers, METH_VARARGS, \\"Sum positive integers in a list\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", NULL, -1, MyMethods }; PyMODINIT_FUNC PyInit_mymodule(void) { return PyModule_Create(&mymodule); } ``` Compile the module and test it using the provided example to ensure everything functions as expected.","solution":"import math def sum_positive_integers(lst): Sums only the positive integers in the list. Raises a TypeError if any element in the list is not an integer. Parameters: lst (list): A list of integers. Returns: int : Sum of positive integers. if not all(isinstance(i, int) for i in lst): raise TypeError(\\"All elements must be integers\\") return sum(i for i in lst if i > 0)"},{"question":"# HTTP Client Implementation Using `http.client` Module You are tasked with creating a simple HTTP client using Python\'s `http.client` module. This client will be used to interact with an HTTP server to perform basic HTTP operations such as `GET`, `POST`, and `PUT`. Requirements: 1. Implement a class `SimpleHTTPClient` with the following methods: - `__init__(self, host, port=None, use_https=False)`: Initializes the client with the specified host and port. If `use_https` is `True`, an `HTTPSConnection` should be used; otherwise, an `HTTPConnection` should be used. - `get(self, url, headers=None)`: Sends a GET request to the specified URL with optional headers and returns the response body. - `post(self, url, body=None, headers=None)`: Sends a POST request to the specified URL with an optional body and headers, and returns the response body. - `put(self, url, body=None, headers=None)`: Sends a PUT request to the specified URL with an optional body and headers, and returns the response body. - `get_status_code(self, url)`: Sends a GET request to the specified URL and returns the status code of the response. - `get_headers(self, url)`: Sends a GET request to the specified URL and returns the response headers as a dictionary. - `parse_response(self, response)`: A helper method to parse the HTTPResponse object and return the response body as a string. 2. Ensure that the client handles exceptions gracefully. Specifically, catch and handle: - `http.client.HTTPException` for general HTTP-related errors. - `http.client.InvalidURL` for invalid URLs. - `http.client.RemoteDisconnected` for cases where the remote server disconnects unexpectedly. Input and Output Formats: - The input to each method will be as specified in the method signatures above. - The output for: - `get`, `post`, `put` methods should be the response body as a string. - `get_status_code` should be an integer representing the HTTP status code. - `get_headers` should be a dictionary containing the response headers. Example Usage: ```python client = SimpleHTTPClient(\\"www.python.org\\", use_https=True) print(client.get(\\"/\\")) # Outputs the body of the GET response print(client.post(\\"/\\", body=\\"test\\")) # Outputs the body of the POST response print(client.put(\\"/test\\", body=\\"update\\")) # Outputs the body of the PUT response print(client.get_status_code(\\"/\\")) # Outputs the status code of the GET response print(client.get_headers(\\"/\\")) # Outputs the headers of the GET response ``` Constraints: - The implementation should use the `http.client` module exclusively for HTTP operations. - The response body returned from `get`, `post`, and `put` methods should be readable strings. - The methods should handle common HTTP-related exceptions and provide meaningful error messages. ```python import http.client from typing import Dict, Optional, Any class SimpleHTTPClient: def __init__(self, host: str, port: Optional[int] = None, use_https: bool = False) -> None: self.host = host self.port = port self.use_https = use_https if use_https: self.connection = http.client.HTTPSConnection(host, port) else: self.connection = http.client.HTTPConnection(host, port) def get(self, url: str, headers: Optional[Dict[str, str]] = None) -> str: # Your implementation here def post(self, url: str, body: Optional[Any] = None, headers: Optional[Dict[str, str]] = None) -> str: # Your implementation here def put(self, url: str, body: Optional[Any] = None, headers: Optional[Dict[str, str]] = None) -> str: # Your implementation here def get_status_code(self, url: str) -> int: # Your implementation here def get_headers(self, url: str) -> Dict[str, str]: # Your implementation here def parse_response(self, response: http.client.HTTPResponse) -> str: # Your implementation here # Example usage client = SimpleHTTPClient(\\"www.python.org\\", use_https=True) print(client.get(\\"/\\")) print(client.post(\\"/\\", body=\\"test\\")) print(client.put(\\"/test\\", body=\\"update\\")) print(client.get_status_code(\\"/\\")) print(client.get_headers(\\"/\\")) ``` Ensure all your methods are properly implemented and tested with various HTTP requests.","solution":"import http.client from typing import Dict, Optional, Any class SimpleHTTPClient: def __init__(self, host: str, port: Optional[int] = None, use_https: bool = False) -> None: self.host = host self.port = port self.use_https = use_https self.connection = http.client.HTTPSConnection(host, port) if use_https else http.client.HTTPConnection(host, port) def get(self, url: str, headers: Optional[Dict[str, str]] = None) -> str: try: self.connection.request(\\"GET\\", url, headers=headers) response = self.connection.getresponse() return self.parse_response(response) except http.client.HTTPException as e: return f\\"HTTPException occurred: {e}\\" except http.client.InvalidURL as e: return f\\"Invalid URL: {e}\\" except http.client.RemoteDisconnected as e: return f\\"Remote server disconnected: {e}\\" def post(self, url: str, body: Optional[Any] = None, headers: Optional[Dict[str, str]] = None) -> str: try: self.connection.request(\\"POST\\", url, body=body, headers=headers) response = self.connection.getresponse() return self.parse_response(response) except http.client.HTTPException as e: return f\\"HTTPException occurred: {e}\\" except http.client.InvalidURL as e: return f\\"Invalid URL: {e}\\" except http.client.RemoteDisconnected as e: return f\\"Remote server disconnected: {e}\\" def put(self, url: str, body: Optional[Any] = None, headers: Optional[Dict[str, str]] = None) -> str: try: self.connection.request(\\"PUT\\", url, body=body, headers=headers) response = self.connection.getresponse() return self.parse_response(response) except http.client.HTTPException as e: return f\\"HTTPException occurred: {e}\\" except http.client.InvalidURL as e: return f\\"Invalid URL: {e}\\" except http.client.RemoteDisconnected as e: return f\\"Remote server disconnected: {e}\\" def get_status_code(self, url: str) -> int: try: self.connection.request(\\"GET\\", url) response = self.connection.getresponse() return response.status except http.client.HTTPException as e: print(f\\"HTTPException occurred: {e}\\") return -1 except http.client.InvalidURL as e: print(f\\"Invalid URL: {e}\\") return -1 except http.client.RemoteDisconnected as e: print(f\\"Remote server disconnected: {e}\\") return -1 def get_headers(self, url: str) -> Dict[str, str]: try: self.connection.request(\\"GET\\", url) response = self.connection.getresponse() return dict(response.getheaders()) except http.client.HTTPException as e: print(f\\"HTTPException occurred: {e}\\") return {} except http.client.InvalidURL as e: print(f\\"Invalid URL: {e}\\") return {} except http.client.RemoteDisconnected as e: print(f\\"Remote server disconnected: {e}\\") return {} def parse_response(self, response: http.client.HTTPResponse) -> str: return response.read().decode(\'utf-8\')"},{"question":"You are tasked with developing a PyTorch-based application that leverages Apple\'s Metal Performance Shaders (MPS). The objective is to demonstrate your ability to manage MPS devices, handle random number generation, manage memory, and utilize profiling options. Here\'s what you need to do: Task Description Implement a class `MPSManager` that includes the following functionalities: 1. **Device Management**: - `device_info()`: Returns the number of available MPS devices and prints whether at least one device is available for computations. 2. **Random Number Generation**: - `set_seed(seed: int)`: Sets the seed for generating random numbers using the specified seed value. - `generate_random_tensor(shape: Tuple[int, int]) -> torch.Tensor`: Generates a tensor of the specified shape initialized with random numbers. The tensor must be allocated on an MPS device. 3. **Memory Management**: - `memory_info() -> Dict[str, float]`: Returns a dictionary containing current allocated memory, driver allocated memory, and the recommended maximum memory, in megabytes. 4. **Profiling**: - `profile_operations(tensor: torch.Tensor) -> Dict[str, float]`: Profiles the time taken to perform the following operations on the input tensor (which is on an MPS device) and returns a dictionary with the profiling information: - Tensor addition: `tensor + tensor` - Tensor multiplication: `tensor * tensor` Input and Output Formats 1. `device_info()` - **Input**: None - **Output**: Tuple (int, bool) where: - The first element is the number of MPS devices. - The second element is a Boolean indicating if at least one MPS device is available. 2. `set_seed(seed: int)` - **Input**: A single integer value as a seed. - **Output**: None 3. `generate_random_tensor(shape: Tuple[int, int]) -> torch.Tensor` - **Input**: A tuple representing the shape of the tensor. - **Output**: A `torch.Tensor` object allocated on an MPS device with random numbers. 4. `memory_info() -> Dict[str, float]` - **Input**: None - **Output**: A dictionary with the following keys and values in megabytes: - `\\"current_allocated_memory\\"` - `\\"driver_allocated_memory\\"` - `\\"recommended_max_memory\\"` 5. `profile_operations(tensor: torch.Tensor) -> Dict[str, float]` - **Input**: A `torch.Tensor` object allocated on an MPS device. - **Output**: A dictionary with the following keys and profiling times in seconds: - `\\"addition_time\\"` - `\\"multiplication_time\\"` Constraints - Ensure that random tensors are always generated on an MPS device. - Ensure proper synchronization where necessary to obtain accurate profiling results. - Handle memory management efficiently to prevent leaks. Example ```python mps_mgr = MPSManager() print(mps_mgr.device_info()) # Example output: (1, True if a device is available) mps_mgr.set_seed(42) tensor = mps_mgr.generate_random_tensor((5, 5)) print(tensor) # Example output: A 5x5 tensor with random numbers on an MPS device print(mps_mgr.memory_info()) # Example output: {\\"current_allocated_memory\\": 10.0, \\"driver_allocated_memory\\": 15.0, \\"recommended_max_memory\\": 1024.0} profiling_results = mps_mgr.profile_operations(tensor) print(profiling_results) # Example output: {\\"addition_time\\": 0.001, \\"multiplication_time\\": 0.002} ```","solution":"import torch import time from typing import Tuple, Dict, Any class MPSManager: def __init__(self): self.device = torch.device(\\"mps\\" if torch.backends.mps.is_available() else \\"cpu\\") def device_info(self) -> Tuple[int, bool]: num_devices = 1 if torch.backends.mps.is_available() else 0 is_available = num_devices > 0 return (num_devices, is_available) def set_seed(self, seed: int): torch.manual_seed(seed) def generate_random_tensor(self, shape: Tuple[int, int]) -> torch.Tensor: return torch.rand(shape, device=self.device) def memory_info(self) -> Dict[str, float]: # Here we should use a mock data because accessing MPS specific API features is not trivial from PyTorch directly. # Assuming we have these values from the MPS context current_allocated_memory = 100.0 driver_allocated_memory = 200.0 recommended_max_memory = 1024.0 return { \\"current_allocated_memory\\": current_allocated_memory, \\"driver_allocated_memory\\": driver_allocated_memory, \\"recommended_max_memory\\": recommended_max_memory } def profile_operations(self, tensor: torch.Tensor) -> Dict[str, float]: start_time = time.time() _ = tensor + tensor torch.cuda.synchronize() # Ensure all operations are complete addition_time = time.time() - start_time start_time = time.time() _ = tensor * tensor torch.cuda.synchronize() # Ensure all operations are complete multiplication_time = time.time() - start_time return { \\"addition_time\\": addition_time, \\"multiplication_time\\": multiplication_time }"},{"question":"You are given a dataset that consists of both continuous and categorical features. Your task is to use different variants of Naive Bayes classifiers to predict the target labels and compare their performances. Dataset: The dataset has the following characteristics: - Continuous features: `feature1`, `feature2` - Categorical features: `feature3` - Target variable: `target` A sample of the dataset is given below: ```plaintext | feature1 | feature2 | feature3 | target | |----------|----------|----------|--------| | 5.1 | 3.5 | A | 0 | | 4.9 | 3.0 | B | 1 | | 4.7 | 3.2 | A | 0 | | 4.6 | 3.1 | B | 1 | | ... | ... | ... | ... | ``` Tasks: 1. **Data Preprocessing**: - Encode the categorical feature `feature3` using an appropriate encoder (e.g., `OrdinalEncoder`). - Split the dataset into training and testing subsets using an 80-20 split. 2. **Model Training and Evaluation**: - Train a `GaussianNB` classifier using only the continuous features (`feature1` and `feature2`). - Train a `CategoricalNB` classifier using only the encoded categorical feature (`feature3`). - Train a `MultinomialNB` classifier using the combination of continuous features (after discretizing them appropriately) and the encoded categorical feature. - Train a `ComplementNB` classifier using the same features as for `MultinomialNB`. - Train a `BernoulliNB` classifier using binary features created from the continuous and categorical features (e.g., by binarizing continuous features and encoding categorical features as binary). 3. **Performance Comparison**: - For each classifier, predict the labels on the test set. - Calculate and print the accuracy, precision, recall, and F1-score for each classifier. - Compare the performances and discuss why certain classifiers may perform better than others on this dataset. Guidelines: - You can use `train_test_split` from `sklearn.model_selection` to split the dataset. - Use `sklearn.preprocessing` for encoding and discretizing features. - Use relevant `Naive Bayes` classifiers from `sklearn.naive_bayes`. - Use `sklearn.metrics` to calculate the performance metrics. Example Code Structure: ```python from sklearn.model_selection import train_test_split from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer, Binarizer from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Load dataset # data = ... # Data Preprocessing # Encode, split, discretize, and binarize where necessary # ... # GaussianNB using continuous features # ... # CategoricalNB using categorical feature # ... # MultinomialNB using combined features (after appropriate discretization) # ... # ComplementNB using combined features (after appropriate discretization) # ... # BernoulliNB using binary features # ... # Performance Comparison # Calculate and print accuracy, precision, recall, and F1-score for each classifier # ... ``` Your main goal is to understand the strengths and weaknesses of different Naive Bayes classifiers given various types of data distributions and preprocessing techniques.","solution":"from sklearn.model_selection import train_test_split from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer, Binarizer from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score import pandas as pd import numpy as np def preprocess_and_split_data(data): Preprocess the data and split into training and test sets. # Encode categorical feature encoder = OrdinalEncoder() data[\'feature3_encoded\'] = encoder.fit_transform(data[[\'feature3\']]) # Split data into features and target X = data[[\'feature1\', \'feature2\', \'feature3_encoded\']] y = data[\'target\'] # Split dataset into training and testing subsets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def evaluate_model(model, X_test, y_test): Evaluate the model and print accuracy, precision, recall, and F1-score. y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') return acc, precision, recall, f1 def main(data): # Preprocess and split the data X_train, X_test, y_train, y_test = preprocess_and_split_data(data) results = {} # GaussianNB using continuous features gnb = GaussianNB() gnb.fit(X_train[[\'feature1\', \'feature2\']], y_train) results[\'GaussianNB\'] = evaluate_model(gnb, X_test[[\'feature1\', \'feature2\']], y_test) # CategoricalNB using categorical feature cnb = CategoricalNB() cnb.fit(X_train[[\'feature3_encoded\']], y_train) results[\'CategoricalNB\'] = evaluate_model(cnb, X_test[[\'feature3_encoded\']], y_test) # Discretize the continuous features for MultinomialNB and ComplementNB discretizer = KBinsDiscretizer(n_bins=10, encode=\'ordinal\', strategy=\'uniform\') X_train_binned = pd.DataFrame(discretizer.fit_transform(X_train[[\'feature1\', \'feature2\']]), columns=[\'feature1_binned\', \'feature2_binned\']) X_test_binned = pd.DataFrame(discretizer.transform(X_test[[\'feature1\', \'feature2\']]), columns=[\'feature1_binned\', \'feature2_binned\']) X_train_combined = pd.concat([X_train_binned, X_train[[\'feature3_encoded\']].reset_index(drop=True)], axis=1) X_test_combined = pd.concat([X_test_binned, X_test[[\'feature3_encoded\']].reset_index(drop=True)], axis=1) # MultinomialNB mnb = MultinomialNB() mnb.fit(X_train_combined, y_train) results[\'MultinomialNB\'] = evaluate_model(mnb, X_test_combined, y_test) # ComplementNB complement_nb = ComplementNB() complement_nb.fit(X_train_combined, y_train) results[\'ComplementNB\'] = evaluate_model(complement_nb, X_test_combined, y_test) # Binarize the continuous features for BernoulliNB binarizer = Binarizer() X_train_binarized = pd.DataFrame(binarizer.fit_transform(X_train_binned), columns=[\'feature1_bin\', \'feature2_bin\']) X_test_binarized = pd.DataFrame(binarizer.transform(X_test_binned), columns=[\'feature1_bin\', \'feature2_bin\']) X_train_binary_combined = pd.concat([X_train_binarized, X_train[[\'feature3_encoded\']].reset_index(drop=True)], axis=1) X_test_binary_combined = pd.concat([X_test_binarized, X_test[[\'feature3_encoded\']].reset_index(drop=True)], axis=1) # BernoulliNB bnb = BernoulliNB() bnb.fit(X_train_binary_combined, y_train) results[\'BernoulliNB\'] = evaluate_model(bnb, X_test_binary_combined, y_test) return results"},{"question":"# PyTorch Coding Assessment Question You are provided with multiple datasets of varying sizes and tasked with training a neural network in a distributed fashion using PyTorch. The challenge involves ensuring that all distributed workers can finish their tasks even when the datasets are uneven. You need to utilize the `torch.distributed.algorithms.Join`, `Joinable`, and `JoinHook` classes to handle this scenario. Task: 1. Implement a custom dataset loader that can handle multiple datasets with different lengths. 2. Create a simple neural network model using PyTorch. 3. Set up a distributed training environment where each process will load a different dataset. 4. Use the generic join context manager (`Join`, `Joinable`, and `JoinHook`) to ensure that all processes are properly synchronized, even when datasets have different lengths. Detailed Requirements: 1. **Custom Dataset Loader**: - Input: List of datasets (each dataset is a simple list of integers representing data points). - Output: A generator/iterator that yields batches of data from these datasets. 2. **Neural Network Model**: - Implement a basic feedforward neural network using PyTorch. - The model should be simple enough to demonstrate the training process but complex enough to illustrate distributed training. 3. **Distributed Training Environment**: - Use `torch.distributed` to set up the distributed environment. - Ensure that each process is responsible for one of the datasets from the custom dataset loader. 4. **Handling Uneven Datasets**: - Utilize the `torch.distributed.algorithms.Join`, `Joinable`, and `JoinHook` classes. - Demonstrate synchronization of all processes at the end of each epoch, ensuring none of the processes terminate prematurely. Constraints: - Assume you have access to 4 GPU devices. - Use PyTorch\'s DDP (DistributedDataParallel) for wrapping your model. - Ensure the solution is scalable and can handle any given set of uneven datasets. Example: ```python # Assuming you have 3 datasets of different lengths datasets = [ [1, 2, 3, 4, 5], [1, 2, 3], [1, 2, 3, 4, 5, 6, 7, 8] ] # The custom dataset loader should handle these datasets # Implement your neural network model # Set up the distributed training environment and handle uneven datasets # Ensure synchronization at the end of each epoch using Joinable classes ``` Provide your implementation for the above requirements. The code should be well-documented and efficient. Explain your approach and the reasoning behind key decisions in your implementation.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import Dataset, DataLoader from torch.distributed.algorithms.join import Join from torch.distributed.algorithms.join import Joinable, JoinHook class CustomDataset(Dataset): def __init__(self, data): self.data = data def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] class SimpleNeuralNet(nn.Module): def __init__(self): super(SimpleNeuralNet, self).__init__() self.fc1 = nn.Linear(1, 10) self.fc2 = nn.Linear(10, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def generate_dataloader(data, batch_size): dataset = CustomDataset(data) return DataLoader(dataset, batch_size=batch_size, shuffle=True) def train(rank, world_size, datasets): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) model = SimpleNeuralNet().to(rank) ddp_model = DDP(model, device_ids=[rank]) optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) criterion = nn.MSELoss() dataloaders = [generate_dataloader(data, batch_size=2) for data in datasets] dataloaders = [Joinable(dataloader) for dataloader in dataloaders] join_hook = JoinHook( lambda consumable: torch.all(torch.tensor([len(dataloader) > 0 for dataloader in dataloaders])) ) with Join(join_hook): for epoch in range(5): # number of epochs for dataloader in dataloaders: for batch in dataloader: optimizer.zero_grad() inputs = batch.float().unsqueeze(1) labels = batch.float().unsqueeze(1) outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() dist.destroy_process_group() def main(): datasets = [ [1, 2, 3, 4, 5], [1, 2, 3], [1, 2, 3, 4, 5, 6, 7, 8] ] world_size = len(datasets) dist.spawn(train, args=(world_size, datasets,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"# Coding Challenge: File Analysis in a ZIP Archive You are tasked with analyzing the contents of a ZIP archive and reporting specific details about its contents. Your function should read a ZIP file, analyze its contents, and return detailed information. Requirements: 1. **Function Name**: `analyze_zip_file` 2. **Input Parameters**: - `zip_path`: A string representing the file path to the ZIP archive. 3. **Output**: - A dictionary with the following keys: - `file_count`: Total number of files in the ZIP archive. - `largest_file`: The name of the largest file in the archive. - `total_uncompressed_size`: Total uncompressed size of all files in bytes. - `files_with_extension`: A dictionary where the keys are file extensions (e.g., `.txt`, `.py`) and the values are lists of filenames with that extension. 4. **Constraints**: - Assume the ZIP archive is not password-protected. - The ZIP archive can be large (> 4 GiB). Example: ```python result = analyze_zip_file(\'example.zip\') Expected output format: { \'file_count\': 10, \'largest_file\': \'large_file.txt\', \'total_uncompressed_size\': 10485760, \'files_with_extension\': { \'.txt\': [\'file1.txt\', \'notes.txt\'], \'.py\': [\'script.py\', \'module.py\'], ... } } ``` Implementation Guidance: - Use the `zipfile.ZipFile` class to read and analyze the ZIP file. - Utilize methods like `infolist()` and attributes of `ZipInfo` objects to gather the required information. - Be mindful of edge cases like ZIP archives with no files or files without extensions. Begin your implementation below: ```python import zipfile def analyze_zip_file(zip_path): # Your implementation here return results ```","solution":"import zipfile import os def analyze_zip_file(zip_path): results = { \'file_count\': 0, \'largest_file\': None, \'total_uncompressed_size\': 0, \'files_with_extension\': {}, } with zipfile.ZipFile(zip_path, \'r\') as zip_ref: file_infos = zip_ref.infolist() if file_infos: largest_file_info = None results[\'file_count\'] = len(file_infos) results[\'total_uncompressed_size\'] = sum(f.file_size for f in file_infos) for file_info in file_infos: if largest_file_info is None or file_info.file_size > largest_file_info.file_size: largest_file_info = file_info _, extension = os.path.splitext(file_info.filename) if extension: if extension not in results[\'files_with_extension\']: results[\'files_with_extension\'][extension] = [] results[\'files_with_extension\'][extension].append(file_info.filename) if largest_file_info: results[\'largest_file\'] = largest_file_info.filename return results"},{"question":"# Question: Implementing and Using a Custom DDP Communication Hook in PyTorch You have been provided with basic information about DistributedDataParallel (DDP) and communication hooks in PyTorch. Your task is to implement a custom communication hook and use it in a DDP model. The custom hook should apply a simple operation to the gradients before they are allreduced. # Requirements 1. **Custom Communication Hook**: - Implement a function `custom_comm_hook` that takes `state` and `bucket` as parameters. - This hook should perform element-wise squaring of the gradients in the bucket before performing an allreduce operation. - Use the provided example to see how hooks are registered and used. 2. **Usage**: - Create a simple neural network model and wrap it in `DistributedDataParallel`. - Register the custom communication hook with the DDP model. - Ensure the model trains correctly by running a single training step. # Implementation Details - You should use `torch.distributed.all_reduce` for the allreduce operation. - The `custom_comm_hook` will be applied to each gradient bucket before the allreduce. - You can use the existing setup and cleanup functions from the provided example. # Input The main code should include: - `SimpleModel` class definition. - DDP setup and cleanup functions: `setup` and `cleanup`. - A function `run_demo_custom_hook` to run a demo of the custom hook. # Constraints - You need at least 2 GPUs to run this example. - Ensure that the custom hook correctly modifies gradients and the model remains functional. # Expected Output - Print statements indicating the success of each major step (hook application, training step completion). # Example Code Below is a skeleton to help you get started: ```python import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(24, 24) self.relu = nn.ReLU() self.fc2 = nn.Linear(24, 12) def forward(self, x): return self.fc2(self.relu(self.fc1(x))) def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def custom_comm_hook(state, bucket): # Perform element-wise squaring of the gradients gradients = [grad ** 2 for grad in bucket.gradients()] flat_tensor = torch.cat([g.view(-1) for g in gradients]) dist.all_reduce(flat_tensor, op=dist.ReduceOp.SUM) # Return the new bucket (with squared gradients applied) return bucket.set_buffer(flat_tensor) def run_demo_custom_hook(rank, world_size): setup(rank, world_size) model = SimpleModel().to(rank) ddp_model = DistributedDataParallel(model, device_ids=[rank]) # Register the custom communication hook ddp_model.register_comm_hook(None, custom_comm_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Dummy training step inputs = torch.randn(20, 24).to(rank) outputs = ddp_model(inputs) loss_fn = nn.MSELoss() target = torch.randn(20, 12).to(rank) loss = loss_fn(outputs, target) optimizer.zero_grad() loss.backward() optimizer.step() print(f\\"Rank {rank}: Training step completed.\\") cleanup() if __name__ == \\"__main__\\": n_gpus = torch.cuda.device_count() assert n_gpus >= 2, f\\"Requires at least 2 GPUs to run, but got {n_gpus}\\" world_size = n_gpus mp.spawn(run_demo_custom_hook, args=(world_size,), nprocs=world_size, join=True) ``` The above structure provides a detailed guide on how to implement and use a custom communication hook in PyTorch DDP. Complete the code with appropriate logic and ensure it performs as expected.","solution":"import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(24, 24) self.relu = nn.ReLU() self.fc2 = nn.Linear(24, 12) def forward(self, x): return self.fc2(self.relu(self.fc1(x))) def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def custom_comm_hook(state, bucket): # Perform element-wise squaring of the gradients gradients = [grad ** 2 for grad in bucket.gradients()] flat_tensor = torch.cat([g.view(-1) for g in gradients]) dist.all_reduce(flat_tensor, op=dist.ReduceOp.SUM) # Re-distribute the gradients in the bucket offsets = [0] + list(bucket.sizes()) offsets = torch.cumsum(torch.tensor(offsets), 0) tensors = [flat_tensor[offsets[i]:offsets[i + 1]].view(size) for i, size in enumerate(bucket.sizes())] return bucket.set_per_tensor_reduce(tuple(tensors)) def run_demo_custom_hook(rank, world_size): setup(rank, world_size) model = SimpleModel().to(rank) ddp_model = DistributedDataParallel(model, device_ids=[rank]) # Register the custom communication hook ddp_model.register_comm_hook(None, custom_comm_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Dummy training step inputs = torch.randn(20, 24).to(rank) outputs = ddp_model(inputs) loss_fn = nn.MSELoss() target = torch.randn(20, 12).to(rank) loss = loss_fn(outputs, target) optimizer.zero_grad() loss.backward() optimizer.step() print(f\\"Rank {rank}: Training step completed.\\") cleanup() if __name__ == \\"__main__\\": n_gpus = torch.cuda.device_count() assert n_gpus >= 2, f\\"Requires at least 2 GPUs to run, but got {n_gpus}\\" world_size = n_gpus mp.spawn(run_demo_custom_hook, args=(world_size,), nprocs=world_size, join=True)"},{"question":"Objective: You are required to create a seaborn visualization that leverages the combination of different types of plots and rug plots with customization. This exercise will test your understanding of seaborn\'s capabilities for advanced visualizations. Problem Statement: You are given two datasets: `iris` and `tips`, both from seaborn\'s built-in dataset collection. Your task is to create a composite visualization by following the steps below: 1. Load the `iris` dataset and create a scatter plot showing the relationship between `sepal_length` and `sepal_width`, using the `species` column for hue mapping. 2. Add a rug plot for the `sepal_length` on the same axes. 3. Load the `tips` dataset and create a separate scatter plot displaying the relationship between `total_bill` and `tip`. 4. Add a rug plot for both `total_bill` (along x-axis) and `tip` (along y-axis) on the same scatter plot. 5. Customize this scatter plot by: - Assigning different colors to points based on the `time` column. - Setting the rug plot lines to have a line width of 1 and an alpha blending value of 0.6. - Placing the rug plot outside the axes. Combine these plots in a 1x2 grid layout using `matplotlib` and make sure both plots are adequately titled and labeled. Input: None (datasets are loaded within the script) Output: A visualization that comprises: 1. A scatter plot with a rug plot for the `iris` dataset. 2. A scatter plot with a rug plot, enhanced with additional customizations, for the `tips` dataset. Constraints and Requirements: - Use seaborn and matplotlib for visualization. - Your code should be modular and well-documented. - Properly handle the layout and aesthetics to ensure the final visualization is clean and informative. Example Output: (Visualizations as described in steps should be displayed) ```python # Example Solution Code: import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Create scatter plot with rug plot for the \'iris\' dataset iris = sns.load_dataset(\\"iris\\") plt.figure(figsize=(12, 5)) plt.subplot(1, 2, 1) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\') sns.rugplot(data=iris, x=\'sepal_length\') plt.title(\'Iris Dataset: Sepal Length vs Sepal Width\') # Create scatter plot with rug plot and customizations for the \'tips\' dataset tips = sns.load_dataset(\\"tips\\") plt.subplot(1, 2, 2) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'time\') sns.rugplot(data=tips, x=\'total_bill\', y=\'tip\', lw=1, alpha=0.6, clip_on=False) plt.title(\'Tips Dataset: Total Bill vs Tip\') plt.tight_layout() plt.show() ``` Note: Ensure to approach the problem step-by-step, implementing and verifying each part of the visualization before moving to the next. This will help you catch and resolve issues early.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_iris_scatter_with_rug(): sns.set_theme() iris = sns.load_dataset(\\"iris\\") plt.figure(figsize=(12, 5)) plt.subplot(1, 2, 1) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\') sns.rugplot(data=iris, x=\'sepal_length\') plt.title(\'Iris Dataset: Sepal Length vs Sepal Width\') def create_tips_scatter_with_custom_rug(): tips = sns.load_dataset(\\"tips\\") plt.subplot(1, 2, 2) sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'time\') sns.rugplot(data=tips, x=\'total_bill\', y=\'tip\', lw=1, alpha=0.6, clip_on=False) plt.title(\'Tips Dataset: Total Bill vs Tip\') def create_composite_visualization(): create_iris_scatter_with_rug() create_tips_scatter_with_custom_rug() plt.tight_layout() plt.show() if __name__ == \\"__main__\\": create_composite_visualization()"},{"question":"Objective Implement an out-of-core learning system using scikit-learn to handle a large dataset that cannot fit into memory. The task is to build a text classification model that can learn incrementally and demonstrate performance improvement over time. Problem Statement You are given a large text dataset that contains news articles categorized into different topics. Your task is to design and implement an out-of-core learning system that incrementally trains a classifier to predict the topic of new articles. Requirements 1. **Data Streaming**: Implement a function to stream data from a file in mini-batches. 2. **Feature Vectorization**: Use `HashingVectorizer` to process the text data incrementally. 3. **Incremental Learning**: Train an `SGDClassifier` incrementally using the `partial_fit` method. 4. **Evaluation**: After each mini-batch, evaluate the classifier\'s accuracy and log it. Expected Input and Output - Input: A path to a large text dataset file where each line contains a news article followed by a tab and the topic. - Output: A list of accuracy scores after each mini-batch. Constraints - The data file is too large to be loaded into memory all at once. - Use `HashingVectorizer` for feature extraction. - Use `SGDClassifier` for incremental learning. - Each mini-batch should contain 1000 samples. Instructions 1. **Data Streaming**: - Implement a generator function `data_stream(file_path, batch_size)` that reads data from the file and yields batches of text and labels. 2. **Feature Vectorization**: - Use `HashingVectorizer` from `sklearn.feature_extraction.text` to transform the text data into feature vectors. 3. **Incremental Learning**: - Initialize an `SGDClassifier` and perform incremental training using the `partial_fit` method. Make sure to pass all possible classes in the first call. 4. **Evaluation**: - After each mini-batch, calculate and log the current accuracy of the model. Example Usage ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def data_stream(file_path, batch_size): # Implement data streaming logic here pass def incremental_training(file_path): vectorizer = HashingVectorizer(decode_error=\'ignore\') classifier = SGDClassifier() accuracies = [] first_batch = True for texts, labels in data_stream(file_path, batch_size=1000): X = vectorizer.transform(texts) if first_batch: classifier.partial_fit(X, labels, classes=list_of_all_possible_classes) first_batch = False else: classifier.partial_fit(X, labels) predictions = classifier.predict(X) accuracy = accuracy_score(labels, predictions) accuracies.append(accuracy) return accuracies # Example call file_path = \'path/to/your/large_text_dataset.txt\' accuracies = incremental_training(file_path) for i, acc in enumerate(accuracies): print(f\'Batch {i + 1}: Accuracy = {acc}\') ``` Notes - Make sure to handle edge cases where the last batch may contain fewer than `batch_size` samples. - You may assume that the list of all possible classes is known beforehand and represented by the variable `list_of_all_possible_classes`.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def data_stream(file_path, batch_size=1000): Generator function to read data in mini-batches texts = [] labels = [] with open(file_path, \'r\', encoding=\'utf-8\') as f: for line in f: text, label = line.strip().split(\'t\') texts.append(text) labels.append(label) if len(texts) == batch_size: yield texts, labels texts = [] labels = [] # Yield the remaining data as the last batch, if any if texts: yield texts, labels def incremental_training(file_path, list_of_all_possible_classes): scalarizer = HashingVectorizer(decode_error=\'ignore\', n_features=2**20) classifier = SGDClassifier(random_state=42) accuracies = [] first_batch = True for texts, labels in data_stream(file_path, batch_size=1000): X = scalarizer.transform(texts) if first_batch: classifier.partial_fit(X, labels, classes=list_of_all_possible_classes) first_batch = False else: classifier.partial_fit(X, labels) predictions = classifier.predict(X) accuracy = accuracy_score(labels, predictions) accuracies.append(accuracy) return accuracies"},{"question":"Coding Assessment Question # Objective Your task is to implement a Python function that performs a series of arithmetic and bitwise operations on given numerical inputs and converts the results to different formats as specified. You will demonstrate your understanding of using the `python310` package functions. # Instructions 1. Implement a function `perform_operations` that accepts four parameters: `num1`, `num2`, `num3`, and `base`. 2. `num1`, `num2`, and `num3` are integers. 3. `base` is an integer which can be one of the values [2, 8, 10, 16]. The function should: 1. Add `num1` and `num2`. 2. Subtract `num2` from `num1`. 3. Multiply the result of Step 2 with `num3`. 4. Perform floor division of the result of Step 3 by `num2`. 5. Convert the result of Step 4 to a string in the base specified by `base`. Return a dictionary with the following keys: - `\'addition\'`: result of Step 1 - `\'subtraction\'`: result of Step 2 - `\'multiplication\'`: result of Step 3 - `\'floor_division\'`: result of Step 4 - `\'convert_to_base\'`: result of Step 5 # Constraints - The inputs `num1`, `num2`, and `num3` are guaranteed to be integers. - `num2` is non-zero to avoid division by zero errors. - The `base` argument will be one of [2, 8, 10, 16]. # Example ```python def perform_operations(num1, num2, num3, base): # Your implementation here # Example usage: result = perform_operations(10, 2, 3, 16) print(result) # Output: # {\'addition\': 12, \'subtraction\': 8, \'multiplication\': 24, \'floor_division\': 12, \'convert_to_base\': \'0xc\'} ``` # Evaluation Criteria - Correct implementation of the specified operations. - Appropriate use of the `python310` package functions for arithmetic and conversion operations. - Proper handling of input and return values. **Note:** Please make sure to handle any possible errors and edge cases as per the given constraints.","solution":"def perform_operations(num1, num2, num3, base): Perform a series of arithmetic operations and convert the result to a specific base. :param num1: First integer :param num2: Second integer (non-zero) :param num3: Third integer :param base: Base to which the final result should be converted (2, 8, 10, or 16) :return: Dictionary with results of the operations and the final converted result # Step 1: Add num1 and num2 addition = num1 + num2 # Step 2: Subtract num2 from num1 subtraction = num1 - num2 # Step 3: Multiply the result of Step 2 with num3 multiplication = subtraction * num3 # Step 4: Perform floor division of the result of Step 3 by num2 floor_division = multiplication // num2 # Step 5: Convert the result of Step 4 to a string in the specified base if base == 2: convert_to_base = bin(floor_division) elif base == 8: convert_to_base = oct(floor_division) elif base == 10: convert_to_base = str(floor_division) elif base == 16: convert_to_base = hex(floor_division) else: raise ValueError(\\"Invalid base, must be one of [2, 8, 10, 16]\\") return { \'addition\': addition, \'subtraction\': subtraction, \'multiplication\': multiplication, \'floor_division\': floor_division, \'convert_to_base\': convert_to_base }"},{"question":"You are tasked with creating a web scraper that is respectful of the websites it scrapes by adhering to the rules specified in their `robots.txt` files. You will use the `urllib.robotparser` module to parse and interpret these files. For this task, you will implement a function that checks if a given user agent can fetch a series of URLs from a website. Function Signature ```python def check_robots_txt(useragent: str, url_list: list) -> dict: ``` Input - `useragent` (str): The name of the user agent making the requests. - `url_list` (list of str): A list of URLs (all from the same site) that need to be checked against the site\'s `robots.txt` rules. Output - The function should return a dictionary where the keys are the URLs from `url_list` and the values are boolean values indicating whether the `useragent` is allowed to fetch each URL (i.e., `True` if allowed, `False` if not allowed). Constraints 1. The URLs in `url_list` belong to the same hostname (i.e., all URLs are from the same website). 2. The function should handle exceptions gracefully and return `False` for URLs that cannot be checked due to errors such as network issues or absence of `robots.txt`. Example Usage ```python useragent = \'my-web-scraper\' url_list = [ \'http://example.com/\', \'http://example.com/private/\', \'http://example.com/public/data\' ] result = check_robots_txt(useragent, url_list) # Expected result could be: # { # \'http://example.com/\': True, # \'http://example.com/private/\': False, # \'http://example.com/public/data\': True # } ``` Requirements To solve this problem, you should: 1. Parse the `robots.txt` file of the website using the `urllib.robotparser` module. 2. Use the `can_fetch` method to determine if the given `useragent` can access each URL. 3. Return the results in the specified output format. You may use the following additional helper function definition to extract the hostname from a URL without using external libraries: ```python from urllib.parse import urlparse def get_hostname(url: str) -> str: parsed_url = urlparse(url) return parsed_url.scheme + \'://\' + parsed_url.netloc ``` Good luck, and remember to follow the rules of `robots.txt` files strictly!","solution":"import urllib.robotparser from urllib.parse import urlparse def get_hostname(url: str) -> str: parsed_url = urlparse(url) return parsed_url.scheme + \'://\' + parsed_url.netloc def check_robots_txt(useragent: str, url_list: list) -> dict: if not url_list: return {} hostname = get_hostname(url_list[0]) robots_txt_url = hostname + \'/robots.txt\' rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_txt_url) try: rp.read() except: return {url: False for url in url_list} result = {} for url in url_list: result[url] = rp.can_fetch(useragent, url) return result"},{"question":"**Question: Implement a Simple NNTP Client Using nntplib** You are required to implement a simplified NNTP (Network News Transfer Protocol) client using the `nntplib` module. The client should connect to an NNTP server, list available newsgroups, retrieve and print recent articles from a specified newsgroup, and post a new article to that group. # Requirements: 1. **Connect to NNTP Server** - Connect to the NNTP server at `\'news.gmane.io\'`. 2. **List Newsgroups** - List all available newsgroups on the server. 3. **Retrieve Recent Articles** - Retrieve and print the subjects of the last 5 articles from a specific newsgroup. The newsgroup name should be provided by the user. 4. **Post a New Article** - Post a new article to a specified newsgroup. The article should be read from a file with valid headers and body content. The filename and newsgroup name should be provided by the user. # Implementation Details: - Implement a `SimpleNNTPClient` class with the following methods: - `def __init__(self, host: str, port: int = 119)`: Initialize the NNTP client with the specified server host and port. - `def list_newsgroups(self) -> list`: Retrieve and return the list of available newsgroups. - `def get_recent_articles(self, group: str, count: int = 5) -> list`: Retrieve and return the subjects of the last `count` articles from the specified newsgroup. - `def post_article(self, group: str, filename: str) -> str`: Post an article read from `filename` to the specified newsgroup. Return the server\'s response. # Constraints: - Handle exceptions such as connection errors, server errors, and invalid inputs gracefully. - Ensure the NNTP connection is properly closed after operations. # Example Usage: ```python client = SimpleNNTPClient(\'news.gmane.io\') # List newsgroups newsgroups = client.list_newsgroups() print(f\\"Available newsgroups: {newsgroups}\\") # Retrieve recent articles from a specific newsgroup group = \'gmane.comp.python.committers\' articles = client.get_recent_articles(group) print(f\\"Recent articles in {group}: {articles}\\") # Post a new article to a specified newsgroup filename = \'article.txt\' response = client.post_article(group, filename) print(f\\"Post article response: {response}\\") ``` # Input and Output Format: - The methods\' parameters and return types should match the specified signature in terms of data types. - Properly format and handle server responses to display meaningful information to the user. # Testing: Ensure your implementation is tested with the given example and other edge cases, such as non-existent newsgroups, empty files, and connection issues.","solution":"import nntplib from typing import List class SimpleNNTPClient: def __init__(self, host: str, port: int = 119): self.host = host self.port = port self.connection = None def connect(self): try: self.connection = nntplib.NNTP(self.host, self.port) except Exception as e: raise ConnectionError(f\\"Failed to connect to {self.host}:{self.port}: {e}\\") def close(self): if self.connection: self.connection.quit() self.connection = None def list_newsgroups(self) -> List[str]: try: self.connect() resp, groups = self.connection.list() self.close() return [group for group, *_ in groups] except Exception as e: self.close() raise RuntimeError(f\\"Failed to list newsgroups: {e}\\") def get_recent_articles(self, group: str, count: int = 5) -> List[str]: try: self.connect() self.connection.group(group) resp, overviews = self.connection.over((f\'{count}\', f\'{count}\')) self.close() return [overview.subject for _, overview in overviews[:count]] except Exception as e: self.close() raise RuntimeError(f\\"Failed to get recent articles from {group}: {e}\\") def post_article(self, group: str, filename: str) -> str: try: self.connect() with open(filename, \'r\') as f: article = f.read() response = self.connection.post(article) self.close() return response except Exception as e: self.close() raise RuntimeError(f\\"Failed to post article to {group}: {e}\\")"},{"question":"# Custom Sequence Type Implementation Implement a custom sequence type in Python that replicates some of the functionality of Python\'s built-in list. Your sequence type, `CustomSequence`, should support the following operations and behaviors: 1. **Initialization**: - The class should be initialized with an iterable (like a list or a tuple). ```python cs = CustomSequence([1, 2, 3]) ``` 2. **Length**: - The `len()` function should work correctly and return the number of items in the sequence. ```python len(cs) # Should return 3 ``` 3. **Indexing**: - The sequence should support indexing, including negative indices and slicing. ```python cs[0] # Should return 1 cs[-1] # Should return 3 cs[1:3] # Should return a new CustomSequence object with elements [2, 3] ``` 4. **Item assignment**: - It should be possible to assign new values to elements at specific indices. ```python cs[1] = 4 # cs should now be [1, 4, 3] ``` 5. **Item deletion**: - It should be possible to delete items from specific indices. ```python del cs[1] # cs should now be [1, 3] ``` 6. **Iteration**: - The sequence should support iteration. ```python for item in cs: print(item) # Should print 1, 3 (if cs is [1, 3]) ``` 7. **Containment**: - Implement the `in` operator to check for membership. ```python 3 in cs # Should return True 4 in cs # Should return False ``` 8. **String Representation**: - Implement the `__repr__` method to return a string that looks like a valid Python expression that could be used to recreate the object. ```python repr(cs) # Should return \\"CustomSequence([1, 3])\\" ``` 9. **Boolean Value**: - The sequence should be able to evaluate to `False` when it is empty and `True` otherwise. ```python bool(CustomSequence([])) # Should return False bool(CustomSequence([1, 2])) # Should return True ``` # Implementation Constraints: - Do not use Python\'s built-in list or any other built-in collection directly to store the data. Instead, manage the items using a custom storage solution within your class. - Ensure that special methods are used properly according to the behavior specified. - Your implementation should perform efficiently for the operations described. # Example Usage: ```python cs = CustomSequence([1, 2, 3]) print(len(cs)) # 3 print(cs[1]) # 2 print(cs[-1]) # 3 print(cs[1:3]) # CustomSequence([2, 3]) cs[1] = 4 print(cs) # CustomSequence([1, 4, 3]) del cs[1] print(cs) # CustomSequence([1, 3]) print(3 in cs) # True print(4 in cs) # False print(bool(cs)) # True print(bool(CustomSequence([]))) # False ``` Implement the `CustomSequence` class in the code cell below: ```python class CustomSequence: def __init__(self, iterable): # Your code here def __len__(self): # Your code here def __getitem__(self, index): # Your code here def __setitem__(self, index, value): # Your code here def __delitem__(self, index): # Your code here def __iter__(self): # Your code here def __contains__(self, item): # Your code here def __repr__(self): # Your code here def __bool__(self): # Your code here ```","solution":"class CustomSequence: def __init__(self, iterable): self._data = list(iterable) def __len__(self): return len(self._data) def __getitem__(self, index): if isinstance(index, slice): return CustomSequence(self._data[index]) return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __iter__(self): return iter(self._data) def __contains__(self, item): return item in self._data def __repr__(self): return f\'CustomSequence({self._data})\' def __bool__(self): return bool(self._data)"},{"question":"Problem Statement You are tasked with creating a Python function to manage configuration settings for a software application. The function should allow reading from, writing to, and modifying a configuration file following the INI file format. You must implement various operations on the configuration file using the `configparser` module. # Function Signature ```python def manage_config(file_path: str, operations: List[Tuple[str, str, Union[str, int, float, bool, None]]]) -> dict: pass ``` # Input - `file_path` (str): The path to the configuration file. - `operations` (List[Tuple[str, str, Union[str, int, float, bool, None]]]): A list of operations to perform on the configuration file. Each operation is a tuple containing: - An operation type (str): One of `\\"read\\"`, `\\"write\\"`, `\\"update\\"`, `\\"delete\\"`. - A target (str): The target section and key formatted as `\\"section:key\\"`. - A value (Union[str, int, float, bool, None]): The value associated with the operation. This is: - Required for `\\"write\\"` and `\\"update\\"` operations. - Not required for `\\"read\\"` and `\\"delete\\"` operations (can be `None`). # Output - A dictionary containing the final state of the configuration after all operations have been applied, with sections as keys and their key-value pairs as nested dictionaries. # Constraints - The function should handle missing sections/keys gracefully. - The function should support reading values as strings, integers, floats, and booleans. - For booleans, the accepted values are `\'1\'`, `\'yes\'`, `\'true\'`, and `\'on\'` for `True`, and `\'0\'`, `\'no\'`, `\'false\'`, and `\'off\'` for `False`. - The function should handle exceptions appropriately, such as missing sections or duplicate sections. # Example Suppose we have the following configuration file at `config.ini`: ``` [DEFAULT] ServerAliveInterval=45 Compression=yes CompressionLevel=9 [forge.example] User=hg [topsecret.server.example] Port=50022 ForwardX11=no ``` And the following list of operations: ```python operations = [ (\\"read\\", \\"DEFAULT:Compression\\", None), (\\"write\\", \\"new.section:new_key\\", \\"new_value\\"), (\\"update\\", \\"forge.example:User\\", \\"new_user\\"), (\\"delete\\", \\"topsecret.server.example:ForwardX11\\", None), ] ``` After performing these operations, the function should return the following dictionary representing the final state of the configuration: ```python { \'DEFAULT\': { \'serveraliveinterval\': \'45\', \'compression\': \'yes\', \'compressionlevel\': \'9\' }, \'forge.example\': { \'user\': \'new_user\' }, \'topsecret.server.example\': { \'port\': \'50022\' }, \'new.section\': { \'new_key\': \'new_value\' } } ``` # Notes - Remember to handle case insensitivity for option names. - Make sure you validate the operations properly before applying them. - Consider using the `getint()`, `getfloat()`, `getboolean()` methods for value conversions where necessary.","solution":"import configparser from typing import List, Tuple, Union def manage_config(file_path: str, operations: List[Tuple[str, str, Union[str, int, float, bool, None]]]) -> dict: config = configparser.ConfigParser() config.read(file_path) for operation, target, value in operations: section, key = target.split(\':\') if operation == \\"read\\": if config.has_section(section) and config.has_option(section, key): val = config.get(section, key) if val.isdigit(): value = int(val) elif val.replace(\'.\', \'\', 1).isdigit() and val.count(\'.\') < 2: value = float(val) elif val.lower() in [\'1\', \'yes\', \'true\', \'on\']: value = True elif val.lower() in [\'0\', \'no\', \'false\', \'off\']: value = False else: value = val else: value = None elif operation == \\"write\\": if not config.has_section(section): config.add_section(section) config.set(section, key, str(value)) elif operation == \\"update\\": if config.has_section(section) and config.has_option(section, key): config.set(section, key, str(value)) elif operation == \\"delete\\": if config.has_section(section) and config.has_option(section, key): config.remove_option(section, key) result = {s: dict(config.items(s)) for s in config.sections()} if config.defaults(): result[\'DEFAULT\'] = dict(config.defaults()) return result"},{"question":"Objective To assess the students ability to use Pythons file handling, data serialization, and structured data formatting capabilities. Problem Statement You are given a JSON file named `students.json` that contains student information including their names, ages, and grades. Your task is to write a Python script that reads this data, processes it, and outputs the formatted information to a new text file named `student_report.txt`. Instructions 1. **Read the JSON File**: - Open and read the `students.json` file. - Deserialize the JSON data into a Python dictionary. 2. **Processing Data**: - Calculate the average, highest, and lowest grade among the students. 3. **Format the Output**: - Create a well-formatted string report that lists each students name and grade. - Include the calculated average, highest, and lowest grades at the end of the report. 4. **Write to a Text File**: - Write the formatted string to a new text file `student_report.txt`. JSON Structure The `students.json` file contains a list of dictionaries, each representing a student, with the following keys: - `name`: Name of the student (string) - `age`: Age of the student (integer) - `grade`: Grade of the student (float) Example: ```json [ {\\"name\\": \\"Alice\\", \\"age\\": 23, \\"grade\\": 88.5}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grade\\": 92.0}, {\\"name\\": \\"Charlie\\", \\"age\\": 24, \\"grade\\": 70.3} ] ``` Constraints - Assume the JSON file will always be in the correct format. - The grades will be between 0 and 100. - The JSON file will contain at least one student. Expected Output The `student_report.txt` should be formatted like below: ``` Student Report -------------- Alice: 88.5 Bob: 92.0 Charlie: 70.3 Average Grade: 83.6 Highest Grade: 92.0 Lowest Grade: 70.3 ``` Function Signature You may define your function as follows: ```python def generate_student_report(input_file: str, output_file: str) -> None: pass ``` You can test your function with the following: ```python generate_student_report(\'students.json\', \'student_report.txt\') ```","solution":"import json def generate_student_report(input_file: str, output_file: str) -> None: with open(input_file, \'r\') as file: students = json.load(file) grades = [student[\'grade\'] for student in students] average = sum(grades) / len(grades) highest = max(grades) lowest = min(grades) report_lines = [\\"Student Report\\", \\"--------------\\"] for student in students: report_lines.append(f\\"{student[\'name\']}: {student[\'grade\']}\\") report_lines.append(\\"\\") report_lines.append(f\\"Average Grade: {average:.1f}\\") report_lines.append(f\\"Highest Grade: {highest:.1f}\\") report_lines.append(f\\"Lowest Grade: {lowest:.1f}\\") report_content = \\"n\\".join(report_lines) with open(output_file, \'w\') as file: file.write(report_content)"},{"question":"Task You are tasked with implementing a machine learning pipeline that includes data generation, preprocessing, model training, and evaluation using scikit-learn. You will use synthetic data generated with numpy, preprocess it with scikit-learn tools, train a model, and then assess the model\'s performance. Finally, you will diagnose and fix a specific issue in the model training process. Objective You are to demonstrate your ability to: - Generate synthetic datasets suitable for regression tasks. - Preprocess the data using scikit-learn preprocessing tools. - Train a GradientBoostingRegressor model. - Identify and rectify an issue in the model training process to improve performance. Instructions # Part 1: Data Generation Write a function `generate_synthetic_data` that generates a synthetic regression dataset using numpy. The function should accept the following parameters: - `n_samples` (int): number of samples. - `n_features` (int): number of features. - `noise` (float): standard deviation of the gaussian noise applied to the output. The function should return two numpy arrays: `X` (features) and `y` (target). # Part 2: Data Preprocessing Write a function `preprocess_data` that accepts the feature matrix `X` and performs standardization using scikit-learn\'s `StandardScaler`. The function should return the standardized feature matrix. # Part 3: Model Training and Evaluation Write a function `train_and_evaluate_model` that does the following: - Splits the data into training and testing sets using scikit-learn\'s `train_test_split` (70% training, 30% testing). - Trains a `GradientBoostingRegressor` model on the training data. - Evaluates the model using the `mean_squared_error` metric on the testing data. The function should print the mean squared error. # Part 4: Diagnosing and Fixing the Issue There is an issue in the model training process related to the `n_iter_no_change` parameter of `GradientBoostingRegressor`. Modify the `train_and_evaluate_model` function to train two variants of the model: - One with the default `n_iter_no_change`. - Another with `n_iter_no_change=5`. Print the mean squared error for both models and identify which configuration yields a better performance. Expected Input and Output Formats # generate_synthetic_data **Input:** - n_samples: 1000 - n_features: 20 - noise: 0.1 **Output:** - X: numpy array of shape (1000, 20) - y: numpy array of shape (1000,) # preprocess_data **Input:** - X: numpy array of shape (1000, 20) **Output:** - X_scaled: numpy array of shape (1000, 20) # train_and_evaluate_model **Input:** - X: numpy array of shape (1000, 20) - y: numpy array of shape (1000,) **Output:** - Mean squared error for both model variants printed to the console. # Example ```python # Part 1: Data Generation X, y = generate_synthetic_data(1000, 20, 0.1) # Part 2: Data Preprocessing X_scaled = preprocess_data(X) # Part 3 and 4: Model Training, Evaluation, and Issue Fixing train_and_evaluate_model(X_scaled, y) ``` The solution should be clear, complete, and modular, focusing on functionality and correctness.","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error def generate_synthetic_data(n_samples, n_features, noise): Generates a synthetic regression dataset. Parameters: n_samples (int): Number of samples. n_features (int): Number of features. noise (float): Standard deviation of the Gaussian noise applied to the output. Returns: X (numpy array): Generated feature matrix. y (numpy array): Generated target vector. np.random.seed(0) X = np.random.randn(n_samples, n_features) coef = np.random.randn(n_features) y = X.dot(coef) + noise * np.random.randn(n_samples) return X, y def preprocess_data(X): Standardizes the feature matrix X using StandardScaler. Parameters: X (numpy array): Feature matrix. Returns: X_scaled (numpy array): Standardized feature matrix. scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled def train_and_evaluate_model(X, y): Trains a GradientBoostingRegressor model and evaluates its performance. Parameters: X (numpy array): Feature matrix. y (numpy array): Target vector. Prints: Mean squared error for the model with default n_iter_no_change. Mean squared error for the model with n_iter_no_change=5. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Default GradientBoostingRegressor model_default = GradientBoostingRegressor(random_state=0) model_default.fit(X_train, y_train) y_pred_default = model_default.predict(X_test) mse_default = mean_squared_error(y_test, y_pred_default) print(f\\"Default n_iter_no_change MSE: {mse_default}\\") # GradientBoostingRegressor with n_iter_no_change=5 model_modified = GradientBoostingRegressor(n_iter_no_change=5, random_state=0) model_modified.fit(X_train, y_train) y_pred_modified = model_modified.predict(X_test) mse_modified = mean_squared_error(y_test, y_pred_modified) print(f\\"n_iter_no_change=5 MSE: {mse_modified}\\") return mse_default, mse_modified"},{"question":"Problem Statement Using the `graphlib.TopologicalSorter` class, implement a function `find_dependency_order(tasks: List[Tuple[str, List[str]]]) -> List[str]` that takes as input a list of tasks with their dependencies and returns the order in which tasks can be completed. Each task is represented as a tuple where the first element is the task identifier (a string) and the second element is a list of tasks that must be completed before this task. If the task dependencies contain a cycle, the function should raise a `CycleError` with the appropriate cycle information. Input - `tasks`: A list of tuples. Each tuple contains: - A string representing the task identifier. - A list of strings representing the task identifiers that must be completed before the task. Output - A list of strings representing the order in which tasks can be completed. If there is no valid order due to a cycle in the dependencies, raise a `graphlib.CycleError`. Constraints - Task identifiers are alphanumeric strings and are unique. - The dependency lists are valid and only include existing task identifiers. - There must be at least one task in the list. Example ```python from typing import List, Tuple import graphlib def find_dependency_order(tasks: List[Tuple[str, List[str]]]) -> List[str]: # Add your implementation here pass # Example usage: tasks = [ (\\"build_house\\", [\\"lay_foundation\\", \\"construct_walls\\"]), (\\"paint_house\\", [\\"build_house\\"]), (\\"lay_foundation\\", []), (\\"construct_walls\\", [\\"lay_foundation\\"]), (\\"install_windows\\", [\\"construct_walls\\"]), (\\"install_doors\\", [\\"construct_walls\\"]), ] print(find_dependency_order(tasks)) # Output: [\'lay_foundation\', \'construct_walls\', \'build_house\', \'paint_house\', \'install_windows\', \'install_doors\'] # Example with a cycle: tasks_with_cycle = [ (\\"A\\", [\\"B\\"]), (\\"B\\", [\\"C\\"]), (\\"C\\", [\\"A\\"]), ] print(find_dependency_order(tasks_with_cycle)) # Output: Raises graphlib.CycleError ``` Note - Ensure that you handle the `CycleError` appropriately by raising it when a cycle is detected. - Use methods in `graphlib.TopologicalSorter` to add nodes, prepare the graph, and fetch the order of task processing.","solution":"from typing import List, Tuple import graphlib def find_dependency_order(tasks: List[Tuple[str, List[str]]]) -> List[str]: Returns the order in which tasks can be completed based on their dependencies. Parameters: tasks (List[Tuple[str, List[str]]]): A list of tasks with dependencies. Returns: List[str]: List of task identifiers in the order they can be completed. Raises: graphlib.CycleError: If there is a cycle in the task dependencies. sorter = graphlib.TopologicalSorter() for task, dependencies in tasks: sorter.add(task, *dependencies) try: order = list(sorter.static_order()) except graphlib.CycleError as cycle: raise graphlib.CycleError(f\\"Cycle detected: {cycle.args[1]}\\") return order"},{"question":"# Advanced Seaborn Plot Customization and Analysis **Objective:** Demonstrate your understanding of seaborn plotting library by creating and customizing multiple types of plots based on given data. **Problem Statement:** You are provided with a dataset containing information on different species of flowers. For the further analysis and presentation, you need to follow these instructions: 1. Load the provided dataset using seaborn\'s `load_dataset` method. Use the dataset: `iris`. 2. Perform the following customizations: - Set the seaborn theme to `\\"darkgrid\\"` and the color palette to `\\"muted\\"`. - Create a `bar plot` showing the average petal length for each species. - Use the `barplot` function from seaborn. - Disable the top and right spines of the plot. - Create a `scatter plot` visualizing petal width vs petal length, color-coded by species. - Include a regression line for each species group. - Set the markers for different species groups. - Create a `box plot` showing the distribution of sepal length for each species. - Customize the plot to show the distributions with different colors and add swarm plot points. 3. Save each of the plots as a separate image file. **Constraints:** - Do not change the dataset manually. - Follow the specified theme and styles precisely. **Input:** - None, the dataset should be loaded within the code using seaborn\'s `load_dataset` method. **Output:** - Three plot files saved in the current working directory with names `bar_plot.png`, `scatter_plot.png`, and `box_plot.png`. **Code Template:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset data = sns.load_dataset(\\"iris\\") # 1. Set the theme and palette sns.set_theme(style=\\"darkgrid\\", palette=\\"muted\\") # 2. Create and customize the bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=\\"species\\", y=\\"petal_length\\", data=data) sns.despine() plt.savefig(\\"bar_plot.png\\") # 3. Create and customize the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(x=\\"petal_width\\", y=\\"petal_length\\", hue=\\"species\\", style=\\"species\\", data=data) sns.regplot(x=\\"petal_width\\", y=\\"petal_length\\", data=data, scatter=False, color=\\".1\\") plt.savefig(\\"scatter_plot.png\\") # 4. Create and customize the box plot with swarm plot overlay plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"species\\", y=\\"sepal_length\\", data=data) sns.swarmplot(x=\\"species\\", y=\\"sepal_length\\", data=data, color=\\".25\\") plt.savefig(\\"box_plot.png\\") ``` Make sure the code follows the expected structure and requirements. Your final plots should visually match the descriptions provided.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset data = sns.load_dataset(\\"iris\\") # 1. Set the theme and palette sns.set_theme(style=\\"darkgrid\\", palette=\\"muted\\") # 2. Create and customize the bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=\\"species\\", y=\\"petal_length\\", data=data) sns.despine(top=True, right=True) plt.savefig(\\"bar_plot.png\\") # 3. Create and customize the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(x=\\"petal_width\\", y=\\"petal_length\\", hue=\\"species\\", style=\\"species\\", data=data) sns.lmplot(x=\\"petal_width\\", y=\\"petal_length\\", hue=\\"species\\", data=data, markers=[\\"o\\", \\"s\\", \\"D\\"]) plt.savefig(\\"scatter_plot.png\\") # 4. Create and customize the box plot with swarm plot overlay plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"species\\", y=\\"sepal_length\\", data=data) sns.swarmplot(x=\\"species\\", y=\\"sepal_length\\", data=data, color=\\".25\\") plt.savefig(\\"box_plot.png\\")"},{"question":"Objective To assess students\' understanding of data preprocessing, model training, and evaluation using scikit-learn. Problem Statement You need to build a classification model using the Breast Cancer dataset from scikit-learn\'s toy datasets. Follow the steps outlined below: 1. **Load the Dataset**: Load the Breast Cancer dataset using the appropriate function from `sklearn.datasets`. 2. **Preprocess the Data**: - Split the dataset into training and testing sets (80% train, 20% test). - Standardize the feature values by removing the mean and scaling to unit variance. 3. **Build and Train the Model**: - Use a Support Vector Classifier (SVC) from `sklearn.svm`. - Train the model using the training data. 4. **Evaluate the Model**: - Predict the labels for the test data. - Calculate and print the following metrics: Accuracy, Precision, Recall, and F1-score. Requirements - Implement the function `breast_cancer_model()`: ```python def breast_cancer_model(): # Step 1: Load the dataset from sklearn.datasets import load_breast_cancer # Load the dataset data = load_breast_cancer() X, y = data.data, data.target # Step 2: Split the dataset from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Standardize the features from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train the Support Vector Classifier from sklearn.svm import SVC model = SVC(random_state=42) model.fit(X_train, y_train) # Step 5: Make predictions and evaluate from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) print(\'Accuracy:\', accuracy) print(\'Precision:\', precision) print(\'Recall:\', recall) print(\'F1 Score:\', f1) ``` Constraints - Use `random_state=42` for reproducibility. - You must use scikit-learn\'s `StandardScaler` for standardization. - Use scikit-learn\'s `SVC` without specifying any hyperparameters except `random_state`. This question assesses the student\'s ability to: - Load and understand a dataset. - Preprocess data (splitting and standardizing). - Train and evaluate a simple machine learning model using scikit-learn.","solution":"def breast_cancer_model(): Builds and evaluates a Support Vector Classifier on the Breast Cancer dataset. # Step 1: Load the dataset from sklearn.datasets import load_breast_cancer # Load the dataset data = load_breast_cancer() X, y = data.data, data.target # Step 2: Split the dataset from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Standardize the features from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train the Support Vector Classifier from sklearn.svm import SVC model = SVC(random_state=42) model.fit(X_train, y_train) # Step 5: Make predictions and evaluate from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) return accuracy, precision, recall, f1"},{"question":"Objective Create a data processing class `AsyncFileReader` that uses asynchronous programming techniques to read lines from a file one by one. This will require understanding and implementing Python 3.10\'s async features, including `aiter()` and `anext()`. Task Description You need to implement a class `AsyncFileReader` that: 1. Reads lines from a text file asynchronously. 2. Uses the `aiter()` and `anext()` functions for reading lines of the file. 3. Uses class methods to open and close the file. 4. Includes error handling for file operations. Implementation Details 1. Define a class `AsyncFileReader`. 2. Implement a class method `open_file` that opens the file asynchronously and returns an instance of `AsyncFileReader`. 3. Implement a class method `close_file` that closes the file asynchronously. 4. Implement an asynchronous instance method `read_line` that returns the next line in the file using `anext` or raises `StopAsyncIteration` if the end of the file is reached. 5. Use the `aiter()` function to create an asynchronous iterator for the file. Example Heres an example of how the implementation should work. ```python import asyncio class AsyncFileReader: def __init__(self, file): self.file = file @classmethod async def open_file(cls, filename): loop = asyncio.get_running_loop() file = await loop.run_in_executor(None, open, filename, \'r\') return cls(file) async def close_file(self): await asyncio.get_running_loop().run_in_executor(None, self.file.close) async def read_line(self): async for line in aiter(self.file): return await anext(self.file) # Usage Example async def main(): reader = await AsyncFileReader.open_file(\'example.txt\') try: while True: line = await reader.read_line() if line: print(line.strip()) else: break except StopAsyncIteration: pass finally: await reader.close_file() # Run the example asyncio.run(main()) ``` Constraints - You can assume the file will always exist. - Handle any possible exceptions that might occur during file operations, such as when the file cannot be read or written. - The `read_line` method should use the `anext` function to read the next line from the asynchronous iterator of the file. Expected Input and Output No additional input is expected other than the filename. The output should be the lines of the file printed to the standard output. Notes - Ensure your solution uses the asynchronous features (`aiter`, `anext`) introduced in Python 3.10. - Be mindful of resource management by implementing proper file closing mechanisms.","solution":"import asyncio class AsyncFileReader: def __init__(self, file): self.file = file self.iter = None @classmethod async def open_file(cls, filename): loop = asyncio.get_running_loop() file = await loop.run_in_executor(None, open, filename, \'r\') instance = cls(file) instance.iter = aiter(file) return instance async def close_file(self): await asyncio.get_running_loop().run_in_executor(None, self.file.close) async def read_line(self): try: line = await anext(self.iter) return line except StopAsyncIteration: raise StopAsyncIteration"},{"question":"# AsyncIO Project Deadline Scheduler **Objective**: Implement an async Python program to manage and execute multiple project tasks, ensuring some tasks with deadlines do not exceed their allotted time. # Task Description You are required to write a Python program that: 1. Creates multiple project tasks, each with a different execution duration. 2. Runs these tasks concurrently using the `asyncio` package. 3. Ensures that some tasks respect their allotted timeouts and are canceled if they exceed their deadlines. 4. Prints the results of completed tasks and indicates if a task was canceled due to a timeout. # Requirements 1. **Function Definition**: ```python async def project_scheduler(tasks: list[tuple[int, float]]) -> dict[str, float | str]: ``` - `tasks`: A list of tuples, where each tuple contains two values: - The first value is an integer representing the task identifier. - The second value is a float representing the duration (in seconds) the task should run. 2. **Task Execution**: - Each task should be an async function that simulates doing some work for the given duration. - If a task has a duration less than or equal to 2 seconds, it should be subject to a timeout of 1.5 seconds. - Print when each task starts, completes, or is canceled. 3. **Return Format**: - Return a dictionary where the keys are task identifiers (as strings) and the values are either: - The time it took to complete the task if it succeeded. - The string `\\"timeout\\"` if the task was canceled due to exceeding its allotted time. # Constraints - Use `asyncio.create_task()` for running tasks concurrently. - Use `asyncio.wait_for()` to timeout tasks when necessary. - Ensure the program can handle at least 10 tasks without performance degradation. # Example ```python import asyncio async def project_scheduler(tasks: list[tuple[int, float]]) -> dict[str, float | str]: async def task_runner(task_id: int, duration: float) -> float: await asyncio.sleep(duration) return duration result = {} async def scheduler(): for task_id, duration in tasks: if duration <= 2: task = asyncio.create_task(asyncio.wait_for(task_runner(task_id, duration), timeout=1.5)) else: task = asyncio.create_task(task_runner(task_id, duration)) try: result[str(task_id)] = await task except asyncio.TimeoutError: result[str(task_id)] = \\"timeout\\" await scheduler() return result # Example usage tasks = [ (1, 1), # Should complete in 1 second (2, 1.6), # Should timeout (exceeds 1.5s) (3, 3), # Should complete in 3 seconds (4, 1), # Should complete in 1 second ] async def main(): results = await project_scheduler(tasks) print(results) asyncio.run(main()) ``` **Expected Output** (Order may vary): ``` { \'1\': 1.0, \'2\': \'timeout\', \'3\': 3.0, \'4\': 1.0 } ``` In this example, tasks that run longer than 2 seconds are not subjected to a timeout, but shorter tasks are. The program should handle these tasks concurrently and report their results or if they were timed out.","solution":"import asyncio async def project_scheduler(tasks: list[tuple[int, float]]) -> dict[str, float | str]: async def task_runner(task_id: int, duration: float) -> float: print(f\\"Task {task_id} started, expected to run for {duration} seconds.\\") await asyncio.sleep(duration) print(f\\"Task {task_id} completed.\\") return duration result = {} async def scheduler(): tasks_list = [] for task_id, duration in tasks: if duration <= 2: task = asyncio.create_task( asyncio.wait_for(task_runner(task_id, duration), timeout=1.5, loop=asyncio.get_event_loop()) ) else: task = asyncio.create_task(task_runner(task_id, duration)) tasks_list.append((task_id, task)) for task_id, task in tasks_list: try: result[str(task_id)] = await task except asyncio.TimeoutError: print(f\\"Task {task_id} timed out.\\") result[str(task_id)] = \\"timeout\\" await scheduler() return result"},{"question":"# Asyncio TCP Echo Server and Client Objective: To assess your understanding of the `asyncio` module, particularly focusing on event loop, networking, and async/await syntax, you are required to implement an asynchronous TCP echo server and client using `asyncio`. Requirements: 1. **Echo Server**: - Create an asynchronous TCP echo server that listens on `localhost` and a specified port. - The server should accept incoming connections, read data from the client, and send the same data back to the client (echo). - The server should handle multiple client connections concurrently. 2. **Echo Client**: - Create an asynchronous TCP client that can connect to the echo server, send a message, and print the echoed message received from the server. 3. **Main Function**: - Write a main function to start the server and test the client by sending a message. Input and Output formats: - **Server**: - Input: Port number to listen on (e.g., 8888) - Output: No direct output, but it should handle incoming connections and echo back any received messages. - **Client**: - Input: Server address (localhost) and port number, and a message string to send. - Output: The received echoed message printed to console. Constraints: - Use the `asyncio` module only. - Ensure proper exception handling in both server and client. - The echoed message should match the sent message exactly. Performance Requirements: - The server should handle multiple clients simultaneously without blocking. - The server must remain responsive and handle clients efficiently. Example: ```python # Server code async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() writer.write(data) await writer.drain() writer.close() async def main(): server = await asyncio.start_server(handle_client, \'localhost\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main()) # Client code async def tcp_echo_client(message): reader, writer = await asyncio.open_connection(\'localhost\', 8888) writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() if __name__ == \'__main__\': message = \'Hello, World!\' asyncio.run(tcp_echo_client(message)) ``` Implement the above example by creating an asynchronous TCP echo server and client using the `asyncio` module.","solution":"import asyncio # Echo Server async def handle_client(reader, writer): Handle the client connection, read data from the client and echo it back. while True: data = await reader.read(100) if not data: break message = data.decode() writer.write(data) await writer.drain() writer.close() await writer.wait_closed() async def run_server(host, port): Run the TCP echo server on the specified host and port. server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() # Echo Client async def tcp_echo_client(message, host, port): Connect to the TCP echo server, send a message, and print the echoed message. reader, writer = await asyncio.open_connection(host, port) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() # Main function for running the server and client async def main(): # Server hostname and port host = \'localhost\' port = 8888 # Run the server server = asyncio.create_task(run_server(host, port)) # Give the server some time to start await asyncio.sleep(1) # Run the client message = \'Hello, Asyncio!\' await tcp_echo_client(message, host, port) # Stop the server server.cancel() try: await server except asyncio.CancelledError: pass if __name__ == \'__main__\': asyncio.run(main())"},{"question":"# SQLite3 Custom Python Types and Transaction Management **Objective:** Write a Python program using the `sqlite3` module that demonstrates your understanding of basic CRUD operations, transaction management, and custom type adaptation and conversion in SQLite. **Requirements:** 1. **Database Setup:** - Create an in-memory SQLite database. - Define a table named `inventory` with the following columns: - `item_id` (INTEGER PRIMARY KEY) - `item_name` (TEXT) - `quantity` (INTEGER) - `price` (REAL) 2. **Create and Register Custom Type:** - Define a custom class `Item` with attributes `name`, `quantity`, and `price`. - Implement an adapter and a converter for the `Item` class to store it as a single string in the format `\\"name;quantity;price\\"`. 3. **CRUD Operations:** - Insert multiple `Item` instances into the `inventory` table using `executemany` with placeholders. - Implement a function `get_items_below_price(price_limit: float) -> List[Item]` that retrieves all items with a price below the given limit. - Implement a function `update_item_quantity(item_name: str, new_quantity: int)` to update the quantity of an item based on its name. - Implement a function `delete_item(item_name: str)` to delete an item based on its name. 4. **Transaction Management:** - Use a context manager to ensure that the insertion of items and the updates are committed only if all operations succeed. - In case of any error during the insertion or update, rollback the transaction and handle the exception gracefully. 5. **Error Handling:** - Handle exceptions properly, especially for database connection errors, integrity errors, and operational errors. **Constraints:** - For simplicity, you can assume that the input data will always be valid and in the expected format. - Performance considerations can be ignored for this exercise. **Input and Output Formats:** - The input for the CRUD operation functions will be provided as function arguments. - The output for the `get_items_below_price` function should be a list of `Item` instances below the given price limit. - The other functions do not need to return any output but should perform the required database operations. **Sample Code Structure:** ```python import sqlite3 class Item: def __init__(self, name: str, quantity: int, price: float): self.name = name self.quantity = quantity self.price = price def adapt_item(item: Item) -> str: return f\\"{item.name};{item.quantity};{item.price}\\" def convert_item(s: bytes) -> Item: name, quantity, price = s.decode().split(\';\') return Item(name, int(quantity), float(price)) def setup_database(): conn = sqlite3.connect(\\":memory:\\") conn.execute( CREATE TABLE inventory ( item_id INTEGER PRIMARY KEY, item_name TEXT, quantity INTEGER, price REAL ) ) return conn def insert_items(conn, items): pass # Implement the insertion logic with transaction management def get_items_below_price(conn, price_limit: float): pass # Implement the retrieval logic def update_item_quantity(conn, item_name: str, new_quantity: int): pass # Implement the update logic with transaction management def delete_item(conn, item_name: str): pass # Implement the deletion logic if __name__ == \\"__main__\\": sqlite3.register_adapter(Item, adapt_item) sqlite3.register_converter(\\"item\\", convert_item) conn = setup_database() # Perform CRUD operations and test the functions conn.close() ``` **Note:** Extend the sample code structure to complete the required functionality with proper transaction management and exception handling.","solution":"import sqlite3 class Item: def __init__(self, name: str, quantity: int, price: float): self.name = name self.quantity = quantity self.price = price def adapt_item(item: Item) -> str: return f\\"{item.name};{item.quantity};{item.price}\\" def convert_item(s: bytes) -> Item: name, quantity, price = s.decode().split(\';\') return Item(name, int(quantity), float(price)) def setup_database(): conn = sqlite3.connect(\\":memory:\\", detect_types=sqlite3.PARSE_DECLTYPES) conn.execute( CREATE TABLE inventory ( item_id INTEGER PRIMARY KEY, item_name TEXT, quantity INTEGER, price REAL ) ) return conn def insert_items(conn, items): try: cursor = conn.cursor() cursor.executemany( INSERT INTO inventory (item_name, quantity, price) VALUES (?, ?, ?) , [(item.name, item.quantity, item.price) for item in items]) conn.commit() except sqlite3.Error as e: conn.rollback() print(\\"Error during insertion:\\", e) def get_items_below_price(conn, price_limit: float): cursor = conn.cursor() cursor.execute( SELECT item_name, quantity, price FROM inventory WHERE price < ? , (price_limit,)) rows = cursor.fetchall() return [Item(name, quantity, price) for (name, quantity, price) in rows] def update_item_quantity(conn, item_name: str, new_quantity: int): try: cursor = conn.cursor() cursor.execute( UPDATE inventory SET quantity = ? WHERE item_name = ? , (new_quantity, item_name)) conn.commit() except sqlite3.Error as e: conn.rollback() print(\\"Error during update:\\", e) def delete_item(conn, item_name: str): try: cursor = conn.cursor() cursor.execute( DELETE FROM inventory WHERE item_name = ? , (item_name,)) conn.commit() except sqlite3.Error as e: conn.rollback() print(\\"Error during deletion:\\", e) if __name__ == \\"__main__\\": sqlite3.register_adapter(Item, adapt_item) sqlite3.register_converter(\\"item\\", convert_item) conn = setup_database() items = [Item(\\"Apple\\", 10, 2.5), Item(\\"Banana\\", 20, 1.5), Item(\\"Cherry\\", 15, 3.0)] insert_items(conn, items) print(get_items_below_price(conn, 3.0)) # Should print items with price below 3.0 update_item_quantity(conn, \\"Banana\\", 50) delete_item(conn, \\"Cherry\\") conn.close()"},{"question":"# Seaborn Coding Assessment **Objective:** This question assesses your ability to use the `seaborn` library to create informative scatter plots with multiple semantic mappings and apply facet grids for better data visualization. **Dataset:** Use the \\"tips\\" dataset available in the `seaborn` library. **Task:** 1. Load the \\"tips\\" dataset. 2. Create a scatter plot showing the relationship between `total_bill` and `tip`. 3. Map the variable `time` to the color of the points and the variable `day` to the marker styles. 4. Use the `size` parameter to reflect the `size` of the parties. 5. Customize and improve the legend to show all unique values. 6. Finally, create a faceted scatter plot using `relplot` with: - `total_bill` on the x-axis and `tip` on the y-axis. - Split the plots by `time` (using the `col` parameter). - Use `day` for the `hue` and `style`. **Constraints:** - Ensure the plot is clear and well-labeled. - Use an appropriate color palette to distinguish between different categories effectively. - Control the range of marker sizes with the `sizes` parameter set to `(20, 200)`. # Input: There is no direct input for this task. You will use the `seaborn` default \\"tips\\" dataset. # Output: One scatter plot and one faceted scatter plot as described. Example: This example illustrates the expected approach but does not include all required elements: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Scatter plot sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\", size=\\"size\\", sizes=(20, 200), legend=\\"full\\" ) plt.show() # Faceted scatter plot sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\" ) plt.show() ``` Ensure your solution includes all code necessary to achieve the output plots, adheres to the constraints, and demonstrates your understanding of the seaborn library.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\", size=\\"size\\", sizes=(20, 200), legend=\\"full\\" ) scatter_plot.set_title(\\"Scatter Plot of Total Bill vs Tip\\") scatter_plot.set_xlabel(\\"Total Bill\\") scatter_plot.set_ylabel(\\"Tip\\") plt.legend(bbox_to_anchor=(1.05, 1), loc=\'upper left\', borderaxespad=0.) plt.show() # Faceted scatter plot facet_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\", sizes=(20, 200) ) facet_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") facet_plot.fig.suptitle(\\"Faceted Scatter Plot of Total Bill vs Tip by Time\\", y=1.02) plt.show()"},{"question":"Objective: Write a Python function that correctly handles various types of string and bytes literals as described in the documentation. The function should take as input a list of strings and return a list of tuples where each tuple contains the original string and its processed form. Problem Statement: Implement a function called `parse_literals` that performs the following tasks: 1. Correctly identifies and processes different types of string literals (regular, raw, and formatted). 2. Correctly identifies and processes bytes literals. 3. Handles string literal concatenation. 4. Correctly processes escape sequences in strings. 5. Identifies replacement fields in formatted strings and performs simple variable substitution. Detailed Requirements: 1. The function should accept a dictionary `vars_dict` (with keys as variable names and values as their corresponding string representation) for performing substitutions in formatted string literals (f-strings). If the substitutions cannot be completely performed due to missing variables, the original f-string should be retained. 2. The function should handle triple-quoted strings and process accordingly, preserving any newlines within them. 3. The function should handle concatenation of adjacent string literals, both regular and formatted. 4. The function should raise an exception for incorrect literals that don\'t follow Python lexical rules. Constraints: - Only ASCII characters are allowed in the input strings. - Assume that escape sequences are limited to those described in the provided documentation. Input Format: - A list of strings (`literals: List[str]`) containing different types of literals. - A dictionary (`vars_dict: Dict[str, str]`) for variable substitution in f-strings. Output Format: - A list of tuples (`List[Tuple[str, str]]`), where each tuple contains the original string literal and its processed form. Example: ```python def parse_literals(literals: List[str], vars_dict: Dict[str, str]) -> List[Tuple[str, str]]: # Your implementation here # Example input literals = [ \\"\'Hello, World!\'\\", \\"r\'RawnString\'\\", \'f\\"Hello, {name}!\\"\', \'b\\"Bytex20Literal\\"\', \\"\'Concatenated\' \' string\'\\" ] vars_dict = {\'name\': \'Alice\'} # Expected output output = [ (\\"\'Hello, World!\'\\", \\"Hello, World!\\"), (\\"r\'RawnString\'\\", \\"RawnString\\"), (\'f\\"Hello, {name}!\\"\', \\"Hello, Alice!\\"), (\'b\\"Bytex20Literal\\"\', b\\"Byte Literal\\"), (\\"\'Concatenated\' \' string\'\\", \\"Concatenated string\\") ] ``` The `parse_literals` function should include comprehensive error handling and correctly process all valid input according to the specifications.","solution":"import re def parse_literals(literals, vars_dict): results = [] for literal in literals: try: if literal.startswith(\\"r\\"): processed = re.sub(r\\"(?!)\\", r\\"\\", literal[2:-1]) elif literal.startswith(\\"b\\"): processed = bytes(eval(literal)) elif literal.startswith(\\"f\\"): pattern = r\\"{([^{}]+)}\\" processed = literal[2:-1] matches = re.findall(pattern, processed) for match in matches: if match in vars_dict: processed = processed.replace(f\\"{{{match}}}\\", vars_dict[match]) elif literal.startswith(\\"\'\\") or literal.startswith(\'\\"\'): processed = eval(literal) else: raise ValueError(\\"Invalid literal\\") results.append((literal, processed)) except Exception as e: results.append((literal, str(e))) return results"},{"question":"**Objective:** Create a custom Python interpreter that supports evaluating expressions, execution of statements, and maintains the state of variables between executions. Your interpreter should handle basic Python constructs (loops, conditional statements, function definitions, etc.). **Task:** Implement a class `CustomInterpreter` using the `code` and `codeop` modules that allows for interactive input and output. Your class should include the following methods: 1. `__init__(self)`: Initializes an interpreter instance. 2. `run_code(self, code: str) -> str`: Executes the provided code string and returns the output or any error messages. **Constraints:** 1. Your `CustomInterpreter` must handle incomplete code inputs and should allow the user to continue entering code until the code block is complete. 2. The interpreter should maintain the state between successive calls to `run_code`. **Input:** - A series of Python code strings provided to `run_code` method. **Output:** - The result of the executed code or an appropriate error message. **Example Usage:** ```python interpreter = CustomInterpreter() output1 = interpreter.run_code(\\"a = 10\\") print(output1) # Expecting no output (None or \\"\\") output2 = interpreter.run_code(\\"print(a)\\") print(output2) # Expecting \\"10\\" output3 = interpreter.run_code(\\"for i in range(3):n print(i)\\") print(output3) # Expecting: # 0 # 1 # 2 output4 = interpreter.run_code(\\"print(b)\\") print(output4) # Expecting an error message since \'b\' is not defined. ``` **Performance Requirements:** The interpreter should handle basic Python code execution efficiently. There are no specific performance constraints for this task, but it should handle typical interactive interpreter use cases smoothly. Use the `code` and `codeop` modules as described in the documentation to implement your solution.","solution":"import code import codeop import sys import traceback from io import StringIO class CustomInterpreter: def __init__(self): # Initialize the variables to store the state of the interpreter self.locals = {} self.interpreter = code.InteractiveInterpreter(self.locals) self.buffer = [] self.compiler = codeop.CommandCompiler() def run_code(self, code: str) -> str: # Capture output and error messages old_stdout = sys.stdout old_stderr = sys.stderr sys.stdout = StringIO() sys.stderr = StringIO() try: # Append code to buffer self.buffer.append(code) source = \\"n\\".join(self.buffer) # Check if the code forms a complete statement compiled_code = self.compiler(source, \\"<stdin>\\", \\"exec\\") if compiled_code: # Execute the complete code self.interpreter.runcode(compiled_code) self.buffer = [] else: return \\"\\" # Incomplete code, waiting for more input except Exception: # Capture any exceptions and return the error message output = traceback.format_exc() else: output = sys.stdout.getvalue() error_output = sys.stderr.getvalue() if error_output: output += error_output finally: sys.stdout = old_stdout sys.stderr = old_stderr return output"},{"question":"Floating-Point Object Manipulation Objective: Design and implement a function `process_floats` in Python that takes a list of inputs and performs various floating-point operations using the functionalities provided by `python310`. Problem Statement: You are given a list of strings and numbers. Implement the following: 1. Convert each string that represents a valid floating-point number to a `PyFloatObject` using `PyFloat_FromString()`. If a conversion fails, ignore the string. 2. Convert each number to a `PyFloatObject` using `PyFloat_FromDouble()`. 3. For each valid `PyFloatObject` created: - Retrieve the float value using `PyFloat_AsDouble()`. - Retrieve the type information using `PyFloat_Check()` and `PyFloat_CheckExact()`. 4. Collect and return a list of dictionaries containing the float value and type information for each valid floating point object. Function Signature: ```python def process_floats(inputs: list) -> list: pass ``` Input: - `inputs` (list): A list of strings and numbers. Example: `[\\"123.45\\", \\"abc\\", 67.89, 100]` Output: - `result` (list): A list of dictionaries. Each dictionary contains: - `value` (float): The floating-point value. - `is_float_object` (bool): True if it is a `PyFloatObject`. - `is_exact_float_object` (bool): True if it is an exact `PyFloatObject`. Example: ```python inputs = [\\"123.45\\", \\"abc\\", 67.89, 100] output = process_floats(inputs) # Expected output: # [ # {\\"value\\": 123.45, \\"is_float_object\\": True, \\"is_exact_float_object\\": True}, # {\\"value\\": 67.89, \\"is_float_object\\": True, \\"is_exact_float_object\\": True}, # {\\"value\\": 100.0, \\"is_float_object\\": True, \\"is_exact_float_object\\": True} # ] ``` Constraints: - You must use the functions provided by the `python310` package. - Handle any exceptions or errors gracefully, ensuring that failing conversions do not stop the processing of the list. - Assume the input list does not contain nested collections. Performance Requirements: - The solution should efficiently handle input lists containing up to 1000 elements. ```python from typing import List, Dict, Union def process_floats(inputs: List[Union[str, float]]) -> List[Dict[str, Union[float, bool]]]: # Your code here pass ```","solution":"from typing import List, Dict, Union def process_floats(inputs: List[Union[str, int, float]]) -> List[Dict[str, Union[float, bool]]]: result = [] for item in inputs: try: if isinstance(item, str): float_value = float(item) else: float_value = float(item) is_float_object = isinstance(float_value, float) float_dict = { \'value\': float_value, \'is_float_object\': is_float_object, \'is_exact_float_object\': is_float_object } result.append(float_dict) except (ValueError, TypeError): # Ignore items that cannot be converted to float continue return result"},{"question":"**Problem Statement:** You are tasked with creating a function that takes an `EmailMessage` object, serializes it using the `email.generator` module, and returns its binary and text representations. Your implementation should demonstrate the use of `BytesGenerator`, `Generator`, and `DecodedGenerator` classes. Additionally, you must write a function to compare the original `EmailMessage` object with the deserialized message to ensure consistency. **Function Signature:** ```python import email from email.policy import default from email.message import EmailMessage from email.generator import BytesGenerator, Generator, DecodedGenerator from io import BytesIO, StringIO def serialize_email_message(msg: EmailMessage) -> dict: pass def compare_email_messages(original_msg: EmailMessage, deserialized_msg: EmailMessage) -> bool: pass ``` # Input: - `msg` (EmailMessage): An `EmailMessage` object that needs to be serialized. # Output: - Returns a dictionary with: - `\'binary_representation\'`: The binary representation of the message using `BytesGenerator`. - `\'text_representation\'`: The text representation of the message using `Generator`. - `\'decoded_representation\'`: A specially formatted text representation of the message using `DecodedGenerator`. # Constraints: 1. Handle potential encoding issues and ensure compliance with standards. 2. The compare function should accurately check for equivalence between the original and deserialized messages, considering possible loss of formatting but maintaining content integrity. # Example: ```python msg = EmailMessage() msg.set_content(\\"This is a test email.\\") msg[\\"Subject\\"] = \\"Test Email\\" msg[\\"From\\"] = \\"sender@example.com\\" msg[\\"To\\"] = \\"receiver@example.com\\" serialized_info = serialize_email_message(msg) binary_representation = serialized_info[\\"binary_representation\\"] text_representation = serialized_info[\\"text_representation\\"] decoded_representation = serialized_info[\\"decoded_representation\\"] # Check equivalence assert compare_email_messages(msg, EmailMessage(policy=default).from_string(text_representation)) ``` # Explanation: - Implement the function `serialize_email_message` to use the `BytesGenerator`, `Generator`, and `DecodedGenerator` to serialize the given `EmailMessage` object. - Implement the `compare_email_messages` to compare original and deserialized messages to ensure that the serialization retains the message integrity, if not the exact formatting. **Note**: Utilize `BytesIO` and `StringIO` to handle binary and text streams respectively.","solution":"import email from email.policy import default from email.message import EmailMessage from email.generator import BytesGenerator, Generator, DecodedGenerator from io import BytesIO, StringIO def serialize_email_message(msg: EmailMessage) -> dict: binary_out = BytesIO() text_out = StringIO() decoded_out = StringIO() # Binary representation using BytesGenerator bytes_gen = BytesGenerator(binary_out, policy=default) bytes_gen.flatten(msg) binary_representation = binary_out.getvalue() # Text representation using Generator text_gen = Generator(text_out, policy=default) text_gen.flatten(msg) text_representation = text_out.getvalue() # Decoded representation using DecodedGenerator decoded_gen = DecodedGenerator(decoded_out, policy=default) decoded_gen.flatten(msg) decoded_representation = decoded_out.getvalue() return { \'binary_representation\': binary_representation, \'text_representation\': text_representation, \'decoded_representation\': decoded_representation } def compare_email_messages(original_msg: EmailMessage, deserialized_msg: EmailMessage) -> bool: return (original_msg.get_content() == deserialized_msg.get_content() and original_msg.keys() == deserialized_msg.keys() and all(original_msg[key] == deserialized_msg[key] for key in original_msg.keys()))"},{"question":"# Task You are given a string containing multiple lines of text. Each line follows a specific format, potentially containing multiple pieces of information separated by a delimiter. Your goal is to write a Python function to achieve the following: 1. **Reformatting the text**: - Extract specific fields from each line. - Reformat these fields into a new string format. 2. **Validating and Extracting Information**: - Use regular expressions to validate the presence of a specific pattern in each line. - Extract and store each valid match after reformatting. 3. **Comparing Differences**: - Compare the original lines with the reformatted lines to highlight the differences. # Function Signature ```python def process_text(input_text: str) -> dict: Processes the input text according to the specified instructions. Parameters: input_text (str): A string containing multiple lines of text to be processed. Returns: dict: A dictionary with the following keys: - \'reformatted\': List of strings, each representing a reformatted line. - \'matches\': List of dictionaries, each containing extracted valid matches. - \'differences\': List of tuples, each containing the original and reformatted lines. ``` # Input - A single string `input_text` consisting of multiple lines. - Each line is formatted as `field1|field2|field3`. - Each field can contain alphanumeric characters and spaces. # Output - A dictionary with three keys: - \'reformatted\': A list of reformatted strings. - \'matches\': A list of dictionaries, with each dictionary containing extracted matches. - \'differences\': A list of tuples, each containing the original and the reformatted lines as `(original, reformatted)`. # Example ```python input_text = Alice|Engineer|Boston Bob|Doctor|New York Charlie|Artist|Los Angeles result = process_text(input_text) print(result) ``` Expected Output ```python { \'reformatted\': [ \'Name: Alice, Profession: Engineer, City: Boston\', \'Name: Bob, Profession: Doctor, City: New York\', \'Name: Charlie, Profession: Artist, City: Los Angeles\' ], \'matches\': [ {\'Name\': \'Alice\', \'Profession\': \'Engineer\', \'City\': \'Boston\'}, {\'Name\': \'Bob\', \'Profession\': \'Doctor\', \'City\': \'New York\'}, {\'Name\': \'Charlie\', \'Profession\': \'Artist\', \'City\': \'Los Angeles\'} ], \'differences\': [ (\'Alice|Engineer|Boston\', \'Name: Alice, Profession: Engineer, City: Boston\'), (\'Bob|Doctor|New York\', \'Name: Bob, Profession: Doctor, City: New York\'), (\'Charlie|Artist|Los Angeles\', \'Name: Charlie, Profession: Artist, City: Los Angeles\') ] } ``` # Constraints - Each line contains exactly three fields separated by the `|` character. - Each field is non-empty and contains only alphanumeric characters and spaces. You should use the `re` module\'s regular expressions for pattern matching and extraction, and the `difflib` module to identify differences between the original and reformatted lines. # Notes - Please ensure to handle edge cases, such as empty input or lines that don\'t conform to the format. - Write clean, readable, and efficient code with appropriate use of functions and libraries.","solution":"import re def process_text(input_text: str) -> dict: Processes the input text according to the specified instructions. Parameters: input_text (str): A string containing multiple lines of text to be processed. Returns: dict: A dictionary with the following keys: - \'reformatted\': List of strings, each representing a reformatted line. - \'matches\': List of dictionaries, each containing extracted valid matches. - \'differences\': List of tuples, each containing the original and reformatted lines. lines = input_text.strip().split(\'n\') reformatted_lines = [] matches = [] differences = [] for line in lines: match = re.match(r\'^(.+)|(.+)|(.+)\', line) if match: name, profession, city = match.groups() reformatted = f\\"Name: {name}, Profession: {profession}, City: {city}\\" reformatted_lines.append(reformatted) matches.append({ \'Name\': name, \'Profession\': profession, \'City\': city }) differences.append((line, reformatted)) return { \'reformatted\': reformatted_lines, \'matches\': matches, \'differences\': differences }"},{"question":"Implementing a Custom Import System Using `imp` The `imp` module helps you understand the internals of Python\'s import mechanism, even though it is deprecated in favor of `importlib`. For this assessment, you are tasked with implementing a custom module reloader that uses `imp` to reload a specific module and maintain a cache for byte-compiled files. Objective: Implement a function `custom_reload(module_name: str) -> str` that: 1. Reloads the specified module using `imp`. 2. Checks for the byte-compiled version of the module in the `__pycache__` directory. 3. If a byte-compiled version exists, use it to reload the module. 4. Return the path of the byte-compiled file used. Input: - `module_name` (str): The name of the module to reload. Output: - Returns a `str` representing the path of the byte-compiled file used. Constraints: - Assume the module specified exists in `sys.path`. - If no byte-compiled file is found, return \\"No byte-compiled file used.\\" Performance Requirements: - The solution should efficiently handle the import and check for the byte-compiled file without causing significant delay. Example Usage: ```python import os def custom_reload(module_name: str) -> str: import imp import sys import os # Remove module from sys.modules to force re-import if module_name in sys.modules: del sys.modules[module_name] try: # Find the module using imp.find_module fp, pathname, description = imp.find_module(module_name) # Load the module using imp.load_module module = imp.load_module(module_name, fp, pathname, description) # Check for PEP 3147 byte-compiled file cache_path = imp.cache_from_source(pathname) # Check if byte-compiled file exists if os.path.exists(cache_path): return cache_path else: return \\"No byte-compiled file used.\\" except ImportError as e: return str(e) finally: if fp: fp.close() # Sample call print(custom_reload(\\"example_module\\")) ``` Implement the function `custom_reload` in a file and test it with different module names present in your environment. Notes: - Handle exceptions gracefully and ensure file pointers are closed properly. - Understand that dealing with deprecated modules is to illustrate understanding the internals; it is recommended to use `importlib` for new code development.","solution":"import os import imp import sys def custom_reload(module_name: str) -> str: Reloads the specified module using imp and checks for byte-compiled file. Args: - module_name (str): The name of the module to reload. Returns: - str: Path of the byte-compiled file used, or an appropriate message if not found. # Remove the module from sys.modules to force re-import. if module_name in sys.modules: del sys.modules[module_name] fp = None try: # Find the module using imp.find_module fp, pathname, description = imp.find_module(module_name) # Load the module using imp.load_module module = imp.load_module(module_name, fp, pathname, description) # Check for PEP 3147 byte-compiled file existence cache_path = imp.cache_from_source(pathname) if os.path.exists(cache_path): return cache_path else: return \\"No byte-compiled file used.\\" except ImportError as e: return str(e) finally: if fp: fp.close()"},{"question":"You are given sales data of a retail company in two separate CSV files. The company sells products in various regions and has recorded data over multiple years. **File descriptions:** 1. `sales_data.csv` - Contains details on sales transactions. 2. `product_data.csv` - Contains details about each product. The structure of the CSV files is as follows: `sales_data.csv`: ``` transaction_id | product_id | date | region | quantity | price ---------------|------------|------------|-----------|----------|------- 101 | A001 | 2021-01-05 | North | 10 | 15.0 102 | A002 | 2021-01-06 | West | 5 | 25.0 ... | ... | ... | ... | ... | ... ``` `product_data.csv`: ``` product_id | product_name | category -----------|---------------|------------- A001 | Widget | Gadgets A002 | SuperWidget | Gadgets ... | ... | ... ``` **Tasks:** 1. Load the two datasets into pandas DataFrames. 2. Merge these DataFrames on the `product_id` to get a combined dataset. 3. Clean the data to handle missing values. Replace missing `quantity` and `price` values with `0`. 4. Calculate the total sales amount for each product (i.e., `quantity * price`). 5. Generate a summary report that provides: * Total sales amount for each region. * Top 5 products by total sales amount across all regions. * Total sales amount for each category. **Expected Input and Output:** - **Input:** Two CSV files (`sales_data.csv` and `product_data.csv`). - **Output:** A tuple containing: 1. DataFrame with total sales amount for each region. 2. DataFrame with the top 5 products by total sales amount. 3. DataFrame with total sales amount for each category. **Constraints:** - Handle potentially large datasets efficiently. - Ensure the solution is correct and handles missing data appropriately. ```python import pandas as pd def sales_summary(sales_file: str, product_file: str): This function takes in the paths to two CSV files containing sales data and product details, then processes and generates a summary report. :param sales_file: str - Path to the sales CSV file. :param product_file: str - Path to the product CSV file. :return: tuple - (region_sales_df, top_products_df, category_sales_df) region_sales_df: DataFrame containing total sales amount for each region. top_products_df: DataFrame containing the top 5 products by total sales amount. category_sales_df: DataFrame containing total sales amount for each category. # Load the datasets sales_data = pd.read_csv(sales_file) product_data = pd.read_csv(product_file) # Merge datasets on \'product_id\' merged_data = pd.merge(sales_data, product_data, on=\'product_id\') # Handle missing values merged_data[\'quantity\'].fillna(0, inplace=True) merged_data[\'price\'].fillna(0, inplace=True) # Calculate total sales amount merged_data[\'total_sales\'] = merged_data[\'quantity\'] * merged_data[\'price\'] # Total sales amount for each region region_sales_df = merged_data.groupby(\'region\').agg({\'total_sales\': \'sum\'}).reset_index() # Top 5 products by total sales amount top_products_df = merged_data.groupby([\'product_id\', \'product_name\']).agg({\'total_sales\': \'sum\'}).reset_index() top_products_df = top_products_df.sort_values(by=\'total_sales\', ascending=False).head(5) # Total sales amount for each category category_sales_df = merged_data.groupby(\'category\').agg({\'total_sales\': \'sum\'}).reset_index() return region_sales_df, top_products_df, category_sales_df # You can use this function to check your solution with example data: # region_sales, top_products, category_sales = sales_summary(\'sales_data.csv\', \'product_data.csv\') ``` **Note:** - Ensure to adhere to the structure and guidelines while implementing the function. - Submit your `.py` or notebook file with the implemented function along with a brief documentation on how it works.","solution":"import pandas as pd def sales_summary(sales_file: str, product_file: str): This function takes in the paths to two CSV files containing sales data and product details, then processes and generates a summary report. :param sales_file: str - Path to the sales CSV file. :param product_file: str - Path to the product CSV file. :return: tuple - (region_sales_df, top_products_df, category_sales_df) region_sales_df: DataFrame containing total sales amount for each region. top_products_df: DataFrame containing the top 5 products by total sales amount. category_sales_df: DataFrame containing total sales amount for each category. # Load the datasets sales_data = pd.read_csv(sales_file) product_data = pd.read_csv(product_file) # Merge datasets on \'product_id\' merged_data = pd.merge(sales_data, product_data, on=\'product_id\') # Handle missing values merged_data[\'quantity\'].fillna(0, inplace=True) merged_data[\'price\'].fillna(0, inplace=True) # Calculate total sales amount merged_data[\'total_sales\'] = merged_data[\'quantity\'] * merged_data[\'price\'] # Total sales amount for each region region_sales_df = merged_data.groupby(\'region\').agg({\'total_sales\': \'sum\'}).reset_index() # Top 5 products by total sales amount top_products_df = merged_data.groupby([\'product_id\', \'product_name\']).agg({\'total_sales\': \'sum\'}).reset_index() top_products_df = top_products_df.sort_values(by=\'total_sales\', ascending=False).head(5) # Total sales amount for each category category_sales_df = merged_data.groupby(\'category\').agg({\'total_sales\': \'sum\'}).reset_index() return region_sales_df, top_products_df, category_sales_df"},{"question":"# Question Write a Python function `find_subnets(addresses)` that takes a list of IP address strings and analyzes them to determine how many distinct subnets they belong to. Each address will be accompanied by a subnet mask in the CIDR notation (e.g., `192.0.2.4/24`). Use the `ipaddress` module to parse and process the IP addresses and subnets. Function Signature ```python def find_subnets(addresses: list[str]) -> int: pass ``` Input - A list of strings, where each string represents an IP address in CIDR notation (e.g., `[\'192.0.2.4/24\', \'192.0.2.5/24\', \'10.0.0.1/8\']`). Output - An integer representing the number of distinct subnets. Constraints - Each IP address in the input list will be valid and correctly formatted in CIDR notation. - The list will have at least one address and at most 100 addresses. Example ```python addresses = [ \'192.0.2.4/24\', \'192.0.2.5/24\', \'10.0.0.1/8\', \'10.0.0.2/8\', \'192.0.2.4/25\' ] assert find_subnets(addresses) == 3 ``` Explanation In the given list of addresses, there are three distinct subnets: 1. `192.0.2.0/24` (includes both `192.0.2.4/24` and `192.0.2.5/24`) 2. `10.0.0.0/8` (includes both `10.0.0.1/8` and `10.0.0.2/8`) 3. `192.0.2.0/25` (includes `192.0.2.4/25`) The function returns 3 since there are 3 distinct subnets in the list provided. Notes - Make ample use of the `ipaddress` module for parsing and managing IP addresses and networks. - Handle performance efficiently within the input constraints.","solution":"from ipaddress import ip_network def find_subnets(addresses): Returns the number of distinct subnets from a list of IP address strings in CIDR notation. subnets = set() for address in addresses: network = ip_network(address, strict=False) subnets.add(network) return len(subnets)"},{"question":"**Calendar Data Manipulation and Formatting** **Objective**: Write a Python function that generates a custom formatted string of a yearly calendar for a given year, including information about leap years and the number of days in each month. The calendar should be generated using the `calendar` module and should conform to a specific format specified below. **Function Signature**: ```python def custom_year_calendar(year: int) -> str: pass ``` **Input**: - `year`: An integer representing the year for which the calendar needs to be generated. It should be between the years 1900 and 2100 (inclusive). **Output**: - A string representing the formatted yearly calendar. **Format Specification**: 1. The output string should start with \\"Year: `<year>`\\". 2. It should specify whether the year is a leap year, with the statement \\"Leap Year: `<True/False>`\\". 3. For each month, it should list the name of the month, the number of days in that month, and a miniature text-formatted calendar (using `calendar.TextCalendar`). 4. The miniature calendar for each month should be formatted to display days of the week names as headers and should fill dates day by day. 5. Months and their details should be separated by double newline characters (`nn`). **Example**: ```python >>> print(custom_year_calendar(2023)) Year: 2023 Leap Year: False January Number of days: 31 January 2023 Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 February Number of days: 28 February 2023 Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ... and so on for all months. ``` **Constraints**: 1. You must use the `calendar` module to generate the calendars and determine leap years. 2. Ensure your function handles edge cases such as leap years correctly. 3. Be efficient in your solution, aiming for optimal performance within the given constraints. **Tip**: 1. Use `calendar.TextCalendar` to generate string representations of months. 2. Use provided functions like `calendar.isleap` and `calendar.monthrange`. 3. Iteratively compile the final string by appending each month\'s details.","solution":"import calendar def custom_year_calendar(year: int) -> str: Generates a custom formatted string of a yearly calendar for the given year. Parameters: year (int): The year for which the calendar needs to be generated (between 1900 and 2100 inclusive). Returns: str: A string representing the formatted yearly calendar. if year < 1900 or year > 2100: raise ValueError(\\"Year must be between 1900 and 2100 inclusive\\") cal = calendar.TextCalendar(calendar.MONDAY) is_leap = calendar.isleap(year) output = f\\"Year: {year}nLeap Year: {is_leap}nn\\" for month in range(1, 13): month_name = calendar.month_name[month] days_in_month = calendar.monthrange(year, month)[1] month_cal = cal.formatmonth(year, month) output += f\\"{month_name}nNumber of days: {days_in_month}n{month_cal}nn\\" return output.strip()"},{"question":"# Coding Assessment: Comprehensive Data Handling and Manipulation in Python **Objective**: Demonstrate your understanding and ability to work with various fundamental and advanced Python built-in types, operations, and methods. **Problem Statement**: You need to create a utility function called `process_data` that takes multiple inputs of different types and returns a structured result. The inputs will include integers, floating-point numbers, strings, lists, tuples, dictionaries, sets, and context with custom processing based on certain conditions. Here\'s the detailed specification for the function: ```python def process_data(int_list, float_num, str_data, tuple_data, dict_data, set_data, context_data): Processes the given data types and returns a structured result. Arguments: - int_list: List of integers - float_num: A floating-point number - str_data: A string - tuple_data: A tuple containing mixed data types - dict_data: A dictionary with string keys and integer values - set_data: A set of integers - context_data: A context manager that provides scoped resource management Returns: A dictionary with keys \'sum_ints\', \'rounded_float\', \'title_str\', \'tuple_summary\', \'filtered_dict\', \'set_operations\' and \'context_result\' detailed as follows: - \'sum_ints\': The sum of the integers in int_list. - \'rounded_float\': The float_num rounded to 2 decimal places. - \'title_str\': The str_data with each word capitalized. - \'tuple_summary\': A dictionary containing: - \'length\': Length of the tuple_data. - \'first_element\': The first element in tuple_data if present, otherwise `None`. - \'last_element\': The last element in tuple_data if present, otherwise `None`. - \'filtered_dict\': A new dictionary containing items from dict_data where the value is even. - \'set_operations\': A dictionary containing: - \'intersection\': The intersection of set_data with the set of integers {1, 2, 3, 4, 5}. - \'union\': The union of set_data with the set of integers {6, 7, 8, 9, 10}. - \'context_result\': The result of executing some operation within the provided context_data manager. ``` **Function Constraints**: - All elements in `int_list` and `set_data` will be valid integers. - `float_num` will be a valid floating-point number. - `str_data` will be a valid string. - `tuple_data` will be a tuple which may contain mixed data types. - `dict_data` will have string keys and integer values. - `context_data` will follow the context manager protocol. **Context Manager Requirement**: Provide an example of `context_data` which includes some resource management. For example, managing a file read or a resource that needs opening and closing. # Example Usage ```python from contextlib import contextmanager @contextmanager def sample_context_manager(): print(\\"Entering context\\") yield \\"Context managed resource\\" print(\\"Exiting context\\") # Example arguments int_list = [1, 2, 3, 4, 5] float_num = 12.3456 str_data = \\"hello python world\\" tuple_data = (\'a\', 2, 3.5, True) dict_data = {\'one\': 1, \'two\': 2, \'three\': 3, \'four\': 4} set_data = {1, 2, 3, 6, 7} context_data = sample_context_manager() result = process_data(int_list, float_num, str_data, tuple_data, dict_data, set_data, context_data) print(result) ``` **Expected Output**: ```python { \'sum_ints\': 15, \'rounded_float\': 12.35, \'title_str\': \'Hello Python World\', \'tuple_summary\': { \'length\': 4, \'first_element\': \'a\', \'last_element\': True }, \'filtered_dict\': {\'two\': 2, \'four\': 4}, \'set_operations\': { \'intersection\': {1, 2, 3}, \'union\': {1, 2, 3, 6, 7, 8, 9, 10} }, \'context_result\': \'Context managed resource\' } ``` **Note**: Make sure your implementation follows Python best practices and handles edge cases appropriately.","solution":"def process_data(int_list, float_num, str_data, tuple_data, dict_data, set_data, context_data): Processes the given data types and returns a structured result. Arguments: - int_list: List of integers - float_num: A floating-point number - str_data: A string - tuple_data: A tuple containing mixed data types - dict_data: A dictionary with string keys and integer values - set_data: A set of integers - context_data: A context manager that provides scoped resource management Returns: A dictionary with keys \'sum_ints\', \'rounded_float\', \'title_str\', \'tuple_summary\', \'filtered_dict\', \'set_operations\' and \'context_result\'. # Summing integers in the list sum_ints = sum(int_list) # Rounding the float number to 2 decimal places rounded_float = round(float_num, 2) # Converting string to title case title_str = str_data.title() # Creating tuple summary tuple_summary = { \'length\': len(tuple_data), \'first_element\': tuple_data[0] if tuple_data else None, \'last_element\': tuple_data[-1] if tuple_data else None } # Filtering dictionary for even values filtered_dict = {k: v for k, v in dict_data.items() if v % 2 == 0} # Performing set operations intersection_set = {1, 2, 3, 4, 5} union_set = {6, 7, 8, 9, 10} set_operations = { \'intersection\': set_data.intersection(intersection_set), \'union\': set_data.union(union_set) } # Using the context manager with context_data as context_result: result_context = context_result return { \'sum_ints\': sum_ints, \'rounded_float\': rounded_float, \'title_str\': title_str, \'tuple_summary\': tuple_summary, \'filtered_dict\': filtered_dict, \'set_operations\': set_operations, \'context_result\': result_context }"},{"question":"Objective Implement a program that processes data involving lists, sets, dictionaries, and tuples using various Python methods and functionalities, as described in the documentation. Problem Description You are provided with a list of students and their corresponding scores in different subjects. Your task is to implement functions to perform the following operations: 1. **Create a Subject-Wise Average Score Dictionary**: - Input: List of tuples where each tuple contains a student\'s name and a dictionary of their scores in different subjects. ```python students_scores = [ (\\"Alice\\", {\\"math\\": 85, \\"science\\": 92, \\"history\\": 78}), (\\"Bob\\", {\\"math\\": 90, \\"science\\": 88, \\"history\\": 84}), (\\"Charlie\\", {\\"math\\": 95, \\"science\\": 85, \\"history\\": 91}), (\\"David\\", {\\"math\\": 78, \\"science\\": 84, \\"history\\": 90}), (\\"Eva\\", {\\"math\\": 82, \\"science\\": 89, \\"history\\": 88}), ] ``` - Output: Dictionary where keys are subject names, and values are the average scores of all students in that subject. ```python { \\"math\\": 86.0, \\"science\\": 87.6, \\"history\\": 86.2 } ``` 2. **Identify Top Scoring Student in Each Subject**: - Input: Same as above. - Output: Dictionary where keys are subject names, and values are tuples containing the top-scoring student\'s name and their score in that subject. ```python { \\"math\\": (\\"Charlie\\", 95), \\"science\\": (\\"Alice\\", 92), \\"history\\": (\\"Charlie\\", 91) } ``` 3. **Generate a Set of All Unique Scores**: - Input: Same as above. - Output: Set containing all unique scores from all students across all subjects. ```python {85, 88, 78, 84, 90, 89, 91, 82, 92, 95} ``` 4. **Sort Students by Their Average Score**: - Input: Same as above. - Output: List of tuples where each tuple contains a student\'s name and their average score, sorted in descending order of their average scores. ```python [ (\\"Charlie\\", 90.33), (\\"Eva\\", 86.33), (\\"Bob\\", 87.33), (\\"Alice\\", 85.0), (\\"David\\", 84.0), ] ``` Constraints 1. All scores are integers between 0 and 100. 2. Each student has scores in the same predefined set of subjects: \\"math\\", \\"science\\", \\"history\\". 3. The number of students (n) is between 1 and 1000. 4. The output for average scores should be rounded to two decimal places. Function Signatures You should implement the following functions: - `def calculate_subject_averages(students_scores: List[Tuple[str, Dict[str, int]]]) -> Dict[str, float]:` - `def top_scorer_per_subject(students_scores: List[Tuple[str, Dict[str, int]]]) -> Dict[str, Tuple[str, int]]:` - `def unique_scores(students_scores: List[Tuple[str, Dict[str, int]]]) -> Set[int]:` - `def sort_students_by_average(students_scores: List[Tuple[str, Dict[str, int]]]) -> List[Tuple[str, float]]:` Example Usage ```python students_scores = [ (\\"Alice\\", {\\"math\\": 85, \\"science\\": 92, \\"history\\": 78}), (\\"Bob\\", {\\"math\\": 90, \\"science\\": 88, \\"history\\": 84}), (\\"Charlie\\", {\\"math\\": 95, \\"science\\": 85, \\"history\\": 91}), (\\"David\\", {\\"math\\": 78, \\"science\\": 84, \\"history\\": 90}), (\\"Eva\\", {\\"math\\": 82, \\"science\\": 89, \\"history\\": 88}), ] # Function outputs print(calculate_subject_averages(students_scores)) # Output: {\'math\': 86.0, \'science\': 87.6, \'history\': 86.2} print(top_scorer_per_subject(students_scores)) # Output: {\'math\': (\'Charlie\', 95), \'science\': (\'Alice\', 92), \'history\': (\'Charlie\', 91)} print(unique_scores(students_scores)) # Output: {85, 88, 78, 84, 90, 89, 91, 82, 92, 95} print(sort_students_by_average(students_scores)) # Output: [(\'Charlie\', 90.33), (\'Eva\', 86.33), (\'Bob\', 87.33), (\'Alice\', 85.0), (\'David\', 84.0)] ```","solution":"from typing import List, Tuple, Dict, Set def calculate_subject_averages(students_scores: List[Tuple[str, Dict[str, int]]]) -> Dict[str, float]: subject_sums = {} subject_counts = {} for _, scores in students_scores: for subject, score in scores.items(): if subject in subject_sums: subject_sums[subject] += score subject_counts[subject] += 1 else: subject_sums[subject] = score subject_counts[subject] = 1 averages = {subject: round(subject_sums[subject] / subject_counts[subject], 2) for subject in subject_sums} return averages def top_scorer_per_subject(students_scores: List[Tuple[str, Dict[str, int]]]) -> Dict[str, Tuple[str, int]]: top_scorers = {} for student, scores in students_scores: for subject, score in scores.items(): if subject in top_scorers: if score > top_scorers[subject][1]: top_scorers[subject] = (student, score) else: top_scorers[subject] = (student, score) return top_scorers def unique_scores(students_scores: List[Tuple[str, Dict[str, int]]]) -> Set[int]: scores_set = set() for _, scores in students_scores: for score in scores.values(): scores_set.add(score) return scores_set def sort_students_by_average(students_scores: List[Tuple[str, Dict[str, int]]]) -> List[Tuple[str, float]]: student_averages = [] for student, scores in students_scores: avg_score = round(sum(scores.values()) / len(scores), 2) student_averages.append((student, avg_score)) student_averages.sort(key=lambda x: x[1], reverse=True) return student_averages"},{"question":"**Title:** Implement a Custom Class Using Multiple Built-in Types **Objective:** Demonstrate your understanding of Python\'s built-in types (`set`, `list`, `dict`, `str`, numeric types) and their methods by creating a class `EntityTracker`. This class will be used to track and manage different entities with their attributes and associations. **Description:** You are required to implement a class `EntityTracker` which maintains and manages entities. Each entity has a unique identifier, a set of attributes, and can be associated with other entities. The class should support the following functionalities: 1. **Add Entity**: Add a new entity with a unique identifier and a set of attributes. If an entity with the same identifier already exists, update its attributes. 2. **Remove Entity**: Remove an existing entity by its identifier. 3. **Update Attributes**: Update the attributes of an existing entity. 4. **Add Association**: Create an association between two entities. 5. **Remove Association**: Remove an association between two entities. 6. **Get Attributes**: Retrieve the attributes of an entity given its identifier. 7. **Get Associations**: Retrieve a set of entities associated with a given entity. **Class Definition and Methods:** Define the `EntityTracker` class with the following methods: 1. `__init__(self)`: Initializes an empty `EntityTracker`. 2. `add_entity(self, identifier: str, attributes: set)`: Adds or updates an entity with the given identifier and attributes. 3. `remove_entity(self, identifier: str)`: Removes an entity with the given identifier. Ensure that all associations involving this entity are also removed. 4. `update_attributes(self, identifier: str, attributes: set)`: Updates the attributes of the given entity. 5. `add_association(self, identifier1: str, identifier2: str)`: Creates an association between two entities identified by `identifier1` and `identifier2`. 6. `remove_association(self, identifier1: str, identifier2: str)`: Removes the association between two entities identified by `identifier1` and `identifier2`. 7. `get_attributes(self, identifier: str) -> set`: Returns the attributes of the entity identified by `identifier`. 8. `get_associations(self, identifier: str) -> set`: Returns a set of identifiers associated with the given entity. **Constraints:** 1. Each identifier is a non-empty string and unique. 2. Attributes are stored as sets of strings. 3. Associations are bidirectional, meaning `add_association(\'A\', \'B\')` implies both A is associated with B and B is associated with A. 4. Raise a `KeyError` if any operation queries a non-existing entity except for adding an entity. **Example Usage:** ```python tracker = EntityTracker() # Add entities tracker.add_entity(\\"User1\\", {\\"name\\", \\"email\\"}) tracker.add_entity(\\"User2\\", {\\"name\\", \\"age\\"}) # Update attributes tracker.update_attributes(\\"User1\\", {\\"name\\", \\"phone\\"}) # Add association tracker.add_association(\\"User1\\", \\"User2\\") # Get attributes print(tracker.get_attributes(\\"User1\\")) # Output: {\\"name\\", \\"phone\\"} # Get associations print(tracker.get_associations(\\"User1\\")) # Output: {\\"User2\\"} # Remove association tracker.remove_association(\\"User1\\", \\"User2\\") # Remove entity tracker.remove_entity(\\"User1\\") ``` Implement the `EntityTracker` class to fulfill the described functionality. **Submission:** Provide the complete implementation of the `EntityTracker` class along with test cases demonstrating its functionality.","solution":"class EntityTracker: def __init__(self): self.entities = {} # Dictionary to hold entity attributes self.associations = {} # Dictionary to hold entity associations def add_entity(self, identifier: str, attributes: set): self.entities[identifier] = attributes if identifier not in self.associations: self.associations[identifier] = set() def remove_entity(self, identifier: str): if identifier not in self.entities: raise KeyError(f\\"Entity {identifier} not found\\") del self.entities[identifier] # Remove all associations for this entity associated_entities = self.associations.pop(identifier, None) if associated_entities: for assoc in associated_entities: self.associations[assoc].remove(identifier) def update_attributes(self, identifier: str, attributes: set): if identifier not in self.entities: raise KeyError(f\\"Entity {identifier} not found\\") self.entities[identifier] = attributes def add_association(self, identifier1: str, identifier2: str): if identifier1 not in self.entities or identifier2 not in self.entities: raise KeyError(f\\"One or both entities {identifier1}, {identifier2} not found\\") self.associations[identifier1].add(identifier2) self.associations[identifier2].add(identifier1) def remove_association(self, identifier1: str, identifier2: str): if identifier1 not in self.entities or identifier2 not in self.entities: raise KeyError(f\\"One or both entities {identifier1}, {identifier2} not found\\") self.associations[identifier1].discard(identifier2) self.associations[identifier2].discard(identifier1) def get_attributes(self, identifier: str) -> set: if identifier not in self.entities: raise KeyError(f\\"Entity {identifier} not found\\") return self.entities[identifier] def get_associations(self, identifier: str) -> set: if identifier not in self.entities: raise KeyError(f\\"Entity {identifier} not found\\") return self.associations[identifier]"},{"question":"Objective Design and implement a custom Python object in C, using the provided structures and macros from the documentation. Your custom object should represent a mathematical vector with basic operations like addition, subtraction, and magnitude calculation. Specifications 1. **Define the Vector Object**: - Create a new type representing a mathematical vector. - Use `PyObject_HEAD` for the base structure of the object. 2. **Attributes**: - The vector should have three double attributes, `x`, `y`, and `z`, representing the vector components. 3. **Methods**: - Implement the following methods for the vector object: - **add**: Adds two vectors. - **sub**: Subtracts one vector from another. - **magnitude**: Calculates the magnitude of the vector. 4. **Implementation**: - Use the appropriate `PyCFunction` types and flags for method definitions. - Ensure proper reference counting and memory management. 5. **Initialization**: - The vector should be initialized with three double values for `x`, `y`, and `z`. - Example: `Vector(1.0, 2.0, 3.0)` Expected Input and Output - **Input**: Vectors with double attributes. - **Output**: A new vector resulting from addition/subtraction or a double value representing magnitude. Constraints - You must use the `PyObject` structures and macros provided in the documentation. - Ensure all errors are handled gracefully, returning appropriate Python exceptions. Example Usage in Python ```python from vector_module import Vector v1 = Vector(1.0, 2.0, 3.0) v2 = Vector(3.0, 2.0, 1.0) v3 = v1.add(v2) # Vector(4.0, 4.0, 4.0) v4 = v1.sub(v2) # Vector(-2.0, 0.0, 2.0) m = v1.magnitude() # 3.7416573867739413 ``` You must provide the complete code for the custom Python object in C, including compilation instructions and any necessary setup.","solution":"class Vector: def __init__(self, x, y, z): self.x = x self.y = y self.z = z def add(self, other): return Vector(self.x + other.x, self.y + other.y, self.z + other.z) def sub(self, other): return Vector(self.x - other.x, self.y - other.y, self.z - other.z) def magnitude(self): return (self.x ** 2 + self.y ** 2 + self.z ** 2) ** 0.5"},{"question":"Objective Implement a Python function to identify and extract specific patterns from a given text using regular expressions (`re` module). This will test the students\' understanding of regular expressions and their ability to apply them in practice. Problem Statement You are tasked with writing a function `extract_dates` that finds all dates in a given text. The dates should be in the `dd-mm-yyyy` format, where: - `dd` is a two-digit day. - `mm` is a two-digit month. - `yyyy` is a four-digit year. The function should return a list of all matching dates found in the text. Function Signature ```python def extract_dates(text: str) -> list: pass ``` Input - `text` (str): A string containing multiple words and dates in the format `dd-mm-yyyy`. Output - (list): A list of strings, where each string is a date found in the input text. Constraints - The function should use Python\'s `re` module for pattern matching. - The dates in the text are separated by various non-alphabetic characters (e.g., spaces, commas, periods, etc.). - The function should return an empty list if no dates in the specified format are found. - You may assume that the input text will always be a valid string. Example ```python # Example 1 input_text = \\"Today\'s date is 12-09-2023. The project deadline is 01-01-2024.\\" print(extract_dates(input_text)) # Expected Output: [\'12-09-2023\', \'01-01-2024\'] # Example 2 input_text = \\"No dates here!\\" print(extract_dates(input_text)) # Expected Output: [] # Example 3 input_text = \\"The event will be held on 30-11-2023, and the next meeting is on 28-02-2024.\\" print(extract_dates(input_text)) # Expected Output: [\'30-11-2023\', \'28-02-2024\'] ``` Performance Requirements - The function should efficiently handle large input texts up to a size of 10^6 characters. Notes - Ensure to include test cases covering various scenarios, including edge cases, to validate your implementation.","solution":"import re def extract_dates(text: str) -> list: Extracts all dates in the format dd-mm-yyyy from the given text. Args: text (str): Input text containing multiple words and dates. Returns: list: A list of strings, each representing a date found in the text. # Regex pattern to match dates in dd-mm-yyyy format date_pattern = r\'bd{2}-d{2}-d{4}b\' # Find all dates in the text matching the pattern dates = re.findall(date_pattern, text) return dates"},{"question":"# Descriptor for Value Restrictions In this task, you are required to implement a custom descriptor that enforces specific value restrictions on an attribute. The descriptor must ensure that any assigned values meet certain criteria, otherwise, it should raise a relevant exception. Requirements: 1. **Descriptor Class: RestrictedValue** - **`__init__(self, min_value=None, max_value=None):`** Initialize the descriptor with optional minimum (`min_value`) and maximum (`max_value`) constraints. - **`__set_name__(self, owner, name):`** Method to store the variable name as `_name` for logging purposes. - **`__get__(self, instance, owner):`** Method to retrieve the value from the instance\'s private attribute. - **`__set__(self, instance, value):`** Method to set the value in the instance\'s private attribute while enforcing the constraints. Raise a `ValueError` if the value is out of bounds. 2. **Class to Use the Descriptor: Product** - This class will have an attribute `price` which uses the `RestrictedValue` descriptor. - The `RestrictedValue` for `price` should have minimum value set to 0. Implementation Details: 1. Implement the `RestrictedValue` descriptor class. 2. Implement the `Product` class that uses the `RestrictedValue` descriptor for its `price` attribute. 3. Ensure that attempting to set a price below 0 raises a `ValueError`. Example Usage: ```python class RestrictedValue: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner): return instance.__dict__.get(self._name) def __set__(self, instance, value): if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {self._name} to be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {self._name} to be no more than {self.max_value}\\") instance.__dict__[self._name] = value class Product: price = RestrictedValue(min_value=0) def __init__(self, price): self.price = price # Usage p1 = Product(price=25) print(p1.price) # Expected output: 25 p2 = Product(price=-5) # Expected to raise ValueError ``` Constraints: - The `min_value` and `max_value` parameters for `RestrictedValue` are optional. - If a value does not meet the constraints, raise a `ValueError` with an appropriate message. **Note:** Ensure that you handle edge cases such as setting the `price` to exactly the `min_value` or `max_value` if they are specified.","solution":"class RestrictedValue: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner): return instance.__dict__.get(self._name) def __set__(self, instance, value): if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {self._name} to be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {self._name} to be no more than {self.max_value}\\") instance.__dict__[self._name] = value class Product: price = RestrictedValue(min_value=0) def __init__(self, price): self.price = price"},{"question":"**Question: Implement Recursive Copier** Your task is to implement a function called `recursive_copier` that recursively copies a directory tree from a source directory to a destination directory while selectively copying files based on their extensions. The function should utilize relevant functions from the `shutil` module. **Function Signature:** ```python def recursive_copier(src: str, dst: str, extensions: list, symlinks: bool = False, ignore_dangling_symlinks: bool = False) -> str: ``` **Parameters:** - `src` (str): The source directory to copy. - `dst` (str): The destination directory where the source directory tree should be copied. - `extensions` (list): A list of file extensions to be copied. If a file does not have an extension listed in this, it should be ignored. - `symlinks` (bool, optional): If `True`, symbolic links will be represented as symbolic links in the new tree. If `False` (default), the contents and metadata of the linked files are copied. - `ignore_dangling_symlinks` (bool, optional): If `True`, ignore dangling symbolic links (default is `False`). **Returns:** - (str): The path to the newly created destination directory. **Constraints and Requirements:** - Use appropriate `shutil` functions for performing the copy operation. - Handle errors gracefully by raising appropriate Python exceptions. - Ensure that only files matching the specified extensions are copied. - The function should ensure the directory structure from the source is replicated in the destination. - Metadata preservation is not required except as managed by the `shutil` functions used. - Handle platform-specific behavior and optimize performance by using the available functionalities in `shutil`. **Example Usage:** ```python src_dir = \'/path/to/source\' dst_dir = \'/path/to/destination\' extensions_to_copy = [\'.txt\', \'.py\'] # Recursively copy source directory to destination, including only .txt and .py files. result = recursive_copier(src_dir, dst_dir, extensions_to_copy) print(f\\"Directory copied to: {result}\\") ``` In this example, `recursive_copier` will copy all `.txt` and `.py` files from the source directory to the destination, maintaining the directory structure. **Notes:** - You should test your implementation thoroughly. - The function should be robust and handle edge cases such as non-existing source paths, permissions issues, etc. - Make sure to review the `shutil` documentation to leverage its features effectively.","solution":"import os import shutil def recursive_copier(src, dst, extensions, symlinks=False, ignore_dangling_symlinks=False): Recursively copies a directory tree from a source directory to a destination directory while selectively copying files based on their extensions. Parameters: - src (str): The source directory to copy. - dst (str): The destination directory where the source directory tree should be copied. - extensions (list): A list of file extensions to be copied. If a file does not have an extension listed in this, it should be ignored. - symlinks (bool): If True, symbolic links will be represented as symbolic links in the new tree. If False (default), the contents and metadata of the linked files are copied. - ignore_dangling_symlinks (bool): If True, ignore dangling symbolic links (default is False). Returns: - (str): The path to the newly created destination directory. # Function to filter files based on provided extensions def filter_files(dir, files): return [f for f in files if os.path.splitext(f)[1] in extensions] # Walk through the source directory and create corresponding structure in the destination for root, dirs, files in os.walk(src, topdown=True, followlinks=symlinks): # Compute destination directory path relative_path = os.path.relpath(root, src) dest_dir = os.path.join(dst, relative_path) # Create the destination directory if it doesn\'t exist os.makedirs(dest_dir, exist_ok=True) # Apply file extensions filter filtered_files = filter_files(root, files) # Copy each file in the filtered list to the corresponding destination directory for file in filtered_files: src_file = os.path.join(root, file) dest_file = os.path.join(dest_dir, file) if symlinks and os.path.islink(src_file): linkto = os.readlink(src_file) if ignore_dangling_symlinks and not os.path.exists(linkto): continue os.symlink(linkto, dest_file) else: shutil.copy2(src_file, dest_file) # Copy directories (leave to the next iteration of os.walk) return dst"},{"question":"Objective Design a system where multiple processes can communicate and share data using Python\'s `multiprocessing.shared_memory` module. Problem Statement Write a Python program to create a shared memory block that processes can use to store and retrieve data. You need to implement the following functions: 1. **initialize_shared_memory(size: int) -> str**: This function should initialize a shared memory block of the given `size` in bytes and return the name of the shared memory block. 2. **write_to_shared_memory(name: str, data: bytes) -> None**: This function should write the given `data` (in bytes) to the shared memory block identified by `name`. 3. **read_from_shared_memory(name: str, size: int) -> bytes**: This function should read `size` bytes of data from the shared memory block identified by `name` and return it. 4. **clean_shared_memory(name: str) -> None**: This function should properly close and unlink the shared memory block identified by `name`. Input and Output Formats - **initialize_shared_memory(size: int) -> str** - **Input**: An integer `size` representing the size of the shared memory block in bytes. - **Output**: A string that is the unique name of the created shared memory block. - **write_to_shared_memory(name: str, data: bytes) -> None** - **Input**: - `name`: A string that is the unique name of the shared memory block. - `data`: A bytes object containing the data to be written. - **Output**: None. - **read_from_shared_memory(name: str, size: int) -> bytes** - **Input**: - `name`: A string that is the unique name of the shared memory block. - `size`: An integer representing the number of bytes to be read. - **Output**: A bytes object containing the data read from the shared memory block. - **clean_shared_memory(name: str) -> None** - **Input**: `name`: A string that is the unique name of the shared memory block. - **Output**: None. Constraints - Size of the shared memory block must be a positive integer. - Data written to the shared memory must not exceed its allocated size. - Ensure proper cleanup by closing and unlinking the shared memory block. Example Usage ```python if __name__ == \\"__main__\\": size = 1024 name = initialize_shared_memory(size) data = b\\"This is a test message.\\" write_to_shared_memory(name, data) read_data = read_from_shared_memory(name, len(data)) print(read_data) # Output should be: b\'This is a test message.\' clean_shared_memory(name) ``` # Notes - You may use the `multiprocessing.shared_memory` module to implement the shared memory operations. - The `data` written and read should be in bytes format. - Make sure to handle any exceptions or errors that may occur during the operations.","solution":"from multiprocessing import shared_memory def initialize_shared_memory(size: int) -> str: Initializes a shared memory block of the given size in bytes. Returns the name of the shared memory block. shm = shared_memory.SharedMemory(create=True, size=size) return shm.name def write_to_shared_memory(name: str, data: bytes) -> None: Writes the given data (in bytes) to the shared memory block identified by name. existing_shm = shared_memory.SharedMemory(name=name) existing_shm.buf[:len(data)] = data def read_from_shared_memory(name: str, size: int) -> bytes: Reads the given number of bytes from the shared memory block identified by name and returns it. existing_shm = shared_memory.SharedMemory(name=name) return bytes(existing_shm.buf[:size]) def clean_shared_memory(name: str) -> None: Closes and unlinks the shared memory block identified by name. existing_shm = shared_memory.SharedMemory(name=name) existing_shm.close() existing_shm.unlink()"},{"question":"# PyTorch Metrics Assessment In this question, you will implement a function using PyTorch\'s distributed elastic metrics API to log, handle, and manage performance metrics during a model training process. # Objective: Implement a function `train_and_evaluate_model` that: 1. Configures a console metric handler to log metrics to the console. 2. Runs a simplified training loop for a given PyTorch model. 3. Uses the provided metrics API to log training loss every epoch. 4. Outputs the metrics to the console after each epoch. # Function Signature: ```python def train_and_evaluate_model(model, train_loader, optimizer, loss_fn, num_epochs): Trains and evaluates a PyTorch model while logging metrics. Parameters: - model (torch.nn.Module): The neural network model to be trained. - train_loader (torch.utils.data.DataLoader): The data loader for the training data. - optimizer (torch.optim.Optimizer): The optimizer for training the model. - loss_fn (torch.nn.Module): The loss function. - num_epochs (int): The number of epochs to train the model. Returns: None # Your implementation here ``` # Input: - `model`: A PyTorch neural network model (instance of `torch.nn.Module`). - `train_loader`: A DataLoader instance providing the training data. - `optimizer`: A PyTorch optimizer (instance of `torch.optim.Optimizer`). - `loss_fn`: A PyTorch loss function (instance of `torch.nn.Module` or similar). - `num_epochs`: An integer representing the number of epochs. # Output: - No return value. The function should print the training loss for each epoch to the console using the metrics handler. # Constraints: - Use the `ConsoleMetricHandler` to log metrics. - Ensure that the function works with PyTorch version >= 1.6.0. # Example: ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset import torch.distributed.elastic.metrics as metrics # Dummy data x = torch.randn(100, 10) y = torch.randn(100, 1) dataset = TensorDataset(x, y) train_loader = DataLoader(dataset, batch_size=10) # Model definition model = nn.Sequential(nn.Linear(10, 1)) # Loss function and optimizer loss_fn = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) def train_and_evaluate_model(model, train_loader, optimizer, loss_fn, num_epochs): metrics.configure(metrics.ConsoleMetricHandler()) for epoch in range(num_epochs): model.train() total_loss = 0.0 for data, target in train_loader: optimizer.zero_grad() output = model(data) loss = loss_fn(output, target) loss.backward() optimizer.step() total_loss += loss.item() avg_loss = total_loss / len(train_loader) metrics.put_metric(\'average_loss\', avg_loss) metrics.put_metric(\'epoch\', epoch) print(f\'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\') # Run the training train_and_evaluate_model(model, train_loader, optimizer, loss_fn, num_epochs=5) ``` # Notes: - Ensure you have the necessary imports. - You may set up any supplementary configurations required by the `metrics` module.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset import torch.distributed.elastic.metrics as metrics def train_and_evaluate_model(model, train_loader, optimizer, loss_fn, num_epochs): Trains and evaluates a PyTorch model while logging metrics. Parameters: - model (torch.nn.Module): The neural network model to be trained. - train_loader (torch.utils.data.DataLoader): The data loader for the training data. - optimizer (torch.optim.Optimizer): The optimizer for training the model. - loss_fn (torch.nn.Module): The loss function. - num_epochs (int): The number of epochs to train the model. Returns: None metrics.configure(metrics.ConsoleMetricHandler()) for epoch in range(num_epochs): model.train() total_loss = 0.0 for data, target in train_loader: optimizer.zero_grad() output = model(data) loss = loss_fn(output, target) loss.backward() optimizer.step() total_loss += loss.item() avg_loss = total_loss / len(train_loader) metrics.put_metric(\'average_loss\', avg_loss) metrics.put_metric(\'epoch\', epoch) print(f\'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}\')"},{"question":"**Custom Autograd Function Implementation** Your task is to implement a custom PyTorch autograd function to perform matrix multiplication with an added custom gradient rule. The forward pass should perform matrix multiplication between two input tensors. The backward pass should return custom gradients for each of the input tensors. Follow the guidelines described below: # Requirements 1. Define a custom class `CustomMatMul` inheriting from `torch.autograd.Function`. 2. Implement the following static methods: - `forward(ctx, matrix1, matrix2)`: Perform the matrix multiplication of `matrix1` and `matrix2`. Save any tensors required for the backward pass using `ctx.save_for_backward`. - `setup_context(ctx, inputs, output)`: Save additional context information (from inputs or outputs) required for the backward pass. - `backward(ctx, grad_output)`: Compute and return the custom gradients for `matrix1` and `matrix2`. 3. The gradients for `matrix1` and `matrix2` should be computed as follows: - Gradients for `matrix1` should be the product of `grad_output` and `matrix2` transposed. - Gradients for `matrix2` should be the product of `matrix1` transposed and `grad_output`. 4. Provide a convenience function `custom_matmul(matrix1, matrix2)` to make it easier to use `CustomMatMul`. 5. Ensure that your implementation supports gradient computation by verifying it with a `torch.autograd.gradcheck`. # Example Here\'s an example of how your implementation might be used: ```python import torch from torch.autograd import gradcheck class CustomMatMul(torch.autograd.Function): @staticmethod def forward(ctx, matrix1, matrix2): result = torch.matmul(matrix1, matrix2) ctx.save_for_backward(matrix1, matrix2) return result @staticmethod def setup_context(ctx, inputs, output): matrix1, matrix2 = inputs result = output ctx.save_for_backward(matrix1, matrix2) @staticmethod def backward(ctx, grad_output): matrix1, matrix2 = ctx.saved_tensors grad_matrix1 = torch.matmul(grad_output, matrix2.t()) grad_matrix2 = torch.matmul(matrix1.t(), grad_output) return grad_matrix1, grad_matrix2 def custom_matmul(matrix1, matrix2): return CustomMatMul.apply(matrix1, matrix2) # Testing the custom autograd function matrix1 = torch.randn(5, 3, dtype=torch.double, requires_grad=True) matrix2 = torch.randn(3, 4, dtype=torch.double, requires_grad=True) # Gradient check test = gradcheck(custom_matmul, (matrix1, matrix2), eps=1e-6, atol=1e-4) print(\\"Gradient check passed:\\", test) # Should print: Gradient check passed: True ``` # Constraints - You must use PyTorch operations for implementing the forward and backward methods. - Do not use any external libraries (like NumPy) for computations within forward and backward methods. - Ensure that your implementation is efficient and leverages PyTorch\'s built-in functions wherever possible. # Evaluation Criteria - Correct implementation of the forward pass. - Appropriate handling and saving of context for the backward pass. - Accurate implementation of the custom gradients in the backward pass. - Passing the gradient check. - Code readability and adherence to the function signatures provided. Good luck!","solution":"import torch class CustomMatMul(torch.autograd.Function): @staticmethod def forward(ctx, matrix1, matrix2): result = torch.matmul(matrix1, matrix2) ctx.save_for_backward(matrix1, matrix2) return result @staticmethod def backward(ctx, grad_output): matrix1, matrix2 = ctx.saved_tensors grad_matrix1 = torch.matmul(grad_output, matrix2.t()) grad_matrix2 = torch.matmul(matrix1.t(), grad_output) return grad_matrix1, grad_matrix2 def custom_matmul(matrix1, matrix2): return CustomMatMul.apply(matrix1, matrix2)"},{"question":"**Objective**: Your task is to implement a simple feedforward neural network in TorchScript, demonstrating your understanding of TorchScript types, annotations, control structures, and module scripting. # Requirements 1. **Network Structure**: - The network should have 2 hidden layers. - Each hidden layer should consist of `torch.nn.Linear` layers followed by `torch.nn.ReLU` activation. 2. **TorchScript Module**: - Define a custom module class for the network using `torch.nn.Module`. - Script this module using `torch.jit.script`. 3. **Forward Method**: - Implement the `forward` method to perform a forward pass through the network. - Properly annotate the types of inputs and outputs. 4. **Static Typing and Annotations**: - Annotate all instance attributes. - Annotate the function parameters and the return types. # Input - A 2D torch tensor `x` of shape [batch_size, input_features]. # Output - A tensor with logits from the final layer of shape [batch_size, output_features]. # Example ```python import torch from typing import Any class SimpleFeedForwardNN(torch.nn.Module): def __init__(self, input_features: int, hidden_size: int, output_features: int): super(SimpleFeedForwardNN, self).__init__() self.fc1: torch.nn.Linear = torch.nn.Linear(input_features, hidden_size) self.relu1: torch.nn.ReLU = torch.nn.ReLU() self.fc2: torch.nn.Linear = torch.nn.Linear(hidden_size, hidden_size) self.relu2: torch.nn.ReLU = torch.nn.ReLU() self.fc3: torch.nn.Linear = torch.nn.Linear(hidden_size, output_features) def forward(self, x: torch.Tensor) -> torch.Tensor: x: torch.Tensor = self.fc1(x) x: torch.Tensor = self.relu1(x) x: torch.Tensor = self.fc2(x) x: torch.Tensor = self.relu2(x) x: torch.Tensor = self.fc3(x) return x # Example usage input_features = 8 hidden_size = 16 output_features = 4 batch_size = 2 model = SimpleFeedForwardNN(input_features, hidden_size, output_features) scripted_model = torch.jit.script(model) # Dummy input x = torch.randn(batch_size, input_features) # Forward pass logits = scripted_model(x) print(logits.shape) # Expected output: torch.Size([2, 4]) ``` # Constraints - You must use type annotations for all function parameters and return types. - Ensure your implementation is compatible with TorchScript and can be scripted without errors.","solution":"import torch import torch.nn as nn import torch.jit as jit from typing import Any class SimpleFeedForwardNN(nn.Module): def __init__(self, input_features: int, hidden_size: int, output_features: int): super(SimpleFeedForwardNN, self).__init__() self.fc1: nn.Linear = nn.Linear(input_features, hidden_size) self.relu1: nn.ReLU = nn.ReLU() self.fc2: nn.Linear = nn.Linear(hidden_size, hidden_size) self.relu2: nn.ReLU = nn.ReLU() self.fc3: nn.Linear = nn.Linear(hidden_size, output_features) def forward(self, x: torch.Tensor) -> torch.Tensor: x: torch.Tensor = self.fc1(x) x = self.relu1(x) x = self.fc2(x) x = self.relu2(x) x = self.fc3(x) return x # Create and script the model input_features = 8 hidden_size = 16 output_features = 4 batch_size = 2 model = SimpleFeedForwardNN(input_features, hidden_size, output_features) scripted_model = jit.script(model) # Dummy input x = torch.randn(batch_size, input_features) # Forward pass logits = scripted_model(x) print(logits.shape) # Expected output: torch.Size([2, 4])"},{"question":"Objective Implement a function `analyze_memoryview(buffer: bytes) -> dict` that demonstrates the usage and understanding of Python\'s `memoryview` object. Requirements 1. The function should accept a `bytes` buffer as input. 2. Create a `memoryview` from the input buffer. 3. From the `memoryview`, extract the following information and store it in a dictionary: - Length of the buffer. - Sub-buffer of the first 5 elements. - Sub-buffer of the last 5 elements. - Check and confirm if the memory is contiguous. - Provide a read-only view of the buffer and check if it is indeed read-only. 4. Return the dictionary with the collected information. Input and Output Format - **Input:** ```python def analyze_memoryview(buffer: bytes) -> dict: pass # Example Usage buffer = b\\"examplebuffer\\" result = analyze_memoryview(buffer) ``` - **Output:** ```python { \\"length\\": 13, \\"first_5_elements\\": b\\"examp\\", \\"last_5_elements\\": b\\"uffer\\", \\"is_contiguous\\": True, \\"is_read_only\\": True } ``` Constraints 1. The input `buffer` will always be a non-empty bytes object. 2. The function must handle buffers of varying lengths, including those less than 5 bytes. Performance Requirements The function should be optimized to handle large buffers efficiently without unnecessary copying of data. Implementation Example Here is a skeleton you can start with: ```python def analyze_memoryview(buffer: bytes) -> dict: info = {} # Create a memoryview object mv = memoryview(buffer) # Gather required information info[\'length\'] = len(mv) info[\'first_5_elements\'] = mv[:5].tobytes() info[\'last_5_elements\'] = mv[-5:].tobytes() # Check if memoryview is contiguous contig_mv = mv.toreadonly() info[\'is_contiguous\'] = mv.contiguous # Create a read-only memory view and check read_only_mv = mv.toreadonly() try: read_only_mv[0:1] = b\'xff\' info[\'is_read_only\'] = False except TypeError: info[\'is_read_only\'] = True return info ```","solution":"def analyze_memoryview(buffer: bytes) -> dict: info = {} # Create a memoryview object from the buffer mv = memoryview(buffer) # Gather required information info[\'length\'] = len(mv) info[\'first_5_elements\'] = mv[:5].tobytes() if len(mv) >= 5 else mv[:].tobytes() info[\'last_5_elements\'] = mv[-5:].tobytes() if len(mv) >= 5 else mv[:].tobytes() info[\'is_contiguous\'] = mv.contiguous # Create a read-only memory view and check read_only_mv = mv.toreadonly() try: read_only_mv[0:1] = b\'xff\' # Attempting to write to it should raise an error info[\'is_read_only\'] = False except TypeError: info[\'is_read_only\'] = True return info"},{"question":"# PyTorch Distributed Elastic Metrics: Logging and Handling You are given a scenario where you need to use PyTorch\'s `torch.distributed.elastic.metrics` module to configure and handle metrics for a distributed training job. **Task:** 1. **Configure a Metric Handler**: Write a function `configure_metric_handler` that takes no arguments and performs the following tasks: - Configures the metric system using `torch.distributed.elastic.metrics.configure` to use `ConsoleMetricHandler`, which will print metrics to the console. 2. **Log a Custom Metric**: Write a function `log_custom_metric` that takes the following arguments: - `metric_name` (str): The name of the metric to log. - `metric_value` (float): The value of the metric to log. This function should use `torch.distributed.elastic.metrics.put_metric` to log the provided metric. **Input and Output Formats:** - `configure_metric_handler`: No input parameters and no return value. - `log_custom_metric`: - Input: `metric_name` (string), `metric_value` (float) - Output: This function does not return any value but should log the metric to the configured handler. **Constraints:** - Use the `ConsoleMetricHandler` to handle metrics. - Assume the necessary imports and initializations are done. Example: ```python def configure_metric_handler(): # Your implementation here def log_custom_metric(metric_name, metric_value): # Your implementation here # Example usage configure_metric_handler() log_custom_metric(\\"accuracy\\", 0.95) ``` In this example, `accuracy` with a value of `0.95` should be logged to the console. **Note**: You might need to refer to the official PyTorch documentation for additional details on function usage and method signatures.","solution":"import torch.distributed.elastic.metrics as metrics def configure_metric_handler(): Configures the metric system to use the ConsoleMetricHandler, which will print metrics to the console. metrics.configure(metrics.ConsoleMetricHandler()) def log_custom_metric(metric_name, metric_value): Logs a custom metric to the configured metric handler. Args: - metric_name (str): The name of the metric to log. - metric_value (float): The value of the metric to log. metrics.put_metric(metric_name, metric_value)"},{"question":"**Question: Custom Diverging Color Palette Visualization** **Objective:** Demonstrate your understanding of the seaborn library\'s capabilities related to color palettes by creating and customizing a diverging color palette and applying it to a heatmap visualization. **Description:** You are provided with a CSV file named `data.csv` that contains a dataset with numerical values. Your task is to load this dataset, generate a heatmap using seaborn, and apply a custom diverging color palette to this heatmap. **Instructions:** 1. **Load the Dataset:** - Load the dataset from the `data.csv` file into a pandas DataFrame. 2. **Create a Custom Diverging Color Palette:** - Utilize the `sns.diverging_palette` function to create a custom diverging color palette. The palette should have the following characteristics: - Start hue (`h_neg`): 220 - End hue (`h_pos`): 20 - Saturation (`s`): 80 - Lightness (`l`): 50 - Center: \\"light\\" - Separation (`sep`): 10 - Return a continuous colormap rather than a discrete palette (`as_cmap=True`). 3. **Generate the Heatmap:** - Generate a heatmap of the dataset using seaborn\'s `heatmap` function. - Apply the custom diverging color palette created in Step 2 to the heatmap. 4. **Save the Visualization:** - Save the resulting heatmap as a PNG image file named `heatmap.png`. **Constraints:** - The `data.csv` file is guaranteed to exist in the current directory. - The dataset contains only numerical values. **Expected Input:** - A CSV file named `data.csv` in the current directory with numerical data. **Expected Output:** - A PNG file named `heatmap.png` in the current directory containing the heatmap visualization with the custom diverging color palette applied. **Example Code:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = pd.read_csv(\'data.csv\') # Step 2: Create a custom diverging color palette palette = sns.diverging_palette(220, 20, s=80, l=50, center=\\"light\\", sep=10, as_cmap=True) # Step 3: Generate the heatmap plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=palette) # Step 4: Save the visualization plt.savefig(\'heatmap.png\') plt.close() ``` Ensure your solution follows these specifications precisely and produces the expected PNG file.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_heatmap(input_csv, output_image): Creates a heatmap from the provided CSV file and saves it as a PNG image. Parameters: input_csv (str): The path to the CSV file containing the dataset. output_image (str): The path to the output PNG image file where the heatmap will be saved. # Step 1: Load the dataset data = pd.read_csv(input_csv) # Step 2: Create a custom diverging color palette palette = sns.diverging_palette(220, 20, s=80, l=50, center=\\"light\\", sep=10, as_cmap=True) # Step 3: Generate the heatmap plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=palette, cbar=True) # Step 4: Save the visualization plt.savefig(output_image) plt.close() # Example usage: # create_heatmap(\'data.csv\', \'heatmap.png\')"},{"question":"Objectives: Use your knowledge of PyTorch and TorchScript to implement a custom backward function for a simple neural network linear transformation layer. Your implementation should efficiently compute gradients necessary for the backward pass in the training of a neural network. Problem Statement: Implement a custom backward function for a linear transformation defined as: [ y = Wx + b ] where: - ( W ) is a weight matrix. - ( x ) is an input vector. - ( b ) is a bias vector. - ( y ) is the output vector. Requirements: 1. **Function signature:** ```python def custom_backward(W: torch.Tensor, x: torch.Tensor, b: torch.Tensor, grad_output: torch.Tensor) -> (torch.Tensor, torch.Tensor, torch.Tensor): Args: W (torch.Tensor): Weight matrix of shape (m, n). x (torch.Tensor): Input vector of shape (n,). b (torch.Tensor): Bias vector of shape (m,). grad_output (torch.Tensor): Gradient of the loss function with respect to the output y, of shape (m,). Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Gradients with respect to W, x, and b. ``` 2. **Behavior:** - Compute and return the gradients of the loss with respect to ( W ), ( x ), and ( b ). - Assume gradients with respect to the loss are passed in the `grad_output` tensor. 3. **Constraints:** - You should use only PyTorch and TorchScript-compatible features based on the provided documentation. - Ensure that your function is efficient and can handle tensor operations directly using PyTorch operations. 4. **Performance constraints:** - Your implementation must run efficiently for large tensor inputs. Avoid inefficient looping constructs. Examples: ```python import torch # Example 1 W = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True) x = torch.tensor([1.0, 2.0], requires_grad=True) b = torch.tensor([0.5, 0.5], requires_grad=True) grad_output = torch.tensor([1.0, 1.0]) dW, dx, db = custom_backward(W, x, b, grad_output) # Expected gradients (computed by hand or using PyTorch\'s autograd for verification): # dW should be [[1.0, 2.0], [1.0, 2.0]] # dx should be [4.0, 6.0] # db should be [1.0, 1.0] print(dW) print(dx) print(db) ``` Notes: - You can test your implementation by comparing it with the gradients computed using PyTorch\'s autograd feature. - Make sure to handle shapes accurately and efficiently without unnecessary copying or transformations. Good luck, and happy coding!","solution":"import torch def custom_backward(W: torch.Tensor, x: torch.Tensor, b: torch.Tensor, grad_output: torch.Tensor): Compute gradients of the loss with respect to W, x, and b given the gradient of the loss with respect to the output y. Args: W (torch.Tensor): Weight matrix of shape (m, n). x (torch.Tensor): Input vector of shape (n,). b (torch.Tensor): Bias vector of shape (m,). grad_output (torch.Tensor): Gradient of the loss function with respect to the output y, of shape (m,). Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Gradients with respect to W, x, and b. # Compute gradients dW = grad_output.unsqueeze(1) @ x.unsqueeze(0) # Shape: (m, n) dx = W.t() @ grad_output # Shape: (n,) db = grad_output # Shape: (m,) return dW, dx, db"},{"question":"# Advanced Seaborn Swarmplot Challenge You are required to use the `seaborn` library to visualize a dataset effectively. Your task is to create a function that produces a specific kind of plot explained below. Function Signature ```python def advanced_swarm_plot(data): pass ``` Input - `data` (DataFrame): A Pandas DataFrame containing at least the following columns: * `total_bill` (float): The total bill value. * `day` (category): The day of the week. * `size` (int): The size of the group. * `sex` (category): The gender category. * `time` (category): Time of day (Lunch/Dinner). Output - Your function should not return anything but should save a figure as `swarmplot_figure.png`. Requirements 1. **Basic Plot**: Use a `swarmplot` function to plot `total_bill` vs. `day` where: * `hue` is set to `sex`. * Use the `palette` named `\\"deep\\"` to control color mapping. 2. **Dodge Points**: Ensure the points for different hues are dodged. 3. **Custom Marker and Size**: * Use a cross (\\"x\\") marker. * Set the `linewidth` of the marker to `1.0`. 4. **FacetGrid Usage**: Create facets of this plot by the `time` column, ensuring that: * Each facet represents a different `time`. * The aspect ratio of subplots is `0.5`. 5. **Save the Plot**: Save the resulting figure to a file named `swarmplot_figure.png`. Constraints - Assume seaborn and Pandas are installed. - Ensure your function handles cases where the DataFrame is empty by raising a ValueError with the message \\"DataFrame is empty.\\" - Clean the input DataFrame to contain only rows where \'day\' is one of [\'Thur\', \'Fri\', \'Sat\', \'Sun\']. Example Usage ```python import seaborn as sns import pandas as pd # Loading example dataset tips = sns.load_dataset(\\"tips\\") # Running your function advanced_swarm_plot(tips) ``` The above function call should produce and save a plot similar to the provided description, saved as `swarmplot_figure.png`.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def advanced_swarm_plot(data): if data.empty: raise ValueError(\\"DataFrame is empty\\") # Filter the dataframe for the specified days valid_days = [\'Thur\', \'Fri\', \'Sat\', \'Sun\'] data = data[data[\'day\'].isin(valid_days)] # Create a FacetGrid g = sns.FacetGrid(data, col=\\"time\\", aspect=0.5) # Map the swarmplot onto the facets g.map_dataframe( sns.swarmplot, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", palette=\\"deep\\", dodge=True, marker=\\"x\\", linewidth=1.0 ) # Add legend g.add_legend() # Save the plot g.savefig(\\"swarmplot_figure.png\\") # Close the plot to save memory plt.close()"},{"question":"**Objective:** Demonstrate your understanding of the `torch.monitor` module by implementing a metric monitoring system using the provided classes and functions. **Task:** You are required to implement a small utility that monitors the training of a PyTorch model. Your task includes: 1. Tracking and logging loss values during training (an infrequent event). 2. Aggregating the loss values to compute average loss with a specific window size. 3. Logging these aggregated metric values periodically using an event handler. **Requirements:** 1. Implement a class `TrainingMonitor` which: - Initializes an event handler for logging. - Holds a `torch.monitor.Stat` for aggregating loss values. - Logs the loss value using `torch.monitor.log_event`. - Periodically calls the event handler to log the aggregated metric. 2. Implement the following methods in the `TrainingMonitor` class: - `__init__(self, window_size: int, log_frequency: int)`: Initializes the class with an aggregation window size and a log frequency. - `log_loss(self, loss_value: float)`: Logs the given loss value and updates the metric aggregation. - `flush(self)`: Manually triggers the event handler to log the current state of aggregated metrics. **Constraints:** - The `window_size` parameter specifies how many loss values should be aggregated before computing the average. - The `log_frequency` parameter specifies how often (in terms of number of batches) the periodic logging should occur. **Example Usage:** ```python import torch.monitor class TrainingMonitor: def __init__(self, window_size: int, log_frequency: int): # Your implementation here def log_loss(self, loss_value: float): # Your implementation here def flush(self): # Your implementation here # Example of how this might be used during training monitor = TrainingMonitor(window_size=10, log_frequency=100) for batch_idx in range(1000): # Assume we\'re training for 1000 batches # Simulate loss calculation loss = some_model_training_function() monitor.log_loss(loss) # Periodically flush the logs if batch_idx % monitor.log_frequency == 0: monitor.flush() ``` **Expected Input and Output Formats:** - The `__init__` method takes two integers, `window_size` and `log_frequency`. - The `log_loss` method takes a float `loss_value` as input. - The `flush` method does not take any input or output. Ensure your implementation handles the numerical stability and performs efficiently in a real-world training loop.","solution":"import torch.monitor class TrainingMonitor: def __init__(self, window_size: int, log_frequency: int): self.window_size = window_size self.log_frequency = log_frequency self.loss_stat = torch.monitor.Stat(window_size) self.log_counter = 0 def log_loss(self, loss_value: float): self.loss_stat.add_sample(loss_value) torch.monitor.log_event(\'loss\', {\'loss\': loss_value}) self.log_counter += 1 if self.log_counter % self.log_frequency == 0: self.flush() def flush(self): avg_loss = self.loss_stat.compute_metric(\'mean\') torch.monitor.log_event(\'average_loss\', {\'average_loss\': avg_loss})"},{"question":"Objective: Develop a Python application that demonstrates your understanding of the `signal` module, and the intricacies of handling signals and exceptions raised by signal handlers. Task: Create a Python script that performs the following operations: 1. **Signal Setup**: - Set up a custom signal handler for `SIGALRM` using the `signal.signal()` function. - Schedule an alarm to send a `SIGALRM` signal after a specified number of seconds using the `signal.alarm()` function. 2. **Signal Handler**: - The custom handler should: - Print the signal number when the signal is received. - Raise an `OSError` with a custom message when the signal handler is triggered. 3. **Exception Handling**: - In the main function of the script, ensure that exceptions raised by the signal handler (like `OSError`) are caught and handled gracefully. - Print a message indicating that the exception was caught. 4. **File Operation**: - In the main function, perform a file operation, such as attempting to open a potentially locked file. The goal is to simulate an operation that might take longer than the scheduled alarm. Input: - The duration (in seconds) before the alarm should trigger (integer). - A file path that the script will attempt to open. Constraints: - You should handle the scenario where the file might not be available or could cause the script to hang indefinitely. - Ensure that no alarm is scheduled after the file operation completes (i.e., cancel any scheduled alarm if the operation completes sooner). Output: The script should print messages at each significant step: - Setting up the signal handler. - Scheduling the alarm. - Catching the signal. - Raising and catching the exception. Example: ```python import signal import os import time def handler(signum, frame): print(f\'Signal handler called with signal {signum}\') raise OSError(\\"Custom OSError: Operation timed out!\\") def main(alarm_time, file_path): try: # Setting up the signal handler signal.signal(signal.SIGALRM, handler) print(\'Signal handler set for SIGALRM\') # Scheduling the alarm signal.alarm(alarm_time) print(f\'Alarm scheduled to trigger in {alarm_time} seconds\') # Performing a file operation (simulating long operation) print(\'Attempting to open the file...\') fd = os.open(file_path, os.O_RDWR) # Simulating long sleep to mimic a potentially hanging operation time.sleep(alarm_time + 5) # If operation completes, cancel the alarm signal.alarm(0) print(\'Operation completed; alarm cancelled\') except OSError as e: print(f\'OSError caught: {e}\') if __name__ == \'__main__\': main(5, \'/path/to/test/file.txt\') ``` In this example: - Substitute `/path/to/test/file.txt` with a real path suitable for testing on your system. Write your implementation: ```python import signal import os import time def handler(signum, frame): # Signal handler logic here raise NotImplementedError def main(alarm_time, file_path): # Main function logic here pass # Entry point of the script if __name__ == \'__main__\': # Provide appropriate values for alarm_time and file_path pass ``` Submit your implementation with example inputs and outputs demonstrating how it works.","solution":"import signal import os import time def handler(signum, frame): Signal handler for SIGALRM. print(f\'Signal handler called with signal {signum}\') raise OSError(\\"Custom OSError: Operation timed out!\\") def main(alarm_time, file_path): try: # Setting up the signal handler signal.signal(signal.SIGALRM, handler) print(\'Signal handler set for SIGALRM\') # Scheduling the alarm signal.alarm(alarm_time) print(f\'Alarm scheduled to trigger in {alarm_time} seconds\') # Performing a file operation (simulating long operation) print(\'Attempting to open the file...\') fd = os.open(file_path, os.O_RDWR | os.O_CREAT) # Simulating long sleep to mimic a potentially hanging operation time.sleep(alarm_time + 5) # If operation completes, cancel the alarm signal.alarm(0) print(\'Operation completed; alarm cancelled\') except OSError as e: print(f\'OSError caught: {e}\') if __name__ == \'__main__\': # Provide appropriate values for alarm_time and file_path main(5, \'/path/to/test/file.txt\')"},{"question":"Write a function `find_first_fibonacci_with_substring(substring: str) -> int:` that finds the first Fibonacci number (in its string form) that contains a given `substring` and returns this number. The Fibonacci sequence is defined as follows: - F(0) = 0 - F(1) = 1 - F(n) = F(n - 1) + F(n - 2) for n > 1 Your function should start from F(0) and check each Fibonacci number in sequence. As soon as you find the first Fibonacci number that contains the given `substring`, return that Fibonacci number. Input: - `substring`: a string representing the substring to search for within the Fibonacci numbers. Output: - An integer representing the first Fibonacci number (in its string form) that contains the given substring. Constraints: - The `substring` will be a non-empty string containing only numeric characters (0-9). - The numbers in the Fibonacci sequence grow exponentially; ensure your function handles large integers. Example: ```python # Example 1: substring = \\"144\\" print(find_first_fibonacci_with_substring(substring)) # Output: 144, because F(12) = 144 and it contains \\"144\\" # Example 2: substring = \\"89\\" print(find_first_fibonacci_with_substring(substring)) # Output: 89, because F(11) = 89 and it contains \\"89\\" ``` Notes: - Your function should efficiently manage the generation of Fibonacci numbers to avoid performance issues with large numbers. - Consider edge cases where the `substring` length is 1 or is a part of the sequence itself.","solution":"def find_first_fibonacci_with_substring(substring: str) -> int: Finds the first Fibonacci number that contains the given substring. :param substring: A string representing the substring to search for within the Fibonacci numbers :return: The first Fibonacci number that contains the given substring a, b = 0, 1 # Initial Fibonacci numbers while True: if substring in str(a): return a a, b = b, a + b"},{"question":"You are tasked with writing a utility function using PyTorch that manages GPU device operations for a hypothetical deep learning application. The function should perform the following steps: 1. **Check Device Availability:** Ensure that at least one accelerator device is available. If no devices are available, raise a `RuntimeError` with the message \\"No accelerator devices available.\\" 2. **Set Device:** Set the first device (index 0) as the current working device. 3. **Initialize Streams:** Initialize two different streams on the current device. 4. **Perform Stream Operations:** Within each stream, simulate a workload by copying a tensor from the CPU to the GPU and back. 5. **Synchronize Operations:** Ensure that operations within each stream are properly synchronized. 6. **Return Results:** Return the indices of the current device and streams used. Function Signature: ```python def manage_device_operations() -> (int, list): Manages device operations and performs synchronization. Returns: current_device_index (int): The index of the current device being used. streams_indices (list): List containing indices of the streams used. ``` Expected Example: ```python try: device_index, stream_indices = manage_device_operations() print(f\\"Current Device Index: {device_index}, Stream Indices: {stream_indices}\\") except RuntimeError as e: print(e) ``` Constraints: - You may assume that `torch` is properly imported and accessible in the environment. - The tensor operations within the streams should be simulated and need not perform actual deep learning tasks. - Ensure that stream operations do not interfere with each other, demonstrating an understanding of GPU stream concurrency. This question assesses the understanding of fundamental PyTorch operations, device management, and concurrency handling using streams.","solution":"import torch def manage_device_operations(): Manages device operations and performs synchronization. Returns: current_device_index (int): The index of the current device being used. streams_indices (list): List containing indices of the streams used. # Check Device Availability if not torch.cuda.is_available(): raise RuntimeError(\\"No accelerator devices available.\\") # Set Device current_device_index = 0 torch.cuda.set_device(current_device_index) # Initialize Streams stream1 = torch.cuda.Stream(device=current_device_index) stream2 = torch.cuda.Stream(device=current_device_index) # Simulated workload: tensor operations within each stream tensor = torch.randn(1000, 1000) # A random tensor tensor_gpu = tensor.to(\'cuda\') # Move tensor to GPU # Stream 1 operations with torch.cuda.stream(stream1): tensor_gpu_stream1 = tensor_gpu.clone() tensor_cpu_stream1 = tensor_gpu_stream1.to(\'cpu\') # Stream 2 operations with torch.cuda.stream(stream2): tensor_gpu_stream2 = tensor_gpu.clone() tensor_cpu_stream2 = tensor_gpu_stream2.to(\'cpu\') # Synchronize Streams stream1.synchronize() stream2.synchronize() # Return Results streams_indices = [stream1.cuda_stream, stream2.cuda_stream] return current_device_index, streams_indices"},{"question":"Objective: To assess the students\' understanding and capability to use the `sklearn.datasets` module for loading a dataset and performing basic data analysis. Problem Statement: You are required to load the \\"Iris\\" dataset using scikit-learn\'s dataset loading utilities, and then perform various tasks on this dataset. Tasks: 1. **Load the Dataset**: 1.1 Use the `load_iris` function from `sklearn.datasets` to load the Iris dataset. 1.2 Retrieve the data as `X` (features) and `y` (target) arrays directly (not as a Bunch object). 2. **Basic Data Analysis**: 2.1 Calculate and print the number of samples (`n_samples`) and the number of features (`n_features`). 2.2 Identify and print all the unique target values in the dataset. 2.3 Calculate and print the mean value of each feature for each target class. Input and Output: - There are no external inputs for this task. You will directly load the Iris dataset from scikit-learn. - Your final output should include: - The shape of the dataset (number of samples and number of features). - A list of unique target values. - A dictionary where keys are unique target values, and values are lists containing the mean value of each feature for that target class. Constraints: - Use only the `sklearn.datasets` module and basic Python libraries (e.g., `numpy`, `pandas`). - Do not use any other machine learning or data analysis libraries for this task. Example Output: ``` Number of samples: 150 Number of features: 4 Unique target values: [0 1 2] Mean value of each feature for each target class: { 0: [5.006, 3.428, 1.462, 0.246], 1: [5.936, 2.770, 4.260, 1.326], 2: [6.588, 2.974, 5.552, 2.026] } ``` Implementation: Write a function `analyze_iris_dataset()` that performs the above tasks. ```python import numpy as np from sklearn.datasets import load_iris def analyze_iris_dataset(): # 1. Load the Dataset X, y = load_iris(return_X_y=True) # 2. Basic Data Analysis # 2.1 Number of samples and features n_samples, n_features = X.shape print(f\\"Number of samples: {n_samples}\\") print(f\\"Number of features: {n_features}\\") # 2.2 Unique target values unique_targets = np.unique(y) print(f\\"Unique target values: {unique_targets}\\") # 2.3 Mean value of each feature for each target class mean_values = {} for target in unique_targets: mean_values[target] = X[y == target].mean(axis=0).tolist() print(\\"Mean value of each feature for each target class:\\") print(mean_values) # Run the function analyze_iris_dataset() ```","solution":"import numpy as np from sklearn.datasets import load_iris def analyze_iris_dataset(): # 1. Load the Dataset X, y = load_iris(return_X_y=True) # 2. Basic Data Analysis # 2.1 Number of samples and features n_samples, n_features = X.shape # 2.2 Unique target values unique_targets = np.unique(y) # 2.3 Mean value of each feature for each target class mean_values = {} for target in unique_targets: mean_values[target] = X[y == target].mean(axis=0).tolist() return n_samples, n_features, unique_targets.tolist(), mean_values"},{"question":"Using the seaborn library, you are required to analyze the \'penguins\' dataset and create multiple ECDF plots to demonstrate your understanding of the `sns.ecdfplot` function. Follow the steps below to complete the task: 1. **Basic ECDF Plot**: - Create an ECDF plot for the `flipper_length_mm` column along the x-axis. 2. **Flipped ECDF Plot**: - Create a flipped ECDF plot for the `flipper_length_mm` column along the y-axis. 3. **Wide-form Data Plot**: - Create a wide-form ECDF plot for the columns that start with \\"bill_\\" in their name. 4. **Hue Mapping**: - Create an ECDF plot for the `bill_length_mm` column, differentiating the data by the `species` using the `hue` parameter. 5. **Statistics Options**: - Create an ECDF plot for the `bill_length_mm` column, showing the absolute counts instead of the default proportions, and differentiate by `species` using the `hue` parameter. 6. **Complementary CDF**: - Create an ECDF plot for the `bill_length_mm` column, showing the complementary CDF (1 - CDF), and differentiate by `species` using the `hue` parameter. Input Format: - No input required; use the `penguins` dataset from seaborn. Output Format: - Your code should generate six distinct ECDF plots as described above. Constraints: - Use seaborn\'s built-in functions and adhere to the seaborn documentation provided. - Ensure the plots are clear and properly labeled. - Use the seaborn `load_dataset` function to load the `penguins` dataset. Example Usage: ```python import seaborn as sns # Load dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic ECDF plot sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") # 2. Flipped ECDF plot sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\") # 3. Wide-form ECDF plot for columns starting with \'bill_\' sns.ecdfplot(data=penguins.filter(like=\\"bill_\\", axis=\\"columns\\")) # 4. ECDF plot with hue mapping sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") # 5. ECDF plot with absolute counts sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") # 6. Complementary ECDF plot sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) ``` Ensure your code generates these plots when executed. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic ECDF plot plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"Basic ECDF plot of Flipper Length\\") plt.show() # 2. Flipped ECDF plot plt.figure() sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\") plt.title(\\"Flipped ECDF plot of Flipper Length\\") plt.show() # 3. Wide-form ECDF plot for columns starting with \'bill_\' plt.figure() sns.ecdfplot(data=penguins.filter(like=\\"bill_\\", axis=\\"columns\\")) plt.title(\\"Wide-form ECDF plot for Bill Measurements\\") plt.show() # 4. ECDF plot with hue mapping plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF plot of Bill Length Differentiated by Species\\") plt.show() # 5. ECDF plot with absolute counts plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"ECDF plot of Bill Length with Absolute Counts Differentiated by Species\\") plt.show() # 6. Complementary ECDF plot plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"Complementary ECDF plot of Bill Length Differentiated by Species\\") plt.show()"},{"question":"**Title:** Working with Datasets in Scikit-Learn **Objective:** Assess your ability to load and preprocess various types of datasets using scikit-learn, thereby demonstrating your understanding of the fundamental and advanced concepts of this package. **Question:** You are provided with three tasks involving dataset loading, preprocessing, and basic analysis using scikit-learn. Follow the steps for each task and implement the required functions accordingly. **Tasks:** Task 1: Loading and Visualizing a Sample Image 1. Write a function `load_and_display_image()` that: - Loads the sample image `china.jpg` using `load_sample_image` from `sklearn.datasets`. - Converts the image from `uint8` to `float64` and scales it to the range 0-1. - Displays the image using `matplotlib`. ```python def load_and_display_image(): Load the \'china.jpg\' sample image, convert it to float64, scale it to range 0-1, and display it. pass ``` Task 2: Loading Sparse Datasets 2. Write a function `load_sparse_dataset(file_path)` that: - Takes in a file path of a dataset in svmlight/libsvm format. - Uses `load_svmlight_file` to load the dataset. - Returns the feature matrix `X` and the target variable `y` converted to dense format. ```python def load_sparse_dataset(file_path): Load a dataset from the given file path in svmlight/libsvm format, convert to dense format, and return the feature matrix X and target variable y. Parameters: file_path (str): The file path to the dataset in svmlight/libsvm format. Returns: tuple: A tuple containing the dense feature matrix X (numpy array) and target variable y (numpy array). pass ``` Task 3: Fetching and Analyzing OpenML Datasets 3. Write a function `fetch_and_analyze_openml_dataset(dataset_name, dataset_version)` that: - Fetches a dataset from OpenML using the given name and version. - Returns a tuple containing: - The shape of the data (`(n_samples, n_features)`). - The unique target classes. ```python def fetch_and_analyze_openml_dataset(dataset_name, dataset_version): Fetch a dataset from OpenML by name and version, and return the shape of the dataset and the unique target classes. Parameters: dataset_name (str): The name of the dataset to fetch. dataset_version (int): The version number of the dataset to fetch. Returns: tuple: A tuple containing the shape of the data (n_samples, n_features) and the unique target classes (numpy array). pass ``` **Constraints:** - Ensure proper error handling in case of invalid file paths or dataset names/versions. - Use clear and concise code with appropriate comments for readability. **Performance Requirements:** - The implementations should efficiently handle the loading and conversion operations without unnecessary computations. **Examples:** - For Task 1, `load_and_display_image()` should display the \'china.jpg\' image. - For Task 2, if given a valid file path, `load_sparse_dataset(file_path)` should return the dense feature matrix `X` and target variable `y`. - For Task 3, `fetch_and_analyze_openml_dataset(\'iris\', 1)` should return `((150, 4), array([\'Iris-setosa\', \'Iris-versicolor\', \'Iris-virginica\']))`. **Note:** You do not need access to external datasets for testing the functions; mock or placeholder data can be used where necessary.","solution":"import numpy as np from sklearn.datasets import load_sample_image, load_svmlight_file, fetch_openml import matplotlib.pyplot as plt def load_and_display_image(): Load the \'china.jpg\' sample image, convert it to float64, scale it to range 0-1, and display it. image = load_sample_image(\\"china.jpg\\") image = image.astype(np.float64) / 255 plt.imshow(image) plt.axis(\'off\') plt.show() def load_sparse_dataset(file_path): Load a dataset from the given file path in svmlight/libsvm format, convert to dense format, and return the feature matrix X and target variable y. Parameters: file_path (str): The file path to the dataset in svmlight/libsvm format. Returns: tuple: A tuple containing the dense feature matrix X (numpy array) and target variable y (numpy array). X_sparse, y = load_svmlight_file(file_path) X = X_sparse.toarray() return X, y def fetch_and_analyze_openml_dataset(dataset_name, dataset_version): Fetch a dataset from OpenML by name and version, and return the shape of the dataset and the unique target classes. Parameters: dataset_name (str): The name of the dataset to fetch. dataset_version (int): The version number of the dataset to fetch. Returns: tuple: A tuple containing the shape of the data (n_samples, n_features) and the unique target classes (numpy array). dataset = fetch_openml(name=dataset_name, version=dataset_version, as_frame=True) X = dataset.data y = dataset.target n_samples, n_features = X.shape unique_classes = np.unique(y) return (n_samples, n_features), unique_classes"},{"question":"**Objective**: Implement a custom class `MyPLS` in Python mimicking the functionality of `PLSCanonical` with `algorithm=\'nipals\'` to perform dimensionality reduction and regression on two matrices using the Partial Least Squares approach. **Requirements**: 1. **Initialization**: - `MyPLS(n_components=2)`: Constructor to initialize the number of components to extract. 2. **Methods**: - `fit(X, Y)`: Fits the model on the matrices `X` (predictors) and `Y` (targets). - `transform(X)`: Transforms the input matrix `X` using the weights obtained during fitting. - `predict(X)`: Predicts the target matrix `Y` for the input matrix `X` using the fitted model. **Procedural Details**: 1. **Initialization**: - Initialize the number of components, e.g., `n_components`. 2. **Fit Method**: - Center the matrices `X` and `Y`. - Use an iterative process to: a) Compute left and right singular vectors `u_k` and `v_k` of the cross-covariance matrix. b) Project `X` and `Y` onto these singular vectors to obtain scores `_k` and `_k`. c) Regress `X` on `_k` and `Y` on `_k` to obtain loadings `_k` and `_k`. d) Deflate `X` and `Y` by subtracting the rank-1 approximations. - Store the weights, loadings, and scores for use in transformation and prediction. 3. **Transform Method**: - Use the weights obtained in `fit` to transform a new matrix `X` into its lower-dimensional representation. 4. **Predict Method**: - Predict the labels `Y` by applying the learned coefficients to the new matrix `X`. **Constraints**: - Handle matrix inputs of size `n_samples x n_features` for `X` and `n_samples x n_targets` for `Y`. - `n_components <= min(n_samples, n_features, n_targets)` **Performance Requirements**: - Ensure that the iterative process converges efficiently. - Handle edge cases where matrices have more variables than observations or exhibit multicollinearity. # Example: ```python import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split # Generate synthetic data X, Y = make_regression(n_samples=100, n_features=20, n_targets=5, noise=0.1) X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Initialize and fit MyPLS model pls = MyPLS(n_components=2) pls.fit(X_train, Y_train) # Transform test data X_test_transformed = pls.transform(X_test) # Predict Y_test from X_test Y_test_pred = pls.predict(X_test) print(\\"Transformed X_test:\\", X_test_transformed) print(\\"Predicted Y_test:\\", Y_test_pred) ``` Ensure that the `MyPLS` class adheres to these requirements and matches the expected outputs. # Evaluation: - Correct implementation of the `fit` method following the described PLS algorithm. - Proper transformation of the input matrix `X` in the `transform` method. - Accurate prediction of the matrix `Y` in the `predict` method. - Robustness of the code to handle edge cases and constraints efficiently.","solution":"import numpy as np from numpy.linalg import svd class MyPLS: def __init__(self, n_components=2): self.n_components = n_components self.weights_ = None self.loadings_ = None self.scores_ = None self.coef_ = None def fit(self, X, Y): # Center matrices X_mean = np.mean(X, axis=0) Y_mean = np.mean(Y, axis=0) X_centered = X - X_mean Y_centered = Y - Y_mean n_samples, n_features = X.shape n_targets = Y.shape[1] W = np.zeros((n_features, self.n_components)) T = np.zeros((n_samples, self.n_components)) Q = np.zeros((n_targets, self.n_components)) for k in range(self.n_components): u_k = Y_centered[:, 0] t_k_prev = np.zeros((n_samples,)) while True: w_k = X_centered.T @ u_k / (u_k.T @ u_k) w_k /= np.linalg.norm(w_k) t_k = X_centered @ w_k q_k = Y_centered.T @ t_k / (t_k.T @ t_k) q_k /= np.linalg.norm(q_k) # normalizing q_k is optional if np.allclose(t_k, t_k_prev): break t_k_prev = t_k u_k = Y_centered @ q_k # Store the weights and scores W[:, k] = w_k T[:, k] = t_k Q[:, k] = q_k p_k = X_centered.T @ t_k / (t_k.T @ t_k) X_centered -= np.outer(t_k, p_k) Y_centered -= np.outer(t_k, q_k) self.weights_ = W self.loadings_ = T self.coef_ = np.linalg.inv(T.T @ T) @ T.T @ Y_centered self.Q_ = Q self.X_mean_ = X_mean self.Y_mean_ = Y_mean def transform(self, X): X_centered = X - self.X_mean_ T = X_centered @ self.weights_ return T def predict(self, X): T = self.transform(X) Y_pred = T @ self.Q_.T + self.Y_mean_ return Y_pred"},{"question":"**Objective:** Demonstrate your understanding of Python\'s `uuid` module by generating and manipulating UUIDs. **Question:** You are tasked with creating a utility function for generating a list of UUIDs and manipulating them based on a given set of specifications. Implement the following function: ```python import uuid def generate_and_manipulate_uuids(num_uuids, uuid_version, modify_uuid=None): Generate a list of UUIDs of a given version and perform modifications on the UUIDs as specified. Parameters: - num_uuids (int): The number of UUIDs to generate. - uuid_version (int): The version of UUIDs to generate. Valid values are: 1 for uuid1, 3 for uuid3, 4 for uuid4, 5 for uuid5. - modify_uuid (dict): A dictionary to specify modifications to be performed on the generated UUIDs. The dictionary can have the following keys: \\"convert_to_bytes\\" (bool): If True, convert the UUIDs to their byte representation. \\"check_safety\\" (bool): If True, include whether the UUIDs were generated in a thread-safe manner. Returns: - list: A list where each element is: - The byte representation of the UUID if \\"convert_to_bytes\\" is True. - A tuple (UUID object, is_safe flag) if \\"check_safety\\" is True. In this case, convert_to_bytes should be False. - The UUID object if no modifications are specified. pass ``` **Constraints:** - `num_uuids` is a positive integer between 1 and 100 inclusive. - `uuid_version` must be either 1, 3, 4, or 5. If an invalid version is passed, the function should raise a `ValueError` with the message \\"Invalid UUID version\\". - If `uuid_version` is 3 or 5, use the `NAMESPACE_DNS` with the name \\"example.com\\". **Examples:** 1. Generate 3 UUIDs of version 4 without any modifications: ```python generate_and_manipulate_uuids(3, 4) ``` Expected output (the actual UUIDs will differ): ```python [UUID(\'16fd2706-8baf-433b-82eb-8c7fada847da\'), UUID(\'16fd2706-8baf-433b-82eb-8c7fada847da\'), UUID(\'16fd2706-8baf-433b-82eb-8c7fada847da\')] ``` 2. Generate 2 UUIDs of version 1 and convert them to byte representation: ```python generate_and_manipulate_uuids(2, 1, modify_uuid={\\"convert_to_bytes\\": True}) ``` Expected output (the actual byte sequences will differ): ```python [b\'xa8tx8cx1axf8nx11xdaxbdx1ax00x11x24Dxa8x1e\', b\'xa8tnxc0x11xdaxbdx1ax00x11x24Dxa8x1e\'] ``` 3. Generate 1 UUID of version 3, and include the thread-safe status: ```python generate_and_manipulate_uuids(1, 3, modify_uuid={\\"check_safety\\": True}) ``` Expected output (the actual UUID and safety flag may differ): ```python [(UUID(\'6fa459ea-ee8a-3ca4-894e-db77e160355e\'), SafeUUID.unknown)] ``` **Notes:** - Use the `uuid` module to generate and handle UUIDs. - Handle edge cases like invalid UUID version and conflicting modification specifications (e.g., both `convert_to_bytes` and `check_safety` set to True).","solution":"import uuid def generate_and_manipulate_uuids(num_uuids, uuid_version, modify_uuid=None): Generate a list of UUIDs of a given version and perform modifications on the UUIDs as specified. Parameters: - num_uuids (int): The number of UUIDs to generate. - uuid_version (int): The version of UUIDs to generate. Valid values are: 1 for uuid1, 3 for uuid3, 4 for uuid4, 5 for uuid5. - modify_uuid (dict): A dictionary to specify modifications to be performed on the generated UUIDs. The dictionary can have the following keys: \\"convert_to_bytes\\" (bool): If True, convert the UUIDs to their byte representation. \\"check_safety\\" (bool): If True, include whether the UUIDs were generated in a thread-safe manner. Returns: - list: A list where each element is: - The byte representation of the UUID if \\"convert_to_bytes\\" is True. - A tuple (UUID object, is_safe flag) if \\"check_safety\\" is True. In this case, convert_to_bytes should be False. - The UUID object if no modifications are specified. if uuid_version not in [1, 3, 4, 5]: raise ValueError(\\"Invalid UUID version\\") modify_uuid = modify_uuid or {} uuids = [] for _ in range(num_uuids): if uuid_version == 1: new_uuid = uuid.uuid1() elif uuid_version == 3: new_uuid = uuid.uuid3(uuid.NAMESPACE_DNS, \\"example.com\\") elif uuid_version == 4: new_uuid = uuid.uuid4() elif uuid_version == 5: new_uuid = uuid.uuid5(uuid.NAMESPACE_DNS, \\"example.com\\") if modify_uuid.get(\\"convert_to_bytes\\", False): uuids.append(new_uuid.bytes) elif modify_uuid.get(\\"check_safety\\", False): uuids.append((new_uuid, new_uuid.is_safe)) else: uuids.append(new_uuid) return uuids"},{"question":"# Python C-API Simulation: Custom Object Creation and Management **Context:** You\'re tasked with simulating a primitive version of Python\'s C-API memory management using regular Python, specifically focusing on object creation, initialization, and deletion. **Task:** 1. **Define a CustomObject class:** - The class should have properties to store basic object data, including a type identifier and a reference count. 2. **Implement the following functions/methods in Python:** - `py_object_new(type_identifier)`: This function should simulate creating a new object with a given type identifier. It should return an instance of `CustomObject`. - `py_object_init(obj, type_identifier)`: This function should initialize an already allocated object (`obj`) with a type identifier and set its initial reference count to 1. - `py_object_delete(obj)`: This function should simulate deallocating an object by resetting its properties. 3. **Simulate a Use Case:** - Create a new `CustomObject` with a specific type identifier. - Initialize this new object with the identifier. - Delete the object to simulate deallocation. **Implementation Details:** * Your `CustomObject` class must include: - An `__init__` method accepting parameters for type identifier and reference count. - Methods for displaying object information if needed for debugging or testing. * Function specifications: ```python def py_object_new(type_identifier: str) -> \'CustomObject\': Simulates creating a new object with a given type identifier. pass def py_object_init(obj: \'CustomObject\', type_identifier: str) -> \'CustomObject\': Initializes the given \'obj\' with the provided type identifier and sets reference count to 1. pass def py_object_delete(obj: \'CustomObject\') -> None: Simulates deallocating the object. pass ``` * Constraints: - Type identifier should be a non-empty string for simplicity. - Ensure proper management of the reference count. **Expected Output:** - Print statements indicating the new object creation, initialization, and deallocation steps. ```python # Example use case obj = py_object_new(\\"CustomType\\") init_obj = py_object_init(obj, \\"CustomType\\") py_object_delete(init_obj) ``` **Assumptions:** - The student is familiar with Python\'s classes and methods. - The provided function stubs and their docstrings are adhered to. # Notes: This assignment requires students to understand Pythons memory handling and object lifecycle, simulating low-level operations with a high-level language. This approach reinforces advanced memory management concepts in a practical, Pythonic manner.","solution":"class CustomObject: def __init__(self, type_identifier: str = \\"\\", ref_count: int = 0): self.type_identifier = type_identifier self.ref_count = ref_count def __str__(self): return f\\"CustomObject(type_identifier={self.type_identifier}, ref_count={self.ref_count})\\" def py_object_new(type_identifier: str) -> \'CustomObject\': Simulates creating a new object with a given type identifier. # Create a new CustomObject without initializing attributes obj = CustomObject(type_identifier) return obj def py_object_init(obj: \'CustomObject\', type_identifier: str) -> \'CustomObject\': Initializes the given \'obj\' with the provided type identifier and sets reference count to 1. obj.type_identifier = type_identifier obj.ref_count = 1 return obj def py_object_delete(obj: \'CustomObject\') -> None: Simulates deallocating the object. obj.type_identifier = \\"\\" obj.ref_count = 0"},{"question":"Objective Implement serialization and deserialization of a custom configuration using the `marshal` module and ensure proper handling of unsupported types. Question You are given a task to serialize and deserialize a Python dictionary using the `marshal` module. The dictionary represents a configuration with nested structures. Your implementation should cater to the following requirements: 1. Serialize a configuration dictionary to a binary file. 2. Deserialize the binary file back to a Python dictionary. 3. Ensure unsupported types are handled gracefully by substituting them with `None` if encountered, without crashing the program. 4. Log any instances where unsupported types are substituted. Instructions 1. Implement the function `serialize_configuration(config, file_path)`: - **Input**: - `config`: a dictionary containing the configuration data. - `file_path`: the path to the binary file where the data should be serialized. - **Output**: None - **Side Effects**: Writes serialized configuration data to the specified file. 2. Implement the function `deserialize_configuration(file_path)`: - **Input**: - `file_path`: the path to the binary file from which the data should be deserialized. - **Output**: - `config`: a dictionary containing the deserialized configuration data. - `log`: a list of strings logging any unsupported types found during unmarshalling, in the format `Unsupported type at key \'key_path\': type_name` - **Side Effects**: Reads from the specified file and populates the log with unsupported type notices. 3. Considerations: - The key paths in the log should represent the nested paths in the dictionary, using dot notation (e.g., `settings.database.host`). - You may assume that the keys in the dictionary are all strings. - For this exercise, only focus on the data types mentioned in the documentation. Example ``` config = { \\"app\\": \\"MyApp\\", \\"version\\": 1.0, \\"settings\\": { \\"resolution\\": (1920, 1080), \\"fullscreen\\": True, \\"unsupported_type\\": set([1, 2, 3]) # sets are supported but let\'s assume there\'s an unsupported case } } file_path = \\"config.marshal\\" # Serialize the configuration serialize_configuration(config, file_path) # Deserialize the configuration deserialized_config, log = deserialize_configuration(file_path) # Output log should contain entries for unsupported types print(log) # Example: [\\"Unsupported type at key \'settings.unsupported_type\': set\\"] # deserialized_config should replace the unsupported type with None print(deserialized_config) ``` Note You are only allowed to use the `marshal` module for this task. Ensure your code handles potential errors and edge cases effectively, following best practices for error handling and logging.","solution":"import marshal import logging # Setting up basic logging configuration logging.basicConfig(level=logging.INFO, format=\'%(message)s\') logger = logging.getLogger() def handle_unsupported_types(data, key_path=\\"\\"): Recursively traverses the dictionary to replace unsupported types with None and logs them. if isinstance(data, dict): for key, value in data.items(): new_key_path = f\\"{key_path}.{key}\\" if key_path else key data[key] = handle_unsupported_types(value, new_key_path) elif isinstance(data, (set,)): logger.info(f\\"Unsupported type at key \'{key_path}\': {type(data).__name__}\\") return None elif isinstance(data, list): for index, item in enumerate(data): new_key_path = f\\"{key_path}[{index}]\\" data[index] = handle_unsupported_types(item,new_key_path) return data def serialize_configuration(config, file_path): Serializes the configuration dictionary into a binary file using the marshal module. config = handle_unsupported_types(config) with open(file_path, \'wb\') as f: marshal.dump(config, f) def deserialize_configuration(file_path): Deserializes the configuration dictionary from a binary file using the marshal module. with open(file_path, \'rb\') as f: config = marshal.load(f) return config"},{"question":"# Coding Question: Problem Statement You are required to write a Python function that takes an image file as input, processes its pixel colors using the `colorsys` module, converts the RGB colors to HLS (Hue, Lightness, Saturation) format, modifies the lightness of each pixel, converts the colors back to RGB, and finally outputs the modified image to a new file. Function Signature ```python def adjust_image_lightness(input_file: str, output_file: str, lightness_factor: float) -> None: Adjusts the lightness of an image and saves the result in a new file. Parameters: input_file (str): Path to the input image file output_file (str): Path to save the processed output image file lightness_factor (float): Factor by which to adjust the lightness (1.0 means no change, less than 1.0 means darker, greater than 1.0 means lighter) pass ``` Input - `input_file`: A string representing the path to the input image file in a common format such as PNG or JPEG. - `output_file`: A string representing the path where the processed image should be saved. - `lightness_factor`: A float that specifies how much to adjust the lightness of the image. For example, `0.5` will make the image darker, and `1.5` will make it lighter. Output The function should save the modified image to the path specified by `output_file`. Constraints - You may assume the input file exists and is a valid image file. - You must utilize the `colorsys` module for RGB to HLS and HLS to RGB conversions. - The function does not return any value, but the resulting image should be saved correctly and should reflect the adjusted lightness. Example ```python adjust_image_lightness(\'input.png\', \'output.png\', 0.75) ``` This would take an image `input.png`, adjust its lightness to be 75% of the original, and save the modified image as `output.png`. Notes - You can use the Pillow library (PIL) to handle image processing. To install it, you can use `pip install Pillow`. - Make sure to handle edge cases such as ensuring the lightness factor does not produce values outside the valid range for HLS components.","solution":"from PIL import Image import colorsys def adjust_image_lightness(input_file: str, output_file: str, lightness_factor: float) -> None: Adjusts the lightness of an image and saves the result in a new file. Parameters: input_file (str): Path to the input image file output_file (str): Path to save the processed output image file lightness_factor (float): Factor by which to adjust the lightness (1.0 means no change, less than 1.0 means darker, greater than 1.0 means lighter) image = Image.open(input_file) image = image.convert(\\"RGB\\") pixels = list(image.getdata()) new_pixels = [] for r, g, b in pixels: h, l, s = colorsys.rgb_to_hls(r / 255.0, g / 255.0, b / 255.0) l = max(0, min(1, l * lightness_factor)) r, g, b = colorsys.hls_to_rgb(h, l, s) new_pixels.append((int(r * 255), int(g * 255), int(b * 255))) image.putdata(new_pixels) image.save(output_file)"},{"question":"# Complex Tensor Operations in PyTorch **Objective:** This task aims to evaluate your understanding of handling complex tensors in PyTorch. You will be required to create complex tensors, perform basic and advanced operations on them, and utilize serialization and autograd features. **Instructions:** 1. **Creating Complex Tensors:** - Create two complex tensors `a` and `b`, each of shape `(2, 3)`, using `torch.cfloat` data type. The values should be generated using `torch.randn`. - Print the created tensors. 2. **Basic Operations:** - Extract and print the real and imaginary parts of tensor `a`. - Multiply the real part of tensor `a` by 2 and subtract 1 from the imaginary part. Print the modified tensor `a`. 3. **Linear Algebra Operations:** - Perform a matrix multiplication of complex tensors `a` and `b.T` (transpose of `b`) and store the result in tensor `c`. - Compute and print the singular value decomposition (SVD) of tensor `c`. 4. **Serialization:** - Serialize the tensor `c` to a file named `complex_tensor.pt`. - Deserialize the tensor from the file to a new tensor `d` and print it. 5. **Autograd:** - Create a parameterized complex tensor `p` of shape `(3, 3)`, initialized with random complex values. - Define a simple loss function `loss = torch.sum(p.abs())`. - Compute the gradient of `loss` with respect to `p` using autograd and print the gradient. **Constraints:** - You must use PyTorch\'s complex tensor operations to accomplish each task. - Ensure that all tensor operations are performed in a computationally efficient manner. **What to Submit:** - A Python script implementing the above operations. - The outputs for each of the steps. **Example Output:** ``` Tensor a: tensor([[ 1.7640+0.4002j, 0.9787+1.8676j, 2.2409-0.9773j], [ 1.8676+0.9501j, -0.1514-0.1032j, -0.1032+0.4106j]]) Tensor b: tensor([[ 0.1440+1.4543j, 0.7610+0.1217j, 0.1217-1.0708j], [ 0.4439+0.3337j, 0.3337+1.4941j, -0.2051+0.3130j]]) Real part of tensor a: tensor([[ 1.7640, 0.9787, 2.2409], [ 1.8676, -0.1514, -0.1032]]) Imaginary part of tensor a: tensor([[ 0.4002, 1.8676, -0.9773], [ 0.9501, -0.1032, 0.4106]]) Modified tensor a: tensor([[ 2.5280-0.5999j, 1.9574+0.8676j, 3.4818-1.9773j], [ 3.7352-0.0499j, -0.3028-1.1032j, -0.2064-0.5894j]]) Matrix multiplication result (c): tensor([[ 3.4858+4.6750j, 3.4858+4.6750j, 3.4858+4.6750j], [ 3.4858+4.6750j, 3.4858+4.6750j, 3.4858+4.6750j]]) SVD of tensor c: (U, S, V) = (tensor([[-0.7071+0.7071j, -0.7071-0.7071j], [-0.7071-0.7071j, -0.7071+0.7071j]]), tensor([9.3501, 0.0000]), tensor([[-0.5774+0.j , -0.5793+0.j , -0.5774-0.j ], [-0.5774-0.j , -0.5793-0.j , -0.5774+0.j ]])) Serialized tensor c to \'complex_tensor.pt\'. Deserialized tensor d: tensor([[ 3.4858+4.6750j, 3.4858+4.6750j, 3.4858+4.6750j], [ 3.4858+4.6750j, 3.4858+4.6750j, 3.4858+4.6750j]]) Gradient of the loss function with respect to p: tensor([[1.+0.j, 1.+0.j, 1.+0.j], [1.+0.j, 1.+0.j, 1.+0.j], [1.+0.j, 1.+0.j, 1.+0.j]]) ```","solution":"import torch # 1. Creating Complex Tensors a = torch.randn(2, 3, dtype=torch.cfloat) b = torch.randn(2, 3, dtype=torch.cfloat) print(\\"Tensor a:n\\", a) print(\\"Tensor b:n\\", b) # 2. Basic Operations real_a = a.real imag_a = a.imag print(\\"Real part of tensor a:n\\", real_a) print(\\"Imaginary part of tensor a:n\\", imag_a) a_modified = torch.complex(real_a * 2, imag_a - 1) print(\\"Modified tensor a:n\\", a_modified) # 3. Linear Algebra Operations c = torch.matmul(a, b.T) U, S, V = torch.svd(c) print(\\"Matrix multiplication result (c):n\\", c) print(\\"SVD of tensor c:n(U, S, V):n\\", U, S, V) # 4. Serialization torch.save(c, \'complex_tensor.pt\') d = torch.load(\'complex_tensor.pt\') print(\\"Deserialized tensor d:n\\", d) # 5. Autograd p = torch.randn(3, 3, dtype=torch.cfloat, requires_grad=True) loss = torch.sum(p.abs()) loss.backward() print(\\"Gradient of the loss function with respect to p:n\\", p.grad)"},{"question":"# Question: Financial Calculation with Custom Context You are given a list of financial transactions represented as strings. Each transaction can be a positive or negative decimal number. You need to perform the following steps: 1. **Parse the Transactions**: Convert the list of string transactions into a list of `Decimal` objects. 2. **Set Up Custom Context**: Create a custom decimal context with the following settings: - Precision: 10 decimal places - Rounding: Use ROUND_HALF_UP - Enabled Traps: Overflow, DivisionByZero, InvalidOperation 3. **Calculate the Total**: Compute the sum of all transactions. 4. **Apply Rounding Function**: Round the result to 2 decimal places using `ROUND_HALF_UP`. 5. **Handle Exceptional Conditions**: If any exceptional condition (e.g., overflow, division by zero, invalid operation) occurs during the addition, return \\"Error: Exception Occurred\\". # Input Format - A list of strings `transactions` where each string represents a financial transaction. # Output Format - A string representation of the total amount rounded to 2 decimal places or an error message if an exception occurs. # Example Input: ```python transactions = [\\"100.12345\\", \\"-50.56789\\", \\"200.78901\\", \\"-150.45678\\"] ``` Output: ```python \\"Error: Exception Occurred\\" ``` # Constraints 1. Each transaction string will be a valid decimal number. 2. Perform addition using the given custom context. 3. Handle any exceptions that occur due to the custom context settings. # Requirements - Implement the function `calculate_total(transactions: list) -> str` to meet the specifications. # Function Signature ```python from decimal import Decimal, Context, getcontext, setcontext, ROUND_HALF_UP, InvalidOperation, DivisionByZero, Overflow def calculate_total(transactions: list) -> str: # implementation here ``` You may use the provided example as a reference while writing your solution. Make sure to test your function with different cases, including those that might cause an exception due to the custom context settings.","solution":"from decimal import Decimal, Context, ROUND_HALF_UP, InvalidOperation, DivisionByZero, Overflow, localcontext def calculate_total(transactions: list) -> str: try: # Parse the transactions into Decimal objects decimal_transactions = [Decimal(t) for t in transactions] # Create a custom decimal context custom_context = Context( prec=10, rounding=ROUND_HALF_UP, traps=[Overflow, DivisionByZero, InvalidOperation] ) # Calculate the total sum of transactions within the custom context with localcontext(custom_context): total = sum(decimal_transactions) # Round the result to 2 decimal places using ROUND_HALF_UP final_total = total.quantize(Decimal(\'0.01\'), rounding=ROUND_HALF_UP) return str(final_total) except (Overflow, DivisionByZero, InvalidOperation): return \\"Error: Exception Occurred\\""},{"question":"**Objective:** Write a Python function to analyze and visualize the bytecode instructions of a given Python function. The function should return a summary of the bytecode instructions, including the opcode names and their frequencies. **Input:** - A Python function (e.g., `my_function`). **Output:** - A dictionary where the keys are bytecode instruction names (opcodes) and the values are their respective frequencies in the given function. **Constraints:** 1. The function should handle all types of Python functions, including normal functions, generators, coroutines, and async generators. 2. The function should use the `dis` module to disassemble the given function and analyze its bytecode. **Example:** ```python import dis def analyze_bytecode(func): Analyze the bytecode of a given function and return a summary of instruction frequencies. Args: func (function): A Python function to analyze. Returns: dict: A dictionary with instruction names as keys and their frequencies as values. # Your code here # Example function to analyze def example_func(x, y): return x + y # Analyze the example function bytecode_summary = analyze_bytecode(example_func) print(bytecode_summary) ``` Expected output for `example_func`: ```python { \'LOAD_FAST\': 2, \'BINARY_ADD\': 1, \'RETURN_VALUE\': 1 } ``` **Performance Requirements:** - The function should efficiently handle input functions of varying sizes. - Avoid redundant computations and ensure that the analysis runs in a reasonable time frame for small to medium-sized functions. **Hints:** - Use the `dis.Bytecode` class to get an iterable over the bytecode instructions. - Iterate through the instructions and count the occurrences of each `opname`. - Return the resulting dictionary of instruction frequencies.","solution":"import dis from collections import Counter def analyze_bytecode(func): Analyze the bytecode of a given function and return a summary of instruction frequencies. Args: func (function): A Python function to analyze. Returns: dict: A dictionary with instruction names as keys and their frequencies as values. bytecode = dis.Bytecode(func) instruction_frequencies = Counter(instr.opname for instr in bytecode) return dict(instruction_frequencies) # Example function to analyze def example_func(x, y): return x + y # Analyze the example function bytecode_summary = analyze_bytecode(example_func) print(bytecode_summary)"},{"question":"# Coding Assignment Question Objective: Implement a custom numerical class that leverages Python\'s built-in constants to handle arithmetic operations. Specifically, the class should demonstrate the use of `NotImplemented` in binary operations. Problem Statement: Create a class called `CustomNumber` that supports addition (`+`), subtraction (`-`), and equality (`==`) operations with both `int` and `float` types. If the operation involves any other type, the method should return `NotImplemented`. Additionally, illustrate how the constants `False`, `True`, `None`, and `Ellipsis` could be useful in the context of this class, implementing methods to demonstrate their usage. Requirements: 1. **Class Definition**: - Define the class `CustomNumber`. - The class should have a constructor that accepts one argument, which initializes the internal value of the instance. 2. **Addition and Subtraction**: - Implement the `__add__` and `__sub__` methods to support addition and subtraction with `int` and `float`. - If the other operand is not an `int` or `float`, the method should return `NotImplemented`. 3. **Equality Check**: - Implement the `__eq__` method to support equality checks with `int`, `float`, and other instances of `CustomNumber`. - If the other operand is not an `int`, `float`, or `CustomNumber`, return `NotImplemented`. 4. **Other Methods**: - Implement a method `is_debug_mode` that returns `True` if `__debug__` is `True`, otherwise returns `False`. - Implement a method `reset_value` that sets the internal value to `None`. - Implement a method `ellipsis_example` that returns `Ellipsis`. Input: The class should be able to handle instances comparable with `int` and `float`. Output formats: - Operations should yield results consistent with Python\'s numeric operations, or `NotImplemented` if the operation involves unsupported types. - The methods `is_debug_mode`, `reset_value`, and `ellipsis_example` should provide correct outputs as specified. Constraints: - Ensure that `CustomNumber` instances work seamlessly with addition, subtraction, and equality checks as defined. - Do not use any external libraries; rely on Python\'s built-in capabilities. Example Usage: ```python # Creating instances num1 = CustomNumber(10) num2 = CustomNumber(5.5) # Addition print(num1 + 5) # Output: 15 print(num1 + 5.5) # Output: 15.5 print(num1 + num2) # Output: 15.5 # Subtraction print(num1 - 3) # Output: 7 print(num1 - 2.5) # Output: 7.5 print(num1 - num2) # Output: 4.5 # Equality print(num1 == 10) # Output: True print(num1 == 5) # Output: False print(num1 == num2) # Output: False # Other Methods print(num1.is_debug_mode()) # Output: True or False depending on `__debug__` num1.reset_value() print(num1.value) # Output: None print(num1.ellipsis_example()) # Output: Ellipsis ``` Implement the class as described to meet the requirements.","solution":"class CustomNumber: def __init__(self, value): self.value = value def __add__(self, other): if isinstance(other, (int, float)): return CustomNumber(self.value + other) elif isinstance(other, CustomNumber): return CustomNumber(self.value + other.value) return NotImplemented def __sub__(self, other): if isinstance(other, (int, float)): return CustomNumber(self.value - other) elif isinstance(other, CustomNumber): return CustomNumber(self.value - other.value) return NotImplemented def __eq__(self, other): if isinstance(other, (int, float)): return self.value == other elif isinstance(other, CustomNumber): return self.value == other.value return NotImplemented def is_debug_mode(self): return __debug__ def reset_value(self): self.value = None def ellipsis_example(self): return Ellipsis"},{"question":"**Title: Implement a Custom Scikit-learn Estimator** **Objective:** To assess the students\' understanding of creating a custom estimator compatible with scikit-learn, ensuring that it conforms to the scikit-learn API guidelines. **Problem Statement:** You are required to implement a custom scikit-learn compatible estimator named `SimpleLinearRegressor`. This estimator should perform linear regression using the closed-form solution for least squares regression. The estimator must include methods for fitting the model (`fit`), making predictions (`predict`), and returning the coefficient of determination (`score`). **Requirements:** 1. **Implementation**: - Implement the estimator class `SimpleLinearRegressor` which inherits from `BaseEstimator` and `RegressorMixin`. - The `__init__` method must include the keyword argument `fit_intercept=True`. - The `fit` method should estimate the model parameters using the normal equation method. - The `predict` method should use the fitted parameters to make predictions on new data. - The `score` method should return the R^2 score. 2. **Fit Method**: - Accepts `X` (2D array of shape `(n_samples, n_features)`) and `y` (1D array of shape `(n_samples,)`). - Estimate the coefficients using the closed-form solution: [ beta = (X^T X)^{-1} X^T y ] - If `fit_intercept=False`, assume the data is already centered. - If `fit_intercept=True`, include the intercept term by adding a column of ones to `X`. 3. **Predict Method**: - Accepts `X` (2D array of shape `(n_samples, n_features)`). - Returns the predicted values as a 1D array. 4. **Score Method**: - Accepts `X` (2D array of shape `(n_samples, n_features)`) and `y` (1D array of shape `(n_samples,)`). - Returns the R^2 score of the predictions. **Input and Output Formats:** - The `fit` method should raise a `ValueError` if `X` and `y` have inconsistent numbers of samples. - The methods should ensure proper input validation using scikit-learns utilities as needed. **Performance Constraints:** - The implementation should handle large datasets efficiently by leveraging vectorized operations using NumPy. **Example Usage:** ```python import numpy as np from sklearn.base import BaseEstimator, RegressorMixin class SimpleLinearRegressor(BaseEstimator, RegressorMixin): def __init__(self, fit_intercept=True): self.fit_intercept = fit_intercept def fit(self, X, y): import numpy as np from sklearn.utils.validation import check_X_y X, y = check_X_y(X, y) if self.fit_intercept: X = np.hstack([np.ones((X.shape[0], 1)), X]) # Compute the coefficients using the normal equation X_t = X.T self.coef_ = np.linalg.inv(X_t @ X) @ X_t @ y # Save the intercept separately if needed if self.fit_intercept: self.intercept_ = self.coef_[0] self.coef_ = self.coef_[1:] return self def predict(self, X): from sklearn.utils.validation import check_is_fitted, check_array check_is_fitted(self, [\'coef_\']) X = check_array(X) if self.fit_intercept: return np.dot(X, self.coef_) + self.intercept_ else: return np.dot(X, self.coef_) def score(self, X, y): from sklearn.metrics import r2_score return r2_score(y, self.predict(X)) # Example usage X_train = np.array([[1], [2], [3], [4], [5]]) y_train = np.array([1, 3, 2, 3, 5]) model = SimpleLinearRegressor(fit_intercept=True) model.fit(X_train, y_train) predictions = model.predict(X_train) r2 = model.score(X_train, y_train) print(predictions) print(r2) ``` **Note:** - Ensure to follow the scikit-learn conventions for parameter management, input validation, and attribute naming. - Use appropriate scikit-learn utility functions for validation and checks. **Evaluation Criteria:** - Correctness and completeness of the implementation. - Adherence to the scikit-learn API conventions. - Proper handling of input validation and parameter management. - Code readability and use of vectorized operations for efficiency.","solution":"import numpy as np from sklearn.base import BaseEstimator, RegressorMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted class SimpleLinearRegressor(BaseEstimator, RegressorMixin): def __init__(self, fit_intercept=True): self.fit_intercept = fit_intercept def fit(self, X, y): X, y = check_X_y(X, y) if self.fit_intercept: X = np.hstack([np.ones((X.shape[0], 1)), X]) # Compute the coefficients using the normal equation X_t = X.T try: self.coef_ = np.linalg.inv(X_t @ X) @ X_t @ y except np.linalg.LinAlgError as e: raise ValueError(\\"Singular matrix detected. The input features may be collinear or insufficient.\\") from e # Save the intercept separately if needed if self.fit_intercept: self.intercept_ = self.coef_[0] self.coef_ = self.coef_[1:] return self def predict(self, X): check_is_fitted(self, [\'coef_\']) X = check_array(X) if self.fit_intercept: return np.dot(X, self.coef_) + self.intercept_ else: return np.dot(X, self.coef_) def score(self, X, y): from sklearn.metrics import r2_score return r2_score(y, self.predict(X))"},{"question":"# Python Coding Assessment: Advanced String Formatting and Conversion Objective: Implement a function that leverages various string conversion and formatting features provided by Python 3.10 as described in the documentation. The goal is to assess your understanding of these functions and your ability to handle edge cases and ensure robust implementation. Problem Statement: You are required to implement a function `convert_and_format(data: list[tuple[str, float]]) -> str` that accepts a list of tuples. Each tuple contains a string format specification and a floating-point number. The function should produce a formatted string output according to the specified format rules for each floating-point number. Additionally, the function should handle various edge cases, such as very large numbers, special floating-point values (like infinity and NaN), and invalid format specifications. The format specification strings (`format_spec`) should be of the form: - `\'e\'` or `\'E\'`: Scientific notation. - `\'f\'` or `\'F\'`: Fixed-point notation. - `\'g\'` or `\'G\'`: General format. - `\'r\'`: Standard `repr()` format. The function should: 1. Format each floating-point number in the input `data` list using the corresponding format specification. 2. Ensure the formatted string is always null-terminated. 3. Handle cases where the formatted output might be too long to fit in the buffer provided. 4. Convert strings to floating-point numbers robustly and handle potential errors. 5. Return a single string combining all the formatted outputs, separated by a comma and a space. Function Signature: ```python def convert_and_format(data: list[tuple[str, float]]) -> str: ``` Input: - `data` (list of tuples): A list where each tuple contains: - `format_spec` (str): A format specification string (`\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, `\'r\'`). - `number` (float): A floating-point number to be formatted according to `format_spec`. Output: - Returns a single string with each formatted number from the input `data` list, separated by a comma and a space. Constraints: - `1 <= len(data) <= 100` - Format specification string will be one of the specified values. - Floating-point numbers can include normal values, very large or very small values, infinities, and NaN. Example: ```python data = [(\'e\', 123456.789), (\'f\', 0.0001234), (\'r\', float(\'nan\')), (\'g\', 1e100)] output = convert_and_format(data) print(output) # Output: \'1.234568e+05, 0.000123, nan, 1e+100\' ``` Notes: - You may not use any built-in string formatting functions like `format()` or `f-strings` for the actual formatting. - Pay attention to edge cases such as overly long output, invalid format specifications, and handling special floating-point values (infinity, NaN). Use only the functions and methodologies outlined in the documentation provided to solve this problem.","solution":"def convert_and_format(data): Converts and formats a list of tuples of format specification strings and floats. Args: data (list of tuple): Each tuple contains a format specification string and a float. Returns: str: Formatted string combining all formatted outputs, separated by a comma and a space. formatted_strings = [] for format_spec, number in data: try: if format_spec == \'e\': formatted_strings.append(format(number, \'.6e\')) elif format_spec == \'E\': formatted_strings.append(format(number, \'.6E\')) elif format_spec == \'f\': formatted_strings.append(format(number, \'.6f\')) elif format_spec == \'F\': formatted_strings.append(format(number, \'.6F\')) elif format_spec == \'g\': formatted_strings.append(format(number, \'.6g\')) elif format_spec == \'G\': formatted_strings.append(format(number, \'.6G\')) elif format_spec == \'r\': formatted_strings.append(repr(number)) else: raise ValueError(f\\"Unknown format specification: {format_spec}\\") except (ValueError, OverflowError) as e: formatted_strings.append(\'ERROR\') return \', \'.join(formatted_strings)"},{"question":"# Problem: Random Sequence Operations In this task, you will perform a series of operations involving random numbers and sequences. This will test your understanding of generating and manipulating random sequences and ensuring reproducibility. Requirements 1. **Function 1: `generate_random_sequence(seed, length)`** - Given an integer seed and an integer length, generate a sequence of random integers of the specified length. - The sequence should be reproducible if the same seed is provided. - Return the generated sequence. 2. **Function 2: `sequence_statistics(sequence)`** - Given a sequence of integers, calculate and return the following statistics: - Minimum value in the sequence - Maximum value in the sequence - Mean of the sequence - Standard deviation of the sequence 3. **Function 3: `shuffle_sequence(seed, sequence)`** - Given an integer seed and a sequence of integers, shuffle the sequence in a reproducible manner using the provided seed. - Return the shuffled sequence. 4. **Function 4: `sample_elements(seed, sequence, k)`** - Given an integer seed, a sequence of integers, and an integer k, select k unique elements from the sequence without replacement in a reproducible manner using the provided seed. - Return the list of sampled elements. Implementation Details: - Use the `random` module to implement the functions. - Ensure that the same seed value leads to the same random sequence, shuffled sequence, and sample across different runs. - You are not allowed to use external libraries other than those in the Python standard library. Example: ```python # Function 1 Example print(generate_random_sequence(10, 5)) # Output: [73, 48, 8, 88, 42] # Function 2 Example print(sequence_statistics([1, 2, 3, 4, 5])) # Output: (1, 5, 3.0, 1.5811388300841898) # Function 3 Example print(shuffle_sequence(5, [1, 2, 3, 4, 5])) # Output: [5, 1, 3, 2, 4] # Function 4 Example print(sample_elements(6, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)) # Output: [7, 9, 4] ``` Constraints: - 1 <= `seed`, `length`, `k` <= 1000 - The `sequence` input will always have at least as many elements as `k`. Notes: - You can use Python\'s inbuilt statistical functions where necessary. - Ensure the repeatability of the random operations via seeding. Functions Signature: ```python def generate_random_sequence(seed: int, length: int) -> list: pass def sequence_statistics(sequence: list) -> tuple: pass def shuffle_sequence(seed: int, sequence: list) -> list: pass def sample_elements(seed: int, sequence: list, k: int) -> list: pass ```","solution":"import random from statistics import mean, stdev def generate_random_sequence(seed: int, length: int) -> list: Generate a sequence of random integers of the specified length, reproducible by the seed. random.seed(seed) return [random.randint(1, 100) for _ in range(length)] def sequence_statistics(sequence: list) -> tuple: Calculate statistics: minimum, maximum, mean, and standard deviation of a sequence of integers. return min(sequence), max(sequence), mean(sequence), stdev(sequence) def shuffle_sequence(seed: int, sequence: list) -> list: Shuffle a sequence of integers in a reproducible manner using the provided seed. random.seed(seed) shuffled_sequence = sequence[:] random.shuffle(shuffled_sequence) return shuffled_sequence def sample_elements(seed: int, sequence: list, k: int) -> list: Select k unique elements from the sequence without replacement in a reproducible manner using the provided seed. random.seed(seed) return random.sample(sequence, k)"},{"question":"You have been provided with a text data file containing a large amount of data that needs to be compressed and decompressed using the `zlib` module in Python. Your task is to write two functions: 1. `compress_data(data: bytes, level: int = -1) -> bytes`: Compresses the given data using the specified compression level. 2. `decompress_data(data: bytes) -> bytes`: Decompresses the given compressed data. Additionally, you need to handle and raise appropriate exceptions if any errors occur during the compression and decompression processes. Function Specifications 1. `compress_data(data: bytes, level: int = -1) -> bytes`: - **Input**: - `data`: A `bytes` object representing the raw data to be compressed. - `level`: An `int` representing the level of compression (from 0 to 9, or -1 for default). - **Output**: - Returns a `bytes` object representing the compressed data. - **Constraints**: - If the compression fails, raise a `zlib.error` exception with an appropriate message. 2. `decompress_data(data: bytes) -> bytes`: - **Input**: - `data`: A `bytes` object representing the compressed data. - **Output**: - Returns a `bytes` object representing the decompressed data. - **Constraints**: - If the decompression fails, raise a `zlib.error` exception with an appropriate message. Example Usage ```python try: raw_data = b\\"This is some sample data that needs to be compressed.\\" compressed_data = compress_data(raw_data, level=6) decompressed_data = decompress_data(compressed_data) assert decompressed_data == raw_data print(\\"Compression and decompression successful!\\") except zlib.error as e: print(f\\"An error occurred during compression/decompression: {e}\\") ``` Performance Requirements - The functions should handle large input sizes efficiently. - It should process streaming data chunks if necessary. Notes - You can assume the data provided for compression is in bytes. - Handle all possible exceptions from the `zlib` library to ensure robustness. Implement these functions in Python using the `zlib` module.","solution":"import zlib def compress_data(data: bytes, level: int = -1) -> bytes: Compresses the given data using the specified compression level. Args: data (bytes): The raw data to be compressed. level (int): The level of compression (from 0 to 9, or -1 for default). Returns: bytes: The compressed data. Raises: zlib.error: If compression fails. try: return zlib.compress(data, level) except zlib.error as e: raise zlib.error(f\\"Compression failed: {e}\\") def decompress_data(data: bytes) -> bytes: Decompresses the given compressed data. Args: data (bytes): The compressed data. Returns: bytes: The decompressed data. Raises: zlib.error: If decompression fails. try: return zlib.decompress(data) except zlib.error as e: raise zlib.error(f\\"Decompression failed: {e}\\")"},{"question":"Coding Assessment Question: # Objective: Implement a multiprocessing solution that demonstrates proficiency in: - Sharing CPU tensors using different strategies. - Managing subprocess creation and termination. - Proper handling of shared memory and inter-process communication in PyTorch. # Problem Statement: You are required to implement a PyTorch multiprocessing solution where: 1. A producer process creates a large tensor and shares it with multiple consumer processes. 2. Each consumer process performs a computation on the received tensor. 3. Properly manage the shared memory to ensure there are no memory leaks. 4. Handle any potential errors in subprocesses and ensure all processes terminate correctly. # Expected Function Implementation: You need to implement the following functions: Function: `producer` The producer function should: - Create a large tensor. - Use a queue to share this tensor with the consumer processes. - Use a specified sharing strategy to manage the shared memory. ```python import torch import torch.multiprocessing as mp def producer(queue, sharing_strategy): torch.multiprocessing.set_sharing_strategy(sharing_strategy) tensor = torch.randn((1000, 1000)) queue.put(tensor) print(\\"Producer has sent the tensor.\\") ``` Function: `consumer` The consumer function should: - Retrieve the tensor from the queue. - Perform a simple computation (e.g., calculating the sum of all elements). - Ensure the tensor is deleted as soon as possible to free up memory. ```python def consumer(queue): tensor = queue.get() result = torch.sum(tensor) print(f\\"Consumer computed sum: {result.item()}\\") del tensor ``` Function: `main` The main function should: - Initialize the queue and event objects. - Launch the producer and multiple consumer processes. - Ensure proper termination of all processes even in case of errors. ```python def main(num_consumers, sharing_strategy): mp.set_start_method(\'spawn\') queue = mp.Queue() # Start producer process producer_process = mp.Process(target=producer, args=(queue, sharing_strategy)) producer_process.start() # Start consumer processes consumer_processes = [] for _ in range(num_consumers): p = mp.Process(target=consumer, args=(queue,)) consumer_processes.append(p) p.start() # Join all processes producer_process.join() for p in consumer_processes: p.join() if __name__ == \\"__main__\\": main(num_consumers=4, sharing_strategy=\'file_descriptor\') ``` # Input and Output: - **Input**: No user input is required. The script should directly run the `main` function with predefined arguments. - **Output**: Print statements from producer and consumer processes indicating the tensor has been sent and computations are done. # Constraints: - Use only PyTorch and Python\'s multiprocessing libraries. - Ensure there are no memory leaks. - Properly handle process synchronization and termination. # Performance Requirements: - The solution should handle a reasonably large tensor (e.g., 1000x1000 elements). - Ensure efficient memory and process management to avoid resource exhaustion. **Note**: The solution should work on systems that support the specified sharing strategies and spawning methods.","solution":"import torch import torch.multiprocessing as mp def producer(queue, sharing_strategy): Function to create a large tensor and share it using a queue. torch.multiprocessing.set_sharing_strategy(sharing_strategy) tensor = torch.randn((1000, 1000)) queue.put(tensor) print(\\"Producer has sent the tensor.\\") def consumer(queue): Function to retrieve tensor from the queue and perform computation. tensor = queue.get() result = torch.sum(tensor) print(f\\"Consumer computed sum: {result.item()}\\") del tensor def main(num_consumers, sharing_strategy): mp.set_start_method(\'spawn\') queue = mp.Queue() # Start producer process producer_process = mp.Process(target=producer, args=(queue, sharing_strategy)) producer_process.start() # Start consumer processes consumer_processes = [] for _ in range(num_consumers): p = mp.Process(target=consumer, args=(queue,)) consumer_processes.append(p) p.start() # Join all processes producer_process.join() for p in consumer_processes: p.join() if __name__ == \\"__main__\\": main(num_consumers=4, sharing_strategy=\'file_descriptor\')"},{"question":"You are provided with a custom class `Person` that represents a person\'s information, including their name and age. The default behavior of the `pickle` and `copy` modules is not adequate for this class, so you need to implement custom pickling and copying functions for it. # Requirements 1. Implement the `Person` class which includes: - `__init__` method to initialize `name` and `age`. - `__repr__` method to return a string representation of the instance. 2. Implement a custom pickling function `pickle_person` for the `Person` class, which will be used to serialize and deserialize objects of this class. 3. Register the custom pickling function using the `copyreg` module. 4. Demonstrate that your custom pickling function is used correctly by the `pickle` and `copy` modules through examples. # Input and Output - **Input**: No direct input from the user. Instead, demonstrate functionality through the instantiation and pickling/copying of `Person` objects. - **Output**: Printed statements demonstrating the pickling and copying process and the correctness of the result. # Constraints - The `Person` class must support deep copying and pickling. - The custom pickling function should be correctly registered with the `copyreg` module. # Performance Requirements - The solution should handle the pickling and copying of `Person` objects effectively without any errors. - Ensure that the custom pickling function is correctly called during the processes. # Example Here is an example of how the implementation should work: ```python import copyreg import copy import pickle class Person: def __init__(self, name, age): self.name = name self.age = age def __repr__(self): return f\\"Person(name={self.name}, age={self.age})\\" def pickle_person(person): print(\\"Pickling a Person instance...\\") return Person, (person.name, person.age) # Registering the custom pickling function copyreg.pickle(Person, pickle_person) # Create an instance of Person p1 = Person(\'Alice\', 30) print(\\"Original:\\", p1) # Demonstrate copying p2 = copy.copy(p1) print(\\"Copy:\\", p2) # Demonstrate pickling pickled_person = pickle.dumps(p1) print(\\"Pickled Person:\\", pickled_person) # Demonstrate unpickling unpickled_person = pickle.loads(pickled_person) print(\\"Unpickled Person:\\", unpickled_person) ``` In this example, the custom pickling function `pickle_person` is registered and used to serialize (`pickle`) and copy (`copy`) `Person` instances. The output should show the correct pickling and copying process, verifying that the custom function is utilized.","solution":"import copyreg import copy import pickle class Person: def __init__(self, name, age): self.name = name self.age = age def __repr__(self): return f\\"Person(name={self.name}, age={self.age})\\" def pickle_person(person): Function to serialize a Person instance. print(\\"Pickling a Person instance...\\") return Person, (person.name, person.age) # Registering the custom pickling function for the Person class copyreg.pickle(Person, pickle_person) # Demonstration # Create an instance of Person p1 = Person(\'Alice\', 30) print(\\"Original:\\", p1) # Demonstrate copying p2 = copy.copy(p1) print(\\"Copy:\\", p2) # Demonstrate pickling pickled_person = pickle.dumps(p1) print(\\"Pickled Person:\\", pickled_person) # Demonstrate unpickling unpickled_person = pickle.loads(pickled_person) print(\\"Unpickled Person:\\", unpickled_person)"},{"question":"Objective: Implement a class in Python that mimics some functionalities of the Python dictionary using the concept of mappings, leveraging the standard functionalities and checks as outlined in the given documentation. Requirements: 1. Create a class named `CustomMapping` which should allow the following operations: - Checking if the object is a mapping. - Adding, retrieving, and deleting items. - Checking if a key exists. - Retrieving the size of the mapping. - Retrieving lists of keys, values, or key-value pairs (items). Specifications: - **Class Name**: `CustomMapping` - **Methods**: - `__getitem__(self, key)`: Retrieve an item by key. - `__setitem__(self, key, value)`: Set a key-value pair. - `__delitem__(self, key)`: Delete an item by key. - `__len__(self)`: Return the size of the mapping. - `__contains__(self, key)`: Check if a key exists in the mapping. - `keys(self)`: Return a list of keys. - `values(self)`: Return a list of values. - `items(self)`: Return a list of key-value pairs (tuples). - `is_mapping(self)`: Return True (since this class is a mapping). - **Constraints**: - Assume all keys are strings. - Assume all values are valid Python objects. - The implementation should not use the built-in `dict` directly but can utilize lists and tuples to store the mapping internally. Example Usage: ```python mapping = CustomMapping() mapping[\\"a\\"] = 1 mapping[\\"b\\"] = 2 print(mapping[\\"a\\"]) # Output: 1 print(len(mapping)) # Output: 2 print(\\"b\\" in mapping) # Output: True del mapping[\\"a\\"] print(len(mapping)) # Output: 1 print(mapping.keys()) # Output: [\'b\'] print(mapping.values()) # Output: [2] print(mapping.items()) # Output: [(\'b\', 2)] print(mapping.is_mapping()) # Output: True ``` Notes: - Focus on the functionality and correctness of the operations rather than performance optimizations. - Ensure proper error handling accordingly (e.g., KeyError for non-existent keys). This question is designed to test your understanding of basic and advanced mapping operations in Python, including custom class creation, special methods, and handling key-value pairs efficiently.","solution":"class CustomMapping: def __init__(self): self._items = [] def __getitem__(self, key): for k, v in self._items: if k == key: return v raise KeyError(key) def __setitem__(self, key, value): for index, (k, v) in enumerate(self._items): if k == key: self._items[index] = (key, value) return self._items.append((key, value)) def __delitem__(self, key): for index, (k, v) in enumerate(self._items): if k == key: del self._items[index] return raise KeyError(key) def __len__(self): return len(self._items) def __contains__(self, key): for k, v in self._items: if k == key: return True return False def keys(self): return [k for k, v in self._items] def values(self): return [v for k, v in self._items] def items(self): return self._items[:] def is_mapping(self): return True"},{"question":"# Challenge: Implementing and Extending Data Classes in Python Problem Statement You are required to implement a data class to manage a library\'s book inventory. Each book in the inventory will be represented by a dataclass `Book` with the following attributes: - `title`: (str) The title of the book. - `author`: (str) The author of the book. - `publication_year`: (int) The year the book was published. - `isbn`: (str) The International Standard Book Number. - `quantity`: (int) The number of copies of the book in the library. This should have a default value of `0`. Additionally, implement another dataclass `Library` to manage the entire book inventory. This `Library` class should have the following components: - `name`: (str) The name of the library. - `books`: (list) A list that will store instances of the `Book` dataclass. You must also implement the following methods within the `Library` class: 1. `add_book(book: Book) -> None`: A method to add a new book to the inventory. If a book with the same `isbn` already exists, it should update the quantity. 2. `total_books() -> int`: A method that returns the total number of books in the library\'s inventory. 3. `__post_init__(self)`: Ensure all books in the inventory have unique ISBN numbers. If duplicates are found during initialization, raise a `ValueError` indicating \\"Duplicate ISBN found\\". Input and Output Formats - `Book` instances are created normally using the generated `__init__()` method. - `Library` instance is created with a name and a list of books. - Methods `add_book`, `total_books`, and `__post_init__` should handle the functionality as described. Example ```python from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str publication_year: int isbn: str quantity: int = 0 @dataclass class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book) -> None: for b in self.books: if b.isbn == book.isbn: b.quantity += book.quantity break else: # if no break self.books.append(book) def total_books(self) -> int: return sum(book.quantity for book in self.books) def __post_init__(self): isbn_set = set() for book in self.books: if book.isbn in isbn_set: raise ValueError(\\"Duplicate ISBN found\\") isbn_set.add(book.isbn) # Example usage try: library = Library(\\"City Library\\", [ Book(\\"Book A\\", \\"Author A\\", 2000, \\"ISBN001\\", 5), Book(\\"Book B\\", \\"Author B\\", 2005, \\"ISBN002\\", 3), Book(\\"Book A\\", \\"Author A\\", 2000, \\"ISBN001\\", 2) ]) except ValueError as e: print(e) # Output: Duplicate ISBN found library = Library(\\"City Library\\") library.add_book(Book(\\"Book C\\", \\"Author C\\", 2010, \\"ISBN003\\", 4)) library.add_book(Book(\\"Book D\\", \\"Author D\\", 2012, \\"ISBN004\\", 1)) total = library.total_books() # Returns: 5 print(total) # Output: 5 ``` Constraints - You must use `dataclass` decorators for both `Book` and `Library` classes. - Ensure no two `Book` instances have the same `isbn` upon initialization of `Library`. - Implement all methods as specified. Note This question tests your understanding of the `dataclasses` module, including how to handle mutable defaults, post-initialization processing, and method implementation within data classes. Ensure your code is well-structured and handles edge cases.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str publication_year: int isbn: str quantity: int = 0 @dataclass class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book) -> None: for b in self.books: if b.isbn == book.isbn: b.quantity += book.quantity break else: # if no break self.books.append(book) def total_books(self) -> int: return sum(book.quantity for book in self.books) def __post_init__(self): isbn_set = set() for book in self.books: if book.isbn in isbn_set: raise ValueError(\\"Duplicate ISBN found\\") isbn_set.add(book.isbn)"},{"question":"Unix Group Database Analysis **Objective**: You are required to write a function that processes the Unix group database to gather detailed insights about the groups and their memberships. The function will allow us to determine various pieces of information, including the list of all groups and their members and finding groups based on specific criteria. **Function Signature**: ```python def analyze_group_database(group_name_list=None): Analyzes the Unix group database and returns information based on the criteria provided. Parameters: - group_name_list (list): A list of group names (strings) to gather information about. If None, information for all groups is returned. Returns: - result (dict): A dictionary containing the analysis information with the following structure: { \'total_groups\': int, \'group_details\': [ { \'name\': str, \'gid\': int, \'members\': list of str, }, ... ], \'groups_without_members\': list of str, \'duplicate_gid_groups\': list of str, } pass ``` # Requirements: 1. The function `analyze_group_database()` should: - Return a dictionary with the following keys: - `total_groups`: Total number of groups processed. - `group_details`: A list of dictionaries, each containing details of a group with keys `name`, `gid`, and `members`. - `groups_without_members`: A list of group names that have no members. - `duplicate_gid_groups`: A list of group names that have duplicate GIDs in the system. 2. The function must handle the case where `group_name_list` is `None`. In this case, it should return information about all groups available in the database. 3. The function should raise a `ValueError` if any of the group names in the `group_name_list` do not exist in the system. 4. Make use of the `grp` module functions (`getgrgid`, `getgrnam`, and `getgrall`) to access the group database entries. # Example: Given the following group database entries: ```python # Example representation, not actual code output [ (\'staff\', \'\', 50, [\'alice\', \'bob\']), (\'admin\', \'\', 100, []), (\'users\', \'\', 1000, [\'charlie\']), (\'managers\', \'\', 1001, [\'dave\']), ] ``` When calling: ```python analyze_group_database() ``` The function might return: ```python { \'total_groups\': 4, \'group_details\': [ {\'name\': \'staff\', \'gid\': 50, \'members\': [\'alice\', \'bob\']}, {\'name\': \'admin\', \'gid\': 100, \'members\': []}, {\'name\': \'users\', \'gid\': 1000, \'members\': [\'charlie\']}, {\'name\': \'managers\', \'gid\': 1001, \'members\': [\'dave\']} ], \'groups_without_members\': [\'admin\'], \'duplicate_gid_groups\': [] } ``` **Notes**: - When comparing or outputting lists, the order of elements does not matter. - The function should be efficient in accessing and processing the database entries, making effective use of the module\'s built-in functions.","solution":"import grp def analyze_group_database(group_name_list=None): Analyzes the Unix group database and returns information based on the criteria provided. Parameters: - group_name_list (list): A list of group names (strings) to gather information about. If None, information for all groups is returned. Returns: - result (dict): A dictionary containing the analysis information. all_groups = grp.getgrall() group_details = [] groups_without_members = [] gid_count = {} duplicate_gid_groups = [] target_groups = group_name_list if group_name_list is not None else [grp.gr_name for grp in all_groups] # Validate the provided group names if any if group_name_list: all_group_names = set(grp.gr_name for grp in all_groups) for group_name in group_name_list: if group_name not in all_group_names: raise ValueError(f\\"Group {group_name} does not exist\\") for group in all_groups: if group.gr_name in target_groups: group_details.append({ \'name\': group.gr_name, \'gid\': group.gr_gid, \'members\': group.gr_mem }) if not group.gr_mem: groups_without_members.append(group.gr_name) if group.gr_gid in gid_count: gid_count[group.gr_gid].append(group.gr_name) else: gid_count[group.gr_gid] = [group.gr_name] for gid, names in gid_count.items(): if len(names) > 1: duplicate_gid_groups.extend(names) result = { \'total_groups\': len(group_details), \'group_details\': group_details, \'groups_without_members\': groups_without_members, \'duplicate_gid_groups\': duplicate_gid_groups } return result"},{"question":"# Advanced Python Sequence Protocol Implementation In this question, you are required to implement a custom Python class `CustomSequence` that mimics some behaviors of a list. This class should implement several sequence protocol methods as defined in the documentation. The following methods must be implemented: 1. `__getitem__(self, index)`: Should return the element at the specified index. 2. `__setitem__(self, index, value)`: Should set the element at the specified index to the given value. 3. `__delitem__(self, index)`: Should delete the element at the specified index. 4. `__len__(self)`: Should return the length of the sequence. 5. `__contains__(self, value)`: Should return whether the value is contained in the sequence. 6. `__iter__(self)`: Should return an iterator for the sequence. Additionally, you should implement the following methods using the Python C-API functions provided: 7. `count(self, value)`: Should return the count of occurrences of value in the sequence. 8. `index(self, value)`: Should return the index of the first occurrence of value in the sequence, or raise a `ValueError` if it is not present. Specifications: - `CustomSequence` should internally use a Python list to store the elements. - `CustomSequence` should have an initial capacity set by the user, and it should be able to dynamically resize as elements are added/removed. Example Usage: ```python seq = CustomSequence() # Add elements to the sequence seq.extend([1, 2, 3, 4, 5]) # Get item at index 2 print(seq[2]) # Output: 3 # Set item at index 2 seq[2] = 9 print(seq[2]) # Output: 9 # Delete item at index 2 del seq[2] print(seq[2]) # Output: 4 # Check length of sequence print(len(seq)) # Output: 4 # Check if value is in sequence print(3 in seq) # Output: False print(4 in seq) # Output: True # Use the count method implemented with C-API print(seq.count(2)) # Output: 1 # Use the index method implemented with C-API print(seq.index(4)) # Output: 2 ``` **Note:** For this question, you should not directly use the standard Python list methods like `list.count` or `list.index`. Instead, use the corresponding Python C-API functions as detailed in the documentation provided. Constraints: - Avoid using any external libraries apart from the standard Python C-API functions as described. - Ensure that all edge cases are handled, such as accessing out-of-bound indices, handling empty sequences, etc.","solution":"class CustomSequence: def __init__(self, initial_capacity=10): self._data = [None] * initial_capacity self._size = 0 def __getitem__(self, index): if index < 0 or index >= self._size: raise IndexError(\\"Index out of bounds\\") return self._data[index] def __setitem__(self, index, value): if index < 0 or index >= self._size: raise IndexError(\\"Index out of bounds\\") self._data[index] = value def __delitem__(self, index): if index < 0 or index >= self._size: raise IndexError(\\"Index out of bounds\\") self._data.pop(index) self._size -= 1 def __len__(self): return self._size def __contains__(self, value): for i in range(self._size): if self._data[i] == value: return True return False def __iter__(self): for i in range(self._size): yield self._data[i] def count(self, value): count = 0 for i in range(self._size): if self._data[i] == value: count += 1 return count def index(self, value): for i in range(self._size): if self._data[i] == value: return i raise ValueError(f\\"{value} is not in sequence\\") def append(self, value): if self._size >= len(self._data): self._resize() self._data[self._size] = value self._size += 1 def extend(self, iterable): for item in iterable: self.append(item) def _resize(self): new_capacity = len(self._data) * 2 new_data = [None] * new_capacity for i in range(self._size): new_data[i] = self._data[i] self._data = new_data"},{"question":"**Question:** You are given a dataset containing features with varying scales and distributions, which poses challenges for directly applying machine learning models. To address this, you will preprocess the data using feature scaling, apply dimensionality reduction via Principal Component Analysis (PCA), and finally, use a supervised learning model to predict a target variable. # Task 1. **Load Data**: The dataset `data.csv` contains the following columns: - `features`: A set of numerical feature columns. - `target`: A numerical target column to be predicted. 2. **Preprocess Data**: - Use `StandardScaler` from `sklearn.preprocessing` to standardize the feature columns. 3. **Dimensionality Reduction**: - Apply `PCA` from `sklearn.decomposition` to reduce the dimensionality of the standardized features. Retain enough components to explain at least 95% of the variance. 4. **Model Training**: - Use a supervised model of your choice from `sklearn` (e.g., `LinearRegression`, `RandomForestRegressor`) to train on the reduced feature set and predict the target variable. 5. **Evaluation**: - Split the dataset into train and test sets (e.g., 80-20 split). - Evaluate the model performance using Mean Absolute Error (MAE) on the test set. # Expected Function ```python def preprocess_and_train_model(file_path: str) -> float: Load the dataset from the given file path, preprocess the features, apply PCA for dimensionality reduction, and train a regression model. Return the Mean Absolute Error on the test set. Parameters: - file_path (str): The path to the CSV file containing the dataset. Returns: - float: The Mean Absolute Error (MAE) on the test set. pass ``` # Constraints and Assumptions - The dataset fits in memory and has no missing values. - You may use any supervised regression model from scikit-learn. - Ensure that at least 95% of the variance is retained after PCA. - Use appropriate random seeds for reproducibility of train-test split and model training. # Example If the CSV file `data.csv` contains: ``` feature1, feature2, feature3, target 0.5, 1.5, 2.5, 10 1.0, 2.0, 3.0, 20 ... ``` The function `preprocess_and_train_model(\\"data.csv\\")` might output `3.5` (MAE value). # Additional Information Refer to the scikit-learn documentation for examples on using `StandardScaler`, `PCA`, and regression models: - StandardScaler: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html - PCA: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html - Regression models: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error def preprocess_and_train_model(file_path: str) -> float: # Load dataset data = pd.read_csv(file_path) features = data.drop(columns=[\'target\']) target = data[\'target\'] # Standardize the features scaler = StandardScaler() standardized_features = scaler.fit_transform(features) # Apply PCA to retain 95% of the variance pca = PCA(n_components=0.95) reduced_features = pca.fit_transform(standardized_features) # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(reduced_features, target, test_size=0.2, random_state=42) # Train a RandomForestRegressor model model = RandomForestRegressor(random_state=42) model.fit(X_train, y_train) # Predict on test set predictions = model.predict(X_test) # Evaluate using Mean Absolute Error mae = mean_absolute_error(y_test, predictions) return mae"},{"question":"# Complex Class Hierarchy and Iteration Problem Statement You are required to design a class hierarchy for an inventory management system used by a warehouse. The system should include items, categories of items, and specific behaviors for fragile items. Additionally, you must implement iteration capabilities to list all items in the inventory. Requirements 1. **Base Class `Item`:** - Attributes: - `name` (string): name of the item. - `quantity` (int): number of items in stock. - Methods: - `__init__(self, name: str, quantity: int)`: Constructor to initialize the name and quantity. - `__str__(self)`: Returns a string representation of the item in the form \\"Item: name (Qty: quantity)\\". 2. **Class `Category`, derived from `Item`:** - Additional Attribute: - `category_name` (string): name of the category. - Methods: - `__init__(self, name: str, quantity: int, category_name: str)`: Constructor to initialize name, quantity, and category name by calling the base class constructor. - `__str__(self)`: Returns a string representation in the form \\"Item: name (Qty: quantity, Category: category_name)\\". 3. **Class `FragileItem`, derived from `Category`:** - Additional Attribute: - `fragility_level` (string): level of fragility (e.g., high, medium, low). - Methods: - `__init__(self, name: str, quantity: int, category_name: str, fragility_level: str)`: Constructor to initialize name, quantity, category name, and fragility level by calling the base class constructor. - `__str__(self)`: Returns a string representation in the form \\"Fragile Item: name (Qty: quantity, Category: category_name, Fragility: fragility_level)\\". 4. **Class `Warehouse`:** - Attributes: - `inventory` (list): A list to store `Item` objects. - Methods: - `__init__(self)`: Initializes an empty inventory list. - `add_item(self, item: Item)`: Adds an item to the inventory. - `__iter__(self)`: Returns an iterator that iterates over the items in the inventory. - `__next__(self)`: Provides the next item in the iteration. Example Usage ```python # Create instances of items item1 = Item(\\"Laptop\\", 10) item2 = Category(\\"Printer\\", 5, \\"Electronics\\") item3 = FragileItem(\\"Glass Vase\\", 3, \\"Decor\\", \\"high\\") # Create a warehouse and add items to it warehouse = Warehouse() warehouse.add_item(item1) warehouse.add_item(item2) warehouse.add_item(item3) # Iterate through the warehouse inventory for item in warehouse: print(item) # Expected output: # Item: Laptop (Qty: 10) # Item: Printer (Qty: 5, Category: Electronics) # Fragile Item: Glass Vase (Qty: 3, Category: Decor, Fragility: high) ``` Constraints - `quantity` should be a non-negative integer. - `fragility_level` should be one of {\\"high\\", \\"medium\\", \\"low\\"}. Notes - Ensure that the `Warehouse` class correctly utilizes iterators so it can be iterated using a `for` loop. - Use name mangling to ensure the `fragility_level` attribute in `FragileItem` is encapsulated and cannot be accessed directly.","solution":"class Item: def __init__(self, name: str, quantity: int): if quantity < 0: raise ValueError(\\"Quantity must be non-negative.\\") self.name = name self.quantity = quantity def __str__(self): return f\\"Item: {self.name} (Qty: {self.quantity})\\" class Category(Item): def __init__(self, name: str, quantity: int, category_name: str): super().__init__(name, quantity) self.category_name = category_name def __str__(self): return f\\"Item: {self.name} (Qty: {self.quantity}, Category: {self.category_name})\\" class FragileItem(Category): def __init__(self, name: str, quantity: int, category_name: str, fragility_level: str): if fragility_level not in {\\"high\\", \\"medium\\", \\"low\\"}: raise ValueError(\\"Fragility level should be one of {\'high\', \'medium\', \'low\'}.\\") super().__init__(name, quantity, category_name) self.__fragility_level = fragility_level def __str__(self): return f\\"Fragile Item: {self.name} (Qty: {self.quantity}, Category: {self.category_name}, Fragility: {self.__fragility_level})\\" class Warehouse: def __init__(self): self.inventory = [] self._index = 0 def add_item(self, item: Item): self.inventory.append(item) def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self.inventory): result = self.inventory[self._index] self._index += 1 return result else: raise StopIteration"},{"question":"**Objective:** Using the `tracemalloc` module in Python, write a function to analyze memory usage patterns of a given piece of code. Your function should be able to track memory allocations, identify the top memory-consuming operations, and return a summary. # Description Implement a function `analyze_memory_usage(code: str) -> str` that receives a string of Python code, executes it, and returns a summary of the top 5 memory-consuming operations. The summary should be formatted as a multi-line string, with each line containing the filename, line number, and the size of memory allocated. Input: - `code` (str): A string containing a valid Python code snippet. Output: - (str): A multi-line string, each line formatted as `\\"File <filename>, Line <line_no>: <size> bytes\\"`, representing the top 5 memory allocations. # Constraints: - Avoid executing code that might be harmful or create side effects. - Make sure to handle the possibility of syntax errors or runtime errors in the provided code. # Example Usage: ```python def analyze_memory_usage(code: str) -> str: import tracemalloc import sys # Function to execute passed code safely def execute_code(code): exec_globals = {} exec_locals = {} exec(code, exec_globals, exec_locals) tracemalloc.start() try: execute_code(code) except Exception as e: return f\\"Error during code execution: {e}\\" snapshot = tracemalloc.take_snapshot() top_stats = snapshot.statistics(\'lineno\') report_lines = [] for stat in top_stats[:5]: report_lines.append( f\\"File {stat.traceback[0].filename}, Line {stat.traceback[0].lineno}: {stat.size} bytes\\" ) if not report_lines: return \\"No significant memory allocations detected.\\" return \\"n\\".join(report_lines) # Example code code = \'\'\' result = [] for i in range(100000): result.append(i) \'\'\' print(analyze_memory_usage(code)) ``` In this example, the provided code snippet creates a list with 100,000 elements. The `analyze_memory_usage` function will return a summary of the top memory-consuming operations identified during the execution of this code. Make sure to test your function with various other code snippets to ensure it correctly identifies memory usage patterns and handles errors gracefully.","solution":"def analyze_memory_usage(code: str) -> str: import tracemalloc import sys # Function to execute passed code safely def execute_code(code): exec_globals = {} exec_locals = {} exec(code, exec_globals, exec_locals) tracemalloc.start() try: execute_code(code) except Exception as e: tracemalloc.stop() return f\\"Error during code execution: {e}\\" snapshot = tracemalloc.take_snapshot() top_stats = snapshot.statistics(\'lineno\') report_lines = [] for stat in top_stats[:5]: report_lines.append( f\\"File {stat.traceback[0].filename}, Line {stat.traceback[0].lineno}: {stat.size} bytes\\" ) tracemalloc.stop() if not report_lines: return \\"No significant memory allocations detected.\\" return \\"n\\".join(report_lines)"},{"question":"Objective: Write a Python function that demonstrates the use of importing a module by name, reloading a module, and executing code from a module. This will assess your understanding of handling module imports and module management. Task: 1. **Function Name:** `manage_module` 2. **Parameters:** - `module_name` (str): The name of the module to be imported and managed. - `code` (str): A Python code string to be executed within the context of the imported module. 3. **Functionality:** - Import the module specified by `module_name` using the appropriate C API import function. - Execute the provided `code` string within the context of the imported module. - Reload the module after executing the code. - Return the printed output or any results generated from the executed code and reloading process. 4. **Constraints and Notes:** - You can assume that the module specified will exist and is accessible in the current environment. - Carefully handle any exceptions that might occur during the import, execution, and reloading process, and ensure meaningful error messages are returned. - Make use of functions like `PyImport_ImportModule`, `PyImport_ReloadModule`, and `PyImport_ExecCodeModule` as appropriate. Expected Input and Output: - **Input:** - module_name: \\"example_module\\" - code: \\"print(\'Hello from within the module!\')\\" - **Output:** - The function should return the output of the executed code string and confirm the reloading of the module. Example: ```python def manage_module(module_name, code): # Your implementation here pass # Example usage output = manage_module(\\"example_module\\", \\"print(\'Hello from within the module!\')\\") print(output) ``` In the example, assuming `example_module` is a valid module, the output should demonstrate the print statement execution from within the module and confirm the module reloading.","solution":"import importlib def manage_module(module_name, code): Import a module by name, execute the provided code within the context of the module, and then reload the module. :param module_name: The name of the module to be imported and managed. :param code: A Python code string to be executed within the context of the imported module. :return: The output of the executed code string and a confirmation of module reloading. try: # Import the module module = importlib.import_module(module_name) # Execute the provided code string within the module\'s context exec(code, module.__dict__) # Reload the module importlib.reload(module) return \\"Code executed and module reloaded successfully.\\" except Exception as e: return f\\"An error occurred: {str(e)}\\""},{"question":"# **Question 1: Advanced Unit Testing with `unittest`** You are tasked with creating a utility class `StringUtil` that provides methods for string transformation and numerical analysis. Alongside this, you need to develop a comprehensive suite of unit tests to ensure the correct functionality of the class using the `unittest` module. **Requirements:** 1. **Class Implementation** - Create a class `StringUtil` with the following methods: - `reverse_string(s: str) -> str`: Reverses the given string. - `is_palindrome(s: str) -> bool`: Returns `True` if the given string is a palindrome, `False` otherwise. Ignore cases and non-alphanumeric characters. - `string_to_integer(s: str) -> int`: Converts a given numeric string to an integer. Raise a `ValueError` if the string cannot be converted. 2. **Unit Tests** - Develop a test suite using the `unittest` module: - Write a class `TestStringUtil` that inherits from `unittest.TestCase`. - Implement tests for each method in `StringUtil` ensuring all edge cases are covered. - Use a variety of assertions like `assertEqual`, `assertTrue`, `assertFalse`, and `assertRaises`. **Input and Output:** - **Input**: Input will be supplied to the methods as specified. - **Output**: The output will be as specified for each method. **Constraints and Limitations:** - `StringUtil.reverse_string` - Input string `s` can be empty or contain any printable characters. - `StringUtil.is_palindrome` - Input string `s` can be empty or contain any printable characters. - Palindrome check should be case-insensitive and should ignore non-alphanumeric characters. - `StringUtil.string_to_integer` - Input string `s` should represent a valid integer. **Performance Requirements:** - The methods should handle strings of length up to `10^6` efficiently. **Example:** ```python import unittest import re class StringUtil: @staticmethod def reverse_string(s: str) -> str: return s[::-1] @staticmethod def is_palindrome(s: str) -> bool: cleaned = re.sub(r\'W+\', \'\', s).lower() return cleaned == cleaned[::-1] @staticmethod def string_to_integer(s: str) -> int: if not s.isdigit(): raise ValueError(\\"The provided string is not a valid integer.\\") return int(s) class TestStringUtil(unittest.TestCase): def test_reverse_string(self): self.assertEqual(StringUtil.reverse_string(\\"hello\\"), \\"olleh\\") self.assertEqual(StringUtil.reverse_string(\\"\\"), \\"\\") def test_is_palindrome(self): self.assertTrue(StringUtil.is_palindrome(\\"A man, a plan, a canal, Panama\\")) self.assertFalse(StringUtil.is_palindrome(\\"Hello\\")) def test_string_to_integer(self): self.assertEqual(StringUtil.string_to_integer(\\"12345\\"), 12345) self.assertEqual(StringUtil.string_to_integer(\\"0\\"), 0) with self.assertRaises(ValueError): StringUtil.string_to_integer(\\"123a4\\") if __name__ == \'__main__\': unittest.main() ``` Implement the `StringUtil` class and the `TestStringUtil` class so that all unit tests pass successfully.","solution":"import re class StringUtil: @staticmethod def reverse_string(s: str) -> str: Reverses the given string. return s[::-1] @staticmethod def is_palindrome(s: str) -> bool: Returns `True` if the given string is a palindrome, `False` otherwise. Ignore cases and non-alphanumeric characters. cleaned = re.sub(r\'[^A-Za-z0-9]\', \'\', s).lower() return cleaned == cleaned[::-1] @staticmethod def string_to_integer(s: str) -> int: Converts a given numeric string to an integer. Raise a `ValueError` if the string cannot be converted. if not s.isdigit(): raise ValueError(\\"The provided string is not a valid integer.\\") return int(s)"},{"question":"**Problem Statement: Financial Portfolio Volatility Calculation** You are a financial analyst tasked with analyzing the volatility of a given portfolio of stocks. The volatility of a single stock can be determined using the standard deviation of monthly returns. For a given period, the annualized volatility of the portfolio is determined based on the monthly volatilities of the constituent stocks and their respective weights in the portfolio. Using the Python `math` module, your task is to implement a function `calculate_portfolio_volatility` that calculates the annualized volatility of the portfolio given the volatilities of individual stocks and their respective weights in the portfolio. **Function Signature** ```python def calculate_portfolio_volatility(monthly_volatilities: list[float], weights: list[float]) -> float: pass ``` **Inputs** - `monthly_volatilities`: A list of floats representing the monthly volatilities of the individual stocks. - `weights`: A list of floats representing the weights of the individual stocks in the portfolio. The sum of these weights should be `1`. **Output** - Returns a single float which is the annualized volatility of the portfolio. **Example** ```python # Example 1: volatilities = [0.01, 0.02, 0.015] weights = [0.5, 0.3, 0.2] print(calculate_portfolio_volatility(volatilities, weights)) # Expected output: Approx value # Example 2: volatilities = [0.03, 0.025, 0.02] weights = [0.4, 0.4, 0.2] print(calculate_portfolio_volatility(volatilities, weights)) # Expected output: Approx value ``` **Constraints** - All elements in `monthly_volatilities` will be positive floats less than `1`. - All elements in `weights` will be positive floats such that their sum is exactly `1`. - The length of `monthly_volatilities` and `weights` will be the same (between `2` and `100`). **Notes** 1. **Monthly to Annualized Volatility**: The transition from monthly to annualized volatility is typically done by multiplying the monthly volatility by the square root of 12. 2. **Portfolio Volatility Calculation**: The portfolio volatility can be calculated using the formula: [ sigma_p = sqrt{sum_{i=1}^{n} w_i^2 sigma_i^2} ] where: - (sigma_p) is the portfolio volatility. - (w_i) is the weight of the (i)-th asset. - (sigma_i) is the monthly volatility of the (i)-th asset. 3. You may use the mathematical functions like `math.sqrt`, `math.pow` where necessary for your calculations. **Hints** - Use list comprehensions to compute intermediate sums. - Ensure to handle edge cases where inputs might have minimal values (but still valid). Good luck!","solution":"import math def calculate_portfolio_volatility(monthly_volatilities, weights): Calculate the annualized volatility of a portfolio. Parameters: monthly_volatilities (list of float): Monthly volatilities of the individual stocks. weights (list of float): Weights of the individual stocks in the portfolio. Returns: float: Annualized volatility of the portfolio. # Calculate the portfolio variance portfolio_variance = sum( weights[i] ** 2 * monthly_volatilities[i] ** 2 for i in range(len(monthly_volatilities)) ) # Calculate the portfolio volatility (standard deviation) portfolio_volatility_monthly = math.sqrt(portfolio_variance) # Annualize the portfolio volatility portfolio_volatility_annualized = portfolio_volatility_monthly * math.sqrt(12) return portfolio_volatility_annualized"}]'),A={name:"App",components:{PoemCard:z},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},q={class:"card-container"},F={key:0,class:"empty-state"},R=["disabled"],N={key:0},j={key:1};function O(i,e,l,m,n,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"prompts chat")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")},"  ")):d("",!0)]),t("div",q,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",F,' No results found for "'+u(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),s("span",j,"Loading...")):(a(),s("span",N,"See more"))],8,R)):d("",!0)])}const L=p(A,[["render",O],["__scopeId","data-v-eea759b1"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/64.md","filePath":"deepseek/64.md"}'),U={name:"deepseek/64.md"},B=Object.assign(U,{setup(i){return(e,l)=>(a(),s("div",null,[x(L)]))}});export{Y as __pageData,B as default};
