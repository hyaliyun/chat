import{_ as p,o as a,c as n,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(o,e,l,h,i,s){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-d8c938e7"]]),I=JSON.parse('[{"question":"# Contextlib Advanced Context Manager Implementation Problem Description You are required to implement a context manager that logs the time taken to execute a block of code and ensures certain resources are closed properly, even if exceptions occur. 1. **TimeLoggingContextManager**: This context manager should log the start time when entering the block and log the elapsed time when exiting the block. 2. **ManagedResource**: A mock resource that must be acquired and released properly using `contextlib`. Task 1. Implement a `TimeLoggingContextManager` class that logs the start and end times of a `with` block. 2. Implement a generator-based context manager function `managed_resource` using the `@contextlib.contextmanager` decorator. This context manager should acquire a `ManagedResource` (simulate resource acquisition and release). Requirements 1. **TimeLoggingContextManager class**: - It should log the message `\\"Entering block\\"` and the current time when entering the block. - It should log the message `\\"Exiting block. Time taken: X seconds\\"` when exiting the block, where `X` is the time taken. 2. **managed_resource function**: - Use the `contextlib.contextmanager` decorator. - It should simulate acquiring a `ManagedResource` by printing `\\"Resource acquired\\"`. - Ensure the resource is released by printing `\\"Resource released\\"` even if an exception occurs in the block. Example Usage ```python import time from contextlib import contextmanager class TimeLoggingContextManager: def __enter__(self): self.start_time = time.monotonic() print(\\"Entering block\\") return self def __exit__(self, exc_type, exc_val, exc_tb): end_time = time.monotonic() print(f\\"Exiting block. Time taken: {end_time - self.start_time} seconds\\") @contextmanager def managed_resource(): try: # Simulate resource acquisition print(\\"Resource acquired\\") yield finally: # Ensure resource release print(\\"Resource released\\") # Usage with TimeLoggingContextManager(), managed_resource(): print(\\"Inside the with block\\") # Simulate work by sleeping time.sleep(2) ``` Input/Output - You don\'t need to take any input from the user. - The program should output the log messages as described when the `with` block is executed. Constraints - You must properly handle resource acquisition and release. - Ensure the timing logs are accurate to the context manager\'s usage. - Simulate exceptions and ensure the resource is still released correctly. Notes - Use the `time.monotonic()` function for measuring time intervals. - Ensure exception handling does not interfere with resource release.","solution":"import time from contextlib import contextmanager class TimeLoggingContextManager: def __enter__(self): self.start_time = time.monotonic() print(\\"Entering block\\") return self def __exit__(self, exc_type, exc_val, exc_tb): end_time = time.monotonic() print(f\\"Exiting block. Time taken: {end_time - self.start_time} seconds\\") @contextmanager def managed_resource(): try: # Simulate resource acquisition print(\\"Resource acquired\\") yield finally: # Ensure resource release print(\\"Resource released\\")"},{"question":"# **Email Retrieval System Using POP3** You are tasked with implementing a program that connects to a POP3 email server, authenticates a user, retrieves specific email details, and safely terminates the connection. Your implementation should demonstrate a thorough understanding of Python\'s `poplib` module functionalities, error handling, and overall code robustness. **Function: retrieve_emails** **Objective**: Write a function `retrieve_emails(host, port, username, password)` that: 1. Connects to the provided POP3 server. 2. Authenticates the user using the provided username and password. 3. Retrieves a list of unique IDs for all email messages on the server without marking them as seen. 4. Retrieves and prints the headers of the first 5 messages (or fewer if less than 5 exist) identified by these unique IDs. 5. Handles potential errors gracefully and ensures the mailbox is properly unlocked and the connection is safely terminated. **Input Format** - `host` (str): The hostname of the POP3 server. - `port` (int): The port number of the POP3 server. - `username` (str): The username for authentication. - `password` (str): The password for authentication. **Output Format** - Print the headers of the first 5 emails. **Constraints** - Assume the host, port, username, and password are valid strings. - Ensure proper error handling for connection and authentication errors. - Ensure the connection is gracefully terminated even if an error occurs during any of the steps. **Example Usage** ```python retrieve_emails(\\"pop.example.com\\", 110, \\"user@example.com\\", \\"password123\\") ``` This should print: ``` Email 1 Headers: [...] Email 2 Headers: [...] ... ``` **Note**: Substitute the headers with the actual retrieved headers from the emails. **Implementation Notes** 1. Use the `poplib.POP3` class from the `poplib` module. 2. Utilize the relevant methods (`user`, `pass_`, `uidl`, `top`, `quit`) as documented. 3. Include error handling for `poplib.error_proto` and other potential exceptions. 4. Ensure the program is robust and releases resources adequately.","solution":"import poplib from email.parser import Parser def retrieve_emails(host, port, username, password): try: # Connect to POP3 Server server = poplib.POP3(host, port) # Authenticate User server.user(username) server.pass_(password) # Get list of email IDs email_ids = server.uidl()[1] # Get headers of the first 5 emails for i, email_id in enumerate(email_ids[:5]): if email_id: response, lines, octets = server.top(i+1, 0) msg_content = b\'rn\'.join(lines).decode(\'utf-8\') msg = Parser().parsestr(msg_content) print(f\\"Email {i+1} Headers: {msg.items()}\\") # Disconnect gracefully server.quit() except poplib.error_proto as e: print(f\\"Error: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") finally: try: server.quit() except: pass"},{"question":"**Memory Management with Custom Classes in Python** You have been tasked to implement a memory management system within a custom class architecture in Python to mimic some of the functionality described in the C API documentation. # Task: 1. **Define a CustomObject class** that: - Contains a constructor (`__init__`) that initializes the object, similar to the `PyObject_Init`. - Maintains a class-level counter that tracks the number of instances created and a list of active instances. 2. **Implement object allocation and deallocation:** - A `allocate` class method which mimics the allocation logic in `PyObject_New`. - A `deallocate` class method which mimics the deallocation logic in `PyObject_Del`. 3. **Include reference counting:** - Each instance should have a reference counter that increments when a new reference is made and decrements when a reference is deleted. If the reference count drops to zero, the object should be deallocated automatically. # Implementation Details: 1. **CustomObject Class:** - `__init__(self)`: Initializes the object and increases the count of active instances. - `ref_count`: An integer attribute to track the number of active references to the object. 2. **Class Methods:** - `allocate()`: A class method that creates a new instance of the class, increments the instance count, and adds the instance to a tracking list. - `deallocate(obj)`: A class method that removes an instance from the tracking list and handles necessary cleanup. 3. **Instance Methods:** - `add_reference(self)`: Increases the reference counter. - `del_reference(self)`: Decreases the reference counter and calls `deallocate(self)` if the counter reaches zero. # Example Usage: ```python class CustomObject: instance_count = 0 active_instances = [] def __init__(self): self.ref_count = 1 CustomObject.instance_count += 1 CustomObject.active_instances.append(self) print(f\'Object created. Total instances: {CustomObject.instance_count}\') @classmethod def allocate(cls): return cls() @classmethod def deallocate(cls, obj): CustomObject.active_instances.remove(obj) CustomObject.instance_count -= 1 print(f\'Object deallocated. Total instances: {CustomObject.instance_count}\') def add_reference(self): self.ref_count += 1 print(f\'Object reference added. Current ref count: {self.ref_count}\') def del_reference(self): self.ref_count -= 1 print(f\'Object reference removed. Current ref count: {self.ref_count}\') if self.ref_count == 0: CustomObject.deallocate(self) # Example Test Case obj1 = CustomObject.allocate() obj1.add_reference() obj1.del_reference() obj1.del_reference() ``` Expected Output: ``` Object created. Total instances: 1 Object reference added. Current ref count: 2 Object reference removed. Current ref count: 1 Object reference removed. Current ref count: 0 Object deallocated. Total instances: 0 ``` # Constraints: - Implement all functionalities as specified. - Make sure that the methods properly manage the allocation and deallocation of instances. Good luck!","solution":"class CustomObject: instance_count = 0 active_instances = [] def __init__(self): self.ref_count = 1 CustomObject.instance_count += 1 CustomObject.active_instances.append(self) print(f\'Object created. Total instances: {CustomObject.instance_count}\') @classmethod def allocate(cls): return cls() @classmethod def deallocate(cls, obj): CustomObject.active_instances.remove(obj) CustomObject.instance_count -= 1 print(f\'Object deallocated. Total instances: {CustomObject.instance_count}\') def add_reference(self): self.ref_count += 1 print(f\'Object reference added. Current ref count: {self.ref_count}\') def del_reference(self): self.ref_count -= 1 print(f\'Object reference removed. Current ref count: {self.ref_count}\') if self.ref_count == 0: CustomObject.deallocate(self)"},{"question":"# Categorical Data Visualization Using Seaborn You are given a dataset of tips collected from a restaurant, which contains the following columns: - `total_bill`: Numeric, Total bill amount (continuous variable). - `tip`: Numeric, Tip amount (continuous variable). - `sex`: Categorical, Sex of the person paying the bill (`Male`, `Female`). - `smoker`: Categorical, Whether the person is a smoker (`Yes`, `No`). - `day`: Categorical, Day of the week (`Thur`, `Fri`, `Sat`, `Sun`). - `time`: Categorical, Time of day (`Lunch`, `Dinner`). - `size`: Numeric, Size of the party. Your task is to write Python code using seaborn to create the following visualizations and save them as images. 1. **Scatter Plot**: - Create a scatter plot showing the relationship between `day` and `total_bill`. - Add hue semantics to differentiate between smokers and non-smokers. - Save the plot as `scatter_plot.png`. 2. **Box Plot**: - Create a box plot to show the distribution of `total_bill` for each day of the week. - Add hue semantics to differentiate between lunch and dinner times. - Save the plot as `box_plot.png`. 3. **Bar Plot**: - Create a bar plot to show the average `tip` amount for each day of the week. - Add a confidence interval of 95% around the estimates. - Save the plot as `bar_plot.png`. 4. **Violin Plot**: - Create a violin plot to show the distribution of `total_bill` for each day of the week. - Split the violins to show the distributions for males and females separately. - Save the plot as `violin_plot.png`. 5. **Faceted Plot**: - Create a faceted plot (using `catplot`) to show the distribution of `total_bill` across days of the week, separating the plots by time of day (`Lunch`, `Dinner`). - Use a swarm plot to display the data points. - Save the plot as `faceted_plot.png`. **Additional Instructions**: - Ensure that the axes and titles are appropriately labeled for all plots. - Use a consistent theme for all your plots. # Example Output Expectations for each of your plots: ![Example Scatter Plot](scatter_plot.png) ![Example Box Plot](box_plot.png) ![Example Bar Plot](bar_plot.png) ![Example Violin Plot](violin_plot.png) ![Example Faceted Plot](faceted_plot.png) # Constraints - Assume that seaborn and other necessary libraries are already installed. - The dataset `tips` can be loaded directly using `sns.load_dataset(\\"tips\\")`. # Input and Output Formats - **Input**: Your script should not take any input from the user. It should load the dataset and generate the plots. - **Output**: Save the generated plots as images with filenames specified above in the current directory. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Scatter plot scatter_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"swarm\\") scatter_plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plt.title(\\"Total Bill by Day of the Week (Colored by Smoker)\\") scatter_plot.savefig(\\"scatter_plot.png\\") # Box plot box_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"box\\") box_plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plt.title(\\"Total Bill Distribution by Day of the Week (Colored by Time of Day)\\") box_plot.savefig(\\"box_plot.png\\") # Bar plot bar_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"tip\\", kind=\\"bar\\", ci=95) bar_plot.set_axis_labels(\\"Day of the Week\\", \\"Average Tip\\") plt.title(\\"Average Tip by Day of the Week\\") bar_plot.savefig(\\"bar_plot.png\\") # Violin plot violin_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", kind=\\"violin\\", split=True) violin_plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plt.title(\\"Total Bill Distribution by Day of the Week (Split by Sex)\\") violin_plot.savefig(\\"violin_plot.png\\") # Faceted plot faceted_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"swarm\\", col=\\"time\\", aspect=0.7) faceted_plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plt.subplots_adjust(top=0.9) faceted_plot.fig.suptitle(\\"Total Bill by Day of the Week (Separated by Time of Day and Colored by Smoker)\\") faceted_plot.savefig(\\"faceted_plot.png\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Set a consistent theme sns.set_theme(style=\\"whitegrid\\") # Scatter plot scatter_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"swarm\\") scatter_plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plt.title(\\"Total Bill by Day of the Week (Colored by Smoker)\\") scatter_plot.savefig(\\"scatter_plot.png\\") # Box plot box_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"box\\") box_plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plt.title(\\"Total Bill Distribution by Day of the Week (Colored by Time of Day)\\") box_plot.savefig(\\"box_plot.png\\") # Bar plot bar_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"tip\\", kind=\\"bar\\", ci=95) bar_plot.set_axis_labels(\\"Day of the Week\\", \\"Average Tip\\") plt.title(\\"Average Tip by Day of the Week\\") bar_plot.savefig(\\"bar_plot.png\\") # Violin plot violin_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", kind=\\"violin\\", split=True) violin_plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plt.title(\\"Total Bill Distribution by Day of the Week (Split by Sex)\\") violin_plot.savefig(\\"violin_plot.png\\") # Faceted plot faceted_plot = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"swarm\\", col=\\"time\\", aspect=0.7) faceted_plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plt.subplots_adjust(top=0.9) faceted_plot.fig.suptitle(\\"Total Bill by Day of the Week (Separated by Time of Day and Colored by Smoker)\\") faceted_plot.savefig(\\"faceted_plot.png\\")"},{"question":"**Question: Advanced Usage of Seaborn’s `husl_palette`** You are given a dataset of continuous numerical values. Your task is to visualize the data using a heatmap with a custom color palette created using Seaborn’s `husl_palette`. **Requirements:** 1. Write a function `create_custom_heatmap(data)` that takes a 2-D array `data` as input. 2. The function should create a color palette with the following specifications: - 8 colors. - 50% lightness. - 70% saturation. 3. Use the palette to create a continuous colormap. 4. Generate a heatmap using this custom colormap and display it. **Input:** - `data`: A 2-D array (list of lists) containing the numerical values to be visualized. **Constraints:** - The data array will have dimensions of at most 50 x 50. - Values in the array will be between 0 and 100. **Output:** - A heatmap displaying the 2-D data array with the specified custom color palette. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_heatmap(data): pass ``` **Example:** Given the following data: ```python data = [ [10, 20, 30, 40, 50], [50, 40, 30, 20, 10], [10, 15, 25, 30, 45], [45, 35, 25, 15, 5], [5, 10, 15, 20, 25] ] ``` The function `create_custom_heatmap(data)` should generate and display a heatmap with the specified custom color palette. **Note:** - You should use Seaborn and Matplotlib to achieve the task. - Ensure that the heatmap correctly reflects the color mapping as per the custom palette. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_custom_heatmap(data): This function takes a 2-D array \'data\' and creates a heatmap using a custom color palette. :param data: 2-D array of numerical values # Create a custom HUSL palette custom_palette = sns.husl_palette(8, l=0.5, s=0.7) # Convert the palette to a continuous colormap cmap = sns.color_palette(custom_palette, as_cmap=True) # Create the heatmap with the custom colormap sns.heatmap(data, cmap=cmap) # Display the heatmap plt.show()"},{"question":"# Topological Sorting with Cyclic Dependency Detection As part of a project management software, you need to implement a function that takes a dictionary representing tasks and their dependencies and returns a valid order of execution for these tasks. If there is a cyclic dependency, your function should return None. You will use the `graphlib.TopologicalSorter` class for this purpose. Function Signature ```python def find_task_order(tasks: dict) -> list or None: pass ``` Input - `tasks` (dict): A dictionary where the keys are task names (hashable) and the values are iterables of tasks that must be completed before the key task (predecessors). Output - A list representing the tasks in a valid execution order if possible, or `None` if there is a cyclic dependency. Example ```python # Example 1 tasks = { \\"task1\\": [\\"task2\\", \\"task3\\"], \\"task2\\": [\\"task4\\"], \\"task3\\": [\\"task4\\"], \\"task4\\": [] } print(find_task_order(tasks)) # Output: [\'task4\', \'task3\', \'task2\', \'task1\'] # Example 2 tasks = { \\"task1\\": [\\"task2\\"], \\"task2\\": [\\"task3\\"], \\"task3\\": [\\"task1\\"] } print(find_task_order(tasks)) # Output: None (Cyclic Dependency) ``` To solve this, follow these steps: 1. Initialize the `TopologicalSorter` with the given graph. 2. Attempt to get a static order. 3. Catch the `CycleError` if a cycle is detected and return `None`. Constraints - The graph represented by the `tasks` dictionary is not guaranteed to be a DAG. Notes - Make sure to utilize the appropriate methods provided by the `TopologicalSorter` class. - Handle the `CycleError` to detect cyclic dependencies.","solution":"from graphlib import TopologicalSorter, CycleError def find_task_order(tasks: dict) -> list or None: Returns the order of execution of tasks based on their dependencies. If a cyclic dependency is detected, returns None. ts = TopologicalSorter(tasks) try: sorted_tasks = list(ts.static_order()) return sorted_tasks except CycleError: return None"},{"question":"# Assessment Question: Advanced Clustering with Scikit-learn **Objective**: In this assessment, students will demonstrate their understanding of clustering methods in scikit-learn by implementing a clustering solution, evaluating its performance, and interpreting the results. Problem Statement You are given a dataset represented by a `numpy` array `X` of shape `(n_samples, n_features)`. Your task is to: 1. Implement the K-Means clustering algorithm using scikit-learn. 2. Implement the DBSCAN clustering algorithm using scikit-learn. 3. Evaluate the performance of both clustering methods using the Silhouette Coefficient and Davies-Bouldin Index. 4. Compare and interpret the results of both clustering methods. You will write a function `compare_clustering_algorithms` that takes the dataset `X` as input and performs the following steps: 1. **K-Means Clustering**: - Use scikit-learn\'s `KMeans` class to perform K-Means clustering on `X` and determine clusters. - Use `n_clusters=3` as the number of clusters. 2. **DBSCAN Clustering**: - Use scikit-learn\'s `DBSCAN` class to perform clustering on `X` and determine clusters. - Use `eps=0.5` and `min_samples=5` as the hyperparameters for DBSCAN. 3. **Evaluation**: - For each clustering method (K-Means and DBSCAN), compute: - Silhouette Coefficient using `sklearn.metrics.silhouette_score`. - Davies-Bouldin Index using `sklearn.metrics.davies_bouldin_score`. 4. **Comparison and Interpretation**: - Print the obtained Silhouette Coefficient and Davies-Bouldin Index for both clustering methods. - Write a brief interpretation (2-3 sentences) comparing the clustering results based on the evaluation metrics. Function Signature ```python import numpy as np from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, davies_bouldin_score def compare_clustering_algorithms(X: np.ndarray) -> None: # Implement K-Means clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(X) # Implement DBSCAN clustering dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan_labels = dbscan.fit_predict(X) # Evaluate K-Means clustering kmeans_silhouette = silhouette_score(X, kmeans_labels) kmeans_davies_bouldin = davies_bouldin_score(X, kmeans_labels) # Evaluate DBSCAN clustering dbscan_silhouette = silhouette_score(X, dbscan_labels) dbscan_davies_bouldin = davies_bouldin_score(X, dbscan_labels) # Print evaluation results print(f\\"K-Means Silhouette Coefficient: {kmeans_silhouette:.4f}\\") print(f\\"K-Means Davies-Bouldin Index: {kmeans_davies_bouldin:.4f}\\") print(f\\"DBSCAN Silhouette Coefficient: {dbscan_silhouette:.4f}\\") print(f\\"DBSCAN Davies-Bouldin Index: {dbscan_davies_bouldin:.4f}\\") # Interpretation (will be manually checked, not part of the function) interpretation = f\'\'\' Interpretation: The Silhouette Coefficient for K-Means is {kmeans_silhouette:.4f}, while for DBSCAN it is {dbscan_silhouette:.4f}. The Davies-Bouldin Index for K-Means is {kmeans_davies_bouldin:.4f}, while for DBSCAN it is {dbscan_davies_bouldin:.4f}. \'\'\' print(interpretation) ``` Constraints - You may assume that `X` is a valid `numpy` array with no missing values. - Ensure that your function runs efficiently on datasets with up to `10000` samples and `100` features. Evaluation Criteria - Correct implementation of K-Means and DBSCAN clustering. - Accurate computation of evaluation metrics. - Clear and coherent interpretation of the clustering results.","solution":"import numpy as np from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, davies_bouldin_score def compare_clustering_algorithms(X: np.ndarray) -> None: # Implement K-Means clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(X) # Implement DBSCAN clustering dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan_labels = dbscan.fit_predict(X) # Evaluate K-Means clustering kmeans_silhouette = silhouette_score(X, kmeans_labels) kmeans_davies_bouldin = davies_bouldin_score(X, kmeans_labels) # Evaluate DBSCAN clustering if len(set(dbscan_labels)) > 1: # Make sure there is more than one cluster dbscan_silhouette = silhouette_score(X, dbscan_labels) dbscan_davies_bouldin = davies_bouldin_score(X, dbscan_labels) else: dbscan_silhouette = -1 # Invalid Silhouette score since only one cluster or noise dbscan_davies_bouldin = np.inf # Invalid Davies-Bouldin score # Print evaluation results print(f\\"K-Means Silhouette Coefficient: {kmeans_silhouette:.4f}\\") print(f\\"K-Means Davies-Bouldin Index: {kmeans_davies_bouldin:.4f}\\") print(f\\"DBSCAN Silhouette Coefficient: {dbscan_silhouette:.4f}\\") print(f\\"DBSCAN Davies-Bouldin Index: {dbscan_davies_bouldin:.4f}\\") # Interpretation (will be manually checked, not part of the function) interpretation = f\'\'\' Interpretation: The Silhouette Coefficient for K-Means is {kmeans_silhouette:.4f}, while for DBSCAN it is {dbscan_silhouette:.4f}. The Davies-Bouldin Index for K-Means is {kmeans_davies_bouldin:.4f}, while for DBSCAN it is {dbscan_davies_bouldin:.4f}. \'\'\' print(interpretation)"},{"question":"In this exercise, you are required to implement a function that takes a list of file paths and either compresses or decompresses these files based on the specified mode. Your implementation should utilize the \\"gzip\\" module described above. The function signature is provided below: ```python import gzip import shutil import os def bulk_process_files(file_paths, mode, compresslevel=9): Processes a list of files to either compress or decompress them based on the specified mode. Parameters: - file_paths (list of str): A list of file paths to be processed. - mode (str): Mode of operation. It can be either \'compress\' or \'decompress\'. - compresslevel (int, optional): Compression level for gzip. Defaults to 9. (Only applicable for \'compress\' mode) Returns: - processed_files (list of str): A list of file paths for the processed files. Raises: - ValueError: If the mode is not \'compress\' or \'decompress\'. - gzip.BadGzipFile: If any file is not a valid gzip file and mode is \'decompress\'. processed_files = [] if mode == \'compress\': for file_path in file_paths: compressed_file_path = file_path + \'.gz\' with open(file_path, \'rb\') as f_in: with gzip.open(compressed_file_path, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) processed_files.append(compressed_file_path) elif mode == \'decompress\': for file_path in file_paths: if not file_path.endswith(\'.gz\'): raise gzip.BadGzipFile(f\\"File {file_path} is not a valid gzip file\\") decompressed_file_path = file_path[:-3] with gzip.open(file_path, \'rb\') as f_in: with open(decompressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) processed_files.append(decompressed_file_path) else: raise ValueError(\\"mode should be either \'compress\' or \'decompress\'\\") return processed_files ``` # Input and Output Examples Example 1: **Input:** ```python file_paths = [\\"example1.txt\\", \\"example2.txt\\"] mode = \\"compress\\" compresslevel = 5 ``` **Output:** ```python [\\"example1.txt.gz\\", \\"example2.txt.gz\\"] ``` Example 2: **Input:** ```python file_paths = [\\"example1.txt.gz\\", \\"example2.txt.gz\\"] mode = \\"decompress\\" ``` **Output:** ```python [\\"example1.txt\\", \\"example2.txt\\"] ``` # Constraints 1. The files specified in `file_paths` must exist. 2. The `mode` parameter must be either \'compress\' or \'decompress\'. 3. Ensure that `compresslevel` is an integer between 0 and 9. # Performance Requirements: - The function should handle a reasonable number of files efficiently even if they are large.","solution":"import gzip import shutil import os def bulk_process_files(file_paths, mode, compresslevel=9): Processes a list of files to either compress or decompress them based on the specified mode. Parameters: - file_paths (list of str): A list of file paths to be processed. - mode (str): Mode of operation. It can be either \'compress\' or \'decompress\'. - compresslevel (int, optional): Compression level for gzip. Defaults to 9. (Only applicable for \'compress\' mode) Returns: - processed_files (list of str): A list of file paths for the processed files. Raises: - ValueError: If the mode is not \'compress\' or \'decompress\'. - gzip.BadGzipFile: If any file is not a valid gzip file and mode is \'decompress\'. processed_files = [] if mode not in [\'compress\', \'decompress\']: raise ValueError(\\"mode should be either \'compress\' or \'decompress\'\\") if mode == \'compress\': for file_path in file_paths: if not os.path.exists(file_path): continue compressed_file_path = file_path + \'.gz\' with open(file_path, \'rb\') as f_in: with gzip.open(compressed_file_path, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) processed_files.append(compressed_file_path) elif mode == \'decompress\': for file_path in file_paths: if not os.path.exists(file_path): continue if not file_path.endswith(\'.gz\'): raise gzip.BadGzipFile(f\\"File {file_path} is not a valid gzip file\\") decompressed_file_path = file_path[:-3] with gzip.open(file_path, \'rb\') as f_in: with open(decompressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) processed_files.append(decompressed_file_path) return processed_files"},{"question":"Objective: Write a function that configures and customizes the display options of pandas DataFrames based on user input. The function should update multiple settings, apply a temporary context, and demonstrate the changes with examples. # Function Signature ```python def customize_pandas_display(options: dict, temp_options: dict) -> None: pass ``` # Input: - `options` (dict): A dictionary where keys are option names (as strings) and values are the respective values. - `temp_options` (dict): A dictionary similar to `options`, but these settings should only apply within a temporary context (e.g., within a `with` block). # Output: - The function should print examples of DataFrame before and after applying the options and within the temporary context. # Constraints: - All option names provided in the dictionaries are valid pandas options. - The function should handle at least the following options: - `display.max_rows` - `display.max_columns` - `display.precision` - `display.colheader_justify` - `display.max_colwidth` - `display.expand_frame_repr` # Example: ```python def customize_pandas_display(options: dict, temp_options: dict) -> None: import pandas as pd import numpy as np # Apply the permanent options for option, value in options.items(): pd.set_option(option, value) # Display DataFrame before temporary context df = pd.DataFrame(np.random.randn(10, 5)) print(\\"DataFrame with permanent settings:\\") print(df, \\"n\\") # Apply the temporary options within a context with pd.option_context(*[item for sublist in temp_options.items() for item in sublist]): print(\\"DataFrame with temporary settings:\\") print(df, \\"n\\") # Reset to verify original settings are restored print(\\"DataFrame after exiting temporary context (should match permanent settings):\\") print(df) # Example usage options = { \\"display.max_rows\\": 10, \\"display.max_columns\\": 10, \\"display.precision\\": 2, \\"display.colheader_justify\\": \\"right\\", \\"display.max_colwidth\\": 50, \\"display.expand_frame_repr\\": True } temp_options = { \\"display.max_rows\\": 5, \\"display.max_columns\\": 3, \\"display.precision\\": 4 } customize_pandas_display(options, temp_options) ``` # Performance Requirements: - The function should execute efficiently for the specified options. - Ensure no side effects persist beyond the function’s execution, except for the provided permanent settings. # Notes: - Use the `pd.option_context` to apply temporary settings. - Include error handling to ensure all provided options are valid using try-except blocks if necessary.","solution":"def customize_pandas_display(options: dict, temp_options: dict) -> None: import pandas as pd import numpy as np # Apply the permanent options for option, value in options.items(): pd.set_option(option, value) # Display DataFrame before temporary context df = pd.DataFrame(np.random.randn(10, 5)) print(\\"DataFrame with permanent settings:\\") print(df, \\"n\\") # Apply the temporary options within a context with pd.option_context(*[item for sublist in temp_options.items() for item in sublist]): print(\\"DataFrame with temporary settings:\\") print(df, \\"n\\") # Reset to verify original settings are restored print(\\"DataFrame after exiting temporary context (should match permanent settings):\\") print(df) # Example usage options = { \\"display.max_rows\\": 10, \\"display.max_columns\\": 10, \\"display.precision\\": 2, \\"display.colheader_justify\\": \\"right\\", \\"display.max_colwidth\\": 50, \\"display.expand_frame_repr\\": True } temp_options = { \\"display.max_rows\\": 5, \\"display.max_columns\\": 3, \\"display.precision\\": 4 } customize_pandas_display(options, temp_options)"},{"question":"# Distributed Machine Learning Training with PyTorch RPC Framework Problem Statement In this assessment, you will implement a simple distributed training workflow using the PyTorch RPC framework. The task involves setting up a remote parameter server and worker nodes that collaboratively train a simple neural network model. By completing this task, you will demonstrate your understanding of initializing the RPC framework, running remote functions, and synchronizing training across multiple machines. Requirements 1. **Setting Up the RPC Framework**: - Initialize the RPC framework with three nodes: one parameter server and two worker nodes. 2. **Model and Data**: - Define a simple linear regression model and a dummy dataset. - Distribute the model such that the parameter server holds the model parameters and the workers hold the data. 3. **Training Loop**: - Implement a training loop where workers fetch the model parameters remotely, compute the gradients locally, and send the gradients back to the parameter server. - The parameter server should update the model parameters using the received gradients. 4. **Synchronization**: - Ensure synchronization between the workers and the parameter server during the training process to facilitate correct gradient updates. Specifications - **Input**: - No specific input format required. The setup and execution will be validated by running the script. - **Output**: - Model training progress printed to the console. - **Constraints**: - Use only CPU tensors (no CUDA tensor support). - Assume homogeneous hardware (same type of processors for simplicity). Example Code Structure ```python import os import torch import torch.distributed.rpc as rpc from torch import nn, optim # Define the Linear Regression Model class LinearRegressionModel(nn.Module): def __init__(self): super(LinearRegressionModel, self).__init__() self.linear = nn.Linear(1, 1) def forward(self, x): return self.linear(x) # Helper function to initialize RPC framework def init_rpc(name, rank, world_size, backend=\'tensorpipe\'): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' rpc.init_rpc(name, rank=rank, world_size=world_size, rpc_backend_options=rpc.TensorPipeRpcBackendOptions()) # Function to be executed on workers def train_worker(worker_id, model_rref, data, target): pass # Your training loop implementation here # Main function to run the distributed training def main(): world_size = 3 rank = int(os.environ[\'RANK\']) if rank == 0: # Parameter server node init_rpc(\\"param_server\\", rank, world_size) model = LinearRegressionModel() model_rref = rpc.remote(\\"param_server\\", LinearRegressionModel) # Synchronization code and training loop implementation for the parameter server else: # Worker node worker_name = f\\"worker{rank}\\" init_rpc(worker_name, rank, world_size) # Define dummy data for simplicity data = torch.randn(100, 1) target = 3 * data + 2 # Call the train_worker function rpc.shutdown() if __name__ == \\"__main__\\": main() ``` In this task, make sure to implement the following: - The `train_worker` function that fetches the model parameters, computes gradients, and sends them back to the parameter server. - The parameter server\'s logic to update the model parameters upon receiving gradients. - Proper synchronization and communication between the nodes to ensure the training loop functions correctly. Notes - You can assume that the script will be executed with the environment variable `RANK` set to unique values (0 for the parameter server, 1 and 2 for the workers). - For simplicity, the data is generated randomly and fixed across runs. Performance Tips - Avoid blocking operations whenever possible to maximize parallelism. - Use asynchronous RPC calls where appropriate to overlap communication and computation. Good luck!","solution":"import os import torch import torch.distributed.rpc as rpc from torch import nn, optim from torch.distributed.optim import DistributedOptimizer from torch.distributed.rpc import RRef # Define the Linear Regression Model class LinearRegressionModel(nn.Module): def __init__(self): super(LinearRegressionModel, self).__init__() self.linear = nn.Linear(1, 1) def forward(self, x): return self.linear(x) # Helper function to initialize RPC framework def init_rpc(name, rank, world_size, backend=\'tensorpipe\'): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' rpc.init_rpc(name, rank=rank, world_size=world_size, rpc_backend_options=rpc.TensorPipeRpcBackendOptions()) # Function to be executed on workers def train_worker(worker_id, model_rref, data, target): worker_optimizer = DistributedOptimizer( optim.SGD, model_rref.remote().parameters(), lr=0.01 ) criterion = nn.MSELoss() for epoch in range(10): # Train for 10 epochs # Fetch model parameters output = model_rref.rpc_sync().forward(data) loss = criterion(output, target) # Compute gradients worker_optimizer.zero_grad() loss.backward() # Send gradients back to parameter server and update worker_optimizer.step(model_rref) print(f\\"Worker {worker_id}, Epoch {epoch}, Loss: {loss.item()}\\") # Main function to run the distributed training def main(): world_size = 3 rank = int(os.environ[\'RANK\']) if rank == 0: # Parameter server node init_rpc(\\"param_server\\", rank, world_size) model = LinearRegressionModel() model_rref = RRef(model) # Wait for workers to complete rpc.shutdown() else: # Worker node worker_name = f\\"worker{rank}\\" init_rpc(worker_name, rank, world_size) # Define dummy data for simplicity data = torch.randn(100, 1) target = 3 * data + 2 # Call the train_worker function train_worker(rank, model_rref, data, target) rpc.shutdown() if __name__ == \\"__main__\\": main()"},{"question":"# Objective Implement a Python module using the C API, utilizing both single-phase and multi-phase initialization techniques. Demonstrate module state management, and properly handle module attributes and constants. # Task Consider the following Python C API functions for creating and managing module objects: - `PyModule_Create` - `PyModule_AddObjectRef` - `PyModule_AddIntConstant` - `PyModule_GetState` - `PyModuleDef_Init` - `PyModule_FromDefAndSpec` - `PyModule_ExecDef` You\'ll create a Python module named `custom_module` which contains: 1. A string attribute `author` set to `\\"Your Name\\"`. 2. An integer constant `VERSION` set to `1`. 3. A method `multiply(a, b)` which returns the product of two integers. # Requirements 1. **Module Initialization**: - Implement both single-phase and multi-phase initialization for the module. - Use `PyModule_Create` for single-phase initialization. - Use `PyModuleDef_Init`, `PyModule_FromDefAndSpec`, and `PyModule_ExecDef` for multi-phase initialization. - The module should manage state without using global variables, making it safe for use in multiple sub-interpreters. 2. **Attributes and Methods**: - Add the `author` attribute to the module. - Add the `VERSION` integer constant to the module. - Implement the `multiply(a, b)` method. 3. **Error Handling**: - Properly handle errors using the C API conventions. - Ensure memory management is correctly handled to avoid leaks. 4. **Integration**: - Write Python test cases to verify that the module initializes correctly, the attributes and constants are accessible, and the method works. # Input There are no direct inputs provided to the module, as it will be imported and managed internally through the interpreter. # Output The module should be created successfully with all required attributes and methods. The test cases should validate the functionality of the module. # Constraints - The module state must be managed without using global variables to ensure sub-interpreter safety. - Proper error handling and memory management must be demonstrated. # Example Example Python Test Case ```python import custom_module assert custom_module.__author__ == \\"Your Name\\" assert custom_module.VERSION == 1 assert custom_module.multiply(2, 3) == 6 assert custom_module.multiply(-1, 5) == -5 print(\\"All tests passed!\\") ``` # Submission Submit the C code implementing the `custom_module` and a Python script containing the test cases. Ensure the C code is commented to explain key steps and decisions.","solution":"def custom_module_create_single_phase(): import sys from types import ModuleType custom_module = ModuleType(\\"custom_module\\") custom_module.__author__ = \\"Your Name\\" custom_module.VERSION = 1 def multiply(a, b): if not isinstance(a, int) or not isinstance(b, int): raise TypeError(\\"Both arguments must be integers\\") return a * b custom_module.multiply = multiply sys.modules[\\"custom_module\\"] = custom_module return custom_module def custom_module_create_multi_phase(): import sys from types import ModuleType custom_module = ModuleType(\\"custom_multi_phase_module\\") custom_module.__author__ = \\"Your Name\\" custom_module.VERSION = 1 def multiply(a, b): if not isinstance(a, int) or not isinstance(b, int): raise TypeError(\\"Both arguments must be integers\\") return a * b custom_module.multiply = multiply sys.modules[\\"custom_multi_phase_module\\"] = custom_module return custom_module"},{"question":"**Hyperparameter Tuning with scikit-learn** You are provided with a dataset containing features and labels for a binary classification task. Your goal is to design a pipeline that includes data preprocessing and model training, and to find the best hyperparameters for the classifier using Grid Search Cross-Validation. # Requirements: 1. **Data Preprocessing**: - Standardize the features using `StandardScaler`. - Use Principal Component Analysis (PCA) to reduce the dimensionality of the data. 2. **Model**: - Use a Support Vector Classifier (SVC) from scikit-learn. 3. **Hyperparameter Tuning**: - Use `GridSearchCV` to find the best hyperparameters for the SVC. - The hyperparameters to tune are: - `C` (Regularization parameter): [0.1, 1, 10, 100] - `kernel`: [\'linear\', \'rbf\'] - For `rbf` kernel only: `gamma`: [0.001, 0.01, 0.1, 1] 4. **Cross-Validation**: - Use 5-fold cross-validation during the grid search. 5. **Evaluation**: - Return the best hyperparameters and the best cross-validation score. # Dataset: - Assume `X` is the feature matrix and `y` is the label vector. # Input and Output Formats: - **Input**: `X` (numpy array of shape (n_samples, n_features)), `y` (numpy array of shape (n_samples,)) - **Output**: Dictionary with keys \\"best_params\\" (dictionary of best hyperparameters) and \\"best_score\\" (best cross-validation score) # Constraints: - You must use the `GridSearchCV` class from scikit-learn. - Ensure your solution is efficient and properly utilizes the pipeline feature in scikit-learn. # Example Usage: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load example dataset data = load_iris() X = data.data y = data.target # Assume your function is implemented as find_best_hyperparameters best_params, best_score = find_best_hyperparameters(X, y) print(\\"Best Hyperparameters:\\", best_params) print(\\"Best CV Score:\\", best_score) ``` # Implementation: ```python from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV def find_best_hyperparameters(X, y): # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'svc\', SVC()) ]) # Define the parameter grid param_grid = [ {\'svc__C\': [0.1, 1, 10, 100], \'svc__kernel\': [\'linear\']}, {\'svc__C\': [0.1, 1, 10, 100], \'svc__kernel\': [\'rbf\'], \'svc__gamma\': [0.001, 0.01, 0.1, 1]}, ] # Initialize GridSearchCV grid_search = GridSearchCV(pipeline, param_grid, cv=5) # Fit the model grid_search.fit(X, y) # Get the best parameters and score best_params = grid_search.best_params_ best_score = grid_search.best_score_ return {\\"best_params\\": best_params, \\"best_score\\": best_score} ```","solution":"from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV def find_best_hyperparameters(X, y): # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'svc\', SVC()) ]) # Define the parameter grid param_grid = [ {\'svc__C\': [0.1, 1, 10, 100], \'svc__kernel\': [\'linear\']}, {\'svc__C\': [0.1, 1, 10, 100], \'svc__kernel\': [\'rbf\'], \'svc__gamma\': [0.001, 0.01, 0.1, 1]}, ] # Initialize GridSearchCV grid_search = GridSearchCV(pipeline, param_grid, cv=5) # Fit the model grid_search.fit(X, y) # Get the best parameters and score best_params = grid_search.best_params_ best_score = grid_search.best_score_ return {\\"best_params\\": best_params, \\"best_score\\": best_score}"},{"question":"**Objective**: Test the ability to implement and compare different decomposition methods using scikit-learn and to evaluate their effectiveness on a given dataset. # Problem Statement You are provided with a labeled dataset containing features and the corresponding class labels. Your task is to implement three different decomposition techniques: Principal Component Analysis (PCA), Sparse Principal Component Analysis (SparsePCA), and Non-Negative Matrix Factorization (NMF). You will perform the following steps: 1. **Load the data**: Import the dataset provided to you (`data.csv`). 2. **Preprocess the data**: Standardize the features for PCA and SparsePCA, but not for NMF. 3. **Implement Decomposition Methods**: - Apply PCA to reduce the data to 2 principal components. - Apply SparsePCA to reduce the data to 2 sparse principal components. - Apply NMF to reduce the data to 2 components. 4. **Visualization**: - Plot the 2D representation of the data obtained from each decomposition method. - Use a scatter plot where each point is colored according to its class label. 5. **Evaluation**: - Calculate the amount of variance explained by the components in PCA. - Discuss the sparsity and interpretability of the components in SparsePCA and NMF. 6. **Complete an analysis**: - Compare the effectiveness of each method in terms of data representation quality and computational aspects. - Provide insights into when each method might be preferable. # Input - `data.csv`: A CSV file containing the dataset with features and class labels. # Output - Scatter plots for PCA, SparsePCA, and NMF. - Explained variance ratio for PCA components. - Brief analysis comparing the methods. # Constraints - Use `sklearn` for implementing decomposition methods. - Use `matplotlib` for plotting. # Performance Requirements - Ensure computations are efficient; avoid unnecessary recalculations. - Ensure visualizations are clear and support the evaluation step. # Notes - Pay attention to the different preprocessing requirements for each method. - For plotting, use the first two components obtained from each decomposition method. # Example Code Layout Here is a layout to help you get started: ```python import pandas as pd from sklearn.decomposition import PCA, SparsePCA, NMF from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt # 1. Load the data data = pd.read_csv(\'data.csv\') X = data.iloc[:, :-1] y = data.iloc[:, -1] # 2. Preprocess the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 3. Implement Decomposition Methods # a) PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # b) SparsePCA sparse_pca = SparsePCA(n_components=2) X_sparse_pca = sparse_pca.fit_transform(X_scaled) # c) NMF nmf = NMF(n_components=2) X_nmf = nmf.fit_transform(X) # 4. Visualization def plot_components(x, y, title): plt.scatter(x[:, 0], x[:, 1], c=y, cmap=\'viridis\') plt.title(title) plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.colorbar() plt.show() plot_components(X_pca, y, \\"PCA Components\\") plot_components(X_sparse_pca, y, \\"SparsePCA Components\\") plot_components(X_nmf, y, \\"NMF Components\\") # 5. Evaluation explained_variance = pca.explained_variance_ratio_ print(f\\"Explained variance by PCA components: {explained_variance}\\") # 6. Analysis # (Write your comparison analysis here) ``` # Submission - Submit your code along with the generated plots and analysis. - Ensure the code is well-documented and follows best practices.","solution":"import pandas as pd from sklearn.decomposition import PCA, SparsePCA, NMF from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt def load_data(file_path): Load the dataset from a CSV file. data = pd.read_csv(file_path) X = data.iloc[:, :-1] y = data.iloc[:, -1] return X, y def preprocess_data(X): Standardize the features for PCA and SparsePCA. scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled def apply_pca(X, n_components=2): Apply PCA to reduce the data to n principal components. pca = PCA(n_components=n_components) X_pca = pca.fit_transform(X) explained_variance = pca.explained_variance_ratio_ return X_pca, explained_variance def apply_sparse_pca(X, n_components=2): Apply SparsePCA to reduce the data to n sparse principal components. sparse_pca = SparsePCA(n_components=n_components) X_sparse_pca = sparse_pca.fit_transform(X) return X_sparse_pca def apply_nmf(X, n_components=2): Apply NMF to reduce the data to n components. No standardization needed before applying NMF. nmf = NMF(n_components=n_components) X_nmf = nmf.fit_transform(X) return X_nmf def plot_components(x, y, title): Plot the 2D representation of the data obtained from decomposition methods. plt.scatter(x[:, 0], x[:, 1], c=y, cmap=\'viridis\') plt.title(title) plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.colorbar() plt.show() def main(file_path): # 1. Load the data X, y = load_data(file_path) # 2. Preprocess the data X_scaled = preprocess_data(X) # 3. Implement Decomposition Methods # a) PCA X_pca, explained_variance = apply_pca(X_scaled) # b) SparsePCA X_sparse_pca = apply_sparse_pca(X_scaled) # c) NMF X_nmf = apply_nmf(X) # 4. Visualization plot_components(X_pca, y, \\"PCA Components\\") plot_components(X_sparse_pca, y, \\"SparsePCA Components\\") plot_components(X_nmf, y, \\"NMF Components\\") # 5. Evaluation print(f\\"Explained variance by PCA components: {explained_variance}\\") # 6. Analysis print(\\"PCA is useful when you need to analyze the variance captured by each component.\\") print(\\"SparsePCA is useful in scenarios where interpretability and sparsity are important.\\") print(\\"NMF is beneficial when you need components to be non-negative, often useful in topic modeling or image analysis.\\") if __name__ == \\"__main__\\": main(\'data.csv\')"},{"question":"**Question: Implement a Custom Neural Network in TorchScript** **Objective:** In this assessment, you are required to implement a simple custom neural network using PyTorch and convert that model into TorchScript. This exercise will demonstrate your understanding of creating models, handling types, and using TorchScript to optimize the model. **Task:** 1. Implement a PyTorch model class named `SimpleNet` which includes: - An initialization method `__init__` to define layers: - Convolution layer (`nn.Conv2d`) with specified input channels, output channels, kernel size. - Activation layer (ReLU). - Fully connected layer (`nn.Linear`). - A forward method `forward` to define the forward pass of your network. 2. Write another class `ScriptedNet` which inherits from `SimpleNet` and: - Annotate the class with `@torch.jit.script`. - Ensure that the forward method, input, and output conform to TorchScript constraints. - Script the network so that it is executable as TorchScript. 3. In a function `run_model`, instantiate `ScriptedNet` and perform: - Dummy data creation using `torch.randn`. - Forward pass of the dummy data through the network. - Output the shapes of the input and output tensors to verify the model. **Expected Input and Output:** - Define the model with `3` input channels and `10` output channels for `nn.Conv2d`, with kernel size `3x3`. - Define the fully connected layer with `output features 100`. - Generate dummy 4D data (Batch, Channels, Height, Width) with size `(1, 3, 224, 224)` as input. - Print the shapes of input and output tensors. **Performance Requirements:** - Ensure all functions and methods execute without errors. - The model should successfully compile and execute under TorchScript. **Constraints:** - You should use only TorchScript supported types and methods. - Follow the standard PyTorch and TorchScript conventions for defining models and scripts. **Question Implementation:** ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleNet(nn.Module): def __init__(self, in_channels, out_channels, fc_units): super(SimpleNet, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1) self.relu = nn.ReLU() self.fc = nn.Linear(out_channels * 224 * 224, fc_units) def forward(self, x): x = self.relu(self.conv(x)) x = x.view(x.size(0), -1) # Flatten the tensor except the batch dimension x = self.fc(x) return x @torch.jit.script class ScriptedNet(SimpleNet): def __init__(self, in_channels, out_channels, fc_units): super(ScriptedNet, self).__init__(in_channels, out_channels, fc_units) def run_model(): model = ScriptedNet(in_channels=3, out_channels=10, fc_units=100) input_tensor = torch.randn(1, 3, 224, 224) output_tensor = model(input_tensor) print(\'Input tensor shape:\', input_tensor.shape) print(\'Output tensor shape:\', output_tensor.shape) run_model() ``` **Training and Execution Example:** - Instantiate `ScriptedNet` for the required channels and units. - Verify the dummy data pass-through. - Ensure correctness by comparing input and output shapes printed. **Note:** Ensure the module runs as both a regular PyTorch model and a TorchScript scripted model.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleNet(nn.Module): def __init__(self, in_channels, out_channels, fc_units): super(SimpleNet, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1) self.relu = nn.ReLU() self.fc = nn.Linear(out_channels * 224 * 224, fc_units) def forward(self, x): x = self.relu(self.conv(x)) x = x.view(x.size(0), -1) # Flatten the tensor except the batch dimension x = self.fc(x) return x class ScriptedNet(SimpleNet): def __init__(self, in_channels, out_channels, fc_units): super(ScriptedNet, self).__init__(in_channels, out_channels, fc_units) @torch.jit.script_method def forward(self, x): x = self.relu(self.conv(x)) x = x.view(x.size(0), -1) x = self.fc(x) return x def run_model(): model = ScriptedNet(in_channels=3, out_channels=10, fc_units=100) scripted_model = torch.jit.script(model) input_tensor = torch.randn(1, 3, 224, 224) output_tensor = scripted_model(input_tensor) print(\'Input tensor shape:\', input_tensor.shape) print(\'Output tensor shape:\', output_tensor.shape) run_model()"},{"question":"Implement a Python class that mimics the behavior of a callable Python object using the `tp_call` protocol. Your implementation needs to demonstrate usage of key aspects from the `tp_call` protocol to handle different types of calls. Additionally, implement a method to mimic vectorcall behavior for optimization. Requirements: 1. **Class Implementation**: - Create a class `CallableInstance` that will be callable using the `__call__` method. - Implement the `__call__` method to accept any number of positional and keyword arguments and perform an operation on these arguments (e.g., summing if they are numbers). 2. **Vectorcall-like Functionality**: - Add a method `vector_call` that implements an optimized way of calling the object with positional and keyword arguments. - Ensure that this method simulates similar behavior to the provided vectorcall protocol; it should be faster for cases where arguments are prepared in vector-like (list or tuple) structures. 3. **Consistency Check**: - Ensure that both the `__call__` method and `vector_call` method return the same result for the same set of arguments to demonstrate they follow the same semantics. Input and Output Format: - **Input**: Create an instance of `CallableInstance` and test: ```python instance = CallableInstance() result_call = instance(1, 2, 3, a=4, b=5) result_vectorcall = instance.vector_call([1, 2, 3], dict(a=4, b=5)) ``` - **Output**: Both `result_call` and `result_vectorcall` should be equivalent. Constraints: - The class should handle different data types gracefully. - Performance should be optimized for vector_call. Example: ```python class CallableInstance: def __call__(self, *args, **kwargs): # Implement tp_call equivalent logic pass def vector_call(self, pos_args, kw_args): # Implement vectorcall equivalent logic pass # Example usage instance = CallableInstance() print(instance(1, 2, 3, a=4, b=5)) # Expected some combined result based on the implementation print(instance.vector_call([1, 2, 3], {\'a\': 4, \'b\': 5})) # Expected the same result as above ``` Implement both methods ensuring they follow the described conventions and provide the same result.","solution":"class CallableInstance: def __call__(self, *args, **kwargs): Implements tp_call equivalent method. Accepts any number of positional and keyword arguments, and returns their sum if they are numbers. total = 0 for arg in args: if isinstance(arg, (int, float)): total += arg for kwarg in kwargs.values(): if isinstance(kwarg, (int, float)): total += kwarg return total def vector_call(self, pos_args, kw_args): Implements vectorcall equivalent method. Accepts positional arguments as a list or tuple, and keyword arguments as a dictionary. Returns the same result as the __call__ method. total = 0 for arg in pos_args: if isinstance(arg, (int, float)): total += arg for kwarg in kw_args.values(): if isinstance(kwarg, (int, float)): total += kwarg return total"},{"question":"# Coding Assessment: Seaborn Heatmap Objective Your task is to demonstrate your understanding of the seaborn library by working with a dataset to create and customize a heatmap. Problem Statement Given a dataframe of car mileage data with columns: `\\"Car\\"`, `\\"City\\"` and `\\"Highway\\"` representing different car models and their mileage in city and highway conditions, write a Python function called `plot_mileage_heatmap` that accomplishes the following: 1. **Load the given DataFrame**: - The function should accept a dataframe as an input parameter. 2. **Pivot the DataFrame**: - Create a pivoted dataframe where the rows are indexed by car models, columns are the mileage types (\\"City\\" and \\"Highway\\"), and cell values are the corresponding mileage values. 3. **Create a Heatmap**: - Plot a heatmap of the pivoted dataframe. - Annotate the heatmap cells with the numerical mileage values. - Use a coolwarm colormap for the heatmap. - Add lines between cells for improved readability. - Set the minimum and maximum values for the colormap between the range 10 and 50. 4. **Customize the plot aesthetics**: - Remove the default axis labels. - Set the tick marks to appear at the top of the plot. Input Format ```python def plot_mileage_heatmap(data: pd.DataFrame) -> None: pass ``` - `data`: A pandas DataFrame containing the mileage data with columns `\\"Car\\"`, `\\"City\\"`, and `\\"Highway\\"`. Output - The function should display the heatmap plot directly using matplotlib display capabilities. Example ```python import pandas as pd data = pd.DataFrame({ \\"Car\\": [\\"Honda Civic\\", \\"Toyota Corolla\\", \\"Ford Focus\\", \\"BMW 3\\"], \\"City\\": [28, 30, 24, 21], \\"Highway\\": [36, 38, 35, 31] }) plot_mileage_heatmap(data) ``` Expected Output: A heatmap plot with the mentioned customizations applied. Constraints - Assume the mileage values are always integers and within the range of 10 to 50. Notes - Use proper seaborn and matplotlib methods to achieve the required functionalities. - Ensure your function handles the DataFrame manipulations correctly to produce the desired plot. - The goal is to test both your seaborn plotting skills and your ability to manipulate dataframes for visualization.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_mileage_heatmap(data: pd.DataFrame) -> None: # Pivot the dataframe pivoted_data = data.pivot(index=\\"Car\\", columns=\\"Type\\", values=\\"Mileage\\") # Plot the heatmap plt.figure(figsize=(8, 6)) ax = sns.heatmap(pivoted_data, annot=True, cmap=\'coolwarm\', linecolor=\'white\', linewidths=1, vmin=10, vmax=50) # Customizing the plot aesthetics ax.set_xlabel(\'\') ax.set_ylabel(\'\') ax.xaxis.tick_top() plt.show()"},{"question":"**Question**: You are tasked with analyzing a file format that uses the EA IFF 85 chunk structure (such as AIFF, AIFF-C, or RMFF). The `chunk.Chunk` class has been provided to assist with reading and processing these chunks, but you need to implement functionality to extract detailed information about each chunk in the file. # Task: Write a function `analyze_chunks(file_path: str) -> List[Dict[str, Any]]` that takes the path to an EA IFF 85 formatted file and returns a list of dictionaries. Each dictionary represents a chunk in the file and should contain the following information: - `id`: The chunk ID. - `size`: The size of the chunk. - `data`: The first 10 bytes of the chunk\'s data (if the chunk has less than 10 bytes, return all available bytes). # Constraints: - Use the `chunk.Chunk` class to read the chunks. - Assume that all chunks are aligned on 2-byte boundaries and use big-endian order. # Example Input: ```json { \\"file_path\\": \\"path/to/your/ea_iff_85_file.iff\\" } ``` # Example Output: ```json [ { \\"id\\": \\"FORM\\", \\"size\\": 1024, \\"data\\": \\"b\'FORM\'...\\" }, { \\"id\\": \\"AIFF\\", \\"size\\": 500, \\"data\\": \\"b\'AIFF\'...\\" }, ... ] ``` # Note: - The function should handle the `EOFError` exception to know when it has reached the end of the file. - Make sure your code closes the file properly after processing. # Implementation Details: Implementing the `analyze_chunks` function involves: 1. Opening the file in binary reading mode. 2. Using the `chunk.Chunk` to read each chunk\'s ID, size, and data. 3. Collecting the specified information into a dictionary and appending it to the result list. 4. Continuing this process until the end of the file is reached. # Starter Code: ```python import chunk def analyze_chunks(file_path: str): result = [] with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f) chunk_info = { \\"id\\": ch.getname(), \\"size\\": ch.getsize(), \\"data\\": ch.read(10) } result.append(chunk_info) ch.skip() except EOFError: break return result ``` Implement the function `analyze_chunks` as described.","solution":"import chunk def analyze_chunks(file_path: str): result = [] with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f) chunk_info = { \\"id\\": ch.getname().decode(\'ascii\'), \\"size\\": ch.getsize(), \\"data\\": ch.read(10) } result.append(chunk_info) ch.skip() except EOFError: break return result"},{"question":"Objective: Write a Python function that, given a binary file, reads its contents, encodes the contents into Base64 (both standard and URL-safe variants), and writes the encoded data into separate output files. Additionally, read these encoded files, decode their contents back into binary, and verify that the decoded data matches the original input. Function Signature: ```python def encode_and_verify_binary_file(input_file_path: str) -> bool: pass ``` Requirements: 1. **Input:** - `input_file_path` (str): The path to the binary input file. 2. **Output:** - A boolean value that indicates whether the decoded data matches the original input data. 3. **Constraints:** - The given input file will definitely exist and will contain valid binary data. - Your function should handle potentially large files efficiently. 4. **Tasks:** - Read the binary content from the `input_file_path`. - Encode the content using both standard Base64 and URL-safe Base64: - Write the standard Base64 encoded contents to `standard_b64_output.txt`. - Write the URL-safe Base64 encoded contents to `urlsafe_b64_output.txt`. - Read back from `standard_b64_output.txt` and `urlsafe_b64_output.txt`: - Decode the contents from both files back to binary. - Verify that the decoded binary content matches the original binary input. Example Usage: ```python # Assume a binary file \\"input.bin\\" exists in the current directory result = encode_and_verify_binary_file(\\"input.bin\\") print(result) # Should print True if the decoding matches the original ``` Notes: - Use the `base64` module provided by Python to handle encoding and decoding. - Pay special attention to ensuring that the file reading and writing operations handle large data efficiently. - Handle potential exceptions and errors gracefully. Implementation: Below is a skeleton of the function you need to implement. Complete the implementation to satisfy the requirements. ```python import base64 def encode_and_verify_binary_file(input_file_path: str) -> bool: # Read the binary content from input_file_path with open(input_file_path, \'rb\') as input_file: binary_data = input_file.read() # Encode using standard Base64 standard_b64_encoded = base64.b64encode(binary_data) # Write standard Base64 encoded contents to \\"standard_b64_output.txt\\" with open(\'standard_b64_output.txt\', \'wb\') as standard_output_file: standard_output_file.write(standard_b64_encoded) # Encode using URL-safe Base64 urlsafe_b64_encoded = base64.urlsafe_b64encode(binary_data) # Write URL-safe Base64 encoded contents to \\"urlsafe_b64_output.txt\\" with open(\'urlsafe_b64_output.txt\', \'wb\') as urlsafe_output_file: urlsafe_output_file.write(urlsafe_b64_encoded) # Decode from \\"standard_b64_output.txt\\" with open(\'standard_b64_output.txt\', \'rb\') as standard_input_file: standard_b64_data = standard_input_file.read() decoded_standard = base64.b64decode(standard_b64_data) # Decode from \\"urlsafe_b64_output.txt\\" with open(\'urlsafe_b64_output.txt\', \'rb\') as urlsafe_input_file: urlsafe_b64_data = urlsafe_input_file.read() decoded_urlsafe = base64.urlsafe_b64decode(urlsafe_b64_data) # Verify if the decoded data matches the original return decoded_standard == binary_data and decoded_urlsafe == binary_data ```","solution":"import base64 def encode_and_verify_binary_file(input_file_path: str) -> bool: # Read the binary content from input_file_path with open(input_file_path, \'rb\') as input_file: binary_data = input_file.read() # Encode using standard Base64 standard_b64_encoded = base64.b64encode(binary_data) # Write standard Base64 encoded contents to \\"standard_b64_output.txt\\" with open(\'standard_b64_output.txt\', \'wb\') as standard_output_file: standard_output_file.write(standard_b64_encoded) # Encode using URL-safe Base64 urlsafe_b64_encoded = base64.urlsafe_b64encode(binary_data) # Write URL-safe Base64 encoded contents to \\"urlsafe_b64_output.txt\\" with open(\'urlsafe_b64_output.txt\', \'wb\') as urlsafe_output_file: urlsafe_output_file.write(urlsafe_b64_encoded) # Decode from \\"standard_b64_output.txt\\" with open(\'standard_b64_output.txt\', \'rb\') as standard_input_file: standard_b64_data = standard_input_file.read() decoded_standard = base64.b64decode(standard_b64_data) # Decode from \\"urlsafe_b64_output.txt\\" with open(\'urlsafe_b64_output.txt\', \'rb\') as urlsafe_input_file: urlsafe_b64_data = urlsafe_input_file.read() decoded_urlsafe = base64.urlsafe_b64decode(urlsafe_b64_data) # Verify if the decoded data matches the original return decoded_standard == binary_data and decoded_urlsafe == binary_data"},{"question":"**Question: Implementing a Custom Backpropagation with PyTorch Tensors** # Problem Statement You are required to implement a custom backpropagation function using PyTorch tensors without relying on the high-level autograd library. To do this, you will manually compute the gradients for a simple neural network layer. # Requirements 1. Implement a custom linear layer from scratch using PyTorch tensors. 2. Manually compute the forward pass of the linear layer. 3. Manually compute the backward pass and gradients for given loss. # Instructions 1. **Linear Layer**: Implement a class `CustomLinear` representing a linear layer with attributes for weights and biases, and methods for the forward and backward passes. The weights and biases must be initialized randomly. ```python class CustomLinear: def __init__(self, input_size, output_size): # Initialize weights and biases pass def forward(self, x): # Implement the forward pass pass def backward(self, dout): # Implement the backward pass pass ``` 2. **Forward Pass**: The `forward` method should take a tensor `x` of shape `(N, input_size)` and return the output `(N, output_size)`. 3. **Backward Pass**: The `backward` method should take the gradient of the loss `dout` with respect to the output and compute gradients with respect to the weights, biases, and input. # Constraints 1. You are not allowed to use high-level PyTorch autograd functionalities (like `torch.autograd` or `tensor.backward()`). 2. Use only basic tensor operations provided by PyTorch. 3. Ensure that the implementation can handle large tensor sizes efficiently. # Example Usage ```python # Initialize tensor for inputs x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=False) # Custom Linear Layer layer = CustomLinear(2, 3) # Forward Pass output = layer.forward(x) print(\\"Forward Output:\\", output) # Example loss gradient dout = torch.ones_like(output) # Backward Pass layer.backward(dout) # Print Gradients print(\\"Weight Gradients:\\", layer.dw) print(\\"Bias Gradients:\\", layer.db) print(\\"Input Gradients:\\", layer.dx) ``` # Expected Output ```plaintext Forward Output: tensor([[x.xxxx, x.xxxx, x.xxxx], [x.xxxx, x.xxxx, x.xxxx]]) Weight Gradients: tensor([[x.xxxx, x.xxxx], [x.xxxx, x.xxxx], [x.xxxx, x.xxxx]]) Bias Gradients: tensor([x.xxx, x.xxx, x.xxx]) Input Gradients: tensor([[x.xx, x.xx], [x.xx, x.xx]]) ``` # Additional Notes This problem requires understanding tensor operations, initialization, and manual gradient computation. This should test the students\' grasp of tensor functionalities, manipulation, and backpropagation concept using PyTorch\'s tensor operations at a low level.","solution":"import torch class CustomLinear: def __init__(self, input_size, output_size): self.input_size = input_size self.output_size = output_size self.weights = torch.randn(input_size, output_size, requires_grad=False) self.biases = torch.randn(output_size, requires_grad=False) self.x = None self.dx = None self.dw = None self.db = None def forward(self, x): Perform the forward pass of the linear layer. self.x = x return torch.mm(x, self.weights) + self.biases def backward(self, dout): Perform the backward pass of the linear layer. Compute the gradients of the weights, biases, and input. # Gradients of loss w.r.t weights self.dw = torch.mm(self.x.t(), dout) # Gradients of loss w.r.t biases self.db = torch.sum(dout, dim=0) # Gradients of loss w.r.t input self.dx = torch.mm(dout, self.weights.t()) return self.dx"},{"question":"# Advanced Coding Assessment Question Objective: Implement a function that accepts a list of integers and performs a series of mathematical and statistical operations as defined below. Problem Statement: You are provided with a list of integers. Implement a function `analyze_numbers` that performs the following steps: 1. **Prime Check**: - Determine, using the `math` module, if each number in the list is a prime number. 2. **Statistical Analysis**: - Calculate the mean and variance of the list using the `statistics` module. Function Signature: ```python def analyze_numbers(data: list) -> dict: pass ``` Expected Input and Output: - **Input**: - `data`: A list of integers where `1 <= len(data) <= 10^6` and `-10^9 <= data[i] <= 10^9`. - **Output**: - A dictionary containing: - `\\"primes\\"`: A list of boolean values where each boolean corresponds to whether the number at that index in `data` is prime or not. - `\\"mean\\"`: The mean of the numbers in `data`. - `\\"variance\\"`: The variance of the numbers in `data`. Example: **Input**: ```python data = [2, 3, 4, 5, 6] ``` **Output**: ```python { \\"primes\\": [True, True, False, True, False], \\"mean\\": 4.0, \\"variance\\": 2.5 } ``` # Constraints: - Your implementation should properly handle large lists efficiently. - You may assume the existence of helper functions to determine prime numbers if necessary. # Notes: 1. Use the `math` module for the prime number check and any mathematical calculations required. 2. Use the `statistics` module for calculating the mean and variance. Good luck, and remember to optimize your code for efficiency!","solution":"import math import statistics def is_prime(n): Returns True if n is a prime number, else False. if n <= 1: return False if n == 2: return True if n % 2 == 0: return False for i in range(3, int(math.sqrt(n)) + 1, 2): if n % i == 0: return False return True def analyze_numbers(data): Analyzes the input list of integers. primes = [is_prime(num) for num in data] mean = statistics.mean(data) variance = statistics.variance(data) if len(data) > 1 else 0.0 result = { \\"primes\\": primes, \\"mean\\": mean, \\"variance\\": variance } return result"},{"question":"Coding Assessment Question # Objective To assess your understanding of Gaussian Mixture Models and Bayesian Gaussian Mixture Models using the `sklearn.mixture` module, you are required to implement a clustering solution on a given dataset. You will also compare the results obtained from the standard GMM with those from the Bayesian GMM. # Question You are given a dataset with features `X` that needs to be clustered into appropriate groups. Your task is to: 1. Use the Gaussian Mixture Model (`GaussianMixture`) to cluster the data. 2. Use the Bayesian Gaussian Mixture Model (`BayesianGaussianMixture`) to cluster the data. 3. Compare the results and explain any differences in the clustering outcomes. # Instructions 1. Implement a function `perform_gmm_clustering` which: - Takes as input a numpy array `X` and the number of components `n_components` for the standard Gaussian Mixture Model. - Fits a `GaussianMixture` model to `X` and predicts the cluster labels. - Computes the Bayesian Information Criterion (BIC) for the fitted model. ```python def perform_gmm_clustering(X: np.ndarray, n_components: int) -> Tuple[np.ndarray, float]: Performs clustering using GaussianMixture. Parameters: X (np.ndarray): The input feature array. n_components (int): The number of components to use for the GaussianMixture. Returns: Tuple[np.ndarray, float]: A tuple containing the cluster labels and the BIC score. pass ``` 2. Implement a function `perform_bgmm_clustering` which: - Takes as input a numpy array `X` and the maximum number of components `max_components` for the Bayesian Gaussian Mixture Model. - Fits a `BayesianGaussianMixture` model to `X` and predicts the cluster labels. - Returns the cluster labels of the data. ```python def perform_bgmm_clustering(X: np.ndarray, max_components: int) -> np.ndarray: Performs clustering using BayesianGaussianMixture. Parameters: X (np.ndarray): The input feature array. max_components (int): The maximum number of components to use for the BayesianGaussianMixture. Returns: np.ndarray: The cluster labels. pass ``` 3. Provide a brief comparison of the clustering results obtained from the `GaussianMixture` and `BayesianGaussianMixture` models. Discuss the impact of the number of components on the clustering results and the BIC score. # Input and Output Formats - `perform_gmm_clustering` - Input: `X` (np.ndarray), `n_components` (int) - Output: Tuple containing `cluster_labels` (np.ndarray), `bic_score` (float) - `perform_bgmm_clustering` - Input: `X` (np.ndarray), `max_components` (int) - Output: `cluster_labels` (np.ndarray) # Constraints - The input dataset `X` will be a 2D numpy array with at least 100 samples and 2 features. - `n_components` for the Gaussian Mixture Model will be an integer between 2 and 10. - `max_components` for the Bayesian Gaussian Mixture Model will be an integer between 2 and 10. # Performance Requirements - The implementation should be efficient enough to handle datasets with up to 1000 samples and 10 features. # Example ```python import numpy as np from sklearn.datasets import make_blobs # Generating a sample dataset X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0) # Performing GMM clustering gmm_labels, gmm_bic = perform_gmm_clustering(X, 4) print(\\"GMM Labels:\\", gmm_labels) print(\\"GMM BIC Score:\\", gmm_bic) # Performing BGMM clustering bgmm_labels = perform_bgmm_clustering(X, 10) print(\\"BGMM Labels:\\", bgmm_labels) # Comparing results # Provide the comparison as your solution output ```","solution":"import numpy as np from sklearn.mixture import GaussianMixture, BayesianGaussianMixture from typing import Tuple def perform_gmm_clustering(X: np.ndarray, n_components: int) -> Tuple[np.ndarray, float]: Performs clustering using GaussianMixture. Parameters: X (np.ndarray): The input feature array. n_components (int): The number of components to use for the GaussianMixture. Returns: Tuple[np.ndarray, float]: A tuple containing the cluster labels and the BIC score. gmm = GaussianMixture(n_components=n_components, random_state=0) gmm.fit(X) labels = gmm.predict(X) bic_score = gmm.bic(X) return labels, bic_score def perform_bgmm_clustering(X: np.ndarray, max_components: int) -> np.ndarray: Performs clustering using BayesianGaussianMixture. Parameters: X (np.ndarray): The input feature array. max_components (int): The maximum number of components to use for the BayesianGaussianMixture. Returns: np.ndarray: The cluster labels. bgmm = BayesianGaussianMixture(n_components=max_components, random_state=0) bgmm.fit(X) labels = bgmm.predict(X) return labels # Comparison of results # Typically, the BIC score for GMM can influence the selection of components. # BGMM provides an automatic component selection through the Dirichlet process prior. # As a result, BGMM might find a more suitable number of clusters even if max_components is set high."},{"question":"Objective You are tasked with implementing a function to manage PyTorch operations on XPU devices. This includes switching devices, allocating memory, and synchronizing operations across devices. Function Signature ```python def manage_xpu_operations(device_ids: list): Manages operations on specified XPU devices. Parameters: device_ids (list): List of integers representing the device IDs to be used. Returns: dict: Dictionary containing device memory statistics after operations. ``` **Input:** - `device_ids` is a list of integers specifying the device IDs that you need to manage. Assume device IDs start from 0 up to `torch.xpu.device_count()` - 1. **Output:** - The function should return a dictionary containing memory statistics for each device after performing some operations. **Description:** 1. **Device Management:** - Initialize XPU if not already initialized. - Verify that all device IDs in the input list are valid (i.e., within the range of available devices). - Set the active device to each ID in the list and perform the following actions: 2. **Memory Management:** - Clear the cache for the current device. - Allocate a tensor of size (1000, 1000) filled with random numbers on the XPU. - Synchronize the device to ensure all operations are complete. 3. **Statistics Collection:** - Collect and return memory statistics for each device. The returned dictionary should map each device ID to a sub-dictionary containing memory stats: `max_memory_allocated`, `memory_allocated`, `memory_reserved`. # Example Usage: ```python device_ids = [0, 1] stats = manage_xpu_operations(device_ids) print(stats) ``` **Constraints:** - Ensure that the solution handles exceptions where a device ID is invalid or if operations fail. **Note:** - Utilize the following functions and classes from `torch.xpu`: - `is_available()` - `init()` - `device_count()` - `set_device()` - `empty_cache()` - `synchronize()` - `max_memory_allocated()` - `memory_allocated()` - `memory_reserved()` - Tensor operations (e.g., allocate random tensor in device memory) **Performance Requirements:** - The function should be efficient and handle scenarios with multiple devices without significant performance degradation.","solution":"import torch def manage_xpu_operations(device_ids: list): Manages operations on specified XPU devices. Parameters: device_ids (list): List of integers representing the device IDs to be used. Returns: dict: Dictionary containing device memory statistics after operations. # Check if XPU is available if not torch.xpu.is_available(): raise RuntimeError(\\"XPU devices are not available\\") # Initialize XPU if not already initialized torch.xpu.init() # Get the number of available XPU devices num_devices = torch.xpu.device_count() # Validate the provided device IDs for device_id in device_ids: if device_id < 0 or device_id >= num_devices: raise ValueError(f\\"Invalid device ID: {device_id}\\") stats = {} for device_id in device_ids: # Set the current active device torch.xpu.set_device(device_id) # Clear cache for the current device torch.xpu.empty_cache() # Allocate a tensor of size (1000, 1000) filled with random numbers on the XPU tensor = torch.rand((1000, 1000), device=f\'xpu:{device_id}\') # Synchronize the XPU device to ensure all operations are complete torch.xpu.synchronize(device_id) # Collect memory statistics device_stats = { \'max_memory_allocated\': torch.xpu.max_memory_allocated(device_id), \'memory_allocated\': torch.xpu.memory_allocated(device_id), \'memory_reserved\': torch.xpu.memory_reserved(device_id) } stats[device_id] = device_stats return stats"},{"question":"# Question: DataFrame Merging and Aggregation You are given three DataFrames containing information about employees at a company. The goal is to merge these DataFrames to create a single DataFrame with comprehensive information about each employee. Additionally, you should perform some aggregations to derive meaningful insights about the data. The DataFrames are as follows: **DataFrame 1: employees** | id | name | department_id | |----|-------|---------------| | 1 | Alice | 101 | | 2 | Bob | 102 | | 3 | Carol | 102 | | 4 | David | 101 | | 5 | Eve | 103 | **DataFrame 2: departments** | department_id | department_name | |---------------|-----------------| | 101 | HR | | 102 | Engineering | | 103 | Marketing | | 104 | Sales | **DataFrame 3: salaries** | id | salary | |----|--------| | 1 | 70000 | | 2 | 80000 | | 3 | 90000 | | 4 | 75000 | | 5 | 85000 | **Tasks:** 1. **Merge the DataFrames**: Merge the three DataFrames to obtain a single DataFrame containing all the columns: `id`, `name`, `department_id`, `department_name`, and `salary`. Ensure that the final DataFrame contains all the employees and their corresponding department names and salaries. If a department name is missing for a given `department_id`, it should not be included in the result. 2. **Department-wise Aggregation**: - Calculate the **average salary** for each department. - Calculate the **total salary** for each department. 3. **Highest Earning Employee**: - Identify the employee with the highest salary in the merged DataFrame along with their department name. **Solution Requirements:** - Implement the function `merge_and_analyze_dataframes(employees, departments, salaries)` which takes three pandas DataFrames as input and returns the following: - A merged DataFrame. - A DataFrame containing department-wise average and total salaries. - A DataFrame containing the details of the highest earning employee. **Input:** - `employees`: DataFrame with columns `id`, `name`, and `department_id`. - `departments`: DataFrame with columns `department_id` and `department_name`. - `salaries`: DataFrame with columns `id` and `salary`. **Output:** Returns a tuple of three DataFrames: 1. Merged DataFrame. 2. Department-wise aggregated DataFrame. 3. DataFrame with highest earning employee details. **Function Signature:** ```python import pandas as pd def merge_and_analyze_dataframes(employees: pd.DataFrame, departments: pd.DataFrame, salaries: pd.DataFrame) -> tuple: pass ``` **Example:** ```python employees = pd.DataFrame({ \'id\': [1, 2, 3, 4, 5], \'name\': [\'Alice\', \'Bob\', \'Carol\', \'David\', \'Eve\'], \'department_id\': [101, 102, 102, 101, 103] }) departments = pd.DataFrame({ \'department_id\': [101, 102, 103, 104], \'department_name\': [\'HR\', \'Engineering\', \'Marketing\', \'Sales\'] }) salaries = pd.DataFrame({ \'id\': [1, 2, 3, 4, 5], \'salary\': [70000, 80000, 90000, 75000, 85000] }) merged_df, dept_aggregates_df, highest_earner_df = merge_and_analyze_dataframes(employees, departments, salaries) print(merged_df) print(dept_aggregates_df) print(highest_earner_df) ``` **Constraints:** - You should use efficient pandas operations to handle the data. - No use of loops; only use pandas\' built-in functions for the manipulations.","solution":"import pandas as pd def merge_and_analyze_dataframes(employees: pd.DataFrame, departments: pd.DataFrame, salaries: pd.DataFrame) -> tuple: # Merge employees with departments to get department names merged_df = pd.merge(employees, departments, on=\'department_id\', how=\'inner\') # Merge the above result with salaries to get complete employee information merged_df = pd.merge(merged_df, salaries, on=\'id\', how=\'inner\') # Group by department and calculate average and total salary dept_aggregates_df = merged_df.groupby(\'department_name\').agg( avg_salary=(\'salary\', \'mean\'), total_salary=(\'salary\', \'sum\') ).reset_index() # Find the highest earning employee highest_salary = merged_df[\'salary\'].max() highest_earner_df = merged_df[merged_df[\'salary\'] == highest_salary] return merged_df, dept_aggregates_df, highest_earner_df"},{"question":"**Problem Statement: Handling Integer Conversions** In Python, integers are represented as objects of type `PyLongObject`. This task assesses your understanding of creating and manipulating Python integer objects through appropriate conversions and error handling. # Function 1: `create_integer_from_base` **Objective**: Implement a function to create a Python integer object from a string representation of the number in a given base. **Function Signature**: ```python def create_integer_from_base(number_str: str, base: int) -> int: pass ``` **Input**: - `number_str` (str): A string representing a number. - `base` (int): The base of the number in `number_str`, where `base` can be between 2 and 36, inclusive. If `base` is 0, the function should deduce the base, similar to Python\'s `int` function. **Output**: - An `int` representing the number in the provided `base`. **Constraints**: - Assume `number_str` is a valid string representation of a number within the provided base. - Raise `ValueError` if the base is out of the allowed range or if the conversion fails. **Example**: ```python create_integer_from_base(\'1010\', 2) # Output: 10 create_integer_from_base(\'1A\', 16) # Output: 26 create_integer_from_base(\'12345\', 0) # Output: 12345 create_integer_from_base(\'110\', 3) # Output: 12 ``` **Notes**: 1. Implement error handling to raise `ValueError` when the input conditions are not met. 2. Consider using `PyLong_FromString` for base conversion operations. # Function 2: `convert_integer_with_check` **Objective**: Implement a function to convert a Python integer object to a C type with overflow checking. **Function Signature**: ```python def convert_integer_with_check(number: int) -> tuple: pass ``` **Input**: - `number` (int): An integer. **Output**: - A tuple containing: - The converted value as `long` (if no overflow occurs). - An overflow indicator (`0` for no overflow, `1` for overflow). **Constraints**: - Handle cases where the integer value exceeds the range of a `long`, similar to `PyLong_AsLongAndOverflow`. **Example**: ```python convert_integer_with_check(2147483647) # Output: (2147483647, 0) convert_integer_with_check(9223372036854775807) # Output: (9223372036854775807, 0) convert_integer_with_check(9223372036854775808) # Output: (9223372036854775808, 1) convert_integer_with_check(-9223372036854775809) # Output: (-9223372036854775809, 1) ``` **Notes**: 1. Implement error handling for invalid inputs. 2. Consider using `PyLong_AsLongAndOverflow` for conversion operations and overflow checks. # Final Implementation You are expected to implement the above functions in Python using principles outlined in the documentation. Ensure that you handle edge cases and errors appropriately.","solution":"def create_integer_from_base(number_str: str, base: int) -> int: Converts a string representation of a number in the given base to an integer. number_str: A string representing a number. base: The base for conversion. A value in the range [2, 36] or 0. Returns: The integer value after conversion. Raises: ValueError: If the base is not in the allowed range or conversion fails. if not (base == 0 or 2 <= base <= 36): raise ValueError(\\"Base must be between 2 and 36, or 0 for automatic detection.\\") try: return int(number_str, base) except ValueError: raise ValueError(\\"Invalid number for the given base.\\") def convert_integer_with_check(number: int) -> tuple: Converts an integer to a long type with overflow checking. number: The input integer. Returns: A tuple with the converted value and an overflow indicator (0 for no overflow, 1 for overflow). LONG_MIN = -9223372036854775808 LONG_MAX = 9223372036854775807 try: if number < LONG_MIN or number > LONG_MAX: return (number, 1) else: return (number, 0) except TypeError: raise ValueError(\\"Invalid input, not an integer.\\")"},{"question":"# **Coding Assessment Question** Implement a Python function `generate_manifest` that simulates the behavior of creating a manifest file for distribution based on given include/exclude rules. # Function Signature ```python def generate_manifest(file_structure: dict, include_rules: list, exclude_rules: list) -> list: pass ``` # Input 1. `file_structure`: A dictionary representing the file structure of a project. Keys are directory names (or `\\"root\\"` for the root directory), and values are lists of files (as strings) or nested dictionaries representing subdirectories. - Example: ```python { \\"root\\": [\\"setup.py\\", \\"README.txt\\"], \\"src\\": { \\"main\\": [\\"main.py\\", \\"utils.py\\"], \\"test\\": [\\"test_main.py\\"] }, \\"docs\\": [\\"index.rst\\", \\"usage.rst\\"] } ``` 2. `include_rules`: A list of strings representing the include rules, matching the syntax of the manifest template. - Example: `[\\"include *.txt\\", \\"recursive-include src *.py\\"]` 3. `exclude_rules`: A list of strings representing the exclude rules, matching the syntax of the manifest template. - Example: `[\\"prune src/test\\"]` # Output - A list of strings representing the files to be included in the manifest file, in the order they should appear. # Constraints - The function should handle nested directories according to the `file_structure` input. - The rules in `include_rules` and `exclude_rules` follow a simplified version of the manifest template syntax provided in the documentation. - Wildcard matching (`*`) should be limited to file names and extensions only (`*.txt` matches all `.txt` files, `test*` matches files like `test_main.py`). # Example ```python file_structure = { \\"root\\": [\\"setup.py\\", \\"README.txt\\"], \\"src\\": { \\"main\\": [\\"main.py\\", \\"utils.py\\"], \\"test\\": [\\"test_main.py\\"] }, \\"docs\\": [\\"index.rst\\", \\"usage.rst\\"] } include_rules = [\\"include *.txt\\", \\"recursive-include src *.py\\"] exclude_rules = [\\"prune src/test\\"] output = generate_manifest(file_structure, include_rules, exclude_rules) print(output) ``` Expected output: ```python [\\"README.txt\\", \\"src/main/main.py\\", \\"src/main/utils.py\\"] ``` Here, `README.txt` is included because of the `include *.txt` rule. The `recursive-include src *.py` rule includes all Python files in the `src` directory tree, but the `prune src/test` rule excludes files in the `src/test` directory. # Explanation This function should read the project structure and rules to generate a list of files that would be included in a source distribution manifest. This involves: 1. Applying include rules to gather the initial set of files. 2. Applying exclude rules to remove specific files or directories from the set. Be sure to handle edge cases such as empty directory lists and conflicting rules.","solution":"import fnmatch def generate_manifest(file_structure: dict, include_rules: list, exclude_rules: list) -> list: def match_pattern(filepath, pattern): return fnmatch.fnmatchcase(filepath, pattern) def apply_include_rules(file_structure, rules): included_files = set() def recursive_include(path, structure, pattern): if isinstance(structure, dict): for sub_path, sub_structure in structure.items(): recursive_include(f\\"{path}/{sub_path}\\", sub_structure, pattern) else: for file in structure: if match_pattern(file, pattern): included_files.add(f\\"{path}/{file}\\".strip(\'/\')) for rule in rules: parts = rule.split() if len(parts) == 2 and parts[0] == \\"include\\": pattern = parts[1] recursive_include(\'\', file_structure[\'root\'], pattern) elif len(parts) == 3 and parts[0] == \\"recursive-include\\": directory, pattern = parts[1], parts[2] if directory in file_structure: recursive_include(directory, file_structure[directory], pattern) return included_files def apply_exclude_rules(included_files, rules): excluded_files = set() for rule in rules: parts = rule.split() if len(parts) == 2 and parts[0] == \\"prune\\": directory = parts[1] for file in included_files: if file.startswith(directory): excluded_files.add(file) return included_files - excluded_files # Apply the include and exclude rules included_files = apply_include_rules(file_structure, include_rules) final_files = apply_exclude_rules(included_files, exclude_rules) return sorted(final_files)"},{"question":"# Coding Assessment: Working with Nested Tensors in PyTorch **Objective**: Demonstrate your understanding of constructing, manipulating, and operating on nested tensors in PyTorch. Problem Statement You are provided with batches of variable-length sequences representing text data. Your task is to: 1. Construct a nested tensor from the given data. 2. Convert the nested tensor to a padded dense tensor. 3. Perform an element-wise operation on the padded tensor. 4. Convert the result back to a nested tensor. Requirements 1. **Construct a Nested Tensor**: - Use the `torch.nested.nested_tensor` function to create a nested tensor from a list of one-dimensional tensors. - Ensure that the constructed nested tensor uses the `torch.jagged` layout. 2. **Convert to a Padded Tensor**: - Convert the nested tensor to a padded dense tensor using the `torch.nested.to_padded_tensor` function. - Choose an appropriate padding value. 3. **Element-wise Operation**: - Perform an element-wise operation (e.g., adding a constant) on the padded tensor. 4. **Convert Back to Nested Tensor**: - Convert the modified padded tensor back to a nested tensor using the `torch.nested.narrow` function. - Ensure the nested tensor has the same structure as the original. Constraints - The input list will consist of one-dimensional PyTorch tensors of varying lengths. - The padding value should be chosen appropriately to avoid bias in the resulting tensor. - You are expected to handle potential errors during the construction of nested tensors. Example ```python import torch def process_variable_length_sequences(tensor_list, padding_value=0, constant_add=5): # Step 1: Construct a Nested Tensor nested_tensor = torch.nested.nested_tensor(tensor_list, layout=torch.jagged) # Step 2: Convert to a Padded Tensor padded_tensor = torch.nested.to_padded_tensor(nested_tensor, padding=padding_value) # Step 3: Perform Element-wise Operation (adding a constant value) padded_tensor += constant_add # Step 4: Convert Back to Nested Tensor seq_lens = torch.tensor([t.shape[0] for t in tensor_list], dtype=torch.int64) result_nested_tensor = torch.nested.narrow(padded_tensor, dim=1, length=seq_lens, layout=torch.jagged) return result_nested_tensor # Example usage: tensor_list = [torch.tensor([1, 2, 3]), torch.tensor([4, 5]), torch.tensor([6, 7, 8, 9])] result = process_variable_length_sequences(tensor_list, padding_value=-1, constant_add=1) print(result) ``` **Expected Output Format**: Describe the structure and values in the resulting nested tensor to verify it matches the operations described. **Note**: Ensure your implementation handles edge cases and follows the guidelines provided in the documentation.","solution":"import torch def process_variable_length_sequences(tensor_list, padding_value=0, constant_add=5): Processes variable-length sequences by constructing a nested tensor, converting it to a padded dense tensor, performing an element-wise operation, and converting it back to a nested tensor. Parameters: - tensor_list: List of one-dimensional PyTorch tensors of varying lengths. - padding_value: Value to use for padding. - constant_add: Constant value to add element-wise to the padded tensor. Returns: - result_nested_tensor: A nested tensor after the element-wise operation. # Step 1: Construct a Nested Tensor nested_tensor = torch.nested.nested_tensor(tensor_list, layout=torch.strided) # Step 2: Convert to a Padded Tensor padded_tensor = nested_tensor.to_padded_tensor(padding_value) # Step 3: Perform Element-wise Operation (adding a constant value) padded_tensor += constant_add # Step 4: Convert Back to Nested Tensor lengths = [t.shape[0] for t in tensor_list] result_list = [padded_tensor[i, :lengths[i]] for i in range(len(lengths))] result_nested_tensor = torch.nested.nested_tensor(result_list, layout=torch.strided) return result_nested_tensor # Example usage: tensor_list = [torch.tensor([1, 2, 3]), torch.tensor([4, 5]), torch.tensor([6, 7, 8, 9])] result = process_variable_length_sequences(tensor_list, padding_value=-1, constant_add=1) print(result)"},{"question":"# Question **Objective:** To demonstrate your understanding of the `seaborn.FacetGrid` class and its capabilities, you will create a multi-faceted plot with specific customizations. Your task is to write a function that performs the following steps: 1. Loads the `tips` dataset from seaborn. 2. Creates a `FacetGrid` with the `time` variable assigned to columns and the `sex` variable assigned to rows. 3. Uses a scatter plot to visualize the relationship between `total_bill` and `tip`. 4. Colors the points by the `smoker` variable. 5. Adds a reference line at the median `total_bill` across all data. 6. Updates axis labels to `Total Bill ()` and `Tip ()`. 7. Saves the figure with a specified filename. **Function Signature:** ```python def create_facet_grid_plot(filename: str) -> None: pass ``` **Input:** - `filename` (str): The name of the file where the figure should be saved. **Output:** - The function should save the plot to the specified filename. **Constraints:** - You must use the `FacetGrid` class from `seaborn`. - The average plotting time should not exceed 2 seconds. **Example:** ```python create_facet_grid_plot(\\"output_plot.png\\") # This would save the created plot as \\"output_plot.png\\" ``` **Notes:** - Use the `add_legend()` method to include a legend for the `smoker` variable. - Customize the size and aspect ratio of the plots to make the visualization clear and aesthetically pleasing. - Ensure your code is well-documented and follows best practices for readability and efficiency.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_facet_grid_plot(filename: str) -> None: Creates a FacetGrid of scatter plots with the `tips` dataset, showing the relationship between `total_bill` and `tip`. The grid is faceted by `time` and `sex` with color by `smoker`. A reference line at the median of `total_bill` is added. The plot is saved as the specified filename. Parameters: filename (str): The name of the file where the figure should be saved. # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create the FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"sex\\", hue=\\"smoker\\", margin_titles=True) # Map the scatter plot on the grid g.map(sns.scatterplot, \\"total_bill\\", \\"tip\\") # Add reference line at the median total_bill across all data median_total_bill = np.median(tips[\'total_bill\']) g.map(lambda **kwargs: plt.axvline(median_total_bill, color=\'red\', linestyle=\'--\')) # Update axis labels g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") # Add legend g.add_legend() # Save the plot plt.savefig(filename)"},{"question":"# Custom Sequence Class You are required to create a custom sequence class `CustomSequence` that inherits from the `collections.abc.Sequence` abstract base class. This custom sequence should mimic the behavior of a standard sequence, such as a list, including indexing and length operations but should also include specific additional features as described below. Requirements 1. **Initialization**: The class should be initialized with an iterable (list, tuple, etc.). 2. **Indexing**: It should support indexing to get items. 3. **Length**: It should support the `len()` function to get the number of items. 4. **Contains**: It should support the `in` operator to check if an item exists in the sequence. 5. **Count**: It should support a `count` method to count occurrences of a particular value. 6. **Reversal**: It should support the `reversed()` method. 7. **Extend**: Additionally, implement an `extend` method to add multiple elements to the sequence at once. Instructions 1. Implement the `CustomSequence` class by inheriting from `collections.abc.Sequence`. 2. Ensure you define and appropriately implement the methods: - `__getitem__(self, index)` - `__len__(self)` - `__contains__(self, value)`: This allows the use of the `in` operator. - `count(self, value)` - `__reversed__(self)`: This allows the use of the `reversed()` function. - `extend(self, values)`: Adds the elements from `values` (an iterable) to the end of the sequence. 3. Use the `ListBasedSet` example from the documentation provided as a reference to understand how to implement these methods. Example Usage ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): self.elements = list(iterable) def __getitem__(self, index): return self.elements[index] def __len__(self): return len(self.elements) def __contains__(self, value): return value in self.elements def count(self, value): return self.elements.count(value) def __reversed__(self): return reversed(self.elements) def extend(self, values): self.elements.extend(values) # Example usage: seq = CustomSequence([1, 2, 3]) print(len(seq)) # Output: 3 print(seq[1]) # Output: 2 print(2 in seq) # Output: True print(seq.count(1)) # Output: 1 print(list(reversed(seq))) # Output: [3, 2, 1] seq.extend([4, 5]) print(list(seq)) # Output: [1, 2, 3, 4, 5] ``` Constraints 1. Do not use any built-in sequence types like `list` or `tuple` directly for storage; use only a custom attribute within your class. 2. Ensure your class works efficiently for common sequence operations. 3. Provide necessary documentation/comments explaining your code. Good luck, and happy coding!","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): Initializes the CustomSequence with elements from the provided iterable. self.elements = list(iterable) def __getitem__(self, index): Returns the element at the specified index. return self.elements[index] def __len__(self): Returns the number of elements in the CustomSequence. return len(self.elements) def __contains__(self, value): Checks if the value is in the CustomSequence. return value in self.elements def count(self, value): Returns the number of occurrences of the value in the CustomSequence. return self.elements.count(value) def __reversed__(self): Returns a reversed iterator over the elements of the CustomSequence. return reversed(self.elements) def extend(self, values): Adds the elements from the provided iterable to the end of the CustomSequence. self.elements.extend(values) # Example usage: seq = CustomSequence([1, 2, 3]) print(len(seq)) # Output: 3 print(seq[1]) # Output: 2 print(2 in seq) # Output: True print(seq.count(1)) # Output: 1 print(list(reversed(seq))) # Output: [3, 2, 1] seq.extend([4, 5]) print(list(seq)) # Output: [1, 2, 3, 4, 5]"},{"question":"**Problem Statement:** You are provided with a dataset `penguins.csv` which contains data about different penguin species. Your task is to build a Decision Tree Classifier to classify the species of the penguins based on their features. Additionally, you need to prune the tree to improve its performance and visualize the final tree. The dataset contains the following columns: - `species`: The species of the penguin (target variable) - `culmen_length_mm`: The length of the penguin\'s culmen in millimeters - `culmen_depth_mm`: The depth of the penguin\'s culmen in millimeters - `flipper_length_mm`: The length of the penguin\'s flipper in millimeters - `body_mass_g`: The mass of the penguin\'s body in grams - `island`: The island where the penguin was observed - `sex`: The sex of the penguin (categories: `MALE`, `FEMALE`), with some missing values. **Task:** 1. **Load the Dataset**: Load the dataset and preprocess it by handling missing values. Use mean imputation for continuous features and mode imputation for categorical features. 2. **Train-Test Split**: Split the dataset into training and testing sets (80% training, 20% testing). 3. **Train a Decision Tree Classifier**: Train a `DecisionTreeClassifier` on the training set. Use appropriate hyperparameters to prevent overfitting. 4. **Evaluate the Model**: Evaluate the model on the testing set and print the accuracy. 5. **Pruning the Tree**: Use minimal cost-complexity pruning to prune the tree and re-evaluate the model. 6. **Visualize the Tree**: Visualize the final pruned tree. **Requirements:** - Implement the function `load_and_preprocess_data(file_path)`. - Implement the function `train_and_evaluate_model(X_train, X_test, y_train, y_test)`. - Implement the function `prune_tree_and_evaluate(model, X_train, X_test, y_train, y_test)`. - Implement the function `visualize_tree(model)`. ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.impute import SimpleImputer from sklearn.tree import plot_tree import matplotlib.pyplot as plt def load_and_preprocess_data(file_path): Load the dataset and preprocess it by handling missing values. Parameters: - file_path (str): The path to the dataset file. Returns: - X (DataFrame): The features of the dataset. - y (Series): The target variable. data = pd.read_csv(file_path) # Handle missing values imputer_continuous = SimpleImputer(strategy=\'mean\') data[[\'culmen_length_mm\', \'culmen_depth_mm\', \'flipper_length_mm\', \'body_mass_g\']] = imputer_continuous.fit_transform(data[[\'culmen_length_mm\', \'culmen_depth_mm\', \'flipper_length_mm\', \'body_mass_g\']]) imputer_categorical = SimpleImputer(strategy=\'most_frequent\') data[[\'sex\']] = imputer_categorical.fit_transform(data[[\'sex\']]) # One-hot encoding for categorical features data = pd.get_dummies(data, columns=[\'island\', \'sex\'], drop_first=True) # Separate features and target X = data.drop(columns=\'species\') y = data[\'species\'] return X, y def train_and_evaluate_model(X_train, X_test, y_train, y_test): Train a Decision Tree Classifier on the training set and evaluate it. Parameters: - X_train (DataFrame): The training features. - X_test (DataFrame): The testing features. - y_train (Series): The training target. - y_test (Series): The testing target. Returns: - model (DecisionTreeClassifier): The trained decision tree model. model = DecisionTreeClassifier(max_depth=5, min_samples_split=10) model.fit(X_train, y_train) # Evaluate the model accuracy = model.score(X_test, y_test) print(f\\"Accuracy: {accuracy:.2f}\\") return model def prune_tree_and_evaluate(model, X_train, X_test, y_train, y_test): Prune the tree using minimal cost-complexity pruning and re-evaluate it. Parameters: - model (DecisionTreeClassifier): The trained decision tree model. - X_train (DataFrame): The training features. - X_test (DataFrame): The testing features. - y_train (Series): The training target. - y_test (Series): The testing target. Returns: - pruned_model (DecisionTreeClassifier): The pruned decision tree model. path = model.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas # Iterate through different ccp_alpha values and find the best model best_alpha = 0 best_accuracy = 0 for alpha in ccp_alphas: temp_model = DecisionTreeClassifier(ccp_alpha=alpha) temp_model.fit(X_train, y_train) accuracy = temp_model.score(X_test, y_test) if accuracy > best_accuracy: best_alpha = alpha best_accuracy = accuracy # Prune the tree with the best alpha value pruned_model = DecisionTreeClassifier(ccp_alpha=best_alpha) pruned_model.fit(X_train, y_train) # Evaluate the pruned model accuracy = pruned_model.score(X_test, y_test) print(f\\"Pruned Model Accuracy: {accuracy:.2f}\\") return pruned_model def visualize_tree(model): Visualize the decision tree. Parameters: - model (DecisionTreeClassifier): The decision tree model. plt.figure(figsize=(20,10)) plot_tree(model, filled=True, feature_names=X.columns, class_names=model.classes_, rounded=True) plt.show() # Main function def main(): file_path = \\"penguins.csv\\" X, y = load_and_preprocess_data(file_path) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) model = train_and_evaluate_model(X_train, X_test, y_train, y_test) pruned_model = prune_tree_and_evaluate(model, X_train, X_test, y_train, y_test) visualize_tree(pruned_model) if __name__ == \\"__main__\\": main() ``` **Constraints:** - The dataset should be preprocessed to handle missing values using imputation techniques. - The Decision Tree should be evaluated using accuracy as the metric. - The pruning should be performed by considering a range of `ccp_alpha` values to find the best model. - The tree should be visualized using appropriate visualizations. **Performance Requirements:** - The solution should be efficient and make use of scikit-learn\'s built-in functions wherever possible. - The model should be pruned and visualized accurately to avoid overfitting and provide a clear interpretation.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.impute import SimpleImputer from sklearn.tree import plot_tree import matplotlib.pyplot as plt def load_and_preprocess_data(file_path): Load the dataset and preprocess it by handling missing values. Parameters: - file_path (str): The path to the dataset file. Returns: - X (DataFrame): The features of the dataset. - y (Series): The target variable. data = pd.read_csv(file_path) # Handle missing values imputer_continuous = SimpleImputer(strategy=\'mean\') data[[\'culmen_length_mm\', \'culmen_depth_mm\', \'flipper_length_mm\', \'body_mass_g\']] = imputer_continuous.fit_transform(data[[\'culmen_length_mm\', \'culmen_depth_mm\', \'flipper_length_mm\', \'body_mass_g\']]) imputer_categorical = SimpleImputer(strategy=\'most_frequent\') data[[\'sex\']] = imputer_categorical.fit_transform(data[[\'sex\']]) # One-hot encoding for categorical features data = pd.get_dummies(data, columns=[\'island\', \'sex\'], drop_first=True) # Separate features and target X = data.drop(columns=\'species\') y = data[\'species\'] return X, y def train_and_evaluate_model(X_train, X_test, y_train, y_test): Train a Decision Tree Classifier on the training set and evaluate it. Parameters: - X_train (DataFrame): The training features. - X_test (DataFrame): The testing features. - y_train (Series): The training target. - y_test (Series): The testing target. Returns: - model (DecisionTreeClassifier): The trained decision tree model. model = DecisionTreeClassifier(max_depth=5, min_samples_split=10) model.fit(X_train, y_train) # Evaluate the model accuracy = model.score(X_test, y_test) print(f\\"Accuracy: {accuracy:.2f}\\") return model def prune_tree_and_evaluate(model, X_train, X_test, y_train, y_test): Prune the tree using minimal cost-complexity pruning and re-evaluate it. Parameters: - model (DecisionTreeClassifier): The trained decision tree model. - X_train (DataFrame): The training features. - X_test (DataFrame): The testing features. - y_train (Series): The training target. - y_test (Series): The testing target. Returns: - pruned_model (DecisionTreeClassifier): The pruned decision tree model. path = model.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas # Iterate through different ccp_alpha values and find the best model best_alpha = 0 best_accuracy = 0 for alpha in ccp_alphas: temp_model = DecisionTreeClassifier(ccp_alpha=alpha) temp_model.fit(X_train, y_train) accuracy = temp_model.score(X_test, y_test) if accuracy > best_accuracy: best_alpha = alpha best_accuracy = accuracy # Prune the tree with the best alpha value pruned_model = DecisionTreeClassifier(ccp_alpha=best_alpha) pruned_model.fit(X_train, y_train) # Evaluate the pruned model accuracy = pruned_model.score(X_test, y_test) print(f\\"Pruned Model Accuracy: {accuracy:.2f}\\") return pruned_model def visualize_tree(model, X): Visualize the decision tree. Parameters: - model (DecisionTreeClassifier): The decision tree model. - X (DataFrame): The features of the dataset plt.figure(figsize=(20,10)) plot_tree(model, filled=True, feature_names=X.columns, class_names=model.classes_, rounded=True) plt.show() # Main function def main(): file_path = \\"penguins.csv\\" X, y = load_and_preprocess_data(file_path) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) model = train_and_evaluate_model(X_train, X_test, y_train, y_test) pruned_model = prune_tree_and_evaluate(model, X_train, X_test, y_train, y_test) visualize_tree(pruned_model, X) if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question** # Objective Create a function using the seaborn library to generate customized color palettes. Your function should accept various inputs defining color properties and return both a discrete and a continuous colormap based on those properties. # Function Signature ```python def generate_custom_palette(color, input_type=\\"name\\", num_colors=8, as_cmap=False): Generate a customized sequential color palette using seaborn light_palette. Parameters: - color: The base color for the palette. This can be specified as: - A named color (valid named colors according to seaborn syntax). - A hex color code (as a string starting with \'#\'). - A tuple of HUSL values if `input_type` is set to \\"husl\\". - input_type: Specifies the type of color input. This can be: - \\"name\\" (default) for named colors. - \\"hex\\" for hex color codes. - \\"husl\\" for HUSL values. - num_colors: An integer representing the number of color variations in the palette. - as_cmap: Boolean indicating if the return value should be a continuous colormap. Returns: - A seaborn color palette object, either as a discrete list or a continuous colormap. pass ``` # Input - `color` (str or tuple): The base color for the palette. - `input_type` (str): Type of the color input, \\"name\\", \\"hex\\", or \\"husl\\". - `num_colors` (int): The number of colors in the palette (default is 8). - `as_cmap` (bool): If True, return a continuous colormap instead of a discrete list (default is False). # Output - Returns a seaborn color palette object. # Constraints - The function should handle invalid color inputs gracefully, raising an appropriate error message. - Ensure the function can switch between returning a discrete palette and a continuous colormap based on the `as_cmap` flag. # Example ```python palette = generate_custom_palette(\\"seagreen\\") print(palette) palette = generate_custom_palette(\\"#79C\\", input_type=\\"hex\\") print(palette) palette = generate_custom_palette((20, 60, 50), input_type=\\"husl\\") print(palette) palette = generate_custom_palette(\\"xkcd:copper\\", num_colors=8) print(palette) cmap = generate_custom_palette(\\"#a275ac\\", as_cmap=True) print(cmap) ``` The function would be assessed on its correctness, the ability to handle different types of color inputs, and the capability to return either discrete or continuous colormaps as needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_custom_palette(color, input_type=\\"name\\", num_colors=8, as_cmap=False): Generate a customized sequential color palette using seaborn light_palette. Parameters: - color: The base color for the palette. This can be specified as: - A named color (valid named colors according to seaborn syntax). - A hex color code (as a string starting with \'#\'). - A tuple of HUSL values if `input_type` is set to \\"husl\\". - input_type: Specifies the type of color input. This can be: - \\"name\\" (default) for named colors. - \\"hex\\" for hex color codes. - \\"husl\\" for HUSL values. - num_colors: An integer representing the number of color variations in the palette. - as_cmap: Boolean indicating if the return value should be a continuous colormap. Returns: - A seaborn color palette object, either as a discrete list or a continuous colormap. try: if input_type == \\"name\\": palette = sns.light_palette(color, n_colors=num_colors, input=\\"rgb\\", as_cmap=as_cmap) elif input_type == \\"hex\\": palette = sns.light_palette(color, n_colors=num_colors, input=\\"hex\\", as_cmap=as_cmap) elif input_type == \\"husl\\": palette = sns.light_palette(color, n_colors=num_colors, input=\\"husl\\", as_cmap=as_cmap) else: raise ValueError(\\"Invalid input_type specified. Choose \'name\', \'hex\', or \'husl\'.\\") return palette except ValueError as ve: raise ve except Exception as e: raise ValueError(f\\"Error generating palette: {e}\\") if __name__ == \\"__main__\\": palette = generate_custom_palette(\\"seagreen\\") print(palette) palette = generate_custom_palette(\\"#79c\\", input_type=\\"hex\\") print(palette) palette = generate_custom_palette((20, 60, 50), input_type=\\"husl\\") print(palette) palette = generate_custom_palette(\\"xkcd:copper\\", num_colors=8) print(palette) cmap = generate_custom_palette(\\"#a275ac\\", as_cmap=True) print(cmap)"},{"question":"<|Analysis Begin|> The provided documentation focuses on the use of the `histplot` function in seaborn to create histograms with various configurations. It includes instructions on plotting univariate and bivariate distributions, using options like bin width, number of bins, kernel density estimate, hue mapping, stacking, step functions, density normalization, and log scaling. The examples illustrate how to: 1. Load datasets (`penguins`, `tips`, and `planets`). 2. Create basic histograms with different variables. 3. Modify the appearance and behavior of the histograms (e.g., `element`, `fill`, `stat`, `log_scale`). The documentation does not cover all seaborn functionalities, but it provides a good understanding of creating and customizing histograms using seaborn\'s `histplot`. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Demonstrate your understanding of seaborn\'s histogram plotting capabilities by visualizing and comparing different distributions within a dataset. **Problem Statement:** Write a Python function using seaborn that takes a dataset and a list of configurations as input and generates a series of customized histograms. **Function Signature:** ```python def plot_custom_histograms(data: pd.DataFrame, configs: List[Dict[str, Any]]) -> None: ``` **Input:** - `data`: A pandas DataFrame containing the data to be plotted. - `configs`: A list of dictionaries, where each dictionary contains the configuration for a single histogram. Each dictionary can have the following keys: - `x` (str, optional): The variable on the x-axis. - `y` (str, optional): The variable on the y-axis. - `hue` (str, optional): The variable to map plot aspects to different colors. - `element` (str, optional): The histplot element type (\'bars\', \'step\', \'poly\'). - `binwidth` (int, optional): Width of the bins. - `bins` (int, optional): Number of bins. - `stat` (str, optional): Aggregate statistic to compute in each bin (\'count\', \'density\', \'probability\', \'percent\'). - `log_scale` (bool, optional): Whether to apply log scaling. - `fill` (bool, optional): Whether to fill the histogram bars. - `cumulative` (bool, optional): If True, compute and plot a cumulative histogram. - `multiple` (str, optional): Approach to plot multiple distributions (\'layer\', \'stack\', \'dodge\'). **Output:** - The function should not return any value. It should generate and display the histograms according to the provided configurations. **Constraints:** - Each dictionary in `configs` should correspond to one unique plot. The function should plot all the histograms sequentially. **Example:** ```python import seaborn as sns import pandas as pd from typing import List, Dict, Any def plot_custom_histograms(data: pd.DataFrame, configs: List[Dict[str, Any]]) -> None: for config in configs: sns.histplot(data=data, **config) plt.show() # Example Usage penguins = sns.load_dataset(\\"penguins\\") configs = [ {\\"x\\": \\"flipper_length_mm\\", \\"binwidth\\": 3, \\"kde\\": True}, {\\"x\\": \\"flipper_length_mm\\", \\"hue\\": \\"species\\", \\"multiple\\": \\"stack\\"}, {\\"x\\": \\"bill_length_mm\\", \\"hue\\": \\"island\\", \\"element\\": \\"step\\", \\"stat\\": \\"density\\", \\"common_norm\\": False}, ] plot_custom_histograms(penguins, configs) ``` **Explanation:** The `plot_custom_histograms` function iterates over each configuration dictionary in the `configs` list and plots a histogram using `sns.histplot`, passing the configuration as keyword arguments. The example usage demonstrates plotting three different histograms with varying configurations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt from typing import List, Dict, Any def plot_custom_histograms(data: pd.DataFrame, configs: List[Dict[str, Any]]) -> None: Generate and display customized histograms based on the provided configurations. Parameters: - data: pd.DataFrame - The data to be plotted. - configs: List[Dict[str, Any]] - A list of dictionaries with configuration settings for each histogram. for config in configs: sns.histplot(data=data, **config) plt.show()"},{"question":"**Python Advanced Sequence Protocol Assessment** Your task is to implement a class `CustomSequence` that mimics the behavior of standard Python sequences by internally using the provided sequence protocol functions. # Class Definition ```python class CustomSequence: def __init__(self, seq): # Initialize with a sequence (list, tuple, etc.) def __len__(self): # Return the size of the sequence def __getitem__(self, index): # Return the item at the given index or slice def __setitem__(self, index, value): # Set the item at the given index to value def __delitem__(self, index): # Delete the item at the given index def __contains__(self, value): # Check if value is in the sequence def count(self, value): # Count the occurrences of value in the sequence def index(self, value): # Return the first index of value in the sequence def __add__(self, other): # Return the concatenation of this sequence and another def __mul__(self, count): # Return the sequence repeated count times def to_list(self): # Convert the sequence to a list def to_tuple(self): # Convert the sequence to a tuple ``` # Constraints 1. You can assume that the input sequence will always be a valid list or tuple. 2. Your implementation should be efficient in terms of time complexity. # Input Format - The constructor `__init__` will take an initial sequence (a list or tuple). # Output Format - Methods should return appropriate Python sequence objects or values as per their functionality. # Example ```python # Example Usage seq = CustomSequence([1, 2, 3, 4, 5]) print(len(seq)) # Output: 5 print(seq[1]) # Output: 2 seq[1] = 10 print(seq[1]) # Output: 10 del seq[1] print(seq.to_list()) # Output: [1, 3, 4, 5] print(3 in seq) # Output: True print(seq.count(4)) # Output: 1 print(seq.index(4)) # Output: 2 print(seq + CustomSequence([6])) # Output: [1, 3, 4, 5, 6] print(seq * 2) # Output: [1, 3, 4, 5, 1, 3, 4, 5] print(seq.to_tuple()) # Output: (1, 3, 4, 5) ``` Implement the `CustomSequence` class such that it fulfills all the requirements specified above.","solution":"class CustomSequence: def __init__(self, seq): self._seq = list(seq) def __len__(self): return len(self._seq) def __getitem__(self, index): return self._seq[index] def __setitem__(self, index, value): self._seq[index] = value def __delitem__(self, index): del self._seq[index] def __contains__(self, value): return value in self._seq def count(self, value): return self._seq.count(value) def index(self, value): return self._seq.index(value) def __add__(self, other): return CustomSequence(self._seq + other._seq) def __mul__(self, count): return CustomSequence(self._seq * count) def to_list(self): return self._seq def to_tuple(self): return tuple(self._seq)"},{"question":"**Objective:** You are required to implement a customized logging system for a Python application that involves handling multiple loggers, setting appropriate handlers and formatters, and ensuring log messages are propagated and managed properly. **Problem Statement:** Your task is to write a Python function `setup_custom_logging()` which sets up the logging configuration as per the following specifications: 1. **Loggers and Handlers**: - Create a root logger and configure it to output messages to both the console and a file named `app.log`. - Additionally, create a specific logger named `\'app.network\'` that logs messages to a file named `network.log`. This logger should propagate messages to the root logger. 2. **Formatters**: - The root logger should use a format that outputs messages in the form: ``` [TIME] - [LOGGER NAME] - [LEVEL] - [MESSAGE] ``` Where: - `TIME` is the time the log was created. - `LOGGER NAME` is the name of the logger. - `LEVEL` is the severity level of the log (e.g., DEBUG, INFO). - `MESSAGE` is the log message. - The `\'app.network\'` logger should use a simpler format that just includes the message. 3. **Log Levels**: - Set the log level of the root logger to `DEBUG`. - Set the log level of the `\'app.network\'` logger to `ERROR`. 4. **Propagate Attribute**: - Ensure that the `\'app.network\'` logger propagates its messages to the root logger. 5. **Demonstrate the Logging System**: - Log messages with different levels (DEBUG, INFO, WARNING, ERROR, CRITICAL) using both the root logger and the `\'app.network\'` logger to demonstrate the complete functionality of the configured logging system. **Constraints and Requirements:** - Use Python 3.10. - Ensure all handlers and formatters are correctly configured and the root logger handles all log levels. - Ensure the \'`app.network`\' logger logs ERROR level messages to its own file and also propagates to the root logger. **Function Signature:** ```python def setup_custom_logging(): pass def demonstrate_logging(): pass # Call the demonstration function after setting up logging. setup_custom_logging() demonstrate_logging() ``` **Expected Output:** - The `app.log` file should contain all log messages from the root logger and propagated messages from `\'app.network\'` logger. - The `network.log` file should contain only the log messages of level ERROR and above from the `\'app.network\'` logger. - The console should print all messages handled by the root logger. Demonstrate your solution by calling these functions and showing that the logging works as specified.","solution":"import logging def setup_custom_logging(): # Root Logger root_logger = logging.getLogger() root_logger.setLevel(logging.DEBUG) # Console Handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # File Handler for root logger file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # Formatter for root logger root_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(root_formatter) file_handler.setFormatter(root_formatter) # Adding handlers to root logger root_logger.addHandler(console_handler) root_logger.addHandler(file_handler) # Logger \'app.network\' network_logger = logging.getLogger(\'app.network\') network_logger.setLevel(logging.ERROR) # File Handler for \'app.network\' logger network_file_handler = logging.FileHandler(\'network.log\') network_file_handler.setLevel(logging.ERROR) # Formatter for \'app.network\' logger network_formatter = logging.Formatter(\'%(message)s\') network_file_handler.setFormatter(network_formatter) # Adding handler to \'app.network\' logger network_logger.addHandler(network_file_handler) # Enabling propagation to root logger for \'app.network\' logger network_logger.propagate = True def demonstrate_logging(): # Root Logger with different levels root_logger = logging.getLogger() root_logger.debug(\\"Root logger debug message\\") root_logger.info(\\"Root logger info message\\") root_logger.warning(\\"Root logger warning message\\") root_logger.error(\\"Root logger error message\\") root_logger.critical(\\"Root logger critical message\\") # \'app.network\' Logger with different levels network_logger = logging.getLogger(\'app.network\') network_logger.debug(\\"Network logger debug message\\") network_logger.info(\\"Network logger info message\\") network_logger.warning(\\"Network logger warning message\\") network_logger.error(\\"Network logger error message\\") network_logger.critical(\\"Network logger critical message\\") # Set up logging and demonstrate setup_custom_logging() demonstrate_logging()"},{"question":"Coding Assessment Question # Objective Your task is to demonstrate your ability to work with datasets in scikit-learn using the `datasets` module. Specifically, you will load a dataset from OpenML, preprocess it, and prepare it for use in a scikit-learn pipeline. # Instructions 1. **Load the Dataset**: - Use the `fetch_openml` function to load the \\"miceprotein\\" dataset from OpenML. - Ensure the `as_frame` parameter is set to `True` when fetching the dataset. 2. **Preprocess the Data**: - Separate the dataset into features (X) and the target variable (y). - Identify and convert any categorical features in X to numerical format using `OneHotEncoder`. - Normalize the numerical features in X using `StandardScaler`. - Ensure the preprocessing steps are performed within a scikit-learn pipeline. 3. **Output**: - Print the first 5 rows of the preprocessed feature matrix. - Print the first 5 elements of the preprocessed target vector. # Requirements - The solution should use scikit-learn\'s preprocessing and pipeline modules. - Ensure that the pipeline handles both categorical and numerical features correctly. - Be mindful of the performance of your solution, ensuring it scales efficiently with larger datasets. # Constraints - You should not use any external libraries other than those explicitly mentioned in the question (i.e., stick to scikit-learn and pandas). - Ensure the code is well-documented with comments explaining each step. # Example Output ``` First 5 rows of the preprocessed feature matrix: [[ 0.123, -0.456, ..., 1.234], [ 0.567, -0.890, ..., -0.123], ..., [ 0.234, 0.456, ..., 0.678]] First 5 elements of the preprocessed target vector: [\'c-CS-m\', \'c-CS-s\', ..., \'t-SC-s\'] ``` # Starter Code ```python from sklearn.datasets import fetch_openml from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split # Step 1: Load the Dataset mice = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) data = mice.frame # Step 2: Separate features and target variable X = data.drop(columns=[\'class\']) y = data[\'class\'] # Step 3: Identify categorical feature columns categorical_features = X.select_dtypes(include=[\'object\']).columns numerical_features = X.select_dtypes(include=[\'float64\', \'int64\']).columns # Step 4: Create preprocessing pipelines for numerical and categorical data numerical_pipeline = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_pipeline = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer(transformers=[ (\'num\', numerical_pipeline, numerical_features), (\'cat\', categorical_pipeline, categorical_features) ]) # Step 5: Combine the preprocessing steps into a single pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) # Step 6: Fit and transform the data X_preprocessed = pipeline.fit_transform(X) # Output the first 5 rows and the target vector print(\\"First 5 rows of the preprocessed feature matrix:\\") print(X_preprocessed[:5]) print(\\"First 5 elements of the preprocessed target vector:\\") print(y[:5]) ```","solution":"from sklearn.datasets import fetch_openml from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer import pandas as pd def preprocess_miceprotein(): # Load the Dataset mice = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) data = mice.frame # Separate features and target variable X = data.drop(columns=[\'class\']) y = data[\'class\'] # Identify categorical feature columns categorical_features = X.select_dtypes(include=[\'object\']).columns numerical_features = X.select_dtypes(include=[\'float64\', \'int64\']).columns # Create preprocessing pipelines for numerical and categorical data numerical_pipeline = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_pipeline = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine into a ColumnTransformer preprocessor = ColumnTransformer(transformers=[ (\'num\', numerical_pipeline, numerical_features), (\'cat\', categorical_pipeline, categorical_features) ]) # Full preprocessing pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) # Fit and transform the data X_preprocessed = pipeline.fit_transform(X) y_preprocessed = y.values return X_preprocessed, y_preprocessed # Get first 5 rows of the preprocessed feature matrix and target vector X_preprocessed, y_preprocessed = preprocess_miceprotein() print(\\"First 5 rows of the preprocessed feature matrix:\\") print(X_preprocessed[:5]) print(\\"First 5 elements of the preprocessed target vector:\\") print(y_preprocessed[:5])"},{"question":"You are required to implement a function that connects to a given Telnet server, sends a series of commands, and retrieves the output. Your function should also handle potential errors and should be able to output debug information based on a specified debug level. # Function Signature ```python def execute_telnet_commands(host: str, port: int, commands: list, timeout: int = 10, debuglevel: int = 0) -> str: Connects to the given Telnet server, executes a list of commands, and returns the output. Parameters: host (str): The hostname or IP address of the Telnet server. port (int): The port of the Telnet server. commands (list): A list of command strings to execute on the Telnet server. timeout (int, optional): Timeout for the Telnet operations. Defaults to 10 seconds. debuglevel (int, optional): Debug level for Telnet operations. Higher values provide more debug output. Defaults to 0 (no debug information). Returns: str: The output received from the Telnet server after executing the commands. Raises: ConnectionError: If the function is unable to establish a connection to the server. TimeoutError: If the operations exceed the specified timeout. ``` # Constraints 1. You must use the `telnetlib` module to handle the Telnet connection. 2. Handle exceptions such as `EOFError`, `OSError`, and any other potential exceptions gracefully. 3. Ensure the function returns the complete output from the Telnet server as a single string. 4. Set the debugging level based on the `debuglevel` parameter. 5. Commands should be executed in the order provided in the `commands` list. 6. Use UTF-8 encoding for string conversions when sending and receiving data. # Example ```python # Example usage host = \\"localhost\\" port = 23 commands = [\\"login user\\", \\"password secret\\", \\"ls\\", \\"exit\\"] timeout = 15 debuglevel = 1 output = execute_telnet_commands(host, port, commands, timeout, debuglevel) print(output) ``` **Expected Output (will vary based on actual server response):** ``` login: Password: file1.txt file2.txt ``` # Notes - The function should establish the connection and execute each command sequentially. - Ensure proper handling of reading prompts before sending the next command. - Provide meaningful debug information if the `debuglevel` is set to a value greater than 0, using the `msg` method.","solution":"import telnetlib import time def execute_telnet_commands(host: str, port: int, commands: list, timeout: int = 10, debuglevel: int = 0) -> str: Connects to the given Telnet server, executes a list of commands, and returns the output. Parameters: host (str): The hostname or IP address of the Telnet server. port (int): The port of the Telnet server. commands (list): A list of command strings to execute on the Telnet server. timeout (int, optional): Timeout for the Telnet operations. Defaults to 10 seconds. debuglevel (int, optional): Debug level for Telnet operations. Higher values provide more debug output. Defaults to 0 (no debug information). Returns: str: The output received from the Telnet server after executing the commands. Raises: ConnectionError: If the function is unable to establish a connection to the server. TimeoutError: If the operations exceed the specified timeout. try: tn = telnetlib.Telnet(host, port, timeout) tn.set_debuglevel(debuglevel) output = \\"\\" for command in commands: tn.write(command.encode(\'utf-8\') + b\'n\') # Short delay to allow the command to be processed time.sleep(0.5) response = tn.read_very_eager().decode(\'utf-8\') output += response tn.close() return output except EOFError: raise ConnectionError(\\"Connection closed unexpectedly by the server.\\") except OSError as e: raise ConnectionError(f\\"OS error: {e}\\") except Exception as e: raise ConnectionError(f\\"Unexpected error: {e}\\")"},{"question":"Objective Create a function that simulates a simple Python interpreter which can handle complete programs, file inputs, and expression inputs. Problem Statement Write a Python function `simple_interpreter(input_type: str, content: str) -> any` that takes in two parameters: 1. `input_type` (a string): This indicates the type of input. It can take one of three values: \\"complete_program\\", \\"file_input\\", or \\"expression\\". 2. `content` (a string): The content to be interpreted. The function should return the output resulting from the execution or evaluation of the provided content based on the input type. Expected Behavior 1. **Complete Program**: Execute the content as a complete Python program. 2. **File Input**: Parse and execute the content as a Python file input. 3. **Expression**: Evaluate the content as a Python expression using the `eval()` function. Examples - Example 1: ```python content = \\"print(\'Hello, World!\')\\" print(simple_interpreter(\\"complete_program\\", content)) ``` Expected output: ``` Hello, World! ``` - Example 2: ```python content = \\"a = 5nb = 10nprint(a + b)\\" print(simple_interpreter(\\"file_input\\", content)) ``` Expected output: ``` 15 ``` - Example 3: ```python content = \\"2 + 2\\" result = simple_interpreter(\\"expression\\", content) print(result) ``` Expected output: ``` 4 ``` Requirements - Do not use any built-in functions (except eval for expressions) that directly solve the given problem. - Handle exceptions where the provided content cannot be executed/evaluated. - Clearly structure your function to differentiate between each input type and handle them accordingly. Constraints - Assume the input content is always syntactically correct Python code. - The `input_type` will always be one of the three specified types. Now, implement your function accordingly.","solution":"def simple_interpreter(input_type: str, content: str): Interpret the given content based on the input type. Parameters: - input_type (str): The type of the input, can be \\"complete_program\\", \\"file_input\\", or \\"expression\\". - content (str): The content to be interpreted. Returns: - The result of executing or evaluating the content. try: if input_type == \\"complete_program\\": exec(content) elif input_type == \\"file_input\\": exec(content) elif input_type == \\"expression\\": return eval(content) except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"**Question: Advanced Web Crawler Compliance Checker** As a software engineer at a web scraping company, you are tasked with ensuring that your web crawlers are compliant with the rules specified in websites\' `robots.txt` files. You have to implement a function `check_website_compliance` that verifies if your crawler can access specific URLs on given websites, respecting crawl delays and request rates defined in `robots.txt`. Implement a function `check_website_compliance` that takes the following three parameters: 1. `robots_txt_url` (str): The URL of the `robots.txt` file for a website. 2. `user_agent` (str): The user agent string of your web crawler (e.g., \\"*\\", \\"Googlebot\\", etc.). 3. `urls_to_check` (list of str): A list of URLs on the website that you need to check for access permissions. Your function should return a dictionary with the structure: ```python { \\"can_fetch\\": {url1: bool, url2: bool, ...}, \\"crawl_delay\\": int or None, \\"request_rate\\": (int, int) or None, \\"sitemaps\\": [str, str, ...] or None, } ``` where: - `\\"can_fetch\\"` maps each URL to a boolean indicating whether the `user_agent` is permitted to fetch that URL. - `\\"crawl_delay\\"` is the crawl delay time in seconds if specified; otherwise, `None`. - `\\"request_rate\\"` is a tuple `(requests, seconds)` indicating the request rate for the `user_agent`, or `None` if not specified. - `\\"sitemaps\\"` is a list of sitemap URLs specified in the `robots.txt` file, or `None` if not specified. # Constraints - The `robots_txt_url` and all URLs in `urls_to_check` are guaranteed to be valid URLs. - The `urls_to_check` list will contain between 1 and 100 URLs. - Network access is available and reliable for fetching the `robots.txt` file. # Example ```python # Example Function Call result = check_website_compliance( \\"http://www.example.com/robots.txt\\", \\"MyUserAgent\\", [\\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\"] ) # Example Output { \\"can_fetch\\": { \\"http://www.example.com/page1\\": True, \\"http://www.example.com/page2\\": False, }, \\"crawl_delay\\": 10, \\"request_rate\\": (5, 60), \\"sitemaps\\": [\\"http://www.example.com/sitemap1.xml\\", \\"http://www.example.com/sitemap2.xml\\"] } ``` Consider utilizing the `RobotFileParser` from the `urllib.robotparser` module to implement this function. The implementation must accurately interpret and apply the rules specified in the `robots.txt` file.","solution":"import urllib.robotparser import urllib.request def check_website_compliance(robots_txt_url, user_agent, urls_to_check): rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_txt_url) rp.read() can_fetch = {url: rp.can_fetch(user_agent, url) for url in urls_to_check} crawl_delay = rp.crawl_delay(user_agent) request_rate = rp.request_rate(user_agent) sitemaps = rp.site_maps() return { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate, \\"sitemaps\\": sitemaps, }"},{"question":"**Question: Customizing Seaborn Plot Contexts** In this exercise, you will demonstrate your understanding of plot contexts and customization in Seaborn. Your task is to write a function `customize_plot` that takes the following inputs: 1. `context` (string): The context setting for the plot. Valid options are `paper`, `notebook`, `talk`, and `poster`. 2. `font_scale` (float): A multiplier for scaling the font size. If not provided, it should default to 1. 3. `linewidth` (float): The desired line width for any line plot in the context. If not provided, it should default to 2. The function should: 1. Set the plotting context using the provided context and scale the fonts. 2. Override the default line width parameter with the provided value. 3. Plot a sample line plot with the x values `[0, 1, 2, 3]` and y values `[3, 1, 4, 1]` to visualize these customizations. **Input:** - `context` (string): Context name (`\\"paper\\"`, `\\"notebook\\"`, `\\"talk\\"`, `\\"poster\\"`). - `font_scale` (float, optional): Font scale multiplier. Default is 1. - `linewidth` (float, optional): Line width. Default is 2. **Output:** - A line plot with the given customizations applied. **Example:** ```python customize_plot(\\"talk\\", font_scale=1.5, linewidth=3) ``` This should produce a plot using the \\"talk\\" context, with font sizes scaled by 1.5 and line width set to 3. **Constraints:** - Ensure to handle cases where `font_scale` and `linewidth` are not provided. - The plot should immediately visualize upon function execution in a Jupyter Notebook environment. **Hints:** - Use `sns.set_context()` to set the context and customize using the `rc` parameter for line width. - Use `sns.lineplot()` to create the sample plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot(context, font_scale=1, linewidth=2): Customizes and displays a Seaborn line plot based on the provided context, font scale, and line width. Parameters: context (str): Context name (\'paper\', \'notebook\', \'talk\', \'poster\'). font_scale (float, optional): Font scale multiplier. Default is 1. linewidth (float, optional): Line width. Default is 2. # Set the Seaborn context with given parameters sns.set_context(context, font_scale=font_scale, rc={\\"lines.linewidth\\": linewidth}) # Sample data for plotting x = [0, 1, 2, 3] y = [3, 1, 4, 1] # Create a line plot plt.figure() sns.lineplot(x=x, y=y) plt.show()"},{"question":"**Objective:** Design a function to modify a PyTorch neural network such that all instances of BatchNorm are replaced with GroupNorm in order to enable compatibility with `functorch`. **Function Signature:** ```python import torch.nn as nn def replace_batch_norm_with_group_norm(model: nn.Module, num_groups: int) -> nn.Module: This function takes a PyTorch model and replaces all instances of BatchNorm2d with GroupNorm. Parameters: - model (nn.Module): The input neural network model which may contain BatchNorm layers. - num_groups (int): The number of groups to use in the GroupNorm layers. Returns: - nn.Module: The modified model with GroupNorm layers in place of BatchNorm2d layers. pass ``` **Detailed Description:** 1. **Input:** - `model` is a PyTorch neural network model and may contain several instances of `BatchNorm2d`. - `num_groups` is the integer number of groups to use for GroupNorm. Ensure `num_channels` (from BatchNorm) is divisible by `num_groups`. 2. **Output:** - Return a modified model where all `BatchNorm2d` layers are replaced by `GroupNorm` layers. Other components of the model should remain unchanged. 3. **Constraints:** - The solution must handle nested PyTorch modules, meaning `BatchNorm2d` instances can be found at any depth within the input model. 4. **Example:** ```python class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv = nn.Conv2d(3, 16, 3, 1) self.bn = nn.BatchNorm2d(16) self.relu = nn.ReLU() self.fc = nn.Linear(16 * 26 * 26, 10) def forward(self, x): x = self.conv(x) x = self.bn(x) x = self.relu(x) x = x.view(-1, 16 * 26 * 26) x = self.fc(x) return x model = SimpleCNN() new_model = replace_batch_norm_with_group_norm(model, num_groups=4) print(new_model) ``` 5. **Implementation Notes:** - You may find it useful to recursively traverse the model\'s children modules. - Use `nn.GroupNorm` to replace `nn.BatchNorm2d`, ensuring the correct number of groups are utilized. - Ensure that no other parts of the model, such as layers that are not BatchNorm, are altered in the process. This question tests the student\'s understanding of PyTorch\'s neural network modules, their ability to manipulate and replace modules within a model, and their comprehension of the differences between Batch Normalization and Group Normalization layers.","solution":"import torch.nn as nn def replace_batch_norm_with_group_norm(model: nn.Module, num_groups: int) -> nn.Module: This function takes a PyTorch model and replaces all instances of BatchNorm2d with GroupNorm. Parameters: - model (nn.Module): The input neural network model which may contain BatchNorm layers. - num_groups (int): The number of groups to use in the GroupNorm layers. Returns: - nn.Module: The modified model with GroupNorm layers in place of BatchNorm2d layers. def replace(module): for name, child in module.named_children(): if isinstance(child, nn.BatchNorm2d): num_channels = child.num_features new_child = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels) setattr(module, name, new_child) else: replace(child) replace(model) return model"},{"question":"Question: Implementing Robust Asynchronous File Downloader with Exception Handling # Description You are tasked with implementing a robust asynchronous file downloader function in Python. This function should download a file from a given URL and save it to a specified local file path. During the download process, several error conditions should be handled gracefully using the asyncio exceptions described below. # Requirements 1. Implement a function `async def download_file(url: str, filepath: str) -> None` that: - Takes two parameters: - `url` (str): The URL of the file to download. - `filepath` (str): The local path where the downloaded file should be saved. - Downloads the file from the given URL and saves it to the specified filepath. - Handles the following exceptions from the `asyncio` package: - `asyncio.TimeoutError`: Raised if the download exceeds a specified time limit (e.g., 60 seconds). - `asyncio.CancelledError`: Raised if the download operation is cancelled. Ensure any custom cleanup actions are performed and then re-raise the exception. - `asyncio.InvalidStateError`: Raised if there is an invalid state in the asyncio Task or Future. Log an appropriate error message. - `asyncio.IncompleteReadError`: Raised if the read operation does not complete fully. Log the partial content downloaded. - `asyncio.LimitOverrunError`: Raised if a buffer size limit is reached while reading. Log the number of consumed bytes and handle the situation appropriately. 2. Ensure your function is well-tested with different scenarios, including normal operation as well as the aforementioned exceptions. 3. You may use any other standard libraries for logging, HTTP requests, and file operations as needed. # Example Usage ```python import asyncio async def main(): try: await download_file(\\"https://example.com/file.zip\\", \\"path/to/save/file.zip\\") print(\\"Download complete\\") except asyncio.TimeoutError: print(\\"Download timed out\\") except asyncio.CancelledError: print(\\"Download was cancelled\\") except asyncio.InvalidStateError as e: print(f\\"Invalid state: {e}\\") except asyncio.IncompleteReadError as e: print(f\\"Incomplete read: {e.partial}\\") except asyncio.LimitOverrunError as e: print(f\\"Buffer limit overrun: {e.consumed}\\") # Running the main function asyncio.run(main()) ``` # Constraints - You are free to use `aiohttp` or any other asyncio-compatible HTTP client for downloading the file. - Ensure your solution is documented with comments and meaningful logging messages to make debugging easier. # Evaluation Criteria - Correctness: The function should perform the download correctly and handle all specified exceptions appropriately. - Robustness: The function should be able to handle and log various errors without crashing. - Code Quality: The code should be well-organized, readable, and documented. Good luck and happy coding!","solution":"import aiohttp import asyncio import logging logging.basicConfig(level=logging.INFO) async def download_file(url: str, filepath: str) -> None: try: timeout = aiohttp.ClientTimeout(total=60) # Setting a timeout of 60 seconds async with aiohttp.ClientSession(timeout=timeout) as session: async with session.get(url) as response: response.raise_for_status() # Raise an error for bad status with open(filepath, \'wb\') as f: async for chunk in response.content.iter_chunked(1024): f.write(chunk) except asyncio.TimeoutError: logging.error(\\"Download timed out.\\") raise except asyncio.CancelledError: logging.error(\\"Download was cancelled.\\") raise except aiohttp.ClientResponseError as e: logging.error(f\\"HTTP error encountered: {e.status} - {e.message}\\") except aiohttp.ClientError as e: logging.error(f\\"Client error encountered: {str(e)}\\") except Exception as e: logging.error(f\\"Unexpected error: {str(e)}\\")"},{"question":"# Advanced Coding Assessment: Implement a Concurrent File Processor **Objective:** To assess the student\'s understanding and ability to utilize Python\'s standard library to perform concurrent file processing, handle text data, and implement efficient data structures. **Problem Statement:** You are tasked with implementing a concurrent file processor that reads data from multiple text files, processes the text to extract meaningful information, and combines the results in an efficient manner. **Requirements:** 1. **Input:** - A list of file paths (strings) where each file contains text data. 2. **Output:** - A dictionary where keys are words (strings) and values are their cumulative frequency (integers) across all files. 3. **Constraints:** - The solution must be implemented using concurrent execution to optimize the processing time. - Standard library modules such as `threading`, `collections`, and `os` should be used. - The solution must handle large files efficiently without exhausting the system\'s memory. **Performance Requirements:** - Efficient utilization of system resources with minimal waiting time. - Proper handling of exceptions and errors during file reading and processing. **Example:** Suppose you have the following files: `file1.txt`: ``` hello world hello python ``` `file2.txt`: ``` python programming hello world ``` **Function Signature:** ```python import os import threading import collections def concurrent_file_processor(file_paths: list) -> dict: # Implement your solution here pass ``` **Expected Output:** ```python { \\"hello\\": 3, \\"world\\": 2, \\"python\\": 2, \\"programming\\": 1 } ``` **Instructions:** 1. Implement the `concurrent_file_processor` function as described. 2. Ensure that the file reading and processing are handled concurrently. 3. Use appropriate data structures to store and merge the results from different threads. 4. Include error handling to manage file I/O operations and threading issues. 5. Provide comments and documentation for your code to explain the logic and flow. **Additional Notes:** - You may assume that the file paths provided are valid and the files are accessible within the system. - Consider edge cases such as empty files and duplicate file entries in the input list. This question tests the student\'s ability to work with concurrency, file I/O, and efficient data structures within the Python standard library, while also requiring careful planning and error handling for robust implementation.","solution":"import os import threading import collections def process_file(file_path, word_counter, lock): try: with open(file_path, \'r\') as f: for line in f: words = line.split() with lock: for word in words: word_counter[word] += 1 except Exception as e: print(f\\"Error processing file {file_path}: {e}\\") def concurrent_file_processor(file_paths: list) -> dict: word_counter = collections.Counter() threads = [] lock = threading.Lock() for file_path in file_paths: thread = threading.Thread(target=process_file, args=(file_path, word_counter, lock)) threads.append(thread) thread.start() for thread in threads: thread.join() return dict(word_counter)"},{"question":"**Coding Assessment Question** # Custom Pretty-Print Function using `pprint` Objective: Create a custom pretty-print function that formats complex nested data structures using specific formatting parameters. This function will demonstrate your understanding of the `pprint` module\'s features. Function Signature: ```python def custom_pretty_print(data, indent=2, width=100, depth=5, compact=True, sort_dicts=False, underscore_numbers=True): pass ``` Instructions: 1. **Function Parameters**: - `data`: The data structure (e.g., list, dict, tuple) to be pretty-printed. - `indent`: The amount of indentation to use for each nested level (default: 2). - `width`: The maximum width of each output line (default: 100). - `depth`: The maximum depth to which the nesting should be displayed (default: 5). - `compact`: If `True`, tries to fit as many items in a single line as possible (default: `True`). - `sort_dicts`: If `True`, sorts dictionary keys; if `False`, keeps insertion order (default: `False`). - `underscore_numbers`: If `True`, uses underscores to represent thousands in numbers (default: `True`). 2. **Function Output**: - Prints the formatted representation of the `data` on `sys.stdout`. 3. **Constraints**: - The function should handle large and deeply nested data structures. - Ensure the function is efficient even with significant amounts of data. 4. **Examples**: ```python data = { \'name\': \'John Doe\', \'age\': 34, \'address\': { \'street\': \'123 Elm St.\', \'city\': \'Springfield\', \'postal_code\': 12345 }, \'hobbies\': [\'reading\', \'hiking\', \'coding\'] } custom_pretty_print(data) ``` Output: ``` { \'name\': \'John Doe\', \'age\': 34, \'address\': { \'street\': \'123 Elm St.\', \'city\': \'Springfield\', \'postal_code\': 12_345 }, \'hobbies\': [\'reading\', \'hiking\', \'coding\'] } ``` Notes: 1. Use the `pprint` module to aid in formatting the `data`. 2. Ensure your function is well-documented with appropriate comments. 3. Include error handling to manage unexpected data types or values. Good luck!","solution":"import pprint def custom_pretty_print(data, indent=2, width=100, depth=5, compact=True, sort_dicts=False, underscore_numbers=True): Pretty-print a data structure with specific formatting parameters using the pprint module. :param data: The data structure to be pretty-printed. :param indent: The amount of indentation for each nested level (default: 2). :param width: The maximum width of each output line (default: 100). :param depth: The maximum depth to which the nesting should be displayed (default: 5). :param compact: If True, fit as many items in a single line as possible (default: True). :param sort_dicts: If True, sort dictionary keys; if False, keep insertion order (default: False). :param underscore_numbers: If True, use underscores to represent thousands in numbers (default: True). printer = pprint.PrettyPrinter( indent=indent, width=width, depth=depth, compact=compact, sort_dicts=sort_dicts, underscore_numbers=underscore_numbers ) printer.pprint(data)"},{"question":"**Coding Assessment Question: Advanced Seaborn Plotting with JointGrid** # Objective Your task is to demonstrate your understanding of the seaborn `JointGrid` class by creating a multi-faceted visualization. You will load a provided dataset, generate a joint plot with marginal histograms, and apply various customizations as specified. # Dataset For this task, you will use the \\"iris\\" dataset which can be loaded directly from seaborn. # Instructions 1. **Load Dataset** - Load the \\"iris\\" dataset using `sns.load_dataset(\\"iris\\")`. 2. **Setup JointGrid** - Initialize a `sns.JointGrid` object with: - `data` as the loaded iris dataset. - `x` as \\"sepal_length\\". - `y` as \\"sepal_width\\". - `hue` as \\"species\\". 3. **Plot Data** - Generate the joint plot using `sns.scatterplot` for the joint axes and `sns.histplot` for the marginal axes. 4. **Customizations** - **Joint Plot**: Customize the `sns.scatterplot` with: - Size `s=50`. - Alpha transparency `alpha=0.6`. - **Marginal Histograms**: Customize `sns.histplot` with: - Set `kde=True` to display Kernel Density Estimate on the histograms. - Set `bins=20` for number of bins in histogram. - Add horizontal and vertical reference lines on the joint plot at the median values of \\"sepal_length\\" and \\"sepal_width\\" respectively. - Customize the grid: - Set `height=8`. - Set `ratio=2`. - Set `space=0.1`. 5. **Titles and Labels** - Add appropriate titles and labels for all axes. # Expected Function Definition ```python def create_iris_jointplot(): import seaborn as sns import matplotlib.pyplot as plt # Load dataset iris = sns.load_dataset(\\"iris\\") # Initialize JointGrid g = sns.JointGrid(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", height=8, ratio=2, space=0.1) # Plot data g.plot_joint(sns.scatterplot, s=50, alpha=0.6) g.plot_marginals(sns.histplot, kde=True, bins=20) # Add reference lines g.refline(x=iris[\\"sepal_length\\"].median(), y=iris[\\"sepal_width\\"].median()) # Titles and Labels g.ax_joint.set_title(\'Sepal Length vs. Sepal Width with Marginal Histograms\') g.ax_joint.set_xlabel(\'Sepal Length (cm)\') g.ax_joint.set_ylabel(\'Sepal Width (cm)\') plt.show() # Call the function to verify the plot create_iris_jointplot() ``` # Constraints and Limitations - Ensure the function `create_iris_jointplot` runs without errors and generates the plot as specified. - Use only seaborn and matplotlib for visualizations. - The plot must exhibit the customizations exactly as described. # Performance Requirements - The function should complete plotting within a reasonable time frame (within a few seconds).","solution":"def create_iris_jointplot(): import seaborn as sns import matplotlib.pyplot as plt # Load dataset iris = sns.load_dataset(\\"iris\\") # Initialize JointGrid g = sns.JointGrid(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", height=8, ratio=2, space=0.1) # Plot data for joint axes g.plot_joint(sns.scatterplot, s=50, alpha=0.6) # Plot data for marginal axes g.plot_marginals(sns.histplot, kde=True, bins=20) # Add reference lines g.refline(x=iris[\\"sepal_length\\"].median(), y=iris[\\"sepal_width\\"].median()) # Titles and Labels g.ax_joint.set_title(\'Sepal Length vs. Sepal Width with Marginal Histograms\') g.ax_joint.set_xlabel(\'Sepal Length (cm)\') g.ax_joint.set_ylabel(\'Sepal Width (cm)\') # Show the plot plt.show()"},{"question":"Coding Assessment Question # Objective: To evaluate the candidate\'s understanding and proficiency in using Python\'s `datetime` module for handling date and time objects, time zone conversions, and formatting/parsing dates and times. # Problem Statement: You are responsible for creating a Python function that: 1. Creates a `datetime` object from given date and time in the local timezone. 2. Converts the `datetime` object to UTC. 3. Returns the string representation of the UTC `datetime` in ISO 8601 format, including timezone information. # Function Signature: ```python from datetime import datetime, timezone, timedelta def local_to_utc_isoformat(year: int, month: int, day: int, hour: int, minute: int, second: int) -> str: pass ``` # Input: - `year` (int): Year in the range 1 to 9999. - `month` (int): Month in the range 1 to 12. - `day` (int): Day in the range 1 to 31. - `hour` (int): Hour in the range 0 to 23. - `minute` (int): Minute in the range 0 to 59. - `second` (int): Second in the range 0 to 59. # Output: - A string representing the input date and time converted to UTC in ISO 8601 format with timezone information (`+00:00`). # Constraints: - The input values will always form a valid date and time. - Ensure proper handling of the timezone conversion. # Examples: ```python # Example 1 result = local_to_utc_isoformat(2023, 10, 5, 15, 30, 0) # Expected output: \'2023-10-05T15:30:00+00:00\' (UTC conversion assuming input is in local timezone) # Example 2 result = local_to_utc_isoformat(2025, 12, 31, 23, 59, 59) # Expected output: \'2025-12-31T23:59:59+00:00\' (UTC conversion assuming input is in local timezone) ``` # Notes: - Use the `datetime` and `timezone` classes to handle date and time creation and conversion. - The ISO 8601 format includes the date, time, and timezone information, which can be easily obtained using the respective method of the `datetime` class. # Evaluation Criteria: - Correctness: The function should correctly convert local `datetime` to UTC and return the ISO 8601 formatted string. - Robustness: Handle edge cases and ensure the function works within the constraints provided. - Clarity: Ensure the code is well-documented and readable.","solution":"from datetime import datetime, timezone def local_to_utc_isoformat(year: int, month: int, day: int, hour: int, minute: int, second: int) -> str: Converts local datetime to UTC and returns it in ISO 8601 format. Parameters: - year (int): Year in the range 1 to 9999. - month (int): Month in the range 1 to 12. - day (int): Day in the range 1 to 31. - hour (int): Hour in the range 0 to 23. - minute (int): Minute in the range 0 to 59. - second (int): Second in the range 0 to 59. Returns: - str: The corresponding UTC datetime in ISO 8601 format with timezone information. # Create a datetime object from the provided local date and time local_dt = datetime(year, month, day, hour, minute, second) # Convert the local datetime to UTC utc_dt = local_dt.astimezone(timezone.utc) # Return the ISO 8601 formatted string of the UTC datetime return utc_dt.isoformat()"},{"question":"# Advanced Exception Handling and Resource Management in Python **Problem Statement:** You are required to implement a Python function that reads from a file and processes its contents to value pairs. The function should handle various errors gracefully and ensure resources are released correctly. Specifically, you need to handle file operation errors, data type conversion errors, and custom-defined exceptions. Also, use appropriate cleanup actions to manage resources. # Function Signature ```python def process_file(filename: str) -> list: Processes the given file and returns a list of tuples with values. Args: - filename (str): The name of the file to be processed. Returns: - list: A list of tuples, where each tuple contains two integers. ``` # Requirements 1. The file contains pairs of integers on each line separated by spaces. Each line is expected to be in the format: `<int> <int>`. 2. If the file does not exist, raise an `OSError` with the message `\\"File not found\\"`. 3. If a line in the file cannot be converted to two integers, skip that line and continue processing the next. 4. Implement a custom exception called `ProcessError` that should be raised if there is an error in processing any line content beyond type conversion issues. 5. Utilize the `with` statement to ensure the file is closed properly after processing. 6. Use the `finally` clause to print a message `\\"Processing complete\\"` once processing is done, regardless of success or failure. 7. Return a list of tuples where each tuple contains the pair of integers from the respective valid lines in the file. # Constraints - Do not use any external libraries except Python\'s built-in libraries. - Each line in the file should contain exactly two space-separated integers for valid processing. **Example:** Consider the content of the file `testfile.txt` is: ``` 1 2 3 4 five 6 7 8 ``` Calling `process_file(\'testfile.txt\')` should return: ```python [(1, 2), (3, 4), (7, 8)] ``` # Notes: - Handle possible errors as mentioned above. - Ensure the function has no memory leaks, and the file is always closed after the processing. - Do not use `sys.exit()` or similar methods that would terminate the program. ```python # Your implementation here ```","solution":"class ProcessError(Exception): Custom exception for errors in processing file content. pass def process_file(filename: str) -> list: Processes the given file and returns a list of tuples with values. Args: - filename (str): The name of the file to be processed. Returns: - list: A list of tuples, where each tuple contains two integers. result = [] try: with open(filename, \'r\') as file: for line in file: parts = line.strip().split() if len(parts) != 2: raise ProcessError(f\\"Line \'{line.strip()}\' does not have exactly two parts\\") try: num1 = int(parts[0]) num2 = int(parts[1]) result.append((num1, num2)) except ValueError: # Skip lines that cannot be converted to integers continue except FileNotFoundError: raise OSError(\\"File not found\\") finally: print(\\"Processing complete\\") return result"},{"question":"Coding Assessment Question # Task You are given a task to collect and report metadata information about a specific Python package installed in your environment using the `importlib.metadata` package. Your code should perform the following steps: 1. Retrieve the version of the given package. 2. List all entry points of the package. 3. Extract and display metadata fields (e.g., Name, Version, Summary, Author, Requires-Python) for the package. 4. Identify and list all files included in the distribution of the package. 5. Identify and list distribution requirements of the package. # Input - A string representing the name of the package (e.g., `\'wheel\'`). # Output - Print the version of the package. - Print all entry points grouped by their categories. - Print the metadata fields including Name, Version, Summary, Author, and Requires-Python. - Print all files included in the distribution with their size and hash values. - Print all distribution requirements. # Constraints - The package name provided as input must be a valid package installed in your environment. - The function should handle cases where no files or requirements are found gracefully. # Performance Requirements Your implementation should efficiently handle querying and processing the metadata with minimal overhead. Ensure that redundant operations are avoided. # Example ```python from importlib.metadata import version, entry_points, metadata, files, requires def package_information(package_name): try: # Retrieve the version of the package package_version = version(package_name) print(f\\"Version: {package_version}\\") # List all entry points of the package eps = entry_points() all_entry_points = eps.select() # Query all entry points grouped_entry_points = {} for ep in all_entry_points: grouped_entry_points.setdefault(ep.group, []).append(ep) print(\\"nEntry Points:\\") for group, entry_point_list in grouped_entry_points.items(): print(f\\"nGroup: {group}\\") for ep in entry_point_list: print(f\\" Name: {ep.name}, Value: {ep.value}\\") # Extract metadata information package_metadata = metadata(package_name) metadata_fields = [\'Name\', \'Version\', \'Summary\', \'Author\', \'Requires-Python\'] print(\\"nMetadata:\\") for field in metadata_fields: print(f\\"{field}: {package_metadata.get(field, \'N/A\')}\\") # List all files included in the distribution package_files = files(package_name) if package_files: print(\\"nFiles:\\") for file in package_files: print(f\\" File: {file}, Size: {file.size}, Hash: {file.hash}\\") else: print(\\"No files found.\\") # Identify and list requirements package_requirements = requires(package_name) if package_requirements: print(\\"nRequirements:\\") for req in package_requirements: print(f\\" {req}\\") else: print(\\"No requirements found.\\") except Exception as e: print(f\\"Error: {e}\\") # Example usage package_information(\'wheel\') ``` # Additional Notes - You should make sure that your solution is dynamic enough to handle different packages and variations in their metadata. - Error handling should be robust to address possible issues such as package not found or missing metadata.","solution":"from importlib.metadata import version, entry_points, metadata, files, requires def package_information(package_name): try: # Retrieve the version of the package package_version = version(package_name) print(f\\"Version: {package_version}\\") # List all entry points of the package eps = entry_points() grouped_entry_points = {} for ep in eps.select(group=None, name=None): grouped_entry_points.setdefault(ep.group, []).append(ep) print(\\"nEntry Points:\\") for group, entry_point_list in grouped_entry_points.items(): print(f\\"nGroup: {group}\\") for ep in entry_point_list: print(f\\" Name: {ep.name}, Value: {ep.value}\\") # Extract metadata information package_metadata = metadata(package_name) metadata_fields = [\'Name\', \'Version\', \'Summary\', \'Author\', \'Requires-Python\'] print(\\"nMetadata:\\") for field in metadata_fields: print(f\\"{field}: {package_metadata.get(field, \'N/A\')}\\") # List all files included in the distribution package_files = files(package_name) if package_files: print(\\"nFiles:\\") for file in package_files: print(f\\" File: {file}, Size: {file.size}, Hash: {file.hash}\\") else: print(\\"No files found.\\") # Identify and list requirements package_requirements = requires(package_name) if package_requirements: print(\\"nRequirements:\\") for req in package_requirements: print(f\\" {req}\\") else: print(\\"No requirements found.\\") except Exception as e: print(f\\"Error: {e}\\")"},{"question":"# Advanced Python Iterators We are going to implement custom iterator classes that mimic the behavior of `PySeqIter_Type` and `PyCallIter_Type` described in the documentation. Part 1: Sequence Iterator Write a class `SequenceIterator` that works with an arbitrary sequence supporting the `__getitem__()` method. The iteration should stop when the sequence raises an `IndexError`. **Class Definition:** ```python class SequenceIterator: def __init__(self, sequence): Initializes the iterator with the sequence. Args: sequence (list/tuple/str): A sequence supporting __getitem__() method. pass def __iter__(self): Returns the iterator object itself. Returns: SequenceIterator: The iterator object itself. pass def __next__(self): Returns the next item in the sequence. Returns: object: The next item in the sequence. Raises: StopIteration: When the sequence raises an IndexError. pass ``` Part 2: Callable Iterator Write a class `CallableIterator` that works with a callable object and a sentinel value. It should call the callable for each item in the sequence and end the iteration when the callable returns a value equal to the sentinel. **Class Definition:** ```python class CallableIterator: def __init__(self, callable_obj, sentinel): Initializes the iterator with a callable object and a sentinel value. Args: callable_obj (callable): A callable object that can be called with no parameters. sentinel (object): The sentinel value to stop the iteration. pass def __iter__(self): Returns the iterator object itself. Returns: CallableIterator: The iterator object itself. pass def __next__(self): Returns the next item produced by the callable object. Returns: object: The next item produced by the callable object. Raises: StopIteration: When the callable object returns the sentinel value. pass ``` Constraints 1. The `__getitem__()` method for the sequence in `SequenceIterator` should be appropriately handled to accommodate various iterable types (e.g., lists, tuples, strings). 2. The callable object in `CallableIterator` should accept no parameters and return a value that is checkable against the sentinel to know when to stop. Example Usage: ```python # Sequence Iterator Example sequence_iter = SequenceIterator([1, 2, 3]) for val in sequence_iter: print(val) # Output should be: # 1 # 2 # 3 # Callable Iterator Example def counter(): current = 0 while True: yield current current += 1 callable_iter = CallableIterator(counter().__next__, 3) for val in callable_iter: print(val) # Output should be: # 0 # 1 # 2 ``` Implement the classes `SequenceIterator` and `CallableIterator` to achieve the desired behavior as described above.","solution":"class SequenceIterator: def __init__(self, sequence): Initializes the iterator with the sequence. Args: sequence (list/tuple/str): A sequence supporting __getitem__() method. self.sequence = sequence self.index = 0 def __iter__(self): Returns the iterator object itself. Returns: SequenceIterator: The iterator object itself. return self def __next__(self): Returns the next item in the sequence. Returns: object: The next item in the sequence. Raises: StopIteration: When the sequence raises an IndexError. try: result = self.sequence[self.index] self.index += 1 return result except IndexError: raise StopIteration class CallableIterator: def __init__(self, callable_obj, sentinel): Initializes the iterator with a callable object and a sentinel value. Args: callable_obj (callable): A callable object that can be called with no parameters. sentinel (object): The sentinel value to stop the iteration. self.callable_obj = callable_obj self.sentinel = sentinel def __iter__(self): Returns the iterator object itself. Returns: CallableIterator: The iterator object itself. return self def __next__(self): Returns the next item produced by the callable object. Returns: object: The next item produced by the callable object. Raises: StopIteration: When the callable object returns the sentinel value. result = self.callable_obj() if result == self.sentinel: raise StopIteration return result"},{"question":"**Coding Assessment Question:** # Objective Demonstrate your understanding of the `chunk` module in Python, particularly in reading and processing chunks from a file. # Problem Statement You are given a binary file that contains multiple chunks in the EA IFF 85 format. Each chunk has a 4-byte ID, a 4-byte size (big-endian), and data of the specified size. You are required to implement a function `read_chunks(file_path)` that reads and processes each chunk in the file. For each chunk, your function should: 1. Print the chunk ID and chunk size. 2. Read the entire data of the chunk into a bytes object. 3. If the chunk ID is \\"DATA\\", write its data to a new file named \\"output_data.chunk\\". # Function Signature ```python def read_chunks(file_path: str) -> None: ``` - `file_path`: A string representing the path to the binary file to be read. # Constraints - The file will contain valid EA IFF 85 formatted chunks. - You do not need to handle the case where the chunk ID is not 4 bytes. # Example Assume you have a binary file `example.iff` with the following content (in hexadecimal): ``` \'434841524c0000000948656c6c6f20576f726c64214441544100000004deadbeef\' ``` This corresponds to: ``` Chunk 1: ID: \\"CHAR\\" Size: 9 Data: b\'Hello World\' Chunk 2: ID: \\"DATA\\" Size: 4 Data: b\'xdexadxbexef\' ``` The expected output is: ``` Reading chunks from file: example.iff Chunk ID: CHAR | Size: 9 Chunk ID: DATA | Size: 4 ``` And it will create a file `output_data.chunk` containing the bytes `b\'xdexadxbexef\'`. # Notes - Use the `chunk` module for handling the chunk reading. - Ensure proper handling of the file and chunk operations to avoid resource leaks. # Instructions 1. Implement the `read_chunks` function as specified. 2. Test your function with different binary files to ensure its correctness.","solution":"import chunk def read_chunks(file_path: str) -> None: Reads and processes chunks from the specified binary file in EA IFF 85 format. For each chunk, prints the chunk ID and size. If the chunk ID is \\"DATA\\", writes its data to a file named \\"output_data.chunk\\". Args: file_path: The path to the binary file to read. try: with open(file_path, \'rb\') as f: while True: chunk_id = f.read(4) if len(chunk_id) < 4: break # End of file or invalid format chunk_size_data = f.read(4) if len(chunk_size_data) < 4: break # End of file or invalid format chunk_size = int.from_bytes(chunk_size_data, byteorder=\'big\') print(f\\"Chunk ID: {chunk_id.decode(\'ascii\')} | Size: {chunk_size}\\") chunk_data = f.read(chunk_size) if chunk_id == b\'DATA\': with open(\\"output_data.chunk\\", \\"wb\\") as output_file: output_file.write(chunk_data) except FileNotFoundError: print(f\\"File {file_path} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Email Management using `imaplib` **Problem Statement:** You are required to write a Python function `fetch_unread_emails(username, password, imap_server)` using the `imaplib` module that connects to an IMAP server and fetches all unread emails from the inbox. For each unread email, you need to output its subject line and sender\'s email address. **Function Signature:** ```python def fetch_unread_emails(username: str, password: str, imap_server: str) -> List[Tuple[str, str]]: ``` # Input and Output Formats: - **Input:** - `username`: A string representing the username of the email account. - `password`: A string representing the password of the email account. - `imap_server`: A string representing the IMAP server address (e.g., \'imap.gmail.com\'). - **Output:** - A list of tuples where each tuple contains: - The subject of the unread email (str) - The sender\'s email address (str) # Constraints: - Handle any potential exceptions such as failed login or connection issues gracefully. - You should use secure SSL connection to the IMAP server. - You might need to handle data parsing for extracting the subject and sender from the email content properly. # Example Usage: ```python username = \\"your_email@example.com\\" password = \\"your_password\\" imap_server = \\"imap.example.com\\" emails = fetch_unread_emails(username, password, imap_server) for subject, sender in emails: print(f\\"Subject: {subject}, Sender: {sender}\\") ``` # Notes: - Use the `IMAP4_SSL` class for connecting via SSL. - You can utilize methods like `login`, `select`, `search`, and `fetch` from the `IMAP4_SSL` class. - IMAP search command for unread emails is `(UNSEEN)`. - To parse email content, especially the subject and sender, you might need to use the `email` library for properly handling different email formats. Your implementation should showcase the use of `imaplib` and properly manage the connection lifecycle, ensure secure connection, handle exceptions, and correctly parse email data.","solution":"import imaplib import email from email.header import decode_header from typing import List, Tuple def fetch_unread_emails(username: str, password: str, imap_server: str) -> List[Tuple[str, str]]: try: # Connect to the server using SSL mail = imaplib.IMAP4_SSL(imap_server) # Login to the account mail.login(username, password) # Select the inbox mail.select(\\"inbox\\") # Search for all unread emails status, messages = mail.search(None, \'(UNSEEN)\') email_list = [] # Loop through each unread email for num in messages[0].split(): status, msg_data = mail.fetch(num, \'(RFC822)\') for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) # Decode the email subject subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): # If it\'s a bytes, decode to str subject = subject.decode(encoding if encoding else \\"utf-8\\") # Get the sender\'s email address from_ = msg.get(\\"From\\") email_list.append((subject, from_)) # Logout and close the connection mail.logout() return email_list except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"You will implement a Python C-extension module that demonstrates the usage of cell objects to store and manipulate variables across different scopes, simulating the behavior of closures. Objective Create a Python C-extension module named `cell_extension` with the following functionalities: 1. **Create a Cell**: Implement a function `create_cell(value: object) -> object:` which creates and returns a new cell object containing the provided value. 2. **Get Cell Value**: Implement a function `get_cell_value(cell: object) -> object:` which returns the value contained within the given cell object. 3. **Set Cell Value**: Implement a function `set_cell_value(cell: object, value: object) -> None:` which sets the value of the given cell object to the specified value. 4. **Check Cell Object**: Implement a function `is_cell_object(obj: object) -> bool:` which returns `True` if the provided object is a cell object, otherwise `False`. Requirements - A cell object should be able to store any type of object (e.g., integers, strings, lists, custom objects). - Safety checks should be adhered to while implementing these functions to avoid segmentation faults and other potential issues. Performance Constraints - The operations should handle typical usage without significant performance concerns. - Ensure that memory management is appropriately handled, avoiding memory leaks. Sample Usage Here is an example of how the functions might be used in a Python script: ```python import cell_extension # Create a cell with an initial value cell = cell_extension.create_cell(42) # Check if it is a cell object print(cell_extension.is_cell_object(cell)) # Output: True # Get the value stored in the cell print(cell_extension.get_cell_value(cell)) # Output: 42 # Set a new value in the cell cell_extension.set_cell_value(cell, \\"PythonCell\\") # Get the updated value print(cell_extension.get_cell_value(cell)) # Output: PythonCell ``` You are provided with the interfaces to be implemented. Use the given functions related to cell objects as described in the documentation. ```python # Required Interfaces for cell_extension module def create_cell(value: object) -> object: pass def get_cell_value(cell: object) -> object: pass def set_cell_value(cell: object, value: object) -> None: pass def is_cell_object(obj: object) -> bool: pass ``` Happy coding!","solution":"import ctypes class Cell(ctypes.Structure): _fields_ = [(\\"value\\", ctypes.py_object)] def create_cell(value): Creates and returns a properly-typed cell object containing the provided value. cell = Cell() cell.value = value return cell def get_cell_value(cell): Returns the value contained within the given cell object. if isinstance(cell, Cell): return cell.value else: raise TypeError(\\"Provided object is not a cell.\\") def set_cell_value(cell, value): Sets the value of the given cell object to the specified value. if isinstance(cell, Cell): cell.value = value else: raise TypeError(\\"Provided object is not a cell.\\") def is_cell_object(obj): Returns True if the provided object is a cell object, otherwise False. return isinstance(obj, Cell)"},{"question":"**Question:** You are tasked with performing parallel computations on multiple CPU devices using PyTorch. To do this, you need to implement a function that performs the following steps: 1. Checks the availability of CPU devices. 2. Sets a particular CPU device for computation. 3. Creates and uses a stream to perform some dummy calculations. 4. Ensures all streams complete their tasks. Your task is to implement the function `parallel_computations_on_cpu()` that handles the above steps. The function should also output the current device and stream statuses for debugging purposes. # Function Signature ```python def parallel_computations_on_cpu(dummy_work: callable, num_devices: int) -> None: # Your implementation here ``` # Input - `dummy_work`: A callable function that performs some CPU-bound work; it takes no arguments and returns no values. - `num_devices`: An integer specifying how many CPU devices to use for the parallel computations. # Output - None # Implementation Requirements 1. Check if the required number of CPU devices are available. If not, raise a RuntimeError. 2. Use the function `set_device()` to set the current CPU device to each of the available ones for parallel work. 3. Create a stream with the `stream()` function for each device. 4. Use the `StreamContext` to ensure computations are run within the correct context. 5. Use `synchronize()` to wait for all streams to complete. 6. Print the device index and stream status after each computation for debugging purposes. # Constraints - Assume `dummy_work` takes a non-negligible amount of time but represents some meaningful CPU-bound task. - Performance efficiency is crucial; ensure computations do not run sequentially. # Example Usage ```python def dummy_computation(): # Simulate some CPU-bound task import time time.sleep(1) # Call the function with a dummy computation and 2 devices parallel_computations_on_cpu(dummy_computation, 2) ``` # Example Output ``` Using device 0, stream <Stream object> Using device 1, stream <Stream object> ``` **Note**: The actual stream object details may vary based on implementation.","solution":"import torch def parallel_computations_on_cpu(dummy_work: callable, num_devices: int) -> None: Perform parallel computations on multiple CPU devices using PyTorch. :param dummy_work: A callable function that performs some CPU-bound work. :param num_devices: An integer specifying how many CPU devices to use for the parallel computations. :return: None # Check the number of available CPU devices available_devices = torch.multiprocessing.cpu_count() if num_devices > available_devices: raise RuntimeError(f\\"Requested {num_devices} devices, but only {available_devices} are available.\\") # Create a process for each device to perform the dummy work processes = [] for i in range(num_devices): process = torch.multiprocessing.Process(target=execute_on_device, args=(i, dummy_work,)) processes.append(process) process.start() # Wait for all processes to complete for process in processes: process.join() def execute_on_device(device_id: int, dummy_work: callable) -> None: Execute the dummy work on a given device. :param device_id: The ID of the device to run the work on. :param dummy_work: The function representing the dummy work. :return: None # Set the device for computation print(f\\"Using device {device_id}\\") # Perform the dummy computation dummy_work() # Print completion message print(f\\"Completed work on device {device_id}\\")"},{"question":"**Problem Statement:** You are tasked with implementing a Python function that can create a compressed archive of a set of files using the `gzip` algorithm and then decompress it to verify the content. The function should demonstrate proficiency in handling file operations, compression, and error handling. **Function Signature:** ```python def create_and_verify_archive(file_names: List[str], archive_name: str) -> bool: Create a compressed archive of the provided files and verify its integrity. Parameters: file_names (List[str]): List of file paths to be archived. archive_name (str): Name of the resulting archive file (without extension). Returns: bool: True if the archive is successfully created, compressed, decompressed, and verified, else False. ``` **Input:** 1. `file_names`: A list of strings, where each string is a valid file path to be included in the archive. 2. `archive_name`: A string representing the name of the resulting archive without the `.gz` extension. **Output:** - The function should return `True` if the archive creation, compression, decompression, and verification steps are successful and the content of the decompressed files matches the original content. - Otherwise, it should return `False`. **Constraints:** - All files listed in `file_names` exist and are readable. - The created archive should be compressed using the `gzip` algorithm. - When decompressed, the archive should contain the exact same files with the same content. **Example:** ```python # assume we have files \'file1.txt\' and \'file2.txt\' available in the same directory file_names = [\'file1.txt\', \'file2.txt\'] archive_name = \'my_archive\' result = create_and_verify_archive(file_names, archive_name) print(result) # Output should be True if everything works correctly. ``` **Performance Requirements:** - The implementation should efficiently handle creating and decompressing the archive, even with a large number of files or larger file sizes. - The student must ensure proper error handling for scenarios such as corrupted compression, missing files, partial data compression/decompression, etc. **Advanced Consideration:** Students should consider edge cases like empty files, large files, and binary files. They should also ensure that the processes can handle interruptions gracefully and verify the integrity correctly. **Hints:** - You may use the `gzip` module for compression and decompression. - Remember to clean up any temporary files created during the process.","solution":"import os import gzip import shutil from typing import List def create_and_verify_archive(file_names: List[str], archive_name: str) -> bool: try: # Create the gzip archive with gzip.open(f\'{archive_name}.gz\', \'wb\') as archive: for file_name in file_names: with open(file_name, \'rb\') as f: shutil.copyfileobj(f, archive) # Verify the archive by decompressing and comparing content with gzip.open(f\'{archive_name}.gz\', \'rb\') as archive: extracted_files_content = [] for file_name in file_names: content = archive.read(os.path.getsize(file_name)) extracted_files_content.append(content) for i, file_name in enumerate(file_names): with open(file_name, \'rb\') as f: original_content = f.read() if original_content != extracted_files_content[i]: return False return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"**Objective:** Implement a custom class `DeepChainMap` that extends the `ChainMap` class from the `collections` module. The `DeepChainMap` should allow writes and deletions to affect all mappings in the chain, not just the first one. Instructions: 1. **Class Definition:** - Define a class called `DeepChainMap` that inherits from `collections.ChainMap`. 2. **Method Overrides:** - Override the `__setitem__` method to update a key wherever it is found in the chain. - Override the `__delitem__` method to delete a key wherever it is found in the chain. 3. **Additional Methods:** - Add a method `increase_all` that increases the value associated with a given key by a specified amount across all mappings that contain the key. 4. **Input and Output:** - Instantiate `DeepChainMap` with a variable number of dictionaries. - Implement `__setitem__` and `__delitem__` methods that can increase and delete values across all dictionaries respectively when called. - Implement the `increase_all` method that takes a key and an integer, increasing the value of the key by the integer in all dictionaries where the key exists. 5. **Example:** ```python from collections import ChainMap class DeepChainMap(ChainMap): def __setitem__(self, key, value): for mapping in self.maps: if key in mapping: mapping[key] = value return self.maps[0][key] = value def __delitem__(self, key): for mapping in self.maps: if key in mapping: del mapping[key] return raise KeyError(key) def increase_all(self, key, amount): for mapping in self.maps: if key in mapping: mapping[key] += amount # Example Usage d1 = {\'a\': 1, \'b\': 2} d2 = {\'b\': 3, \'c\': 4} d3 = {\'c\': 5, \'d\': 6} deep_chain_map = DeepChainMap(d1, d2, d3) # Updating a key across all mappings deep_chain_map[\'b\'] = 10 assert d1[\'b\'] == 10 assert d2[\'b\'] == 10 # Deleting a key across all mappings del deep_chain_map[\'c\'] assert \'c\' not in d2 assert \'c\' not in d3 # Increasing a key\'s value across all mappings deep_chain_map.increase_all(\'a\', 5) assert d1[\'a\'] == 6 ``` Implement the class and its methods to ensure it functions as demonstrated in the example. **Constraints:** - Assume keys and values will be of types compatible with dictionaries. - The `increase_all` method will only be tested with integer values. **Performance Requirements:** - Ensure methods run efficiently with time complexity close to O(N) where N is the number of mappings, considering the worst-case scenario when key operations need to traverse through all mappings.","solution":"from collections import ChainMap class DeepChainMap(ChainMap): def __setitem__(self, key, value): for mapping in self.maps: if key in mapping: mapping[key] = value self.maps[0][key] = value def __delitem__(self, key): found = False for mapping in self.maps: if key in mapping: del mapping[key] found = True if not found: raise KeyError(key) def increase_all(self, key, amount): for mapping in self.maps: if key in mapping: mapping[key] += amount"},{"question":"# Question: Advanced Logging System Implementation **Objective**: Implement an advanced logging system that demonstrates the ability to use multiple loggers, handlers, and formatters. **Problem Statement**: You are required to create a logging system for a Python application consisting of multiple modules. This logging system should perform the following: 1. Create a logger for the application (`app_logger`) and configure it to log messages to both a console and a file. 2. Create a separate logger for a module called `module_logger` and configure it to log messages to a different file. 3. Set up different logging levels for each logger. 4. Use custom formatters to include the time of logging, logger name, logging level, and the message in the logs. 5. Include a specific handler to send critical error logs to an email address (you can simulate this handler without actual email functionality). 6. Ensure proper propagation of log messages across different loggers and handlers. **Implementation Details**: 1. **Application Logger Configuration**: - `app_logger`: Logs to both console and \\"app.log\\" file. - Log level: INFO. - Format: `%(asctime)s - %(name)s - %(levelname)s - %(message)s`. 2. **Module Logger Configuration**: - `module_logger`: Logs to \\"module.log\\" file. - Log level: DEBUG. - Format: `%(asctime)s - %(name)s - %(levelname)s - %(message)s`. 3. **Critical Error Handler**: - A handler that captures CRITICAL logs and \\"sends\\" them (simulate only) to an email address. 4. **Example module**: - Write a module named `example_module.py` that uses the `module_logger` to log some DEBUG, INFO, WARNING, and ERROR messages. 5. **Application Script**: - Write a script `main_app.py` that initializes the `app_logger`, logs some INFO and ERROR messages, and imports `example_module` to generate logs from it. **Constraints/Requirements**: - The logging configuration should be done programmatically. - Use Python\'s built-in logging library only. **Input/Output**: - There are no specific inputs or outputs for this script. The objective is to ensure logs are correctly written to the specified files and console, and critical messages are simulated to be sent to an email. # Example Output If you run the `main_app.py`, you should see logs like: **Console**: ``` INFO - 2020-12-12 12:12:12,123 - app_logger - Application started ERROR - 2020-12-12 12:12:12,456 - app_logger - An error occurred DEBUG - 2020-12-12 12:12:13,789 - module_logger - Module debug message ... ``` **app.log**: ``` 2020-12-12 12:12:12,123 - app_logger - INFO - Application started 2020-12-12 12:12:12,456 - app_logger - ERROR - An error occurred ``` **module.log**: ``` 2020-12-12 12:12:13,789 - module_logger - DEBUG - Module debug message ... ``` Implementation of sending emails can be simulated by printing something like: ``` CRITICAL: Sending email: A critical error occurred. ``` Good luck!","solution":"import logging from logging.handlers import SMTPHandler def setup_loggers(): # Application Logger Configuration app_logger = logging.getLogger(\'app_logger\') app_logger.setLevel(logging.INFO) # Console Handler for app_logger console_handler = logging.StreamHandler() console_format = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_format) # File Handler for app_logger file_handler = logging.FileHandler(\'app.log\') file_handler.setFormatter(console_format) # Adding Handlers to app_logger app_logger.addHandler(console_handler) app_logger.addHandler(file_handler) # Module Logger Configuration module_logger = logging.getLogger(\'module_logger\') module_logger.setLevel(logging.DEBUG) # File Handler for module_logger module_file_handler = logging.FileHandler(\'module.log\') module_file_format = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') module_file_handler.setFormatter(module_file_format) # Adding Handlers to module_logger module_logger.addHandler(module_file_handler) # Critical Error Handler to simulate email sending class SimulatedSMTPHandler(logging.Handler): def emit(self, record): if record.levelno == logging.CRITICAL: print(f\\"CRITICAL: Sending email: {record.msg}\\") email_handler = SimulatedSMTPHandler() email_handler.setLevel(logging.CRITICAL) app_logger.addHandler(email_handler) module_logger.addHandler(email_handler) return app_logger, module_logger # Call setup_loggers at the beginning app_logger, module_logger = setup_loggers() def log_example_messages(): app_logger.info(\'Application started\') app_logger.error(\'An error occurred\') module_logger.debug(\'Module debug message\') module_logger.info(\'Module info message\') module_logger.warning(\'Module warning message\') module_logger.error(\'Module error message\') module_logger.critical(\'Module critical message\') if __name__ == \\"__main__\\": log_example_messages()"},{"question":"# Dynamic Python Code Execution You are required to write a Python C-extension that provides a function allowing users to run dynamically provided Python code strings and capture its output. Function Specification **Function Name:** `run_dynamic_code` **Inputs:** - `code: str` - A string containing the Python code to be executed. - `globals: dict` - A dictionary representing the global namespace in which the code should be executed. - `locals: dict` - A dictionary representing the local namespace in which the code should be executed. **Outputs:** - Returns a tuple `(result: any, success: bool)`: - `result` - The result of executing the code. - `success` - A boolean value indicating whether the code execution was successful (`True` if successful, `False` otherwise). **Constraints:** - The code must be written as a C-extension module. - Proper error handling must be implemented to capture exceptions and ensure they do not crash the interpreter. - The function should be efficient and avoid unnecessary overhead. Example Usage ```python from your_c_extension_module import run_dynamic_code code = \\"print(\'Hello, World!\'); x = 10 + 20; y = x * 2; result = y\\" globals_dict = {} locals_dict = {} result, success = run_dynamic_code(code, globals_dict, locals_dict) print(result) # Should print the value of `result` in the executed code print(success) # Should print True if the code executed successfully ``` Performance Requirements - The solution should handle typical Python code execution efficiently without significant delays. - Graceful handling of errors and exceptions should be ensured to avoid crashes. Implement this function as a C-extension to demonstrate your understanding of interacting with the Python interpreter dynamically.","solution":"def run_dynamic_code(code, globals=None, locals=None): Executes a Python code string and captures its output. Args: code (str): A string containing the Python code to be executed. globals (dict): A dictionary representing the global namespace. locals (dict): A dictionary representing the local namespace. Returns: tuple: (result, success) result: The result of executing the code. success: A boolean indicating whether the code execution was successful. try: exec(code, globals, locals) result = locals.get(\'result\', None) return result, True except Exception as e: return str(e), False"},{"question":"# Advanced Python Scope Management with Cell Objects Objective: You are required to implement a series of functions that manage variables using `Cell` objects, demonstrating an understanding of variable scope and closure in Python. The goal is to achieve correct behavior for capturing and updating variables in nested functions using `Cell` objects. Task: Implement the following functions: 1. **create_cell(value)** - **Input:** A value of any type (`value`). - **Output:** A new cell object containing `value`. - **Example:** ```python cell = create_cell(10) # cell should be a cell object containing the value 10 ``` 2. **get_cell_value(cell)** - **Input:** A cell object (`cell`). - **Output:** The value contained within the cell object. - **Example:** ```python value = get_cell_value(cell) # value should be the original value contained in cell, e.g., 10 ``` 3. **set_cell_value(cell, value)** - **Input:** A cell object (`cell`) and a new value (`value`) of any type. - **Output:** None. The function should update the value contained in the cell object to `value`. - **Example:** ```python set_cell_value(cell, 20) new_value = get_cell_value(cell) # new_value should now be 20 ``` Constraints: - You are **not allowed** to use Python\'s built-in `CellType` directly in your implementations. - You should use the provided C API functions for managing the cell objects, simulating their usage in pure Python functions as accurately as possible. Example Usage: ```python cell = create_cell(5) assert get_cell_value(cell) == 5 set_cell_value(cell, 15) assert get_cell_value(cell) == 15 ``` Performance Requirement: Your implementations should be efficient in both time and space complexity, considering the low-level nature of cell management and variable access.","solution":"def create_cell(value): Create a cell object with the given value. def cell_func(): return value return cell_func.__closure__[0] def get_cell_value(cell): Get the value from the cell object. return cell.cell_contents def set_cell_value(cell, value): Set a new value in the cell object. cell.cell_contents = value"},{"question":"You are given a set of points in a 2D plane and their corresponding labels. Your task is to implement a k-nearest neighbors classifier using `scikit-learn` that can classify new points based on their nearest neighbors in the training set. You will validate your classifier using k-fold cross-validation. Requirements 1. Use `KNeighborsClassifier` class from the `sklearn.neighbors` module. 2. Implement k-fold cross-validation (k=5) to evaluate the accuracy of your classifier. 3. Use `accuracy_score` from the `sklearn.metrics` module to measure the performance of your model. # Input Format 1. A list of training points where each point is represented as a tuple of two coordinates (x, y). 2. A list of corresponding labels for the training points. 3. A list of test points where each point is represented as a tuple of two coordinates (x, y). 4. The number of neighbors `k` for the `KNeighborsClassifier`. # Output Format 1. Predicted labels for the test points. 2. Mean accuracy from the k-fold cross-validation on the training data. # Constraints - Assume each point in the dataset has a unique set of coordinates. # Example ```python # Input training_points = [(-1, -1), (-2, -1), (-3, -2), (1, 1), (2, 1), (3, 2)] training_labels = [0, 0, 0, 1, 1, 1] test_points = [(0, 0), (4, 4)] k = 3 # Output # Predicted labels [0, 1] # Mean accuracy from k-fold cross-validation 0.8 ``` # Function Signature ```python from typing import List, Tuple def knn_classifier(training_points: List[Tuple[int, int]], training_labels: List[int], test_points: List[Tuple[int, int]], k: int) -> Tuple[List[int], float]: # Your implementation here pass ``` # Notes - You will need to use `KNeighborsClassifier` for classification. - To implement k-fold cross-validation, you can use the `KFold` class from the `sklearn.model_selection` module. - Make sure to use the Euclidean distance metric for the classifier. - Achieving an accurate and efficient implementation will demonstrate a strong understanding of both K-Nearest Neighbors and cross-validation concepts in `scikit-learn`.","solution":"from typing import List, Tuple from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import KFold from sklearn.metrics import accuracy_score import numpy as np def knn_classifier(training_points: List[Tuple[int, int]], training_labels: List[int], test_points: List[Tuple[int, int]], k: int) -> Tuple[List[int], float]: # Converting list of points to numpy array for sklearn compatibility X = np.array(training_points) y = np.array(training_labels) test_X = np.array(test_points) # Initialize the KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=k) # Perform k-fold cross-validation kf = KFold(n_splits=5, shuffle=True, random_state=1) accuracies = [] for train_index, val_index in kf.split(X): X_train, X_val = X[train_index], X[val_index] y_train, y_val = y[train_index], y[val_index] knn.fit(X_train, y_train) y_pred = knn.predict(X_val) accuracies.append(accuracy_score(y_val, y_pred)) # Calculate mean accuracy mean_accuracy = np.mean(accuracies) # Fit on the whole training data and predict test points knn.fit(X, y) test_predictions = knn.predict(test_X) return test_predictions.tolist(), mean_accuracy"},{"question":"**Objective**: Implement a Python function that dynamically creates a code object representing a simple function, and then uses various utility functions to manipulate and inspect this code object. # Problem Statement You are required to write a function `create_and_inspect_code_object` that performs the following tasks: 1. **Create a Code Object**: Use the given parameters to create a new code object representing a simple function. The function should just return the sum of its arguments, `a` and `b`. 2. **Inspect the Code Object**: - Check if the created object is a code object. - Retrieve the number of free variables. - Find the line number corresponding to a specific byte offset in the code. The function should return a dictionary containing the results of these operations. # Function Signature ```python def create_and_inspect_code_object(argcount: int, nlocals: int, stacksize: int, flags: int, byte_offset: int) -> dict: ``` # Parameters - `argcount` (int): The number of arguments the function takes. - `nlocals` (int): The number of local variables used in the function. - `stacksize` (int): The required stack size. - `flags` (int): Execution flags for the code object. - `byte_offset` (int): A byte offset for which to find the corresponding line number. # Output - A dictionary with the following keys: - `\'is_code_object\'`: A boolean indicating if the created object is a code object. - `\'num_free_vars\'`: An integer representing the number of free variables in the code object. - `\'line_number\'`: An integer representing the line number corresponding to the given byte offset. # Constraints - You should use the CPython\'s code object API to create and manipulate the code object. - You may assume valid parameter values are provided as input. # Example ```python result = create_and_inspect_code_object(argcount=2, nlocals=2, stacksize=2, flags=0, byte_offset=4) print(result) # Output might be: # { # \'is_code_object\': True, # \'num_free_vars\': 0, # \'line_number\': 1 # } ``` # Notes - The example function to be created can be visualized as: ```python def simple_add(a, b): return a + b ``` - You should handle the internal mechanics using the code object API provided in the documentation.","solution":"import types def create_and_inspect_code_object(argcount: int, nlocals: int, stacksize: int, flags: int, byte_offset: int) -> dict: # Define the function in bytecode manually using \'compile\' and \'exec\' simple_add_code = compile(\'def simple_add(a, b):n return a + b\', \'<string>\', \'exec\') # Extract the code object of the simple_add function simple_add_func_code = simple_add_code.co_consts[0] # Verify if the created object is a code object is_code_object = isinstance(simple_add_func_code, types.CodeType) # Retrieve number of free variables num_free_vars = simple_add_func_code.co_freevars # Find the line number corresponding to the specific byte offset line_number = simple_add_func_code.co_firstlineno # Return a dictionary with the required information return { \\"is_code_object\\": is_code_object, \\"num_free_vars\\": len(num_free_vars), \\"line_number\\": simple_add_func_code.co_firstlineno }"},{"question":"**Advanced Python Asyncio Problem** # Objective: Write a Python script to demonstrate the utilization of asynchronous event loops in a way that accounts for platform-specific limitations and differences as outlined in the documentation provided. # Problem Statement: Create an asyncio-based server program that: 1. Opens a TCP connection to handle incoming messages. 2. Can send and receive data asynchronously. 3. Accounts for the platform-specific constraints mentioned in the provided documentation, ensuring compatibility and correct functionality across Windows and macOS. # Requirements: 1. **TCP Communication**: Set up a simple TCP server that processes incoming connections and messages asynchronously. 2. **Platform Handling**: Implement appropriate checks and fallbacks to handle differences in event loop selections and functionality between Windows and macOS. 3. **Error Handling**: Implement robust error handling to manage unsupported features on respective platforms. 4. **Documentation**: Clearly document how the implemented solution adheres to the mentioned platform-specific constraints. # Input and Output: - **Input**: No direct input to the script. The script should handle incoming TCP messages. - **Output**: The server should print the incoming messages to the console and send a confirmation message back to the client. # Constraints: - Use `asyncio` for handling asynchronous communication. - Ensure compatibility with both Windows and macOS, following the provided limitations for each platform. - The server should handle multiple clients concurrently. # Performance: - The server should efficiently manage multiple connections without significant delays. - Properly handle edge cases such as connection drops or unsupported features. **Additional Information:** To help you get started, the following code snippet sets up a basic asynchronous TCP server using `asyncio` without specific platform handling. Use this as a foundation to build your solution: ```python import asyncio async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") response = \'Message Received\' writer.write(response.encode()) await writer.drain() print(\\"Closing the connection\\") writer.close() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() asyncio.run(main()) ``` Adapt this to handle the platform-specific constraints and ensure robust error handling and functionality.","solution":"import asyncio import sys # Platform-specific workaround for event loops on Windows and macOS def get_event_loop(): if sys.platform == \'win32\': # Windows specific event loop policy to ensure consistent behavior asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy()) return asyncio.get_event_loop() async def handle_client(reader, writer): try: data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") response = \'Message Received\' writer.write(response.encode()) await writer.drain() print(\\"Closing the connection\\") writer.close() await writer.wait_closed() except Exception as e: print(f\\"Error handling client: {e}\\") async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": loop = get_event_loop() loop.run_until_complete(main())"},{"question":"Coding Assessment Question You are tasked with creating a custom descriptor in Python that manages an attribute with special validation rules. Use the concept of descriptors to create a class that ensures the attribute it manages adheres to specific constraints. # Requirements 1. **Descriptor Class**: - Define a custom descriptor class `ValidatedAttribute` that manages a single attribute. - The descriptor should ensure the attribute value assigned to it is an integer and within the range 1 to 100 inclusive. - If the value being set is not an integer or not within the specified range, raise a `ValueError` with a message indicating the validation error. 2. **Usage**: - Create a class `MyClass` that uses `ValidatedAttribute` for its `number` attribute. # Function Signatures ```python class ValidatedAttribute: def __init__(self): # Initialize any required variables here pass def __get__(self, instance, owner): # Implement the getter method here pass def __set__(self, instance, value): # Implement the setter method here pass class MyClass: number = ValidatedAttribute() def __init__(self, number): self.number = number ``` # Input and Output 1. Input: Sequence of operations on an instance of `MyClass` - Example: ```python m = MyClass(42) print(m.number) m.number = 101 ``` - Output should handle invalid operations gracefully by raising appropriate exceptions. 2. Output: Should correctly reflect the state changes in the attribute and validation messages - Example Output: ``` 42 ValueError: value must be an integer between 1 and 100 ``` # Constraints - You must utilize the descriptor protocol (`__get__`, `__set__`, and optionally `__delete__`) to manage the attribute. - Performance should be efficient and in accordance with descriptor usage in Python. # Additional Requirements - Thoroughly test your implementation to ensure correct behavior for valid and invalid inputs. - Provide appropriate docstrings and comments in your code. Good luck!","solution":"class ValidatedAttribute: def __init__(self): self._name = \'_\' + self.__class__.__name__ + \'_value\' def __get__(self, instance, owner): return getattr(instance, self._name, None) def __set__(self, instance, value): if not isinstance(value, int): raise ValueError(\\"Value must be an integer\\") if not (1 <= value <= 100): raise ValueError(\\"Value must be an integer between 1 and 100\\") setattr(instance, self._name, value) class MyClass: number = ValidatedAttribute() def __init__(self, number): self.number = number"},{"question":"Coding Assessment Question # Objective Implement a Python function that utilizes the `hashlib` module to create a secure hash for a given message. Your function should demonstrate an understanding of various hashing concepts including using different digest sizes and keyed hashing. # Problem Statement Write a Python function `secure_hash(message: str, key: str, digest_size: int) -> str` that: 1. Takes in three parameters: - `message`: A string representing the input message to be hashed. - `key`: A string used for keyed hashing. - `digest_size`: An integer specifying the desired digest size in bytes. 2. Returns the hexadecimal digest of the hashed message. # Constraints - The `digest_size` must be between 1 and 64 bytes inclusive. - You must use the Blake2 hash function with keyed hashing from the `hashlib` module. - Ensure that the output digest is of the specified size. # Example ```python def secure_hash(message: str, key: str, digest_size: int) -> str: # Your implementation here # Sample usage print(secure_hash(\\"Hello, World!\\", \\"mysecretkey\\", 32)) # Expected: A 64-character hexadecimal string (since 32 bytes is 64 hex characters long) ``` # Notes - If `digest_size` is not within the valid range, raise a `ValueError` with the message \\"Digest size must be between 1 and 64\\". - Assume `message` and `key` are non-empty strings. # Detailed Steps 1. Validate the `digest_size` parameter. 2. Create a BLAKE2 hash object using the `hashlib.blake2b` method, with the provided key and desired digest size. 3. Update the hash object with the provided `message`. 4. Return the hexadecimal digest of the hash. # Testing You should test your function with various messages, keys, and digest sizes to ensure it handles all specified scenarios correctly.","solution":"import hashlib def secure_hash(message: str, key: str, digest_size: int) -> str: Generates a secure hash using BLAKE2 keyed hashing. :param message: The input message to be hashed. :param key: The key used for keyed hashing. :param digest_size: The desired digest size in bytes. :return: The hexadecimal digest of the hashed message. if not (1 <= digest_size <= 64): raise ValueError(\\"Digest size must be between 1 and 64\\") # Create a BLAKE2b hash object with the specified key and digest size h = hashlib.blake2b(key=key.encode(), digest_size=digest_size) h.update(message.encode()) return h.hexdigest()"},{"question":"**Objective:** Demonstrate your understanding of exception handling and the use of the `cgitb` module in a CGI script. Problem Statement: You are tasked with creating a simple CGI script that calculates the factorial of a non-negative integer provided by the user via an HTTP GET request. Implement detailed exception handling using the `cgitb` module to provide informative error messages to the user. Requirements: 1. **Input:** - A non-negative integer provided as a query parameter named `num` in the HTTP GET request. 2. **Output:** - The calculated factorial of the input number displayed in the browser. - In the event of an error (e.g., invalid input, missing input, runtime errors), use `cgitb` to show detailed error information formatted in HTML. 3. **Constraints:** - The input number should be a non-negative integer. If not, the script should handle the exception gracefully. 4. **Functions to Implement:** - `factorial(n)`: A function that calculates the factorial of a non-negative integer `n`. - `main()`: The main function to handle the CGI request, calculate the factorial, and manage exceptions using `cgitb`. 5. **Performance Requirement:** - The solution should handle typical web request scenarios efficiently. Factorial computation for values up to 20 should be sufficient. Implementation Details: 1. **Create a file named `factorial_cgi.py`:** ```python import cgi import cgitb import sys # Enable cgitb for detailed error reports cgitb.enable(display=1, logdir=None, context=5, format=\'html\') def factorial(n): Calculate the factorial of a non-negative integer n. if not isinstance(n, int) or n < 0: raise ValueError(\\"Input must be a non-negative integer\\") if n == 0: return 1 result = 1 for i in range(1, n + 1): result *= i return result def main(): Main function to handle CGI request and calculate the factorial. print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() try: num = int(form.getvalue(\'num\')) result = factorial(num) print(f\\"<html><body><h2>Factorial of {num} is {result}</h2></body></html>\\") except Exception: # Handle exceptions using cgitb cgitb.handler() if __name__ == \\"__main__\\": main() ``` 2. **Test your script:** - Deploy the script on a compatible web server with CGI support. - Access the script via a web browser or a tool like `curl` to confirm it correctly calculates the factorial and displays errors using `cgitb` when necessary. Example Usage: - Accessing the script with `http://yourserver.com/factorial_cgi.py?num=5` should display: ``` <html><body><h2>Factorial of 5 is 120</h2></body></html> ``` - Accessing the script with invalid inputs `http://yourserver.com/factorial_cgi.py?num=abc` should display a detailed HTML error report. Make sure to follow the requirements closely to demonstrate a thorough understanding of CGI scripting and error handling with `cgitb`. **Note:** This module is deprecated since Python version 3.11. Ensure your Python environment supports this module or use a compatible version for this exercise.","solution":"import cgi import cgitb import sys # Enable cgitb for detailed error reports cgitb.enable(display=1, logdir=None, context=5, format=\'html\') def factorial(n): Calculate the factorial of a non-negative integer n. if not isinstance(n, int) or n < 0: raise ValueError(\\"Input must be a non-negative integer\\") if n == 0: return 1 result = 1 for i in range(1, n + 1): result *= i return result def main(): Main function to handle CGI request and calculate the factorial. print(\\"Content-Type: text/html\\") print() form = cgi.FieldStorage() try: num = int(form.getvalue(\'num\')) result = factorial(num) print(f\\"<html><body><h2>Factorial of {num} is {result}</h2></body></html>\\") except Exception: # Handle exceptions using cgitb cgitb.handler() if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement:** You are required to implement a function that sorts a list of dictionaries based on multiple keys using functional programming principles. # Function Specification: 1. **Function Name:** `multi_key_sort` 2. **Inputs:** - `data`: A list of dictionaries. Each dictionary contains keys \'name\', \'age\', and \'score\'. - `keys`: A list of keys (strings) by which to sort the dictionaries. Each key may be prefixed with a minus sign (`-`) to indicate descending order for that particular key. 3. **Output:** A list of dictionaries sorted based on the specified keys. # Constraints: - The input list can contain between 1 and 1000 dictionaries. - Each dictionary will have exactly the keys \'name\' (string), \'age\' (integer), and \'score\' (float). - The length of the `keys` list can range from 1 to 3. - The sorting should be stable, i.e., if two dictionaries are considered equal based on the sorting keys, their relative order in the input list should be maintained. - The function should make use of functional programming concepts as much as possible, leveraging Python\'s built-in capabilities. # Example: ```python data = [ {\'name\': \'Alice\', \'age\': 30, \'score\': 85.7}, {\'name\': \'Bob\', \'age\': 25, \'score\': 90.5}, {\'name\': \'Charlie\', \'age\': 35, \'score\': 85.7}, {\'name\': \'David\', \'age\': 30, \'score\': 88.0} ] keys = [\'-score\', \'age\'] output = multi_key_sort(data, keys) # Expected Output: # [ # {\'name\': \'Bob\', \'age\': 25, \'score\': 90.5}, # {\'name\': \'David\', \'age\': 30, \'score\': 88.0}, # {\'name\': \'Alice\', \'age\': 30, \'score\': 85.7}, # {\'name\': \'Charlie\', \'age\': 35, \'score\': 85.7} # ] ``` # Detailed Solution: ```python from functools import cmp_to_key def multi_key_sort(data, keys): def compare(x, y): for key in keys: if key.startswith(\'-\'): k = key[1:] if x[k] != y[k]: return -1 if x[k] > y[k] else 1 else: k = key if x[k] != y[k]: return -1 if x[k] < y[k] else 1 return 0 return sorted(data, key=cmp_to_key(compare)) # Example usage data = [ {\'name\': \'Alice\', \'age\': 30, \'score\': 85.7}, {\'name\': \'Bob\', \'age\': 25, \'score\': 90.5}, {\'name\': \'Charlie\', \'age\': 35, \'score\': 85.7}, {\'name\': \'David\', \'age\': 30, \'score\': 88.0} ] keys = [\'-score\', \'age\'] print(multi_key_sort(data, keys)) ``` **Notes:** - The function relies on comparing elements based on the given keys, and the comparison direction is determined by the presence of the minus sign. - The solution ensures stability and makes use of Python\'s `sorted` function combined with the `cmp_to_key` from `functools` to handle custom comparisons in a functional programming style.","solution":"from functools import cmp_to_key def multi_key_sort(data, keys): def compare(x, y): for key in keys: reverse = False if key.startswith(\'-\'): reverse = True k = key[1:] else: k = key if x[k] != y[k]: if reverse: return -1 if x[k] > y[k] else 1 else: return -1 if x[k] < y[k] else 1 return 0 return sorted(data, key=cmp_to_key(compare))"},{"question":"**Python File Handling and Low-Level IO Operations** **Objective:** Demonstrate your understanding of Python\'s file handling capabilities and the low-level C API interactions by implementing a function that reads and writes using file descriptors and custom hooks. **Problem Statement:** You are required to implement a function `custom_file_handler(fd: int, write_data: str) -> str` that performs the following tasks: 1. Creates a Python file object from the given file descriptor `fd`. 2. Sets a custom open code hook that logs every file opening attempt. 3. Writes the string `write_data` to the file using the appropriate Python C API function. 4. Reads back the content from the file from the beginning using the appropriate Python C API function. 5. Returns the content read from the file after writing. **Function Signature:** ```python def custom_file_handler(fd: int, write_data: str) -> str: pass ``` **Input:** - `fd` (int): A valid file descriptor for an already opened file. - `write_data` (str): A string of data to be written to the file. **Output:** - Returns a string which is the content read from the file after writing `write_data`. **Constraints:** - The function should handle exceptions where necessary and ensure the file descriptor is managed appropriately. - Assume the file is opened in a mode that allows both read and write operations. - The hook function should log the file path being opened to the standard output. **Example:** ```python import os import tempfile # Create a temporary file and get the file descriptor temp_file = tempfile.NamedTemporaryFile(delete=False) fd = temp_file.fileno() temp_file.close() # Write and read data using custom file handler data_to_write = \\"Hello, World!\\" result = custom_file_handler(fd, data_to_write) assert result == data_to_write ``` **Hints:** - Refer to Python\'s `io` module for higher-level file handling if needed, and ensure that the low-level C API functions are employed for core file operations. - Use the `PyFile_FromFd` to create a file object and manage exceptions. - Use `PyFile_WriteObject` or `PyFile_WriteString` to write data to the file object. - Use `PyFile_GetLine` to read back data from the file object. - Implement and register a hook function using `PyFile_SetOpenCodeHook`. **Performance Requirements:** - Ensure the function runs efficiently within a reasonable time frame, managing file descriptors and memory usage effectively.","solution":"import os import sys def custom_file_handler(fd: int, write_data: str) -> str: Handles reading and writing to a file from a file descriptor using Python\'s low-level C API. import ctypes from ctypes import py_object # Loading the Python C API libpython = ctypes.CDLL(None) # Defining necessary functions and structures from the Python C API PyFile_FromFd = libpython.PyFile_FromFd PyFile_FromFd.argtypes = [ ctypes.c_int, # fd ctypes.c_char_p, # mode ctypes.c_int, # buffering py_object, # encoding py_object, # errors py_object, # newline ctypes.c_int, # closefd py_object # opener ] PyFile_FromFd.restype = py_object # Defining the \\"hook\\" function def open_code_hook(path, solver): print(f\\"Opening file: {path.decode()}\\") return solver(path) # Registering the hook function PyFile_SetOpenCodeHook = libpython.PyFile_SetOpenCodeHook PyFile_SetOpenCodeHook.argtypes = [ctypes.c_void_p, py_object] PyFile_SetOpenCodeHook.restype = None PyFile_SetOpenCodeHook(ctypes.CFUNCTYPE(py_object, ctypes.c_char_p, py_object)(open_code_hook), None) # Creating a file object from the given FD file_object = PyFile_FromFd( fd, b\\"r+\\", # assuming file supports read and write 0, None, None, None, 1, None ) # Writing to the file PyFile_WriteObject = libpython.PyFile_WriteObject PyFile_WriteObject.argtypes = [py_object, py_object, ctypes.c_int] PyFile_WriteObject.restype = ctypes.c_int data_to_write = ctypes.py_object(write_data) result = PyFile_WriteObject(data_to_write, file_object, 0) # Seeking to the start of the file seek = file_object.seek seek(0) # Reading the content back PyFile_GetLine = libpython.PyFile_GetLine PyFile_GetLine.argtypes = [py_object, ctypes.c_int] PyFile_GetLine.restype = ctypes.c_char_p content_read = PyFile_GetLine(file_object, 1024) # Assuming max read length is 1024 # Closing the file object (decrementing refcount) Py_DecRef = libpython.Py_DecRef Py_DecRef.argtypes = [py_object] Py_DecRef(file_object) return content_read.decode()"},{"question":"**Objective**: Implement a function that utilizes scikit-learn\'s validation and learning curve functionalities to evaluate a given estimator on a dataset. You will plot and interpret these curves to understand the estimator\'s performance in terms of bias and variance. Instructions: 1. **Function Signature:** ```python def evaluate_model(estimator, X, y, param_name, param_range, train_sizes, cv): Evaluate the model using validation and learning curves. Parameters: - estimator: scikit-learn estimator instance (e.g., SVC(kernel=\'linear\')) - X: numpy array of shape (n_samples, n_features), Feature data. - y: numpy array of shape (n_samples,), Target labels. - param_name: str, Name of the hyperparameter to evaluate. - param_range: array-like, The values of the parameter that will be evaluated. - train_sizes: array-like, Relative or absolute numbers of training examples that will be used to generate the learning curve. - cv: int, cross-validation generator or an iterable, Determines the cross-validation splitting strategy. Returns: - A dictionary containing: - \'validation_curve\': (train_scores, valid_scores) of the validation curve using the given param_name and param_range. - \'learning_curve\': (train_sizes, train_scores, valid_scores) of the learning curve using the given train_sizes. pass ``` 2. **Requirements:** - Use the `validation_curve` function to find the training and validation scores for the specified parameter values. - Use the `learning_curve` function to find the training and validation scores for the specified sizes of the training set. - Ensure that `cv` is used for cross-validation in both functions. - Return the scores as a dictionary containing \'validation_curve\' and \'learning_curve\'. 3. **Constraints:** - The function should handle both classification and regression tasks, as long as the `estimator` provided is appropriate for the task. - The function should handle datasets of varying sizes efficiently. 4. **Example Usage:** ```python from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle import numpy as np # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Define estimator and parameters estimator = SVC(kernel=\'linear\') param_name = \\"C\\" param_range = np.logspace(-7, 3, 10) train_sizes = [50, 80, 110] cv = 5 # Evaluate model result = evaluate_model(estimator, X, y, param_name, param_range, train_sizes, cv) print(\\"Validation Curve - Training Scores:n\\", result[\'validation_curve\'][0]) print(\\"Validation Curve - Validation Scores:n\\", result[\'validation_curve\'][1]) print(\\"Learning Curve - Train Sizes:n\\", result[\'learning_curve\'][0]) print(\\"Learning Curve - Training Scores:n\\", result[\'learning_curve\'][1]) print(\\"Learning Curve - Validation Scores:n\\", result[\'learning_curve\'][2]) ``` 5. **Evaluation:** - Your solution will be evaluated based on correctness, efficiency, and clarity. - Ensure the code is well-documented and follows best practices for readability and maintainability. - Interpret the results of the validation and learning curves in comments or as part of the returned dictionary to show understanding of bias, variance, and model performance. **Good Luck!**","solution":"import numpy as np from sklearn.model_selection import validation_curve, learning_curve def evaluate_model(estimator, X, y, param_name, param_range, train_sizes, cv): Evaluate the model using validation and learning curves. Parameters: - estimator: scikit-learn estimator instance (e.g., SVC(kernel=\'linear\')) - X: numpy array of shape (n_samples, n_features), Feature data. - y: numpy array of shape (n_samples,), Target labels. - param_name: str, Name of the hyperparameter to evaluate. - param_range: array-like, The values of the parameter that will be evaluated. - train_sizes: array-like, Relative or absolute numbers of training examples that will be used to generate the learning curve. - cv: int, cross-validation generator or an iterable, Determines the cross-validation splitting strategy. Returns: - A dictionary containing: - \'validation_curve\': (train_scores, valid_scores) of the validation curve using the given param_name and param_range. - \'learning_curve\': (train_sizes, train_scores, valid_scores) of the learning curve using the given train_sizes. # Compute the validation curve train_scores_val, valid_scores_val = validation_curve( estimator, X, y, param_name=param_name, param_range=param_range, cv=cv) # Compute the learning curve train_sizes_lc, train_scores_lc, valid_scores_lc = learning_curve( estimator, X, y, train_sizes=train_sizes, cv=cv) # Return the results as a dictionary results = { \'validation_curve\': (train_scores_val, valid_scores_val), \'learning_curve\': (train_sizes_lc, train_scores_lc, valid_scores_lc) } return results"},{"question":"# Event Scheduler Implementation You have been tasked to implement a custom event scheduler using Python\'s `sched` module. The scheduler should be capable of handling various events based on both absolute and relative times, manage their priorities, and handle exceptions gracefully. Your solution should demonstrate the following capabilities: 1. **Time Management:** - Implement functions to get the current time and delay execution. 2. **Event Scheduling:** - Schedule events using `enter` for relative times and `enterabs` for absolute times. - Ensure events scheduled at the same time respect their priorities. 3. **Event Management:** - List the scheduled events. - Cancel events when necessary. 4. **Execution and Exception Handling:** - Execute events in a non-blocking manner. - Handle exceptions raised during event execution without crashing the scheduler. - Maintain thread safety during execution. # Requirements 1. **Functions to Implement:** - `current_time() -> float`: Returns the current time in seconds since the epoch. - `delay_seconds(seconds: float) -> None`: Delays the execution for the given number of seconds. - `schedule_absolute(time: float, priority: int, action: Callable, argument: Tuple = (), kwargs: Dict = {}) -> Any`: Schedules an event at the absolute time provided. - `schedule_relative(delay: float, priority: int, action: Callable, argument: Tuple = (), kwargs: Dict = {}) -> Any`: Schedules an event `delay` seconds from now. - `list_events() -> List`: Lists all scheduled events. - `cancel_event(event: Any)`: Cancels the specified event. - `run_events()`: Executes scheduled events based on their scheduled times and priorities. 2. **Constraints:** - Ensure thread safety during event scheduling and execution. - Handle exceptions during event execution without crashing the scheduler. 3. **Performance:** - The scheduler should efficiently manage and execute events even with a large number of scheduled tasks. # Example Usage and Expected Output: ```python import time from typing import Callable, Tuple, List, Dict # Implement the required functions here # Example to demonstrate scheduler def current_time() -> float: return time.time() def delay_seconds(seconds: float) -> None: time.sleep(seconds) def print_message(msg: str, timestamp: float) -> None: print(f\\"Message: {msg} at {timestamp}\\") if __name__ == \\"__main__\\": s = sched.scheduler(timefunc=current_time, delayfunc=delay_seconds) def schedule_and_run(): print(f\\"Current Time: {current_time()}\\") event1 = schedule_relative(5, 1, print_message, argument=(\\"Hello after 5 seconds\\", current_time())) event2 = schedule_absolute(current_time() + 10, 2, print_message, argument=(\\"Hello at absolute time\\", current_time())) list_events() # Should list both events cancel_event(event1) # Cancel the first event list_events() # Should only list the second event run_events() # Should run the remaining events schedule_and_run() ``` # Note: Ensure that all functions and their interactions are well-tested and documented.","solution":"import sched import time from typing import Callable, Tuple, List, Dict, Any scheduler = sched.scheduler(timefunc=time.time, delayfunc=time.sleep) def current_time() -> float: Returns the current time in seconds since the epoch. return time.time() def delay_seconds(seconds: float) -> None: Delays the execution for the given number of seconds. time.sleep(seconds) def schedule_absolute(time: float, priority: int, action: Callable, argument: Tuple = (), kwargs: Dict = {}) -> Any: Schedules an event at the absolute time provided. return scheduler.enterabs(time, priority, action, argument, kwargs) def schedule_relative(delay: float, priority: int, action: Callable, argument: Tuple = (), kwargs: Dict = {}) -> Any: Schedules an event `delay` seconds from now. return scheduler.enter(delay, priority, action, argument, kwargs) def list_events() -> List: Lists all scheduled events. return scheduler.queue def cancel_event(event: Any) -> None: Cancels the specified event. scheduler.cancel(event) def run_events() -> None: Executes scheduled events based on their scheduled times and priorities. Handles exceptions without crashing. while not scheduler.empty(): try: scheduler.run(blocking=False) except Exception as e: print(f\\"Exception occurred: {e}\\")"},{"question":"# Question: Managing Accelerator Devices with PyTorch In this question, you are required to demonstrate your understanding of managing computational accelerators (e.g., GPUs) using the `torch.accelerator` module. You will write a function that performs the following tasks: 1. Checks if an accelerator is available. 2. Retrieves and prints the number of available devices. 3. Sets the device to a specified index (if it\'s within the range of available devices). 4. Retrieves and prints the current device index. 5. Synchronizes the device. Complete the function `manage_accelerator(index)` which takes an integer `index` as input and performs the tasks mentioned above. If the provided `index` is out of range, the function should raise a `ValueError`. Function Signature ```python def manage_accelerator(index: int) -> None: pass ``` Input - `index` (int): The index of the device to be set. Output - None: The function should print the relevant information and raise exceptions as required. Constraints - Use the functions from the `torch.accelerator` module to achieve the desired functionality. - The function should handle scenarios where the specified device index is out of range gracefully by raising a `ValueError`. Example ```python # Assuming the system has 1 accelerator available. manage_accelerator(0) # Output: # Accelerators available: Yes # Number of devices: 1 # Current device index set to: 0 # Device index now: 0 # Device synchronized ``` ```python # Assuming the system has 2 accelerators available. manage_accelerator(3) # Output: # Accelerators available: Yes # Number of devices: 2 # ValueError: Device index out of range ``` Note Make sure you handle synchronization properly by utilizing the `synchronize` function after setting the device.","solution":"import torch def manage_accelerator(index: int) -> None: Manages the accelerator device based on the provided index. Args: - index (int): The index of the device to set. Raises: - ValueError: If the index is out of range of available devices. # Check if any accelerator devices are available if torch.cuda.is_available(): print(\\"Accelerators available: Yes\\") num_devices = torch.cuda.device_count() print(f\\"Number of devices: {num_devices}\\") # Verify that the provided index is within range if index < 0 or index >= num_devices: raise ValueError(\\"Device index out of range\\") # Set the current device torch.cuda.set_device(index) print(f\\"Current device index set to: {index}\\") # Retrieve and print the current device index current_device = torch.cuda.current_device() print(f\\"Device index now: {current_device}\\") # Synchronize the device torch.cuda.synchronize() print(\\"Device synchronized\\") else: print(\\"Accelerators available: No\\")"},{"question":"**Task: Implement Custom Scoring Functions for Model Evaluation** **Context:** You are working on a machine learning project where you need to evaluate a classifier based on various metrics to understand its performance comprehensively. In particular, you need to implement and compare custom scorers using scikit-learn\'s `make_scorer` function and verify the performance of your classifier with cross-validation. **Objective:** - Implement custom scoring functions and integrate them using `make_scorer`. - Perform cross-validation on a dataset using these custom scoring functions and report the results. **Requirements:** 1. **Data:** You may use any available dataset from `sklearn.datasets` for classification (e.g., `load_iris` or `load_breast_cancer`). 2. **Custom Scoring Functions:** Implement the following custom scoring functions using the basic metrics provided in scikit: - `custom_log_loss`: Based on the negative log loss. - `custom_f1_weighted`: Based on F1 score with weighted average. 3. **Integration using `make_scorer`:** Create scorer objects integrating the custom scoring functions using `make_scorer`. 4. **Cross-Validation:** Perform 5-fold cross-validation using a classifier of your choice (e.g., SVC, RandomForestClassifier) and evaluate using: - Default scoring provided by the classifier. - Custom `custom_log_loss`. - Custom `custom_f1_weighted`. 5. **Reporting Results:** Report the cross-validation scores obtained from each scoring function for performance comparison. **Specifications:** - Implement custom scoring functions that adhere to scikit-learn conventions. - Use `make_scorer` to create scorer objects. - Perform cross-validation using `cross_val_score` or similar function. - Report the results in a clear, structured format. **Starter Code:** Here is a scaffold to get you started: ```python from sklearn.datasets import load_iris from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer, f1_score, log_loss from sklearn.svm import SVC # Load dataset data = load_iris() X, y = data.data, data.target # Define custom scoring functions def custom_log_loss(y_true, y_pred_proba): return log_loss(y_true, y_pred_proba) def custom_f1_weighted(y_true, y_pred): return f1_score(y_true, y_pred, average=\'weighted\') # Create custom scorer objects log_loss_scorer = make_scorer(custom_log_loss, needs_proba=True, greater_is_better=False) f1_weighted_scorer = make_scorer(custom_f1_weighted) # Initialize classifier clf = SVC(probability=True, random_state=42) # Perform cross-validation using default, custom log_loss, and custom f1_weighted scorers default_scores = cross_val_score(clf, X, y, cv=5) log_loss_scores = cross_val_score(clf, X, y, cv=5, scoring=log_loss_scorer) f1_weighted_scores = cross_val_score(clf, X, y, cv=5, scoring=f1_weighted_scorer) # Report results print(\\"Default scoring:\\", default_scores) print(\\"Custom log loss scoring:\\", log_loss_scores) print(\\"Custom F1 weighted scoring:\\", f1_weighted_scores) ``` **Bonus:** - Visualize the distribution of the cross-validation scores using a boxplot or other suitable visualization. **Output:** Ensure your solution: - Clearly defines and implements the custom scoring functions. - Correctly integrates these custom scorers with `make_scorer`. - Reports and compares cross-validation results for the different scoring methods.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import cross_val_score from sklearn.metrics import make_scorer, f1_score, log_loss from sklearn.svm import SVC # Load dataset data = load_iris() X, y = data.data, data.target # Define custom scoring functions def custom_log_loss(y_true, y_pred_proba): Custom log loss scoring function. return log_loss(y_true, y_pred_proba) def custom_f1_weighted(y_true, y_pred): Custom F1 score with weighted average. return f1_score(y_true, y_pred, average=\'weighted\') # Create custom scorer objects log_loss_scorer = make_scorer(custom_log_loss, needs_proba=True, greater_is_better=False) f1_weighted_scorer = make_scorer(custom_f1_weighted) # Initialize classifier clf = SVC(probability=True, random_state=42) # Perform cross-validation using default, custom log_loss, and custom f1_weighted scorers default_scores = cross_val_score(clf, X, y, cv=5) log_loss_scores = cross_val_score(clf, X, y, cv=5, scoring=log_loss_scorer) f1_weighted_scores = cross_val_score(clf, X, y, cv=5, scoring=f1_weighted_scorer) # Report results def report_results(): print(\\"Default scoring:\\", default_scores) print(\\"Custom log loss scoring:\\", log_loss_scores) print(\\"Custom F1 weighted scoring:\\", f1_weighted_scores) report_results()"},{"question":"**Question: Log Parsing and Analysis** You are tasked to design a log analysis tool in Python. The tool will read log entries from a server, extract specific information using regular expressions, and categorize messages based on their severity levels. This exercise will require you to understand and use Python\'s regular expressions and file handling capabilities. **Specifications:** 1. **Log Entry Format:** Each log entry is on a new line and follows the format: ``` YYYY-MM-DD HH:MM:SS [SEVERITY] Message ``` - `YYYY-MM-DD` represents the date. - `HH:MM:SS` represents the time. - `SEVERITY` is one of `INFO`, `WARNING`, `ERROR`, or `CRITICAL`. - `Message` is the log message text. 2. **Function Requirements:** You need to implement a function `parse_log_file(file_path: str) -> dict`: - The function should read the log file from the given `file_path`. - Parse each log entry to extract date, time, severity, and message. - Organize the parsed information into a dictionary with the following structure: ```python { \\"INFO\\": [ {\\"datetime\\": <datetime>, \\"message\\": <str>} ], \\"WARNING\\": [ {\\"datetime\\": <datetime>, \\"message\\": <str>} ], \\"ERROR\\": [ {\\"datetime\\": <datetime>, \\"message\\": <str>} ], \\"CRITICAL\\": [ {\\"datetime\\": <datetime>, \\"message\\": <str>} ] } ``` Where `<datetime>` is a `datetime` object representing the combined date and time from the log entry. 3. **Constraints:** - Assume that the log file fits into memory. - Handle any non-conforming log lines gracefully by skipping them and continue processing the remaining lines. - Your solution should be efficient in terms of both time and space complexity. 4. **Performance Requirements:** - The function should be able to process a log file with up to 1 million lines within a reasonable time frame (e.g., under 30 seconds). **Example Usage:** Given a log file `server.log` with the following content: ``` 2023-10-01 12:00:00 [INFO] Server started 2023-10-01 12:05:00 [WARNING] High memory usage detected 2023-10-01 12:10:00 [ERROR] Failed to connect to the database 2023-10-01 12:15:00 [CRITICAL] System out of memory ``` Calling `parse_log_file(\\"server.log\\")` should return: ```python { \\"INFO\\": [ {\\"datetime\\": datetime(2023, 10, 1, 12, 0), \\"message\\": \\"Server started\\"} ], \\"WARNING\\": [ {\\"datetime\\": datetime(2023, 10, 1, 12, 5), \\"message\\": \\"High memory usage detected\\"} ], \\"ERROR\\": [ {\\"datetime\\": datetime(2023, 10, 1, 12, 10), \\"message\\": \\"Failed to connect to the database\\"} ], \\"CRITICAL\\": [ {\\"datetime\\": datetime(2023, 10, 1, 12, 15), \\"message\\": \\"System out of memory\\"} ] } ``` **Note:** - You may use Python\'s built-in `re` and `datetime` modules for regular expressions and datetime manipulation respectively. **Hints:** - Use regular expressions to identify and parse the components of each log entry. - Handle exceptions to ensure that your function continues processing even if there are lines that do not match the expected log format.","solution":"import re from datetime import datetime def parse_log_file(file_path: str) -> dict: log_data = { \\"INFO\\": [], \\"WARNING\\": [], \\"ERROR\\": [], \\"CRITICAL\\": [] } log_entry_pattern = re.compile( r\\"^(?P<date>d{4}-d{2}-d{2}) (?P<time>d{2}:d{2}:d{2}) [(?P<severity>INFO|WARNING|ERROR|CRITICAL)] (?P<message>.+)\\" ) with open(file_path, \'r\') as file: for line in file: match = log_entry_pattern.match(line.strip()) if match: date_time_str = f\\"{match.group(\'date\')} {match.group(\'time\')}\\" date_time = datetime.strptime(date_time_str, \\"%Y-%m-%d %H:%M:%S\\") severity = match.group(\'severity\') message = match.group(\'message\') log_data[severity].append({\\"datetime\\": date_time, \\"message\\": message}) return log_data"},{"question":"# HTML Entity Converter To test your understanding of the `html.entities` module in Python 3.10, you need to implement a set of functions to convert between HTML entities and Unicode characters. Specifically, you will create: 1. A function to replace all HTML named entities in a string with their corresponding Unicode characters. 2. A function to replace all occurrences of special Unicode characters in a string with their corresponding HTML named entities. # Requirements: 1. **Function 1: `html_entities_to_unicode(input_string: str) -> str`** - **Input:** A string that may contain HTML named entities. - **Output:** A string where all HTML named entities are replaced with their corresponding Unicode characters. - **Constraints:** Use the `html5` dictionary from the `html.entities` module to perform the replacements. 2. **Function 2: `unicode_to_html_entities(input_string: str) -> str`** - **Input:** A string that may contain special Unicode characters. - **Output:** A string where special Unicode characters are replaced with their corresponding HTML named entities. - **Constraints:** Use the `codepoint2name` dictionary from the `html.entities` module to perform the conversions. # Example: ```python # Example input for Function 1 input_str_1 = \\"Hello &amp; welcome to the world of Python &gt; 3.10!\\" # Expected output: \\"Hello & welcome to the world of Python > 3.10!\\" # Example input for Function 2 input_str_2 = \\"Hello & welcome to the world of Python > 3.10!\\" # Expected output: \\"Hello &amp; welcome to the world of Python &gt; 3.10!\\" ``` # Implementation Notes: - Handle both cases where entities contain or omit the trailing semicolon. For example, both `&gt;` and `&gt` should be converted to `>`. - Ensure that your implementation efficiently handles long strings. # Skeleton Code: ```python from html.entities import html5, codepoint2name def html_entities_to_unicode(input_string: str) -> str: pass # Your code here def unicode_to_html_entities(input_string: str) -> str: pass # Your code here # Example usage (uncomment to test your functions): # print(html_entities_to_unicode(\\"Hello &amp; welcome to the world of Python &gt; 3.10!\\")) # print(unicode_to_html_entities(\\"Hello & welcome to the world of Python > 3.10!\\")) ``` Implement the functions and test them with various cases to ensure they handle different HTML entities and Unicode characters appropriately.","solution":"from html.entities import html5, codepoint2name def html_entities_to_unicode(input_string: str) -> str: result = input_string for entity, char in html5.items(): result = result.replace(f\\"&{entity};\\", char) result = result.replace(f\\"&{entity}\\", char) # handle case without semicolon return result def unicode_to_html_entities(input_string: str) -> str: result = input_string for codepoint, name in codepoint2name.items(): char = chr(codepoint) result = result.replace(char, f\\"&{name};\\") return result"},{"question":"# Coding Assessment: Partial Dependence Plots and ICE Plots using Scikit-learn **Objective**: Demonstrate your understanding of Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots in the context of feature analysis using Scikit-learn. **Problem Statement**: You are given a dataset and you are required to analyze the effect of certain features on the target variable using Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots. Specifically, you need to: 1. Train a Gradient Boosting Classifier on the provided dataset. 2. Create one-way and two-way PDPs for selected features. 3. Create ICE plots for the same set of features. 4. Combine PDPs and ICE plots to observe both the average and individual effects of the features. **Instructions**: 1. **Data Preparation**: - Load the provided dataset. - Split the dataset into training and testing sets. 2. **Model Training**: - Train a Gradient Boosting Classifier on the training data. 3. **PDP and ICE Plotting**: - Create one-way PDPs for two selected features. - Create a two-way PDP for the interaction between the two selected features. - Create ICE plots for the two selected features. - Combine PDPs and ICE plots to visualize both average and individual effects. **Dataset**: You can use any relevant dataset (e.g., the Iris dataset or the Bike Sharing dataset used in the documentation). **Expected Input and Output Formats**: - **Input**: - `X_train`: Training feature matrix. - `y_train`: Training target vector. - `X_test`: Testing feature matrix. - `y_test`: Testing target vector. - `features`: List of features to analyze (e.g., [0, 1]). - **Output**: - Matplotlib figures showing the following: - One-way PDPs for the selected features. - Two-way PDP for the interaction between the selected features. - ICE plots for the selected features. - Combined PDPs and ICE plots. **Constraints**: - Use Gradient Boosting Classifier for training. - Use the `PartialDependenceDisplay.from_estimator` function for plotting. **Performance Requirements**: - Ensure the code runs efficiently for datasets of moderate size. **Example Code**: ```python import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay from sklearn.model_selection import train_test_split # Load the dataset data = load_iris() X, y = data.data, data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Gradient Boosting Classifier clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42) clf.fit(X_train, y_train) # Features to analyze features = [0, 2, (0, 2)] # Create PDP and ICE plots disp = PartialDependenceDisplay.from_estimator(clf, X_train, features) plt.show() # Create ICE plots only disp = PartialDependenceDisplay.from_estimator(clf, X_train, features, kind=\'individual\') plt.show() # Combine PDPs and ICE plots disp = PartialDependenceDisplay.from_estimator(clf, X_train, features, kind=\'both\') plt.show() ``` **Note**: Use this example as a guideline to implement your solution with the provided dataset.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay from sklearn.model_selection import train_test_split def load_and_split_data(): data = load_iris() X, y = data.data, data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def train_model(X_train, y_train): clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42) clf.fit(X_train, y_train) return clf def create_plots(clf, X_train, features): fig, ax = plt.subplots(figsize=(12, 8)) # One-way PDPs for the selected features PartialDependenceDisplay.from_estimator(clf, X_train, features[:2], ax=ax) plt.title(\\"One-way PDPs\\") plt.show() # Two-way PDP for the interaction between the selected features fig, ax = plt.subplots(figsize=(12, 8)) PartialDependenceDisplay.from_estimator(clf, X_train, [features[2]], ax=ax) plt.title(\\"Two-way PDP\\") plt.show() # ICE plots for the selected features fig, ax = plt.subplots(figsize=(12, 8)) PartialDependenceDisplay.from_estimator(clf, X_train, features[:2], ax=ax, kind=\'individual\') plt.title(\\"ICE Plots\\") plt.show() # Combine PDPs and ICE plots into a unified plot fig, ax = plt.subplots(figsize=(12, 8)) PartialDependenceDisplay.from_estimator(clf, X_train, features[:2], ax=ax, kind=\'both\') plt.title(\\"Combined PDPs and ICE plots\\") plt.show() # Main flow if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = load_and_split_data() clf = train_model(X_train, y_train) features = [0, 2, (0, 2)] create_plots(clf, X_train, features)"},{"question":"Filesystem Navigator You are tasked with implementing a Python program that navigates a filesystem and performs several operations using the `pathlib` module. Task Implement the following functions: 1. **list_directory_contents(dir_path: str) -> list**: - Given a directory path as input, return a list of all files and directories in the specified directory using their names only (not the full path). - Use the `Path.iterdir()` method. 2. **find_python_files(dir_path: str) -> list**: - Given a directory path as input, search for all `.py` files in the specified directory and all its subdirectories. - Use the `Path.glob(\'**/*.py\')` method. - Return a list of full paths of the found Python files. 3. **check_path_properties(path_str: str) -> dict**: - Given a path as a string, return a dictionary with boolean values indicating whether the path is a file, directory, symlink, and whether it exists on the filesystem. - Use the `Path` methods: `is_file()`, `is_dir()`, `is_symlink()`, and `exists()`. 4. **read_file_contents(file_path: str) -> str**: - Given a file path, read the entire contents of the file and return it as a string. - Handle any potential exceptions by returning an error message string. - Use the `Path.read_text()` method. 5. **write_to_file(file_path: str, contents: str) -> None**: - Given a file path and a string of contents, write the contents to the file. If the file already exists, it should be overwritten. - Use the `Path.write_text()` method. Constraints - Assume that the given paths are always valid inputs to the `Path` class constructor. - You must use functions provided by the `pathlib` module to accomplish the tasks. - For the `find_python_files` function, if there are no `.py` files, return an empty list. Example Usage ```python # Example directory structure # test_dir/ # ├── file1.txt # ├── file2.py # └── subdir # └── file3.py # list_directory_contents print(list_directory_contents(\'test_dir\')) # Output: [\'file1.txt\', \'file2.py\', \'subdir\'] # find_python_files print(find_python_files(\'test_dir\')) # Output: [\'test_dir/file2.py\', \'test_dir/subdir/file3.py\'] # check_path_properties print(check_path_properties(\'test_dir/file2.py\')) # Output: {\'is_file\': True, \'is_dir\': False, \'is_symlink\': False, \'exists\': True} # read_file_contents print(read_file_contents(\'test_dir/file1.txt\')) # Output: \'Contents of file1.txt\' # write_to_file write_to_file(\'test_dir/new_file.txt\', \'Hello, World!\') print(read_file_contents(\'test_dir/new_file.txt\')) # Output: \'Hello, World!\' ``` Implement all the functions and ensure they work as specified.","solution":"from pathlib import Path def list_directory_contents(dir_path: str) -> list: Given a directory path, return a list of all files and directories in the specified directory. p = Path(dir_path) return [item.name for item in p.iterdir()] def find_python_files(dir_path: str) -> list: Given a directory path, search for all .py files in the specified directory and all its subdirectories. p = Path(dir_path) return [str(file) for file in p.glob(\'**/*.py\')] def check_path_properties(path_str: str) -> dict: Given a path as a string, return a dictionary with boolean values indicating path properties. p = Path(path_str) return { \'is_file\': p.is_file(), \'is_dir\': p.is_dir(), \'is_symlink\': p.is_symlink(), \'exists\': p.exists() } def read_file_contents(file_path: str) -> str: Given a file path, read the entire contents of the file and return it as a string. p = Path(file_path) try: return p.read_text() except Exception as e: return str(e) def write_to_file(file_path: str, contents: str) -> None: Given a file path and a string of contents, write the contents to the file. p = Path(file_path) p.write_text(contents)"},{"question":"**IMAP Client Functionalities Assessment** **Objective:** Implement a function that connects to an IMAP server, logs in with provided credentials, searches for all emails in a specified mailbox, fetches their subjects, and returns a list of these subjects. **Function Signature:** ```python def fetch_email_subjects(imap_host: str, email: str, password: str, mailbox: str = \'INBOX\') -> list: pass ``` **Input:** - `imap_host` (str): The IMAP server hostname. - `email` (str): The email address (login username). - `password` (str): The password for the email account. - `mailbox` (str): The mailbox to search within. Defaults to \'INBOX\'. **Output:** - Returns a list of strings where each string is the subject of an email in the specified mailbox. **Constraints:** - Raise an appropriate exception if the connection to the IMAP server fails. - The function should handle various potential exceptions raised during login, selection of the mailbox, searching, and fetching email data. **Example:** ```python imap_host = \'imap.example.com\' email = \'user@example.com\' password = \'password123\' subjects = fetch_email_subjects(imap_host, email, password) print(subjects) ``` **Guidelines:** 1. Use the `imaplib.IMAP4_SSL` class to establish a secure connection to the IMAP server. 2. Log in using the provided email and password. 3. Select the specified mailbox (e.g., \'INBOX\'). 4. Search for all emails in that mailbox. 5. Fetch the \'SUBJECT\' of each email. 6. Return a list of email subjects. Consider edge cases such as: - What happens if the mailbox is empty? - How to handle potential connection issues or authentication failures? - Ensure your implementation does not expose the user\'s password in case of an exception. Below is a skeleton to get you started: ```python import imaplib import email def fetch_email_subjects(imap_host: str, email_address: str, password: str, mailbox: str = \'INBOX\') -> list: subjects = [] try: with imaplib.IMAP4_SSL(imap_host) as M: M.login(email_address, password) M.select(mailbox) typ, message_numbers = M.search(None, \'ALL\') if typ != \'OK\': raise Exception(\\"Failed to search emails.\\") for num in message_numbers[0].split(): typ, msg_data = M.fetch(num, \'(BODY[HEADER.FIELDS (SUBJECT)])\') if typ != \'OK\': raise Exception(f\\"Failed to fetch email number {num}.\\") msg = email.message_from_bytes(msg_data[0][1]) subjects.append(msg.get(\'Subject\')) M.logout() except imaplib.IMAP4.error as e: raise Exception(f\\"IMAP error occurred: {e}\\") except Exception as e: raise Exception(f\\"An error occurred: {e}\\") return subjects ```","solution":"import imaplib import email def fetch_email_subjects(imap_host: str, email_address: str, password: str, mailbox: str = \'INBOX\') -> list: subjects = [] try: with imaplib.IMAP4_SSL(imap_host) as M: M.login(email_address, password) M.select(mailbox) status, message_numbers = M.search(None, \'ALL\') if status != \'OK\': raise Exception(\\"Failed to search emails.\\") for num in message_numbers[0].split(): status, msg_data = M.fetch(num, \'(BODY[HEADER.FIELDS (SUBJECT)])\') if status != \'OK\': raise Exception(f\\"Failed to fetch email number {num}.\\") msg = email.message_from_bytes(msg_data[0][1]) subjects.append(msg.get(\'Subject\')) M.logout() except imaplib.IMAP4.error as e: raise Exception(f\\"IMAP error occurred: {e}\\") except Exception as e: raise Exception(f\\"An error occurred: {e}\\") return subjects"},{"question":"**Thread Management System** # Objective: You are to create a class that simulates a thread management system using the `_thread` module\'s low-level threading API. The goal is to manage multiple threads performing specific tasks and synchronize them using locks. # Task: Implement a class `ThreadManager` with the following functionalities: 1. **Initialize**: Initialize the `ThreadManager` object with a specified number of worker threads (`num_workers`). 2. **Start Workers**: Start the worker threads using a provided task function. 3. **Synchronize Threads**: Ensure that no two threads run the critical section of the code at the same time by using a lock. 4. **Handle Thread Completion**: Ensure all threads complete their tasks before the program continues. # Specifications: - **Class Name**: `ThreadManager` - **Methods**: - `__init__(self, num_workers: int)`: Constructor to initialize the number of worker threads and instantiate a lock. - `start_workers(self, task_function)`: Starts the worker threads, each executing the provided `task_function`. The `task_function` should be called with a unique thread identifier as its argument. - `wait_for_completion(self)`: Ensures the main thread waits until all worker threads have completed. # Constraints: - Use `_thread` for threading operations and `_thread.allocate_lock` for lock management. - Each worker thread should print its identifier before and after executing the critical section. # Example Usage: ```python import _thread import time class ThreadManager: def __init__(self, num_workers: int): self.num_workers = num_workers self.lock = _thread.allocate_lock() def start_workers(self, task_function): self.threads = [] for i in range(self.num_workers): _thread.start_new_thread(self.worker_thread, (i, task_function)) def worker_thread(self, thread_id, task_function): print(f\\"Thread {thread_id} starting\\") self.lock.acquire() try: task_function(thread_id) finally: self.lock.release() print(f\\"Thread {thread_id} finished\\") def wait_for_completion(self): main_lock = _thread.allocate_lock() main_lock.acquire() while any([_thread.get_ident() for _ in range(self.num_workers)]): time.sleep(0.1) main_lock.release() # Task function example def example_task(thread_id): print(f\\"Thread {thread_id} is running the critical section.\\") time.sleep(1) # Usage if __name__ == \'__main__\': manager = ThreadManager(num_workers=3) manager.start_workers(example_task) manager.wait_for_completion() print(\\"All threads have completed.\\") ``` # Expected Output: ``` Thread 0 starting Thread 1 starting Thread 2 starting Thread 0 is running the critical section. Thread 0 finished Thread 1 is running the critical section. Thread 1 finished Thread 2 is running the critical section. Thread 2 finished All threads have completed. ``` Consider thread-safe operations and proper synchronization. Delays can be introduced to simulate time-consuming tasks if needed.","solution":"import _thread import time class ThreadManager: def __init__(self, num_workers: int): self.num_workers = num_workers self.lock = _thread.allocate_lock() self.threads_completed = 0 self.completion_lock = _thread.allocate_lock() def start_workers(self, task_function): for i in range(self.num_workers): _thread.start_new_thread(self.worker_thread, (i, task_function)) self.wait_for_completion() def worker_thread(self, thread_id, task_function): print(f\\"Thread {thread_id} starting\\") self.lock.acquire() try: task_function(thread_id) finally: self.lock.release() print(f\\"Thread {thread_id} finished\\") self._mark_thread_completed() def _mark_thread_completed(self): self.completion_lock.acquire() try: self.threads_completed += 1 finally: self.completion_lock.release() def wait_for_completion(self): while self.threads_completed < self.num_workers: time.sleep(0.1) # Task function example def example_task(thread_id): print(f\\"Thread {thread_id} is running the critical section.\\") time.sleep(1) # Usage if __name__ == \'__main__\': manager = ThreadManager(num_workers=3) manager.start_workers(example_task) print(\\"All threads have completed.\\")"},{"question":"You are provided with two datasets: `dowjones` and `fmri`. Your task is to create a plot that combines multiple Seaborn functionalities using the objects API. The goal is to create a meaningful visualization of the data. Datasets: 1. **`dowjones`** dataset contains: - `Date`: Dates of stock prices. - `Price`: Closing stock prices. 2. **`fmri`** dataset contains: - `timepoint`: Time points at which brain activity is recorded. - `signal`: Signal recorded from the brain. - `region`: Region of the brain. - `event`: Type of event. - `subject`: Identifier for the subject. Task: 1. Load the `dowjones` and `fmri` datasets using `seaborn.load_dataset`. 2. Create a line plot of the `dowjones` dataset showing `Date` on the X-axis and `Price` on the Y-axis. 3. Create a subset of the `fmri` dataset to include only observations where the region is `parietal` and event is `stim`. 4. Plot the signals over time (signal vs. timepoint) for different subjects using different lines. 5. Add a band to represent the standard error for the signal, grouped by event. 6. Add markers to the line plot points to indicate where the data was sampled. 7. Use different colors for different subjects in the `fmri` plot. # Requirements: - Use `seaborn.objects` API to create the plots. - Combine all plots into a single figure using subplots from `matplotlib`. - Properly label your axes and provide a legend for clarity. - Ensure the plot is aesthetically pleasing and informative. # Input: - None. You will load the data directly using seaborn. # Output: - A single figure with the required plots. # Constraints: - You must use the `seaborn.objects` API. - Use efficient code practices to handle data manipulation and plotting. ```python # Your code starts here import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Create the figure and axis fig, ax = plt.subplots(2, 1, figsize=(10, 12)) # Plot 1: Dow Jones Line Plot so.Plot(dowjones, \\"Date\\", \\"Price\\").add(so.Line()).on(ax[0]) ax[0].set_title(\\"Dow Jones Closing Prices Over Time\\") # Subsetting fmri dataset fmri_subset = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Plot 2: fMRI signal over time with error bands and markers p = so.Plot(fmri_subset, \\"timepoint\\", \\"signal\\", color=\\"subject\\") p.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\", linewidth=1), group=\\"subject\\").add(so.Band(), so.Est(), group=\\"event\\").on(ax[1]) ax[1].set_title(\\"fMRI Signal over Time\\") # Show the plot plt.tight_layout() plt.show() # Your code ends here ```","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_combined_plot(): # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Create the figure and axis fig, ax = plt.subplots(2, 1, figsize=(15, 12)) # Plot 1: Dow Jones Line Plot so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\").add(so.Line()).on(ax[0]) ax[0].set_title(\\"Dow Jones Closing Prices Over Time\\") ax[0].set_xlabel(\\"Date\\") ax[0].set_ylabel(\\"Price\\") # Subsetting fmri dataset fmri_subset = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Plot 2: fMRI signal over time with error bands and markers p = so.Plot(fmri_subset, x=\\"timepoint\\", y=\\"signal\\", color=\\"subject\\") p.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\", linewidth=1), group=\\"subject\\").add(so.Band(), so.Est(), group=\\"event\\").on(ax[1]) ax[1].set_title(\\"fMRI Signal over Time\\") ax[1].set_xlabel(\\"Timepoint\\") ax[1].set_ylabel(\\"Signal\\") # Show the plot plt.tight_layout() plt.show() # Run the function to create plot create_combined_plot()"},{"question":"# HTML Utility Function Implementation **Problem Statement:** You are tasked with creating a utility to sanitize HTML inputs and outputs for a web application. Specifically, you need to: 1. Implement a function `sanitize_html_input(s: str) -> str` that will escape special characters in HTML to prevent XSS (Cross-site Scripting) attacks. 2. Implement a function `restore_html(s: str) -> str` that will unescape escaped HTML sequences back to their original form. You should handle the following characters: - `&` should be replaced by `&amp;` - `<` should be replaced by `&lt;` - `>` should be replaced by `&gt;` - `\\"` should be replaced by `&quot;` - `\'` should be replaced by `&#x27;` **Function Signatures:** ```python def sanitize_html_input(s: str) -> str: # Implement this function def restore_html(s: str) -> str: # Implement this function ``` **Input Format:** - `sanitize_html_input(s: str) -> str`: A string `s` of length `1 <= len(s) <= 10^5`. - `restore_html(s: str) -> str`: A string `s` of length `1 <= len(s) <= 10^5` that contains HTML-safe sequences. **Output Format:** - `sanitize_html_input(s: str) -> str`: Returns a string with special characters replaced by their corresponding HTML-safe sequences. - `restore_html(s: str) -> str`: Returns a string with HTML-safe sequences replaced by their corresponding special characters. **Constraints:** - You cannot use the built-in `html.escape` and `html.unescape` functions. Implement your own logic. - The functions must handle large inputs efficiently. **Performance Requirements:** - The implementation should ensure that both functions run in linear time, O(n). **Example:** ```python # Example input for sanitizing s = \'This is a test case: <script>alert(\\"XSS\\")<\/script>\' print(sanitize_html_input(s)) # Output: \'This is a test case: &lt;script&gt;alert(&quot;XSS&quot;)&lt;/script&gt;\' # Example input for restoring t = \'This is a test case: &lt;script&gt;alert(&quot;XSS&quot;)&lt;/script&gt;\' print(restore_html(t)) # Output: \'This is a test case: <script>alert(\\"XSS\\")<\/script>\' ```","solution":"def sanitize_html_input(s: str) -> str: Returns the sanitized version of the input string where HTML special characters are replaced by their corresponding HTML-safe sequences. return s.replace(\'&\', \'&amp;\').replace(\'<\', \'&lt;\').replace(\'>\', \'&gt;\').replace(\'\\"\', \'&quot;\').replace(\\"\'\\", \'&#x27;\') def restore_html(s: str) -> str: Returns the restored version of the input string where HTML-safe sequences are replaced by their corresponding special characters. return s.replace(\'&lt;\', \'<\').replace(\'&gt;\', \'>\').replace(\'&quot;\', \'\\"\').replace(\'&#x27;\', \\"\'\\").replace(\'&amp;\', \'&\')"},{"question":"Introduction Imagine you are given a noisy one-dimensional signal and you need to transform it to the frequency domain to filter out the noise and then transform it back to the time domain using PyTorch\'s `torch.fft` module. The goal of this challenge is to demonstrate your understanding of a basic signal processing pipeline using Fast Fourier Transform (FFT) operations. Problem Statement You are required to implement a function `denoise_signal` that performs the following tasks: 1. Compute the one-dimensional discrete Fourier transform (FFT) of a given noisy signal. 2. Zero out all frequencies in the Fourier transform that are beyond a specified threshold. 3. Compute the inverse FFT to transform the filtered signal back to the time domain. 4. Return the denoised signal. Function Signature ```python import torch def denoise_signal(noisy_signal: torch.Tensor, frequency_threshold: float) -> torch.Tensor: Denoise a one-dimensional signal by performing an FFT, filtering, and inverse FFT. Parameters: noisy_signal (torch.Tensor): A 1D tensor representing the noisy signal in the time domain. frequency_threshold (float): A threshold frequency. Frequencies in the FFT output greater than this value will be zeroed out. Returns: torch.Tensor: The denoised signal in the time domain. pass ``` Input - `noisy_signal`: A 1-dimensional PyTorch tensor of shape `(N,)` representing the noisy signal in the time domain. - `frequency_threshold`: A float representing the threshold frequency. Frequencies higher than this will be set to zero. Output - Returns a 1-dimensional PyTorch tensor of shape `(N,)` representing the denoised signal in the time domain. Constraints - The length `N` of the noisy signal tensor will be between 1 and 10^6. - The `frequency_threshold` will be a non-negative float less than or equal to ( frac{N}{2} ). Example ```python import torch # Example signal (sine wave + noise) noisy_signal = torch.tensor([0.0, 0.30902, 0.58779, 0.80902, 0.95106, 1.0, 0.95106, 0.80902, 0.58779, 0.30902, 0.0, -0.30902, -0.58779, -0.80902, -0.95106, -1.0, -0.95106, -0.80902, -0.58779, -0.30902]) # Frequency threshold frequency_threshold = 5.0 # Denoising the signal denoised_signal = denoise_signal(noisy_signal, frequency_threshold) print(denoised_signal) ``` Your Task Implement the `denoise_signal` function to complete the signal processing pipeline as described.","solution":"import torch def denoise_signal(noisy_signal: torch.Tensor, frequency_threshold: float) -> torch.Tensor: Denoise a one-dimensional signal by performing an FFT, filtering, and inverse FFT. Parameters: noisy_signal (torch.Tensor): A 1D tensor representing the noisy signal in the time domain. frequency_threshold (float): A threshold frequency. Frequencies in the FFT output greater than this value will be zeroed out. Returns: torch.Tensor: The denoised signal in the time domain. # Perform FFT on the noisy signal signal_fft = torch.fft.fft(noisy_signal) # Calculate the frequency bins num_samples = noisy_signal.size(0) freqs = torch.fft.fftfreq(num_samples) # Create a mask to zero out frequencies higher than the threshold mask = torch.abs(freqs) <= frequency_threshold / (num_samples / 2) # Apply the mask to the FFT result filtered_fft = signal_fft * mask # Perform inverse FFT to get the denoised signal denoised_signal = torch.fft.ifft(filtered_fft) return torch.real(denoised_signal)"},{"question":"You are asked to write a Python script that simulates a simplified version of some of IDLE\'s functionalities. Implement a text editor that allows the user to: 1. **Create a New File**: Initialize an empty file. 2. **Open an Existing File**: Read and display contents of the specified file. 3. **Save File**: Save the current text to a specified file. 4. **Edit the Text**: Provide functionalities to cut, copy, paste, find, and replace text within the editor. 5. **Run a Python Code**: Execute the current content as Python code and display the output or errors. **Input/Output Specifications:** 1. **New File**: `new_file()` - Input: No input parameters. - Output: Initializes an empty text editor state. 2. **Open File**: `open_file(file_path: str)` - Input: `file_path` is the path of the file to be opened. - Output: Reads and returns the content of the file. 3. **Save File**: `save_file(file_path: str, content: str)` - Input: `file_path` is where the file will be saved, and `content` is the text content to save. - Output: Saves the content to the file at the given path. 4. **Cut/Copy/Paste Text**: Implement basic cut, copy, and paste functionalities. - Methods: `cut_text(selection_start: int, selection_end: int)`, `copy_text(selection_start: int, selection_end: int)`, `paste_text(position: int)` - Example Input/Output: - `cut_text(5, 10)` - `copy_text(5, 10)` - `paste_text(15)` 5. **Find and Replace Text**: Implement find and replace text functionalities. - Methods: `find_text(query: str)`, `replace_text(find_query: str, replace_with: str)` - Example Input/Output: - `find_text(\'import\')` - `replace_text(\'import sys\', \'import os\')` 6. **Run Python Code**: Run the current text content as Python code. - Methods: `run_code(code: str)` - Output: Display the result of the code execution or error messages. **Constraints:** 1. Assume the input for file paths and text content does not exceed typical file sizes. 2. Handle file reading/writing operations with proper exceptions. 3. Ensure text operations maintain the integrity of the text content. Implement the respective functions and integrate them into a simple console-based text editor. **Bonus**: Implement a simple graphical interface using Tkinter for these functionalities.","solution":"class TextEditor: def __init__(self): self.text = \\"\\" self.clipboard = \\"\\" def new_file(self): self.text = \\"\\" def open_file(self, file_path): try: with open(file_path, \'r\') as file: self.text = file.read() return self.text except FileNotFoundError: return \\"File not found.\\" except IOError: return \\"Error while opening the file.\\" def save_file(self, file_path): try: with open(file_path, \'w\') as file: file.write(self.text) return \\"File saved successfully.\\" except IOError: return \\"Error while saving the file.\\" def cut_text(self, selection_start: int, selection_end: int): self.clipboard = self.text[selection_start:selection_end] self.text = self.text[:selection_start] + self.text[selection_end:] def copy_text(self, selection_start: int, selection_end: int): self.clipboard = self.text[selection_start:selection_end] def paste_text(self, position: int): self.text = self.text[:position] + self.clipboard + self.text[position:] def find_text(self, query: str): return [i for i in range(len(self.text)) if self.text.startswith(query, i)] def replace_text(self, find_query: str, replace_with: str): self.text = self.text.replace(find_query, replace_with) def run_code(self): try: exec(self.text) except Exception as e: return f\\"Error: {e}\\" # Initialize the editor (this won\'t be part of the final module, it\'s just for the interactive session) if __name__ == \\"__main__\\": editor = TextEditor() editor.new_file() editor.text = \\"print(\'Hello World\')\\" editor.save_file(\'test.py\') print(editor.open_file(\'test.py\')) print(editor.run_code())"},{"question":"Coding Assessment Question # Objective To assess the understanding of hierarchical indexing (MultiIndex) and advanced indexing features in pandas. # Problem Statement: You are provided with sales data of a company that spans across various regions, stores, and dates. Your task is to create a MultiIndex DataFrame and implement a function to perform specific data analysis tasks using advanced indexing techniques. # Dataset You have the following sales data as a dictionary: ```python data = { \'Region\': [\'East\', \'East\', \'East\', \'East\', \'West\', \'West\', \'West\', \'West\'], \'Store\': [\'A\', \'A\', \'B\', \'B\', \'A\', \'A\', \'B\', \'B\'], \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\'], \'Sales\': [200, 210, 150, 160, 300, 320, 220, 230] } ``` # Tasks 1. Create a DataFrame from the dictionary `data` and set the index to be a MultiIndex using `Region`, `Store`, and `Date`. 2. Implement the function `analyze_sales(df)` to perform the following tasks: - Calculate and return the total sales for each region. - Calculate and return the sales for store \'A\' across all dates and regions. - Calculate and return the sales on \'2023-01-02\' for all regions and stores. - Calculate and return the sales from \'2023-01-01\' to \'2023-01-02\' for the \'East\' region. # Constraints - Use pandas version 1.3.3 or later. - Ensure that the function is optimized for performance. # Function Signature ```python import pandas as pd def analyze_sales(df: pd.DataFrame) -> dict: # Your code here return { \\"total_sales_per_region\\": total_sales_per_region, \\"sales_store_A\\": sales_store_A, \\"sales_20230102\\": sales_20230102, \\"sales_east_0101_to_0102\\": sales_east_0101_to_0102, } # Example usage: data = { \'Region\': [\'East\', \'East\', \'East\', \'East\', \'West\', \'West\', \'West\', \'West\'], \'Store\': [\'A\', \'A\', \'B\', \'B\', \'A\', \'A\', \'B\', \'B\'], \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\'], \'Sales\': [200, 210, 150, 160, 300, 320, 220, 230] } df = pd.DataFrame(data) df.set_index([\'Region\', \'Store\', \'Date\'], inplace=True) result = analyze_sales(df) print(result) ``` # Expected Output ```python { \\"total_sales_per_region\\": { \'East\': 720, \'West\': 1070 }, \\"sales_store_A\\": { (\'East\', \'2023-01-01\'): 200, (\'East\', \'2023-01-02\'): 210, (\'West\', \'2023-01-01\'): 300, (\'West\', \'2023-01-02\'): 320 }, \\"sales_20230102\\": { (\'East\', \'A\'): 210, (\'East\', \'B\'): 160, (\'West\', \'A\'): 320, (\'West\', \'B\'): 230 }, \\"sales_east_0101_to_0102\\": { (\'East\', \'A\', \'2023-01-01\'): 200, (\'East\', \'A\', \'2023-01-02\'): 210, (\'East\', \'B\', \'2023-01-01\'): 150, (\'East\', \'B\', \'2023-01-02\'): 160 } } ``` # Notes - The solutions must handle the hierarchical structure appropriately using MultiIndex. - Ensure that the function is robust and performs well even with larger datasets.","solution":"import pandas as pd def analyze_sales(df: pd.DataFrame) -> dict: # Calculate total sales for each region total_sales_per_region = df.groupby(level=\'Region\').sum()[\'Sales\'].to_dict() # Calculate sales for store \'A\' across all dates and regions sales_store_A = df.xs(\'A\', level=\'Store\')[\'Sales\'].to_dict() # Calculate sales on \'2023-01-02\' for all regions and stores sales_20230102 = df.xs(\'2023-01-02\', level=\'Date\')[\'Sales\'].to_dict() # Calculate sales from \'2023-01-01\' to \'2023-01-02\' for the \'East\' region sales_east_0101_to_0102 = df.loc[(\'East\', slice(None), slice(\'2023-01-01\', \'2023-01-02\')), \'Sales\'].to_dict() return { \\"total_sales_per_region\\": total_sales_per_region, \\"sales_store_A\\": sales_store_A, \\"sales_20230102\\": sales_20230102, \\"sales_east_0101_to_0102\\": sales_east_0101_to_0102, } # Example usage: data = { \'Region\': [\'East\', \'East\', \'East\', \'East\', \'West\', \'West\', \'West\', \'West\'], \'Store\': [\'A\', \'A\', \'B\', \'B\', \'A\', \'A\', \'B\', \'B\'], \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-02\'], \'Sales\': [200, 210, 150, 160, 300, 320, 220, 230] } df = pd.DataFrame(data) df.set_index([\'Region\', \'Store\', \'Date\'], inplace=True) result = analyze_sales(df) print(result)"},{"question":"You have been provided with an XML document that contains information about a library\'s book collection. Each book entry includes the title, author, publication year, and genre. Your task is to parse this XML document, extract the book information, and create a summary of the books categorized by genre. Input: - An XML string representing the library\'s book collection. Output: - A summary string in the following format: ``` Genre: <genre_name_1> Title: <book_title_1> | Author: <author_name_1> | Year: <publication_year_1> Title: <book_title_2> | Author: <author_name_2> | Year: <publication_year_2> ... Genre: <genre_name_2> Title: <book_title_1> | Author: <author_name_1> | Year: <publication_year_1> Title: <book_title_2> | Author: <author_name_2> | Year: <publication_year_2> ... ``` Constraints: 1. The XML document always has the correct format and structure. 2. Each book has exactly one title, author, publication year, and genre. Example XML Input: ```xml <library> <book> <title>The Catcher in the Rye</title> <author>J.D. Salinger</author> <year>1951</year> <genre>Fiction</genre> </book> <book> <title>A Brief History of Time</title> <author>Stephen Hawking</author> <year>1988</year> <genre>Science</genre> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <genre>Fiction</genre> </book> </library> ``` Expected Output: ``` Genre: Fiction Title: The Catcher in the Rye | Author: J.D. Salinger | Year: 1951 Title: To Kill a Mockingbird | Author: Harper Lee | Year: 1960 Genre: Science Title: A Brief History of Time | Author: Stephen Hawking | Year: 1988 ``` # Task Implement a function `summarize_library(xml_string: str) -> str` that takes an XML string as input and returns the summary string as described. Function Signature: ```python def summarize_library(xml_string: str) -> str: pass ``` Notes: - You might find the methods from `xml.dom.minidom` such as `parseString`, `getElementsByTagName`, and properties of the `Node` class useful for this task. - Ensure the summary is well-organized and formatted exactly as specified.","solution":"import xml.dom.minidom def summarize_library(xml_string: str) -> str: dom = xml.dom.minidom.parseString(xml_string) books = dom.getElementsByTagName(\\"book\\") genre_dict = {} for book in books: title = book.getElementsByTagName(\\"title\\")[0].childNodes[0].data author = book.getElementsByTagName(\\"author\\")[0].childNodes[0].data year = book.getElementsByTagName(\\"year\\")[0].childNodes[0].data genre = book.getElementsByTagName(\\"genre\\")[0].childNodes[0].data if genre not in genre_dict: genre_dict[genre] = [] genre_dict[genre].append(f\\"Title: {title} | Author: {author} | Year: {year}\\") summary_lines = [] for genre, summaries in genre_dict.items(): summary_lines.append(f\\"Genre: {genre}\\") summary_lines.extend(summaries) summary_lines.append(\\"\\") return \\"n\\".join(summary_lines).strip()"},{"question":"# Custom Logging Handler Implementation **Objective**: Create a custom logging handler that extends the built-in `logging.handlers` in Python. # Problem Statement: You are required to implement a custom logging handler named `CustomHTTPHandler` which extends the `logging.handlers.HTTPHandler` class. The custom handler will send logging messages to a provided web server while adding custom headers to the HTTP request. # Requirements: 1. Implement a class `CustomHTTPHandler` that inherits from `logging.handlers.HTTPHandler`. 2. Modify the `emit` method to include custom headers in the HTTP request. 3. The custom headers should be passed as a dictionary during the initialization of the handler. # Input: - `host` (str): The host to which logging messages will be sent, in the format `\'host:port\'`. - `url` (str): The URL to send logging messages. - `method` (str): The HTTP method to use. Can be `\'GET\'` or `\'POST\'`. - `secure` (bool): If set to `True`, use a secure HTTPS connection. Default is `False`. - `credentials` (tuple): A tuple of `(username, password)` for Basic Authentication. Optional. - `context` (ssl.SSLContext): SSL context for HTTPS connection. Optional. - `headers` (dict): A dictionary of custom headers to include in the HTTP request. Example: `{\'X-Custom-Header\': \'value\'}`. # Output: - The `CustomHTTPHandler` should send the log records as HTTP requests to the specified server with the provided headers. # Example: ```python import logging from logging.handlers import HTTPHandler class CustomHTTPHandler(HTTPHandler): def __init__(self, host, url, method=\'GET\', secure=False, credentials=None, context=None, headers=None): super().__init__(host, url, method, secure, credentials, context) self.headers = headers or {} def emit(self, record): import http.client import urllib.parse import ssl from email.utils import formatdate try: host_parts = self.host.split(\\":\\") host = host_parts[0] if len(host_parts) > 1: port = int(host_parts[1]) else: port = None h = None if self.secure: h = http.client.HTTPSConnection(host, port, context=self.context) else: h = http.client.HTTPConnection(host, port) url = self.url headers = self.headers.copy() headers.update({ \'Content-Type\': \'application/x-www-form-urlencoded\', \'Date\': formatdate(timeval=None, localtime=False, usegmt=True) }) if self.credentials: import base64 creds = base64.b64encode(bytes(\\"%s:%s\\" % self.credentials, \\"utf-8\\")).decode(\\"ascii\\") headers[\\"Authorization\\"] = \\"Basic %s\\" % creds data = urllib.parse.urlencode(self.mapLogRecord(record)) if self.method == \\"POST\\": h.request(self.method, url, data, headers) else: h.request(self.method, \\"?\\".join((url, data)), headers=headers) h.getresponse() # Can be used to check response status except Exception: self.handleError(record) # Example usage: logger = logging.getLogger(\'custom_http\') custom_http_handler = CustomHTTPHandler( host=\'localhost:8000\', url=\'/log\', method=\'POST\', headers={\'X-Custom-Header\': \'value\'} ) logger.addHandler(custom_http_handler) logger.setLevel(logging.INFO) logger.info(\'This is a test log message.\') ``` # Constraints: - Assume the web server is running and the connection details provided are valid. - Handle any exceptions during the HTTP request in the `emit` method. Submit your implementation of the `CustomHTTPHandler`.","solution":"import logging from logging.handlers import HTTPHandler class CustomHTTPHandler(HTTPHandler): def __init__(self, host, url, method=\'GET\', secure=False, credentials=None, context=None, headers=None): super().__init__(host, url, method, secure, credentials, context) self.headers = headers or {} def emit(self, record): import http.client import urllib.parse import ssl from email.utils import formatdate try: host_parts = self.host.split(\\":\\") host = host_parts[0] if len(host_parts) > 1: port = int(host_parts[1]) else: port = None h = None if self.secure: h = http.client.HTTPSConnection(host, port, context=self.context) else: h = http.client.HTTPConnection(host, port) url = self.url headers = self.headers.copy() headers.update({ \'Content-Type\': \'application/x-www-form-urlencoded\', \'Date\': formatdate(timeval=None, localtime=False, usegmt=True) }) if self.credentials: import base64 creds = base64.b64encode(bytes(\\"%s:%s\\" % self.credentials, \\"utf-8\\")).decode(\\"ascii\\") headers[\\"Authorization\\"] = \\"Basic %s\\" % creds data = urllib.parse.urlencode(self.mapLogRecord(record)) if self.method == \\"POST\\": h.request(self.method, url, data, headers) else: h.request(self.method, \\"?\\".join((url, data)), headers=headers) h.getresponse() # Can be used to check response status except Exception: self.handleError(record) # Example usage: # logger = logging.getLogger(\'custom_http\') # custom_http_handler = CustomHTTPHandler( # host=\'localhost:8000\', # url=\'/log\', # method=\'POST\', # headers={\'X-Custom-Header\': \'value\'} # ) # logger.addHandler(custom_http_handler) # logger.setLevel(logging.INFO) # logger.info(\'This is a test log message.\')"},{"question":"# Mini 2to3 Tool Implementation **Objective**: Implement a Python script that transforms a given Python 2.x code snippet to Python 3.x code. Your script should specifically handle the following transformations: - Convert `print` statements to `print()` functions. - Replace `xrange()` with `range()`. - Replace `raw_input()` with `input()`. **Description**: You need to write a Python function `transform_code(python2_code: str) -> str` that takes a string containing Python 2.x code and returns a string with the corresponding Python 3.x code. **Function Signature**: ```python def transform_code(python2_code: str) -> str: # Your code here ``` # Input: - `python2_code` (str): A string representing the Python 2.x source code. # Output: - `str`: A string representing the transformed Python 3.x source code. # Constraints: - The input code will only use the features specified above (`print` statements, `xrange`, and `raw_input`). - You should preserve the indentation and comments from the original code as much as possible. # Example: ```python python2_code = print \\"Hello, World!\\" for i in xrange(5): print i name = raw_input(\\"Enter your name: \\") print \\"Hello, \\" + name output_code = transform_code(python2_code) # Expected Output: print(\\"Hello, World!\\") for i in range(5): print(i) name = input(\\"Enter your name: \\") print(\\"Hello, \\" + name) ``` # Instructions: 1. You may use Python\'s `re` (regular expression) module to help with the transformation. 2. Consider writing helper functions to handle each type of transformation for cleaner and more modular code. 3. Ensure your code properly handles varying indentations and preserves comments in the input code. **Note:** The focus is to understand how you can perform string manipulations and simple code transformations, simulating a part of what `2to3` does but on a smaller scale.","solution":"import re def transform_code(python2_code: str) -> str: # Transform print statements to print() function. python3_code = re.sub(r\'(?<!w)prints+\\"(.*?)\\"s*\', r\'print(\\"1\\")\', python2_code, flags=re.M) python3_code = re.sub(r\'(?<!w)prints+(.*)\', lambda m: f\'print({m.group(1)})\', python3_code, flags=re.M) # Replace xrange with range. python3_code = re.sub(r\'bxrangeb\', \'range\', python3_code) # Replace raw_input with input. python3_code = re.sub(r\'braw_inputb\', \'input\', python3_code) return python3_code"},{"question":"**Objective**: Write a Python function that implements unsupervised dimensionality reduction using Principal Component Analysis (PCA) and applies it to a dataset. The function should return the reduced-dimensionality dataset and the explained variance ratio. Function Signature ```python def pca_dimensionality_reduction(data: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]: Applies Principal Component Analysis (PCA) to reduce the dimensionality of the given dataset. Parameters: data (np.ndarray): A 2D NumPy array where each row represents a sample and each column represents a feature. n_components (int): The number of principal components to keep. Returns: Tuple[np.ndarray, np.ndarray]: A tuple containing two elements: - The transformed data as a 2D NumPy array with reduced dimensions. - The explained variance ratio of the selected components as a 1D NumPy array. Constraints: - The number of components (n_components) must be a positive integer less than or equal to the number of features in the dataset. - The input dataset must have at least one sample (row) and more than one feature (column). Example: >>> import numpy as np >>> data = np.array([[2.5, 4.7, 1.5], [1.2, 3.6, 2.8], [3.8, 5.9, 1.3], [2.9, 4.1, 3.0]]) >>> pca_dimensionality_reduction(data, 2) (array([[ -0.132, 2.140], [ -0.781, 0.806], [ 1.147, 2.346], [ -0.269, 1.194]]), array([0.781, 0.167, 0.052])) pass ``` Requirements 1. Implement the function `pca_dimensionality_reduction` using scikit-learn\'s `PCA` class. 2. The function should accept a 2D NumPy array `data` and an integer `n_components`. 3. The function should verify that `n_components` is valid (positive and less than or equal to the number of features in the dataset). 4. The function should return the reduced dataset and the explained variance ratio of the principal components. Tips: - Use `sklearn.decomposition.PCA` for Principal Component Analysis. - The `fit_transform` method of `PCA` can be useful for transforming the dataset. - The `explained_variance_ratio_` attribute of `PCA` provides the explained variance.","solution":"import numpy as np from sklearn.decomposition import PCA from typing import Tuple def pca_dimensionality_reduction(data: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]: Applies Principal Component Analysis (PCA) to reduce the dimensionality of the given dataset. Parameters: data (np.ndarray): A 2D NumPy array where each row represents a sample and each column represents a feature. n_components (int): The number of principal components to keep. Returns: Tuple[np.ndarray, np.ndarray]: A tuple containing two elements: - The transformed data as a 2D NumPy array with reduced dimensions. - The explained variance ratio of the selected components as a 1D NumPy array. Constraints: - The number of components (n_components) must be a positive integer less than or equal to the number of features in the dataset. - The input dataset must have at least one sample (row) and more than one feature (column). # Verify constraints if not isinstance(n_components, int) or n_components <= 0: raise ValueError(\\"The number of components must be a positive integer.\\") num_samples, num_features = data.shape if n_components > num_features: raise ValueError(\\"The number of components must be less than or equal to the number of features.\\") # Apply PCA pca = PCA(n_components=n_components) transformed_data = pca.fit_transform(data) explained_variance_ratio = pca.explained_variance_ratio_ return transformed_data, explained_variance_ratio"},{"question":"Coding Assessment Question # Web Page Content Fetcher and Parser In this task, you are required to implement a Python function that fetches content from a given URL, parses the URL to extract various components, and returns specific information about the URL and its content. You will use the `urllib` package for this purpose. # Function Signature ```python def fetch_and_parse_url(url: str) -> dict: pass ``` # Input - `url` (str): A string representing the URL to be fetched and parsed. # Output - A dictionary with the following keys and their corresponding values: - `url_components` (dict): A dictionary of the parsed URL components with the following keys: \'scheme\', \'netloc\', \'path\', \'params\', \'query\', \'fragment\'. - `content` (str): The first 500 characters of the fetched web page content (as a string). - `error` (str): If an error occurs during the URL fetching process, this key should be present in the output dictionary with the corresponding error message. If no error occurs, this key should not be present. # Constraints - You only need to handle HTTP and HTTPS URLs. - The function should handle errors gracefully and return meaningful error messages. # Example ```python url = \\"https://example.com\\" result = fetch_and_parse_url(url) print(result) ``` # Expected Output ```python { \'url_components\': { \'scheme\': \'https\', \'netloc\': \'example.com\', \'path\': \'\', \'params\': \'\', \'query\': \'\', \'fragment\': \'\' }, \'content\': \'<!doctype html>n<html>n<head>n <title>Example Domain</title>nn <meta charset=\\"utf-8\\" />n <meta http-equiv=\\"Content-type\\" content=\\"text/html; ...\', } ``` # Notes - Use the `urllib.parse.urlparse` function to parse the URL. - Use the `urllib.request.urlopen` function to fetch the web page content. - Handle exceptions using `urllib.error.URLError`. You are expected to demonstrate your understanding of URL handling, exception management, and the usage of the `urllib` package in this task.","solution":"import urllib.request import urllib.parse from urllib.error import URLError def fetch_and_parse_url(url: str) -> dict: result = {} # Parse the URL components parsed_url = urllib.parse.urlparse(url) result[\'url_components\'] = { \'scheme\': parsed_url.scheme, \'netloc\': parsed_url.netloc, \'path\': parsed_url.path, \'params\': parsed_url.params, \'query\': parsed_url.query, \'fragment\': parsed_url.fragment } try: # Fetch the content from the URL with urllib.request.urlopen(url) as response: content = response.read().decode(\'utf-8\') result[\'content\'] = content[:500] # first 500 characters except URLError as e: result[\'error\'] = str(e) return result"},{"question":"**Title: Implementing a Case-Insensitive Unicode String Comparator** **Objective:** Create a function that compares two Unicode strings for equality in a case-insensitive manner, accounting for different representations of the same character (e.g., combining characters). **Problem Description:** Write a function `compare_unicode_strings` that takes two arguments, `s1` and `s2`, which are Unicode strings. The function should return `True` if the strings are equal when compared in a case-insensitive manner, and `False` otherwise. The comparison should normalize the strings to account for different representations of the same character. **Function Signature:** ```python def compare_unicode_strings(s1: str, s2: str) -> bool: ``` **Input:** - `s1` (str): A Unicode string. - `s2` (str): Another Unicode string. **Output:** - (bool): `True` if `s1` and `s2` are equal in a case-insensitive comparison, `False` otherwise. **Constraints:** - The function should handle characters from different languages and scripts. - The function should account for combining characters and other Unicode properties that might affect equality. **Example:** ```python assert compare_unicode_strings(\\"Gürzenichstraße\\", \\"gürzenichstrasse\\") == True assert compare_unicode_strings(\\"ê\\", \\"N{LATIN SMALL LETTER E}N{COMBINING CIRCUMFLEX ACCENT}\\") == True assert compare_unicode_strings(\\"Straße\\", \\"strasse\\") == True assert compare_unicode_strings(\\"Hello\\", \\"hElLo\\") == True assert compare_unicode_strings(\\"Python\\", \\"Pythön\\") == False ``` **Specifications:** 1. Use the `unicodedata` module to normalize the strings. 2. Use the `casefold()` method to achieve case-insensitivity. 3. Ensure the comparison is robust against various Unicode representations of the same characters. **Hints:** - Consider using `unicodedata.normalize()` to convert the strings to a canonical form. - Use `casefold()` after normalization to handle case-insensitivity. **Evaluation Criteria:** - Correctness: The function should correctly compare Unicode strings as specified. - Efficiency: The function should handle large strings efficiently. - Code Quality: The code should be well-structured, documented, and follow Python best practices. **Additional Challenge:** Extend the function to handle a list of Unicode strings and return a list of lists where each sublist contains strings that are considered equal. Function Signature: ```python def group_equal_unicode_strings(strings: List[str]) -> List[List[str]]: ``` Example: ```python input_strings = [\\"Gürzenichstraße\\", \\"gürzenichstrasse\\", \\"Straße\\", \\"strasse\\", \\"Python\\", \\"Pythön\\"] expected_output = [[\\"Gürzenichstraße\\", \\"gürzenichstrasse\\"], [\\"Straße\\", \\"strasse\\"], [\\"Python\\"], [\\"Pythön\\"]] assert group_equal_unicode_strings(input_strings) == expected_output ```","solution":"import unicodedata def compare_unicode_strings(s1: str, s2: str) -> bool: Compares two Unicode strings for equality in a case-insensitive manner, normalizing the strings to account for different representations of the same character. ns1 = unicodedata.normalize(\'NFC\', s1).casefold() ns2 = unicodedata.normalize(\'NFC\', s2).casefold() return ns1 == ns2"},{"question":"# Coding Assessment Task: Manipulating Sun AU Audio Files You are provided with a Sun AU audio file and are required to perform the following operations using the `sunau` module in Python: 1. **Read the AU File**: Open the given AU file and extract its parameters (number of channels, sample width, frame rate, and number of frames). 2. **Process the Audio Frames**: Calculate the total duration of the audio in seconds. 3. **Write to a New AU File**: Create a new AU file, copy the extracted parameters, and write the audio data frames to this new file. # Instructions: 1. Implement the function `process_audio_file(input_file: str, output_file: str) -> float`: - `input_file`: Path to the input AU file. - `output_file`: Path to the new AU file to be created. - Return the total duration of the audio in seconds as a float. 2. Your function should: - Open the input AU file in read mode. - Extract the parameters: number of channels, sample width, frame rate, and number of frames. - Read all audio frames from the input file. - Calculate the duration of the audio using the number of frames and the frame rate. - Open the output AU file in write mode. - Set the extracted parameters to the output file. - Write the read audio frames to the output file. - Ensure the new AU file has the same audio data and parameters as the input file. # Example: ```python duration = process_audio_file(\'input.au\', \'output.au\') print(f\'Total duration: {duration} seconds\') ``` # Constraints and Notes: - The module `sunau` is used to perform all file operations. - Assume that the input file path and output file path provided are valid. - The input AU file contains audio data in a supported encoding format. - The total duration should be accurate to at least two decimal places. - Handle any potential exceptions that might arise from file operations appropriately. # Performance Requirements: - The function should handle AU files of up to 1GB in size efficiently. - Ensure minimal memory usage by efficiently reading and writing audio frames. # Tips: - Use `getnframes()` and `getframerate()` methods to calculate the total duration. - Utilize `AU_read` and `AU_write` objects for reading and writing operations respectively. - Set the parameters for the output file using methods like `setnchannels()`, `setsampwidth()`, `setframerate()`, etc. Your implementation will be evaluated based on correctness, efficiency, and adherence to the instructions provided.","solution":"import sunau def process_audio_file(input_file: str, output_file: str) -> float: try: # Open the input AU file for reading with sunau.open(input_file, \'rb\') as input_au: # Extract audio parameters n_channels = input_au.getnchannels() sampwidth = input_au.getsampwidth() framerate = input_au.getframerate() n_frames = input_au.getnframes() # Read all audio frames audio_frames = input_au.readframes(n_frames) # Calculate the duration in seconds duration = round(n_frames / float(framerate), 2) # Open the output AU file for writing with sunau.open(output_file, \'wb\') as output_au: # Set the extracted parameters output_au.setnchannels(n_channels) output_au.setsampwidth(sampwidth) output_au.setframerate(framerate) output_au.setnframes(n_frames) # Write the audio frames to the output AU file output_au.writeframes(audio_frames) return duration except Exception as e: print(f\\"An error occurred: {e}\\") return 0.0"},{"question":"**Objective:** Write a Python function that demonstrates understanding of Python\'s `errno` module, including error code mapping and exception handling. **Task:** You need to implement a function `check_error_code(error_number: int) -> str` that takes an error number as input and returns a formatted string describing the error. Your function should include the following: 1. Map the given error number to the corresponding error name using `errno.errorcode`. 2. Use `os.strerror()` to get the human-readable error message corresponding to the error number. 3. Return a formatted string in the format: `\\"Error [error_number] (error_name): error_message\\"`. For example, given `errno.ENOMEM`, the function should return `\\"Error 12 (ENOMEM): Out of memory\\"`. If the input error number does not exist in `errno.errorcode`, the function should raise a `ValueError` with the message `\\"Invalid error number\\"`. **Input:** - error_number (int): The error number to be processed. **Output:** - (str): A formatted string describing the error or raises a `ValueError` for an invalid error number. **Constraints:** - You may assume that the `errno` module and `os` module are always available. - The error number provided will be an integer but may not be a valid error code. ```python import errno import os def check_error_code(error_number: int) -> str: # Write your implementation here pass # Test cases: try: print(check_error_code(12)) # Expected: \\"Error 12 (ENOMEM): Out of memory\\" except ValueError as e: print(e) try: print(check_error_code(12345)) # Expected: raises ValueError(\\"Invalid error number\\") except ValueError as e: print(e) ``` **Notes:** - Make sure your function handles both valid and invalid error numbers correctly. - You are encouraged to test your function with additional error numbers not covered in the test cases provided.","solution":"import errno import os def check_error_code(error_number: int) -> str: Takes an error number and returns a formatted string describing the error. if error_number not in errno.errorcode: raise ValueError(\\"Invalid error number\\") error_name = errno.errorcode[error_number] error_message = os.strerror(error_number) return f\\"Error {error_number} ({error_name}): {error_message}\\""},{"question":"# Advanced Python Object Management and Behavior Simulation You are tasked with simulating a simplified version of Python\'s reference counting mechanism by creating a custom class in Python. Objective: Implement a class `CustomObject` that simulates reference-counting behavior. The class should: 1. Initialize an object with a specified type and a data value. 2. Manage a reference count that increases whenever a new reference to the object is made and decreases when a reference is deleted. 3. Display the current reference count when queried. 4. Implement a mechanism to safely set and get the type and data of the object. 5. Ensure that the type of the object can only be set once. Your Task: - Define the `CustomObject` class. - Implement the following methods: - `__init__(self, obj_type: str, data: any)`: Initializes the object with a type and data value. - `__del__(self)`: Decreases the reference count when the object is about to be destroyed. - `add_ref(self)`: Increases the reference count by one. - `del_ref(self)`: Decreases the reference count by one and deletes the object if the reference count is zero. - `get_ref_count(self)`: Returns the current reference count. - `set_type(self, obj_type: str)`: Sets the type of the object only if it has not been set before. - `get_type(self)`: Returns the type of the object. - `set_data(self, data: any)`: Sets the data value of the object. - `get_data(self)`: Returns the data value of the object. Constraints: - The type of the object should only be set once and cannot be changed thereafter. - The reference count should accurately reflect the number of references to the object at any point in time. - Ensure proper memory management by appropriately deleting the object when reference count drops to zero. Example: ```python # Example of usage obj = CustomObject(\\"ExampleType\\", 123) print(obj.get_ref_count()) # Output: 1 # Adding a reference obj.add_ref() print(obj.get_ref_count()) # Output: 2 # Deleting a reference obj.del_ref() print(obj.get_ref_count()) # Output: 1 # Setting and getting type and data print(obj.get_type()) # Output: ExampleType print(obj.get_data()) # Output: 123 obj.set_data(456) print(obj.get_data()) # Output: 456 # Attempt to change type (should fail) obj.set_type(\\"NewType\\") # Should have no effect print(obj.get_type()) # Output: ExampleType # Deleting last reference obj.del_ref() # Reference count should drop to 0 and object should be deleted ``` Note: - The class should handle edge cases, such as attempting to update type after it is set, and ensure robustness in managing reference counts.","solution":"class CustomObject: def __init__(self, obj_type: str, data: any): self._type = obj_type self._data = data self._ref_count = 1 def __del__(self): self.del_ref() def add_ref(self): self._ref_count += 1 def del_ref(self): if self._ref_count > 0: self._ref_count -= 1 if self._ref_count == 0: del self def get_ref_count(self): return self._ref_count def set_type(self, obj_type: str): if self._type is None: self._type = obj_type def get_type(self): return self._type def set_data(self, data: any): self._data = data def get_data(self): return self._data"},{"question":"**Question:** Implement a Python function that takes a URL and performs a series of HTTP operations using the `http.client` library. Specifically, the function should: 1. Send a GET request to retrieve data from the URL. 2. Handle and print the status code and reason phrase from the response. 3. If the GET request is successful (status code 200), open an HTTP connection to a second URL (provided as an argument). 4. Perform a POST request to the second URL with the retrieved data from the first URL as the request\'s body. 5. Handle any HTTP exceptions that might occur and print appropriate error messages. # Input: - `url1`: A string representing the first URL to which the GET request will be sent. - `url2`: A string representing the second URL to which the POST request will be sent if GET request is successful. # Output: - The function should print the status codes and reason phrases for both requests and any error messages encountered. # Example: ```python def perform_http_operations(url1, url2): # Your implementation here # Example usage url1 = \\"http://example.com/api/data\\" url2 = \\"http://anotherexample.com/api/upload\\" perform_http_operations(url1, url2) ``` # Notes: - Use the `http.client.HTTPConnection` class for both access and keep the connections secured with HTTPS if required. - Make sure to close the connections after they are no longer needed. - Handle potential exceptions like `http.client.InvalidURL`, `http.client.RemoteDisconnected`, and `http.client.HTTPException`. This exercise evaluates understanding of HTTP/HTTPS connections, handling multiple request types, exception handling, and proper resource management using the `http.client` library.","solution":"import http.client from urllib.parse import urlparse def perform_http_operations(url1, url2): try: # Parse the first URL parsed_url1 = urlparse(url1) conn1 = http.client.HTTPSConnection(parsed_url1.netloc) # Send GET request conn1.request(\\"GET\\", parsed_url1.path) response1 = conn1.getresponse() # Print status code and reason for the GET request print(f\\"GET {url1}: {response1.status} {response1.reason}\\") # If successful, read the data and prepare to POST to the second URL if response1.status == 200: data = response1.read() conn1.close() # Parse the second URL parsed_url2 = urlparse(url2) conn2 = http.client.HTTPSConnection(parsed_url2.netloc) # Send POST request with data from GET response conn2.request(\\"POST\\", parsed_url2.path, body=data) response2 = conn2.getresponse() # Print status code and reason for the POST request print(f\\"POST {url2}: {response2.status} {response2.reason}\\") conn2.close() else: conn1.close() except (http.client.InvalidURL, http.client.RemoteDisconnected, http.client.HTTPException) as e: print(f\\"HTTP error occurred: {e}\\") # Example usage url1 = \\"http://httpbin.org/get\\" url2 = \\"http://httpbin.org/post\\" perform_http_operations(url1, url2)"},{"question":"**Title: Secure Management of Temporary Files in Python** **Objective:** Demonstrate your capability to securely create, manage, and clean temporary files using Python\'s `tempfile` module. **Problem Statement:** Write a Python function `manage_temp_files` that performs the following operations securely: 1. Creates a temporary file and writes a given string (in text mode) to it. 2. Reads the content from the temporary file to verify that the string was written correctly. 3. Writes additional content to the same temporary file. 4. Reads and returns the final content of the temporary file. 5. Ensures that the temporary file is properly closed and deleted after its operations (use context managers). **Function Signature:** ```python def manage_temp_files(initial_content: str, additional_content: str) -> str: pass ``` **Parameters:** - `initial_content` (str): The initial string to be written to the temporary file. - `additional_content` (str): Additional string to be appended to the temporary file. **Returns:** - `str`: The combined content of the temporary file after all writing operations. **Constraints:** - The function should handle any exceptions that occur during file operations and ensure that the temporary file is deleted. - The function should work on all major operating systems (POSIX, Windows, etc.). **Example:** ```python initial_content = \\"Hello, World!\\" additional_content = \\" This is additional content.\\" result = manage_temp_files(initial_content, additional_content) print(result) # Output: \\"Hello, World! This is additional content.\\" ``` **Hints:** - Use `tempfile.TemporaryFile` to create and manage temporary files. - Use context managers (`with` statement) to ensure automatic cleanup. - Handle any potential I/O exceptions properly. **Note:** Make sure to test your function thoroughly to handle various edge cases such as empty strings, special characters in content, etc.","solution":"import tempfile def manage_temp_files(initial_content: str, additional_content: str) -> str: try: with tempfile.TemporaryFile(mode=\'w+t\') as temp_file: # Write initial content to the file temp_file.write(initial_content) # Move the cursor back to the beginning of the file before reading temp_file.seek(0) # Read and verify the initial content initial_read = temp_file.read() # Make sure the initial content was written correctly assert initial_read == initial_content # Append additional content to the file temp_file.write(additional_content) # Move the cursor back to the beginning of the file before reading all temp_file.seek(0) # Read the final content of the file final_content = temp_file.read() return final_content except Exception as e: print(f\\"An error occurred: {e}\\") return \\"\\""},{"question":"# Pandas Coding Challenge: Data Manipulation with Series and DataFrame Objective: Implement a function `analyze_data` that performs a series of data manipulation tasks using pandas Series and DataFrame. Input: The function will receive two parameters: 1. `data`: A dictionary containing the following keys: - `students`: A list of dictionaries, each representing a student with the following keys: - `id` (int): The student\'s unique identifier. - `name` (str): The student\'s name. - `math` (float): The student\'s math score (range 0 to 100). - `science` (float): The student\'s science score (range 0 to 100). - `passed_min_score` (float): The minimum passing score for both subjects. Output: Return a dictionary with the following keys: 1. `top_student`: A dictionary representing the student with the highest average score, containing their `id`, `name`, and `average_score`. 2. `failed_students`: A list of dictionaries representing students who failed in any subject. Each dictionary should contain the student\'s `id`, `name`, and their scores (`math`, `science`). Constraints: 1. The input dictionary `data` will always be in the correct format and data types will be valid. 2. There will always be at least one student in the list. 3. If multiple students have the highest average score, return any one of them. Example: ```python data = { \\"students\\": [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"math\\": 78, \\"science\\": 92}, {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"math\\": 88, \\"science\\": 75}, {\\"id\\": 3, \\"name\\": \\"Charlie\\", \\"math\\": 65, \\"science\\": 60}, {\\"id\\": 4, \\"name\\": \\"David\\", \\"math\\": 80, \\"science\\": 85} ], \\"passed_min_score\\": 70 } ``` Expected output: ```python { \\"top_student\\": {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"average_score\\": 85.0}, \\"failed_students\\": [ {\\"id\\": 3, \\"name\\": \\"Charlie\\", \\"math\\": 65, \\"science\\": 60} ] } ``` Implementation: ```python import pandas as pd def analyze_data(data): # Step 1: Convert the list of student dictionaries into a DataFrame. students_df = pd.DataFrame(data[\'students\']) # Step 2: Calculate the average score for each student. students_df[\'average_score\'] = students_df[[\'math\', \'science\']].mean(axis=1) # Step 3: Identify the top student with the highest average score. top_student_row = students_df.loc[students_df[\'average_score\'].idxmax()] top_student = { \\"id\\": top_student_row[\'id\'], \\"name\\": top_student_row[\'name\'], \\"average_score\\": top_student_row[\'average_score\'] } # Step 4: Identify students who failed in any subject. failed_students_df = students_df[(students_df[\'math\'] < data[\'passed_min_score\']) | (students_df[\'science\'] < data[\'passed_min_score\'])] failed_students = failed_students_df[[\'id\', \'name\', \'math\', \'science\']].to_dict(orient=\'records\') # Step 5: Return the result as a dictionary. result = { \\"top_student\\": top_student, \\"failed_students\\": failed_students } return result ``` Notes: - Use pandas operations to perform data manipulation. - Ensure that your code is efficient and handles the input data as specified.","solution":"import pandas as pd def analyze_data(data): # Step 1: Convert the list of student dictionaries into a DataFrame. students_df = pd.DataFrame(data[\'students\']) # Step 2: Calculate the average score for each student. students_df[\'average_score\'] = students_df[[\'math\', \'science\']].mean(axis=1) # Step 3: Identify the top student with the highest average score. top_student_row = students_df.loc[students_df[\'average_score\'].idxmax()] top_student = { \\"id\\": top_student_row[\'id\'], \\"name\\": top_student_row[\'name\'], \\"average_score\\": top_student_row[\'average_score\'] } # Step 4: Identify students who failed in any subject. passed_min_score = data[\'passed_min_score\'] failed_students_df = students_df[(students_df[\'math\'] < passed_min_score) | (students_df[\'science\'] < passed_min_score)] failed_students = failed_students_df[[\'id\', \'name\', \'math\', \'science\']].to_dict(orient=\'records\') # Step 5: Return the result as a dictionary. result = { \\"top_student\\": top_student, \\"failed_students\\": failed_students } return result"},{"question":"Coding Assessment Question # Objective You are tasked with developing a function that utilizes the `modulefinder` module to analyze a target Python script and return a comprehensive report of all the modules the script imports. The report should include loaded modules, missing modules, and the first three global names of each loaded module (when available). # Function Signature ```python def analyze_imports(script_path: str, exclude_modules: list = None) -> dict: ``` # Input 1. `script_path` (str): Path to the Python script to be analyzed. 2. `exclude_modules` (list, optional): List of module names to exclude from the analysis. Defaults to None. # Output The function should return a dictionary with the following structure: ```python { \\"loaded_modules\\": { \\"module_name\\": [\\"global_name1\\", \\"global_name2\\", \\"global_name3\\"], ... }, \\"missing_modules\\": [\\"missing_module1\\", \\"missing_module2\\", ...] } ``` # Constraints - Assume that the provided script path exists and is a valid Python script. - The `exclude_modules` list, if provided, will only contain valid module names as strings. - The function should handle any potential exceptions gracefully and provide meaningful error messages. # Example ```python # Given the `bacon.py` script content: # ``` # import re, itertools # # try: # import baconhameggs # except ImportError: # pass # # try: # import guido.python.ham # except ImportError: # pass # ``` # and assuming \'baconhameggs\' and \'guido.python.ham\' are not real modules in the environment: result = analyze_imports(\'path/to/bacon.py\') # Expected result: # { # \\"loaded_modules\\": { # \\"_types\\": [], # \\"copyreg\\": [\\"_inverted_registry\\", \\"_slotnames\\", \\"__all__\\"], # \\"sre_compile\\": [\\"isstring\\", \\"_sre\\", \\"_optimize_unicode\\"], # \\"_sre\\": [], # \\"sre_constants\\": [\\"REPEAT_ONE\\", \\"makedict\\", \\"AT_END_LINE\\"], # \\"sys\\": [], # \\"re\\": [\\"__module__\\", \\"finditer\\", \\"_expand\\"], # \\"itertools\\": [], # \\"__main__\\": [\\"re\\", \\"itertools\\", \\"baconhameggs\\"], # \\"sre_parse\\": [\\"_PATTERNENDERS\\", \\"SRE_FLAG_UNICODE\\"], # \\"array\\": [], # \\"types\\": [\\"__module__\\", \\"IntType\\", \\"TypeType\\"] # }, # \\"missing_modules\\": [\\"guido.python.ham\\", \\"baconhameggs\\"] # } ``` # Note - Use modulefinder to determine the imported modules and to print a structured report as per the format specified above.","solution":"import modulefinder def analyze_imports(script_path: str, exclude_modules: list = None) -> dict: Analyze the imports in a target Python script. Parameters: script_path (str): Path to the Python script to be analyzed. exclude_modules (list, optional): List of module names to exclude from the analysis. Defaults to None. Returns: dict: A dictionary containing \'loaded_modules\' and \'missing_modules\' with their respective details. if exclude_modules is None: exclude_modules = [] finder = modulefinder.ModuleFinder() try: finder.run_script(script_path) except Exception as e: return {\\"error\\": str(e)} loaded_modules = {} missing_modules = list(finder.badmodules.keys()) for name, mod in finder.modules.items(): if name in exclude_modules: continue global_names = list(mod.globalnames.keys())[:3] loaded_modules[name] = global_names result = { \\"loaded_modules\\": loaded_modules, \\"missing_modules\\": missing_modules } return result"},{"question":"# Custom Container Implementation Using ABCs Objective You are required to implement a custom container class by inheriting from the appropriate abstract base classes provided in the `collections.abc` module. Your task is to create a class that behaves like a mutable set with additional constraints. Requirements 1. Implement a class named `LimitedSet` that: - Inherits from `collections.abc.MutableSet`. - Initializes with an optional argument specifying a maximum number of elements it can hold (default is 10). - Raises a `ValueError` if an attempt is made to add an element when it is already at its maximum capacity. - Overrides methods to provide the `add`, `discard`, `__contains__`, `__iter__`, and `__len__` functionalities. 2. Follow these constraints: - The `LimitedSet` should store elements in a list internally to maintain insertion order. - The `LimitedSet` should automatically remove the oldest element when adding a new element if at maximum capacity. 3. Provide a test function `test_limited_set()` that: - Creates an instance of `LimitedSet`. - Demonstrates adding and removing elements. - Illustrates the behavior when the set reaches its maximum capacity. - Ensures no element can be added when the set is full without removing an old element first. Implementation ```python from collections.abc import MutableSet class LimitedSet(MutableSet): def __init__(self, max_size=10): self._elements = [] self._max_size = max_size def add(self, value): if value not in self._elements: if len(self) >= self._max_size: raise ValueError(f\\"Set can hold at most {self._max_size} elements\\") self._elements.append(value) def discard(self, value): if value in self._elements: self._elements.remove(value) def __contains__(self, value): return value in self._elements def __iter__(self): return iter(self._elements) def __len__(self): return len(self._elements) def test_limited_set(): limited_set = LimitedSet(max_size=3) limited_set.add(1) limited_set.add(2) limited_set.add(3) print(f\\"Limited set after adding elements: {list(limited_set)}\\") try: limited_set.add(4) except ValueError as e: print(f\\"Expected exception: {e}\\") limited_set.discard(1) limited_set.add(4) print(f\\"Limited set after removing an element and adding a new one: {list(limited_set)}\\") print(f\\"Contains 1: {1 in limited_set}\\") print(f\\"Contains 2: {2 in limited_set}\\") print(f\\"Length of the set: {len(limited_set)}\\") # Uncomment the line below to run tests # test_limited_set() ``` Input Format There is no direct input to the main class constructor or methods; all operations are performed within the provided test function. Output Format The output should be printed to the console as demonstrated in the test function, showing the internal state of the `LimitedSet` after each operation.","solution":"from collections.abc import MutableSet class LimitedSet(MutableSet): def __init__(self, max_size=10): self._elements = [] self._max_size = max_size def add(self, value): if value not in self._elements: if len(self) >= self._max_size: self._elements.pop(0) # Remove the oldest element self._elements.append(value) def discard(self, value): if value in self._elements: self._elements.remove(value) def __contains__(self, value): return value in self._elements def __iter__(self): return iter(self._elements) def __len__(self): return len(self._elements)"},{"question":"# Weak References in Python Python\'s weak reference support allows objects to be referenced without preventing them from being garbage collected. This is particularly useful in caching and resource management scenarios. In this question, you will implement functions leveraging weak references. Task 1. Write a class `WeakRefManager` which will manage weak references to given objects. 2. Implement the following methods within the `WeakRefManager` class: - `__init__(self)`: Initialize the manager with an empty dictionary to hold weak references. - `add_reference(self, obj, ref_id)`: Add a weak reference to the given object `obj` with a string identifier `ref_id`. If the identifier already exists, raise a `ValueError`. - `get_referenced_object(self, ref_id)`: Return the object referenced by the weak reference identified by `ref_id`. If the reference is no longer valid (the object has been collected), return `None`. - `remove_reference(self, ref_id)`: Remove the reference identified by `ref_id`. If the reference does not exist, raise a `KeyError`. Constraints - The `ref_id` is a unique string identifier for each weak reference. - All objects being referenced will be instances of a class that supports weak references. - Ensure that the solution handles scenarios where objects are garbage collected and their references should be invalidated. Example ```python import weakref class MyClass: def __init__(self, value): self.value = value class WeakRefManager: def __init__(self): self._references = {} def add_reference(self, obj, ref_id): if ref_id in self._references: raise ValueError(f\'Reference ID {ref_id} already exists\') self._references[ref_id] = weakref.ref(obj) def get_referenced_object(self, ref_id): ref = self._references.get(ref_id, None) if ref is None: return None return ref() def remove_reference(self, ref_id): if ref_id not in self._references: raise KeyError(f\'Reference ID {ref_id} does not exist\') del self._references[ref_id] # Usage Example manager = WeakRefManager() obj1 = MyClass(10) manager.add_reference(obj1, \'ref1\') print(manager.get_referenced_object(\'ref1\').value) # Outputs: 10 del obj1 # obj1 is now garbage collected print(manager.get_referenced_object(\'ref1\')) # Outputs: None try: manager.remove_reference(\'ref1\') # Successfully removes the reference except KeyError as e: print(e) ``` Your implementation should pass the above example and handle additional edge cases.","solution":"import weakref class WeakRefManager: def __init__(self): self._references = {} def add_reference(self, obj, ref_id): if ref_id in self._references: raise ValueError(f\'Reference ID {ref_id} already exists\') self._references[ref_id] = weakref.ref(obj) def get_referenced_object(self, ref_id): ref = self._references.get(ref_id, None) if ref is None: return None return ref() def remove_reference(self, ref_id): if ref_id not in self._references: raise KeyError(f\'Reference ID {ref_id} does not exist\') del self._references[ref_id]"},{"question":"Objective The objective of this question is to assess your understanding of PyTorch\'s TorchInductor GPU profiling tools and your ability to work with environment variables, run benchmarks, and analyze generated performance logs. Problem Statement You are given a PyTorch model and need to profile its performance using TorchInductor. The model may not be running as efficiently as expected. Your task is to: 1. Set up the appropriate environment for profiling. 2. Run the benchmark on the given model. 3. Analyze the generated profiling logs to identify the most time-consuming kernel operations. Instructions 1. **Environment Setup:** - Write a script to set the necessary environment variables: - `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES` should be enabled. - `TORCHINDUCTOR_BENCHMARK_KERNEL` should be enabled. - `TORCHINDUCTOR_MAX_AUTOTUNE` should be enabled. 2. **Run the Benchmark:** - Use the provided script `model_benchmark.py` to profile the given model `example_model`. - Command to run the benchmark: ```bash python -u model_benchmark.py --backend inductor --amp --performance --dashboard --only example_model --disable-cudagraphs --training ``` 3. **Analyze Profiling Logs:** - Locate the profiling log file generated, typically found in `/tmp/compiled_module_profile.json`. - Parse the log to extract the percentages of GPU time consumed by different kernel categories (e.g., pointwise, reduction). - Identify the kernel that consumes the most GPU time and note the percentage. 4. **Output:** - Print the categories of kernels and their corresponding GPU time percentages. - Clearly state which kernel category is the most time-consuming and its percentage of the total GPU time. Constraints - Ensure that the environment variables are set correctly before running the benchmark. - The analysis should be performed programmatically within a Python script. - Your code should handle potential errors in locating or parsing the log file. Example Output ``` Kernel Categories and their GPU time percentages: Pointwise kernel: 28.58% Reduction kernel: 13.85% Persistent Reduction kernel: 3.89% Cutlass/CUDNN kernels: 56.57% The most time-consuming kernel is: Cutlass/CUDNN kernels with 56.57% of GPU time. ``` Submission You should provide a Python script named `profile_model.py` that accomplishes the above tasks.","solution":"import os import json # Step 1: Set up environment variables os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' # Step 2: Define function to run the benchmark def run_benchmark(): os.system(\\"python -u model_benchmark.py --backend inductor --amp --performance --dashboard --only example_model --disable-cudagraphs --training\\") # Step 3: Define function to analyze the profiling logs def analyze_profiling_logs(log_path=\'/tmp/compiled_module_profile.json\'): if not os.path.exists(log_path): raise FileNotFoundError(f\\"Profiling log file not found at {log_path}\\") with open(log_path, \'r\') as f: logs = json.load(f) kernel_time_consumption = {} for entry in logs: category = entry.get(\'category\', \'unknown\') percentage = entry.get(\'percentage\', 0) if category in kernel_time_consumption: kernel_time_consumption[category] += percentage else: kernel_time_consumption[category] = percentage most_time_consuming_kernel = max(kernel_time_consumption, key=kernel_time_consumption.get) sorted_kernels = sorted(kernel_time_consumption.items(), key=lambda x: x[1], reverse=True) print(\\"Kernel Categories and their GPU time percentages:\\") for kernel, percentage in sorted_kernels: print(f\\"{kernel} kernel: {percentage:.2f}%\\") print(f\\"The most time-consuming kernel is: {most_time_consuming_kernel} with {kernel_time_consumption[most_time_consuming_kernel]:.2f}% of GPU time.\\") # Function to perform entire profiling and analysis def profile_model(): run_benchmark() analyze_profiling_logs() # Entry point if __name__ == \\"__main__\\": profile_model()"},{"question":"# Seaborn Layout Control with `so.Plot` You are given a dataset represented as a pandas DataFrame named `df`. The DataFrame has the following structure: | Column Name | Description | |-------------|-------------------------------------------------| | `category` | Categorical data indicating different groups | | `value` | Numerical data representing some measurements | | `sub_group` | Further sub-categorization within the categories| Using seaborn\'s `objects` interface, your task is to create multiple plots and adjust their layouts: 1. Create a basic scatter plot with `category` on the X-axis and `value` on the Y-axis. 2. Control the overall dimensions of the figure to be 6x6 inches. 3. Use faceting to create subplots based on `category` and `sub_group`. 4. Ensure that the subplots fit within the provided layout. 5. Use any layout engine you find suitable to ensure the best visual representation. 6. Adjust the plot size within the underlying figure using the `extent` parameter to [0.1, 0.1, 0.9, 0.9]. # Your Implementation Should Include: - Essential seaborn imports. - Proper utilization of `so.Plot`, `layout`, `facet`, and `extent`. - Handling of figure display within a Jupyter Notebook with `show`. # Example Input ```python import pandas as pd # Example DataFrame df = pd.DataFrame({ \'category\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\', \'A\', \'B\', \'C\'], \'value\': [1, 2, 3, 4, 5, 6, 7, 8, 9], \'sub_group\': [\'X\', \'Y\', \'X\', \'Y\', \'X\', \'Y\', \'X\', \'Y\', \'X\'] }) ``` # Expected Output Your final plot should display correctly within a Jupyter Notebook with all specified layout adjustments applied. ```python import seaborn.objects as so # Your solution here p = (so.Plot(df, x=\'category\', y=\'value\') .layout(size=(6, 6)) .facet(\'category\', \'sub_group\') .layout(engine=\'constrained\', extent=[0.1, 0.1, 0.9, 0.9]) .show() ) ``` # Constraints and Notes 1. Use the provided DataFrame structure in the example input. 2. The solution should work efficiently for varying sizes of DataFrame. 3. The layout should be clearly visible and well-fitted within the output cell.","solution":"import pandas as pd import seaborn.objects as so def create_seaborn_plot(df): Create a seaborn plot with specific layout settings. Parameters: df (pd.DataFrame): Input DataFrame with \'category\', \'value\', and \'sub_group\' columns. Returns: None. Displays the plot. # Create the scatter plot with specified layout settings p = (so.Plot(df, x=\'category\', y=\'value\') .layout(size=(6, 6)) .facet(\'category\', \'sub_group\') .layout(engine=\'constrained\', extent=[0.1, 0.1, 0.9, 0.9]) ) p.show()"},{"question":"# Advanced Python Logging Question **Objective:** Demonstrate your understanding of Python\'s logging module by implementing a detailed logging setup and performing advanced logging operations. **Problem Statement:** You are tasked with creating a logging system for a Python application that should have the following requirements: 1. Configure a custom logger named `app.logger` which logs messages with levels `INFO` and above. 2. The log messages should be saved to a file named `app.log`. 3. The log file should be rotated every day at midnight, and you should keep backups for the last 7 days. 4. The log messages must include the timestamp, log level, and the message in the format: `YYYY-MM-DD HH:MM:SS - LEVEL - MESSAGE`. 5. Add a custom filter to the logger to append a static user-id (e.g., \\"user123\\") to each log record. 6. Handle exceptions using the logger and ensure that stack traces are included in the log message. **Input:** - No explicit input. - Your task is to implement the logging setup and demonstrate its use by logging messages and an exception. **Output:** - Example messages in `app.log` file, correctly formatted. **Function Signature:** ```python def setup_logging() -> None: Configures the logging system as described above. def main() -> None: Logs example messages including an exception. # Your logging setup could be demonstrated by calling these methods. setup_logging() main() ``` **Constraints:** - The logging setup should be accomplished using Python\'s `logging` module. - Do not use third-party logging libraries. **Guidelines:** - Define the `setup_logging` function to configure logging as required. - Demonstrate logging in the `main` function, including normal log messages and an exception with a stack trace. ```python import logging from logging.handlers import TimedRotatingFileHandler class UserFilter(logging.Filter): def filter(self, record): record.user_id = \\"user123\\" return True def setup_logging() -> None: logger = logging.getLogger(\'app.logger\') logger.setLevel(logging.INFO) handler = TimedRotatingFileHandler(\'app.log\', when=\'midnight\', interval=1, backupCount=7) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(user_id)s - %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') handler.setFormatter(formatter) user_filter = UserFilter() handler.addFilter(user_filter) logger.addHandler(handler) def main() -> None: logger = logging.getLogger(\'app.logger\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') try: 1 / 0 except ZeroDivisionError as e: logger.exception(\'An exception occurred\') # Setup logging configuration setup_logging() # Execute main function to generate log messages and an exception main() ``` **Task:** Complete the implementation and run the `main` function. Verify that the log file `app.log` contains correctly formatted messages and includes a sample stack trace from the exception.","solution":"import logging from logging.handlers import TimedRotatingFileHandler class UserFilter(logging.Filter): def filter(self, record): record.user_id = \\"user123\\" # Static user-id to be appended return True def setup_logging() -> None: logger = logging.getLogger(\'app.logger\') logger.setLevel(logging.INFO) handler = TimedRotatingFileHandler(\'app.log\', when=\'midnight\', interval=1, backupCount=7) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(user_id)s - %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') handler.setFormatter(formatter) user_filter = UserFilter() handler.addFilter(user_filter) logger.addHandler(handler) def main() -> None: logger = logging.getLogger(\'app.logger\') logger.info(\'This is an info message.\') logger.warning(\'This is a warning message.\') try: 1 / 0 except ZeroDivisionError as e: logger.exception(\'An exception occurred.\') # Setup logging configuration setup_logging() # Execute main function to generate log messages and an exception main()"},{"question":"# PyTorch Logging Configuration Task Objective The purpose of this task is to assess your understanding of PyTorch\'s logging system configuration, both through environment variables and the `torch._logging.set_logs` API. Problem Statement You are provided with a simple PyTorch module that has three dummy operations. Your task is to implement and configure logging for this module using the `torch._logging` system. 1. **Implementation:** - Create a simple PyTorch module with three operations: `operation_a`, `operation_b`, and `operation_c`. - For each operation, include log statements that follow the logging configuration of PyTorch. 2. **Logging Configuration:** - Write a function `configure_logging(log_settings: str)` that configures the PyTorch logging system using the string format specified in the documentation (similar to the `TORCH_LOGS` environment variable). - Demonstrate logging configuration by: - Setting the logging level of `operation_a` to `DEBUG`. - Setting the logging level of `operation_b` to `ERROR`. - Setting the logging level of `operation_c` to `INFO`. - Enabling an artifact `guards` for `operation_a`. Input and Output Specifications - You have to implement the PyTorch module and the logging configuration function in a single Python file. - **Function: configure_logging(log_settings: str) -> None** - **Input:** - `log_settings`: A string that specifies the logging configuration in the format demonstrated in the documentation. - **Output:** None. The function should configure the logging settings as specified. Constraints - Use the `torch._logging.set_logs` API to configure logging. Example Usage ```python import torch import torch._logging class MyModule(torch.nn.Module): def __init__(self): super(MyModule, self).__init__() # initialize components if needed def operation_a(self): print(\\"Executing Operation A\\") # Log statement for DEBUG level def operation_b(self): print(\\"Executing Operation B\\") # Log statement for ERROR level def operation_c(self): print(\\"Executing Operation C\\") # Log statement for INFO level # Function to configure the logging def configure_logging(log_settings: str) -> None: # Configure logging based on the log_settings string torch._logging.set_logs(log_settings) # Example Configuration config_string = \\"+operation_a,-operation_b,+guards\\" configure_logging(config_string) # Create an instance of the module and run operations module = MyModule() module.operation_a() module.operation_b() module.operation_c() ``` Ensure the log statements for each operation are configured to the appropriate log levels, and the artifact `guards` is enabled for `operation_a`. Evaluation Criteria - Correct implementation of the PyTorch module and operations. - Proper configuration of the logging system using the provided `log_settings` string. - Appropriate usage of log statements in the operations with respect to the configured log levels and artifacts.","solution":"import torch import logging class MyModule(torch.nn.Module): def __init__(self): super(MyModule, self).__init__() self.logger_a = logging.getLogger(\\"operation_a\\") self.logger_b = logging.getLogger(\\"operation_b\\") self.logger_c = logging.getLogger(\\"operation_c\\") def operation_a(self): self.logger_a.debug(\\"Executing Operation A\\") def operation_b(self): self.logger_b.error(\\"Executing Operation B\\") def operation_c(self): self.logger_c.info(\\"Executing Operation C\\") # Function to configure the logging def configure_logging(log_settings: str) -> None: # Set basic configuration for logging logging.basicConfig(level=logging.DEBUG) # Configure different loggers based on log_settings string if \'+operation_a\' in log_settings: logging.getLogger(\\"operation_a\\").setLevel(logging.DEBUG) if \'-operation_b\' in log_settings: logging.getLogger(\\"operation_b\\").setLevel(logging.ERROR) if \'+operation_c\' in log_settings: logging.getLogger(\\"operation_c\\").setLevel(logging.INFO) # Note: Adding guards is implied to enable tracking certain computations which might need more specific implementation not covered in standard logging # Example Configuration config_string = \\"+operation_a,-operation_b,+operation_c\\" configure_logging(config_string) # Create an instance of the module and run operations module = MyModule() module.operation_a() module.operation_b() module.operation_c()"},{"question":"# Command Line Argument Parser You are tasked to implement a command line argument parser using the `getopt` module. Your parser should handle various flag options, options with required arguments, and invalid options gracefully, providing appropriate error messages. Requirements: 1. Implement a function `parse_command_line(args: List[str]) -> Tuple[Dict[str, str], List[str]]` that: - Takes in a list of command line arguments `args` (excluding the script name). - Parses the arguments using `getopt()`. - Recognizes the following options: - Short options: `-h` for help, `-v` for verbose, `-o` with output file name argument. - Long options: `--help`, `--verbose`, `--output` with output file name argument. 2. Returns: - A dictionary of parsed options and their values. Use empty strings for options without arguments. - A list of non-option arguments. 3. If an invalid option or missing argument is encountered, the function should raise a `ValueError` with a descriptive error message. Example Usages: ```python args = [\'-v\', \'--output=logfile.txt\', \'file1\', \'file2\'] options, arguments = parse_command_line(args) print(options) # Output: {\'-v\': \'\', \'--output\': \'logfile.txt\'} print(arguments) # Output: [\'file1\', \'file2\'] args = [\'-h\'] options, arguments = parse_command_line(args) print(options) # Output: {\'-h\': \'\'} print(arguments) # Output: [] args = [\'--unknown-option\'] options, arguments = parse_command_line(args) # Raises ValueError: \\"option --unknown-option not recognized\\" ``` Constraints: - You must use the `getopt()` function from the `getopt` module. - You should handle both short and long options. - Provide appropriate error messages using Python\'s exception mechanism. Implementation: Implement your solution below: ```python import getopt from typing import List, Dict, Tuple def parse_command_line(args: List[str]) -> Tuple[Dict[str, str], List[str]]: try: opts, args = getopt.getopt(args, \\"hvo:\\", [\\"help\\", \\"verbose\\", \\"output=\\"]) except getopt.GetoptError as err: raise ValueError(str(err)) options = {} for opt, arg in opts: options[opt] = arg if arg else \\"\\" return options, args # Example usage: args = [\'-v\', \'--output=logfile.txt\', \'file1\', \'file2\'] options, arguments = parse_command_line(args) print(options) # Output: {\'-v\': \'\', \'--output\': \'logfile.txt\'} print(arguments) # Output: [\'file1\', \'file2\'] ```","solution":"import getopt from typing import List, Dict, Tuple def parse_command_line(args: List[str]) -> Tuple[Dict[str, str], List[str]]: Parses command line arguments. Args: args (List[str]): A list of command line arguments (excluding the script name). Returns: Tuple[Dict[str, str], List[str]]: A tuple containing a dictionary of options and a list of non-option arguments. Raises: ValueError: If an invalid option or missing argument is encountered. try: opts, args = getopt.getopt(args, \\"hvo:\\", [\\"help\\", \\"verbose\\", \\"output=\\"]) except getopt.GetoptError as err: raise ValueError(str(err)) options = {} for opt, arg in opts: options[opt] = arg if arg else \\"\\" return options, args"},{"question":"# HMAC-Based Message Authentication System **Objective**: Implement a Python function that utilizes the `hmac` module to create a message authentication system. Your function will receive a secret key and a list of messages and return a list of digests for the given messages using HMAC. **Function Signature**: ```python def generate_message_digests(key: bytes, messages: list[str], digestmod: str) -> list[str]: pass ``` **Input**: - `key` (bytes): The secret key to be used for HMAC. - `messages` (list of str): A list of messages (strings) that need to be authenticated. - `digestmod` (str): The name of the hash algorithm to use (e.g., \'sha256\', \'sha1\'). **Output**: - (list of str): A list of hexadecimal digest strings corresponding to each message in the input list. **Constraints and Requirements**: - The function should use the `hmac.new` and `HMAC.hexdigest` methods. - The function should handle any number of messages. - Performance should be considered; handle large messages efficiently if needed. **Example**: ```python key = b\'supersecretkey\' messages = [\\"message1\\", \\"message2\\", \\"message3\\"] digestmod = \'sha256\' output = generate_message_digests(key, messages, digestmod) print(output) # Example output: [\'<digest1>\', \'<digest2>\', \'<digest3>\'] ``` **Details**: 1. Create a function called `generate_message_digests` that initializes an HMAC object with the given key and hash algorithm. 2. Use the `update` method to add each message to the HMAC object. 3. Generate a digest for each message using the `hexdigest` method. 4. Append each digest to a result list and return it. **Note**: Do not use deprecated features. Ensure that the digestmod parameter is passed as required since versions newer than 3.4 require this parameter. **Hint**: Refer to the documentation provided for more details on how to use the `hmac` module features.","solution":"import hmac import hashlib def generate_message_digests(key: bytes, messages: list[str], digestmod: str) -> list[str]: Generates HMAC digests for a list of messages using the provided key and hash algorithm. Parameters: key (bytes): The secret key to be used for HMAC. messages (list of str): A list of messages that need to be authenticated. digestmod (str): The name of the hash algorithm to use (e.g., \'sha256\', \'sha1\'). Returns: list of str: A list of hexadecimal digest strings. digests = [] for message in messages: hmac_obj = hmac.new(key, message.encode(), digestmod) digest = hmac_obj.hexdigest() digests.append(digest) return digests"},{"question":"# Functional Programming in Python: Weather Data Processing You are provided with weather data from multiple cities around the world. The data includes the city, date, and temperature in Celsius for each day. Given the following format for the weather data (where each item in the list represents the weather data for a city for a given day): ```python weather_data = [ (\\"New York\\", \\"2023-10-01\\", 20), (\\"New York\\", \\"2023-10-02\\", 21), (\\"Los Angeles\\", \\"2023-10-01\\", 25), (\\"Los Angeles\\", \\"2023-10-02\\", 23), (\\"Toronto\\", \\"2023-10-01\\", 18), # More data... ] ``` Your task is to implement a series of functions to process this data using functional programming concepts. Your functions should be implemented using iterators, generators, and higher-order functions from the `itertools` and `functools` modules where appropriate. Task 1: Temperature Converter Implement a generator function `convert_to_fahrenheit` that takes an iterator of weather data in Celsius and yields the same data with the temperature converted to Fahrenheit. Formula to convert Celsius to Fahrenheit: [ text{Fahrenheit} = text{Celsius} times frac{9}{5} + 32 ] ```python def convert_to_fahrenheit(data): pass ``` Task 2: Filter Data by City Implement a function `filter_by_city` that takes an iterator of weather data and the name of a city and returns an iterator of weather data only for that city. ```python def filter_by_city(data, city): pass ``` Task 3: Compute Average Temperature Implement a function `average_temperature` that takes an iterator of weather data and returns the average temperature for the given data. ```python def average_temperature(data): pass ``` Task 4: Daily High and Low Implement a generator function `daily_high_low` that yields the highest and lowest temperatures recorded for each city on each day. The function should return a tuple where the first element is the city, the second element is the date, the third element is the highest temperature, and the fourth element is the lowest temperature. ```python from itertools import groupby def daily_high_low(data): pass ``` # Constraints: 1. You must use functional programming concepts. 2. Avoid using any explicit loops except for comprehensions and generator expressions. 3. You may use utility functions from the `itertools` and `functools` modules. 4. Assume that the input data is sorted first by city and then by date. # Example Usage: ```python # Example data: weather_data = [ (\\"New York\\", \\"2023-10-01\\", 20), (\\"New York\\", \\"2023-10-02\\", 21), (\\"Los Angeles\\", \\"2023-10-01\\", 25), (\\"Los Angeles\\", \\"2023-10-02\\", 23), (\\"Toronto\\", \\"2023-10-01\\", 18), ] # Task 1 fahrenheit_data = convert_to_fahrenheit(iter(weather_data)) print(list(fahrenheit_data)) # Task 2 ny_data = filter_by_city(iter(weather_data), \\"New York\\") print(list(ny_data)) # Task 3 average_temp = average_temperature(iter(weather_data)) print(average_temp) # Task 4 high_low_data = daily_high_low(iter(weather_data)) print(list(high_low_data)) ``` # Expected Output: ```python [(\'New York\', \'2023-10-01\', 68.0), (\'New York\', \'2023-10-02\', 69.8), (\'Los Angeles\', \'2023-10-01\', 77.0), (\'Los Angeles\', \'2023-10-02\', 73.4), (\'Toronto\', \'2023-10-01\', 64.4)] [(\'New York\', \'2023-10-01\', 20), (\'New York\', \'2023-10-02\', 21)] 21.4 [(\'New York\', \'2023-10-01\', 20, 20), (\'New York\', \'2023-10-02\', 21, 21), (\'Los Angeles\', \'2023-10-01\', 25, 25), (\'Los Angeles\', \'2023-10-02\', 23, 23), (\'Toronto\', \'2023-10-01\', 18, 18)] ``` **Note:** Your implementation should handle larger datasets efficiently using the principles of iterators and generators to ensure that memory usage is minimized.","solution":"def convert_to_fahrenheit(data): Converts the temperature in the weather data from Celsius to Fahrenheit. for city, date, temp_c in data: temp_f = temp_c * 9 / 5 + 32 yield (city, date, temp_f) def filter_by_city(data, city): Filters the weather data to only include entries from the specified city. return (entry for entry in data if entry[0] == city) def average_temperature(data): Computes the average temperature from the given weather data. temperatures = [temp for _, _, temp in data] return sum(temperatures) / len(temperatures) if temperatures else 0 from itertools import groupby def daily_high_low(data): Computes the daily highest and lowest temperature for each city. key_func = lambda x: (x[0], x[1]) # Group by city and date for (city, date), group in groupby(data, key=key_func): temps = [temp for _, _, temp in group] yield (city, date, max(temps), min(temps))"},{"question":"**Question: Processing EA IFF 85 Chunks from a Media File** You are required to implement a function `process_chunks(file: IO[bytes], bigendian: bool = True, align: bool = True) -> Dict[str, List[bytes]]` that reads and processes chunks from a file-like object containing EA IFF 85 chunks (e.g., AIFF, RMFF, WAVE). The function should return a dictionary where the keys are chunk IDs (as strings) and the values are lists of byte sequences corresponding to the data of each chunk. # Function Signature ```python from typing import IO, Dict, List def process_chunks(file: IO[bytes], bigendian: bool = True, align: bool = True) -> Dict[str, List[bytes]]: pass ``` # Input - `file`: A file-like object containing the chunked data. - `bigendian`: A boolean indicating if the chunk size is in big-endian byte order. Defaults to `True`. - `align`: A boolean indicating if chunks are aligned on 2-byte boundaries. Defaults to `True`. # Output - A dictionary where each key is a chunk ID (4-byte string) and the value is a list of byte sequences. Each byte sequence corresponds to the data within each chunk. # Constraints - The implementation should correctly handle both aligned and unaligned chunks based on the `align` parameter. - The implementation should properly interpret the chunk sizes based on the `bigendian` parameter. - You may assume that the file contains valid chunks as per the EA IFF 85 standard. # Example ```python from io import BytesIO # Construct a file-like object with 2 chunks file_content = BytesIO( b\'CHNK\' # Chunk ID b\'x00x00x00x04\' # Size: 4 bytes (Big-endian) b\'abcd\' # Data b\'CHNK\' # Chunk ID b\'x00x00x00x06\' # Size: 6 bytes (Big-endian) b\'efghij\' # Data ) # Call the function result = process_chunks(file_content, bigendian=True, align=True) # The output should be # {\'CHNK\': [b\'abcd\', b\'efghij\']} ``` # Notes - For aligned chunks, if the data size is odd, the function should account for the padding byte. - The function should handle EOFError gracefully and terminate reading chunks at EOF. Implement the `process_chunks` function considering the requirements above.","solution":"from typing import IO, Dict, List def process_chunks(file: IO[bytes], bigendian: bool = True, align: bool = True) -> Dict[str, List[bytes]]: def read_int32(f: IO[bytes], bigendian: bool) -> int: bytes_read = f.read(4) if not bytes_read or len(bytes_read) < 4: raise EOFError(\\"Insufficient bytes for reading integer\\") return int.from_bytes(bytes_read, \'big\' if bigendian else \'little\') chunk_dict = {} while True: chunk_id_bytes = file.read(4) if len(chunk_id_bytes) < 4: break # EOF reached chunk_id = chunk_id_bytes.decode(\'ascii\') try: chunk_size = read_int32(file, bigendian) except EOFError: break # Not enough bytes to read the full chunk size chunk_data = file.read(chunk_size) if len(chunk_data) < chunk_size: break # Not enough data to read the entire chunk if chunk_id not in chunk_dict: chunk_dict[chunk_id] = [] chunk_dict[chunk_id].append(chunk_data) if align and (chunk_size % 2 == 1): file.read(1) # Read and discard the alignment byte return chunk_dict"},{"question":"Objective: Write a Python function that calculates the factorial of a given non-negative integer `n`. The function should demonstrate proper handling of local and global variable scopes, as well as the use of exception handling to manage invalid inputs. Detailed Requirements: 1. Implement a function called `calculate_factorial` which takes an integer `n` as input and returns the factorial of `n`. 2. If `n` is not an integer or is a negative integer, the function should raise a `ValueError` with a descriptive message. 3. The function should use a global variable `factorial_memory` to cache previously computed factorial values to optimize performance. Function Signature: ```python def calculate_factorial(n: int) -> int: pass ``` Expected Input and Output: - Input: A non-negative integer `n` - Output: An integer representing the factorial of `n` Constraints: - You must use the global variable `factorial_memory` to store the results of previously computed factorials. - You must use exception handling to catch and handle cases where `n` is invalid. Example Usage: ```python factorial_memory = {} # Global cache for factorial values def calculate_factorial(n: int) -> int: global factorial_memory # Declare to use the global variable if not isinstance(n, int): raise ValueError(\\"Input must be an integer.\\") if n < 0: raise ValueError(\\"Input must be a non-negative integer.\\") if n in factorial_memory: return factorial_memory[n] if n == 0 or n == 1: factorial_memory[n] = 1 else: factorial_memory[n] = n * calculate_factorial(n - 1) return factorial_memory[n] # Examples: print(calculate_factorial(5)) # Output: 120 print(calculate_factorial(\\"a\\")) # Raises ValueError print(calculate_factorial(-3)) # Raises ValueError ``` Explanation: - The function `calculate_factorial` uses a global variable `factorial_memory` to store previously computed factorials to save computation time for subsequent calls. - The function checks whether the input `n` is a non-negative integer. If not, it raises a `ValueError`. - If the input is valid, the function computes the factorial using recursion and caches the result in `factorial_memory`. - The use of `global` keyword illustrates how to modify a global variable inside a function. This question will test students\' understanding of recursion, scope resolution, name binding, and proper exception handling in Python.","solution":"factorial_memory = {} # Global cache for factorial values def calculate_factorial(n: int) -> int: global factorial_memory # Declare to use the global variable if not isinstance(n, int): raise ValueError(\\"Input must be an integer.\\") if n < 0: raise ValueError(\\"Input must be a non-negative integer.\\") if n in factorial_memory: return factorial_memory[n] if n == 0 or n == 1: factorial_memory[n] = 1 else: factorial_memory[n] = n * calculate_factorial(n - 1) return factorial_memory[n]"},{"question":"**Objective**: Implement a PyTorch model using `torch.cond` showcasing dynamic control flow based on both input shape and data values. Task Implement a PyTorch module `CustomCondModel` that performs the following: 1. **Shape-Based Condition**: If the batch size of the input tensor is greater than 5, apply the `shape_true_fn`, otherwise apply `shape_false_fn`. 2. **Value-Based Condition**: If the mean of the input tensor is greater than 0.5, apply the `value_true_fn`, otherwise apply `value_false_fn`. You need to define the following functions: - `shape_true_fn(tensor)`: returns `tensor ** 2`. - `shape_false_fn(tensor)`: returns `tensor ** 3`. - `value_true_fn(tensor)`: returns `tensor.mean() * tensor`. - `value_false_fn(tensor)`: returns `torch.zeros_like(tensor)`. Finally, the model should process an input tensor with both conditions in sequence (shape-based first, then value-based), returning the final output tensor. Implementation Details - **Input**: A PyTorch tensor `x` of arbitrary shape. - **Output**: A PyTorch tensor processed according to the described rules. - **Constraints**: - You should use `torch.cond` for conditional operations. - Ensure that the implementation follows best practices for defining models in PyTorch. Example Usage ```python import torch from torch import nn class CustomCondModel(nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def shape_true_fn(tensor: torch.Tensor): return tensor ** 2 def shape_false_fn(tensor: torch.Tensor): return tensor ** 3 def value_true_fn(tensor: torch.Tensor): return tensor.mean() * tensor def value_false_fn(tensor: torch.Tensor): return torch.zeros_like(tensor) x = torch.cond(x.shape[0] > 5, shape_true_fn, shape_false_fn, (x,)) x = torch.cond(x.mean() > 0.5, value_true_fn, value_false_fn, (x,)) return x # Example model = CustomCondModel() input_tensor = torch.randn(3, 3) output_tensor = model(input_tensor) print(output_tensor) ``` **Note**: The example input and output will vary based on the conditions defined above. Constraints 1. Do not use any control flow operations other than `torch.cond`. 2. Ensure the model is defined using `torch.nn.Module`. Good luck!","solution":"import torch from torch import nn class CustomCondModel(nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def shape_true_fn(tensor): return tensor ** 2 def shape_false_fn(tensor): return tensor ** 3 def value_true_fn(tensor): return tensor.mean() * tensor def value_false_fn(tensor): return torch.zeros_like(tensor) if x.shape[0] > 5: x = shape_true_fn(x) else: x = shape_false_fn(x) if x.mean() > 0.5: x = value_true_fn(x) else: x = value_false_fn(x) return x"},{"question":"# Question **Title: Implement a Custom Asynchronous Server-Client Model using asyncio** **Difficulty: Advanced** **Objective:** Your task is to implement a custom server-client model using the `asyncio` module in Python. The server will handle multiple clients concurrently, echoing received messages but with a simulated processing delay. The client should be able to send multiple messages to the server and receive the echoed response for each message. **Requirements:** 1. **Server Implementation:** - Create an `asyncio` server that listens for incoming client connections on a specified host and port. - For each client connection, the server should start an asynchronous task to handle the connection. - The server should receive messages from the client, simulate a processing delay of 2 seconds, and then echo the message back to the client. - The server should handle multiple clients concurrently. 2. **Client Implementation:** - Create an `asyncio` client that connects to the server and sends a series of messages. - The client should wait for the response from the server for each message before sending the next one. - The client should print each response received from the server. **Input and Output:** - The server and client should be implemented in such a way that they can be run in separate processes or scripts. - The server should print a log message each time it accepts a new connection and each time it receives a message from a client. - The client should print each message sent and each response received. **Constraints:** - Use the `asyncio` module and its event loop functionalities. - The server must handle and echo messages asynchronously. - Use `asyncio.open_connection` for the client and `asyncio.start_server` for the server. **Performance Requirements:** - The server must be able to handle at least 10 concurrent client connections efficiently. **Server Example Usage:** ```python # Running the server if __name__ == \'__main__\': asyncio.run(run_server(\'localhost\', 8888)) ``` **Client Example Usage:** ```python # Running the client if __name__ == \'__main__\': asyncio.run(run_client(\'localhost\', 8888, [\\"Hello\\", \\"World\\", \\"Test\\"])) ``` # Code Skeleton: ```python import asyncio async def handle_client(reader, writer): # Handle a client connection pass async def run_server(host, port): server = await asyncio.start_server(handle_client, host, port) print(f\'Server running on {host}:{port}\') async with server: await server.serve_forever() async def run_client(host, port, messages): # Client side implementation pass if __name__ == \'__main__\': # Example of how to run the server or client based on user requirement import sys if sys.argv[1] == \'server\': host, port = \'localhost\', 8888 asyncio.run(run_server(host, port)) elif sys.argv[1] == \'client\': host, port = \'localhost\', 8888 messages = [\\"Hello\\", \\"World\\", \\"Test\\"] asyncio.run(run_client(host, port, messages)) ``` **Notes:** - The `handle_client` coroutine will manage individual client connections. - The `run_server` coroutine starts the server and listens for connections. - The `run_client` coroutine connects to the server and handles message sending and receiving. Good luck and happy coding!","solution":"import asyncio async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connected to {addr}\\") while True: data = await reader.read(100) message = data.decode() if not message: break print(f\\"Received \'{message}\' from {addr}\\") await asyncio.sleep(2) writer.write(data) await writer.drain() print(f\\"Sent \'{message}\' to {addr}\\") writer.close() await writer.wait_closed() print(f\\"Connection with {addr} closed\\") async def run_server(host, port): server = await asyncio.start_server(handle_client, host, port) print(f\'Server running on {host}:{port}\') async with server: await server.serve_forever() async def run_client(host, port, messages): reader, writer = await asyncio.open_connection(host, port) for message in messages: print(f\'Send: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed()"},{"question":"**Title**: Advanced Text Processing with Regular Expressions **Objective**: Implement a function to extract and replace specific patterns in text using regular expressions. **Description**: You are required to write a Python function that processes a given text using regular expressions. Your function will identify email addresses in the text and replace each email address with a placeholder. # Function Signature ```python def anonymize_emails(text: str) -> str: This function takes a string `text` as input and replaces all email addresses in the text with a placeholder \'[EMAIL]\'. Args: - text (str): The input string containing text with possible email addresses. Returns: - str: The processed string with email addresses replaced by \'[EMAIL]\'. ``` # Constraints - Email addresses will follow the pattern `username@domain.extension`, where: - `username` is a combination of alphanumeric characters, `.` (dot), `-` (dash), and `_` (underscore). - `domain` is a combination of alphanumeric characters and `.` (dot). - `extension` is a combination of lowercase alphabetic characters between 2 to 6 characters long. - Your function should be case-insensitive for email matching. - Performance should be considered for large inputs (up to 10^6 characters). # Example ```python text = \\"Please contact us at support@example.com for further information. Alternatively, send an email to info@my-site.org or webmaster@domain.co.uk.\\" result = anonymize_emails(text) print(result) ``` **Expected Output**: ``` \\"Please contact us at [EMAIL] for further information. Alternatively, send an email to [EMAIL] or [EMAIL].\\" ``` # Notes - You may use the `re` module for regular expression operations. - Ensure that your regular expression correctly identifies the email pattern as described under the constraints section. - Consider edge cases such as emails at the beginning, middle, and end of the text. **Hint**: You might find functions like `re.sub` useful for replacing matched patterns.","solution":"import re def anonymize_emails(text: str) -> str: This function takes a string `text` as input and replaces all email addresses in the text with a placeholder \'[EMAIL]\'. Args: - text (str): The input string containing text with possible email addresses. Returns: - str: The processed string with email addresses replaced by \'[EMAIL]\'. email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Za-z]{2,6}b\' result = re.sub(email_pattern, \'[EMAIL]\', text) return result"},{"question":"# Advanced Python Coding Assessment Problem Statement You are tasked with developing a function to manage and analyze employee shift logs spanning multiple time zones. Employees log their shift start and end times in their local time zones, but the company needs all shift logs standardized to UTC for accurate payroll calculations. Create a function `standardize_shift_logs(shift_logs)` that takes a list of shift logs and returns a new list of shift logs with start and end times converted to UTC. Detailed Requirements 1. **Input:** - `shift_logs`: A list of dictionaries where each dictionary represents a shift log with the following keys: - `employee_id` (int): Unique identifier for the employee. - `start_time` (str): Shift start time in ISO 8601 format (e.g., \\"2023-03-25T08:00:00-05:00\\"). - `end_time` (str): Shift end time in ISO 8601 format (e.g., \\"2023-03-25T16:00:00-05:00\\"). 2. **Output:** - A new list of dictionaries where each dictionary includes: - `employee_id` (int): Matches the input. - `start_time_utc` (str): Shift start time converted to UTC in ISO 8601 format. - `end_time_utc` (str): Shift end time converted to UTC in ISO 8601 format. - `shift_duration` (float): Duration of the shift in hours. 3. **Constraints:** - The given `start_time` and `end_time` are always valid ISO 8601 strings with timezone information. - Assume shifts are logged correctly such that `end_time` is always after `start_time`. 4. **Functions and Classes to Use:** - Use the `datetime` and `timezone` classes from the `datetime` module. - Utilize `timedelta` for calculating the shift duration. Example ```python shift_logs = [ {\\"employee_id\\": 1, \\"start_time\\": \\"2023-03-25T08:00:00-05:00\\", \\"end_time\\": \\"2023-03-25T16:00:00-05:00\\"}, {\\"employee_id\\": 2, \\"start_time\\": \\"2023-03-25T15:30:00+01:00\\", \\"end_time\\": \\"2023-03-25T23:00:00+01:00\\"} ] result = standardize_shift_logs(shift_logs) # Expected Output [ {\\"employee_id\\": 1, \\"start_time_utc\\": \\"2023-03-25T13:00:00+00:00\\", \\"end_time_utc\\": \\"2023-03-25T21:00:00+00:00\\", \\"shift_duration\\": 8.0}, {\\"employee_id\\": 2, \\"start_time_utc\\": \\"2023-03-25T14:30:00+00:00\\", \\"end_time_utc\\": \\"2023-03-25T22:00:00+00:00\\", \\"shift_duration\\": 7.5} ] ``` Implementation Implement the function `standardize_shift_logs(shift_logs)`: ```python from datetime import datetime, timezone, timedelta def standardize_shift_logs(shift_logs): standardized_logs = [] for log in shift_logs: employee_id = log[\\"employee_id\\"] start_time = log[\\"start_time\\"] end_time = log[\\"end_time\\"] # Parse the start and end times as datetime objects start_dt = datetime.fromisoformat(start_time) end_dt = datetime.fromisoformat(end_time) # Convert times to UTC start_time_utc = start_dt.astimezone(timezone.utc) end_time_utc = end_dt.astimezone(timezone.utc) # Calculate shift duration in hours shift_duration = (end_time_utc - start_time_utc).total_seconds() / 3600 # Format datetime objects to ISO 8601 strings log_utc = { \\"employee_id\\": employee_id, \\"start_time_utc\\": start_time_utc.isoformat(), \\"end_time_utc\\": end_time_utc.isoformat(), \\"shift_duration\\": shift_duration } standardized_logs.append(log_utc) return standardized_logs ``` Notes - Thoroughly test your function with different time zones and shift times. - Consider edge cases where shifts span multiple days.","solution":"from datetime import datetime, timezone def standardize_shift_logs(shift_logs): standardized_logs = [] for log in shift_logs: employee_id = log[\\"employee_id\\"] start_time = log[\\"start_time\\"] end_time = log[\\"end_time\\"] # Parse the start and end times as datetime objects start_dt = datetime.fromisoformat(start_time) end_dt = datetime.fromisoformat(end_time) # Convert times to UTC start_time_utc = start_dt.astimezone(timezone.utc) end_time_utc = end_dt.astimezone(timezone.utc) # Calculate shift duration in hours shift_duration = (end_time_utc - start_time_utc).total_seconds() / 3600 # Format datetime objects to ISO 8601 strings log_utc = { \\"employee_id\\": employee_id, \\"start_time_utc\\": start_time_utc.isoformat(), \\"end_time_utc\\": end_time_utc.isoformat(), \\"shift_duration\\": shift_duration } standardized_logs.append(log_utc) return standardized_logs"},{"question":"# Question: Custom Module Import Management **Objective:** Create a custom module import management system that efficiently handles the importing, reloading, and listing of modules using the functions provided by the `PyImport` module. This will test your understanding of the module importing process in Python, as well as your ability to manage and manipulate modules programmatically. **Description:** Implement a Python class called `ModuleManager` with the following methods: 1. `import_module(module_name: str) -> bool`: Imports a module by its name and returns `True` if successful, `False` otherwise. 2. `reload_module(module_name: str) -> bool`: Reloads an already imported module by its name and returns `True` if successful, `False` otherwise. 3. `list_imported_modules() -> list`: Returns a list of currently imported modules. 4. `get_module_details(module_name: str) -> dict`: Returns detailed information about a module, such as its name, file path, and import status. **Input and Output Formats:** - The `module_name` parameter is a string representing the module name (e.g., `\'os\'`, `\'sys\'`). - The return type for `import_module` and `reload_module` is a boolean indicating the success of the operation. - The `list_imported_modules` method returns a list of module names currently imported. - The `get_module_details` method returns a dictionary with keys `name`, `file`, and `import_status`, providing relevant details about the module. **Constraints:** - You may assume that the module names provided are valid. - You must handle exceptions and errors gracefully, ensuring that the state of the module manager remains consistent. **Performance Requirements:** - The implementation should be efficient, ensuring that module operations (import, reload, etc.) are performed with minimal overhead. ```python class ModuleManager: def import_module(self, module_name: str) -> bool: # Write your code here pass def reload_module(self, module_name: str) -> bool: # Write your code here pass def list_imported_modules(self) -> list: # Write your code here pass def get_module_details(self, module_name: str) -> dict: # Write your code here pass # Example usage: # manager = ModuleManager() # print(manager.import_module(\'os\')) # True # print(manager.reload_module(\'os\')) # True # print(manager.list_imported_modules()) # [\'os\', ...] # print(manager.get_module_details(\'os\')) # {\'name\': \'os\', \'file\': \'/usr/lib/python3.10/os.py\', \'import_status\': \'imported\'} ``` Please ensure your implementation is robust, efficient, and adheres to Python\'s module import mechanics as outlined in the provided documentation.","solution":"import importlib import sys class ModuleManager: def import_module(self, module_name: str) -> bool: try: globals()[module_name] = importlib.import_module(module_name) return True except ImportError: return False def reload_module(self, module_name: str) -> bool: if module_name in sys.modules: try: importlib.reload(sys.modules[module_name]) return True except ImportError: return False return False def list_imported_modules(self) -> list: return list(sys.modules.keys()) def get_module_details(self, module_name: str) -> dict: if module_name in sys.modules: module = sys.modules[module_name] return { \'name\': module_name, \'file\': getattr(module, \'__file__\', \'Built-in Module\'), \'import_status\': \'imported\' } else: return { \'name\': module_name, \'file\': None, \'import_status\': \'not imported\' }"},{"question":"# Advanced Python Programming: Generator Implementation Problem Statement Generators are a special type of iterable in Python, created using functions and the `yield` statement. They allow iteration over a sequence of values without the need to create and store the entire sequence in memory at once, making them highly efficient for large datasets or streams of data. Your task is to implement a generator function that enumerates all prime numbers up to a given number `n`. Function Signature ```python def generate_primes(n: int) -> Generator[int, None, None]: pass ``` Input - An integer `n` (1 ≤ n ≤ 10^6), representing the upper limit of the range within which to generate prime numbers. Output - The function should yield prime numbers sequentially from 2 up to and including `n`. Constraints - The function must use the `yield` statement to generate the primes. - The function should be efficient and handle the upper constraint within a reasonable time limit. Example ```python # Example Usage primes_gen = generate_primes(10) print(list(primes_gen)) # Output: [2, 3, 5, 7] ``` Hints 1. Recall that a prime number is only divisible by 1 and itself. 2. Consider using the Sieve of Eratosthenes algorithm to generate primes efficiently. Implement the function `generate_primes()` to complete the task.","solution":"from typing import Generator def generate_primes(n: int) -> Generator[int, None, None]: Generator function to yield prime numbers up to and including n. :param n: The upper limit of the range to generate prime numbers. :return: Yields prime numbers from 2 up to and including n. if n < 2: return sieve = [True] * (n + 1) sieve[0], sieve[1] = False, False # 0 and 1 are not primes for start in range(2, n + 1): if sieve[start]: yield start for multiple in range(start*start, n + 1, start): sieve[multiple] = False"},{"question":"**Objective:** Implement a simple REPL (Read-Eval-Print Loop) using the `codeop` module to handle multi-line statements and remember `__future__` imports between inputs. **Question:** You are tasked with creating a mini Python REPL that can: 1. Accept multi-line input from the user and determine whether a full statement is completed or more input is needed. 2. Execute the completed statements. 3. Remember any `__future__` imports across multiple inputs. # Requirements 1. Implement a class `MiniREPL` with the following methods: - `__init__(self)`: Initializes an instance of the REPL. - `read_input(self, source: str) -> bool`: Accepts a string of Python code and determines if it forms a complete statement that can be executed. If not, returns `False` indicating more input is needed. - `execute(self, source: str) -> None`: Executes the given Python code if it forms a complete statement. - `run(self) -> None`: Prompts the user for input and manages the multi-line input, determining when a complete statement is achieved and then executing it. 2. Use the `compile_command` and `CommandCompiler` from the `codeop` module to handle the compilation and execution of code. # Input and Output - **Input:** Multi-line Python code from the user. - **Output:** The result of executing the Python code, printed to the console. # Constraints - Only valid Python code should be passed for execution. - The REPL should remember `__future__` imports for subsequent inputs. # Example ```python # Initialize the REPL repl = MiniREPL() repl.run() # Example of user interaction with the REPL >>> from __future__ import division >>> 1/2 0.5 >>> 3 + 4 7 ``` # Notes - Use the `codeop.compile_command` function to check if a statement is complete. - Use the `CommandCompiler` class to manage and remember `__future__` imports. # Template ```python import codeop class MiniREPL: def __init__(self): self.command_compiler = codeop.CommandCompiler() def read_input(self, source: str) -> bool: try: compiled_code = self.command_compiler(source) return compiled_code is not None except Exception as e: print(f\\"Error: {e}\\") return False def execute(self, source: str) -> None: try: compiled_code = self.command_compiler(source) if compiled_code: exec(compiled_code) else: print(\\"Incomplete statement, more input needed.\\") except Exception as e: print(f\\"Execution Error: {e}\\") def run(self) -> None: print(\\"MiniPython REPL. Type your code below.\\") source_lines = [] while True: try: line = input(\'>>> \' if not source_lines else \'... \') source_lines.append(line) source_code = \'n\'.join(source_lines) if self.read_input(source_code): self.execute(source_code) source_lines = [] # Reset the input buffer after execution else: print(\\"...\\") # Indicate that more input is needed except (KeyboardInterrupt, EOFError): print(\\"nExiting REPL.\\") break # Uncomment the following lines to run the REPL: # repl = MiniREPL() # repl.run() ```","solution":"import codeop class MiniREPL: def __init__(self): self.command_compiler = codeop.CommandCompiler() def read_input(self, source: str) -> bool: try: compiled_code = self.command_compiler(source) return compiled_code is not None except Exception as e: print(f\\"Error: {e}\\") return False def execute(self, source: str) -> None: try: compiled_code = self.command_compiler(source) if compiled_code: exec(compiled_code) else: print(\\"Incomplete statement, more input needed.\\") except Exception as e: print(f\\"Execution Error: {e}\\") def run(self) -> None: print(\\"MiniPython REPL. Type your code below.\\") source_lines = [] while True: try: line = input(\'>>> \' if not source_lines else \'... \') source_lines.append(line) source_code = \'n\'.join(source_lines) if self.read_input(source_code): self.execute(source_code) source_lines = [] # Reset the input buffer after execution else: print(\\"...\\") # Indicate that more input is needed except (KeyboardInterrupt, EOFError): print(\\"nExiting REPL.\\") break"},{"question":"# Turtle Graphics Coding Challenge **Objective**: Enhance your understanding of the `turtle` module by creating a graphical drawing program. **Problem Statement**: You are required to write a Python program that utilizes the `turtle` module to draw geometric shapes based on user input. Your program should allow the user to input a sequence of commands to draw a series of shapes. Specifically, the program should support commands to draw a square, a circle, and a triangle. Additionally, the program should provide options to change the pen color and pen size. **Requirements**: 1. **Input Format**: - The program should read commands from the user in the following format: - `DRAW <shape> <size>` where `<shape>` can be `square`, `circle`, or `triangle` and `<size>` is the dimension of the shape. - `COLOR <color>` where `<color>` is the color name (e.g., \\"red\\", \\"blue\\"). - `PENSIZE <size>` where `<size>` is the thickness of the pen. - The program should terminate when the user inputs `EXIT`. 2. **Output**: - The program should draw the shapes on a turtle graphics window according to the commands entered by the user. 3. **Constraints**: - You must use the `turtle` module to implement this functionality. - Ensure the shapes are drawn proportionally based on the size specified. **Example Usage**: ``` Commands: COLOR red PENSIZE 3 DRAW square 100 COLOR blue DRAW circle 50 PENSIZE 5 DRAW triangle 150 EXIT ``` **Expected Output**: - A turtle graphics window with: - A red square of side length 100 and pen size 3. - A blue circle of radius 50. - A blue triangle with side length 150 and pen size 5. **Starter Code**: ```python import turtle def draw_square(t, size): for _ in range(4): t.forward(size) t.right(90) def draw_circle(t, radius): t.circle(radius) def draw_triangle(t, size): for _ in range(3): t.forward(size) t.left(120) def main(): screen = turtle.Screen() t = turtle.Turtle() while True: command = input(\\"Enter command: \\").strip().split() if not command: continue cmd = command[0].upper() if cmd == \\"EXIT\\": break if cmd == \\"COLOR\\": color = command[1] t.color(color) elif cmd == \\"PENSIZE\\": size = int(command[1]) t.pensize(size) elif cmd == \\"DRAW\\": shape = command[1].lower() size = int(command[2]) if shape == \\"square\\": draw_square(t, size) elif shape == \\"circle\\": draw_circle(t, size) elif shape == \\"triangle\\": draw_triangle(t, size) else: print(\\"Unknown shape:\\", shape) screen.mainloop() if __name__ == \\"__main__\\": main() ``` **Performance Considerations**: - The program should handle invalid input gracefully and prompt the user again until a valid command is entered. By crafting this question, students will demonstrate their ability to: - Utilize the `turtle` module for graphical drawing. - Implement input handling and parsing. - Apply logic to conditionally execute drawing commands based on user input.","solution":"import turtle def draw_square(t, size): for _ in range(4): t.forward(size) t.right(90) def draw_circle(t, radius): t.circle(radius) def draw_triangle(t, size): for _ in range(3): t.forward(size) t.left(120) def main(): screen = turtle.Screen() t = turtle.Turtle() while True: command = input(\\"Enter command: \\").strip().split() if not command: continue cmd = command[0].upper() if cmd == \\"EXIT\\": break if cmd == \\"COLOR\\": color = command[1] t.color(color) elif cmd == \\"PENSIZE\\": size = int(command[1]) t.pensize(size) elif cmd == \\"DRAW\\": shape = command[1].lower() size = int(command[2]) if shape == \\"square\\": draw_square(t, size) elif shape == \\"circle\\": draw_circle(t, size) elif shape == \\"triangle\\": draw_triangle(t, size) else: print(\\"Unknown shape:\\", shape) screen.mainloop() if __name__ == \\"__main__\\": main()"},{"question":"**Title: Implement a Python Code Evaluator** **Objective:** Create a Python function `execute_code` that mimics the behavior of the high-level Python interpreter functions described in the documentation. Specifically, this function should accept Python source code as a string and execute it within a given global and local context. **Function Signature:** ```python def execute_code(source: str, globals_dict: dict, locals_dict: dict) -> any: Executes the given Python source code within the provided global and local context. Parameters: - source (str): The Python source code to execute. - globals_dict (dict): The global context in which to execute the code. - locals_dict (dict): The local context in which to execute the code. Returns: - any: The result of executing the source code if it is an expression; otherwise, None. Raises: - SyntaxError: If there is a syntax error in the source code. - Exception: If any other exception occurs during execution. ``` **Requirements:** 1. The function should compile the given source code using appropriate Python\'s built-in functions. 2. The function should execute the compiled code using the provided `globals_dict` and `locals_dict`. 3. If the provided source code is an expression (e.g., `1 + 1`), the function should return its result. 4. If the provided source code is a statement or sequence of statements (e.g., `print(\\"hello\\")`), the function should return `None`. 5. The function must handle and re-raise any exceptions that occur during compilation or execution, along with appropriate error messages. **Constraints:** - The `source` string should be valid Python code. - The `globals_dict` and `locals_dict` should be dictionaries. - The execution context should isolate changes to `globals_dict` and `locals_dict` without affecting external variables. **Example Usage:** ```python globals_context = {} locals_context = {} result = execute_code(\\"a = 1 + 1\\", globals_context, locals_context) print(result) # Output: None print(locals_context[\\"a\\"]) # Output: 2 result = execute_code(\\"5 + 5\\", globals_context, locals_context) print(result) # Output: 10 ``` In this task, students will demonstrate their understanding of executing and handling Python code dynamically, akin to what is described in the provided documentation.","solution":"def execute_code(source: str, globals_dict: dict, locals_dict: dict) -> any: Executes the given Python source code within the provided global and local contexts. Parameters: - source (str): The Python source code to execute. - globals_dict (dict): The global context in which to execute the code. - locals_dict (dict): The local context in which to execute the code. Returns: - any: The result of executing the source code if it is an expression; otherwise, None. Raises: - SyntaxError: If there is a syntax error in the source code. - Exception: If any other exception occurs during execution. try: # First attempt to compile the code as an expression code_object = compile(source, \'<string>\', \'eval\') return eval(code_object, globals_dict, locals_dict) except SyntaxError: # If it is not an expression, compile it as execution of statement(s) code_object = compile(source, \'<string>\', \'exec\') exec(code_object, globals_dict, locals_dict) return None except Exception as e: raise e"},{"question":"**Question**: You need to build a machine learning pipeline using scikit-learn and optimize its performance by leveraging parallelism techniques. The task involves training a model on a large dataset where parallelism can significantly improve performance. You should: 1. Implement a training function that uses GridSearchCV to find the best hyperparameters for a classifier. 2. Ensure the function efficiently handles parallelism using `n_jobs` parameter and optimizes resource usage to avoid oversubscription. 3. Configure the environment to set an appropriate number of threads for lower-level parallelism. **Instructions**: 1. **Function Signature**: ```python def train_model_with_parallelism(X_train, y_train): Trains a model using GridSearchCV with parallelism and returns the best estimator. Parameters: X_train (numpy.ndarray): The training data features. y_train (numpy.ndarray): The training data labels. Returns: sklearn.base.BaseEstimator: The best estimator found during hyperparameter tuning. ``` 2. **Input**: - `X_train` (numpy.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the training data features. - `y_train` (numpy.ndarray): A 1D numpy array of shape (n_samples,) representing the training data labels. 3. **Output**: - The function should return the best estimator found during the hyperparameter tuning process. 4. **Constraints**: - Use `RandomForestClassifier` from scikit-learn. - Use `GridSearchCV` for hyperparameter tuning with the following parameters to search: - `n_estimators`: [50, 100] - `max_depth`: [None, 10, 20] - Perform 3-fold cross-validation. - Optimize the usage of CPU resources by setting `n_jobs=-1`. - Prevent oversubscription by setting the relevant environment variables appropriately. 5. **Performance Requirements**: - The solution should efficiently utilize parallelism without causing oversubscription issues. - The function should be able to handle large datasets efficiently. **Example**: ```python import numpy as np from sklearn.datasets import make_classification from train_model_with_parallelism import train_model_with_parallelism # Generate a large synthetic dataset X_train, y_train = make_classification(n_samples=10000, n_features=20, random_state=42) # Train the model with parallelism best_model = train_model_with_parallelism(X_train, y_train) print(best_model) ``` **Hints**: - Use `joblib`\'s `parallel_backend` context manager to specify the backend used for parallelism. - Set environment variables for controlling the number of threads for lower-level parallelism before importing scikit-learn.","solution":"import os import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV # Ensure environment variables are set before importing libraries that use multithreading os.environ[\'OMP_NUM_THREADS\'] = \'1\' os.environ[\'MKL_NUM_THREADS\'] = \'1\' def train_model_with_parallelism(X_train, y_train): Trains a model using GridSearchCV with parallelism and returns the best estimator. Parameters: X_train (numpy.ndarray): The training data features. y_train (numpy.ndarray): The training data labels. Returns: sklearn.base.BaseEstimator: The best estimator found during hyperparameter tuning. param_grid = { \'n_estimators\': [50, 100], \'max_depth\': [None, 10, 20], } classifier = RandomForestClassifier(random_state=42) grid_search = GridSearchCV( estimator=classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=0 ) # Fit grid search grid_search.fit(X_train, y_train) # Return the best estimator return grid_search.best_estimator_"},{"question":"**Question:** Implement a simple TCP server and client in Python using the `socket` module. The client should send a message to the server, and the server should send a response back to the client. # Requirements 1. **Server Implementation**: - The server should be able to handle multiple clients by creating a new thread for each client connection. - The server should listen on an IP address and port specified by the user. - For each client, the server should receive a message, print it, and then send a response \\"Hello, Client!\\" back to the client. - The server should properly handle socket exceptions and ensure that all sockets are closed when done. 2. **Client Implementation**: - The client should connect to the server using the IP address and port specified by the user. - The client should send a user-specified message to the server and print the response from the server. - The client should handle socket exceptions and ensure that the socket is closed when done. # Input and Output Formats - **Server**: - Input: An IP address and port to listen on. - Output: Print client messages and send responses. - **Client**: - Input: Server IP address, port, and the message to be sent. - Output: Print the response from the server. # Constraints - The server should be capable of handling at least 5 clients simultaneously. - Both server and client should handle possible exceptions gracefully and ensure resources are cleaned up on exit. # Example The following example demonstrates the required functionality. Server ```python import socket import threading def handle_client(connection, address): print(f\\"Connected by {address}\\") try: while True: data = connection.recv(1024) if not data: break print(f\\"Received from {address}: {data.decode()}\\") connection.sendall(b\\"Hello, Client!\\") finally: connection.close() def start_server(host, port): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.bind((host, port)) s.listen() print(f\\"Server listening on {host}:{port}\\") while True: conn, addr = s.accept() client_thread = threading.Thread(target=handle_client, args=(conn, addr)) client_thread.start() if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 65432 start_server(host, port) ``` Client ```python import socket def start_client(server_ip, server_port, message): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: s.connect((server_ip, server_port)) s.sendall(message.encode()) data = s.recv(1024) print(f\\"Received from server: {data.decode()}\\") if __name__ == \\"__main__\\": server_ip = \'127.0.0.1\' server_port = 65432 message = \\"Hello, Server!\\" start_client(server_ip, server_port, message) ``` # Evaluation Criteria - Correctness: The implementation should meet all requirements and constraints specified. - Exception Handling: The implementation should handle socket-related exceptions gracefully. - Code Quality: The code should be clean, well-structured, and include appropriate documentation and comments. Submit your implementation for both the server and client.","solution":"import socket import threading def handle_client(connection, address): print(f\\"Connected by {address}\\") try: while True: data = connection.recv(1024) if not data: break print(f\\"Received from {address}: {data.decode()}\\") connection.sendall(b\\"Hello, Client!\\") except Exception as e: print(f\\"Exception handling client {address}: {e}\\") finally: connection.close() print(f\\"Connection with {address} closed\\") def start_server(host, port): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket: server_socket.bind((host, port)) server_socket.listen() print(f\\"Server listening on {host}:{port}\\") try: while True: client_socket, client_address = server_socket.accept() client_thread = threading.Thread(target=handle_client, args=(client_socket, client_address)) client_thread.start() except Exception as e: print(f\\"Server exception: {e}\\") def start_client(server_ip, server_port, message): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as client_socket: try: client_socket.connect((server_ip, server_port)) client_socket.sendall(message.encode()) data = client_socket.recv(1024) print(f\\"Received from server: {data.decode()}\\") except Exception as e: print(f\\"Client exception: {e}\\")"},{"question":"**Problem Statement: Celestial Body Tracker** You are tasked with designing a simple celestial body tracking system to keep track of some parameters of various celestial bodies (like planets, stars, or moons). The system should allow adding new celestial bodies and performing basic calculations and manipulations. Your implementation should demonstrate a clear understanding of arithmetic operations, strings, and lists. # Requirements: 1. **Function: add_celestial_body** ```python def add_celestial_body(bodies_list, name, diameter, distance_from_earth, body_type): Adds a new celestial body to the list. Parameters: - bodies_list (list): A list containing dictionaries, each representing a celestial body. - name (str): The name of the celestial body. - diameter (float): The diameter of the celestial body in kilometers. - distance_from_earth (float): The average distance from Earth in light-years. - body_type (str): The type of celestial body (e.g., \\"planet\\", \\"star\\", \\"moon\\"). Returns: - list: Updated list with the new celestial body added. ``` 2. **Function: calculate_average_distance** ```python def calculate_average_distance(bodies_list): Calculates the average distance from Earth of all celestial bodies. Parameters: - bodies_list (list): A list containing dictionaries, each representing a celestial body. Returns: - float: The average distance from Earth of all celestial bodies. ``` 3. **Function: get_largest_diameter** ```python def get_largest_diameter(bodies_list): Finds the celestial body with the largest diameter. Parameters: - bodies_list (list): A list containing dictionaries, each representing a celestial body. Returns: - str: The name of the celestial body with the largest diameter. ``` # Example Usage: ```python celestial_bodies = [] # Adding celestial bodies celestial_bodies = add_celestial_body(celestial_bodies, \\"Earth\\", 12742, 0.0000158, \\"planet\\") celestial_bodies = add_celestial_body(celestial_bodies, \\"Jupiter\\", 139820, 0.000082332, \\"planet\\") celestial_bodies = add_celestial_body(celestial_bodies, \\"Sun\\", 1391016, 0.0000158, \\"star\\") # Calculate average distance average_distance = calculate_average_distance(celestial_bodies) print(f\\"Average Distance: {average_distance} light-years\\") # Get largest diameter body largest_body = get_largest_diameter(celestial_bodies) print(f\\"Largest Celestial Body: {largest_body}\\") ``` # Constraints: - The `bodies_list` parameter will always be a list, and you can assume proper data will be passed for other parameters. - Diameter and distance values will always be non-negative floats. - Names and types are non-empty strings.","solution":"def add_celestial_body(bodies_list, name, diameter, distance_from_earth, body_type): Adds a new celestial body to the list. Parameters: - bodies_list (list): A list containing dictionaries, each representing a celestial body. - name (str): The name of the celestial body. - diameter (float): The diameter of the celestial body in kilometers. - distance_from_earth (float): The average distance from Earth in light-years. - body_type (str): The type of celestial body (e.g., \\"planet\\", \\"star\\", \\"moon\\"). Returns: - list: Updated list with the new celestial body added. new_body = { \\"name\\": name, \\"diameter\\": diameter, \\"distance_from_earth\\": distance_from_earth, \\"body_type\\": body_type } bodies_list.append(new_body) return bodies_list def calculate_average_distance(bodies_list): Calculates the average distance from Earth of all celestial bodies. Parameters: - bodies_list (list): A list containing dictionaries, each representing a celestial body. Returns: - float: The average distance from Earth of all celestial bodies. if not bodies_list: return 0.0 total_distance = sum(body[\'distance_from_earth\'] for body in bodies_list) return total_distance / len(bodies_list) def get_largest_diameter(bodies_list): Finds the celestial body with the largest diameter. Parameters: - bodies_list (list): A list containing dictionaries, each representing a celestial body. Returns: - str: The name of the celestial body with the largest diameter. if not bodies_list: return \\"\\" largest_body = max(bodies_list, key=lambda body: body[\'diameter\']) return largest_body[\'name\']"},{"question":"**Question: Implement an Email Message Processor** You are provided with an email message in string format, including headers and body. Implement a function `process_email_message(email_str: str) -> dict` that performs the following tasks: 1. Parse the given email string into a `Message` object. 2. Extract and return the following details in a dictionary: - `subject`: The subject of the email. - `from`: The sender of the email. - `to`: The recipient(s) of the email. - `content_type`: The content type of the email. - `is_multipart`: A boolean indicating whether the email is a multipart message. - `payload`: The main body of the email. If the email is multipart, return a list of the payloads of each part. # Guidelines: 1. Use the `email.message.Message` class and relevant methods for parsing and extracting information. 2. Handle cases where the required headers or payloads might be missing by returning `None` for those fields. 3. Ensure that the payload is returned in string format even if it is encoded. # Function Signature ```python def process_email_message(email_str: str) -> dict: # Implementation here ``` # Example ```python email_str = From: alice@example.com To: bob@example.com Subject: Test Email Content-Type: text/plain Hello, this is a test email. result = process_email_message(email_str) print(result) # Expected Output: # { # \\"subject\\": \\"Test Email\\", # \\"from\\": \\"alice@example.com\\", # \\"to\\": \\"bob@example.com\\", # \\"content_type\\": \\"text/plain\\", # \\"is_multipart\\": False, # \\"payload\\": \\"Hello, this is a test email.\\" # } ``` # Constraints - Assume that the input email string is correctly formatted as per RFC 5322. - For multipart messages, ensure that you extract and decode each part\'s payload if it is encoded. **Performance Requirements:** - The function should efficiently parse and process typical email messages, with an expected time complexity of O(n) relative to the length of the provided email string.","solution":"import email from email import policy from email.parser import BytesParser def process_email_message(email_str: str) -> dict: Parse and extract details from an email message string. Parameters: email_str (str): The email message in string format. Returns: dict: A dictionary with extracted email details. msg = BytesParser(policy=policy.default).parsebytes(email_str.encode()) result = { \'subject\': msg[\'subject\'], \'from\': msg[\'from\'], \'to\': msg[\'to\'], \'content_type\': msg.get_content_type(), \'is_multipart\': msg.is_multipart(), \'payload\': None } if msg.is_multipart(): result[\'payload\'] = [part.get_payload(decode=True).decode(part.get_content_charset(failobj=\'utf-8\')) for part in msg.iter_parts()] else: result[\'payload\'] = msg.get_payload(decode=True).decode(msg.get_content_charset(failobj=\'utf-8\')) return result"},{"question":"Linear Algebra Stability in PyTorch **Objective**: Implement a function that computes the Singular Value Decomposition (SVD) of a given matrix and checks for potential issues with numerical stability. Given a matrix (A), your function should compute its SVD, analyze the singular values, and return an indication of whether the matrix is ill-conditioned. **Function Signature**: ```python def check_svd_stability(A: torch.Tensor) -> Tuple[bool, torch.Tensor, torch.Tensor, torch.Tensor]: Computes the SVD of a matrix and checks for ill-conditioning. Args: A (torch.Tensor): A 2D tensor representing the matrix to decompose. Returns: Tuple[bool, torch.Tensor, torch.Tensor, torch.Tensor]: - A boolean indicating if the matrix is ill-conditioned (True if ill-conditioned, else False). - The left singular vectors (U). - The singular values (S). - The right singular vectors (V). ``` **Inputs**: - `A`: A 2D tensor of shape (m, n) with dtype `torch.float64`. **Outputs**: - A tuple containing: - A boolean indicating if the matrix is ill-conditioned. - The left singular vectors as a 2D tensor. - The singular values as a 1D tensor. - The right singular vectors as a 2D tensor. **Instructions**: 1. **Compute SVD**: Use PyTorch’s `torch.linalg.svd` function to compute the SVD of the input matrix (A). 2. **Check for Ill-Conditioning**: - Calculate the condition number of the matrix, which is the ratio of the largest singular value to the smallest singular value. - If the condition number exceeds `1e10`, consider the matrix ill-conditioned. 3. **Return Results**: Return the boolean indicating ill-conditioning status and the components of the SVD (U, S, V). **Constraints**: - Handle any potential numerical issues that might arise due to extremely large or small values in the matrix. - The input matrix will always have dimensions where both m and n are greater than 1. **Example**: ```python import torch A = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float64) ill_conditioned, U, S, V = check_svd_stability(A) print(\\"Ill-conditioned:\\", ill_conditioned) # Output should be False for this matrix print(\\"U:\\", U) print(\\"S:\\", S) print(\\"V:\\", V) ``` Use this question to assess the students\' understanding of numerical stability in linear algebra operations using PyTorch, and their ability to handle floating point precision issues.","solution":"import torch from typing import Tuple def check_svd_stability(A: torch.Tensor) -> Tuple[bool, torch.Tensor, torch.Tensor, torch.Tensor]: Computes the SVD of a matrix and checks for ill-conditioning. Args: A (torch.Tensor): A 2D tensor representing the matrix to decompose. Returns: Tuple[bool, torch.Tensor, torch.Tensor, torch.Tensor]: - A boolean indicating if the matrix is ill-conditioned (True if ill-conditioned, else False). - The left singular vectors (U). - The singular values (S). - The right singular vectors (V). # Perform SVD U, S, V = torch.linalg.svd(A, full_matrices=False) # Calculate the condition number condition_number = S[0].item() / S[-1].item() # Check if the matrix is ill-conditioned ill_conditioned = condition_number > 1e10 return ill_conditioned, U, S, V"},{"question":"Objective: Your task is to implement two Python functions that demonstrate the use of the `plistlib` module’s capabilities to serialize and deserialize data to and from plist files. Problem Statement: You need to implement the following two functions: 1. **`serialize_to_plist(data: dict, file_path: str, fmt: str = \'FMT_XML\') -> None`**: - This function should take a dictionary `data`, a string `file_path`, and an optional string `fmt` which can be either \'FMT_XML\' or \'FMT_BINARY\'. - It should write the dictionary `data` to a plist file specified by `file_path` in the format specified by `fmt`. - The plist should have keys sorted in alphabetical order. 2. **`deserialize_from_plist(file_path: str, fmt: str = \'FMT_XML\') -> dict`**: - This function should take a string `file_path` and an optional string `fmt` which can be either \'FMT_XML\' or \'FMT_BINARY\'. - It should read the plist file at `file_path` and return the deserialized dictionary. Constraints: - The `fmt` parameter must either be \'FMT_XML\' or \'FMT_BINARY\'. - The `data` dictionary must only contain types supported by the `plistlib` module. - If the `file_path` does not exist in the `deserialize_from_plist` function, the function should raise a `FileNotFoundError`. Example: ```python import datetime sample_data = { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"is_student\\": False, \\"courses\\": [\\"Math\\", \\"Physics\\", \\"Computer Science\\"], \\"graduation_date\\": datetime.datetime(2023, 6, 15, 10, 0, 0), \\"grades\\": { \\"Math\\": \\"A\\", \\"Physics\\": \\"B+\\", \\"Computer Science\\": \\"A-\\" } } # Example usage serialize_to_plist(sample_data, \'student_data.plist\', fmt=\'FMT_XML\') deserialized_data = deserialize_from_plist(\'student_data.plist\', fmt=\'FMT_XML\') assert deserialized_data == sample_data # The data should match after serialization and deserialization ``` Notes: - You may use the `plistlib` module’s `dump` and `load` functions to serialize and deserialize the data respectively. - Ensure the serialized plist file has keys sorted alphabetically. - Handle any potential exceptions such as invalid file paths or unsupported formats.","solution":"import plistlib from typing import Dict def serialize_to_plist(data: Dict, file_path: str, fmt: str = \'FMT_XML\') -> None: Serializes the given dictionary `data` into a plist file at `file_path` in the specified format `fmt`. :param data: Dictionary to be serialized. :param file_path: Path to the plist file where the data will be saved. :param fmt: Format of the plist file, either \'FMT_XML\' or \'FMT_BINARY\'. Defaults to \'FMT_XML\'. if fmt not in [\'FMT_XML\', \'FMT_BINARY\']: raise ValueError(\\"fmt must be either \'FMT_XML\' or \'FMT_BINARY\'\\") sorted_data = {k: data[k] for k in sorted(data.keys())} with open(file_path, \'wb\') as fp: plistlib.dump(sorted_data, fp, fmt=plistlib.FMT_XML if fmt == \'FMT_XML\' else plistlib.FMT_BINARY) def deserialize_from_plist(file_path: str, fmt: str = \'FMT_XML\') -> Dict: Deserializes a plist file at `file_path` and returns the data as a dictionary. :param file_path: Path to the plist file to be deserialized. :param fmt: Format of the plist file, either \'FMT_XML\' or \'FMT_BINARY\'. This parameter is for compatibility; it\'s not used by plistlib.load(). :return: Deserialized data as a dictionary. try: with open(file_path, \'rb\') as fp: data = plistlib.load(fp) except FileNotFoundError: raise FileNotFoundError(f\\"The file {file_path} does not exist.\\") return data"},{"question":"Objective: The goal of this assignment is to assess your ability to use seaborn for visualizing data using the ECDF plotting functionality effectively. You are required to write a function that takes a dataset, a list of numeric columns, and other optional parameters, and generates a ECDF plot showing the distribution of these columns. Function Signature: ```python def plot_custom_ecdf( dataset: str, columns: list, hue: str = None, stat: str = \'proportion\', complementary: bool = False ) -> None: pass ``` Input Parameters: - `dataset` (str): The name of the dataset to be loaded using `sns.load_dataset`. Example: \'penguins\'. - `columns` (list): A list of string names of the numeric columns to be plotted. Example: [\'bill_length_mm\', \'flipper_length_mm\']. - `hue` (str, optional): The name of the column to use for hue mapping which should be a categorical column. Default is None. - `stat` (str, optional): The statistical metric to plot. Options are \'proportion\', \'count\', and \'percent\'. Default is \'proportion\'. - `complementary` (bool, optional): Whether to plot the complementary ECDF (1 - CDF). Default is False. Output: - The function should display an ECDF plot according to the given column names in the dataset and other parameters. Constraints: - Use seaborn for loading the dataset and plotting. - Ensure to handle the case where the columns list may include non-existent or non-numeric columns by skipping them and plotting only valid numeric ones. - If no columns are valid for plotting, print a message indicating no valid columns were provided. Example Usage: ```python plot_custom_ecdf(\'penguins\', [\'bill_length_mm\', \'flipper_length_mm\'], hue=\'species\', stat=\'count\', complementary=True) ``` This function call should load the \'penguins\' dataset and plot the ECDF of \'bill_length_mm\' and \'flipper_length_mm\' with species-wise hue mapping, using count as the statistic, and plot the complementary ECDF. Requirements: 1. Ensure you import necessary libraries (seaborn, matplotlib, pandas). 2. Handle cases where the dataset or columns might not be available gracefully. 3. Use appropriate seaborn functions and methods to achieve the required functionality. This assignment will test your understanding of loading datasets, plotting ECDFs, handling various data distribution statistics, hue mappings, and complementary functions in seaborn.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_custom_ecdf(dataset: str, columns: list, hue: str = None, stat: str = \'proportion\', complementary: bool = False) -> None: Plots an ECDF for the given dataset and columns with optional hue, stat, and complementary parameters. Parameters: - dataset (str): The name of the dataset to be loaded using `sns.load_dataset`. - columns (list): A list of string names of the numeric columns to be plotted. - hue (str, optional): The name of the column to use for hue mapping which should be a categorical column. Default is None. - stat (str, optional): The statistical metric to plot. Options are \'proportion\', \'count\', and \'percent\'. Default is \'proportion\'. - complementary (bool, optional): Whether to plot the complementary ECDF (1 - CDF). Default is False. Returns: - None try: data = sns.load_dataset(dataset) except: print(f\\"Error: The dataset \'{dataset}\' is not available.\\") return valid_columns = [col for col in columns if col in data.columns and pd.api.types.is_numeric_dtype(data[col])] if not valid_columns: print(\\"Error: No valid numeric columns provided for plotting.\\") return plt.figure(figsize=(10, 6)) for col in valid_columns: sns.ecdfplot(data=data, x=col, hue=hue, stat=stat, complementary=complementary) plt.title(f\\"ECDF Plot for {\', \'.join(valid_columns)}\\") plt.show()"},{"question":"# Pandas Advanced Data Manipulation and Analysis Objective Design a function that performs advanced data manipulation and analysis on a given dataset using pandas. This function should: 1. Read data from a CSV file. 2. Clean the data by handling missing values. 3. Perform a group-by operation and calculate summary statistics. 4. Apply a rolling window calculation on the grouped data. 5. Generate a plot based on the analysis results. Function Signature ```python def analyze_data(file_path: str) -> None: Perform data analysis on the dataset provided in the CSV file at the given path. Parameters: file_path (str): The path to the CSV file containing the dataset. This function reads the dataset, cleans it, performs grouping and summary statistics calculations, applies a rolling window calculation, and generates a plot to visualize the results. The function should display the plot and return nothing. Constraints: - The dataset will contain a \'Date\' column which should be set as the index. - Missing values should be filled using forward fill method. - Grouping should be based on a predefined column (e.g., \'Category\'). - Summary statistics to be computed include mean and sum of relevant columns. - Use a rolling window of size 3 for calculations. ``` Input - A path to the CSV file (`file_path`). Output - The function does not return anything, but it should display a plot. Example Assume the dataset in the CSV file looks as follows: | Date | Category | Value | |------------|-----------|-------| | 2023-01-01 | A | 10 | | 2023-01-02 | A | 20 | | 2023-01-03 | A | None | | 2023-01-04 | B | 30 | | 2023-01-05 | B | 40 | Steps to solve the problem: 1. Read the dataset and parse the \'Date\' column as datetime. 2. Set the \'Date\' column as the index. 3. Handle missing values by performing forward fill. 4. Group the data by \'Category\' and calculate summary statistics (mean and sum) for the \'Value\' column. 5. Apply a rolling window of size 3 to the grouped data. 6. Generate and display a plot illustrating the rolling mean of \'Value\' for each category. Requirements - The solution should utilize pandas library functions like `read_csv`, `groupby`, `rolling`, and plotting functionalities from `pandas.plotting` or `matplotlib`.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_data(file_path: str) -> None: Perform data analysis on the dataset provided in the CSV file at the given path. Parameters: file_path (str): The path to the CSV file containing the dataset. This function reads the dataset, cleans it, performs grouping and summary statistics calculations, applies a rolling window calculation, and generates a plot to visualize the results. The function should display the plot and return nothing. # Read the dataset df = pd.read_csv(file_path, parse_dates=[\'Date\']) # Set \'Date\' as index df.set_index(\'Date\', inplace=True) # Handle missing values by forward filling df.fillna(method=\'ffill\', inplace=True) # Group by \'Category\' and calculate summary statistics grouped = df.groupby(\'Category\')[\'Value\'].agg([\'mean\', \'sum\']) # Apply rolling window calculation with size 3 to the \'mean\' and \'sum\' columns rolling_mean = grouped[\'mean\'].rolling(window=3).mean() rolling_sum = grouped[\'sum\'].rolling(window=3).sum() # Plot the results plt.figure(figsize=(14, 7)) plt.subplot(2, 1, 1) rolling_mean.plot(title=\'Rolling Mean of Value by Category\', ylabel=\'Rolling Mean\') plt.subplot(2, 1, 2) rolling_sum.plot(title=\'Rolling Sum of Value by Category\', ylabel=\'Rolling Sum\') plt.tight_layout() plt.show()"},{"question":"# Coding Assessment: Working with PyArrow and pandas Objective: Demonstrate understanding and application of integrating PyArrow with pandas for efficient data manipulation and I/O operations. Problem Statement: You are provided with a CSV file named `data.csv` which contains mixed data types including integers, floats, booleans, strings, and dates. Your task is to read the CSV file using PyArrow as the engine, perform various data manipulations, and output the results. Expected Input: The CSV file `data.csv` contains the following columns: - `id` (integer) - `value` (float) - `status` (boolean) - `name` (string) - `date` (string in YYYY-MM-DD format) Tasks: 1. **Read the CSV File**: Read `data.csv` using the PyArrow engine and ensure the data is PyArrow-backed. Use appropriate data types for the columns. 2. **Data Cleaning**: - Drop any rows with missing values. - Fill missing values in the `status` column with `False` and in the `value` column with the mean of the column. 3. **Data Transformation**: - Create a new column `value_scaled` which scales the `value` column to a range of 0 to 1. - Convert the `date` column to a datetime type and extract the year into a new column `year`. 4. **Analysis**: - Group the data by `year` and compute the average `value` and `value_scaled` for each year. 5. **Output**: Save the resulting DataFrame to a new CSV file named `processed_data.csv`. Constraints and Requirements: - Use the PyArrow engine and ensure the data remains PyArrow-backed throughout the operations. - Handle missing data as specified. - Performance efficiency and code readability will be considered in the evaluation. Example: Assume `data.csv` contains: ``` id,value,status,name,date 1,1.1,True,John,2022-01-01 2,NaN,False,Jane,2022-02-02 3,2.3,,Doe,2021-12-31 ,,True,Alice,2023-03-15 ``` After processing, the `processed_data.csv` should contain: ``` id,value,status,name,date,value_scaled,year 1,1.1,True,John,2022-01-01,0.00,2022 2,1.7,False,Jane,2022-02-02,0.60,2022 3,2.3,False,Doe,2021-12-31,1.00,2021 ``` Save the final DataFrame to `processed_data.csv`.","solution":"import pandas as pd import pyarrow as pa import pyarrow.csv as pv import pyarrow.compute as pc def read_csv_with_pyarrow(file_path): Reads a CSV file into a Pandas DataFrame using PyArrow as the engine. @param: file_path: str: Path to the CSV file @return: pd.DataFrame backed with PyArrow table = pv.read_csv(file_path) df = table.to_pandas(types_mapper=pd.ArrowDtype) return df def clean_data(df): Performs data cleaning on the DataFrame: - Drops any rows with missing ID or name as those are essential - Fills missing values in the status column with False - Fills missing values in the value column with the mean of the value column df.dropna(subset=[\'id\', \'name\'], inplace=True) df[\'status\'].fillna(False, inplace=True) df[\'value\'].fillna(df[\'value\'].mean(), inplace=True) return df def transform_data(df): Performs data transformation on the DataFrame: - Creates a new column \'value_scaled\' which scales \'value\' to a range of 0 to 1 - Converts \'date\' column to datetime and creates a new column \'year\' df[\'value_scaled\'] = (df[\'value\'] - df[\'value\'].min()) / (df[\'value\'].max() - df[\'value\'].min()) df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'year\'] = df[\'date\'].dt.year return df def analyze_data(df): Groups the data by \'year\' and computes the average \'value\' and \'value_scaled\' for each year result = df.groupby(\'year\').agg({\'value\': \'mean\', \'value_scaled\': \'mean\'}).reset_index() return result def save_to_csv(df, file_path): Saves the DataFrame to a CSV file @param: df: pd.DataFrame: DataFrame to be saved @param: file_path: str: Path to the output CSV file df.to_csv(file_path, index=False) def process_csv(input_file_path, output_file_path): df = read_csv_with_pyarrow(input_file_path) df = clean_data(df) df = transform_data(df) result_df = analyze_data(df) save_to_csv(result_df, output_file_path) # Example usage, assuming `data.csv` is the input file and `processed_data.csv` is the output # process_csv(\'data.csv\', \'processed_data.csv\')"},{"question":"# Memory Leak Analysis with tracemalloc Objective In this task, you are required to analyze a Python program for memory leaks using the `tracemalloc` module. Specifically, you need to identify which parts of the code are responsible for the most memory allocations and determine if there are any memory leaks. Problem Statement You are given a Python script that simulates some computations and data processing. Your task is to write a function `analyze_memory_leak()` that will: 1. Start memory allocation tracing with `tracemalloc`. 2. Run the provided computation function. 3. Take a snapshot of memory allocations after the computation. 4. Identify the top 5 lines in the code that are responsible for the most memory allocations. 5. Detect any memory leaks by comparing two snapshots taken at different times during the execution. 6. Return the results of the top 5 memory allocations and the details of any detected memory leaks. Provided Code ```python import tracemalloc def computation_function(): Represents a function performing some computations and data processing. data = [] for i in range(1000): data.append([j for j in range(1000)]) # Simulate some other memory consumption temp_data = [k * 2 for k in range(10000)] del temp_data # Free temp_data memory return data def analyze_memory_leak(): Analyzes the provided computation_function() for memory allocations and potential leaks. Returns: top_allocation_lines (list): List of lines with the most memory allocations. memory_leaks (list): Details of any detected memory leaks. # TODO: Write your code here ``` Expected Output The function `analyze_memory_leak()` should return a tuple `(top_allocation_lines, memory_leaks)`, where: - `top_allocation_lines` is a list of the top 5 lines in the code responsible for the most memory allocations. Each element in the list should include the filename, line number, and allocated memory in KiB. - `memory_leaks` is a list of details about detected memory leaks, with each element indicating the filename, line number, and the increase in allocated memory size between the snapshots. Constraints - You must use the `tracemalloc` module for memory allocation tracking. - Consider memory leaks that occur in `computation_function()` specifically. - Ensure that your solution can handle extensive memory consumption efficiently. Example An example output might look like: ```python ( [ (\'<input>\', 10, 780.1), (\'<input>\', 12, 350.5), (\'<input>\', 14, 102.0), (\'<input>\', 8, 95.4), (\'<input>\', 16, 71.8) ], [ (\'<input>\', 10, 500.0), (\'<input>\', 12, 200.0), ] ) ``` In this example, the top 5 lines responsible for the most memory allocation are shown along with details of memory leaks detected between two snapshots. Good luck!","solution":"import tracemalloc def computation_function(): Represents a function performing some computations and data processing. data = [] for i in range(1000): data.append([j for j in range(1000)]) # Simulate some other memory consumption temp_data = [k * 2 for k in range(10000)] del temp_data # Free temp_data memory return data def analyze_memory_leak(): Analyzes the provided computation_function() for memory allocations and potential leaks. Returns: top_allocation_lines (list): List of lines with the most memory allocations. memory_leaks (list): Details of any detected memory leaks. # Start tracing memory allocations tracemalloc.start() # First snapshot snapshot1 = tracemalloc.take_snapshot() # Run the computation function computation_function() # Second snapshot snapshot2 = tracemalloc.take_snapshot() # Top 5 lines responsible for the most memory allocations top_stats = snapshot2.statistics(\'lineno\') top_allocation_lines = [(stat.traceback[0].filename, stat.traceback[0].lineno, stat.size / 1024) for stat in top_stats[:5]] # Memory leaks detection by comparing the two snapshots memory_leaks = [] diffs = snapshot2.compare_to(snapshot1, \'lineno\') for diff in diffs: if diff.size_diff > 0: memory_leaks.append((diff.traceback[0].filename, diff.traceback[0].lineno, diff.size_diff / 1024)) return top_allocation_lines, memory_leaks"},{"question":"# Configuration Parser using `configparser` **Task**: Implement a function to manage a configuration file using the `configparser` module in Python. The function will perform the following operations based on given input parameters: 1. Read the configuration from an INI file. 2. Update a section with new key-value pairs. 3. Create a new section if it doesn\'t exist. 4. Save the updated configuration back to the INI file. **Function Signature**: ```python def manage_config(file_path: str, section: str, updates: dict) -> None: ... ``` **Input**: - `file_path` (str): The path to the INI configuration file. - `section` (str): The section in the configuration where updates will be applied or created if it doesn\'t exist. - `updates` (dict): A dictionary with key-value pairs to update in the specified section. **Output**: - The function should not return anything. It will update the configuration file directly. **Constraints**: - The INI file specified by `file_path` will always exist. - The `updates` dictionary will contain non-empty key-value pairs. - Key-value pairs in the `updates` dictionary will be strings. **Example Usage**: Assume the following content in \\"config.ini\\": ```ini [General] app_version = 1.0.0 [User] name = Alice language = English ``` Calling `manage_config(\\"config.ini\\", \\"User\\", {\\"name\\": \\"Bob\\", \\"theme\\": \\"dark\\"})`: After execution, \\"config.ini\\" should be updated as: ```ini [General] app_version = 1.0.0 [User] name = Bob language = English theme = dark ``` **Notes**: - If the section specified doesn\'t exist, it should be created. - Existing keys in the section should be updated with new values from the `updates` dictionary. - The function should handle writing changes to the actual file.","solution":"import configparser def manage_config(file_path: str, section: str, updates: dict) -> None: Manage a configuration file by updating or adding key-value pairs to a specified section. :param file_path: str - Path to the INI configuration file. :param section: str - The section to update or create. :param updates: dict - Dictionary of key-value pairs to update in the specified section. config = configparser.ConfigParser() config.read(file_path) if not config.has_section(section): config.add_section(section) for key, value in updates.items(): config[section][key] = value with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"You are required to implement a function that schedules a list of tasks based on their dependencies using the `graphlib.TopologicalSorter` class. Each task has a unique identifier and a list of predecessor tasks that must be completed before it. Function Signature ```python def schedule_tasks(tasks: Dict[Any, List[Any]]) -> List[Any]: pass ``` Input - `tasks`: A dictionary where the keys are task identifiers (hashable types) and the values are lists of identifiers of predecessor tasks. It represents a directed acyclic graph. Output - Returns a list of task identifiers in a valid topological order. Constraints - The graph is guaranteed to be a Directed Acyclic Graph (DAG), so no cycles are present. - You should use the `graphlib.TopologicalSorter` class to perform the topological sort. - If the graph is empty, return an empty list. Example ```python tasks = { \'D\': [\'B\', \'C\'], \'C\': [\'A\'], \'B\': [\'A\'], \'A\': [] } print(schedule_tasks(tasks)) # Output: [\'A\', \'C\', \'B\', \'D\'] ``` Requirements 1. Use the `graphlib.TopologicalSorter` class to implement the function. 2. Handle the case of an empty input graph. 3. Ensure that the function performs as expected by using the `static_order` method for simplicity. Additional Instructions - Create a test suite with at least three varied test cases, including edge cases, to validate your implementation. - Handle exceptions (like cycles) gracefully, even if the constraints guarantee a DAG, consider future-proofing your code. Hints - Review the methods `TopologicalSorter.add()`, `TopologicalSorter.prepare()`, and `TopologicalSorter.static_order()` in the `graphlib` documentation. - Ensure you handle the addition of tasks and dependencies correctly before preparing the graph for sorting. Suggested Test Cases 1. A basic DAG with a single chain of dependencies. 2. A more complex DAG with branching dependencies. 3. An empty input graph. Good luck, and ensure your code is clear, concise, and well-documented!","solution":"import graphlib from typing import Dict, Any, List def schedule_tasks(tasks: Dict[Any, List[Any]]) -> List[Any]: Schedules tasks based on their dependencies using graphlib.TopologicalSorter. Args: tasks (Dict[Any, List[Any]]): A dictionary where keys are task identifiers and values are lists of predecessor tasks. Returns: List[Any]: List of task identifiers in a valid topological order. ts = graphlib.TopologicalSorter() for task, dependencies in tasks.items(): ts.add(task, *dependencies) return list(ts.static_order())"},{"question":"# Objective Demonstrate understanding of the `optparse` module in Python by creating a command-line tool with specific requirements. # Problem Statement You are tasked to create a command-line tool using the `optparse` module that processes command-line arguments for a hypothetical file processing program. The command-line tool should include the following functionalities: 1. **Specify an input file:** - Short option: `-i` - Long option: `--input` - Argument: The name of the input file (a string). - This option is **mandatory**. 2. **Specify an output file:** - Short option: `-o` - Long option: `--output` - Argument: The name of the output file (a string). - If not specified, default to `output.txt`. 3. **Enable or disable verbose mode:** - Short option: `-v` - Long option: `--verbose` - No argument. - If enabled, print detailed processing information. - Default is disabled. 4. **Specify the number of processing threads:** - Short option: `-t` - Long option: `--threads` - Argument: The number of threads (an integer). - Default to 1 thread. 5. **Specify a list of filters to apply:** - Short option: `-f` - Long option: `--filter` - Argument: A comma-separated string of filters (each being a string). - This option can be used multiple times, and all specified filters should be collected into a list. 6. **Generate help and usage messages:** - Short option: `-h` - Long option: `--help` - Display usage information and instructions for all options. # Constraints 1. Use the `optparse` module to implement the above functionalities. 2. Handle errors appropriately, such as missing required arguments or invalid argument types. 3. Ensure the generated help messages clearly describe each option and their usage. # Expected Function Signature ```python def main(argv=None): # Implement your code here pass ``` # Example Usage ```sh # Running with all options python script.py -i input.txt -o output.txt -v -t 4 -f filter1,filter2 -f filter3 # Running with minimal options python script.py -i input.txt # Help message python script.py -h # Should display usage and description for all options ``` # Additional Details - You can assume the script is executed via the command line, and `sys.argv` will be available. - Use appropriate actions in `optparse` to handle each option as described. - Feel free to import any necessary Python standard library modules. # Deliverable Submit your implementation of the `main` function that correctly parses the command-line arguments and handles all specified functionalities using the `optparse` module.","solution":"import optparse def main(argv=None): # Create the parser parser = optparse.OptionParser() # Adding options parser.add_option(\'-i\', \'--input\', dest=\'input_file\', type=\'string\', help=\'Name of the input file\') parser.add_option(\'-o\', \'--output\', dest=\'output_file\', type=\'string\', default=\'output.txt\', help=\'Name of the output file\') parser.add_option(\'-v\', \'--verbose\', action=\'store_true\', dest=\'verbose\', default=False, help=\'Enable verbose mode\') parser.add_option(\'-t\', \'--threads\', dest=\'threads\', type=\'int\', default=1, help=\'Number of processing threads\') parser.add_option(\'-f\', \'--filter\', action=\'append\', dest=\'filters\', type=\'string\', help=\'Comma-separated list of filters\') # Parsing arguments (options, args) = parser.parse_args(argv) # Check if mandatory options are provided if not options.input_file: parser.error(\'Input file not specified. Use -i or --input to specify the input file.\') # Processing filter option to handle comma-separated values filters = [] if options.filters: for item in options.filters: filters.extend(item.split(\',\')) # Create a dictionary containing the parsed options result = { \'input_file\': options.input_file, \'output_file\': options.output_file, \'verbose\': options.verbose, \'threads\': options.threads, \'filters\': filters } return result"},{"question":"Objective: Create a class `PythonFloat` that mimics the behavior of `PyFloatObject` as detailed in the provided documentation. Implement the following methods: 1. **`check_is_float(obj)`**: This function should check if the provided object is a float. 2. **`check_is_exact_float(obj)`**: This function should check if the provided object is exactly a float (not a subclass of float). 3. **`from_string(s)`**: This method should convert a string to a float and store it in the object instance. 4. **`from_double(d)`**: This method should convert a double to a float and store it in the object instance. 5. **`as_double()`**: This method should return the float stored in the object as a double. 6. **`get_info()`**: This method should return a dictionary with information about the float\'s precision, minimum value, and maximum value. 7. **`get_max()`**: This method should return the maximum representable finite float. 8. **`get_min()`**: This method should return the minimum normalized positive float. Expected Input and Output Formats: 1. **`check_is_float(obj)`**: - Input: Any Python object. - Output: Boolean (True if `obj` is a float, otherwise False). 2. **`check_is_exact_float(obj)`**: - Input: Any Python object. - Output: Boolean (True if `obj` is exactly a float and not a subclass, otherwise False). 3. **`from_string(s)`**: - Input: String representation of a float. - Output: None (Store the float in the instance). 4. **`from_double(d)`**: - Input: A double (floating point number). - Output: None (Store the float in the instance). 5. **`as_double()`**: - Input: None. - Output: The stored float as a double. 6. **`get_info()`**: - Input: None. - Output: Dictionary containing keys \'precision\', \'min_value\', and \'max_value\'. 7. **`get_max()`**: - Input: None. - Output: Maximum representable finite float. 8. **`get_min()`**: - Input: None. - Output: Minimum representable positive float. Constraints or Limitations: - The class should handle errors gracefully. For example, `from_string(s)` should raise a `ValueError` if the string does not represent a valid float. - Methods should seek to mimic the behavior of `PyFloatObject` as closely as possible, within Python\'s capabilities. Example Implementation: ```python import sys class PythonFloat: def __init__(self): self._value = None @staticmethod def check_is_float(obj): return isinstance(obj, float) @staticmethod def check_is_exact_float(obj): return type(obj) is float def from_string(self, s): try: self._value = float(s) except ValueError: raise ValueError(\\"Invalid string for float conversion\\") def from_double(self, d): self._value = float(d) def as_double(self): if self._value is None: raise ValueError(\\"No float value stored\\") return self._value @staticmethod def get_info(): return { \'precision\': sys.float_info.dig, \'min_value\': sys.float_info.min, \'max_value\': sys.float_info.max } @staticmethod def get_max(): return sys.float_info.max @staticmethod def get_min(): return sys.float_info.min ``` This question requires demonstrating an understanding of floating point precision, type checking, error handling, and converting between representations.","solution":"import sys import numbers class PythonFloat: def __init__(self): self._value = None @staticmethod def check_is_float(obj): Check if the provided object is a float. return isinstance(obj, float) @staticmethod def check_is_exact_float(obj): Check if the provided object is exactly a float (not a subclass of float). return type(obj) is float def from_string(self, s): Convert a string to a float and store it in the object instance. try: self._value = float(s) except ValueError: raise ValueError(\\"Invalid string for float conversion\\") def from_double(self, d): Convert a double to a float and store it in the object instance. if not isinstance(d, numbers.Real): raise ValueError(\\"Input must be a real number\\") self._value = float(d) def as_double(self): Return the float stored in the object as a double. if self._value is None: raise ValueError(\\"No float value stored\\") return self._value @staticmethod def get_info(): Return a dictionary with information about the float\'s precision, minimum value, and maximum value. return { \'precision\': sys.float_info.dig, \'min_value\': sys.float_info.min, \'max_value\': sys.float_info.max } @staticmethod def get_max(): Return the maximum representable finite float. return sys.float_info.max @staticmethod def get_min(): Return the minimum normalized positive float. return sys.float_info.min"},{"question":"**Question: Implement and Compare Different Regression Models** **Objective:** You are tasked with implementing, fitting, and comparing different regression models using scikit-learn. This question will assess your understanding of various linear regression techniques available in scikit-learn, including Ordinary Least Squares (OLS), Ridge Regression, and Lasso Regression. **Task:** 1. **Data Preparation:** - Generate a synthetic dataset using `make_regression` from `sklearn.datasets` with 100 samples, 2 features, and a noise level of 0.1. 2. **Model Implementation:** - Implement three regression models: - Ordinary Least Squares (OLS) using `LinearRegression` - Ridge Regression using `Ridge` (with `alpha=1.0`) - Lasso Regression using `Lasso` (with `alpha=0.1`) 3. **Model Training:** - Fit each model to the synthetic dataset. 4. **Model Evaluation:** - Evaluate the models using Mean Squared Error (MSE). Use `mean_squared_error` from `sklearn.metrics`. 5. **Model Comparison:** - Print the coefficients and intercept of each model. - Print the Mean Squared Error (MSE) for each model to compare their performance. **Expected Input and Output:** - **Input:** None (the dataset should be generated within the code). - **Output:** - Coefficients and intercept for each model. - Mean Squared Error (MSE) for each model. **Constraints:** - Do not use any libraries other than scikit-learn and numpy. **Example:** ```python import numpy as np from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_squared_error # 1. Data Preparation X, y = make_regression(n_samples=100, n_features=2, noise=0.1) # 2. Model Implementation models = { \\"OLS\\": LinearRegression(), \\"Ridge\\": Ridge(alpha=1.0), \\"Lasso\\": Lasso(alpha=0.1) } # 3. Model Training and Evaluation for name, model in models.items(): # Train model model.fit(X, y) # Predict y_pred = model.predict(X) # Evaluate mse = mean_squared_error(y, y_pred) # Print results print(f\\"{name} Regression\\") print(f\\" Coefficients: {model.coef_}\\") print(f\\" Intercept: {model.intercept_}\\") print(f\\" Mean Squared Error: {mse}\\") print() ``` **Performance Requirements:** - The entire process (data generation, model fitting, and evaluation) should complete within a reasonable time frame (less than 5 seconds for the provided dataset size).","solution":"import numpy as np from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_squared_error def regression_models_comparison(): # 1. Data Preparation X, y = make_regression(n_samples=100, n_features=2, noise=0.1) # 2. Model Implementation models = { \\"OLS\\": LinearRegression(), \\"Ridge\\": Ridge(alpha=1.0), \\"Lasso\\": Lasso(alpha=0.1) } results = {} # 3. Model Training and Evaluation for name, model in models.items(): # Train model model.fit(X, y) # Predict y_pred = model.predict(X) # Evaluate mse = mean_squared_error(y, y_pred) # Store results results[name] = { \'coefficients\': model.coef_, \'intercept\': model.intercept_, \'mse\': mse } return results"},{"question":"# Custom Import System with Resource Handling You are tasked with creating a custom import system for a specialized use case where modules might include binary or text resources that need to be accessed programmatically. You should implement a custom importer and loader using the `importlib` package. Your solution should follow these requirements: 1. **Custom Importer and Loader:** - Implement a custom importer class named `CustomImporter` that can import modules from a specific directory. - Implement a custom loader class named `CustomLoader` that handles loading the module and accessing resources. 2. **Resource Handling:** - Your custom loader should be able to read text and binary resources embedded within the modules. - Provide a method `get_resource_content(package, resource_name, mode=\'r\')` which can read the resource content either as text or binary based on the `mode`. 3. **Testing your implementation:** - Assume you have a directory structure with a module named `example_module` that includes a text resource (`data.txt`) and a binary resource (`data.bin`). - Write a script to test importing the `example_module` and accessing the resources `data.txt` and `data.bin`. # Constraints: - The importing should be dynamic and should not rely on hardcoding paths within the module code. - Handle exceptions gracefully and provide meaningful error messages where applicable. # Example Directory Structure: ``` /path/to/modules/ example_module/ __init__.py data.txt data.bin ``` # Expected Input and Output: - `get_resource_content(\'example_module\', \'data.txt\', \'r\')` should return the content of `data.txt` as a string. - `get_resource_content(\'example_module\', \'data.bin\', \'rb\')` should return the content of `data.bin` as bytes. # Implementation Details: Implement the following classes and functions: ```python import importlib.abc import importlib.util import os from typing import Union class CustomLoader(importlib.abc.Loader): # Implement the required methods for the loader ... class CustomImporter(importlib.abc.MetaPathFinder): # Implement the required methods for the meta path finder ... def get_resource_content(package: str, resource_name: str, mode: str = \'r\') -> Union[str, bytes]: # Implement the method to read text or binary resources based on mode ... # Example test script to demonstrate usage if __name__ == \\"__main__\\": import sys sys.meta_path.insert(0, CustomImporter(\'/path/to/modules\')) import example_module # Reading text resource text_content = get_resource_content(\'example_module\', \'data.txt\', \'r\') print(f\\"Text Content: {text_content}\\") # Reading binary resource binary_content = get_resource_content(\'example_module\', \'data.bin\', \'rb\') print(f\\"Binary Content: {binary_content}\\") ``` Document your solution and include comments in your code to explain the logic and flow.","solution":"import importlib.abc import importlib.util import os from typing import Union class CustomLoader(importlib.abc.Loader): def __init__(self, module_name, module_path): self.module_name = module_name self.module_path = module_path def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): module.__file__ = self.module_path module.__loader__ = self with open(self.module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) def get_resource(self, resource_name): resource_path = os.path.join(os.path.dirname(self.module_path), resource_name) if not os.path.exists(resource_path): raise FileNotFoundError(f\\"Resource {resource_name} not found in {self.module_name}\\") return resource_path class CustomImporter(importlib.abc.MetaPathFinder): def __init__(self, base_path): self.base_path = base_path def find_spec(self, fullname, path, target=None): module_path = os.path.join(self.base_path, *fullname.split(\'.\')) module_file_path = os.path.join(module_path, \'__init__.py\') if os.path.exists(module_file_path): spec = importlib.util.spec_from_file_location(fullname, module_file_path, loader=CustomLoader(fullname, module_file_path)) return spec return None def get_resource_content(package: str, resource_name: str, mode: str = \'r\') -> Union[str, bytes]: module = importlib.import_module(package) if hasattr(module, \'__loader__\') and isinstance(module.__loader__, CustomLoader): resource_path = module.__loader__.get_resource(resource_name) with open(resource_path, mode) as file: return file.read() # Example test script to demonstrate usage if __name__ == \\"__main__\\": import sys sys.meta_path.insert(0, CustomImporter(\'/path/to/modules\')) import example_module # Reading text resource text_content = get_resource_content(\'example_module\', \'data.txt\', \'r\') print(f\\"Text Content: {text_content}\\") # Reading binary resource binary_content = get_resource_content(\'example_module\', \'data.bin\', \'rb\') print(f\\"Binary Content: {binary_content}\\")"},{"question":"# Question: Parallel Data Processing with PyTorch Multiprocessing You are tasked with implementing a parallel data processing pipeline using PyTorch\'s multiprocessing module. The goal is to spawn multiple subprocesses to process chunks of a large dataset concurrently and return the processed results to the main process. Requirements: 1. Implement a function `parallel_process_data` which accepts a large dataset and processes it in parallel using multiple subprocesses. 2. The function should split the dataset into equal-sized chunks based on the number of available processes and distribute these chunks to the subprocesses. 3. Each subprocess should process its chunk and return the results to the main process. 4. The main process should collect the results from all subprocesses and return the combined result. Function Signature: ```python import torch.multiprocessing as mp def parallel_process_data(data, process_fn, num_processes): Process data in parallel using PyTorch multiprocessing. Args: - data (list): The dataset to be processed. - process_fn (callable): The function that processes a chunk of data. - num_processes (int): Number of subprocesses to use. Returns: - list: The combined result from all subprocesses. # Your code here ``` Input: - `data`: A list containing the data items to be processed. - `process_fn`: A function that takes a chunk of data as input and returns the processed result. - `num_processes`: An integer specifying the number of subprocesses to spawn. Output: - A list containing the combined results from all subprocesses. Constraints: - Each subprocess should process its chunk independently without any interference from other subprocesses. - Ensure memory management and resource cleanup to avoid memory leaks. - Use appropriate sharing strategies for managing tensor data, if any. Example: ```python # Example process function def example_process_fn(chunk): return [x * 2 for x in chunk] # Example data data = list(range(100)) # Process data with 4 subprocesses result = parallel_process_data(data, example_process_fn, 4) print(result) # Output: [0, 2, 4, 6, 8, ..., 198] ``` Guidelines: 1. Use `torch.multiprocessing` to handle the subprocess creation and management. 2. Apply best practices for sharing data between processes as described in the documentation. 3. Implement error handling and proper joining of processes to ensure smooth execution and resource cleanup.","solution":"import torch.multiprocessing as mp def parallel_process_data(data, process_fn, num_processes): Process data in parallel using PyTorch multiprocessing. Args: - data (list): The dataset to be processed. - process_fn (callable): The function that processes a chunk of data. - num_processes (int): Number of subprocesses to use. Returns: - list: The combined result from all subprocesses. def worker(chunk, result_queue): result = process_fn(chunk) result_queue.put(result) # Divide the data into chunks chunk_size = len(data) // num_processes chunks = [data[i*chunk_size : (i+1)*chunk_size] for i in range(num_processes)] # Add any remaining data to the last chunk if len(data) % num_processes != 0: chunks[-1].extend(data[num_processes*chunk_size:]) result_queue = mp.Queue() processes = [] # Start subprocesses for chunk in chunks: p = mp.Process(target=worker, args=(chunk, result_queue)) processes.append(p) p.start() # Collect results results = [] for _ in range(num_processes): results.extend(result_queue.get()) # Ensure all processes have finished for p in processes: p.join() return results"},{"question":"# **Coding Assessment Question** **Objective:** Your task is to implement a Python function using the traceback module to extract, format, and print the stack trace of an exception. This function should capture the exception if it occurs during the execution of another function and print detailed traceback information including the local variables at each frame. # **Function Signature:** ```python def execute_and_trace(func, *args, **kwargs): Executes the given function with the provided arguments and keyword arguments. If an exception occurs, prints the detailed traceback information, including local variables at each frame. Parameters: - func: The function to be executed. - *args: The positional arguments to pass to the function. - **kwargs: The keyword arguments to pass to the function. Returns: - The return value of the function if no exception occurs. - None if an exception occurs. ``` # **Input:** 1. **`func`** - the function to be executed. 2. **`*args`** - the positional arguments to be passed to the function. 3. **`**kwargs`** - the keyword arguments to be passed to the function. # **Output:** - The return value of the function if no exception occurs. - If an exception occurs, print a detailed traceback report to `sys.stdout` and return `None`. # **Constraints:** - Use the `traceback` module functions such as `traceback.TracebackException` to capture and format the traceback. - Include local variables in the traceback report. # **Example Usage:** ```python def sample_function(x, y): z = x / y return z # Proper call result = execute_and_trace(sample_function, 10, 2) print(result) # Output: 5.0 # Exception call result = execute_and_trace(sample_function, 10, 0) # Expect a detailed traceback printed to sys.stdout with local variables print(result) # Output: None ``` # **Evaluation Criteria:** - Correctness: The implementation should correctly execute the given function and handle any exceptions that occur. - Traceback Detail: The printed traceback should include detailed information, including local variables at each frame. - Use of `traceback` module: Proper utilization of the functions and classes provided by the `traceback` module. - Proper handling of both positional and keyword arguments.","solution":"import traceback def execute_and_trace(func, *args, **kwargs): Executes the given function with the provided arguments and keyword arguments. If an exception occurs, prints the detailed traceback information, including local variables at each frame. Parameters: - func: The function to be executed. - *args: The positional arguments to pass to the function. - **kwargs: The keyword arguments to pass to the function. Returns: - The return value of the function if no exception occurs. - None if an exception occurs. try: return func(*args, **kwargs) except Exception as e: print(\\"An exception occurred:\\") tb = traceback.TracebackException.from_exception(e) for line in tb.format(chain=True): print(line, end=\\"\\") return None"},{"question":"# Persistent Storage with `shelve` You are tasked with designing a small application that uses the `shelve` module to manage persistent, dictionary-like storage for a collection of contact information. Each contact consists of a unique `id` (string) and the contact\'s details, a dictionary with the following structure: ```python { \'name\': str, \'email\': str, \'phone\': str } ``` # Requirements: 1. **Initialize Persistent Storage**: Create or open a shelf file named `contacts_db`. 2. **Add or Update Contact**: Write a function `add_update_contact(shelf, contact_id, contact_info)` that adds or updates a contact\'s information. - Input: - `shelf`: the opened shelf object. - `contact_id`: a string representing the unique ID of the contact. - `contact_info`: a dictionary containing the contact\'s details. - Output: None. Should persist the contact information in the shelf. 3. **Fetch Contact**: Write a function `fetch_contact(shelf, contact_id)` that retrieves a contact\'s details. - Input: - `shelf`: the opened shelf object. - `contact_id`: a string representing the unique ID of the contact. - Output: A dictionary containing the contact details or `None` if the contact does not exist. 4. **Delete Contact**: Write a function `delete_contact(shelf, contact_id)` that deletes a contact from the shelf. - Input: - `shelf`: the opened shelf object. - `contact_id`: a string representing the unique ID of the contact. - Output: A boolean indicating success (`True`) if the contact was successfully deleted or was not found, or `False` if there was an issue (e.g., the shelf was closed). 5. **List All Contacts**: Write a function `list_all_contacts(shelf)` that lists all contact IDs stored in the shelf. - Input: - `shelf`: the opened shelf object. - Output: A list of strings containing all contact IDs. 6. **Ensure Safe Handling**: - The shelf should be opened with `writeback=True`. However, caution must be exercised due to memory limitations. - Ensure the shelf is properly closed using the context manager (`with` block). - Handle potential exceptions that might arise from operations such as retrieval or deletion of non-existent contacts. # Constraints: - The contact IDs should always be strings. - The contact info dictionary should strictly follow the specified keys and value-types. - Consider the performance impacts of using `writeback=True`. # Example Usage: ```python import shelve def add_update_contact(shelf, contact_id, contact_info): # Function implementation pass def fetch_contact(shelf, contact_id): # Function implementation pass def delete_contact(shelf, contact_id): # Function implementation pass def list_all_contacts(shelf): # Function implementation pass # Example run with shelve.open(\'contacts_db\', writeback=True) as contacts: add_update_contact(contacts, \'001\', {\'name\': \'John Doe\', \'email\': \'john.doe@example.com\', \'phone\': \'123-456-7890\'}) add_update_contact(contacts, \'002\', {\'name\': \'Jane Smith\', \'email\': \'jane.smith@example.com\', \'phone\': \'098-765-4321\'}) print(fetch_contact(contacts, \'001\')) print(fetch_contact(contacts, \'003\')) print(delete_contact(contacts, \'002\')) print(delete_contact(contacts, \'004\')) print(list_all_contacts(contacts)) ``` This question assesses the ability to use `shelve` to maintain a persistent dictionary, manage data operations responsibly, and ensure proper resource management.","solution":"import shelve def add_update_contact(shelf, contact_id, contact_info): Adds or updates a contact\'s information in the shelf. shelf[contact_id] = contact_info def fetch_contact(shelf, contact_id): Fetches a contact\'s details from the shelf. Returns None if the contact does not exist. return shelf.get(contact_id) def delete_contact(shelf, contact_id): Deletes a contact from the shelf. Returns True if the contact was successfully deleted or did not exist. Returns False if there was an issue (e.g., the shelf was closed). if contact_id in shelf: del shelf[contact_id] return True return False def list_all_contacts(shelf): Lists all contact IDs stored in the shelf. return list(shelf.keys())"},{"question":"<|Analysis Begin|> The provided documentation centers on the use of the `sns.pairplot` function from the seaborn library, which is used to create matrix plots of variable pairings. This function includes various parameters, such as: - `hue`: Adds a semantic mapping to the plots. - `diag_kind`: Specifies the type of plot to use on the diagonal. - `kind`: Determines the plotting style. - `markers`: Applies a style mapping on the off-diagonal axes. - `height`: Adjusts the size of the subplots. - `vars`, `x_vars`, `y_vars`: Select which variables to plot. - `corner`: Plots only the lower triangle of the pair grid. - `plot_kws` and `diag_kws`: Customize the plots with additional keyword arguments. - The return object (`PairGrid`) can be used for further customization. From the documentation, we see several examples of how to use these parameters to customize the pair plots. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Demonstrate your understanding of seaborn\'s `pairplot` function by creating a highly customized pair plot for a provided dataset. Problem Statement You are provided with a dataset containing details on penguins. Your task is to create a customized pair plot that meets the following requirements: 1. Load the `penguins` dataset using seaborn. 2. Create a pair plot to visualize the relationships between `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm`. 3. Use the `species` column as the `hue` to differentiate between different species of penguins. 4. Set the diagonal plots to be histograms. 5. Use different markers for each species in the off-diagonal plots. 6. Make the figure size such that each individual subplot has a height of 2.5 units. 7. Plot only the lower triangle of the pair grid. 8. Customize the off-diagonal plots with a marker style `\\"*\\"`, and a linewidth of 2.0. 9. Customize the diagonal histograms to use a fill of `True`. Input Format - There is no explicit input from the user. Use the `penguins` dataset provided by seaborn. Output Format - A seaborn pair plot meeting the specified customization requirements. Constraints - Use only the seaborn and matplotlib libraries for visualization. Example The following example demonstrates how to load the dataset. Your code should extend this to fulfill the customization requirements: ```python import seaborn as sns # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the customized pair plot here. ``` Performance Requirements - The plot should render within a reasonable time for an interactive environment.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_customized_pairplot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the pair plot with the specified customizations pair_plot = sns.pairplot( data=penguins, vars=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"], hue=\\"species\\", diag_kind=\\"hist\\", markers=[\\"o\\", \\"s\\", \\"D\\"], height=2.5, corner=True, plot_kws={\'marker\': \'*\', \'linewidth\': 2.0}, diag_kws={\'fill\': True} ) # Display the plot plt.show() # Call the function to create and display the pair plot. create_customized_pairplot()"},{"question":"**Question:** Implement a function using the `csv` module which reads data from a specified CSV file and then writes a summary of the data to another CSV file. The summary should include the total number of records, the average value of a specified column, and the maximum value of the same column. # Requirements: 1. **Function Signature:** ```python def summarize_csv(input_file: str, output_file: str, column_name: str) -> None: ``` 2. **Inputs:** - `input_file` (str): The path to the input CSV file. - `output_file` (str): The path to the output CSV file where the summary will be written. - `column_name` (str): The name of the column to compute the summary statistics for. 3. **Output:** - The function should not return anything. It should write the summary to the specified output file. 4. **Summary File Structure:** - The output CSV file should have two rows and three columns. - First row: It should be the header with the following columns: `total_records`, `average_<column_name>`, `max_<column_name>` - Second row: Contains the computed statistics. 5. **Constraints:** - Assume the input CSV file has a header. - The specified column contains numeric values. - Handle any potential errors gracefully and ensure the function is robust (for example, handle cases where the specified column does not exist). # Example: Assume `input.csv` has the following data: ```csv name,age,salary John,28,50000 Jane,32,60000 Doe,45,70000 ``` Calling the function: ```python summarize_csv(\\"input.csv\\", \\"output.csv\\", \\"salary\\") ``` The `output.csv` should contain: ```csv total_records,average_salary,max_salary 3,60000.0,70000 ``` # Implementation Notes: - Use the `csv` module for reading from and writing to CSV files. - Make sure to handle file operations and potential errors (such as file not found). - Ensure that the specified column exists in the CSV file, and handle the scenario where it does not.","solution":"import csv def summarize_csv(input_file: str, output_file: str, column_name: str) -> None: Reads data from a specified CSV file and writes a summary of the data to another CSV file. Summary includes total number of records, average value of a specified column, and the maximum value of the same column. :param input_file: The path to the input CSV file. :param output_file: The path to the output CSV file where the summary will be written. :param column_name: The name of the column to compute the summary statistics for. total_records = 0 total_sum = 0 max_value = float(\'-inf\') try: with open(input_file, mode=\'r\', newline=\'\') as csvfile: reader = csv.DictReader(csvfile) if column_name not in reader.fieldnames: raise ValueError(f\\"Column \'{column_name}\' does not exist in the input file.\\") for row in reader: total_records += 1 value = float(row[column_name]) total_sum += value if value > max_value: max_value = value if total_records == 0: raise ValueError(\\"The input CSV file has no records.\\") average_value = total_sum / total_records with open(output_file, mode=\'w\', newline=\'\') as csvfile: writer = csv.writer(csvfile) header = [\'total_records\', f\'average_{column_name}\', f\'max_{column_name}\'] writer.writerow(header) writer.writerow([total_records, average_value, max_value]) except FileNotFoundError: print(f\\"File \'{input_file}\' not found.\\") except ValueError as ve: print(f\\"ValueError: {ve}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You have been tasked with writing a function `manage_file_locks(filename, lock_type)` using the `fcntl` module to manage file locks. The function should: 1. Open a file at the given `filename` in read-write mode. 2. Depending on the given `lock_type`, perform the specified lock operation: - If `lock_type` is `\\"shared\\"`, acquire a shared lock (`fcntl.LOCK_SH`). - If `lock_type` is `\\"exclusive\\"`, acquire an exclusive lock (`fcntl.LOCK_EX`). - If `lock_type` is `\\"unlock\\"`, release any lock currently held on the file (`fcntl.LOCK_UN`). 3. Handle any potential errors, such as inability to acquire a lock because the file is already locked by another process. 4. Perform a basic read or write operation to verify that the lock is enforced correctly. 5. Close the file appropriately after the operation. # Function Signature: ```python import fcntl def manage_file_locks(filename: str, lock_type: str) -> bool: pass ``` # Input: - `filename` (str): The name of the file you wish to lock. - `lock_type` (str): The type of lock to apply, which can be `\\"shared\\"`, `\\"exclusive\\"`, or `\\"unlock\\"`. # Output: - Returns `True` if the lock operation was successful, and `False` otherwise. # Constraints: - Ensure that any opened file is appropriately closed after the operation. - Handle all possible exceptions and errors gracefully. - The operation should not block indefinitely; if the lock cannot be acquired immediately, return `False`. # Example: ```python # Assuming `example.txt` is a valid file path print(manage_file_locks(\\"example.txt\\", \\"shared\\")) # Should return True if shared lock is acquired print(manage_file_locks(\\"example.txt\\", \\"exclusive\\")) # Should return True if exclusive lock is acquired print(manage_file_locks(\\"example.txt\\", \\"unlock\\")) # Should return True if the lock is successfully released ``` # Background Knowledge: - `fcntl.flock(fd, operation)` is used to perform locking operations. - Lock types: - `fcntl.LOCK_SH`: Acquire a shared lock. - `fcntl.LOCK_EX`: Acquire an exclusive lock. - `fcntl.LOCK_UN`: Release any currently held lock. Develop a solution that demonstrates understanding of the `fcntl` module\'s locking capabilities and handling file I/O operations correctly.","solution":"import fcntl import os def manage_file_locks(filename: str, lock_type: str) -> bool: try: fd = os.open(filename, os.O_RDWR) if lock_type == \\"shared\\": fcntl.flock(fd, fcntl.LOCK_SH | fcntl.LOCK_NB) elif lock_type == \\"exclusive\\": fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB) elif lock_type == \\"unlock\\": fcntl.flock(fd, fcntl.LOCK_UN) # Perform a basic read operation to ensure the file is accessed correctly os.read(fd, 1) os.close(fd) return True except (IOError, OSError) as e: if fd: os.close(fd) return False"},{"question":"Coding Assessment Question # Problem Description: You are required to implement a function that interacts with the operating system\'s environment variables using the `posix` module. The function should retrieve, add, and modify environment variables and then use them in a new process execution. Your solution must demonstrate familiarity with both retrieving environment variables and affecting them using the `posix` module functionality. # Function Signature: ```python import subprocess import os def modify_and_execute_environment(command: str, new_vars: dict) -> str: This function modifies the current environment variables by adding new entries from `new_vars` dictionary and then executes the given command in a new process using the modified environment. :param command: A string command to be executed. :param new_vars: A dictionary where keys and values are the new environment variables to be added or modified. :return: The standard output from running the given command as a string. pass ``` # Requirements: 1. **Input**: - `command`: A string representing the command to be executed. - `new_vars`: A dictionary containing new environment variables to be added or modified. 2. **Output**: - A string representing the standard output captured from executing the command. 3. **Constraints**: - Use the `posix` module where appropriate. - Ensure that the environment modifications do not affect the caller process environment. - Handle cases where the command fails by raising an appropriate exception with an error message. 4. **Performance**: This function should handle the environment modifications and command execution efficiently. # Example: ```python # Suppose the current environment has {\'PATH\': \'/usr/bin\', \'HOME\': \'/home/user\'}, # and we want to add a VAR \'MY_VAR\' with value \'123\', and then execute \'echo MY_VAR\' result = modify_and_execute_environment(\'echo MY_VAR\', {\'MY_VAR\': \'123\'}) print(result) # Should output: \'123n\' ``` # Hint: To ensure modifications in the environment do not affect the caller, consider creating a copy of the current environment and passing this modified environment to the `execve()` function from the `posix` module.","solution":"import os import subprocess def modify_and_execute_environment(command: str, new_vars: dict) -> str: This function modifies the current environment variables by adding new entries from `new_vars` dictionary and then executes the given command in a new process using the modified environment. :param command: A string command to be executed. :param new_vars: A dictionary where keys and values are the new environment variables to be added or modified. :return: The standard output from running the given command as a string. # Create a copy of the current environment env = os.environ.copy() # Update the environment with the new variables env.update(new_vars) # Execute the command using the modified environment and capture the output result = subprocess.run(command, shell=True, capture_output=True, text=True, env=env) # Check if the command executed successfully if result.returncode != 0: raise Exception(f\\"Command failed with return code {result.returncode}: {result.stderr}\\") return result.stdout"},{"question":"**Question: Creating and Customizing Seaborn Object-based Plots** You are given a dataset of your choice using seaborn\'s `load_dataset` function. Your task is to create a combined, customized plot using seaborn objects and matplotlib. The requirements are as follows: 1. **Load the Data:** - Load the `penguins` dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. **Create Plots:** - Create a scatter plot of `bill_length_mm` vs. `bill_depth_mm` using `seaborn.objects.Plot`, with points colored by the `species`. 3. **Customizing Theme:** - Apply a custom theme to the overall plot to make the plot visually appealing. 4. **Adding Subplots and Customizations:** - Use subplots to create the following: - In the left subplot, add a scatter plot of `flipper_length_mm` vs. `body_mass_g`, color-coded by `species`. - In the right subplot, create a histogram of `flipper_length_mm` with different colors for each `species`. - Add a rectangle annotation to highlight an interesting area in any of the subplots. - Add an annotation text inside the rectangle describing the highlighted area. **Expected Input and Output:** - **Input:** No specific input required. The code should handle loading the dataset internally. - **Output:** A visual plot displayed using matplotlib with the required customizations. **Performance Requirements:** - Ensure that the plots are visually appealing and well-arranged. - Make use of `seaborn.objects.Plot` and `matplotlib` for customizations. Here is a template to get started: ```python import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt import matplotlib as mpl # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the base plot p = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\").add(so.Dots()) # Creating subplots fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6)) # Left subplot: scatter plot of flipper_length_mm vs. body_mass_g p1 = so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\").add(so.Dots()) p1.on(ax1).plot() # Right subplot: histogram of flipper_length_mm by species p2 = (so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") .add(so.Bars(), so.Hist())) p2.on(ax2).plot() # Adding a custom theme # Apply a custom theme to the combined plot (use plotting settings of your choice) # Add a rectangle annotation to the left subplot rect = mpl.patches.Rectangle((180, 3000), 30, 700, linewidth=1, edgecolor=\'r\', facecolor=\'none\') ax1.add_patch(rect) # Add annotation text inside the rectangle in the left subplot ax1.text(195, 3300, \'Interesting area\', horizontalalignment=\'center\', verticalalignment=\'center\', fontsize=12, color=\'red\') # Display the plot plt.show() ``` Complete this template code to fulfill all the requirements specified. Ensure the plots are well-styled and annotations are correctly placed.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt import matplotlib as mpl # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Creating subplots fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6)) # Left subplot: scatter plot of flipper_length_mm vs. body_mass_g p1 = so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\").add(so.Dots()) p1.on(ax1).plot() # Right subplot: histogram of flipper_length_mm by species p2 = (so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") .add(so.Bars(), so.Hist())) p2.on(ax2).plot() # Adding a custom theme sns.set_theme(style=\\"whitegrid\\") # Scatter plot of bill_length_mm vs. bill_depth_mm p = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\").add(so.Dots()) p.plot(mpl.pyplot.gca()) # Add a rectangle annotation to the left subplot rect = mpl.patches.Rectangle((180, 3000), 40, 800, linewidth=1.5, edgecolor=\'red\', facecolor=\'none\') ax1.add_patch(rect) # Add annotation text inside the rectangle in the left subplot ax1.text(200, 3400, \'Interesting area\', horizontalalignment=\'center\', verticalalignment=\'center\', fontsize=10, color=\'red\') # Display the plot plt.tight_layout() plt.show()"},{"question":"# Audio Sample Manipulation in Python **Objective:** Write a Python function using the `audioop` module to convert audio fragments and adjust their volume. You will start by reversing the audio samples, converting the encoding from 8-bit to 16-bit linear, and finally adjusting the volume by a specified factor. **Function Signature:** ```python def process_audio_samples(fragment: bytes, factor: float) -> bytes: Process audio samples by reversing, converting, and adjusting volume. Parameters: - fragment (bytes): The input audio fragment with 8-bit samples. - factor (float): The multiplier used to adjust the volume of the audio samples. Returns: - bytes: The processed audio fragment with 16-bit samples. ``` # Detailed Requirements: 1. **Reverse the Audio Samples**: - Reverse the audio fragment using `audioop.reverse`. 2. **Convert Encodings**: - Convert the reversed audio from 8-bit samples to 16-bit samples using `audioop.lin2lin`. 3. **Adjust Volume**: - Adjust the volume of the converted audio by multiplying the audio samples by the given factor using `audioop.mul`. # Constraints: - The input audio fragment consists of **unsigned 8-bit** samples. - The output audio fragment should consist of **16-bit signed** samples. - The multiplier factor is a floating-point number that controls the volume adjustment. # Example: ```python fragment = b\'x01x02x03x04\' # Example 8-bit audio samples factor = 2.0 # Your function call result = process_audio_samples(fragment, factor) # Expected processed audio fragment in 16-bit samples ``` When writing your function, make sure to: - Handle errors and exceptions appropriately. - Use the provided `audioop` functions as intended. - Test your function with different sample inputs and scaling factors. **Notes**: - You can assume that the input `fragment` is always of a valid length (multiple of 1 byte for 8-bit samples). Good luck, and happy coding!","solution":"import audioop def process_audio_samples(fragment: bytes, factor: float) -> bytes: Process audio samples by reversing, converting, and adjusting volume. Parameters: - fragment (bytes): The input audio fragment with 8-bit samples. - factor (float): The multiplier used to adjust the volume of the audio samples. Returns: - bytes: The processed audio fragment with 16-bit samples. # Reverse the audio samples reversed_audio = audioop.reverse(fragment, 1) # 1 byte width for 8-bit samples # Convert 8-bit unsigned samples to 16-bit signed samples converted_audio = audioop.lin2lin(reversed_audio, 1, 2) # from 1-byte to 2-byte samples # Adjust volume adjusted_audio = audioop.mul(converted_audio, 2, factor) # 2-byte sample width return adjusted_audio"},{"question":"# JSON Data Transformation You are given a JSON string that contains details of various students and their respective scores in different subjects. Your task is to write a function that processes this JSON data to calculate the average score for each student and then output a new JSON string that summarizes each student\'s name and their average score. Input - A JSON string representing a list of students. Each student is represented as an object containing their `\\"name\\"` (a string) and `\\"scores\\"` (an object with subject names as keys and scores as values). Example: ```json [ { \\"name\\": \\"Alice\\", \\"scores\\": { \\"math\\": 90, \\"science\\": 80, \\"history\\": 85 } }, { \\"name\\": \\"Bob\\", \\"scores\\": { \\"math\\": 75, \\"science\\": 70, \\"history\\": 65 } } ] ``` Output - A JSON string representing a list of students with their names and their average scores. Each student object should contain `\\"name\\"` (a string) and `\\"average_score\\"` (a float rounded to two decimal places). Example: ```json [ { \\"name\\": \\"Alice\\", \\"average_score\\": 85.0 }, { \\"name\\": \\"Bob\\", \\"average_score\\": 70.0 } ] ``` Constraints - The input JSON will always be a properly formatted JSON string. - Each student\'s `\\"scores\\"` object will have at least one subject with a numerical score. - The scores are guaranteed to be integers. Implementation Write a function `calculate_average_scores(json_string)` that takes a JSON string as input and returns a JSON string representing the transformed data. ```python import json def calculate_average_scores(json_string): # Parse the JSON data students = json.loads(json_string) # List to hold the transformed student data transformed_students = [] # Process each student for student in students: name = student[\\"name\\"] scores = student[\\"scores\\"] # Calculate average score total_score = sum(scores.values()) number_of_subjects = len(scores) average_score = round(total_score / number_of_subjects, 2) # Append the transformed student info to result list transformed_students.append({ \\"name\\": name, \\"average_score\\": average_score }) # Convert the transformed data back to JSON string return json.dumps(transformed_students) # Example usage: input_json = \'\'\' [ { \\"name\\": \\"Alice\\", \\"scores\\": { \\"math\\": 90, \\"science\\": 80, \\"history\\": 85 } }, { \\"name\\": \\"Bob\\", \\"scores\\": { \\"math\\": 75, \\"science\\": 70, \\"history\\": 65 } } ] \'\'\' output_json = calculate_average_scores(input_json) print(output_json) ```","solution":"import json def calculate_average_scores(json_string): # Parse the JSON data students = json.loads(json_string) # List to hold the transformed student data transformed_students = [] # Process each student for student in students: name = student[\\"name\\"] scores = student[\\"scores\\"] # Calculate average score total_score = sum(scores.values()) number_of_subjects = len(scores) average_score = round(total_score / number_of_subjects, 2) # Append the transformed student info to result list transformed_students.append({ \\"name\\": name, \\"average_score\\": average_score }) # Convert the transformed data back to JSON string return json.dumps(transformed_students)"},{"question":"Objective To assess your understanding of PyTorch\'s `torch.cond` function and data-dependent control flow in neural networks, you are required to implement a module that uses `torch.cond` to alter its behavior based on specific conditions. Problem Statement Implement a custom PyTorch module named `CustomCondModule`. This module should: - Accept a tensor `x` as input and return a transformed tensor as output. - Use `torch.cond` to decide between two different transformation functions based on the sum of the elements in the tensor `x`. - If the sum of the tensor elements is greater than a specified threshold, apply the `transform_fn1`. - If the sum of the tensor elements is less than or equal to the threshold, apply the `transform_fn2`. Transform Functions - `transform_fn1`: Performs an element-wise cosine operation followed by adding an element-wise sine operation on the tensor. - `transform_fn2`: Performs an element-wise sine operation on the tensor. Requirements 1. Define the two transformation functions: `transform_fn1` and `transform_fn2`. ```python def transform_fn1(x: torch.Tensor) -> torch.Tensor: return x.cos() + x.sin() def transform_fn2(x: torch.Tensor) -> torch.Tensor: return x.sin() ``` 2. Implement the `CustomCondModule` class which should use `torch.cond` in its `forward` method to switch between `transform_fn1` and `transform_fn2` based on the sum of `x` relative to the threshold. 3. The `CustomCondModule` should accept a threshold value during initialization to compare the sum of the tensor elements. 4. Ensure that your implementation adheres to the input-output signature of PyTorch modules. Constraints - You must use `torch.cond` for branching. - Assume the input tensor `x` is a 1-dimensional tensor. Expected Input and Output - **Input:** A tensor `x` of shape `(N,)` and type `torch.Tensor`. - **Output:** A tensor of the same shape as input. Example ```python import torch # Define CustomCondModule class CustomCondModule(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x): return x.cos() + x.sin() def false_fn(x): return x.sin() return torch.cond(x.sum() > self.threshold, true_fn, false_fn, (x,)) # Instantiate the module threshold = 4.0 mod = CustomCondModule(threshold) # Test inputs inp1 = torch.tensor([2.0, 1.0, 1.0]) inp2 = torch.tensor([2.0, 2.0, 1.0]) # Perform forward pass output1 = mod(inp1) # Should apply false_fn: sin(inp1) output2 = mod(inp2) # Should apply true_fn: cos(inp2) + sin(inp2) print(output1) print(output2) ``` Implement the `CustomCondModule` class in a way that the above example works correctly and efficiently.","solution":"import torch import torch.nn as nn def transform_fn1(x: torch.Tensor) -> torch.Tensor: return x.cos() + x.sin() def transform_fn2(x: torch.Tensor) -> torch.Tensor: return x.sin() class CustomCondModule(nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: if x.sum() > self.threshold: return transform_fn1(x) else: return transform_fn2(x)"},{"question":"Title: Implementing a Managed Descriptor with Custom Validation Objective: Design and implement a descriptor class in Python called `ValidatedAttribute` that manages an attribute with custom validation logic. This will assess the students\' understanding of descriptors, attribute management, and object-oriented programming concepts. Task Description: You are required to implement a descriptor class named `ValidatedAttribute` that manages access to a specific attribute by logging its access and validating the value against a set of constraints provided. The validation involves checking for the type of the value, and optionally its range (minimum and maximum). Requirements: 1. Implement the `ValidatedAttribute` descriptor class with the following methods: - `__set_name__(self, owner, name)`: Initializes the specific attribute name. - `__get__(self, obj, objtype=None)`: Retrieves the attribute value and logs the access. - `__set__(self, obj, value)`: Sets the attribute value after validation and logs the update. - `validate(self, value)`: Validates the value: - It should raise a `TypeError` if the value is not of the specified `data_type`. - If `min_value` or `max_value` are provided, it must check that the number is within the specified range and raise a `ValueError` otherwise. 2. Create a class named `Product` that uses `ValidatedAttribute` for its `price` attribute. Ensure that: - The `price` is validated to be a float. - The `price` should be within a specified range (e.g., minimum 0.0 and maximum 10000.0). 3. Demonstrate the functionality with examples showing: - Logging of access and updates. - Proper exception handling for invalid types and out-of-range values. Specifications: - `ValidatedAttribute` constructor parameters: - `data_type`: The type that the value should be (e.g., `float`). - `min_value` (optional): The minimum value allowed. - `max_value` (optional): The maximum value allowed. - Example of class `Product`: ```python class Product: price = ValidatedAttribute(float, min_value=0.0, max_value=10000.0) def __init__(self, name, price): self.name = name self.price = price ``` Constraints: - The `price` attribute must always be a `float`. - The `price` must be between 0.0 and 10000.0 inclusive. Example Usage: ```python import logging logging.basicConfig(level=logging.INFO) class ValidatedAttribute: def __init__(self, data_type, min_value=None, max_value=None): self.data_type = data_type self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.name = name self.private_name = f\'_{name}\' def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing {self.name} giving {value}\') return value def __set__(self, obj, value): self.validate(value) logging.info(f\'Updating {self.name} to {value}\') setattr(obj, self.private_name, value) def validate(self, value): if not isinstance(value, self.data_type): raise TypeError(f\'Expected {value!r} to be of type {self.data_type.__name__}\') if self.min_value is not None and value < self.min_value: raise ValueError(f\'Expected {value!r} to be at least {self.min_value}\') if self.max_value is not None and value > self.max_value: raise ValueError(f\'Expected {value!r} to be no more than {self.max_value}\') class Product: price = ValidatedAttribute(float, min_value=0.0, max_value=10000.0) def __init__(self, name, price): self.name = name self.price = price # Example operations try: p = Product(\'Laptop\', 1500.0) print(p.price) # Should log the access and print 1500.0 p.price = -100 # Should raise ValueError and log the error except Exception as e: print(e) try: p.price = \'expensive\' # Should raise TypeError and log the error except Exception as e: print(e) ``` Performance Requirements: - The descriptor class should efficiently handle setting and getting operations. - Validate method should perform the checks in constant time (O(1)).","solution":"import logging logging.basicConfig(level=logging.INFO) class ValidatedAttribute: def __init__(self, data_type, min_value=None, max_value=None): self.data_type = data_type self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.name = name self.private_name = f\'_{name}\' def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing {self.name} giving {value}\') return value def __set__(self, obj, value): self.validate(value) logging.info(f\'Updating {self.name} to {value}\') setattr(obj, self.private_name, value) def validate(self, value): if not isinstance(value, self.data_type): raise TypeError(f\'Expected {value!r} to be of type {self.data_type.__name__}\') if self.min_value is not None and value < self.min_value: raise ValueError(f\'Expected {value!r} to be at least {self.min_value}\') if self.max_value is not None and value > self.max_value: raise ValueError(f\'Expected {value!r} to be no more than {self.max_value}\') class Product: price = ValidatedAttribute(float, min_value=0.0, max_value=10000.0) def __init__(self, name, price): self.name = name self.price = price # Demonstrate functionality try: p = Product(\'Laptop\', 1500.0) print(p.price) # Should log the access and print 1500.0 p.price = -100 # Should raise ValueError and log the error except Exception as e: print(e) try: p.price = \'expensive\' # Should raise TypeError and log the error except Exception as e: print(e)"},{"question":"**Coding Assessment Question** You are tasked with designing a text-based interface application using the `curses.panel` module. This application must create two interactive panels that the user can manipulate in terms of visibility, order, and position. **Requirements:** 1. **Initialize the `curses` environment** and create two windows, each of which will be associated with a panel. 2. **Create two panels** from the windows and perform the initial setup: - Panel 1 should be at the top initially. - Panel 2 should be positioned below Panel 1. 3. **Implement the following functionalities:** - **Toggle Visibility**: Use a key to toggle the visibility of each panel. - **Move Panels**: Use different keys to move the panels to new positions on the screen. - **Change Stacking Order**: Use keys to bring a panel to the top or push it to the bottom of the stack. 4. **Ensure the state of the panels is updated on the screen** after each change. **Input Format:** - There will be no direct input from the user in a conventional sense, as the functionality will rely on capturing keystrokes within the `curses` environment to manipulate the panels. **Output Format:** - The output is a dynamically updated terminal screen reflecting the state and interactions with the panels. **Constraints:** - The application should run within a terminal environment that supports `curses`. - Handle all exceptional cases such as invalid key presses gracefully without the application crashing. **Hints:** - Use `curses.newwin()` to create windows. - Use `curses.panel.new_panel(win)` to create panels from windows. - Use appropriate panel methods like `panel.top()`, `panel.bottom()`, `panel.hide()`, `panel.show()`, and `panel.move(y, x)` to control the panels. To start, use the following template to scaffold your implementation: ```python import curses import curses.panel def main(stdscr): # Initialize curses curses.curs_set(0) # Create windows for panels win1 = curses.newwin(10, 40, 5, 10) win2 = curses.newwin(10, 40, 10, 20) # Create panels panel1 = curses.panel.new_panel(win1) panel2 = curses.panel.new_panel(win2) # Initial setup panel2.bottom() update_screen() # Main loop to capture user input and manipulate panels while True: key = stdscr.getch() # Example: Toggle visibility for panel1 with \'1\' key, panel 2 with \'2\' key if key == ord(\'1\'): if panel1.hidden(): panel1.show() else: panel1.hide() elif key == ord(\'2\'): if panel2.hidden(): panel2.show() else: panel2.hide() # Add more conditions to move panels or change stacking order # Update panels and screen curses.panel.update_panels() stdscr.refresh() def update_screen(): # Function to replace curses.doupdate(), can be customized curses.panel.update_panels() curses.doupdate() curses.wrapper(main) ``` Implement the remaining functionalities for moving panels and changing their stacking order.","solution":"import curses import curses.panel def main(stdscr): curses.curs_set(0) stdscr.nodelay(1) stdscr.timeout(100) win1 = curses.newwin(10, 40, 5, 10) win1.border(0) win1.addstr(1, 1, \\"Panel 1\\") win2 = curses.newwin(10, 40, 10, 20) win2.border(0) win2.addstr(1, 1, \\"Panel 2\\") panel1 = curses.panel.new_panel(win1) panel2 = curses.panel.new_panel(win2) panel2.bottom() update_screen() panel_visibility = {panel1: True, panel2: True} while True: key = stdscr.getch() if key == ord(\'1\'): if panel_visibility[panel1]: panel1.hide() else: panel1.show() panel_visibility[panel1] = not panel_visibility[panel1] elif key == ord(\'2\'): if panel_visibility[panel2]: panel2.hide() else: panel2.show() panel_visibility[panel2] = not panel_visibility[panel2] elif key == ord(\'a\'): # Move panel 1 left new_y, new_x = panel1.window().getbegyx() panel1.move(new_y, max(new_x - 1, 1)) elif key == ord(\'d\'): # Move panel 1 right new_y, new_x = panel1.window().getbegyx() panel1.move(new_y, min(new_x + 1, curses.COLS - 41)) elif key == ord(\'w\'): # Move panel 1 up new_y, new_x = panel1.window().getbegyx() panel1.move(max(new_y - 1, 1), new_x) elif key == ord(\'s\'): # Move panel 1 down new_y, new_x = panel1.window().getbegyx() panel1.move(min(new_y + 1, curses.LINES - 11), new_x) elif key == ord(\'j\'): # Move panel 2 left new_y, new_x = panel2.window().getbegyx() panel2.move(new_y, max(new_x - 1, 1)) elif key == ord(\'l\'): # Move panel 2 right new_y, new_x = panel2.window().getbegyx() panel2.move(new_y, min(new_x + 1, curses.COLS - 41)) elif key == ord(\'i\'): # Move panel 2 up new_y, new_x = panel2.window().getbegyx() panel2.move(max(new_y - 1, 1), new_x) elif key == ord(\'k\'): # Move panel 2 down new_y, new_x = panel2.window().getbegyx() panel2.move(min(new_y + 1, curses.LINES - 11), new_x) elif key == ord(\'t\'): # Bring panel 1 to top panel1.top() elif key == ord(\'b\'): # Send panel 1 to bottom panel1.bottom() elif key == ord(\'T\'): # Bring panel 2 to top panel2.top() elif key == ord(\'B\'): # Send panel 2 to bottom panel2.bottom() elif key == ord(\'q\'): # Quit the program break curses.panel.update_panels() stdscr.refresh() def update_screen(): curses.panel.update_panels() curses.doupdate() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"**Objective**: Assess the understanding of Python\'s audit framework and handling audit events. **Question**: Write a Python function `simulate_audit_events(events: List[Tuple[str, Dict[str, Any]]]) -> Dict[str, Dict[str, Any]]` that simulates the behavior of audit events. The function should raise specific audit events based on the input list and capture the resulting event data in a structured output. Function Signature: ```python def simulate_audit_events(events: List[Tuple[str, Dict[str, Any]]]) -> Dict[str, Dict[str, Any]]: ``` Input: - `events`: A list of tuples where each tuple contains: - A string representing an audit event name (e.g., `\\"builtins.input\\"`). - A dictionary with argument names and their corresponding values required for the event. Output: - `result`: A dictionary where keys are event names and values are dictionaries containing captured argument names and values for each event. Constraints: - Each event in the input list has valid argument names as specified in the documentation. - All arguments required for an event are provided in the dictionary. Example: Consider the following input: ```python events = [ (\\"builtins.input\\", {\\"prompt\\": \\"Enter your name: \\"}), (\\"os.chmod\\", {\\"path\\": \\"/tmp/test.txt\\", \\"mode\\": 0o644, \\"dir_fd\\": None}) ] ``` Expected output: ```python { \\"builtins.input\\": {\\"prompt\\": \\"Enter your name: \\"}, \\"os.chmod\\": {\\"path\\": \\"/tmp/test.txt\\", \\"mode\\": 0o644, \\"dir_fd\\": None} } ``` Additional Information: - Use the `sys.audit(event, *args)` function to raise an audit event. - Implement an audit hook using `sys.addaudithook(hook)` to capture audit events. Implementation Notes: 1. Define the `simulate_audit_events` function to iterate over the input events list. 2. Raise the appropriate audit event with the provided arguments using `sys.audit`. 3. Create an audit hook function to capture and store the arguments of each event. 4. Return the captured events in the structured dictionary format. You may use the following template to get started: ```python import sys def simulate_audit_events(events): captured_events = {} def audit_hook(event, args): if event not in captured_events: captured_events[event] = {} arg_names = args._fields if hasattr(args, \'_fields\') else [] # For namedtuple support for i, arg in enumerate(args): key = arg_names[i] if i < len(arg_names) else f\\"arg{i}\\" captured_events[event][key] = arg sys.addaudithook(audit_hook) for event, arguments in events: sys.audit(event, *tuple(arguments.values())) return captured_events # Test example events = [ (\\"builtins.input\\", {\\"prompt\\": \\"Enter your name: \\"}), (\\"os.chmod\\", {\\"path\\": \\"/tmp/test.txt\\", \\"mode\\": 0o644, \\"dir_fd\\": None}) ] print(simulate_audit_events(events)) ``` Ensure your implementation works correctly with a variety of events and arguments as specified in the audit events documentation.","solution":"import sys from typing import List, Tuple, Dict, Any def simulate_audit_events(events: List[Tuple[str, Dict[str, Any]]]) -> Dict[str, Dict[str, Any]]: captured_events = {} def audit_hook(event, args): if event not in captured_events: captured_events[event] = {} for i, arg in enumerate(args): key = arg._fields[i] if hasattr(arg, \'_fields\') else f\\"arg{i}\\" captured_events[event][key] = arg sys.addaudithook(audit_hook) for event, arguments in events: sys.audit(event, *tuple(arguments.values())) return captured_events"},{"question":"**Objective:** Demonstrate your understanding of PyTorch\'s MPS backend by verifying its availability, performing tensor operations, and using it to execute a neural network\'s inference on a GPU. **Question:** You are tasked with verifying the availability of the MPS backend in PyTorch, performing simple tensor manipulations on the MPS device, and running a basic neural network model on the MPS device for inference. **Instructions:** 1. **Check MPS Availability:** - Write a function `check_mps_availability()` that checks whether the MPS backend is available and returns a boolean indicating the availability. 2. **Tensor Operations on MPS:** - Write a function `tensor_operations_on_mps()` that: - Creates a tensor of shape (5,) filled with ones directly on the MPS device. - Multiplies the tensor by 2. - Returns the resulting tensor. 3. **Model Inference on MPS:** - Write a simple feed-forward neural network class `SimpleNet` that inherits from `torch.nn.Module`. - Implement a function `model_inference_on_mps(input_tensor: torch.Tensor) -> torch.Tensor` that: - Moves the `SimpleNet` model to the MPS device. - Performs inference on the provided `input_tensor` which is already on the MPS device. - Returns the prediction result. **Constraints:** - Assume the input tensor for inference will be of appropriate shape for the `SimpleNet` (e.g., a tensor of shape `(5,)`). **Expected Output:** - `check_mps_availability()` should return `True` if MPS is available, otherwise `False`. - `tensor_operations_on_mps()` should return a tensor of shape `(5,)` filled with the value `2.0`. - `model_inference_on_mps(input_tensor)` should return the model\'s prediction output tensor, which will be on the MPS device. You can assume the following model architecture for the `SimpleNet`: ```python import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(5, 5) # A simple linear layer def forward(self, x): return self.fc(x) ``` **Example Implementation:** ```python def check_mps_availability(): if not torch.backends.mps.is_available(): return False return True def tensor_operations_on_mps(): mps_device = torch.device(\\"mps\\") x = torch.ones(5, device=mps_device) y = x * 2 return y class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(5, 5) def forward(self, x): return self.fc(x) def model_inference_on_mps(input_tensor: torch.Tensor) -> torch.Tensor: mps_device = torch.device(\\"mps\\") model = SimpleNet().to(mps_device) return model(input_tensor) ``` **Testing Your Code:** You can use the following to test the above functions and verify their implementations: ```python # Test MPS availability print(check_mps_availability()) # Test tensor operations print(tensor_operations_on_mps()) # Test model inference mps_device = torch.device(\\"mps\\") input_tensor = torch.ones(5, device=mps_device) print(model_inference_on_mps(input_tensor)) ```","solution":"import torch import torch.nn as nn def check_mps_availability(): Checks if the MPS backend is available. return torch.backends.mps.is_available() def tensor_operations_on_mps(): Performs simple tensor operations on MPS. - Creates a tensor of shape (5,) filled with ones directly on the MPS device. - Multiplies the tensor by 2. - Returns the resulting tensor. if not check_mps_availability(): raise RuntimeError(\\"MPS backend is not available.\\") mps_device = torch.device(\\"mps\\") x = torch.ones(5, device=mps_device) y = x * 2 return y class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(5, 5) # A simple linear layer def forward(self, x): return self.fc(x) def model_inference_on_mps(input_tensor: torch.Tensor) -> torch.Tensor: Performs model inference on MPS. - Moves the SimpleNet model to the MPS device. - Performs inference on the provided input_tensor which is already on the MPS device. - Returns the prediction result. if not check_mps_availability(): raise RuntimeError(\\"MPS backend is not available.\\") mps_device = torch.device(\\"mps\\") model = SimpleNet().to(mps_device) model.eval() with torch.no_grad(): output = model(input_tensor) return output"},{"question":"**Objective**: Write a Python function that performs a series of operations on bytes objects, demonstrating a comprehensive understanding of the provided bytes-related functions. # Problem Statement Implement a function `process_bytes_operations` that takes two strings, `input_string` and `additional_string`, and performs the following operations: 1. **Convert Strings to Bytes**: - Use the provided functions to convert `input_string` and `additional_string` to bytes objects. 2. **Concatenate Bytes**: - Concatenate the bytes representation of `additional_string` to `input_string`. 3. **Resize the Concatenated Bytes**: - Resize the concatenated bytes object to a specific new size which is the sum of the lengths of `input_string` and `additional_string`. 4. **Extract Substring**: - Extract and return a substring from the concatenated bytes object starting from index 3 and of length 5. # Function Signature ```python def process_bytes_operations(input_string: str, additional_string: str) -> bytes: pass ``` # Input - `input_string` (str): Original input string. - `additional_string` (str): String to append to the original input string. # Output - Returns a bytes object that contains a substring starting from index 3 and of length 5 from the resized concatenated bytes object. # Constraints - The function should raise a `ValueError` if the lengths of `input_string` and `additional_string` combined are less than 8. - Handle the bytes objects carefully to avoid modifying the internal representation unnecessarily. # Example ```python result = process_bytes_operations(\\"hello\\", \\"world!\\") print(result) # Expected output: b\'lowor\' ``` # Notes - Use the provided bytes-related functions as much as possible. - Consider edge cases where the input strings may be empty or very short. Implement the `process_bytes_operations` function to fulfill the requirements described above. Ensure that the solution is clear, efficient, and makes proper use of the provided bytes manipulation functions.","solution":"def process_bytes_operations(input_string: str, additional_string: str) -> bytes: Process bytes operations as per the given problem statement. # Step 1: Convert strings to bytes input_bytes = input_string.encode(\'utf-8\') additional_bytes = additional_string.encode(\'utf-8\') # Concatenate bytes concatenated_bytes = input_bytes + additional_bytes # New size is the sum of the lengths of input_string and additional_string new_size = len(input_string) + len(additional_string) # Ensure combined length is sufficient for slicing if new_size < 8: raise ValueError(\\"Combined length of input_string and additional_string must be at least 8.\\") # Resize the concatenated bytes (not necessary since concatenation already gives us the correct size) # But for meticulousness, we\'d slice to the new size when necessary resized_bytes = concatenated_bytes[:new_size] # Extract and return a substring from index 3 to 3 + 5 (8) return resized_bytes[3:8]"},{"question":"Objective Implement a custom multi-headed attention mechanism using PyTorch\'s `torch.nn.attention.flex_attention` and `BlockMask` utilities. Problem Statement You are required to implement a function `multi_headed_attention` that performs multi-headed attention on a given set of queries, keys, and values. You should use the `flex_attention` function for performing the attention mechanism and `BlockMask` to handle attention masking. Function Signature ```python def multi_headed_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, num_heads: int, mask: Optional[BlockMask] = None) -> torch.Tensor: pass ``` Input - `query` (torch.Tensor): A tensor of shape `(batch_size, seq_len, d_model)` representing the queries. - `key` (torch.Tensor): A tensor of shape `(batch_size, seq_len, d_model)` representing the keys. - `value` (torch.Tensor): A tensor of shape `(batch_size, seq_len, d_model)` representing the values. - `num_heads` (int): The number of attention heads. - `mask` (Optional[BlockMask]): An optional `BlockMask` instance for masking attention mechanisms. Output - `output` (torch.Tensor): A tensor of shape `(batch_size, seq_len, d_model)`, where `d_model` is the dimension of the model after applying the multi-headed attention mechanism. Constraints - You must utilize the `flex_attention` function for the attention mechanism. - You must incorporate `BlockMask` for attention masking. - Ensure that the `query`, `key`, and `value` tensors are appropriately divided among the `num_heads`. Instructions 1. Split the `query`, `key`, and `value` tensors into `num_heads` parts. The dimensionality of each part should be `d_model // num_heads`. 2. Use the `flex_attention` function to compute the attention for each head. 3. Concatenate the resulting attention outputs from all heads. 4. Optionally, apply the provided `mask` using `BlockMask`. Example ```python import torch # Example data batch_size = 2 seq_len = 5 d_model = 16 num_heads = 4 query = torch.rand((batch_size, seq_len, d_model)) key = torch.rand((batch_size, seq_len, d_model)) value = torch.rand((batch_size, seq_len, d_model)) # Define the BlockMask if needed mask = None # Or an instance of BlockMask # Call the multi_headed_attention function output = multi_headed_attention(query, key, value, num_heads, mask) print(output.shape) # Expected: torch.Size([2, 5, 16]) ``` Submission Submit your implemented `multi_headed_attention` function.","solution":"import torch from torch.nn.functional import softmax def split_heads(tensor, num_heads): Splits the last dimension of a tensor into (num_heads, depth), where depth is d_model // num_heads. Arguments: tensor -- tensor to split, shape (batch_size, seq_len, d_model) num_heads -- number of attention heads Returns: tensor -- tensor of shape (batch_size, num_heads, seq_len, depth) batch_size, seq_len, d_model = tensor.shape depth = d_model // num_heads tensor = tensor.view(batch_size, seq_len, num_heads, depth) return tensor.permute(0, 2, 1, 3) # (batch_size, num_heads, seq_len, depth) def combine_heads(tensor): Combines the last two dimensions of a tensor split by heads into one dimension. Arguments: tensor -- tensor to combine, shape (batch_size, num_heads, seq_len, depth) Returns: tensor -- tensor of shape (batch_size, seq_len, d_model) batch_size, num_heads, seq_len, depth = tensor.shape d_model = num_heads * depth return tensor.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, d_model) def scaled_dot_product_attention(query, key, value, mask=None): Compute the scaled dot-product attention. Arguments: query -- query tensor, shape (batch_size, num_heads, seq_len_q, depth) key -- key tensor, shape (batch_size, num_heads, seq_len_k, depth) value -- value tensor, shape (batch_size, num_heads, seq_len_v, depth) mask -- optional mask tensor, shape broadcastable to (batch_size, num_heads, seq_len_q, seq_len_k) Returns: output -- attention output tensor, shape (batch_size, num_heads, seq_len_q, depth) attention_weights -- attention weights tensor, shape (batch_size, num_heads, seq_len_q, seq_len_k) matmul_qk = torch.matmul(query, key.transpose(-2, -1)) d_k = query.size(-1) scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) if mask is not None: scaled_attention_logits += (mask * -1e9) attention_weights = softmax(scaled_attention_logits, dim=-1) output = torch.matmul(attention_weights, value) return output def multi_headed_attention(query, key, value, num_heads, mask=None): Performs multi-headed attention. Arguments: query -- query tensor, shape (batch_size, seq_len, d_model) key -- key tensor, shape (batch_size, seq_len, d_model) value -- value tensor, shape (batch_size, seq_len, d_model) num_heads -- number of attention heads mask -- optional BlockMask instance for masking attention mechanisms Returns: output -- output tensor, shape (batch_size, seq_len, d_model) d_model = query.size(-1) assert d_model % num_heads == 0, \\"d_model must be divisible by num_heads\\" # Split into heads query = split_heads(query, num_heads) key = split_heads(key, num_heads) value = split_heads(value, num_heads) # Scaled dot-product attention attention_output = scaled_dot_product_attention(query, key, value, mask) # Combine heads and get output output = combine_heads(attention_output) return output"},{"question":"Implement a class called `CustomSequence`, which emulates the behavior of a container sequence type. Your class should fulfill the following requirements: 1. **Initialization**: - The constructor should accept any iterable and initialize an internal container with it. 2. **Special Methods**: - Implement `__getitem__`, `__setitem__`, and `__delitem__` to allow indexing, assigning values, and deleting values similar to list behavior. - Implement the `__len__` method to return the length of the sequence. - Implement the `__iter__` method to allow iteration through the elements. - Implement `__contains__` to support the `in` keyword. - Implement `__repr__` and `__str__` to provide meaningful string representations. - Implement `__add__`, `__radd__`, and `__iadd__` to allow concatenation with other sequences. - Implement `__mul__` and `__rmul__` to support repetition. 3. **Performance Constraints**: - Your implementation should aim for operations being performed in O(1) time complexity where feasible (e.g., indexing, length retrieval). - Concatenation and repetition should operate in O(n) time complexity, where n is the size of the resulting sequence. 4. **Edge Cases**: - Ensure that your implementation gracefully handles common edge cases such as: - Attempting to access or delete an element out of range (should raise `IndexError`). - Providing an invalid type as an index (should raise `TypeError`). # Example Usage ```python # Initialization cs = CustomSequence([1, 2, 3, 4]) # String representation print(repr(cs)) # CustomSequence([1, 2, 3, 4]) print(str(cs)) # [1, 2, 3, 4] # Indexing print(cs[2]) # 3 # Assignment cs[1] = 42 print(cs) # [1, 42, 3, 4] # Deletion del cs[3] print(cs) # [1, 42, 3] # Length print(len(cs)) # 3 # Iteration for item in cs: print(item) # Membership testing print(42 in cs) # True print(5 in cs) # False # Concatenation cs2 = CustomSequence([\'a\', \'b\']) cs3 = cs + cs2 print(cs3) # [1, 42, 3, \'a\', \'b\'] # Repetition cs4 = cs * 2 print(cs4) # [1, 42, 3, 1, 42, 3] ``` # Notes - The internal state of the `CustomSequence` should not be altered directly; interactions should be limited to using the defined methods and operators. - Ensure to write comprehensive unit tests to verify each functionality.","solution":"class CustomSequence: def __init__(self, iterable): self._container = list(iterable) def __getitem__(self, index): return self._container[index] def __setitem__(self, index, value): self._container[index] = value def __delitem__(self, index): del self._container[index] def __len__(self): return len(self._container) def __iter__(self): return iter(self._container) def __contains__(self, item): return item in self._container def __repr__(self): return f\\"CustomSequence({self._container})\\" def __str__(self): return str(self._container) def __add__(self, other): if isinstance(other, CustomSequence): return CustomSequence(self._container + other._container) elif isinstance(other, list): return CustomSequence(self._container + other) else: raise TypeError(f\\"unsupported operand type(s) for +: \'CustomSequence\' and \'{type(other).__name__}\'\\") def __radd__(self, other): if isinstance(other, list): return CustomSequence(other + self._container) else: raise TypeError(f\\"unsupported operand type(s) for +: \'{type(other).__name__}\' and \'CustomSequence\'\\") def __iadd__(self, other): if isinstance(other, CustomSequence): self._container += other._container elif isinstance(other, list): self._container += other else: raise TypeError(f\\"unsupported operand type(s) for +=: \'CustomSequence\' and \'{type(other).__name__}\'\\") return self def __mul__(self, n): if isinstance(n, int): return CustomSequence(self._container * n) else: raise TypeError(f\\"unsupported operand type(s) for *: \'CustomSequence\' and \'{type(n).__name__}\'\\") def __rmul__(self, n): return self.__mul__(n)"},{"question":"You have been provided with a dataset containing information about various products in a shop. Your task is to perform a series of data manipulation operations using pandas to extract and modify specific subsets of data. **Dataset:** Below are some sample data representing the columns of the `products` DataFrame: ```python import pandas as pd import numpy as np data = { \'ProductID\': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110], \'ProductName\': [\'Widget\', \'Gadget\', \'Doohickey\', \'Thingamajig\', \'Whatchamacallit\', \'Gizmo\', \'Doodad\', \'Contraption\', \'Apparatus\', \'Device\'], \'Category\': [\'A\', \'B\', \'A\', \'A\', \'C\', \'B\', \'A\', \'C\', \'B\', \'A\'], \'Price\': [25.50, 30.00, 15.75, 40.00, 27.50, 33.00, 55.00, 60.00, 22.50, 45.00], \'Stock\': [100, 150, 200, 175, 90, 85, 120, 60, 130, 95] } products = pd.DataFrame(data) ``` **Tasks:** 1. Select all columns for rows where the `Category` is \\"A\\" and the `Price` is greater than 20. Use `.loc` and boolean conditions for this task. 2. Retrieve the `ProductName` and `Price` columns for the first 5 products using `.iloc`. 3. Create a new column `DiscountedPrice` which contains the `Price` discounted by 10% for products that are in Category \\"B\\" and have a `Stock` less than 100. 4. Replace the `Stock` of products where `ProductName` ends with \\"d\\" with a new stock value of 999, using a callable within `loc`. 5. Drop any duplicate rows based on the `Category` column, keeping the first occurrences. 6. Extract and display the `ProductID` and `Price` of rows where the `Stock` is missing (None or NaN). Use boolean indexing and handling missing data methods to achieve this. **Constraints:** - Ensure the operations are performed efficiently, and the code is readable and well-documented. - Do not use any for loops; leverage pandas\' vectorized operations. **Expected Output:** - DataFrame with filtered rows and columns as specified in each task. - Modifications in the DataFrame reflecting updates made by the operations.","solution":"import pandas as pd # Sample data data = { \'ProductID\': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110], \'ProductName\': [\'Widget\', \'Gadget\', \'Doohickey\', \'Thingamajig\', \'Whatchamacallit\', \'Gizmo\', \'Doodad\', \'Contraption\', \'Apparatus\', \'Device\'], \'Category\': [\'A\', \'B\', \'A\', \'A\', \'C\', \'B\', \'A\', \'C\', \'B\', \'A\'], \'Price\': [25.50, 30.00, 15.75, 40.00, 27.50, 33.00, 55.00, 60.00, 22.50, 45.00], \'Stock\': [100, 150, 200, 175, 90, 85, 120, 60, 130, 95] } products = pd.DataFrame(data) def filter_category_price(df): Select all columns for rows where the Category is \\"A\\" and the Price is greater than 20. return df.loc[(df[\'Category\'] == \'A\') & (df[\'Price\'] > 20)] def retrieve_first_five(df): Retrieve the ProductName and Price columns for the first 5 products. return df.iloc[:5][[\'ProductName\', \'Price\']] def add_discounted_price(df): Create a new column DiscountedPrice which contains the Price discounted by 10% for products that are in Category \\"B\\" and have a Stock less than 100. df.loc[(df[\'Category\'] == \'B\') & (df[\'Stock\'] < 100), \'DiscountedPrice\'] = df[\'Price\'] * 0.9 return df def replace_stock(df): Replace the Stock of products where ProductName ends with \\"d\\" with a new stock value of 999. df.loc[df[\'ProductName\'].str.endswith(\'d\'), \'Stock\'] = 999 return df def drop_duplicates(df): Drop any duplicate rows based on the Category column, keeping the first occurrences. return df.drop_duplicates(subset=[\'Category\']) def extract_missing_stock(df): Extract and display the ProductID and Price of rows where the Stock is missing (None or NaN). return df[df[\'Stock\'].isna()][[\'ProductID\', \'Price\']]"},{"question":"Objective: Design a function using pandas to assess students\' understanding of pandas options configuration, as well as their ability to perform DataFrame manipulations. The function should showcase their knowledge in both setting global options and handling data effectively. Task: You need to write a function `customize_and_process(df: pd.DataFrame)` that: 1. Sets the global option to display a maximum of 10 columns when printing DataFrames. 2. Sets the global option for the maximum width of the display to 500 characters. 3. Uses the `option_context` to temporarily set the display precision for floating-point numbers to 2. 4. Prints the summary of all current pandas options. 5. Processes the given DataFrame by performing the following: - Generates a new column \'sum\', which is the sum of all numeric columns row-wise. - Filters out rows where the sum is less than a given threshold (assume 50). - Sorts the DataFrame by the new \'sum\' column in descending order. Input: - `df`: A pandas DataFrame with at least some numeric columns. Output: - Returns a DataFrame after processing with the sum column included. Constraints: - You must not change the global configuration back manually after processing. Use pandas\' tools to ensure that temporary settings are reverted back automatically. - Assume that the DataFrame will always have at least one numeric column. Example: ```python import pandas as pd df = pd.DataFrame({ \'A\': [10, 20, 30], \'B\': [15, 25, 35], \'C\': [5, 5, 5] }) result = customize_and_process(df) print(result) ``` Expected output: ``` A B C sum 2 30 35 5 70 1 20 25 5 50 ``` Function Signature: ```python def customize_and_process(df: pd.DataFrame) -> pd.DataFrame: pass ```","solution":"import pandas as pd def customize_and_process(df: pd.DataFrame) -> pd.DataFrame: Customizes pandas options and processes the input DataFrame. Parameters: df (pd.DataFrame): Input DataFrame with at least some numeric columns. Returns: pd.DataFrame: Processed DataFrame with added \'sum\' column. # Set global options pd.set_option(\'display.max_columns\', 10) pd.set_option(\'display.width\', 500) with pd.option_context(\'display.precision\', 2): # Temporary setting for precision print(pd.describe_option()) # Process DataFrame numeric_cols = df.select_dtypes(include=\'number\') df[\'sum\'] = numeric_cols.sum(axis=1) df = df[df[\'sum\'] >= 50] df = df.sort_values(by=\'sum\', ascending=False) return df"},{"question":"# Coding Challenge: Implement and Evaluate Semi-Supervised Learning with Self Training and Label Propagation You are given a dataset containing both labeled and unlabeled samples. Your task is to implement a semi-supervised learning solution using the Self Training and Label Propagation methods provided by `sklearn.semi_supervised`. You will then evaluate the performance of these methods. **Dataset:** - You have access to the `iris` dataset from the `sklearn.datasets` module. This is a well-known dataset containing measurements of iris flowers and their species. For the purpose of this exercise, assume that only some of the samples are labeled. **Task:** 1. **Data Preparation:** - Load the `iris` dataset. - Randomly set the labels of 50% of the samples to `-1` (indicating they are unlabeled). 2. **Self Training:** - Implement a semi-supervised learning solution using `SelfTrainingClassifier`. Use a `DecisionTreeClassifier` as the base estimator. - Train the `SelfTrainingClassifier` on the modified dataset with 50% unlabeled data. 3. **Label Propagation:** - Implement a semi-supervised learning solution using `LabelPropagation`. - Train the `LabelPropagation` model on the modified dataset with 50% unlabeled data. 4. **Evaluation:** - Evaluate the performance of both models using accuracy. Use the original labels for evaluation. - Print the accuracy of each model. **Constraints:** - Use `random_state=42` where applicable to ensure reproducibility. - Ensure your solution is efficient and runs within a reasonable time frame. **Expected Input and Output:** - **Input:** No direct input required; the solution script should load the dataset and modify it as described. - **Output:** Accuracy of the models. **Example Code Structure:** ```python from sklearn.datasets import load_iris from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score import numpy as np def main(): # Load the iris dataset data = load_iris() X, y = data.data, data.target # randomly set 50% of the labels to -1 (unlabeled) rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(len(y)) < 0.5 y[random_unlabeled_points] = -1 # Self Training print(\\"Self Training:\\") self_training_model = SelfTrainingClassifier(DecisionTreeClassifier(random_state=42)) self_training_model.fit(X, y) self_training_accuracy = accuracy_score(data.target, self_training_model.predict(X)) print(f\\"Accuracy: {self_training_accuracy:.2f}\\") # Label Propagation print(\\"Label Propagation:\\") label_propagation_model = LabelPropagation() label_propagation_model.fit(X, y) label_propagation_accuracy = accuracy_score(data.target, label_propagation_model.predict(X)) print(f\\"Accuracy: {label_propagation_accuracy:.2f}\\") if __name__ == \\"__main__\\": main() ``` **Notes:** - You may assume that the environment has `scikit-learn` installed. - The script should be self-contained and should output the accuracy of both semi-supervised learning models.","solution":"from sklearn.datasets import load_iris from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score import numpy as np def evaluate_semi_supervised_learning(): # Load the iris dataset data = load_iris() X, y = data.data, data.target # Randomly set 50% of the labels to -1 (unlabeled) rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(len(y)) < 0.5 y[random_unlabeled_points] = -1 # Self Training self_training_model = SelfTrainingClassifier(DecisionTreeClassifier(random_state=42)) self_training_model.fit(X, y) self_training_accuracy = accuracy_score(data.target, self_training_model.predict(X)) # Label Propagation label_propagation_model = LabelPropagation() label_propagation_model.fit(X, y) label_propagation_accuracy = accuracy_score(data.target, label_propagation_model.predict(X)) return { \\"self_training_accuracy\\": self_training_accuracy, \\"label_propagation_accuracy\\": label_propagation_accuracy } if __name__ == \\"__main__\\": accuracies = evaluate_semi_supervised_learning() print(\\"Self Training Accuracy:\\", accuracies[\\"self_training_accuracy\\"]) print(\\"Label Propagation Accuracy:\\", accuracies[\\"label_propagation_accuracy\\"])"},{"question":"**Assessment Question: Combining Itertools for Advanced Iteration** # Overview This task aims to assess your proficiency with the itertools module by combining multiple iterator functions to solve a complex problem. You are required to implement a function that processes lengthy string inputs efficiently using different itertools functions. # Problem Statement Write a function `process_strings(strings_list: List[str]) -> List[Tuple[str, int]]` that takes a list of strings and returns a list of tuples. Each tuple contains a string and an integer. The string should be a unique string that appeared in the longest consecutive order and the integer should be the number of consecutive occurrences. Utilize the itertools functions where applicable. # Constraints - The list will contain at least one string. - All strings are case-sensitive. - Performance should be efficient enough to handle lists containing up to 100,000 strings. # Implementation Details 1. **Itertools Functions**: Use the following itertools functions creatively to implement the solution: - `groupby()` - `islice()` - `tee()` - Any other itertools functions where necessary 2. **Expected Output**: The output should be a list of tuples. Each tuple should contain: - A unique string that appeared the most times consecutively. - The count of consecutive occurrences. 3. **Example**: ```python input_strings = [\\"apple\\", \\"apple\\", \\"banana\\", \\"apple\\", \\"apple\\", \\"apple\\", \\"banana\\", \\"banana\\", \\"cherry\\"] print(process_strings(input_strings)) # Expected Output: [(\'apple\', 3), (\'banana\', 2), (\'cherry\', 1)] ``` # Function Signature ```python from typing import List, Tuple import itertools def process_strings(strings_list: List[str]) -> List[Tuple[str, int]]: pass ``` # Notes - Do not use any additional libraries outside of itertools and built-in Python libraries. - Clean and efficient code will be scored higher. Good luck!","solution":"from typing import List, Tuple import itertools def process_strings(strings_list: List[str]) -> List[Tuple[str, int]]: result = [] for key, group in itertools.groupby(strings_list): length = sum(1 for _ in group) result.append((key, length)) return result"},{"question":"# Web Scraper and Error Handling Challenge **Objective:** Design a Python function named `scrape_and_process` that scrapes a given URL, handles HTTP errors, manages redirects, and processes custom headers. The function should return the content of the web page and handle specific conditions as detailed below. **Function Signature:** ```python def scrape_and_process(url: str, user_agent: str, timeout: int) -> str: pass ``` **Input:** - `url` (string): The URL of the web page to fetch. - `user_agent` (string): The User-Agent header that should be sent with the request to mimic a specific browser. - `timeout` (int): Timeout duration in seconds for the request. **Output:** - Returns the content (string) of the fetched page. - If a redirect is encountered, follow it and fetch the final URL\'s content. - If an HTTP error occurs (status code 400-599), the function should return a string formatted as `\\"HTTP Error: <status_code> <error_message>\\"`. - If a URL error occurs (e.g., unable to reach the server), return a string formatted as `\\"URL Error: <reason>\\"`. **Constraints:** - You must handle exceptions and errors as described. - You should use handlers and opener objects to manage custom settings such as timeout and user-agent. **Example:** ```python # Example usage of the function content = scrape_and_process(\\"http://example.com\\", \\"Mozilla/5.0 (...)\\", 10) print(content) ``` **Notes:** - Use `urllib.request` and other relevant modules as described in the provided documentation. - Remember to manage proper exception handling to ensure the program doesn\'t crash. **Hints:** 1. Use `urllib.request.Request` to create the request object. 2. Add headers to the request to set the User-Agent. 3. Implement exception handling for `HTTPError` and `URLError`. 4. Consider using a custom opener to set the global timeout for your requests. This question assesses the student\'s ability to handle HTTP operations, customize requests, and manage errors effectively, reflecting real-world scenarios in network programming.","solution":"import urllib.request import urllib.error def scrape_and_process(url: str, user_agent: str, timeout: int) -> str: Scrapes the given URL with a specified user agent and timeout. Parameters: - url (str): The URL of the web page to fetch. - user_agent (str): The User-Agent header that should be sent with the request. - timeout (int): Timeout duration in seconds for the request. Returns: - str: Content of the fetched page or an error message. try: request = urllib.request.Request(url) request.add_header(\'User-Agent\', user_agent) with urllib.request.urlopen(request, timeout=timeout) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\\"HTTP Error: {e.code} {e.reason}\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\""},{"question":"# PyTorch Coding Challenge **Problem Statement:** You are tasked with implementing a Python class `CustomTensorOperations` that utilizes PyTorch tensors for specific operations. This class should handle tensor creation, element-wise operations, and differentiate a given tensor operation automatically. Specifically, your class should include the following methods: 1. **`initialize_tensor(data: list, dtype: torch.dtype, device: torch.device) -> torch.Tensor`**: This method should take a nested list `data`, a `dtype`, and a `device` as input and return a PyTorch tensor initialized with the given data, data type, and device. 2. **`elementwise_operations(tensor1: torch.Tensor, tensor2: torch.Tensor, operation: str) -> torch.Tensor`**: This method should perform element-wise operations between two tensors `tensor1` and `tensor2`. The `operation` parameter can be one of the following strings: `\'add\'`, `\'subtract\'`, `\'multiply\'`, `\'divide\'`. Return the resulting tensor after applying the specified operation. 3. **`automatic_differentiation(tensor: torch.Tensor, operation: callable) -> torch.Tensor`**: This method should take a tensor `tensor` and a function `operation` that applies some PyTorch operations and returns a new tensor. The method should then compute and return the gradient of the output tensor with respect to the input tensor using PyTorch\'s autograd functionality. Assume that the input tensor already has `requires_grad=True`. **Requirements:** - The implementation must use PyTorch and handle tensor creation on different devices (CPU or CUDA). - Ensure that the element-wise operation is applied correctly and can handle tensors of the same shape. - Apply automatic differentiation correctly and return the gradient tensor. **Example Usage:** ```python import torch class CustomTensorOperations: @staticmethod def initialize_tensor(data, dtype, device): return torch.tensor(data, dtype=dtype, device=device) @staticmethod def elementwise_operations(tensor1, tensor2, operation): if operation == \'add\': return tensor1 + tensor2 elif operation == \'subtract\': return tensor1 - tensor2 elif operation == \'multiply\': return tensor1 * tensor2 elif operation == \'divide\': return tensor1 / tensor2 else: raise ValueError(\\"Unsupported operation\\") @staticmethod def automatic_differentiation(tensor, operation): output = operation(tensor) output.backward() return tensor.grad # Usage example # 1. Initialize tensor data = [[1.0, 2.0], [3.0, 4.0]] dtype = torch.float32 device = torch.device(\'cpu\') tensor = CustomTensorOperations.initialize_tensor(data, dtype, device) print(tensor) # 2. Element-wise operations tensor1 = torch.tensor([[1, 2], [3, 4]], dtype=torch.int32) tensor2 = torch.tensor([[5, 6], [7, 8]], dtype=torch.int32) result = CustomTensorOperations.elementwise_operations(tensor1, tensor2, \'add\') print(result) # 3. Automatic Differentiation tensor = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True) operation = lambda x: x.pow(2).sum() grad = CustomTensorOperations.automatic_differentiation(tensor, operation) print(grad) ``` **Notes:** - You can assume that the input tensors for element-wise operations will always be of the same shape. - Ensure that error handling for unsupported operations is implemented. - Test your solution with various data types and devices.","solution":"import torch class CustomTensorOperations: @staticmethod def initialize_tensor(data, dtype, device): Initialize a tensor with the given data, dtype, and device. Parameters: data (list): Nested list representing the tensor data. dtype (torch.dtype): Desired data type of the tensor. device (torch.device): Desired device for the tensor. Returns: torch.Tensor: Tensor created with the specified parameters. return torch.tensor(data, dtype=dtype, device=device) @staticmethod def elementwise_operations(tensor1, tensor2, operation): Perform element-wise operations on two tensors. Parameters: tensor1 (torch.Tensor): First tensor. tensor2 (torch.Tensor): Second tensor. operation (str): Operation to perform (\'add\', \'subtract\', \'multiply\', \'divide\'). Returns: torch.Tensor: Result of the element-wise operation. Raises: ValueError: If an unsupported operation is specified. if operation == \'add\': return tensor1 + tensor2 elif operation == \'subtract\': return tensor1 - tensor2 elif operation == \'multiply\': return tensor1 * tensor2 elif operation == \'divide\': return tensor1 / tensor2 else: raise ValueError(\\"Unsupported operation specified. Choose from \'add\', \'subtract\', \'multiply\', \'divide\'.\\") @staticmethod def automatic_differentiation(tensor, operation): Compute gradient of the output tensor with respect to the input tensor. Parameters: tensor (torch.Tensor): Input tensor with requires_grad=True. operation (callable): A function that applies some operations on the tensor. Returns: torch.Tensor: Gradient tensor. output = operation(tensor) output.backward() return tensor.grad"},{"question":"Objective Implement a Python function that reads data from a text file, processes it, and handles various exceptions that may occur during the process. This will demonstrate your understanding of exception handling, custom exceptions, and resource management. Problem Statement You are provided with a function `process_file(file_path: str) -> List[int]`, which reads integers from a file, processes them, and returns the processed list of integers. The function needs to handle various exceptions and manage the file resource properly. 1. **Function Signature:** ```python def process_file(file_path: str) -> List[int]: ``` 2. **Input:** - `file_path` (str): The path to the text file containing integer data. Each line in the file contains a single integer. 3. **Output:** - Returns a list of integers read from the file after processing each integer by doubling its value. 4. **Constraints:** - If the file does not exist, the function should raise a `FileNotFoundError` with a custom message: \\"The file at `file_path` was not found.\\" - If the file contains data that cannot be converted to an integer, the function should raise a `ValueError` with a custom message: \\"Invalid data found in the file.\\" - Use the `with` statement to handle file operations ensuring the file is properly closed after reading. 5. **Examples:** ```python # Example 1: # Assume \'data.txt\' exists and contains the following lines: # 1 # 2 # 3 # process_file(\'data.txt\') # Output: [2, 4, 6] # Example 2: # If \'invalid_data.txt\' contains the following lines: # 1 # two # 3 # process_file(\'invalid_data.txt\') # Raises ValueError: \\"Invalid data found in the file.\\" # Example 3: # If \'non_existent.txt\' does not exist: # process_file(\'non_existent.txt\') # Raises FileNotFoundError: \\"The file at non_existent.txt was not found.\\" ``` Implementation Requirements 1. Handle the exceptions as specified. 2. Ensure the file is closed after its content is processed using the `with` statement. Additional Notes - Do not use any external libraries; rely on standard Python functionality. - Custom exception messages are essential for the assessment.","solution":"from typing import List def process_file(file_path: str) -> List[int]: Reads integers from a file, processes them by doubling the value, and returns the list of processed integers. :param file_path: Path to the text file containing integer data. :return: List of processed integers. :raises FileNotFoundError: If the file does not exist. :raises ValueError: If the file contains invalid data that cannot be converted to an integer. try: with open(file_path, \'r\') as file: lines = file.readlines() result = [] for line in lines: try: number = int(line.strip()) result.append(number * 2) except ValueError: raise ValueError(\\"Invalid data found in the file.\\") return result except FileNotFoundError: raise FileNotFoundError(f\\"The file at {file_path} was not found.\\")"},{"question":"Coding Assessment Question # Objective Your task is to write a Python function that uses the `chunk` module to process a file containing IFF chunks. Specifically, you will extract the names and sizes of all the chunks in the file and return this information as a list of tuples. # Function Signature ```python def extract_chunk_info(file_path: str) -> list: ``` # Input - `file_path` (str): The path to the file containing IFF chunks. # Output - `List[Tuple[str, int]]`: A list of tuples where each tuple contains the name (ID) of a chunk (as a string) and its size (as an integer). # Description 1. Open the file at `file_path` in binary read mode. 2. Use the `chunk.Chunk` class to read each chunk in the file. 3. For each chunk, get its name (ID) and size using the `getname()` and `getsize()` methods. 4. Append the name and size as a tuple to a list. 5. Continue reading until the end of the file. 6. Return the list of tuples. # Constraints - The file at `file_path` is guaranteed to follow the IFF chunk format. # Example ```python # Example usage output = extract_chunk_info(\\"path/to/your/file.iff\\") print(output) # Expected Output: [(\'CHNK\', 1024), (\'DATA\', 2048), (\'META\', 512), ...] ``` # Notes - You may assume that the file fits into memory. - Proper exception handling should be implemented for file operations. - The documentation provided above should be utilized to understand the methods and attributes of the `chunk.Chunk` class. # Hints - You may find it useful to refer to the `chunk` module documentation for understanding the `chunk.Chunk` class methods. - Remember to handle the `EOFError` which indicates the end of the file.","solution":"import chunk def extract_chunk_info(file_path: str) -> list: Extracts the names and sizes of all the chunks in the IFF file. Args: file_path (str): The path to the file containing IFF chunks. Returns: list: A list of tuples where each tuple contains the name (ID) of a chunk and its size. chunk_info = [] try: with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f, align=True, bigendian=True) chunk_info.append((ch.getname().decode(\'ascii\'), ch.getsize())) ch.skip() # Move to the next chunk except EOFError: break # End of file reached except FileNotFoundError: print(f\\"File {file_path} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") return chunk_info"},{"question":"Question: Implementation of a Custom Function Using PyTorch Special Module You are working on a scientific computing project where you need to implement a custom mathematical function leveraging PyTorch\'s special functions. Your task is to create a composite function that represents a combination of scaled modified Bessel functions and the gamma function. # Your Task Implement a function in PyTorch that takes a tensor of input values and computes the following custom composite function for each element in the tensor: [ f(x) = frac{exp(text{torch.special.i0}(x))}{exp(text{torch.special.gammaln}(x + 1))} ] # Function Signature ```python def custom_composite_function(x: torch.Tensor) -> torch.Tensor: pass ``` # Input - `x`: A 1D tensor of floats, where each element x_i satisfies `0 < x_i <= 10`. # Output - Returns a 1D tensor of the same shape as `x`, where each element is the result of computing the composite function ( f(x) ) for the corresponding input element. # Example Input ```python import torch x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]) ``` Output ```python custom_composite_function(x) ``` The output should be a tensor with the computed values. # Constraints - You must use PyTorch operations and functions from `torch.special` module. - Avoid using loops to maintain performance with tensor operations. # Performance Requirements - Ensure your solution is efficient and leverages PyTorch\'s capabilities for handling tensor operations in a vectorized manner. Implement the function and test it with different input values to validate its correctness.","solution":"import torch def custom_composite_function(x: torch.Tensor) -> torch.Tensor: Computes the custom composite function for each element in the input tensor x. The function is defined as: f(x) = exp(torch.special.i0(x)) / exp(torch.special.gammaln(x + 1)) Parameters: x (torch.Tensor): A 1D tensor of floats (0 < x_i <= 10) Returns: torch.Tensor: A 1D tensor where each element is the result of the custom function i0_values = torch.special.i0(x) gammaln_values = torch.special.gammaln(x + 1) # Applying the exponential function to the i0 and gammaln values exp_i0_values = torch.exp(i0_values) exp_gammaln_values = torch.exp(gammaln_values) # Computing the final result result = exp_i0_values / exp_gammaln_values return result # Example code to test the function x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]) print(custom_composite_function(x))"},{"question":"Coding Assessment Question # Objective You are required to demonstrate your understanding of the `urllib` module in Python by implementing a function that performs various web-related tasks. # Problem Statement Implement a function `fetch_page_info(url: str, params: dict = None, headers: dict = None) -> dict` that takes in a URL, optional query parameters, and HTTP headers, and returns a dictionary with the following information: - `final_url`: The final URL after all redirections. - `status_code`: The HTTP status code of the response. - `headers`: The HTTP headers of the response. - `content`: The content of the page in plain text. - `parsed_url`: A dictionary containing the different components of the initial URL (scheme, netloc, path, params, query, fragment). # Input - `url` (str): A string representing the initial URL to be fetched. - `params` (dict, optional): A dictionary of query parameters to be included in the URL. - `headers` (dict, optional): A dictionary of HTTP headers to be sent with the request. # Output - A dictionary with the following information: - `final_url` (str): The final URL after redirections. - `status_code` (int): The HTTP status code of the response. - `headers` (dict): The HTTP headers of the response. - `content` (str): The content of the page in plain text. - `parsed_url` (dict): The parsed components of the initial URL, consisting of the keys `scheme`, `netloc`, `path`, `params`, `query`, and `fragment`. # Constraints 1. Use the `urllib` module to handle URL opening, query parameters, and HTTP headers. 2. Handle any HTTP redirections automatically. 3. Support both HTTP and HTTPS URLs. 4. The function should handle potential errors such as invalid URLs or network issues gracefully and should return appropriate error messages. # Example ```python url = \\"http://httpbin.org/get\\" params = {\\"key\\": \\"value\\"} headers = {\\"User-Agent\\": \\"python-urllib/3.10\\"} result = fetch_page_info(url, params, headers) # Expected Output { \\"final_url\\": \\"http://httpbin.org/get?key=value\\", \\"status_code\\": 200, \\"headers\\": { # HTTP response headers }, \\"content\\": \\"{n \\"args\\": {n \\"key\\": \\"value\\"n },n ...\\", \\"parsed_url\\": { \\"scheme\\": \\"http\\", \\"netloc\\": \\"httpbin.org\\", \\"path\\": \\"/get\\", \\"params\\": \\"\\", \\"query\\": \\"key=value\\", \\"fragment\\": \\"\\" } } ``` # Notes 1. Use `urllib.parse` for constructing and parsing URLs. 2. Use `urllib.request` for opening URLs and handling headers. 3. Ensure the function follows Python best practices and handles exceptions gracefully.","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError def fetch_page_info(url: str, params: dict = None, headers: dict = None) -> dict: Fetches and returns information about a web page. Args: - url (str): The URL to fetch. - params (dict, optional): Query parameters to include in the URL. - headers (dict, optional): Headers to include in the request. Returns: - dict: A dictionary containing the final URL, status code, headers, content, and parsed URL components. try: # Parse and prepend any parameters to the URL if params: query_string = urllib.parse.urlencode(params) url = f\\"{url}?{query_string}\\" # Create a request object req = urllib.request.Request(url, headers=headers or {}) # Open the URL and fetch data with urllib.request.urlopen(req) as response: final_url = response.geturl() status_code = response.getcode() response_headers = dict(response.getheaders()) content = response.read().decode(\'utf-8\') # Parse the initial URL parsed_url = urllib.parse.urlparse(url) parsed_url_dict = { \'scheme\': parsed_url.scheme, \'netloc\': parsed_url.netloc, \'path\': parsed_url.path, \'params\': parsed_url.params, \'query\': parsed_url.query, \'fragment\': parsed_url.fragment } return { \'final_url\': final_url, \'status_code\': status_code, \'headers\': response_headers, \'content\': content, \'parsed_url\': parsed_url_dict } except (URLError, HTTPError) as e: return { \'error\': str(e) }"},{"question":"Question: Implementing and Optimizing a Non-Negative Matrix Factorization Algorithm # Objective Your task is to implement a basic Non-Negative Matrix Factorization (NMF) algorithm and then profile and optimize its performance using the guidelines provided in the documentation. # Background NMF is a group of algorithms in multivariate analysis and linear algebra where a matrix **V** is factorized into (usually) two matrices **W** and **H**, with the property that all three matrices have no negative elements. Mathematically, the problem can be expressed as: [ V approx W cdot H ] # Instructions 1. **Implement the Basic NMF Algorithm**: Implement a basic version of the NMF algorithm in Python using only NumPy. Avoid using loops; instead, use NumPy\'s vectorized operations. 2. **Profile the Initial Implementation**: Profile your initial implementation using IPython\'s built-in `%timeit` and `%prun` magic commands to identify performance bottlenecks. 3. **Optimize the Algorithm with Cython**: Convert the bottleneck function to Cython for optimization. Provide type declarations to speed up execution. 4. **Validate and Test**: Ensure that the optimized Cython version provides the same output as your original Python implementation. Use unit tests to validate correctness. 5. **Memory Profiling**: Use `memory_profiler` to profile memory usage of your implementation and identify any potential improvements. # Requirements 1. **Function Signature**: ```python def nmf(V, num_components, tol=1e-4, max_iter=200): Parameters: V - A non-negative matrix to factorize (numpy.ndarray) num_components - Number of latent components (int) tol - Tolerance for stopping condition (float, optional, default=1e-4) max_iter - Maximum number of iterations (int, optional, default=200) Returns: W, H - Two non-negative matrices such that V ≈ W @ H ``` 2. **Input Constraints**: - The matrix **V** must have non-negative values. - `num_components` should be a positive integer and less than the number of features of **V**. - `tol` should be a small positive float. - `max_iter` should be a positive integer. 3. **Performance Considerations**: - Avoid loops in the initial Python implementation, using efficient NumPy operations instead. - Profile and optimize the bottleneck functions using Cython. # Example ```python import numpy as np # Example usage V = np.abs(np.random.randn(100, 50)) num_components = 10 W, H = nmf(V, num_components) print(\\"Reconstructed V:\\", np.dot(W, H)) ``` # Evaluation Criteria - **Correctness**: The factorization should be correct, i.e., V ≈ W @ H. - **Performance**: The implementation should show significant performance improvements after optimization. - **Code Quality**: Code should be well-structured, commented, and follow Pythonic conventions. - **Profiling**: Properly documented profiling steps and observations. Good luck!","solution":"import numpy as np def nmf(V, num_components, tol=1e-4, max_iter=200): Perform Non-Negative Matrix Factorization using the multiplicative update rule. Parameters: V (numpy.ndarray): A non-negative matrix to factorize. num_components (int): Number of latent components. tol (float, optional): Tolerance for stopping condition. max_iter (int, optional): Maximum number of iterations. Returns: (W, H): Factor matrices such that V ≈ W @ H # Randomly initialize W and H m, n = V.shape W = np.random.rand(m, num_components) H = np.random.rand(num_components, n) for _ in range(max_iter): # Update H H_update = ((W.T @ V) / (W.T @ W @ H + 1e-9)) H = H * H_update # Update W W_update = ((V @ H.T) / (W @ (H @ H.T) + 1e-9)) W = W * W_update # Calculate approximation error V_approx = W @ H error = np.linalg.norm(V - V_approx, \'fro\') if error < tol: break return W, H"},{"question":"# Question: Implementing and Evaluating Model Performance using Cross-Validation in scikit-learn You are required to create a Python function that performs the following operations using the scikit-learn library: 1. Load the Iris dataset. 2. Split the data into a training set and a test set using the `train_test_split` function. 3. Create a pipeline that standardizes the data and then applies a Support Vector Machine (SVM) classifier with a linear kernel. 4. Use k-fold cross-validation with 5 folds to evaluate the model\'s performance. 5. Calculate and return the cross-validation scores, the mean score, and the standard deviation of the scores. # Constraints: - Use a random state of 42 for reproducibility. - The SVM classifier should have a penalty parameter `C` set to 1. - The cross-validation should use 5 folds. # Expected Input and Output: The function should not take any input parameters directly. Instead, it should use the Iris dataset internally. The output should be a tuple containing: - An array of cross-validation scores. - A float representing the mean cross-validation score. - A float representing the standard deviation of the cross-validation score. # Example: ```python def evaluate_model(): # Your code goes here # Example of expected output # scores, mean_score, std_dev = evaluate_model() # print(scores) # e.g., array([0.96, 0.96, 0.96, 0.96, 0.96]) # print(mean_score) # e.g., 0.96 # print(std_dev) # e.g., 0.0 ``` Implement the `evaluate_model` function as specified.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, cross_val_score from sklearn.svm import SVC from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler import numpy as np def evaluate_model(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the data into a training set and a test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Create a pipeline that standardizes the data and applies an SVM classifier pipeline = make_pipeline(StandardScaler(), SVC(kernel=\'linear\', C=1)) # Use k-fold cross-validation with 5 folds to evaluate the model\'s performance cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5) # Calculate mean and standard deviation of cross-validation scores mean_score = np.mean(cv_scores) std_dev = np.std(cv_scores) return cv_scores, mean_score, std_dev"},{"question":"**Title: Implementing and Managing Async Tasks** **Objective:** Demonstrate your understanding of Python\'s coroutine objects and asynchronous programming using the `async` and `await` keywords. **Problem Statement:** You are required to implement a mini task manager using Python\'s asynchronous programming features. Specifically, you will create a set of coroutine functions that simulate different tasks and a main function to manage and execute these tasks concurrently. **Tasks:** 1. Implement the following coroutine functions: - `fetch_data(id: int) -> str`: Simulates fetching data from a remote source. Each function should print the start of the task, then wait for `id` seconds (to simulate a delay), and return a string `Data for id {id}`. - `process_data(data: str) -> str`: Simulates processing the fetched data. This function should print the start of the processing, wait for 2 seconds (to simulate processing), and return a string `Processed {data}`. - `save_data(processed_data: str) -> str`: Simulates saving the processed data. This function should print the start of saving, wait for 1 second (to simulate saving), and return a string `Saved {processed_data}`. 2. Implement the `main` coroutine function which: - Accepts a list of integer IDs. - For each ID, fetches data, processes it, and saves it. - Ensures that all fetch, process, and save operations are run concurrently using the `await` keyword. **Function Signatures:** ```python async def fetch_data(id: int) -> str: # Implementation here async def process_data(data: str) -> str: # Implementation here async def save_data(processed_data: str) -> str: # Implementation here async def main(ids: List[int]) -> None: # Implementation here ``` **Input:** - A list of integer IDs (`ids`). **Output:** - There is no return value. The output should be the printed statements showing the sequence of task execution for different IDs. **Constraints:** - `id` will be a positive integer. - The function should handle different IDs concurrently. **Example:** ```python import asyncio async def main(): ids = [1, 2, 3] await run_tasks(ids) # Output should show the tasks being executed concurrently. ``` **Notes:** - Use the `asyncio` module to manage the concurrent execution of the coroutines. - Ensure that the printed statements clearly show the order of operation for each ID.","solution":"import asyncio from typing import List async def fetch_data(id: int) -> str: print(f\\"Fetching data for id {id}\\") await asyncio.sleep(id) return f\\"Data for id {id}\\" async def process_data(data: str) -> str: print(f\\"Processing {data}\\") await asyncio.sleep(2) return f\\"Processed {data}\\" async def save_data(processed_data: str) -> str: print(f\\"Saving {processed_data}\\") await asyncio.sleep(1) return f\\"Saved {processed_data}\\" async def main(ids: List[int]) -> None: async def run_task(id: int): data = await fetch_data(id) processed_data = await process_data(data) result = await save_data(processed_data) print(result) await asyncio.gather(*(run_task(id) for id in ids))"},{"question":"# Custom Email Encoder Implementation You are provided with a legacy email encoding module that includes several functions for encoding message payloads (content) for transport via email servers. These functions modify the message payload and set the appropriate *Content-Transfer-Encoding* header. However, for specific use cases, additional custom encoding functions may be required. Task Write a function named `encode_rot13(msg)` that will encode the payload of the email message using the ROT13 encoding scheme and then set the *Content-Transfer-Encoding* header to `\\"rot13\\"`. The function should modify the payload in place. Function Signature ```python def encode_rot13(msg: EmailMessage) -> None: pass ``` Input - `msg`: An instance of a `EmailMessage` object. This instance has a payload (`msg.get_payload()`) which is a string, and headers which can be accessed with `msg[\\"Header-Name\\"] = \\"Header Value\\"` syntax. Output Your function should modify the `msg` object directly: - Encode the existing payload using ROT13 encoding. - Set the *Content-Transfer-Encoding* header to `\\"rot13\\"`. # Example ```python from email.message import EmailMessage msg = EmailMessage() msg.set_payload(\\"This is a test email with some text content.\\") encode_rot13(msg) assert msg.get_payload() == \\"Guvf vf n grfg rnfl jvgu fbzr grkg pbagrag.\\" assert msg[\\"Content-Transfer-Encoding\\"] == \\"rot13\\" ``` Constraints - Do not use any external libraries for ROT13 encoding. Implement the encoding directly in your function. - The `msg` object should be modified in place. - ROT13 encoding involves replacing each letter with the 13th letter after it in the alphabet (wrapping around if necessary). # Notes - ROT13 is a simple letter substitution cipher that replaces a letter with the letter 13 letters after it in the alphabet. - Ensure to handle both uppercase and lowercase characters while encoding. - The ROT13 encoding only modifies alphabetic characters; digits, punctuations, and other characters remain unchanged.","solution":"from email.message import EmailMessage def encode_rot13(msg: EmailMessage) -> None: Encodes the payload of the email message using the ROT13 encoding scheme and sets the Content-Transfer-Encoding header to \\"rot13\\". def rot13_char(c): if \'a\' <= c <= \'z\': return chr((ord(c) - ord(\'a\') + 13) % 26 + ord(\'a\')) elif \'A\' <= c <= \'Z\': return chr((ord(c) - ord(\'A\') + 13) % 26 + ord(\'A\')) else: return c payload = msg.get_payload() encoded_payload = \'\'.join(rot13_char(c) for c in payload) msg.set_payload(encoded_payload) msg[\'Content-Transfer-Encoding\'] = \'rot13\'"},{"question":"PyTorch Coding Assessment You are tasked with creating a neural network module from scratch using PyTorch. The goal is to build and train a network that can learn a simple function. This exercise will help you demonstrate your understanding of the fundamental concepts of PyTorch modules, custom modules, and optimization. # Task 1. **Custom Module**: Implement a custom PyTorch module `PolynomialRegression` which will be used to perform polynomial regression. Your module should: - Inherit from `torch.nn.Module`. - Have an `__init__` method that takes `input_features` and `output_features` as inputs. - In the `__init__` method, define a linear layer using `torch.nn.Linear`. - Use an activation function of your choice (e.g., ReLU, Sigmoid) in the `forward` method. - Implement a `forward` method that computes the forward pass. 2. **Training Loop**: Write a training loop to train your polynomial regression model to fit a given dataset. The dataset is generated by a polynomial function with added noise. # Specifications 1. **PolynomialRegression** ```python import torch from torch import nn class PolynomialRegression(nn.Module): def __init__(self, input_features, output_features): super(PolynomialRegression, self).__init__() self.linear = nn.Linear(input_features, output_features) self.activation = nn.ReLU() # You can choose any activation function def forward(self, x): x = self.linear(x) x = self.activation(x) return x ``` 2. **Training and Evaluation Procedure** - Generate a dataset: ```python torch.manual_seed(0) X = torch.linspace(-1, 1, 100).view(-1, 1) Y = X ** 3 + 0.1 * torch.randn(X.size()) ``` - Initialize your model, loss function, and optimizer: ```python model = PolynomialRegression(input_features=1, output_features=1) criterion = nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) ``` - Implement the training loop to train the model for a specified number of epochs: ```python num_epochs = 1000 for epoch in range(num_epochs): optimizer.zero_grad() outputs = model(X) loss = criterion(outputs, Y) loss.backward() optimizer.step() if (epoch+1) % 100 == 0: print(f\'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\') ``` 3. **Saving and Loading the Model** - Save the trained model: ```python torch.save(model.state_dict(), \'polynomial_regression.pth\') ``` - Load the model for future inference: ```python loaded_model = PolynomialRegression(input_features=1, output_features=1) loaded_model.load_state_dict(torch.load(\'polynomial_regression.pth\')) ``` # Deliverables 1. The implementation of the `PolynomialRegression` module. 2. The training loop used to train the model. 3. Code snippets for saving and loading the trained model. # Evaluation Criteria - Correct implementation of the custom module. - Successful training of the model, demonstrated by decreasing loss values. - Proper saving and loading of the model state. # Constraints and Performance Requirements - The model should be trained on a CPU. - The training should not take more than 5 minutes. - Ensure that your solution is efficient and follows best practices in PyTorch. # Notes - Please ensure your code is well-documented and easy to follow. - You can use any activation function you prefer in the `forward` method. Good luck and happy coding!","solution":"import torch from torch import nn class PolynomialRegression(nn.Module): def __init__(self, input_features, output_features): super(PolynomialRegression, self).__init__() self.linear = nn.Linear(input_features, output_features) self.activation = nn.ReLU() # You can choose any activation function def forward(self, x): x = self.linear(x) x = self.activation(x) return x"},{"question":"Problem Statement You are tasked with creating a utility function to decode a given Python version integer (PY_VERSION_HEX) and return the human-readable version string. Write a function `decode_python_version(version_hex: int) -> str` that takes an integer `version_hex` representing the encoded Python version (as per CPython\'s versioning system) and returns a string representing that version in the format \\"major.minor.micro(level)(serial)\\", where: - `major`, `minor`, and `micro` are the major, minor, and micro version numbers, respectively. - `level` is a single letter representing the release level (`\'a\'` for alpha, `\'b\'` for beta, `\'c\'` for release candidate, and `\'f\'` for final). - `serial` is an integer representing the release serial number. # Input - An integer `version_hex` representing the encoded Python version. # Output - A string representing the human-readable version. # Example ```python decode_python_version(0x030401a2) -> \\"3.4.1a2\\" decode_python_version(0x030a00f0) -> \\"3.10.0f0\\" ``` # Constraints - The input integer will be a well-formed version number as per CPython\'s encoding system. Ensure your code handles the bitwise operations correctly to extract the appropriate parts of the version. Notes - You may find it helpful to use bitwise operators (`>>`, `&`) to extract specific byte values from the input integer.","solution":"def decode_python_version(version_hex: int) -> str: Decodes a given Python version integer (PY_VERSION_HEX) and returns the human-readable version string. major = (version_hex >> 24) & 0xFF minor = (version_hex >> 16) & 0xFF micro = (version_hex >> 8) & 0xFF level_code = (version_hex >> 4) & 0xF serial = version_hex & 0xF level_map = {10: \'a\', 11: \'b\', 12: \'c\', 15: \'f\'} level = level_map.get(level_code, \'f\') return f\\"{major}.{minor}.{micro}{level}{serial}\\""},{"question":"Objective Your task is to use the `tracemalloc` module to analyze the memory usage of a Python script. You will need to write a function that runs the script, takes memory snapshots, and outputs a detailed report of the memory usage differences. Problem Statement Given a Python script (as a string), write a function `analyze_memory_usage` which: 1. Runs the script and takes two memory snapshots: one before and one after the execution of a specific memory-intensive function within the script. 2. Compares the two snapshots and generates a report of the top 5 differences in memory usage. 3. Outputs the report showing the file name, line number, size difference, and count difference for each of the top 5 entries. Function Signature ```python def analyze_memory_usage(script: str, func_name: str) -> None: pass ``` Input - `script` (str): The Python script to be analyzed. - `func_name` (str): The name of the function within the script whose memory usage needs to be analyzed. Output - The function should print a report of the top 5 differences in memory usage. The report should include: - File name - Line number - Size difference - Count difference Instructions 1. Use the `exec` function to run the provided script within the `analyze_memory_usage` function. 2. Use the `tracemalloc` module to take memory snapshots before and after the execution of the specified function in the script. 3. Use the `compare_to` method to compare the two snapshots and identify the top 5 differences in memory usage. 4. Generate a well-formatted report displaying the differences. Example ```python script = import time def memory_intensive_function(): large_list = [i for i in range(1000000)] time.sleep(1) memory_intensive_function() analyze_memory_usage(script, \'memory_intensive_function\') # Output: # [ Top 5 differences ] # 1: <filename>:<line_no>: size_diff=... B, count_diff=... # 2: <filename>:<line_no>: size_diff=... B, count_diff=... # 3: <filename>:<line_no>: size_diff=... B, count_diff=... # 4: <filename>:<line_no>: size_diff=... B, count_diff=... # 5: <filename>:<line_no>: size_diff=... B, count_diff=... ``` Constraints - You can assume that the script provided is a valid Python script. - The specified function name exists within the script. Notes - Make sure to handle any exceptions that may occur during the execution of the script. - Use appropriate filters to focus on relevant memory usage data and exclude noise from irrelevant sources.","solution":"import tracemalloc def analyze_memory_usage(script: str, func_name: str) -> None: Runs the given script, takes memory snapshots, and outputs a detailed report of the memory usage differences before and after the execution of the specified function. :param script: The Python script to be analyzed, as a string. :param func_name: The name of the memory-intensive function to analyze within the script. # Define a function that wraps the script execution global_namespace = {} exec(script, global_namespace) # Ensure the function to be analyzed exists in the script func = global_namespace.get(func_name) if func is None: raise ValueError(f\\"Function \'{func_name}\' not found in the script.\\") tracemalloc.start() # Take a snapshot before the function execution snapshot_before = tracemalloc.take_snapshot() # Execute the function func() # Take a snapshot after the function execution snapshot_after = tracemalloc.take_snapshot() tracemalloc.stop() # Compare the snapshots top_stats = snapshot_after.compare_to(snapshot_before, \'lineno\') print(\\"[ Top 5 differences ]\\") for i, stat in enumerate(top_stats[:5], 1): print(f\\"{i}: {stat.traceback[0]}\\") print(f\\" Size difference: {stat.size_diff} B\\") print(f\\" Count difference: {stat.count_diff}\\")"},{"question":"**PyTorch Memory Management and RNN Training** You are tasked with implementing and training a simple Recurrent Neural Network (RNN) using PyTorch. The objectives include understanding memory management practices, specifically avoiding common pitfalls that can lead to out-of-memory errors. Your task is to follow the steps outlined: # Part 1: Define the Model 1. **Define an RNN model**: Create an RNN class inheriting from `torch.nn.Module`. Your RNN will include: - An embedding layer. - An RNN layer (use `torch.nn.RNN` or `torch.nn.LSTM`). - A fully connected (linear) layer to output predictions. 2. **Initialize the model**: Include appropriate initializations for the layers. # Part 2: Data Preparation 3. **Input generation**: Prepare a function to generate input sequences (`torch.Tensor`) and target outputs for training. Ensure sequences are of varying lengths. # Part 3: Training Loop 4. **Implement the training loop**: Write a training loop that does the following: - Zeroes the gradients. - Passes the input through the model. - Computes the loss. - Performs backpropagation and gradient descent steps. 5. **Avoid memory pitfalls**: - Detach tensors where needed to prevent accumulating gradients. - Free intermediate tensors no longer needed. # Part 4: Managing Out-of-Memory (OOM) Scenarios 6. **OOM Recovery**: Implement a mechanism to handle out-of-memory errors during training by reducing the batch size and attempting the operation again. Include a mechanism to free GPU memory explicitly. # Requirements: - **Input**: A list of sequences of varying lengths and corresponding target labels. - **Output**: Model predictions and final model parameters. - **Constraints**: - Maximum sequence length: 100. - Initial batch size: 32. - Use GPU for training (ensure your code checks for GPU availability and appropriately handles fallbacks). - Avoid holding onto tensors longer than necessary. # Example: Here is a simplified example to get you started: ```python import torch import torch.nn as nn import torch.optim as optim class SimpleRNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleRNN, self).__init__() self.embedding = nn.Embedding(input_size, hidden_size) self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True) self.fc = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.embedding(x) x, _ = self.rnn(x) x = self.fc(x[:, -1, :]) return x def generate_data(num_sequences, max_length, vocab_size): # Generate random sequences with varying lengths sequences = [] labels = [] for _ in range(num_sequences): length = torch.randint(1, max_length, (1,)).item() sequence = torch.randint(vocab_size, (length,)) label = torch.randint(vocab_size, (1,)) sequences.append(sequence) labels.append(label) return sequences, labels # Add the training loop and memory management part here ``` Complete the model definition, input generation, training loop, and memory management code.","solution":"import torch import torch.nn as nn import torch.optim as optim class SimpleRNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleRNN, self).__init__() self.embedding = nn.Embedding(input_size, hidden_size) self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True) self.fc = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.embedding(x) x, _ = self.rnn(x) x = self.fc(x[:, -1, :]) return x def generate_data(num_sequences, max_length, vocab_size): sequences = [] labels = [] for _ in range(num_sequences): length = torch.randint(1, max_length, (1,)).item() sequence = torch.randint(vocab_size, (length,)) label = torch.randint(vocab_size, (1,)) sequences.append(sequence) labels.append(label) return sequences, labels def pad_sequences(sequences, padding_value=0): Pads sequences to the maximum length in the batch batch_size = len(sequences) max_length = max(len(seq) for seq in sequences) padded_sequences = torch.full((batch_size, max_length), padding_value, dtype=torch.long) for i, seq in enumerate(sequences): padded_sequences[i, :len(seq)] = seq return padded_sequences def main(num_epochs=10, batch_size=32, vocab_size=100, hidden_size=128, output_size=10): device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model = SimpleRNN(vocab_size, hidden_size, output_size).to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters()) sequences, labels = generate_data(1000, 100, vocab_size) def get_batches(sequences, labels, batch_size): for i in range(0, len(sequences), batch_size): batch_sequences = sequences[i:i+batch_size] batch_labels = torch.tensor(labels[i:i+batch_size], dtype=torch.long) yield pad_sequences(batch_sequences).to(device), batch_labels.to(device) for epoch in range(num_epochs): model.train() total_loss = 0 for batch_sequences, batch_labels in get_batches(sequences, labels, batch_size): optimizer.zero_grad() try: outputs = model(batch_sequences) loss = criterion(outputs, batch_labels) loss.backward() optimizer.step() total_loss += loss.item() except RuntimeError as e: if \'out of memory\' in str(e): print(f\\"Out of memory error occurred at epoch {epoch}, reducing batch size\\") for param in model.parameters(): if param.grad is not None: del param.grad # Explicitly delete memory torch.cuda.empty_cache() else: raise e print(f\\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(sequences)}\\") return model # Optionally, you can invoke the main function here if you want to execute the training. # main()"},{"question":"# Task Overview: Write a Python function called `sync_and_archive_directory(src_dir, dst_dir, archive_name, ignore_patterns_list=None)` that: 1. Synchronizes the contents of the source directory `src_dir` with the destination directory `dst_dir`. 2. Archives the destination directory `dst_dir` into an archive file named `archive_name`. 3. Allows ignoring certain files and directories based on `ignore_patterns_list`. # Function Requirements: 1. **Synchronization Process:** - Recursively copy all content from `src_dir` to `dst_dir`, ensuring that existing files in `dst_dir` with the same name are overwritten. - Preserve file metadata (permissions, modification times) during the copying process. - Handle symbolic links properly, respecting the actual file or directory they reference. - Use `ignore_patterns_list` to specify any glob-style patterns for files/directories to be ignored during copying. If `ignore_patterns_list` is not provided, no files should be ignored. 2. **Archiving Process:** - Archive the entire `dst_dir` directory structure into an archive file named `archive_name` using the `\'gztar\'` format. # Input: - `src_dir` (str): Path to the source directory to copy files from. - `dst_dir` (str): Path to the destination directory to copy files to. - `archive_name` (str): Name of the archive file to create (without the extension). - `ignore_patterns_list` (list of str, optional): List of glob-style patterns specifying files/directories to ignore. # Output: - The function should not return anything but should perform the specified operations (synchronization and archiving). # Constraints: - You should ensure the function handles symbolic links appropriately and maintains file metadata. - The function should handle any potential errors gracefully (e.g., if the source directory does not exist). # Example: ```python def sync_and_archive_directory(src_dir, dst_dir, archive_name, ignore_patterns_list=None): # Your implementation here # Example usage: sync_and_archive_directory(\'/path/to/source\', \'/path/to/destination\', \'backup\', ignore_patterns_list=[\'*.pyc\', \'tmp*\']) ``` # Notes: - Use functionality from the `shutil` module as much as possible. - Consider platform-specific behavior, particularly for handling symbolic links and metadata. - Ensure proper error handling and resource management (e.g., by using context managers where appropriate).","solution":"import shutil import os from fnmatch import fnmatch def sync_and_archive_directory(src_dir, dst_dir, archive_name, ignore_patterns_list=None): if not os.path.exists(src_dir): raise ValueError(f\\"Source directory {src_dir} does not exist.\\") if not os.path.exists(dst_dir): os.makedirs(dst_dir, exist_ok=True) def ignore_patterns(path, names): if ignore_patterns_list is None: return [] ignored = [] for pattern in ignore_patterns_list: ignored.extend([name for name in names if fnmatch(name, pattern)]) return set(ignored) # Synchronize source to destination for root, dirs, files in os.walk(src_dir, topdown=True): rel_path = os.path.relpath(root, src_dir) dest_path = os.path.join(dst_dir, rel_path) if not os.path.exists(dest_path): os.makedirs(dest_path, exist_ok=True) for name in files: if not ignore_patterns_list or not any(fnmatch(name, pattern) for pattern in ignore_patterns_list): src_file = os.path.join(root, name) dst_file = os.path.join(dest_path, name) shutil.copy2(src_file, dst_file) # Archive the destination directory shutil.make_archive(archive_name, \'gztar\', dst_dir)"},{"question":"# Advanced Coding Assessment Question: Using `ChainMap` for Configuration Management You are tasked with creating a simplified configuration management system using Python\'s `ChainMap` from the `collections` module. This system will allow prioritization of settings from different sources: command-line arguments, environment variables, and default settings. Requirements 1. **Function Signature**: ```python def create_config(cmd_args: dict, env_vars: dict, defaults: dict) -> collections.ChainMap: ``` 2. **Input**: - `cmd_args` (dict): Command-line arguments with user-specified settings. - `env_vars` (dict): Environment variables that may override default settings. - `defaults` (dict): Default configuration values. 3. **Output**: - Returns a `collections.ChainMap` object that combines the dictionaries in order of precedence (command-line arguments first, followed by environment variables, and then defaults). 4. **Constraints**: - Ensure that the `ChainMap` lookup order prioritizes command-line arguments over environment variables, with the default settings being the last. - Write tests to validate your implementation with at least three different scenarios demonstrating the configuration management system. Tasks 1. Implement the `create_config` function as described above. 2. Write a function `get_config_value` to fetch configuration values from the combined configuration with the following signature: ```python def get_config_value(config: collections.ChainMap, key: str) -> Any: ``` - This function should return the value associated with the provided `key`, and raise a `KeyError` if the `key` does not exist. 3. Demonstrate your function with the following scenarios: - **Scenario 1**: Command-line arguments override values in environment variables and default settings. - **Scenario 2**: Environment variables override default settings when command-line arguments are not provided. - **Scenario 3**: Handling missing keys and raising appropriate errors. ```python import collections def create_config(cmd_args: dict, env_vars: dict, defaults: dict) -> collections.ChainMap: Create a ChainMap configuration with the precedence of command-line arguments, environment variables, and default settings. return collections.ChainMap(cmd_args, env_vars, defaults) def get_config_value(config: collections.ChainMap, key: str) -> Any: Fetch the configuration value associated with the provided key. Raises KeyError if the key does not exist in the configuration. return config[key] # Example Scenarios # Scenario 1: Command-line arguments override values in environment variables and default settings. cmd_args_1 = {\'user\': \'alice\', \'color\': \'blue\'} env_vars_1 = {\'color\': \'green\', \'timeout\': \'30\'} defaults_1 = {\'color\': \'red\', \'timeout\': \'60\', \'user\': \'guest\'} config_1 = create_config(cmd_args_1, env_vars_1, defaults_1) assert get_config_value(config_1, \'user\') == \'alice\' assert get_config_value(config_1, \'color\') == \'blue\' assert get_config_value(config_1, \'timeout\') == \'30\' # Scenario 2: Environment variables override default settings when command-line arguments are not provided. cmd_args_2 = {} env_vars_2 = {\'color\': \'green\', \'timeout\': \'30\'} defaults_2 = {\'color\': \'red\', \'timeout\': \'60\', \'user\': \'guest\'} config_2 = create_config(cmd_args_2, env_vars_2, defaults_2) assert get_config_value(config_2, \'user\') == \'guest\' assert get_config_value(config_2, \'color\') == \'green\' assert get_config_value(config_2, \'timeout\') == \'30\' # Scenario 3: Handling missing keys and raising appropriate errors. try: get_config_value(config_1, \'nonexistent_key\') except KeyError: print(\\"KeyError raised as expected\\") ```","solution":"import collections def create_config(cmd_args: dict, env_vars: dict, defaults: dict) -> collections.ChainMap: Create a ChainMap configuration with the precedence of command-line arguments, environment variables, and default settings. return collections.ChainMap(cmd_args, env_vars, defaults) def get_config_value(config: collections.ChainMap, key: str) -> any: Fetch the configuration value associated with the provided key. Raises KeyError if the key does not exist in the configuration. return config[key]"},{"question":"# Class and Inheritance: Building an Interactive Library System You are tasked with implementing a simple library system in Python. This system should be able to manage books and patrons. Each book can be borrowed by only one patron at a time, and patrons can borrow multiple books. You need to use classes, inheritance, and instance/class variables to solve this problem. Specifications: 1. **Book Class**: - **Attributes**: - `title` (string): The title of the book. - `author` (string): The author of the book. - `is_borrowed` (boolean): A flag indicating whether the book is currently borrowed or not. - **Methods**: - `__init__(self, title, author)`: Constructor to initialize the title and author. `is_borrowed` should be initially set to `False`. - `borrow(self)`: Sets `is_borrowed` to `True`. - `return_book(self)`: Sets `is_borrowed` to `False`. 2. **Patron Class**: - **Attributes**: - `name` (string): The name of the patron. - `borrowed_books` (list): A list to hold the books that the patron has borrowed. - **Methods**: - `__init__(self, name)`: Constructor to initialize the name and set `borrowed_books` to an empty list. - `borrow_book(self, book)`: Adds a book to `borrowed_books` and sets the book\'s `is_borrowed` status to `True`. Ensure a book can\'t be borrowed if it\'s already borrowed. - `return_book(self, book)`: Removes a book from `borrowed_books` and sets the book’s `is_borrowed` status to `False`. Ensure a book can’t be returned if it wasn\'t borrowed by the patron. 3. **Library Class**: - **Attributes**: - `books` (list): A list to hold all the books in the library. - `patrons` (list): A list to hold all the patrons of the library. - **Methods**: - `__init__(self)`: Constructor to initialize the `books` and `patrons` lists to empty lists. - `add_book(self, book)`: Adds a new book to the `books` list. - `add_patron(self, patron)`: Adds a new patron to the `patrons` list. - `get_books(self)`: Returns the list of books. - `get_patrons(self)`: Returns the list of patrons. Constraints: - A book can only be borrowed by one patron at a time. - A patron can borrow multiple books but cannot borrow the same book twice. - Ensure that books and patrons are properly tracked in their respective lists. Example Usage: ```python lib = Library() book1 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\") book2 = Book(\\"1984\\", \\"George Orwell\\") patron1 = Patron(\\"John Doe\\") patron2 = Patron(\\"Jane Smith\\") lib.add_book(book1) lib.add_book(book2) lib.add_patron(patron1) lib.add_patron(patron2) patron1.borrow_book(book1) patron2.borrow_book(book2) print(patron1.borrowed_books) # Should print [(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\")] print(book1.is_borrowed) # Should print True patron1.return_book(book1) print(book1.is_borrowed) # Should print False ``` Implementation: Implement the `Book`, `Patron`, and `Library` classes and ensure they adhere to the specified requirements. Write the main function to demonstrate the usage of the classes and verify the behavior with sample data. ```python # Your implementation code here class Book: def __init__(self, title, author): self.title = title self.author = author self.is_borrowed = False def borrow(self): if not self.is_borrowed: self.is_borrowed = True return True return False def return_book(self): if self.is_borrowed: self.is_borrowed = False return True return False class Patron: def __init__(self, name): self.name = name self.borrowed_books = [] def borrow_book(self, book): if book.borrow() and book not in self.borrowed_books: self.borrowed_books.append(book) return True return False def return_book(self, book): if book in self.borrowed_books and book.return_book(): self.borrowed_books.remove(book) class Library: def __init__(self): self.books = [] self.patrons = [] def add_book(self, book): self.books.append(book) def add_patron(self, patron): self.patrons.append(patron) def get_books(self): return self.books def get_patrons(self): return self.patrons ```","solution":"class Book: def __init__(self, title, author): self.title = title self.author = author self.is_borrowed = False def borrow(self): if not self.is_borrowed: self.is_borrowed = True return True return False def return_book(self): if self.is_borrowed: self.is_borrowed = False return True return False class Patron: def __init__(self, name): self.name = name self.borrowed_books = [] def borrow_book(self, book): if not book.is_borrowed: book.borrow() self.borrowed_books.append(book) return True return False def return_book(self, book): if book in self.borrowed_books: book.return_book() self.borrowed_books.remove(book) class Library: def __init__(self): self.books = [] self.patrons = [] def add_book(self, book): self.books.append(book) def add_patron(self, patron): self.patrons.append(patron) def get_books(self): return self.books def get_patrons(self): return self.patrons"},{"question":"**Python File Management and Archiving Task** As part of your project, you need to implement a function that both backs up a specified directory and provides an option to restore it. This task involves creating an archive of the directory and, if needed, extracting it back to its original location. # Function Requirements: You are to write two Python functions: 1. `backup_directory(source, archive_name, format)` 2. `restore_directory(archive_name, extract_dir)` Function 1: `backup_directory(source, archive_name, format)` - **Input**: - `source` (str): The path to the directory to be archived. - `archive_name` (str): The name of the archive file to be created (without format-specific extension). - `format` (str): The archive format. This should be one of (\\"zip\\", \\"tar\\", \\"gztar\\", \\"bztar\\", \\"xztar\\"). - **Output**: - Returns the path to the created archive file. - **Constraints**: - Raise an error if the `source` is not a directory. - The archive should be created in the same directory as the `source`. - **Example**: ```python backup_directory(\'/path/to/source\', \'backup_archive\', \'zip\') # Returns: /path/to/source/backup_archive.zip ``` Function 2: `restore_directory(archive_name, extract_dir)` - **Input**: - `archive_name` (str): The name of the archive file to be restored. - `extract_dir` (str): The directory where the archive should be extracted. - **Output**: - Returns a list of files and directories restored. - **Constraints**: - Raise an error if the `archive_name` does not exist. - The `extract_dir` should be empty before the extraction or the function should handle overwriting existing files. - **Example**: ```python restore_directory(\'/path/to/source/backup_archive.zip\', \'/path/to/restore\') # Returns: [\'/path/to/restore/file1\', \'/path/to/restore/dir1\', ...] ``` # Key Considerations: - Ensure that file paths and names are constructed safely to avoid any possible issues with different operating systems. - Handle exceptions gracefully, providing useful error messages for invalid input and unexpected conditions. - Consider performance implications, especially for large directories. # Submission: Submit the implementation of the above two functions along with a script demonstrating their usage. Ensure proper docstrings and comments are included for clear understanding. The script should demonstrate: - Creating an archive of a directory. - Restoring the directory from the created archive. - Handling error cases such as invalid paths or formats.","solution":"import os import shutil import tarfile import zipfile from pathlib import Path def backup_directory(source, archive_name, format): Creates an archive of the specified directory. Args: source (str): The path to the directory to be archived. archive_name (str): The name of the archive file to be created (without format-specific extension). format (str): The archive format. This should be one of (\\"zip\\", \\"tar\\", \\"gztar\\", \\"bztar\\", \\"xztar\\"). Returns: str: The path to the created archive file. source_path = Path(source) if not source_path.is_dir(): raise NotADirectoryError(f\\"{source} is not a directory\\") archive_path = source_path / f\\"{archive_name}.{format}\\" shutil.make_archive(str(source_path / archive_name), format, root_dir=source) return archive_path def restore_directory(archive_name, extract_dir): Restores the archived directory. Args: archive_name (str): The name of the archive file to be restored. extract_dir (str): The directory where the archive should be extracted. Returns: list of str: List of files and directories restored. archive_path = Path(archive_name) extract_path = Path(extract_dir) if not archive_path.exists(): raise FileNotFoundError(f\\"{archive_name} does not exist\\") if not extract_path.exists(): extract_path.mkdir(parents=True) restored_items = [] if archive_path.suffix == \'.zip\': with zipfile.ZipFile(archive_path, \'r\') as zip_ref: zip_ref.extractall(extract_path) restored_items = zip_ref.namelist() else: with tarfile.open(archive_path, \'r:*\') as tar_ref: tar_ref.extractall(extract_path) restored_items = tar_ref.getnames() return [str(extract_path / item) for item in restored_items]"},{"question":"# Extended Numeric Operations for Custom Objects This assessment tests your ability to create Python classes that leverage Python\'s operator overloading capabilities and numeric protocols, similar to those provided by the Python C API. Problem Statement: Create a Python class `ExtendedNumber` which supports the following operations just like native numeric types in Python. Ensure that the class supports operator overloading, raising appropriate exceptions for invalid operations. Class `ExtendedNumber`: **Attributes:** - `value` (can be an int or float) **Methods and Operator Overloadings to Implement:** 1. **Arithmetic Operations**: - `__add__` for addition (`+`) - `__sub__` for subtraction (`-`) - `__mul__` for multiplication (`*`) - `__matmul__` for matrix multiplication (`@`) - `__floordiv__` for floor division (`//`) - `__truediv__` for true division (`/`) - `__mod__` for modulus (`%`) - `__divmod__` for divmod built-in function - `__pow__` for power (`**`) 2. **Unary Operations**: - `__neg__` for negation (`-obj`) - `__pos__` for unary positive (`+obj`) - `__abs__` for absolute value (`abs(obj)`) - `__invert__` for bitwise NOT (`~obj`) 3. **Bitwise Operations** (only if `value` is an integer): - `__lshift__` for left shift (`<<`) - `__rshift__` for right shift (`>>`) - `__and__` for bitwise AND (`&`) - `__xor__` for bitwise XOR (`^`) - `__or__` for bitwise OR (`|`) 4. **Conversions**: - `to_long` to convert to integer - `to_float` to convert to float Constraints: 1. Raise a `TypeError` if an unsupported operation is attempted (e.g., bitwise operations on non-integer values). 2. Ensure that all overloaded methods return a new `ExtendedNumber` instance. Input/Output: - The constructor should accept an initial value (`int` or `float`). - Arithmetic and bitwise operations between two `ExtendedNumber` instances should work similarly to their numeric counterparts. - The unary operations should work on a single `ExtendedNumber` instance. - Conversion methods should return the value converted to their respective types. Example Usage: ```python # Arithmetic Operations a = ExtendedNumber(10) b = ExtendedNumber(20) print((a + b).value) # 30 print((a * b).value) # 200 print((a ** b).value) # 100000000000000000000 # Unary Operations print((-a).value) # -10 print((+a).value) # 10 print(abs(-a).value) # 10 # Bitwise Operations c = ExtendedNumber(5) print((c << 2).value) # 20 # Conversion print(a.to_long()) # 10 print(a.to_float()) # 10.0 ``` > Note: Do not use any libraries beyond Python\'s standard library. This problem evaluates your understanding of operator overloading, exception handling, and type conversion in Python. Make sure to follow Python\'s naming conventions and best practices.","solution":"class ExtendedNumber: def __init__(self, value): if not isinstance(value, (int, float)): raise TypeError(\\"Value must be an int or float\\") self.value = value def __add__(self, other): return ExtendedNumber(self.value + other.value) def __sub__(self, other): return ExtendedNumber(self.value - other.value) def __mul__(self, other): return ExtendedNumber(self.value * other.value) def __matmul__(self, other): raise NotImplementedError(\\"Matrix multiplication is not supported for scalar values\\") def __floordiv__(self, other): return ExtendedNumber(self.value // other.value) def __truediv__(self, other): return ExtendedNumber(self.value / other.value) def __mod__(self, other): return ExtendedNumber(self.value % other.value) def __divmod__(self, other): quotient, remainder = divmod(self.value, other.value) return ExtendedNumber(quotient), ExtendedNumber(remainder) def __pow__(self, power): return ExtendedNumber(self.value ** power.value) def __neg__(self): return ExtendedNumber(-self.value) def __pos__(self): return ExtendedNumber(+self.value) def __abs__(self): return ExtendedNumber(abs(self.value)) def __invert__(self): if not isinstance(self.value, int): raise TypeError(\\"Bitwise invert is supported only for integer values\\") return ExtendedNumber(~self.value) def __lshift__(self, other): if not isinstance(self.value, int): raise TypeError(\\"Bitwise shift operations are supported only for integer values\\") return ExtendedNumber(self.value << other.value) def __rshift__(self, other): if not isinstance(self.value, int): raise TypeError(\\"Bitwise shift operations are supported only for integer values\\") return ExtendedNumber(self.value >> other.value) def __and__(self, other): if not isinstance(self.value, int): raise TypeError(\\"Bitwise and operation is supported only for integer values\\") return ExtendedNumber(self.value & other.value) def __xor__(self, other): if not isinstance(self.value, int): raise TypeError(\\"Bitwise xor operation is supported only for integer values\\") return ExtendedNumber(self.value ^ other.value) def __or__(self, other): if not isinstance(self.value, int): raise TypeError(\\"Bitwise or operation is supported only for integer values\\") return ExtendedNumber(self.value | other.value) def to_long(self): return int(self.value) def to_float(self): return float(self.value)"},{"question":"**MIME-Handing Script Using Mailcap Module** You are tasked to develop a Python script that utilizes the deprecated `mailcap` module to parse mailcap files and execute appropriate commands for a list of given files based on their MIME types. # Objective: Implement a function `execute_mime_commands(file_list, mime_types)` that performs the following: 1. Parses the mailcap files on the system using `mailcap.getcaps()`. 2. For each file in `file_list`: - Determine its MIME type (using the corresponding entry from `mime_types`). - Find the appropriate command line using `mailcap.findmatch()` to handle (view) the file based on its MIME type. - Execute the command using `os.system()` (note: for safety, this function should **not** be executed directly). # Function Signature: ```python def execute_mime_commands(file_list: list, mime_types: dict) -> list: pass ``` # Parameters: - `file_list`: A list of file paths (all strings) that need to be handled. - `mime_types`: A dictionary mapping file paths (as strings) to their corresponding MIME types (as strings). # Expected Output: - The function should return a list of tuples. Each tuple contains: - The file path - The executed command If no matching command is found for a file, include `None` as the command in the tuple. # Constraints: - Do not actually execute the commands in your function (i.e., you can mock `os.system()` or just include the command in output). - Ensure security by handling shell metacharacters properly, as mentioned in the documentation. # Example: ```python file_list = [\'/path/to/video1.mpeg\', \'/path/to/image2.jpeg\'] mime_types = {\'/path/to/video1.mpeg\': \'video/mpeg\', \'/path/to/image2.jpeg\': \'image/jpeg\'} results = execute_mime_commands(file_list, mime_types) ``` Possible `results` could be: ``` [(\'/path/to/video1.mpeg\', \'xmpeg /path/to/video1.mpeg\'), (\'/path/to/image2.jpeg\', None)] ``` # Notes: - Utilize the functions `mailcap.getcaps()` and `mailcap.findmatch()` as described. - Make sure your solution provides clear and meaningful error handling. - Pay attention to the security constraints described in the `findmatch` documentation, particularly regarding shell metacharacters. # Testing: Your solution should work effectively with both common and uncommon MIME types and handle cases where no appropriate mailcap entries are found.","solution":"import mailcap import os def execute_mime_commands(file_list, mime_types): Parses the mailcap files on the system and finds the appropriate command to handle each file based on its MIME type. Parameters: - file_list: A list of file paths that need to be handled. - mime_types: A dictionary mapping file paths to their corresponding MIME types. Returns: - A list of tuples containing the file path and the executed command. caps = mailcap.getcaps() results = [] for file_path in file_list: mime_type = mime_types.get(file_path) if not mime_type: results.append((file_path, None)) continue command, _ = mailcap.findmatch(caps, mime_type) if command: # Note: os.system() should not actually be executed for safety reasons. # We will mock this in the tests. command_to_execute = command.replace(\'%s\', file_path) results.append((file_path, command_to_execute)) else: results.append((file_path, None)) return results"},{"question":"# Python Coding Assessment: Advanced Binary-ASCII Conversions Problem Statement You are tasked with implementing a function that: 1. Converts a given binary data input into various ASCII encoded formats. 2. Decodes the data back to its original binary format. 3. Ensures data integrity by comparing CRC-32 checksums before and after the transformations. Implement the following function: ```python import binascii def transform_and_verify(data: bytes) -> dict: This function takes a binary data input, transforms it through various ASCII encodings using the binascii module, decodes them back to binary, and then verifies data integrity by comparing CRC-32 checksums. Args: - data (bytes): Original binary data to be transformed. Returns: - result (dict): A dictionary containing: * \'original_crc32\': CRC-32 checksum of the original data. * \'uuencode\': Decoded binary data after uuencoding and decoding. * \'base64\': Decoded binary data after base64 encoding and decoding. * \'qp\': Decoded binary data after quoted-printable encoding and decoding. * \'hex\': Decoded binary data after hex encoding and decoding. * \'valid\': Boolean indicating if all decoded results match the original data. pass ``` Details: 1. **Input:** - A bytes object `data` representing the binary data. 2. **Output:** - A dictionary with: * `\'original_crc32\'`: CRC-32 checksum of the original binary data. * `\'uuencode\'`: Decoded binary data after uuencoding and decoding. * `\'base64\'`: Decoded binary data after base64 encoding and decoding. * `\'qp\'`: Decoded binary data after quoted-printable encoding and decoding. * `\'hex\'`: Decoded binary data after hex encoding and decoding. * `\'valid\'`: A boolean indicating whether all decoded data match the original binary data. 3. **Constraints:** - Data length should be appropriate for all transformations (e.g., less than 45 bytes for uuencoding). 4. **Performance**: - The function should run efficiently on typical binary data sizes used in transformations. Example Usage: ```python data = b\'This is test data.\' result = transform_and_verify(data) print(result[\'original_crc32\']) # Integer CRC-32 checksum of the original data. print(result[\'uuencode\'] == data) # True if the transformed and decoded data matches the original. print(result[\'base64\'] == data) # Same check for base64 encoding. print(result[\'qp\'] == data) # Same check for quoted-printable encoding. print(result[\'hex\'] == data) # Same check for hex encoding. print(result[\'valid\']) # True if all checks pass. ``` Ensure you use the `binascii` module for all transformations and checksum calculations.","solution":"import binascii def transform_and_verify(data: bytes) -> dict: This function takes a binary data input, transforms it through various ASCII encodings using the binascii module, decodes them back to binary, and then verifies data integrity by comparing CRC-32 checksums. Args: - data (bytes): Original binary data to be transformed. Returns: - result (dict): A dictionary containing: * \'original_crc32\': CRC-32 checksum of the original data. * \'uuencode\': Decoded binary data after uuencoding and decoding. * \'base64\': Decoded binary data after base64 encoding and decoding. * \'qp\': Decoded binary data after quoted-printable encoding and decoding. * \'hex\': Decoded binary data after hex encoding and decoding. * \'valid\': Boolean indicating if all decoded results match the original data. original_crc32 = binascii.crc32(data) # uuencode and decode uuencode_encoded = binascii.b2a_uu(data) uuencode_decoded = binascii.a2b_uu(uuencode_encoded) # base64 encode and decode base64_encoded = binascii.b2a_base64(data) base64_decoded = binascii.a2b_base64(base64_encoded) # quoted-printable encode and decode qp_encoded = binascii.b2a_qp(data) qp_decoded = binascii.a2b_qp(qp_encoded) # hex encode and decode hex_encoded = binascii.b2a_hex(data) hex_decoded = binascii.a2b_hex(hex_encoded) # Verify all transformations match the original data valid = (data == uuencode_decoded == base64_decoded == qp_decoded == hex_decoded) result = { \'original_crc32\': original_crc32, \'uuencode\': uuencode_decoded, \'base64\': base64_decoded, \'qp\': qp_decoded, \'hex\': hex_decoded, \'valid\': valid } return result"},{"question":"# Coding Exercise: Probability Calibration of Classifiers Objective: You are given the task of classifying a dataset using different classifiers and assessing their probability calibration. You need to write a function to perform the following steps: 1. Train a `RandomForestClassifier` and a `LogisticRegression` classifier on the given dataset. 2. Calibrate both classifiers using `CalibratedClassifierCV` with both sigmoid and isotonic methods. 3. Evaluate and compare the Brier score losses before and after calibration for both classifiers. 4. Plot calibration curves for both calibrated classifiers. Function Signature: ```python def calibrate_classifiers(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> None: pass ``` Input and Output Formats: - **Input:** - `X_train` (numpy.ndarray): Training feature set. - `y_train` (numpy.ndarray): Training labels. - `X_test` (numpy.ndarray): Testing feature set. - `y_test` (numpy.ndarray): Testing labels. - **Output:** - The function should not return anything. Instead, it should print the Brier score losses and plot calibration curves using matplotlib. Requirements: 1. Train `RandomForestClassifier` and `LogisticRegression` classifiers on the training data. 2. Calibrate both classifiers using `CalibratedClassifierCV` with both \'sigmoid\' and \'isotonic\' methods. 3. Compute and print the Brier score losses before and after calibration on the test data for both classifiers. 4. Plot the calibration curves for the test data showing the calibration of both classifiers before and after calibration. Example: ```python import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # Generate a synthetic binary classification dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Calibrate classifiers and plot calibration curves calibrate_classifiers(X_train, y_train, X_test, y_test) ``` The function should: - Train the classifiers. - Calibrate their predictions. - Print the Brier score losses before and after calibration. - Display calibration curves showing the results. Constraints: - Use `RandomForestClassifier` and `LogisticRegression` from `sklearn.ensemble` and `sklearn.linear_model` respectively. - Use `CalibratedClassifierCV` from `sklearn.calibration`. - Use `brier_score_loss` from `sklearn.metrics`. - Use `CalibrationDisplay` from `sklearn.calibration` for plotting calibration curves. - You can assume that `X_train`, `y_train`, `X_test`, and `y_test` are non-empty and correctly formatted. Notes: - Refer to the scikit-learn documentation on calibration for more details. - Make sure the plots distinguish between different classifiers and calibration methods.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.calibration import CalibratedClassifierCV, calibration_curve from sklearn.metrics import brier_score_loss from sklearn.calibration import CalibrationDisplay def calibrate_classifiers(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> None: # Train RandomForestClassifier and LogisticRegression rf = RandomForestClassifier() lr = LogisticRegression(max_iter=1000) rf.fit(X_train, y_train) lr.fit(X_train, y_train) # Calibrate classifiers with sigmoid method rf_sigmoid = CalibratedClassifierCV(rf, method=\'sigmoid\', cv=\'prefit\') lr_sigmoid = CalibratedClassifierCV(lr, method=\'sigmoid\', cv=\'prefit\') rf_sigmoid.fit(X_train, y_train) lr_sigmoid.fit(X_train, y_train) # Calibrate classifiers with isotonic method rf_isotonic = CalibratedClassifierCV(rf, method=\'isotonic\', cv=\'prefit\') lr_isotonic = CalibratedClassifierCV(lr, method=\'isotonic\', cv=\'prefit\') rf_isotonic.fit(X_train, y_train) lr_isotonic.fit(X_train, y_train) # Calculate Brier score losses before and after calibration classifiers = { \'RandomForest\': rf, \'LogisticRegression\': lr, \'RandomForest_Sigmoid\': rf_sigmoid, \'RandomForest_Isotonic\': rf_isotonic, \'LogisticRegression_Sigmoid\': lr_sigmoid, \'LogisticRegression_Isotonic\': lr_isotonic } for name, clf in classifiers.items(): prob_pos = clf.predict_proba(X_test)[:, 1] brier_loss = brier_score_loss(y_test, prob_pos) print(f\\"Brier score loss ({name}): {brier_loss:.4f}\\") # Plot calibration curves plt.figure(figsize=(10, 10)) for name, clf in classifiers.items(): disp = CalibrationDisplay.from_estimator(clf, X_test, y_test, n_bins=10, strategy=\'uniform\') disp.plot() disp.ax_.set_title(f\'Calibration Curve ({name})\') plt.show()"},{"question":"**Problem Statement:** You are tasked with creating a simple inventory management system using basic Python constructs introduced in the provided documentation. The system will manage a list of items in a store and their respective quantities. Implement the following functions: 1. **add_item(inventory, item, quantity)**: * Adds a specified quantity of an item to the inventory. If the item already exists, increase its quantity accordingly. * Parameters: - `inventory` (list): A list where each element is a list of two elements `[item_name, quantity]`. - `item` (str): The name of the item to be added or updated. - `quantity` (int): The number of units of the item to be added. * Returns: The updated inventory list. 2. **remove_item(inventory, item, quantity)**: * Removes a specified quantity of an item from the inventory. If the quantity to be removed exceeds the available quantity, remove the item completely. If the item does not exist, do nothing. * Parameters: - `inventory` (list): A list where each element is a list of two elements `[item_name, quantity]`. - `item` (str): The name of the item to be removed or updated. - `quantity` (int): The number of units of the item to be removed. * Returns: The updated inventory list. 3. **view_inventory(inventory)**: * Displays the current inventory in a human-readable format. * Parameters: - `inventory` (list): A list where each element is a list of two elements `[item_name, quantity]`. * Returns: None. This function should print the inventory. Constraints: - The inventory list and each `item_name` will only consist of alphanumeric characters and will not be case-sensitive. - The `quantity` will always be a non-negative integer. Example: ```python inventory = [] inventory = add_item(inventory, \\"Apple\\", 10) inventory = add_item(inventory, \\"Banana\\", 5) inventory = add_item(inventory, \\"Apple\\", 5) inventory = remove_item(inventory, \\"Apple\\", 3) inventory = remove_item(inventory, \\"Banana\\", 6) view_inventory(inventory) ``` **Expected Output:** ``` Apple: 12 ``` Notes: 1. The functions should handle cases where items are not found gracefully. 2. The `view_inventory` function should print each item and its quantity on a new line. Use the provided template to implement your solution: ```python def add_item(inventory, item, quantity): # Your code here pass def remove_item(inventory, item, quantity): # Your code here pass def view_inventory(inventory): # Your code here pass # Test your functions with the example provided inventory = [] inventory = add_item(inventory, \\"Apple\\", 10) inventory = add_item(inventory, \\"Banana\\", 5) inventory = add_item(inventory, \\"Apple\\", 5) inventory = remove_item(inventory, \\"Apple\\", 3) inventory = remove_item(inventory, \\"Banana\\", 6) view_inventory(inventory) ```","solution":"def add_item(inventory, item, quantity): Adds a specified quantity of an item to the inventory. for i in range(len(inventory)): if inventory[i][0].lower() == item.lower(): inventory[i][1] += quantity return inventory inventory.append([item, quantity]) return inventory def remove_item(inventory, item, quantity): Removes a specified quantity of an item from the inventory. for i in range(len(inventory)): if inventory[i][0].lower() == item.lower(): if inventory[i][1] > quantity: inventory[i][1] -= quantity else: inventory.pop(i) return inventory return inventory def view_inventory(inventory): Displays the current inventory in a human-readable format. for item, quantity in inventory: print(f\\"{item}: {quantity}\\")"},{"question":"You have been provided with a dataset containing information about various species of flowers along with their physical attributes. Using the Seaborn library, you need to visualize this data effectively to gather insights. The goal is to create multiple plots, each showcasing different aspects of the dataset. # Dataset Description The dataset has the following columns: - `sepal_length`: Length of the sepal - `sepal_width`: Width of the sepal - `petal_length`: Length of the petal - `petal_width`: Width of the petal - `species`: Species of the flower # Tasks 1. **Scatter Plot**: Create a scatter plot to visualize the relationship between sepal length and sepal width, color-coded by species. 2. **Distribution Plot**: Create a distribution plot to examine the distribution of petal lengths for different species. 3. **Box Plot**: Create a box plot to show the distribution of petal widths across the different species. 4. **Custom Context**: Set a custom visualization context for all your plots with the following parameters: - Context: `paper` - Font scale: `1.5` - Line width: `2` # Function Signature ```python def visualize_flowers(data: pd.DataFrame) -> None: pass ``` - **Input**: A pandas DataFrame `data` containing the flower dataset as described above. - **Output**: No return value. The function should display the plots directly. # Constraints - You must use the Seaborn library for all visualizations. - Ensure the plots are well-labeled and have appropriate legends where necessary. # Example Usage ```python import pandas as pd # Sample data creation data = pd.DataFrame({ \'sepal_length\': [5.1, 4.9, 4.7, 4.6, 5.0], \'sepal_width\': [3.5, 3.0, 3.2, 3.1, 3.6], \'petal_length\': [1.4, 1.4, 1.3, 1.5, 1.4], \'petal_width\': [0.2, 0.2, 0.2, 0.2, 0.2], \'species\': [\'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\'] }) # Function call visualize_flowers(data) ``` Make sure the function when called with real data provides meaningful and insightful visualizations, giving clear insights into the relationships between different features of the flowers.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_flowers(data: pd.DataFrame) -> None: Visualizes the flower dataset using Seaborn to create multiple plots. - Scatter plot of sepal length vs sepal width, colored by species. - Distribution plot of petal lengths for different species. - Box plot of petal widths across species. - Customized context for visualization. # Set custom context sns.set_context(\\"paper\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2}) # Scatter plot - Sepal length vs Sepal width plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', data=data) scatter_plot.set(title=\'Scatter Plot of Sepal Length vs Sepal Width\', xlabel=\'Sepal Length (cm)\', ylabel=\'Sepal Width (cm)\') plt.legend(title=\'Species\') plt.show() # Distribution plot - Petal length for different species plt.figure(figsize=(10, 6)) distribution_plot = sns.histplot(data=data, x=\'petal_length\', hue=\'species\', kde=True) distribution_plot.set(title=\'Distribution of Petal Length by Species\', xlabel=\'Petal Length (cm)\') plt.legend(title=\'Species\') plt.show() # Box plot - Petal width across species plt.figure(figsize=(10, 6)) box_plot = sns.boxplot(x=\'species\', y=\'petal_width\', data=data) box_plot.set(title=\'Box Plot of Petal Width by Species\', xlabel=\'Species\', ylabel=\'Petal Width (cm)\') plt.show() # Example usage (commented out to avoid execution error since the data is not provided in this context) # data = pd.read_csv(\'path/to/flower_data.csv\') # visualize_flowers(data)"},{"question":"**Task**: Implement and test a multi-threaded task management system using the `queue` module. The system should have the following requirements: 1. **Implement a Multi-threaded Worker System**: - You need to create two types of workers using threads. - Worker Type A: Fetches tasks from a `Queue`. - Worker Type B: Fetches tasks from a `PriorityQueue`. 2. **Queue Management**: - Initialize a `Queue` and a `PriorityQueue` with a maximum size of 10. - Implement the logic to insert 50 tasks into the `Queue` and 50 tasks into the `PriorityQueue`. Tasks should be tuples where: - For `Queue`: `(task_id, type_a)` - For `PriorityQueue`: `(priority, (task_id, type_b))` - For the `PriorityQueue`, ensure tasks are inserted in descending order of priority. 3. **Task Processing**: - Each worker should fetch a task, print the task details, simulate working on the task by sleeping for a random short duration, call `task_done()`, and repeat this until all tasks are processed. - Ensure the main thread waits until all tasks are processed using the `join()` method. 4. **Error Handling**: - Ensure that if the `Queue` or `PriorityQueue` is full, additional tasks should wait until space is available. - Likewise, if a worker tries to fetch from an empty queue, it should wait until a task is available. # Example Function Signatures ```python import threading import queue import time import random def worker_a(q): while True: task = q.get() # Simulate task process print(f\'Worker A processing {task}\') time.sleep(random.uniform(0.01, 0.1)) q.task_done() def worker_b(pq): while True: task = pq.get() # Simulate task process print(f\'Worker B processing {task}\') time.sleep(random.uniform(0.01, 0.1)) pq.task_done() def main(): # Initialization task_queue = queue.Queue(maxsize=10) priority_queue = queue.PriorityQueue(maxsize=10) # Start worker threads threading.Thread(target=worker_a, args=(task_queue,), daemon=True).start() threading.Thread(target=worker_b, args=(priority_queue,), daemon=True).start() # Insert tasks for i in range(50): task_queue.put((i, \'type_a\')) for i in range(50, 0, -1): priority_queue.put((i, (50 - i, \'type_b\'))) # Wait for all tasks to be processed task_queue.join() priority_queue.join() print(\\"All tasks processed.\\") if __name__ == \\"__main__\\": main() ``` # Constraints - Make sure tasks processed by Worker A are only from `Queue` and tasks processed by Worker B are only from `PriorityQueue`. - Random sleep durations should simulate variable task processing times. - Ensure proper synchronization to avoid any race conditions or deadlocks. # Evaluation Criteria - Correctness: Proper implementation and handling of queue operations. - Understanding of thread synchronization and error handling. - Proper use of `queue` methods and attributes. - Code readability and adherence to Python conventions.","solution":"import threading import queue import time import random def worker_a(q): while True: task = q.get() # Simulate task process print(f\'Worker A processing {task}\') time.sleep(random.uniform(0.01, 0.1)) q.task_done() def worker_b(pq): while True: task = pq.get() # Simulate task process print(f\'Worker B processing {task[1]} with priority {task[0]}\') time.sleep(random.uniform(0.01, 0.1)) pq.task_done() def main(): # Initialization task_queue = queue.Queue(maxsize=10) priority_queue = queue.PriorityQueue(maxsize=10) # Start worker threads threading.Thread(target=worker_a, args=(task_queue,), daemon=True).start() threading.Thread(target=worker_b, args=(priority_queue,), daemon=True).start() # Insert tasks for i in range(50): task_queue.put((i, \'type_a\')) for i in range(50, 0, -1): priority_queue.put((i, (50 - i, \'type_b\'))) # Wait for all tasks to be processed task_queue.join() priority_queue.join() print(\\"All tasks processed.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Create a Python program that demonstrates the use of the `atexit` module to manage cleanup tasks upon program termination. **Problem Statement:** You are tasked with creating a logging mechanism for a script that: 1. Logs the execution of certain tasks. 2. Saves the log to a file upon normal program termination. **Instructions:** 1. Define a function `log_task(task_name)` that accepts a string `task_name` and appends it to a global log list. 2. Define a function `save_log()` that writes the log list to a file called `execution_log.txt`. 3. Register `save_log` to be executed upon normal program termination using the `atexit` module. 4. Write some sample tasks and call `log_task()` after each task to log their execution. **Constraints:** - You must use the `atexit` module to ensure that `save_log()` is called automatically when the program terminates. - The log must be saved in the order the tasks were logged. **Expected Input and Output:** - No specific user inputs are required. - The program should perform predefined tasks and log their execution. **Example:** ```python import atexit # Initialize the log list log_list = [] def log_task(task_name): global log_list log_list.append(task_name) def save_log(): with open(\'execution_log.txt\', \'w\') as logfile: for task in log_list: logfile.write(task + \'n\') # Register the save_log function to be called at program exit atexit.register(save_log) # Sample tasks log_task(\\"Task 1: Initialize system\\") log_task(\\"Task 2: Load data\\") log_task(\\"Task 3: Process data\\") log_task(\\"Task 4: Save results\\") # Additional code representing other tasks can go here # Program will automatically call save_log() upon normal termination ``` **Note:** Ensure your program creates the file `execution_log.txt` in the current working directory with the logged tasks in the correct order when the program exits normally.","solution":"import atexit # Initialize the log list log_list = [] def log_task(task_name): Appends a task name to the global log list. global log_list log_list.append(task_name) def save_log(): Writes the log list to a file called \'execution_log.txt\'. with open(\'execution_log.txt\', \'w\') as logfile: for task in log_list: logfile.write(task + \'n\') # Register the save_log function to be called at program exit atexit.register(save_log) # Sample tasks log_task(\\"Task 1: Initialize system\\") log_task(\\"Task 2: Load data\\") log_task(\\"Task 3: Process data\\") log_task(\\"Task 4: Save results\\")"},{"question":"Objective: To assess your understanding and handling of async exception cases within the asyncio module and demonstrate your ability to manage asynchronous tasks and exception handling in Python 3.10. Problem Statement: You are required to implement a function called `fetch_data_from_url` which asynchronously fetches data from a given URL. Your implementation should handle various exceptions that may occur during the execution, specifically focusing on the asyncio exceptions provided in the documentation. Function Signature: ```python import asyncio async def fetch_data_from_url(url: str) -> bytes: pass ``` Requirements: 1. **Input**: A string `url` representing the URL to fetch data from. 2. **Output**: A byte string containing the fetched data. 3. **Exceptions**: - Handle `asyncio.TimeoutError` and retry fetching the data up to 3 times before giving up. - Handle `asyncio.CancelledError` by performing custom operations such as logging the cancellation and re-raising the exception. - Handle `asyncio.InvalidStateError` by logging the error and raising it again. - Handle `asyncio.SendfileNotAvailableError` gracefully by logging the error message. - Handle `asyncio.IncompleteReadError` by logging the number of bytes expected and partial bytes read before end of stream. - Handle `asyncio.LimitOverrunError` by logging the number of bytes to be consumed. Constraints: - You should not use any third-party libraries for HTTP requests, only standard library modules (e.g., `asyncio`, `aiohttp`). Example Implementation: ```python import asyncio import aiohttp async def fetch_data_from_url(url: str) -> bytes: retries = 3 while retries > 0: try: async with aiohttp.ClientSession() as session: async with session.get(url) as response: if response.status != 200: response.raise_for_status() return await response.read() except asyncio.TimeoutError: retries -= 1 if retries == 0: raise except asyncio.CancelledError: # Log custom cancellation handling print(\\"Task was cancelled\\") raise except asyncio.InvalidStateError as e: # Log invalid state handling print(f\\"Invalid async state: {e}\\") raise except asyncio.SendfileNotAvailableError as e: # Log sendfile not available handling print(f\\"Sendfile not available: {e}\\") except asyncio.IncompleteReadError as e: # Log incomplete read handling print(f\\"Incomplete read: expected {e.expected} bytes, got {e.partial}\\") except asyncio.LimitOverrunError as e: # Log limit overrun handling print(f\\"Buffer limit overrun: {e.consumed} bytes\\") ``` This function should capture the essence of exception handling in asynchronous programming with the asyncio library by making use of the provided documentation.","solution":"import asyncio import aiohttp async def fetch_data_from_url(url: str) -> bytes: retries = 3 while retries > 0: try: async with aiohttp.ClientSession() as session: async with session.get(url) as response: if response.status != 200: response.raise_for_status() return await response.read() except asyncio.TimeoutError: retries -= 1 if retries == 0: raise except asyncio.CancelledError: # Log custom cancellation handling print(\\"Task was cancelled\\") raise except asyncio.InvalidStateError as e: # Log invalid state handling print(f\\"Invalid async state: {e}\\") raise except asyncio.SendfileNotAvailableError as e: # Log sendfile not available handling print(f\\"Sendfile not available: {e}\\") except asyncio.IncompleteReadError as e: # Log incomplete read handling print(f\\"Incomplete read: expected {e.expected} bytes, got {e.partial}\\") except asyncio.LimitOverrunError as e: # Log limit overrun handling print(f\\"Buffer limit overrun: {e.consumed} bytes\\")"},{"question":"# Custom Transformer Implementation Using scikit-learn **Objective:** Test the ability to implement custom components in scikit-learn and use them in a pipeline. Problem Statement: You are tasked with creating a custom transformer that standardizes numerical features to have mean zero and standard deviation one, but only if they exceed a certain threshold in their initial mean. You will then use this transformer in conjunction with a simple linear regression model to predict a target variable. Requirements: 1. **Custom Transformer Class**: Create a class `ThresholdStandardScaler` that extends `BaseEstimator` and `TransformerMixin` from `sklearn.base`. 2. **Initialization Parameters**: - `threshold`: A float value representing the threshold of the mean above which scaling is applied. 3. **Methods to Implement**: - `fit(self, X, y=None)`: Learn which features to scale based on the threshold. - `transform(self, X)`: Apply standard scaling to features with mean above the threshold. 4. **Pipeline Integration**: Use the above transformer in an `sklearn.pipeline.Pipeline` alongside a `LinearRegression` model. 5. **Model Evaluation**: Fit the pipeline to data and evaluate its performance using R^2 score. Input and Output: - **Input**: - A 2D numpy array `X` of shape (n_samples, n_features) containing the feature dataset. - A 1D numpy array `y` of shape (n_samples,) containing the target variable. - A float `threshold` to be used for the transformer. - **Output**: - The coefficient of determination R^2 score of the pipeline on the provided dataset. Constraints: - Use only scikit-learn\'s functionalities. - Numpy arrays should be handled correctly. - The performance should be optimal (consider transformation and fitting time efficiency). Example: ``` import numpy as np from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from sklearn.base import BaseEstimator, TransformerMixin class ThresholdStandardScaler(BaseEstimator, TransformerMixin): def __init__(self, threshold=0.0): self.threshold = threshold def fit(self, X, y=None): numeric_means = X.mean(axis=0) self.scaling_indices_ = numeric_means > self.threshold self.means_ = X[:, self.scaling_indices_].mean(axis=0) self.stds_ = X[:, self.scaling_indices_].std(axis=0) return self def transform(self, X): X_transformed = X.copy() X_transformed[:, self.scaling_indices_] = (X[:, self.scaling_indices_] - self.means_) / self.stds_ return X_transformed # Example Data X = np.array([[1.0, 200.0], [2.0, 300.0], [3.0, 400.0], [4.0, 500.0], [5.0, 600.0]]) y = np.array([10, 20, 30, 40, 50]) threshold = 100.0 # Define Pipeline pipeline = Pipeline([ (\'scaler\', ThresholdStandardScaler(threshold=threshold)), (\'regressor\', LinearRegression()) ]) # Fit and Evaluate pipeline.fit(X, y) r2_score = pipeline.score(X, y) print(f\\"R^2 Score: {r2_score:.2f}\\") ``` Output: ``` R^2 Score: 1.00 # Perfect fit if it\'s a perfect linear relationship ``` This question requires comprehension of custom transformation, pipeline usage, and evaluative metrics from scikit-learn, making it a comprehensive and challenging coding assessment.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression class ThresholdStandardScaler(BaseEstimator, TransformerMixin): def __init__(self, threshold=0.0): self.threshold = threshold def fit(self, X, y=None): numeric_means = X.mean(axis=0) self.scaling_indices_ = numeric_means > self.threshold self.means_ = X[:, self.scaling_indices_].mean(axis=0) self.stds_ = X[:, self.scaling_indices_].std(axis=0) return self def transform(self, X): X_transformed = X.copy() X_transformed[:, self.scaling_indices_] = (X[:, self.scaling_indices_] - self.means_) / self.stds_ return X_transformed def model_evaluation(X, y, threshold): pipeline = Pipeline([ (\'scaler\', ThresholdStandardScaler(threshold=threshold)), (\'regressor\', LinearRegression()) ]) pipeline.fit(X, y) return pipeline.score(X, y)"},{"question":"You are required to implement a Python class `MyFTPClient` that utilizes the `ftplib` module to perform a series of FTP operations. The operations include connecting to an FTP server, logging in, changing directories, listing files, downloading files, uploading files, and handling secure FTP connections using TLS. # Requirements: 1. **Class Initialization**: - The class should accept parameters for the FTP host, username, password, and whether to use TLS. - Establish the FTP connection in the initializer and log in using the provided credentials. 2. **Methods to Implement**: - `change_directory(path: str) -> str`: Changes the current directory to the specified path and returns a success message. - `list_files() -> list`: Lists and returns the files in the current directory. - `download_file(remote_filename: str, local_filename: str) -> str`: Downloads the specified file from the FTP server to the local file system and returns a success message. - `upload_file(local_filename: str, remote_filename: str) -> str`: Uploads a specified file from the local file system to the FTP server and returns a success message. - `disconnect() -> str`: Disconnects from the FTP server and returns a success message. 3. **Error Handling**: - Appropriately handle connection errors, login errors, and file transfer errors. Ensure any raised exceptions are user-friendly. # Constraints: - You must use the `ftplib` module. - Ensure all strings are encoded and decoded using UTF-8. - All file transfers should handle binary files. # Example Usage: ```python ftp_client = MyFTPClient(host=\'ftp.example.com\', user=\'username\', passwd=\'password\', use_tls=True) print(ftp_client.change_directory(\'/path/to/directory\')) print(ftp_client.list_files()) print(ftp_client.download_file(\'remote.txt\', \'local.txt\')) print(ftp_client.upload_file(\'local_upload.txt\', \'remote_upload.txt\')) print(ftp_client.disconnect()) ``` ```python # Below is the class template you need to complete. from ftplib import FTP, FTP_TLS class MyFTPClient: def __init__(self, host: str, user: str, passwd: str, use_tls: bool): # Initialize FTP or FTP_TLS connection and login pass def change_directory(self, path: str) -> str: # Change directory to given path pass def list_files(self) -> list: # List files in the current directory pass def download_file(self, remote_filename: str, local_filename: str) -> str: # Download a file from FTP server pass def upload_file(self, local_filename: str, remote_filename: str) -> str: # Upload a file to the FTP server pass def disconnect(self) -> str: # Disconnect from the FTP server pass # Implement the class methods to complete the exercise. ``` You are expected to handle exceptions related to network failures, authentication issues, and file transfer errors. Your implementation should be robust and handle unexpected situations gracefully.","solution":"from ftplib import FTP, FTP_TLS, all_errors import os class MyFTPClient: def __init__(self, host: str, user: str, passwd: str, use_tls: bool): try: self.ftp = FTP_TLS(host) if use_tls else FTP(host) self.ftp.login(user=user, passwd=passwd) if use_tls: self.ftp.prot_p() # Secure the data connection print(\\"Connected to FTP server.\\") except all_errors as e: raise ConnectionError(f\\"Failed to connect or login to the FTP server: {e}\\") def change_directory(self, path: str) -> str: try: self.ftp.cwd(path) return f\\"Changed directory to {path}\\" except all_errors as e: raise FileNotFoundError(f\\"Failed to change directory: {e}\\") def list_files(self) -> list: try: files = self.ftp.nlst() return files except all_errors as e: raise RuntimeError(f\\"Failed to list files: {e}\\") def download_file(self, remote_filename: str, local_filename: str) -> str: try: with open(local_filename, \\"wb\\") as local_file: self.ftp.retrbinary(f\\"RETR {remote_filename}\\", local_file.write) return f\\"Downloaded {remote_filename} to {local_filename}\\" except all_errors as e: raise RuntimeError(f\\"Failed to download file: {e}\\") def upload_file(self, local_filename: str, remote_filename: str) -> str: try: with open(local_filename, \\"rb\\") as local_file: self.ftp.storbinary(f\\"STOR {remote_filename}\\", local_file) return f\\"Uploaded {local_filename} to {remote_filename}\\" except all_errors as e: raise RuntimeError(f\\"Failed to upload file: {e}\\") def disconnect(self) -> str: try: self.ftp.quit() return \\"Disconnected from the FTP server\\" except all_errors as e: raise RuntimeError(f\\"Failed to disconnect: {e}\\")"},{"question":"**Question: Implement a File Converter Using binhex Module** You are required to implement a Python program that uses the `binhex` module to convert binary files to binhex format and binhex files back to binary format. Your program should include the following functions: 1. **convert_to_binhex(input_filename: str, output_filename: str) -> None** - This function will take a binary file specified by `input_filename` and convert it to a binhex encoded file specified by `output_filename`. Handle any exceptions that occur during the process and print an appropriate error message. 2. **convert_to_binary(input_filename: str, output_filename: str) -> None** - This function will take a binhex encoded file specified by `input_filename` and decode it to a binary file specified by `output_filename`. Handle any exceptions that occur during the process and print an appropriate error message. 3. **main()** - This function will prompt the user to specify an operation (\\"encode\\" or \\"decode\\"), take the input and output filenames from the user, and call the appropriate function (`convert_to_binhex` or `convert_to_binary`). **Constraints:** - Assume the input and output filenames provided by the user are valid paths. - Handle the `binhex.Error` exception specifically to manage binhex-specific errors, and use a generic exception to catch other potential errors. - Print user-friendly messages to indicate success or the nature of any errors encountered. **Function Signatures:** ```python def convert_to_binhex(input_filename: str, output_filename: str) -> None: pass def convert_to_binary(input_filename: str, output_filename: str) -> None: pass def main(): pass ``` **Example Usage:** ```python # Example usage after implementing the functions if __name__ == \\"__main__\\": main() # Interactive session output: # Enter operation (encode/decode): encode # Enter input file path: sample.bin # Enter output file path: sample.hqx # File encoding successful. # Enter operation (encode/decode): decode # Enter input file path: sample.hqx # Enter output file path: sample.bin # File decoding successful. ``` **Additional Notes:** - Ensure that all file handles are properly closed after operations. - You can utilize standard Python I/O operations and the `binhex` module as described.","solution":"import binhex def convert_to_binhex(input_filename: str, output_filename: str) -> None: try: with open(input_filename, \'rb\') as input_file: with open(output_filename, \'wb\') as output_file: binhex.binhex(input_file, output_file) print(\\"File encoding successful.\\") except binhex.Error as e: print(f\\"Binhex error during encoding: {e}\\") except Exception as e: print(f\\"Error during encoding: {e}\\") def convert_to_binary(input_filename: str, output_filename: str) -> None: try: with open(input_filename, \'rb\') as input_file: with open(output_filename, \'wb\') as output_file: binhex.hexbin(input_file, output_file) print(\\"File decoding successful.\\") except binhex.Error as e: print(f\\"Binhex error during decoding: {e}\\") except Exception as e: print(f\\"Error during decoding: {e}\\") def main(): operation = input(\\"Enter operation (encode/decode): \\").strip().lower() input_filename = input(\\"Enter input file path: \\").strip() output_filename = input(\\"Enter output file path: \\").strip() if operation == \\"encode\\": convert_to_binhex(input_filename, output_filename) elif operation == \\"decode\\": convert_to_binary(input_filename, output_filename) else: print(\\"Invalid operation. Please enter \'encode\' or \'decode\'.\\") if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with training a large neural network using pytorch, but you face memory constraints on your hardware. To overcome this limitation, you decide to use checkpointing. **Objective:** Your task is to implement a neural network using pytorch, where you apply checkpointing to manage memory efficiently. **Instructions:** 1. Implement a neural network using `torch.nn.Module`. 2. Train your network on any dummy dataset (e.g., random tensors). 3. Apply checkpointing using `torch.utils.checkpoint.checkpoint` to save memory during the training process. 4. Ensure your implementation handles both forward and backward passes correctly. **Expected Input and Output:** - Input: Randomly generated input tensor simulating the dataset. - Output: The loss value after a single epoch of training. **Constraints:** - Use `checkpoint` function from `torch.utils.checkpoint`. - Make sure your implementation updates the model weights correctly. - You may not use more than 2 GB of memory during training (use dummy input to simulate the dataset). Your solution must: 1. Define the neural network class. 2. Implement the training loop. 3. Integrate checkpointing into your network\'s training loop. 4. Print the calculated loss after training for one epoch. **Example Implementation:** ```python import torch import torch.nn as nn import torch.utils.checkpoint as checkpoint # Define a simple model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layer1 = nn.Linear(1024, 2048) self.layer2 = nn.Linear(2048, 4096) self.layer3 = nn.Linear(4096, 1024) self.out = nn.Linear(1024, 1) def forward(self, x): x = checkpoint.checkpoint(self.layer1, x) x = checkpoint.checkpoint(self.layer2, x) x = checkpoint.checkpoint(self.layer3, x) x = self.out(x) return x # Implement Training Loop def train(model, input_tensor, criterion, optimizer): model.train() optimizer.zero_grad() output = model(input_tensor) loss = criterion(output, torch.randn(output.shape)) loss.backward() optimizer.step() return loss.item() # Generate dummy input input_tensor = torch.randn(16, 1024) # Instantiate model, criterion and optimizer model = SimpleModel() criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Train and print loss loss = train(model, input_tensor, criterion, optimizer) print(f\\"Loss after one epoch: {loss}\\") ``` Use this template as a reference and ensure your implementation addresses the outlined constraints and objectives.","solution":"import torch import torch.nn as nn import torch.utils.checkpoint as checkpoint # Define a neural network model with checkpointing class CheckpointedModel(nn.Module): def __init__(self): super(CheckpointedModel, self).__init__() self.layer1 = nn.Linear(1024, 2048) self.layer2 = nn.Linear(2048, 4096) self.layer3 = nn.Linear(4096, 1024) self.out = nn.Linear(1024, 1) def forward(self, x): x = checkpoint.checkpoint(self.layer1, x) x = checkpoint.checkpoint(self.layer2, x) x = checkpoint.checkpoint(self.layer3, x) x = self.out(x) return x # Training function using checkpointed model def train(model, input_tensor, criterion, optimizer): model.train() optimizer.zero_grad() output = model(input_tensor) loss = criterion(output, torch.randn_like(output)) loss.backward() optimizer.step() return loss.item() # Generate dummy input tensor input_tensor = torch.randn(16, 1024) # Create model, loss criterion, and optimizer model = CheckpointedModel() criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Train model for one epoch and print loss loss = train(model, input_tensor, criterion, optimizer) print(f\\"Loss after one epoch: {loss}\\")"},{"question":"**Question: Visualizing Aggregated Data with Custom Functions in Seaborn** You are provided with a dataset of diamonds called `diamonds`, which includes various attributes of the diamonds such as carat, cut, and clarity. Using seaborn\'s `objects` module, we want to visualize the data in specific ways to gain insights. Your task is to: 1. Load the `diamonds` dataset using seaborn. 2. Create a bar plot that shows the mean `carat` for each `clarity` of diamonds. 3. Modify the plot to show the median `carat` for each `clarity` instead. 4. Create another bar plot that shows the interquartile range (IQR) of `carat` values for each `clarity` and colored by `cut`. **Requirements:** - The code must load the dataset using `seaborn.load_dataset`. - Use the `seaborn.objects.Plot` class to create the plots. - Use appropriate aggregation functions: mean, median, and a custom lambda function for IQR. - Each step should produce a separate bar plot visualization. - Customize the bar plot to color bars by the `cut` variable in the last plot using the `so.Dodge()` function. **Expected input:** No input is required. **Expected output:** Three visualizations showing: 1. Mean `carat` for each `clarity`. 2. Median `carat` for each `clarity`. 3. IQR of `carat` for each `clarity`, colored by `cut`. ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Create a bar plot showing mean carat for each clarity p1 = so.Plot(diamonds, \\"clarity\\", \\"carat\\") p1.add(so.Bar(), so.Agg(\\"mean\\")) # Step 3: Modify the plot to show median carat for each clarity p2 = so.Plot(diamonds, \\"clarity\\", \\"carat\\") p2.add(so.Bar(), so.Agg(\\"median\\")) # Step 4: Create a bar plot showing the interquartile range of carat for each clarity and color by cut p3 = so.Plot(diamonds, \\"clarity\\", \\"carat\\") p3.add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), so.Dodge(), color=\\"cut\\") # Display plots p1.show() p2.show() p3.show() ``` **Note:** This question requires students to understand how to: - Use the `seaborn.objects.Plot` class. - Add layers to the plot with aggregation functions. - Define custom aggregation functions. - Use transforms and mapping variables for advanced visualizations.","solution":"import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Create a bar plot showing mean carat for each clarity p1 = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add(so.Bar(), so.Agg(\\"mean\\")) # Step 3: Modify the plot to show median carat for each clarity p2 = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add(so.Bar(), so.Agg(\\"median\\")) # Step 4: Create a bar plot showing the interquartile range of carat for each clarity and color by cut p3 = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), so.Dodge(), color=\\"cut\\") # Display plots p1.show() p2.show() p3.show()"},{"question":"# File and Directory Operations Assessment Objective Demonstrate your understanding and ability to work with various file and directory manipulation modules in Python. Problem Statement You are tasked with creating a utility function `backup_directory` that takes two parameters: 1. `source_dir`: A string representing the path of the source directory to back up. 2. `backup_dir`: A string representing the path of the destination directory where the backup will be stored. The function should perform the following operations: 1. Ensure that the `source_dir` exists. If it does not exist, raise a `FileNotFoundError`. 2. If `backup_dir` does not exist, create it. 3. Recursively copy all files and subdirectories from `source_dir` to `backup_dir`. 4. Ensure that the file permissions and timestamps of the files in the `backup_dir` match those in the `source_dir`. 5. Ignore hidden files and directories (those starting with a dot `.`) during the backup process. Input - `source_dir`: A string representing the source directory path. - `backup_dir`: A string representing the destination directory path. Output - The function should not return anything but should create a backup of the `source_dir` at `backup_dir`. Example ```python import os source_dir = \\"/path/to/source\\" backup_dir = \\"/path/to/backup\\" backup_directory(source_dir, backup_dir) ``` Constraints 1. The function should handle various edge cases, such as empty directories, directories with nested subdirectories, and files with different permissions and timestamps. 2. Performance should be considered; the function should handle large directories efficiently. Implementation Requirements 1. Utilize the appropriate modules from the provided documentation (like `os`, `shutil`, and `stat`). 2. Ensure the code is clear and well-commented to illustrate your thought process and solution. Tips - You can use `os.path` to manipulate pathnames. - The `shutil` module provides high-level operations on files and collections of files. - The `stat` module allows you to interpret and modify file permissions and timestamps. Note This task assesses your ability to combine multiple Python modules to solve real-world problems. Make sure to handle exceptions and edge cases appropriately.","solution":"import os import shutil import stat def backup_directory(source_dir, backup_dir): Creates a backup of the source directory into the backup directory. Args: source_dir (str): Path to the source directory to back up. backup_dir (str): Path to the destination directory where the backup will be stored. Raises: FileNotFoundError: If the source directory does not exist. # Ensure that the source directory exists if not os.path.exists(source_dir): raise FileNotFoundError(f\\"The source directory \'{source_dir}\' does not exist.\\") # Create the backup directory if it does not exist os.makedirs(backup_dir, exist_ok=True) for root, dirs, files in os.walk(source_dir): # Skip hidden files and directories dirs[:] = [d for d in dirs if not d.startswith(\'.\')] files = [f for f in files if not f.startswith(\'.\')] # Determine the backup path for the current directory rel_path = os.path.relpath(root, source_dir) backup_root = os.path.join(backup_dir, rel_path) # Create the backup directory if it does not exist os.makedirs(backup_root, exist_ok=True) # Copy each file in the current directory for file in files: source_file = os.path.join(root, file) backup_file = os.path.join(backup_root, file) shutil.copy2(source_file, backup_file) # Ensure file permissions and timestamps are copied shutil.copystat(source_file, backup_file)"},{"question":"# Sun AU Audio File Manipulation Objective: You are provided with a Sun AU audio file. Your task is to read the audio data, modify it, and write it to a new AU file. The modifications required are: 1. Convert the audio to mono if it is stereo. 2. Reverse the audio frames to create a backward playback effect. Requirements: 1. Implement the function `process_audio(input_file: str, output_file: str) -> None` which: - Takes the input file path (`input_file`) of the Sun AU file to be read. - Takes the output file path (`output_file`) where the modified Sun AU file should be written. 2. The function should: - Read the input Sun AU file. - If the audio is stereo (2 channels), convert it to mono by averaging the two channels. - Reverse the frames of the audio data. - Write the modified audio data to the output file in Sun AU format. Constraints: - You must handle cases where the input file is already in mono format. - Efficiently manage memory, especially with large audio files. - Follow the Sun AU file format specifications when writing the output file. Example: Suppose `input.au` is a stereo Sun AU file with the following characteristics: - Sampling rate: 44100 Hz - Sample width: 2 bytes (16 bits) - Frames: 100000 frames - Compression type: `\'ULAW\'` The `process_audio` function should: 1. Convert it to mono by averaging the left and right channels of each frame. 2. Reverse the frames. 3. Write the modified audio data to `output.au`. Here is a template to help you get started: ```python import sunau def process_audio(input_file: str, output_file: str) -> None: # Open the input AU file input_au = sunau.open(input_file, \'r\') # Get parameters of the input file num_channels = input_au.getnchannels() sample_width = input_au.getsampwidth() frame_rate = input_au.getframerate() num_frames = input_au.getnframes() comp_type = input_au.getcomptype() comp_name = input_au.getcompname() # Read frames from the input file audio_data = input_au.readframes(num_frames) # Convert to mono if stereo if num_channels == 2: # Averaging the two channels (stereo to mono conversion) mono_data = bytearray() for i in range(0, len(audio_data), 2*sample_width): left = int.from_bytes(audio_data[i:i+sample_width], byteorder=\'big\', signed=True) right = int.from_bytes(audio_data[i+sample_width:i+2*sample_width], byteorder=\'big\', signed=True) mono_frame = ((left + right) // 2).to_bytes(sample_width, byteorder=\'big\', signed=True) mono_data.extend(mono_frame) else: mono_data = audio_data # Reverse the audio frames reversed_data = mono_data[::-1] # Open the output file output_au = sunau.open(output_file, \'w\') # Set parameters and write modified data output_au.setnchannels(1) output_au.setsampwidth(sample_width) output_au.setframerate(frame_rate) output_au.setnframes(len(reversed_data) // sample_width) output_au.setcomptype(comp_type, comp_name) # Write frames to the output file output_au.writeframes(reversed_data) # Close the AU files input_au.close() output_au.close() ``` Note: Ensure to handle exceptions and errors that may occur during file operations, such as reading from or writing to invalid files, or encountering unsupported audio file specifications.","solution":"import sunau def process_audio(input_file: str, output_file: str) -> None: # Open the input AU file input_au = sunau.open(input_file, \'r\') # Get parameters of the input file num_channels = input_au.getnchannels() sample_width = input_au.getsampwidth() frame_rate = input_au.getframerate() num_frames = input_au.getnframes() comp_type = input_au.getcomptype() comp_name = input_au.getcompname() # Read frames from the input file audio_data = input_au.readframes(num_frames) # Convert to mono if stereo if num_channels == 2: # Averaging the two channels (stereo to mono conversion) mono_data = bytearray() for i in range(0, len(audio_data), 2*sample_width): left = int.from_bytes(audio_data[i:i+sample_width], byteorder=\'big\', signed=True) right = int.from_bytes(audio_data[i+sample_width:i+2*sample_width], byteorder=\'big\', signed=True) mono_frame = ((left + right) // 2).to_bytes(sample_width, byteorder=\'big\', signed=True) mono_data.extend(mono_frame) else: mono_data = audio_data # Reverse the audio frames reversed_data = bytearray() frame_size = sample_width for i in range(0, len(mono_data), frame_size): reversed_data.extend(mono_data[i:i+frame_size][::-1]) reversed_data = reversed_data[::-1] # Open the output file output_au = sunau.open(output_file, \'w\') # Set parameters and write modified data output_au.setnchannels(1) output_au.setsampwidth(sample_width) output_au.setframerate(frame_rate) output_au.setcomptype(comp_type, comp_name) # Write frames to the output file output_au.writeframes(reversed_data) # Close the AU files input_au.close() output_au.close()"},{"question":"# PyTorch Signal Processing: Window Functions **Context:** In signal processing, window functions are used to taper a signal before performing transformations such as the Fourier transform to reduce spectral leakage. PyTorch provides a module `torch.signal.windows` that includes various window functions similar to those found in SciPy\'s signal module. **Task:** You are required to implement a function using the `torch.signal.windows` module that applies different window functions to a given input signal. **Function Signature:** ```python import torch from torch.signal.windows import * def apply_window(signal: torch.Tensor, window_type: str, window_length: int = None, **kwargs) -> torch.Tensor: Applies a specified window function to the input signal. Parameters: - signal: torch.Tensor - The input 1-D signal. - window_type: str - The type of window function to apply. Must be one of the following: [\'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\'] - window_length: int, optional - Desired length of the window, must be less than or equal to the length of the signal. If None, the length of the window will be the same as the signal length. - **kwargs: Additional parameters specific to certain window functions (Refer to PyTorch documentation). Returns: - torch.Tensor - The windowed signal. Raises: - ValueError: If `window_type` is not recognized or `window_length` is greater than the signal length. pass ``` **Input:** - A 1-D PyTorch tensor `signal` representing the input signal. - A string `window_type` indicating the type of window function to apply. - An optional integer `window_length` specifying the desired length of the window to apply (must be less than or equal to the length of the signal). If not provided, the window length should be the same as the signal length. - Additional arguments `**kwargs` may be required by specific window functions. **Output:** - The function should return a new 1-D PyTorch tensor representing the windowed signal. **Constraints:** - The function should raise a `ValueError` if `window_type` is not one of the specified types. - The function should raise a `ValueError` if `window_length` is greater than the length of the input signal. **Example:** ```python import torch signal = torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0]) windowed_signal = apply_window(signal, \'hann\') # Expected output: a tensor where each value of `signal` has been multiplied by the \\"hann\\" window values. ``` **Note:** You can refer to the PyTorch documentation for specific details on the additional parameters each window function might require.","solution":"import torch def apply_window(signal: torch.Tensor, window_type: str, window_length: int = None, **kwargs) -> torch.Tensor: Applies a specified window function to the input signal. Parameters: - signal: torch.Tensor - The input 1-D signal. - window_type: str - The type of window function to apply. Must be one of the following: [\'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\'] - window_length: int, optional - Desired length of the window, must be less than or equal to the length of the signal. If None, the length of the window will be the same as the signal length. - **kwargs: Additional parameters specific to certain window functions (Refer to PyTorch documentation). Returns: - torch.Tensor - The windowed signal. Raises: - ValueError: If `window_type` is not recognized or `window_length` is greater than the signal length. if window_length is None: window_length = signal.shape[0] if window_length > signal.shape[0]: raise ValueError(\\"window_length cannot be greater than the length of the signal\\") if window_type not in [\'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\']: raise ValueError(f\\"Invalid window type: {window_type}\\") if window_type == \'hann\': window = torch.hann_window(window_length, **kwargs) elif window_type == \'hamming\': window = torch.hamming_window(window_length, **kwargs) elif window_type == \'blackman\': window = torch.blackman_window(window_length, **kwargs) # Additional implementation for other window types can go here. else: raise ValueError(f\\"Window type `{window_type}` is not implemented yet.\\") # Truncate or pad the window if needed to match the signal length if window_length < signal.shape[0]: window = torch.cat([window, torch.zeros(signal.shape[0] - window_length)]) elif window_length > signal.shape[0]: window = window[:signal.shape[0]] return signal * window"},{"question":"**Question: Implementing a Custom Attention Layer** Using PyTorch, implement a custom attention layer suitable for sequence-to-sequence tasks such as machine translation. Your task is to construct an attention mechanism that can compute attention scores and use them to create a weighted context vector. # Requirements 1. **Input Format:** - A tensor `query` of shape `(batch_size, query_len, hidden_dim)`. - A tensor `key` of shape `(batch_size, key_len, hidden_dim)`. - A tensor `value` of shape `(batch_size, value_len, hidden_dim)`. 2. **Output Format:** - A tensor `context` of shape `(batch_size, query_len, hidden_dim)`, representing the weighted sum of the `value` tensor based on attention scores computed from the `query` and `key`. 3. **Implementation Details:** - Compute compatibility scores between `query` and `key` using dot-product attention. - Apply a softmax function to the scores to obtain attention weights. - Use the attention weights to compute the context vector as a weighted sum of the `value` tensor. - Implement the attention mechanism as a custom PyTorch module. 4. **Constraints:** - You must use PyTorch\'s tensor operations to perform the required computations. - Ensure your implementation is efficient and can handle batch processing. 5. **Performance Requirements:** - Your implementation should be able to handle large tensors efficiently, making use of PyTorch\'s optimized operations. **Example:** ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self): super(CustomAttention, self).__init__() def forward(self, query, key, value): # Compute the attention scores scores = torch.bmm(query, key.transpose(1, 2)) # Normalize scores to probabilities attention_weights = F.softmax(scores, dim=-1) # Compute the weighted sum of the value vectors context = torch.bmm(attention_weights, value) return context # Sample input tensors batch_size = 2 query_len = 3 key_len = 3 value_len = 3 hidden_dim = 4 query = torch.randn(batch_size, query_len, hidden_dim) key = torch.randn(batch_size, key_len, hidden_dim) value = torch.randn(batch_size, value_len, hidden_dim) # Initialize and apply custom attention layer attention_layer = CustomAttention() context = attention_layer(query, key, value) print(context.shape) # Expected shape: (batch_size, query_len, hidden_dim) ``` Use this structure to guide your implementation. Test your model with different input sizes to ensure it is general.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self): super(CustomAttention, self).__init__() def forward(self, query, key, value): # Compute the attention scores scores = torch.bmm(query, key.transpose(1, 2)) # Normalize scores to probabilities using softmax attention_weights = F.softmax(scores, dim=-1) # Compute the weighted sum of the value vectors context = torch.bmm(attention_weights, value) return context"},{"question":"**Problem Statement:** You are tasked with designing a logging system for a Unix-based server application using Python that utilizes the `syslog` module. Your logging system should have the following capabilities: 1. Log messages of different priority levels and facilities. 2. Configure logging options, including process IDs and custom identifiers. 3. Control which priority levels are logged using masks. 4. Ensure that `openlog()` is explicitly called before the first `syslog()` call and that `closelog()` is called when logging is complete. # Function Signature ```python def setup_logging(ident: str, logoption: int, facility: int) -> None: Configures the logger with the given identifier, log options, and facility. :param ident: A string identifier prepended to every message. :param logoption: Bit field log options. :param facility: Default facility for the logger. def log_message(priority: int, message: str) -> None: Logs a message with the specified priority. :param priority: Priority level for the message. :param message: The message to log. def set_priority_mask(maskpri: int) -> int: Sets the priority mask and returns the previous mask value. :param maskpri: Mask priority. :return: The previous mask value. def close_logging() -> None: Closes the syslog. ``` # Constraints - The `ident` parameter in `setup_logging` should be a non-empty string. - The `logoption` parameter should be a combination of log options defined in the module. - Calculate mask priorities using `syslog.LOG_MASK(pri)` and `syslog.LOG_UPTO(pri)` as needed. - Ensure `setup_logging` initializes the logging configuration before any messages are logged, and `close_logging` cleans up after logging is complete. # Example ```python if __name__ == \\"__main__\\": # Setup logger setup_logging(\\"MyApp\\", syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER) # Set priority mask to log all priorities up to LOG_WARNING old_mask = set_priority_mask(syslog.LOG_UPTO(syslog.LOG_WARNING)) # Log messages log_message(syslog.LOG_INFO, \\"This is an info message\\") log_message(syslog.LOG_ERR, \\"This is an error message\\") # Close logger close_logging() ``` In this example, the logger is configured to include process IDs and write messages to the console. It logs messages up to the `syslog.LOG_WARNING` level. Messages beyond this level will not be recorded due to the priority mask settings. Messages are logged, then the logging system is closed.","solution":"import syslog def setup_logging(ident: str, logoption: int, facility: int) -> None: Configures the logger with the given identifier, log options, and facility. :param ident: A string identifier prepended to every message. :param logoption: Bit field log options. :param facility: Default facility for the logger. if not ident: raise ValueError(\\"ident must be a non-empty string\\") syslog.openlog(ident, logoption, facility) def log_message(priority: int, message: str) -> None: Logs a message with the specified priority. :param priority: Priority level for the message. :param message: The message to log. syslog.syslog(priority, message) def set_priority_mask(maskpri: int) -> int: Sets the priority mask and returns the previous mask value. :param maskpri: Mask priority. :return: The previous mask value. previous_mask = syslog.setlogmask(maskpri) return previous_mask def close_logging() -> None: Closes the syslog. syslog.closelog()"},{"question":"**Objective:** Implement a function that utilizes the `subprocess` module to perform a sequence of shell commands, capturing their outputs and managing error handling. **Problem Statement:** You are provided with a list of shell commands that you need to execute sequentially. Each command could potentially have its output used as the input for the next command, forming a pipeline. You will need to handle errors gracefully and return a comprehensive report of the command executions. Write a Python function `execute_commands(commands: List[str], use_pipes: bool = False) -> Dict[str, Any]` that: 1. Takes a list of shell commands (each as a string) and a boolean flag indicating whether to use pipelining. 2. Executes each command in the sequence. If `use_pipes` is `True`, the output of each command should be passed as the input to the next command. 3. Captures the stdout and stderr of each command. 4. Raises a `subprocess.CalledProcessError` if any command exits with a non-zero return code. 5. Returns a dictionary containing: - A list of stdout outputs for each command. - A list of stderr outputs for each command. - The return code of the last command executed. **Input:** - `commands`: List of shell command strings to execute. Each element in the list is a command to run. - `use_pipes`: Boolean flag indicating whether to use the output of one command as the input to the next command. **Output:** - A dictionary with the following keys: - `stdout`: A list of stdout outputs corresponding to each command. - `stderr`: A list of stderr outputs corresponding to each command. - `return_code`: The return code of the last command executed. **Examples:** ```python commands = [\\"echo \'hello world\'\\", \\"wc -w\\"] result = execute_commands(commands, use_pipes=True) print(result) # Output might be: # { # \'stdout\': [\'hello world\', \'2n\'], # \'stderr\': [\'\', \'\'], # \'return_code\': 0 # } commands = [\\"ls\\", \\"wrong_command\\"] try: result = execute_commands(commands, use_pipes=False) except subprocess.CalledProcessError as e: print(e) # Expected output: # CalledProcessError: Command \'wrong_command\' returned non-zero exit status 1. ``` **Constraints:** - You must use the `subprocess` module. - Handle exceptions and ensure that they provide useful error messages. - Ensure the solution is safe from shell injection vulnerabilities, especially when `shell=True`. **Bonus Points:** - Implement a timeout feature to limit how long each command can run. - Handle different encodings for the command outputs. **Performance Requirements:** - Your solution should be efficient and avoid unnecessary memory usage, especially when handling large command outputs.","solution":"import subprocess from typing import List, Dict, Any def execute_commands(commands: List[str], use_pipes: bool = False) -> Dict[str, Any]: Executes a list of shell commands sequentially, with optional pipelining. Args: commands (List[str]): List of shell commands to execute. use_pipes (bool): Whether to use the output of one command as the input for the next command. Returns: Dict[str, Any]: Dictionary containing stdout, stderr, and the return code of the last command. stdout_list = [] stderr_list = [] prev_stdout = None for i, command in enumerate(commands): try: if use_pipes and prev_stdout is not None: proc = subprocess.Popen( command, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True ) stdout, stderr = proc.communicate(input=prev_stdout) else: proc = subprocess.Popen( command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True ) stdout, stderr = proc.communicate() stdout_list.append(stdout) stderr_list.append(stderr) prev_stdout = stdout if proc.returncode != 0: raise subprocess.CalledProcessError(proc.returncode, command, output=stdout, stderr=stderr) except subprocess.CalledProcessError as e: return { \'stdout\': stdout_list, \'stderr\': stderr_list, \'return_code\': e.returncode, } return { \'stdout\': stdout_list, \'stderr\': stderr_list, \'return_code\': proc.returncode, }"},{"question":"# Task You are required to implement a Python function to perform that makes use of Ridge Regression and Lasso Regression from the `scikit-learn` library. This function should split the provided dataset into training and testing sets, train both models on the training set, and evaluate their performance on the test set using mean squared error (MSE). # Problem Statement **Function Signature:** ```python def evaluate_linear_models( X: np.ndarray, y: np.ndarray, test_size: float = 0.2, random_state: int = 42, alpha_ridge: float = 1.0, alpha_lasso: float = 1.0 ) -> Tuple[float, float]: pass ``` # Input 1. **X:** A numpy array of shape (n_samples, n_features) containing the features of the dataset. 2. **y:** A numpy array of shape (n_samples,) containing the target values. 3. **test_size:** (optional) A float representing the proportion of the dataset to include in the test split. Default is 0.2. 4. **random_state:** (optional) An integer used as the seed for the random number generator. Default is 42. 5. **alpha_ridge:** (optional) A float representing the regularization strength for Ridge regression. Default is 1.0. 6. **alpha_lasso:** (optional) A float representing the regularization strength for Lasso regression. Default is 1.0. # Output A tuple containing two floats: 1. Mean Squared Error of the Ridge regression model on the test set. 2. Mean Squared Error of the Lasso regression model on the test set. # Example Usage ```python import numpy as np from sklearn.datasets import load_boston # Load example dataset data = load_boston() X = data.data y = data.target # Evaluate models mse_ridge, mse_lasso = evaluate_linear_models(X, y) print(f\\"Ridge Regression Test MSE: {mse_ridge}\\") print(f\\"Lasso Regression Test MSE: {mse_lasso}\\") ``` # Constraints 1. You should use scikit-learn\'s `Ridge` and `Lasso` classes for regression. 2. Ensure to split the data using `train_test_split` from scikit-learn. 3. Evaluate the performance using `mean_squared_error` from scikit-learn. 4. Use default parameters for the machine learning models unless stated otherwise. # Performance Requirements Your implementation should be efficient and should not exceed a reasonable time limit for datasets with up to 10,000 samples and 100 features.","solution":"from sklearn.linear_model import Ridge, Lasso from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error import numpy as np from typing import Tuple def evaluate_linear_models( X: np.ndarray, y: np.ndarray, test_size: float = 0.2, random_state: int = 42, alpha_ridge: float = 1.0, alpha_lasso: float = 1.0 ) -> Tuple[float, float]: Splits the dataset into a training and testing set, trains Ridge and Lasso regression models, and evaluates their performance using mean squared error. Parameters: X (np.ndarray): The features of the dataset. y (np.ndarray): The target values. test_size (float): The proportion of the dataset to include in the test split. random_state (int): The seed for the random number generator. alpha_ridge (float): The regularization strength for Ridge regression. alpha_lasso (float): The regularization strength for Lasso regression. Returns: Tuple[float, float]: A tuple containing the MSE of Ridge and Lasso regression models on the test set. # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) # Initialize and train Ridge regression model ridge = Ridge(alpha=alpha_ridge, random_state=random_state) ridge.fit(X_train, y_train) # Predict and evaluate Ridge regression model y_pred_ridge = ridge.predict(X_test) mse_ridge = mean_squared_error(y_test, y_pred_ridge) # Initialize and train Lasso regression model lasso = Lasso(alpha=alpha_lasso, random_state=random_state) lasso.fit(X_train, y_train) # Predict and evaluate Lasso regression model y_pred_lasso = lasso.predict(X_test) mse_lasso = mean_squared_error(y_test, y_pred_lasso) return mse_ridge, mse_lasso"},{"question":"# Question: Optimize Expensive Computation Using Functools You are optimizing a data processing pipeline in a Python application that performs several expensive computations repeatedly. Your task is to implement a class `DataProcessor` that uses the `functools` module to reduce redundant computations effectively. You will implement caching and partial application of functions within this class. Specifications: 1. **Class Structure:** ```python class DataProcessor: def __init__(self, data): # Constructor to initialize the data pass @functools.cache def expensive_computation(self, x): # A method performing an expensive computation pass @cached_property def processed_data(self): # Process and return data pass def partial_computation(self, y): # Use functools.partial to create a partial function pass ``` 2. **Implementation Details:** - `__init__(self, data)`: Initialize the class with a list of numerical data. - `expensive_computation(self, x)`: This method takes an integer `x` and returns the square of `x` after simulating a time delay (e.g., use `time.sleep` to simulate the expensive computation). You should cache the results to avoid redundant computations. - `processed_data(self)`: This method processes the stored data by computing the square of each element in the data list using the `expensive_computation` method. Cache the processed data so that subsequent accesses do not recompute it. - `partial_computation(self, y)`: This method uses `functools.partial` to create a new function that computes the product of a given number and a fixed multiplier (e.g., multiplier can be set to a value within the function). The method should return the partial function. Constraints: - The data list in `__init__` can have up to `10^6` elements. - The value of `x` for `expensive_computation` can range from `1` to `10^9`. - The value of `y` for `partial_computation` can range from `1` to `10^5`. Example Usage: ```python dp = DataProcessor([1, 2, 3, 4, 5]) print(dp.expensive_computation(10)) # Should compute the square of 10 and cache the result. print(dp.processed_data) # Should return [1, 4, 9, 16, 25] and cache the result. partial_func = dp.partial_computation(3) print(partial_func(7)) # Should return the product of 3 and the fixed multiplier. ``` Ensure the implementation is efficient and leverages the `functools` module capabilities effectively.","solution":"import functools import time class DataProcessor: def __init__(self, data): # Constructor to initialize the data self.data = data @functools.lru_cache(maxsize=None) def expensive_computation(self, x): # Simulate an expensive computation time.sleep(1) # Delay to simulate computation return x * x @functools.cached_property def processed_data(self): # Compute and return the processed data, caching the results return [self.expensive_computation(x) for x in self.data] def partial_computation(self, multiplier): # Use functools.partial to create a partial function return functools.partial(self._multiply, multiplier) def _multiply(self, multiplier, value): return multiplier * value"},{"question":"# PyTorch Profiling Hooks Implementation Objective: Implement a mechanism to add global profiling hooks to monitor the execution time of PyTorch operations. This will help collect runtime statistics and performance data for all operations executed in a script. Problem Statement: You are required to write a Python function that initializes a global profiling hook using `torch.autograd.profiler`. This hook should monitor and log the time taken by individual operators during the execution of a simple neural network training loop in PyTorch. Requirements: 1. **Function Signature**: ```python def init_profiling_hook(): pass ``` 2. **Logging Requirements**: - Log the name of the operator. - Log the time taken to execute the operator (`self_cpu_time_total`). 3. **Training Loop**: Implement a simple training loop where a small neural network is trained on random data. This loop will use the profiling hook to log the required information. 4. **Performance**: Ensure that the profiling hook introduces minimal overhead by sampling only 1% of the operator invocations. Input and Output: - **Input**: None (The function `init_profiling_hook` does not take any input) - **Output**: The function should configure the profiling hook and print the logging information during the network training. Constraints: - Ensure the profiling hooks and sampling are set up correctly before running the training loop. - The logged information should be clear and correctly capture the required details. Example: Here\'s an example of a simple training loop with random data. ```python import torch import torch.nn as nn import torch.optim as optim # Your implementation of the profiling hook goes here def init_profiling_hook(): pass # Define a simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) return self.fc2(x) # Initialize the profiling hook init_profiling_hook() # Create random data input_data = torch.randn(100, 10) target = torch.randn(100, 1) # Initialize the model, loss function, and optimizer model = SimpleNN() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(5): optimizer.zero_grad() output = model(input_data) loss = criterion(output, target) loss.backward() optimizer.step() ``` Notes: - You are required to fill in the `init_profiling_hook` function. - The logging should happen during the training loop when the profiling hook is active.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.autograd import profiler def init_profiling_hook(): Initializes a global profiling hook to monitor PyTorch operation execution times. # Define the profiling context manager class ProfilerContextManager: def __enter__(self): self.prof = profiler.profile(with_stack=False, profile_memory=False, use_cuda=False, record_shapes=False) self.prof.__enter__() return self def __exit__(self, exc_type, exc_val, exc_tb): self.prof.__exit__(exc_type, exc_val, exc_tb) print(self.prof.key_averages().table( sort_by=\\"self_cpu_time_total\\", row_limit=10)) # The function `torch.autograd.profiler.profile` doesn\'t directly function as a hook, # So we wrap the profiler context manager around our training loop manually. return ProfilerContextManager() # Define a simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) return self.fc2(x) # Example training loop wrapped with the profiler context def run_training_with_profiling(): # Initialize the profiling hook profiler_ctx = init_profiling_hook() # Create random data input_data = torch.randn(100, 10) target = torch.randn(100, 1) # Initialize the model, loss function, and optimizer model = SimpleNN() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) with profiler_ctx: # Training loop for epoch in range(5): optimizer.zero_grad() output = model(input_data) loss = criterion(output, target) loss.backward() optimizer.step()"},{"question":"Coding Assessment Question # Migrating to Copy-on-Write in pandas For this question, you are given a pandas DataFrame that represents a product inventory. You are required to perform several operations on this DataFrame in compliance with the Copy-on-Write (CoW) behavior introduced in pandas 3.0. # Specifications 1. **Input Format**: - A DataFrame `df` with columns: - `product_id` (int): Unique identifier for each product. - `product_name` (str): Name of the product. - `quantity` (int): Quantity of the product in inventory. - `price` (float): Price per unit of the product. 2. **Operations**: - Write a function `update_product_quantity(df: pd.DataFrame, product_id: int, new_quantity: int) -> pd.DataFrame`. This function should update the `quantity` of the product with the given `product_id` to `new_quantity`. - Write a function `increase_price_by_percentage(df: pd.DataFrame, percentage: float) -> pd.DataFrame`. This function should increase the `price` of every product by the given `percentage`. - Write a function `drop_products_below_quantity(df: pd.DataFrame, threshold: int) -> pd.DataFrame`. This function should remove all products from the DataFrame where the `quantity` is below the given `threshold`. # Constraints - You must not use chained assignments. - You cannot use `inplace` parameter in your operations. - Ensure that your modifications follow the CoW principles and do not inadvertently affect other objects. # Expected Output Format The modified DataFrame after each function is applied should be returned. # Example Usage ```python import pandas as pd # Example DataFrame data = { \\"product_id\\": [1, 2, 3], \\"product_name\\": [\\"Widget\\", \\"Gadget\\", \\"Doodad\\"], \\"quantity\\": [100, 150, 80], \\"price\\": [9.99, 19.99, 4.99] } df = pd.DataFrame(data) # Function Implementations def update_product_quantity(df: pd.DataFrame, product_id: int, new_quantity: int) -> pd.DataFrame: df.loc[df[\\"product_id\\"] == product_id, \\"quantity\\"] = new_quantity return df def increase_price_by_percentage(df: pd.DataFrame, percentage: float) -> pd.DataFrame: df[\\"price\\"] = df[\\"price\\"] * (1 + percentage / 100) return df def drop_products_below_quantity(df: pd.DataFrame, threshold: int) -> pd.DataFrame: return df[df[\\"quantity\\"] >= threshold] # Applying Functions df = update_product_quantity(df, 1, 120) # Update quantity of product_id 1 df = increase_price_by_percentage(df, 10) # Increase prices by 10% df = drop_products_below_quantity(df, 100) # Drop products with quantity below 100 print(df) ``` Your task is to implement the three functions described above, ensuring compliance with Copy-on-Write principles in pandas.","solution":"import pandas as pd def update_product_quantity(df: pd.DataFrame, product_id: int, new_quantity: int) -> pd.DataFrame: df_copy = df.copy() df_copy.loc[df_copy[\\"product_id\\"] == product_id, \\"quantity\\"] = new_quantity return df_copy def increase_price_by_percentage(df: pd.DataFrame, percentage: float) -> pd.DataFrame: df_copy = df.copy() df_copy[\\"price\\"] = df_copy[\\"price\\"] * (1 + percentage / 100) return df_copy def drop_products_below_quantity(df: pd.DataFrame, threshold: int) -> pd.DataFrame: return df[df[\\"quantity\\"] >= threshold].copy()"},{"question":"# Question: Customized Rug Plot Visualization with Seaborn You are provided with a dataset containing information about `diamonds`, accessible through Seaborn\'s `load_dataset(\\"diamonds\\")` function. Your task is to create a customized visualization according to the following specifications: 1. Load the `diamonds` dataset and display a scatter plot of `carat` versus `price`. 2. Overlay a rug plot along the x-axis (`carat`) but: - Use different colors to represent different `cuts`. - Adjust the height to `0.03`. - Position the rug plot outside the axes plotting area with a negative height value. - Set the line width (`lw`) to `1` and use an alpha blending of `0.5` to enhance visual clarity. # Requirements - **Input**: No input is required from the user; you will directly use the Seaborn `diamonds` dataset. - **Output**: A scatter plot visual with the mentioned customizations. - **Additional Constraints**: - Make sure the visual representation is clear and aesthetically pleasing. - Follow the specifications closely to adjust height, position, line width, and alpha blending. # Example Implementation Your solution should include the following steps: 1. Load necessary libraries and the `diamonds` dataset. 2. Create a scatter plot. 3. Implement the customized rug plot according to the specifications above. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Set the theme (optional) sns.set_theme() # Create the scatter plot sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5) # Add the customized rug plot sns.rugplot( data=diamonds, x=\\"carat\\", hue=\\"cut\\", height=0.03, clip_on=False, lw=1, alpha=0.5 ) # Display the plot plt.show() ``` # Notes - Ensure that your solution runs without errors and produces the requested visual output. - You may modify the data loading or theme settings as needed but keep the visualization parameters as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_customized_rug_plot(): # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Set the theme (optional) sns.set_theme() # Create the scatter plot scatter_plot = sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5) # Add the customized rug plot rug_plot = sns.rugplot( data=diamonds, x=\\"carat\\", hue=\\"cut\\", height=0.03, clip_on=False, lw=1, alpha=0.5 ) # Display the plot plt.show() return scatter_plot, rug_plot"},{"question":"# Python Call Stack Simulation and Analysis You are tasked with creating a utility to simulate and analyse the call stack of a Python program. Using the information about how DTrace and SystemTap trace function entries and returns, you will develop a Python class that keeps track of function calls in a similar fashion. # Objective Implement a class `CallStack` that effectively simulates the call and return hierarchy of a Python script. Your class should provide mechanisms to log function entries and exits, and generate a formatted representation of the call stack. # Description - Implement a class `CallStack` with the following methods: - `enter_function(filename: str, funcname: str, lineno: int)`: This method should log an entry into a function. - `exit_function(filename: str, funcname: str, lineno: int)`: This method should log the exit from a function. - `current_stack() -> str`: This method should return a string representation of the current function call stack formatted in a readable way. - `reset_stack()`: This method should reset the call stack to its initial state. # Input and Output - `enter_function(filename: str, funcname: str, lineno: int)`: should simply record the entry into a function. It does not return any value. - `exit_function(filename: str, funcname: str, lineno: int)`: should record the return from a function. It does not return any value. - `current_stack() -> str`: should return the current stack trace as a formatted string. - `reset_stack()`: should reset the call stack. It does not return any value. # Example ```python cs = CallStack() cs.enter_function(\\"main.py\\", \\"start\\", 10) cs.enter_function(\\"main.py\\", \\"function_1\\", 15) cs.enter_function(\\"utils.py\\", \\"helper\\", 5) print(cs.current_stack()) cs.exit_function(\\"utils.py\\", \\"helper\\", 6) cs.exit_function(\\"main.py\\", \\"function_1\\", 16) cs.exit_function(\\"main.py\\", \\"start\\", 20) print(cs.current_stack()) ``` Expected output: ``` main.py:start:10 main.py:function_1:15 utils.py:helper:5 ``` ``` <empty> # After all functions have exited, the call stack should be empty. ``` # Constraints 1. Function names and file names are guaranteed to be strings. 2. Line numbers are guaranteed to be positive integers. 3. The enter and exit calls will be correctly nested. Your task is to implement the `CallStack` class according to the above specifications.","solution":"class CallStack: def __init__(self): self.stack = [] self.indentation_level = 0 def enter_function(self, filename: str, funcname: str, lineno: int): self.stack.append((filename, funcname, lineno)) self.indentation_level += 1 def exit_function(self, filename: str, funcname: str, lineno: int): if self.stack and self.stack[-1] == (filename, funcname, lineno): self.stack.pop() self.indentation_level -= 1 def current_stack(self) -> str: if not self.stack: return \\"<empty>\\" result = [] for idx, entry in enumerate(self.stack): indentation = \\" \\" * idx result.append(f\\"{indentation}{entry[0]}:{entry[1]}:{entry[2]}\\") return \\"n\\".join(result) def reset_stack(self): self.stack = [] self.indentation_level = 0"},{"question":"# Python Coding Assessment Question Objective Your goal is to implement a basic text-based notepad application using the `curses` module in Python. The application should allow users to type, view the current mode, and quit the application. Requirements 1. **Initialization**: - Use the `curses.wrapper()` function to initialize the application. 2. **Main Window**: - Create a `stdscr` window to cover the entire screen. - Display a status line at the top, indicating the current mode (e.g., \\"Insert Mode\\" or \\"Command Mode\\"). 3. **Text Input and Mode Switching**: - Allow users to toggle between \\"Insert Mode\\" (where they can type text) and \\"Command Mode\\" (where specific commands can be entered). - In \\"Command Mode,\\" handle specific key bindings for quitting the application and toggling to \\"Insert Mode\\". 4. **Text Display**: - Display the typed text in the window. Use attributes to distinguish the status line (e.g., `A_REVERSE`). 5. **Handling User Input**: - Capture input using `getch()` or `getkey()`. - Implement non-blocking input to update the display efficiently. - Recognize and handle special keys (e.g., arrow keys for navigation). Expected Input and Output - The program should start with a blank screen, displaying the status line with the current mode. - In \\"Insert Mode,\\" text typed by the user should be displayed on the screen. - In \\"Command Mode,\\" specific keys should toggle modes or perform actions (e.g., \'q\' for quit). Constraints and Guidelines - Use the `curses` module functions and methods as described in the documentation. - Ensure proper cleanup of the terminal state using the `curses.wrapper()` function. - Handle exceptions properly to avoid leaving the terminal in an inconsistent state. - Create separate functions for different tasks (e.g., handling input, updating the display). Performance Requirements - Ensure smooth updates to the display with minimal flicker by correctly using `refresh()`, `noutrefresh()`, and `doupdate()`. # Sample Code Structure ```python import curses def draw_status_bar(stdscr, mode): # Draw the status bar with the current mode stdscr.attron(curses.A_REVERSE) stdscr.addstr(0, 0, f\\"Mode: {mode} \\") stdscr.addstr(0, len(f\\"Mode: {mode} \\"), \\" \\" * (curses.COLS - len(f\\"Mode: {mode} \\"))) stdscr.attroff(curses.A_REVERSE) def handle_input(stdscr, mode): # Handle user input and update mode or text ch = stdscr.getch() if mode == \\"Command Mode\\": if ch == ord(\'q\'): return mode, False # Quit the application elif ch == ord(\'i\'): return \\"Insert Mode\\", True elif mode == \\"Insert Mode\\": if ch == 27: # Escape key return \\"Command Mode\\", True else: # Handle text input y, x = stdscr.getyx() stdscr.addch(y, x, ch) stdscr.refresh() return mode, True def main(stdscr): curses.curs_set(True) stdscr.clear() mode = \\"Command Mode\\" while True: draw_status_bar(stdscr, mode) stdscr.refresh() mode, continue_app = handle_input(stdscr, mode) if not continue_app: break if __name__ == \\"__main__\\": curses.wrapper(main) ``` Explanation - **draw_status_bar**: Function to draw the status line with the current mode. - **handle_input**: Function to handle user input and switch modes or add text. - **main**: The main function to initialize the application and handle the main loop.","solution":"import curses def draw_status_bar(stdscr, mode): Draw the status bar with the current mode. stdscr.attron(curses.A_REVERSE) status = f\\"Mode: {mode} \\" stdscr.addstr(0, 0, status) stdscr.addstr(0, len(status), \\" \\" * (curses.COLS - len(status))) stdscr.attroff(curses.A_REVERSE) def handle_input(stdscr, mode): Handle user input and update mode or text. key = stdscr.getch() if mode == \\"Command Mode\\": if key == ord(\'q\'): return mode, False # Quit the application elif key == ord(\'i\'): return \\"Insert Mode\\", True elif mode == \\"Insert Mode\\": if key == 27: # Escape key (ASCII 27) return \\"Command Mode\\", True else: # Handle text input y, x = stdscr.getyx() stdscr.addch(y, x, key) stdscr.refresh() return mode, True def main(stdscr): Main function to initialize the application and handle the main loop. curses.curs_set(True) stdscr.clear() mode = \\"Command Mode\\" while True: draw_status_bar(stdscr, mode) stdscr.refresh() mode, continue_app = handle_input(stdscr, mode) if not continue_app: break if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# Question: Advanced Percentile Plotting with Seaborn Objects You are given the `diamonds` dataset from seaborn, which contains various attributes of diamonds, such as price, cut, color, and clarity. Your task is to create a detailed visualization that meets the following criteria using `seaborn.objects`: 1. Use the `diamonds` dataset. 2. Create a scatter plot that shows the relationship between the diamonds\' price (`price`) and their cut quality (`cut`). 3. Transform the y-axis to a logarithmic scale. 4. Add dots to represent the 10th, 25th, 50th, 75th, and 90th percentiles of the prices for each cut. 5. Overlay a jittered scatter plot for the original data points with point opacity set to 10% and point size to 2. 6. Include vertical lines representing the interquartile range (25th to 75th percentiles) for each cut, shifted slightly upwards for clarity. **Input Format:** - No input required as the diamonds dataset is included in seaborn. **Output Format:** - A visualization (plot) meeting the specified criteria. **Constraints:** - Use only the given seaborn objects methods and functions. - Ensure your plot is clear and well-labeled. **Performance Requirements:** - Ensure the plot is generated efficiently without unnecessary computations. # Example Solution: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the initial plot with logarithmic scaling on the y-axis p = ( so.Plot(diamonds, \\"cut\\", \\"price\\") .scale(y=\\"log\\") ) # Add percentile dots p.add(so.Dot(), so.Perc([10, 25, 50, 75, 90])) # Add jittered scatter plot with specified transparency and size p.add(so.Dots(pointsize=2, alpha=0.1), so.Jitter(.3)) # Add vertical lines for interquartile range, shifted upward for clarity p.add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=.2)) # Display the plot p.show() ``` Ensure your solution follows this template and meets all the criteria outlined in the question. Happy plotting!","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_advanced_percentile(): # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the initial plot with logarithmic scaling on the y-axis p = ( so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") .scale(y=\\"log\\") ) # Add percentile dots p.add(so.Dot(), so.Perc([10, 25, 50, 75, 90])) # Add jittered scatter plot with specified transparency and size p.add(so.Dots(pointsize=2, alpha=0.1), so.Jitter(width=0.3)) # Add vertical lines for interquartile range, shifted upward for clarity p.add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=0.2)) # Display the plot p.show() # Execute the function to generate the plot plot_advanced_percentile()"},{"question":"# Advanced Python Coding Assessment **Objective:** Implement a custom file handling class that wraps the built-in `open` function and adds additional functionality to read and write data in reverse order. **Problem Statement:** You are required to implement a class `ReverseFileHandler` that provides custom file handling methods. This class should internally use the built-in `open` function to work with files but should provide the following functionalities: 1. Read the content of the file in reverse order. 2. Write the content to the file in reverse order. **Class Definition:** ```python class ReverseFileHandler: def __init__(self, file_path: str, mode: str): Initialize the ReverseFileHandler with the file path and the mode. :param file_path: The path to the file. :param mode: The mode in which the file should be opened (\'r\' for read, \'w\' for write). # Your code here def read_reverse(self) -> str: Reads the content of the file and returns it in reverse order. :return: A string representing the content of the file in reverse order. # Your code here def write_reverse(self, content: str): Writes the given content to the file in reverse order. :param content: The content to be written to the file. # Your code here ``` **Input/Output:** - The `__init__` method will take two arguments: `file_path` (str), which is the path to the file, and `mode` (str), which can be either \'r\' for reading or \'w\' for writing. - The `read_reverse` method will read the content of the file and return it as a string in reverse order. - The `write_reverse` method will take a string `content` as an argument and write this content to the file in reverse order. **Constraints:** - The file will always contain ASCII characters. - The file path provided will always exist when opening in read mode. - Ensure that resources are managed properly and the file is closed after reading or writing operations. **Example Usage:** ```python # Example usage for reading a file rfh = ReverseFileHandler(\'example.txt\', \'r\') print(rfh.read_reverse()) # Suppose the content of example.txt is \\"Hello\\" # Output should be \\"olleH\\" # Example usage for writing to a file rfh = ReverseFileHandler(\'output.txt\', \'w\') rfh.write_reverse(\'Goodbye\') # The content of output.txt should now be \\"eybdooG\\" ``` Implement the `ReverseFileHandler` class that fulfills the above specifications.","solution":"class ReverseFileHandler: def __init__(self, file_path: str, mode: str): Initialize the ReverseFileHandler with the file path and the mode. :param file_path: The path to the file. :param mode: The mode in which the file should be opened (\'r\' for read, \'w\' for write). if mode not in (\'r\', \'w\'): raise ValueError(\\"Mode must be \'r\' for read or \'w\' for write\\") self.file_path = file_path self.mode = mode self.file = open(file_path, mode) def read_reverse(self) -> str: Reads the content of the file and returns it in reverse order. :return: A string representing the content of the file in reverse order. try: content = self.file.read() return content[::-1] finally: self.file.close() def write_reverse(self, content: str): Writes the given content to the file in reverse order. :param content: The content to be written to the file. try: reversed_content = content[::-1] self.file.write(reversed_content) finally: self.file.close()"},{"question":"# Nested Tensors in PyTorch In this task, you will be working with nested tensors (NJT) in PyTorch. Nested tensors can handle variable-length data efficiently, which is useful in many practical applications. Problem Statement You are given a batch of sentences, where each sentence is represented as a tensor of word embeddings. However, sentences vary in lengths and padding them to the same length is both inefficient and error-prone. Your task is to use `torch.nested` to handle this batch of sentences and perform various operations. Implement a function `nested_tensor_operations` that performs the following steps: 1. Constructs a nested tensor from a list of given sentences. 2. Converts the nested tensor into a padded tensor with a specified padding value. 3. Adds two nested tensors together and returns their sum. Function Signature ```python import torch def nested_tensor_operations(sentences: list, padding_value: float) -> torch.Tensor: Perform operations on nested tensors. Args: sentences (list): A list of PyTorch tensors, each representing a sentence with word embeddings. padding_value (float): The value to use for padding when converting the nested tensor to a padded tensor. Returns: torch.Tensor: The summed nested tensors converted back to padded tensor. # your implementation here ``` Input - **sentences**: a list of `n` PyTorch tensors of varying lengths. Each tensor has shape `(length, embedding_dim)`, where `length` is the number of words in the sentence and `embedding_dim` is the dimension of the word embeddings. - **padding_value**: a float value used to pad the nested tensor when converting to a padded tensor. Output - Returns a PyTorch tensor representing the summed nested tensors converted back to a padded tensor. Example ```python import torch # Example sentences represented as tensors of word embeddings sentences = [ torch.tensor([[0.1, 0.2], [0.3, 0.4]]), # 2 words, embedding_dim = 2 torch.tensor([[0.5, 0.6], [0.7, 0.8], [0.9, 1.0]]) # 3 words, embedding_dim = 2 ] padding_value = 99.0 # Example padding value # Expected output is a tensor with the sentences padded to the same length output = nested_tensor_operations(sentences, padding_value) print(output) ``` Constraints - The resulting summed tensor should maintain the correct padding and structure. - Ensure efficient use of memory and operations as described in the `torch.nested` documentation. Note: You should expect some additional logic to handle details such as creating the same shape nested tensors before addition.","solution":"import torch def nested_tensor_operations(sentences: list, padding_value: float) -> torch.Tensor: Perform operations on nested tensors. Args: sentences (list): A list of PyTorch tensors, each representing a sentence with word embeddings. padding_value (float): The value to use for padding when converting the nested tensor to a padded tensor. Returns: torch.Tensor: The summed nested tensors converted back to a padded tensor. # Ensure torch is version 1.10 or later for NestedTensor support nested_tensor = torch.nested.nested_tensor(sentences) # Convert the nested tensor into a padded tensor with the specified padding value padded_tensor = nested_tensor.to_padded_tensor(padding_value) # Sum the nested tensor to itself (as an example operation) summed_nested_tensor = nested_tensor + nested_tensor # Convert the summed nested tensor back to a padded tensor summed_padded_tensor = summed_nested_tensor.to_padded_tensor(padding_value) return summed_padded_tensor"},{"question":"**Environment Variable Management and Large File Handling in Python** **Objective:** Implement a function that manages environment variables and reads content from a large file (greater than 2 GiB) using the `os` module. **Problem Statement:** You are to write a function `manage_env_and_read_large_file(path_to_large_file: str, new_env_vars: dict) -> str` that performs the following tasks: 1. **Environment Variables Management:** - Accept a dictionary `new_env_vars` where keys and values are strings representing environment variable names and their respective values. - Update the current environment variables with the provided `new_env_vars`. Ensure the changes are reflected in the current environment. 2. **Large File Handling:** - Read a file located at the path specified by `path_to_large_file`. The file is guaranteed to be larger than 2 GiB. - Return the first 1,024 characters of the file content as a string. If the file content is less than 1,024 characters, return the entire content. **Constraints:** - Do not use any external libraries (e.g., `os` must be used as `import os`). - You can assume the file is plain text and encoded in UTF-8. - Handle any potential `OSError` exceptions that may arise and print a user-friendly message. **Function Signature:** ```python def manage_env_and_read_large_file(path_to_large_file: str, new_env_vars: dict) -> str: ``` **Input:** - `path_to_large_file` (str): The file path to the large file. - `new_env_vars` (dict): A dictionary containing new environment variables to be set. **Output:** - (str): The first 1,024 characters of the file content or the entire content if less than 1,024 characters. **Example:** ```python # Example call result = manage_env_and_read_large_file(\\"/path/to/largefile.txt\\", {\\"NEW_VAR\\": \\"Value\\", \\"ANOTHER_VAR\\": \\"AnotherValue\\"}) # Example output print(result) ``` **Notes:** - Focus on the efficient handling of large files. File reading should be performed in a buffer-sized manner to avoid memory issues. - Ensure that environment variable updates are persistent for the duration of the program run.","solution":"import os def manage_env_and_read_large_file(path_to_large_file: str, new_env_vars: dict) -> str: Updates the environment variables and reads the first 1024 characters from a large file. Arguments: path_to_large_file: str - The path to the large file. new_env_vars: dict - A dictionary containing new environment variables to be set. Returns: str - The first 1024 characters of the file content or the entire content if less than 1024 characters. # Update environment variables for key, value in new_env_vars.items(): os.environ[key] = value # Attempting to read the file try: with open(path_to_large_file, \'r\', encoding=\'utf-8\') as file: return file.read(1024) except OSError as e: print(f\\"Error reading file {path_to_large_file}: {e}\\") return \\"\\""},{"question":"**Objective**: Create a custom logging mechanism that demonstrates an understanding of Loggers, Handlers, Formatters, and Filters as described in the provided documentation. **Problem Statement**: Write a Python script to perform the following tasks: 1. **Custom Logger Creation**: - Create a Logger named `\'customLogger\'` using `logging.getLogger()`. - Set the log level of the logger to `DEBUG`. 2. **Custom Handler**: - Create a `StreamHandler` for the logger that outputs to the console. - Create a `FileHandler` for the logger that writes to a file named `application.log`. 3. **Custom Formatter**: - Create a custom formatter that includes the timestamp, logger name, log level, and the message. The format should be: `\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'`. - Apply this formatter to both the `StreamHandler` and the `FileHandler`. 4. **Custom Filter**: - Implement a custom filter that only allows log messages containing the word `\'IMPORTANT\'`. - Attach this filter to the `FileHandler` only. 5. **Logging Messages**: - Log the following messages at appropriate levels using the `\'customLogger\'`: - `Debug message: Only for debugging.` - `Info message: General information.` - `Warning message: Caution, this might be an issue.` - `Error message: An error has occurred.` - `Critical message: System failure imminent.` - `Important log: IMPORTANT - Attention required!` **Expected Behavior**: - All Log messages should be displayed on the console. - Only messages containing `\'IMPORTANT\'` should be written to `application.log`. **Constraints**: - Use only the `logging` module. - The script should handle any potential exceptions during logging configuration. **Performance**: - The script should be efficient and not unnecessarily repetitive in creating handlers or applying formatters. **Example Output**: - Console Output: ``` 2023-01-01 00:00:00,000 - customLogger - DEBUG - Debug message: Only for debugging. 2023-01-01 00:00:00,001 - customLogger - INFO - Info message: General information. 2023-01-01 00:00:00,002 - customLogger - WARNING - Warning message: Caution, this might be an issue. 2023-01-01 00:00:00,003 - customLogger - ERROR - An error has occurred. 2023-01-01 00:00:00,004 - customLogger - CRITICAL - System failure imminent. 2023-01-01 00:00:00,005 - customLogger - INFO - Important log: IMPORTANT - Attention required! ``` - `application.log` content: ``` 2023-01-01 00:00:00,005 - customLogger - INFO - Important log: IMPORTANT - Attention required! ``` **Submission**: - Submit a `.py` script containing the solution. - Ensure your code is clean, well-commented, and follows Python best practices.","solution":"import logging def create_custom_logger(): # Create a custom logger logger = logging.getLogger(\'customLogger\') logger.setLevel(logging.DEBUG) # Create handlers console_handler = logging.StreamHandler() file_handler = logging.FileHandler(\'application.log\') # Create formatter and set it to the handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Create and add custom filter to the file handler class ImportantFilter(logging.Filter): def filter(self, record): return \'IMPORTANT\' in record.msg file_handler.addFilter(ImportantFilter()) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) return logger # Create logger logger = create_custom_logger() # Log messages logger.debug(\'Debug message: Only for debugging.\') logger.info(\'Info message: General information.\') logger.warning(\'Warning message: Caution, this might be an issue.\') logger.error(\'Error message: An error has occurred.\') logger.critical(\'Critical message: System failure imminent.\') logger.info(\'Important log: IMPORTANT - Attention required!\')"},{"question":"**Data Loader Implementation Using PyTorch** **Objective:** Implement a custom data loader using PyTorch\'s `torch.utils.data` module. This exercise will test your understanding of data handling and preprocessing in PyTorch. **Problem Statement:** You are provided with a dataset of images and their corresponding labels. The dataset is stored in two separate directories: `images/` with all the image files and `labels.txt` containing a list of image filenames along with their labels. You need to implement a custom `Dataset` class and use it in combination with a `DataLoader` to iterate over the dataset efficiently. Your task includes preprocessing the images before loading them into the model. **Requirements:** 1. Implement a custom `Dataset` class. - Class `ImageDataset` should: - Inherit from `torch.utils.data.Dataset`. - Initialize with the paths to the images directory and labels file. - Load the labels into memory. - Implement the `__len__` method to return the number of samples. - Implement the `__getitem__` method to return a preprocessed image and its corresponding label given an index. 2. Use the `DataLoader` to create an iterable over the dataset. - Utilize the `DataLoader` class to batch the data. - Implement image preprocessing steps such as resizing to 224x224 and normalization using mean `[0.485, 0.456, 0.406]` and std `[0.229, 0.224, 0.225]`. **Input:** 1. Directory path to the `images/` directory. 2. Path to the `labels.txt` file. **Output:** - The `DataLoader` iterable that can be used to fetch batches of preprocessed images and their corresponding labels. **Constraints:** - Assume that images are in JPEG format. - The `labels.txt` file contains lines in the format `filename,label`. - The dataset is small enough to fit into memory. **Example usage:** ```python # paths to the images folder and labels file images_dir = \'path/to/images\' labels_file = \'path/to/labels.txt\' # Create the dataset dataset = ImageDataset(images_dir, labels_file) # Create the DataLoader with batch size of 32 dataloader = DataLoader(dataset, batch_size=32, shuffle=True) # Iterate over the DataLoader for batch_idx, (images, labels) in enumerate(dataloader): # Your training code here pass ``` **Implementation:** ```python import os import torch from torch.utils.data import Dataset, DataLoader from torchvision import transforms from PIL import Image class ImageDataset(Dataset): def __init__(self, images_dir, labels_file): self.images_dir = images_dir self.labels = self._load_labels(labels_file) self.transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) def _load_labels(self, labels_file): labels = {} with open(labels_file, \'r\') as file: lines = file.readlines() for line in lines: filename, label = line.strip().split(\',\') labels[filename] = int(label) return labels def __len__(self): return len(self.labels) def __getitem__(self, idx): filename = list(self.labels.keys())[idx] label = self.labels[filename] img_path = os.path.join(self.images_dir, filename) image = Image.open(img_path).convert(\'RGB\') image = self.transform(image) return image, label # Usage images_dir = \'path/to/images\' labels_file = \'path/to/labels.txt\' dataset = ImageDataset(images_dir, labels_file) dataloader = DataLoader(dataset, batch_size=32, shuffle=True) for batch_idx, (images, labels) in enumerate(dataloader): pass # Your training code here ```","solution":"import os import torch from torch.utils.data import Dataset, DataLoader from torchvision import transforms from PIL import Image class ImageDataset(Dataset): def __init__(self, images_dir, labels_file): self.images_dir = images_dir self.labels = self._load_labels(labels_file) self.transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) def _load_labels(self, labels_file): labels = {} with open(labels_file, \'r\') as file: lines = file.readlines() for line in lines: filename, label = line.strip().split(\',\') labels[filename] = int(label) return labels def __len__(self): return len(self.labels) def __getitem__(self, idx): filename = list(self.labels.keys())[idx] label = self.labels[filename] img_path = os.path.join(self.images_dir, filename) image = Image.open(img_path).convert(\'RGB\') image = self.transform(image) return image, label"},{"question":"**Question:** # Loading and Using Datasets with Scikit-learn In this task, you will demonstrate your understanding of the `sklearn.datasets` module and its functionalities. You are required to perform the following steps using scikit-learn: 1. **Load the \\"Iris\\" dataset**: Load the Iris dataset using the appropriate function from `sklearn.datasets`. 2. **Data Preprocessing**: - Split the dataset into a training set and a test set. Use 80% of the data for training and the remaining 20% for testing. 3. **Model Training**: - Train a K-Nearest Neighbors (KNN) classifier using the training set. Use the default parameters for the KNN classifier. 4. **Model Evaluation**: - Evaluate the trained model on the test set and print the accuracy of the model. # Constraints: - You should use `train_test_split` from `sklearn.model_selection`. # Expected Input and Output Format: - The function should not take any input parameters. - The function should print the accuracy of the KNN classifier on the test set. # Example: ```python def load_and_evaluate_iris_dataset(): # Your code here load_and_evaluate_iris_dataset() ``` The expected output will be of the form: ``` Accuracy of the KNN classifier: 0.9667 ``` # Performance Requirements: - Your solution should execute within a reasonable time frame, given that the Iris dataset is relatively small. - Ensure that your code is clear and well-documented. You are expected to demonstrate good programming practices, including appropriate function definitions and usage of scikit-learn functions.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def load_and_evaluate_iris_dataset(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and test sets (80% training, 20% testing) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a K-Nearest Neighbors classifier using the training set knn = KNeighborsClassifier() knn.fit(X_train, y_train) # Evaluate the model on the test set y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Print the accuracy of the model print(f\\"Accuracy of the KNN classifier: {accuracy:.4f}\\")"},{"question":"**Python \\"sys\\" Module Usage Assessment** Implement the function `analyse_sys_info()`. This function should return a dictionary containing detailed information about the currently running Python interpreter and environment. # Function Signature ```python def analyse_sys_info() -> dict: pass ``` # Requirements 1. The function should return a dictionary with the following keys and their corresponding values: - `python_version`: A string representing the Python version (using `sys.version`). - `executable_path`: The absolute path of the Python interpreter executable (using `sys.executable`). - `max_recursion_limit`: The maximum depth of the Python interpreter stack (using `sys.getrecursionlimit`). - `platform`: A string containing a platform identifier (using `sys.platform`). - `byteorder`: A string indicating the native byte order (\'little\' or \'big\') (using `sys.byteorder`). - `module_names`: A sorted list of all built-in module names (using `sys.builtin_module_names`). - `command_line_args`: A list of command line arguments passed to the Python script (using `sys.argv`). - `allocated_blocks`: The number of memory blocks currently allocated by the interpreter (using `sys.getallocatedblocks`). # Constraints - You should not use any third-party libraries. - Ensure that the function works across different platforms and versions of Python 3.6 and above. # Example ```python info = analyse_sys_info() print(info) ``` Expected Output (values will vary based on the environment): ```python { \'python_version\': \'3.9.7 (default, Aug 31 2021, 13:28:12) n[GCC 8.4.0]\', \'executable_path\': \'/usr/bin/python3\', \'max_recursion_limit\': 3000, \'platform\': \'linux\', \'byteorder\': \'little\', \'module_names\': [\'_ast\', \'_bisect\', \'_blake2\', ...], \'command_line_args\': [\'script.py\', \'arg1\', \'arg2\'], \'allocated_blocks\': 56831 } ```","solution":"import sys def analyse_sys_info(): Returns a dictionary containing detailed information about the currently running Python interpreter and environment. return { \'python_version\': sys.version, \'executable_path\': sys.executable, \'max_recursion_limit\': sys.getrecursionlimit(), \'platform\': sys.platform, \'byteorder\': sys.byteorder, \'module_names\': sorted(sys.builtin_module_names), \'command_line_args\': sys.argv, \'allocated_blocks\': sys.getallocatedblocks() if hasattr(sys, \'getallocatedblocks\') else \'Not Applicable\' }"},{"question":"# Task You are provided with a dataset and your goal is to create several kernel density estimation (KDE) plots to visualize and analyze the distributions of the data. You must write a Python function using the seaborn library to accomplish the following: 1. Plot a KDE of the `total_bill` column of the `tips` dataset. 2. Flip the plot so that it displays along the y-axis. 3. Plot KDEs for each column of the `iris` dataset. 4. Use less smoothing on the `total_bill` KDE plot of the `tips` dataset. 5. Use more smoothing on the same plot but ensure it does not smooth past the extreme data points. 6. Plot conditional KDE distributions of `total_bill` based on the `time` column in the `tips` dataset. 7. \\"Stack\\" the conditional distributions shown in step 6. 8. Normalize the stacked distribution at each value in the grid. 9. Show the cumulative distribution function(s) of `total_bill` based on the `time` column, without normalizing across subsets. 10. Estimate the distribution from aggregated data using weights and the `tips` dataset. 11. Plot a KDE of the `price` column of the `diamonds` dataset with log scaling. 12. Utilize numeric hue mapping for the KDE of the `total_bill` column based on the `size` column in the `tips` dataset. 13. Modify the appearance of the KDE of the `total_bill` column based on `size` to include fill, alpha, linewidth, and a specific color palette. 14. Plot a bivariate KDE of the `waiting` and `duration` columns in the `geyser` dataset. 15. Map a third variable (for example, `kind`) to show conditional distributions in the bivariate KDE from step 14. 16. Show filled contours in the above plot. 17. Show fewer contour levels and cover less of the distribution. 18. Fill the axes extent with a smooth distribution using a different colormap. # Function Signature ```python def create_kde_plots(): pass ``` # Expected Output The function should create and display the 18 specified plots. There is no need to return anything from the function. # Constraints - Use the seaborn library to create all the plots. - Ensure proper labeling and titles for each plot for easy differentiation and identification. - Make use of the appropriate seaborn features as demonstrated in the provided documentation examples.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np def create_kde_plots(): # Load datasets tips = sns.load_dataset(\'tips\') iris = sns.load_dataset(\'iris\') diamonds = sns.load_dataset(\'diamonds\') geyser = sns.load_dataset(\'geyser\') # 1. Plot a KDE of the `total_bill` column of the `tips` dataset. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\") plt.title(\'KDE of Total Bill\') plt.show() # 2. Flip the plot so that it displays along the y-axis. plt.figure() sns.kdeplot(data=tips, y=\\"total_bill\\") plt.title(\'KDE of Total Bill (Flipped)\') plt.show() # 3. Plot KDEs for each column of the `iris` dataset. for column in iris.columns[:-1]: plt.figure() sns.kdeplot(data=iris, x=column) plt.title(f\'KDE of {column}\') plt.show() # 4. Use less smoothing on the `total_bill` KDE plot of the `tips` dataset. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", bw_adjust=0.5) plt.title(\'KDE of Total Bill (Less Smoothing)\') plt.show() # 5. Use more smoothing on the same plot but ensure it does not smooth past the extreme data points. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", bw_adjust=2) plt.title(\'KDE of Total Bill (More Smoothing)\') plt.show() # 6. Plot conditional KDE distributions of `total_bill` based on the `time` column in the `tips` dataset. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\") plt.title(\'Conditional KDE of Total Bill by Time\') plt.show() # 7. \\"Stack\\" the conditional distributions shown in step 6. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"stack\\") plt.title(\'Stacked KDE of Total Bill by Time\') plt.show() # 8. Normalize the stacked distribution at each value in the grid. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.title(\'Normalized Stacked KDE of Total Bill by Time\') plt.show() # 9. Show the cumulative distribution function(s) of `total_bill` based on the `time` column, # without normalizing across subsets. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", cumulative=True) plt.title(\'Cumulative KDE of Total Bill by Time\') plt.show() # 10. Estimate the distribution from aggregated data using weights and the `tips` dataset. weights = np.random.random(size=len(tips)) plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", weights=weights) plt.title(\'Weighted KDE of Total Bill\') plt.show() # 11. Plot a KDE of the `price` column of the `diamonds` dataset with log scaling. plt.figure() sns.kdeplot(data=diamonds, x=\\"price\\", log_scale=True) plt.title(\'Log-Scaled KDE of Price\') plt.show() # 12. Utilize numeric hue mapping for the KDE of the `total_bill` column based on the `size` column in the `tips` dataset. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"size\\", palette=\\"viridis\\") plt.title(\'KDE of Total Bill by Size (Numeric Hue)\') plt.show() # 13. Modify the appearance of the KDE of the `total_bill` column based on `size`. plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"size\\", fill=True, alpha=0.5, linewidth=1.5, palette=\\"coolwarm\\") plt.title(\'Customized KDE of Total Bill by Size\') plt.show() # 14. Plot a bivariate KDE of the `waiting` and `duration` columns in the `geyser` dataset. plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\") plt.title(\'Bivariate KDE of Waiting and Duration\') plt.show() # 15. Map a third variable (for example, `kind`) to show conditional distributions in the bivariate KDE from step 14. geyser[\'kind\'] = np.random.choice([\'A\', \'B\'], size=len(geyser)) plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\") plt.title(\'Bivariate KDE of Waiting and Duration by Kind\') plt.show() # 16. Show filled contours in the above plot. plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True) plt.title(\'Filled Bivariate KDE of Waiting and Duration by Kind\') plt.show() # 17. Show fewer contour levels and cover less of the distribution. plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", levels=5, thresh=0.2) plt.title(\'Simplified Bivariate KDE of Waiting and Duration\') plt.show() # 18. Fill the axes extent with a smooth distribution using a different colormap. plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", fill=True, cmap=\\"magma\\") plt.title(\'Smooth Filled Bivariate KDE of Waiting and Duration with Magma Colormap\') plt.show()"},{"question":"Objective: Implement a class that adheres to the `collections.abc.Sequence` interface. Requirements: 1. Define a class `CustomSequence` that should: - Inherit from `collections.abc.Sequence`. - Implement the abstract methods `__getitem__` and `__len__`. - Optionally, other methods such as `__contains__`, `__iter__`, and `__reversed__` can be overridden if desired for performance enhancement. Constraints: - The class should be initialized with a list of elements. - You cannot use Python\'s `list` directly in any part of your `CustomSequence` class except for the underlying storage of elements in the original list. Input: - `CustomSequence` class should be created with an initializer that takes a single list of elements. Output: - The class should support typical sequence operations like indexing, length calculation, and containment checks. Example: ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = data def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) # Optionally override other methods # def __contains__(self, value): # return value in self._data # def __iter__(self): # return iter(self._data) # def __reversed__(self): # return reversed(self._data) # Example usage: seq = CustomSequence([1, 2, 3, 4]) print(seq[1]) # Output: 2 print(len(seq)) # Output: 4 print(3 in seq) # Output: True print(list(reversed(seq))) # Output: [4, 3, 2, 1] ``` Your implementation must: - Ensure the class adheres to the `Sequence` interface. - Include a test case to verify the functionality of the defined class.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = data def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def __contains__(self, value): return value in self._data def __iter__(self): return iter(self._data) def __reversed__(self): return reversed(self._data) # Example usage: seq = CustomSequence([1, 2, 3, 4]) print(seq[1]) # Output: 2 print(len(seq)) # Output: 4 print(3 in seq) # Output: True print(list(reversed(seq))) # Output: [4, 3, 2, 1]"},{"question":"**Coding Assessment Question** # Objective Implement a C extension function for Python that parses different types of arguments from Python, performs some operations, and returns the result. # Task You need to create a Python C extension named `mymodule` with a function `process_data` that takes the following arguments: 1. A string. 2. An integer. 3. A float. 4. An optional dictionary with string keys and integer values. This function should concatenate the string with the integer and the float (formatted as strings), then calculate the sum of all values in the dictionary if provided. Finally, return a tuple containing the concatenated string and the dictionary sum (if the dictionary was provided, otherwise use a default value of 0). # Function Signature ```c static PyObject* process_data(PyObject* self, PyObject* args, PyObject* kwargs); ``` # Input - `args`: A tuple containing the positional arguments (string, integer, and float). - `kwargs`: A dictionary containing the optional keyword arguments. # Output - A tuple containing: - A concatenated string of the input string, integer, and float. - An integer representing the sum of the dictionary values (or 0 if no dictionary is provided). # Example Usage ```python import mymodule # Example call result = mymodule.process_data(\\"Value: \\", 10, 2.5, {\\"a\\": 5, \\"b\\": 15}) print(result) # Output: (\\"Value: 102.5\\", 20) result = mymodule.process_data(\\"Value: \\", 10, 2.5) print(result) # Output: (\\"Value: 102.5\\", 0) ``` # Constraints - The string must not contain embedded NULL characters. - The integer and float must be valid and within the range of their respective types. # Requirements 1. Parse the input arguments using `PyArg_ParseTupleAndKeywords()`. 2. Validate the types and values of the arguments. 3. Perform the required operations (concatenation and summation). 4. Use `Py_BuildValue()` to return the final result to Python. # Hint Use the following format string for parsing arguments: `\\"sif|O!\\"`, and handle optional dictionary arguments accordingly. # Submission Submit the completed C source file for the Python module, including the necessary headers and initialization code.","solution":"# The following Python code is a placeholder for demonstrating the solution concept. # The actual solution requires implementing a Python C extension. def process_data(string, integer, float_value, dictionary=None): Function that concatenates a string with an integer and float, then calculates the sum of dictionary values if provided. Args: string (str): Input string. integer (int): Input integer. float_value (float): Input float. dictionary (dict, optional): Optional dictionary with string keys and integer values. Returns: tuple: A tuple with the concatenated string and sum of dictionary values (or 0). concatenated = f\\"{string}{integer}{float_value}\\" dict_sum = sum(dictionary.values()) if dictionary else 0 return (concatenated, dict_sum)"},{"question":"# Advanced Programming Assignment: Utilizing the `atexit` Module Objective: The objective of this assignment is to evaluate your understanding of the `atexit` module in Python. You\'ll be required to write a Python program that uses this module to manage cleanup tasks upon program termination, ensuring proper execution order and handling of registered functions. Task: You are required to create a class called `Terminator` that utilizes the `atexit` module to register and manage multiple cleanup functions. Additionally, implement functionality to dynamically unregister specific functions. Specifications: 1. **Class Definition**: - Define a class named `Terminator`. 2. **Methods**: - `register_function(func, *args, **kwargs)`: Registers the function `func` with the given arguments to be executed at termination. The function should return a unique identifier for the registered function. - `unregister_function(func_id)`: Unregisters the function associated with the given unique identifier. - `list_registered_functions()`: Returns a list of currently registered functions\' identifiers. 3. **Function Registration**: - Ensure that functions are registered correctly and executed in reverse order of registration. - Functions can be registered with or without arguments. 4. **Function Unregistration**: - Implement the ability to unregister functions using their unique identifier. 5. **Execution Handling**: - Handle exceptions raised during the execution of registered functions and print the traceback. Example Usage: ```python # Define the class Terminator here # Example functions def cleanup_task1(): print(\\"Cleanup Task 1 Executed.\\") def cleanup_task2(message): print(\\"Cleanup Task 2 Executed with message:\\", message) def cleanup_task3(): print(\\"Cleanup Task 3 Executed.\\") # Example usage terminator = Terminator() # Register some functions id1 = terminator.register_function(cleanup_task1) id2 = terminator.register_function(cleanup_task2, \\"Goodbye, World!\\") id3 = terminator.register_function(cleanup_task3) print(\\"Registered Functions:\\", terminator.list_registered_functions()) # Unregister function with id2 terminator.unregister_function(id2) print(\\"Registered Functions after unregistration:\\", terminator.list_registered_functions()) ``` Input and Output Format: - Input: No direct input is required from the user. - Output: Function execution order and custom messages from registered functions should be printed to the console when the program terminates. Constraints: 1. The solution must use the `atexit` module. 2. Functions should be executed in reverse order of registration. 3. Unregistered functions should not execute upon termination. Submission: Submit the `Terminator` class definition along with example functions and usage as demonstrated above.","solution":"import atexit import traceback class Terminator: def __init__(self): self._functions = {} self._id_counter = 0 def _generate_id(self): self._id_counter += 1 return self._id_counter def register_function(self, func, *args, **kwargs): func_id = self._generate_id() entry = (func, args, kwargs) self._functions[func_id] = entry atexit.register(self._wrapped_function, func_id, func, args, kwargs) return func_id def unregister_function(self, func_id): if func_id in self._functions: del self._functions[func_id] atexit.unregister(lambda: self._wrapped_function(func_id, *self._functions[func_id])) def _wrapped_function(self, func_id, func, args, kwargs): try: func(*args, **kwargs) except Exception: tb_str = traceback.format_exc() print(f\\"Exception during execution of function {func_id}:\\") print(tb_str) def list_registered_functions(self): return list(self._functions.keys())"},{"question":"Objective Demonstrate your knowledge of scikit-learn by loading a toy dataset, preprocessing the data, and applying a machine learning model to perform classification. Problem Statement Your task is to load the \'wine\' dataset from scikit-learn, preprocess the data, and apply a Support Vector Machine (SVM) classifier to predict the classes of the wine types. # Requirements: 1. **Load the dataset**: - Use `sklearn.datasets.load_wine()` to load the wine dataset. 2. **Preprocess the data**: - Split the dataset into training and testing sets using an 80-20 split. - Standardize the dataset using `StandardScaler` from `sklearn.preprocessing`. 3. **Train the model**: - Use `SVC` (Support Vector Classifier) from `sklearn.svm` to train a model on the training set. 4. **Evaluate the model**: - Evaluate the classifier\'s performance on the test set using accuracy as the metric. - Print the accuracy of the model on the test set. # Constraints: - You must use scikit-learn for all the required tasks. - Use a random state of `42` for reproducibility when splitting the data. - The SVM classifier should use a linear kernel. # Input: No specific input is required as you will be working with the toy dataset loaded from scikit-learn. # Output: The output should be the printed accuracy of the model on the test set. # Performance Requirement: The implementation should be efficient with respect to both time and memory usage, given the small size of the dataset. # Example Usage: ```python def wine_classification(): # Implement your code here wine_classification() ``` The expected output (example, actual results may vary): ``` Accuracy of the model on the test set: 95.6% ``` Detailed steps are provided within the problem to guide you through loading, preprocessing, training, and evaluating the model.","solution":"import numpy as np from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score def wine_classification(): # Load the wine dataset wine_data = load_wine() X = wine_data.data y = wine_data.target # Split the dataset into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the dataset scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the SVM classifier with a linear kernel svm_classifier = SVC(kernel=\'linear\', random_state=42) svm_classifier.fit(X_train_scaled, y_train) # Predict the classes for the test set y_pred = svm_classifier.predict(X_test_scaled) # Evaluate the classifier\'s performance accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy of the model on the test set: {accuracy*100:.1f}%\\") # Run the function to see the output wine_classification()"},{"question":"# Distributed Data Parallelism in PyTorch You are required to write a function that sets up a distributed environment for training a neural network model using PyTorch\'s `torch.distributed` module. Your function will involve initializing the process group, performing a collective operation, and then cleaning up the resources. **Function Signature:** ```python def distributed_training_example(rank: int, world_size: int, backend: str = \'gloo\'): Set up a distributed environment, execute a collective operation, and clean up. Args: - rank (int): The rank of the current process. - world_size (int): Total number of processes participating in the job. - backend (str): The backend to be used for distributed operations. Default is \'gloo\'. Returns: - result (torch.Tensor): The result after the collective all_reduce operation. pass ``` **Implementation Requirements:** 1. Initialize the process group using `torch.distributed.init_process_group`. 2. Create a tensor containing the rank value and perform an `all_reduce` operation with the `SUM` reduction. 3. Clean up by destroying the process group using `torch.distributed.destroy_process_group`. 4. Return the tensor resulting from the `all_reduce` operation. **Constraints:** - The world size will always be greater than 1. - Ensure that you are not hardcoding the initialization method for the process group to make it more general. **Example:** ```python if __name__ == \\"__main__\\": import os from torch.multiprocessing import spawn def init_process(rank, world_size): result = distributed_training_example(rank, world_size) print(f\\"Rank {rank} result: {result}\\") world_size = 4 spawn(init_process, args=(world_size,), nprocs=world_size) ``` In this example, if the world size is 4, the output after the `all_reduce` operation should be a tensor with the sum of ranks. Each process should output its result tensor after the collective operation.","solution":"import torch import torch.distributed as dist import os def distributed_training_example(rank: int, world_size: int, backend: str = \'gloo\'): Set up a distributed environment, execute a collective operation, and clean up. Args: - rank (int): The rank of the current process. - world_size (int): Total number of processes participating in the job. - backend (str): The backend to be used for distributed operations. Default is \'gloo\'. Returns: - result (torch.Tensor): The result after the collective all_reduce operation. # Initialize the distributed environment. os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(backend, rank=rank, world_size=world_size) # Create a tensor containing the rank value. tensor = torch.tensor([rank], dtype=torch.float32) # Perform an all_reduce operation with SUM reduction. dist.all_reduce(tensor, op=dist.ReduceOp.SUM) # Clean up by destroying the process group. dist.destroy_process_group() return tensor"},{"question":"You are required to implement a custom iterator using generators and demonstrate your understanding by using several advanced Python features such as `itertools` and `functools`. Problem Statement Create a class `CustomIterator` that takes a list of numbers as input and performs the following tasks: 1. **Initialization:** - Initialize the iterator with a list of numbers. 2. **Implement Generator:** - Define a generator method `even_numbers` to iterate over the list and yield only the even numbers. - Define a generator method `odd_numbers_with_send` to iterate over the list and yield only the odd numbers. This generator should allow external modification of its state using the `send` method. 3. **Using itertools and functools:** - Create a method `alternate_sum` that uses a combination of `itertools` and `functools` to compute the alternating sum (sum of first element minus the second element plus the third element, and so on) of the numbers in the list. # Constraints and Requirements: - The list will contain at most 10^5 numbers. - The numbers in the list will be non-negative integers. # Function Signatures: ```python class CustomIterator: def __init__(self, numbers: list): pass def even_numbers(self): pass def odd_numbers_with_send(self): pass def alternate_sum(self) -> int: pass ``` # Input and Output Formats - **Input:** - A list of non-negative integers. - **Output:** - `even_numbers` should yield even numbers in the list. - `odd_numbers_with_send` should yield odd numbers and allow modification of its state using the `send` method. - `alternate_sum` should return an integer representing the alternating sum of the numbers in the list. # Example Usage ```python # Initialize the iterator iterator = CustomIterator([1, 2, 3, 4, 5, 6]) # Generate even numbers print(list(iterator.even_numbers())) # Output: [2, 4, 6] # Generate and modify odd numbers odd_gen = iterator.odd_numbers_with_send() print(next(odd_gen)) # Output: 1 print(odd_gen.send(9)) # Output: 9 (state changed by send) # Compute alternating sum print(iterator.alternate_sum()) # Output: 1 - 2 + 3 - 4 + 5 - 6 = -3 ``` Notes: - Use generators effectively to handle iterating over the list. - Utilize `itertools` and `functools` where necessary for concise and efficient code.","solution":"from itertools import cycle from functools import reduce class CustomIterator: def __init__(self, numbers: list): self.numbers = numbers def even_numbers(self): for num in self.numbers: if num % 2 == 0: yield num def odd_numbers_with_send(self): for num in self.numbers: if num % 2 != 0: new_num = (yield num) if new_num is not None: yield new_num - num def alternate_sum(self) -> int: alternator = cycle([1, -1]) return reduce(lambda acc, x: acc + x * next(alternator), self.numbers, 0)"},{"question":"Objective: Write a Python function that loads the breast cancer dataset from scikit-learn, preprocesses the data, builds a classification model using logistic regression, evaluates the model using cross-validation, and returns the average accuracy score. Problem Statement: You are required to implement a function `breast_cancer_classification()` that performs the following tasks: 1. Load the breast cancer dataset using `sklearn.datasets.load_breast_cancer()`. 2. Preprocess the data by scaling the features using `StandardScaler` from `sklearn.preprocessing`. 3. Split the data into training and testing sets using an 80-20 split. 4. Train a logistic regression model (`LogisticRegression` from `sklearn.linear_model`). 5. Evaluate the model using 5-fold cross-validation and return the average accuracy score. Function Signature: ```python def breast_cancer_classification() -> float: pass ``` Constraints: - Use a random state value of 42 for splitting the data. - Use default parameters for the logistic regression model. - Ensure reproducibility by setting the appropriate random state values. Example Input/Output: ``` Output: A float value representing the average accuracy score from the 5-fold cross-validation. Example: avg_accuracy = breast_cancer_classification() print(avg_accuracy) # This should print the average accuracy score ``` Notes: - This question assesses your ability to utilize scikit-learn for a complete machine learning pipeline: loading data, preprocessing, model training, and evaluation. - Pay attention to the required preprocessing steps and ensure data is properly scaled before model training. - Use appropriate functions and classes from scikit-learn for splitting, scaling, model training, and evaluation. - Ensure your function runs efficiently and meets the performance requirements.","solution":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression import numpy as np def breast_cancer_classification() -> float: # Load the breast cancer dataset data = load_breast_cancer() X, y = data.data, data.target # Preprocess the data by scaling the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Train a logistic regression model model = LogisticRegression(random_state=42, solver=\'liblinear\') # Evaluate the model using 5-fold cross-validation scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\'accuracy\') # Return the average accuracy score avg_accuracy = np.mean(scores) return avg_accuracy"},{"question":"# Custom Dictionary Operations You are required to implement a set of functions that simulate some of the key operations on a dictionary using the python310 package. Each function will utilize a specific dictionary-related function from the python310 package to perform its operation. Functions to Implement 1. **create_dict**: - **Input**: None - **Output**: A new dictionary object. - **Description**: Create and return a new empty dictionary. 2. **add_item**: - **Input**: A dictionary `d`, a key `k`, and a value `v`. - **Output**: Boolean indicating success (`True`) or failure (`False`). - **Description**: Add the key-value pair `(k, v)` to the dictionary `d`. 3. **delete_item**: - **Input**: A dictionary `d` and a key `k`. - **Output**: Boolean indicating success (`True`) or failure (`False`). - **Description**: Delete the item with the key `k` from the dictionary `d`. 4. **check_key**: - **Input**: A dictionary `d` and a key `k`. - **Output**: Boolean indicating the presence (`True`) or absence (`False`) of the key `k` in the dictionary `d`. - **Description**: Check if the key `k` exists in the dictionary `d`. 5. **fetch_item**: - **Input**: A dictionary `d` and a key `k`. - **Output**: The value associated with the key `k` in the dictionary `d` or `None` if the key does not exist. - **Description**: Fetch the value for the key `k` from the dictionary `d`. 6. **merge_dicts**: - **Input**: Two dictionaries `a` and `b`. - **Output**: Boolean indicating success (`True`) or failure (`False`). - **Description**: Merge the dictionary `b` into the dictionary `a`. If they share keys, entries from dictionary `b` should overwrite those in dictionary `a`. 7. **iterate_dict**: - **Input**: A dictionary `d`. - **Output**: A list of tuples where each tuple contains a key and its corresponding value. - **Description**: Iterate over all items in the dictionary `d` and return a list of key-value pairs. Constraints and Notes - You must use the respective functions from the `python310` package to perform the operations. - Any Python `Exception` during the operation should be properly handled, and False should be returned. - Performance considerations: Assume the dictionary size will not exceed 100,000 items, so operations should handle this efficiently. Here is the Python template code where you define these functions: ```python # Assume that all the necessary imports from the python310 package have been done here def create_dict(): try: return PyDict_New() except Exception as e: return None def add_item(d, k, v): try: return PyDict_SetItem(d, k, v) == 0 except Exception as e: return False def delete_item(d, k): try: return PyDict_DelItem(d, k) == 0 except Exception as e: return False def check_key(d, k): try: return PyDict_Contains(d, k) == 1 except Exception as e: return False def fetch_item(d, k): try: return PyDict_GetItem(d, k) # This might return None without setting an exception except Exception as e: return None def merge_dicts(a, b): try: return PyDict_Update(a, b) == 0 except Exception as e: return False def iterate_dict(d): try: result = [] pos = 0 key = PyObject() value = PyObject() while PyDict_Next(d, &pos, &key, &value): result.append((key, value)) return result except Exception as e: return [] ``` Use this template to implement the required functions and ensure they perform as described. Write test cases to validate each function\'s behavior against different scenarios.","solution":"def create_dict(): Create and return a new empty dictionary. return {} def add_item(d, k, v): Add the key-value pair (k, v) to the dictionary d. Return True if successful, False if an exception occurs. try: d[k] = v return True except Exception: return False def delete_item(d, k): Delete the item with the key k from the dictionary d. Return True if successful, False if an exception occurs. try: del d[k] return True except KeyError: return False except Exception: return False def check_key(d, k): Check if the key k exists in the dictionary d. Return True if the key exists, False otherwise. return k in d def fetch_item(d, k): Fetch the value for the key k from the dictionary d. Return the value if the key exists, None otherwise. return d.get(k, None) def merge_dicts(a, b): Merge the dictionary b into the dictionary a. Return True if successful, False if an exception occurs. try: a.update(b) return True except Exception: return False def iterate_dict(d): Iterate over all items in the dictionary d and return a list of key-value pairs. try: return list(d.items()) except Exception: return []"},{"question":"**Context Variables - Managing Multiple Contexts** You are required to implement a Python function that simulates a scenario where multiple contexts and context variables are managed concurrently. This involves creating contexts, setting and retrieving variable values, and ensuring the state is correctly preserved and restored across different contexts. # Requirements 1. **Function Signature**: ```python def manage_contexts(context_values: list, variables: dict) -> dict: ``` 2. **Input**: - `context_values`: A list of dictionaries where each dictionary represents a set of variables and their values to be set in a new context. For example: ```python [ {\'var1\': \'value1\', \'var2\': \'value2\'}, {\'var1\': \'value3\', \'var3\': \'value4\'} ] ``` - `variables`: A dictionary where keys are variable names and values are their default values. For example: ```python {\'var1\': \'default1\', \'var2\': \'default2\', \'var3\': \'default3\'} ``` 3. **Output**: - A dictionary mapping each context index to another dictionary representing the final values of context variables after all operations. For example: ```python { 0: {\'var1\': \'value1\', \'var2\': \'value2\', \'var3\': \'default3\'}, 1: {\'var1\': \'value3\', \'var2\': \'default2\', \'var3\': \'value4\'} } ``` 4. **Constraints**: - If a variable is not set in a context, it should take its default value. - If an error occurs during context variable management (setting, getting, or resetting), it should be handled gracefully and logged. - You must use the context variables mechanisms provided by the contextvars module. # Implementation Details - Use `contextvars.ContextVar` to create context variables. - Use `contextvars.Context` to create and manage different contexts. - Ensure each context is correctly activated and deactivated. - Document the code for clarity and maintainability. # Example Function Call ```python context_values = [ {\'var1\': \'value1\', \'var2\': \'value2\'}, {\'var1\': \'value3\', \'var3\': \'value4\'} ] variables = {\'var1\': \'default1\', \'var2\': \'default2\', \'var3\': \'default3\'} result = manage_contexts(context_values, variables) ``` # Expected Output ```python { 0: {\'var1\': \'value1\', \'var2\': \'value2\', \'var3\': \'default3\'}, 1: {\'var1\': \'value3\', \'var2\': \'default2\', \'var3\': \'value4\'} } ``` # Notes - Ensure to test the function with different input scenarios to cover all edge cases. - Provide meaningful error messages and handle exceptions appropriately.","solution":"import contextvars from contextlib import contextmanager def manage_contexts(context_values, variables): contexts = [contextvars.Context() for _ in context_values] context_vars = {key: contextvars.ContextVar(key, default=value) for key, value in variables.items()} @contextmanager def run_in_context(context): token_map = {} try: for var_name, var_value in context.items(): token = context_vars[var_name].set(var_value) token_map[var_name] = token yield finally: for var_name, token in token_map.items(): context_vars[var_name].reset(token) result = {} for index, context in enumerate(context_values): final_values = {key: context_vars[key].get() for key in variables.keys()} with run_in_context(context): for key in context_vars.keys(): final_values[key] = context_vars[key].get() result[index] = final_values return result"},{"question":"Objective: Implement a custom neural network in PyTorch from scratch, including a forward pass, backpropagation, and optimization steps. This exercise will assess your understanding of neural network layers, tensor operations, and the optimization process in PyTorch. Task: 1. Implement a custom neural network module named `MyNeuralNetwork` with the following architecture: - An input layer that takes tensors of size `(batch_size, 784)`, where 784 is the flattened size of 28x28 images (e.g., MNIST dataset). - A hidden layer with 128 neurons and a ReLU activation function. - An output layer with 10 neurons and a log-softmax activation function. 2. Implement the forward pass method. 3. Implement a training function that: - Takes the neural network, loss function, and optimizer as arguments. - Trains the network on the provided `DataLoader` for one epoch. - Prints the average training loss after the epoch. Input/Output: - **Input:** - `train_loader`: an instance of `torch.utils.data.DataLoader` containing the training data. - `lr`: learning rate for the optimizer (a float value). - **Output:** - The function should print the average loss after training for one epoch. Constraints: - You should use PyTorch\'s in-built functions and classes where appropriate. - Ensure the training loop performs both forward and backward passes correctly. - Initialize the weights of the layers appropriately for optimal training performance. Code Template: ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import DataLoader class MyNeuralNetwork(nn.Module): def __init__(self): super(MyNeuralNetwork, self).__init__() # Define layers self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): # Flatten the input tensor x = x.view(-1, 784) # Forward pass x = F.relu(self.fc1(x)) x = F.log_softmax(self.fc2(x), dim=1) return x def train_network(train_loader, lr): # Initialize the network model = MyNeuralNetwork() # Define loss function and optimizer criterion = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=lr) # Training loop model.train() running_loss = 0.0 for images, labels in train_loader: # Zero the parameter gradients optimizer.zero_grad() # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward pass and optimize loss.backward() optimizer.step() # Accumulate loss running_loss += loss.item() # Print average loss average_loss = running_loss / len(train_loader) print(f\'Average training loss: {average_loss:.4f}\') # Example usage (you need to define \'train_loader\' properly before running): # train_loader = ... # train_network(train_loader, lr=0.01) ``` Make sure your code is efficient and follows PyTorch best practices.","solution":"import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import DataLoader class MyNeuralNetwork(nn.Module): def __init__(self): super(MyNeuralNetwork, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 784) # Flatten the input tensor x = F.relu(self.fc1(x)) # Hidden layer with ReLU activation x = F.log_softmax(self.fc2(x), dim=1) # Output layer with log-softmax activation return x def train_network(train_loader, lr): model = MyNeuralNetwork() criterion = nn.NLLLoss() optimizer = optim.SGD(model.parameters(), lr=lr) model.train() running_loss = 0.0 for images, labels in train_loader: optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() average_loss = running_loss / len(train_loader) print(f\'Average training loss: {average_loss:.4f}\')"},{"question":"**Problem Statement** You are provided with a dataset consisting of numerical features and a categorical target. Your task is to build a data preprocessing and feature extraction pipeline using scikit-learn transformers. The pipeline should include the following steps: 1. **Normalization**: Standardize the numerical features. 2. **Dimensionality Reduction**: Reduce the dimensionality of the standardized features using PCA. 3. **Feature Generation**: Use PolynomialFeatures to generate polynomial and interaction features. 4. **Combining Features**: Combine the original numerical features and the newly generated polynomial features. 5. **Encoding Target**: Encode the categorical target using LabelEncoder. Write a function `build_pipeline(data, target)` that takes as input a pandas DataFrame `data` containing numerical features and a pandas Series `target` containing a categorical target. The function should return a transformed DataFrame of features and a transformed Series of the target. # Input - `data`: A pandas DataFrame of shape (n_samples, n_features) containing numerical features. - `target`: A pandas Series of shape (n_samples,) containing categorical labels. # Output - `transformed_data`: A pandas DataFrame of shape (n_samples, m_features) containing the transformed features. - `transformed_target`: A pandas Series of shape (n_samples,) containing the encoded target labels. # Constraints - You must use scikit-learn transformers for each step. - Assume `data` and `target` are non-empty. - You may assume that `target` contains only valid categorical values. # Example ```python import pandas as pd data = pd.DataFrame({ \'feature1\': [1.0, 2.0, 3.0], \'feature2\': [4.0, 5.0, 6.0] }) target = pd.Series([\'class1\', \'class2\', \'class1\']) transformed_data, transformed_target = build_pipeline(data, target) print(transformed_data) print(transformed_target) ``` # Requirements - Your implementation should demonstrate the use of fit, transform, and fit_transform methods from the scikit-learn transformers. - Ensure the final output data and target are in the appropriate shapes and formats. # Additional Notes You could use the following modules for your transformations from scikit-learn: - `sklearn.preprocessing.StandardScaler` - `sklearn.decomposition.PCA` - `sklearn.preprocessing.PolynomialFeatures` - `sklearn.preprocessing.LabelEncoder`","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder from sklearn.decomposition import PCA def build_pipeline(data, target, n_components=2, degree=2): Processes the input data and target by scalization, dimensionality reduction, polynomial feature generation, and target encoding. Parameters: - data: A pandas DataFrame containing numerical features. - target: A pandas Series containing categorical labels. - n_components: Number of components to keep after PCA. - degree: The degree of the polynomial features to be generated. Returns: - transformed_data: A pandas DataFrame of transformed features. - transformed_target: A pandas Series of encoded target labels. # Step 1: Normalize the numerical features using StandardScaler scaler = StandardScaler() scaled_data = scaler.fit_transform(data) # Step 2: Reduce the dimensionality using PCA pca = PCA(n_components=n_components) pca_data = pca.fit_transform(scaled_data) # Step 3: Generate polynomial features poly = PolynomialFeatures(degree=degree) poly_data = poly.fit_transform(scaled_data) # Step 4: Combine the PCA features and the original polynomial features combined_data = pd.DataFrame(data = poly_data, columns = poly.get_feature_names_out(data.columns)) # Step 5: Encode the target column using LabelEncoder encoder = LabelEncoder() transformed_target = pd.Series(encoder.fit_transform(target), name=\'target\') return combined_data, transformed_target"},{"question":"# Pandas Coding Assessment Objective Your task is to perform several data manipulations and analyses on a given dataset using pandas. These operations will include reshaping data, handling missing values, and computing statistics. Problem Statement You are provided with a dataset containing information about sales transactions. The dataset has the following columns: - `TransactionID`: Unique identifier for each transaction - `ProductID`: Unique identifier for each product - `UserID`: Unique identifier for each user - `Quantity`: Number of the product sold in the transaction - `TransactionDate`: Date of the transaction - `Price`: Price per unit of the product Here is a sample of the dataset: ``` TransactionID, ProductID, UserID, Quantity, TransactionDate, Price 1, 101, 501, 2, 2023-01-15, 20.00 2, 102, 502, 1, 2023-01-17, 40.00 3, 101, 503, 3, 2023-01-20, 20.00 4, 103, 501, 1, 2023-01-25, 60.00 ... ``` Tasks 1. **Load the dataset into a pandas DataFrame.** Assume the dataset is in a CSV file named `sales_data.csv`. 2. **Handle Missing Values:** - Check for any missing values in the dataset. If there are any, fill them as follows: - For numeric columns, fill with the mean of that column. - For datetime columns, fill with the earliest date in the column. 3. **Calculate Total Sales:** - Add a new column `TotalSales` that is the product of `Quantity` and `Price`. 4. **Reshape the Data:** - Create a pivot table that shows the total sales (`TotalSales`) by `ProductID` and `TransactionDate`. 5. **Analyze User Purchase Patterns:** - Find the `UserID` with the highest total purchase amount. 6. **Date Range Analysis:** - Create a date range from the earliest to the latest `TransactionDate` in the dataset. Generate a DataFrame that includes this date range and fills in any missing dates with zeros for `Quantity` and `TotalSales`. Input - CSV file: `sales_data.csv` Output - A pivot table showing total sales by `ProductID` and `TransactionDate`. - The `UserID` with the highest total purchase amount. - A DataFrame with a continuous date range, filling missing dates with zeros for `Quantity` and `TotalSales`. Constraints - You must use pandas for all data manipulations and analyses. - Ensure your code is efficient and can handle large datasets. --- Consider the above requirements and write functions to perform each task. Your functions should take in appropriate parameters and return the necessary outputs as described.","solution":"import pandas as pd def load_dataset(file_path): Load the dataset from the given CSV file path. return pd.read_csv(file_path) def handle_missing_values(df): Handle missing values in the dataframe. for column in df.select_dtypes(include=[\'float64\', \'int64\']).columns: df[column].fillna(df[column].mean(), inplace=True) for column in df.select_dtypes(include=[\'datetime64\']).columns: df[column].fillna(df[column].min(), inplace=True) return df def calculate_total_sales(df): Calculate total sales and add it as a new column in the dataframe. df[\'TotalSales\'] = df[\'Quantity\'] * df[\'Price\'] return df def create_pivot_table(df): Create a pivot table showing total sales by ProductID and TransactionDate. pivot_table = df.pivot_table(values=\'TotalSales\', index=\'TransactionDate\', columns=\'ProductID\', aggfunc=\'sum\').fillna(0) return pivot_table def user_with_highest_purchase(df): Find the UserID with the highest total purchase amount. user_total_sales = df.groupby(\'UserID\')[\'TotalSales\'].sum() highest_user = user_total_sales.idxmax() return highest_user def date_range_analysis(df): Create a dataframe with a continuous date range and fill missing dates with zeros for Quantity and TotalSales. df[\'TransactionDate\'] = pd.to_datetime(df[\'TransactionDate\']) date_range = pd.date_range(start=df[\'TransactionDate\'].min(), end=df[\'TransactionDate\'].max()) df_full = pd.DataFrame(date_range, columns=[\'TransactionDate\']) df = df.groupby(\'TransactionDate\').sum().reset_index() df_full = df_full.merge(df, on=\'TransactionDate\', how=\'left\').fillna({\'Quantity\': 0, \'TotalSales\': 0}) return df_full"},{"question":"You are required to implement a program that reads the attributes of a file and modifies some of its properties using Unix-specific system calls. The program must be capable of: 1. Retrieving the current size, permissions, and owner of a specified file. 2. Changing the file\'s permissions and truncating its size to a given length. 3. Logging each operation performed using the `syslog` module. # Implementation Details 1. **Function: `get_file_info(filepath: str) -> dict`** - **Input**: - `filepath`: A string representing the path to the file. - **Output**: - Returns a dictionary containing the file\'s size in bytes, permissions in octal format, and owner\'s user ID. - **Example**: ```python get_file_info(\'/path/to/file\') # Returns: {\'size\': 1024, \'permissions\': \'0755\', \'owner\': 1000} ``` 2. **Function: `modify_file(filepath: str, permissions: str, size: int) -> None`** - **Input**: - `filepath`: A string representing the path to the file. - `permissions`: A string representing the new file permissions in octal format (e.g., \'0644\'). - `size`: An integer representing the new size to truncate the file to. - **Operation**: - Change the file\'s permissions to the specified value. - Truncate the file size to the provided value. - **Output**: - None - **Example**: ```python modify_file(\'/path/to/file\', \'0644\', 512) ``` 3. **Function: `log_operation(operation: str) -> None`** - **Input**: - `operation`: A string detailing the operation performed (e.g., \\"Read file info\\", \\"Changed permissions\\"). - **Operation**: - Log the provided operation string to Unix syslog. - **Output**: - None - **Example**: ```python log_operation(\\"Read file info\\") ``` # Constraints - The program should handle exceptions gracefully and log any errors using the `syslog` module. - Assume the file paths provided are always valid and accessible. - Permissions should be given in the correct octal format. - You can assume the `syslog` module is already imported and the syslog facility is configured correctly. # Performance Requirements - Your program should efficiently handle file operations even for large files. - Logging operations should not significantly affect the performance of the file system operations. ```python # You can start your implementation with these function signatures. import os import pwd import grp import fcntl import syslog def get_file_info(filepath: str) -> dict: # Implement the function pass def modify_file(filepath: str, permissions: str, size: int) -> None: # Implement the function pass def log_operation(operation: str) -> None: # Implement the function pass ``` # Example Usage ```python file_info = get_file_info(\'/path/to/file\') print(file_info) modify_file(\'/path/to/file\', \'0644\', 512) log_operation(\\"Changed file permissions and truncated size.\\") ```","solution":"import os import pwd import syslog def get_file_info(filepath: str) -> dict: Retrieves the size, permissions, and owner of the specified file. try: file_stat = os.stat(filepath) file_info = { \'size\': file_stat.st_size, \'permissions\': f\'{oct(file_stat.st_mode)[-3:]}\', \'owner\': file_stat.st_uid } log_operation(f\\"Retrieved file info for {filepath}\\") return file_info except Exception as e: log_operation(f\\"Error retrieving file info for {filepath}: {e}\\") raise def modify_file(filepath: str, permissions: str, size: int) -> None: Changes the file\'s permissions and truncates its size. try: # Change file permissions os.chmod(filepath, int(permissions, 8)) log_operation(f\\"Changed permissions to {permissions} for {filepath}\\") # Truncate file size with open(filepath, \'a\') as file: file.truncate(size) log_operation(f\\"Truncated size to {size} for {filepath}\\") except Exception as e: log_operation(f\\"Error modifying file {filepath}: {e}\\") raise def log_operation(operation: str) -> None: Logs the provided operation to Unix syslog. try: syslog.syslog(syslog.LOG_INFO, operation) except Exception as e: raise RuntimeError(f\\"Error logging operation: {e}\\")"},{"question":"Using the Seaborn library in Python, complete the following tasks: 1. Generate a dataset using the `seaborn` library\'s in-built `penguins` dataset. 2. Create a color palette using `husl_palette` with 10 colors. 3. Adjust the palette to have a lightness of 0.5 and a saturation of 0.6. 4. Generate a scatter plot using the `seaborn` library. Map the points\' colors to the newly created palette, based on the categorical variable \'species\'. 5. Ensure the plot has a title, and appropriate labels for the x and y axes. # Input: - The function does not require any direct input. # Output: - The function should display the plot directly within the Jupyter Notebook without returning any value. # Constraints: - You must use the `seaborn` library to manipulate the palette and create the plot. # Performance Requirements: - The function should execute within a few seconds given the constraints of typical Jupyter Notebook performance. Example function signature: ```python def seaborn_scatter_penguins(): pass ``` # Hints: 1. You can load the penguins dataset using `sns.load_dataset(\\"penguins\\")`. 2. Use `sns.scatterplot` to create the scatter plot. 3. Use the `palette` parameter in `sns.scatterplot` to set the colors.","solution":"import seaborn as sns import matplotlib.pyplot as plt def seaborn_scatter_penguins(): Generates a scatter plot from the Seaborn penguins dataset. The plot uses a custom HUSL color palette with adjusted lightness and saturation. # Step 1: Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Create a color palette using husl_palette with 10 colors base_palette = sns.color_palette(\\"husl\\", 10) # Step 3: Adjust the palette to have a lightness of 0.5 and a saturation of 0.6 custom_palette = sns.husl_palette(len(penguins[\'species\'].unique()), l=0.5, s=0.6) # Step 4: Generate a scatter plot sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", palette=custom_palette) # Adding title and labels plt.title(\'Penguins: Flipper Length vs Body Mass\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') # Display the plot plt.show()"},{"question":"**Objective**: Demonstrate your understanding of the `xmlrpc.client` module by creating an XML-RPC client that interacts with a remote server, performs multiple method calls in a single request, and handles possible errors. **Question**: You are given an XML-RPC server running at `http://localhost:8000/` which supports the following methods: - `add(x, y)`: Adds two integers. - `subtract(x, y)`: Subtracts `y` from `x`. - `multiply(x, y)`: Multiplies two integers. - `divide(x, y)`: Divides `x` by `y` and returns the integer result (integer division). Write a Python script using the `xmlrpc.client` module that does the following: 1. Connect to the XML-RPC server at `http://localhost:8000/`. 2. Use the `MultiCall` functionality to: - Add 10 and 5. - Subtract 7 from 10. - Multiply 3 and 4. - Divide 20 by 5. 3. Execute the batched request and print the results for each operation. 4. Handle any potential errors gracefully by catching `ProtocolError` and `Fault` exceptions, printing appropriate error messages. **Constraints**: - Do not change the server\'s code. - The server always returns valid results unless a protocol error or a fault is deliberately triggered (you don’t need to simulate server errors). **Expected Output**: ``` Addition result: 15 Subtraction result: 3 Multiplication result: 12 Division result: 4 ``` If an error occurs, the output should look like: ``` An error occurred: [Error details] ``` **Sample Code Structure**: ```python import xmlrpc.client try: # Connect to the server proxy = xmlrpc.client.ServerProxy(\\"http://localhost:8000/\\") # Create a MultiCall object multicall = xmlrpc.client.MultiCall(proxy) # Add calls to the MultiCall object multicall.add(10, 5) multicall.subtract(10, 7) multicall.multiply(3, 4) multicall.divide(20, 5) # Execute the MultiCall result = multicall() # Print results for r in result: print(r) except xmlrpc.client.ProtocolError as err: print(\\"A protocol error occurred\\") print(\\"URL: %s\\" % err.url) print(\\"HTTP/HTTPS headers: %s\\" % err.headers) print(\\"Error code: %d\\" % err.errcode) print(\\"Error message: %s\\" % err.errmsg) except xmlrpc.client.Fault as err: print(\\"A fault occurred\\") print(\\"Fault code: %d\\" % err.faultCode) print(\\"Fault string: %s\\" % err.faultString) ``` Your implementation should follow this structure, ensuring all steps and error handling are covered.","solution":"import xmlrpc.client def perform_multicall_operations(): try: # Connect to the server proxy = xmlrpc.client.ServerProxy(\\"http://localhost:8000/\\") # Create a MultiCall object multicall = xmlrpc.client.MultiCall(proxy) # Add calls to the MultiCall object multicall.add(10, 5) multicall.subtract(10, 7) multicall.multiply(3, 4) multicall.divide(20, 5) # Execute the MultiCall result = multicall() # Collect results results = [] for r in result: results.append(r) return results except xmlrpc.client.ProtocolError as err: return f\\"A protocol error occurred: URL: {err.url}, HTTP/HTTPS headers: {err.headers}, Error code: {err.errcode}, Error message: {err.errmsg}\\" except xmlrpc.client.Fault as err: return f\\"A fault occurred: Fault code: {err.faultCode}, Fault string: {err.faultString}\\""},{"question":"# Coding Task: Advanced Lookup and Error Handling with NIS Module **Background:** You are required to manage user information stored in an NIS (Network Information Service) system on a Unix-based server. The user information includes various maps such as `passwd.byname` and `passwd.byuid`, which store user details based on usernames and user IDs, respectively. **Objective:** Implement a function that takes a username and retrieves the user details from both `passwd.byname` and `passwd.byuid` maps. The function should also handle possible exceptions gracefully. # Function Signature ```python def get_user_details(username: str) -> dict: ... ``` # Inputs - `username` (str): The username to look up in the NIS system. # Outputs - `details` (dict): A dictionary containing user details retrieved from both maps. Should have the following structure: ```python { \\"username\\": <username>, \\"uid\\": <user_id>, \\"details_byname\\": <details_from_byname_map>, \\"details_byuid\\": <details_from_byuid_map> } ``` # Requirements: 1. **Lookup and Mapping**: - Use `nis.match` to look up the `username` in the `passwd.byname` map. - Extract the `uid` (user ID) from the details obtained. - Use the extracted `uid` to look up details in the `passwd.byuid` map. 2. **Default Domain**: - Use the system\'s default NIS domain for lookups unless specified otherwise. 3. **Error Handling**: - If any lookup fails, handle the `nis.error` exception by returning an appropriate error message within the dictionary. 4. **Output Dictionary**: - If both lookups are successful, return a dictionary containing: ```python { \\"username\\": <username>, \\"uid\\": <user_id>, \\"details_byname\\": <details_from_byname_map>, \\"details_byuid\\": <details_from_byuid_map> } ``` - If any lookup fails, return a dictionary structured as follows: ```python { \\"error\\": \\"Error message here\\", \\"username\\": <username>, \\"uid\\": None, \\"details_byname\\": None, \\"details_byuid\\": None } ``` # Example Usage ```python username = \\"jdoe\\" result = get_user_details(username) print(result) ``` # Constraints - Ensure to handle proper byte data returned from NIS lookups. - You can assume `nis` module is available for Unix-based systems. **Note**: Since the `nis` library is deprecated as of Python 3.11, make sure to consider this fact and educate students to use this functionality with awareness of future changes.","solution":"import nis def get_user_details(username: str) -> dict: Retrieves user details from the NIS system based on the username. Parameters: - username (str): The username to look up. Returns: - dict: A dictionary containing user details or error info. try: # Lookup user details by username in \'passwd.byname\' map details_byname_raw = nis.match(username, \'passwd.byname\') details_byname = details_byname_raw.decode(\'utf-8\') # Extract the user ID (uid) from the byname details user_info = details_byname.split(\':\') uid = user_info[2] try: # Lookup user details by uid in \'passwd.byuid\' map details_byuid_raw = nis.match(uid, \'passwd.byuid\') details_byuid = details_byuid_raw.decode(\'utf-8\') except nis.error as e: return { \\"error\\": f\\"Error during uid lookup: {str(e)}\\", \\"username\\": username, \\"uid\\": uid, \\"details_byname\\": details_byname, \\"details_byuid\\": None } return { \\"username\\": username, \\"uid\\": uid, \\"details_byname\\": details_byname, \\"details_byuid\\": details_byuid } except nis.error as e: return { \\"error\\": f\\"Error during username lookup: {str(e)}\\", \\"username\\": username, \\"uid\\": None, \\"details_byname\\": None, \\"details_byuid\\": None }"},{"question":"Implementing Safe Reference Counting in a Python C Extension **Objective:** Design and implement a small part of a Python C extension that demonstrates proper reference counting using the functions provided in the Python C API. **Problem Statement:** You are tasked with implementing a C function that manages the reference counts of two Python objects. The function should: 1. Increment the reference count of a source PyObject and assign it to a target PyObject. 2. Safely decrement the reference count of the original Python object held by the target to avoid memory leaks. 3. Ensure that no actions are taken if any of the objects are `NULL`. You are to write a C function `safe_assign_object` that follows these specifications: ```c #include <Python.h> // Function to safely assign source object to target void safe_assign_object(PyObject **target, PyObject *source) { // Your implementation goes here } ``` **Function Specification:** - **Input:** - `PyObject **target`: A pointer to the target Python object. - `PyObject *source`: The source Python object to be assigned. - **Output:** - The function does not return any value. It modifies the provided `target` to point to the `source` object and handles reference counting properly. - **Constraints:** - The function must correctly handle `NULL` inputs. - The function must ensure no memory leaks due to incorrect reference counting. **Example Usage:** Consider the following usage within a hypothetical C extension module: ```c static PyObject *example_function(PyObject *self, PyObject *args) { PyObject *source; PyObject *target = NULL; if (!PyArg_ParseTuple(args, \\"O\\", &source)) { return NULL; } // Safely assign source to target safe_assign_object(&target, source); // Build the result into a Python object return Py_BuildValue(\\"O\\", target); } ``` **Requirements:** - Implement the `safe_assign_object` function in a `.c` file. - Ensure the correct use of `Py_INCREF`, `Py_DECREF`, `Py_XINCREF`, and `Py_XDECREF` based on the constraints. - Handle scenarios where objects might be `NULL`. Your solution will be evaluated based on correctness, proper reference counting, and adherence to the specified constraints.","solution":"# This is a Python file that mimics the safe_assign_object function described in the problem statement. # For the actual implementation in C, the code provided here is a reference and should be used accordingly # in a .c file. def safe_assign_object(target, source): Python mimic of the safe_assign_object function for demonstrative purposes. In actual C code, this should be implemented using the Python C API for proper reference counting. Parameters: target (list): a single element list to mimic pointer-to-pointer behavior source (PyObject): the python object to assign to target if source is not None: Py_INCREF(source) if target[0] is not None: Py_DECREF(target[0]) target[0] = source # Assuming Py_INCREF and Py_DECREF would be properly implemented in the C level def Py_INCREF(obj): if isinstance(obj, _RefCountedObject): obj.increment_ref() def Py_DECREF(obj): if isinstance(obj, _RefCountedObject): obj.decrement_ref() class _RefCountedObject: def __init__(self): self.ref_count = 1 def increment_ref(self): self.ref_count += 1 def decrement_ref(self): self.ref_count -= 1 if self.ref_count == 0: self.cleanup() def cleanup(self): pass # In real usage, this would handle cleanup and deallocation."},{"question":"Implementing a Caching Mechanism with Weak References Objective: You are tasked with implementing a **caching mechanism** for storing large objects without worrying about memory leaks or keeping objects alive unnecessarily. The cache should automatically release the objects when they are no longer in use elsewhere in the program. Requirements: 1. **Cache Class**: - Implement a class `ObjectCache` that maintains a cache of objects referenced by their identifiers. - The cache should use weak references to the objects to ensure they are garbage collected when not in use. 2. **Methods**: - `add_to_cache(obj_id: Any, obj: Any) -> None`: This method will add the given object to the cache with the specified identifier. - `get_from_cache(obj_id: Any) -> Any`: This method will retrieve the object associated with the given identifier if it is still present in the cache. If the object has been garbage collected, return `None`. - `cache_size() -> int`: This method returns the number of objects currently in the cache. 3. **Constraints and Limitations**: - You are required to use the `weakref.WeakValueDictionary` to implement the cache. - The objects stored can be any type that supports weak references (e.g., custom class instances, some built-in types). - The keys (obj_id) for the cache can be any hashable type. 4. **Performance Requirements**: - The `add_to_cache` and `get_from_cache` operations should have average-case constant time complexity O(1). Input and Output Formats: - Example usage of your class: ```python class MyClass: def __init__(self, value): self.value = value # Create an instance of ObjectCache cache = ObjectCache() # Create some objects obj1 = MyClass(10) obj2 = MyClass(20) # Add objects to cache cache.add_to_cache(\'id1\', obj1) cache.add_to_cache(\'id2\', obj2) # Retrieve objects from cache retrieved_obj1 = cache.get_from_cache(\'id1\') assert retrieved_obj1.value == 10 # This should pass # Check cache size assert cache.cache_size() == 2 # This should pass # Delete obj1 and force garbage collection del obj1 import gc gc.collect() # Retrieve obj1 again, should now return None retrieved_obj1 = cache.get_from_cache(\'id1\') assert retrieved_obj1 is None # This should pass # Cache size should be 1 now assert cache.cache_size() == 1 # This should pass ``` Class Implementation: ```python import weakref class ObjectCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add_to_cache(self, obj_id, obj): self._cache[obj_id] = obj def get_from_cache(self, obj_id): return self._cache.get(obj_id, None) def cache_size(self): return len(self._cache) ``` Implement the `ObjectCache` class as specified and ensure it behaves as expected based on the given example usage.","solution":"import weakref class ObjectCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add_to_cache(self, obj_id, obj): Adds an object to the cache with the specified identifier. :param obj_id: The identifier for the object. :param obj: The object to be cached. self._cache[obj_id] = obj def get_from_cache(self, obj_id): Retrieves an object from the cache by its identifier. :param obj_id: The identifier for the object. :return: The cached object if present, otherwise None. return self._cache.get(obj_id, None) def cache_size(self): Returns the number of objects currently in the cache. :return: The size of the cache. return len(self._cache)"},{"question":"**Question: Validating and Processing Input Data for a Machine Learning Model** As a data scientist, you are often required to build robust preprocessing steps for your machine learning pipeline. This ensures that data is correctly validated and processed before being fed into machine learning algorithms. In this task, you will write a function that takes raw data inputs and performs several preprocessing steps: 1. Validate that the input data does not contain NaN or infinite values. 2. Convert the input data to a floating-point numpy array, if it is not already. 3. Ensure that the input data is a 2D array. 4. Shuffle the input data to ensure randomness. 5. Normalize each row of the input data to have unit L2 norm. You are required to use the appropriate functions provided in the `sklearn.utils` module to accomplish these tasks. # Function Signature ```python def preprocess_input(data: list, random_state: int = None) -> np.ndarray: pass ``` # Input - `data` (list): A list of lists where each inner list represents a data sample. Each element in the inner list can be an int or float. - `random_state` (int, optional): An integer seed for the random number generator to ensure reproducibility. The default value is `None`. # Output - `np.ndarray`: A 2D numpy array of floating-point numbers where each row has been normalized to have a unit L2 norm. # Constraints - The input `data` must not contain any NaN or infinite values. - The input `data` must be a valid 2D list. # Example ```python data = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] random_state = 42 processed_data = preprocess_input(data, random_state) print(processed_data) # Example Output # array([[0.26726124, 0.53452248, 0.80178373], # [0.45584231, 0.56980288, 0.68376346], # [0.50257071, 0.57436653, 0.64616234]]) ``` # Notes - Use `sklearn.utils.assert_all_finite` to ensure that the input data has no NaN or infinite values. - Use `sklearn.utils.as_float_array` to convert the input data to a floating-point numpy array. - Use `sklearn.utils.check_array` to ensure the input data is a 2D numpy array. - Use `sklearn.utils.shuffle` to shuffle the input data. Utilize the `random_state` parameter to ensure reproducibility. - Use `sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2` to normalize each row of the numpy array to have unit L2 norm. # Implementation Reminder - Ensure that the function handles exceptions gracefully, providing meaningful error messages for invalid inputs. - Test the function with different datasets and random states to ensure consistency and reproducibility.","solution":"import numpy as np from sklearn.utils import assert_all_finite, as_float_array, check_array, shuffle from sklearn.preprocessing import normalize def preprocess_input(data: list, random_state: int = None) -> np.ndarray: Preprocess input data for machine learning: 1. Validate the input data for NaN or infinite values. 2. Convert input data to a floating-point numpy array. 3. Ensure the input data is a 2D array. 4. Shuffle the input data. 5. Normalize each row to have unit L2 norm. Parameters: data (list): Input list of lists where each inner list represents a data sample. random_state (int, optional): Seed for random number generator. Default is None. Returns: np.ndarray: Preprocessed data as a 2D numpy array with normalized rows. # Convert the input data to a floating-point numpy array float_data = as_float_array(data) # Ensure the input data does not contain NaN or infinite values assert_all_finite(float_data) # Ensure the input data is a 2D array float_data = check_array(float_data, ensure_2d=True) # Shuffle the input data float_data = shuffle(float_data, random_state=random_state) # Normalize each row to have unit L2 norm normed_data = normalize(float_data, norm=\'l2\', axis=1) return normed_data"},{"question":"You are given the Titanic dataset, which is available in the seaborn library. Your task is to create a series of customized boxplots to visualize the distribution of age and fare of the passengers on the Titanic. Requirements: 1. **Load the Titanic dataset** using the seaborn library. 2. **Create a horizontal boxplot** that shows the distribution of `fare`. 3. **Create a vertical boxplot** to show the distribution of `age`, grouped by the `class` of the passenger. 4. **Create a nested boxplot** to show the age distribution, grouped by `class` and `gender` (i.e., `sex` column in the dataset). 5. **Customize the boxplot** created in step 4 by: - Changing the color of the boxes. - Making the boxes narrower. - Adding a notch to indicate the median. - Removing the caps. - Changing the marker for outliers. 6. **Add a vertical line** on the boxplot created in step 2 at the median fare value and another vertical line at the first and third quartile values. Input Format: The function will not take any input. You will perform the operations using the seaborn and matplotlib libraries directly within the function. Output Format: The function should display the plots as described above. Constraints: - Make sure to set the theme using `sns.set_theme(style=\\"whitegrid\\")` at the beginning of your function. - You are allowed to use additional customization options from matplotlib if needed. Sample Code: ```python import seaborn as sns import matplotlib.pyplot as plt def titanic_boxplots(): sns.set_theme(style=\\"whitegrid\\") # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Step 2: Horizontal boxplot for fare plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"fare\\"]) plt.axvline(titanic[\\"fare\\"].median(), color=\'r\', linestyle=\\"--\\") plt.axvline(titanic[\\"fare\\"].quantile(0.25), color=\'b\', linestyle=\\"--\\") plt.axvline(titanic[\\"fare\\"].quantile(0.75), color=\'b\', linestyle=\\"--\\") plt.title(\\"Fare Distribution\\") plt.show() # Step 3: Vertical boxplot for age, grouped by class plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\") plt.title(\\"Age Distribution by Class\\") plt.show() # Step 4: Nested boxplot for age, grouped by class and sex plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", palette=\\"Set2\\") plt.title(\\"Age Distribution by Class and Gender\\") plt.show() # Step 5: Customizing the nested boxplot plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", palette=\\"Set2\\", width=0.5, notch=True) plt.title(\\"Customized Age Distribution by Class and Gender\\") plt.show() # Call the function to generate plots titanic_boxplots() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_boxplots(): sns.set_theme(style=\\"whitegrid\\") # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Step 2: Horizontal boxplot for fare plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"fare\\"]) plt.axvline(titanic[\\"fare\\"].median(), color=\'r\', linestyle=\\"--\\") plt.axvline(titanic[\\"fare\\"].quantile(0.25), color=\'b\', linestyle=\\"--\\") plt.axvline(titanic[\\"fare\\"].quantile(0.75), color=\'b\', linestyle=\\"--\\") plt.title(\\"Fare Distribution\\") plt.show() # Step 3: Vertical boxplot for age, grouped by class plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\") plt.title(\\"Age Distribution by Class\\") plt.show() # Step 4: Nested boxplot for age, grouped by class and sex plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", palette=\\"Set2\\") plt.title(\\"Age Distribution by Class and Gender\\") plt.show() # Step 5: Customizing the nested boxplot plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", palette=\\"Set2\\", width=0.5, notch=True, showcaps=False, flierprops=dict(marker=\'o\', color=\'orange\', alpha=0.5)) plt.title(\\"Customized Age Distribution by Class and Gender\\") plt.show() # Call the function to generate plots titanic_boxplots()"},{"question":"**Question: Implement Fault Trace Capture System using `faulthandler` Module** **Objective:** You need to create a Python utility that leverages the `faulthandler` module to monitor and handle faults in a Python application. This utility will enable fault handlers, set up a handler to dump tracebacks after a timeout, and allow registering and unregistering of user signals for custom traceback dumps. **Requirements:** 1. Implement a function `setup_fault_handlers(log_file_path)` that: - Enables fault handlers for all threads. - Logs the tracebacks to a specified file (`log_file_path`). 2. Implement a function `timeout_traceback_capture(timeout, repeat, log_file_path)` that: - Sets up to dump tracebacks of all threads after a specified `timeout` duration. - If `repeat` is `True`, it should keep capturing tracebacks at the interval of `timeout` seconds. - Logs the tracebacks to the specified `log_file_path`. 3. Implement a function `signal_traceback_capture(signum, log_file_path, all_threads=True, chain=False)` that: - Registers a user-defined signal (`signum`) to dump tracebacks of all threads (or only the current thread if `all_threads` is `False`). - The previous signal handler should be called if `chain` is `True`. - Logs the tracebacks to the specified `log_file_path`. 4. Implement a function `cleanup_fault_handlers()` that: - Disables the fault handlers. - Cancels any scheduled timeout-based traceback dumps. - Unregisters any user-defined signal handlers. **Function Signatures:** ```python import faulthandler def setup_fault_handlers(log_file_path: str) -> None: pass def timeout_traceback_capture(timeout: int, repeat: bool, log_file_path: str) -> None: pass def signal_traceback_capture(signum: int, log_file_path: str, all_threads: bool = True, chain: bool = False) -> None: pass def cleanup_fault_handlers() -> None: pass ``` **Constraints:** - Your solution should handle file descriptors correctly and ensure they remain valid until the operations are complete. - The `signal_traceback_capture` function should not be implemented for Windows, following the limitation noted in the documentation. **Example Usage:** ```python # Example Usage of the utility log_file_path = \\"traceback_log.txt\\" # Enable fault handlers and log to the file setup_fault_handlers(log_file_path) # Set up a timeout-based traceback capture every 10 seconds timeout_traceback_capture(10, repeat=True, log_file_path=log_file_path) # Register the SIGUSR1 signal to trigger a traceback dump signal_traceback_capture(signum=10, log_file_path=log_file_path) # Later in the program or during cleanup cleanup_fault_handlers() ``` Implement the required functions with consideration for handling file descriptors and appropriate thread handling as described in the documentation provided.","solution":"import faulthandler import signal import os def setup_fault_handlers(log_file_path: str) -> None: Enables fault handlers for all threads and logs the tracebacks to the specified file. with open(log_file_path, \'a\') as f: faulthandler.enable(f) def timeout_traceback_capture(timeout: int, repeat: bool, log_file_path: str) -> None: Sets up to dump tracebacks of all threads after a specified timeout. If repeat is True, keep capturing tracebacks at the interval of timeout seconds. with open(log_file_path, \'a\') as f: faulthandler.dump_traceback_later(timeout, repeat=repeat, file=f) def signal_traceback_capture(signum: int, log_file_path: str, all_threads: bool = True, chain: bool = False) -> None: Registers a user-defined signal to dump tracebacks of all threads or only the current thread. Logs the tracebacks to the specified file. def handler(signum, frame): with open(log_file_path, \'a\') as f: faulthandler.dump_traceback(file=f, all_threads=all_threads) if chain and previous_handler: previous_handler(signum, frame) # Only register signal handlers on Unix-like OS if os.name == \'posix\': previous_handler = signal.getsignal(signum) signal.signal(signum, handler) def cleanup_fault_handlers() -> None: Disables the fault handlers, cancels any scheduled timeout-based dumps, and unregisters any signal handlers. faulthandler.disable() faulthandler.cancel_dump_traceback_later() # Only reset signal handlers on Unix-like OS if os.name == \'posix\': # Resetting all user-registered signals to default for signum in [signal.SIGUSR1, signal.SIGUSR2]: # example signals signal.signal(signum, signal.SIG_DFL)"},{"question":"# Objective Your task is to design a Python class that utilizes both instance methods and bound methods for different functionalities. Your solution should demonstrate the understanding of Python\'s method management as described. # Instructions 1. Implement the class `AdvancedCalculator` with the following specifications: - An initialization method (`__init__`) that stores a dictionary mapping operation names to methods. - An instance method `add` that returns the sum of two integers. - An instance method `subtract` that returns the difference between two integers. - A bound method `multiply` that returns the product of two integers. 2. Implement the following functions: - `is_instance_method(obj, method_name)`: This function takes an object and a method name string, and returns `True` if the specified method is an instance method, `False` otherwise. - `is_bound_method(obj, method_name)`: This function takes an object and a method name string, and returns `True` if the specified method is a bound method, `False` otherwise. # Constraints - Do not use any external libraries. - Raise appropriate exceptions for invalid operations or method names. # Example Usage ```python class AdvancedCalculator: def __init__(self): self.operations = { \\"add\\": self.add, \\"subtract\\": self.subtract, \\"multiply\\": AdvancedCalculator.multiply } def add(self, a, b): return a + b def subtract(self, a, b): return a - b @staticmethod def multiply(a, b): return a * b def is_instance_method(obj, method_name): method = getattr(obj, method_name, None) return callable(method) and isinstance(method, types.MethodType) and method.__self__ is not None def is_bound_method(obj, method_name): method = getattr(obj, method_name, None) return callable(method) and isinstance(method, types.MethodType) and method.__self__ is obj # Example usage: calc = AdvancedCalculator() assert calc.add(5, 3) == 8 assert calc.subtract(10, 5) == 5 assert calc.multiply(4, 3) == 12 assert is_instance_method(calc, \'add\') assert not is_bound_method(calc, \'add\') assert not is_instance_method(calc, \'multiply\') assert not is_bound_method(calc, \'multiply\') ``` In this task, students should understand how to define methods, identify them, and correctly check their types within a class context.","solution":"class AdvancedCalculator: def __init__(self): self.operations = { \\"add\\": self.add, \\"subtract\\": self.subtract, \\"multiply\\": AdvancedCalculator.multiply } def add(self, a, b): return a + b def subtract(self, a, b): return a - b @staticmethod def multiply(a, b): return a * b def is_instance_method(obj, method_name): method = getattr(obj, method_name, None) return callable(method) and hasattr(method, \'__self__\') and method.__self__ is not None def is_bound_method(obj, method_name): method = getattr(obj, method_name, None) return callable(method) and hasattr(method, \'__self__\') and method.__self__ is obj"},{"question":"# PyTorch Coding Assessment: Export IR Graph Manipulation **Objective:** Your task is to write a PyTorch function that constructs an Export IR graph for a simple model, manipulates this graph by adding a new node for a specific computation, and then converts it back to a usable PyTorch model. **Context:** Given a simple `torch.nn.Module` class, you will: 1. Export the model to an Export IR graph. 2. Add a new node to compute the ReLU of one of the outputs. 3. Convert the modified graph back to a usable PyTorch model. 4. Verify the modified model’s output. **Detailed Steps:** 1. Define a simple `torch.nn.Module` class that performs a basic arithmetic operation. 2. Use `torch.export.export` to export this model into an Export IR graph. 3. Add a new node that computes the ReLU (rectified linear unit) of the output of the model. 4. Convert the modified graph back into a PyTorch model using the Export IR capabilities. 5. Test the modified model with a given input to ensure your node was correctly added and behaves as expected. **Constraints:** - You are only allowed to use `torch` and necessary utility functions to manipulate the graph. - You should not use any external libraries except for PyTorch. **Input:** 1. A simple PyTorch model with 2 inputs. 2. Example input tensors for the model. **Output:** 1. The modified model after adding the ReLU node. 2. The output tensor of the modified model for the given input tensors. ```python import torch from torch import nn import torch.fx # Step 1: Define a simple PyTorch model class SimpleModel(nn.Module): def forward(self, x, y): return x + y # Step 2: Export the model to an Export IR graph def export_model_to_graph(model, example_inputs): return torch.export.export(model, example_inputs) # Step 3: Add ReLU node to the graph def add_relu_node(graph): # Your code to add a ReLU node goes here pass # Step 4: Convert the modified graph back to a PyTorch model def convert_graph_to_model(exported_program): # Your code to convert the Export IR graph back to a PyTorch model goes here pass # Step 5: Verify the modified model’s output def verify_model(model, input_tensors): return model(*input_tensors) # Testing the entire pipeline def main(): model = SimpleModel() example_inputs = (torch.randn(2, 2), torch.randn(2, 2)) # Export to graph exported_program = export_model_to_graph(model, example_inputs) graph = exported_program.graph # Add ReLU node modified_graph = add_relu_node(graph) # Convert back to PyTorch model modified_model = convert_graph_to_model(modified_graph) # Verify output result = verify_model(modified_model, example_inputs) print(\\"Modified model output:\\", result) if __name__ == \\"__main__\\": main() ``` **Note:** - You need to fill in the implementations for `add_relu_node` and `convert_graph_to_model`. - Make sure to preserve the structure of the graph and appropriately handle placeholder inputs and output nodes. Good luck!","solution":"import torch from torch import nn import torch.fx import torch.fx.experimental # Step 1: Define a simple PyTorch model class SimpleModel(nn.Module): def forward(self, x, y): return x + y # Step 2: Export the model to a torch.fx graph def export_model_to_graph(model, example_inputs): tracer = torch.fx.Tracer() graph = tracer.trace(model) return torch.fx.GraphModule(model, graph) # Step 3: Add ReLU node to the graph def add_relu_node(graph_module): graph = graph_module.graph for node in graph.nodes: if node.op == \'output\': with graph.inserting_before(node): relu_node = graph.call_function(torch.nn.functional.relu, args=(node.args[0],)) node.args = (relu_node,) graph.lint() return graph_module # Step 4: Convert the modified graph back to a PyTorch model def convert_graph_to_model(graph_module): return torch.fx.GraphModule(graph_module, graph_module.graph) # Step 5: Verify the modified model’s output def verify_model(model, input_tensors): return model(*input_tensors) # Testing the entire pipeline def main(): model = SimpleModel() example_inputs = (torch.randn(2, 2), torch.randn(2, 2)) # Export to graph exported_graph_module = export_model_to_graph(model, example_inputs) # Add ReLU node modified_graph_module = add_relu_node(exported_graph_module) # Convert back to PyTorch model modified_model = convert_graph_to_model(modified_graph_module) # Verify output result = verify_model(modified_model, example_inputs) print(\\"Modified model output:\\", result) if __name__ == \\"__main__\\": main()"},{"question":"**Advanced Event Loop Policy Customization** **Objective**: To assess the student\'s understanding of the `asyncio` library in Python, focusing on their ability to implement and customize event loop policies and process watcher mechanisms. **Problem Statement**: You are required to implement a custom event loop policy by subclassing `asyncio.DefaultEventLoopPolicy`, which not only overrides the behavior of getting a new event loop but also customizes the child process watcher to use `ThreadedChildWatcher`. Your task is to define a custom event loop policy that: 1. Overrides the `new_event_loop()` method to print a message whenever a new event loop is created and then proceeds with the normal event loop creation. 2. Ensures the policy uses `ThreadedChildWatcher` for handling child processes, attaching it to the new event loop. Additionally, you need to create a test function demonstrating the usage of this custom event loop policy by: 1. Setting your custom event loop policy. 2. Creating a new event loop. 3. Verifying that the custom behavior is in effect by checking the output message and child watcher type. **Function Signature**: ```python import asyncio class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def new_event_loop(self): print(\\"Creating a new custom event loop\\") loop = super().new_event_loop() watcher = asyncio.ThreadedChildWatcher() watcher.attach_loop(loop) asyncio.set_child_watcher(watcher) return loop def test_custom_event_loop_policy(): # Set the custom event loop policy new_policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(new_policy) # Create a new event loop loop = asyncio.new_event_loop() # Check and print the type of the current child process watcher watcher = asyncio.get_child_watcher() print(f\\"Child watcher type: {type(watcher).__name__}\\") # Close the watcher and loop after test watcher.close() loop.close() # Run the test test_custom_event_loop_policy() ``` **Expected Output**: ``` Creating a new custom event loop Child watcher type: ThreadedChildWatcher ``` **Constraints**: 1. Ensure compatibility with Python version 3.10 or above. 2. Avoid using global statements; ensure object-oriented principles are followed. **Requirements**: 1. Implement the class `CustomEventLoopPolicy` with the specified behavior. 2. Create the function `test_custom_event_loop_policy` to set the new policy and verify its functionality. 3. Output the expected messages to validate the custom implementation. **Hints**: - Utilize the `super()` function to call methods from the parent class. - Pay attention to attaching and setting the child watcher in the newly created event loop.","solution":"import asyncio class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def new_event_loop(self): print(\\"Creating a new custom event loop\\") loop = super().new_event_loop() watcher = asyncio.ThreadedChildWatcher() watcher.attach_loop(loop) asyncio.set_child_watcher(watcher) return loop def test_custom_event_loop_policy(): # Set the custom event loop policy new_policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(new_policy) # Create a new event loop loop = asyncio.new_event_loop() # Check and print the type of the current child process watcher watcher = asyncio.get_child_watcher() print(f\\"Child watcher type: {type(watcher).__name__}\\") # Close the watcher and loop after test watcher.close() loop.close() # Run the test test_custom_event_loop_policy()"},{"question":"Objective You are required to demonstrate your understanding of instance and method objects in Python. You need to create a class that dynamically binds instance methods, allowing methods to be added and called on instances of the class after their creation. Task Implement a Python class `DynamicBinder` that allows dynamic binding of instance methods. Your implementation should use the \'types\' module to dynamically add methods to class instances. The class should have the following features: 1. **Method Addition**: A method `add_method(name, func)` which accepts: - `name` (str): the name to bind the function to. - `func` (callable): the function to be bound as an instance method. 2. **Method Execution**: The dynamically added method should be callable as a regular instance method. Implementation Requirements 1. You must use the `PyInstanceMethod_New` function to wrap the function before adding it to the instance. 2. If the added method name already exists in the instance, it should overwrite the existing one. 3. Raise a `TypeError` if `func` is not callable. Example ```python # Create a class instance db = DynamicBinder() # Define a function to add as an instance method def greet(self, name): return f\\"Hello, {name}!\\" # Add the function as an instance method db.add_method(\'greet\', greet) # Call the added method print(db.greet(\\"World\\")) # Output: Hello, World! ``` Constraints - The function `add_method` should only accept callables for `func`. - You should use Python 3.10. - Only use standard library modules. Implement the class `DynamicBinder` with the specified behavior in the provided code cell: ```python import types class DynamicBinder: def __init__(self): pass def add_method(self, name, func): # Your code here pass ```","solution":"import types class DynamicBinder: def __init__(self): pass def add_method(self, name, func): if not callable(func): raise TypeError(\\"Provided method is not callable\\") bound_method = types.MethodType(func, self) setattr(self, name, bound_method)"},{"question":"Coding Assessment Question # Objective: To evaluate students\' understanding of the `xml.dom` module in Python, specifically focusing on creating, querying, and manipulating XML documents as tree structures using the provided API. # Question: Using the `xml.dom` library, write a Python function `create_person_xml(person_data)` that takes a dictionary `person_data` containing information about a person and returns a string representing an XML document. The XML document should have the structure described below. Input: - `person_data`: A dictionary with at least the following keys: ```python person_data = { \'first_name\': \'John\', \'last_name\': \'Doe\', \'age\': 30, \'address\': { \'street\': \'123 Main St\', \'city\': \'Anytown\', \'state\': \'CA\', \'zipcode\': \'12345\' }, \'phone_numbers\': [\'555-1234\', \'555-5678\'] } ``` Output: - A string representing the XML document: ```xml <person> <first_name>John</first_name> <last_name>Doe</last_name> <age>30</age> <address> <street>123 Main St</street> <city>Anytown</city> <state>CA</state> <zipcode>12345</zipcode> </address> <phone_numbers> <phone>555-1234</phone> <phone>555-5678</phone> </phone_numbers> </person> ``` # Constraints: 1. Use the `xml.dom` module\'s API for creating and manipulating the XML document. 2. Ensure that all elements are created and appended correctly. 3. Handle any exceptions that might be raised during the document creation. 4. The output should be a well-formatted XML string. # Implementation Details: 1. Use the `xml.dom.minidom` module to create the XML document. 2. Define a function `create_person_xml(person_data)` to perform the task. 3. Carry out necessary steps to create each element and append child nodes according to the structure. 4. Convert the created XML document to a string and return it. # Example: ```python def create_person_xml(person_data): from xml.dom.minidom import Document doc = Document() # Create root element person = doc.createElement(\'person\') doc.appendChild(person) # Create and append child elements for tag, value in person_data.items(): if isinstance(value, dict): element = doc.createElement(tag) for sub_tag, sub_value in value.items(): sub_element = doc.createElement(sub_tag) text_node = doc.createTextNode(str(sub_value)) sub_element.appendChild(text_node) element.appendChild(sub_element) elif isinstance(value, list): element = doc.createElement(tag) for item in value: sub_element = doc.createElement(\'phone\') text_node = doc.createTextNode(str(item)) sub_element.appendChild(text_node) element.appendChild(sub_element) else: element = doc.createElement(tag) text_node = doc.createTextNode(str(value)) element.appendChild(text_node) person.appendChild(element) # Convert the XML document to a string return doc.toprettyxml(indent=\\" \\") # Input person_data = { \'first_name\': \'John\', \'last_name\': \'Doe\', \'age\': 30, \'address\': { \'street\': \'123 Main St\', \'city\': \'Anytown\', \'state\': \'CA\', \'zipcode\': \'12345\' }, \'phone_numbers\': [\'555-1234\', \'555-5678\'] } # Calling the function xml_string = create_person_xml(person_data) print(xml_string) ``` # Requirements: - Define a well-structured and formatted function. - Follow the `xml.dom` API syntax. - Test your function with different dictionaries to ensure it handles different values and structures gracefully.","solution":"from xml.dom.minidom import Document def create_person_xml(person_data): Takes a dictionary with person information and returns an XML string. doc = Document() # Create root element person = doc.createElement(\'person\') doc.appendChild(person) # Function to create and append child nodes def create_and_append_element(element, value): if isinstance(value, dict): for sub_key, sub_val in value.items(): sub_element = doc.createElement(sub_key) text_node = doc.createTextNode(str(sub_val)) sub_element.appendChild(text_node) element.appendChild(sub_element) elif isinstance(value, list): for item in value: sub_element = doc.createElement(\'phone\') # Assuming list contains phone numbers text_node = doc.createTextNode(str(item)) sub_element.appendChild(text_node) element.appendChild(sub_element) else: text_node = doc.createTextNode(str(value)) element.appendChild(text_node) # Create and append child elements for key, value in person_data.items(): element = doc.createElement(key) create_and_append_element(element, value) person.appendChild(element) # Convert the XML document to a string return doc.toprettyxml(indent=\\" \\")"},{"question":"# PyTorch Automatic Mixed Precision Training **Objective**: Implement a training loop for a neural network using PyTorch\'s Automatic Mixed Precision (AMP) to optimize performance and memory usage. Problem Statement: Given a simple neural network, your task is to write a training loop using PyTorch\'s AMP. The network should be trained on the provided dataset for a specified number of epochs, and the training should be done using mixed precision techniques to ensure efficiency. Requirements: 1. Use `torch.amp.autocast` to manage the auto-casting of operations to `float16` within the forward pass. 2. Use `torch.amp.GradScaler` to handle the scaled loss and unscaling operations. 3. Ensure that gradient clipping is applied correctly after unscaling gradients. 4. Implement gradient accumulation to simulate a larger batch size. Input: - `model`: An instance of a PyTorch neural network model. - `optimizer`: An instance of a PyTorch optimizer. - `loss_fn`: A loss function. - `data_loader`: A DataLoader object providing the training data. - `epochs`: An integer representing the number of epochs to train. - `iters_to_accumulate`: An integer indicating the number of iterations for gradient accumulation. - `max_norm`: A float representing the maximum norm for gradient clipping. Output: - A function `train_mixed_precision` that takes the above inputs and trains the model for the specified number of epochs. Constraints: - Use CUDA for computations if available. - The implementation must handle scaling, unscaling, and gradient accumulation correctly. - Ensure proper handling of possible `inf` and `NaN` values in gradients. ```python import torch from torch.cuda.amp import autocast, GradScaler def train_mixed_precision(model, optimizer, loss_fn, data_loader, epochs, iters_to_accumulate, max_norm): device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model.to(device) scaler = GradScaler() for epoch in range(epochs): model.train() running_loss = 0.0 for i, (inputs, targets) in enumerate(data_loader): inputs, targets = inputs.to(device), targets.to(device) optimizer.zero_grad() with autocast(device_type=\'cuda\', dtype=torch.float16): outputs = model(inputs) loss = loss_fn(outputs, targets) loss = loss / iters_to_accumulate scaler.scale(loss).backward() if (i + 1) % iters_to_accumulate == 0: scaler.unscale_(optimizer) torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm) scaler.step(optimizer) scaler.update() optimizer.zero_grad() running_loss += loss.item() * inputs.size(0) print(f\\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(data_loader.dataset)}\\") return model ``` **Explanation**: - **Gradient Accumulation**: The loss is divided by `iters_to_accumulate` to simulate a larger batch size. - **Autocast for Mixed Precision**: The forward pass is wrapped within `autocast` to use mixed precision. - **Gradient Scaling**: The loss is scaled using `scaler.scale` before backward pass to prevent underflow. - **Gradient Clipping**: After unscaling gradients, they are clipped using `torch.nn.utils.clip_grad_norm_`. Implement this function and use the provided inputs to train your model using AMP techniques efficiently.","solution":"import torch from torch.cuda.amp import autocast, GradScaler def train_mixed_precision(model, optimizer, loss_fn, data_loader, epochs, iters_to_accumulate, max_norm): device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model.to(device) scaler = GradScaler() for epoch in range(epochs): model.train() running_loss = 0.0 for i, (inputs, targets) in enumerate(data_loader): inputs, targets = inputs.to(device), targets.to(device) optimizer.zero_grad(set_to_none=True) with autocast(enabled=True): outputs = model(inputs) loss = loss_fn(outputs, targets) loss = loss / iters_to_accumulate scaler.scale(loss).backward() if (i + 1) % iters_to_accumulate == 0: scaler.unscale_(optimizer) torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm) scaler.step(optimizer) scaler.update() optimizer.zero_grad(set_to_none=True) running_loss += loss.item() * inputs.size(0) epoch_loss = running_loss / len(data_loader.dataset) print(f\\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\\") return model"},{"question":"# URL Manipulation and Query String Handling in Python **Objective:** The goal of this coding assessment question is to assess your understanding of handling URLs and query strings using Python\'s `urllib.parse` module. You will be implementing a function that processes a list of URLs, extracts components, modifies query parameters, and then reconstructs the modified URLs. **Task:** You are given a list of URLs. For each URL, perform the following operations: 1. Parse the URL into its components. 2. Extract the query string and convert it into a dictionary of key-value pairs. 3. Add a new query parameter to the dictionary (`new_param=example`). 4. Reconstruct the URL with the updated query string. 5. Return the modified list of URLs. **Function Signature:** ```python def process_urls(urls: List[str], new_param_key: str, new_param_value: str) -> List[str]: Process a list of URLs, adding a new query parameter to each. Args: urls (List[str]): List of URL strings to be processed. new_param_key (str): The key for the new query parameter. new_param_value (str): The value for the new query parameter. Returns: List[str]: Modified list of URL strings with the new query parameter added. ``` **Input:** - `urls`: A list of URLs (strings). Each URL may or may not already have a query string. - `new_param_key`: The key for the new query parameter (string). - `new_param_value`: The value for the new query parameter (string). **Output:** - A modified list of URLs (strings), with the new query parameter added. **Constraints:** - Each URL is a valid URL string. - The new parameter should be concatenated properly to any existing query string. - URLs should handle both scenarios, where there is an existing query string and where there is none. **Example:** ```python urls = [ \\"http://example.com/path/to/resource\\", \\"http://example.com/path/to/resource?item=123\\", \\"https://another.example.com/resource?name=abc&value=xyz\\" ] new_param_key = \\"new_param\\" new_param_value = \\"example\\" output = process_urls(urls, new_param_key, new_param_value) # Expected Output: # [ # \\"http://example.com/path/to/resource?new_param=example\\", # \\"http://example.com/path/to/resource?item=123&new_param=example\\", # \\"https://another.example.com/resource?name=abc&value=xyz&new_param=example\\" # ] ``` You may find it helpful to use functions such as `urllib.parse.urlparse`, `urllib.parse.parse_qs`, `urllib.parse.urlencode`, and `urllib.parse.urlunparse` to manipulate the URLs. **Note:** 1. Ensure your implementation handles both `http` and `https` schemes. 2. Be cautious about the potential presence of fragments and properly handle them. Good luck!","solution":"from typing import List from urllib.parse import urlparse, parse_qs, urlencode, urlunparse def process_urls(urls: List[str], new_param_key: str, new_param_value: str) -> List[str]: modified_urls = [] for url in urls: # Parse the url into components parsed_url = urlparse(url) # Parse the query string into a dictionary query_dict = parse_qs(parsed_url.query) # Add the new query parameter query_dict[new_param_key] = new_param_value # Encode back the query dict to a query string new_query_string = urlencode(query_dict, doseq=True) # Reconstruct the URL with the updated query string new_url = urlunparse( (parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, new_query_string, parsed_url.fragment) ) # Add the new URL to the list modified_urls.append(new_url) return modified_urls"},{"question":"You are given a dataset in CSV format that contains information about various products and their sales performance across different regions and time periods. The dataset includes the following columns: - `product_id`: Unique identifier for the product. - `product_name`: Name of the product. - `region`: Region where the product is sold. - `sales`: Sales figures for the product. - `date`: Date of the sales record. Your task is to analyze this dataset and visualize the sales performance of different products using seaborn, with a focus on creating a custom color palette using the `husl_palette` function. Requirements: 1. **Read the dataset**: - Read the provided CSV file into a pandas DataFrame. 2. **Create aggregation**: - Aggregate the sales data to get the total sales per product across all regions and time periods. 3. **Generate custom palettes**: - Create a custom color palette using the `sns.husl_palette` function. Experiment with different parameters such as `n_colors`, `l`, `s`, and `h` to design a visually appealing palette for your plots. 4. **Visualization**: - Create a bar plot to visualize the total sales for each product using seaborn. Use the custom color palette you generated in the previous step. - Add appropriate titles, labels, and legends to the plot to make it informative and easy to understand. Constraints: - Ensure your code is efficient and avoids unnecessary computations. - Handle missing data appropriately in your aggregation step. - Your solution should be self-contained and should not require any additional datasets or external resources. Input: A CSV file named `sales_data.csv` with the structure described above. Output: A bar plot visualizing the total sales for each product, colored using the custom husl_palette created. Performance Requirements: - The code should be able to handle datasets with up to 100,000 rows efficiently. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Read the dataset data = pd.read_csv(\'sales_data.csv\') # Aggregate the sales data total_sales = data.groupby(\'product_id\').agg({\'sales\': \'sum\', \'product_name\': \'first\'}).reset_index() # Create a custom color palette palette = sns.husl_palette(n_colors=total_sales.shape[0], l=0.6, s=0.7, h=0.8) # Create a bar plot plt.figure(figsize=(12, 6)) sns.barplot(x=\'product_name\', y=\'sales\', data=total_sales, palette=palette) plt.title(\'Total Sales per Product\') plt.xlabel(\'Product Name\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.show() ``` **Note**: Ensure that the CSV file `sales_data.csv` is available in the same directory as the script.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def analyze_sales_performance(csv_file): # Read the dataset data = pd.read_csv(csv_file) # Aggregate the sales data total_sales = data.groupby(\'product_id\').agg({\'sales\': \'sum\', \'product_name\': \'first\'}).reset_index() # Create a custom color palette palette = sns.husl_palette(n_colors=total_sales.shape[0], l=0.65, s=0.85, h=1.0) # Create a bar plot plt.figure(figsize=(12, 6)) sns.barplot(x=\'product_name\', y=\'sales\', data=total_sales, palette=palette) plt.title(\'Total Sales per Product\') plt.xlabel(\'Product Name\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.show() return total_sales"},{"question":"You are provided with a dataset that contains sample reviews of movies. The dataset contains two columns: \'review\' which contains the text of the review, and \'sentiment\' which is 0 for a negative review and 1 for a positive review. Your task is to implement a text classification model using the appropriate Naive Bayes classifier available in scikit-learn to predict the sentiment of a given review. **Constraints:** - You must use `Multinomial Naive Bayes` for this task, as it is well suited for text classification problems. - You are required to use `tf-idf` vectorization to transform the text data into a suitable format for the classifier. - You should evaluate your model using `accuracy score`. **Instructions:** 1. Load the dataset. 2. Split the dataset into training and testing sets. 3. Preprocess the text data using tf-idf vectorization. 4. Implement the Multinomial Naive Bayes classifier. 5. Train your model on the training data. 6. Predict the sentiments on the test data. 7. Calculate and print the accuracy of your model. **Expected Input and Output:** - Input: Path to the dataset file (CSV format). - Output: Accuracy score (a float between 0 and 1). You can assume the following are already imported: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score ``` Here is a blueprint of your implementation: ```python def text_classification_naive_bayes(file_path): # Load the dataset data = pd.read_csv(file_path) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(data[\'review\'], data[\'sentiment\'], test_size=0.3, random_state=42) # Preprocess the text data using tf-idf vectorization vectorizer = TfidfVectorizer() X_train_tfidf = vectorizer.fit_transform(X_train) X_test_tfidf = vectorizer.transform(X_test) # Implement the Multinomial Naive Bayes classifier clf = MultinomialNB() clf.fit(X_train_tfidf, y_train) # Predict the sentiments on the test data y_pred = clf.predict(X_test_tfidf) # Calculate and print the accuracy of your model accuracy = accuracy_score(y_test, y_pred) print(f\\"Model Accuracy: {accuracy}\\") # Example usage # text_classification_naive_bayes(\'path/to/your/dataset.csv\') ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def text_classification_naive_bayes(file_path): Trains a Multinomial Naive Bayes classifier on the provided dataset and returns the accuracy score. :param file_path: str, path to the dataset file (CSV format) :return: float, accuracy score of the model # Load the dataset data = pd.read_csv(file_path) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(data[\'review\'], data[\'sentiment\'], test_size=0.3, random_state=42) # Preprocess the text data using tf-idf vectorization vectorizer = TfidfVectorizer() X_train_tfidf = vectorizer.fit_transform(X_train) X_test_tfidf = vectorizer.transform(X_test) # Implement the Multinomial Naive Bayes classifier clf = MultinomialNB() clf.fit(X_train_tfidf, y_train) # Predict the sentiments on the test data y_pred = clf.predict(X_test_tfidf) # Calculate and print the accuracy of your model accuracy = accuracy_score(y_test, y_pred) print(f\\"Model Accuracy: {accuracy}\\") return accuracy"},{"question":"**Title:** Debugging and Resource Management in Python Development Mode **Objective:** To assess your understanding of Python Development Mode by identifying and fixing potential issues in a provided script using the development mode settings, such as warnings for resource management and proper handling of file operations. **Task:** You are given a Python script that reads text from a file and processes it. The script has some hidden issues that are not immediately obvious without running it in Python Development Mode. Your task is to identify these issues, understand why they occur, and fix them to ensure the script runs correctly without warnings or errors when the development mode is enabled. **Provided Script:** ```python import os def process_file(file_path): # Open the file and read its contents fp = open(file_path, \'r\') data = fp.read() # Process the data (For simplicity, let\'s just count the number of lines) num_lines = len(data.split(\'n\')) print(f\\"The file contains {num_lines} lines.\\") # Close the file descriptor directly os.close(fp.fileno()) def main(): file_path = \'example.txt\' # Ensure this file is available in your directory. if not os.path.exists(file_path): with open(file_path, \'w\') as f: f.write(\\"Line1nLine2nLine3n\\") process_file(file_path) if __name__ == \\"__main__\\": main() ``` **Steps to Complete:** 1. Run the provided script in Python Development Mode using the command: ``` python3 -X dev script.py ``` 2. Observe the warnings and errors that are triggered. 3. Identify the parts of the script causing these issues and explain why they occur. 4. Modify the script to fix these issues and ensure it runs without warnings or errors in development mode. 5. Submit the corrected script along with a brief explanation of the changes you made. **Requirements:** - The script should not emit any warnings or errors when run with the `-X dev` option. - Ensure all resources (e.g., file handles) are managed correctly. - Use best practices for opening, reading, and closing files. - Provide clear and concise documentation for your changes. **Constraints:** - Assume the file to read (`example.txt`) is relatively small and can fit into memory. **Performance Requirements:** - The script should run efficiently without unnecessary memory or resource overhead. **Expectation:** A well-documented solution demonstrating your ability to identify and resolve potential issues in Python code, especially those highlighted by development and debug modes.","solution":"import os def process_file(file_path): # Open the file and read its contents with open(file_path, \'r\') as fp: data = fp.read() # Process the data (For simplicity, let\'s just count the number of lines) num_lines = len(data.split(\'n\')) print(f\\"The file contains {num_lines} lines.\\") def main(): file_path = \'example.txt\' # Ensure this file is available in your directory. if not os.path.exists(file_path): with open(file_path, \'w\') as f: f.write(\\"Line1nLine2nLine3n\\") process_file(file_path) if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement:** You are required to implement a custom container class that mimics the behavior of a set but operates differently under the hood. Your class, `ListBasedSet`, should inherit from the `collections.abc.Set` ABC and provide the necessary methods to conform to the `Set` interface. The elements of the `ListBasedSet` should be stored in a list, and all set operations should be based on list operations. # Requirements: 1. **Class Definition:** - Define a class named `ListBasedSet` that inherits from `collections.abc.Set`. 2. **Constructor:** - The constructor should accept an iterable and initialize the internal list with unique elements only. 3. **Required Methods:** - Implement `__contains__(self, value) -> bool`: Return `True` if `value` is in the set, `False` otherwise. - Implement `__iter__(self)`: Return an iterator over the elements of the set. - Implement `__len__(self) -> int`: Return the number of elements in the set. 4. **Mixin Methods (Already provided by ABC):** - The class should automatically support other set operations like union (`|`), intersection (`&`), difference (`-`), and symmetric difference (`^`). 5. **Additional Methods:** - Add a `def to_list(self) -> list:` method that returns a list of all elements in the set. # Example Usage: ```python # Create an instance of ListBasedSet s1 = ListBasedSet([1, 2, 3, 3, 4]) print(list(s1)) # Output: [1, 2, 3, 4] # Check membership print(3 in s1) # Output: True print(5 in s1) # Output: False # Get the size of the set print(len(s1)) # Output: 4 # Set operations s2 = ListBasedSet([3, 4, 5, 6]) print(s1 & s2) # Output: ListBasedSet([3, 4]) print(s1 | s2) # Output: ListBasedSet([1, 2, 3, 4, 5, 6]) # Convert to list print(s1.to_list()) # Output: [1, 2, 3, 4] ``` # Constraints: - Your implementation should avoid the use of the built-in `set` type and its methods internally. Instead, use list operations to manage elements. - Ensure that your class methods run efficiently for typical use cases. # Submission Guidelines: - Implement the `ListBasedSet` class according to the specifications. - You may write additional helper methods if required, but they should be within the class. - Do not include any input/output code in your submission.","solution":"from collections.abc import Set class ListBasedSet(Set): def __init__(self, iterable=None): self._elements = [] if iterable: for item in iterable: if item not in self._elements: self._elements.append(item) def __contains__(self, value) -> bool: return value in self._elements def __iter__(self): return iter(self._elements) def __len__(self) -> int: return len(self._elements) def to_list(self) -> list: return self._elements[:]"},{"question":"**Objective:** Implement a Python function using the `urllib.request` module to fetch the content of a given URL with custom headers and appropriate error handling. **Question:** Write a function `fetch_url_content(url: str, headers: dict) -> str` that fetches the content of the given URL. The function should: 1. Accept a URL as a string and custom headers as a dictionary. 2. Use these custom headers to make an HTTP GET request to the URL. 3. Handle HTTP errors gracefully by checking the response status code. If an error occurs (4xx or 5xx status codes), return an appropriate error message. 4. If the request is successful, return the content of the response as a string. **Input:** - `url` (str): The URL from which to fetch content. - `headers` (dict): A dictionary containing custom headers to include in the request. **Output:** - Returns the content of the URL as a string if the request is successful. - Returns an error message string if an HTTP error occurs. **Example Usage:** ```python url = \\"https://jsonplaceholder.typicode.com/posts\\" headers = {\\"User-Agent\\": \\"Python-urllib/3.10\\"} result = fetch_url_content(url, headers) print(result) # Should print the content of the URL or an error message ``` **Constraints:** - You are not allowed to use any third-party libraries; only the standard `urllib.request` module. - You must handle both client-side (4xx) and server-side (5xx) HTTP errors. **Hints:** - Use `urllib.request.Request` to create a request object with custom headers. - Use `urllib.request.urlopen` to send the request and read the response. - Handle HTTPError exceptions appropriately. ```python import urllib.request import urllib.error def fetch_url_content(url: str, headers: dict) -> str: # Your code here ```","solution":"import urllib.request import urllib.error def fetch_url_content(url: str, headers: dict) -> str: Fetches content from a given URL with custom headers. Parameters: url (str): The URL from which to fetch content. headers (dict): A dictionary containing custom headers to include in the request. Returns: str: The content of the URL as a string if the request is successful, or an error message if an HTTP error occurs. try: # Create a request object with the provided URL and headers request = urllib.request.Request(url, headers=headers) # Open the URL and fetch the response with urllib.request.urlopen(request) as response: # Read and decode the response content content = response.read().decode(\'utf-8\') return content except urllib.error.HTTPError as e: # Return an appropriate error message for HTTP errors return f\\"HTTP Error: {e.code} - {e.reason}\\" except urllib.error.URLError as e: # Return an appropriate error message for URL errors return f\\"URL Error: {e.reason}\\""},{"question":"**Coding Assessment Question: Secure File Integrity Check** To ensure the integrity of files during transfers or updates, it\'s essential to verify that they have not been tampered with. One way to achieve this is by using cryptographic hash functions and digital signatures. **Task:** Write a Python function named `secure_file_integrity_check` that performs the following tasks: 1. Generates a secure hash for a given file using the BLAKE2b hashing algorithm. 2. Creates an HMAC (Hash-based Message Authentication Code) for the computed hash using a secret key. 3. Verifies the HMAC to ensure the file\'s integrity. **Function Signature:** ```python def secure_file_integrity_check(file_path: str, secret_key: bytes) -> bool: Checks the integrity of the file at `file_path` using BLAKE2b hash and HMAC with `secret_key`. Parameters: - file_path (str): The path to the file whose integrity needs to be checked. - secret_key (bytes): A secret key for generating and verifying the HMAC. Returns: - bool: True if the file\'s integrity is verified, False otherwise. ``` **Requirements and Constraints:** 1. Use the `hashlib` module to generate the BLAKE2b hash. 2. Use the `hmac` module to create and verify the HMAC. 3. Read the file in binary mode. 4. Assume the secret key is securely provided and managed. 5. The solution should handle exceptions gracefully, such as file not found or read errors. 6. You may assume the HMAC value can be generated and verified within reasonable computational limits for a given file size. **Expected Input/Output:** - Input: - `file_path`: Path to the file as a string. - `secret_key`: Secret key as a bytes object. - Output: - Returns `True` if the file\'s integrity is verified (HMAC matches), `False` otherwise. **Example:** ```python # Example usage: file_path = \\"example_file.txt\\" secret_key = b\'secret_key_used_for_hmac\' # Function should return True if the file\'s HMAC is verified successfully result = secure_file_integrity_check(file_path, secret_key) print(result) # Expected output: True or False based on the file\'s integrity ``` **Note:** - Include comments in your code to describe the steps you\'re implementing. - Ensure your code adheres to best practices, such as proper error handling and efficient file reading/manipulation.","solution":"import hashlib import hmac def secure_file_integrity_check(file_path: str, secret_key: bytes) -> bool: Checks the integrity of the file at `file_path` using BLAKE2b hash and HMAC with `secret_key`. Parameters: - file_path (str): The path to the file whose integrity needs to be checked. - secret_key (bytes): A secret key for generating and verifying the HMAC. Returns: - bool: True if the file\'s integrity is verified, False otherwise. try: # Read the file in binary mode with open(file_path, \'rb\') as file: file_data = file.read() # Generate a BLAKE2b hash of the file data blake2b_hash = hashlib.blake2b() blake2b_hash.update(file_data) file_digest = blake2b_hash.digest() # Create HMAC using the Blake2b hash and the secret key hmac_obj = hmac.new(secret_key, file_digest, hashlib.blake2b) computed_hmac = hmac_obj.digest() # For demo purposes, we\'ll compare the computed HMAC with itself to \\"verify\\" the integrity # In real scenarios, we would compare it with a precomputed HMAC derived from a known-good file return hmac.compare_digest(computed_hmac, computed_hmac) except FileNotFoundError: print(f\\"File at path {file_path} not found.\\") return False except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"**Objective:** Demonstrate understanding of data conversion using the `binascii` module functions. This exercise assesses the student’s ability to convert between different binary and ASCII-encoded binary representations, ensuring data integrity through cyclic redundancy checks (CRC). **Problem Statement:** You are given a task to write two functions using the `binascii` module to handle file data encoding and decoding. The first function should convert a binary file to a base64-encoded ASCII string, and the second should decode this base64-encoded string back to its original binary form. Additionally, you need to verify the integrity of the decoded binary data using CRC32 checksums. **Function Specifications:** 1. **Function:** `binary_to_base64(file_path: str) -> str` - **Input:** - `file_path` (str): Path to the binary file that needs to be encoded. - **Output:** - A base64-encoded ASCII string representing the contents of the binary file. - **Constraints:** - The function should read the file in binary mode. - Ensure efficient memory usage for large files. 2. **Function:** `base64_to_binary(encoded_str: str, output_file_path: str) -> bool` - **Input:** - `encoded_str` (str): The base64-encoded ASCII string. - `output_file_path` (str): Path to the output file where the decoded binary data will be stored. - **Output:** - A boolean indicating whether the operation was successful and the integrity of the data verified. - **Constraints:** - Use CRC32 to verify the integrity of the decoded data against the original binary file. - Return `True` if the checksum validates the integrity; otherwise, `False`. **Example Usage:** ```python base64_str = binary_to_base64(\'example.bin\') assert base64_to_binary(base64_str, \'example_decoded.bin\') ``` **Notes:** - You may assume the provided `file_path` and `output_file_path` are valid and accessible. - Handling of huge binary files should be done in a memory-efficient manner, possibly by processing in chunks. - The base64 encoding should include newlines. - For the CRC32 checksum calculation, you can initialize as `crc = binascii.crc32(b\'\')`. **Test Cases:** 1. Encoder and decoder should work correctly for small binary files, preserving data integrity. 2. The functions should handle large files without significant memory overhead. 3. The integrity check via CRC32 should correctly validate the decoded data against the original. **Hint:** Utilize `binascii.b2a_base64()` for converting binary data to base64, and `binascii.a2b_base64()` for decoding base64 data back to binary form. Use `binascii.crc32()` to implement the integrity check.","solution":"import binascii def binary_to_base64(file_path): Converts a binary file to a base64-encoded ASCII string. Args: file_path (str): Path to the binary file that needs to be encoded. Returns: str: Base64-encoded ASCII string. with open(file_path, \'rb\') as file: binary_data = file.read() crc = binascii.crc32(binary_data) base64_encoded = binascii.b2a_base64(binary_data).decode(\'utf-8\') return base64_encoded, crc def base64_to_binary(encoded_str, output_file_path, original_crc): Decodes a base64-encoded ASCII string back to its original binary form and verifies data integrity using CRC32. Args: encoded_str (str): The base64-encoded ASCII string. output_file_path (str): Path to the output file where the decoded binary data will be stored. original_crc (int): The original CRC32 checksum of the binary data. Returns: bool: True if integrity check is verified, False otherwise. binary_data = binascii.a2b_base64(encoded_str.encode(\'utf-8\')) new_crc = binascii.crc32(binary_data) with open(output_file_path, \'wb\') as file: file.write(binary_data) return new_crc == original_crc"},{"question":"# Python Coding Assessment Question Problem Statement You are required to implement a function `uppercase_words(input_file: str, output_file: str) -> None` that reads words from a given text file (one word per line), converts them to uppercase using the pipeline defined by the `pipes.Template` class, and writes the result to another specified file. Detailed Requirements - The function should initialize a `pipes.Template` object. - Append a command using the `pipes.Template.append` method to convert the text to uppercase (the shell command for this is `\'tr a-z A-Z\'`). - Use the `pipes.Template.open` method to open the input file for reading and the output file for writing through the pipeline. - Ensure robust error handling and clean resource management (properly closing files, etc.). - This function should work on Unix-like systems with a POSIX-compatible shell. Input - `input_file` (str): Path to the input text file containing words in lowercase (one word per line). - `output_file` (str): Path to the output text file where the uppercase words should be written. Output - None Constraints - You can assume the input file exists and is readable. - The input file will not be empty and will contain alphanumeric words only. Example Suppose the content of `input_file` is: ``` hello world foo bar ``` After running the function, the content of `output_file` should be: ``` HELLO WORLD FOO BAR ``` Notes - This question tests the student\'s ability to work with deprecated modules and their method functionalities. - Students are expected to know file operations in Python and basic shell command usage. # Function Signature ```python def uppercase_words(input_file: str, output_file: str) -> None: pass ```","solution":"import pipes def uppercase_words(input_file: str, output_file: str) -> None: Reads words from the input_file, converts them to uppercase using a pipeline, and writes the result to the output_file. template = pipes.Template() template.append(\'tr a-z A-Z\', \'--\') with template.open(input_file, \'r\') as in_f, open(output_file, \'w\') as out_f: for line in in_f: out_f.write(line)"},{"question":"You have been given the task of simulating a simplified version of the Unix `.netrc` file processing in Python using the `netrc` module described. Follow the guidelines below to implement your solution: Objective Design a function `parse_netrc(file_content: str) -> dict` that takes the content of a netrc file as a string input and returns a dictionary where: - Each key is a hostname. - Each value is a dictionary with keys `login`, `account`, and `password`, holding respective authentication details. Input - `file_content`: A string containing the contents of a netrc file. Each entry in the netrc file will follow the format: ``` machine <hostname> login <login-name> password <password> ``` Output - A dictionary representation of the netrc file content. Constraints - Assume the netrc file content is properly formatted. - You should handle the case where no host entries are present by returning an empty dictionary. - The `netrc` object should use a default host entry if a specific host is not found. Example For the input string: ``` machine host1 login user1 password pass1 machine host2 login user2 password pass2 default login default_user password default_pass ``` The function `parse_netrc(file_content)` should return: ```python { \'host1\': { \'login\': \'user1\', \'account\': None, \'password\': \'pass1\' }, \'host2\': { \'login\': \'user2\', \'account\': None, \'password\': \'pass2\' }, \'default\': { \'login\': \'default_user\', \'account\': None, \'password\': \'default_pass\' } } ``` Requirements 1. Use the `netrc` class for parsing the netrc file content. 2. Safely handle the `NetrcParseError`. 3. Ensure the solution returns a dictionary with the appropriate structure. Function Signature ```python from typing import Optional def parse_netrc(file_content: str) -> dict: pass ``` Notes - You may assume that the `account` field will be `None` if not provided. - The `netrc` object should be initialized by passing the path to a temporary file created from `file_content` for this task. # Hint You can use Python’s `tempfile` module to create a temporary file from the `file_content` string for initializing the `netrc` object.","solution":"import tempfile import netrc def parse_netrc(file_content: str) -> dict: with tempfile.NamedTemporaryFile(delete=False) as temp_netrc: temp_netrc.write(file_content.encode(\'utf-8\')) temp_netrc_path = temp_netrc.name try: parsed_netrc = netrc.netrc(temp_netrc_path) host_data = {} for host, auth_info in parsed_netrc.hosts.items(): login, account, password = auth_info host_data[host] = { \'login\': login, \'account\': account, \'password\': password } return host_data except netrc.NetrcParseError: return {} finally: import os os.remove(temp_netrc_path)"},{"question":"# **Coding Assessment Question** Objective: Using the `filecmp` module, write a Python function that compares the contents of two directories and generates a detailed report in a specified format. This assessment tests your understanding of file and directory comparison, utilization of class attributes, and report generation. Task: Implement the function `generate_comparison_report(dir1: str, dir2: str, ignore: list = None, hide: list = None) -> str`. This function should: 1. Use the `filecmp.dircmp` class to compare the directories `dir1` and `dir2`. 2. Ignore and hide files as specified in the `ignore` and `hide` lists, respectively. 3. Generate a comparison report in the following format: ``` Comparison Report between dir1 and dir2: - Files only in dir1: [<file1>, <file2>, ...] - Files only in dir2: [<file1>, <file2>, ...] - Common files: [<file1>, <file2>, ...] - Identical files: [<file1>, <file2>, ...] - Different files: [<file1>, <file2>, ...] - Errors comparing files: [<file1>, <file2>, ...] ``` 4. Recursively include subdirectory comparisons in the report. Function Signature: ```python def generate_comparison_report(dir1: str, dir2: str, ignore: list = None, hide: list = None) -> str: ``` Input: - `dir1 (str)`: The path to the first directory. - `dir2 (str)`: The path to the second directory. - `ignore (list)`: A list of file names to ignore (default is `None`). - `hide (list)`: A list of file names to hide (default is `None`). Output: - `str`: A detailed comparison report string in the specified format. Constraints: - Both `dir1` and `dir2` must be valid directories. - The function should handle potential file reading issues, such as permission errors, and include them in the errors section of the report. Example: ```python # Example Usage dir1 = \'/path/to/dir1\' dir2 = \'/path/to/dir2\' ignore = [\'.DS_Store\'] hide = [\'__pycache__\'] report = generate_comparison_report(dir1, dir2, ignore, hide) print(report) ``` This question requires a comprehensive understanding of the `filecmp` module, especially the `dircmp` class, and the ability to generate a formatted and detailed report of directory comparisons.","solution":"import filecmp def generate_comparison_report(dir1: str, dir2: str, ignore: list = None, hide: list = None) -> str: dcmp = filecmp.dircmp(dir1, dir2, ignore=ignore, hide=hide) report = f\\"Comparison Report between {dir1} and {dir2}:n\\" report += f\\"- Files only in {dir1}: {dcmp.left_only}n\\" report += f\\"- Files only in {dir2}: {dcmp.right_only}n\\" report += f\\"- Common files: {dcmp.common}n\\" report += f\\"- Identical files: {dcmp.same_files}n\\" report += f\\"- Different files: {dcmp.diff_files}n\\" report += f\\"- Errors comparing files: {dcmp.common_funny}n\\" if dcmp.subdirs: for subdir, sub_dcmp in dcmp.subdirs.items(): sub_report = generate_comparison_report(sub_dcmp.left, sub_dcmp.right, ignore, hide) report += f\\"nComparison Report for subdirectory {subdir}:n{sub_report}\\" return report"},{"question":"Named Tensor Operations You are required to write a function in PyTorch that leverages the named tensor capabilities. The function should handle tensors with named dimensions and perform a series of operations, ensuring dimension names are managed correctly. Your task is to implement two sub-functions within the main function: `process_tensors`. Function Specifications 1. **main function**: `process_tensors` - **Inputs**: - `tensor_1`: A 3D tensor with named dimensions. - `tensor_2`: Another 3D tensor with named dimensions. - **Outputs**: - A resulting tensor after performing specified operations. 2. **sub-function**: `intersect_names` - **Description**: This function calculates the intersection of dimension names between `tensor_1` and `tensor_2`. - **Inputs**: - `tensor_1`: A 3D tensor with named dimensions. - `tensor_2`: Another 3D tensor with named dimensions. - **Output**: A set of names representing the intersection of dimension names. 3. **sub-function**: `align_and_multiply` - **Description**: This function aligns `tensor_2` with `tensor_1` by matching the dimension names. If alignment is successful, it multiplies the tensors element-wise. If they cannot be aligned dimension-wise, it should raise an appropriate error. - **Inputs**: - `tensor_1`: A 3D tensor with named dimensions. - `tensor_2`: Another 3D tensor with named dimensions. - **Output**: A tensor resulting from element-wise multiplication of aligned tensors. Performance Requirements - Ensure the function operates efficiently, maintaining a time complexity suitable for large tensor operations. - Handle scenarios where tensors may or may not have matching dimension names. # Example Usage ```python import torch def process_tensors(tensor_1, tensor_2): def intersect_names(tensor_1, tensor_2): return set(tensor_1.names).intersection(set(tensor_2.names)) def align_and_multiply(tensor_1, tensor_2): try: tensor_2 = tensor_2.align_to(*tensor_1.names) return tensor_1 * tensor_2 except RuntimeError as e: raise ValueError(\\"Alignment of tensors failed\\") from e # Implement usage of the above functions intersection = intersect_names(tensor_1, tensor_2) result = align_and_multiply(tensor_1, tensor_2) return result # Example tensors creation and processing tensor_1 = torch.rand(2, 3, 4, names=(\'Batch\', \'Channel\', \'Width\')) tensor_2 = torch.rand(3, 2, 4, names=(\'Channel\', \'Batch\', \'Width\')) # Processing the tensors result_tensor = process_tensors(tensor_1, tensor_2) print(result_tensor) ``` **Notes**: 1. The tensors `tensor_1` and `tensor_2` are expected to initially have named dimensions. 2. The function needs to handle and align names correctly before processing. 3. The student should ensure the correct implementation of sub-functions while adhering to the given pointer.","solution":"import torch def process_tensors(tensor_1, tensor_2): Process the given named tensors by calculating the intersection of dimension names, aligning the tensors, and multiplying them element-wise. Args: tensor_1 (torch.Tensor): First 3D tensor with named dimensions. tensor_2 (torch.Tensor): Second 3D tensor with named dimensions. Returns: torch.Tensor: The resulting tensor after element-wise multiplication. def intersect_names(tensor_1, tensor_2): Calculate the intersection of dimension names between tensor_1 and tensor_2. Args: tensor_1 (torch.Tensor): First tensor with named dimensions. tensor_2 (torch.Tensor): Second tensor with named dimensions. Returns: set: The set of shared dimension names. return set(tensor_1.names).intersection(set(tensor_2.names)) def align_and_multiply(tensor_1, tensor_2): Align tensor_2 with tensor_1 by matching dimension names and perform element-wise multiplication. Args: tensor_1 (torch.Tensor): First tensor with named dimensions. tensor_2 (torch.Tensor): Second tensor with named dimensions. Returns: torch.Tensor: The resulting tensor after element-wise multiplication. Raises: ValueError: If the alignment fails. try: tensor_2_aligned = tensor_2.align_to(*tensor_1.names) return tensor_1 * tensor_2_aligned except RuntimeError as e: raise ValueError(\\"Alignment of tensors failed\\") from e # Determine the intersection of dimension names (for additional operations if needed) intersection = intersect_names(tensor_1, tensor_2) # Align and multiply the tensors result = align_and_multiply(tensor_1, tensor_2) return result # Example tensors creation and processing tensor_1 = torch.rand(2, 3, 4, names=(\'Batch\', \'Channel\', \'Width\')) tensor_2 = torch.rand(3, 2, 4, names=(\'Channel\', \'Batch\', \'Width\')) # Processing the tensors result_tensor = process_tensors(tensor_1, tensor_2) print(result_tensor)"},{"question":"**Coding Assessment Question: Pandas Styling and Export** **Objective:** Demonstrate your understanding of the `pandas` `Styler` class by creating a DataFrame, applying styles, and exporting the styled DataFrame. **Problem Statement:** You are provided with the sales data of a company for different months. Your task is to: 1. Create a DataFrame from the given data. 2. Apply various styles to highlight key information. 3. Format the data for better readability. 4. Export the styled DataFrame to an HTML file. **Data:** ``` { \'Month\': [\'January\', \'February\', \'March\', \'April\', \'May\'], \'Sales\': [20000, 30000, 25000, 26000, 22000], \'Profit\': [5000, 7000, 6000, 6500, 4800], \'Growth Rate\': [0.05, 0.10, 0.08, 0.07, 0.06] } ``` **Requirements:** 1. **Create a DataFrame** from the provided data. 2. **Highlight** the cell with the maximum and minimum sales. 3. Apply a **background gradient** based on the profit values. 4. **Format** the \'Sales\' and \'Profit\' columns to include a dollar sign and commas as thousand separators. 5. **Format** the \'Growth Rate\' column as percentages with two decimal places. 6. Export the styled DataFrame to an HTML file named \'styled_sales.html\'. **Expected Input and Output Formats:** - **Input:** No direct input as the data is provided within the problem statement. - **Output:** HTML file named \'styled_sales.html\'. **Constraints:** - Use only the methods from the `Styler` class to apply styles and format data. - Ensure the HTML file generated is standalone and can be opened in a web browser to view the styled DataFrame. **Performance:** - Ensure that the solution is efficient with respect to applying styles and exporting the DataFrame. **Function Signature:** ```python def style_and_export_sales_data(): pass ``` **Sample HTML Output:** After applying the styles and exporting, opening \'styled_sales.html\' should display a visually styled DataFrame with appropriate highlights, gradients, and formats. **Guidelines:** - Carefully read the documentation for methods like `highlight_max`, `highlight_min`, `background_gradient`, `format`, and `to_html`. - Test your function to ensure styles are correctly applied and the HTML file is generated as expected. **Note:** Submit only the function `style_and_export_sales_data()` and ensure all styles are applied using the `pandas` `Styler` methods.","solution":"import pandas as pd def style_and_export_sales_data(): # Data data = { \'Month\': [\'January\', \'February\', \'March\', \'April\', \'May\'], \'Sales\': [20000, 30000, 25000, 26000, 22000], \'Profit\': [5000, 7000, 6000, 6500, 4800], \'Growth Rate\': [0.05, 0.10, 0.08, 0.07, 0.06] } # Create DataFrame df = pd.DataFrame(data) # Style DataFrame df_styled = df.style .highlight_max(subset=[\'Sales\'], color=\'lightgreen\') .highlight_min(subset=[\'Sales\'], color=\'lightcoral\') .background_gradient(subset=[\'Profit\'], cmap=\'viridis\') .format({ \'Sales\': \'{:,.0f}\', \'Profit\': \'{:,.0f}\', \'Growth Rate\': \'{:.2%}\' }) # Export to HTML df_styled.to_html(\'styled_sales.html\')"},{"question":"**Dataset Description:** You are provided with a dataset containing information on various products in a store. The dataset is in CSV format and contains the following columns: * `ProductID`: Unique identifier for the product * `Category`: Category to which the product belongs * `Price`: Price of the product * `Quantity`: Quantity of the product available in stock * `Sales`: Total sales of the product **Instructions:** 1. Read the dataset into a pandas DataFrame. 2. Use pandas options to configure the display settings for the following: * Set the display precision for floating-point numbers to 2 decimal places. * Configure pandas to display up to 50 rows and all columns when printing the DataFrame. 3. Write a function `category_summary(df)` that accepts the DataFrame and returns a summary dictionary with the following information for each category: * Total number of products * Average price of the products * Total sales for the category Ensure your function meets the following constraints: * The function should handle any realistic size of the dataset efficiently. * Return the summary as a dictionary where keys are the categories and values are another dictionary containing `total_products`, `average_price`, and `total_sales`. **Expected Input and Output:** * Input: DataFrame with the structure as described above. * Output: Dictionary of dictionaries summarizing the product categories. ```python import pandas as pd def category_summary(df): Args: df : pd.DataFrame : DataFrame containing product information Returns: dict : Dictionary summarizing total products, average price, and total sales for each category. # Your implementation here # Example usage: # df = pd.read_csv(\'products.csv\') # summary = category_summary(df) ``` For example, given the following DataFrame: | ProductID | Category | Price | Quantity | Sales | |-----------|----------|-------|----------|-------| | 1 | A | 10.50 | 100 | 1050 | | 2 | B | 20.75 | 50 | 1037.5| | 3 | A | 12.30 | 75 | 922.5 | | 4 | B | 22.00 | 30 | 660 | | 5 | C | 15.00 | 20 | 300 | The output should be: ```python { \'A\': {\'total_products\': 2, \'average_price\': 11.40, \'total_sales\': 1972.5}, \'B\': {\'total_products\': 2, \'average_price\': 21.38, \'total_sales\': 1697.5}, \'C\': {\'total_products\': 1, \'average_price\': 15.00, \'total_sales\': 300.0} } ```","solution":"import pandas as pd # Configuring pandas display options pd.options.display.precision = 2 pd.options.display.max_rows = 50 pd.options.display.max_columns = None def category_summary(df): Args: df : pd.DataFrame : DataFrame containing product information Returns: dict : Dictionary summarizing total products, average price, and total sales for each category. summary = {} grouped = df.groupby(\'Category\').agg( total_products=(\'ProductID\', \'size\'), average_price=(\'Price\', \'mean\'), total_sales=(\'Sales\', \'sum\') ).reset_index() for _, row in grouped.iterrows(): category = row[\'Category\'] summary[category] = { \'total_products\': row[\'total_products\'], \'average_price\': round(row[\'average_price\'], 2), \'total_sales\': round(row[\'total_sales\'], 2) } return summary"},{"question":"You have been tasked with implementing a simple asynchronous chat server using the deprecated `asynchat` module. Your server should be able to handle multiple clients, receive messages from any client, and broadcast these messages to all connected clients. **Requirements:** 1. Implement a subclass of `asynchat.async_chat` called `ChatHandler`. 2. `ChatHandler` should handle incoming data by collecting it until a newline character (`n`) is encountered. 3. Upon encountering the newline character, the message should be broadcast to all connected clients. 4. Implement an `AsyncChatServer` class that listens for incoming connections and spawns a `ChatHandler` for each client. **Input:** - Connection requests from multiple clients. - Messages from clients, delimited by `n`. **Output:** - Messages should be broadcast to all clients with the format: `\\"[username]: message\\"`. - Implement a simple username protocol where the first message received from a client is considered their username. **Constraints:** - You must handle disconnections gracefully. - Ensure all exceptions are caught and logged without crashing the server. - For simplicity, you can limit the maximum number of clients to 10. # Example: Let\'s say two clients connect to your chat server: 1. Client 1 sends: `Alicen` - This is recognized as the username for Client 1. 2. Client 2 sends: `Bobn` - This is recognized as the username for Client 2. 3. Client 1 sends: `Hello, everyone!n` - The server should broadcast: `[Alice]: Hello, everyone!` to both Alice and Bob. 4. Client 2 sends: `Hi Alice!n` - The server should broadcast: `[Bob]: Hi Alice!` to both Alice and Bob. # Implementation Steps: 1. Create the `ChatHandler` class. 2. Create the `AsyncChatServer` class to manage client connections. 3. Implement methods to collect data, recognize terminators, and broadcast messages. ```python import asynchat import asyncore class ChatHandler(asynchat.async_chat): def __init__(self, sock, server): super().__init__(sock=sock) self.server = server self.set_terminator(b\'n\') self.data = [] self.username = None def collect_incoming_data(self, data): self.data.append(data) def found_terminator(self): message = b\\"\\".join(self.data).decode(\'utf-8\').strip() self.data = [] if self.username is None: # The first message is the username self.username = message self.server.broadcast(f\\"{self.username} has joined the chat.n\\") else: self.server.broadcast(f\\"[{self.username}]: {message}n\\") def handle_close(self): self.server.remove_handler(self) self.close() class AsyncChatServer(asyncore.dispatcher): def __init__(self, address): super().__init__() self.create_socket() self.set_reuse_addr() self.bind(address) self.listen(5) self.clients = [] def handle_accept(self): sock, _ = self.accept() handler = ChatHandler(sock, self) self.clients.append(handler) def broadcast(self, message): for client in self.clients: client.push(message.encode(\'utf-8\')) def remove_handler(self, handler): self.clients.remove(handler) if __name__ == \\"__main__\\": server = AsyncChatServer((\'localhost\', 5000)) asyncore.loop() ```","solution":"import asynchat import asyncore class ChatHandler(asynchat.async_chat): def __init__(self, sock, server): super().__init__(sock=sock) self.server = server self.set_terminator(b\'n\') self.data = [] self.username = None def collect_incoming_data(self, data): self.data.append(data) def found_terminator(self): message = b\\"\\".join(self.data).decode(\'utf-8\').strip() self.data = [] if self.username is None: self.username = message self.server.broadcast(f\\"{self.username} has joined the chat.n\\") else: self.server.broadcast(f\\"[{self.username}]: {message}n\\") def handle_close(self): self.server.remove_handler(self) self.close() if self.username: self.server.broadcast(f\\"{self.username} has left the chat.n\\") class AsyncChatServer(asyncore.dispatcher): def __init__(self, address): super().__init__() self.create_socket() self.set_reuse_addr() self.bind(address) self.listen(5) self.clients = [] def handle_accept(self): sock, _ = self.accept() handler = ChatHandler(sock, self) self.clients.append(handler) def broadcast(self, message): for client in self.clients: client.push(message.encode(\'utf-8\')) def remove_handler(self, handler): self.clients.remove(handler) if __name__ == \\"__main__\\": server = AsyncChatServer((\'localhost\', 5000)) asyncore.loop()"},{"question":"Objective: You are required to implement a function that utilizes the PyTorch `torch.nn.attention` module to create a custom attention mechanism. This function will be used in a mini neural network model for sequence data classification. Description: 1. **Function Name**: `custom_attention_net` 2. **Input**: - `input_tensor` (torch.Tensor): A 3D tensor of shape `(batch_size, seq_len, input_size)`, where `batch_size` is the number of sequences, `seq_len` is the length of each sequence, and `input_size` is the size of the input features. - `hidden_dim` (int): The hidden dimension size for the attention mechanism. 3. **Output**: - Returns a reshaped tensor after applying the custom attention mechanism and an additional feedforward layer, of shape `(batch_size, final_output_dim)` where `final_output_dim` is a dimension determined by your implementation. Constraints and Requirements: - Use `torch.nn.attention` submodules to implement the attention mechanism. - Implement a basic neural network that includes an attention layer followed by a fully connected layer. - Ensure the implementation is efficient and utilizes GPU acceleration if available. Performance: - The implementation should handle input tensors up to a shape of `(64, 100, 256)` within a reasonable amount of time and memory usage. Example Usage: ```python import torch # Example input tensor with batch_size=2, seq_len=5, and input_size=10 input_tensor = torch.rand(2, 5, 10) # Hidden dimension size for the attention mechanism hidden_dim = 20 # Call your custom attention network function output_tensor = custom_attention_net(input_tensor, hidden_dim) print(output_tensor.shape) # Expected shape (2, final_output_dim) ``` Hints: - You may need to implement custom layers using `torch.nn.Module` class. - Utilize appropriate PyTorch functions to ensure GPU compatibility. - Look into `flex_attention` and `bias` submodules for necessary components.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomAttentionNet(nn.Module): def __init__(self, input_size, hidden_dim): super(CustomAttentionNet, self).__init__() self.hidden_dim = hidden_dim # Attention mechanism components self.query = nn.Linear(input_size, hidden_dim) self.key = nn.Linear(input_size, hidden_dim) self.value = nn.Linear(input_size, hidden_dim) self.attention = nn.MultiheadAttention(hidden_dim, num_heads=1) # Feedforward layer self.fc = nn.Linear(hidden_dim, 1) # Output size 1 for binary classification def forward(self, input_tensor): batch_size, seq_len, _ = input_tensor.size() # Create query, key, and value matrices query = self.query(input_tensor) # (batch_size, seq_len, hidden_dim) key = self.key(input_tensor) # (batch_size, seq_len, hidden_dim) value = self.value(input_tensor) # (batch_size, seq_len, hidden_dim) # Rearrange tensors to fit the MultiheadAttention input format query = query.permute(1, 0, 2) # (seq_len, batch_size, hidden_dim) key = key.permute(1, 0, 2) # (seq_len, batch_size, hidden_dim) value = value.permute(1, 0, 2) # (seq_len, batch_size, hidden_dim) # Apply attention attn_output, _ = self.attention(query, key, value) attn_output = attn_output.permute(1, 0, 2) # (batch_size, seq_len, hidden_dim) # Pooling to get a fixed-size tensor attn_output = torch.mean(attn_output, dim=1) # (batch_size, hidden_dim) # Apply feedforward layer output_tensor = self.fc(attn_output) # (batch_size, 1) return output_tensor def custom_attention_net(input_tensor, hidden_dim): input_size = input_tensor.size(2) model = CustomAttentionNet(input_size, hidden_dim) # Move the model to GPU if available device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model.to(device) input_tensor = input_tensor.to(device) return model(input_tensor)"},{"question":"You are provided with the `penguins` dataset from the `seaborn` library. Your task is to visualize the dataset using Seaborn\'s `objects` interface, specifically focusing on using the `Jitter` transformation effectively. **Function Signature:** ```python def visualize_penguins(): pass ``` # Instructions: 1. Load the `penguins` dataset using `seaborn`\'s `load_dataset` function. 2. Create a plot to visualize the relationship between the `species` and their `body_mass_g` while applying a moderate amount of jitter to the `species` axis. 3. Create a second plot to visualize the relationship between the `body_mass_g` and `flipper_length_mm` with specific jitter-values in both `x` (200 units) and `y` (5 units) directions. # Constraints: - You must use the `seaborn.objects` API. - Each visualization should be created in a single code cell. - Ensure the plots are clearly labeled with appropriate titles. # Output: - The function does not return any values but should display the plots as described. **Example Output:** 1. Plot 1: Jitter applied on `species` axis with `body_mass_g`. - Title: \\"Body Mass by Species with Jitter\\" 2. Plot 2: Jitter applied on both `x` and `y` axes for `body_mass_g` and `flipper_length_mm`. - Title: \\"Jittered Relationship between Body Mass and Flipper Length\\" ```python # Plot 1 ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(0.5)) .title(\\"Body Mass by Species with Jitter\\") ) # Plot 2 ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1)) .add(so.Dots(), so.Jitter(x=200, y=5)) .title(\\"Jittered Relationship between Body Mass and Flipper Length\\") ) ``` **Notes:** - Focus on clarity and presentation of the plotted data. - Ensure the jitter values are applied correctly.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def visualize_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Plot 1: Body Mass by Species with Jitter plot1 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dot(), so.Jitter(0.2)) .label(title=\\"Body Mass by Species with Jitter\\") ) plot1.show() # Plot 2: Jittered Relationship between Body Mass and Flipper Length plot2 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dot(), so.Jitter(x=200, y=5)) .label(title=\\"Jittered Relationship between Body Mass and Flipper Length\\") ) plot2.show()"},{"question":"Objective: To assess the student\'s ability to use weak references in the \\"weakref\\" module to manage memory efficiently and implement a cache system. Problem Statement: You are required to implement a simple caching system for large objects using weak references. The goal is to ensure that objects are cached, but can be garbage collected when no longer in use elsewhere in the program. Details: 1. Implement a `Cache` class that will cache objects using their IDs as keys. 2. The cache should store objects using weak references so that objects can be garbage collected when they are no longer referenced. 3. Implement the following methods in the `Cache` class: - `add_to_cache(obj)`: Adds an object to the cache and returns its ID. - `get_from_cache(obj_id)`: Retrieves an object from the cache using its ID. If the object is no longer in the cache, returns `None`. Constraints: - Use the `weakref` module to manage weak references. - Ensure that the cache does not prevent objects from being garbage collected. - Your implementation should be thread-safe. Input and Output: - The `add_to_cache(obj)` method accepts an object and returns a unique ID for the object. - The `get_from_cache(obj_id)` method accepts an object ID and returns the corresponding object if it is still in the cache; otherwise, it returns `None`. Example: ```python import weakref class Cache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add_to_cache(self, obj): obj_id = id(obj) self._cache[obj_id] = obj return obj_id def get_from_cache(self, obj_id): return self._cache.get(obj_id, None) # Usage if __name__ == \\"__main__\\": cache = Cache() class LargeObject: def __init__(self, data): self.data = data obj = LargeObject(\\"some large data\\") obj_id = cache.add_to_cache(obj) # Retrieve the object from cache retrieved_obj = cache.get_from_cache(obj_id) print(retrieved_obj.data) # Output: some large data # Delete the original reference del obj # Try to retrieve the object from cache again retrieved_obj = cache.get_from_cache(obj_id) print(retrieved_obj) # Output: None, since the object has been garbage collected ``` Explanation: - The `Cache` class uses `weakref.WeakValueDictionary` to store objects using their IDs as keys. - The `add_to_cache` method adds the object to the cache and returns the object\'s ID. - The `get_from_cache` method retrieves the object using its ID. If the object has been garbage collected, it returns `None`. By using weak references in the cache, we ensure that objects can be garbage collected when no other strong references to them exist, thus preventing memory leaks in the application.","solution":"import weakref from threading import Lock class Cache: def __init__(self): self._cache = weakref.WeakValueDictionary() self._lock = Lock() def add_to_cache(self, obj): obj_id = id(obj) with self._lock: self._cache[obj_id] = obj return obj_id def get_from_cache(self, obj_id): with self._lock: return self._cache.get(obj_id, None)"},{"question":"**Objective:** Demonstrate your understanding of file control and I/O operations using the `fcntl` module in Python. **Task Description:** You are required to implement a Python function `file_lock_operations` that performs the following operations on a given file: 1. Opens a specified file for reading and writing. 2. Attempts to set a non-blocking exclusive lock on the file. 3. Reads the contents of the file. 4. Prints the contents of the file to the console. 5. Modifies the contents of the file by appending a provided string. 6. Releases the lock on the file. 7. Properly handles any possible exceptions that may occur during these operations. **Function Signature:** ```python def file_lock_operations(file_path: str, append_str: str) -> None: pass ``` **Input:** - `file_path` (str): The path to the file to be operated on. - `append_str` (str): The string to append to the file\'s contents. **Output:** - The function does not return any value but should print the initial contents of the file before appending. **Constraints:** - You must use the `fcntl.flock` function to lock the file. - Properly handle any exceptions raised, particularly those related to file I/O and locking. - Assume the file exists and is writable. **Example:** ```python # Assuming the file \'example.txt\' initially contains \\"Hello world.\\" file_lock_operations(\'example.txt\', \' Python\') ``` **Expected Output:** ``` Hello world. ``` After the above function call, the content of `example.txt` should be: ``` Hello world. Python ``` **Additional Notes:** 1. Ensure that the lock is acquired before performing any read or write operations. 2. Release the lock once the operation is complete. 3. Use exception handling to manage file I/O and locking errors gracefully.","solution":"import fcntl def file_lock_operations(file_path: str, append_str: str) -> None: Opens a specified file for reading and writing, attempts to set a non-blocking exclusive lock on the file, reads the contents of the file, prints the contents, modifies the contents by appending a provided string, and releases the lock on the file. :param file_path: The path to the file to be operated on. :param append_str: The string to append to the file\'s contents. try: with open(file_path, \'r+\') as f: try: fcntl.flock(f, fcntl.LOCK_EX | fcntl.LOCK_NB) # Read current contents contents = f.read() print(contents) # Move the cursor to the end of the file f.seek(0, 2) # Append the new string to the file f.write(append_str) finally: # Always release the lock fcntl.flock(f, fcntl.LOCK_UN) except IOError as e: print(f\\"An I/O error occurred: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# Python Coding Assessment: Advanced Cookie Management Objective Design and implement a set of functions to manage HTTP cookies using the `http.cookies` module. The functions should demonstrate an understanding of cookie creation, manipulation, and retrieval of cookie values with constraints that require careful handling of cookie attributes and exception management. Task You are required to implement the following functions: 1. **create_simple_cookie** 2. **add_cookie_with_attributes** 3. **retrieve_cookie_value** Function Specifications 1. **create_simple_cookie** - **Input**: A dictionary where keys are cookie names and values are cookie values. - **Output**: A `SimpleCookie` object containing the provided cookies. - **Example**: ```python cookies_dict = {\\"user\\": \\"john_doe\\", \\"session_id\\": \\"abc123\\"} cookie = create_simple_cookie(cookies_dict) # Expected output: a SimpleCookie object with the specified cookies ``` 2. **add_cookie_with_attributes** - **Input**: A `SimpleCookie` object, a cookie name (string), a cookie value (string), and a dictionary of attributes (keys should be valid RFC 2109 attributes and values as strings). - **Output**: The same `SimpleCookie` object with the new cookie and attributes added. - **Example**: ```python simple_cookie = create_simple_cookie({\\"user\\": \\"john_doe\\"}) attributes = {\\"path\\": \\"/\\", \\"secure\\": \\"True\\"} updated_cookie = add_cookie_with_attributes(simple_cookie, \\"session_id\\", \\"abc123\\", attributes) # Expected output: The SimpleCookie object now includes \\"session_id\\" with the specified attributes ``` 3. **retrieve_cookie_value** - **Input**: A `SimpleCookie` object, a cookie name (string). - **Output**: The value of the cookie if it exists, otherwise raise a `CookieError`. - **Example**: ```python cookie = create_simple_cookie({\\"user\\": \\"john_doe\\"}) value = retrieve_cookie_value(cookie, \\"user\\") # Expected output: \\"john_doe\\" ``` - **Note**: Handle the case where the cookie does not exist by raising a `CookieError`. Constraints 1. All cookie names and values are strings composed of valid RFC 2109 characters. 2. Attribute keys provided in `add_cookie_with_attributes` should be validated. 3. Raise appropriate exceptions for any invalid operations. Performance Ensure that your solution is efficient and handles typical use cases under expected constraints. Example Usage ```python from http.cookies import SimpleCookie, CookieError def create_simple_cookie(cookie_dict): # Your implementation here pass def add_cookie_with_attributes(cookie, name, value, attributes): # Your implementation here pass def retrieve_cookie_value(cookie, name): # Your implementation here pass # Example Test try: cookie_dict = {\\"user\\": \\"john_doe\\", \\"session_id\\": \\"abc123\\"} simple_cookie = create_simple_cookie(cookie_dict) attributes = {\\"path\\": \\"/\\", \\"secure\\": \\"True\\"} updated_cookie = add_cookie_with_attributes(simple_cookie, \\"cart_id\\", \\"def456\\", attributes) user_value = retrieve_cookie_value(simple_cookie, \\"user\\") print(user_value) # Output: john_doe non_existing_value = retrieve_cookie_value(simple_cookie, \\"non_existing\\") # Should raise CookieError except CookieError as e: print(f\\"CookieError: {e}\\") ``` Implement these functions while adhering to the outlined specifications and constraints. Evaluate the robustness of your code with various test cases to ensure correct functionality.","solution":"from http.cookies import SimpleCookie, CookieError def create_simple_cookie(cookie_dict): Creates a SimpleCookie object with the cookies from the provided dictionary. Args: cookie_dict (dict): A dictionary with cookie names as keys and cookie values as values. Returns: SimpleCookie: A SimpleCookie object with the specified cookies. cookie = SimpleCookie() for key, value in cookie_dict.items(): cookie[key] = value return cookie def add_cookie_with_attributes(cookie, name, value, attributes): Adds a cookie with a specified name, value, and attributes to a SimpleCookie object. Args: cookie (SimpleCookie): The SimpleCookie object to which the cookie should be added. name (str): The name of the cookie. value (str): The value of the cookie. attributes (dict): A dictionary of attributes for the cookie. Returns: SimpleCookie: The updated SimpleCookie object. cookie[name] = value for attr_key, attr_value in attributes.items(): if attr_key in [\'expires\', \'path\', \'comment\', \'domain\', \'max-age\', \'secure\', \'version\', \'httponly\']: cookie[name][attr_key] = attr_value else: raise CookieError(f\\"Invalid attribute: {attr_key}\\") return cookie def retrieve_cookie_value(cookie, name): Retrieves the value of the specified cookie name from a SimpleCookie object. Args: cookie (SimpleCookie): The SimpleCookie object from which the cookie value should be retrieved. name (str): The name of the cookie. Returns: str: The value of the cookie. Raises: CookieError: If the specified cookie name does not exist. if name in cookie: return cookie[name].value else: raise CookieError(f\\"Cookie \'{name}\' not found.\\")"},{"question":"# Question: You are given an XML document representing a simple library catalog consisting of books. Each `book` element has several sub-elements including `title`, `author`, and `year`. Your task is to write a Python function that performs the following operations using the `xml.etree.ElementTree` module: 1. **Parse the given XML string.** 2. **Extract and return the titles of all books published by a given author.** 3. **Add a new book to the catalog.** 4. **Modify the year of publication for a specific book title.** 5. **Convert the modified XML tree back into a string.** Function Signature: ```python def manage_library_catalog(xml_data: str, author_name: str, new_book: dict, target_book_title: str, new_year: str) -> str: ``` Parameters: - `xml_data` (str): A string containing the XML data of the library catalog. - `author_name` (str): The name of the author whose book titles need to be extracted. - `new_book` (dict): A dictionary with keys `title`, `author`, and `year` representing the new book to be added. - `target_book_title` (str): The title of the book for which the year of publication needs to be updated. - `new_year` (str): The new year of publication for the specified book. Returns: - A string containing the modified XML data. Example XML Data: ```xml <library> <book> <title>Python 101</title> <author>John Doe</author> <year>2020</year> </book> <book> <title>XML Basics</title> <author>Jane Smith</author> <year>2018</year> </book> </library> ``` Example Usage: ```python xml_data = <library> <book> <title>Python 101</title> <author>John Doe</author> <year>2020</year> </book> <book> <title>XML Basics</title> <author>Jane Smith</author> <year>2018</year> </book> </library> author_name = \\"Jane Smith\\" new_book = {\\"title\\": \\"Advanced XML\\", \\"author\\": \\"Jane Smith\\", \\"year\\": \\"2021\\"} target_book_title = \\"Python 101\\" new_year = \\"2022\\" modified_xml = manage_library_catalog(xml_data, author_name, new_book, target_book_title, new_year) print(modified_xml) ``` Constraints: - Assume `xml_data` is well-formed. - If the `target_book_title` does not exist, no changes should be made to the publication year. - The new book should be added as the last child of the library. Detailed Function Implementation: 1. **Parsing XML:** Use `ElementTree.fromstring()` to parse the XML string. 2. **Finding Books by Author:** Traverse the XML tree and collect titles of books with the given author. 3. **Adding a Book:** Create a new `book` element and append it to the library. 4. **Modifying Publication Year:** Locate the specified book and update its year. 5. **Return XML String:** Convert the modified XML tree to a string using `ElementTree.tostring()`.","solution":"import xml.etree.ElementTree as ET def manage_library_catalog(xml_data: str, author_name: str, new_book: dict, target_book_title: str, new_year: str) -> str: Manages a library catalog by parsing an XML string, extracting book titles, adding a new book, modifying the year of a specific book, and returning the updated XML string. # Parse the XML string root = ET.fromstring(xml_data) # Extract titles of books by the given author titles_by_author = [] for book in root.findall(\'book\'): author = book.find(\'author\').text if author == author_name: title = book.find(\'title\').text titles_by_author.append(title) # Add the new book to the catalog new_book_element = ET.SubElement(root, \'book\') new_title_element = ET.SubElement(new_book_element, \'title\') new_title_element.text = new_book[\'title\'] new_author_element = ET.SubElement(new_book_element, \'author\') new_author_element.text = new_book[\'author\'] new_year_element = ET.SubElement(new_book_element, \'year\') new_year_element.text = new_book[\'year\'] # Modify the year of the specific book title for book in root.findall(\'book\'): title = book.find(\'title\').text if title == target_book_title: year_element = book.find(\'year\') year_element.text = new_year # Convert the modified XML tree back to a string modified_xml = ET.tostring(root, encoding=\'unicode\') return modified_xml"},{"question":"# **Coding Assessment Question** Objective Implement a Python function to retrieve and process information from the Unix password database using the `pwd` module. The function should return a list of user login names along with their user ID and home directory, but only for specific users as described below. Function Signature ```python def get_user_details(user_names: list) -> list: ``` Input - `user_names`: A list of strings, each representing a login name that needs to be processed. Example: `[\'user1\', \'user2\']` Output - The function should return a list of tuples. Each tuple contains: - The user\'s login name - The user\'s numerical user ID (`uid`) - The user\'s home directory Example: `[(\'user1\', 1001, \'/home/user1\'), (\'user2\', 1002, \'/home/user2\')]` Constraints - You should handle the situation where a user name does not exist in the password database. - In such cases, skip that user and do not include it in the result list. - You should use the `pwd.getpwnam()` function to retrieve the user\'s password entry. Example ```python user_names = [\'alice\', \'bob\', \'nonexistentuser\'] result = get_user_details(user_names) # Example output, assuming \'alice\' and \'bob\' exist: # [(\'alice\', 1000, \'/home/alice\'), (\'bob\', 1001, \'/home/bob\')] ``` Notes - Your solution should be efficient and handle the potential absence of requested usernames gracefully without throwing errors. - Ensure to import the necessary module (`pwd`) for your implementation.","solution":"import pwd def get_user_details(user_names): Retrieve and process user details from the Unix password database. Args: user_names (list): List of user login names to be processed. Returns: list: List of tuples containing login name, UID, and home directory for each found user. user_details = [] for user_name in user_names: try: user_info = pwd.getpwnam(user_name) user_details.append((user_name, user_info.pw_uid, user_info.pw_dir)) except KeyError: # User does not exist, skip the user continue return user_details"},{"question":"**Coding Assessment Question** # Objective This question aims to test your understanding of using the MPS backend in PyTorch to perform GPU computations on MacOS devices. # Problem Statement You are provided with a simple neural network model and some input data. Your task is to: 1. Check if the MPS backend is available on the current device. 2. If available, move both the input data and the model to the MPS device. 3. Perform a forward pass of the model using the input data and return the output. # Requirements Implement the following function: ```python import torch def run_on_mps(input_data): Perform computations using the MPS backend if available. Parameters: input_data (torch.Tensor): A tensor containing input data. Returns: torch.Tensor: The output tensor after performing a forward pass of the model on the input data. # Check if MPS is available if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available on this device.\\") # Define a simple neural network model class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = torch.nn.Linear(5, 1) # A simple linear layer def forward(self, x): return self.linear(x) # Move the input data and the model to MPS device mps_device = torch.device(\\"mps\\") input_data = input_data.to(mps_device) model = SimpleNet().to(mps_device) # Perform a forward pass output = model(input_data) return output ``` # Constraints - Your solution should raise a `RuntimeError` with the message \\"MPS backend is not available on this device.\\" if the MPS backend is not accessible. - Assume that the input_data is a tensor of appropriate dimensions for the model. - The model defined is a simple linear layer for demonstration purposes. # Example ```python import torch # Example input data input_data = torch.ones(5) # A tensor with 5 elements try: output = run_on_mps(input_data) print(\\"Model output:\\", output) except RuntimeError as e: print(e) ``` In this example, the code checks if the MPS backend is available, moves the model and input data to the MPS device, and performs a computation. If MPS is not available, it raises a runtime error.","solution":"import torch def run_on_mps(input_data): Perform computations using the MPS backend if available. Parameters: input_data (torch.Tensor): A tensor containing input data. Returns: torch.Tensor: The output tensor after performing a forward pass of the model on the input data. # Check if MPS is available if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available on this device.\\") # Define a simple neural network model class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = torch.nn.Linear(5, 1) # A simple linear layer def forward(self, x): return self.linear(x) # Move the input data and the model to MPS device mps_device = torch.device(\\"mps\\") input_data = input_data.to(mps_device) model = SimpleNet().to(mps_device) # Perform a forward pass output = model(input_data) return output"},{"question":"# Advanced Coding Assessment Question **Objective**: Implement a Python module using an embedded C extension module workflow provided by the `python310` package documentation. **Task**: Write a Python script that utilizes the `python310` package to create, add attributes and methods to a module, and then retrieve and manipulate its state. This exercise tests the student\'s understanding of module management and extension embedding. **Instructions**: 1. **Module Creation**: - Create a new module named `\\"example_module\\"` using `PyModule_NewObject`. 2. **Add Attributes/Methods**: - Add the following attributes to your module using the appropriate `PyModule_Add*` functions: - An integer constant named `\\"int_value\\"` with value `123`. - A string constant named `\\"str_value\\"` with value `\\"Hello, World!\\"`. 3. **Function Implementation**: - Implement a simple function `add(a, b)` to the module that takes two integers as parameters and returns their sum. 4. **Manage Module State**: - Create and manage a state for your module to store the count of how many times the `add` function has been called. This count should be a module-level integer attribute named `\\"add_call_count\\"`. 5. **Retrieve and Display Information**: - Write Python code to import your module, use the `add` function multiple times, and print out the final value of the `\\"add_call_count\\"` attribute. 6. **Constraints**: - Ensure that the module and all its attributes/functions are set up correctly without raising any errors. - Use proper Python and C embedding techniques based on the documentation provided. **Expected Output**: 1. Successful creation of the module with the specified attributes. 2. Correct implementation and call of the `add` function. 3. Accurate tracking and displaying of the `add_call_count`. **Example**: ```python # Assuming the module and attributes are set up correctly, the usage might look like this: import example_module print(example_module.int_value) # Output: 123 print(example_module.str_value) # Output: Hello, World! print(example_module.add(3, 5)) # Output: 8 print(example_module.add(10, 20)) # Output: 30 print(example_module.add_call_count) # Output: 2 ``` This problem will assess the student\'s ability to work with both Python and embedded C extension modules, understand module state management, and manipulate module attributes and functions dynamically.","solution":"# Import the required Python C API modules import importlib.util import sys from types import ModuleType # Define a helper function to create a module using C extension-like structure in Python class ExampleModule(ModuleType): def __init__(self, name): super().__init__(name) self.int_value = 123 self.str_value = \\"Hello, World!\\" self.add_call_count = 0 def add(self, a, b): self.add_call_count += 1 return a + b # Create the module object example_module = ExampleModule(\'example_module\') # Insert the newly created module into sys.modules sys.modules[\'example_module\'] = example_module # Make sure the module is available to import importlib.import_module(\'example_module\')"},{"question":"**Question: Implement a Simple Asynchronous Task Scheduler** You are tasked with implementing a basic asynchronous task scheduler using the asyncio library. The scheduler should manage and run a set of given tasks concurrently, ensuring that all tasks are executed asynchronously. # Requirements: 1. Implement a function `task_scheduler(tasks: List[Coroutine], max_concurrent_tasks: int) -> List[Any]` that: - Accepts a list of coroutines (`tasks`). - Accepts an integer (`max_concurrent_tasks`) specifying the maximum number of tasks that can run concurrently. - Returns a list of results after all tasks have completed. 2. The function should: - Use asyncio\'s facilities for running tasks concurrently. - Ensure that no more than `max_concurrent_tasks` tasks are running at the same time. # Input: - `tasks`: A list of coroutines (functions defined with `async` keyword and using `await`). - `max_concurrent_tasks`: An integer specifying the maximum number of concurrent tasks to run. # Output: - A list of results, corresponding to the completion of each task given in the input list. # Constraints: - The number of tasks `n` in the `tasks` list will be between 1 and 1000. - The `max_concurrent_tasks` will be between 1 and 100. # Example: ```python import asyncio async def example_task(n): await asyncio.sleep(n) return f\\"Task {n} completed\\" tasks = [example_task(i) for i in range(1, 6)] max_concurrent_tasks = 2 # Calling the task_scheduler results = asyncio.run(task_scheduler(tasks, max_concurrent_tasks)) print(results) ``` Output: ``` [\'Task 1 completed\', \'Task 2 completed\', \'Task 3 completed\', \'Task 4 completed\', \'Task 5 completed\'] ``` # Notes: - Ensure your implementation handles the scheduling and execution of tasks correctly. - Use appropriate asyncio APIs to accomplish the task. Good luck!","solution":"import asyncio from typing import List, Coroutine, Any async def task_scheduler(tasks: List[Coroutine], max_concurrent_tasks: int) -> List[Any]: Schedules and runs the provided list of tasks concurrently, with a limit on the maximum number of concurrent tasks. Args: - tasks: List of coroutine tasks to be executed. - max_concurrent_tasks: Maximum number of concurrent tasks allowed. Returns: - List of results after all tasks have completed. semaphore = asyncio.Semaphore(max_concurrent_tasks) async def sem_task(task): async with semaphore: return await task results = await asyncio.gather(*[sem_task(task) for task in tasks]) return results"},{"question":"# PyTorch Coding Assessment Question Objective Your task is to demonstrate your understanding of the PyTorch tensor storage system by implementing a function that manipulates tensor storages in specific ways. This exercise will test your comprehension of advanced tensor manipulations and storage sharing concepts. Problem Statement You need to implement the function `manipulate_tensor_storage` that performs the following steps: 1. Create a tensor of size (4, 3) filled with ones of `dtype=torch.float32`. 2. Retrieve the underlying untyped storage of the tensor. 3. Create a deep copy of the storage and fill it with zeros. 4. Set the original tensor to use the zero-filled storage while maintaining its original `dtype`, shape, and stride. 5. Create a view of the tensor with shape (2, 6) and check if it shares the same storage as the original tensor. 6. Return the original tensor, its modified storage, and the view. Function Signature ```python def manipulate_tensor_storage() -> (torch.Tensor, torch.UntypedStorage, torch.Tensor): pass ``` Expected Output - A tuple containing: 1. The original tensor with the modified storage. 2. The zero-filled storage used by the original tensor. 3. The view of the tensor with shape (2, 6). Constraints - You should not make assumptions about the device; ensure compatibility with CPUs. - Direct manipulation of storage should be used cautiously, reflecting the educational nature of this exercise rather than a production use case. Example Execution ```python original_tensor, zeroed_storage, tensor_view = manipulate_tensor_storage() # Checking the outputs assert original_tensor.equal(torch.zeros(4, 3)) assert list(zeroed_storage) == [0] * 48 # Since torch.float32 takes up 4 bytes per element assert tensor_view.shape == torch.Size([2, 6]) assert tensor_view.untyped_storage().data_ptr() == zeroed_storage.data_ptr() ``` Notes - Make sure to handle tensor metadata correctly to ensure original shapes and strides. - The view must share the same storage, verified by checking the `data_ptr` of the tensor and its view. - Use PyTorch 1.8.0 or higher for your implementation.","solution":"import torch def manipulate_tensor_storage(): # Step 1: Create a tensor of size (4, 3) filled with ones of dtype=torch.float32. tensor = torch.ones((4, 3), dtype=torch.float32) # Step 2: Retrieve the underlying untyped storage of the tensor. original_storage = tensor.untyped_storage() # Step 3: Create a deep copy of the storage and fill it with zeros. zeroed_storage = original_storage.clone() zeroed_storage.fill_(0) # Step 4: Set the original tensor to use the zero-filled storage while maintaining its original dtype, shape, and stride. tensor = torch.tensor([], dtype=tensor.dtype).set_(zeroed_storage, tensor.storage_offset(), tensor.size(), tensor.stride()) # Step 5: Create a view of the tensor with shape (2, 6) and check if it shares the same storage as the original tensor. tensor_view = tensor.view(2, 6) # Step 6: Return the original tensor, its modified storage, and the view. return tensor, zeroed_storage, tensor_view"},{"question":"# Floating Point Arithmetic and Precision Handling Python\'s standard floating-point arithmetic is rooted in binary fractions, which introduces precision errors when representing certain decimal fractions. Your task is to write a Python function to demonstrate the issues with floating-point arithmetic and how to handle them using Python\'s tools. Function Signature: ```python def precise_sum(numbers: list, use_fsum: bool = False) -> float: Computes the sum of a list of floating-point numbers. Parameters: - numbers (list): A list of floating-point numbers to be summed. - use_fsum (bool): If True, use math.fsum() for summation to mitigate precision errors, otherwise use the built-in sum() function. Returns: - float: The computed sum of the list. ``` Requirements: 1. **Input**: - `numbers`: A list of floating-point numbers (e.g., `[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]`). - `use_fsum`: A boolean flag determining whether to use `math.fsum` or the built-in `sum` function. 2. **Output**: - A single floating-point number that is the sum of the input list. 3. **Constraints**: - The list can contain between 0 and 1,000,000 floating-point numbers. - The numbers in the list will each be within the range of [-1e10, 1e10]. 4. **Special Conditions**: - Demonstrate the issue with precision when using the standard `sum` function. - Show how the `math.fsum` function can alleviate this precision error. Example Usage: ```python # Expecting typical precision issues print(precise_sum([0.1] * 10)) # Output may not be exactly 1.0 # Expecting accurate summation print(precise_sum([0.1] * 10, use_fsum=True)) # Output should be exactly 1.0 ``` Additional Notes: - Use string formatting or other methods to print the results in a detailed manner, showcasing the differences when precision issues arise. - Handle edge cases such as an empty list gracefully.","solution":"import math def precise_sum(numbers: list, use_fsum: bool = False) -> float: Computes the sum of a list of floating-point numbers. Parameters: - numbers (list): A list of floating-point numbers to be summed. - use_fsum (bool): If True, use math.fsum() for summation to mitigate precision errors, otherwise use the built-in sum() function. Returns: - float: The computed sum of the list. if use_fsum: return math.fsum(numbers) else: return sum(numbers)"},{"question":"# Question: Creating and Customizing Trajectory Path Plots with Seaborn You are provided with a dataset named \\"healthexp\\", which contains information about health expenditures and life expectancy across different countries and years. Your task is to create a customized path plot using seaborn to visualize the trajectory of life expectancy against health expenditures. Instructions: 1. **Load the Dataset:** - Use `seaborn.load_dataset()` to load the \\"healthexp\\" dataset into a DataFrame. 2. **Sort the Dataset:** - Sort the DataFrame by \\"Country\\" and \\"Year\\". 3. **Create the Path Plot:** - Create a path plot using `so.Plot()` to depict the relationship between \\"Spending_USD\\" (x-axis) and \\"Life_Expectancy\\" (y-axis) colored by \\"Country\\". 4. **Customize the Path Plot:** - Add markers to the plot with circles (`marker=\\"o\\"`). - Set the point size to 2 (`pointsize=2`). - Set the line width to 0.75 (`linewidth=.75`). - Change the fill color of the markers to white (`fillcolor=\\"w\\"`). 5. **Display the Plot:** - Display the final customized path plot. # Constraints: - Ensure that you use seaborn\'s object-oriented interface (`seaborn.objects`) for creating and customizing the plot. - Verify the dataset has been loaded and sorted correctly before creating the plot. # Example Output: Your plot should look similar to the example below, which shows trajectories of life expectancy against health spending for different countries over time, with each country\'s trajectory displayed in a unique color. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Sort the dataset by Country and Year healthexp = healthexp.sort_values([\\"Country\\", \\"Year\\"]) # Create the path plot p = so.Plot(healthexp, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")) # Display the plot p.show() ``` This question assesses your ability to manipulate datasets, create visualizations, and customize plots using seaborn.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_trajectory_plot(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Sort the dataset by Country and Year healthexp = healthexp.sort_values([\\"Country\\", \\"Year\\"]) # Create the path plot p = so.Plot(healthexp, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")) # Display the plot p.show()"},{"question":"**Objective:** Write a Python function that takes a list of user names and returns a dictionary mapping each user name to their respective home directory. **Function Signature:** ```python def get_user_home_directories(user_names: List[str]) -> Dict[str, str]: pass ``` **Input:** - `user_names` (List[str]): A list of user names. Each user name is a string. It is guaranteed that all names in this list exist in the password database. **Output:** - (Dict[str, str]): A dictionary where each key is a user name from the input list, and its corresponding value is the user\'s home directory. **Constraints:** - The function should handle any number of user names efficiently. - You may assume the pwd module is available and imported. **Example:** ```python # Example user names list user_names = [\'root\', \'guest\', \'john\'] # Expected Output (the values will vary according to the system) # { # \'root\': \'/root\', # \'guest\': \'/home/guest\', # \'john\': \'/home/john\' # } # Call the function print(get_user_home_directories(user_names)) ``` **Note:** - Make sure to handle cases where the `pw_dir` attribute might be missing or empty. - This task requires an understanding of the `pwd` module, handling the tuple-like password database entries, and using dictionary comprehensions or appropriate iteration techniques.","solution":"import pwd from typing import List, Dict def get_user_home_directories(user_names: List[str]) -> Dict[str, str]: Returns a dictionary mapping each user name to their respective home directory. user_home_dirs = {} for user_name in user_names: try: pwd_entry = pwd.getpwnam(user_name) home_dir = pwd_entry.pw_dir if pwd_entry.pw_dir else \\"/home/\\" + user_name user_home_dirs[user_name] = home_dir except KeyError: user_home_dirs[user_name] = None # in case the user name does not exist return user_home_dirs"},{"question":"# Task: Implement Conditional Tensor Operations Using `torch.cond` In this task, you will demonstrate your understanding of PyTorch\'s `torch.cond` function by implementing a custom neural network module that changes its behavior based on the shape or values of input tensors. # Problem Description You need to implement a class `ShapeValueDependentModule` that inherits `torch.nn.Module`. This class will use `torch.cond` to decide between two different operations based on the shape of the input tensor and its sum value. # Requirements 1. **Class Definition**: - Define a class `ShapeValueDependentModule` that inherits from `torch.nn.Module`. 2. **Constructor (`__init__` method)**: - Initialize the superclass. - No additional parameters are required. 3. **Forward Method**: - Implement the `forward` method which takes a single input tensor `x`. 4. **Control Flow**: - If the shape of `x` (number of elements in the first dimension) is greater than 5, further check the sum of elements in `x`. - If the sum of elements in `x` is greater than 10, apply `torch.exp(x)`. - Otherwise, apply `torch.sqrt(x)`. - If the shape of `x` is less than or equal to 5, apply `torch.log(x + 1)`. 5. **Function Implementation**: - You need to implement at least three functions (`exp_fn`, `sqrt_fn`, `log_fn`) to be used with `torch.cond`. # Inputs - `x: torch.Tensor` - A tensor containing input data, which is at least a one-dimensional tensor. # Outputs - A tensor that is the result of applying the appropriate operation based on the control flow conditions. # Example Usage: ```python import torch class ShapeValueDependentModule(torch.nn.Module): def __init__(self): super().__init__() def exp_fn(self, x: torch.Tensor): return torch.exp(x) def sqrt_fn(self, x: torch.Tensor): return torch.sqrt(x) def log_fn(self, x: torch.Tensor): return torch.log(x + 1) def forward(self, x: torch.Tensor) -> torch.Tensor: def shape_cond(): def sum_cond(): return torch.cond(x.sum() > 10, self.exp_fn, self.sqrt_fn, (x,)) return sum_cond() return torch.cond(x.shape[0] > 5, shape_cond, self.log_fn, (x,)) # Example Test model = ShapeValueDependentModule() input_tensor1 = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]) output1 = model(input_tensor1) print(output1) input_tensor2 = torch.tensor([0.5, 1.0, 1.5]) output2 = model(input_tensor2) print(output2) ``` In this example: - For `input_tensor1` with more than 5 elements, the sum is 21, so `torch.exp` is applied. - For `input_tensor2` with 3 elements, `torch.log(x + 1)` is applied as the input shape is less than or equal to 5. **Constraints**: - Use at least three different functions (`exp_fn`, `sqrt_fn`, `log_fn`) that you define. - Use `torch.cond` for both shape and value-based decision making. **Grading Criteria**: - Correct implementation of the class and methods. - Proper use of `torch.cond` for conditional operations. - Handling different input scenarios correctly as per the requirements.","solution":"import torch class ShapeValueDependentModule(torch.nn.Module): def __init__(self): super().__init__() def exp_fn(self, x: torch.Tensor): return torch.exp(x) def sqrt_fn(self, x: torch.Tensor): return torch.sqrt(x) def log_fn(self, x: torch.Tensor): return torch.log(x + 1) def forward(self, x: torch.Tensor) -> torch.Tensor: shape_cond = x.shape[0] > 5 if shape_cond: sum_cond = x.sum() > 10 if sum_cond: return self.exp_fn(x) else: return self.sqrt_fn(x) else: return self.log_fn(x) # Example usage model = ShapeValueDependentModule() input_tensor1 = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]) output1 = model(input_tensor1) print(output1) input_tensor2 = torch.tensor([0.5, 1.0, 1.5]) output2 = model(input_tensor2) print(output2)"},{"question":"**Task: Analyzing Customer Data Using pandas** You are provided with a CSV file named `customer_data.csv` that consists of the following columns: - `CustomerID`: Unique identifier for each customer. - `Name`: Name of the customer. - `Country`: Country of the customer. - `Age`: Age of the customer. - `PurchasedAmount`: The total amount spent by the customer. - `LoyaltyPoints`: Loyalty points accumulated by the customer. **Your task is to perform the following operations using pandas, and implement each step in a separate function:** 1. **Load and Inspect Data:** Implement a function `load_and_inspect(file_path: str) -> pd.DataFrame` to load the CSV data into a DataFrame and return the first 5 rows of the DataFrame. 2. **Filter Customers by Country:** Implement a function `filter_customers_by_country(df: pd.DataFrame, country: str) -> pd.DataFrame` to filter customers from a specific country and return this filtered DataFrame. 3. **High Spend Customers:** Implement a function `high_spend_customers(df: pd.DataFrame, threshold: float = 1000.0) -> pd.DataFrame` to filter customers who have purchased amounts greater than the specified threshold. 4. **Assign Loyalty Points:** Implement a function `assign_loyalty_points(df: pd.DataFrame) -> pd.DataFrame` which assigns additional loyalty points to customers based on the following conditions: - If `PurchasedAmount` is greater than 500 and less than or equal to 1000, add 50 points. - If `PurchasedAmount` is greater than 1000, add 100 points. 5. **Calculate Average Age per Country:** Implement a function `average_age_per_country(df: pd.DataFrame) -> pd.Series` that calculates and returns the average age of customers for each country. 6. **Group by Age Range:** Implement a function `group_by_age_range(df: pd.DataFrame) -> pd.DataFrame` that adds a new column `AgeRange` to the DataFrame based on the customer\'s age and then groups the DataFrame by this new column. Categories for age ranges are: - `Youth` (age < 30) - `Adult` (30 <= age < 60) - `Senior` (age >= 60) **Additional Information:** - Assume the CSV file is properly formatted. - Avoid using global variables; pass necessary data as parameters. **Example Input:** ```plaintext CustomerID,Name,Country,Age,PurchasedAmount,LoyaltyPoints 1,John Doe,USA,29,650,150 2,Jane Smith,Canada,34,300,200 3,Emily Davis,UK,42,1200,350 4,Daniel Wilson,USA,65,870,400 5,Alice Johnson,Australia,23,750,100 ``` **Expected Functions Implementation:** ```python import pandas as pd def load_and_inspect(file_path: str) -> pd.DataFrame: pass def filter_customers_by_country(df: pd.DataFrame, country: str) -> pd.DataFrame: pass def high_spend_customers(df: pd.DataFrame, threshold: float = 1000.0) -> pd.DataFrame: pass def assign_loyalty_points(df: pd.DataFrame) -> pd.DataFrame: pass def average_age_per_country(df: pd.DataFrame) -> pd.Series: pass def group_by_age_range(df: pd.DataFrame) -> pd.DataFrame: pass ``` Ensure that your code demonstrates good practices for data manipulation in pandas.","solution":"import pandas as pd def load_and_inspect(file_path: str) -> pd.DataFrame: Loads the CSV data into a DataFrame and returns the first 5 rows. df = pd.read_csv(file_path) return df.head() def filter_customers_by_country(df: pd.DataFrame, country: str) -> pd.DataFrame: Filters customers from a specific country. return df[df[\'Country\'] == country] def high_spend_customers(df: pd.DataFrame, threshold: float = 1000.0) -> pd.DataFrame: Filters customers who have purchased amounts greater than the specified threshold. return df[df[\'PurchasedAmount\'] > threshold] def assign_loyalty_points(df: pd.DataFrame) -> pd.DataFrame: Assigns additional loyalty points based on the PurchasedAmount conditions. conditions = [ (df[\'PurchasedAmount\'] > 500) & (df[\'PurchasedAmount\'] <= 1000), (df[\'PurchasedAmount\'] > 1000) ] choices = [50, 100] df[\'LoyaltyPoints\'] += pd.Series(pd.cut(df[\'PurchasedAmount\'], bins=[-1, 500, 1000, float(\'inf\')], labels=[0, 50, 100]).astype(int)) return df def average_age_per_country(df: pd.DataFrame) -> pd.Series: Calculates and returns the average age of customers for each country. return df.groupby(\'Country\')[\'Age\'].mean() def group_by_age_range(df: pd.DataFrame) -> pd.DataFrame: Adds a new column `AgeRange` to the DataFrame based on the customer\'s age and groups the DataFrame by this new column. df[\'AgeRange\'] = pd.cut(df[\'Age\'], bins=[-1, 30, 60, float(\'inf\')], labels=[\'Youth\', \'Adult\', \'Senior\']) return df.groupby(\'AgeRange\').size().reset_index(name=\'Count\')"},{"question":"# Advanced Python C API and Memory Management Assessment **Objective:** You are to implement a custom Python object using Python’s C API for memory management. This involves using the functions detailed in the documentation for allocating and managing memory on the heap. **Task:** 1. **Define a new custom Python type:** Implement a simple Python type in C, for instance, a custom integer type that supports basic arithmetic operations. 2. **Object memory management:** Use the functions `_PyObject_New`, `PyObject_Init`, and `PyObject_Del` for memory allocation and initialization. 3. **Reference counting and cyclic garbage collection:** Make sure your implementation correctly handles reference counting and integrates with Python’s garbage collector. 4. **Expose the custom type to Python:** Make your custom type available as a Python extension module. **Requirements:** 1. **Custom Type Specifications:** - Should be a basic integer type. - Should support basic arithmetic operations: addition, subtraction, multiplication, and division. - Should have a custom method to return the integer\'s string representation. 2. **Memory Management Functions:** - Use the functions `_PyObject_New`, `PyObject_Init`, and `PyObject_Del` correctly to manage heap allocation. - Ensure the object integrates with Python’s cyclic garbage collector if dynamic memory allocation within the object is used. 3. **Extension Module:** - Provide an appropriate `PyModuleDef` structure. - Implement the initialization function to add the custom type to the module. - Demonstrate creating instances of the custom type and using its methods from Python. **Input:** Your C code should define a new Python type and an extension module. **Output:** Document the steps to build the module and provide a sample Python script using the custom type. # Example: ```c // Basic outline of the C code #include <Python.h> // Define the custom type structure typedef struct { PyObject_HEAD int value; } CustomIntObject; // Implement the memory management functions static PyObject * CustomInt_new(PyTypeObject *type, PyObject *args, PyObject *kwds) { CustomIntObject *self; self = (CustomIntObject *)_PyObject_New(type); if (self != NULL) { self->value = 0; // Initialize to 0 } return (PyObject *)self; } static void CustomInt_dealloc(CustomIntObject *self) { PyObject_Del(self); } static int CustomInt_init(CustomIntObject *self, PyObject *args, PyObject *kwds) { if (!PyArg_ParseTuple(args, \\"i\\", &self->value)) { return -1; } return 0; } static PyMethodDef CustomInt_methods[] = { {\\"to_string\\", (PyCFunction)CustomInt_to_string, METH_NOARGS, \\"Return the string representation of the integer\\"}, {NULL} // Sentinel }; static PyTypeObject CustomIntType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"customint.CustomInt\\", .tp_basicsize = sizeof(CustomIntObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT, .tp_new = CustomInt_new, .tp_init = (initproc)CustomInt_init, .tp_dealloc = (destructor)CustomInt_dealloc, .tp_methods = CustomInt_methods, }; // Module definition static PyModuleDef customintmodule = { PyModuleDef_HEAD_INIT, .m_name = \\"customint\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_customint(void) { PyObject *m; if (PyType_Ready(&CustomIntType) < 0) { return NULL; } m = PyModule_Create(&customintmodule); if (m == NULL) { return NULL; } Py_INCREF(&CustomIntType); PyModule_AddObject(m, \\"CustomInt\\", (PyObject *)&CustomIntType); return m; } ``` In your documentation, be sure to include: - Steps to compile the module. - Example usage script.","solution":"def is_palindrome(s: str) -> bool: Checks if a given string s is a palindrome. A palindrome is a string that reads the same forwards as backwards. This function ignores spaces and casing. :param s: The string to check. :return: True if the string is a palindrome, False otherwise. # Remove spaces and convert to lowercase normalized_str = \'\'.join(s.split()).lower() # Check if the normalized string is equal to its reverse return normalized_str == normalized_str[::-1]"},{"question":"<|Analysis Begin|> The provided documentation about the `torch.func` API gives insight into function transforms available in PyTorch, such as `vmap`, `grad`, `grad_and_value`, and others. These transforms are used to compute derivatives (like Jacobians and Hessians), perform automatic differentiation, and work with model parameters in a functional manner. The documentation includes examples demonstrating: 1. Computing Jacobians of functions and models. 2. Using the `functional_call` function to perform operations like computing the Jacobian over model parameters by treating them as inputs. Key points extracted from the documentation: - PyTorch function transforms for advanced neural network operations. - `functional_call` for manipulating model parameters. - The calculation of Jacobians as a significant operation. Based on this analysis, a challenging question can be designed to assess a student\'s ability to compute a function\'s Jacobian and manipulate neural network parameters using `torch.func`. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: You are given a neural network model and a set of parameters. Your task is to implement a function that computes the Jacobian matrix of the model\'s output with respect to its parameters. The Jacobian matrix provides information about the rate of change of the model\'s output with respect to small changes in its parameters, which is essential for various optimization algorithms. Function Signature: ```python def compute_jacobian(model: torch.nn.Module, x: torch.Tensor) -> Dict[str, torch.Tensor]: Computes the Jacobian matrix of the model\'s output with respect to its parameters. Parameters: - model (torch.nn.Module): The PyTorch model. - x (torch.Tensor): The input tensor to the model. Returns: - Dict[str, torch.Tensor]: A dictionary where the keys are the parameter names and the values are the corresponding Jacobian tensors. ``` Input: - `model`: An instance of `torch.nn.Module`. For this assessment, assume the model has already been defined and contains parameters. - `x`: A `torch.Tensor` which will be used as the input to the model. The tensor shape will match the input requirements of the model. Output: - A dictionary where each key is the name of a parameter of the model, and each value is a `torch.Tensor` that represents the Jacobian of the model\'s output with respect to that parameter. Constraints: - Do not use any loops to compute the Jacobian. - Use the `torch.func.functional_call` and `torch.func.jacrev` functions as demonstrated in the documentation. - Ensure the Jacobian computation is efficient and leverages PyTorch\'s automatic differentiation capabilities. Example: ```python import torch import torch.nn as nn from torch.func import functional_call, jacrev class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(3, 3) def forward(self, x): return self.linear(x) model = SimpleModel() x = torch.randn(1, 3) # Example input tensor jacobian = compute_jacobian(model, x) for param_name, jac in jacobian.items(): print(f\\"Jacobian for {param_name}: {jac}\\") ``` In this example, you should see the Jacobian matrices corresponding to each parameter of the `SimpleModel` printed out. Make sure your implementation adheres to the function signature and input/output formats specified above. **Note:** You need to import necessary modules and define the model appropriately as per the provided example. Your function should work with any model as long as it conforms to the `torch.nn.Module` structure.","solution":"import torch from torch.func import functional_call, jacrev from typing import Dict def compute_jacobian(model: torch.nn.Module, x: torch.Tensor) -> Dict[str, torch.Tensor]: Computes the Jacobian matrix of the model\'s output with respect to its parameters. Parameters: - model (torch.nn.Module): The PyTorch model. - x (torch.Tensor): The input tensor to the model. Returns: - Dict[str, torch.Tensor]: A dictionary where the keys are the parameter names and the values are the corresponding Jacobian tensors. params = {name: param for name, param in model.named_parameters()} def model_output(params, x): return functional_call(model, params, x) jacobians = jacrev(model_output, argnums=0)(params, x) return jacobians"},{"question":"You are tasked with creating a Python module using the Python C API that utilizes multi-phase initialization. The module should perform the following tasks: 1. Define a module named `CustomModule`. 2. During the creation phase: - Utilize a custom `create_module` function to create the module object. 3. During the execution phase: - Add a function `add_numbers` that takes two integers and returns their sum. - Add an integer constant `VERSION` with the value 1. # Specifications: - You should use the structures and functions cited in the provided documentation, such as `PyModuleDef`, `PyModule_Create`, `PyModule_Create2`, `PyMethodDef`, and `PyModuleExecDef`. - Ensure proper memory management practices as outlined in the documentation, particularly concerning reference counting. - Demonstrate the correctness of the module by writing a small Python script that imports your module, calls `add_numbers`, and prints the result. # Constraints: - Ensure the module follows best practices for initialization and memory management. - Your module should be robust against possible errors during the initialization phase. # Example: Write your solution in the form of a `.c` file that includes the proper initialization functions and a corresponding Python script that tests your module. ```c #include <Python.h> /* Function to be added to the module */ static PyObject *add_numbers(PyObject *self, PyObject *args) { int a, b; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) { return NULL; } return PyLong_FromLong(a + b); } /* Module methods */ static PyMethodDef CustomModuleMethods[] = { {\\"add_numbers\\", add_numbers, METH_VARARGS, \\"Add two numbers\\"}, {NULL, NULL, 0, NULL} }; /* Module definition */ static struct PyModuleDef CustomModuleDef = { PyModuleDef_HEAD_INIT, \\"CustomModule\\", /* name of the module */ NULL, /* module documentation */ -1, /* size of per-interpreter module state */ CustomModuleMethods }; /* Create module object */ static PyObject* create_module(PyObject* spec, PyModuleDef* def) { PyObject *module = PyModule_Create(def); return module; } /* Execution phase */ static int exec_module(PyObject *module) { if (PyModule_AddIntConstant(module, \\"VERSION\\", 1) < 0) { return -1; /* Error occurred */ } return 0; } /* Definition of slots for multi-phase initialization */ static struct PyModuleDef_Slot CustomModuleSlots[] = { {Py_mod_create, (void*)create_module}, {Py_mod_exec, (void*)exec_module}, {0, NULL} }; /* Module definition with slots for multiphase initialization */ static struct PyModuleDef CustomModuleDef = { PyModuleDef_HEAD_INIT, \\"CustomModule\\", NULL, -1, CustomModuleMethods, CustomModuleSlots }; /* Initialization function */ PyMODINIT_FUNC PyInit_CustomModule(void) { return PyModuleDef_Init(&CustomModuleDef); } ``` ```python # test_custom_module.py import CustomModule def test(): result = CustomModule.add_numbers(3, 4) print(f\\"3 + 4 = {result}\\") print(f\\"VERSION = {CustomModule.VERSION}\\") if __name__ == \\"__main__\\": test() ``` # Deliverables: - A `.c` file implementing the CustomModule. - A Python script for testing the module.","solution":"def add(a, b): Returns the sum of a and b. return a + b VERSION = 1"},{"question":"Understanding and Creating Python Code Objects Objective Implement a function that creates a custom Python code object using provided compiler flags, arguments, and metadata. Then, write a function to extract metadata and analyze the code object. Background A code object is a low-level representation of code in Python. It can represent everything from single lines of code to entire modules. Python\'s `compile()` function generates code objects from source code. Additionally, the Python C API provides functions to manipulate these objects directly. Task 1. **Create a Function to Generate Code Object:** Write a function `create_code_object(filename: str, funcname: str, firstlineno: int, src_code: str) -> types.CodeType` that: - Takes the parameters `filename`, `funcname`, `firstlineno`, and `src_code`. - Compiles the `src_code` into a code object. - Returns the generated code object. Note: Use the `compile()` function in Python to generate the code object. 2. **Analyze Code Object:** Write a function `analyze_code_object(co: types.CodeType) -> dict` that: - Takes a `co` parameter representing a code object. - Extracts and returns metadata about the code object, including: - `argcount`: Number of positional arguments. - `nlocals`: Total number of local variables used. - `stacksize`: Size of the stack required. - `flags`: Compiler flags indicating compilation options. 3. **Helper Function to Pretty Print Metadata:** Write a helper function `pretty_print_code_metadata(metadata: dict)` that: - Takes the metadata dictionary returned by `analyze_code_object()`. - Prints the metadata in a readable format. Constraints - You can assume the `src_code` will not have syntax errors. - You are expected to use the `types.CodeType` class and relevant attributes. Example ```python import types def create_code_object(filename: str, funcname: str, firstlineno: int, src_code: str) -> types.CodeType: # Implementation here def analyze_code_object(co: types.CodeType) -> dict: # Implementation here def pretty_print_code_metadata(metadata: dict): # Implementation here # Example usage: src_code = def example_function(x, y): return x + y filename = \\"example.py\\" funcname = \\"example_function\\" firstlineno = 1 code_obj = create_code_object(filename, funcname, firstlineno, src_code) metadata = analyze_code_object(code_obj) pretty_print_code_metadata(metadata) ``` **Expected Output:** ``` argcount: 2 nlocals: 2 stacksize: 2 flags: 67 ``` Explanation: 1. `argcount` represents the number of positional arguments (`x` and `y`). 2. `nlocals` refers to local variables within the function (`x` and `y`). 3. `stacksize` is the stack used by the function during execution. 4. `flags` represent any special compiler flags during compilation.","solution":"import types def create_code_object(filename: str, funcname: str, firstlineno: int, src_code: str) -> types.CodeType: Compiles the provided source code into a code object. :param filename: The name of the file containing the source code. :param funcname: The function name. :param firstlineno: The first line number where the function is defined. :param src_code: The source code to be compiled. :return: A types.CodeType object representing the compiled code. compiled_code = compile(src_code, filename, \'exec\') for const in compiled_code.co_consts: if isinstance(const, types.CodeType) and const.co_name == funcname: return const raise ValueError(\\"Function not found in the source code.\\") def analyze_code_object(co: types.CodeType) -> dict: Extracts metadata from a code object. :param co: A types.CodeType object representing the compiled code. :return: A dictionary with the extracted metadata. metadata = { \'argcount\': co.co_argcount, \'nlocals\': co.co_nlocals, \'stacksize\': co.co_stacksize, \'flags\': co.co_flags } return metadata def pretty_print_code_metadata(metadata: dict): Prints the code object metadata in a readable format. :param metadata: A dictionary containing the metadata. for key, value in metadata.items(): print(f\\"{key}: {value}\\") # Example usage: if __name__ == \\"__main__\\": src_code = def example_function(x, y): return x + y filename = \\"example.py\\" funcname = \\"example_function\\" firstlineno = 1 code_obj = create_code_object(filename, funcname, firstlineno, src_code) metadata = analyze_code_object(code_obj) pretty_print_code_metadata(metadata)"},{"question":"**Objective:** Evaluate your understanding of hyper-parameter tuning in Scikit-learn by implementing a function to perform parameter optimization using `GridSearchCV` and `RandomizedSearchCV`. This function should demonstrate your ability to set up parameter grids, execute searches, and interpret the results. **Task:** Implement a function `optimize_hyperparameters` that takes the following inputs: - `estimator`: An instance of a Scikit-learn estimator (e.g., `SVC`, `RandomForestClassifier`). - `param_grid`: A dictionary specifying the parameter grid for `GridSearchCV`. - `param_dist`: A dictionary specifying the parameter distributions for `RandomizedSearchCV`. - `X_train`: Training feature data. - `y_train`: Training target data. - `X_test`: Testing feature data. - `y_test`: Testing target data. The function should: 1. Perform an exhaustive grid search using `GridSearchCV` to find the best hyper-parameters. 2. Perform a randomized parameter search using `RandomizedSearchCV` to find the best hyper-parameters. 3. Return a dictionary containing: - The best parameters and the best score from `GridSearchCV`. - The best parameters and the best score from `RandomizedSearchCV`. **Constraints:** - You can assume the provided estimator supports the parameters you aim to optimize. - Limit the number of iterations in `RandomizedSearchCV` to 20. - Use 5-fold cross-validation for both search methods. - `X_train`, `X_test`, `y_train`, and `y_test` are `numpy` arrays or pandas dataframes. **Function Signature:** ```python def optimize_hyperparameters(estimator, param_grid, param_dist, X_train, y_train, X_test, y_test): pass ``` **Example:** ```python from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import numpy as np # Load iris dataset data = load_iris() X = data.data y = data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define estimator, param_grid, and param_dist estimator = SVC() param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] param_dist = { \'C\': np.logspace(-3, 3, 7), \'gamma\': [\'scale\', \'auto\'], \'kernel\': [\'linear\', \'rbf\'] } # Call the function result = optimize_hyperparameters(estimator, param_grid, param_dist, X_train, y_train, X_test, y_test) # Print the result print(result) ``` **Expected Output:** The `result` dictionary should contain the best parameters and scores from both `GridSearchCV` and `RandomizedSearchCV`. ```python { \'grid_search\': { \'best_params\': {\'C\': 10, \'gamma\': 0.001, \'kernel\': \'rbf\'}, \'best_score\': 0.95 }, \'randomized_search\': { \'best_params\': {\'C\': 10, \'gamma\': \'scale\', \'kernel\': \'rbf\'}, \'best_score\': 0.95 } } ``` **Notes:** - Ensure you handle any potential errors during the search process and provide meaningful output indicating the issue. - Use appropriate random states where possible to ensure reproducibility.","solution":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV def optimize_hyperparameters(estimator, param_grid, param_dist, X_train, y_train, X_test, y_test): Performs hyper-parameter tuning using GridSearchCV and RandomizedSearchCV. Parameters: - estimator: An instance of a Scikit-learn estimator. - param_grid: A dictionary specifying the parameter grid for GridSearchCV. - param_dist: A dictionary specifying the parameter distributions for RandomizedSearchCV. - X_train: Training feature data. - y_train: Training target data. - X_test: Testing feature data. - y_test: Testing target data. Returns: - result: A dictionary containing the best parameters and scores from both search methods. # Grid Search grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5) grid_search.fit(X_train, y_train) # Randomized Search randomized_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_dist, n_iter=20, cv=5, random_state=42) randomized_search.fit(X_train, y_train) result = { \'grid_search\': { \'best_params\': grid_search.best_params_, \'best_score\': grid_search.best_score_ }, \'randomized_search\': { \'best_params\': randomized_search.best_params_, \'best_score\': randomized_search.best_score_ } } return result"},{"question":"You are tasked with creating a simple system using the \\"dbm\\" database interfaces provided by Python. This system will allow users to store, retrieve, and manage authentication credentials (username and passwords) for different services. Task Description 1. **Create a class `CredentialStore` that encapsulates the operations for storing and retrieving credentials using the `dbm` module.** 2. **The class should support the following operations:** - `__init__(self, filename: str, db_type: str = \'dbm.dumb\')`: Initialize the database with the given filename and database type (`dbm.gnu`, `dbm.ndbm`, or `dbm.dumb`). Create the database if it doesn\'t exist. - `add_credential(self, service: str, username:str, password: str)`: Add a new credential for a given service. If the service already exists, overwrite the existing credentials. - `get_credential(self, service: str) -> tuple`: Retrieve the username and password for a given service. Return `None` if the service does not exist. - `delete_credential(self, service: str)`: Delete the credential for a given service. Raise an appropriate error if the database is in read-only mode. - `list_services(self) -> list`: Return a list of all services stored in the database. 3. **Use context management to ensure the database is properly closed after operations are performed.** 4. **Ensure proper conversion of strings to bytes for storing keys and values.** 5. **Handle potential errors gracefully, such as I/O errors or key errors.** Example Usage ```python # Initialize the credential store store = CredentialStore(\'credentials_db\') # Add credentials store.add_credential(\'gmail\', \'user@gmail.com\', \'password123\') store.add_credential(\'github\', \'user@github.com\', \'ghp_pass123\') # Retrieve credentials print(store.get_credential(\'gmail\')) # Output: (\'user@gmail.com\', \'password123\') print(store.get_credential(\'github\')) # Output: (\'user@github.com\', \'ghp_pass123\') # List all services print(store.list_services()) # Output: [\'gmail\', \'github\'] # Delete a credential store.delete_credential(\'github\') print(store.get_credential(\'github\')) # Output: None ``` Constraints - The filename provided should be a valid string. - The db_type should be one of the supported variants: \'dbm.gnu\', \'dbm.ndbm\', or \'dbm.dumb\'. Default is \'dbm.dumb\'. - Handle situations where the database file does not exist or is unreadable. - Ensure that keys and values are bytes. Performance Requirements The operations should be efficient enough to handle a reasonable number of credentials (up to a few thousand entries) without significant performance degradation.","solution":"import dbm class CredentialStore: def __init__(self, filename: str, db_type: str = \'dbm.dumb\'): self.filename = filename self.db_type = db_type self.db = None def _open_db(self, mode=\'c\'): try: self.db = dbm.open(self.filename, mode) except Exception as e: raise RuntimeError(\\"Failed to open DB\\") from e def _close_db(self): if self.db: self.db.close() self.db = None def add_credential(self, service: str, username: str, password: str): self._open_db(\'c\') try: self.db[service.encode()] = f\\"{username}:{password}\\".encode() finally: self._close_db() def get_credential(self, service: str) -> tuple: self._open_db(\'r\') try: value = self.db.get(service.encode(), None) if value is None: return None username, password = value.decode().split(\':\', 1) return username, password finally: self._close_db() def delete_credential(self, service: str): self._open_db(\'c\') try: del self.db[service.encode()] finally: self._close_db() def list_services(self) -> list: self._open_db(\'r\') try: return [key.decode() for key in self.db.keys()] finally: self._close_db()"},{"question":"# Coding Challenge: Objective: Evaluate your ability to use Python\'s `asyncio` library to manage subprocesses. Task: You are required to write an asynchronous Python function that takes a list of shell commands and runs them concurrently. The function should return a dictionary with the command as the key and a tuple containing the standard output and the return code as the value. Function Signature: ```python import asyncio async def run_commands(commands: list[str]) -> dict[str, tuple[str, int]]: # Your implementation here ``` Input: - `commands`: A list of shell commands (strings) to be executed concurrently. Output: - A dictionary where each key is a command (string from the input list), and each value is a tuple containing: - The standard output of the command. - The return code of the command. Example: ```python commands = [\\"echo \'Hello World\'\\", \\"ls /non_existent_dir\\"] results = await run_commands(commands) ``` Given the above example, the expected output should be: ```python { \\"echo \'Hello World\'\\": (\\"Hello Worldn\\", 0), \\"ls /non_existent_dir\\": (\\"\\", 1) } ``` Constraints: - You must use the `asyncio.create_subprocess_shell()` function to run the commands. - Use `asyncio.gather()` to run the commands concurrently. - Make sure to handle both standard output and error streams, combining them if necessary. Performance Requirements: - Ensure that the function handles multiple commands efficiently and does not block waiting for individual commands to complete. Notes: - Carefully manage the subprocess streams to avoid deadlocks. - Pay attention to potential exceptions when the commands fail to run.","solution":"import asyncio async def run_command(command): proc = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() return command, (stdout.decode().strip() if stdout else stderr.decode().strip(), proc.returncode) async def run_commands(commands: list[str]) -> dict[str, tuple[str, int]]: tasks = [run_command(command) for command in commands] results = await asyncio.gather(*tasks) return dict(results)"},{"question":"**Question: URL Parser and Constructor** You are tasked with creating a Python utility to manipulate and validate URLs for a web crawling application. This utility should perform the following functions: 1. **Parse a URL** Given a URL string, parse it into components. 2. **Remove Query Parameters** Given a URL string and a list of query parameter keys, return the URL with these parameters removed. 3. **Construct a Full URL** Given a base URL and a relative URL, construct the absolute URL. # Function 1: `parse_url` **Input:** - A single string representing a URL. **Output:** - A dictionary with the keys: \\"scheme\\", \\"netloc\\", \\"path\\", \\"params\\", \\"query\\", \\"fragment\\", \\"username\\", \\"password\\", \\"hostname\\", and \\"port\\". **Example:** ```python input: \\"http://user:pass@host.com:80/path;params?query=arg#frag\\" output: { \\"scheme\\": \\"http\\", \\"netloc\\": \\"user:pass@host.com:80\\", \\"path\\": \\"/path\\", \\"params\\": \\"params\\", \\"query\\": \\"query=arg\\", \\"fragment\\": \\"frag\\", \\"username\\": \\"user\\", \\"password\\": \\"pass\\", \\"hostname\\": \\"host.com\\", \\"port\\": 80 } ``` # Function 2: `remove_query_params` **Input:** - A single string representing a URL. - A list of strings representing query parameter keys to be removed. **Output:** - A string representing the updated URL with the specified query parameters removed. **Example:** ```python input: (\\"http://host.com/path?name=John&age=30\\", [\\"name\\"]) output: \\"http://host.com/path?age=30\\" ``` **Note:** If all query parameters are removed, the URL should not include an empty `?`. # Function 3: `construct_full_url` **Input:** - A string representing the base URL. - A string representing the relative URL. **Output:** - A string representing the absolute URL. **Example:** ```python input: (\\"http://host.com/path/\\", \\"relative/path\\") output: \\"http://host.com/path/relative/path\\" ``` Implement these three functions in Python using the `urllib.parse` module. ```python from urllib.parse import urlparse, urlunparse, urlencode, urlsplit, urlunsplit, urldefrag, parse_qs, urljoin def parse_url(url): # Your implementation here def remove_query_params(url, params_to_remove): # Your implementation here def construct_full_url(base, relative): # Your implementation here ``` Ensure your implementation is efficient and handles edge cases appropriately. **Constraints:** 1. The input URLs will be well-formed. 2. The URLs will use schemes supported by `urllib.parse`. 3. The list of query parameter keys for removal will not be empty. 4. Do not use external libraries other than Python’s built-in `urllib.parse`.","solution":"from urllib.parse import urlparse, urlunparse, urlencode, parse_qs, urljoin def parse_url(url): parsed = urlparse(url) result = { \\"scheme\\": parsed.scheme, \\"netloc\\": parsed.netloc, \\"path\\": parsed.path, \\"params\\": parsed.params, \\"query\\": parsed.query, \\"fragment\\": parsed.fragment, \\"username\\": parsed.username, \\"password\\": parsed.password, \\"hostname\\": parsed.hostname, \\"port\\": parsed.port } return result def remove_query_params(url, params_to_remove): url_parts = urlparse(url) query_params = parse_qs(url_parts.query) # Remove specified query parameters for param in params_to_remove: if param in query_params: del query_params[param] # Reconstruct the query string new_query = urlencode(query_params, doseq=True) # Reassemble the URL parts new_url_parts = url_parts._replace(query=new_query) return urlunparse(new_url_parts) def construct_full_url(base, relative): return urljoin(base, relative)"},{"question":"**Question:** You have been provided with a dataset `penguins` which contains the following columns: - `species`: Species name of the penguins. - `island`: Island name where the penguin was found. - `bill_length_mm`: The length of the bill in millimeters. - `bill_depth_mm`: The depth of the bill in millimeters. - `flipper_length_mm`: The length of the flipper in millimeters. - `body_mass_g`: The body mass of the penguin in grams. - `sex`: Sex of the penguin. Using the seaborn package, you are required to create a `FacetGrid` to visualize this data. The facets should be organized by rows and columns based on specific variables, and multiple attributes of the `FacetGrid` must be customized. The final plot should be saved to a file named “penguin_facet_plot.png”. **Requirements:** 1. Load the `penguins` dataset using seaborn (you can assume it is available in seaborn\'s repository). 2. Create a `FacetGrid` with the following specifications: - The rows should represent different islands (`island`). - The columns should represent different species (`species`). 3. Use a scatter plot to plot `bill_length_mm` on the x-axis and `body_mass_g` on the y-axis. 4. Use different colors (`hue`) to distinguish between male and female penguins (`sex`). 5. Add a legend to the plot. 6. Add a horizontal reference line at the median body mass across the entire dataset. 7. Customize the size and aspect ratio of the plot as follows: - Height: 4 - Aspect: 1.2 8. Label the axes as \\"Bill Length (mm)\\" and \\"Body Mass (g)\\". 9. Adjust the subplot titles to reflect the `species` and `island`. 10. Ensure tight layout and save the plot as \\"penguin_facet_plot.png\\". **Constraints:** - You should use `sns.set_theme(style=\\"ticks\\")` to set the theme at the beginning. - Make sure all facets have consistent axis limits, with x-axis range from 30 to 60 and y-axis range from 2000 to 6000. **Input and Output Formats:** - **Input:** The `penguins` dataset is to be loaded using seaborn. - **Output:** A file named \\"penguin_facet_plot.png\\" that displays the customized `FacetGrid`. ```python import seaborn as sns # Set the theme sns.set_theme(style=\\"ticks\\") # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the FacetGrid g = sns.FacetGrid(penguins, row=\\"island\\", col=\\"species\\", height=4, aspect=1.2) # Map the scatter plot g.map_dataframe(sns.scatterplot, x=\\"bill_length_mm\\", y=\\"body_mass_g\\", hue=\\"sex\\") # Add a legend g.add_legend() # Add a horizontal reference line at the median body mass median_body_mass = penguins[\\"body_mass_g\\"].median() g.refline(y=median_body_mass) # Customize axes labels and adjust titles g.set_axis_labels(\\"Bill Length (mm)\\", \\"Body Mass (g)\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") # Set consistent x and y limits g.set(xlim=(30, 60), ylim=(2000, 6000)) # Ensure tight layout and save the plot g.tight_layout() g.savefig(\\"penguin_facet_plot.png\\") ```","solution":"import seaborn as sns def create_penguin_facet_plot(): # Set the theme sns.set_theme(style=\\"ticks\\") # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the FacetGrid g = sns.FacetGrid(penguins, row=\\"island\\", col=\\"species\\", height=4, aspect=1.2) # Map the scatter plot g.map_dataframe(sns.scatterplot, x=\\"bill_length_mm\\", y=\\"body_mass_g\\", hue=\\"sex\\") # Add a legend g.add_legend() # Add a horizontal reference line at the median body mass median_body_mass = penguins[\\"body_mass_g\\"].median() g.refline(y=median_body_mass) # Customize axes labels and adjust titles g.set_axis_labels(\\"Bill Length (mm)\\", \\"Body Mass (g)\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") # Set consistent x and y limits g.set(xlim=(30, 60), ylim=(2000, 6000)) # Ensure tight layout and save the plot g.tight_layout() g.savefig(\\"penguin_facet_plot.png\\")"},{"question":"Advanced Set Operations In Python, `set` and `frozenset` are collections of unique elements. While the `set` is mutable, enabling addition and removal of elements, `frozenset` is immutable. Your task is to implement a class `AdvancedSetOperations` that leverages both `set` and `frozenset` for advanced operations. Class: `AdvancedSetOperations` Implement the following methods: 1. **Constructor `__init__(self, initial_values=None)`**: - Initializes the instance with a `set` containing `initial_values` if provided, else initializes with an empty set. 2. **Method `add_elements(self, elements)`**: - Adds a list of `elements` to the set. Any non-hashable elements (i.e., elements that can’t be added to a set) are ignored. - Returns the updated set. 3. **Method `remove_elements(self, elements)`**: - Removes a list of `elements` from the set if they exist. Ignore elements that are not found in the set. - Returns the updated set. 4. **Method `convert_to_frozenset(self)`**: - Converts and returns the current set as a frozenset. 5. **Method `check_element(self, element)`**: - Checks if an `element` exists in the set or frozenset. - Returns `True` if it exists, else returns `False`. # Constraints - You should handle exceptions like TypeError when adding non-hashable elements. - Your methods should handle edge cases such as empty lists for adding or removing elements. # Example Usage ```python # Initialize with some values aso = AdvancedSetOperations(initial_values=[1, 2, 3]) print(aso.add_elements([4, 5])) # Output: {1, 2, 3, 4, 5} print(aso.remove_elements([3, 6])) # Output: {1, 2, 4, 5} print(aso.check_element(2)) # Output: True print(aso.check_element(6)) # Output: False fset = aso.convert_to_frozenset() print(fset) # Output: frozenset({1, 2, 4, 5}) print(aso.check_element(4)) # Output: True ``` # Note - The methods should handle all necessary type checks and errors internally without crashing. - Ensure to write clean and efficient code. Implementation ```python class AdvancedSetOperations: def __init__(self, initial_values=None): self._set = set(initial_values) if initial_values else set() def add_elements(self, elements): for elem in elements: try: self._set.add(elem) except TypeError: pass # Ignore non-hashable elements return self._set def remove_elements(self, elements): for elem in elements: self._set.discard(elem) # Discard silently ignores if the element is not present return self._set def convert_to_frozenset(self): return frozenset(self._set) def check_element(self, element): return element in self._set ```","solution":"class AdvancedSetOperations: def __init__(self, initial_values=None): self._set = set(initial_values) if initial_values else set() def add_elements(self, elements): for elem in elements: try: self._set.add(elem) except TypeError: pass # Ignore non-hashable elements return self._set def remove_elements(self, elements): for elem in elements: self._set.discard(elem) # Discard silently ignores if the element is not present return self._set def convert_to_frozenset(self): return frozenset(self._set) def check_element(self, element): return element in self._set or element in frozenset(self._set)"},{"question":"**Objective:** Your task is to write a function that reads a file and processes its content. Additionally, you need to ensure that the function does not raise any ResourceWarnings and utilizes best practices for resource management. **Question:** Part 1: Function Implementation Write a function `process_file_lines(filename: str) -> int` that takes the path to a text file and returns the number of lines in the file. The function should: 1. Open the file for reading. 2. Count the number of lines in the file. 3. Ensure that the file is properly closed to avoid ResourceWarnings. 4. Use best practices for managing file resources. ```python def process_file_lines(filename: str) -> int: # Your code here ``` Part 2: Test the Function with Python Development Mode Enabled Write a script to test your function under Python Development Mode. The script should: 1. Enable the Development Mode. 2. Call the `process_file_lines` function with a sample file and print the result. 3. Ensure that no ResourceWarnings are raised when the script is executed. ```python if __name__ == \\"__main__\\": # Your code here to test process_file_lines() function ``` **Input:** - A single string `filename` representing the path to a text file. **Output:** - An integer representing the number of lines in the file. **Constraints:** - You can assume that the file exists and is readable. - The file can be large, but you should strive to have a minimal memory footprint. **Performance Requirements:** - The function should run in O(n) time complexity, where n is the number of lines in the file. - The function should have O(1) additional space complexity, aside from the input file. **Example:** Given a file `example.txt` with the following content: ``` Line 1 Line 2 Line 3 ``` Your function should return: ``` 3 ``` **Note:** Run your script with the following command to enable Development Mode: ```bash python3 -X dev your_script.py ``` **Hint:** Consider using context managers to handle file operations efficiently.","solution":"def process_file_lines(filename: str) -> int: Reads a file and returns the number of lines in the file. :param filename: Path to the text file :return: Number of lines in the file with open(filename, \'r\') as file: return sum(1 for _ in file)"},{"question":"# Python Coding Assessment You are provided with an external library capable of handling various float operations at the C level. Utilizing the provided Python C API functions, implement a utility function in Python that performs the following tasks: Function 1: `create_float_from_string` **Objective**: Create a floating point object from a string representation. **Input**: - `s (str)`: A string representing a numerical value. **Output**: - Returns a Python float object created from the string. Function 2: `is_exact_float` **Objective**: Verify if a given object is exactly a Python float (not a subtype). **Input**: - `obj (any)`: Any Python object. **Output**: - Returns `True` if `obj` is specifically a float, `False` otherwise. Function 3: `get_float_max_min` **Objective**: Retrieve the maximum and minimum floating point values. **Output**: - Returns a tuple containing the maximum and minimum representable float values. Constraints - Do not use Python\'s built-in float conversion methods directly within the functions where indicated. - Ensure your implementation adheres to the provided C API functionality. - Handle potential errors gracefully where applicable. ```python def create_float_from_string(s: str) -> float: # Fill in the implementation utilizing PyFloat_FromString pass def is_exact_float(obj) -> bool: # Fill in the implementation utilizing PyFloat_CheckExact pass def get_float_max_min() -> tuple: # Fill in the implementation utilizing PyFloat_GetMax and PyFloat_GetMin pass # Example Usage # Note: The actual functions you write will not directly use the C API syntax in Python, # hence you should assume these C API functions have analogous high-level intermediary functions. s = \\"1234.56\\" print(create_float_from_string(s)) # Expected: 1234.56 obj = 1234.56 print(is_exact_float(obj)) # Expected: True print(get_float_max_min()) # Expected: (max_float, min_float) ```","solution":"import sys def create_float_from_string(s: str) -> float: Create a floating point object from a string representation. try: return float(s) except ValueError: raise ValueError(f\\"Cannot convert string to float: {s}\\") def is_exact_float(obj) -> bool: Verify if a given object is exactly a Python float (not a subtype). return type(obj) is float def get_float_max_min() -> tuple: Retrieve the maximum and minimum floating point values. return (sys.float_info.max, sys.float_info.min)"},{"question":"# Advanced Coding Assessment: Custom Iterator and Asynchronous Iterator You are tasked with implementing two classes in Python: 1. **CustomIterator**: A class that behaves like a typical Python iterator. 2. **AsyncIterator**: A class that behaves like a Python asynchronous iterator. CustomIterator Implement a class `CustomIterator` that generates an infinite sequence of Fibonacci numbers. # Requirements: - The class should be initialized with two starting values. - Implement the `__iter__()` and `__next__()` methods. # Expected Behavior: ```python fib = CustomIterator(0, 1) for _ in range(10): print(next(fib)) # Output should print the first 10 Fibonacci numbers starting from 0, 1 ``` AsyncIterator Implement a class `AsyncIterator` that asynchronously generates squared values from 0 to a given limit. # Requirements: - The class should be initialized with a limit. - Implement the `__aiter__()` and `__anext__()` methods. # Expected Behavior: ```python import asyncio async def run_async_iterator(): async for value in AsyncIterator(5): print(value) # Output should print 0, 1, 4, 9, 16 in separate lines # To execute the async function asyncio.run(run_async_iterator()) ``` # Input and Output Formats **CustomIterator:** - Initialization: `CustomIterator(start1: int, start2: int)` - Methods: `__iter__() -> CustomIterator`, `__next__() -> int` **AsyncIterator:** - Initialization: `AsyncIterator(limit: int)` - Methods: `__aiter__() -> AsyncIterator`, `__anext__() -> int` # Constraints - For `CustomIterator`, assume the starting values are non-negative integers. - For `AsyncIterator`, the limit should be a non-negative integer. - Implement proper error handling for the end of iteration in both iterators. # Performance Requirements - The `CustomIterator` should handle very large Fibonacci numbers efficiently. - The `AsyncIterator` should yield values without blocking the event loop excessively. # Additional Notes - Utilize Python\'s built-in iterator support (`__iter__`, `__next__`) and asynchronous iterator support (`__aiter__`, `__anext__`). - Avoid using external libraries; use only standard Python features.","solution":"class CustomIterator: def __init__(self, start1, start2): self.a = start1 self.b = start2 def __iter__(self): return self def __next__(self): current = self.a self.a, self.b = self.b, self.a + self.b return current import asyncio class AsyncIterator: def __init__(self, limit): self.limit = limit self.current = 0 async def __aiter__(self): return self async def __anext__(self): if self.current > self.limit: raise StopAsyncIteration result = self.current ** 2 self.current += 1 await asyncio.sleep(0) # simulate non-blocking behavior return result"},{"question":"Objective: Demonstrate your understanding of seaborn\'s `husl_palette` function by generating customized color palettes and using them to visualize data. Background: Seaborn\'s `husl_palette` function allows users to create color palettes with specific properties like lightness, saturation, and the starting point for hue sampling. Task: 1. **Generate Color Palettes**: Write a function `generate_palettes` that returns three different HUSL color palettes based on the following specifications: - Palette 1: 8 colors with default lightness and saturation. - Palette 2: 10 colors with lightness set to 0.5 and saturation set to 0.7. - Palette 3: 6 colors with lightness set to 0.4, saturation set to 0.6, and hue starting at 0.3. ```python def generate_palettes(): Returns: tuple: A tuple containing three lists of color palettes. Palette 1 - 8 colors with default lightness and saturation. Palette 2 - 10 colors with lightness 0.5 and saturation 0.7. Palette 3 - 6 colors with lightness 0.4, saturation 0.6, and hue starting at 0.3. pass ``` 2. **Visualize Data**: Use the `iris` dataset from seaborn to create three different visualizations (scatter plots) using each of the generated palettes. Use `sepal_length` on the x-axis and `sepal_width` on the y-axis. 1. For Palette 1, assign each species a unique color. 2. For Palette 2, assign the colors based on the `sepal_length` values. 3. For Palette 3, assign the colors based on the `sepal_width` values. ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_data(palette1, palette2, palette3): Create three scatter plots using the iris dataset and the provided palettes. Args: palette1 (list): First color palette with 8 colors. palette2 (list): Second color palette with 10 colors. palette3 (list): Third color palette with 6 colors. iris = sns.load_dataset(\'iris\') # Plot 1: Palette 1 with unique species colors plt.figure(figsize=(10, 6)) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette1) plt.title(\'Scatter Plot with Palette 1\') plt.show() # Plot 2: Palette 2 with colors based on sepal_length plt.figure(figsize=(10, 6)) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'sepal_length\', palette=palette2) plt.title(\'Scatter Plot with Palette 2\') plt.show() # Plot 3: Palette 3 with colors based on sepal_width plt.figure(figsize=(10, 6)) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'sepal_width\', palette=palette3) plt.title(\'Scatter Plot with Palette 3\') plt.show() ``` Constraints: - Ensure good practice with proper function definitions and necessary imports. - Each function should handle the input specifications accurately. - Visualizations must be clear and correctly use the specified color palettes. Expected Output: - Three functions: `generate_palettes`, `visualize_data`. - Visual representation of the data using the specified palettes. Use these functions to demonstrate your comprehension of seaborn color palettes and their application in data visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_palettes(): Returns: tuple: A tuple containing three lists of color palettes. Palette 1 - 8 colors with default lightness and saturation. Palette 2 - 10 colors with lightness 0.5 and saturation 0.7. Palette 3 - 6 colors with lightness 0.4, saturation 0.6, and hue starting at 0.3. palette1 = sns.husl_palette(8) palette2 = sns.husl_palette(10, l=0.5, s=0.7) palette3 = sns.husl_palette(6, l=0.4, s=0.6, h=0.3) return (palette1, palette2, palette3) def visualize_data(palette1, palette2, palette3): Create three scatter plots using the iris dataset and the provided palettes. Args: palette1 (list): First color palette with 8 colors. palette2 (list): Second color palette with 10 colors. palette3 (list): Third color palette with 6 colors. iris = sns.load_dataset(\'iris\') # Plot 1: Palette 1 with unique species colors plt.figure(figsize=(10, 6)) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette1) plt.title(\'Scatter Plot with Palette 1\') plt.show() # Plot 2: Palette 2 with colors based on sepal_length plt.figure(figsize=(10, 6)) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'sepal_length\', palette=palette2) plt.title(\'Scatter Plot with Palette 2\') plt.show() # Plot 3: Palette 3 with colors based on sepal_width plt.figure(figsize=(10, 6)) sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'sepal_width\', palette=palette3) plt.title(\'Scatter Plot with Palette 3\') plt.show()"},{"question":"# Email Message Structure Analysis and Extraction You are provided with a set of email messages in the form of `email.message.Message` objects. Your task is to write a function that extracts and processes specific parts of these email messages based on given criteria. Function Signature ```python def extract_email_parts(messages: List[email.message.Message], maintype: str, subtype: str = None, decode: bool = False) -> Dict[str, List[str]]: pass ``` Input - `messages`: A list of `email.message.Message` objects representing the email messages to be processed. - `maintype`: A string representing the main MIME type (e.g., \\"text\\", \\"image\\") to filter the email subparts. - `subtype`: An optional string representing the sub MIME type (e.g., \\"plain\\", \\"html\\"). If not provided, only the `maintype` is used for filtering. - `decode`: A boolean indicating whether to decode the payloads of the subparts. Output - Returns a dictionary where the keys are the subject lines of the emails and the values are lists of strings. Each list contains the payloads of the subparts that match the given `maintype` and optional `subtype`. Constraints - The email messages can have multiple subparts. - Each email may have nested subparts. - The function should skip subparts that do not match the given `maintype` and `subtype`. - Subparts with payloads that are not strings should be skipped. Example ```python from email import message_from_string email_str1 = Subject: Test email Content-Type: multipart/mixed; boundary=\\"===============4474022185245899586==\\" --===============4474022185245899586== Content-Type: text/plain; charset=\\"utf-8\\" Content-Transfer-Encoding: 7bit This is the body of the test email. --===============4474022185245899586== Content-Type: application/octet-stream Content-Transfer-Encoding: base64 SGVsbG8gd29ybGQh --===============4474022185245899586==-- email_str2 = Subject: Another test email Content-Type: multipart/alternative; boundary=\\"===============4474022185245899586==\\" --===============4474022185245899586== Content-Type: text/html; charset=\\"utf-8\\" Content-Transfer-Encoding: 7bit <html><body>This is the HTML body of the test email.</body></html> --===============4474022185245899586== Content-Type: text/plain; charset=\\"utf-8\\" Content-Transfer-Encoding: 7bit This is the plain text body of the test email. --===============4474022185245899586==-- msg1 = message_from_string(email_str1) msg2 = message_from_string(email_str2) result = extract_email_parts([msg1, msg2], \\"text\\", \\"plain\\") # Expected Output: # { # \\"Test email\\": [\\"This is the body of the test email.\\"], # \\"Another test email\\": [\\"This is the plain text body of the test email.\\"] # } ``` -------- Ensure your code is well-documented and uses appropriate error handling where necessary. Your implementation should ideally make use of the `email.iterators` module as described.","solution":"from email.message import Message from typing import List, Dict, Optional import email def extract_email_parts(messages: List[Message], maintype: str, subtype: Optional[str] = None, decode: bool = False) -> Dict[str, List[str]]: result = {} for msg in messages: subject = msg[\'Subject\'] if subject not in result: result[subject] = [] # Helper function to extract MIME parts recursively def extract_recursive(part): if part.get_content_maintype() == \'multipart\': for subpart in part.get_payload(): extract_recursive(subpart) elif (part.get_content_maintype() == maintype and (subtype is None or part.get_content_subtype() == subtype)): payload = part.get_payload(decode=decode) if isinstance(payload, bytes) and decode: payload = payload.decode(part.get_content_charset() or \'utf-8\') if isinstance(payload, str): result[subject].append(payload) extract_recursive(msg) return result"},{"question":"# Question: Kernel Density Estimation and Data Visualization with Scikit-Learn **Objective:** To assess your ability to implement Kernel Density Estimation (KDE) for a given dataset using scikit-learn and visualize the results effectively. **Problem Statement:** You are provided with a 2D dataset sampled from an unknown distribution. Your task is to perform Kernel Density Estimation using various kernel functions and visualize the estimated density. You will also compare and discuss the effects of different bandwidth values on the KDE results. **Data:** The dataset is a 2D numpy array `data` with shape (n_samples, 2). An example dataset is provided for your reference: ```python import numpy as np # Example data: two clusters of points data = np.vstack([np.random.normal(0, 1, (100, 2)), np.random.normal(5, 1, (100, 2))]) ``` **Requirements:** 1. **Function Implementation:** - Define a function `perform_kde(data, bandwidth, kernel)` that: - Takes in a dataset (`data`), a bandwidth value (`bandwidth`), and a kernel type (`kernel`). - Performs Kernel Density Estimation using the specified parameters. - Returns an array of log density values for each sample in the dataset. ```python def perform_kde(data, bandwidth, kernel): Perform Kernel Density Estimation on the given data. Parameters: data (numpy.ndarray): 2D array of shape (n_samples, 2). bandwidth (float): Bandwidth value for KDE. kernel (str): Kernel type to use for KDE (\'gaussian\', \'tophat\', \'epanechnikov\', etc.). Returns: numpy.ndarray: Array of log density values of each sample in the dataset. pass # Your implementation here ``` 2. **Visualization:** - Generate and visualize KDE results using the following methods: - Plot the original data points. - Generate a grid of points covering the range of the data. - Compute and plot the KDE values on this grid, using contour plots to show density levels. - Implement a function `visualize_kde(data, bandwidth, kernel, grid_size=100)` that: - Takes in the dataset (`data`), bandwidth value (`bandwidth`), kernel type (`kernel`), and grid size (`grid_size`). - Calls `perform_kde` to get KDE results. - Visualizes the data points and KDE estimate using contour plots. ```python import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def visualize_kde(data, bandwidth, kernel, grid_size=100): Visualize the KDE results along with the original data points. Parameters: data (numpy.ndarray): 2D array of shape (n_samples, 2). bandwidth (float): Bandwidth value for KDE. kernel (str): Kernel type to use for KDE. grid_size (int): Size of the grid for visualization. pass # Your implementation here # Example visualization call visualize_kde(data, bandwidth=0.5, kernel=\'gaussian\') ``` 3. **Comparison and Analysis:** - Perform KDE with different kernel types (`gaussian`, `tophat`, `epanechnikov`, etc.) and various bandwidth values (e.g., 0.1, 0.5, 1.0). - Plot the different KDE results and provide a brief analysis on: - How the choice of kernel affects the density estimation. - The impact of different bandwidth values on the smoothness of the KDE. **Input Format:** - A 2D numpy array `data` of shape (n_samples, 2). - Bandwidth value for KDE. - Kernel type as a string. - Grid size for visualization (optional, default is 100). **Output Format:** - The `perform_kde` function should return a 1D numpy array of log density values corresponding to each sample in the dataset. - The `visualize_kde` function should display the data points and KDE estimate using contour plots. **Constraints:** - Assume `data` contains at least 50 samples. - The bandwidth value should be a positive float. - The kernel type should be one of the valid kernel options supported by scikit-learn\'s `KernelDensity` class. **Performance Requirements:** - The KDE implementation should efficiently handle datasets with up to 10,000 samples. - Visualization should be clear and informative, providing meaningful insights into the underlying density estimation. **Example:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity data = np.vstack([np.random.normal(0, 1, (100, 2)), np.random.normal(5, 1, (100, 2))]) def perform_kde(data, bandwidth, kernel): kde = KernelDensity(bandwidth=bandwidth, kernel=kernel) kde.fit(data) log_density = kde.score_samples(data) return log_density def visualize_kde(data, bandwidth, kernel, grid_size=100): x_min, y_min = data.min(axis=0) - 1 x_max, y_max = data.max(axis=0) + 1 x_grid = np.linspace(x_min, x_max, grid_size) y_grid = np.linspace(y_min, y_max, grid_size) x_mesh, y_mesh = np.meshgrid(x_grid, y_grid) grid_samples = np.vstack([x_mesh.ravel(), y_mesh.ravel()]).T kde = KernelDensity(bandwidth=bandwidth, kernel=kernel) kde.fit(data) log_density = kde.score_samples(grid_samples).reshape(grid_size, grid_size) plt.figure(figsize=(10, 6)) plt.scatter(data[:, 0], data[:, 1], s=5, label=\'Data points\') plt.contourf(x_mesh, y_mesh, np.exp(log_density), levels=20, cmap=\'Blues\') plt.colorbar(label=\'Density\') plt.title(f\'Kernel Density Estimationn(kernel={kernel}, bandwidth={bandwidth})\') plt.xlabel(\'X\') plt.ylabel(\'Y\') plt.legend() plt.show() # Example usage visualize_kde(data, bandwidth=0.5, kernel=\'gaussian\') ``` **Deliverables:** - Implement the `perform_kde` function. - Implement the `visualize_kde` function. - Perform a comparison using different kernels and bandwidths and summarize your findings.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def perform_kde(data, bandwidth, kernel): Perform Kernel Density Estimation on the given data. Parameters: data (numpy.ndarray): 2D array of shape (n_samples, 2). bandwidth (float): Bandwidth value for KDE. kernel (str): Kernel type to use for KDE (\'gaussian\', \'tophat\', \'epanechnikov\', etc.). Returns: numpy.ndarray: Array of log density values of each sample in the dataset. kde = KernelDensity(bandwidth=bandwidth, kernel=kernel) kde.fit(data) log_density = kde.score_samples(data) return log_density def visualize_kde(data, bandwidth, kernel, grid_size=100): Visualize the KDE results along with the original data points. Parameters: data (numpy.ndarray): 2D array of shape (n_samples, 2). bandwidth (float): Bandwidth value for KDE. kernel (str): Kernel type to use for KDE. grid_size (int): Size of the grid for visualization. x_min, y_min = data.min(axis=0) - 1 x_max, y_max = data.max(axis=0) + 1 x_grid = np.linspace(x_min, x_max, grid_size) y_grid = np.linspace(y_min, y_max, grid_size) x_mesh, y_mesh = np.meshgrid(x_grid, y_grid) grid_samples = np.vstack([x_mesh.ravel(), y_mesh.ravel()]).T kde = KernelDensity(bandwidth=bandwidth, kernel=kernel) kde.fit(data) log_density = kde.score_samples(grid_samples).reshape(grid_size, grid_size) plt.figure(figsize=(10, 6)) plt.scatter(data[:, 0], data[:, 1], s=5, label=\'Data points\') plt.contourf(x_mesh, y_mesh, np.exp(log_density), levels=20, cmap=\'Blues\') plt.colorbar(label=\'Density\') plt.title(f\'Kernel Density Estimationn(kernel={kernel}, bandwidth={bandwidth})\') plt.xlabel(\'X\') plt.ylabel(\'Y\') plt.legend() plt.show()"},{"question":"**Objective:** Design and implement a Python function that simulates part of an email client. This function will copy all messages from one Mailbox format to another Mailbox format, ensuring proper conversion and handling of message-specific flags and attributes. **Task:** Write a function `copy_mailbox(source_path: str, destination_path: str, source_format: str, destination_format: str) -> None` that: 1. Takes the paths of the source and destination mailboxes and their respective formats as input. 2. Copies all messages from the source mailbox to the destination mailbox, converting the message format as needed. 3. Ensures proper handling of message state flags/attributes during conversion. 4. Prints a summary of the total messages copied. **Input:** - `source_path` (str): Path to the source mailbox. - `destination_path` (str): Path to the destination mailbox. - `source_format` (str): Format of the source mailbox, one of {\'Maildir\', \'mbox\', \'MH\', \'Babyl\', \'MMDF\'}. - `destination_format` (str): Format of the destination mailbox, one of {\'Maildir\', \'mbox\', \'MH\', \'Babyl\', \'MMDF\'}. **Output:** - None (the function should print the summary directly) **Constraints:** - The source mailbox should already exist with at least one message; the destination mailbox may or may not exist. - You may assume that the provided mailbox paths are valid. **Example Usage:** ```python copy_mailbox(\'/path/to/source_maildir\', \'/path/to/destination_mbox\', \'Maildir\', \'mbox\') ``` **Notes:** 1. You need to use the appropriate mailbox subclasses based on the provided formats. 2. You must handle the conversion of formats, ensuring that message flags and attributes are retained and properly converted. 3. Ensure that proper locking mechanisms are used for mailboxes that require it. **Hints:** - Refer to the `mailbox` module documentation provided above for the methods available in each class. - Look at the provided example at the end of the documentation for copying messages between different formats.","solution":"import mailbox def copy_mailbox(source_path: str, destination_path: str, source_format: str, destination_format: str) -> None: Copy all messages from one mailbox format to another, ensuring proper conversion and handling of message-specific flags and attributes. Parameters: - source_path (str): Path to the source mailbox. - destination_path (str): Path to the destination mailbox. - source_format (str): Format of the source mailbox. - destination_format (str): Format of the destination mailbox. Returns: - None (prints the summary directly) format_classes = { \'mbox\': mailbox.mbox, \'Maildir\': mailbox.Maildir, \'MMDF\': mailbox.MMDF, \'MH\': mailbox.MH, \'Babyl\': mailbox.Babyl, } # Ensure the provided formats are valid if source_format not in format_classes or destination_format not in format_classes: raise ValueError(f\\"Invalid mailbox format specified. Supported formats are: {list(format_classes.keys())}\\") # Open the source and destination mailboxes source_mailbox = format_classes[source_format](source_path) destination_mailbox = format_classes[destination_format](destination_path) # Initialize the counter for the number of messages copied message_count = 0 # Copy messages while retaining flags and attributes for message in source_mailbox: destination_mailbox.add(message) message_count += 1 destination_mailbox.flush() destination_mailbox.close() source_mailbox.close() print(f\\"Successfully copied {message_count} messages from {source_format} to {destination_format}.\\")"},{"question":"**Task: Implement a Text-Based To-Do List Application Using Python\'s `curses` Module** # Objective: Design a text-based To-Do List application using the `curses` module to control the text-mode display. The application should allow users to view, add, and remove items from their to-do list. # Function to Implement: ```python def todo_app(): pass ``` # Requirements: 1. **Initialization and Termination:** - Initialize the `curses` application using `curses.wrapper()` to handle terminal state restoration on exception. 2. **User Interface:** - Display a window with the title \\"To-Do List Application\\" at the top of the screen. - Show the list of To-Do items. - Provide options for adding a new item or removing an existing item: - Press \'a\' to add a new item. - Press \'d\' to delete the currently selected item. - Use the arrow keys (UP/DOWN) to navigate through the list. - Press \'q\' to quit the application. 3. **Adding and Removing Items:** - When \'a\' is pressed, prompt the user to enter a new item using `curses.textpad`. - When \'d\' is pressed, remove the currently selected item from the list. 4. **Attributes:** - Highlight the currently selected item using reverse video (`curses.A_REVERSE`). - Use colors to differentiate the title, the list, and the input prompts if color is supported. # Constraints: - The to-do list can contain a maximum of 100 items. - Each to-do item is a string with a maximum length of 50 characters. - The application should gracefully handle any unexpected inputs or errors, ensuring the terminal state is always restored on exit. # Example Execution: ``` +--------------------------------------------+ | Title: To-Do List Application | +--------------------------------------------+ | ** Buy groceries | | Walk the dog | | Finish the project | |--------------------------------------------| | Press \'a\' to add, \'d\' to delete, \'q\' to quit| +--------------------------------------------+ ``` This task is challenging as it requires understanding of various `curses` functionalities such as window management, text display and attributes, user input handling, and proper initialization and termination of curses applications.","solution":"import curses from curses import textpad def todo_app(stdscr): def draw_title(): stdscr.addstr(0, 0, \\"To-Do List Application\\", curses.A_BOLD) stdscr.addstr(1, 0, \\"-\\" * 50) def draw_tips(): stdscr.addstr(curses.LINES-2, 0, \\"Press \'a\' to add, \'d\' to delete, \'q\' to quit\\") def display_todo_list(): for idx, item in enumerate(todo_list): if idx == current_index: stdscr.addstr(idx + 2, 0, item, curses.A_REVERSE) else: stdscr.addstr(idx + 2, 0, item) def add_item(): curses.echo() stdscr.addstr(curses.LINES-1, 0, \\"Enter new item: \\") new_item = stdscr.getstr().decode(\\"utf-8\\") curses.noecho() if new_item: todo_list.append(new_item) stdscr.move(curses.LINES-1, 0) stdscr.clrtoeol() stdscr.clear() curses.curs_set(0) stdscr.keypad(1) todo_list = [] current_index = 0 while True: stdscr.clear() draw_title() draw_tips() display_todo_list() stdscr.refresh() key = stdscr.getch() if key == ord(\'q\'): break elif key == ord(\'a\'): add_item() elif key == ord(\'d\') and todo_list: todo_list.pop(current_index) elif key == curses.KEY_DOWN and todo_list: current_index = min(current_index + 1, len(todo_list) - 1) elif key == curses.KEY_UP and todo_list: current_index = max(0, current_index - 1) stdscr.clear() stdscr.refresh() if __name__ == \\"__main__\\": curses.wrapper(todo_app)"},{"question":"**Question: Implement a `ChainCounter` Class** Design and implement a Python class `ChainCounter` that integrates functionalities from both `Counter` and `ChainMap`. Your `ChainCounter` should: 1. Inherit from both `collections.Counter` and `collections.ChainMap`. 2. Support all standard `Counter` and `ChainMap` operations. 3. Provide a method `merge_counts()` that merges the counts from all underlying mappings into the first mapping. 4. Override the `total()` method from `Counter` to calculate the sum of counts across all mappings in the `ChainMap`. 5. Override the `most_common()` method to return the most common elements across all mappings, similar to `Counter`. # Requirements: - You must not use any external libraries besides `collections`. - The class should handle any number of mappings passed to `ChainMap`. - Performance should be considered; aim for efficiency in merging and querying. # Input and Output: The methods should function as follows: - `merge_counts()`: This method does not take any parameters. It merges all counts from the underlying mappings into the first mapping and returns `None`. - `total()`: This method returns an integer representing the sum of counts across all mappings. - `most_common([n])`: This method returns a list of the `n` most common elements and their counts across all mappings, sorted by frequency in descending order. If `n` is not provided or is `None`, it should return all elements. # Example Usage: ```python from collections import Counter, ChainMap from typing import List, Tuple, Union class ChainCounter(Counter, ChainMap): def __init__(self, *maps): Counter.__init__(self) ChainMap.__init__(self, *maps) def merge_counts(self): first_map = self.maps[0] for other_map in self.maps[1:]: for key, count in other_map.items(): first_map[key] += count def total(self) -> int: return sum(sum(map.values()) for map in self.maps) def most_common(self, n: Union[None, int] = None) -> List[Tuple]: combined_counter = Counter() for map in self.maps: combined_counter.update(map) return combined_counter.most_common(n) # Example: base = {\'apples\': 10, \'oranges\': 5} shop1_sales = {\'apples\': 1, \'bananas\': 2} shop2_sales = {\'oranges\': 3, \'bananas\': 1} chain_counter = ChainCounter(base, shop1_sales, shop2_sales) print(chain_counter.total()) # Output: 22 print(chain_counter.most_common(2)) # Output: [(\'apples\', 11), (\'oranges\', 8)] chain_counter.merge_counts() print(chain_counter) # Output: ChainCounter({\'apples\': 11, \'oranges\': 8, \'bananas\': 3}) ``` # Constraints: 1. Do not override the `__getitem__`, `__setitem__`, or `__delitem__` methods provided by `ChainMap` and `Counter`. 2. The functionalities of both `ChainMap` and `Counter` should be preserved. 3. Ensure that the methods work for any number of mappings.","solution":"from collections import Counter, ChainMap from typing import List, Tuple, Union class ChainCounter(Counter, ChainMap): def __init__(self, *maps): Counter.__init__(self) ChainMap.__init__(self, *maps) def merge_counts(self): first_map = self.maps[0] for other_map in self.maps[1:]: for key, count in other_map.items(): first_map[key] = first_map.get(key, 0) + count def total(self) -> int: return sum(sum(map.values()) for map in self.maps) def most_common(self, n: Union[None, int] = None) -> List[Tuple]: combined_counter = Counter() for map in self.maps: combined_counter.update(map) return combined_counter.most_common(n)"},{"question":"# Question Objective: Design a Python function that connects to a POP3 server using SSL, authenticates with the server, retrieves the list of email message IDs, fetches the full content of each message, and returns the messages in a formatted dictionary. Requirements: - The function should be named `retrieve_emails_from_pop3_ssl`. - The function should take the following parameters: - `host` (str): The POP3 server address. - `port` (int): The port for the POP3 SSL connection. - `username` (str): The username for authentication. - `password` (str): The password for authentication. - The function should return a dictionary where: - The keys are message IDs. - The values are the full content of each respective email message, represented as a single string. Implementation Details: - Use the `POP3_SSL` class from the `poplib` module to handle the SSL connection. - Use the `user` and `pass_` methods to authenticate with the server. - Use the `uidl` method to obtain a list of unique IDs for each email message. - Use the `retr` method to fetch the full content of each email message. - Retrieve and store the content of each email as a single string in the dictionary. - Ensure proper handling of exceptions that may arise during the connection, authentication, or retrieval process. Example Usage: ```python def retrieve_emails_from_pop3_ssl(host, port, username, password): # implementation here # Sample call to the function (assuming a valid server and credentials are provided): messages = retrieve_emails_from_pop3_ssl(\'pop.example.com\', 995, \'user@example.com\', \'securepassword\') print(messages) ``` Constraints: - Do not use any third-party libraries; only use the `poplib` module and standard Python libraries. - Ensure the function handles exceptions gracefully and provides meaningful error messages. Sample Output: ```python { \'1-UID\': \'Subject: Test Email 1nFrom: alice@example.comnTo: bob@example.comnnThis is a test message.\', \'2-UID\': \'Subject: Test Email 2nFrom: carol@example.comnTo: bob@example.comnnThis is another test message.\' } ```","solution":"import poplib def retrieve_emails_from_pop3_ssl(host, port, username, password): Connects to a POP3 server using SSL, authenticates, retrieves email message IDs, fetches the full content of each message, and returns the emails in a formatted dictionary. email_dict = {} try: # Connect to the POP3 server using SSL pop3_server = poplib.POP3_SSL(host, port) # Authentication pop3_server.user(username) pop3_server.pass_(password) # Get unique IDs for messages response, listings, octets = pop3_server.uidl() # Enumerate through the message listings for listing in listings: # Message ID usually looks something like \'1 UID12345\' msg_num, msg_id = listing.decode().split() # Fetch the full email message by its number response, lines, octets = pop3_server.retr(int(msg_num)) # Join the lines into a single string msg_content = \\"n\\".join(line.decode() for line in lines) # Store the message content in the dictionary with its unique ID email_dict[msg_id] = msg_content # Close the connection pop3_server.quit() except Exception as e: print(f\\"An error occurred: {e}\\") return email_dict"},{"question":"Objective In this exercise, you are required to visualize a given dataset using `seaborn.violinplot` with various customizations. Your task is to demonstrate your understanding of creating and customizing violin plots using the `seaborn` library. Problem Statement 1. **Dataset Loading**: Load the \'tips\' dataset using `seaborn.load_dataset(\\"tips\\")`. 2. **Basic Violin Plot**: Create a violin plot to display the distribution of total bill amounts. 3. **Bivariate Grouped Violin Plot**: Create a violin plot to display the distribution of total bill amounts grouped by the day of the week. 4. **Enhanced Customization:** - Create a grouped violin plot by day and differentiate by the smoker status (`smoker` column). - Split the violins to visualize the distribution for smokers and non-smokers more clearly. - Make the inner representation of the violins show the quartiles. - Adjust the bandwidth of the KDE to 0.7 to refine the smoothing. 5. **Normalized Width**: Create a violin plot showing the distribution of tips by day and normalize the width of each violin to represent the number of observations. 6. **Advanced Customization**: - Create a violin plot of total bill amounts by day. - Use a gap of 0.2 between the violins. - Change the default axis to match the `native_scale` parameter. - Add customized inner representations of points for each observation. Input No specific input is needed as `seaborn` will load the dataset directly within the code using `sns.load_dataset(\\"tips\\")`. Expected Output The output should be the plots displayed using matplotlib functionality which seaborn uses for plotting. Constraints - Use `seaborn` version 0.13.0 or higher. - Ensure the plots are appropriately labeled and scaled for better readability. Example Code Structure ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = sns.load_dataset(\\"tips\\") # Basic Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"total_bill\\"]) plt.title(\\"Distribution of Total Bill Amounts\\") plt.show() # Bivariate Grouped Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\") plt.title(\\"Total Bill Amounts by Day\\") plt.show() # Enhanced Customization plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", split=True, inner=\\"quart\\", bw=0.7) plt.title(\\"Total Bill Amounts by Day, Differentiated by Smoker Status\\") plt.show() # Normalized Width plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"tip\\", inner=\\"point\\", density_norm=\\"count\\") plt.title(\\"Tip Amounts by Day with Normalized Width\\") plt.show() # Advanced Customization plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\", gap=0.2, native_scale=True, inner=\\"point\\") plt.title(\\"Total Bill Amounts by Day with Customized Inner Points\\") plt.show() ``` Ensure each plot is properly labeled with titles and axes for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_violin_plots(): # Load the dataset df = sns.load_dataset(\\"tips\\") # Basic Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"total_bill\\"]) plt.title(\\"Distribution of Total Bill Amounts\\") plt.show() # Bivariate Grouped Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\") plt.title(\\"Total Bill Amounts by Day\\") plt.show() # Enhanced Customization plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", split=True, inner=\\"quart\\", bw=0.7) plt.title(\\"Total Bill Amounts by Day, Differentiated by Smoker Status\\") plt.show() # Normalized Width plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"tip\\", inner=\\"point\\", scale=\\"count\\") plt.title(\\"Tip Amounts by Day with Normalized Width\\") plt.show() # Advanced Customization plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"day\\", y=\\"total_bill\\", width=0.8, inner=\\"point\\", scale=\\"width\\") plt.title(\\"Total Bill Amounts by Day with Customized Inner Points\\") plt.show()"},{"question":"# Seaborn Advanced Visualization Task You are required to demonstrate your understanding of the Seaborn library by plotting some advanced visualizations using a built-in dataset of Seaborn. Follow the steps and requirements below to complete this task. **Task Description**: You will work with the `titanic` dataset and produce a comprehensive visualization report that addresses the following points: 1. **Load and Inspect the Dataset**: - Load the `titanic` dataset. - Display the first five rows of the dataset. 2. **Basic Count Plot**: - Create a count plot displaying the number of passengers in each class (i.e., `first`, `second`, `third`). 3. **Count Plot with Hue**: - Enhance the previous count plot by adding a hue to distinguish between those who survived and those who did not. 4. **Normalize to Percentages**: - Modify the count plot from step 3 to show percentages instead of raw counts. 5. **Advanced Visualization**: - Create a `FacetGrid` showing `age` distributions across different passenger `classes` and with respect to whether they `survived` or not. - Use `kdeplot` (Kernel Density Estimate plot) for this visualization to display the density estimate of the `age` distribution. 6. **Customizing the Plot**: - Apply a suitable Seaborn theme that enhances the readability of the plot. - Include plot titles, axis labels, and a legend to make the plot informative and visually appealing. **Expected Output**: 1. Load and display: ```python import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") print(titanic.head()) ``` 2. Basic count plot: ```python sns.countplot(data=titanic, x=\\"class\\") ``` 3. Count plot with hue: ```python sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\") ``` 4. Normalized to percentages: ```python sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\") ``` 5. Advanced visualization: ```python g = sns.FacetGrid(titanic, col=\\"class\\", row=\\"survived\\", margin_titles=True) g.map(sns.kdeplot, \\"age\\", fill=True) ``` 6. Customizing the plot: ```python import seaborn as sns # Set theme sns.set_theme(style=\\"whitegrid\\") # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Create FacetGrid g = sns.FacetGrid(titanic, col=\\"class\\", row=\\"survived\\", margin_titles=True) g.map(sns.kdeplot, \\"age\\", fill=True) # Add titles and labels g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\\"Age Distribution by Class and Survival Status on Titanic\\", fontsize=16) g.set_axis_labels(\\"Age\\", \\"Density\\") g.add_legend() ``` **Constraints**: - The work should be done in a Jupyter notebook or similar environment where you can execute and display the plots inline. - Pay attention to the readability and clarity of your plots. - Use built-in theme settings in seaborn to style your plots appropriately. **Performance Requirements**: - Your code should run efficiently and handle the `titanic` dataset without performance degradation.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load and Inspect the Dataset def load_and_inspect_dataset(): titanic = sns.load_dataset(\\"titanic\\") return titanic.head() # Step 2: Basic Count Plot def basic_count_plot(titanic): plt.figure(figsize=(8, 6)) sns.countplot(data=titanic, x=\\"class\\") plt.title(\\"Number of Passengers in Each Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Count\\") plt.show() # Step 3: Count Plot with Hue def count_plot_with_hue(titanic): plt.figure(figsize=(8, 6)) sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\") plt.title(\\"Number of Passengers by Class and Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Count\\") plt.legend(title=\\"Survived\\", labels=[\'No\', \'Yes\']) plt.show() # Step 4: Normalize to Percentages def count_plot_percentage(titanic): plt.figure(figsize=(8, 6)) counts = titanic.groupby([\\"class\\", \\"survived\\"]).size().reset_index(name=\\"count\\") total_counts = titanic.groupby(\\"class\\").size().reset_index(name=\\"total_count\\") merged = counts.merge(total_counts, on=\\"class\\") merged[\\"percentage\\"] = merged[\\"count\\"] / merged[\\"total_count\\"] * 100 sns.barplot(data=merged, x=\\"class\\", y=\\"percentage\\", hue=\\"survived\\") plt.title(\\"Percentage of Passengers by Class and Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Percentage\\") plt.legend(title=\\"Survived\\", labels=[\'No\', \'Yes\']) plt.show() # Step 5: Advanced Visualization using FacetGrid def facetgrid_age_distribution(titanic): g = sns.FacetGrid(titanic, col=\\"class\\", row=\\"survived\\", margin_titles=True) g.map(sns.kdeplot, \\"age\\", fill=True) plt.show() # Step 6: Customizing the Plot def customized_facetgrid_age_distribution(titanic): sns.set_theme(style=\\"whitegrid\\") g = sns.FacetGrid(titanic, col=\\"class\\", row=\\"survived\\", margin_titles=True) g.map(sns.kdeplot, \\"age\\", fill=True) g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\\"Age Distribution by Class and Survival Status on Titanic\\", fontsize=16) g.set_axis_labels(\\"Age\\", \\"Density\\") g.add_legend() plt.show()"},{"question":"**Question: Understanding Tensor Shapes in PyTorch** In this exercise, you will work with tensors using PyTorch and explore their shapes using the `torch.Size` class. # Task Implement a function `tensor_shape_properties` that takes a PyTorch tensor as input and returns a dictionary with the following keys: - `\\"num_dimensions\\"`: The number of dimensions of the tensor. - `\\"shape\\"`: The shape of the tensor as a list. - `\\"size_of_dim_2\\"`: The size of the second dimension of the tensor (use 1-based indexing). If the tensor has fewer than 2 dimensions, return `None` for this value. - `\\"total_elements\\"`: The total number of elements in the tensor. # Function Signature ```python import torch from typing import Dict, List, Any def tensor_shape_properties(tensor: torch.Tensor) -> Dict[str, Any]: pass ``` # Example ```python import torch # Example 1 tensor1 = torch.ones(10, 20, 30) result1 = tensor_shape_properties(tensor1) # Expected output: # { # \\"num_dimensions\\": 3, # \\"shape\\": [10, 20, 30], # \\"size_of_dim_2\\": 20, # \\"total_elements\\": 6000 # } # Example 2 tensor2 = torch.ones(5) # A 1-dimensional tensor result2 = tensor_shape_properties(tensor2) # Expected output: # { # \\"num_dimensions\\": 1, # \\"shape\\": [5], # \\"size_of_dim_2\\": None, # \\"total_elements\\": 5 # } ``` # Constraints - The tensor passed to the function will always be a valid PyTorch tensor. - The tensor can be of any dimension (>= 0). # Instructions 1. You should use the `.size()` method of the tensor to get its shape. 2. Use the `torch.Size` object to extract necessary information about the tensor. 3. Ensure your function handles edge cases, such as tensors with fewer dimensions. Write your solution in the function stub provided.","solution":"import torch from typing import Dict, Any def tensor_shape_properties(tensor: torch.Tensor) -> Dict[str, Any]: shape = tensor.size() num_dimensions = len(shape) size_of_dim_2 = shape[1] if num_dimensions >= 2 else None total_elements = tensor.numel() return { \\"num_dimensions\\": num_dimensions, \\"shape\\": list(shape), \\"size_of_dim_2\\": size_of_dim_2, \\"total_elements\\": total_elements }"},{"question":"Coding Assessment Question # Objective The objective of this exercise is to assess your understanding of data preprocessing, model training, evaluation, and visualization using the scikit package. # Problem Statement You are given a dataset in CSV format containing information about house prices in a city. Your task is to: 1. Preprocess the dataset. 2. Train a machine learning model to predict house prices. 3. Evaluate the performance of the model. 4. Visualize the results. # Dataset The dataset `house_prices.csv` contains the following columns: - `ID`: Unique identifier for each house. - `LotArea`: Lot size in square feet. - `YearBuilt`: Original construction date. - `1stFlrSF`: First Floor square feet. - `2ndFlrSF`: Second floor square feet. - `FullBath`: Number of full bathrooms. - `BedroomAbvGr`: Number of bedrooms above grade. - `TotRmsAbvGrd`: Total rooms above grade (does not include bathrooms). - `SalePrice`: Sale price of the house. # Requirements 1. **Data Preprocessing** - Handle any missing values. - Perform feature scaling. - Optionally, perform feature engineering if necessary. 2. **Model Training** - Split the dataset into training and testing sets. - Train a regression model to predict the `SalePrice` using the other features. 3. **Model Evaluation** - Evaluate the model using appropriate metrics (e.g., Mean Absolute Error, R^2 Score). 4. **Visualization** - Plot the actual vs. predicted values for the test set. # Input - The file path to `house_prices.csv`. # Output - Print the evaluation metrics. - Display the plot comparing actual vs. predicted values. # Constraints - Use scikit-learn for model training and evaluation. - Use matplotlib or seaborn for visualization. # Example Function Signature ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, r2_score import matplotlib.pyplot as plt import seaborn as sns def house_price_prediction(file_path: str): # Read the dataset df = pd.read_csv(file_path) # Data preprocessing # Handle missing values df.fillna(method=\'ffill\', inplace=True) # Feature scaling features = [\'LotArea\', \'YearBuilt\', \'1stFlrSF\', \'2ndFlrSF\', \'FullBath\', \'BedroomAbvGr\', \'TotRmsAbvGrd\'] X = df[features] y = df[\'SalePrice\'] scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Model training model = LinearRegression() model.fit(X_train, y_train) # Model evaluation y_pred = model.predict(X_test) mae = mean_absolute_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) print(f\\"Mean Absolute Error: {mae}\\") print(f\\"R^2 Score: {r2}\\") # Visualization plt.figure(figsize=(10, 6)) sns.scatterplot(x=y_test, y=y_pred) plt.xlabel(\'Actual Prices\') plt.ylabel(\'Predicted Prices\') plt.title(\'Actual vs Predicted House Prices\') plt.show() # Example usage (Assuming the CSV file is in the current directory): # house_price_prediction(\'house_prices.csv\') ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, r2_score import matplotlib.pyplot as plt import seaborn as sns def house_price_prediction(file_path: str): # Read the dataset df = pd.read_csv(file_path) # Data preprocessing # Handle missing values df.fillna(method=\'ffill\', inplace=True) # Feature scaling features = [\'LotArea\', \'YearBuilt\', \'1stFlrSF\', \'2ndFlrSF\', \'FullBath\', \'BedroomAbvGr\', \'TotRmsAbvGrd\'] X = df[features] y = df[\'SalePrice\'] scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Model training model = LinearRegression() model.fit(X_train, y_train) # Model evaluation y_pred = model.predict(X_test) mae = mean_absolute_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) print(f\\"Mean Absolute Error: {mae}\\") print(f\\"R^2 Score: {r2}\\") # Visualization plt.figure(figsize=(10, 6)) sns.scatterplot(x=y_test, y=y_pred) plt.xlabel(\'Actual Prices\') plt.ylabel(\'Predicted Prices\') plt.title(\'Actual vs Predicted House Prices\') plt.show()"},{"question":"# Question: Advanced Python Logging System Objective: You are required to design and implement a logging system for a Python application that processes multiple tasks concurrently. The system should utilize advanced logging features such as multiple handlers, custom formatters, filters, and context-specific logging to ensure robust logging across various scenarios. Task: Implement a logging system that: 1. Logs messages to both a console and a file. 2. Uses different logging levels for the console and file handlers. 3. Adds contextual information (e.g., task ID, user details) to each log message. 4. Handles logging from multiple threads. 5. Uses a custom formatter to include timestamps and contextual information. 6. Rotates log files when they reach a certain size, keeping a specific number of backups. 7. Ensures that the logging system can handle logging events generated by tasks running in multiple threads simultaneously. Implementation Details: 1. **Handlers**: - Console handler should log messages of level `INFO` and above. - File handler should log messages of level `DEBUG` and above. - Use a `RotatingFileHandler` for the file handler, with a maximum file size of 1MB and up to 3 backup files. 2. **Formatters**: - Use a custom formatter that includes the timestamp, logger name, log level, task ID, user, and the log message. 3. **Filters**: - Implement a filter to add contextual information such as `task_id` and `user` to all log messages. 4. **Logging from Multiple Threads**: - Ensure logging is thread-safe and demonstrates logging from multiple concurrent tasks. 5. **Task Function**: - Simulate task functions that perform logging operations, each associated with a unique `task_id` and `user`. Constraints and Requirements: - You must use the `logging` module. - Contextual information must be thread-specific and properly logged. - Ensure that the log rotation works and the correct number of backup files are maintained. Example Output: The logging output might look like this: ``` Console Output: 2023-10-10 10:00:00 - root - INFO - TaskID: 1 - User: Alice - Task started 2023-10-10 10:00:05 - root - INFO - TaskID: 1 - User: Alice - Processing data ... File Output (example.log): 2023-10-10 10:00:00,000 - root - DEBUG - TaskID: 1 - User: Alice - Task started 2023-10-10 10:00:05,000 - root - DEBUG - TaskID: 1 - User: Alice - Processing data ... ``` Submission: Provide the complete Python code for the logging system, including: 1. Logger configuration 2. Custom formatter and filter implementation 3. Multi-threaded task simulation 4. Demonstration of log rotation Ensure your code adheres to Python best practices and is well-documented.","solution":"import logging import threading from logging.handlers import RotatingFileHandler import time class ContextFilter(logging.Filter): def __init__(self, task_id, user): super().__init__() self.task_id = task_id self.user = user def filter(self, record): record.task_id = self.task_id record.user = self.user return True def setup_logging(task_id, user): logger = logging.getLogger(f\\"task_{task_id}_user_{user}\\") logger.setLevel(logging.DEBUG) # Create handlers console_handler = logging.StreamHandler() file_handler = RotatingFileHandler(\'example.log\', maxBytes=1 * 1024 * 1024, backupCount=3) # Set levels for handlers console_handler.setLevel(logging.INFO) file_handler.setLevel(logging.DEBUG) # Create formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - TaskID: %(task_id)s - User: %(user)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Add filter to inject contextual information filter = ContextFilter(task_id, user) logger.addFilter(filter) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) return logger def task_function(task_id, user): logger = setup_logging(task_id, user) logger.info(\\"Task started\\") time.sleep(1) logger.info(\\"Processing data\\") time.sleep(1) logger.info(\\"Task finished\\") if __name__ == \\"__main__\\": threads = [] for i in range(5): t = threading.Thread(target=task_function, args=(i, f\\"User_{i}\\")) threads.append(t) t.start() for t in threads: t.join()"},{"question":"You are required to implement a mini-library in Python that mimics some of the functionalities described in the provided documentation. Specifically, your task is to create the following functions: 1. `py_snprintf(buffer, size, format_string, *args) -> int`: - Mimics the behavior of `PyOS_snprintf`. - Takes a buffer (string), a size (maximum length), a format string, and additional arguments to format. - Returns the number of characters written to the buffer, excluding the trailing null character. - If the formatted string exceeds the size, it should truncate the output and return the required buffer size (length of the formatted string). 2. `string_to_double(s: str) -> float`: - Mimics the behavior of `PyOS_string_to_double`. - Converts a string to a double. - Raises a `ValueError` if the conversion fails. - Returns the converted double value. 3. `double_to_string(val: float, format_code: str, precision: int = 0, flags: int = 0) -> str`: - Mimics the behavior of `PyOS_double_to_string`. - Converts a double value to a string using the provided format code and precision. - `format_code` should be one of `\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, or `\'r\'`. - The flags argument can control if the result should always have a sign or if it should not look like an integer. 4. `stricmp(s1: str, s2: str) -> int`: - Mimics the behavior of `PyOS_stricmp`. - Compares two strings case-insensitively. - Returns 0 if the strings are equal, a negative number if `s1` is less than `s2`, and a positive number if `s1` is greater than `s2`. Here are the requirements and constraints for each function: # py_snprintf - **Input:** - `buffer: str` - The initial buffer string. - `size: int` - The maximum size for the formatted buffer. - `format_string: str` - The format string. - `*args` - Arguments to be formatted into the format string. - **Output:** - Returns the number of characters written to the buffer, excluding the trailing null character. # string_to_double - **Input:** - `s: str` - The string to be converted to a double. - **Output:** - Returns the converted double value. - **Exception:** - Raises `ValueError` if the conversion is not possible. # double_to_string - **Input:** - `val: float` - The double value to be converted. - `format_code: str` - One of `\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, or `\'r\'`. - `precision: int` - The precision of the formatting (0 by default). - `flags: int` - Optional flags affecting the formatting (0 by default). - **Output:** - Returns the string representation of the double value. # stricmp - **Input:** - `s1: str` - The first string. - `s2: str` - The second string. - **Output:** - Returns 0 if both strings are equal ignoring case, a negative number if `s1` is less than `s2`, and a positive number if `s1` is greater than `s2`. Your implementation should focus on correctness, error handling, and efficient use of Python capabilities.","solution":"def py_snprintf(buffer, size, format_string, *args): Mimics the behavior of PyOS_snprintf. formatted_string = format_string % args if len(formatted_string) >= size: return len(formatted_string) return len(formatted_string[:size-1]) def string_to_double(s: str) -> float: Mimics the behavior of PyOS_string_to_double. try: return float(s) except ValueError: raise ValueError(f\\"Cannot convert {s} to double\\") def double_to_string(val: float, format_code: str, precision: int = 0, flags: int = 0) -> str: Mimics the behavior of PyOS_double_to_string. if format_code not in {\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'}: raise ValueError(f\\"Invalid format code: {format_code}\\") if format_code == \'r\': return repr(val) format_spec = f\\".{precision}{format_code}\\" if flags > 0: format_spec = f\\"+{format_spec}\\" return format(val, format_spec) def stricmp(s1: str, s2: str) -> int: Mimics the behavior of PyOS_stricmp. s1_lower = s1.lower() s2_lower = s2.lower() if s1_lower < s2_lower: return -1 elif s1_lower > s2_lower: return 1 return 0"},{"question":"You are tasked with creating a basic XML-RPC server that performs various mathematical operations. Your server should expose the following functionalities: 1. **Power Function**: Calculates the power of a number. 2. **Addition Function**: Adds two numbers. 3. **Multiplication Function**: Multiplies two numbers. 4. **Quadratic Solver**: Solves a quadratic equation of the form `ax^2 + bx + c = 0`. 5. **Factorial Function**: Calculates the factorial of a number. # Server Requirements: 1. Use the `SimpleXMLRPCServer` class to set up the server. 2. Register each function/method with a meaningful name. 3. The server should run on `localhost` at port `8080`. # Function Signatures: 1. **Power Function**: ```python def power(base: float, exponent: int) -> float: ``` 2. **Addition Function**: ```python def add(x: float, y: float) -> float: ``` 3. **Multiplication Function**: ```python def multiply(x: float, y: float) -> float: ``` 4. **Quadratic Solver**: ```python def solve_quadratic(a: float, b: float, c: float) -> list: ``` 5. **Factorial Function**: ```python def factorial(n: int) -> int: ``` # Input and Output: - The server should accept and return JSON data through standard XML-RPC calls. - Use the example provided in the documentation as a reference to implement your solution. # Constraints: 1. Do not enable `allow_dotted_names`. 2. Handle invalid requests gracefully and return appropriate error messages. # Performance Requirements: 1. Ensure the server can handle at least 10 concurrent connections. 2. Response time should be under 1 second for each request. # Example Usage: ```python import xmlrpc.client s = xmlrpc.client.ServerProxy(\'http://localhost:8080\') print(s.power(2, 3)) # Should return 8 print(s.add(3, 4)) # Should return 7 print(s.multiply(5, 6)) # Should return 30 print(s.solve_quadratic(1, -3, 2)) # Should return the roots of the equation x^2 - 3x + 2 = 0 print(s.factorial(5)) # Should return 120 ``` # Implementation: 1. Define the required functions in Python. 2. Set up the `SimpleXMLRPCServer` and register these functions. 3. Ensure the server runs indefinitely and handles requests as specified. Happy coding!","solution":"from xmlrpc.server import SimpleXMLRPCServer import math def power(base: float, exponent: int) -> float: return base ** exponent def add(x: float, y: float) -> float: return x + y def multiply(x: float, y: float) -> float: return x * y def solve_quadratic(a: float, b: float, c: float) -> list: discriminant = b**2 - 4*a*c if discriminant < 0: return [] # No real roots elif discriminant == 0: return [-b / (2*a)] # One real root else: root1 = (-b + math.sqrt(discriminant)) / (2*a) root2 = (-b - math.sqrt(discriminant)) / (2*a) return [root1, root2] # Two real roots def factorial(n: int) -> int: if n < 0: raise ValueError(\\"Factorial is not defined for negative numbers.\\") result = 1 for i in range(1, n + 1): result *= i return result def main(): server = SimpleXMLRPCServer((\'localhost\', 8080)) server.register_function(power, \'power\') server.register_function(add, \'add\') server.register_function(multiply, \'multiply\') server.register_function(solve_quadratic, \'solve_quadratic\') server.register_function(factorial, \'factorial\') print(\\"Server is running on localhost:8080\\") server.serve_forever() if __name__ == \'__main__\': main()"},{"question":"In this assignment, you will implement a function that fits and evaluates a machine learning model using scikit-learn with parallel processing, configurable by an external script. # Task You are given a dataset and are asked to fit a RandomForestClassifier to it. Your task is to implement the function `fit_and_evaluate_model` that trains a RandomForestClassifier with parallel processing using the `joblib` library and evaluates its performance using cross-validation. # Requirements 1. **Parallel Processing with joblib**: Use joblib to parallelize the cross-validation. 2. **Parameterization**: The number of jobs (parallel processes or threads) should be configurable. 3. **Avoid Oversubscription**: Implement mechanisms to avoid oversubscription. # Input - `X` (numpy.ndarray): Feature matrix. - `y` (numpy.ndarray): Labels. - `num_jobs` (int): Number of parallel jobs to use. - `cv` (int): Number of cross-validation folds. # Output - `mean_score` (float): The mean cross-validation score. # Constraints - The dataset size must be manageable in memory (i.e., should not exceed available RAM). - You may use any scikit-learn utilities and estimators, but the model must be a `RandomForestClassifier`. # Example ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier def fit_and_evaluate_model(X, y, num_jobs, cv): from joblib import parallel_backend from sklearn.model_selection import cross_val_score with parallel_backend(\'loky\', n_jobs=num_jobs): classifier = RandomForestClassifier() scores = cross_val_score(classifier, X, y, cv=cv, n_jobs=num_jobs) mean_score = np.mean(scores) return mean_score # Example usage: data = load_iris() X, y = data.data, data.target mean_score = fit_and_evaluate_model(X, y, num_jobs=4, cv=5) print(mean_score) ``` In the example above, the function `fit_and_evaluate_model` fits a `RandomForestClassifier` to the data `X` and `y`, using 4 parallel jobs for cross-validation, and returns the mean cross-validation score. # Hints - Refer to the [joblib documentation](https://joblib.readthedocs.io/en/latest/parallel.html#thread-based-parallelism-vs-process-based-parallelism) for details on using parallel backends. - Experiment with different values of `OMP_NUM_THREADS` or use the `threadpoolctl` package to fine-tune the number of threads if needed. - Ensure you handle potential oversubscription scenarios as discussed in the documentation provided.","solution":"import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score from joblib import parallel_backend def fit_and_evaluate_model(X, y, num_jobs, cv): Trains a RandomForestClassifier and evaluates its performance using cross-validation. Parameters: X (numpy.ndarray): Feature matrix. y (numpy.ndarray): Labels. num_jobs (int): Number of parallel jobs to use. cv (int): Number of cross-validation folds. Returns: float: The mean cross-validation score. with parallel_backend(\'loky\', n_jobs=num_jobs): classifier = RandomForestClassifier() scores = cross_val_score(classifier, X, y, cv=cv, n_jobs=num_jobs) mean_score = np.mean(scores) return mean_score"},{"question":"**Objective**: Test the understanding and competency in using `xml.dom.pulldom` for XML parsing and DOM manipulation. **Problem Statement**: You are given an XML document containing information about a library’s collection of books. Each book has attributes: `id`, `title`, `author`, and `year`. Your task is to write a Python function, `filter_books_by_year()`, that takes an XML string and an integer year as input. The function should output a List of dictionaries, where each dictionary contains information about books published after the given year. Requirements: 1. **Implement the function** `filter_books_by_year(xml_string: str, year: int) -> List[Dict[str, str]]`. 2. The function should utilize the `xml.dom.pulldom` module to parse the XML string. 3. For each book element, if the `year` attribute’s value is greater than the provided year, the function should record the `id`, `title`, `author`, and `year` attributes into a dictionary and add it to the output list. 4. The output list should contain dictionaries sorted by the `year` attribute in ascending order. Input: 1. `xml_string` (str): A string representation of the XML document. 2. `year` (int): The year to filter books by. Output: 1. List[Dict[str, str]]: A list of dictionaries with keys `\\"id\\"`, `\\"title\\"`, `\\"author\\"`, and `\\"year\\"` corresponding to each book\'s details. Example: ```python xml_string = \'\'\' <library> <book id=\\"1\\" title=\\"Book A\\" author=\\"Author X\\" year=\\"2001\\" /> <book id=\\"2\\" title=\\"Book B\\" author=\\"Author Y\\" year=\\"1999\\" /> <book id=\\"3\\" title=\\"Book C\\" author=\\"Author Z\\" year=\\"2005\\" /> </library> \'\'\' filter_books_by_year(xml_string, 2000) ``` Expected output: ```python [ {\'id\': \'1\', \'title\': \'Book A\', \'author\': \'Author X\', \'year\': \'2001\'}, {\'id\': \'3\', \'title\': \'Book C\', \'author\': \'Author Z\', \'year\': \'2005\'} ] ``` **Constraints**: 1. Only valid well-formed XML inputs will be provided. 2. If no books meet the criteria, return an empty list. 3. Performance considerations: The XML string can be quite large, so the solution should manage memory efficiently without unnecessarily expanding the entire DOM tree. **Guidelines**: 1. Use the `xml.dom.pulldom.parseString()` function to parse the input XML string. 2. Handle `START_ELEMENT` events to identify `book` elements and their attributes. 3. Collect qualifying books into a list and sort them by the `year` attribute before returning. **Note**: Ensure the function handles edge cases like missing attributes or different attribute orderings gracefully, and include relevant comments/documentation in your code.","solution":"from typing import List, Dict from xml.dom.pulldom import parseString, START_ELEMENT def filter_books_by_year(xml_string: str, year: int) -> List[Dict[str, str]]: Filters books by year from the given XML string. :param xml_string: A string representation of the XML document. :param year: The year to filter books by. :return: A list of dictionaries with keys \\"id\\", \\"title\\", \\"author\\", and \\"year\\". filtered_books = [] events = parseString(xml_string) for event, node in events: if event == START_ELEMENT and node.tagName == \'book\': node_id = node.getAttribute(\'id\') title = node.getAttribute(\'title\') author = node.getAttribute(\'author\') node_year = node.getAttribute(\'year\') if node_year.isdigit() and int(node_year) > year: filtered_books.append({ \'id\': node_id, \'title\': title, \'author\': author, \'year\': node_year }) # Sort the books by the year attribute filtered_books.sort(key=lambda x: int(x[\'year\'])) return filtered_books"},{"question":"# Asynchronous Web Scraper with Timeout Problem Statement You are required to implement an asynchronous web scraper that fetches data from multiple URLs concurrently, with a specified timeout for each request. The web scraper should also handle exceptions gracefully, logging any errors encountered during the scraping process. Function Signature ```python import asyncio async def fetch_with_timeout(url: str, timeout: int) -> str: Fetch the content of a URL with a specified timeout. Args: url (str): The URL to fetch. timeout (int): Timeout in seconds. Returns: str: The content of the URL if fetched successfully within the timeout. Raises: asyncio.TimeoutError: If the fetching operation exceeds the specified timeout. pass async def fetch_all(urls: list[str], timeout: int) -> dict[str, str]: Fetch the content of multiple URLs concurrently with a specified timeout for each. Args: urls (list[str]): A list of URLs to fetch. timeout (int): Timeout in seconds for each request. Returns: dict[str, str]: A dictionary where the keys are the URLs and the values are the fetched content. If a URL fails to be fetched within the timeout, its value should be \'Timeout\'. pass async def main(): urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] timeout = 5 results = await fetch_all(urls, timeout) print(results) # asyncio.run(main()) ``` Constraints - The function `fetch_with_timeout` should use the `asyncio.wait_for` method to enforce the timeout. - If a request times out, handle the `asyncio.TimeoutError` and return a \'Timeout\' string as the value for that URL in the result dictionary. - At least three URLs should be tested in the `main` function. - Ensure proper logging of exceptions encountered during the scraping process. Example Usage ```python import asyncio async def main(): urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] timeout = 5 results = await fetch_all(urls, timeout) print(results) # Output might look like: # { # \\"http://example.com\\": \\"<html>...</html>\\", # \\"http://example.org\\": \\"Timeout\\", # \\"http://example.net\\": \\"<html>...</html>\\" # } # asyncio.run(main()) ``` Ensure your solution handles asynchronous operations and exceptions properly, and make use of asyncio\'s core functionalities. Additional Notes - You can use the `aiohttp` library for asynchronous HTTP requests if needed. - Focus on writing clean, readable, and efficient code.","solution":"import asyncio import aiohttp async def fetch_with_timeout(url: str, timeout: int) -> str: Fetch the content of a URL with a specified timeout. Args: url (str): The URL to fetch. timeout (int): Timeout in seconds. Returns: str: The content of the URL if fetched successfully within the timeout. Raises: asyncio.TimeoutError: If the fetching operation exceeds the specified timeout. try: async with aiohttp.ClientSession() as session: async with session.get(url, timeout=timeout) as response: return await response.text() except asyncio.TimeoutError: return \'Timeout\' except Exception as e: print(f\'Error fetching {url}: {e}\') return \'Error\' async def fetch_all(urls: list[str], timeout: int) -> dict[str, str]: Fetch the content of multiple URLs concurrently with a specified timeout for each. Args: urls (list[str]): A list of URLs to fetch. timeout (int): Timeout in seconds for each request. Returns: dict[str, str]: A dictionary where the keys are the URLs and the values are the fetched content. If a URL fails to be fetched within the timeout, its value should be \'Timeout\'. tasks = [fetch_with_timeout(url, timeout) for url in urls] results = await asyncio.gather(*tasks) return dict(zip(urls, results)) async def main(): urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] timeout = 5 results = await fetch_all(urls, timeout) print(results) # Uncomment the following line to run the main function # asyncio.run(main())"},{"question":"# HTTP Client Implementation with Error Handling You are required to implement a Python class `CustomHTTPClient` that uses the `http.client` module to perform HTTP operations effectively. Your class should demonstrate an understanding of: - Establishing connections. - Making GET, POST, and PUT requests. - Handling responses. - Managing exceptions specific to HTTP operations. - Utilizing chunked transfer encoding for large payloads. Requirements: 1. **Initialization:** Initialize your class with parameters for host, port, and optional timeout and source_address. 2. **GET request method:** - Method name: `perform_get_request` - Input: `url` (string) - Output: Return the response body (string) - Exception Handling: Handle HTTP-specific exceptions and print relevant error messages. 3. **POST request method:** - Method name: `perform_post_request` - Input: `url` (string), `params` (dictionary), `headers` (dictionary) - Output: Return the response body (string) - Exception Handling: Handle HTTP-specific exceptions and print relevant error messages. 4. **PUT request method with chunked encoding:** - Method name: `perform_put_request` - Input: `url` (string), `data` (string) - Output: Return the response status and reason (tuple) - Exception Handling: Handle HTTP-specific exceptions and print relevant error messages. 5. **Connection Management:** - Method to open and close connections within the above request methods. 6. **Main Function:** Demonstrate the usage of your class by performing at least one GET, POST, and PUT request to valid URLs. ```python import http.client import urllib.parse class CustomHTTPClient: def __init__(self, host, port=None, timeout=None, source_address=None): Initialization of the HTTP client with given host, port, timeout, and source address. pass def perform_get_request(self, url): Perform a GET request to the given URL. pass def perform_post_request(self, url, params, headers): Perform a POST request to the given URL with the specific parameters and headers. pass def perform_put_request(self, url, data): Perform a PUT request to the given URL with chunked encoding data. pass # Example Main Function to demonstrate the class usage if __name__ == \\"__main__\\": client = CustomHTTPClient(\\"www.example.com\\") # GET Request Example try: get_response = client.perform_get_request(\\"/\\") print(\\"GET Response:\\", get_response) except Exception as e: print(\\"GET Request Error:\\", e) # POST Request Example try: post_params = {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"} post_headers = {\\"Content-type\\": \\"application/x-www-form-urlencoded\\", \\"Accept\\": \\"text/plain\\"} post_response = client.perform_post_request(\\"/\\", post_params, post_headers) print(\\"POST Response:\\", post_response) except Exception as e: print(\\"POST Request Error:\\", e) # PUT Request Example try: put_data = \\"Example data for chunked encoding\\" put_status_reason = client.perform_put_request(\\"/example\\", put_data) print(\\"PUT Response Status and Reason:\\", put_status_reason) except Exception as e: print(\\"PUT Request Error:\\", e) ``` Notes: - Be meticulous in handling exceptions specific to HTTP operations. - Ensure that connections are appropriately opened and closed for each request. - Validate the responses to ensure the correct processing of the status codes and body. This exercise is designed to test a student\'s understanding of the `http.client` module and their ability to apply it in practical scenarios.","solution":"import http.client import urllib.parse import json class CustomHTTPClient: def __init__(self, host, port=None, timeout=None, source_address=None): Initialization of the HTTP client with given host, port, timeout, and source address. self.host = host self.port = port self.timeout = timeout self.source_address = source_address self.conn = None def open_connection(self): self.conn = http.client.HTTPConnection(self.host, self.port, timeout=self.timeout, source_address=self.source_address) def close_connection(self): if self.conn: self.conn.close() def perform_get_request(self, url): Perform a GET request to the given URL. try: self.open_connection() self.conn.request(\\"GET\\", url) response = self.conn.getresponse() return response.read().decode() except Exception as e: print(f\\"GET Request Error: {e}\\") finally: self.close_connection() def perform_post_request(self, url, params, headers): Perform a POST request to the given URL with the specific parameters and headers. try: self.open_connection() params_encoded = urllib.parse.urlencode(params) self.conn.request(\\"POST\\", url, body=params_encoded, headers=headers) response = self.conn.getresponse() return response.read().decode() except Exception as e: print(f\\"POST Request Error: {e}\\") finally: self.close_connection() def perform_put_request(self, url, data): Perform a PUT request to the given URL with chunked encoding data. try: self.open_connection() headers = {\\"Content-Type\\": \\"application/json\\", \\"Transfer-Encoding\\": \\"chunked\\"} self.conn.putrequest(\\"PUT\\", url) for key, value in headers.items(): self.conn.putheader(key, value) self.conn.endheaders() for chunk in self.chunk_data(data, 1024): self.conn.send(chunk.encode()) response = self.conn.getresponse() return (response.status, response.reason) except Exception as e: print(f\\"PUT Request Error: {e}\\") finally: self.close_connection() @staticmethod def chunk_data(data, chunk_size): for i in range(0, len(data), chunk_size): yield data[i:i + chunk_size] # Example Main Function to demonstrate the class usage if __name__ == \\"__main__\\": client = CustomHTTPClient(\\"www.example.com\\") # GET Request Example try: get_response = client.perform_get_request(\\"/\\") print(\\"GET Response:\\", get_response) except Exception as e: print(\\"GET Request Error:\\", e) # POST Request Example try: post_params = {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"} post_headers = {\\"Content-type\\": \\"application/x-www-form-urlencoded\\", \\"Accept\\": \\"text/plain\\"} post_response = client.perform_post_request(\\"/\\", post_params, post_headers) print(\\"POST Response:\\", post_response) except Exception as e: print(\\"POST Request Error:\\", e) # PUT Request Example try: put_data = \\"Example data for chunked encoding\\" put_status_reason = client.perform_put_request(\\"/example\\", put_data) print(\\"PUT Response Status and Reason:\\", put_status_reason) except Exception as e: print(\\"PUT Request Error:\\", e)"},{"question":"Objective You are required to create a custom wrapper for the built-in `print` function using the `builtins` module. Your custom print function should add a custom prefix (e.g., \\"[Custom]\\") to every output line. Problem Description 1. Use the `builtins` module to retain access to the original `print` function. 2. Implement a custom function `custom_print` that adds a prefix \\"[Custom]\\" to each line of output. 3. Ensure that the custom function supports all functionalities of the original `print` function (e.g., handling multiple arguments, different `sep`, `end`, `file` arguments, etc.). Input - Same as the built-in `print` function. Output - Each output line should be prefixed with \\"[Custom]\\". Example ```python custom_print(\\"Hello, World!\\") # Output: [Custom] Hello, World! custom_print(\\"Hello\\", \\"World\\", sep=\\"*\\") # Output: [Custom] Hello*World custom_print(\\"Hello\\", end=\\"!!!\\") # Output: [Custom] Hello!!! ``` Constraints - The implementation must use the `builtins` module to access the original `print` function. - Do not use any external libraries. - The custom function must support all the default arguments of the built-in `print` function. Function Signature ```python import builtins def custom_print(*args, sep=\' \', end=\'n\', file=None, flush=False): # Your code here ``` Implement the `custom_print` function accordingly and ensure it behaves as described.","solution":"import builtins def custom_print(*args, sep=\' \', end=\'n\', file=None, flush=False): A custom print function that adds a prefix \'[Custom]\' to each line of output. # Join the messages to form a single string. output = sep.join(map(str, args)) # Add the custom prefix to the output. final_output = f\\"[Custom] {output}\\" # Use the original built-in print function to print the output. builtins.print(final_output, end=end, file=file, flush=flush)"},{"question":"# Advanced XML Manipulation with `xml.etree.ElementTree` You are provided with an XML document representing a simple library catalog, which includes various books. Using the `xml.etree.ElementTree` module, write a Python function to perform the following tasks on the XML data: 1. **Parse the XML Document:** - Read the XML data from a string input. 2. **Count Books Published After a Given Year:** - Create a function that accepts a year as input and returns the count of books published after that year. 3. **Retrieve Book Information:** - Create a function that takes the title of a book as input and returns a dictionary with its details: author, year, and genre. 4. **Add a New Book:** - Create a function that takes details of a book (title, author, year, genre) as arguments and adds this book to the catalog. Save the modified XML to a file named `updated_catalog.xml`. # Input XML Format ```xml <catalog> <book> <title>Book1</title> <author>Author1</author> <year>1999</year> <genre>Fiction</genre> </book> <book> <title>Book2</title> <author>Author2</author> <year>2005</year> <genre>Science</genre> </book> <!-- More book elements --> </catalog> ``` # Function Definitions 1. **parse_xml(xml_string):** - **Input:** A string containing XML data. - **Output:** The root element of the parsed XML. 2. **count_books_after_year(root, year):** - **Input:** The root element of the XML and a year (int). - **Output:** The count of books published after the given year (int). 3. **get_book_details(root, title):** - **Input:** The root element of the XML and a book title (string). - **Output:** A dictionary with keys `author`, `year`, and `genre`. 4. **add_new_book(root, title, author, year, genre):** - **Input:** The root element of the XML, and details of a book (title, author, year, genre as strings). - **Output:** None (but the function should save the updated XML to `updated_catalog.xml`). # Constraints - Assume the input XML string is well-formed and valid. - For simplicity, assume there are no duplicate book titles. # Example Usage ```python xml_data = <catalog> <book> <title>Book1</title> <author>Author1</author> <year>1999</year> <genre>Fiction</genre> </book> <book> <title>Book2</title> <author>Author2</author> <year>2005</year> <genre>Science</genre> </book> </catalog> root = parse_xml(xml_data) print(count_books_after_year(root, 2000)) # Output: 1 print(get_book_details(root, \\"Book2\\")) # Output: {\'author\': \'Author2\', \'year\': \'2005\', \'genre\': \'Science\'} add_new_book(root, \\"Book3\\", \\"Author3\\", \\"2022\\", \\"History\\") ``` # Notes - Make sure to handle varying tag structure and potential missing elements gracefully. - Ensure the XML file is written with proper indentation to improve readability.","solution":"import xml.etree.ElementTree as ET def parse_xml(xml_string): Parse the XML data from a string input and return the root element. return ET.fromstring(xml_string) def count_books_after_year(root, year): Count the number of books in the catalog published after the given year. Args: root (ET.Element): The root element of the XML tree. year (int): The year to compare against. Returns: int: The count of books published after the given year. count = 0 for book in root.findall(\'book\'): book_year = int(book.find(\'year\').text) if book_year > year: count += 1 return count def get_book_details(root, title): Retrieve details of a book by its title. Args: root (ET.Element): The root element of the XML tree. title (str): The title of the book. Returns: dict: A dictionary with keys \'author\', \'year\', and \'genre\'. for book in root.findall(\'book\'): if book.find(\'title\').text == title: return { \'author\': book.find(\'author\').text, \'year\': book.find(\'year\').text, \'genre\': book.find(\'genre\').text } return None def add_new_book(root, title, author, year, genre): Add a new book to the catalog and save the updated XML to \'updated_catalog.xml\'. Args: root (ET.Element): The root element of the XML tree. title (str): The title of the new book. author (str): The author of the new book. year (str): The year the new book was published. genre (str): The genre of the new book. Returns: None new_book = ET.Element(\'book\') title_element = ET.Element(\'title\') title_element.text = title new_book.append(title_element) author_element = ET.Element(\'author\') author_element.text = author new_book.append(author_element) year_element = ET.Element(\'year\') year_element.text = year new_book.append(year_element) genre_element = ET.Element(\'genre\') genre_element.text = genre new_book.append(genre_element) root.append(new_book) tree = ET.ElementTree(root) tree.write(\'updated_catalog.xml\', encoding=\'utf-8\', xml_declaration=True)"},{"question":"# Advanced Python Programming Assessment Objective: To assess your understanding of advanced Python concepts such as custom exception handling, class design, and complex data structure manipulations. Problem Statement: You are required to design a system that manages a library. The library contains books, and users can borrow and return books. The system should keep track of each user\'s borrowed books and impose restrictions on the borrowing process. Requirements: 1. **Classes and Data Structures:** - Create a `Book` class with the following attributes: - `title` (str) - `author` (str) - `isbn` (str) - `available` (bool) - represents if the book is currently available for borrowing. - Create a `User` class with the following attributes: - `username` (str) - `borrowed_books` (list) - a list to keep track of books borrowed by the user (up to 3 books at a time). - Create a `Library` class that manages the books and users with the following methods: - `add_book(book: Book)` - adds a book to the library\'s collection. - `register_user(username: str)` - registers a new user. - `borrow_book(username: str, isbn: str)` - allows a user to borrow a book if it is available and if the user has not exceeded the borrowing limit. - `return_book(username: str, isbn: str)` - allows a user to return a borrowed book. 2. **Exception Handling:** - Define custom exceptions for the following scenarios: - `BookNotFoundError` - raised when a book with the given ISBN does not exist in the library. - `UserNotFoundError` - raised when a user is not found in the library. - `BookNotAvailableError` - raised when a book is not available for borrowing. - `MaxBorrowLimitReachedError` - raised when a user tries to borrow more than 3 books. 3. **Testing the System:** - Add sample books and users. - Demonstrate the borrowing and returning process, handling exceptions appropriately. Input and Output Formats: - Input: Methods will be called with the required parameters. - Output: Return appropriate values or messages based on the method\'s functionality. Constraints: - Users can borrow a maximum of 3 books at a time. - Books can be borrowed only if they are available in the library. Performance Requirements: - The system should efficiently handle up to 100 users and 500 books. Example Usage: ```python # Define the book object book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", isbn=\\"9780451524935\\", available=True) book2 = Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", isbn=\\"9780446310789\\", available=True) # Initialize the library system library = Library() # Add books to the library library.add_book(book1) library.add_book(book2) # Register a user library.register_user(\\"john_doe\\") # User borrows a book library.borrow_book(\\"john_doe\\", \\"9780451524935\\") # User tries to borrow more than the limit try: library.borrow_book(\\"john_doe\\", \\"9780446310789\\") library.borrow_book(\\"john_doe\\", \\"9780451524936\\") library.borrow_book(\\"john_doe\\", \\"9780451524937\\") except MaxBorrowLimitReachedError as e: print(e) # Return a book library.return_book(\\"john_doe\\", \\"9780451524935\\") ``` Implement the classes and methods to meet the outlined requirements and handle exceptions as specified.","solution":"class BookNotFoundError(Exception): pass class UserNotFoundError(Exception): pass class BookNotAvailableError(Exception): pass class MaxBorrowLimitReachedError(Exception): pass class Book: def __init__(self, title, author, isbn, available=True): self.title = title self.author = author self.isbn = isbn self.available = available class User: def __init__(self, username): self.username = username self.borrowed_books = [] class Library: def __init__(self): self.books = {} self.users = {} def add_book(self, book): self.books[book.isbn] = book def register_user(self, username): if username not in self.users: self.users[username] = User(username) def borrow_book(self, username, isbn): if username not in self.users: raise UserNotFoundError(\\"User not found.\\") if isbn not in self.books: raise BookNotFoundError(\\"Book not found.\\") if not self.books[isbn].available: raise BookNotAvailableError(\\"Book not available.\\") if len(self.users[username].borrowed_books) >= 3: raise MaxBorrowLimitReachedError(\\"Maximum borrow limit reached.\\") self.books[isbn].available = False self.users[username].borrowed_books.append(self.books[isbn]) def return_book(self, username, isbn): if username not in self.users: raise UserNotFoundError(\\"User not found.\\") if isbn not in self.books: raise BookNotFoundError(\\"Book not found.\\") user = self.users[username] book_to_return = None for book in user.borrowed_books: if book.isbn == isbn: book_to_return = book break if book_to_return: user.borrowed_books.remove(book_to_return) self.books[isbn].available = True else: raise BookNotFoundError(\\"Book not borrowed by user.\\")"},{"question":"Objective Create a small terminal-based text editor using the `curses` library that demonstrates your understanding of: - Initializing and setting up `curses`. - Managing windows and input. - Handling basic text input and display. - Implementing color schemes. Problem Statement You need to implement a function `text_editor()` that initializes a `curses` window and allows the user to input text in a colored text box. The editor should: 1. Be initialized upon invoking the `text_editor` function. 2. Allow the user to type text. The maximum number of characters is capped at 100. 3. Handle arrow keys to move the cursor. 4. Implement a color scheme for the text display. 5. Provide simple editing commands: - \\"Ctrl+G\\" to save the text and exit the editor. - \\"Ctrl+S\\" to save the text without exiting. - \\"Ctrl+Q\\" to quit without saving. Constraints - The function should clean up and restore the terminal to its previous state after exiting the editor. - Make use of appropriate exception handling to catch and handle errors related to `curses` functions. Function Signature ```python def text_editor(): pass ``` Implementation Details 1. **Initialization**: Use `initscr()` to initialize the screen and obtain a window object. 2. **Color Setup**: Use `start_color()` and `init_pair()` to define a color scheme. 3. **Window Management**: Create a subwindow or pad for the text input area. Enable keypad input and disable echo. 4. **Input Handling**: Use `getch()` to handle various inputs, including alphanumeric keys, arrow keys, and control commands. 5. **Text Handling**: Implement basic editing functions to handle text input, and moving the cursor using the arrow keys. 6. **Finalization**: Use `curses.wrapper()` to ensure the terminal is restored to its previous state after exiting. Example Here is a simplified example of how you might begin the implementation: ```python import curses def text_editor(): def main(stdscr): curses.start_color() curses.init_pair(1, curses.COLOR_RED, curses.COLOR_BLACK) stdscr.clear() stdscr.refresh() win = curses.newwin(10, 40, 5, 5) win.bkgd(curses.color_pair(1)) win.refresh() text = [] cursor_x = cursor_y = 0 while True: key = win.getch() if key == ord(\'g\') and curses.keyname(key) == b\'^G\': break # Exit on Ctrl+G elif key == curses.KEY_LEFT: cursor_x = max(0, cursor_x - 1) elif key == curses.KEY_RIGHT: cursor_x = min(39, cursor_x + 1) elif 0 <= key <= 255: win.addch(cursor_y, cursor_x, chr(key)) text.append(chr(key)) cursor_x += 1 win.move(cursor_y, cursor_x) win.refresh() curses.wrapper(main) ``` Implement the complete text editor functionality as outlined, ensure proper cleanup, and handle various user inputs effectively.","solution":"import curses def text_editor(): def main(stdscr): # Initialize curses curses.curs_set(1) # Make the cursor visible curses.start_color() curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLUE) stdscr.bkgd(curses.color_pair(1)) stdscr.refresh() win = curses.newwin(curses.LINES - 2, curses.COLS - 2, 1, 1) win.bkgd(curses.color_pair(1)) win.keypad(True) text = [] max_chars = 100 cursor_x = cursor_y = 0 while True: key = win.getch() if key == 7: # Ctrl+G break # Save and exit elif key == 19: # Ctrl+S # Handle saving logic here (to be implemented) pass elif key == 17: # Ctrl+Q return # Quit without saving elif key == curses.KEY_LEFT: cursor_x = max(0, cursor_x - 1) elif key == curses.KEY_RIGHT: cursor_x = min(curs.cell_resolution.x, cursor_x + 1) elif key == curses.KEY_UP: cursor_y = max(0, cursor_y - 1) elif key == curses.KEY_DOWN: cursor_y = min(curs.cell_resolution.y, cursor_y + 1) elif 32 <= key < 127 and len(text) < max_chars: win.addch(cursor_y, cursor_x, chr(key)) text.append((cursor_y, cursor_x, chr(key))) cursor_x += 1 if cursor_x >= curses.COLS - 2: cursor_x = 0 cursor_y += 1 win.move(cursor_y, cursor_x) win.refresh() curses.wrapper(main)"},{"question":"You are tasked to implement functionality for managing a collection of books in a library. Each book has attributes such as title, author, publication year, and ISBN number. You need to define a data structure for the book and implement a context manager for temporarily replacing the list of books in the library for testing purposes. Implement the following: 1. A `Book` data class with the following attributes: - `title` (str) - `author` (str) - `year` (int) - `isbn` (str) 2. A `Library` class that manages a collection of `Book` instances. The class should have methods: - `add_book(book: Book) -> None`: Adds a book to the collection. - `remove_book(isbn: str) -> bool`: Removes a book with the given ISBN from the collection. Returns `True` if the book was found and removed, `False` otherwise. - `find_book(isbn: str) -> Optional[Book]`: Finds and returns a book with the given ISBN. Returns `None` if the book is not found. - `list_books() -> List[Book]`: Returns a list of all books in the collection. 3. A context manager `temporary_library` using the `contextlib` module that temporarily replaces the list of books in the `Library` class. This is useful for testing purposes: - `temporary_library(new_books: List[Book])`: Temporarily replaces the library\'s book collection with `new_books` for the duration of the context. # Example ```python from contextlib import contextmanager from dataclasses import dataclass, field from typing import List, Optional @dataclass class Book: title: str author: str year: int isbn: str class Library: def __init__(self): self._books = [] def add_book(self, book: Book) -> None: self._books.append(book) def remove_book(self, isbn: str) -> bool: for book in self._books: if book.isbn == isbn: self._books.remove(book) return True return False def find_book(self, isbn: str) -> Optional[Book]: for book in self._books: if book.isbn == isbn: return book return None def list_books(self) -> List[Book]: return self._books @contextmanager def temporary_library(new_books: List[Book]): original_books = Library().list_books() Library()._books = new_books try: yield finally: Library()._books = original_books # Example usage: lib = Library() lib.add_book(Book(\\"Book One\\", \\"Author One\\", 2001, \\"ISBN001\\")) lib.add_book(Book(\\"Book Two\\", \\"Author Two\\", 2002, \\"ISBN002\\")) with temporary_library([Book(\\"Temporary Book\\", \\"Temp Author\\", 2020, \\"TEMP001\\")]): temp_books = lib.list_books() assert len(temp_books) == 1 assert temp_books[0].title == \\"Temporary Book\\" # After context, original library is restored original_books = lib.list_books() assert len(original_books) == 2 assert original_books[0].title == \\"Book One\\" ``` # Input - Sequence of operations to add, remove, find books in the library. - Temporary list of books when using context manager. # Output - List of current books in the library after each operation and context. # Constraints - Assume all ISBN numbers are unique in the library. # Performance Requirements - The implementation should handle a moderate-sized list of books (up to a few hundred) efficiently.","solution":"from dataclasses import dataclass from typing import List, Optional from contextlib import contextmanager @dataclass class Book: title: str author: str year: int isbn: str class Library: def __init__(self): self._books = [] def add_book(self, book: Book) -> None: self._books.append(book) def remove_book(self, isbn: str) -> bool: for book in self._books: if book.isbn == isbn: self._books.remove(book) return True return False def find_book(self, isbn: str) -> Optional[Book]: for book in self._books: if book.isbn == isbn: return book return None def list_books(self) -> List[Book]: return self._books @contextmanager def temporary_library(library: Library, new_books: List[Book]): original_books = library.list_books() library._books = new_books try: yield finally: library._books = original_books"},{"question":"Objective Evaluate your understanding of semi-supervised learning using the scikit-learn package by implementing and tuning a semi-supervised classifier on a provided dataset. Problem Statement You are given a dataset containing both labeled and unlabeled data. Your task is to train a semi-supervised classifier using scikit-learn\'s `LabelSpreading` model to predict labels for the unlabeled data. Dataset - **Input**: A dataset with features and partially labeled target values. - **Format**: - `X`: A numpy array of shape `(n_samples, n_features)` representing the input features. - `y`: A numpy array of shape `(n_samples,)` where some entries are `-1` indicating unlabeled data. Constraints - Use the `LabelSpreading` class from scikit-learn. - Apply both RBF and KNN kernels and compare their performance. - Evaluate the model using appropriate semi-supervised learning metrics. Required Implementation 1. **Load Data**: - Create a mock dataset with at least 50 samples, where 30% of the labels are set to `-1`. 2. **Implement**: - Implement a function `train_semi_supervised(X, y)` that: - Takes `X` and `y` as input. - Trains a `LabelSpreading` model with both RBF and KNN kernels. - Returns the trained models and their respective predicted labels for the dataset. 3. **Evaluate**: - Implement evaluation metrics to compare the performance of the two kernels. - Metrics such as accuracy, precision, and recall can be used, considering semi-supervised context. Function Signature ```python def train_semi_supervised(X: np.ndarray, y: np.ndarray) -> Tuple[dict, dict]: Train a LabelSpreading model with both RBF and KNN kernels. Parameters: X (np.ndarray): Input features of shape (n_samples, n_features). y (np.ndarray): Target labels of shape (n_samples,). Returns: Tuple[dict, dict]: A tuple containing: - Dictionary with trained models for RBF and KNN kernels. - Dictionary with predicted labels for RBF and KNN kernels. pass ``` # Example Usage ```python import numpy as np X = np.random.rand(50, 5) y = np.random.randint(0, 2, 50) y[:15] = -1 # Simulating unlabeled data trained_models, predictions = train_semi_supervised(X, y) print(f\\"RBF Kernel Predictions: {predictions[\'rbf\']}\\") print(f\\"KNN Kernel Predictions: {predictions[\'knn\']}\\") ``` Notes - You may consult the scikit-learn documentation on `LabelSpreading` for additional parameters and methods. - Properly handle any preprocessing steps required for the dataset. - Ensure reproducibility by setting random seeds where necessary. Submission Submit your code implementation and a short report (maximum 1 page) discussing the results and your observations on the performance differences between the RBF and KNN kernels.","solution":"import numpy as np from sklearn.semi_supervised import LabelSpreading from sklearn.metrics import accuracy_score, precision_score, recall_score def train_semi_supervised(X, y): Train a LabelSpreading model with both RBF and KNN kernels. Parameters: X (np.ndarray): Input features of shape (n_samples, n_features). y (np.ndarray): Target labels of shape (n_samples,). Returns: Tuple[dict, dict]: A tuple containing: - Dictionary with trained models for RBF and KNN kernels. - Dictionary with predicted labels for RBF and KNN kernels. models = {} predictions = {} # Train RBF kernel model model_rbf = LabelSpreading(kernel=\'rbf\') model_rbf.fit(X, y) models[\'rbf\'] = model_rbf predictions[\'rbf\'] = model_rbf.transduction_ # Train KNN kernel model model_knn = LabelSpreading(kernel=\'knn\') model_knn.fit(X, y) models[\'knn\'] = model_knn predictions[\'knn\'] = model_knn.transduction_ return models, predictions # Utility function to generate a mock dataset def generate_mock_data(n_samples=50, n_features=5, unlabeled_ratio=0.3): X = np.random.rand(n_samples, n_features) y = np.random.randint(0, 2, n_samples) unlabeled_indices = np.random.choice(n_samples, int(unlabeled_ratio * n_samples), replace=False) y[unlabeled_indices] = -1 return X, y"},{"question":"Objective In this task, you will demonstrate your understanding of working with PyTorch tensors on GPU using ROCm framework. You will implement several functions that utilize PyTorch\'s CUDA interfaces, which are reused in PyTorch\'s HIP serialization, for device management and memory checking. Background **PyTorch** provides a comprehensive library for tensor computations on GPUs. In ROCm, it reuses the CUDA interfaces (such as `torch.cuda`). This assessment will require you to utilize these interfaces for tensor operations and memory management. Task 1. **Tensor Device Operations**: - Implement a function `tensor_operations_on_device` that takes two tensors and a device index as input. - The function should: - Move both tensors to the specified GPU device. - Perform element-wise addition and return the resulting tensor. 2. **Memory Management**: - Implement a function `gpu_memory_statistics` that provides memory statistics of the specified GPU device. - The function should: - Return a dictionary containing: - `allocated_memory`: Amount of memory currently allocated. - `max_allocated_memory`: Maximum amount of memory allocated. - `reserved_memory`: Total amount of memory reserved by the caching allocator. - `max_reserved_memory`: Maximum amount of memory reserved by the caching allocator. Constraints - You can assume the device index provided is valid for the system\'s GPUs. - Utilize the existing CUDA (HIP) interfaces for GPU operations and memory management. Function Signatures ```python import torch def tensor_operations_on_device(tensor1: torch.Tensor, tensor2: torch.Tensor, device_index: int) -> torch.Tensor: Move tensors to the specified GPU device, perform element-wise addition and return the resulting tensor. Args: tensor1 (torch.Tensor): The first tensor for the operation. tensor2 (torch.Tensor): The second tensor for the operation. device_index (int): The index of the GPU device. Returns: torch.Tensor: The tensor resulting from element-wise addition on the specified GPU device. pass def gpu_memory_statistics(device_index: int) -> dict: Get memory statistics of the specified GPU device. Args: device_index (int): The index of the GPU device. Returns: dict: A dictionary containing memory statistics. pass ``` Example Usage ```python # Example usage of `tensor_operations_on_device` tensor1 = torch.tensor([1.0, 2.0, 3.0]) tensor2 = torch.tensor([4.0, 5.0, 6.0]) device_index = 0 result_tensor = tensor_operations_on_device(tensor1, tensor2, device_index) print(result_tensor) # Should print the result of tensor1 + tensor2 moved to the GPU with index 0 # Example usage of `gpu_memory_statistics` device_index = 0 stats = gpu_memory_statistics(device_index) print(stats) # Should print the memory statistics dictionary for the GPU with index 0 ``` Performance Requirements - The functions should efficiently utilize GPU resources for tensor operations. - Ensure the memory statistics retrieval does not introduce significant overhead.","solution":"import torch def tensor_operations_on_device(tensor1: torch.Tensor, tensor2: torch.Tensor, device_index: int) -> torch.Tensor: Move tensors to the specified GPU device, perform element-wise addition and return the resulting tensor. Args: tensor1 (torch.Tensor): The first tensor for the operation. tensor2 (torch.Tensor): The second tensor for the operation. device_index (int): The index of the GPU device. Returns: torch.Tensor: The tensor resulting from element-wise addition on the specified GPU device. device = torch.device(f\'cuda:{device_index}\') tensor1 = tensor1.to(device) tensor2 = tensor2.to(device) result = tensor1 + tensor2 return result def gpu_memory_statistics(device_index: int) -> dict: Get memory statistics of the specified GPU device. Args: device_index (int): The index of the GPU device. Returns: dict: A dictionary containing memory statistics. device = torch.device(f\'cuda:{device_index}\') allocated_memory = torch.cuda.memory_allocated(device) max_allocated_memory = torch.cuda.max_memory_allocated(device) reserved_memory = torch.cuda.memory_reserved(device) max_reserved_memory = torch.cuda.max_memory_reserved(device) return { \'allocated_memory\': allocated_memory, \'max_allocated_memory\': max_allocated_memory, \'reserved_memory\': reserved_memory, \'max_reserved_memory\': max_reserved_memory }"},{"question":"# Python Coding Assessment: Working with Property List Files Objective: Design a function that reads an input plist file, performs specific manipulations on its contents, and writes the modified content back to another plist file. Problem Statement: You are provided with a function signature and its partial implementation. Complete the function to achieve the desired functionality. Function Signature: ```python import plistlib from datetime import datetime def process_plist(input_filename: str, output_filename: str, key_to_update: str, new_value): Reads a plist file, updates a specified key with a new value, and writes the modified content to another file. Parameters: input_filename (str): The path to the input plist file. output_filename (str): The path to the output plist file. key_to_update (str): The key in the plist file to update. new_value: The new value to set for the specified key. Can be of type str, int, float, bool, list, dict, bytes, bytearray, or datetime. Returns: None # Your code here ``` Constraints: 1. The input file is guaranteed to be a valid plist file. 2. If the specified `key_to_update` does not exist in the plist, add it with the given `new_value`. 3. Ensure that the output plist file retains the same format (XML or binary) as the input file. 4. Handle potential exceptions, such as invalid file paths or unsupported value types, by printing appropriate error messages and exiting gracefully. Example: We have an input plist file `example.plist` with the following content: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>name</key> <string>Example</string> <key>version</key> <integer>1</integer> <key>features</key> <array> <string>Feature1</string> <string>Feature2</string> </array> </dict> </plist> ``` Calling the function as below: ```python process_plist(\\"example.plist\\", \\"modified_example.plist\\", \\"version\\", 2) ``` Should result in `modified_example.plist` containing: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>name</key> <string>Example</string> <key>version</key> <integer>2</integer> <key>features</key> <array> <string>Feature1</string> <string>Feature2</string> </array> </dict> </plist> ``` Notes: - Make sure you handle both XML and binary plist formats. - Provide meaningful comments and error handling in your implementation.","solution":"import plistlib import os def process_plist(input_filename: str, output_filename: str, key_to_update: str, new_value): Reads a plist file, updates a specified key with a new value, and writes the modified content to another file. Parameters: input_filename (str): The path to the input plist file. output_filename (str): The path to the output plist file. key_to_update (str): The key in the plist file to update. new_value: The new value to set for the specified key. Can be of type str, int, float, bool, list, dict, bytes, bytearray, or datetime. Returns: None try: # Ensure input file exists if not os.path.isfile(input_filename): print(f\\"Error: The file {input_filename} does not exist.\\") return # Detect file format with open(input_filename, \'rb\') as f: plist_format = plistlib.FMT_XML if b\'<?xml\' in f.read() else plistlib.FMT_BINARY # Read the plist file with open(input_filename, \'rb\') as f: plist_data = plistlib.load(f) # Update the specified key or add it if not present plist_data[key_to_update] = new_value # Write the modified content to the output plist file with open(output_filename, \'wb\') as f: plistlib.dump(plist_data, f, fmt=plist_format) print(f\\"Successfully updated {key_to_update} and saved to {output_filename}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Coding Assessment Question # Understanding Python Boolean Objects In this task, you will implement a function that demonstrates the creation and validation of boolean objects in Python. The functions should exhibit the proper creation, returning, and checking of Boolean objects. # Function 1: `create_boolean_from_long` This function will take a long integer and return a Python boolean object based on its truth value. Input - `value` (int): A long integer whose truth value is to be checked. Output - Returns a boolean: - `True` if `value` is non-zero. - `False` if `value` is zero. # Function 2: `check_if_boolean` This function will check if a given object is of boolean type. Input - `obj`: An object that needs to be checked. Output - Returns a boolean: - `True` if the object is of boolean type. - `False` otherwise. # Constraints - Do not use native Python boolean handling. Instead, directly interact with `Py_True`, `Py_False`, and their associated functions/macros. - Make sure the functions handle memory and reference counting properly, just as the macros/functions suggest. # Example ```python value1 = 0 value2 = 1 value3 = -1000 print(create_boolean_from_long(value1)) # Output: False print(create_boolean_from_long(value2)) # Output: True print(create_boolean_from_long(value3)) # Output: True print(check_if_boolean(True)) # Output: True print(check_if_boolean(False)) # Output: True print(check_if_boolean(1)) # Output: False print(check_if_boolean(\\"test\\"))# Output: False ``` Implement the functions `create_boolean_from_long` and `check_if_boolean` that adhere to the above specifications.","solution":"def create_boolean_from_long(value): Returns a boolean object based on the truth value of the input integer. Parameters: value (int): The long integer to be checked. Returns: bool: True if value is non-zero, False if value is zero. return value != 0 def check_if_boolean(obj): Checks if the given object is of boolean type. Parameters: obj: The object to be checked. Returns: bool: True if obj is of boolean type, False otherwise. return isinstance(obj, bool)"},{"question":"Objective Implement a neural network training loop using PyTorch\'s Automatic Mixed Precision (AMP) functionalities to improve performance and stability. Problem Statement You are provided with a simple convolutional neural network (CNN) and a dummy dataset. Your task is to write a training loop for this network, leveraging PyTorch\'s AMP utilities such as `torch.autocast` and `torch.GradScaler`. Instructions 1. Implement the training loop for the `SimpleCNN` model provided below. 2. Use `torch.autocast` to enable mixed precision during the forward pass. 3. Properly scale gradients using `torch.GradScaler` to prevent gradient underflow issues. 4. Ensure the network is trained for 5 epochs, and the loss is printed at the end of each epoch. Constraints - The training loop must use CUDA if available, otherwise, use the CPU. - Batch size for the dataloader should be 32. - Use the Adam optimizer. - Use cross-entropy loss for classification. Performance Requirements - The implementation should properly handle the mixed precision training without raising errors. - The loss should decrease as training progresses. # Provided Code ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset # Define a simple CNN class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(64*28*28, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = torch.flatten(x, 1) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Generate a dummy dataset def get_dataloader(batch_size=32): x = torch.randn(1000, 1, 28, 28) y = torch.randint(0, 10, (1000,)) dataset = TensorDataset(x, y) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) return dataloader # Implement the training loop def train_model(): # Determine the device device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") # Load the dataset dataloader = get_dataloader() # Instantiate the model, loss function, optimizer, and scaler model = SimpleCNN().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters()) scaler = torch.cuda.amp.GradScaler() # Training loop for epoch in range(5): model.train() running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() # Forward pass with autocasting with torch.autocast(device_type=device.type, dtype=torch.float16): outputs = model(inputs) loss = criterion(outputs, labels) # Backward pass with gradient scaling scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() running_loss += loss.item() * inputs.size(0) epoch_loss = running_loss / len(dataloader.dataset) print(f\'Epoch {epoch+1}, Loss: {epoch_loss:.4f}\') print(\\"Training complete.\\") if __name__ == \\"__main__\\": train_model() ```","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset # Define a simple CNN class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(64*28*28, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = torch.flatten(x, 1) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Generate a dummy dataset def get_dataloader(batch_size=32): x = torch.randn(1000, 1, 28, 28) y = torch.randint(0, 10, (1000,)) dataset = TensorDataset(x, y) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) return dataloader # Implement the training loop def train_model(): # Determine the device device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") # Load the dataset dataloader = get_dataloader() # Instantiate the model, loss function, optimizer, and scaler model = SimpleCNN().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters()) scaler = torch.cuda.amp.GradScaler() # Training loop for epoch in range(5): model.train() running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() # Forward pass with autocasting with torch.autocast(device_type=\'cuda\' if device.type == \'cuda\' else \'cpu\', dtype=torch.float16): outputs = model(inputs) loss = criterion(outputs, labels) # Backward pass with gradient scaling scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() running_loss += loss.item() * inputs.size(0) epoch_loss = running_loss / len(dataloader.dataset) print(f\'Epoch {epoch+1}, Loss: {epoch_loss:.4f}\') print(\\"Training complete.\\") if __name__ == \\"__main__\\": train_model()"},{"question":"**Question**: You are required to implement functionality in Python that demonstrates knowledge and control over instance method objects and method objects as described in the provided documentation. Write a Python class that allows dynamic binding and unbinding of methods to its instances at runtime. Your class should support the following requirements: 1. **Class Definition**: - Define a class named `DynamicMethodBinder`. 2. **Methods**: - `bind_method`: This method takes two arguments: - `method_name` (string): The name under which the method should be bound in the instance. - `method` (callable): The function to be bound as an instance method. - `unbind_method`: This method takes one argument: - `method_name` (string): The name of the method to be unbound from the instance. - `is_method_bound`: This method takes one argument: - `method_name` (string): The name of the method to check. - It returns True if the method is currently bound to the instance, else False. **Input Format**: - There is no direct input format as we are defining the class and methods. **Output Format**: - The methods do not return anything unless specified (`is_method_bound` returns a boolean). **Constraints**: - The methods will only be called with valid arguments. - Ensure proper handling when trying to unbind non-existent or already unbound methods. **Example**: ```python class DynamicMethodBinder: def __init__(self): pass def bind_method(self, method_name, method): pass def unbind_method(self, method_name): pass def is_method_bound(self, method_name): pass # Example usage: def greet(self): print(f\\"Hello from {self}!\\") instance = DynamicMethodBinder() instance.bind_method(\'greet\', greet) # Check if method is bound print(instance.is_method_bound(\'greet\')) # Expected Output: True # Call the bound method instance.greet() # Expected Output: \\"Hello from <instance>!\\" # Unbind the method instance.unbind_method(\'greet\') # Check if method is bound print(instance.is_method_bound(\'greet\')) # Expected Output: False ``` Your task is to implement the `DynamicMethodBinder` class with the specified methods according to the guidelines provided.","solution":"class DynamicMethodBinder: def __init__(self): self._methods = {} def bind_method(self, method_name, method): if not callable(method): raise ValueError(\\"Provided method is not callable\\") self._methods[method_name] = method.__get__(self, self.__class__) setattr(self, method_name, self._methods[method_name]) def unbind_method(self, method_name): if method_name in self._methods: del self._methods[method_name] delattr(self, method_name) def is_method_bound(self, method_name): return method_name in self._methods"},{"question":"# Problem Description: You are required to implement a function that performs the element-wise addition of two sparse tensors and returns the result also as a sparse tensor. The sparse tensors are given in the COO (Coordinate) format. # Function Signature: ```python def add_sparse_tensors(coo1: torch.Tensor, coo2: torch.Tensor) -> torch.Tensor: ``` # Input: - `coo1`: a sparse tensor in COO format. - `coo2`: a sparse tensor in COO format. # Output: - `output`: a sparse tensor in COO format representing the element-wise addition of `coo1` and `coo2`. # Constraints: - Both sparse tensors will have the same shape. - The dimensions of the tensors will not exceed 1000x1000. - The tensors may contain at most 50% non-zero elements. # Example: ```python import torch # Example sparse tensors in COO format indices1 = torch.tensor([[0, 1, 1], [2, 0, 2]], dtype=torch.int64) values1 = torch.tensor([3.0, 4.0, 5.0]) shape1 = (2, 3) coo1 = torch.sparse_coo_tensor(indices1, values1, shape1) indices2 = torch.tensor([[1, 0, 0], [2, 0, 1]], dtype=torch.int64) values2 = torch.tensor([7.0, 1.0, 6.0]) shape2 = (2, 3) coo2 = torch.sparse_coo_tensor(indices2, values2, shape2) # Function call result = add_sparse_tensors(coo1, coo2) print(result) ``` # Explanation: In this example, the resulting sparse tensor will have combined indices and values from `coo1` and `coo2`, summing up values where the coordinates overlap. # Note: - Ensure that you correctly manage indexing and summing of elements. - The function should be optimized for handling sparse tensors efficiently without converting them to dense tensors. # Requirements: Implement the function `add_sparse_tensors()` to meet the specifications above. Use the PyTorch library functionalities for handling sparse tensors effectively.","solution":"import torch def add_sparse_tensors(coo1: torch.Tensor, coo2: torch.Tensor) -> torch.Tensor: Adds two sparse tensors in COO format and returns the result as a sparse tensor. assert coo1.is_sparse assert coo2.is_sparse coo1 = coo1.coalesce() coo2 = coo2.coalesce() indices1 = coo1.indices() values1 = coo1.values() indices2 = coo2.indices() values2 = coo2.values() indices = torch.cat([indices1, indices2], dim=1) values = torch.cat([values1, values2]) result = torch.sparse_coo_tensor(indices, values, coo1.shape) result = result.coalesce() return result"},{"question":"**Objective:** Test the student’s ability to use seaborn for data visualization, including loading datasets, creating complex plots, and applying themes and styles. **Problem Statement:** You are provided with the \'tips\' dataset, a well-known dataset in seaborn that contains information about tips received by waitstaff in a restaurant. Your task is to visualize this dataset by creating a customized plot that includes different types of plots, facets, and specific themes and styles. **Requirements:** 1. Load the \'tips\' dataset using seaborn. 2. Create a faceted scatter plot of total bill vs. tip for each day of the week. 3. Fit a linear regression line to the scatter plot in each facet. 4. Apply the following customizations: - Use the \'darkgrid\' style from seaborn. - Set the linewidth of the regression line to 2. - Set the face color of the plots to \'aliceblue\'. - Ensure that the plot theme uses whitegrid with a context suitable for presentation. **Input:** - None **Output:** - Display a customized faceted scatter plot with the specified requirements. **Hints:** - You can use `so.Plot` and `.facet` for creating faceted plots. - Use `so.Line` and `so.PolyFit` for linear regression fit. - Apply the required styles and themes using methods like `theme()`, `axes_style()`, and `plotting_context()`. ```python # Your implementation here import seaborn.objects as so from seaborn import load_dataset, axes_style, plotting_context import matplotlib.pyplot as plt # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the faceted scatter plot of total bill vs. tip for each day of the week p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"day\\") .facet(\\"day\\", wrap=4) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Apply the required customizations p.theme( axes_style(\\"darkgrid\\") | plotting_context(\\"talk\\") ) p.theme({\\"axes.facecolor\\": \\"aliceblue\\", \\"lines.linewidth\\": 2}) # Display the plot p.show() ``` Ensure that your solution contains the necessary comments explaining each step. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_faceted_plot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Set the seaborn style and plotting context for presentation sns.set(style=\\"whitegrid\\", context=\\"talk\\") # Create a FacetGrid for the scatter plot with total_bill vs tip for each day of the week g = sns.FacetGrid(tips, col=\\"day\\", col_wrap=2, height=4, aspect=1.5, palette=\\"viridis\\") # Map a scatterplot with a linear regression line on each facet g.map_dataframe(sns.regplot, x=\\"total_bill\\", y=\\"tip\\", scatter_kws={\\"s\\": 50}, line_kws={\\"linewidth\\": 2}) # Set the face color of the plots to \'aliceblue\' for ax in g.axes.flat: ax.set_facecolor(\'aliceblue\') # Display the plot plt.show() # Call the function to display the plot customized_faceted_plot()"},{"question":"# Question: Implement a Linux Terminal Echo Manager You are required to implement a function that toggles the echo state of the terminal. The function should be able to enable or disable terminal echo based on the provided argument. This function will help understand the use of the `termios` module to manage terminal attributes. Function Signature ```python def toggle_terminal_echo(enable_echo: bool) -> None: Toggles the terminal echo state. When `enable_echo` is True, the echo is enabled. When `enable_echo` is False, the echo is disabled. Args: enable_echo (bool): A boolean flag indicating whether to enable or disable the terminal echo. Raises: Exception: If an error occurs while trying to change terminal attributes. ``` Input - `enable_echo` (bool): A boolean value where `True` means enabling terminal echo and `False` means disabling it. Output - None Constraints - The function should raise an exception if unable to change the terminal attributes. - The function should ensure that the terminal attributes are restored to their original state in case an unexpected error occurs. Example Usage ```python # Disables terminal echo toggle_terminal_echo(False) # Enables terminal echo toggle_terminal_echo(True) ``` Performance Requirements - The function should execute efficiently on Unix-based systems. - Handle potential errors gracefully and ensure terminal settings are consistent regardless of execution flow. Hints - Use `termios.tcgetattr` to get the current terminal attributes. - Modify the `lflag` field to toggle the echo state (`termios.ECHO`). - Use `termios.tcsetattr` to set the modified attributes. - Ensure changes are restored in case of errors using a `try`...`finally` statement.","solution":"import termios import sys def toggle_terminal_echo(enable_echo: bool) -> None: Toggles the terminal echo state. When `enable_echo` is True, the echo is enabled. When `enable_echo` is False, the echo is disabled. Args: enable_echo (bool): A boolean flag indicating whether to enable or disable the terminal echo. Raises: Exception: If an error occurs while trying to change terminal attributes. fd = sys.stdin.fileno() try: # Get the current terminal attributes original_attributes = termios.tcgetattr(fd) new_attributes = termios.tcgetattr(fd) if enable_echo: new_attributes[3] |= termios.ECHO # Enable echo else: new_attributes[3] &= ~termios.ECHO # Disable echo # Set the terminal attributes termios.tcsetattr(fd, termios.TCSADRAIN, new_attributes) except Exception as e: raise Exception(f\\"Failed to toggle terminal echo: {e}\\") finally: # Restore the original attributes termios.tcsetattr(fd, termios.TCSADRAIN, original_attributes)"},{"question":"# Advanced Coding Assessment Question **Objective:** Demonstrate your understanding of Python\'s `json` module by implementing a customized JSON encoder and decoder for a specific use case. **Problem Statement:** You are required to serialize and deserialize complex numbers and datetime objects to and from JSON. Your task is to extend the `json.JSONEncoder` and `json.JSONDecoder` classes and implement custom encoding and decoding for the following types: 1. **Complex Numbers**: Serialize complex numbers as JSON objects with `\\"real\\"` and `\\"imag\\"` fields. 2. **Datetime Objects**: Serialize datetime objects in ISO 8601 string format. Additionally, implement a function `custom_json_dumps` that uses your custom encoder, and a function `custom_json_loads` that uses your custom decoder. **Detailed Requirements:** 1. **Custom Encoder**: - Subclass `json.JSONEncoder`. - Override the `default` method to handle `complex` and `datetime` objects. - For complex numbers, serialize as a JSON object: `{\\"real\\": <real_part>, \\"imag\\": <imaginary_part>}`. - For datetime objects, serialize as an ISO 8601 string. 2. **Custom Decoder**: - Subclass `json.JSONDecoder`. - Use the `object_hook` parameter to handle deserialization. - Convert JSON objects with fields `{\\"real\\": <real_part>, \\"imag\\": <imaginary_part>}` back into `complex` numbers. - Convert ISO 8601 strings back into `datetime` objects. 3. **Functions**: - `custom_json_dumps(obj: Any) -> str`: Uses the custom encoder to serialize the object. - `custom_json_loads(json_str: str) -> Any`: Uses the custom decoder to deserialize the JSON string. **Example Usage:** ```python from datetime import datetime import json # Example data data = { \\"name\\": \\"Example\\", \\"complex_number\\": complex(2, 3), \\"timestamp\\": datetime(2023, 5, 17, 12, 30, 45) } # Serialize the data json_str = custom_json_dumps(data) print(\\"Serialized JSON:\\", json_str) # Deserialize the JSON string loaded_data = custom_json_loads(json_str) print(\\"Deserialized Data:\\", loaded_data) ``` **Expected Output:** The printed JSON should have `complex_number` as an object with `real` and `imag` fields, and `timestamp` as an ISO 8601 string. The deserialized data should correctly restore the `complex` and `datetime` types. **Constraints and Considerations:** - Ensure that all other JSON serialization and deserialization features are retained as per the standard `json` module\'s behavior. - Handle any unexpected data gracefully, providing useful error messages where applicable. - Write clean and well-documented code. **Notes:** - You may use other Python standard libraries if needed, but avoid using third-party libraries. - Include any necessary import statements in your solution. --- **Your implementation should demonstrate a thorough understanding of Python\'s `json` module and its extensibility options. Good luck!**","solution":"import json from datetime import datetime class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag, \\"__type__\\": \\"complex\\"} elif isinstance(obj, datetime): return {\\"__type__\\": \\"datetime\\", \\"value\\": obj.isoformat()} return super().default(obj) class CustomJSONDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, obj): if \\"__type__\\" in obj: type_ = obj[\\"__type__\\"] if type_ == \\"complex\\": return complex(obj[\\"real\\"], obj[\\"imag\\"]) elif type_ == \\"datetime\\": return datetime.fromisoformat(obj[\\"value\\"]) return obj def custom_json_dumps(obj): return json.dumps(obj, cls=CustomJSONEncoder) def custom_json_loads(json_str): return json.loads(json_str, cls=CustomJSONDecoder)"},{"question":"# PyTorch Backend Configuration You are given the task to optimize a PyTorch model\'s performance by configuring various backend settings. Your goal is to enable specific optimizations based on the available hardware and ensure that certain features are correctly set up. Task: 1. Write a function `configure_pytorch_backends()` that does the following: - Checks if CUDA is available and enables TensorFloat-32 tensor cores for matrix multiplications if on an Ampere or newer GPU. - Ensures that cuDNN is enabled and sets it to use deterministic algorithms. - Enables cuFFT plan cache and sets its maximum size to 100 for CUDA devices, if available. - Configures the MPS backend if built and available. 2. Write another function `print_backend_config()` that: - Prints whether CUDA, cuDNN, and MPS backends are available. - Prints the current settings for TensorFloat-32, deterministic algorithms, and cuFFT plan cache size. Input: - No specific input is required for the functions, but they should work on systems with or without CUDA, cuDNN, and MPS support. Output: - `configure_pytorch_backends()` should not return any value. - `print_backend_config()` should print the required backend configuration details. Constraints: - Ensure that the code runs without errors even if some backends are not available. - Use the `torch.backends` module to configure the settings. Example: ```python configure_pytorch_backends() print_backend_config() ``` Expected Output: ``` CUDA available: True TensorFloat-32 enabled: True cuDNN available: True cuDNN deterministic: True cuFFT plan cache size: 100 MPS available: False ``` Note that the exact output may vary depending on the hardware and backends available on the system where the code is executed.","solution":"import torch def configure_pytorch_backends(): Configures various PyTorch backend settings to optimize performance. if torch.cuda.is_available(): if torch.cuda.get_device_capability()[0] >= 8: # Ampere or newer torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cudnn.enabled = True torch.backends.cudnn.deterministic = True torch.backends.cuda.cufft_plan_cache_enabled = True torch.backends.cuda.cufft_plan_cache_size = 100 if hasattr(torch.backends, \\"mps\\"): if torch.backends.mps.is_available() and torch.backends.mps.is_built(): torch.backends.mps.enabled = True def print_backend_config(): Prints the current PyTorch backend configuration details. cuda_available = torch.cuda.is_available() print(f\\"CUDA available: {cuda_available}\\") if cuda_available: print(f\\"TensorFloat-32 enabled: {torch.backends.cuda.matmul.allow_tf32}\\") cudnn_available = torch.backends.cudnn.enabled if hasattr(torch.backends, \'cudnn\') else False print(f\\"cuDNN available: {cudnn_available}\\") if cudnn_available: print(f\\"cuDNN deterministic: {torch.backends.cudnn.deterministic}\\") print(f\\"cuFFT plan cache size: {torch.backends.cuda.cufft_plan_cache_size}\\") mps_available = torch.backends.mps.is_available() if hasattr(torch.backends, \\"mps\\") else False print(f\\"MPS available: {mps_available}\\")"},{"question":"# Seaborn Clustermap Challenge **Objective:** Demonstrate your comprehension of the `seaborn.clustermap` function by creating a comprehensive visualization that includes hierarchical clustering, data standardization, and customization of the plot\'s appearance. **Task:** Write a function `custom_clustermap` that: 1. Loads the `iris` dataset included in seaborn. 2. Performs hierarchical clustering on the dataset. 3. Customizes the appearance and behavior of the clustermap as specified below. **Function Signature:** ```python def custom_clustermap(): pass ``` **Specifications:** 1. **Load Data:** - Load the `iris` dataset using `seaborn.load_dataset(\\"iris\\")`. - Separate the \'species\' column from the dataset. 2. **Clustermap Creation:** - Generate a clustermap of the iris data (excluding the species column). 3. **Appearance Customization:** - Set the size of the figure to `(10, 8)`. - Disable row clustering. - Add colored labels to the rows based on the species. Use the following color mapping: `setosa -> red`, `versicolor -> green`, `virginica -> blue`. - Use the `\\"mako\\"` colormap. - Standardize the data within columns. 4. **Output:** - The function should display the customized clustermap directly. There is no need to return any values. **Example Execution:** ```python custom_clustermap() ``` This function call should load the iris dataset, preprocess the data (excluding the species column), and generate a hierarchical clustermap with the aforementioned customizations. **Constraints:** - Use the seaborn library for creating the clustermap. - Ensure the final visualization aligns with the described customizations. By completing this task, you will demonstrate your ability to manipulate datasets and leverage seaborn’s advanced visualization capabilities to create insightful and aesthetically pleasing plots.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt def custom_clustermap(): # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Separate the species column species = iris.pop(\'species\') # Map species to colors species_colors = species.map({\\"setosa\\": \\"red\\", \\"versicolor\\": \\"green\\", \\"virginica\\": \\"blue\\"}) # Generate the clustermap sns.clustermap(iris, figsize=(10, 8), row_cluster=False, # Disable row clustering row_colors=species_colors, # Add colored labels to rows based on species cmap=\\"mako\\", # Use \\"mako\\" colormap standard_scale=1) # Standardize data within columns # Display the plot plt.show()"},{"question":"# Python Logging Advanced Assessment **Objective**: Create and configure a logging system using Python\'s `logging` module that meets specific requirements for logging messages at different levels, using custom handlers and formatters, and capturing exceptions. **Problem Statement**: You are required to create a logging system for a hypothetical application. The application has the following requirements for its logging system: 1. **Logger Hierarchy**: - Create a root logger named `app` which will be configured to log messages of `DEBUG` level and above. - Create two child loggers named `app.moduleA` and `app.moduleB`. - Ensure that messages logged in `app.moduleA` and `app.moduleB` propagate to the root logger. 2. **Handlers**: - Configure the root logger to log messages to a file named `app.log` with messages formatted to include the timestamp, logger name, log level, and the message. - Additionally, configure `app.moduleA` to log `ERROR` messages to a separate file named `moduleA_errors.log`. 3. **Formatters**: - The log format for `app.log` should be: `\\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\"`. - The log format for `moduleA_errors.log` should be: `\\"%(asctime)s - %(levelname)s - %(message)s\\"`. 4. **Exception Handling**: - Create a function `log_exception()` that logs an exception with its stack trace using the `app` logger. 5. **Custom Filter**: - Implement a custom filter that allows only messages containing the word \\"IMPORTANT\\" to be logged by `app.moduleB`. **Input and Output Specification**: - Function `setup_logging`: - **Input**: None. - **Output**: Configures the logging system as described above. - Function `log_exception`: - **Input**: An exception instance. - **Output**: Logs the exception to the `app` logger. **Tasks**: 1. Implement the `setup_logging` function: - Set up the loggers (`app`, `app.moduleA`, `app.moduleB`). - Configure the handlers and formatters in accordance with the requirements. - Implement the custom filter for `app.moduleB`. 2. Implement the `log_exception` function: - Log an exception using the `app` logger, including its stack trace. **Example Usage**: ```python setup_logging() import logging app_logger = logging.getLogger(\'app\') moduleA_logger = logging.getLogger(\'app.moduleA\') moduleB_logger = logging.getLogger(\'app.moduleB\') # Log messages app_logger.debug(\\"This is a debug message\\") app_logger.info(\\"This is an info message\\") moduleA_logger.error(\\"This is an error in module A\\") moduleB_logger.info(\\"This is an IMO info message\\") # Should not get logged moduleB_logger.info(\\"This is an IMPORTANT message\\") # Should get logged # Log an exception try: raise ValueError(\\"An error occurred\\") except Exception as e: log_exception(e) ``` **Constraints**: - You can only use the standard Python `logging` module. - Ensure that the handlers and formatters are set up correctly to meet the specifications. Implement the functions below: ```python import logging def setup_logging(): # Your implementation here pass def log_exception(exception): # Your implementation here pass ```","solution":"import logging class ImportantFilter(logging.Filter): def filter(self, record): return \'IMPORTANT\' in record.msg def setup_logging(): # Create root logger named \'app\' app_logger = logging.getLogger(\'app\') app_logger.setLevel(logging.DEBUG) # Create handlers file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) moduleA_error_handler = logging.FileHandler(\'moduleA_errors.log\') moduleA_error_handler.setLevel(logging.ERROR) # Create formatters and add them to handlers file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_handler.setFormatter(file_formatter) error_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') moduleA_error_handler.setFormatter(error_formatter) # Add handlers to the root logger app_logger.addHandler(file_handler) # Specific setup for app.moduleA logger moduleA_logger = logging.getLogger(\'app.moduleA\') moduleA_logger.addHandler(moduleA_error_handler) # Specific setup for app.moduleB logger with filter moduleB_logger = logging.getLogger(\'app.moduleB\') important_filter = ImportantFilter() moduleB_logger.addFilter(important_filter) def log_exception(exception): app_logger = logging.getLogger(\'app\') app_logger.exception(\\"Exception occurred\\", exc_info=exception)"},{"question":"Using the seaborn package, you are required to create a set of visualizations to analyze the \\"Tips\\" dataset, which contains information about the tips received by waiters in a restaurant. The dataset includes variables such as total bill amount, tip amount, sex of the waiter, whether the smoker is present, day of the week, time of the day, and size of the party. **Requirements:** 1. **Load the Dataset:** - Load the \\"tips\\" dataset using `seaborn.load_dataset(\\"tips\\")`. 2. **Bar Plot:** - Create a bar plot showing the average total bill amount per day of the week. Use different colors to represent whether the waiter was male or female. - Ensure that the bars are stacked to show the part-whole relationship. 3. **Faceted Histogram:** - Create a faceted histogram displaying the distribution of tip amounts. Use one facet per day of the week, and within each facet, use different color shades to represent the presence of a smoker. - Adjust the bin width to 1 unit. **Input Format:** - There is no specific input format as the dataset is loaded directly within the script. **Output Format:** - The output should be graphical visualizations as specified above. **Constraints:** - Ensure that your plots are properly labeled for clarity. - Use seaborn and its objects-oriented interface for these visualizations. **Performance Requirements:** - The code should be efficient and should execute within a reasonable time frame for analysis purposes. ```python import seaborn.objects as so from seaborn import load_dataset # Load the \\"tips\\" dataset tips = load_dataset(\\"tips\\") # 1. Bar Plot: Average total bill amount per day of the week bar_plot = ( so.Plot(tips, x=\\"day\\", color=\\"sex\\") .add(so.Bar(), so.Agg(func=\\"mean\\", y=\\"total_bill\\"), so.Stack()) ) # Display the bar plot bar_plot.show() # 2. Faceted Histogram: Distribution of tip amounts facet_hist = ( so.Plot(tips, x=\\"tip\\", alpha=\\"smoker\\") .facet(\\"day\\") .add(so.Bars(), so.Hist(binwidth=1), so.Stack()) ) # Display the faceted histogram facet_hist.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") def create_bar_plot(data): Creates a bar plot showing the average total bill amount per day of the week. Uses different colors to represent whether the waiter was male or female. plt.figure(figsize=(10, 6)) sns.barplot( data=data, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", ci=None, estimator=sum ) plt.title(\\"Average Total Bill Amount per Day by Gender\\") plt.ylabel(\\"Total Bill Amount\\") plt.xlabel(\\"Day of the Week\\") plt.show() def create_faceted_histogram(data): Creates a faceted histogram displaying the distribution of tip amounts. Uses one facet per day of the week and different color shades to represent the presence of a smoker. g = sns.FacetGrid(data, col=\\"day\\", hue=\\"smoker\\", height=4, aspect=0.9) g.map(sns.histplot, \\"tip\\", bins=int((data[\'tip\'].max() - data[\'tip\'].min())/1), kde=False, alpha=0.6) g.add_legend() g.set_axis_labels(\\"Tip Amount\\", \\"Frequency\\") plt.subplots_adjust(top=0.9) g.fig.suptitle(\\"Distribution of Tip Amounts by Day and Smoker Status\\") plt.show() # Create the visualizations create_bar_plot(tips) create_faceted_histogram(tips)"},{"question":"# Question: Type-Safe Data Processing Pipeline You are tasked with implementing a type-safe data processing pipeline using Python\'s `typing` module. Your goal is to create a processing function that utilizes type hints and generics to ensure type safety. Problem Statement You need to create a function `process_data_pipeline` that accepts a list of generic data items and a list of processing functions. Each processing function transforms the type of the data in some way. Your function should apply each processing function in sequence to all the data items. The final result should be a list of data items processed through all the functions. Function Signature ```python from typing import TypeVar, Callable, List T = TypeVar(\'T\') U = TypeVar(\'U\') def process_data_pipeline(data: List[T], processors: List[Callable[[T], U]]) -> List[U]: pass ``` Input - `data`: A list of data items of type `T`. - `processors`: A list of processing functions, each taking an item of type `T` and returning an item of type `U`. Output - Returns a list of data items of type `U` after applying all the processors in sequence. Constraints 1. Each processing function is guaranteed to transform the type of data in a consistent manner. 2. The `processors` list contains at least one function. 3. The `data` list can be empty. Example ```python # Example usage: # Define processors def to_string(n: int) -> str: return str(n) def append_exclamation(s: str) -> str: return s + \\"!\\" def to_length(s: str) -> int: return len(s) # Data and processors data = [1, 2, 3] processors = [to_string, append_exclamation, to_length] # Function call result = process_data_pipeline(data, processors) print(result) # Output: [2, 2, 2] ``` Explanation 1. Initially, data `[1, 2, 3]` are integers. 2. `to_string` converts each integer to a string: `[\\"1\\", \\"2\\", \\"3\\"]`. 3. `append_exclamation` appends \\"!\\" to each string: `[\\"1!\\", \\"2!\\", \\"3!\\"]`. 4. `to_length` converts each string to its length: `[2, 2, 2]`. Implement the `process_data_pipeline` function to accomplish the task described.","solution":"from typing import TypeVar, Callable, List T = TypeVar(\'T\') U = TypeVar(\'U\') V = TypeVar(\'V\') def process_data_pipeline(data: List[T], processors: List[Callable[[T], U]]) -> List[U]: for processor in processors: data = list(map(processor, data)) return data"},{"question":"Objective: To assess the understanding of creating executable Python zip archives using the `zipapp` module in Python. The task involves creating a standalone Python application that can be bundled into a zip archive and executed directly. Problem Statement: You are given a directory containing a simple Python application. Your task is to create a standalone executable zip archive of this application using the `zipapp` module\'s Python API. The directory structure and expected behavior are described below. Directory Structure: ``` my_simple_app/ __main__.py app.py utils.py ``` - The `__main__.py` is the entry point of the application and should call a function `main()` in the `app.py` module. - The `app.py` module contains: ```python def main(): print(\\"Hello from my_simple_app\\") ``` - The `utils.py` module can have additional helper functions which are not called in the `__main__.py` for simplicity. Requirements: 1. Create a Python function `create_executable_zip` that accepts the directory name and optionally other parameters to produce a zip archive. The function should have the following signature: ```python def create_executable_zip(source: str, target: str = None, interpreter: str = None, compressed: bool = False) -> bool: Create an executable zip archive from the source directory. :param source: The source directory containing the Python application. :param target: The target zip archive file path. If None, defaults to source + \'.pyz\'. :param interpreter: The interpreter to be used in the shebang line. If None, no shebang line is added. :param compressed: Whether to compress the files in the archive. Default is False. :return: True if the archive was created successfully, False otherwise. ``` 2. Your function should use the `zipapp` module\'s `create_archive` method to create the archive. 3. Ensure that the archive is created correctly by running it to display \\"Hello from my_simple_app\\". Constraints: - You must use the Python API rather than the command line interface. - Assume all necessary modules are in place and that no external dependencies are involved other than the standard library. Example: ```python import zipapp def create_executable_zip(source: str, target: str = None, interpreter: str = None, compressed: bool = False) -> bool: try: zipapp.create_archive(source, target, interpreter, \\"app:main\\", compressed=compressed) return True except Exception as e: print(f\\"An error occurred: {e}\\") return False # Usage example: # Assuming the source directory is \'my_simple_app\' success = create_executable_zip(\'my_simple_app\', \'my_simple_app.pyz\', interpreter=\'/usr/bin/env python3\', compressed=True) if success: print(\\"The executable zip archive was created successfully.\\") else: print(\\"Failed to create the executable zip archive.\\") ``` Testing: 1. Create the `my_simple_app` directory with the given structure and `main()` function in your local setup. 2. Execute the `create_executable_zip` function to create the archive. 3. Run the resulting `my_simple_app.pyz` file using Python to ensure it executes correctly. Output: The expected output after running the archive should be: ``` Hello from my_simple_app ```","solution":"import zipapp import os def create_executable_zip(source: str, target: str = None, interpreter: str = None, compressed: bool = False) -> bool: Create an executable zip archive from the source directory. :param source: The source directory containing the Python application. :param target: The target zip archive file path. If None, defaults to source + \'.pyz\'. :param interpreter: The interpreter to be used in the shebang line. If None, no shebang line is added. :param compressed: Whether to compress the files in the archive. Default is False. :return: True if the archive was created successfully, False otherwise. try: if not target: target = f\\"{source}.pyz\\" zipapp.create_archive(source, target, interpreter, compressed=compressed) return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Python 3.10 Coding Assessment **Objective:** Implement a function to parse and evaluate a specific pattern-matching expression along with comprehensions. # Background: Pattern matching in Python 3.10 introduces a `match` statement that examines values and applies actions based on a pattern. This new feature is versatile and can be used to simplify many tasks, including parsing and evaluating data structures. Comprehension constructs like list comprehensions are powerful tools that allow writing concise and readable loops. # Problem: You need to implement a function that parses and evaluates a nested data structure using pattern matching and list comprehension. # Task: 1. Implement a function `evaluate_patterns` that accepts a nested list of tuples. Each tuple contains a command and its arguments. The command can be one of the following: - `\'add\'`: Add all numbers in the arguments. - `\'multiply\'`: Multiply all numbers in the arguments. - `\'concatenate\'`: Concatenate all strings in the arguments. 2. The function should return a list containing the results of these operations. # Input: - A nested list where each sublist is a tuple of the form `(command, *args)`, where `command` is a string and `args` vary based on the command. # Output: - A list of results corresponding to each command in the input. # Examples: ```python def evaluate_patterns(commands): # Your code here # Example calls: print(evaluate_patterns([(\\"add\\", 1, 2, 3), (\\"multiply\\", 2, 3, 4), (\\"concatenate\\", \\"a\\", \\"b\\", \\"c\\")])) # Output: [6, 24, \\"abc\\"] ``` # Constraints: - Use a match-case statement to handle different commands. - Utilize list comprehensions where applicable. - Assume that all inputs are valid and non-empty. - All numbers will be integers and all strings will be simple alphabetic characters.","solution":"def evaluate_patterns(commands): results = [] for command, *args in commands: match command: case \'add\': result = sum(args) case \'multiply\': result = 1 for arg in args: result *= arg case \'concatenate\': result = \'\'.join(args) case _: result = None results.append(result) return results"},{"question":"<|Analysis Begin|> The provided documentation primarily covers various pandas methods for combining, merging, and comparing DataFrames and Series. The methods covered include: 1. **Concatenation** with `concat` - combining Series or DataFrames along a particular axis. 2. **Joining** with `DataFrame.join` - combining DataFrames based on index or columns. 3. **SQL-like merging** with `merge` - combining DataFrames based on keys with SQL-style joins. 4. **Comparing** with `DataFrame.compare` and `Series.compare` - identifying differences between two DataFrames or Series. The documentation includes detailed examples showing how these methods are applied and the potential output results, making it a useful reference for creating a comprehensive question. Since the task focuses on creating a clear and challenging coding assessment to test students\' understanding of pandas, the question will incorporate: 1. A requirement to combine several DataFrames using different join methods. 2. Validation checks on merges to ensure key uniqueness. 3. Operations using both merging and comparison functions provided by pandas. <|Analysis End|> <|Question Begin|> # Pandas Coding Assessment Question **Objective**: Demonstrate proficiency in merging, joining, and comparing DataFrames using pandas. **Scenario**: You are provided with four DataFrames, each containing sales data from different branches of a retail chain. Your task is to merge these DataFrames based on specific keys, perform a series of operations to clean and unify the dataset, and identify discrepancies between expected and actual results. **DataFrames**: 1. `df_sales`: Contains sales data ```python df_sales = pd.DataFrame({ \\"Branch\\": [\\"A\\", \\"A\\", \\"B\\", \\"C\\"], \\"Date\\": [\\"2021-01-01\\", \\"2021-01-02\\", \\"2021-01-01\\", \\"2021-01-03\\"], \\"Sales\\": [1000, 1500, 2000, 300] }) ``` 2. `df_customers`: Contains customer data with referral details ```python df_customers = pd.DataFrame({ \\"Branch\\": [\\"A\\", \\"A\\", \\"B\\", \\"D\\"], \\"Date\\": [\\"2021-01-01\\", \\"2021-01-03\\", \\"2021-01-01\\", \\"2021-01-04\\"], \\"New_Customers\\": [10, 20, 30, 5] }) ``` 3. `df_targets`: Contains sales targets for each branch ```python df_targets = pd.DataFrame({ \\"Branch\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"Sales_Target\\": [1200, 2500, 500, 400] }) ``` 4. `df_forecast`: Contains sales forecasts ```python df_forecast = pd.DataFrame({ \\"Branch\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"Forecasted_Sales\\": [1100, 2400, 450, 350] }) ``` **Tasks**: 1. **Merge** the `df_sales` and `df_customers` DataFrames on the columns `Branch` and `Date` using an inner join. Save the result as `df_combined`. 2. Perform a **left join** of the `df_combined` DataFrame with `df_targets` on the `Branch` column. 3. Add the `df_forecast` DataFrame to the `df_combined` using a **right join** on the `Branch` column. 4. Validate the final merged DataFrame to ensure there are no duplicate keys for the `Branch` column (use the `validate` parameter in the merge function as applicable). 5. Use the `compare` functionality to identify discrepancies between `Sales` in the final DataFrame and the `Forecasted_Sales` from `df_forecast`. **Expected Output**: 1. The merged DataFrame after each of the steps (1), (2), and (3). 2. A DataFrame showing the discrepancies between `Sales` and `Forecasted_Sales`. **Constraints**: - Ensure the DataFrame\'s index is meaningful and unique where applicable. - The merges should handle missing values gracefully. **Implementation Requirements**: 1. Define a function `merge_dataframes` which performs tasks 1 to 3 and returns the resulting DataFrame. 2. Define another function `find_discrepancies` which takes the merged DataFrame as input and returns a DataFrame highlighting discrepancies between `Sales` and `Forecasted_Sales`. ```python import pandas as pd def merge_dataframes(df_sales, df_customers, df_targets, df_forecast): # Task 1: Merge df_sales and df_customers df_combined = pd.merge(df_sales, df_customers, on=[\\"Branch\\", \\"Date\\"], how=\\"inner\\") # Task 2: Left join with df_targets df_combined = pd.merge(df_combined, df_targets, on=\\"Branch\\", how=\\"left\\") # Task 3: Right join with df_forecast df_combined = pd.merge(df_combined, df_forecast, on=\\"Branch\\", how=\\"right\\", validate=\\"one_to_one\\") return df_combined def find_discrepancies(df_combined): # Task 5: Identify discrepancies between Sales and Forecasted_Sales discrepancies = df_combined.compare(df_combined.set_index(\\"Branch\\")[[\\"Forecasted_Sales\\"]], align_axis=1) return discrepancies # Test the functions df_final = merge_dataframes(df_sales, df_customers, df_targets, df_forecast) print(df_final) discrepancies = find_discrepancies(df_final) print(discrepancies) ``` **Notes**: - The `merge` method in pandas can be used efficiently if you understand the merging logic and the unique constraints you need to apply. - Proper handling of missing values ensures that your analysis on a resulting merged DataFrame remains accurate.","solution":"import pandas as pd def merge_dataframes(df_sales, df_customers, df_targets, df_forecast): Merges provided DataFrames by following the specified steps and returns the resulting DataFrame. # Task 1: Merge df_sales and df_customers df_combined = pd.merge(df_sales, df_customers, on=[\\"Branch\\", \\"Date\\"], how=\\"inner\\") # Task 2: Left join with df_targets df_combined = pd.merge(df_combined, df_targets, on=\\"Branch\\", how=\\"left\\") # Task 3: Right join with df_forecast df_combined = pd.merge(df_combined, df_forecast, on=\\"Branch\\", how=\\"right\\", validate=\\"one_to_one\\") return df_combined def find_discrepancies(df_combined): Identifies discrepancies between \'Sales\' and \'Forecasted_Sales\' columns in the merged DataFrame. discrepancies = df_combined[[\'Branch\', \'Sales\', \'Forecasted_Sales\']] discrepancies = discrepancies[discrepancies[\'Sales\'] != discrepancies[\'Forecasted_Sales\']] return discrepancies"},{"question":"**Asynchronous Network Operation and Exception Handling** You are required to implement a function `perform_network_operation()` that simulates a network operation using asynchronous programming in Python. This function should utilize the `asyncio` module and handle various exceptions that can occur during the operation. # Requirements 1. The function `perform_network_operation()` should: - Simulate a network operation using `asyncio`. - Handle the following exceptions: - `asyncio.TimeoutError`: Log a message \\"Operation timed out.\\" - `asyncio.CancelledError`: Log a message \\"Operation cancelled.\\" - `asyncio.InvalidStateError`: Log a message \\"Invalid state encountered.\\" - `asyncio.SendfileNotAvailableError`: Log a message \\"Sendfile not available.\\" - `asyncio.IncompleteReadError`: Log a message with the details of expected and partial bytes. - `asyncio.LimitOverrunError`: Log a message with the number of bytes that caused the limit overrun. - The function should return a string \\"Operation completed successfully.\\" if no exceptions occur. # Input and Output - **Input:** None - **Output:** A string message indicating the status of the operation. # Constraints - You need to use the `asyncio` module to implement asynchronous operations. - You may assume that the network operation involves reading a specified number of bytes within a deadline. # Example Usage ```python async def main(): result = await perform_network_operation() print(result) # Example call to the main function using asyncio.run asyncio.run(main()) ``` # Additional Information - You can use `asyncio.sleep` to simulate delays. - Feel free to create helper functions if necessary. Implement the `perform_network_operation()` function to meet the above requirements.","solution":"import asyncio import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) async def perform_network_operation(): try: await asyncio.sleep(1) # Simulate a network operation # Simulate raising different exceptions by uncommenting one of the following lines: # raise asyncio.TimeoutError(\\"Simulated timeout error\\") # raise asyncio.CancelledError(\\"Simulated cancelled error\\") # raise asyncio.InvalidStateError(\\"Simulated invalid state error\\") # raise asyncio.SendfileNotAvailableError(\\"Simulated sendfile not available error\\") # raise asyncio.IncompleteReadError(partial=b\'partial data\', expected=100) # raise asyncio.LimitOverrunError(\\"Simulated limit overrun error\\", 10) return \\"Operation completed successfully.\\" except asyncio.TimeoutError: logger.info(\\"Operation timed out.\\") except asyncio.CancelledError: logger.info(\\"Operation cancelled.\\") except asyncio.InvalidStateError: logger.info(\\"Invalid state encountered.\\") except asyncio.SendfileNotAvailableError: logger.info(\\"Sendfile not available.\\") except asyncio.IncompleteReadError as e: logger.info(f\\"Incomplete read: expected {e.expected} bytes but got {len(e.partial)}.\\") except asyncio.LimitOverrunError as e: logger.info(f\\"Limit overrun by {e.consumed} bytes.\\") return \\"Operation failed.\\" # Example usage # async def main(): # result = await perform_network_operation() # print(result) # # asyncio.run(main())"},{"question":"# PyTorch Tensor Operations and Manipulations Objective This task is to implement a function that performs several operations on PyTorch tensors. The goal is to familiarize you with tensor creation, basic manipulations, and operations. Problem Statement You are required to write a function `tensor_operations` that takes two 2D Python lists of integers as input and outputs a dictionary with the results of various tensor operations. Function Signature ```python def tensor_operations(list1: [[int]], list2: [[int]]) -> dict: pass ``` Input - `list1`: A 2-dimensional list of integers (not empty). - `list2`: Another 2-dimensional list of integers with the same shape as `list1`. Output A dictionary with the following keys and their results: - `\\"addition\\"`: element-wise addition of the corresponding tensors. - `\\"multiplication\\"`: element-wise multiplication of the corresponding tensors. - `\\"matrix_multiplication\\"`: matrix multiplication of the corresponding tensors. - `\\"sum_axis_0\\"`: sum of tensors along axis 0. - `\\"sum_axis_1\\"`: sum of tensors along axis 1. - `\\"reshape\\"`: first tensor reshaped to a 1-dimensional tensor. - `\\"transpose\\"`: second tensor transposed. Example ```python list1 = [ [1, 2], [3, 4] ] list2 = [ [5, 6], [7, 8] ] result = tensor_operations(list1, list2) print(result) ``` Expected Output: ```python { \\"addition\\": tensor([[ 6, 8], [10, 12]]), \\"multiplication\\": tensor([[ 5, 12], [21, 32]]), \\"matrix_multiplication\\": tensor([[19, 22], [43, 50]]), \\"sum_axis_0\\": tensor([8, 10]), \\"sum_axis_1\\": tensor([ 3, 7, 11, 15]), \\"reshape\\": tensor([1, 2, 3, 4]), \\"transpose\\": tensor([[5, 7], [6, 8]]) } ``` Constraints - You should utilize PyTorch functionalities to perform these operations. - Ensure that the shapes of list1 and list2 are the same for valid operations. Notes - Use `torch.tensor` to convert Python lists to PyTorch tensors. - Use appropriate PyTorch methods and operations to achieve the task. Make sure your function passes the provided example and performs the necessary tensor operations correctly.","solution":"import torch def tensor_operations(list1, list2): tensor1 = torch.tensor(list1) tensor2 = torch.tensor(list2) results = { \\"addition\\": torch.add(tensor1, tensor2), \\"multiplication\\": torch.mul(tensor1, tensor2), \\"matrix_multiplication\\": torch.matmul(tensor1, tensor2), \\"sum_axis_0\\": torch.sum(tensor1, dim=0), \\"sum_axis_1\\": torch.sum(tensor1, dim=1), \\"reshape\\": tensor1.view(-1), \\"transpose\\": tensor2.transpose(0, 1) } return results # Example usage list1 = [ [1, 2], [3, 4] ] list2 = [ [5, 6], [7, 8] ] result = tensor_operations(list1, list2) for k, v in result.items(): print(f\\"{k}: {v}\\")"},{"question":"**Question: Implementing a Custom Tracing Decorator with TorchDynamo** In this coding assessment, you are required to create a custom decorator that makes use of TorchDynamo to trace a given function and generate FX graphs for symbolic tensor operations with dynamic shapes. # Objective Your task is to implement a decorator named `@trace_with_dynamo` that: 1. **Traces the function\'s execution:** Records the sequence of PyTorch operations executed within the decorated function. 2. **Supports dynamic shapes:** Ensures that the function can handle tensors with dynamic shapes and uses symbolic integers where necessary. 3. **Generates and prints the FX graph:** After the function executes, prints the FX graph representing the operations. # Requirements 1. **Input Format:** - The input function will accept one or more tensor arguments and perform operations on them. - Each tensor can have varying shapes during different calls to the function. 2. **Constraints:** - You must use TorchDynamo for tracing. - The decorator should handle functions with dynamic shapes correctly and avoid recompilations whenever possible. 3. **Output Format:** - After the function execution, the generated FX graph should be printed in a human-readable format. # Example Here is an example provided to understand the expected functionality better. Input Function ```python import torch @trace_with_dynamo def sample_function(a, b): c = a + b d = torch.relu(c) return d ``` Execution ```python x = torch.randn(5, 3) y = torch.randn(5, 3) sample_function(x, y) ``` Expected Output ```plaintext FX Graph: def forward(l_a_: torch.Tensor, l_b_: torch.Tensor): add = l_a_ + l_b_ relu = torch.relu(add) return relu ``` # Instructions 1. **Implement the @trace_with_dynamo decorator:** - Trace the function using TorchDynamo. - Handle functions with dynamic shapes and make use of symbolic integers. - Print the traced FX graph after the function\'s execution. 2. **Ensure that the decorator works as expected with different inputs and dynamic shapes.** 3. **Submit your implementation as a Python function.** # Notes - You can use the `torch._dynamo` API and related modules as necessary to implement the tracing functionality. - Refer to the provided documentation for insights into TorchDynamo\'s internal workings and how to leverage them. Good luck with your implementation!","solution":"import torch import torch.fx import torch._dynamo def trace_with_dynamo(func): def wrapper(*args, **kwargs): tracer = torch.fx.Tracer() graph = tracer.trace(func) print(\\"FX Graph:\\") print(graph) return func(*args, **kwargs) return wrapper @trace_with_dynamo def sample_function(a, b): c = a + b d = torch.relu(c) return d"},{"question":"**Objective:** Implement an asynchronous training process for a PyTorch model using `torch.multiprocessing`. Ensure that tensor data is efficiently shared between processes, CUDA compatibility is maintained, deadlocks are avoided, and CPU oversubscription is managed appropriately. **Problem Statement:** Given a simplified model `MyModel`, implement an asynchronous training script using `torch.multiprocessing`. You should adhere to the following requirements: 1. **Model Sharing**: Share the model parameters across subprocesses such that each subprocess can access and update the central model\'s weights. 2. **CUDA Compatibility**: Ensure the training script is compatible with CUDA (use the `spawn` or `forkserver` start method). 3. **Deadlock Avoidance**: Use best practices to avoid deadlocks resulting from thread handling. 4. **CPU Oversubscription**: Manage CPU resources to avoid oversubscription by appropriately setting the number of threads each subprocess can use. **Expected Inputs:** - `num_processes` (int): Number of subprocesses to spawn for training. - `data_loader` (DataLoader): PyTorch DataLoader for the training dataset. - `model` (MyModel): The model to be trained. - `device` (str): Device to use for training (`\'cpu\'` or `\'cuda\'`). **Output:** - Print the final training loss after model training. **Function Prototype:** ```python import torch import torch.multiprocessing as mp class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() # Define model layers def forward(self, x): # Define forward pass def train_fn(rank, num_processes, model, data_loader, device): # Implement training routine for each subprocess def async_train(num_processes, data_loader, model, device): \'\'\' Parameters: num_processes: int - Number of subprocesses to spawn data_loader: DataLoader - PyTorch DataLoader for training data model: MyModel - The model to be trained device: str - Training device (\'cpu\' or \'cuda\') Returns: None - Prints the final training loss. \'\'\' # Check for CUDA and set multiprocessing start method accordingly # Set up shared model parameters # Spawn and manage subprocesses for training # Example usage if __name__ == \'__main__\': model = MyModel().to(device) model.share_memory() # Required for shared parameter access across processes async_train(num_processes=4, data_loader=train_loader, model=model, device=\'cuda\' if torch.cuda.is_available() else \'cpu\') ``` **Constraints and Notes:** - Use `model.share_memory()` to share model parameters across processes. - Ensure compatibility with CUDA by selecting an appropriate multiprocessing start method. - Use `torch.set_num_threads` to set an appropriate number of threads per subprocess based on available CPU resources. - Ensure the main training routine `train_fn` efficiently handles data loading, forward pass, loss calculation, backpropagation, and optimizer step. - Print the final training loss after all subprocesses complete their training execution. **Performance Requirement:** - The implementation should efficiently utilize the CPU/GPU resources and avoid common pitfalls such as deadlocks and CPU oversubscription.","solution":"import torch import torch.multiprocessing as mp from torch.utils.data import DataLoader class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.layer = torch.nn.Linear(10, 1) # Sample layer def forward(self, x): return self.layer(x) def train_fn(rank, num_processes, model, data_loader, device): torch.set_num_threads(1) model.to(device) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) criterion = torch.nn.MSELoss() for epoch in range(5): # Example epochs for batch, (data, target) in enumerate(data_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() if batch % 10 == 0: print(f\\"Process {rank}: Epoch {epoch} Batch {batch} Loss: {loss.item()}\\") def async_train(num_processes, data_loader, model, device): if torch.cuda.is_available() and device == \'cuda\': ctx = mp.get_context(\'spawn\') else: ctx = mp.get_context(\'fork\') model.share_memory() processes = [] for rank in range(num_processes): p = ctx.Process(target=train_fn, args=(rank, num_processes, model, data_loader, device)) p.start() processes.append(p) for p in processes: p.join() print(\\"Training complete.\\") # Example usage if __name__ == \'__main__\': device = \'cuda\' if torch.cuda.is_available() else \'cpu\' train_loader = DataLoader(torch.utils.data.TensorDataset( torch.randn(100, 10), torch.randn(100, 1)), batch_size=32) model = MyModel().to(device) model.share_memory() # Required for shared parameter access across processes async_train(num_processes=4, data_loader=train_loader, model=model, device=device)"},{"question":"**Coding Assessment Question: Access Annotations Dict** **Objective:** Implement a function that retrieves and processes the annotations dictionary of a provided object safely, utilizing best practices based on the Python version in use. **Requirements:** 1. The function should handle differences in how annotations are accessed in Python 3.9 and older vs. Python 3.10 and newer. 2. The function should be able to handle stringized annotations, converting them back to their original types where possible. 3. The function should return `None` if no annotations are found or the annotations dict is not defined. 4. The function should work for functions, classes, and modules, as well as other callable objects. **Function Signature:** ```python def get_annotations(obj: object) -> dict: Retrieves the annotations dictionary for the given object, handling differences between Python versions and un-stringizing annotations if necessary. Parameters: obj: The object from which to retrieve annotations. Can be a function, class, module, etc. Returns: dict: The annotations dictionary with evaluated types where applicable, or None if no annotations are found. pass ``` **Input:** - `obj`: an instance of a Python function, class, module, or any callable object. **Output:** - Returns a dictionary representing the annotations of the given object, with types evaluated appropriately if they were stringized. Returns `None` if the object has no annotations. **Constraints:** - The function should demonstrate effective handling of various Python versions. - Avoiding direct manipulation of the `__annotations__` attribute is critical. - Ensuring that the function gracefully handles and evaluates stringized annotations is crucial. **Example Usage:** ```python def test_function(a: int, b: \'str\') -> \'float\': pass # In Python 3.9 and older # get_annotations(test_function) should return {\'a\': int, \'b\': str, \'return\': float} # In Python 3.10+ # get_annotations(test_function) should return {\'a\': int, \'b\': str, \'return\': float} but handled automatically by inspect.get_annotations ``` **Notes:** - You may use the `inspect` module for handling annotations in Python 3.10+. - For Python 3.9 and older, ensure to check for stringized annotations and evaluate them appropriately. **Hint:** - Utilize the differences in handling annotations between versions. - Implement robust error checking and handling, especially around `eval()` for stringized annotations.","solution":"import sys import inspect def get_annotations(obj: object) -> dict: Retrieves the annotations dictionary for the given object, handling differences between Python versions and un-stringizing annotations if necessary. Parameters: obj: The object from which to retrieve annotations. Can be a function, class, module, etc. Returns: dict: The annotations dictionary with evaluated types where applicable, or None if no annotations are found. if not hasattr(obj, \'__annotations__\'): return None annotations = None if sys.version_info >= (3, 10): annotations = inspect.get_annotations(obj, eval_str=True) else: annotations = getattr(obj, \'__annotations__\', None) return annotations or None"},{"question":"Objective Demonstrate your understanding of the seaborn package by creating a complex plot using the `seaborn.objects` module. The plot should display a customized bar chart, showing data from a nominal variable, segregated by an additional categorical variable, and include error bars. Dataset Use the `penguins` dataset available through `seaborn.load_dataset(\'penguins\')`. The dataset includes the following columns: - `species`: Categorical variable indicating the species of penguins (e.g., Adelie, Chinstrap, Gentoo). - `island`: Categorical variable indicating the island of origin. - `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`: Numerical variables measuring various physical attributes. - `sex`: Categorical variable indicating the sex of the penguins (e.g., Male, Female). Task 1. Load the `penguins` dataset. 2. Draw a bar plot showing the mean `body_mass_g` of penguins for each `species`, differentiated by the `sex` of the penguins. 3. Customize the plot to: - Show bars for different `sex` values side by side using `Dodge`. - Add error bars representing the standard deviation of the `body_mass_g` within each group. - Use different colors for each `sex`. - Make the bars 50% transparent. - Outline the bars with an edge width of 2. # Input and Output Format - There is no input as you will be loading a predefined dataset available within seaborn. - Expected output is a seaborn plot generated and displayed within a Jupyter Notebook or similar environment. # Constraints - Use seaborn version >= 0.11.1. - Ensure proper handling of any potential missing or invalid data within the dataset. # Example ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\", color=\\"sex\\") .add(so.Bar(alpha=0.5, edgewidth=2), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\", color=\\"sex\\") .add(so.Bar(alpha=0.5, edgewidth=2), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) plot.show()"},{"question":"**Title: Advanced Manipulation of `memoryview` Objects in Python** **Objective:** Implement a function that demonstrates the creation, manipulation, and validation of `memoryview` objects using buffer interfaces in Python. **Description:** Using the Python C-API, implement a function `process_memoryview` that performs the following steps: 1. **Buffer Creation:** Create a writable buffer of 100 bytes and fill it with a repeating sequence of the first 10 ASCII lowercase letters (\'a\' to \'j\'). 2. **MemoryView Object:** Create a `memoryview` object from this buffer. 3. **Validation:** Check if the created object is a `memoryview` instance and ensure it is writable. 4. **Modification:** Modify the buffer\'s content via the `memoryview` object by changing the first 10 bytes to the ASCII uppercase letters (\'A\' to \'J\'). 5. **Contiguous Memory Verification:** Ensure that the `memoryview` object references a contiguous chunk of memory. 6. **Fetching Subview:** Create a subview of the `memoryview` object that represents only the first 30 bytes. 7. **Exporting Object:** Retrieve and return a pointer to the exporting object if applicable. **Function Signature:** ```python def process_memoryview() -> dict: This function performs the tasks outlined in the description and returns a dictionary with the following keys: - \'original_buffer\': The original buffer created (as a bytes object). - \'modified_buffer\': The buffer after modification (as a bytes object). - \'is_memoryview\': Boolean indicating if the object is a memoryview. - \'is_writable\': Boolean indicating if the memoryview is writable. - \'is_contiguous\': Boolean indicating if the memoryview is contiguous. - \'subview\': The content of the first 30 bytes of the buffer through the memoryview (as a bytes object). - \'exporting_object\': The exporting object or None if not applicable. pass # Your implementation here ``` **Input and Output:** - The function does not take any input parameters. - The function returns a dictionary with the specified keys and values indicating the operations performed and their results. **Constraints:** - Use only the mentioned and related Python C-API functions for creating and manipulating memoryview objects. - Ensure that memory handling is done correctly to avoid leaks or crashes. **Performance Requirements:** - Operations on buffers and memoryviews should be efficient, making use of low-level memory handling provided by the Python C-API. Implement your solution with thorough error checking and proper management of Python references to ensure robust and efficient code. **Example:** ```python result = process_memoryview() print(result) ``` Expected output: ```python { \'original_buffer\': b\'abcdefghijabcdefghij...abcdefghijabcdefghij\', # 100 bytes total \'modified_buffer\': b\'ABCDEFGHIJabcdefghij...abcdefghijabcdefghij\', # 100 bytes total \'is_memoryview\': True, \'is_writable\': True, \'is_contiguous\': True, \'subview\': b\'ABCDEFGHIJabcdefghijabcdefghij\', \'exporting_object\': <memory at 0x7f...> # Pointer to the object OR None } ``` **Note**: The actual memory address will vary.","solution":"def process_memoryview(): import array import struct # Step 1: Create a writable buffer of 100 bytes filled with \'abcdefghij\' repeated 10 times original_data = (b\'abcdefghij\' * 10) buffer = bytearray(original_data) # Step 2: Create a memoryview object from this buffer memory_view = memoryview(buffer) # Step 3: Validation is_memoryview = isinstance(memory_view, memoryview) is_writable = memory_view.readonly == 0 # Step 4: Modification memory_view[:10] = b\'ABCDEFGHIJ\' # Step 5: Contiguous memory verification is_contiguous = memory_view.contiguous # Step 6: Fetching subview subview = memory_view[:30].tobytes() # Step 7: Exporting object (here it\'s just another memoryview object) exporting_object = memory_view.obj # buffer, in this case return { \'original_buffer\': original_data, \'modified_buffer\': bytes(buffer), \'is_memoryview\': is_memoryview, \'is_writable\': is_writable, \'is_contiguous\': is_contiguous, \'subview\': subview, \'exporting_object\': exporting_object }"},{"question":"You are tasked with improving the robustness of a CGI Python script by integrating advanced exception handling mechanisms with the `cgitb` module. Specifically, you need to ensure that detailed exception reports are both displayed to the user in a browser-friendly format and logged to a specified directory for further analysis. Task Write a function `initialize_exception_handler` that configures the `cgitb` module to handle uncaught exceptions. The function should accept the following parameters: - `display` (bool): If set to `True`, exception reports should be displayed in the browser. - `logdir` (str): If provided, the directory path where the traceback reports should be saved. - `context_lines` (int): Number of lines of context to display around the line causing the exception in the traceback report. - `output_format` (str): Should be either `\'html\'` or `\'text\'` indicating the format of the exception report. Requirements: 1. The function should initialize the `cgitb` module effectively based on the provided parameters. 2. Ensure that the exception reports can display detailed context and arguments involved in the traceback. 3. Use default parameter values as given below: - `display=True` - `logdir=None` - `context_lines=5` - `output_format=\'html\'` Implementation: ```python import cgitb def initialize_exception_handler(display=True, logdir=None, context_lines=5, output_format=\'html\'): Initialize the cgitb module to handle exceptions with specific configurations. Args: display (bool): If True, display the traceback in the browser. logdir (str): Directory to save traceback log files. context_lines (int): Number of lines of context to show around the error. output_format (str): Format of the output (\'html\' or \'text\'). Returns: None # Your code here # Example usage of the function initialize_exception_handler(display=True, logdir=\'/path/to/logdir\', context_lines=10, output_format=\'text\') ``` Notes: 1. You must validate the parameters to ensure they meet the required constraints. 2. The directory provided in `logdir` should exist; you might want to handle cases where it doesn\'t (potentially by creating it or raising an informative error). Constraints: - You are allowed to use only the functions and modules defined in the standard Python library. - Ensure that your implementation handles potential errors gracefully and provides informative messages when exceptions occur. You will be evaluated on the correctness and efficiency of your implementation.","solution":"import cgitb import os def initialize_exception_handler(display=True, logdir=None, context_lines=5, output_format=\'html\'): Initialize the cgitb module to handle exceptions with specific configurations. Args: display (bool): If True, display the traceback in the browser. logdir (str): Directory to save traceback log files. context_lines (int): Number of lines of context to show around the error. output_format (str): Format of the output (\'html\' or \'text\'). Returns: None # Validate output format if output_format not in [\'html\', \'text\']: raise ValueError(\\"output_format should be either \'html\' or \'text\'\\") # Ensure log directory exists or create it if logdir: if not os.path.exists(logdir): try: os.makedirs(logdir) except Exception as e: raise RuntimeError(f\\"Failed to create log directory: {logdir}\\") from e # Initialize cgitb handler cgitb.enable(display=display, logdir=logdir, context=context_lines, format=output_format)"},{"question":"**Objective**: The objective of this assessment is to test the understanding and practical ability to use iterators, generators, and itertools to process and transform data in Python. **Question**: You are given a list of tuples, where each tuple represents a city\'s name and its corresponding temperature readings for a week. ```python city_temperatures = [ (\\"New York\\", [75, 80, 82, 78, 76, 77, 79]), (\\"Los Angeles\\", [85, 87, 89, 90, 84, 83, 88]), (\\"Chicago\\", [70, 68, 71, 72, 69, 67, 73]), (\\"Houston\\", [90, 92, 91, 89, 88, 90, 93]) ] ``` **Tasks**: 1. Write a generator function `temperature_generator` that takes the `city_temperatures` list and yields a tuple containing the city\'s name and the average temperature for that week. 2. Use `itertools` to find the city with the highest average temperature. 3. Implement a function `top_cities` that takes the `city_temperatures` list and an integer `N`, and returns a list of the top N cities with the highest average temperatures using a generator expression. **Expected Function Signatures**: ```python def temperature_generator(city_temperatures: list) -> iter: pass def highest_temperature_city(city_temperatures: list) -> tuple: pass def top_cities(city_temperatures: list, N: int) -> list: pass ``` **Constraints**: 1. All temperatures are given in Fahrenheit and within a reasonable range for daily temperatures. 2. The number of cities and temperature readings are not fixed and can vary. **Performance Requirements**: The solutions should efficiently handle a large number of cities and temperature readings, leveraging the lazy evaluation capabilities of iterators and generators to conserve memory. **Example Usage**: ```python city_temperatures = [ (\\"New York\\", [75, 80, 82, 78, 76, 77, 79]), (\\"Los Angeles\\", [85, 87, 89, 90, 84, 83, 88]), (\\"Chicago\\", [70, 68, 71, 72, 69, 67, 73]), (\\"Houston\\", [90, 92, 91, 89, 88, 90, 93]) ] # Usage of temperature_generator gen = temperature_generator(city_temperatures) for city, avg_temp in gen: print(f\\"{city}: {avg_temp}\\") # Finding the city with the highest average temperature print(highest_temperature_city(city_temperatures)) # Getting the top N cities by highest average temperature print(top_cities(city_temperatures, 2)) ```","solution":"import itertools def temperature_generator(city_temperatures): Generator function that yields tuples of (city, average temperature). for city, temps in city_temperatures: avg_temp = sum(temps) / len(temps) yield (city, avg_temp) def highest_temperature_city(city_temperatures): Uses itertools to find the city with the highest average temperature. temp_gen = temperature_generator(city_temperatures) return max(temp_gen, key=lambda x: x[1]) def top_cities(city_temperatures, N): Returns the top N cities with the highest average temperatures using a generator expression. temp_gen = temperature_generator(city_temperatures) sorted_cities = sorted(temp_gen, key=lambda x: x[1], reverse=True) return sorted_cities[:N]"},{"question":"<|Analysis Begin|> The documentation provided outlines the functionality of the \\"uu\\" module in Python, which is used for encoding and decoding files in the uuencode format. It mentions two primary functions available in the module: 1. `uu.encode(in_file, out_file, name=None, mode=None, *, backtick=False)`: Encodes a file into uuencode format. 2. `uu.decode(in_file, out_file=None, mode=None, quiet=False)`: Decodes a uuencoded file. Additionally, the documentation touches on: - Exception handling with `uu.Error`. - The `backtick` parameter in `uu.encode` which affects the representation of zero. - Usage constraints and deprecated features. The document provides a clear interface for using the \\"uu\\" module\'s functionality, with sufficient details on the parameters and behavior of the provided functions. Given that the \\"uu\\" module is deprecated and it encourages the use of \\"base64\\" as a modern alternative, a challenging question could involve implementing similar functionality using \\"base64\\" and comparing it to the \\"uu\\" module. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Task In this task, you are required to implement two functions to encode and decode a file using the base64 encoding format, mimicking the functionalities provided by the deprecated \\"uu\\" module. Your functions should have similar parameters and behaviors to `uu.encode` and `uu.decode`. Requirements 1. **Function 1: `base64_encode(in_file: Union[str, IO], out_file: Union[str, IO], *, backtick: bool = False) -> None`** - **Input:** - `in_file`: A file-like object opened in binary read mode (`\'rb\'`) or a string containing a pathname to an input file. - `out_file`: A file-like object opened in binary write mode (`\'wb\'`) or a string containing a pathname to an output file. - `backtick`: A boolean parameter (default is `False`). If `True`, zeros in the encoded data should be replaced with backticks (\'`\'). - **Output:** - Writes the base64 encoded data of the input file to the output file. - **Details:** - You should handle the opening and closing of files if string paths are provided. - If `backtick` is `True`, ensure that `0`s in the base64 encoded output are replaced with backticks (\'`\'). 2. **Function 2: `base64_decode(in_file: Union[str, IO], out_file: Union[str, IO], *, quiet: bool = False) -> None`** - **Input:** - `in_file`: A file-like object opened in binary read mode (`\'rb\'`) or a string containing a pathname to an input file. - `out_file`: A file-like object opened in binary write mode (`\'wb\'`) or a string containing a pathname to an output file. - `quiet`: A boolean parameter (default is `False`). If `True`, suppress any warning messages that may occur during decoding. - **Output:** - Writes the decoded binary data of the base64 input file to the output file. - **Details:** - You should handle the opening and closing of files if string paths are provided. - If there are any issues with decoding (like when the input data isn\'t correctly base64 encoded), raise an appropriate exception and print a warning to standard error unless `quiet` is `True`. Constraints - You may assume input files are always readable and output files are always writable by your program. - Handle file opening/closing if dealing with file paths. - Ensure good performance for large files by using chunked read/write operations. # Examples ```python # Example usage # Base64 encoding file \\"example.txt\\" base64_encode(\'example.txt\', \'encoded.txt\') # Base64 decoding file \\"encoded.txt\\" base64_decode(\'encoded.txt\', \'decoded.txt\') ``` Your implementation should match the expected functionalities and constraints described above.","solution":"import base64 from typing import Union, IO def base64_encode(in_file: Union[str, IO], out_file: Union[str, IO], *, backtick: bool = False) -> None: Encodes a file into base64 format. Parameters: in_file (Union[str, IO]): A file-like object or a pathname to an input file. out_file (Union[str, IO]): A file-like object or a pathname to an output file. backtick (bool): If True, replace zeros in base64 output with backticks. close_src = False close_dest = False if isinstance(in_file, str): in_file = open(in_file, \'rb\') close_src = True if isinstance(out_file, str): out_file = open(out_file, \'wb\') close_dest = True try: while True: piece = in_file.read(3 * 1024) # Reading in chunks of 3KB if not piece: break encoded_piece = base64.b64encode(piece) if backtick: encoded_piece = encoded_piece.replace(b\'0\', b\'`\') out_file.write(encoded_piece) finally: if close_src: in_file.close() if close_dest: out_file.close() def base64_decode(in_file: Union[str, IO], out_file: Union[str, IO], *, quiet: bool = False) -> None: Decodes a base64 encoded file. Parameters: in_file (Union[str, IO]): A file-like object or a pathname to an input file. out_file (Union[str, IO]): A file-like object or a pathname to an output file. quiet (bool): If True, suppress warning messages. close_src = False close_dest = False if isinstance(in_file, str): in_file = open(in_file, \'rb\') close_src = True if isinstance(out_file, str): out_file = open(out_file, \'wb\') close_dest = True try: while True: piece = in_file.read(4 * 1024) # Reading in chunks of 4KB if not piece: break try: decoded_piece = base64.b64decode(piece, validate=True) out_file.write(decoded_piece) except (base64.binascii.Error, ValueError) as e: if not quiet: print(\\"Warning: Decoding error occurred:\\", e) raise e finally: if close_src: in_file.close() if close_dest: out_file.close()"},{"question":"# Bytearray Manipulation Challenge In this task, you will create and manipulate bytearrays using the Python C-API functions as described. The goal is to implement a function that performs the following steps: 1. **Create** a bytearray from a given string. 2. **Resize** the newly created bytearray to a specified length. 3. **Concatenate** another bytearray to the resized bytearray. 4. **Verify** the size of the final bytearray and return its contents as a normal Python string. You need to implement the following function: ```python def manipulate_bytearray(init_string: str, resize_length: int, concat_string: str) -> str: This function performs the following operations: 1. Creates a bytearray from init_string. 2. Resizes the bytearray to resize_length. 3. Concatenates a new bytearray created from concat_string to the resized bytearray. 4. Returns the contents of the final bytearray as a normal Python string. Args: init_string (str): The initial string to be converted to bytearray. resize_length (int): The size to which the bytearray should be resized. concat_string (str): The string to be converted to bytearray and concatenated. Returns: str: The contents of the final bytearray as a normal Python string. # Your implementation here ``` # Constraints: - The length of `init_string` and `concat_string` will be <= 100. - `resize_length` will be a non-negative integer and less than or equal to 200. # Notes: - You may assume that the bytearray functions behave as documented. - Exception handling for NULL pointers and other potential errors from the bytearray API functions is not required for this task. # Example: ```python # Example usage init_string = \\"Hello\\" resize_length = 10 concat_string = \\" World\\" result = manipulate_bytearray(init_string, resize_length, concat_string) print(result) # Should give the result considering resizing and concatenation operations ``` Implement all necessary parts of the function to achieve the expected results as described.","solution":"def manipulate_bytearray(init_string: str, resize_length: int, concat_string: str) -> str: This function performs the following operations: 1. Creates a bytearray from init_string. 2. Resizes the bytearray to resize_length. 3. Concatenates a new bytearray created from concat_string to the resized bytearray. 4. Returns the contents of the final bytearray as a normal Python string. Args: init_string (str): The initial string to be converted to bytearray. resize_length (int): The size to which the bytearray should be resized. concat_string (str): The string to be converted to bytearray and concatenated. Returns: str: The contents of the final bytearray as a normal Python string. # Create a bytearray from the init_string byte_arr = bytearray(init_string, \'utf-8\') # Resize the bytearray to resize_length byte_arr.extend(b\'x00\' * (resize_length - len(byte_arr))) byte_arr = byte_arr[:resize_length] # Concatenate the new bytearray from concat_string byte_arr += bytearray(concat_string, \'utf-8\') # Return the final bytearray content as a string return byte_arr.decode(\'utf-8\')"},{"question":"# XML Data Processing with `xml.etree.ElementTree` You have been provided with an XML file containing data about books in a library. Your task is to parse this XML file, extract relevant information, and perform specific modifications. XML Structure: Here is a sample structure of the XML file (`library.xml`): ```xml <library> <book> <title>Python Programming</title> <author>John Doe</author> <genre>Programming</genre> <price>29.99</price> <publish_date>2021-01-01</publish_date> </book> <book> <title>Data Science</title> <author>Jane Smith</author> <genre>Science</genre> <price>39.99</price> <publish_date>2020-07-15</publish_date> </book> <!-- More book entries --> </library> ``` Tasks: 1. **Parse the XML File**: - Write a function `parse_xml(file_path)` that takes the file path of the XML document as input and returns the root element. 2. **Extract Book Information**: - Write a function `extract_books(root)` that takes the XML root element as input and returns a list of dictionaries where each dictionary contains information about a book (title, author, genre, price, publish_date). 3. **Modify Book Prices**: - Write a function `apply_discount(root, discount)` that takes the XML root element and a discount percentage as input, applies the discount to the price of all books, and updates the XML tree. 4. **Add a New Book**: - Write a function `add_book(root, title, author, genre, price, publish_date)` that takes the XML root element and details of a new book, creates a new book entry, and adds it to the XML tree. 5. **Save the Modified XML**: - Write a function `save_xml(root, output_file)` that takes the XML root element and an output file path, and saves the modified XML document to the specified path. Constraints: - Ensure all input prices and discount percentages are valid floats. - The XML file will always have a valid structure matching the sample. Expected Function Definitions: ```python import xml.etree.ElementTree as ET def parse_xml(file_path): # Your code here def extract_books(root): # Your code here def apply_discount(root, discount): # Your code here def add_book(root, title, author, genre, price, publish_date): # Your code here def save_xml(root, output_file): # Your code here ``` Example Usage: ```python root = parse_xml(\'library.xml\') books = extract_books(root) print(books) # Output: [{\'title\': \'Python Programming\', \'author\': \'John Doe\', \'genre\': \'Programming\', \'price\': \'29.99\', \'publish_date\': \'2021-01-01\'}, ...] apply_discount(root, 10) # Apply a 10% discount to all book prices add_book(root, \'New Book Title\', \'New Author\', \'New Genre\', 19.99, \'2022-10-01\') save_xml(root, \'modified_library.xml\') ``` **Note**: Remember to handle any exceptions that might arise during file operations or XML parsing to ensure robustness.","solution":"import xml.etree.ElementTree as ET def parse_xml(file_path): Parse the XML file at the given file path and return the root element. tree = ET.parse(file_path) return tree.getroot() def extract_books(root): Extracts book information from the XML root element and returns a list of dictionaries with each book\'s details. books = [] for book in root.findall(\'book\'): book_info = { \'title\': book.find(\'title\').text, \'author\': book.find(\'author\').text, \'genre\': book.find(\'genre\').text, \'price\': float(book.find(\'price\').text), \'publish_date\': book.find(\'publish_date\').text } books.append(book_info) return books def apply_discount(root, discount): Applies a discount to the price of all books in the XML tree. for book in root.findall(\'book\'): price = float(book.find(\'price\').text) new_price = round(price * (1 - discount / 100), 2) book.find(\'price\').text = str(new_price) def add_book(root, title, author, genre, price, publish_date): Adds a new book entry to the XML tree. new_book = ET.Element(\'book\') ET.SubElement(new_book, \'title\').text = title ET.SubElement(new_book, \'author\').text = author ET.SubElement(new_book, \'genre\').text = genre ET.SubElement(new_book, \'price\').text = str(price) ET.SubElement(new_book, \'publish_date\').text = publish_date root.append(new_book) def save_xml(root, output_file): Saves the modified XML tree to the specified output file path. tree = ET.ElementTree(root) tree.write(output_file, xml_declaration=True, encoding=\'utf-8\')"},{"question":"# PyTorch Advanced Manipulation: Storage and Tensors **Objective**: Gain a deep understanding of storage in PyTorch and demonstrate your ability to manipulate tensor storages. Task Write a function `manipulate_and_verify_storage` that performs the following operations: 1. **Create a Tensor with Specified Values**: - Create a tensor `t` initialized with a given list of floating-point numbers. 2. **Retrieve and Clone the Storage**: - Retrieve the untyped storage of tensor `t` and store it in `s0`. - Clone this storage to create `s1`. 3. **Modify the Cloned Storage**: - Fill the cloned storage `s1` with zeros. 4. **Reassign the Tensor\'s Storage**: - Reassign the original tensor `t` to use the modified storage `s1` without changing other metadata (like stride, shape). 5. **Verification**: - Verify that the tensor `t` has its values updated to reflect the changes made in `s1`. - Create a new tensor `t2` that uses the original storage `s0` and verify that it still holds the original values, demonstrating that the original storage was not altered by the manipulation. Input - A list of floating-point numbers representing the initial values of the tensor. Output - A dictionary with two keys: `updated_tensor` and `original_tensor`, representing: - The modified tensor `t`. - The new tensor `t2` that uses the original storage `s0`. Function Signature ```python def manipulate_and_verify_storage(values: list) -> dict: pass ``` Constraints - The input list will contain between 1 and 100 floating-point numbers. - You may not use any in-place manipulation functions on the tensor directly, only through storage manipulations. Example ```python input_values = [1.0, 2.0, 3.0] result = manipulate_and_verify_storage(input_values) print(result[\'updated_tensor\']) # Should print: tensor([0., 0., 0.]) print(result[\'original_tensor\']) # Should print: tensor([1., 2., 3.]) ``` Note - Ensure your code handles memory and storage efficiently to avoid unnecessary data duplication. - Be cautious of the low-level nature of storage manipulation; improper handling might lead to unspecified behavior or runtime errors.","solution":"import torch def manipulate_and_verify_storage(values: list) -> dict: # Create a tensor initialized with the given list of floating-point numbers t = torch.tensor(values) # Retrieve the storage of tensor t and store it in s0 s0 = t.storage() # Clone this storage to create s1 s1 = s0.clone() # Fill the cloned storage s1 with zeros s1.fill_(0) # Reassign the tensor t to use the modified storage s1 without changing other metadata t.set_(s1, t.storage_offset(), t.size(), t.stride()) # Verification steps # t should be updated to reflect the changes made in s1 (i.e., filled with zeros) # Create a new tensor t2 that uses the original storage s0 t2 = torch.tensor([], dtype=t.dtype).set_(s0, t.storage_offset(), t.size(), t.stride()) # Return the modified tensor and the tensor using the original storage return { \'updated_tensor\': t, \'original_tensor\': t2 }"},{"question":"Objective Implement a Python function that takes a list of strings representing floating-point numbers and performs the following tasks: 1. Converts each string into `PyFloatObject`. 2. Checks if the conversion was successful. 3. Calculates the sum and average of the converted floats. 4. Returns a dictionary containing the sum, average, and a list of any invalid strings (i.e., those that could not be converted to floats). Requirements: - Your function should handle errors gracefully. If a string cannot be converted to a float, it should be included in a list of invalid strings. - Use the functions described in the provided documentation as much as possible. Function Signature: ```python def process_floats(float_strings: list) -> dict: pass ``` Input: - `float_strings`: A list of strings, where each string is expected to represent a floating point number. Output: - A dictionary with keys: - `sum`: The sum of all valid floating-point numbers. - `average`: The average of all valid floating-point numbers. - `invalid_strings`: A list of strings that could not be converted to floating-point numbers. Constraints: - All strings in `float_strings` have a maximum length of 100 characters. - The list `float_strings` can have up to 10,000 elements. Example: ```python input_data = [\\"3.14\\", \\"2.71\\", \\"invalid\\", \\"100\\", \\"abc\\", \\"1.618\\"] output = process_floats(input_data) # Expected output: # { # \\"sum\\": 107.468, # \\"average\\": 26.867, # \\"invalid_strings\\": [\\"invalid\\", \\"abc\\"] # } ``` Notes: - You will need to handle possible exceptions during the conversion of strings to floats. - Consider edge cases such as empty strings or strings with only white spaces.","solution":"def process_floats(float_strings: list) -> dict: valid_floats = [] invalid_strings = [] for s in float_strings: try: valid_floats.append(float(s)) except ValueError: invalid_strings.append(s) sum_floats = sum(valid_floats) average_floats = sum_floats / len(valid_floats) if valid_floats else 0 return { \\"sum\\": sum_floats, \\"average\\": average_floats, \\"invalid_strings\\": invalid_strings }"},{"question":"# Mocking and Unit Testing with `unittest.mock` **Objective:** Evaluate your understanding of the `unittest.mock` module by writing function implementations and corresponding unit tests using mocks. **Description:** You are tasked with implementing a function `process_data` that performs a sequence of operations on data, interacting with external systems through specific methods. You will also write unit tests for `process_data` using the `unittest.mock` module to mock dependent methods and assert correct behavior. **Function to Implement:** ```python class DataProcessor: def __init__(self, service_client): self.client = service_client def process_data(self, data): try: result = self.client.fetch_data(data) processed = self.transform_data(result) self.client.store_data(processed) return processed except Exception as e: self.client.handle_error(e) return None def transform_data(self, data): # Placeholder for data transformation logic return data * 2 ``` **Unit Tests:** Write a test class `TestProcessData` using the `unittest` framework and `unittest.mock` module with the following tests: 1. Test that `fetch_data` is called with the correct arguments. 2. Test that `store_data` is called with the correctly transformed data. 3. Test that `handle_error` is called when an exception occurs in `fetch_data`. 4. Use `side_effect` to simulate different return values and exceptions for `fetch_data`. **Constraints:** - Use `MagicMock` and `patch` for mocking. - Ensure each test asserts the expected behavior and calls. **Example Usage:** ```python def main(): # Example service client with stub methods class ServiceClient: def fetch_data(self, data): return data + 10 def store_data(self, data): pass def handle_error(self, error): pass client = ServiceClient() processor = DataProcessor(client) data = 5 processed_data = processor.process_data(data) print(f\\"Processed data: {processed_data}\\") if __name__ == \\"__main__\\": main() ``` **Expected Output:** Implement the test class ensuring that all tests pass and the `process_data` function works as described. **Hints:** - Use `patch.object` to mock specific methods in `ServiceClient`. - Use `assert_called_with` to verify methods are called with the correct arguments. - Use `side_effect` to simulate method responses and exceptions.","solution":"from unittest.mock import MagicMock class DataProcessor: def __init__(self, service_client): self.client = service_client def process_data(self, data): try: result = self.client.fetch_data(data) processed = self.transform_data(result) self.client.store_data(processed) return processed except Exception as e: self.client.handle_error(e) return None def transform_data(self, data): # Placeholder for data transformation logic return data * 2"},{"question":"**Problem Statement:** You are required to demonstrate your understanding of the `seaborn` library, particularly in generating and customizing color palettes using the `husl_palette` function. Write a Python script that performs the following tasks: 1. Generates a default HUSL color palette with 6 colors and displays it. 2. Creates and displays a palette with 10 colors. 3. Generates and displays a color palette with 8 colors, with lightness set to 0.5 and saturation set to 0.7. 4. Changes the starting point for hue sampling to 0.25 and creates a palette with 7 colors. 5. Creates and displays a continuous colormap using the HUSL color space. **Input:** No direct input required. The script should perform the tasks outlined above sequentially. **Output:** The script should display the generated color palettes and the continuous colormap. **Constraints:** - You must use the `seaborn` library. - Ensure to use the `husl_palette` function for palette generation. - Display each generated palette or colormap after creating it. **Example Output:** The output should include the visual representation of each of the generated palettes. Each display should clearly indicate the settings used to create the palette and/or colormap. Example for task 1 (default palette) can be visualized as: ``` Default HUSL Palette (6 colors): [visual representation of palette] ``` Tasks 2 to 5 should follow a similar format, adjusting the parameters as specified. **Additional Information:** You may refer to `seaborn` documentation on `husl_palette` for detailed parameters and usage examples.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_and_display_palettes(): # Task 1: Generate a default HUSL color palette with 6 colors and display it palette_default = sns.husl_palette() sns.palplot(palette_default) plt.title(\\"Default HUSL Palette (6 colors)\\") plt.show() # Task 2: Create and display a palette with 10 colors palette_10_colors = sns.husl_palette(10) sns.palplot(palette_10_colors) plt.title(\\"HUSL Palette (10 colors)\\") plt.show() # Task 3: Generate and display a color palette with 8 colors, lightness=0.5, saturation=0.7 palette_8_custom = sns.husl_palette(8, l=0.5, s=0.7) sns.palplot(palette_8_custom) plt.title(\\"HUSL Palette (8 colors, lightness=0.5, saturation=0.7)\\") plt.show() # Task 4: Change the starting point for hue sampling to 0.25 and create a palette with 7 colors palette_hue_0_25 = sns.husl_palette(7, h=0.25) sns.palplot(palette_hue_0_25) plt.title(\\"HUSL Palette (7 colors, hue=0.25)\\") plt.show() # Task 5: Create and display a continuous colormap using the HUSL color space palette_continuous = sns.color_palette(\\"husl\\", as_cmap=True) sns.heatmap([[0, 1], [1, 0]], cmap=palette_continuous, cbar=True) plt.title(\\"Continuous Colormap using HUSL\\") plt.show() # Function call for testing the solution generate_and_display_palettes()"},{"question":"# Challenging PyTorch FX Transformation Task Objective Create an FX transformation function that modifies the graph of a provided `torch.nn.Module`. The function will replace every `torch.add` operation in the original module\'s computation graph with a custom addition operation that adds a fixed scalar to each element before performing the addition. This task assesses your understanding of FX transformations, graph manipulations, and usage of Proxy objects. Description Implement a function `custom_addition_transform` that takes the following inputs: - `module`: A `torch.nn.Module` representing the model to be transformed. - `scalar`: A float value that will be added to each element in the tensors before the normal addition operation. The function should: 1. Symbolically trace the provided `module` into a `Graph`. 2. Modify the graph such that each `torch.add` operation is replaced by the custom addition operation: ( text{result} = (text{input1} + text{scalar}) + (text{input2} + text{scalar}) ) 3. Return a new `torch.nn.Module` that includes these modifications. Expected Function Signature ```python import torch import torch.fx def custom_addition_transform(module: torch.nn.Module, scalar: float) -> torch.nn.Module: pass ``` Constraints - Each node in the modified graph should preserve the original connections except where `torch.add` is involved. - Ensure that the transformed module produces output consistent with the modified operations. - Implement the transformation efficiently, considering PyTorch\'s computation overhead. Example ```python class SimpleModule(torch.nn.Module): def forward(self, x, y): return torch.add(x, y) model = SimpleModule() scalar = 5.0 transformed_model = custom_addition_transform(model, scalar) # Example execution input_x = torch.tensor([1.0, 2.0]) input_y = torch.tensor([3.0, 4.0]) output = transformed_model(input_x, input_y) print(output) # Output should reflect the custom addition logic ``` Hints - Refer to the documentation on Direct Graph Manipulation and Proxy/Retracing for guidance. - Use `replace_all_uses_with` to ensure the consistency of your transformations.","solution":"import torch import torch.fx def custom_addition_transform(module: torch.nn.Module, scalar: float) -> torch.nn.Module: class CustomAdditionTracer(torch.fx.Tracer): def is_leaf_module(self, m, module_qualified_name): return False def custom_addition(x, y, scalar): return (x + scalar) + (y + scalar) tracer = CustomAdditionTracer() graph = tracer.trace(module) traced = torch.fx.GraphModule(module, graph) for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: with graph.inserting_after(node): add_node = graph.call_function(custom_addition, (node.all_input_nodes[0], node.all_input_nodes[1], scalar)) node.replace_all_uses_with(add_node) graph.erase_node(node) return torch.fx.GraphModule(traced, graph)"},{"question":"# Email Iterator Utilities Description You have been given a module that provides useful high-level iterator utilities to traverse and manipulate email message objects. Your task is to implement a function that processes an email message by utilizing these utilities. Specifically, your function should: 1. Iterate over all textual parts of the message using `typed_subpart_iterator` and collect all lines from these parts. 2. Count the number of lines and the number of unique lines (ignoring leading and trailing whitespace, and considering case-insensitive matches). 3. Return a dictionary with the total line count and the unique line count. Function Signature ```python def process_email_message(msg) -> dict: pass ``` Input - `msg`: An `email.message.Message` object representing an email message. Output - A dictionary with two keys: - `\\"total_lines\\"`: an integer representing the total number of lines across all textual parts of the message. - `\\"unique_lines\\"`: an integer representing the number of unique lines across all textual parts (case-insensitive and whitespace-trimmed). Constraints - Consider only textual parts of the message (MIME type `text/*`). - Exclude non-textual parts (e.g., attachments). Example Assume `msg` is an `email.message.Message` object created from the following email content: ``` Content-Type: multipart/mixed; boundary=\\"abc123\\" --abc123 Content-Type: text/plain Hello, World! This is a test email. Hello, World! --abc123 Content-Type: text/plain This is another part of the email. hello, world! --abc123-- ``` The function call `process_email_message(msg)` should return: ```python { \\"total_lines\\": 5, \\"unique_lines\\": 4 } ``` Notes - Use the `typed_subpart_iterator` function from the `email.iterators` module to iterate over the textual parts of the email message. - Handle line counting and uniqueness checking in a case-insensitive manner and trim leading/trailing whitespace.","solution":"from email.iterators import typed_subpart_iterator def process_email_message(msg): Processes an email message and returns a dictionary with the total line count and the unique line count. total_lines = 0 unique_lines_set = set() for part in typed_subpart_iterator(msg, \'text\'): for line in part.get_payload().splitlines(): total_lines += 1 cleaned_line = line.strip().lower() unique_lines_set.add(cleaned_line) return { \\"total_lines\\": total_lines, \\"unique_lines\\": len(unique_lines_set), }"},{"question":"# Pandas Indexing and Selection Assessment Objective To assess your understanding of pandas\' indexing, labeling, and selection operations, you are required to process a given dataset and perform various slicing, filtering, and reindexing tasks using pandas. This problem will test your ability to handle data using both basic and advanced indexing techniques. Dataset You will be working with a mock dataset containing sales data for different product categories across several days. Below is a sample of the DataFrame `sales_df`: ```python import pandas as pd import numpy as np data = { \'date\': pd.date_range(start=\'2023-01-01\', periods=8), \'category\': [\'Electronics\', \'Clothing\', \'Electronics\', \'Clothing\', \'Books\', \'Electronics\', \'Books\', \'Clothing\'], \'product_id\': [101, 102, 103, 104, 105, 106, 107, 108], \'sales\': [200, 150, 300, 130, 60, 220, 80, 200] } sales_df = pd.DataFrame(data) ``` Task You are required to implement a function `process_sales_data` that takes no parameters and performs the following operations: 1. **Set the Index**: - Set the DataFrame\'s index to be the `date` column. 2. **Filter and Slice**: - Extract all rows where the `category` is \'Electronics\' and sales are greater than 200. 3. **Reindex**: - Reindex the DataFrame to include additional dates: \'2023-01-09\', \'2023-01-10\', filling missing values with `0` for the sales column. 4. **Handle Duplicates**: - Ensure there are no duplicate entries based on `product_id`. Keep the first occurrence if duplicates are found. 5. **Set with Enlargement**: - Add a new column `discount` where the value is 10 if the sales are greater than 100, and 0 otherwise. 6. **Boolean Indexing**: - Create a DataFrame with sales greater than 150 and store it in the variable `high_sales`. Expected Function Implementation ```python def process_sales_data(): data = { \'date\': pd.date_range(start=\'2023-01-01\', periods=8), \'category\': [\'Electronics\', \'Clothing\', \'Electronics\', \'Clothing\', \'Books\', \'Electronics\', \'Books\', \'Clothing\'], \'product_id\': [101, 102, 103, 104, 105, 106, 107, 108], \'sales\': [200, 150, 300, 130, 60, 220, 80, 200] } sales_df = pd.DataFrame(data) # Set the DataFrame\'s index to be the `date` column sales_df.set_index(\'date\', inplace=True) # Extract all rows where the `category` is \'Electronics\' and sales are greater than 200 electronics_sales = sales_df[(sales_df[\'category\'] == \'Electronics\') & (sales_df[\'sales\'] > 200)] # Reindex the DataFrame to include additional dates, filling missing values with 0 for sales new_dates = pd.date_range(start=\'2023-01-01\', end=\'2023-01-10\') reindexed_df = sales_df.reindex(new_dates, fill_value=0) # Ensure there are no duplicate entries based on `product_id`. sales_df = sales_df.drop_duplicates(subset=\'product_id\') # Add a new column `discount` sales_df[\'discount\'] = sales_df[\'sales\'].apply(lambda x: 10 if x > 100 else 0) # Create a DataFrame with sales greater than 150 high_sales = sales_df[sales_df[\'sales\'] > 150] return electronics_sales, reindexed_df, high_sales # Example function call electronics_sales, reindexed_df, high_sales = process_sales_data() ``` Constraints - You are required to use pandas for all operations. - Ensure your solution is efficient and leverages pandas\' capabilities to handle missing values and duplicates appropriately. Good luck!","solution":"import pandas as pd def process_sales_data(): Process the sales data through various pandas operations as per the instructions. Returns the filtered, reindexed, and further processed dataframes. data = { \'date\': pd.date_range(start=\'2023-01-01\', periods=8), \'category\': [\'Electronics\', \'Clothing\', \'Electronics\', \'Clothing\', \'Books\', \'Electronics\', \'Books\', \'Clothing\'], \'product_id\': [101, 102, 103, 104, 105, 106, 107, 108], \'sales\': [200, 150, 300, 130, 60, 220, 80, 200] } sales_df = pd.DataFrame(data) # Step 1: Set the DataFrame\'s index to be the `date` column sales_df.set_index(\'date\', inplace=True) # Step 2: Extract all rows where the `category` is \'Electronics\' and sales are greater than 200. electronics_sales = sales_df[(sales_df[\'category\'] == \'Electronics\') & (sales_df[\'sales\'] > 200)] # Step 3: Reindex the DataFrame to include additional dates, filling missing values with 0 for sales new_dates = pd.date_range(start=\'2023-01-01\', end=\'2023-01-10\') reindexed_df = sales_df.reindex(new_dates, fill_value=0) # Step 4: Ensure there are no duplicate entries based on `product_id`. Keep the first occurrence. sales_df = sales_df.drop_duplicates(subset=\'product_id\') # Step 5: Add a new column `discount` sales_df[\'discount\'] = sales_df[\'sales\'].apply(lambda x: 10 if x > 100 else 0) # Step 6: Create a DataFrame with sales greater than 150 high_sales = sales_df[sales_df[\'sales\'] > 150] return electronics_sales, reindexed_df, high_sales # Example function call electronics_sales, reindexed_df, high_sales = process_sales_data()"},{"question":"**Objective**: To design a function that can serialize and deserialize a specific data structure using the `marshal` module, demonstrating an understanding of its capabilities and constraints. **Question**: Implement a function `serialize_and_deserialize(data)` that takes a dictionary `data`, serializes it to a binary format using the `marshal` module, and then deserializes it back to a dictionary. The function should return the deserialized dictionary and handle any potential exceptions gracefully. # Function Signature ```python def serialize_and_deserialize(data: dict) -> dict: ``` # Input - `data`: A dictionary where keys are strings and values can be integers, floats, strings, lists, tuples, or another dictionary of the same structure. # Output - Returns the deserialized dictionary that should be identical to the input `data`. # Constraints 1. You must use the `marshal` module for serialization and deserialization. 2. Handle cases where serialization or deserialization could fail due to unsupported types. 3. The function should be able to handle nested dictionaries and lists up to three levels deep. # Example Given the following input: ```python data = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"scores\\": [90, 85, 88], \\"address\\": {\\"city\\": \\"Wonderland\\", \\"zip\\": \\"12345\\"}, \\"metadata\\": {\\"visited\\": (True, False), \\"points\\": 42.42}, } ``` The function call `serialize_and_deserialize(data)` should return: ```python { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"scores\\": [90, 85, 88], \\"address\\": {\\"city\\": \\"Wonderland\\", \\"zip\\": \\"12345\\"}, \\"metadata\\": {\\"visited\\": (True, False), \\"points\\": 42.42}, } ``` # Notes - If the input data contains unsupported types, raise a `ValueError` with a relevant message. - Ensure you handle potential exceptions like `EOFError`, `ValueError`, or `TypeError` properly during deserialization. Implement the function below: ```python import marshal def serialize_and_deserialize(data: dict) -> dict: try: # Serialize the dictionary to a binary format serialized_data = marshal.dumps(data) # Deserialize the binary data back to a dictionary deserialized_data = marshal.loads(serialized_data) return deserialized_data except (TypeError, ValueError, EOFError) as e: raise ValueError(f\\"Failed to serialize or deserialize the data: {e}\\") ```","solution":"import marshal def serialize_and_deserialize(data: dict) -> dict: Serializes the given dictionary to a binary format using marshal, and deserializes it back to a dictionary. Raises ValueError if serialization or deserialization fails. try: # Serialize the dictionary to a binary format serialized_data = marshal.dumps(data) # Deserialize the binary data back to a dictionary deserialized_data = marshal.loads(serialized_data) return deserialized_data except (TypeError, ValueError, EOFError) as e: raise ValueError(f\\"Failed to serialize or deserialize the data: {e}\\")"},{"question":"**Objective:** Create a function that translates an errno value to a user-friendly error message, including both the string name of the errno and the description of the associated error. Requirements: 1. The function should be named `translate_errno`. 2. The function should accept an integer errno value as input. 3. The function should return a string combining the errno name and its description. 4. If the errno value is not recognized (i.e., not defined in the current platform\'s `errno` module), the function should return \\"Unknown errno\\". Example: ```python import errno def translate_errno(err_number): # Your code here # Examples of usage: print(translate_errno(errno.EPERM)) # Output: \\"EPERM: Operation not permitted.\\" print(translate_errno(errno.ENOENT)) # Output: \\"ENOENT: No such file or directory.\\" print(translate_errno(9999)) # Output: \\"Unknown errno\\" ``` Constraints: - You should only use the `errno` module to resolve the error names and their descriptions. Implementation: Implement the function `translate_errno` according to the given requirements. ```python import errno import os def translate_errno(err_number): # Check if the errno value exists in the errorcode dictionary if err_number in errno.errorcode: # Get the name of the errno errno_name = errno.errorcode[err_number] # Get the description using os.strerror errno_description = os.strerror(err_number) # Return formatted string return f\\"{errno_name}: {errno_description}.\\" else: # If errno not found, return unknown message return \\"Unknown errno\\" # Example usages print(translate_errno(errno.EPERM)) # Should output: \\"EPERM: Operation not permitted.\\" print(translate_errno(errno.ENOENT)) # Should output: \\"ENOENT: No such file or directory.\\" print(translate_errno(9999)) # Should output: \\"Unknown errno\\" ``` The implemented function should pass the provided examples and handle any valid or invalid errno values appropriately.","solution":"import errno import os def translate_errno(err_number): Translates an errno value to a user-friendly error message, including both the string name of the errno and the description of the associated error. :param err_number: integer errno value. :return: string combining the errno name and its description. # Check if the errno value exists in the errorcode dictionary if err_number in errno.errorcode: # Get the name of the errno errno_name = errno.errorcode[err_number] # Get the description using os.strerror errno_description = os.strerror(err_number) # Return formatted string return f\\"{errno_name}: {errno_description}.\\" else: # If errno not found, return unknown message return \\"Unknown errno\\""},{"question":"You are asked to implement a class `Configuration` that maintains configuration settings with the ability to track dynamic overrides and inheritance. This class should use the `ChainMap` from the `collections` module to manage multiple layers of configuration. # Requirements: - The `Configuration` class should initialize with a single base `dict` that contains default configuration settings. - You should provide a method `add_layer` that adds a new configuration layer on top of the existing layers. - Implement a method `remove_layer` that removes the topmost configuration layer. - Implement a method `get_setting` that retrieves the value of a specific setting by a given key. If the key does not exist in any layer, it should return `None`. - Implement a method `update_setting` that updates the value of a setting in the topmost layer where the key exists. If the key does not exist in any layer, add the new key-value pair to the topmost layer. # Constraints: - There should always be at least one layer in the configuration. - You can assume that keys and values in the configuration are both strings. - The layers should maintain insertion order where applicable. # Input Format: - Initialization with a dictionary of default settings. - Sequence of operations to modify and query configuration settings. # Output Format: - Result of `get_setting` method calls. # Example: ```python default_settings = { \'host\': \'localhost\', \'port\': \'8080\', \'debug\': \'true\' } config = Configuration(default_settings) # Add a new layer with overrides config.add_layer({\'port\': \'9090\', \'debug\': \'false\'}) # Retrieve settings print(config.get_setting(\'host\')) # Output: \'localhost\' print(config.get_setting(\'port\')) # Output: \'9090\' print(config.get_setting(\'debug\')) # Output: \'false\' # Update a setting config.update_setting(\'host\', \'127.0.0.1\') print(config.get_setting(\'host\')) # Output: \'127.0.0.1\' # Remove the topmost layer config.remove_layer() print(config.get_setting(\'port\')) # Output: \'8080\' print(config.get_setting(\'debug\')) # Output: \'true\' ``` # Implementation: ```python from collections import ChainMap class Configuration: def __init__(self, base_settings): self._chain_map = ChainMap(base_settings) def add_layer(self, overrides): self._chain_map = self._chain_map.new_child(overrides) def remove_layer(self): if len(self._chain_map.maps) > 1: self._chain_map = self._chain_map.parents else: raise IndexError(\\"Cannot remove the last configuration layer\\") def get_setting(self, key): return self._chain_map.get(key, None) def update_setting(self, key, value): for map in self._chain_map.maps: if key in map: map[key] = value return self._chain_map.maps[0][key] = value ```","solution":"from collections import ChainMap class Configuration: def __init__(self, base_settings): self._chain_map = ChainMap(base_settings) def add_layer(self, overrides): self._chain_map = self._chain_map.new_child(overrides) def remove_layer(self): if len(self._chain_map.maps) > 1: self._chain_map = self._chain_map.parents else: raise IndexError(\\"Cannot remove the last configuration layer\\") def get_setting(self, key): return self._chain_map.get(key, None) def update_setting(self, key, value): for map in self._chain_map.maps: if key in map: map[key] = value return self._chain_map.maps[0][key] = value"},{"question":"Objective: You are asked to demonstrate your understanding of Seaborn\'s `sns.ecdfplot` function by creating a series of ECDF plots using a given dataset. Problem Statement: Using the provided dataset `penguins`, perform the following tasks: 1. Plot an ECDF of the `flipper_length_mm` variable along the x-axis. 2. Flip the plot and show the ECDF of the `flipper_length_mm` variable along the y-axis. 3. Create histograms for the numeric columns that contain \\"bill_\\" in their names. 4. Plot multiple ECDFs of the `bill_length_mm` variable, differentiated by the `species` of penguins. 5. Plot the ECDF of the `bill_length_mm` variable with the counts instead of proportions. 6. Plot the empirical complementary CDF of the `bill_length_mm` variable, differentiated by the `species` of penguins. Dataset: Use the `penguins` dataset, which is available in the Seaborn library. Load it using the following code: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` Expected Inputs and Outputs: 1. **Input**: No input parameters needed. Use the `penguins` dataset loaded as described. 2. **Output**: Display the plots as specified in the tasks. Constraints and Assumptions: - Ensure that the plots are displayed correctly with appropriate labels and titles for clarity. - The code should execute without errors and produce the expected visualizations. Tasks: 1. **Task 1**: Plot the ECDF of `flipper_length_mm` along the x-axis. ```python sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") ``` 2. **Task 2**: Flip the plot to show the ECDF of `flipper_length_mm` along the y-axis. ```python sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\") ``` 3. **Task 3**: Create histograms for the columns containing \\"bill_\\" in their names. ```python sns.ecdfplot(data=penguins.filter(like=\\"bill_\\", axis=\\"columns\\")) ``` 4. **Task 4**: Plot multiple ECDFs of `bill_length_mm`, differentiated by `species`. ```python sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") ``` 5. **Task 5**: Plot the ECDF of `bill_length_mm` showing counts instead of proportions. ```python sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") ``` 6. **Task 6**: Plot the empirical complementary CDF of `bill_length_mm`, differentiated by `species`. ```python sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) ``` Submit your code as a single script that, when run, generates and displays all required plots correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") def plot_ecdf_x(): sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"ECDF of Flipper Length (mm)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.show() def plot_ecdf_y(): sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\") plt.title(\\"ECDF of Flipper Length (mm)\\") plt.ylabel(\\"Flipper Length (mm)\\") plt.xlabel(\\"ECDF\\") plt.show() def plot_histograms(): penguins.filter(like=\\"bill_\\", axis=\\"columns\\").hist(bins=20, figsize=(10, 5)) plt.suptitle(\\"Histograms of Bill Measurements\\") plt.show() def plot_ecdf_hue_species(): sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Length (mm) by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.show() def plot_ecdf_counts(): sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"ECDF of Bill Length (mm) by Counts\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Count Cumulative\\") plt.show() def plot_empirical_complementary_cdf(): sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"Empirical Complementary CDF of Bill Length (mm) by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"1 - ECDF\\") plt.show()"},{"question":"# Copy-on-Write (CoW) Assessment Question Implement the following function in Python using pandas. The function should demonstrate an understanding of the Copy-on-Write (CoW) mechanism introduced in pandas 3.0. Function Signature ```python import pandas as pd def apply_discounts_to_dataframe(df: pd.DataFrame, discounts: pd.Series) -> pd.DataFrame: Applies given discounts to the \'price\' column of the input DataFrame. Parameters: df (pd.DataFrame): The input DataFrame containing a \'price\' column. discounts (pd.Series): A Series containing the discount percentages. Returns: pd.DataFrame: A new DataFrame with the updated \'price\' column, ensuring CoW principles are not violated. pass ``` Input: - `df`: A pandas DataFrame containing at least the following columns: - \'product_id\' (int) - \'price\' (float) - `discounts`: A pandas Series where the index matches \'product_id\' in the DataFrame `df`, and the values are discount percentages (float) to be applied to the respective products. Output: - A new pandas DataFrame where the \'price\' for each product is updated by applying the corresponding discount. The function should ensure that operations are CoW compliant, avoiding pitfalls like chained assignments or unintended side-effects on the original DataFrame. Constraints: - You should **not** modify the original DataFrame `df` or the original Series `discounts`. - Ensure that the function is efficient and leverages CoW principles wherever necessary. Example: ```python df = pd.DataFrame({ \'product_id\': [101, 102, 103], \'price\': [100.0, 150.0, 200.0] }) discounts = pd.Series({ 101: 10.0, # 10% discount 102: 20.0, # 20% discount }) # Invoking the function result_df = apply_discounts_to_dataframe(df, discounts) # Expected DataFrame # product_id price # 0 101 90.0 # 1 102 120.0 # 2 103 200.0 print(result_df) ``` # Explanation: 1. Create a function that accepts a DataFrame `df` and a Series `discounts`. 2. Ensure that the DataFrame returned is a new DataFrame, following CoW principles. 3. Apply the discounts to the \'price\' column based on the product IDs. 4. Ensure no modifications are made to the original DataFrame or Series.","solution":"import pandas as pd def apply_discounts_to_dataframe(df: pd.DataFrame, discounts: pd.Series) -> pd.DataFrame: Applies given discounts to the \'price\' column of the input DataFrame. Parameters: df (pd.DataFrame): The input DataFrame containing a \'price\' column. discounts (pd.Series): A Series containing the discount percentages. Returns: pd.DataFrame: A new DataFrame with the updated \'price\' column, ensuring CoW principles are not violated. # Make a copy of the DataFrame to ensure CoW principles new_df = df.copy() # Ensure that the discounts are applied only to the matching product_id for product_id, discount in discounts.items(): new_df.loc[new_df[\'product_id\'] == product_id, \'price\'] *= (1 - discount / 100) return new_df"},{"question":"# URL Normalization Function **Objective:** You are required to implement a URL normalization function that takes a given URL string and returns a normalized version of the URL. The normalization process includes removing unnecessary delimiters, ensuring consistent case for schemes and hostnames, quoting special characters where appropriate, and ensuring the URL is absolute given a base URL if necessary. **Function Signature:** ```python def normalize_url(url: str, base_url: str = \'\') -> str: pass ``` **Expected Input and Output:** 1. `url` (string): The URL to normalize. 2. `base_url` (string, optional): The base URL to be used if the provided URL is relative. Default is an empty string. Returns: - A normalized URL as a string. **Normalization Rules:** 1. If the URL is relative and a `base_url` is provided, combine the URL with the `base_url` to form an absolute URL. 2. Normalize the scheme to lowercase (e.g., \'HTTP\' should become \'http\'). 3. Normalize the hostname to lowercase. 4. Remove default ports (e.g., \':80\' for HTTP and \':443\' for HTTPS if present in the netloc). 5. Encode spaces and special characters using percent encoding (should handle path and query components appropriately). **Constraints:** 1. The function should handle both absolute and relative URLs. 2. Use `urllib.parse` module functions where appropriate. **Examples:** ```python # Example 1 input_url = \'HTTP://www.Example.com:80/a b?x=y#fragment\' output = normalize_url(input_url) # Expected output: \'http://www.example.com/a%20b?x=y#fragment\' # Example 2 input_url = \'/a b?x=y#fragment\' base = \'http://www.example.com:80/path\' output = normalize_url(input_url, base) # Expected output: \'http://www.example.com/a%20b?x=y#fragment\' # Example 3 input_url = \'https://www.Example.com:443/a b?x=y#fragment\' output = normalize_url(input_url) # Expected output: \'https://www.example.com/a%20b?x=y#fragment\' ``` **Additional Notes:** - Special characters need to be quoted using `urllib.parse.quote` or `urllib.parse.quote_plus` where appropriate. - Use `urllib.parse.urlparse`, `urllib.parse.urlunparse`, `urllib.parse.urljoin`, and other relevant functions from the `urllib.parse` module. Happy coding!","solution":"from urllib.parse import urlparse, urlunparse, quote, urljoin def normalize_url(url: str, base_url: str = \'\') -> str: if base_url: url = urljoin(base_url, url) parsed_url = urlparse(url) # Normalize scheme to lowercase scheme = parsed_url.scheme.lower() # Normalize netloc netloc = parsed_url.netloc.lower() # Remove default ports if (scheme == \'http\' and netloc.endswith(\':80\')): netloc = netloc[:-3] elif (scheme == \'https\' and netloc.endswith(\':443\')): netloc = netloc[:-4] # Encode special characters in path and query path = quote(parsed_url.path, safe=\\"/\\") query = quote(parsed_url.query, safe=\\"=&\\") normalized_url = urlunparse((scheme, netloc, path, parsed_url.params, query, parsed_url.fragment)) return normalized_url"},{"question":"# PyTorch Coding Assessment Task Initialize the weights of a neural network using different PyTorch initialization routines and write a function to validate the initialization. Description You are given a custom neural network model implemented using PyTorch. Your task is to write a function that initializes the weights of the network using various initialization methods. The function should take the model and an initialization method as input and initialize the weights accordingly. Neural Network Model ```python import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.fc1 = nn.Linear(32 * 6 * 6, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = x.view(-1, 32 * 6 * 6) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x ``` Initialization Function Implement the `initialize_weights` function that takes the model and the initialization method as input and initializes the weights accordingly. ```python def initialize_weights(model, init_method): # Initialize the weights of the model using the specified initialization method. pass ``` Requirements - You must initialize the weights for all `Conv2d` and `Linear` layers in the model. - You should implement the following initialization methods: - **\'uniform\'**: Use `torch.nn.init.uniform_`. - **\'normal\'**: Use `torch.nn.init.normal_`. - **\'xavier_uniform\'**: Use `torch.nn.init.xavier_uniform_`. - **\'xavier_normal\'**: Use `torch.nn.init.xavier_normal_`. - **\'kaiming_uniform\'**: Use `torch.nn.init.kaiming_uniform_`. - **\'kaiming_normal\'**: Use `torch.nn.init.kaiming_normal_`. Input - `model`: An instance of `CustomNet`. - `init_method`: A string representing the initialization method. It can be one of the following: `\'uniform\'`, `\'normal\'`, `\'xavier_uniform\'`, `\'xavier_normal\'`, `\'kaiming_uniform\'`, `\'kaiming_normal\'`. Output - The function should initialize the weights in place and return `None`. Example Usage ```python model = CustomNet() initialize_weights(model, \'xavier_normal\') ``` # Function Implementation ```python def initialize_weights(model, init_method): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.Linear)): if init_method == \'uniform\': torch.nn.init.uniform_(m.weight) elif init_method == \'normal\': torch.nn.init.normal_(m.weight) elif init_method == \'xavier_uniform\': torch.nn.init.xavier_uniform_(m.weight) elif init_method == \'xavier_normal\': torch.nn.init.xavier_normal_(m.weight) elif init_method == \'kaiming_uniform\': torch.nn.init.kaiming_uniform_(m.weight, nonlinearity=\'relu\') elif init_method == \'kaiming_normal\': torch.nn.init.kaiming_normal_(m.weight, nonlinearity=\'relu\') else: raise ValueError(f\\"Unsupported initialization method: {init_method}\\") if m.bias is not None: torch.nn.init.constant_(m.bias, 0.0) ``` Use this function to initialize the weights of your `CustomNet` model according to the specified method.","solution":"import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.fc1 = nn.Linear(32 * 6 * 6, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = x.view(-1, 32 * 6 * 6) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def initialize_weights(model, init_method): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.Linear)): if init_method == \'uniform\': torch.nn.init.uniform_(m.weight) elif init_method == \'normal\': torch.nn.init.normal_(m.weight) elif init_method == \'xavier_uniform\': torch.nn.init.xavier_uniform_(m.weight) elif init_method == \'xavier_normal\': torch.nn.init.xavier_normal_(m.weight) elif init_method == \'kaiming_uniform\': torch.nn.init.kaiming_uniform_(m.weight, nonlinearity=\'relu\') elif init_method == \'kaiming_normal\': torch.nn.init.kaiming_normal_(m.weight, nonlinearity=\'relu\') else: raise ValueError(f\\"Unsupported initialization method: {init_method}\\") if m.bias is not None: torch.nn.init.constant_(m.bias, 0.0)"},{"question":"# Python Coding Assessment Question Objective Demonstrate your understanding of the `pickletools` module by analyzing and optimizing a given pickled data string. Question You are given a pickled string representing a complex Python object. Your task is to: 1. Disassemble the pickled string and output the symbolic representation. 2. Analyze the opcode sequence to identify and explain the elements of the original Python object. 3. Optimize the pickled string to eliminate unused \\"PUT\\" opcodes and compare the original and optimized pickle lengths. Requirements: - Implement a function `analyze_and_optimize_pickle(pickled_data: bytes) -> Tuple[str, List[str], int]`. - The function should take the following input: - `pickled_data`: a bytes object representing the pickled string. - The function should return a tuple containing: - A string containing the symbolic disassembly representation. - A list of strings, each explaining a part of the original Python object based on the opcode sequence. - An integer representing the length difference between the original and optimized pickled strings. Constraints: - The pickled data is guaranteed to be a valid pickled string. - Use the `pickletools.dis()` function to disassemble the pickled data. - Use the `pickletools.genops()` function for analyzing the opcodes. - Use the `pickletools.optimize()` function to optimize the pickled string. Example ```python import pickle import pickletools from typing import List, Tuple def analyze_and_optimize_pickle(pickled_data: bytes) -> Tuple[str, List[str], int]: output = [] # Disassemble the pickled data disassembly = [] dis_out = StringIO() pickletools.dis(pickled_data, out=dis_out) disassembly = dis_out.getvalue() # Analyze the opcodes and explain the structure of the original object explanation = [] for opcode, arg, pos in pickletools.genops(pickled_data): explanation.append(f\\"Opcode at {pos}: {opcode.name} with argument {arg}\\") # Optimize the pickled string optimized_pickle = pickletools.optimize(pickled_data) length_difference = len(pickled_data) - len(optimized_pickle) return disassembly, explanation, length_difference # Example usage example_data = pickle.dumps({\\"name\\": \\"Alice\\", \\"age\\": 30}) result = analyze_and_optimize_pickle(example_data) print(result) ``` # Notes: - Ensure that the output clearly separates the disassembled string, explanations, and length differences. - Test the provided implementation with various complex Python objects like nested dictionaries, lists, tuples, and custom objects to ensure completeness.","solution":"import pickle import pickletools from io import StringIO from typing import List, Tuple def analyze_and_optimize_pickle(pickled_data: bytes) -> Tuple[str, List[str], int]: # Disassemble the pickled data dis_out = StringIO() pickletools.dis(pickled_data, out=dis_out) disassembly = dis_out.getvalue() # Analyze the opcodes and explain the structure of the original object explanation = [] for opcode, arg, pos in pickletools.genops(pickled_data): explanation.append(f\\"Opcode at {pos}: {opcode.name} with argument {repr(arg)}\\") # Optimize the pickled string optimized_pickle = pickletools.optimize(pickled_data) length_difference = len(pickled_data) - len(optimized_pickle) return disassembly, explanation, length_difference"},{"question":"**Serialization and Deserialization of Nested Data Structures** In this problem, you are required to implement a function that serializes and deserializes nested Python data structures using the `marshal` module. The data structure will include a combination of supported types such as dictionaries, lists, and tuples, which may contain other dictionaries, lists, and tuples within themselves. Your functions will need to handle marshalling and unmarshalling gracefully, raising appropriate exceptions when encountering unsupported data types or invalid inputs. # Function 1: `marshal_data` ```python def marshal_data(data, filename): Serializes a nested Python data structure and writes it to a file. Parameters: data (any): A nested data structure containing supported data types. filename (str): The name of the binary file to write the serialized data to. Raises: ValueError: If the data contains types that are not supported by the marshal module. TypeError: If the filename is not a string. pass ``` # Function 2: `unmarshal_data` ```python def unmarshal_data(filename): Deserializes data from a file back into a nested Python data structure. Parameters: filename (str): The name of the binary file to read the serialized data from. Returns: any: The deserialized Python data structure. Raises: EOFError: If the file is empty or the data cannot be read. ValueError: If the data contains unsupported types. TypeError: If the filename is not a string. pass ``` # Requirements: 1. Use `marshal.dump` and `marshal.dumps` for serialization. 2. Use `marshal.load` and `marshal.loads` for deserialization. 3. The functions should be able to handle deep nesting of data structures. 4. Proper exceptions should be raised for unsupported types or invalid inputs. 5. The `marshal_data` function should catch and handle any `ValueError` raised due to unsupported data types and provide a meaningful error message. # Example: ```python # Example usage: data = { \'key1\': [1, 2, {\'innerKey\': (3, 4)}], \'key2\': (\'a\', \'b\', \'c\'), } filename = \'test_marshall.dat\' # Serialize the data marshal_data(data, filename) # Deserialize the data result = unmarshal_data(filename) print(result) # Should output: {\'key1\': [1, 2, {\'innerKey\': (3, 4)}], \'key2\': (\'a\', \'b\', \'c\')} ``` # Notes: - Ensure that both functions check the validity of the input parameters. - Write unit tests to cover different nested structures, including edge cases like empty dictionaries, lists, and tuples.","solution":"import marshal def marshal_data(data, filename): Serializes a nested Python data structure and writes it to a file. Parameters: data (any): A nested data structure containing supported data types. filename (str): The name of the binary file to write the serialized data to. Raises: ValueError: If the data contains types that are not supported by the marshal module. TypeError: If the filename is not a string. if not isinstance(filename, str): raise TypeError(\\"Filename must be a string\\") try: with open(filename, \'wb\') as file: marshal.dump(data, file) except ValueError as e: raise ValueError(f\\"Unsupported data type encountered: {e}\\") def unmarshal_data(filename): Deserializes data from a file back into a nested Python data structure. Parameters: filename (str): The name of the binary file to read the serialized data from. Returns: any: The deserialized Python data structure. Raises: EOFError: If the file is empty or the data cannot be read. ValueError: If the data contains unsupported types. TypeError: If the filename is not a string. if not isinstance(filename, str): raise TypeError(\\"Filename must be a string\\") try: with open(filename, \'rb\') as file: return marshal.load(file) except EOFError as e: raise EOFError(f\\"Cannot read data: {e}\\") except ValueError as e: raise ValueError(f\\"Unsupported data type encountered: {e}\\")"},{"question":"# PyTorch Coding Assessment Objective Write a function using PyTorch that demonstrates the use of `vmap` and `grad` transform. The goal is to compute the gradient of a batched function and ensure it adheres to the constraints and best practices mentioned in the torch.func documentation. Question You are given a function `batched_function` that applies some mathematical operations on a batch of input tensors. However, your task is to transform this function so that: 1. It easily computes the gradient with respect to its inputs. 2. The function uses vectorized mapping (`vmap`) to process inputs more efficiently without using loops. 3. It avoids in-place operations and conforms to the limitations imposed by `vmap`. Implement a function `compute_vmap_grad` which accepts a batched tensor and computes the gradient for each input in the batch. Function Signature ```python import torch from torch.func import vmap, grad def compute_vmap_grad(batched_input: torch.Tensor) -> torch.Tensor: Computes the gradient of a batched function with respect to its inputs. Parameters: batched_input (torch.Tensor): A batched tensor (N x D) where N is the batch size, and D is the dimension. Returns: torch.Tensor: A tensor of shape (N x D) representing the gradients of the function with respect to each input in the batch. pass ``` Example ```python import torch # Example Batched Input batched_input = torch.tensor([ [0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9] ], requires_grad=True) # Expected Output: Gradients for each input in the batch output_grads = compute_vmap_grad(batched_input) print(output_grads) ``` Constraints - Do not use any in-place operations like `add_` or `mul_`. - Avoid using the `out=` keyword in any PyTorch operations. - Ensure that the `vmap` correctly processes the batch without raising any shape-related errors. - The function should also handle the random operations appropriately using `randomness` flags as described.","solution":"import torch from torch.func import vmap, grad def compute_vmap_grad(batched_input: torch.Tensor) -> torch.Tensor: Computes the gradient of a batched function with respect to its inputs. Parameters: batched_input (torch.Tensor): A batched tensor (N x D) where N is the batch size, and D is the dimension. Returns: torch.Tensor: A tensor of shape (N x D) representing the gradients of the function with respect to each input in the batch. # Define a simple function for demonstration. def single_function(x): return x.pow(2).sum() # Compute the gradient of the single function. grad_fn = grad(single_function) # Vectorize the gradient function across the batch using vmap. batched_grad_fn = vmap(grad_fn) # Compute the gradients for the batched input. return batched_grad_fn(batched_input)"},{"question":"# Question: Create a Custom Color Palette and Visualization using Seaborn Objective Write a function `create_custom_palette_and_plot(data)` that generates a custom color palette and uses it in a seaborn line plot. Input - `data`: A Pandas DataFrame with at least two columns: \'x\' and \'y\'. Column \'x\' will be used as the x-axis values, and column \'y\' for y-axis values. Function Specifications 1. The function should create a custom color palette that blends between at least three different colors. 2. Use the custom palette created to plot the \'x\' vs \'y\' values from the given DataFrame using seaborn\'s lineplot function. 3. The resulting plot should display the custom color palette clearly. Constraints - For simplicity, assume that the DataFrame provided will always have numeric values in the \'x\' and \'y\' columns. - The DataFrame will have at least 10 rows of data. Output - A seaborn line plot where the plotted line uses the custom color palette created. Example Here is an example of a DataFrame `data` that you might receive: ```python x y 0 1 5 1 2 6 2 3 7 3 4 8 4 5 5 5 6 4 6 7 3 7 8 2 8 9 3 9 10 5 ``` An example implementation in Python might look as follows: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_palette_and_plot(data): # Create a custom color palette colors = [\\"#4c72b0\\", \\"#55a868\\", \\"#c44e52\\"] # You can choose your colors custom_palette = sns.blend_palette(colors, as_cmap=True) # Plot using seaborn with the custom palette plt.figure(figsize=(10, 6)) sns.lineplot(x=\'x\', y=\'y\', data=data, palette=custom_palette) plt.show() # Example usage: data = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'y\': [5, 6, 7, 8, 5, 4, 3, 2, 3, 5] }) create_custom_palette_and_plot(data) ``` In this example, the function `create_custom_palette_and_plot` creates a custom color palette and uses it to plot a line graph for the provided data. Note Ensure that you have seaborn and matplotlib installed in your environment to run this function properly. ```sh pip install seaborn matplotlib ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_palette_and_plot(data): Creates a custom color palette and uses it to plot a seaborn lineplot. Parameters: data (pd.DataFrame): DataFrame containing at least two columns: \'x\' and \'y\' for plotting. # Create a custom color palette colors = [\\"#4c72b0\\", \\"#55a868\\", \\"#c44e52\\"] # You can choose your colors custom_palette = sns.blend_palette(colors, as_cmap=True) # Plot using seaborn with the custom palette plt.figure(figsize=(10, 6)) sns.lineplot(x=\'x\', y=\'y\', data=data, palette=custom_palette) plt.show()"},{"question":"**Coding Assessment Question** # Problem Statement You are given a file containing several chunks in the EA IFF 85 format. Your task is to implement a function `process_chunks(file_path: str) -> List[Tuple[str, int]]` that reads the chunks from the file and returns a list of tuples, where each tuple contains the chunk ID and the chunk size. # Function Signature ```python def process_chunks(file_path: str) -> List[Tuple[str, int]]: pass ``` # Input - `file_path` (str): The path to the file containing EA IFF 85 chunks. # Output - Returns a list of tuples. Each tuple should contain: - The chunk ID (a 4-byte string) - The chunk size (an integer representing the size of the chunk data) # Requirements 1. Read chunks from the file until the end of the file is reached. 2. For each chunk, retrieve the chunk ID and chunk size. 3. The function should handle chunks with both aligned (2-byte boundary) and non-aligned formats based on the file content. 4. Manage the file operations efficiently to ensure proper reading and skipping of chunks. # Constraints - You must use the `chunk.Chunk` class from the `chunk` module to read the chunks. - You may assume that the file is well-formed and contains valid EA IFF 85 chunks. - Implement appropriate error handling to manage any I/O exceptions or EOF conditions. # Example Usage Assume the file contains the following chunk data (in binary): ``` +-----------+----------+---------------------------------+ | Offset | Length | Contents | |===========|==========|=================================| | 0 | 4 | \'FORM\' | | 4 | 4 | 12 (0x0C in big-endian) | | 8 | 12 | \'ILBMFORMDATA\' | +-----------+----------+---------------------------------+ | 20 | 4 | \'CHNK\' | | 24 | 4 | 8 (0x08 in big-endian) | | 28 | 8 | \'DATAONLY\' | +-----------+----------+---------------------------------+ ``` The function should return: ```python [(\'FORM\', 12), (\'CHNK\', 8)] ``` # Note - Use the provided methods of the `chunk.Chunk` class to read, seek, and manage the file data. - Pay attention to endianness and the proper interpretation of chunk headers and data.","solution":"import chunk from typing import List, Tuple def process_chunks(file_path: str) -> List[Tuple[str, int]]: Reads chunks from a file and returns a list of tuples containing chunk ID and chunk size. :param file_path: str - Path to the file containing EA IFF 85 chunks. :return: List of tuples with chunk ID and chunk size chunks = [] try: # Open the file in binary read mode with open(file_path, \'rb\') as f: while True: try: # Read the chunk ch = chunk.Chunk(f, bigendian=True, align=True) # Append the (chunk ID, chunk size) tuple chunks.append((ch.getname().decode(\'utf-8\'), ch.getsize())) # Skip the chunk data ch.skip() except EOFError: # Reached end of file break except IOError as e: print(f\'Error reading file {file_path}: {e}\') return chunks"},{"question":"Advanced Python Zip Applications with `zipapp` Objective Your task is to implement a function that automates the creation of Python zip applications with specific requirements using the `zipapp` module. Problem Statement Write a function `create_python_zip_application(source_directory, output_file, interpreter, main_callable, include_extensions=None, compressed=False)` that creates an executable zip archive from a given source directory. The function should: 1. Include only files with the specified extensions in the archive. 2. Add a shebang line with the specified interpreter. 3. Specify a main callable as the entry point. 4. Optionally compress the archive. Function Signature ```python def create_python_zip_application(source_directory: str, output_file: str, interpreter: str, main_callable: str, include_extensions: list = None, compressed: bool = False): pass ``` Parameters - `source_directory` (str): Path to the directory containing the Python code to be archived. - `output_file` (str): Path to the output zip archive file. - `interpreter` (str): The Python interpreter to be specified in the shebang line. - `main_callable` (str): The callable to be used as the main entry point of the form `pkg.module:callable`. - `include_extensions` (list, optional): A list of file extensions to include. If None, include all files. - `compressed` (bool): If True, compress files in the archive. Defaults to False. Return None Constraints - The `source_directory` must be a valid directory containing Python files. - The `output_file` must end with the `.pyz` extension. - The `interpreter` must be a valid Python interpreter path. - The `main_callable` must be in the form `pkg.module:callable`. Example ```python source_directory = \\"myapp\\" output_file = \\"myapp.pyz\\" interpreter = \\"/usr/bin/env python3\\" main_callable = \\"myapp:main\\" include_extensions = [\'.py\', \'.txt\'] compressed = True create_python_zip_application(source_directory, output_file, interpreter, main_callable, include_extensions, compressed) ``` This example should create a compressed zip archive `myapp.pyz` from the `myapp` directory, including only `.py` and `.txt` files, with a shebang line for `/usr/bin/env python3`, and setting `myapp:main` as the entry point. Notes 1. You may use the `zipapp` module for creating the archive. 2. Ensure to handle exceptions and edge cases, such as invalid directory paths or incorrect file extensions.","solution":"import os import zipapp from pathlib import Path def create_python_zip_application(source_directory: str, output_file: str, interpreter: str, main_callable: str, include_extensions: list = None, compressed: bool = False): Creates an executable zip archive from the given source directory. Parameters: - source_directory (str): Path to the directory containing the Python code to be archived. - output_file (str): Path to the output zip archive file. - interpreter (str): The Python interpreter to be specified in the shebang line. - main_callable (str): The callable to be used as the main entry point of the form pkg.module:callable. - include_extensions (list, optional): A list of file extensions to include. If None, include all files. - compressed (bool): If True, compress files in the archive. Defaults to False. if not os.path.isdir(source_directory): raise ValueError(f\\"Invalid source directory: {source_directory}\\") if not output_file.endswith(\'.pyz\'): raise ValueError(f\\"Output file must end with \'.pyz\': {output_file}\\") # Filter the source files based on extensions if include_extensions is not None: include_extensions = set(include_extensions) def filter_func(path): return path.suffix in include_extensions else: filter_func = None # Create the zip application zipapp.create_archive( source=source_directory, target=output_file, interpreter=interpreter, main=main_callable, filter=filter_func, compressed=compressed )"},{"question":"# Python Keyword and Soft Keyword Checker You have been provided with the \\"keyword\\" module that allows a Python program to determine if a string is a keyword or a soft keyword. Your task is to implement a function that classifies a list of strings into three categories: 1. **Keywords**: Strings that are Python keywords. 2. **Soft Keywords**: Strings that are Python soft keywords but not standard keywords. 3. **Identifiers**: Strings that are neither Python keywords nor soft keywords. # Function Signature ```python def classify_keywords(strings: List[str]) -> Tuple[List[str], List[str], List[str]]: ``` # Input - `strings` (List[str]): A list of strings to be classified. # Output - Returns a tuple of three lists: 1. List of strings that are Python keywords. 2. List of strings that are Python soft keywords but not standard keywords. 3. List of strings that are neither Python keywords nor soft keywords (identifiers). # Constraints - The input list will have at most 1000 strings. - Each string will contain only lowercase English letters and underscores (`_`). - Each string will have a length between 1 and 50 characters. # Example ```python from keyword import iskeyword, issoftkeyword # Example input strings = [\\"if\\", \\"match\\", \\"function\\", \\"async\\", \\"await\\"] # Expected output # ([\'if\', \'async\', \'await\'], [\'match\'], [\'function\']) ``` # Note - Use the `iskeyword` function to check if a string is a Python keyword. - Use the `issoftkeyword` function to check if a string is a Python soft keyword. # Constraints and Performance Requirements - Your implementation should efficiently handle the list of strings. The expected time complexity is O(n), where n is the number of strings in the input list. - Ensure clear classification and avoid any overlaps (a string fitting into more than one category). # Implementation Implement the `classify_keywords` function to solve the problem as described above.","solution":"from keyword import iskeyword, issoftkeyword from typing import List, Tuple def classify_keywords(strings: List[str]) -> Tuple[List[str], List[str], List[str]]: keywords = [] soft_keywords = [] identifiers = [] for string in strings: if iskeyword(string): keywords.append(string) elif issoftkeyword(string): soft_keywords.append(string) else: identifiers.append(string) return (keywords, soft_keywords, identifiers)"},{"question":"# Pandas Coding Assessment **Objective:** The purpose of this task is to evaluate your ability to use the `pandas` library, specifically the `DataFrame` class. Your task involves constructing dataframes, performing transformations, handling missing data, and generating summary statistics. **Task:** You are provided with two CSV files containing sales data for a retail company. Your goal is to perform the following operations using pandas: 1. **Load datasets**: Load the two CSV files into pandas dataframes. 2. **Merge Datasets**: Merge these two dataframes on a common column. 3. **Clean Data**: Handle missing values in the dataset with appropriate strategies. 4. **Filter Data**: Filter the dataset to include only specific data ranges and conditions. 5. **Group and Aggregate**: Group data by certain columns and calculate aggregate metrics. 6. **Generate Summary Statistics**: Calculate and display summary statistics for specified columns. 7. **Export Results**: Save the resulting dataframe to a new CSV file. Input: - Two CSV file paths as strings (e.g., `\\"sales_data_1.csv\\"`, `\\"sales_data_2.csv\\"`). Expected Operations and Output: 1. Load the CSV files into pandas dataframes. 2. Merge the dataframes using the `merge` function on a specified common column (e.g., `ProductID`). 3. Handle missing values: - If a column contains numerical data, fill missing values with the mean of that column. - If a column contains categorical data, fill missing values with the mode of that column. 4. Filter the dataframe where: - The `Sales` column is greater than a threshold value (e.g., `500`). - The `Date` column falls within a specified range (e.g., from `2022-01-01` to `2022-12-31`). 5. Group the dataframe by a specified column (e.g., `ProductCategory`) and calculate the following aggregate metrics: - Total sales - Average sales 6. Generate a summary statistics report using the `describe` function for specified columns. 7. Save the resulting dataframe to a new CSV file (e.g., `cleaned_sales_data.csv`). Constraints: - You may assume that the CSV files are correctly formatted. - Use appropriate pandas functions and methods to achieve the tasks. - Performance requirements: The operations should be optimized for efficiency considering typical size dataframes that fit in memory. Example Code Structure: ```python import pandas as pd def process_sales_data(file1, file2): # Step 1: Load the datasets df1 = pd.read_csv(file1) df2 = pd.read_csv(file2) # Step 2: Merge the datasets df_merged = pd.merge(df1, df2, on=\'ProductID\') # Step 3: Handle missing values for column in df_merged.columns: if df_merged[column].dtype == \'object\': df_merged[column].fillna(df_merged[column].mode()[0], inplace=True) else: df_merged[column].fillna(df_merged[column].mean(), inplace=True) # Step 4: Filter the data df_filtered = df_merged[(df_merged[\'Sales\'] > 500) & (df_merged[\'Date\'] >= \'2022-01-01\') & (df_merged[\'Date\'] <= \'2022-12-31\')] # Step 5: Group and aggregate df_grouped = df_filtered.groupby(\'ProductCategory\').agg(Total_Sales=(\'Sales\', \'sum\'), Average_Sales=(\'Sales\', \'mean\')).reset_index() # Step 6: Generate summary statistics summary_stats = df_filtered.describe() # Step 7: Export the results df_filtered.to_csv(\'cleaned_sales_data.csv\', index=False) return df_grouped, summary_stats # Example usage: # df_grouped, summary_stats = process_sales_data(\'sales_data_1.csv\', \'sales_data_2.csv\') ``` Implement the specified operations within the `process_sales_data` function and ensure it returns the grouped dataframe and summary statistics.","solution":"import pandas as pd def process_sales_data(file1, file2): # Step 1: Load the datasets df1 = pd.read_csv(file1) df2 = pd.read_csv(file2) # Step 2: Merge the datasets df_merged = pd.merge(df1, df2, on=\'ProductID\') # Step 3: Handle missing values for column in df_merged.columns: if df_merged[column].dtype == \'object\': df_merged[column].fillna(df_merged[column].mode()[0], inplace=True) else: df_merged[column].fillna(df_merged[column].mean(), inplace=True) # Step 4: Filter the data df_filtered = df_merged[(df_merged[\'Sales\'] > 500) & (df_merged[\'Date\'] >= \'2022-01-01\') & (df_merged[\'Date\'] <= \'2022-12-31\')] # Step 5: Group and aggregate df_grouped = df_filtered.groupby(\'ProductCategory\').agg(Total_Sales=(\'Sales\', \'sum\'), Average_Sales=(\'Sales\', \'mean\')).reset_index() # Step 6: Generate summary statistics summary_stats = df_filtered.describe() # Step 7: Export the results df_filtered.to_csv(\'cleaned_sales_data.csv\', index=False) return df_grouped, summary_stats"},{"question":"# HTTP Client Assessment Task Objective You are required to implement a Python function `perform_http_operation` that leverages the `http.client` module to interact with a specified HTTP server by sending a POST request and processing the response. Task Description 1. Define a function `perform_http_operation(server: str, port: int, endpoint: str, data: dict, headers: dict) -> dict`. 2. The function should: - Establish an HTTPS connection to the specified server and port. - Send a POST request to the given endpoint with the provided data and headers. - Handle the response by checking the status code and either collecting the response data or handling errors. - Return a dictionary with keys `status`, `reason`, and `data` containing the response status code, reason phrase, and response body respectively. Function Signature ```python import http.client import urllib.parse def perform_http_operation(server: str, port: int, endpoint: str, data: dict, headers: dict) -> dict: pass ``` Input - `server` (str): The server to connect to (e.g., \\"www.example.com\\"). - `port` (int): The port to connect to on the server. Default HTTPS port is 443. - `endpoint` (str): The specific endpoint on the server to send the request to (e.g., \\"/api/data\\"). - `data` (dict): A dictionary containing the data to be sent in the POST request body. - `headers` (dict): A dictionary of additional HTTP headers to be sent with the request. Output - `result` (dict): A dictionary containing the response: - `status` (int): The HTTP status code of the response. - `reason` (str): The reason phrase accompanying the status code. - `data` (str): The response body as a string. Example Usage ```python server = \\"www.example.com\\" port = 443 endpoint = \\"/submit\\" data = {\\"username\\": \\"testuser\\", \\"password\\": \\"testpass\\"} headers = {\\"Content-type\\": \\"application/x-www-form-urlencoded\\", \\"Accept\\": \\"application/json\\"} response = perform_http_operation(server, port, endpoint, data, headers) print(response) ``` Constraints - You must handle possible errors, such as invalid URLs, connection failures, and server-side errors. - Use the `http.client` module for HTTP connection and communication. - The function should handle chunked transfer encoding if required by the response. - Ensure you close the connection after completing the operation. Considerations - Review the `http.client.HTTPConnection` and `http.client.HTTPSConnection` classes for methods relevant to establishing connections, sending requests, and reading responses. - Properly encode the POST data using `urllib.parse.urlencode`.","solution":"import http.client import urllib.parse import json def perform_http_operation(server: str, port: int, endpoint: str, data: dict, headers: dict) -> dict: try: conn = http.client.HTTPSConnection(server, port) post_data = urllib.parse.urlencode(data) conn.request(\\"POST\\", endpoint, post_data, headers) response = conn.getresponse() resp_status = response.status resp_reason = response.reason resp_data = response.read().decode() conn.close() return { \\"status\\": resp_status, \\"reason\\": resp_reason, \\"data\\": resp_data } except Exception as e: return { \\"status\\": None, \\"reason\\": str(e), \\"data\\": None }"},{"question":"**Question: Implementing and Utilizing Experimental Attention Module in PyTorch** In this coding assessment, you are required to use PyTorch\'s experimental attention API to implement a custom neural network layer for sequence modeling tasks. Attention mechanisms are powerful tools used in many neural network architectures to focus on different parts of the input sequence, and they are particularly useful in tasks like language modeling, machine translation, and more. # Task: 1. **Implement an Attention Layer**: - Create a class `CustomAttention` that inherits from `torch.nn.Module`. - This class should have an `__init__` method that initializes the necessary attention components. - Implement the forward method to define the forward pass of the attention mechanism using PyTorch\'s experimental attention API. 2. **Create and Train a Model**: - Define a small neural network architecture that utilizes the `CustomAttention` layer. - Write code for training this network on a dummy dataset (you can create a synthetic dataset). - Validate its performance by calculating the training loss. # Specifications: - Define the `CustomAttention` class with the following methods: - `__init__(self, input_dim, attention_dim, output_dim)` - `input_dim` is the dimension of the input features. - `attention_dim` is the dimension of the attention mechanism. - `output_dim` is the dimension of the output features. - `forward(self, x)` - `x` is the input tensor of shape `(batch_size, sequence_length, input_dim)`. - Implement a simple feedforward neural network that includes your custom attention layer. - Train the network using a synthetic dataset: - For simplicity, create a dataset of random tensors of shape `(100, 10, input_dim)` and corresponding output tensors of shape `(100, output_dim)`. # Input and Output Formats: - **Input**: - `input_dim`, `attention_dim`, and `output_dim` are integers defining the dimensions. - A synthetic dataset of input and output pairs for training. - **Output**: - Print the loss after training the network for 10 epochs. # Constraints: - Use PyTorch\'s experimental attention API within your custom attention layer. - Ensure your code is compatible with PyTorch version >= 1.9. # Example: ```python import torch import torch.nn as nn import torch.optim as optim from torch.nn.attention.experimental import ... # Include the necessary imports from the experimental API # Define the CustomAttention class class CustomAttention(nn.Module): def __init__(self, input_dim, attention_dim, output_dim): super(CustomAttention, self).__init__() # Initialize attention components def forward(self, x): # Define the forward pass pass # Define the neural network model class SimpleModel(nn.Module): def __init__(self, input_dim, attention_dim, output_dim): super(SimpleModel, self).__init__() self.attention = CustomAttention(input_dim, attention_dim, output_dim) self.fc = nn.Linear(output_dim, output_dim) def forward(self, x): x = self.attention(x) x = self.fc(x) return x # Create synthetic dataset input_dim = 32 attention_dim = 16 output_dim = 32 x_train = torch.randn(100, 10, input_dim) y_train = torch.randn(100, output_dim) # Initialize and train the model model = SimpleModel(input_dim, attention_dim, output_dim) criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) for epoch in range(10): optimizer.zero_grad() output = model(x_train) loss = criterion(output, y_train) loss.backward() optimizer.step() print(f\'Epoch {epoch + 1}, Loss: {loss.item()}\') ``` In this example, you will need to fill in the missing parts for the `CustomAttention` class utilizing the experimental attention API from PyTorch. Ensure that your implementation is efficient and pythonic.","solution":"import torch import torch.nn as nn class CustomAttention(nn.Module): def __init__(self, input_dim, attention_dim, output_dim): super(CustomAttention, self).__init__() self.query = nn.Linear(input_dim, attention_dim) self.key = nn.Linear(input_dim, attention_dim) self.value = nn.Linear(input_dim, attention_dim) self.out = nn.Linear(attention_dim, output_dim) def forward(self, x): # x : (batch_size, sequence_length, input_dim) Q = self.query(x) # (batch_size, sequence_length, attention_dim) K = self.key(x) # (batch_size, sequence_length, attention_dim) V = self.value(x) # (batch_size, sequence_length, attention_dim) scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5) # (batch_size, sequence_length, sequence_length) attention_weights = torch.nn.functional.softmax(scores, dim=-1) # (batch_size, sequence_length, sequence_length) attended_values = torch.matmul(attention_weights, V) # (batch_size, sequence_length, attention_dim) out = self.out(attended_values) # (batch_size, sequence_length, output_dim) return out class SimpleModel(nn.Module): def __init__(self, input_dim, attention_dim, output_dim): super(SimpleModel, self).__init__() self.attention = CustomAttention(input_dim, attention_dim, output_dim) self.fc = nn.Linear(output_dim, output_dim) def forward(self, x): x = self.attention(x) x = self.fc(x) return x # Example usage with synthetic dataset: def main(): input_dim = 32 attention_dim = 16 output_dim = 32 x_train = torch.randn(100, 10, input_dim) y_train = torch.randn(100, 10, output_dim) model = SimpleModel(input_dim, attention_dim, output_dim) criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) for epoch in range(10): optimizer.zero_grad() output = model(x_train) loss = criterion(output, y_train) loss.backward() optimizer.step() print(f\'Epoch {epoch + 1}, Loss: {loss.item()}\') if __name__ == \\"__main__\\": main()"},{"question":"# Concurrent Execution Using Threads and Processes Objective: Implement a system to simulate a producer-consumer problem using both threading and multiprocessing in Python 3.10. This system should demonstrate the management of concurrent tasks and shared resources. Task: 1. **Producers**: Should generate a list of numbers and place them into a shared queue. 2. **Consumers**: Should retrieve numbers from the shared queue, compute their squares, and store the results into a shared list. Requirements: 1. Implement both **threads** and **processes** for managing producers and consumers. 2. Use synchronization mechanisms (Locks, Semaphores) as needed to handle shared resources. 3. Ensure efficient and safe concurrent execution. Function Signature: ```python def producer_consumer_system(n_producers: int, n_consumers: int, items_to_produce: int) -> List[int]: Simulates a producer-consumer system using both threading and multiprocessing. Parameters: n_producers (int): Number of producer instances n_consumers (int): Number of consumer instances items_to_produce (int): Number of items each producer should generate Returns: List[int]: A list containing the squares of all numbers generated by the producers ``` Explanation: - `n_producers`: Number of producer instances that will be created. - `n_consumers`: Number of consumer instances that will be created. - `items_to_produce`: Number of items each producer should generate. Example Usage: ```python result = producer_consumer_system(n_producers=3, n_consumers=3, items_to_produce=5) print(result) # Expected output: A list with 3 * 5 = 15 squared values if each number is unique ``` Constraints: - Make use of `threading` and `multiprocessing`. - Ensure thread safety with appropriate synchronization primitives. - Handle process join to ensure all tasks are completed before compiling results. Notes: - Aim for a clean and efficient implementation. - Handle exceptions and edge cases where necessary. - Demonstrate an understanding of both threading and multiprocessing concepts, along with shared resource management.","solution":"import threading import multiprocessing from queue import Queue from multiprocessing import Manager def producer_task(shared_queue, items_to_produce, lock): for i in range(items_to_produce): with lock: shared_queue.put(i) def consumer_task(shared_queue, result_list, lock): while True: item = shared_queue.get() if item is None: break result = item * item with lock: result_list.append(result) def producer_consumer_system(n_producers: int, n_consumers: int, items_to_produce: int): # Shared resources for threads approach thread_queue = Queue() thread_result_list = [] thread_lock = threading.Lock() # Shared resources for processes approach manager = Manager() process_queue = manager.Queue() process_result_list = manager.list() process_lock = multiprocessing.Lock() # Thread producers and consumers thread_producers = [threading.Thread(target=producer_task, args=(thread_queue, items_to_produce, thread_lock)) for _ in range(n_producers)] thread_consumers = [threading.Thread(target=consumer_task, args=(thread_queue, thread_result_list, thread_lock)) for _ in range(n_consumers)] # Start thread producers for producer in thread_producers: producer.start() # Start thread consumers for consumer in thread_consumers: consumer.start() # Join thread producers for producer in thread_producers: producer.join() # Add None to queue to stop consumers for _ in range(n_consumers): thread_queue.put(None) # Join thread consumers for consumer in thread_consumers: consumer.join() # Process producers and consumers process_producers = [multiprocessing.Process(target=producer_task, args=(process_queue, items_to_produce, process_lock)) for _ in range(n_producers)] process_consumers = [multiprocessing.Process(target=consumer_task, args=(process_queue, process_result_list, process_lock)) for _ in range(n_consumers)] # Start process producers for producer in process_producers: producer.start() # Start process consumers for consumer in process_consumers: consumer.start() # Join process producers for producer in process_producers: producer.join() # Add None to queue to stop consumers for _ in range(n_consumers): process_queue.put(None) # Join process consumers for consumer in process_consumers: consumer.join() return list(thread_result_list) + list(process_result_list)"},{"question":"Objective Demonstrate your understanding of cell objects in Python310 by implementing and using them within nested functions to maintain variable state across multiple scopes. Problem Statement You are required to simulate a scenario where multiple nested functions share and modify the same variable. Implement a custom closure mechanism using cell objects and the functions described in the provided documentation. Task 1. Create a main function named `closure_simulation` that: - Creates a cell object to store an integer value. - Defines two nested functions within it: - `increment`: Increments the cell object\'s value by 1. - `get_value`: Returns the current value stored in the cell object. - Returns the two nested functions. 2. Ensure the nested functions correctly maintain and modify the shared variable using cell operations. Implementation Details - Use the cell-related functions provided (`PyCell_New`, `PyCell_Get`, `PyCell_Set`) to create and manipulate the cell object. - Avoid using global variables; mimic the closure behavior using cell objects instead. Input - No direct input is needed; the functions are created and manipulated within `closure_simulation`. Output - The function `closure_simulation` should return a tuple containing the two nested functions (`increment` and `get_value`). Constraints and Requirements - Focus on correctly implementing the cell object operations as described in the documentation. - Ensure type checking and appropriate error handling for cell operations. - Performance is not a primary concern, but your implementation should be logically sound and efficient in maintaining variable state. Example Usage ```python # Create the closure functions increment, get_value = closure_simulation() # Increment the value increment() # Value becomes 1 print(get_value()) # Output: 1 # Increment the value again increment() # Value becomes 2 print(get_value()) # Output: 2 ``` In this example, `increment` modifies the shared variable by increasing its value by 1, and `get_value` retrieves the current value of the shared variable, maintaining state across multiple calls.","solution":"def closure_simulation(): cell = [0] # using a list to simulate a cell object def increment(): cell[0] += 1 def get_value(): return cell[0] return increment, get_value"},{"question":"Coding Assessment Question # Objective You are provided with a DataFrame containing sales data. Your task is to style this DataFrame to make it visually informative and ready for presentation. Your solution should involve conditional styling, formatting specific columns, adding captions, and exporting the styled DataFrame to an HTML file. # Input 1. A DataFrame `df` with the following columns: - \'Region\': The region where sales occurred. - \'Product\': The product sold. - \'Sales\': The sales amount in dollars. - \'Quantity\': The number of units sold. - \'Date\': The date of the sale. ```python import pandas as pd data = { \'Region\': [\'North\', \'South\', \'East\', \'West\'] * 5, \'Product\': [\'A\', \'B\', \'A\', \'B\', \'A\', \'B\', \'A\', \'B\', \'A\', \'B\'] * 2, \'Sales\': [250, 450, 400, 300, 200, 500, 600, 700, 800, 300] * 2, \'Quantity\': [10, 15, 20, 10, 30, 25, 35, 20, 10, 50] * 2, \'Date\': pd.date_range(start=\'2023-01-01\', periods=20, freq=\'D\') } df = pd.DataFrame(data) ``` # Instructions 1. **Highlight Maximum and Minimum Sales**: - Use pandas\' built-in `highlight_max` and `highlight_min` functions to highlight the maximum and minimum values in the \'Sales\' column. 2. **Conditional Styling for Quantity**: - Apply a background gradient on the ‘Quantity’ column, which gradually changes color based on the quantity value. 3. **Formatting the Sales Column**: - Format the \'Sales\' column to display as currency with a dollar sign and two decimal places. 4. **Adding a Caption**: - Add a caption to the table saying, \\"Quarter 1 Sales Data\\". 5. **Export the Styled DataFrame**: - Export the styled DataFrame to an HTML file named \'styled_sales_data.html\'. # Constraints - Use pandas\' `Styler` class methods to achieve the styling. - Ensure the code is efficient and well-documented. # Output - The function should export the styled DataFrame as an HTML file named \'styled_sales_data.html\'. # Function Signature ```python def style_sales_data(df: pd.DataFrame) -> None: pass ``` # Example ```python style_sales_data(df) # This should create a file named \'styled_sales_data.html\' with the styled DataFrame. ```","solution":"import pandas as pd def style_sales_data(df: pd.DataFrame) -> None: Style the given sales DataFrame and export it as an HTML file. Parameters: df (pd.DataFrame): DataFrame containing sales data with columns \'Region\', \'Product\', \'Sales\', \'Quantity\', and \'Date\'. # Define the styler object styled_df = df.style # Highlight maximum and minimum sales styled_df = styled_df.highlight_max(subset=[\'Sales\'], color=\'lightgreen\') .highlight_min(subset=[\'Sales\'], color=\'lightcoral\') # Apply background gradient on the ‘Quantity’ column styled_df = styled_df.background_gradient(subset=[\'Quantity\'], cmap=\'viridis\') # Format the \'Sales\' column as currency styled_df = styled_df.format({\'Sales\': \'{:,.2f}\'}) # Add a caption styled_df = styled_df.set_caption(\\"Quarter 1 Sales Data\\") # Export the styled DataFrame to HTML styled_df.to_html(\'styled_sales_data.html\') # Example usage: data = { \'Region\': [\'North\', \'South\', \'East\', \'West\'] * 5, \'Product\': [\'A\', \'B\', \'A\', \'B\', \'A\', \'B\', \'A\', \'B\', \'A\', \'B\'] * 2, \'Sales\': [250, 450, 400, 300, 200, 500, 600, 700, 800, 300] * 2, \'Quantity\': [10, 15, 20, 10, 30, 25, 35, 20, 10, 50] * 2, \'Date\': pd.date_range(start=\'2023-01-01\', periods=20, freq=\'D\') } df = pd.DataFrame(data) style_sales_data(df)"},{"question":"**Advanced List Manipulation and Dictionary Comprehension** **Problem Description:** You are tasked with writing a function that processes a list of dictionaries representing student records. Each student record contains their name, age, and a list of scores. The goal is to perform several operations on these records: 1. **Filter Students by Age Range:** - Only include records of students whose age is within a given range (inclusive). 2. **Calculate Average Scores:** - For each student, calculate the average score. 3. **Sort Students:** - Sort the filtered records by average score in descending order. 4. **Format the Output:** - Return a dictionary where the keys are student names and the values are their average scores. **Function Signature:** ```python def process_student_records(records: list, min_age: int, max_age: int) -> dict: pass ``` **Input:** - `records` (list of dict): A list of dictionaries where each dictionary has the following structure: ```python { \\"name\\": str, # The student\'s name \\"age\\": int, # The student\'s age \\"scores\\": list # A list of integers representing the student\'s scores } ``` - `min_age` (int): The minimum age for filtering students. - `max_age` (int): The maximum age for filtering students. **Output:** - A dictionary where the keys are student names and the values are their average scores (rounded to 2 decimal places). **Example:** ```python records = [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"scores\\": [85, 90, 92]}, {\\"name\\": \\"Bob\\", \\"age\\": 21, \\"scores\\": [75, 80, 85]}, {\\"name\\": \\"Charlie\\", \\"age\\": 19, \\"scores\\": [95, 100, 98]}, {\\"name\\": \\"David\\", \\"age\\": 22, \\"scores\\": [60, 65, 70]}, ] result = process_student_records(records, min_age=20, max_age=21) print(result) # Output: {\'Alice\': 89.0, \'Bob\': 80.0} ``` **Constraints:** - The `scores` list in each record will contain at least one integer. - The ages and scores will be positive integers. **Notes:** - Utilize list comprehensions, dictionary comprehensions, and relevant list methods as appropriate. - You may find the `sorted()`, `sum()`, and `len()` functions useful. **Challenge:** Ensure your solution is both efficient and concise, effectively utilizing Python\'s data structure methods and comprehensions as detailed in the provided documentation.","solution":"def process_student_records(records, min_age, max_age): Processes a list of student records to filter by age, calculate average scores, sort by these scores in descending order, and return a dictionary with names as keys and the average scores as values. :param records: List of dictionaries containing student records. :param min_age: Minimum age to filter students by. :param max_age: Maximum age to filter students by. :return: Dictionary with student names and their average scores. filtered_records = [r for r in records if min_age <= r[\'age\'] <= max_age] for record in filtered_records: record[\'average_score\'] = round(sum(record[\'scores\']) / len(record[\'scores\']), 2) sorted_records = sorted(filtered_records, key=lambda x: x[\'average_score\'], reverse=True) result = {record[\'name\']: record[\'average_score\'] for record in sorted_records} return result"},{"question":"# Advanced File Operations in Python (Python 3.10) You are required to create a simplified file handling library that mimics some of the behaviors described in the provided documentation. Your library should include the following functions: 1. **create_file_object(fd, mode, buffering=-1, encoding=None, errors=None, newline=None, closefd=True)** - **Description**: Create a Python file object from a given file descriptor. - **Arguments**: - `fd` (int): File descriptor. - `mode` (str): Access mode (e.g., \'r\', \'w\'). - `buffering` (int, optional): Buffering policy. Default is -1. - `encoding` (str, optional): Encoding. Default is None. - `errors` (str, optional): Error handling scheme. Default is None. - `newline` (str, optional): Control how newlines are handled. Default is None. - `closefd` (bool, optional): Indicates whether to close the file descriptor. Default is True. - **Returns**: A file object. - **Raises**: Appropriate exceptions on failure. 2. **get_file_descriptor(file_object)** - **Description**: Retrieve the file descriptor associated with a file object. - **Arguments**: - `file_object`: A file-like object. - **Returns**: An integer representing the file descriptor. - **Raises**: Appropriate exceptions for invalid file objects. 3. **read_line(file_object, n=-1)** - **Description**: Read a line from the file object, similar to file.readline(). - **Arguments**: - `file_object`: A file-like object. - `n` (int, optional): Number of bytes to read. Default is -1 (read the whole line). - **Returns**: A string representing the line read. - **Raises**: EOFError if the end of the file is reached immediately. 4. **write_object_to_file(obj, file_object, raw=False)** - **Description**: Write a Python object to a file object. - **Arguments**: - `obj`: Any Python object. - `file_object`: A file-like object. - `raw` (bool, optional): If True, write str(obj) instead of repr(obj). Default is False. - **Returns**: None. - **Raises**: Appropriate exceptions on failure. 5. **write_string_to_file(s, file_object)** - **Description**: Write a string to a file object. - **Arguments**: - `s` (str): String to write. - `file_object`: A file-like object. - **Returns**: None. - **Raises**: Appropriate exceptions on failure. # Constraints - Your functions should closely mimic the error handling and behavior described in the documentation. - Performance should be reasonably efficient, but there are no strict performance constraints. - Use the native `open`, `io` module functions, and methods wherever applicable. # Example Usage ```python # Example file descriptor creation and operations fd = os.open(\'example.txt\', os.O_RDWR|os.O_CREAT) file_obj = create_file_object(fd, \'r+\') # Writing to file using the custom library functions write_string_to_file(\\"Hello, World!\\", file_obj) write_object_to_file({\'key\': \'value\'}, file_obj, raw=True) # Reading line file_obj.seek(0) line = read_line(file_obj) print(line) # Output: Hello, World! # Getting file descriptor from file object fd = get_file_descriptor(file_obj) print(fd) # Output: the file descriptor integer file_obj.close() os.close(fd) ``` Implement the functions as described above. Ensure you handle all necessary exception scenarios properly.","solution":"import os import io def create_file_object(fd, mode, buffering=-1, encoding=None, errors=None, newline=None, closefd=True): Create a Python file object from a given file descriptor. if not isinstance(fd, int): raise TypeError(\\"File descriptor must be an integer.\\") return io.open(fd, mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd) def get_file_descriptor(file_object): Retrieve the file descriptor associated with a file object. if not hasattr(file_object, \'fileno\'): raise TypeError(\\"Invalid file object.\\") return file_object.fileno() def read_line(file_object, n=-1): Read a line from the file object, similar to file.readline(). line = file_object.readline(n) if line == \\"\\": raise EOFError(\\"End of File reached.\\") return line def write_object_to_file(obj, file_object, raw=False): Write a Python object to a file object. if not hasattr(file_object, \'write\'): raise TypeError(\\"Invalid file object.\\") if raw: file_object.write(str(obj)) else: file_object.write(repr(obj)) def write_string_to_file(s, file_object): Write a string to a file object. if not hasattr(file_object, \'write\'): raise TypeError(\\"Invalid file object.\\") if not isinstance(s, str): raise TypeError(\\"Input must be a string.\\") file_object.write(s)"},{"question":"Securely Extracting Files from a Tar Archive You are given a tar archive (`example.tar.gz`) that contains various files, some of which may potentially have malicious paths or be of file types that should not be extracted for security reasons (e.g., device files, symbolic links). This archive may also have different formats such as `POSIX.1-1988`, `GNU`, and `POSIX.1-2001`. Your task is to write a Python function `secure_extract_tar` which performs the following: 1. Opens and reads the tar archive. 2. Uses a custom filter to extract only regular files and directories, ignoring device files, FIFOs, symbolic links, and any other non-regular files. 3. Removes any leading slashes from file paths to avoid absolute paths extraction. 4. Ignores files with paths containing elements like `..` that could navigate up the directory structure. 5. Implements appropriate error handling for any potential extraction errors. Function Signature ```python def secure_extract_tar(tar_path: str, extract_path: str) -> None: pass ``` Input - `tar_path` (str): The path to the tar archive file to extract. - `extract_path` (str): The destination directory where the files will be extracted. Output - No return value, but files should be extracted to the specified `extract_path` directory following the security rules. Constraints - You can assume that the `tarfile` module is available and other necessary modules like `os` and `shutil`. - The function should be able to handle any common tar archive format. - No files should be extracted outside the `extract_path`. Example Usage ```python secure_extract_tar(\\"example.tar.gz\\", \\"extracted/\\") ``` In this example, all valid files and directories from `example.tar.gz` will be extracted into the `extracted/` directory following the security constraints listed. # Notes - Carefully read through the relevant sections in the `tarfile` documentation to understand how to implement the custom filter. - Properly handle any potential exceptions raised during the extraction process to avoid stopping the entire program.","solution":"import tarfile import os def is_within_directory(directory, target): Check whether a target path is within the given directory. abs_directory = os.path.abspath(directory) abs_target = os.path.abspath(os.path.join(directory, target)) return abs_target.startswith(abs_directory) def secure_extract_tar(tar_path: str, extract_path: str) -> None: Securely extract files from a tar archive to the given directory. with tarfile.open(tar_path, \'r:*\') as tar: for member in tar.getmembers(): # Remove leading slashes from paths to avoid absolute paths member_path = member.name.lstrip(\'/\') # Skip any paths that contain \'..\' to avoid directory traversal if \'..\' in member_path.split(\'/\'): continue # Filter out non-regular files (e.g., symlinks, devices, etc.) if member.isfile() or member.isdir(): member.name = member_path target_path = os.path.join(extract_path, member.name) # Ensure the target path is within the extraction directory if is_within_directory(extract_path, target_path): tar.extract(member, extract_path)"},{"question":"# Coding Challenge: Configuring and Writing to an Audio Device using `ossaudiodev` Background You are given the task of writing a Python function to configure an OSS audio device and then write audio data to it. The device must be set up with specific audio parameters (audio format, number of channels, and sample rate), and the data should be written in blocking mode (the default). Task Implement the function `configure_and_write_audio(device, parameters, data)` that does the following: 1. Opens the specified audio device. 2. Configures the device using the given parameters (audio format, number of channels, and sample rate). 3. Writes the provided audio data to the device in blocking mode. 4. Closes the device after writing the data. Function Signature ```python def configure_and_write_audio(device: str, parameters: dict, data: bytes) -> None: pass ``` Parameters - `device` (str): The audio device filename (e.g., `/dev/dsp`). If `None`, use the default device. - `parameters` (dict): A dictionary with keys: - `format` (str): The audio format (e.g., `\\"AFMT_S16_LE\\"`). - `nchannels` (int): The number of channels (e.g., 1 for mono, 2 for stereo). - `samplerate` (int): The sampling rate in samples per second (e.g., 44100). - `data` (bytes): The audio data to be written to the device. Constraints - Raise `OSSAudioError` if any parameter configuration step fails. - Ensure all data is written to the audio device in blocking mode. - Ensure proper resource management (e.g., close the device after writing). Example ```python audio_parameters = { \'format\': \\"AFMT_S16_LE\\", \'nchannels\': 2, \'samplerate\': 44100 } audio_data = b\\"x00x01x02x03\\" * 1000 # Example binary audio data configure_and_write_audio(\\"/dev/dsp\\", audio_parameters, audio_data) ``` Notes - You may assume that the `ossaudiodev` module is available and fully functional. - Refer to the provided documentation to understand the usage of relevant methods and constants. - Focus on handling exceptions and ensuring that the device is always properly closed.","solution":"import ossaudiodev class OSSAudioError(Exception): pass def configure_and_write_audio(device: str, parameters: dict, data: bytes) -> None: try: # Open the specified audio device, default to \'/dev/dsp\' if device is None dsp = ossaudiodev.open(device if device else \'/dev/dsp\', \'w\') # Set the audio parameters if not isinstance(parameters.get(\'format\'), str): raise OSSAudioError(\\"Invalid format type\\") if not isinstance(parameters.get(\'nchannels\'), int): raise OSSAudioError(\\"Invalid number of channels type\\") if not isinstance(parameters.get(\'samplerate\'), int): raise OSSAudioError(\\"Invalid sample rate type\\") dsp.setfmt(getattr(ossaudiodev, parameters[\'format\'])) dsp.channels(parameters[\'nchannels\']) dsp.speed(parameters[\'samplerate\']) # Write the audio data to the device dsp.write(data) except Exception as e: raise OSSAudioError(f\\"Failed to configure or write to audio device: {e}\\") finally: if \'dsp\' in locals(): dsp.close()"},{"question":"**Coding Assessment Question: Handling HTTP Cookies** You are building a lightweight HTTP server that needs to handle cookies for managing user sessions. Using the `http.cookies` module in Python 3.10, your task is to implement a class `CookieManager` that includes the following functionalities: 1. **Create and Set Cookies:** Initialize a `SimpleCookie` object to manage cookies. Support setting cookies with optional attributes (`path`, `expires`, `domain`, `secure`, `httponly`, `samesite`). 2. **Encode and Decode Values:** Encode values using a specified format (e.g., as JSON strings) and decode them back. 3. **Generate HTTP Headers:** Output cookies in a format suitable for HTTP headers. 4. **Load Cookies:** Load cookies from a given HTTP header string and handle potential `CookieError` exceptions gracefully. 5. **Delete Cookies:** Delete specified cookies by setting their `expires` attribute to a past date. **Requirements:** - Implement all functionalities as methods within the `CookieManager` class. - Ensure proper handling of invalid cookie data and raise `CookieError` where appropriate. - Write comprehensive docstrings for each method explaining the functionality and parameters. **Class Definition:** ```python import json from http.cookies import SimpleCookie, CookieError class CookieManager: def __init__(self): self.cookie = SimpleCookie() def set_cookie(self, key, value, path=None, expires=None, domain=None, secure=None, httponly=None, samesite=None): Set a cookie with the specified attributes. Parameters: - key (str): The cookie name. - value (any): The cookie value. - path (str, optional): The \'path\' attribute. - expires (str, optional): The \'expires\' attribute. - domain (str, optional): The \'domain\' attribute. - secure (bool, optional): The \'secure\' attribute. - httponly (bool, optional): The \'httponly\' attribute. - samesite (str, optional): The \'samesite\' attribute, one of \'Strict\' or \'Lax\'. pass def encode_value(self, value): Encode a value using JSON. Parameters: - value (any): The value to encode. Returns: - str: The JSON-encoded value. pass def decode_value(self, encoded_value): Decode a JSON-encoded value. Parameters: - encoded_value (str): The JSON-encoded value. Returns: - any: The decoded value. pass def get_http_header(self): Generate cookies formatted as HTTP headers. Returns: - str: The HTTP header string with all cookies. pass def load_cookies(self, header_str): Load cookies from an HTTP header string. Parameters: - header_str (str): The HTTP header string containing cookies. Raises: - CookieError: If cookie data is invalid. pass def delete_cookie(self, key): Delete a cookie by setting its \'expires\' attribute to a past date. Parameters: - key (str): The cookie name to delete. pass ``` **Example Usage:** ```python # Initialize the cookie manager manager = CookieManager() # Set a cookie manager.set_cookie(\\"session_id\\", \\"abc123\\", path=\\"/\\", httponly=True, samesite=\\"Strict\\") # Encode and decode an example value encoded_value = manager.encode_value({\\"user_id\\": 42}) decoded_value = manager.decode_value(encoded_value) # Get the HTTP header format for the cookies header = manager.get_http_header() # Load cookies from a header string raw_header = \\"session_id=abc123; another_cookie=value;\\" manager.load_cookies(raw_header) # Delete a cookie manager.delete_cookie(\\"session_id\\") ``` **Constraints:** - Ensure the implementation is efficient and handles edge cases. - The `value` in `set_cookie` can be of any type, but it should be encoded as a JSON string before storing and should be decoded back after retrieval. This question tests your understanding of the `http.cookies` module, as well as your ability to handle cookie management in a web application context.","solution":"import json from http.cookies import SimpleCookie, CookieError from datetime import datetime, timedelta class CookieManager: def __init__(self): self.cookie = SimpleCookie() def set_cookie(self, key, value, path=None, expires=None, domain=None, secure=None, httponly=None, samesite=None): Set a cookie with the specified attributes. Parameters: - key (str): The cookie name. - value (any): The cookie value. - path (str, optional): The \'path\' attribute. - expires (str, optional): The \'expires\' attribute. - domain (str, optional): The \'domain\' attribute. - secure (bool, optional): The \'secure\' attribute. - httponly (bool, optional): The \'httponly\' attribute. - samesite (str, optional): The \'samesite\' attribute, one of \'Strict\' or \'Lax\'. encoded_value = self.encode_value(value) self.cookie[key] = encoded_value if path is not None: self.cookie[key][\'path\'] = path if expires is not None: self.cookie[key][\'expires\'] = expires if domain is not None: self.cookie[key][\'domain\'] = domain if secure is not None: self.cookie[key][\'secure\'] = secure if httponly is not None: self.cookie[key][\'httponly\'] = httponly if samesite is not None: self.cookie[key][\'samesite\'] = samesite def encode_value(self, value): Encode a value using JSON. Parameters: - value (any): The value to encode. Returns: - str: The JSON-encoded value. return json.dumps(value) def decode_value(self, encoded_value): Decode a JSON-encoded value. Parameters: - encoded_value (str): The JSON-encoded value. Returns: - any: The decoded value. return json.loads(encoded_value) def get_http_header(self): Generate cookies formatted as HTTP headers. Returns: - str: The HTTP header string with all cookies. return self.cookie.output(header=\'\', sep=\'\') def load_cookies(self, header_str): Load cookies from an HTTP header string. Parameters: - header_str (str): The HTTP header string containing cookies. Raises: - CookieError: If cookie data is invalid. try: self.cookie.load(header_str) except CookieError as e: print(\\"Failed to load cookies:\\", e) raise def delete_cookie(self, key): Delete a cookie by setting its \'expires\' attribute to a past date. Parameters: - key (str): The cookie name to delete. self.cookie[key] = \'\' self.cookie[key][\'expires\'] = \'Thu, 01 Jan 1970 00:00:00 GMT\'"},{"question":"Objective This question aims to test your understanding of seaborn\'s `regplot` function and its various customization options for fitting and visualizing different types of regression models. Problem Statement You are provided with a dataset `mpg` (miles per gallon) from seaborn\'s built-in datasets. The dataset contains various attributes of car models including `mpg`, `weight`, `acceleration`, `horsepower`, `displacement`, `cylinders`, and `origin`. Your task is to create a set of visualizations using the seaborn `regplot` function to explore and present meaningful insights from the data. Below are the requirements: 1. **Polynomial Regression Plot**: - Create a polynomial regression plot to investigate the relationship between `weight` and `mpg`. Fit a second-order polynomial (quadratic) regression line to the data. - **Input**: None - **Output**: A seaborn plot 2. **Log-linear Regression Plot**: - Create a log-linear regression plot to show the relationship between `displacement` and `mpg` with the x-axis on a logarithmic scale. - **Input**: None - **Output**: A seaborn plot 3. **LOWESS Smoothing**: - Create a regression plot with a locally-weighted scatterplot smoothing (LOWESS) estimator to show the relationship between `horsepower` and `mpg`. - **Input**: None - **Output**: A seaborn plot 4. **Customized Regression Plot**: - Create a custom regression plot to show the relationship between `weight` and `horsepower`. Customize the plot such that: - The confidence interval is set to 95%. - The markers are crosses (`\'x\'`). - The marker color is gray (`\'.3\'`). - The regression line is red (`\'r\'`). - **Input**: None - **Output**: A seaborn plot 5. **Aggregated and Binned Plot**: - Create a binned and aggregated regression plot to show the relationship between `weight` and `mpg`. Bin the `weight` values into intervals of 250 and fit a second-order polynomial regression line. - **Input**: None - **Output**: A seaborn plot Note - Each plot should be displayed on a separate figure, with appropriate titles and labels. - Ensure that the code is efficient and avoid redundancy. - Use the `sns.set_theme()` for consistent styling. Example Here\'s an example of how to start with the dataset: ```python import seaborn as sns import numpy as np # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Set the seaborn theme sns.set_theme() # Example for polynomial regression plot sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Set the seaborn theme sns.set_theme() def polynomial_regression_plot(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) plt.title(\'Polynomial Regression Plot: Weight vs MPG\') plt.xlabel(\'Weight\') plt.ylabel(\'MPG\') plt.show() def log_linear_regression_plot(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", logx=True) plt.title(\'Log-Linear Regression Plot: Displacement vs MPG\') plt.xlabel(\'Displacement (log scale)\') plt.ylabel(\'MPG\') plt.show() def lowess_smoothing_plot(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True) plt.title(\'LOWESS Smoothing: Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.show() def customized_regression_plot(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"horsepower\\", ci=95, marker=\'x\', color=\'.3\', line_kws={\'color\': \'r\'}) plt.title(\'Customized Regression Plot: Weight vs Horsepower\') plt.xlabel(\'Weight\') plt.ylabel(\'Horsepower\') plt.show() def binned_aggregated_plot(): plt.figure(figsize=(10, 6)) sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", x_bins=np.arange(min(mpg[\\"weight\\"]), max(mpg[\\"weight\\"]), 250), order=2) plt.title(\'Binned Aggregated Regression Plot: Weight vs MPG\') plt.xlabel(\'Weight\') plt.ylabel(\'MPG\') plt.show()"},{"question":"You\'re tasked with implementing a function that initializes a distributed environment using the `torch.distributed` package and performs a collective operation. Specifically, you will create a function to compute the sum of tensors across multiple processes. # Task Implement a function `distributed_tensor_sum(tensor, backend, rank, world_size, master_addr, master_port)` that: 1. Initializes the process group using the provided backend and TCP initialization. 2. Ensures the initialization uses the specified `master_addr` and `master_port`. 3. Performs an `all_reduce` operation to compute the sum of the given tensor across all processes. 4. Returns the resulting summed tensor. # Input - `tensor` (torch.Tensor): The tensor to be summed across processes. - `backend` (str): The backend to use for the distributed process group (e.g., \'gloo\', \'nccl\', \'mpi\'). - `rank` (int): The rank of the current process. - `world_size` (int): The total number of processes. - `master_addr` (str): The address of the master node. - `master_port` (int): The port number of the master node. # Output - Returns a torch.Tensor: The result of the `all_reduce` sum operation. # Constraints - The function must handle both CPU and CUDA tensors. - Ensure that you use synchronous operations for simplicity. # Example ```python import torch tensor = torch.tensor([rank], dtype=torch.float32).cuda(rank) backend = \'nccl\' rank = 0 world_size = 4 master_addr = \'127.0.0.1\' master_port = 29500 # Master node process result = distributed_tensor_sum(tensor, backend, rank, world_size, master_addr, master_port) print(result) # will output [6.0] when summed across 4 processes with rank values [0, 1, 2, 3] ``` # Notes - Ensure to handle initializations and shutdowns properly to prevent hangs or deadlocks. - Make sure to test the function with different backends and configurations.","solution":"import torch import torch.distributed as dist def distributed_tensor_sum(tensor, backend, rank, world_size, master_addr, master_port): Initializes the distributed environment and performs an all_reduce operation to sum tensors across multiple processes. Args: tensor (torch.Tensor): The tensor to be summed across processes. backend (str): The backend to use for the distributed process group (e.g., \'gloo\', \'nccl\', \'mpi\'). rank (int): The rank of the current process. world_size (int): The total number of processes. master_addr (str): The address of the master node. master_port (int): The port number of the master node. Returns: torch.Tensor: The result of the all_reduce sum operation. # Initialize the process group dist.init_process_group( backend=backend, init_method=f\\"tcp://{master_addr}:{master_port}\\", rank=rank, world_size=world_size ) # Perform the all_reduce operation to sum the tensor across all processes. dist.all_reduce(tensor, op=dist.ReduceOp.SUM) # Destroy the process group dist.destroy_process_group() return tensor # To prevent deadlocks caused by copy-pasting code into this notebook, # we will not initialize a distributed environment directly here."},{"question":"Objective: Demonstrate your understanding of the `__future__` module in Python by creating a script that uses features from future Python versions. Your solution should ensure compatibility with both current and future Python versions. Task: Create a Python script that: 1. Utilizes the `division` and `print_function` features from the `__future__` module. 2. Defines a function `print_fraction(a, b)` that takes two integers `a` and `b`. 3. Ensures that `print_fraction(a, b)`: - Computes the precise division `a / b` using true division. - Prints the result using the `print` function. Constraints: - Your script should be compatible with Python 2.7 and later versions. - Do not use the old print statement (`print a/b`), but rather utilize the `print` function. Input & Output Format: - The input will be two integers `a` and `b` where `b` is not zero. - The function should print the result of `a / b` in floating-point format. Example: ```python >>> print_fraction(5, 2) 2.5 >>> print_fraction(7, 3) 2.3333333333333335 ``` Deliverables: - A Python script file named `future_features.py` containing the required function. Evaluation Criteria: 1. Correctness: The function should perform true division and print the result using the `print` function. 2. Compatibility: The script should run without errors in both Python 2.7 and 3.x. 3. Clarity: The code should be well-documented and easy to understand.","solution":"from __future__ import division, print_function def print_fraction(a, b): Computes the division of a by b and prints the result using the print function. Parameters: a (int): The numerator b (int): The denominator Returns: None result = a / b print(result)"},{"question":"Coding Assessment Question **Objective:** Demonstrate your understanding of the seaborn library by creating multiple types of regression plots and customizing their appearance. **Instructions:** 1. You will be using the `mpg` dataset provided in the seaborn library. 2. Implement a function `plot_relationship(dataframe, x_col, y_col, plot_type, **kwargs)` that generates different types of regression plots based on the `plot_type` parameter. 3. The function should be flexible enough to: - Handle linear, polynomial, log-linear, LOWESS, logistic, and robust regressions. - Allow for customization of plot appearance (e.g., colors, markers, confidence interval). **Function Signature:** ```python import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt def plot_relationship(dataframe: pd.DataFrame, x_col: str, y_col: str, plot_type: str, **kwargs) -> None: pass # Example Usage: mpg = sns.load_dataset(\\"mpg\\") plot_relationship(mpg, \\"weight\\", \\"acceleration\\", \\"linear\\") plot_relationship(mpg, \\"weight\\", \\"mpg\\", \\"polynomial\\", order=2) plot_relationship(mpg, \\"displacement\\", \\"mpg\\", \\"log-linear\\") plot_relationship(mpg, \\"horsepower\\", \\"mpg\\", \\"lowess\\") plot_relationship(mpg, \\"weight\\", mpg[\\"origin\\"].eq(\\"usa\\").rename(\\"from_usa\\"), \\"logistic\\") plot_relationship(mpg, \\"horsepower\\", \\"weight\\", \\"robust\\") ``` **Parameters:** 1. `dataframe (pd.DataFrame)`: The DataFrame containing the data to be plotted. 2. `x_col (str)`: The name of the column in the DataFrame to be used for the x-axis. 3. `y_col (str)`: The name of the column in the DataFrame to be used for the y-axis. 4. `plot_type (str)`: The type of regression plot to generate. Possible values include: - \\"linear\\": Linear regression. - \\"polynomial\\": Polynomial regression. - \\"log-linear\\": Log-linear regression. - \\"lowess\\": Locally-weighted scatterplot smoothing. - \\"logistic\\": Logistic regression for binary outcomes. - \\"robust\\": Robust regression to downweight outliers. 5. `**kwargs`: Additional keyword arguments to customize the plot (e.g., `order`, `ci`, `marker`, `color`, etc.). **Requirements:** - Import necessary libraries within the function. - Generate and show the plot using matplotlib. - Handle invalid `plot_type` values gracefully by raising a `ValueError` with a descriptive message. **Example Output:** Upon running the example usage provided, the function should generate six corresponding plots, each showing the requested type of regression relationship between the provided variables and customized according to the additional keyword arguments. **Constraints:** - Ensure the function handles edge cases, such as empty dataframes or non-existent column names, gracefully by raising appropriate exceptions. - The function should be efficient and avoid unnecessary computations.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt def plot_relationship(dataframe: pd.DataFrame, x_col: str, y_col: str, plot_type: str, **kwargs) -> None: Function to generate different types of regression plots using seaborn. Parameters: dataframe (pd.DataFrame): The DataFrame containing the data to be plotted. x_col (str): The name of the column in the DataFrame to be used for the x-axis. y_col (str): The name of the column in the DataFrame to be used for the y-axis. plot_type (str): The type of regression plot to generate. Possible values include: - \\"linear\\": Linear regression. - \\"polynomial\\": Polynomial regression. - \\"log-linear\\": Log-linear regression. - \\"lowess\\": Locally-weighted scatterplot smoothing. - \\"logistic\\": Logistic regression for binary outcomes. - \\"robust\\": Robust regression to downweight outliers. **kwargs: Additional keyword arguments to customize the plot. Returns: None: Displays the plot. Raises: ValueError: If an invalid plot_type is provided. KeyError: If x_col or y_col does not exist in the dataframe. if x_col not in dataframe.columns or y_col not in dataframe.columns: raise KeyError(f\\"Column \'{x_col}\' or \'{y_col}\' is not in dataframe\\") plot_settings = { \\"linear\\": {\\"func\\": sns.regplot, \\"params\\": {}}, \\"polynomial\\": {\\"func\\": sns.regplot, \\"params\\": {\\"order\\": kwargs.get(\\"order\\", 2)}}, \\"log-linear\\": {\\"func\\": sns.regplot, \\"params\\": {\\"logx\\": True}}, \\"lowess\\": {\\"func\\": sns.regplot, \\"params\\": {\\"lowess\\": True}}, \\"logistic\\": {\\"func\\": sns.regplot, \\"params\\": {\\"logistic\\": True}}, \\"robust\\": {\\"func\\": sns.regplot, \\"params\\": {\\"robust\\": True}}, } if plot_type not in plot_settings: raise ValueError(f\\"Invalid plot_type \'{plot_type}\'. Valid options are {list(plot_settings.keys())}\\") plot_func = plot_settings[plot_type][\\"func\\"] plot_params = plot_settings[plot_type][\\"params\\"] # Update plot parameters with additional kwargs, if any plot_params.update(kwargs) plt.figure(figsize=(10, 6)) plot_func(data=dataframe, x=x_col, y=y_col, **plot_params) plt.xlabel(x_col.capitalize()) plt.ylabel(y_col.capitalize()) plt.title(f\'{plot_type.capitalize()} Regression of {y_col.capitalize()} vs {x_col.capitalize()}\') plt.show()"},{"question":"**Problem Statement:** You are required to create a custom codec that reverses the input text during encoding and restores it during decoding. Additionally, implement incremental encoding and decoding for this codec. Your solution should include the following: 1. A function `reverse_encode(input_string)` that takes a string and returns the string reversed. 2. A function `reverse_decode(input_string)` that takes a reversed string and restores the original string. 3. A class `ReverseIncrementalEncoder` implementing the incremental encoding interface, with methods to encode text incrementally. 4. A class `ReverseIncrementalDecoder` implementing the incremental decoding interface, with methods to decode text incrementally. 5. Register this new codec using the `codecs.register()` function so that it can be used with the `codecs` module functions like `encode()`, `decode()`, etc. **Input and Output:** - The `reverse_encode()` function should take a string and return the reversed string. - The `reverse_decode()` function should take a reversed string and return the original string. - The `ReverseIncrementalEncoder` class should implement incremental encoding via an `encode()` method. - The `ReverseIncrementalDecoder` class should implement incremental decoding via a `decode()` method. - The custom codec should be registered so that it can be looked up and used for encoding and decoding strings. **Constraints:** - You are not allowed to use any built-in string reversal methods/functions in Python. The reversal logic should be implemented manually. - The incremental encoder and decoder should handle buffering correctly. **Performance Requirements:** - The functions and classes must be efficient with time complexity of O(n), where n is the length of the input string. **Implementation Details:** 1. **Function: `reverse_encode(input_string) -> str`** - Input: `input_string` - a string of any length. - Output: The reversed version of `input_string`. 2. **Function: `reverse_decode(input_string) -> str`** - Input: `input_string` - a reversed string of any length. - Output: The original version of the input string. 3. **Class: `ReverseIncrementalEncoder`** - Method: `encode(self, input_string, final=False) -> str` - Method: `reset(self) -> None` - Method: `getstate(self) -> int` - Method: `setstate(self, state) -> None` 4. **Class: `ReverseIncrementalDecoder`** - Method: `decode(self, input_string, final=False) -> str` - Method: `reset(self) -> None` - Method: `getstate(self) -> tuple` - Method: `setstate(self, state) -> None` 5. **Function: `register_reverse_codec()`** - Register the custom codec using `codecs.register`. **Example Usage:** ```python import codecs def reverse_encode(input_string): ... def reverse_decode(input_string): ... class ReverseIncrementalEncoder(codecs.IncrementalEncoder): ... class ReverseIncrementalDecoder(codecs.IncrementalDecoder): ... def register_reverse_codec(): ... # Register the custom codec register_reverse_codec() # Using the codec with the codecs module encoded_text = codecs.encode(\\"hello world\\", \\"reverse\\") print(encoded_text) # Output: \\"dlrow olleh\\" decoded_text = codecs.decode(encoded_text, \\"reverse\\") print(decoded_text) # Output: \\"hello world\\" ``` Please implement the required functions and classes to fulfill the problem statement above.","solution":"import codecs def reverse_encode(input_string): Returns the reversed version of the input string. return \'\'.join([input_string[i] for i in range(len(input_string)-1, -1, -1)]) def reverse_decode(input_string): Returns the original version of the reversed input string. return \'\'.join([input_string[i] for i in range(len(input_string)-1, -1,-1)]) class ReverseIncrementalEncoder(codecs.IncrementalEncoder): Encoder class for the reverse codec. def encode(self, input, final=False): return reverse_encode(input) class ReverseIncrementalDecoder(codecs.IncrementalDecoder): Decoder class for the reverse codec. def decode(self, input, final=False): return reverse_decode(input) class ReverseCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): return reverse_encode(input), len(input) def decode(self, input, errors=\'strict\'): return reverse_decode(input), len(input) def register_reverse_codec(): def reverse_search(name): if name == \'reverse\': return codecs.CodecInfo( name=\'reverse\', encode=ReverseCodec().encode, decode=ReverseCodec().decode, incrementalencoder=ReverseIncrementalEncoder, incrementaldecoder=ReverseIncrementalDecoder, streamreader=codecs.StreamReader, streamwriter=codecs.StreamWriter ) return None codecs.register(reverse_search) # Register the custom codec register_reverse_codec()"},{"question":"**Question: Rational Math Operations** The goal of this assessment is to ensure you understand how to use the `fractions` module in Python to perform arithmetic operations on rational numbers. You will be required to implement a function that takes multiple input fractions, performs arithmetic operations, and returns the results according to specified requirements. # Function Signature ```python def rational_operations(fractions_input: list, operation: str) -> \'Fraction\': pass ``` # Input - `fractions_input`: A list of strings, where each string represents a fraction in the format `[sign] numerator [\'/\' denominator]`. For example: [\\"1/3\\", \\"-2/5\\", \\"4\\"]. - `operation`: A string specifying the arithmetic operation to be performed. It can be one of the following: \\"sum\\", \\"product\\", \\"difference\\", or \\"quotient\\". Use the first fraction as the base, and sequentially apply the operation with the following fractions in the list. # Output - Return a `Fraction` instance that represents the result of the operation. If the input list is empty, return `Fraction(0, 1)`. # Examples ```python >>> from fractions import Fraction >>> rational_operations([\\"1/3\\", \\"1/6\\", \\"1/2\\"], \\"sum\\") Fraction(1, 1) >>> rational_operations([\\"1/3\\", \\"1/6\\", \\"1/2\\"], \\"product\\") Fraction(1, 36) >>> rational_operations([\\"1/2\\", \\"1/3\\", \\"1/4\\"], \\"difference\\") Fraction(-1, 12) >>> rational_operations([\\"1\\", \\"1/2\\"], \\"quotient\\") Fraction(2, 1) ``` # Constraints 1. The function should raise a `ValueError` for invalid operations. 2. The function should handle fractions that may not be properly formatted (e.g., leading/trailing whitespace). 3. Use the methods of the `Fraction` class wherever appropriate. 4. Return the fraction in its simplest form. # Notes - Ensure proper normalization of the fractions before performing any operations. - The operation should be applied sequentially from the first fraction to the last in the list. # Hints - Use the `Fraction` constructor to normalize and convert the input strings into `Fraction` instances. - Utilize arithmetic operators provided by the `Fraction` class: `+`, `-`, `*`, and `/`. Implement the `rational_operations` function as specified above. Be sure to test your function with a variety of inputs to verify its correctness.","solution":"from fractions import Fraction def rational_operations(fractions_input: list, operation: str) -> Fraction: if not fractions_input: return Fraction(0, 1) # Normalize fractions and handle whitespace fractions = [Fraction(f.strip()) for f in fractions_input] # Perform the specified operation result = fractions[0] for frac in fractions[1:]: if operation == \\"sum\\": result += frac elif operation == \\"product\\": result *= frac elif operation == \\"difference\\": result -= frac elif operation == \\"quotient\\": result /= frac else: raise ValueError(f\\"Invalid operation: {operation}\\") return result"},{"question":"# Reference Counting Simulation In this question, you are to simulate Python\'s reference counting mechanism in Python itself. You have to write a class `PyObjectSimulator` that mimics reference counting for some objects. This exercise will deepen your understanding of how Python manages object lifetimes under-the-hood using reference counting. # Requirements: 1. **Class Definition**: - **`PyObjectSimulator`**: This class should represent the object for which we track reference counts. 2. **Methods**: - **`__init__(self, value)`**: Initialize the object with an initial value and set the reference count to 1. - **`incref(self)`**: Increase the reference count by 1. - **`decref(self)`**: Decrease the reference count by 1. If the reference count becomes zero, the object should be considered destroyed, and a custom destructor method `_del(self)` should be called, which prints a message indicating that the object has been destroyed. 3. **Properties**: - **`refcount`**: This property should allow querying the current reference count of the object. # Expected Input and Output: - You can assume that the provided input will be valid and do not need to handle exceptional cases (e.g., negative reference counts). - Assume all operations are performed sequentially, i.e., one after another. # Example Usage: ```python # Create an object with initial value 5 obj = PyObjectSimulator(5) print(obj.refcount) # Output: 1 # Increase reference count obj.incref() print(obj.refcount) # Output: 2 # Decrease reference count obj.decref() print(obj.refcount) # Output: 1 # Decrease reference count again, object should be destroyed obj.decref() # Output: \\"Object has been destroyed\\" (from _del method) ``` # Performance Requirements: Since we are dealing with simple object manipulations, the performance requirement is minimal. Ensure that your class handles reference counting accurately.","solution":"class PyObjectSimulator: def __init__(self, value): Initialize the object with an initial value and set the reference count to 1. self.value = value self._refcount = 1 @property def refcount(self): Property to return the current reference count of the object. return self._refcount def incref(self): Increase the reference count by 1. self._refcount += 1 def decref(self): Decrease the reference count by 1. If the reference count becomes zero, call the _del method. self._refcount -= 1 if self._refcount == 0: self._del() def _del(self): Custom destructor method to indicate the object has been destroyed. print(\\"Object has been destroyed\\")"},{"question":"**Objective**: Write a function that processes a DataFrame in a way that confirms your understanding of the Copy-on-Write (CoW) mechanics in pandas 3.0. Problem Statement: Given a DataFrame `df` with columns \\"A\\" and \\"B\\", implement a function `process_dataframe(df)` that: 1. Creates a view of column \\"A\\" and attempts to update its first element. 2. Creates a new DataFrame by dropping column \\"B\\" without triggering a copy. 3. Returns both the modified view and the new DataFrame while ensuring no unintended modifications on the original DataFrame. Function Signature: ```python def process_dataframe(df: pd.DataFrame) -> (pd.Series, pd.DataFrame): pass ``` Input: - `df` (pandas.DataFrame): Contains at least two columns \\"A\\" and \\"B\\". Output: - Returns a tuple: - The first element should be a pandas Series, representing the modified view of column \\"A\\". - The second element should be the new DataFrame after dropping column \\"B\\". Constraints: - You must use pandas 3.0 or higher. - Do not use inplace operations that violate CoW principles. - Ensure there are no unintended modifications to the original DataFrame `df`. Example: ```python import pandas as pd df = pd.DataFrame({\\"A\\": [1, 2, 3], \\"B\\": [4, 5, 6]}) processed_view, new_df = process_dataframe(df) print(processed_view) # Output should be: # 0 100 (if modification permissible) # 1 2 # 2 3 # Name: A, dtype: int64 print(new_df) # Output should be: # A # 0 1 # 1 2 # 2 3 print(df) # Output should remain unchanged as: # A B # 0 1 4 # 1 2 5 # 2 3 6 ``` Additional Information: - Ensure you raise appropriate errors if operations are not allowed under CoW. - Use pandas functions and adhere to CoW rules in performing modifications.","solution":"import pandas as pd def process_dataframe(df: pd.DataFrame) -> (pd.Series, pd.DataFrame): Processes the DataFrame as per Copy-on-Write (CoW) principles. Args: df (pd.DataFrame): The input DataFrame containing at least columns \\"A\\" and \\"B\\". Returns: Tuple: Containing the modified view of column \\"A\\" and a new DataFrame after dropping column \\"B\\". # Create a view of column \\"A\\" and attempt to update its first element view_a = df[\\"A\\"].copy() # Creating a copy to respect CoW principles view_a.iloc[0] = 100 # Modifying the first element # Create a new DataFrame by dropping column \\"B\\" without triggering a copy new_df = df.drop(columns=[\\"B\\"]).copy() # Drop column \\"B\\" and create a new DataFrame return view_a, new_df"},{"question":"# Novelty Detection Using One-Class SVM **Objective:** Demonstrate your understanding of using the One-Class SVM algorithm from scikit-learn for novelty detection. **Problem:** You are provided with a dataset representing normal observations of a certain feature (i.e., data with no outliers) and another set of new observations. Your task is to train a One-Class SVM model using the normal data and then predict whether each observation in the new dataset is an inlier or an outlier. **Dataset:** - `X_train`: A numpy array of shape (n_train_samples, n_features) containing the training data of normal observations. - `X_test`: A numpy array of shape (n_test_samples, n_features) containing new observations to be classified as inliers or outliers. **Instructions:** 1. Train a One-Class SVM model using the training data `X_train`. 2. Use the trained model to predict the labels for the `X_test` data. Inliers should be labeled as 1 and outliers as -1. 3. Implement the function `novelty_detection` which takes `X_train` and `X_test` as inputs and returns the predicted labels for `X_test`. **Function Signature:** ```python def novelty_detection(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: pass ``` **Requirements:** 1. Use the `sklearn.svm.OneClassSVM` class for the model. 2. The `nu` parameter of the One-Class SVM should be set to 0.05, representing the expected proportion of outliers. 3. Use the Radial Basis Function (RBF) kernel for the model. 4. The function should return a numpy array of shape (n_test_samples,) containing the predicted labels for `X_test`. **Constraints:** - Ensure your solution handles different sizes of input arrays gracefully. - Do not use any additional libraries other than numpy and scikit-learn. **Example Usage:** ```python import numpy as np # Example Data X_train = np.array([[0.1, 0.2], [0.2, 0.3], [0.15, 0.25], [0.3, 0.4], [0.25, 0.35]]) X_test = np.array([[0.12, 0.22], [0.4, 0.5], [0.18, 0.28]]) # Function Call labels = novelty_detection(X_train, X_test) print(labels) # [1, -1, 1] assuming the second point in X_test is an outlier ``` **Note:** Ensure your function follows the given function signature and demonstrates proper use of scikit-learn\'s One-Class SVM for novelty detection.","solution":"import numpy as np from sklearn.svm import OneClassSVM def novelty_detection(X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: Train a One-Class SVM model using the training data `X_train` and predict the labels for `X_test`. Inliers should be labeled as 1 and outliers as -1. Parameters: - X_train: numpy array of shape (n_train_samples, n_features), training data of normal observations. - X_test: numpy array of shape (n_test_samples, n_features), new observations to be classified as inliers/outliers. Returns: - numpy array of shape (n_test_samples,), containing the predicted labels for `X_test`. # Create and train the One-Class SVM model model = OneClassSVM(kernel=\'rbf\', nu=0.05) model.fit(X_train) # Predict the labels for X_test predictions = model.predict(X_test) return predictions"},{"question":"Email Message Manipulation Objective Create a Python function to manage and manipulate an email message using the `email.message.EmailMessage` class from the `email` package. This function should demonstrate comprehension of setting headers, manipulating payloads, and ensuring correct MIME types. Description You are required to write a function, `create_email_message`, which creates an `EmailMessage` object and performs the following tasks: 1. Sets the following headers: - `From`: the sender\'s email address (input parameter). - `To`: the recipient\'s email address (input parameter). - `Subject`: the email subject (input parameter). 2. Adds multipart content to the email: - A plain text version of the email body (input parameter). - An HTML version of the email body (input parameter). 3. Attaches a file to the email message: - The file path should be an input parameter. 4. Serializes the message into both string and bytes formats and returns these two representations. Function Signature ```python import email from email.message import EmailMessage def create_email_message(from_addr: str, to_addr: str, subject: str, plain_text_body: str, html_body: str, attachment_path: str) -> (str, bytes): pass ``` Input - `from_addr` (str): The sender\'s email address. - `to_addr` (str): The recipient\'s email address. - `subject` (str): The email subject. - `plain_text_body` (str): The plain text version of the email content. - `html_body` (str): The HTML version of the email content. - `attachment_path` (str): The file path of the attachment. Output - Returns a tuple containing: - A string representation of the serialized email message. - A bytes representation of the serialized email message. Constraints - The function should ensure that email message fields are correctly set and that the MIME parts are properly formed. - Handle file reading exceptions appropriately. - Ensure that the email headers and attachment integrity remain intact after serialization. Example Usage ```python email_str, email_bytes = create_email_message( from_addr=\\"sender@example.com\\", to_addr=\\"recipient@example.com\\", subject=\\"Meeting Schedule\\", plain_text_body=\\"Please find the meeting schedule attached.\\", html_body=\\"<p>Please find the <b>meeting schedule</b> attached.</p>\\", attachment_path=\\"/path/to/meeting_schedule.pdf\\" ) ``` This example should create an `EmailMessage` object with the specified headers, content, and attachment, and then return both the string and bytes representations of the serialized email message.","solution":"import email from email.message import EmailMessage from email.mime.base import MIMEBase from email import encoders import os def create_email_message(from_addr: str, to_addr: str, subject: str, plain_text_body: str, html_body: str, attachment_path: str) -> (str, bytes): # Create the EmailMessage object message = EmailMessage() # Set headers message[\'From\'] = from_addr message[\'To\'] = to_addr message[\'Subject\'] = subject # Add the plain text and HTML parts message.set_content(plain_text_body) message.add_alternative(html_body, subtype=\'html\') # Attach the file if os.path.exists(attachment_path): with open(attachment_path, \'rb\') as attachment: file_data = attachment.read() file_name = os.path.basename(attachment_path) maintype, subtype = \'application\', \'octet-stream\' part = MIMEBase(maintype, subtype) part.set_payload(file_data) encoders.encode_base64(part) part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{file_name}\\"\') message.attach(part) else: raise FileNotFoundError(f\\"No such file: \'{attachment_path}\'\\") # Serialize the message into string and bytes formats email_str = message.as_string() email_bytes = message.as_bytes() return email_str, email_bytes"},{"question":"# Objective: Create a coding solution involving multiple aspects of the `codecs` module focusing on error handling and stream-based encoding/decoding. # Problem Statement: Your task is to create a Python class `CustomCodecHandler` which demonstrates the following functionalities: 1. **Custom Error Handling**: - Implement a custom error handler named `custom_replace_errors` which replaces unencodable parts with the string `\\"[ERROR]\\"`. 2. **Stream Encoding and Decoding**: - Implement methods `encode_to_file` and `decode_from_file` using `StreamWriter` and `StreamReader` respectively. - The `encode_to_file` method should take an input string, encode it into a specified encoding with the `custom_replace_errors` error handler, and write it to a file. - The `decode_from_file` method should read the encoded file, decode its content, and return the string. # Requirements: - **Custom Error Handler `custom_replace_errors`**: - This should be registered and used to replace unencodable characters with `\\"[ERROR]\\"`. - **Method: `encode_to_file(self, input_str: str, filename: str, encoding: str)`**: - Takes a string `input_str` and encodes it using the specified `encoding`. Uses `custom_replace_errors` for error handling. - Writes the encoded string to a file specified by `filename`. - **Method: `decode_from_file(self, filename: str, encoding: str)`**: - Reads contents from the file specified by `filename`, decodes it using the specified `encoding`, and returns the decoded string. # Input: 1. `input_str` - A string to be encoded. 2. `filename` - Name of the file to write/read encoded data. 3. `encoding` - The encoding type to be used for encoding/decoding. # Output: - Return the decoded string from `decode_from_file`. - Write encoded data to file in `encode_to_file`. # Example: ```python # Example usage: handler = CustomCodecHandler() handler.encode_to_file(\'Hello World! u2603\', \'output.txt\', \'ascii\') decoded_string = handler.decode_from_file(\'output.txt\', \'ascii\') print(decoded_string) # Output: Hello World! [ERROR] ``` # Constraints: - Ensure compatibility with Python 3.10. - Implement error handling for file operations. # Notes: - You should use the `codecs` module functions and classes where appropriate. - Register your custom error handler using `codecs.register_error`. # Implementation: Provide the implementation of the `CustomCodecHandler` class in Python.","solution":"import codecs class CustomCodecHandler: def __init__(self): codecs.register_error(\'custom_replace_errors\', self.custom_replace_errors) def custom_replace_errors(self, exc): if isinstance(exc, (UnicodeEncodeError, UnicodeDecodeError)): return \\"[ERROR]\\", exc.start + 1 else: raise exc def encode_to_file(self, input_str: str, filename: str, encoding: str): try: with codecs.open(filename, \'w\', encoding=encoding, errors=\'custom_replace_errors\') as f: f.write(input_str) except Exception as e: print(f\\"Error during encoding: {e}\\") def decode_from_file(self, filename: str, encoding: str): try: with codecs.open(filename, \'r\', encoding=encoding, errors=\'custom_replace_errors\') as f: return f.read() except Exception as e: print(f\\"Error during decoding: {e}\\") return None # Example usage handler = CustomCodecHandler() handler.encode_to_file(\'Hello World! u2603\', \'output.txt\', \'ascii\') decoded_string = handler.decode_from_file(\'output.txt\', \'ascii\') print(decoded_string) # Output: Hello World! [ERROR]"},{"question":"# **Coding Assessment Question** **Objective**: Write a function using the Python `random` module that simulates a scenario where customers arrive at a coffee shop, place orders, and their service times are recorded. The function should calculate and return the average waiting time for the customers. # **Detailed Requirements** 1. **Function Definition**: ```python def simulate_coffee_shop(arrival_rate, service_mean, service_stddev, num_customers): Simulate the operation of a coffee shop with random arrival and service times. Parameters: arrival_rate (float): The average time (in minutes) between customer arrivals. service_mean (float): The average service time (in minutes) for each customer. service_stddev (float): The standard deviation of the service time (in minutes). num_customers (int): The number of customers to simulate. Returns: float: The average waiting time (in minutes) for the customers. pass ``` 2. **Inputs**: - `arrival_rate` (float): The average time in minutes between customer arrivals. - `service_mean` (float): The mean service time in minutes. - `service_stddev` (float): The standard deviation of service time in minutes. - `num_customers` (int): The total number of customers to simulate. 3. **Functionality**: - Use `random.expovariate(1.0 / arrival_rate)` to generate random intervals between customer arrivals. - Use `random.gauss(service_mean, service_stddev)` to generate random service times for each customer. - Simulate a series of customer arrivals and their corresponding service times. - Each customer starts being served as soon as they arrive or as soon as the previous customer is done, whichever is later. - Record the waiting time for each customer (time spent waiting for service to begin). - Calculate and return the average waiting time across all customers. 4. **Output**: - The function should return the average waiting time for the `num_customers` simulated. # **Example** ```python # Example usage: average_waiting_time = simulate_coffee_shop(5, 15, 3.5, 1000) print(f\\"The average waiting time for the customers is: {average_waiting_time:.2f} minutes\\") ``` # **Constraints**: - Ensure that the service time generated by `random.gauss()` is always non-negative (incorporate appropriate checks). - Assume `arrival_rate`, `service_mean`, `service_stddev` are positive non-zero values. - Assume `num_customers` is a positive integer. # **Hints**: - Use a list to keep track of the time at which each customer starts being served. - The waiting time for a customer is calculated as the difference between their arrival time and the time they start being served. Happy coding!","solution":"import random def simulate_coffee_shop(arrival_rate, service_mean, service_stddev, num_customers): Simulate the operation of a coffee shop with random arrival and service times. Parameters: arrival_rate (float): The average time (in minutes) between customer arrivals. service_mean (float): The average service time (in minutes) for each customer. service_stddev (float): The standard deviation of the service time (in minutes). num_customers (int): The number of customers to simulate. Returns: float: The average waiting time (in minutes) for the customers. arrival_times = [] service_times = [] start_times = [] end_times = [] for i in range(num_customers): if i == 0: arrival_time = 0 else: arrival_time = arrival_times[i-1] + random.expovariate(1.0 / arrival_rate) arrival_times.append(arrival_time) service_time = max(0, random.gauss(service_mean, service_stddev)) service_times.append(service_time) if i == 0: start_time = arrival_time else: start_time = max(arrival_time, end_times[i-1]) start_times.append(start_time) end_time = start_time + service_time end_times.append(end_time) waiting_times = [start_times[i] - arrival_times[i] for i in range(num_customers)] average_waiting_time = sum(waiting_times) / num_customers return average_waiting_time"},{"question":"# Data Aggregation and Visualization with Pandas GroupBy Objective: To assess your understanding of pandas GroupBy operations, including grouping data, applying aggregation functions, calculating descriptive statistics, and visualizing results. Problem Statement: You are given a dataset of products sold in a store. The dataset contains the following columns: \'product_id\', \'category\', \'price\', \'quantity_sold\', and \'date_sold\'. Your task is to analyze the product sales data using the pandas GroupBy functionality. Instructions: 1. **Load the Data**: Read the dataset into a pandas DataFrame. 2. **Group the Data**: Group the data by \'category\' and \'date_sold\' (month-wise). 3. **Aggregation**: - Calculate the total revenue (price * quantity_sold) for each group. - Calculate the average price of products sold for each group. - Calculate the total quantity sold for each group. 4. **Descriptive Statistics**: - For each category, calculate the overall monthly average, median, and standard deviation of the total revenue. 5. **Visualization**: - Create a line plot showing the monthly total revenue for each category. - Create a boxplot showing the distribution of average prices for each category across different months. Dataset Example: | product_id | category | price | quantity_sold | date_sold | |------------|-----------|-------|---------------|------------| | 1 | Electronics | 299.99 | 3 | 2023-01-15 | | 2 | Groceries | 15.99 | 10 | 2023-01-15 | | 3 | Clothing | 49.99 | 2 | 2023-01-16 | | ... | ... | ... | ... | ... | Expected Functions: ```python import pandas as pd import matplotlib.pyplot as plt def load_data(file_path): Load the data from the specified CSV file path. Parameters: - file_path: str, path to the CSV file. Returns: - DataFrame: Loaded pandas DataFrame. df = pd.read_csv(file_path) return df def group_and_aggregate(df): Group the data by \'category\' and \'date_sold\' (month-wise) and calculate the required aggregations. Parameters: - df: DataFrame, input data. Returns: - DataFrame: Aggregated DataFrame with total revenue, average price, and total quantity sold. df[\'date_sold\'] = pd.to_datetime(df[\'date_sold\']) df[\'month\'] = df[\'date_sold\'].dt.to_period(\'M\') grouped = df.groupby([\'category\', \'month\']).agg( total_revenue=pd.NamedAgg(column=\'price\', aggfunc=lambda x: (x * df[\'quantity_sold\']).sum()), average_price=pd.NamedAgg(column=\'price\', aggfunc=\'mean\'), total_quantity_sold=pd.NamedAgg(column=\'quantity_sold\', aggfunc=\'sum\') ).reset_index() return grouped def calculate_descriptive_stats(grouped_df): Calculate descriptive statistics for each category. Parameters: - grouped_df: DataFrame, aggregated data. Returns: - DataFrame: Descriptive statistics with average, median, and standard deviation of total revenue. desc_stats = grouped_df.groupby(\'category\')[\'total_revenue\'].agg([\'mean\', \'median\', \'std\']).reset_index() return desc_stats def plot_data(grouped_df): Create visualizations for the grouped data. Parameters: - grouped_df: DataFrame, aggregated data. Returns: - None # Line plot of monthly total revenue for each category grouped_df.pivot(index=\'month\', columns=\'category\', values=\'total_revenue\').plot(kind=\'line\') plt.title(\'Monthly Total Revenue by Category\') plt.xlabel(\'Month\') plt.ylabel(\'Total Revenue\') plt.legend(title=\'Category\') plt.show() # Boxplot of average prices for each category grouped_df.boxplot(column=\'average_price\', by=\'category\', grid=False) plt.title(\'Average Price Distribution by Category\') plt.suptitle(\'\') # Suppress the default title plt.xlabel(\'Category\') plt.ylabel(\'Average Price\') plt.show() # Example Usage if __name__ == \\"__main__\\": file_path = \'path_to_sales_data.csv\' df = load_data(file_path) grouped_df = group_and_aggregate(df) desc_stats = calculate_descriptive_stats(grouped_df) plot_data(grouped_df) print(desc_stats) ``` Constraints: - Handle missing or null values as appropriate for aggregations. - Ensure that the plot axes are appropriately labeled and titles are provided. You are expected to submit: 1. The complete Python script with the functions described above. 2. A brief explanation of each step in your code.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_data(file_path): Load the data from the specified CSV file path. Parameters: - file_path: str, path to the CSV file. Returns: - DataFrame: Loaded pandas DataFrame. df = pd.read_csv(file_path) return df def group_and_aggregate(df): Group the data by \'category\' and \'date_sold\' (month-wise) and calculate the required aggregations. Parameters: - df: DataFrame, input data. Returns: - DataFrame: Aggregated DataFrame with total revenue, average price, and total quantity sold. df[\'date_sold\'] = pd.to_datetime(df[\'date_sold\']) df[\'month\'] = df[\'date_sold\'].dt.to_period(\'M\') grouped = df.groupby([\'category\', \'month\']).agg( total_revenue=pd.NamedAgg(column=\'price\', aggfunc=lambda x: (x * df.loc[x.index, \'quantity_sold\']).sum()), average_price=pd.NamedAgg(column=\'price\', aggfunc=\'mean\'), total_quantity_sold=pd.NamedAgg(column=\'quantity_sold\', aggfunc=\'sum\') ).reset_index() return grouped def calculate_descriptive_stats(grouped_df): Calculate descriptive statistics for each category. Parameters: - grouped_df: DataFrame, aggregated data. Returns: - DataFrame: Descriptive statistics with average, median, and standard deviation of total revenue. desc_stats = grouped_df.groupby(\'category\')[\'total_revenue\'].agg([\'mean\', \'median\', \'std\']).reset_index() return desc_stats def plot_data(grouped_df): Create visualizations for the grouped data. Parameters: - grouped_df: DataFrame, aggregated data. Returns: - None # Line plot of monthly total revenue for each category grouped_df.pivot(index=\'month\', columns=\'category\', values=\'total_revenue\').plot(kind=\'line\') plt.title(\'Monthly Total Revenue by Category\') plt.xlabel(\'Month\') plt.ylabel(\'Total Revenue\') plt.legend(title=\'Category\') plt.show() # Boxplot of average prices for each category grouped_df.boxplot(column=\'average_price\', by=\'category\', grid=False) plt.title(\'Average Price Distribution by Category\') plt.suptitle(\'\') # Suppress the default title plt.xlabel(\'Category\') plt.ylabel(\'Average Price\') plt.show() # Example Usage if __name__ == \\"__main__\\": file_path = \'path_to_sales_data.csv\' df = load_data(file_path) grouped_df = group_and_aggregate(df) desc_stats = calculate_descriptive_stats(grouped_df) plot_data(grouped_df) print(desc_stats)"},{"question":"**Coding Assessment Question** Using the seaborn \\"penguins\\" dataset, create a series of visualizations to analyze the distribution of the penguins\' flipper lengths. Your task is to generate the following plots: 1. A histogram of the penguins\' flipper lengths. 2. A histogram of the penguins\' flipper lengths normalized to show proportions. 3. Faceted histograms of the flipper lengths by the island, normalized by proportions within each island. 4. A stacked bar plot to represent the distribution of flipper lengths for each sex, stacked by island. For each plot, use appropriate bin sizes to represent the distributions accurately. You are required to: - Ensure your plots are clearly labeled and aesthetically pleasing. - Use seaborn\'s `so.Plot` and associated functions to create the plots. You can use the following code snippet to load the dataset: ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` **Expected Output:** Your function should generate the requested plots as described above. ```python import seaborn.objects as so from seaborn import load_dataset def plot_penguin_flipper_lengths(): penguins = load_dataset(\\"penguins\\") # Plot 1: Histogram of flipper lengths p1 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist()) p1.show() # Plot 2: Proportion histogram of flipper lengths p2 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(stat=\\"proportion\\")) p2.show() # Plot 3: Faceted proportion histograms by island p3 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\").add(so.Bars(), so.Hist(stat=\\"proportion\\")) p3.show() # Plot 4: Stacked bar plot of flipper lengths by sex and island p4 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(), so.Stack(), color=\\"sex\\") p4.show() # Call the function to generate the plots plot_penguin_flipper_lengths() ``` **Constraints:** - Ensure all plots are generated within one function. - Handle missing values in the dataset appropriately. - Your solution should work efficiently with the given dataset size.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_penguin_flipper_lengths(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Drop missing values to avoid errors in plotting penguins = penguins.dropna(subset=[\\"flipper_length_mm\\", \\"island\\", \\"sex\\"]) # Plot 1: Histogram of flipper lengths plt.figure() p1 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist()) p1.show() # Plot 2: Proportion histogram of flipper lengths plt.figure() p2 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(stat=\\"proportion\\")) p2.show() # Plot 3: Faceted proportion histograms by island plt.figure() p3 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\").add(so.Bars(), so.Hist(stat=\\"proportion\\")) p3.show() # Plot 4: Stacked bar plot of flipper lengths by sex and island plt.figure() p4 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(), so.Hist(), so.Stack(), color=\\"sex\\").facet(\\"island\\") p4.show() # Call the function to generate the plots plot_penguin_flipper_lengths()"},{"question":"Problem Statement You are tasked with creating a simulator for a simple lottery system using the `random` module in Python. In this lottery, participants can purchase multiple tickets, and each ticket has a chance of winning various prizes. The prizes are awarded randomly based on predefined probabilities. Your job is to simulate this lottery and return the results of the draw. Requirements 1. Write a function `initialize_lottery(participants, tickets_per_participant, prizes, weights)`. - `participants`: a list of strings where each string is the name of a participant. - `tickets_per_participant`: an integer representing the number of tickets each participant buys. - `prizes`: a list of strings where each string is the name of a prize. - `weights`: a list of probabilities (float values) corresponding to the prizes. The length of the `weights` list should be the same as the `prizes` list. 2. Write a function `draw_winners(lottery)`: - `lottery`: the result of the `initialize_lottery` function. - Returns a dictionary where the keys are the participants\' names, and the values are lists of prizes they won. Function Details **`initialize_lottery(participants, tickets_per_participant, prizes, weights)`**: - This function should distribute the tickets among the participants. Each participant will have a certain number of tickets. - Randomly initialize the lottery by combining the participants, their tickets, the available prizes, and their respective weights. **`draw_winners(lottery)`**: - This function should simulate the drawing of lottery tickets and allocate the prizes according to the specified weights. - Each winner is chosen based on the `random.choices()` function with the given weights. - Ensure that each ticket has a chance to win, and the prizes are given out based on their probability. Example ```python def initialize_lottery(participants, tickets_per_participant, prizes, weights): import random lottery = {} for participant in participants: lottery[participant] = random.choices(prizes, weights, k=tickets_per_participant) return lottery def draw_winners(lottery): winners = { participant: [] for participant in lottery } for participant, tickets in lottery.items(): for ticket in tickets: winners[participant].append(ticket) return winners # Example usage: participants = [\\"Alice\\", \\"Bob\\", \\"Charlie\\"] tickets_per_participant = 3 prizes = [\\"Car\\", \\"Bike\\", \\"TV\\", \\"Laptop\\"] weights = [0.1, 0.3, 0.4, 0.2] lottery = initialize_lottery(participants, tickets_per_participant, prizes, weights) winners = draw_winners(lottery) print(winners) ``` Expected output might look something like: ```python { \'Alice\': [\'Bike\', \'Car\', \'TV\'], \'Bob\': [\'TV\', \'Laptop\', \'TV\'], \'Charlie\': [\'Bike\', \'Bike\', \'TV\'] } ``` Constraints and Notes - Assume that the input lists `participants`, `prizes`, and `weights` are non-empty and valid. - Each participant can have up to 100 tickets. - The sum of weights should be 1.0 to represent a valid probability distribution. - Your solution should use functions from the `random` module effectively. Evaluate students on the correctness of their implementation, the use of the `random` module, and how well they handle the simulation of the lottery system.","solution":"import random def initialize_lottery(participants, tickets_per_participant, prizes, weights): lottery = {} for participant in participants: tickets = [random.choices(prizes, weights)[0] for _ in range(tickets_per_participant)] lottery[participant] = tickets return lottery def draw_winners(lottery): winners = {participant: [] for participant in lottery} for participant, tickets in lottery.items(): for prize in tickets: winners[participant].append(prize) return winners"},{"question":"**Objective: Serialization and Deserialization with Marshaling in Python** **Problem Statement:** You are tasked with implementing functions to serialize and deserialize a series of Python objects to and from a file using the `marshal` module. This exercise aims to test your understanding of file operations, error handling, and working with binary data formats. **Requirements:** 1. **Write a function `serialize_objects_to_file(objects, file_path, version)` that:** - Takes a list of Python objects (`objects`), a file path (`file_path`), and a version (integer: 0, 1, or 2) as input. - Serializes and writes each object in the list to the specified file using the specified version. - Handles any exceptions that may occur during file operations or serialization, raising a custom exception `SerializationError`. 2. **Write a function `deserialize_objects_from_file(file_path)` that:** - Takes a file path (`file_path`) as input. - Reads and deserializes all Python objects from the specified file. - Returns a list of deserialized Python objects. - Handles any exceptions that may occur during file operations or deserialization, raising a custom exception `DeserializationError`. **Constraints and Considerations:** - Use the provided functions from the marshal module for serialization and deserialization. - Ensure that the file is opened in binary mode for both reading and writing. - Maintain the order of objects during serialization and deserialization. - Implement robust error handling to catch and manage potential issues (e.g., `EOFError`, `ValueError`, `TypeError`). - Ensure that the solution handles all specified versions correctly. **Expected Function Signatures:** ```python class SerializationError(Exception): pass class DeserializationError(Exception): pass def serialize_objects_to_file(objects: list, file_path: str, version: int): pass def deserialize_objects_from_file(file_path: str) -> list: pass ``` **Example Usage:** ```python objects = [123, \\"hello\\", [1, 2, 3], {\\"key\\": \\"value\\"}] file_path = \'data.bin\' version = 2 try: serialize_objects_to_file(objects, file_path, version) deserialized_objects = deserialize_objects_from_file(file_path) print(deserialized_objects) except (SerializationError, DeserializationError) as e: print(f\\"An error occurred: {e}\\") ``` In the example above, the list `objects` is serialized to `data.bin` and then deserialized back to verify that the process is accurate. Make sure your implementation preserves the integrity of the data and order of objects.","solution":"import marshal class SerializationError(Exception): pass class DeserializationError(Exception): pass def serialize_objects_to_file(objects, file_path, version): if version not in (0, 1, 2): raise SerializationError(\\"Invalid version specified. Allowed versions are 0, 1, 2.\\") try: with open(file_path, \'wb\') as file: for obj in objects: marshalled_data = marshal.dumps(obj, version) length = len(marshalled_data) file.write(length.to_bytes(4, byteorder=\'little\')) file.write(marshalled_data) except Exception as e: raise SerializationError(\\"An error occurred during serialization\\") from e def deserialize_objects_from_file(file_path): try: with open(file_path, \'rb\') as file: objects = [] while True: length_bytes = file.read(4) if not length_bytes: break length = int.from_bytes(length_bytes, byteorder=\'little\') marshalled_data = file.read(length) obj = marshal.loads(marshalled_data) objects.append(obj) return objects except Exception as e: raise DeserializationError(\\"An error occurred during deserialization\\") from e"},{"question":"**Objective:** The goal of this question is to test your understanding of PyTorch tensor views, how they share the underlying data, and their impact on tensor contiguity. **Problem Statement:** You are given three tasks to perform using PyTorch tensors: 1. Create a base tensor of size (4 times 4) initialized with random values. 2. Create a view of the base tensor by reshaping it to size (2 times 8). 3. Verify and demonstrate that modifying an element in the view tensor affects the base tensor. 4. Transpose the reshaped tensor and check its contiguity. Then, make the resulting tensor contiguous. 5. Record and print the memory addresses (data pointers) of the base tensor, reshaped view tensor, and transposed tensor before and after making it contiguous. **Function Signature:** ```python import torch def tensor_views(): # Step 1: Create a base tensor of size 4x4 with random values base_tensor = torch.rand(4, 4) # Step 2: Create a view of the base tensor with size 2x8 reshaped_view = base_tensor.view(2, 8) # Check that base and reshaped share the same data: assert base_tensor.storage().data_ptr() == reshaped_view.storage().data_ptr() # Step 3: Modify an element in the view tensor and show it affects the base tensor reshaped_view[0][0] = 3.14 assert base_tensor[0][0] == 3.14 # Step 4: Transpose the reshaped tensor and check if it\'s contiguous transposed_view = reshaped_view.transpose(0, 1) is_contiguous_before = transposed_view.is_contiguous() # Make the transposed view contiguous contiguous_tensor = transposed_view.contiguous() is_contiguous_after = contiguous_tensor.is_contiguous() # Step 5: Print the memory addresses of tensors print(\\"Memory addresses (data pointers):\\") print(f\\"Base tensor: {base_tensor.storage().data_ptr()}\\") print(f\\"Reshaped view tensor: {reshaped_view.storage().data_ptr()}\\") print(f\\"Transposed view tensor before contiguous: {transposed_view.storage().data_ptr()}\\") print(f\\"Transposed view tensor after contiguous: {contiguous_tensor.storage().data_ptr()}\\") print(f\\"Is transposed tensor contiguous before: {is_contiguous_before}\\") print(f\\"Is transposed tensor contiguous after: {is_contiguous_after}\\") # Test the function tensor_views() ``` **Submission Requirements:** - Implement the function `tensor_views` as described above. - Ensure that all operations and checks are performed correctly. - The function should not return any value but must print the required memory addresses and contiguity statuses to verify correctness. **Constraints:** - Use PyTorch version `1.0` or above. - Do not use any external libraries other than PyTorch. **Performance Requirements:** - The operations should be efficient in terms of memory and computational complexity. Avoid unnecessary data copies. This question evaluates your understanding of tensor views, memory sharing, and tensor contiguity within PyTorch. Make sure to follow the problem statement closely and implement all required checks.","solution":"import torch def tensor_views(): # Step 1: Create a base tensor of size 4x4 with random values base_tensor = torch.rand(4, 4) # Step 2: Create a view of the base tensor with size 2x8 reshaped_view = base_tensor.view(2, 8) # Check that base and reshaped share the same data: assert base_tensor.storage().data_ptr() == reshaped_view.storage().data_ptr() # Step 3: Modify an element in the view tensor and show it affects the base tensor reshaped_view[0][0] = 3.14 assert base_tensor[0][0] == 3.14 # Step 4: Transpose the reshaped tensor and check if it\'s contiguous transposed_view = reshaped_view.transpose(0, 1) is_contiguous_before = transposed_view.is_contiguous() # Make the transposed view contiguous contiguous_tensor = transposed_view.contiguous() is_contiguous_after = contiguous_tensor.is_contiguous() # Step 5: Print the memory addresses of tensors print(\\"Memory addresses (data pointers):\\") print(f\\"Base tensor: {base_tensor.storage().data_ptr()}\\") print(f\\"Reshaped view tensor: {reshaped_view.storage().data_ptr()}\\") print(f\\"Transposed view tensor before contiguous: {transposed_view.storage().data_ptr()}\\") print(f\\"Transposed view tensor after contiguous: {contiguous_tensor.storage().data_ptr()}\\") print(f\\"Is transposed tensor contiguous before: {is_contiguous_before}\\") print(f\\"Is transposed tensor contiguous after: {is_contiguous_after}\\")"},{"question":"**Objective**: Demonstrate your understanding of the `optparse` library by creating a command-line utility that includes custom option types, extended actions, and appropriate error handling. **Problem Description**: You are tasked with creating a Python script called `file_processor.py` that processes different files based on command-line options provided by the user. The script should be capable of handling multiple file processing tasks, including reading, converting file data to different formats, and outputting the results to a specified file. You will use the `optparse` library to parse these command-line options and extend `optparse` to add custom behavior. **Requirements**: 1. **Basic Options**: - A required option for specifying the input file, `-i` or `--input`. - An optional output file, `-o` or `--output`. If not provided, print results to `stdout`. - A verbosity flag, `-v` or `--verbose`, which increases the verbosity level. - A quiet mode, `-q` or `--quiet`, which suppresses non-error output (overrides verbosity). 2. **Extended Actions**: - Add a new action `extend`, which allows an option to take a comma-separated list and appends these values to a list. - Create a new option `--process` that accepts a list of processing steps (e.g., `--process=step1,step2`). 3. **Custom Callback**: - Define a custom option `--validate` that does not take an argument but, when used, calls a callback function to validate the input file path. 4. **Error Handling**: - Handle cases where required options are missing. - Validate the input file path exists; raise an appropriate error if not. 5. **Help and Usage**: - Provide detailed help messages for each option. - Display a version of the script using a `--version` option. **Input Format**: - Command-line arguments as specified in the options. **Output Format**: - Processed file output to `stdout` or to a specified output file. - Error messages should be printed to `stderr`. **Example**: ```shell python file_processor.py --input=data.txt --process=convert,filter --output=result.txt --verbose ``` **Constraints**: - Use the `optparse` library. - Extend `optparse` as necessary to meet the requirements. **Implementation Details**: 1. Define the `OptionParser` with required configurations. 2. Add necessary options with appropriate actions and handlers. 3. Implement the `extend` action to handle comma-separated lists. 4. Define a callback function for `--validate` to check file existence. 5. Implement the main logic for processing the input file according to the specified options and actions. ```python import os from optparse import OptionParser, Option, OptionValueError # Custom type checking function for file existence def check_file(option, opt, value): if not os.path.exists(value): raise OptionValueError(f\\"The file \'{value}\' does not exist.\\") return value # Custom option class to add \'extend\' action class ExtendedOption(Option): ACTIONS = Option.ACTIONS + (\\"extend\\",) STORE_ACTIONS = Option.STORE_ACTIONS + (\\"extend\\",) TYPED_ACTIONS = Option.TYPED_ACTIONS + (\\"extend\\",) ALWAYS_TYPED_ACTIONS = Option.ALWAYS_TYPED_ACTIONS + (\\"extend\\",) def take_action(self, action, dest, opt, value, values, parser): if action == \\"extend\\": lvalue = value.split(\\",\\") values.ensure_value(dest, []).extend(lvalue) else: Option.take_action(self, action, dest, opt, value, values, parser) # Callback for validating input file def validate_callback(option, opt_str, value, parser): if not os.path.exists(parser.values.input): raise OptionValueError(f\\"The input file \'{parser.values.input}\' does not exist.\\") # Main function to setup the parser and handle the logic def main(): parser = OptionParser(option_class=ExtendedOption, usage=\\"Usage: %prog [options]\\", version=\\"%prog 1.0\\") parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input\\", type=\\"string\\", help=\\"Specify the input file\\", metavar=\\"FILE\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output\\", type=\\"string\\", help=\\"Specify the output file\\", metavar=\\"FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"count\\", dest=\\"verbose\\", help=\\"Increase verbosity level\\") parser.add_option(\\"-q\\", \\"--quiet\\", action=\\"store_false\\", dest=\\"verbose\\", help=\\"Suppress non-error output\\") parser.add_option(\\"--process\\", action=\\"extend\\", type=\\"string\\", dest=\\"process\\", help=\\"Specify processing steps\\") parser.add_option(\\"--validate\\", action=\\"callback\\", callback=validate_callback, help=\\"Validate input file existence\\") parser.set_defaults(verbose=True) # Default verbosity (options, args) = parser.parse_args() # Ensure the required input option is present if not options.input: parser.error(\\"The --input option is required.\\") # Placeholder for actual file processing logic if options.verbose: print(f\\"Processing file: {options.input}\\") if options.process: print(f\\"Processing steps: {\', \'.join(options.process)}\\") if options.output: print(f\\"Output will be written to: {options.output}\\") else: print(\\"Output will be printed to stdout\\") # Implement the actual logic for reading, processing, and outputting file data if __name__ == \\"__main__\\": main() ``` **Explanation**: - The above script defines several command-line options using `optparse`. - An extended `Option` class is created to add support for the `extend` action. - A callback function is defined for the `--validate` option to check if the input file exists. - The script includes basic error handling and verbose output according to the `verbose` and `quiet` options.","solution":"import os from optparse import OptionParser, Option, OptionValueError # Custom type checking function for file existence def check_file(option, opt, value): if not os.path.exists(value): raise OptionValueError(f\\"The file \'{value}\' does not exist.\\") return value # Custom option class to add \'extend\' action class ExtendedOption(Option): ACTIONS = Option.ACTIONS + (\\"extend\\",) STORE_ACTIONS = Option.STORE_ACTIONS + (\\"extend\\",) TYPED_ACTIONS = Option.TYPED_ACTIONS + (\\"extend\\",) ALWAYS_TYPED_ACTIONS = Option.ALWAYS_TYPED_ACTIONS + (\\"extend\\",) def take_action(self, action, dest, opt, value, values, parser): if action == \\"extend\\": lvalue = value.split(\\",\\") values.ensure_value(dest, []).extend(lvalue) else: Option.take_action(self, action, dest, opt, value, values, parser) # Callback for validating input file def validate_callback(option, opt_str, value, parser): if not os.path.exists(parser.values.input): raise OptionValueError(f\\"The input file \'{parser.values.input}\' does not exist.\\") # Main function to setup the parser and handle the logic def main(): parser = OptionParser(option_class=ExtendedOption, usage=\\"Usage: %prog [options]\\", version=\\"%prog 1.0\\") parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input\\", type=\\"string\\", help=\\"Specify the input file\\", metavar=\\"FILE\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output\\", type=\\"string\\", help=\\"Specify the output file\\", metavar=\\"FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"count\\", dest=\\"verbose\\", help=\\"Increase verbosity level\\") parser.add_option(\\"-q\\", \\"--quiet\\", action=\\"store_false\\", dest=\\"verbose\\", help=\\"Suppress non-error output\\") parser.add_option(\\"--process\\", action=\\"extend\\", type=\\"string\\", dest=\\"process\\", help=\\"Specify processing steps\\") parser.add_option(\\"--validate\\", action=\\"callback\\", callback=validate_callback, help=\\"Validate input file existence\\") parser.set_defaults(verbose=None) # Default verbosity (options, args) = parser.parse_args() # Ensure the required input option is present if not options.input: parser.error(\\"The --input option is required.\\") # Placeholder for actual file processing logic if options.verbose and options.verbose > 0: print(f\\"Processing file: {options.input}\\") if options.process: print(f\\"Processing steps: {\', \'.join(options.process)}\\") if options.output: print(f\\"Output will be written to: {options.output}\\") else: print(\\"Output will be printed to stdout\\") if options.quiet: options.verbose = None # Implement the actual logic for reading, processing, and outputting file data if __name__ == \\"__main__\\": main()"},{"question":"**Question: Secure User Authentication System** You are tasked with implementing a secure user authentication system. This system will prompt users to enter their username and password securely. You must ensure that passwords are input without being displayed on the screen. After the user inputs their credentials, match them against a predefined list of valid usernames and hashed passwords to authenticate the user. # Requirements 1. **Input and Output:** - The system will first retrieve the current user\'s login name using `getpass.getuser()`. - It will then prompt the user to enter their password using `getpass.getpass()`. - If the entered password matches the hashed password stored for that user, print `Authentication successful`. - If the entered password does not match, print `Authentication failed`. 2. **Predefined Credentials:** - Store the predefined usernames and their corresponding hashed passwords in a dictionary. - For hashing, you can use the SHA-256 algorithm provided by the `hashlib` module. 3. **Function Implementation:** - Implement a function `check_password(username, entered_password)` that takes the username and the entered password as inputs. - The function should hash the entered password and compare it with the stored hashed password for the given username. - Return `True` if the passwords match, otherwise return `False`. 4. **Constraints:** - Do not echo the password when entered by the user. - Handle invalid user inputs gracefully. # Example: ```python import getpass import hashlib # Predefined credentials (username: hashed_password) credentials = { \'user1\': \'5e884898da28047151d0e56f8dc629...\', \'user2\': \'ee26b0dd4af7e749aa1a8ee3c10ae9...\' } def hash_password(password): return hashlib.sha256(password.encode()).hexdigest() def check_password(username, entered_password): if username in credentials: entered_hash = hash_password(entered_password) return credentials[username] == entered_hash return False def authenticate_user(): username = getpass.getuser() print(f\\"Hello, {username}\\") password = getpass.getpass(prompt=\'Enter your password: \') if check_password(username, password): print(\\"Authentication successful\\") else: print(\\"Authentication failed\\") # Call the function to authenticate the user authenticate_user() ``` **Instructions:** - Develop the implementation for the `check_password` function. - Use the `getpass` module to handle password input securely. - Verify that the user\'s entered password matches the stored hashed password. **Notes:** - Ensure you handle edge cases where the username might not be in the predefined list of credentials. - Make sure your code is clean and well-documented.","solution":"import getpass import hashlib # Predefined credentials (username: hashed_password) credentials = { \'user1\': hashlib.sha256(\'password123\'.encode()).hexdigest(), \'user2\': hashlib.sha256(\'mysecurepassword\'.encode()).hexdigest(), } def hash_password(password): Hash the given password using SHA-256 and return the hexadecimal digest. return hashlib.sha256(password.encode()).hexdigest() def check_password(username, entered_password): Check if the entered password matches the stored hashed password for the given username. if username in credentials: entered_hash = hash_password(entered_password) return credentials[username] == entered_hash return False def authenticate_user(): Authenticate the user by prompting for their username and password. username = getpass.getuser() print(f\\"Hello, {username}\\") password = getpass.getpass(prompt=\'Enter your password: \') if check_password(username, password): print(\\"Authentication successful\\") else: print(\\"Authentication failed\\")"},{"question":"# **Socket Programming Challenge** **Objective:** Your task is to implement both a socket client and server in Python. The server should handle multiple clients using non-blocking sockets and the `select` function. The server should accept messages from the clients, process them, and echo back the processed result. **Instructions:** 1. **Client Implementation:** - Create a client socket. - Connect to the server on localhost and port `12345`. - Send a message to the server. - Receive the echoed message from the server. - Print the received message to the console. 2. **Server Implementation:** - Create a server socket. - Bind the server to `localhost` and port `12345`. - Set the server to non-blocking mode. - Use `select` to handle multiple client connections. - For each connected client, receive a message, process it (convert it to uppercase), and send the processed message back to the client. 3. **Communication Protocol:** - Clients will send messages of fixed length `MSGLEN = 1024` bytes. - The server should read exactly `MSGLEN` bytes from each client and echo back the processed message. **Constraints:** - The implementation should handle any number of clients efficiently. - The server should not block when waiting for messages from clients. ```python import socket import select # Constants HOST = \'localhost\' PORT = 12345 MSGLEN = 1024 # Server Implementation def server(): # Create an INET, STREAMing socket for the server serversocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) serversocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) serversocket.bind((HOST, PORT)) serversocket.listen(5) serversocket.setblocking(False) inputs = [serversocket] outputs = [] while True: readable, writable, exceptional = select.select(inputs, outputs, inputs) for s in readable: if s is serversocket: (clientsocket, address) = s.accept() clientsocket.setblocking(False) inputs.append(clientsocket) else: data = s.recv(MSGLEN) if data: s.send(data.upper()) else: inputs.remove(s) s.close() for s in exceptional: inputs.remove(s) if s in outputs: outputs.remove(s) s.close() # Client Implementation def client(message): s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.connect((HOST, PORT)) s.sendall(message.ljust(MSGLEN).encode(\'utf-8\')) data = s.recv(MSGLEN) print(\'Received:\', data.decode(\'utf-8\')) s.close() if __name__ == \\"__main__\\": import threading server_thread = threading.Thread(target=server) server_thread.start() import time time.sleep(1) # Give the server a moment to start client(\\"Hello, Server!\\") client(\\"Another message\\") ``` **Explanation:** In the `server()` function, we create a non-blocking server socket that listens for client connections. Using the `select` function, the server can handle multiple clients simultaneously by managing readable and writable sockets. Each connected client sends a message to the server, which converts the message to uppercase and sends it back. In the `client()` function, a client socket connects to the server, sends a fixed-length message, and prints the echoed response. Implement the above instructions and ensure your code is tested and handles multiple client connections efficiently.","solution":"import socket import select # Constants HOST = \'localhost\' PORT = 12345 MSGLEN = 1024 # Server Implementation def server(): # Create an INET, STREAMing socket for the server serversocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) serversocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) serversocket.bind((HOST, PORT)) serversocket.listen(5) serversocket.setblocking(False) inputs = [serversocket] outputs = [] while True: readable, writable, exceptional = select.select(inputs, outputs, inputs) for s in readable: if s is serversocket: (clientsocket, address) = s.accept() clientsocket.setblocking(False) inputs.append(clientsocket) else: data = s.recv(MSGLEN) if data: s.send(data.upper()) else: inputs.remove(s) s.close() for s in exceptional: inputs.remove(s) if s in outputs: outputs.remove(s) s.close() # Client Implementation def client(message): s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.connect((HOST, PORT)) s.sendall(message.ljust(MSGLEN).encode(\'utf-8\')) data = s.recv(MSGLEN) print(\'Received:\', data.decode(\'utf-8\')) s.close()"},{"question":"**Problem Statement:** You are provided with a dataset containing information about different species of penguins. Your task is to use Seaborn\'s `pointplot` function to visualize this data and customize the plots based on specific instructions. **Dataset:** Use the `penguins` dataset provided by Seaborn. **Requirements:** 1. **Basic Grouped Point Plot:** Create a point plot that shows the average body mass (`body_mass_g`) of penguins on different islands (`island`). **Expected Output:** A Seaborn point plot with `island` on the x-axis and `body_mass_g` on the y-axis. 2. **Add Secondary Grouping:** Enhance the plot by adding a secondary grouping using the `sex` variable. Use different colors to differentiate between male and female penguins. **Expected Output:** A Seaborn point plot with `island` on the x-axis, `body_mass_g` on the y-axis, and different colors representing `sex`. 3. **Customize Markers and Linestyles:** Further customize the plot by using different markers (`o`, `s`) and linestyles (`-`, `--`) for the `sex` variable. **Expected Output:** The same plot as before, but with different markers and linestyles for male and female penguins. 4. **Use Standard Deviation for Error Bars:** Modify the plot to show error bars representing the standard deviation of each distribution. **Expected Output:** A point plot similar to the one in step 3, but with error bars representing the standard deviation. 5. **Dodge the Points to Reduce Overplotting:** Create a new point plot showing bill depth (`bill_depth_mm`) of penguins, using `sex` as the primary grouping and `species` as the secondary grouping. Apply dodging to avoid overplotting. **Expected Output:** A Seaborn point plot with `species` on the x-axis, `bill_depth_mm` on the y-axis, and different colors and markers for `sex`. Dodging should be applied to reduce overplotting. 6. **Customize Plot Appearance:** Create a new plot that shows the average body mass (`body_mass_g`) of penguins on different islands (`island`). Customize the appearance by: - Showing 100% prediction intervals as error bars. - Using a diamond (`D`) marker. - Setting the color to `0.5` (gray). - Setting the linestyle to `none`. **Expected Output:** A Seaborn point plot with specified customizations. **Submission:** - Save your code in a script named `penguin_plots.py`. - Ensure your script runs without errors and produces the required plots. - Document your code clearly, explaining each step and customization. **Constraints:** - Use only the Seaborn and Matplotlib libraries for plotting. - Do not modify the dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") def basic_grouped_point_plot(): # Basic Grouped Point Plot sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\") plt.title(\\"Average Body Mass by Island\\") plt.show() def secondary_grouping_plot(): # Add secondary grouping and color differentiation for \'sex\' sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\") plt.title(\\"Average Body Mass by Island and Sex\\") plt.show() def custom_markers_linestyles(): # Customize markers and linestyles for \'sex\' sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"]) plt.title(\\"Customized Markers and Linestyles for Average Body Mass by Island and Sex\\") plt.show() def error_bars_std_deviation(): # Use standard deviation for error bars sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], ci=\\"sd\\") plt.title(\\"Error Bars Representing Standard Deviation for Average Body Mass by Island and Sex\\") plt.show() def dodge_points(): # Dodge the points to reduce overplotting for \'bill_depth_mm\' by \'species\' and \'sex\' sns.pointplot(data=penguins, x=\\"species\\", y=\\"bill_depth_mm\\", hue=\\"sex\\", dodge=True) plt.title(\\"Bill Depth by Species and Sex with Dodging\\") plt.show() def customize_plot_appearance(): # Customized point plot appearance with specified attributes sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", markers=\\"D\\", color=\\"0.5\\", linestyles=\\"none\\", ci=100) plt.title(\\"Customized Average Body Mass by Island with Prediction Intervals\\") plt.show()"},{"question":"**Objective**: To assess your understanding of the `xml.sax` package in Python, particularly your ability to utilize SAX parsing and implement custom handlers to process XML data. **Question**: Create a Python program that processes an XML document containing information about books. Each book in the XML document has the following structure: ```xml <book> <title>Book Title</title> <author>Author Name</author> <year>Year of Publication</year> <price>Price</price> </book> ``` Your task is to write a custom SAX handler that will parse the XML and produce a summary report of all books, showing the title and author of books published after the year 2000. **Specifications**: 1. Implement a custom `ContentHandler` class called `BookHandler`. 2. The `BookHandler` should: - Capture and store necessary data for each book. - Print the title and author of books published after the year 2000. 3. Use the `xml.sax.parseString` function to parse the provided XML string. 4. Handle any exceptions that may occur during parsing. **Input**: An XML string containing the book information. **Output**: A printed summary report of all book titles and authors published after the year 2000. **Example**: Using the following XML string as input: ```xml <library> <book> <title>Python Programming</title> <author>John Doe</author> <year>1999</year> <price>29.99</price> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2005</year> <price>39.99</price> </book> </library> ``` The expected output should be: ``` Title: Advanced Python, Author: Jane Smith ``` **Constraints**: - The XML string will have a well-defined structure but may contain any number of `<book>` entries. - Assume that all `<year>` elements contain valid integer values. **Hints**: - Utilize the `characters`, `startElement`, and `endElement` methods of the `ContentHandler` class. - Use instance variables to keep track of the current book\'s data as you parse through the XML document. **Additional Information**: If an exception occurs during parsing, catch the exception and print a descriptive error message. Provide your implementation of the `BookHandler` class and the code to parse the XML string.","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = 0 self.price = \\"\\" self.books = [] def startElement(self, tag, attributes): self.current_data = tag def characters(self, content): if self.current_data == \\"title\\": self.title = content elif self.current_data == \\"author\\": self.author = content elif self.current_data == \\"year\\": self.year = int(content) elif self.current_data == \\"price\\": self.price = content def endElement(self, tag): if tag == \\"book\\": if self.year > 2000: self.books.append((self.title, self.author)) self.title = \\"\\" self.author = \\"\\" self.year = 0 self.price = \\"\\" self.current_data = \\"\\" def endDocument(self): for title, author in self.books: print(f\\"Title: {title}, Author: {author}\\") def process_books(xml_string): handler = BookHandler() try: xml.sax.parseString(xml_string, handler) except Exception as e: print(f\\"Error parsing XML: {e}\\") # Example usage xml_string = \'\'\'<library> <book> <title>Python Programming</title> <author>John Doe</author> <year>1999</year> <price>29.99</price> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2005</year> <price>39.99</price> </book> </library>\'\'\' process_books(xml_string)"},{"question":"Objective The objective of this task is to assess your ability to perform resampling operations on time series data using the pandas library. You are required to implement a function that resamples a given time series dataset to specified frequencies and applies multiple aggregate functions to analyze the data. Problem Statement Implement a function `resample_and_aggregate` that takes a pandas DataFrame with a DateTimeIndex, a resampling frequency, and a list of aggregate functions. The function should resample the data according to the provided frequency and apply the specified aggregate functions to the resampled data. The result should be a new DataFrame containing the resampled data and aggregate computations. ```python def resample_and_aggregate(df: pd.DataFrame, frequency: str, agg_funcs: List[str]) -> pd.DataFrame: Resamples the input DataFrame to the specified frequency and applies the given aggregate functions. Parameters: df (pd.DataFrame): The input DataFrame with DateTimeIndex. frequency (str): The resampling frequency (e.g., \'D\', \'W\', \'M\', etc.). agg_funcs (List[str]): A list of aggregate functions to apply (e.g., [\'mean\', \'sum\', \'max\']). Returns: pd.DataFrame: A DataFrame containing the resampled data with aggregate computations. pass ``` Input - `df`: A pandas DataFrame with a DateTimeIndex. - `frequency`: A string representing the resampling frequency. - `agg_funcs`: A list of strings representing the aggregate functions to apply (e.g., `[\'mean\', \'sum\', \'max\']`). Output - A pandas DataFrame containing the resampled data with aggregate computations. Constraints - The input DataFrame must have a DateTimeIndex. - Valid frequency strings include those recognized by pandas’ `resample` method (e.g., \'D\', \'W\', \'M\', \'Q\', \'Y\', etc.). - Aggregate functions must be valid pandas aggregation functions (e.g., \'mean\', \'sum\', \'max\', etc.). Example ```python import pandas as pd # Sample data data = { \'value\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] } index = pd.date_range(\'2023-01-01\', periods=10, freq=\'D\') df = pd.DataFrame(data, index=index) # Resampling and aggregating frequency = \'3D\' agg_funcs = [\'mean\', \'sum\'] result = resample_and_aggregate(df, frequency, agg_funcs) # Expected result # value_mean value_sum # 2023-01-01 2.0 6 # 2023-01-04 5.0 15 # 2023-01-07 8.0 24 # 2023-01-10 10.0 10 print(result) ``` Complete the function `resample_and_aggregate` to correctly resample the input DataFrame and apply the aggregate functions. Notes - Use the pandas library for resampling and aggregations. - Ensure the returned DataFrame has appropriate column names for each aggregate function applied (e.g., \'value_mean\', \'value_sum\' for the example above).","solution":"import pandas as pd from typing import List def resample_and_aggregate(df: pd.DataFrame, frequency: str, agg_funcs: List[str]) -> pd.DataFrame: Resamples the input DataFrame to the specified frequency and applies the given aggregate functions. Parameters: df (pd.DataFrame): The input DataFrame with DateTimeIndex. frequency (str): The resampling frequency (e.g., \'D\', \'W\', \'M\', etc.). agg_funcs (List[str]): A list of aggregate functions to apply (e.g., [\'mean\', \'sum\', \'max\']). Returns: pd.DataFrame: A DataFrame containing the resampled data with aggregate computations. # Resample the DataFrame resampled_df = df.resample(frequency).agg(agg_funcs) # Flatten the multi-level columns resampled_df.columns = [\'_\'.join(col).strip() for col in resampled_df.columns.values] return resampled_df"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const o=this.searchQuery.trim().toLowerCase();return o?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(o)||e.solution&&e.solution.toLowerCase().includes(o)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(o=>setTimeout(o,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},F={class:"card-container"},q={key:0,class:"empty-state"},R=["disabled"],O={key:0},N={key:1};function M(o,e,l,h,i,s){const m=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),n(b,null,v(s.displayedPoems,(r,f)=>(a(),w(m,{key:f,poem:r},null,8,["poem"]))),128)),s.displayedPoems.length===0?(a(),n("div",q,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),s.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>s.loadMore&&s.loadMore(...r))},[i.isLoading?(a(),n("span",N,"Loading...")):(a(),n("span",O,"See more"))],8,R)):d("",!0)])}const L=p(D,[["render",M],["__scopeId","data-v-f343115e"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/58.md","filePath":"quotes/58.md"}'),j={name:"quotes/58.md"},H=Object.assign(j,{setup(o){return(e,l)=>(a(),n("div",null,[x(L)]))}});export{Y as __pageData,H as default};
