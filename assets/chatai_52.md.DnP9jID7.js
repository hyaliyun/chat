import{_ as p,o as a,c as i,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(s,e,l,m,n,o){return a(),i("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-429f128d"]]),I=JSON.parse('[{"question":"# Question: Module Importer with Backward Compatibility Python has deprecated the `imp` module in favor of `importlib`. However, you may encounter legacy code that still relies on `imp`. Your task is to write a function that attempts to find and load a module using `importlib`, but falls back to using `imp` if `importlib` is not available (simulating scenarios where the code might be run in older Python environments). Requirements: 1. Write a function `load_module_compat(name, path=None)` that performs the following: * Tries to find and load the module using `importlib`. If successful, it should return the module object. * If `importlib` is not available, it falls back to using `imp` for finding and loading the module. * If the module cannot be found or loaded using either method, it should raise `ImportError`. 2. Do not assume that `importlib` and `imp` will be available in the same runtime. Your function should handle scenarios where: * Only `importlib` is available. * Only `imp` is available (simulating older Python versions). * Neither `importlib` nor `imp` is available, in which case it should raise `ImportError`. 3. Ensure that file handles are properly managed, i.e., they should be closed after usage. Function Signature: ```python def load_module_compat(name: str, path: Optional[List[str]] = None) -> Any: pass ``` Constraints: 1. The input `name` will be a valid module name as a string. 2. The optional `path` parameter, if provided, will be a list of directory names to search for the module. Example Usage: ```python # Assuming the module \'example_module\' exists in the directory \'example_dir\': module = load_module_compat(\'example_module\', [\'example_dir\']) print(module) ``` Notes: - You must handle any exceptions that arise during the loading of modules and provide appropriate error messages. - This question tests your understanding of module import mechanisms, handling deprecated features, and ensuring backward compatibility.","solution":"import sys def load_module_compat(name, path=None): Attempts to load a module using importlib, falling back to imp if needed. :param name: Name of the module to load. :param path: Optional list of paths to search for the module. :return: The loaded module object. :raises ImportError: If the module cannot be found or loaded. try: import importlib.util spec = importlib.util.find_spec(name, path) if spec is None: raise ImportError(f\\"Module {name} not found using importlib\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module except ImportError as e_importlib: try: import imp file, pathname, description = imp.find_module(name, path) try: module = imp.load_module(name, file, pathname, description) return module finally: if file: file.close() except ImportError as e_imp: raise ImportError(f\\"Module {name} cannot be found. importlib error: {e_importlib}, imp error: {e_imp}\\") # Example usage: # Assuming \'example_module\' exists # module = load_module_compat(\'example_module\', [\'example_dir\']) # print(module)"},{"question":"Custom PyTorch Module Serialization **Objective**: Assess the understanding of creating, saving, and loading custom PyTorch modules, ensuring correct handling of state dictionaries and managing serialization constraints. Problem Statement You are required to create a custom neural network module using PyTorch, save its state dictionary to a file, and then load it back to ensure the model\'s state is preserved correctly. Additionally, demonstrate the impact of tensor view preservation during serialization. Requirements 1. **Custom Module**: Define a custom neural network module named `CustomNet` with the following structure: - Two fully connected (`Linear`) layers: - `fc1`: Takes an input of size 10 and outputs 5. - `fc2`: Takes an input of size 5 and outputs 2. - Include forward pass logic that applies ReLU activation after `fc1`. 2. **Saving the Model**: Save the state dictionary of the `CustomNet` instance to a file named `custom_net_state.pt`. 3. **Loading the Model**: Load the state dictionary from the file and ensure it matches the state of a new instance of `CustomNet`. 4. **Tensor Views**: Demonstrate that tensor views are preserved across saving and loading by: - Creating a tensor and a view of it. - Saving both tensors together. - Modifying the loaded view and showing the changes reflect in the original tensor. Input/Output Formats - **Input**: - None required as the setup and actions are demonstrated within the code. - **Output**: - Print statements showing: - State dictionary before saving. - State dictionary after loading. - The original tensor and its modified view demonstrating the preservation of views. Constraints - Ensure the serialization and deserialization process handles large tensors correctly by keeping views and reducing unnecessary storage size. Performance Requirements - Make efficient use of storage space by avoiding saving large storages if not necessary. - Ensure the preservation of tensor views where applicable. Example ```python import torch import torch.nn as nn import torch.nn.functional as F # Step 1: Define CustomNet class class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) return x # Step 2: Save model state_dict def save_model_state(model, filename): torch.save(model.state_dict(), filename) # Step 3: Load model state_dict def load_model_state(model, filename): state_dict = torch.load(filename) model.load_state_dict(state_dict) # Step 4: Demonstrate tensor view preservation def demonstrate_tensor_view_preservation(): original_tensor = torch.arange(10) tensor_view = original_tensor[::2] torch.save([original_tensor, tensor_view], \'tensor_views.pt\') loaded_original, loaded_view = torch.load(\'tensor_views.pt\') loaded_view *= 2 print(\\"Original Tensor (After Modification):\\", loaded_original) print(\\"View Tensor (After Modification):\\", loaded_view) # Main Execution if __name__ == \\"__main__\\": # CustomNet example model = CustomNet() save_model_state(model, \'custom_net_state.pt\') new_model = CustomNet() load_model_state(new_model, \'custom_net_state.pt\') print(\\"Original Model State:\\", model.state_dict()) print(\\"Loaded Model State:\\", new_model.state_dict()) # Tensor view preservation demonstrate_tensor_view_preservation() ``` Explanation: - **CustomNet**: Implements a basic two-layer neural network. - **save_model_state**: Saves the model\'s state dictionary. - **load_model_state**: Loads the state dictionary into a new model instance. - **demonstrate_tensor_view_preservation**: Ensures tensor views are preserved across save and load procedures. This problem tests the understanding of PyTorch module serialization, state handling, and tensor view preservation.","solution":"import torch import torch.nn as nn import torch.nn.functional as F # Step 1: Define CustomNet class class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) return x # Step 2: Save model state_dict def save_model_state(model, filename): torch.save(model.state_dict(), filename) # Step 3: Load model state_dict def load_model_state(model, filename): state_dict = torch.load(filename) model.load_state_dict(state_dict) # Step 4: Demonstrate tensor view preservation def demonstrate_tensor_view_preservation(): original_tensor = torch.arange(10) tensor_view = original_tensor[::2] torch.save((original_tensor, tensor_view), \'tensor_views.pt\') loaded_original, loaded_view = torch.load(\'tensor_views.pt\') loaded_view *= 2 return loaded_original, loaded_view"},{"question":"You are required to write a set of unit tests for a simple Python module that contains a few functions. Your task is to ensure these functions work correctly by writing comprehensive test cases using the `unittest` module. Additionally, you\'ll need to use some utilities provided in the `test.support` module to enhance your tests. Given Module: `mymodule.py` ```python def add(a, b): return a + b def divide(a, b): if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b def reverse_string(s): if not isinstance(s, str): raise TypeError(\\"Expected a string\\") return s[::-1] ``` Requirements: 1. Write unit tests for all the functions in `mymodule.py` using the `unittest` module. 2. Ensure to cover edge cases, such as: - Adding negative numbers. - Division by zero. - Reversing non-string inputs. 3. Use `test.support.run_unittest` to run your test cases. 4. Use context managers from `test.support` where applicable (e.g., `captured_stdout` or `captured_stdin`). Input: No input from the user. Output: The output should show the results of running the unit tests, indicating which tests passed and which failed. Example: Here is an example of how you might structure one of your test methods using `unittest`: ```python import unittest from test import support import mymodule class TestMyModule(unittest.TestCase): def test_add(self): self.assertEqual(mymodule.add(1, 2), 3) self.assertEqual(mymodule.add(-1, -2), -3) # Add more test cases for the add function def test_divide(self): self.assertRaises(ValueError, mymodule.divide, 1, 0) self.assertEqual(mymodule.divide(4, 2), 2) # Add more test cases for the divide function def test_reverse_string(self): self.assertEqual(mymodule.reverse_string(\\"hello\\"), \\"olleh\\") # Include more test cases for the reverse_string function, including edge cases # Run the tests using a utility function from test.support if __name__ == \\"__main__\\": support.run_unittest(TestMyModule) ``` Write your full solution in the code block below: ```python # Write your complete solution here ```","solution":"def add(a, b): return a + b def divide(a, b): if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b def reverse_string(s): if not isinstance(s, str): raise TypeError(\\"Expected a string\\") return s[::-1]"},{"question":"# Question: Custom Tuple Operations With your understanding of tuple operations in Python, simulate some of the C-level tuple manipulation functions using Python. Implement a Python class `CustomTuple` that mimics some of the provided C-level tuple functionalities. Your class should include the following methods: 1. **`__init__(self, *args)`**: Initialize the `CustomTuple` with a variable number of arguments. 2. **`get_size(self)`**: Return the size of the tuple. 3. **`get_item(self, index)`**: Return the item at the specified index. 4. **`set_item(self, index, value)`**: Modify the tuple by setting the item at the specified index to the given value. 5. **`resize(self, new_size)`**: Resize the tuple to the specified new size, truncating or extending with `None` as necessary. # Constraints: - For `get_item`, if the index is out of bounds, raise an `IndexError`. - For `set_item`, if the index is out of bounds, raise an `IndexError`. - For `resize`, the new size must be non-negative; otherwise, raise a `ValueError`. - When resizing, if the new size is larger, append `None` to the tuple until it reaches the new size. # Example: ```python # Example usage of the CustomTuple class ct = CustomTuple(1, 2, 3) print(ct.get_size()) # Output: 3 print(ct.get_item(1)) # Output: 2 ct.set_item(1, 5) print(ct.get_item(1)) # Output: 5 ct.resize(5) print(ct._data) # Output: (1, 5, 3, None, None) ct.resize(2) print(ct._data) # Output: (1, 5) ``` # Implementation: You need to implement only the class and the specified methods. Here is the class skeleton for your reference: ```python class CustomTuple: def __init__(self, *args): self._data = tuple(args) def get_size(self): Return the size of the tuple. # Your code here def get_item(self, index): Return the item at the specified index. # Your code here def set_item(self, index, value): Modify the tuple by setting the item at the specified index to the given value. # Your code here def resize(self, new_size): Resize the tuple to the specified new size. # Your code here ``` Your solution should correctly implement these methods to fully replicate the required functionalities.","solution":"class CustomTuple: def __init__(self, *args): self._data = list(args) def get_size(self): Return the size of the tuple. return len(self._data) def get_item(self, index): Return the item at the specified index. If the index is out of bounds, raise an IndexError. try: return self._data[index] except IndexError: raise IndexError(\\"Index out of range\\") def set_item(self, index, value): Modify the tuple by setting the item at the specified index to the given value. If the index is out of bounds, raise an IndexError. try: self._data[index] = value except IndexError: raise IndexError(\\"Index out of range\\") def resize(self, new_size): Resize the tuple to the specified new size. If the new size is non-negative, truncate or extend the list with `None` as necessary. If the new size is negative, raise a ValueError. if new_size < 0: raise ValueError(\\"New size must be non-negative\\") if new_size < len(self._data): self._data = self._data[:new_size] else: self._data.extend([None] * (new_size - len(self._data)))"},{"question":"You are tasked with creating a class hierarchy for managing a library system. The system should manage collections of books and journal articles that can be iterated over in various ways. The system should include the following functionalities: 1. Define a base class `LibraryItem` with the following properties and methods: - Properties: - `title` (string): The title of the library item. - `publication_year` (integer): The year the item was published. - Methods: - `__init__(self, title, publication_year)`: Initializes the base properties. - `__str__(self)`: Returns a string representation of the item in the format `{title} ({publication_year})`. 2. Define a derived class `Book` that inherits from `LibraryItem` with the following additional properties and methods: - Properties: - `author` (string): The author of the book. - Methods: - `__init__(self, title, publication_year, author)`: Initializes the properties of `LibraryItem` and the `author` property. - `__str__(self)`: Overrides the base method to include the author in the format `{title} by {author} ({publication_year})`. 3. Define another derived class `JournalArticle` that inherits from `LibraryItem` with the following additional properties and methods: - Properties: - `journal` (string): The journal in which the article was published. - `volume` (string): The volume of the journal. - Methods: - `__init__(self, title, publication_year, journal, volume)`: Initializes the properties of `LibraryItem` and the `journal` and `volume` properties. - `__str__(self)`: Overrides the base method to include the journal and volume in the format `{title} in {journal} vol. {volume} ({publication_year})`. 4. Define an `IterableLibrary` class to manage a collection of `LibraryItem` objects. This class should: - Initialize with an empty list of items. - Provide methods to add `LibraryItem` objects to the collection. - Implement the iterator protocol to allow iteration over the items in the collection. 5. Additionally, implement a generator function within the `IterableLibrary` class to yield all the `LibraryItem` objects published in a specific year. # Input/Output Format - `LibraryItem` class constructor parameters: `title` (str), `publication_year` (int) - `Book` class constructor parameters: `title` (str), `publication_year` (int), `author` (str) - `JournalArticle` class constructor parameters: `title` (str), `publication_year` (int), `journal` (str), `volume` (str) - `IterableLibrary` class methods: - `add_item(self, item: LibraryItem)`: Adds a `LibraryItem` object to the collection. - `__iter__(self)`: Returns an iterator over the collection. - `items_published_in_year(self, year: int)`: Generator function to yield items published in the specified year. # Example Usage ```python # Creating library items book1 = Book(\\"Python Programming\\", 2020, \\"John Doe\\") article1 = JournalArticle(\\"Advanced AI\\", 2021, \\"Journal of AI Research\\", \\"45\\") # Creating the library and adding items library = IterableLibrary() library.add_item(book1) library.add_item(article1) # Iterate over library items for item in library: print(item) # Get items published in a specific year for item in library.items_published_in_year(2021): print(item) ``` # Expected Output ``` Python Programming by John Doe (2020) Advanced AI in Journal of AI Research vol. 45 (2021) Advanced AI in Journal of AI Research vol. 45 (2021) ``` # Constraints 1. The `title` should be a non-empty string. 2. The `publication_year` should be a positive integer. 3. The `author`, `journal`, and `volume` should be non-empty strings. Implement the classes and functions as described to manage the library system and demonstrate iteration and generator functionality.","solution":"class LibraryItem: def __init__(self, title, publication_year): if not title or not isinstance(title, str): raise ValueError(\\"Title must be a non-empty string.\\") if not isinstance(publication_year, int) or publication_year <= 0: raise ValueError(\\"Publication year must be a positive integer.\\") self.title = title self.publication_year = publication_year def __str__(self): return f\\"{self.title} ({self.publication_year})\\" class Book(LibraryItem): def __init__(self, title, publication_year, author): super().__init__(title, publication_year) if not author or not isinstance(author, str): raise ValueError(\\"Author must be a non-empty string.\\") self.author = author def __str__(self): return f\\"{self.title} by {self.author} ({self.publication_year})\\" class JournalArticle(LibraryItem): def __init__(self, title, publication_year, journal, volume): super().__init__(title, publication_year) if not journal or not isinstance(journal, str): raise ValueError(\\"Journal must be a non-empty string.\\") if not volume or not isinstance(volume, str): raise ValueError(\\"Volume must be a non-empty string.\\") self.journal = journal self.volume = volume def __str__(self): return f\\"{self.title} in {self.journal} vol. {self.volume} ({self.publication_year})\\" class IterableLibrary: def __init__(self): self.items = [] def add_item(self, item): if not isinstance(item, LibraryItem): raise ValueError(\\"Item must be an instance of LibraryItem.\\") self.items.append(item) def __iter__(self): return iter(self.items) def items_published_in_year(self, year): if not isinstance(year, int) or year <= 0: raise ValueError(\\"Year must be a positive integer.\\") for item in self.items: if item.publication_year == year: yield item"},{"question":"**Objective**: Write an asynchronous program that demonstrates your understanding of the `asyncio` event loop and its various methods for handling callbacks, tasks, and network connections. **Problem**: Create a Python script using the `asyncio` library to implement a simple chat server. The server should handle multiple clients concurrently, allowing them to send messages to each other in real-time. Use the following guidelines to implement your solution: 1. **Function Definitions**: - `start_server(host: str, port: int) -> None`: This function should start the asyncio server on the specified `host` and `port`. - `handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None`: This coroutine should manage the interaction with a connected client, reading messages and broadcasting them to other connected clients. - `broadcast_message(message: str, writer: asyncio.StreamWriter) -> None`: This coroutine should send a message to all connected clients except the sender. 2. **Expected Behavior**: - Start the server using the `start_server` function. - Utilize `asyncio.start_server` to handle incoming connections and create new tasks for each client using `asyncio.create_task`. - Use appropriate event loop methods to continuously read from client sockets and broadcast messages. - Ensure the server can handle errors gracefully and close client connections properly. 3. **Input and Output**: - **Input**: No direct input to the functions. - **Output**: The program should print connection messages, broadcast messages, and disconnection messages to the console. 4. **Constraints**: - Use the `asyncio` library exclusively for handling asynchronous operations. - The server should handle at least 10 concurrent clients. - Ensure proper synchronization to prevent race conditions when broadcasting messages. **Example Usage**: ```python async def main(): host = \'127.0.0.1\' port = 8888 await start_server(host, port) if __name__ == \'__main__\': asyncio.run(main()) ``` **Hints**: - Utilize the `asyncio.StreamReader` and `asyncio.StreamWriter` for socket communication. - Use the event loop\'s methods such as `create_task` for managing concurrent client connections. - Manage client disconnects and error handling using try-except blocks and proper resource cleanup. Implement the required functions and ensure your server can handle multiple clients efficiently using asyncio. **Submission**: - Submit a single Python file containing the complete implementation of the chat server.","solution":"import asyncio clients = {} async def broadcast_message(message: str, writer: asyncio.StreamWriter) -> None: for client_writer in clients.values(): if client_writer != writer: client_writer.write(message.encode()) await client_writer.drain() async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: addr = writer.get_extra_info(\'peername\') print(f\\"Connected by {addr}\\") clients[addr] = writer try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received message from {addr}: {message}\\") await broadcast_message(message, writer) except Exception as e: print(f\\"Error with {addr}: {e}\\") finally: print(f\\"Disconnected from {addr}\\") del clients[addr] writer.close() await writer.wait_closed() async def start_server(host: str, port: int) -> None: server = await asyncio.start_server(handle_client, host, port) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addrs}\') async with server: await server.serve_forever() async def main(): host = \'127.0.0.1\' port = 8888 await start_server(host, port) if __name__ == \'__main__\': asyncio.run(main())"},{"question":"**Objective:** The purpose of this task is to assess your understanding of named tensors in PyTorch and how various tensor operations handle these named dimensions. You will be creating tensors, performing operations on them, and ensuring that the correct naming conventions are maintained as per the PyTorch documentation. **Problem Statement:** 1. Create two named tensors `A` and `B` as described below: - `A`: A 3-dimensional tensor of shape `(4, 3, 2)` with names `(\'Batch\', \'Height\', \'Width\')`. - `B`: A 3-dimensional tensor of shape `(4, 2, 3)` with names `(\'Batch\', \'Width\', \'Channel\')`. 2. Perform a matrix multiplication such that you contract the `Width` dimension and retain the `Batch` dimension from `A` and the `Channel` dimension from `B`. Assign the result to a tensor `C` and print its names. 3. Create another tensor `D` by summing the tensor `C` over the `Channel` dimension. Ensure the names of the dimensions are correctly propagated and print the names of `D`. # Requirements: - **Input**: None. - **Output**: - Print the names of tensor `C`. - Print the names of tensor `D`. # Constraints: - You must use named tensors to ensure that the dimensions are handled appropriately during operations. # Example Output: ```python # Expected output # Names of tensor C: (\'Batch\', \'Height\', \'Channel\') # Names of tensor D: (\'Batch\', \'Height\') ``` # Notes: 1. You can use random data to initialize the tensors for simplicity. 2. Make sure you handle the named dimensions accordingly during the operations. 3. Ensure that your operations are consistent with the name inference rules mentioned in the documentation. # Implementation ```python import torch # Step 1: Create named tensors A and B A = torch.randn(4, 3, 2, names=(\'Batch\', \'Height\', \'Width\')) B = torch.randn(4, 2, 3, names=(\'Batch\', \'Width\', \'Channel\')) # Step 2: Perform matrix multiplication to contract Width and retain Batch and Channel C = torch.matmul(A.align_to(\'Batch\', \'Height\', \'Width\'), B.align_to(\'Batch\', \'Width\', \'Channel\')).refine_names(\'Batch\', \'Height\', \'Channel\') print(\\"Names of tensor C:\\", C.names) # Step 3: Sum over Channel dimension and print names of resulting tensor D D = C.sum(\'Channel\') print(\\"Names of tensor D:\\", D.names) ```","solution":"import torch # Step 1: Create named tensors A and B A = torch.randn(4, 3, 2, names=(\'Batch\', \'Height\', \'Width\')) B = torch.randn(4, 2, 3, names=(\'Batch\', \'Width\', \'Channel\')) # Step 2: Perform matrix multiplication to contract Width and retain Batch and Channel C = torch.matmul(A.align_to(\'Batch\', \'Height\', \'Width\'), B.align_to(\'Batch\', \'Width\', \'Channel\')).refine_names(\'Batch\', \'Height\', \'Channel\') print(\\"Names of tensor C:\\", C.names) # Step 3: Sum over Channel dimension and print names of resulting tensor D D = C.sum(\'Channel\') print(\\"Names of tensor D:\\", D.names)"},{"question":"Coding Assessment Question # Problem Statement You are tasked with designing a concurrent system using Python\'s `multiprocessing` module to manage a real-world scenario. The problem involves processing and aggregating data from multiple sources concurrently. # Problem Scenario Consider a scenario where you have multiple sensors, each generating data readings at regular intervals. You have to design a concurrent system that collects data from these sensors, processes it, and maintains a running average of the readings from all sensors. # Objectives 1. **Read Sensor Data**: Simulate reading data from multiple sensors using separate processes. 2. **Process Data**: Each process representing a sensor should send its data to a centralized process for aggregation. 3. **Aggregate Data**: The central process should maintain a running average of all sensor readings and print it periodically. 4. **Synchronization**: Ensure synchronization in accessing shared resources. 5. **Graceful Termination**: Ensure all processes terminate gracefully. # Requirements 1. **Sensor Process**: - Each sensor process should generate a random reading between `1` and `100` every second. - Send the generated reading to the central process. 2. **Central Process**: - Maintain a running average of all received sensor readings. - Print the running average every 5 seconds. 3. **Shared Data Management**: - Use the `multiprocessing.Manager` for managing shared data safely. - Use Locks to handle synchronization. 4. **Termination**: - Ensure all processes, including sensor and central processes, terminate gracefully. # Input & Output Requirements - There is no direct input from the user. - The output should be the running average printed to the console every 5 seconds. # Constraints - You should use `multiprocessing.Process` for creating sensor processes. - Use `multiprocessing.Manager` for creating shared state. - Use `multiprocessing.Lock` for synchronization. - Ensure the solution works across different platforms (Windows, UNIX). # Function Signatures ```python import multiprocessing import random import time def sensor_process(sensor_id, data_queue): Function to simulate a sensor reading data. Args: sensor_id (int): Unique identifier for the sensor. data_queue (multiprocessing.Queue): Queue to send data to the central process. pass def central_process(data_queue, running_average, lock): Function to aggregate data from sensors and calculate running average. Args: data_queue (multiprocessing.Queue): Queue to receive data from sensor processes. running_average (multiprocessing.Value): Shared value of the running average. lock (multiprocessing.Lock): Lock for synchronizing access to shared value. pass def main(): Main function to create and manage sensor and central processes. pass if __name__ == \\"__main__\\": main() ``` # Example Output ``` Running average after 5 seconds: 56.3 Running average after 10 seconds: 52.8 Running average after 15 seconds: 54.2 ... ``` # Implementation Details 1. **Sensor Process Function** - Generates a random integer between `1` and `100` every second. - Puts the reading into a shared queue. 2. **Central Process Function** - Continuously reads from the queue, updates the running average, and prints it every 5 seconds with a synchronized access to shared values using locks. 3. **Main Function** - Initializes shared resources using a manager. - Creates multiple sensor processes and a single central process. - Ensures proper starting, synchronization, and joining of processes. Write a complete implementation of this system, ensuring correct handling of processes and shared resources.","solution":"import multiprocessing import random import time import numpy as np def sensor_process(sensor_id, data_queue): Function to simulate a sensor reading data. Args: sensor_id (int): Unique identifier for the sensor. data_queue (multiprocessing.Queue): Queue to send data to the central process. while True: reading = random.randint(1, 100) data_queue.put(reading) time.sleep(1) def central_process(data_queue, running_average, lock): Function to aggregate data from sensors and calculate running average. Args: data_queue (multiprocessing.Queue): Queue to receive data from sensor processes. running_average (multiprocessing.Value): Shared value of the running average. lock (multiprocessing.Lock): Lock for synchronizing access to shared value. readings = [] while True: while not data_queue.empty(): reading = data_queue.get() readings.append(reading) if readings: with lock: current_average = np.mean(np.array(readings)) running_average.value = current_average readings.clear() # reset readings list print(f\\"Running average: {running_average.value:.2f}\\") time.sleep(5) def main(): Main function to create and manage sensor and central processes. manager = multiprocessing.Manager() data_queue = manager.Queue() running_average = manager.Value(\'d\', 0.0) lock = multiprocessing.Lock() sensor_processes = [multiprocessing.Process(target=sensor_process, args=(i, data_queue)) for i in range(5)] central_proc = multiprocessing.Process(target=central_process, args=(data_queue, running_average, lock)) for proc in sensor_processes: proc.start() central_proc.start() try: while True: time.sleep(1) except KeyboardInterrupt: for proc in sensor_processes: proc.terminate() central_proc.terminate() if __name__ == \\"__main__\\": main()"},{"question":"Seaborn Color Palettes Objective: You are required to demonstrate your understanding of seaborn\'s color palette functionalities. The task involves retrieving and visualizing different color palettes, and programmatically applying these palettes to data visualizations. Problem Statement: 1. Write a function `visualize_palette(palette_name, n_colors=10)` that takes in the name of a seaborn color palette and an optional number of colors. The function should: - Retrieve the specified color palette using `sns.color_palette(palette_name, n_colors=n_colors)`. - Display the color palette as a horizontal bar plot using seaborn or matplotlib. 2. Write a function `apply_palette_to_scatterplot(palette_name, dataset, x_col, y_col, hue_col)` that: - Takes in a seaborn color palette name, a dataset (in pandas DataFrame format), column names for the x-axis, y-axis, and the hue. - Applies the specified color palette to a scatter plot of the dataset. - The scatter plot should display distinct colors for different hues as defined by the `hue_col`. 3. Using the `iris` dataset from seaborn\'s built-in datasets: - Visualize the `Set2` palette with 8 colors. - Create a scatter plot of `sepal_length` vs. `sepal_width` colored by `species` using the `Set2` palette. Requirements: - Use the seaborn library for creating and manipulating color palettes. - Use pandas for handling the dataset. - Ensure that your plots are correctly labeled and visually distinguishable. - Provide docstrings for your functions explaining their purpose and parameters. Input and Output Formats: 1. `visualize_palette(palette_name, n_colors=10)` - **Input:** - `palette_name`: str - The name of the seaborn color palette (e.g., \\"Set2\\", \\"pastel\\", \\"husl\\") - `n_colors`: int (optional) - The number of colors to retrieve from the palette (default is 10) - **Output:** A horizontal bar plot visualizing the specified color palette. 2. `apply_palette_to_scatterplot(palette_name, dataset, x_col, y_col, hue_col)` - **Input:** - `palette_name`: str - The name of the seaborn color palette. - `dataset`: pd.DataFrame - The dataset containing the data to be plotted. - `x_col`: str - The column name for the x-axis. - `y_col`: str - The column name for the y-axis. - `hue_col`: str - The column name for the hue (color differentiation). - **Output:** A scatter plot of the dataset with the specified color palette applied. Example Usage: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Visualize the Set2 palette with 8 colors visualize_palette(\\"Set2\\", n_colors=8) # Create a scatter plot using the Set2 palette apply_palette_to_scatterplot(\\"Set2\\", iris, \\"sepal_length\\", \\"sepal_width\\", \\"species\\") ``` Constraints: - You are required to use seaborn and matplotlib for visualizations. - Ensure that your code is efficient and avoids redundancy. - Handle any potential errors gracefully and provide meaningful error messages.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_palette(palette_name, n_colors=10): Visualizes a seaborn color palette as a horizontal bar plot. Parameters: - palette_name (str): The name of the seaborn color palette to visualize (e.g., \\"Set2\\"). - n_colors (int): The number of colors to retrieve from the palette (default is 10). palette = sns.color_palette(palette_name, n_colors=n_colors) sns.palplot(palette) plt.title(f\'{palette_name} palette with {n_colors} colors\') plt.show() def apply_palette_to_scatterplot(palette_name, dataset, x_col, y_col, hue_col): Applies a seaborn color palette to a scatter plot of the given dataset. Parameters: - palette_name (str): The name of the seaborn color palette. - dataset (pd.DataFrame): The dataset (as a pandas DataFrame) containing the data to be plotted. - x_col (str): The column name for the x-axis. - y_col (str): The column name for the y-axis. - hue_col (str): The column name for the hue (color differentiation). palette = sns.color_palette(palette_name) sns.scatterplot(data=dataset, x=x_col, y=y_col, hue=hue_col, palette=palette) plt.title(f\'Scatter plot of {x_col} vs {y_col} by {hue_col} using {palette_name} palette\') plt.show() # Example Usage: if __name__ == \\"__main__\\": iris = sns.load_dataset(\\"iris\\") # Visualize the Set2 palette with 8 colors visualize_palette(\\"Set2\\", n_colors=8) # Create a scatter plot using the Set2 palette apply_palette_to_scatterplot(\\"Set2\\", iris, \\"sepal_length\\", \\"sepal_width\\", \\"species\\")"},{"question":"# Advanced Unicode and Encoding Operations in Python Python provides robust support for Unicode strings and various encoding/decoding schemes via its native APIs. The objective of this problem is to test your understanding of these Unicode operations and encoding conversions. Problem Statement You are given a list of Unicode strings and a target encoding. Your tasks are: 1. Decode each string from a certain assumed encoding (UTF-8) to a Unicode object. 2. Transform the Unicode object based on specific character property checks (e.g., convert to uppercase if the character is lowercase). 3. Encode the transformed Unicode objects back to the target encoding and return the list of encoded byte strings. Implement a function: ```python def process_unicode_strings(unicode_strings: List[str], target_encoding: str) -> List[bytes]: Transforms and encodes a list of Unicode strings. Parameters: unicode_strings (List[str]): List of strings to be processed. target_encoding (str): The target encoding for output strings. Returns: List[bytes]: List of byte strings encoded in the target encoding. ``` Detailed Tasks 1. **Decoding**: Assume that all input strings (`unicode_strings`) are encoded in UTF-8 and decode them into Unicode objects (i.e., Python `str`). 2. **Transformation**: - For every character in each Unicode string: - If the character is a lowercase alphabetic character, convert it to uppercase. - Otherwise, leave it as is. 3. **Encoding**: Encode each transformed Unicode string into the specified `target_encoding`. Constraints - The input list `unicode_strings` contains strings encoded in UTF-8. - The `target_encoding` is a valid encoding known to Python’s encoding system. - The output should be a list of byte strings, each encoded in the `target_encoding`. Example ```python input_strings = [\\"hello\\", \\"world\\", \\"python3.10\\"] target_encoding = \\"utf-16\\" result = process_unicode_strings(input_strings, target_encoding) # Expected result should be the list of encoded byte strings in utf-16 for the transformed strings: # [\\"HELLO\\", \\"WORLD\\", \\"PYTHON3.10\\"] # [b\'xffxfeHx00Ex00Lx00Lx00Ox00\', b\'xffxfeWx00Ox00Rx00Lx00Dx00\', b\'xffxfePx00Yx00Tx00Hx00Ox00Nx003x00.x001x000x00\'] ``` Notes - It is crucial to handle decoding exceptions, ensuring your function behaves robustly with non-decodable input. - Validate the `target_encoding`, handle potential errors gracefully, and provide meaningful error messages if the encoding is invalid or unsupported.","solution":"from typing import List def process_unicode_strings(unicode_strings: List[str], target_encoding: str) -> List[bytes]: Transforms and encodes a list of Unicode strings. Parameters: unicode_strings (List[str]): List of strings to be processed. target_encoding (str): The target encoding for output strings. Returns: List[bytes]: List of byte strings encoded in the target encoding. # List to hold the transformed and encoded byte strings result = [] # Iterate over each input string for s in unicode_strings: try: # Decode the string from UTF-8 to a Unicode object unicode_str = s.decode(\'utf-8\') # Transform the Unicode object transformed_str = \'\'.join([char.upper() if char.islower() else char for char in unicode_str]) # Encode the transformed Unicode object into the target encoding encoded_bytes = transformed_str.encode(target_encoding) # Add to the result list result.append(encoded_bytes) except (UnicodeDecodeError, UnicodeEncodeError) as e: # Handle potential decoding/encoding exceptions raise ValueError(f\\"Error processing unicode string \'{s}\': {e}\\") return result"},{"question":"# Python Coding Assessment: Handling Audit Events Objective: Write a Python function that captures specific audit events and logs them for analysis. The function should demonstrate a comprehensive understanding of registering audit hooks and handling audit events in Python. Problem Statement: You are tasked with tracking and logging specific audit events that occur during the execution of a Python program. Specifically, you need to capture the following events: - `builtins.input` - `os.listdir` - `socket.connect` For each of the captured events, log the event name and its arguments in a structured format. Requirements: 1. Implement a function `setup_audit_logger()` that registers a custom audit hook. 2. The audit hook should capture occurrences of the specified events and log the event name and arguments. 3. The logged data should be written to a file `audit_log.txt` in the following format: ``` Event: <event_name> Arguments: <arg1>, <arg2>, ... ``` Here is a function signature to guide you: ```python import sys def setup_audit_logger(): def audit_hook(event, args): # Define the logic for capturing and logging specified events pass # Add the audit hook to capture events sys.addaudithook(audit_hook) # Example of using the setup_audit_logger function setup_audit_logger() # Code to trigger the specified events input(\\"Enter something: \\") os.listdir(\\".\\") import socket s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) s.connect((\\"www.example.com\\", 80)) ``` Constraints: - Only handle the specified audit events (`builtins.input`, `os.listdir`, `socket.connect`). - Ensure that the logging mechanism does not interfere with the program\'s normal execution. - Write logs to the `audit_log.txt` file in append mode to preserve logs across multiple runs. Evaluation: Your solution will be evaluated based on: - Correctness: Properly capturing and logging the specified events. - Robustness: Ensuring that the audit hook handles arguments gracefully and logs them accurately. - Clarity: Clear and readable code, properly documenting the logic within the audit hook. **Note:** You do not need to handle any other audit events apart from the ones specified in the problem statement.","solution":"import os import socket import sys def setup_audit_logger(): def audit_hook(event, args): if event in [\'builtins.input\', \'os.listdir\', \'socket.connect\']: with open(\\"audit_log.txt\\", \\"a\\") as log_file: log_file.write(f\\"Event: {event}n\\") log_file.write(f\\"Arguments: {\', \'.join(map(str, args))}n\\") sys.addaudithook(audit_hook) # Example of using the setup_audit_logger function # setup_audit_logger() # Example code to trigger the specified events # input(\\"Enter something: \\") # os.listdir(\\".\\") # s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # s.connect((\\"www.example.com\\", 80))"},{"question":"# Question: Implementing and Testing Custom System Audits and Hooks **Objective:** The task requires you to demonstrate your understanding of the `sys` module in Python by implementing a function that sets up a custom auditing hook and tests the hook through various system audit events. Requirements 1. **Function Implementation**: - Implement a function `setup_custom_audit_hook()` that does the following: - Sets up a custom audit hook using `sys.addaudithook()`. - The audit hook should store all events in a list called `audit_logs` along with their associated arguments. 2. **Test the Custom Audit Hook**: - Within the `setup_custom_audit_hook()` function, manually trigger at least three different system audit events using `sys.audit()`. - Ensure that these events and their arguments are correctly logged in the `audit_logs` list. 3. **Function Signature**: ```python def setup_custom_audit_hook() -> list: pass ``` Constraints - Ensure to handle exceptions that might occur within the custom audit hook. - The function should return the `audit_logs` list at the end. - You can use any `sys` module functions or attributes necessary to create and test the audit events. Example Usage ```python audit_logs = setup_custom_audit_hook() for event in audit_logs: print(event) ``` **Expected Output Format**: ``` (\\"event_name_1\\", (arg1, arg2, ...)) (\\"event_name_2\\", (arg1, arg2, ...)) (\\"event_name_3\\", (arg1, arg2, ...)) ``` **Notes:** - Avoid using external libraries for this implementation. - Focus on implementing the functionality using only the `sys` module as described in the provided documentation. **Good Luck!**","solution":"import sys def setup_custom_audit_hook(): audit_logs = [] def custom_audit_hook(event, args): audit_logs.append((event, args)) sys.addaudithook(custom_audit_hook) # Manually trigger some audit events sys.audit(\\"example_event_1\\", \\"arg1\\", \\"arg2\\") sys.audit(\\"example_event_2\\", 42) sys.audit(\\"example_event_3\\", None, {\\"key\\": \\"value\\"}) return audit_logs"},{"question":"You are given an XML string representing a list of books in a library. Each book is represented by an `<book>` element containing tags for the title, author, year of publication, and price. Write a function `parse_books(xml_string)` that parses this XML string and returns a list of dictionaries. Each dictionary should contain the title and author of books published after the year 2000 with a price of more than 20. Input - `xml_string`: A string containing XML data. Output - A list of dictionaries. Each dictionary should have the keys `title` and `author`, with values being the corresponding text from the XML. Example ```python xml_data = <library> <book> <title>Python Basics</title> <author>John Doe</author> <year>1999</year> <price>30</price> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2005</year> <price>40</price> </book> <book> <title>Data Science with Python</title> <author>Mike Johnson</author> <year>2018</year> <price>25</price> </book> <book> <title>Old Python Book</title> <author>Chris Wilson</author> <year>2000</year> <price>15</price> </book> </library> result = parse_books(xml_data) print(result) ``` Expected Output: ```python [ {\'title\': \'Advanced Python\', \'author\': \'Jane Smith\'}, {\'title\': \'Data Science with Python\', \'author\': \'Mike Johnson\'} ] ``` Function Signature ```python def parse_books(xml_string: str) -> list: pass ``` # Constraints - Assume each `<book>` element will always contain `<title>`, `<author>`, `<year>`, and `<price>` tags. - The year and price will always be valid integers. - The XML structure will be valid and well-formed, containing at least one `<book>` element. # Note - Use the `xml.dom.pulldom` module for parsing the XML data. - Only include books that meet both conditions: published after the year 2000 and priced above 20.","solution":"import xml.etree.ElementTree as ET def parse_books(xml_string: str) -> list: Parses an XML string representing a list of books in a library. Returns a list of dictionaries containing the title and author of books published after the year 2000 with a price of more than 20. root = ET.fromstring(xml_string) books = [] for book in root.findall(\'book\'): title = book.find(\'title\').text author = book.find(\'author\').text year = int(book.find(\'year\').text) price = float(book.find(\'price\').text) if year > 2000 and price > 20: books.append({\'title\': title, \'author\': author}) return books"},{"question":"Question: Implement a Task Scheduling System using Python\'s Multiprocessing # Objective: You will develop a task scheduling system that will distribute tasks to worker processes using Python\'s `multiprocessing` package. The system will read a list of tasks, execute them concurrently using worker processes, keep track of their execution statuses, and collect the results. # Requirements: - Implement a function `task_scheduler(tasks, num_workers)` where: - `tasks` is a list of tuples. Each tuple contains a function and its arguments, e.g., `(func, (arg1, arg2))`. - `num_workers` is the number of worker processes to be used. - The function should return a dictionary where: - The keys are the task identifiers (indices in the `tasks` list). - The values are the results of the executed tasks. # Example: ```python from multiprocessing import Pool def task_scheduler(tasks, num_workers): # Implementation here def add(a, b): return a + b def multiply(a, b): return a * b tasks = [(add, (1, 2)), (multiply, (2, 3)), (add, (5, 3)), (multiply, (8, 7))] num_workers = 2 results = task_scheduler(tasks, num_workers) print(results) # Output: {0: 3, 1: 6, 2: 8, 3: 56} ``` # Constraints: 1. The functions used in the tasks are guaranteed to be picklable. 2. The tasks list will contain at least one task and at most 100 tasks. 3. The number of workers will be at least 1 and at most 10. # Hints: 1. Utilize the `multiprocessing.Pool` class to manage worker processes. 2. Use `Pool.map` or `Pool.apply_async` to distribute tasks to the workers. 3. Ensure that you handle the collection of results properly. # Performance Requirements: 1. The system should handle task execution efficiently even if tasks have varying execution times. 2. Proper synchronization must be ensured to avoid any deadlocks or race conditions during result collection.","solution":"from multiprocessing import Pool def execute_task(task): func, args = task return func(*args) def task_scheduler(tasks, num_workers): with Pool(processes=num_workers) as pool: task_indices = list(range(len(tasks))) results = pool.starmap(execute_task, [(tasks[i],) for i in task_indices]) return {i: results[i] for i in task_indices}"},{"question":"# Advanced Python Type Manipulation In this task, you will create a custom Python metaclass and demonstrate its usage by creating dynamic types. The metaclass will include specific features such as type checking and attribute creation at runtime. Requirements 1. **Metaclass Definition**: - Create a metaclass `CustomMeta` which will: - Override the `__new__` method to add a custom attribute `custom_meta_attr` to the class with a value of `True`. 2. **Dynamic Type Creation**: - Using the defined metaclass, dynamically create a new type `DynamicType` with specific attributes and methods: - An attribute `existing_attr` with value `1234`. - A method `dynamic_method` which takes one argument `x` and returns the string \\"Value is {x}\\". 3. **Function Implementation**: - Implement a function `create_dynamic_type()` which performs the following: - Uses `CustomMeta` to create `DynamicType`. - Adds another attribute `added_later` to the `DynamicType` with value `5678`. - Instantiates an object of `DynamicType`. - Returns a tuple containing the instance and a dictionary of all attributes and their values of the instance. Constraints - You may not modify the `__init__` or `__setattr__` methods in the metaclass or dynamic type. - Use Python 3.10 or later. Example ```python class CustomMeta(type): # Implement metaclass here def create_dynamic_type(): # Implement dynamic type creation and instantiation here return instance, attribute_dict # Example usage: instance, attributes = create_dynamic_type() print(instance.existing_attr) # Output: 1234 print(instance.added_later) # Output: 5678 print(instance.dynamic_method(10)) # Output: Value is 10 ``` This assessment tests your understanding of metaclasses, type creation, and dynamic attributes in Python, providing a comprehensive challenge that relates to the foundational concepts outlined in the provided documentation.","solution":"class CustomMeta(type): A custom metaclass that adds a custom attribute to any class that uses it. def __new__(cls, name, bases, dct): # Create the new class new_cls = super().__new__(cls, name, bases, dct) # Add custom attribute new_cls.custom_meta_attr = True return new_cls def create_dynamic_type(): Uses the CustomMeta to create a dynamic type with specific attributes and methods. Adds a new attribute after creation and returns an instance along with its attributes. # Define the attributes and methods for the new type attrs = { \'existing_attr\': 1234, \'dynamic_method\': lambda self, x: f\\"Value is {x}\\" } # Create DynamicType using CustomMeta DynamicType = CustomMeta(\'DynamicType\', (object,), attrs) # Add attribute added_later to the new type setattr(DynamicType, \'added_later\', 5678) # Create an instance of DynamicType instance = DynamicType() # Create a dictionary of all attributes and their values attribute_dict = {attr: getattr(instance, attr) for attr in dir(instance) if not callable(getattr(instance, attr)) and not attr.startswith(\'__\')} return instance, attribute_dict"},{"question":"You are given a pandas DataFrame `df` containing the scores of students in various subjects. Due to a recent update in the company\'s policy, you need to ensure that any data transformations adhere to the Copy-on-Write (CoW) principles of pandas 3.0. Implement a function `process_scores` which will: 1. Create a new DataFrame `subset` containing only the scores of `Math` and `Science` subjects. 2. Update the scores of students in the `Math` subject to 100 in `subset` where their current score is less than 50. Ensure this change does not affect the original DataFrame `df`. 3. Return the modified `subset` DataFrame and the original `df`. # Input - A pandas DataFrame `df` with at least two columns named `Math` and `Science`, and multiple rows representing students\' scores. # Output - A tuple containing: - The modified DataFrame `subset` where the scores of `Math` have been updated. - The original DataFrame `df` to verify it remains unchanged. # Example ```python import pandas as pd df = pd.DataFrame({ \\"Student\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\"], \\"Math\\": [45, 78, 39], \\"Science\\": [88, 92, 60], \\"English\\": [85, 70, 90] }) subset, original_df = process_scores(df) print(subset) # Output: # Math Science # 0 100 88 # 1 78 92 # 2 100 60 print(original_df) # Output: # Student Math Science English # 0 Alice 45 88 85 # 1 Bob 78 92 70 # 2 Charlie 39 60 90 ``` # Constraints - Always use `loc` and avoid chained assignments. - Do not use inplace modifications. - Handle the case where no score needs to be updated. # Implementation Restrictions - You must ensure the function adheres to the CoW principles. - Use pandas version 3.0 or later for your implementation.","solution":"import pandas as pd def process_scores(df): # Create a new DataFrame containing only Math and Science scores subset = df[[\'Math\', \'Science\']].copy() # Update Math scores to 100 where the score is less than 50 subset.loc[subset[\'Math\'] < 50, \'Math\'] = 100 # Return the modified subset and the original DataFrame return subset, df"},{"question":"# Advanced Coding Assessment Question: Utilization of `atexit` for Resource Management **Objective**: Utilize the `atexit` module to manage resources and ensure proper cleanup in a Python program. **Problem Statement**: You are tasked with building a simple resource management system using the `atexit` module. Specifically, you need to create a program that: 1. Manages the initialization and cleanup of a hypothetical resource. 2. Ensures that resource cleanup activities are automatically performed upon normal interpreter termination. 3. Handles any exceptions during cleanup gracefully. **Requirements**: 1. Define a class `ResourceManager` that can: - Open a resource. - Use the resource. - Ensure the resource is properly closed upon program termination. 2. Implement the following methods for the class: - `__init__(self)`: Initializes the resource. Ensure it registers a cleanup method to be executed on program termination. - `use_resource(self)`: Simulates using the resource (e.g., print a message indicating the resource is being used). - `cleanup(self)`: Closes the resource and prints a message indicating the resource has been cleaned up. 3. Register and use the cleanup method with `atexit.register()` and handle multiple instances of `ResourceManager` correctly. 4. Demonstrate using this class in a script where multiple instances are created and managed. # Example Usage: ```python # Example of how the ResourceManager class should be used if __name__ == \\"__main__\\": # Create multiple resource managers resource_manager1 = ResourceManager() resource_manager2 = ResourceManager() # Use the resources resource_manager1.use_resource() resource_manager2.use_resource() # Program terminates normally, triggering cleanup ``` **Constraints**: - Assume that opening a resource and closing a resource are simulated by print statements. - Ensure that cleanup methods are called correctly, in the right order, and handle exceptions in cleanup methods gracefully. **Input/Output**: - **Input**: No direct input, as resource management is controlled by the class methods and normal program flow. - **Output**: Sequence of printed messages indicating resource usage and cleanup. **Performance Requirements**: - The solution should correctly handle the registration and execution order of cleanup methods. - Must ensure that all registered cleanup functions are called even if one raises an exception. The exception should be caught and printed, allowing remaining cleanup functions to execute. # Task: Implement the `ResourceManager` class as specified. Ensure the solution demonstrates an understanding of using the `atexit` module for resource management and cleanup.","solution":"import atexit class ResourceManager: def __init__(self): print(\\"Initializing ResourceManager\\") self.resource_open = True # Register the cleanup method to be called on program termination atexit.register(self.cleanup) def use_resource(self): if self.resource_open: print(\\"Using resource\\") else: print(\\"Attempted to use a resource that is not open.\\") def cleanup(self): if self.resource_open: print(\\"Cleaning up resource\\") try: # Simulate resource cleanup self.resource_open = False print(\\"Resource has been cleaned up\\") except Exception as e: print(f\\"Exception during cleanup: {e}\\")"},{"question":"Objective: Design and implement a Python function that utilizes the `operator` module to perform a series of operations on a list of dictionaries representing students and their grades. The operations include sorting, filtering, and computing aggregated metrics. Problem Description: Given a list of dictionaries where each dictionary represents a student with keys such as `name`, `age`, and `grade`, implement a function that performs the following operations: 1. **Filter**: Remove students who have a grade below a specified threshold. 2. **Sort**: Sort the students by their grades in descending order. In case of a tie, sort by their names in ascending order. 3. **Compute Metrics**: Calculate the average grade of the remaining students. You must use the functions provided by the `operator` module to accomplish these tasks. Function Signature: ```python from typing import List, Dict def process_student_grades(students: List[Dict[str, object]], threshold: int) -> Dict[str, object]: # Your code here ``` Input: - `students`: A list of dictionaries, where each dictionary contains: - `name` (string): The student\'s name. - `age` (integer): The student\'s age. - `grade` (integer): The student\'s grade. - `threshold` (integer): The minimum grade required to pass the filter. Output: - A dictionary containing: - `filtered_students`: The filtered and sorted list of student dictionaries. - `average_grade`: The average grade of the filtered students (rounded to two decimal places). Constraints: - Each student will have a unique name. - The list of students will have at least one student. Example: ```python # Input students = [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grade\\": 88}, {\\"name\\": \\"Bob\\", \\"age\\": 19, \\"grade\\": 70}, {\\"name\\": \\"Charlie\\", \\"age\\": 21, \\"grade\\": 65}, {\\"name\\": \\"Dave\\", \\"age\\": 20, \\"grade\\": 88}, {\\"name\\": \\"Eve\\", \\"age\\": 22, \\"grade\\": 90} ] threshold = 70 # Expected Output { \\"filtered_students\\": [ {\\"name\\": \\"Eve\\", \\"age\\": 22, \\"grade\\": 90}, {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grade\\": 88}, {\\"name\\": \\"Dave\\", \\"age\\": 20, \\"grade\\": 88}, {\\"name\\": \\"Bob\\", \\"age\\": 19, \\"grade\\": 70} ], \\"average_grade\\": 84.0 } ``` Implementation Requirements: - Use `operator.itemgetter` to sort students. - Use `operator.methodcaller` if needed for any string manipulations as part of filtering. - Use `operator.add` or other applicable functions for computing aggregated metrics. Additional Notes: - Document the code and provide necessary comments to explain the usage of the `operator` module functions. - Ensure that the implemented code is efficient and follows best practices for performance and readability.","solution":"from typing import List, Dict import operator def process_student_grades(students: List[Dict[str, object]], threshold: int) -> Dict[str, object]: # Filter out students with grades below the threshold filtered_students = list(filter(lambda student: student[\'grade\'] >= threshold, students)) # Sort the remaining students by grade in descending order and by name in ascending order in case of tie filtered_students.sort(key=operator.itemgetter(\'name\')) filtered_students.sort(key=operator.itemgetter(\'grade\'), reverse=True) # Calculate the average grade of the filtered students if filtered_students: total_grades = sum(student[\'grade\'] for student in filtered_students) average_grade = round(total_grades / len(filtered_students), 2) else: average_grade = 0.0 return { \\"filtered_students\\": filtered_students, \\"average_grade\\": average_grade }"},{"question":"# Persistent Storage with Shelve **Objective:** You are tasked with creating a small application that manages a collection of user contacts using the `shelve` module. Each contact should have the attributes: `name`, `email`, and `phone`. The program should support adding new contacts, retrieving contacts by name, updating existing contacts, deleting contacts, and listing all contacts. **Requirements:** 1. **Persistent Storage Initialization**: - Create a function `init_storage(filename)` that initializes and returns a shelf using the given filename. - Use the default parameters provided by the `shelve.open()` function. 2. **Adding a Contact**: - Create a function `add_contact(storage, name, email, phone)` which adds a new contact to the storage. Each contact should be stored as a dictionary with keys `email` and `phone`. 3. **Retrieving a Contact**: - Create a function `get_contact(storage, name)` that retrieves a contact\'s details by their name. If the contact does not exist, it should return `None`. 4. **Updating a Contact**: - Create a function `update_contact(storage, name, email=None, phone=None)` that updates the email and/or phone number of an existing contact. If the contact does not exist, it should raise a KeyError. 5. **Deleting a Contact**: - Create a function `delete_contact(storage, name)` that deletes the contact with the specified name. If the contact does not exist, it should raise a KeyError. 6. **Listing All Contacts**: - Create a function `list_contacts(storage)` that returns a sorted list of all contact names stored in the shelf. **Constraints:** - The `name` field for each contact must be unique. - The `email` field must be a valid email address format. - The `phone` field must contain only digits and can optionally start with a \'+\' for country code. **Performance Requirements:** - Assume that the number of contacts will not exceed 10,000. Optimize read operations since they will be more frequent than write operations. **Example Usage:** ```python filename = \'contacts.db\' storage = init_storage(filename) add_contact(storage, \'John Doe\', \'john@example.com\', \'+1234567890\') add_contact(storage, \'Jane Smith\', \'jane@example.com\', \'0987654321\') print(get_contact(storage, \'John Doe\')) # Output: {\'email\': \'john@example.com\', \'phone\': \'+1234567890\'} update_contact(storage, \'John Doe\', phone=\'+1111111111\') print(get_contact(storage, \'John Doe\')) # Output: {\'email\': \'john@example.com\', \'phone\': \'+1111111111\'} delete_contact(storage, \'Jane Smith\') print(get_contact(storage, \'Jane Smith\')) # Output: None print(list_contacts(storage)) # Output: [\'John Doe\'] ``` **Implementation Details:** Implement each of the above functions and ensure to properly handle the closing of the shelf either explicitly or using a context manager. Document any assumptions made and test cases to validate your implementation.","solution":"import shelve import re def init_storage(filename): Initializes and returns a shelve storage. :param filename: The name of the file to store the shelve data. :return: A shelve storage. return shelve.open(filename) def is_valid_email(email): Validates the email format. :param email: An email address to validate. :return: True if valid, False otherwise. email_regex = re.compile( r\\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+)\\") return re.match(email_regex, email) is not None def is_valid_phone(phone): Validates the phone number format. :param phone: A phone number to validate. :return: True if valid, False otherwise. phone_regex = re.compile(r\\"^+?[0-9]+\\") return re.match(phone_regex, phone) is not None def add_contact(storage, name, email, phone): Adds a new contact to the storage. :param storage: The shelve storage. :param name: The name of the contact. :param email: The email of the contact. :param phone: The phone number of the contact. if not is_valid_email(email): raise ValueError(f\\"Invalid email format: {email}\\") if not is_valid_phone(phone): raise ValueError(f\\"Invalid phone format: {phone}\\") storage[name] = {\'email\': email, \'phone\': phone} def get_contact(storage, name): Retrieves a contact by name. :param storage: The shelve storage. :param name: The name of the contact. :return: The contact details or None if not found. return storage.get(name) def update_contact(storage, name, email=None, phone=None): Updates an existing contact. :param storage: The shelve storage. :param name: The name of the contact. :param email: The new email of the contact (optional). :param phone: The new phone number of the contact (optional). if name not in storage: raise KeyError(f\\"Contact with name {name} does not exist\\") contact = storage[name] if email: if not is_valid_email(email): raise ValueError(f\\"Invalid email format: {email}\\") contact[\'email\'] = email if phone: if not is_valid_phone(phone): raise ValueError(f\\"Invalid phone format: {phone}\\") contact[\'phone\'] = phone storage[name] = contact def delete_contact(storage, name): Deletes a contact by name. :param storage: The shelve storage. :param name: The name of the contact. if name not in storage: raise KeyError(f\\"Contact with name {name} does not exist\\") del storage[name] def list_contacts(storage): Lists all contact names stored in the shelf. :param storage: The shelve storage. :return: A sorted list of all contact names. return sorted(storage.keys())"},{"question":"**Objective**: To assess your understanding and proficiency with the scikit-learn package, specifically in tuning the decision threshold for classification tasks and utilizing cross-validation with custom scoring metrics. **Problem Statement**: You are provided with a synthetic dataset for binary classification, which simulates a medical diagnosis scenario where identifying true positives is crucial. As a data scientist, your task is to: 1. Train a classification model using a decision tree classifier. 2. Define a custom scorer that prioritizes recall for the positive class (class 1). 3. Use the `TunedThresholdClassifierCV` to tune the decision threshold for the classifier to maximize the recall score. 4. Evaluate and compare the performance of the default classifier and the tuned threshold classifier on a separate test set. **Instructions**: 1. **Load the dataset**: - Use `sklearn.datasets.make_classification` to generate a dataset with 2000 samples, where the data is imbalanced (10% positive class and 90% negative class). Use `random_state=0` for reproducibility. 2. **Train-Test Split**: - Split the dataset into training (80%) and testing (20%) sets using `sklearn.model_selection.train_test_split` with `random_state=0`. 3. **Train a Decision Tree Classifier**: - Use a `DecisionTreeClassifier` with `max_depth=2` and `random_state=0` to train on the training set. - Evaluate the classifier on the test set and report the recall score for the positive class (class 1). 4. **Define a Custom Scorer**: - Use `sklearn.metrics.make_scorer` and `sklearn.metrics.recall_score` to create a custom scorer that maximizes recall for the positive class (class 1). 5. **Tune the Decision Threshold**: - Use `TunedThresholdClassifierCV` with the trained `DecisionTreeClassifier` and the custom scorer. Fit this model on the training data to find the optimal decision threshold. 6. **Evaluate the Tuned Model**: - Evaluate the tuned model on the test set and report the recall score and the new decision threshold. 7. **Comparison**: - Compare the recall score of the default and tuned classifiers on the test set. **Input and Output**: - **Input**: NONE (the code should generate its own dataset) - **Output**: - Print the recall score of the default classifier on the test set. - Print the recall score of the tuned decision threshold classifier on the test set. - Print the new decision threshold. Example Solution Template ```python import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split, TunedThresholdClassifierCV from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import make_scorer, recall_score # Step 1: Generate the dataset X, y = make_classification(n_samples=2000, weights=[0.9, 0.1], random_state=0) # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Step 3: Train the Decision Tree Classifier classifier = DecisionTreeClassifier(max_depth=2, random_state=0) classifier.fit(X_train, y_train) y_pred_default = classifier.predict(X_test) recall_default = recall_score(y_test, y_pred_default) print(f\\"Recall score of the default classifier: {recall_default}\\") # Step 4: Define a Custom Scorer scorer = make_scorer(recall_score, pos_label=1) # Step 5: Tune the Decision Threshold tuned_model = TunedThresholdClassifierCV(classifier, scoring=scorer) tuned_model.fit(X_train, y_train) tuned_threshold = tuned_model.best_threshold_ # Evaluate the tuned model y_pred_tuned = tuned_model.predict(X_test) recall_tuned = recall_score(y_test, y_pred_tuned) print(f\\"Recall score of the tuned classifier: {recall_tuned}\\") print(f\\"New decision threshold: {tuned_threshold}\\") ``` **Constraints**: - You must use a `DecisionTreeClassifier` with `max_depth=2` and `random_state=0`. - Recall for class 1 should be used as the primary metric for optimization. - Ensure the results are reproducible by using `random_state=0` where applicable. - Use cross-validation as described and avoid overfitting.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import make_scorer, recall_score, precision_recall_curve from sklearn.base import BaseEstimator, ClassifierMixin, clone class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, clf, scoring): self.clf = clf self.scoring = scoring def fit(self, X, y): self.clf_ = clone(self.clf) self.clf_.fit(X, y) probs = self.clf_.predict_proba(X)[:, 1] precisions, recalls, thresholds = precision_recall_curve(y, probs) scores = recalls[:-1] self.best_threshold_ = thresholds[scores.argmax()] return self def predict(self, X): probs = self.clf_.predict_proba(X)[:, 1] return (probs >= self.best_threshold_).astype(int) # Step 1: Generate the dataset X, y = make_classification(n_samples=2000, weights=[0.9, 0.1], random_state=0) # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Step 3: Train the Decision Tree Classifier classifier = DecisionTreeClassifier(max_depth=2, random_state=0) classifier.fit(X_train, y_train) y_pred_default = classifier.predict(X_test) recall_default = recall_score(y_test, y_pred_default) print(f\\"Recall score of the default classifier: {recall_default}\\") # Step 4: Define a Custom Scorer scorer = make_scorer(recall_score, pos_label=1) # Step 5: Tune the Decision Threshold tuned_model = TunedThresholdClassifierCV(classifier, scoring=scorer) tuned_model.fit(X_train, y_train) tuned_threshold = tuned_model.best_threshold_ # Evaluate the tuned model y_pred_tuned = tuned_model.predict(X_test) recall_tuned = recall_score(y_test, y_pred_tuned) print(f\\"Recall score of the tuned classifier: {recall_tuned}\\") print(f\\"New decision threshold: {tuned_threshold}\\")"},{"question":"Coding Assessment Question # Objective You are tasked with writing a Python function that processes a given HTML string, replacing all named HTML entities within the string with their corresponding Unicode characters. This demonstrates your understanding of working with the `html.entities` module to map HTML entities to their Unicode equivalents. # Task Implement a function `replace_html_entities(html_string: str) -> str` that takes an HTML string as input and returns a new string with all named HTML entities replaced by their corresponding Unicode characters. # Input - `html_string` (str): A string containing HTML content that may include named HTML entities. # Output - (str): A string where all named HTML entities have been replaced with their corresponding Unicode characters. # Constraints 1. The input string can contain any number of HTML entities, including none. 2. The HTML entities in the string are valid and follow the naming conventions expected by the `html.entities.html5` dictionary. 3. The function should handle both cases where the entity name includes and does not include the trailing semicolon. # Example ```python def replace_html_entities(html_string: str) -> str: import html.entities # Write your code here # Test Cases print(replace_html_entities(\\"The &quot;quote&quot; is a common HTML entity.\\")) # Output: \'The \\"quote\\" is a common HTML entity.\' print(replace_html_entities(\\"Use &amp; to escape the ampersand (&) character.\\")) # Output: \'Use & to escape the ampersand (&) character.\' print(replace_html_entities(\\"Less than symbol: &lt; and greater than: &gt;\\")) # Output: \'Less than symbol: < and greater than: >\' print(replace_html_entities(\\"&eacute; is the HTML entity for é.\\")) # Output: \'é is the HTML entity for é.\' ``` # Notes - You should use the `html.entities.html5` dictionary for mapping the entities. - Consider using regular expressions to identify and replace the HTML entities within the string. - Ensure that your function is efficient and handles typical use cases gracefully.","solution":"import re from html.entities import html5 def replace_html_entities(html_string: str) -> str: # Regular expression to find HTML entities entity_pattern = re.compile(r\'&([a-zA-Z0-9]+);?\') def replace_entity(match): entity = match.group(1) # If the entity is found in the html5 dictionary, return its corresponding character return html5.get(entity, f\'&{entity};\') # Substitute all occurrences of the HTML entities using the replace_entity function return entity_pattern.sub(replace_entity, html_string)"},{"question":"**Question:** You are provided with a dataset named `vehicles` containing information about different types of vehicles. The dataset includes the following columns: - `Type`: Categorical variable representing the type of vehicle (e.g., Car, Truck, Motorcycle). - `Engine_Size`: Numerical variable indicating the size of the engine. - `Fuel_Efficiency`: Numerical variable representing the fuel efficiency of the vehicle in miles per gallon. - `Country`: Categorical variable indicating the country of manufacture (e.g., USA, Japan, Germany). Your task is to write a function `visualize_vehicle_data` that takes the `vehicles` DataFrame as input and generates a set of visualizations using seaborn: 1. **Categorical Scatter Plot**: - A `stripplot` of `Fuel_Efficiency` vs `Type` with jitter enabled. - A `swarmplot` of `Fuel_Efficiency` vs `Type`, colored by `Country`. 2. **Distribution Plots**: - A `boxplot` of `Engine_Size` vs `Type`, colored by `Country`. - A `violinplot` showing the distribution of `Fuel_Efficiency` across `Type`. 3. **Estimate Plots**: - A `barplot` showing the average `Fuel_Efficiency` for each `Type`, with error bars representing the 95% confidence intervals. - A `countplot` showing the number of vehicles manufactured in each `Country`. The function should save each plot as a PNG file with appropriate filenames and labels. Please include legends, titles, and axis labels to make the plots informative. ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_vehicle_data(vehicles): # 1. Categorical Scatter Plot - Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=vehicles, x=\'Type\', y=\'Fuel_Efficiency\', jitter=True) plt.title(\\"Strip Plot of Fuel Efficiency by Vehicle Type\\") plt.savefig(\\"stripplot_fuel_efficiency.png\\") # 2. Categorical Scatter Plot - Swarm Plot plt.figure(figsize=(10, 6)) sns.swarmplot(data=vehicles, x=\'Type\', y=\'Fuel_Efficiency\', hue=\'Country\') plt.title(\\"Swarm Plot of Fuel Efficiency by Vehicle Type and Country\\") plt.legend(title=\'Country\') plt.savefig(\\"swarmplot_fuel_efficiency.png\\") # 3. Distribution Plot - Box Plot plt.figure(figsize=(10, 6)) sns.boxplot(data=vehicles, x=\'Type\', y=\'Engine_Size\', hue=\'Country\') plt.title(\\"Box Plot of Engine Size by Vehicle Type and Country\\") plt.legend(title=\'Country\') plt.savefig(\\"boxplot_engine_size.png\\") # 4. Distribution Plot - Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(data=vehicles, x=\'Type\', y=\'Fuel_Efficiency\') plt.title(\\"Violin Plot of Fuel Efficiency by Vehicle Type\\") plt.savefig(\\"violinplot_fuel_efficiency.png\\") # 5. Estimate Plot - Bar Plot plt.figure(figsize=(10, 6)) sns.barplot(data=vehicles, x=\'Type\', y=\'Fuel_Efficiency\', ci=95) plt.title(\\"Bar Plot of Average Fuel Efficiency by Vehicle Type\\") plt.savefig(\\"barplot_avg_fuel_efficiency.png\\") # 6. Estimate Plot - Count Plot plt.figure(figsize=(10, 6)) sns.countplot(data=vehicles, x=\'Country\') plt.title(\\"Count Plot of Vehicles by Country\\") plt.savefig(\\"countplot_vehicles_country.png\\") plt.close(\'all\') ``` **Constraints:** - The dataset must be a pandas DataFrame with the columns `Type`, `Engine_Size`, `Fuel_Efficiency`, and `Country`. - Ensure that the plots are saved with appropriate filenames. - Handle any potential missing values in the dataset appropriately. Your implementation should demonstrate a clear understanding of seaborn\'s categorical plotting functionalities and produce well-labeled, informative visualizations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_vehicle_data(vehicles): # Handle missing values by dropping them for the purpose of visualization vehicles = vehicles.dropna(subset=[\'Type\', \'Engine_Size\', \'Fuel_Efficiency\', \'Country\']) # 1. Categorical Scatter Plot - Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=vehicles, x=\'Type\', y=\'Fuel_Efficiency\', jitter=True) plt.title(\\"Strip Plot of Fuel Efficiency by Vehicle Type\\") plt.xlabel(\\"Vehicle Type\\") plt.ylabel(\\"Fuel Efficiency (MPG)\\") plt.savefig(\\"stripplot_fuel_efficiency.png\\") plt.close() # 2. Categorical Scatter Plot - Swarm Plot plt.figure(figsize=(10, 6)) sns.swarmplot(data=vehicles, x=\'Type\', y=\'Fuel_Efficiency\', hue=\'Country\') plt.title(\\"Swarm Plot of Fuel Efficiency by Vehicle Type and Country\\") plt.xlabel(\\"Vehicle Type\\") plt.ylabel(\\"Fuel Efficiency (MPG)\\") plt.legend(title=\'Country\') plt.savefig(\\"swarmplot_fuel_efficiency.png\\") plt.close() # 3. Distribution Plot - Box Plot plt.figure(figsize=(10, 6)) sns.boxplot(data=vehicles, x=\'Type\', y=\'Engine_Size\', hue=\'Country\') plt.title(\\"Box Plot of Engine Size by Vehicle Type and Country\\") plt.xlabel(\\"Vehicle Type\\") plt.ylabel(\\"Engine Size\\") plt.legend(title=\'Country\') plt.savefig(\\"boxplot_engine_size.png\\") plt.close() # 4. Distribution Plot - Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(data=vehicles, x=\'Type\', y=\'Fuel_Efficiency\') plt.title(\\"Violin Plot of Fuel Efficiency by Vehicle Type\\") plt.xlabel(\\"Vehicle Type\\") plt.ylabel(\\"Fuel Efficiency (MPG)\\") plt.savefig(\\"violinplot_fuel_efficiency.png\\") plt.close() # 5. Estimate Plot - Bar Plot plt.figure(figsize=(10, 6)) sns.barplot(data=vehicles, x=\'Type\', y=\'Fuel_Efficiency\', ci=95) plt.title(\\"Bar Plot of Average Fuel Efficiency by Vehicle Type\\") plt.xlabel(\\"Vehicle Type\\") plt.ylabel(\\"Average Fuel Efficiency (MPG)\\") plt.savefig(\\"barplot_avg_fuel_efficiency.png\\") plt.close() # 6. Estimate Plot - Count Plot plt.figure(figsize=(10, 6)) sns.countplot(data=vehicles, x=\'Country\') plt.title(\\"Count Plot of Vehicles by Country\\") plt.xlabel(\\"Country\\") plt.ylabel(\\"Count\\") plt.savefig(\\"countplot_vehicles_country.png\\") plt.close()"},{"question":"**Title:** Dynamic Module Import Analysis **Objective:** Use the `modulefinder` module to perform a detailed analysis of a given Python script, report on the imported modules, and implement additional requirements for excluding specific modules and replacing module paths dynamically. **Problem Statement:** You are provided with a Python script (`target_script.py`) that imports several modules. Your task is to write a Python function `analyze_script`, which will: 1. Use the `ModuleFinder` to analyze the provided script file. 2. Exclude certain modules from the analysis. 3. Replace specified module paths. 4. Generate and print a detailed report including: - Loaded modules along with a few of their global names (up to 3). - Modules that were supposed to be imported but are missing. **Function Signature:** ```python def analyze_script(script_path: str, excludes: list = [], replace_paths: list = []): pass ``` **Input:** - `script_path`: A string representing the file path of the Python script to be analyzed. - `excludes`: A list of module names to be excluded from the analysis. Defaults to an empty list. - `replace_paths`: A list of tuples where each tuple contains an old path and the corresponding new path to replace in module paths. Defaults to an empty list. **Output:** - The function should print two sections: - **Loaded modules**: For each loaded module, print the module name followed by up to three global identifiers defined in that module. - **Modules not imported**: List the names of modules that were attempted to be imported but are missing. **Example Usage:** The script `sample_script.py`: ```python import json import non_existent_module try: import another_missing_module except ImportError: pass import re ``` Example call to `analyze_script`: ```python analyze_script(\'sample_script.py\', excludes=[\'json\'], replace_paths=[(\'non_existent_module\', \'some_existing_module\')]) ``` Expected output (may vary depending on the environment): ``` Loaded modules: re: pattern,search,findall __main__: json,non_existent_module,re --------------------------------------------------- Modules not imported: another_missing_module ``` **Constraints:** - Your implementation should handle any valid Python script and generate the appropriate report. - Ensure exception handling for cases where the script file doesn\'t exist or other potential IO errors. **Note:** - Review the example provided in the documentation for understanding the use of `ModuleFinder` class methods. - Ensure to import the `modulefinder` module at the beginning of your script: ```python from modulefinder import ModuleFinder ``` Implement this function to robustly handle the analysis according to the requirements specified above.","solution":"from modulefinder import ModuleFinder import os def analyze_script(script_path: str, excludes: list = [], replace_paths: list = []): Analyzes a Python script for its imported modules, excluding specific ones and replacing module paths dynamically. Parameters: script_path (str): The file path of the Python script to be analyzed. excludes (list): A list of module names to be excluded from the analysis. replace_paths (list): A list of tuples containing old and new paths to replace in module paths. Prints: Loaded modules and their global names. Missing modules that were attempted to be imported but are not found. if not os.path.exists(script_path): print(f\\"File {script_path} does not exist.\\") return finder = ModuleFinder(excludes=excludes) for old_path, new_path in replace_paths: finder.replace_paths.append((old_path, new_path)) finder.run_script(script_path) loaded_modules = finder.modules bad_modules = finder.badmodules print(\\"Loaded modules:\\") for name, mod in loaded_modules.items(): if name in excludes: continue print(f\\"{name}: {\', \'.join(list(mod.globalnames.keys())[:3])}\\") print(\\"---------------------------------------------------\\") print(\\"Modules not imported:\\") for name in bad_modules: print(name)"},{"question":"**Seaborn Assessment Question** Given a diamonds dataset available in seaborn, you are required to perform the following tasks using seaborn objects: 1. Load the `diamonds` dataset from seaborn. 2. Create a bar plot displaying the average carat for each `cut` category. 3. Modify the bar plot to display the median carat for each `cut` category instead. 4. Further, modify the plot to display the interquartile range (IQR) of carat for each `cut` category. (The IQR is calculated as the difference between the 75th and 25th percentiles.) # Expected Input and Output - Your function should not take any input or return any output. Your solution will be evaluated based on the generated plots. # Constraints and Requirements: - Use the seaborn.objects interface for plotting. - Ensure the plots are properly labeled and easy to understand. - Handle the dataset correctly and efficiently. # Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def create_plots(): # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Plot average carat by cut p_avg = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\").add(so.Bar(), so.Agg()) p_avg.show() # Plot median carat by cut p_med = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"median\\")) p_med.show() # Plot IQR of carat by cut p_iqr = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\").add(so.Bar(), so.Agg(lambda x: x.quantile(.75) - x.quantile(.25))) p_iqr.show() ``` Implement the `create_plots` function to generate the required plots.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_plots(): # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Plot average carat by cut p_avg = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\").add(so.Bar(), so.Agg()) p_avg.label(x=\\"Cut\\", y=\\"Average Carat\\", title=\\"Average Carat by Cut\\").show() # Plot median carat by cut p_med = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"median\\")) p_med.label(x=\\"Cut\\", y=\\"Median Carat\\", title=\\"Median Carat by Cut\\").show() # Plot IQR of carat by cut p_iqr = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\").add(so.Bar(), so.Agg(lambda x: x.quantile(.75) - x.quantile(.25))) p_iqr.label(x=\\"Cut\\", y=\\"IQR Carat\\", title=\\"IQR of Carat by Cut\\").show() # Ensure that the plots are displayed properly in noteook environments or script runs plt.show() create_plots()"},{"question":"**Title: Implementing a Nested Dictionary Merger with Enhanced Features** **Objective:** Demonstrate a comprehensive understanding of the `collections.ChainMap` and `collections.Counter` classes by implementing functions that merge nested dictionaries and count occurrences of items using these advanced data structures. **Problem Statement:** You are required to write two functions: `merge_dictionaries` and `count_items`. 1. **Function `merge_dictionaries(dicts: List[Dict[str, Any]]) -> ChainMap:`** - **Input:** A list of dictionaries that may have overlapping keys. Each dictionary can have other dictionaries as values (nested dictionaries). - **Output:** A `ChainMap` that represents a single, updateable view of all the given dictionaries. - **Constraints:** - The function should be able to handle an arbitrary depth of nested dictionaries. - For any key conflict at the same level, the key-value pair in the first dictionary should dominate. 2. **Function `count_items(nested_dict: Dict[str, Any]) -> Counter:`** - **Input:** A dictionary that contains nested dictionaries, where some values are lists of items. - **Output:** A `Counter` object where the keys are the items and the values are the counts of their occurrences. - **Constraints:** - The function should recursively parse through the nested dictionaries and lists to count all items, including those within nested structures. - Assume that the list items are hashable and can be counted using `Counter`. **Example:** ```python # Sample dictionaries for merging dict1 = {\'a\': 1, \'b\': {\'x\': 10, \'y\': 20}} dict2 = {\'b\': {\'x\': 15, \'z\': 25}, \'c\': 5} dicts = [dict1, dict2] # Sample nested dictionary for counting nested_dict = { \'a\': [1, 2, 3], \'b\': { \'x\': [2, 3, 4], \'y\': [1, 3], \'z\': { \'u\': [1, 1, 2], \'v\': [3, 4] } } } # Expected Outputs merged = merge_dictionaries(dicts) # ChainMap({\'a\': 1, \'b\': {\'x\': 10, \'y\': 20}}, {\'b\': {\'x\': 15, \'z\': 25}, \'c\': 5}) item_counts = count_items(nested_dict) # Counter({1: 4, 2: 3, 3: 4, 4: 2}) ``` **Function Signatures:** ```python from collections import ChainMap, Counter from typing import List, Dict, Any def merge_dictionaries(dicts: List[Dict[str, Any]]) -> ChainMap: pass def count_items(nested_dict: Dict[str, Any]) -> Counter: pass ``` **Additional Notes:** - Use the `ChainMap` to achieve the merging functionality. The final `ChainMap` should allow easy access to the combined dictionary keys and values. - Utilize the `Counter` class to count occurrences effectively, ensuring that nested structures are fully parsed and items within lists are appropriately counted. **Assessment Criteria:** - Correct implementation and functionality of the `merge_dictionaries` and `count_items` functions. - Ability to handle nested structures and conflicts in a logical manner. - Efficiency and readability of the code. - Adequate use of `collections.ChainMap` and `collections.Counter`.","solution":"from collections import ChainMap, Counter from typing import List, Dict, Any def merge_dictionaries(dicts: List[Dict[str, Any]]) -> ChainMap: Merges a list of dictionaries into a single ChainMap. For conflicting keys, values from earlier dictionaries in the list dominate. return ChainMap(*dicts) def count_items(nested_dict: Dict[str, Any]) -> Counter: Count occurrences of items in a nested dictionary. Lists of items within the dictionary are recursively parsed to count all items. counter = Counter() def recurse(d): if isinstance(d, dict): for k, v in d.items(): recurse(v) elif isinstance(d, list): for item in d: recurse(item) else: counter[d] += 1 recurse(nested_dict) return counter"},{"question":"**Python Initialization Configuration Task** **Objective:** In this task, you will demonstrate your understanding of initializing Python using the provided configuration structures (`PyPreConfig` and `PyConfig`). You will create a script that uses the Isolated Configuration to run a specific Python command and modify the search path. # Task: 1. **Initialize Python in Isolated Mode**: - Create an isolated configuration using `PyPreConfig_InitIsolatedConfig` and `PyConfig_InitIsolatedConfig`. - Ensure that the configuration isolates Python from environment variables and system locale. 2. **Customize the Search Path**: - Append a custom search path to the Python module search paths (`sys.path`) using `PyWideStringList_Append`. 3. **Run a Specific Python Command**: - Use `PyConfig.run_command` to set a Python command to be executed. - The command should print \\"Hello, Isolated Python!\\" to the console. 4. **Handle Initialization and Execution Errors**: - Make sure to handle exceptions using the appropriate functions (`PyStatus_Exception`, `Py_ExitStatusException`). 5. **Finalize the Python Initialization**: - Finalize the initialization by calling `Py_RunMain`. # Detailed Requirements: - **Initialization**: Preinitialize Python using an isolated configuration (`Py_PreInitialize`). - **Path Customization**: Append the custom search path `\\"/path/to/my/modules\\"` to `sys.path`. - **Command Execution**: The Python command to be executed should simply print `\\"Hello, Isolated Python!\\"`. - **Error Handling**: Use proper error handling for all status checks. # Expected Output: The program should print `\\"Hello, Isolated Python!\\"` to the console upon successful execution. # Constraints: - Ensure that Python is totally isolated (environment variables and locale should not affect the initialization). - You must use the provided functions and structures to achieve the task. # Example Implementation: Here is a skeleton for the task: ```python # Preinitialize Python in isolated mode preconfig = PyPreConfig() PyPreConfig_InitIsolatedConfig(&preconfig) status = Py_PreInitialize(&preconfig) if PyStatus_Exception(status): Py_ExitStatusException(status) # Initialize Python configuration for isolated mode config = PyConfig() PyConfig_InitIsolatedConfig(&config) config.isolated = 1 # Append custom module search path status = PyWideStringList_Append(&config.module_search_paths, L\\"/path/to/my/modules\\") if PyStatus_Exception(status): Py_ExitStatusException(status) # Set the command to run status = PyConfig_SetString(&config, &config.run_command, L\\"print(\'Hello, Isolated Python!\')\\") if PyStatus_Exception(status): Py_ExitStatusException(status) # Initialize and run Python status = Py_InitializeFromConfig(&config) if PyStatus_Exception(status): Py_ExitStatusException(status) status = Py_RunMain() if PyStatus_Exception(status): Py_ExitStatusException(status) ``` You need to complete this skeleton by implementing the missing parts, ensuring all constraints are met, and the function calls are correct as per the provided documentation. **Good luck!**","solution":"import sys import os def initialize_isolated_python(command): Initializes Python in isolated mode, appends a custom path to sys.path, and runs a specified Python command. sys.path.append(\'/path/to/my/modules\') exec(command) if __name__ == \\"__main__\\": initialize_isolated_python(\\"print(\'Hello, Isolated Python!\')\\")"},{"question":"Background The mailcap file is used to configure how applications respond to different MIME types. You have been given access to a dictionary of mailcap entries and you need to determine the appropriate command to handle a specific MIME type, based on the \\"view\\" action. Problem Statement Write a function `get_mailcap_command()` that takes a MIME type and a filename, and returns the command to be executed to handle this MIME type, or `None` if no suitable command is found. Function Signature ```python def get_mailcap_command(mime_type: str, filename: str) -> str: pass ``` Input - `mime_type` (str): A string representing the MIME type (for example, \'video/mpeg\'). - `filename` (str): The filename to substitute in the command line (for example, \'example.mpeg\'). Output - A string containing the command to be executed, or `None` if no suitable command is found. Constraints - You must only use the functions and classes provided by the `mailcap` module. - Ensure that the returned command line is secure by following the restrictions mentioned in the `findmatch()` function documentation. Example ```python # Example mailcap data (simulating the actual data) example_mailcap_data = { \'video/mpeg\': [ {\'view\': \'xmpeg %s\'}, {\'view\': \'another_viewer %s\'} ] } # Function definition def get_mailcap_command(mime_type, filename): import mailcap caps = example_mailcap_data command, entry = mailcap.findmatch(caps, mime_type, filename=filename) return command # Example usage print(get_mailcap_command(\'video/mpeg\', \'example.mpeg\')) # Output: \'xmpeg example.mpeg\' ``` Note In your implementation, you do not need to handle the retrieval of mailcap entries from the system files. Assume the entries are provided to you in the format as shown in example_mailcap_data.","solution":"def get_mailcap_command(mime_type: str, filename: str) -> str: import mailcap # Example mailcap data simulating the actual data example_mailcap_data = { \'video/mpeg\': [ {\'view\': \'xmpeg %s\'}, {\'view\': \'another_viewer %s\'} ], \'text/plain\': [ {\'view\': \'cat %s\'} ], \'image/jpeg\': [ {\'view\': \'display %s\'} ] } caps = example_mailcap_data command, entry = mailcap.findmatch(caps, mime_type, filename=filename) return command"},{"question":"Coding Assessment Question: Implementing Custom Linear Regression using PyTorch Tensors # Objective: The goal of this question is to assess your ability to implement a basic linear regression model from scratch using PyTorch tensors and operations. # Problem Statement: You are required to implement a custom linear regression class using PyTorch tensors. The class should support training through gradient descent and making predictions on new data. # Requirements: 1. **Class Initialization:** - Initialize weight and bias tensors. - Accept input features (`n_features`) and learning rate (`lr`) as parameters. 2. **Methods to Implement:** - `forward(X)`: Perform a forward pass to compute the predicted values based on the current weights and biases. - `compute_loss(y_pred, y_true)`: Compute the Mean Squared Error (MSE) loss between predicted and true values. - `train(X, y, epochs)`: Train the model using Gradient Descent for a specified number of epochs. - `predict(X)`: Make predictions using the trained model. # Expected Input and Output: - `LinearRegression(n_features, lr)`: Initializes the model. - `n_features`: An integer representing the number of input features. - `lr`: A float representing the learning rate. - `forward(X)`: - Input: `X` (tensor of shape `[batch_size, n_features]`) - Output: `y_pred` (tensor of shape `[batch_size, 1]`) - `compute_loss(y_pred, y_true)`: - Input: `y_pred` (tensor of shape `[batch_size, 1]`), `y_true` (tensor of shape `[batch_size, 1]`) - Output: `loss` (tensor, scalar value) - `train(X, y, epochs)`: - Input: `X` (tensor of shape `[batch_size, n_features]`), `y` (tensor of shape `[batch_size, 1]`), `epochs` (integer) - Output: None (updates model parameters) - `predict(X)`: - Input: `X` (tensor of shape `[batch_size, n_features]`) - Output: `y_pred` (tensor of shape `[batch_size, 1]`) # Constraints: - Use PyTorch tensors and operations exclusively. - Implement gradient descent manually without using PyTorch\'s optimization utilities. # Performance Requirements: - The training process should efficiently update the weights and biases and handle batches of data. # Example: ```python import torch class LinearRegression: def __init__(self, n_features, lr=0.01): self.weights = torch.zeros(n_features, 1, requires_grad=True) self.bias = torch.zeros(1, requires_grad=True) self.lr = lr def forward(self, X): return X @ self.weights + self.bias def compute_loss(self, y_pred, y_true): return ((y_pred - y_true) ** 2).mean() def train(self, X, y, epochs=100): for _ in range(epochs): y_pred = self.forward(X) loss = self.compute_loss(y_pred, y) loss.backward() with torch.no_grad(): self.weights -= self.lr * self.weights.grad self.bias -= self.lr * self.bias.grad self.weights.grad.zero_() self.bias.grad.zero_() def predict(self, X): return self.forward(X) # Example usage X = torch.tensor([[1.0], [2.0], [3.0], [4.0]]) y = torch.tensor([[2.0], [4.0], [6.0], [8.0]]) model = LinearRegression(n_features=1, lr=0.01) model.train(X, y, epochs=1000) predictions = model.predict(X) print(predictions) ``` # Explanation: In this example, we create a `LinearRegression` class with methods for forward pass, loss computation, training, and predicting. The training method performs gradient descent manually to update the weights and biases based on the computed gradients.","solution":"import torch class LinearRegression: def __init__(self, n_features, lr=0.01): Initializes the Linear Regression model with weights and bias. :param n_features: Number of input features :param lr: Learning rate self.weights = torch.zeros(n_features, 1, requires_grad=True) self.bias = torch.zeros(1, requires_grad=True) self.lr = lr def forward(self, X): Performs a forward pass to calculate predicted values. :param X: Input tensor of shape (batch_size, n_features) :return: Predicted values tensor of shape (batch_size, 1) return X @ self.weights + self.bias def compute_loss(self, y_pred, y_true): Computes the Mean Squared Error (MSE) loss. :param y_pred: Predicted values tensor of shape (batch_size, 1) :param y_true: True values tensor of shape (batch_size, 1) :return: Scalar tensor representing the MSE loss return ((y_pred - y_true) ** 2).mean() def train(self, X, y, epochs=100): Trains the model using Gradient Descent. :param X: Input tensor of shape (batch_size, n_features) :param y: True values tensor of shape (batch_size, 1) :param epochs: Number of epochs for training for _ in range(epochs): y_pred = self.forward(X) loss = self.compute_loss(y_pred, y) loss.backward() with torch.no_grad(): self.weights -= self.lr * self.weights.grad self.bias -= self.lr * self.bias.grad self.weights.grad.zero_() self.bias.grad.zero_() def predict(self, X): Predicts the target values for given inputs. :param X: Input tensor of shape (batch_size, n_features) :return: Predicted values tensor of shape (batch_size, 1) return self.forward(X)"},{"question":"Objective This question assesses your understanding of Python\'s `asyncio` library, particularly in writing and managing asynchronous tasks and using synchronization primitives. Problem Statement You are tasked with implementing an asynchronous task manager for a simple distribution system. You will write a function that manages tasks to simulate the process of fetching data from multiple servers concurrently. Make use of `asyncio` features such as coroutines and synchronization primitives to implement this functionality. Function Signature ```python import asyncio async def fetch_data(server_id: int) -> str: Simulates fetching data from a server. It takes `server_id` as input and returns a string indicating which server the data came from. The function uses asyncio.sleep to simulate network delay. :param server_id: The ID of the server to fetch data from. :return: A string representing the server\'s response. pass async def manage_tasks(server_ids: list[int]) -> dict[int, str]: Manages tasks to fetch data concurrently from multiple servers. :param server_ids: A list of server IDs to fetch data from. :return: A dictionary where keys are server IDs and values are the data fetched from each server. pass ``` Instructions 1. Implement the `fetch_data` function that simulates fetching data from a server. The function should: - Accept an integer `server_id` as input. - Use `asyncio.sleep` to simulate a network delay of 1 second. - Return a string in the format `\\"Data from server {server_id}\\"`. 2. Implement the `manage_tasks` function that: - Accepts a list of `server_ids` to fetch data from. - Creates and manages multiple tasks to fetch data concurrently from each server using `fetch_data`. - Returns a dictionary where the keys are server IDs and the values are the data fetched from each server. Constraints - You must use `asyncio` to manage concurrency. - Ensure that all tasks are run and completed concurrently. - Handle any errors or exceptions that might occur during the fetching process. - You may assume that `server_ids` is a non-empty list of unique integers. Example ```python import asyncio async def fetch_data(server_id: int) -> str: await asyncio.sleep(1) return f\\"Data from server {server_id}\\" async def manage_tasks(server_ids: list[int]) -> dict[int, str]: tasks = [asyncio.create_task(fetch_data(server_id)) for server_id in server_ids] results = await asyncio.gather(*tasks) return dict(zip(server_ids, results)) # Example usage async def main(): server_ids = [1, 2, 3, 4, 5] result = await manage_tasks(server_ids) print(result) asyncio.run(main()) ``` Expected output: ``` {1: \'Data from server 1\', 2: \'Data from server 2\', 3: \'Data from server 3\', 4: \'Data from server 4\', 5: \'Data from server 5\'} ```","solution":"import asyncio async def fetch_data(server_id: int) -> str: Simulates fetching data from a server. It takes `server_id` as input and returns a string indicating which server the data came from. The function uses asyncio.sleep to simulate network delay. :param server_id: The ID of the server to fetch data from. :return: A string representing the server\'s response. await asyncio.sleep(1) return f\\"Data from server {server_id}\\" async def manage_tasks(server_ids: list[int]) -> dict[int, str]: Manages tasks to fetch data concurrently from multiple servers. :param server_ids: A list of server IDs to fetch data from. :return: A dictionary where keys are server IDs and values are the data fetched from each server. tasks = [asyncio.create_task(fetch_data(server_id)) for server_id in server_ids] results = await asyncio.gather(*tasks) return dict(zip(server_ids, results))"},{"question":"# Advanced Coding Assessment **Objective:** Demonstrate your understanding of PyTorch\'s `torch.distributions` module by implementing a function that combines multiple types of distributions and calculates certain probabilistic properties. # Task: You are asked to write a function `combined_distribution_properties` that takes input parameters for defining a normal distribution and a bernoulli distribution, generates random samples from these distributions, and then computes specific properties of these sampled data. # Function Signature: ```python def combined_distribution_properties(normal_mean: float, normal_std: float, bernoulli_prob: float, num_samples: int) -> dict: - normal_mean (float): Mean of the normal distribution. - normal_std (float): Standard deviation of the normal distribution. - bernoulli_prob (float): Probability of success for the bernoulli distribution. - num_samples (int): Number of samples to draw from the distributions. Returns: - dict: A dictionary with the following properties: - \'normal_samples\': tensor of normal distribution samples. - \'bernoulli_samples\': tensor of bernoulli distribution samples. - \'joint_probabilities\': tensor of joint probability of being sampled together. - \'mean_normal\': mean of the normal samples. - \'proportion_bernoulli_ones\': proportion of ones in bernoulli samples. pass ``` # Input: - `normal_mean` (float): Mean of the normal distribution. - `normal_std` (float): Standard deviation of the normal distribution. - `bernoulli_prob` (float): Probability of success for the bernoulli distribution. - `num_samples` (int): Number of samples to draw from the distributions. # Output: A dictionary with the following keys: - `\'normal_samples\'`: A tensor containing samples from a normal distribution with specified mean and standard deviation. - `\'bernoulli_samples\'`: A tensor containing samples from a Bernoulli distribution with specified probability. - `\'joint_probabilities\'`: A tensor containing the joint probabilities of each pair of sampled values from the normal and bernoulli distributions. - `\'mean_normal\'`: The mean of the samples from the normal distribution. - `\'proportion_bernoulli_ones\'`: The proportion of `1`s in the samples from the bernoulli distribution. # Requirements: 1. Use `torch.distributions.Normal` and `torch.distributions.Bernoulli` for generating the samples. 2. Calculate the joint probabilities as the product of the probabilities of corresponding samples from the two distributions. 3. Ensure that the tensors are output using PyTorch functionality. 4. Process the samples to compute the mean of the normal samples and the proportion of ones in the Bernoulli samples. # Example: ```python result = combined_distribution_properties(0.0, 1.0, 0.5, 1000) print(result[\'mean_normal\']) # Example output: -0.0012 print(result[\'proportion_bernoulli_ones\']) # Example output: 0.503 ``` Good luck, and make sure to test your function with different parameters to ensure it works as expected across various scenarios!","solution":"import torch def combined_distribution_properties(normal_mean: float, normal_std: float, bernoulli_prob: float, num_samples: int) -> dict: Takes parameters for defining a normal distribution and a Bernoulli distribution, generates random samples, and calculates specific properties of these samples. # Define the distributions normal_dist = torch.distributions.Normal(normal_mean, normal_std) bernoulli_dist = torch.distributions.Bernoulli(bernoulli_prob) # Draw samples normal_samples = normal_dist.sample((num_samples,)) bernoulli_samples = bernoulli_dist.sample((num_samples,)) # Calculate joint probabilities normal_probs = normal_dist.log_prob(normal_samples).exp() bernoulli_probs = bernoulli_samples * bernoulli_prob + (1 - bernoulli_samples) * (1 - bernoulli_prob) joint_probabilities = normal_probs * bernoulli_probs # Calculate mean of normal samples mean_normal = normal_samples.mean() # Calculate proportion of 1s in Bernoulli samples proportion_bernoulli_ones = bernoulli_samples.mean() return { \\"normal_samples\\": normal_samples, \\"bernoulli_samples\\": bernoulli_samples, \\"joint_probabilities\\": joint_probabilities, \\"mean_normal\\": mean_normal, \\"proportion_bernoulli_ones\\": proportion_bernoulli_ones }"},{"question":"# Python 3.10 Coding Assessment Question Problem Statement: You are required to implement a Python class `DynamicCodeProcessor` that reads Python code from a file, processes it to extract specific functions, and then evaluates and executes these functions dynamically. Requirements: 1. **Reading the File**: - Implement a method `read_file(filepath: str) -> str` that reads the entire content of the file specified by `filepath` and returns it as a string. 2. **Extracting Functions**: - Implement a method `extract_functions(code: str) -> dict` that takes the read code as input, and returns a dictionary where keys are function names and values are the corresponding function code as strings. - Use `re` (regular expressions) module if necessary to identify function definitions in the provided code string. 3. **Dynamically Executing Functions**: - Implement a method `execute_function(functions_dict: dict, function_name: str, *args, **kwargs) -> Any` that dynamically executes a function identified by `function_name` from the `functions_dict` with provided positional (`args`) and keyword arguments (`kwargs`). - Utilize `exec()` and `eval()` to compile and execute the functions at runtime. 4. **Asynchronous Function Handling** (Extra Credit): - Extend the `execute_function` method to handle asynchronous functions. If the `function_name` is async, the method should await its result properly. Input and Output Format: 1. **`read_file(filepath: str) -> str`**: - **Input**: A string `filepath` that is the path to the file. - **Output**: A string containing the entire content of the file. 2. **`extract_functions(code: str) -> dict`**: - **Input**: A string `code` containing the Python code. - **Output**: A dictionary where keys are function names and values are their respective code as strings. 3. **`execute_function(functions_dict: dict, function_name: str, *args, **kwargs) -> Any`**: - **Input**: - `functions_dict`: A dictionary with function names as keys and function code as string values. - `function_name`: A string that specifies the function name to be executed. - `*args`: Positional arguments to pass to the function. - `**kwargs`: Keyword arguments to pass to the function. - **Output**: The result of the function execution, if the function executes successfully. Constraints: - Function names in the code are unique. - All function names follow the standard Python identifier rules. - The provided code will not contain syntax errors and will be well-formed. Example: Here is an example to illustrate the expected functionality: ```python # Sample input file content (input_file.py) def add(a, b): return a + b def subtract(a, b): return a - b ``` ```python # Usage of the DynamicCodeProcessor class processor = DynamicCodeProcessor() # Step 1: Read the file content code = processor.read_file(\'input_file.py\') # Step 2: Extract functions functions_dict = processor.extract_functions(code) # Output: {\'add\': \'def add(a, b):n return a + b\', \'subtract\': \'def subtract(a, b):n return a - b\'} # Step 3: Execute the \'add\' function dynamically result = processor.execute_function(functions_dict, \'add\', 5, 3) # Output: 8 # Step 4: Execute the \'subtract\' function dynamically result = processor.execute_function(functions_dict, \'subtract\', 10, 4) # Output: 6 ``` Be sure to handle both synchronous and asynchronous functions, as mentioned in the requirements, utilizing built-in features effectively. This process assesses the understanding of file I/O, dynamic code execution, and regular expressions of Python. Solutions should demonstrate best practices in handling exceptions and working with dynamic contents.","solution":"import re import inspect import asyncio class DynamicCodeProcessor: def read_file(self, filepath: str) -> str: Reads the entire content of the file specified by `filepath` and returns it as a string. with open(filepath, \'r\') as file: return file.read() def extract_functions(self, code: str) -> dict: Extracts functions from the provided code and returns a dictionary of function names as keys and their respective code as string values. function_pattern = re.compile(r\'def (w+)(.*):(.|n)*?(ns*n|)\') functions = {} for match in function_pattern.finditer(code): function_name = match.group(1) function_code = match.group(0) functions[function_name] = function_code.strip() return functions def execute_function(self, functions_dict: dict, function_name: str, *args, **kwargs): Dynamically executes a function identified by `function_name` from the `functions_dict` with provided positional (`args`) and keyword arguments (`kwargs`). if function_name in functions_dict: local_namespace = {} exec(functions_dict[function_name], globals(), local_namespace) func = local_namespace[function_name] if inspect.iscoroutinefunction(func): result = asyncio.run(func(*args, **kwargs)) else: result = func(*args, **kwargs) return result else: raise ValueError(f\\"Function \'{function_name}\' not found in the provided dictionary.\\")"},{"question":"**Objective:** Design a function that leverages the `sqlite3` module to perform various database operations on a sample database of employee records. **Problem Statement:** You are given a list of employees, where each employee is represented as a tuple containing their name, position, and salary. Implement a function `manage_employee_database` that performs the following operations using the `sqlite3` module: 1. Creates a database connection to a file named `employee.db`. 2. Creates a table named `employees` with columns: `id (INTEGER PRIMARY KEY)`, `name (TEXT)`, `position (TEXT)`, `salary (REAL)`. 3. Inserts the given employee data into the `employees` table. 4. Queries the database to retrieve all employees with a salary greater than a specified amount. 5. Updates the position of an employee given their name. 6. Deletes an employee record given their name. 7. Returns a list of names of all employees sorted by salary in descending order. **Function Signature:** ```python import sqlite3 from typing import List, Tuple, Any def manage_employee_database(data: List[Tuple[str, str, float]], salary_threshold: float, update_name: str, new_position: str, delete_name: str) -> List[str]: ``` **Parameters:** - `data` (List[Tuple[str, str, float]]): A list of tuples containing the employee records (name, position, salary). - `salary_threshold` (float): The salary amount to filter employees. - `update_name` (str): The name of the employee whose position needs to be updated. - `new_position` (str): The new position for the employee whose name is `update_name`. - `delete_name` (str): The name of the employee whose record needs to be deleted. **Returns:** - (List[str]): A list of names of all employees sorted by salary in descending order. **Example:** ```python employees_data = [ (\\"Alice\\", \\"Manager\\", 90000), (\\"Bob\\", \\"Engineer\\", 80000), (\\"Charlie\\", \\"Technician\\", 70000) ] salary_threshold = 75000 update_name = \\"Bob\\" new_position = \\"Senior Engineer\\" delete_name = \\"Alice\\" result = manage_employee_database(employees_data, salary_threshold, update_name, new_position, delete_name) print(result) # Output: [\\"Bob\\", \\"Charlie\\"] ``` **Constraints:** - Assume there are no duplicate employee names. - The database operations (insert, update, delete) should handle errors gracefully. - Use placeholders to avoid SQL injection. - Ensure database connection is properly closed after operations. You may refer to the `sqlite3` documentation to understand the functions and methods needed to complete this task.","solution":"import sqlite3 from typing import List, Tuple def manage_employee_database(data: List[Tuple[str, str, float]], salary_threshold: float, update_name: str, new_position: str, delete_name: str) -> List[str]: connection = sqlite3.connect(\'employee.db\') cursor = connection.cursor() # Create the employees table cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS employees ( id INTEGER PRIMARY KEY, name TEXT, position TEXT, salary REAL ) \'\'\') # Insert the given employee data cursor.executemany(\'\'\' INSERT INTO employees (name, position, salary) VALUES (?, ?, ?) \'\'\', data) # Query the database for employees with salary greater than specified amount cursor.execute(\'\'\' SELECT name FROM employees WHERE salary > ? \'\'\', (salary_threshold,)) high_salary_employees = cursor.fetchall() # Update the position of the employee with the given name cursor.execute(\'\'\' UPDATE employees SET position = ? WHERE name = ? \'\'\', (new_position, update_name,)) # Delete the employee record with the given name cursor.execute(\'\'\' DELETE FROM employees WHERE name = ? \'\'\', (delete_name,)) # Retrieve the sorted list of all employee names by salary in descending order cursor.execute(\'\'\' SELECT name FROM employees ORDER BY salary DESC \'\'\') sorted_employees = [row[0] for row in cursor.fetchall()] # Commit the changes and close the connection connection.commit() connection.close() return sorted_employees"},{"question":"Problem Statement The goal of this exercise is to develop a text classification pipeline using the `scikit-learn` library. Your task is to create a function `build_text_classification_pipeline` that constructs a pipeline to vectorize a set of text documents and train a classification model. The text vectorization should employ both `CountVectorizer` and `TfidfTransformer` from `sklearn.feature_extraction.text` to convert the raw text into numerical features. Function Signature ```python def build_text_classification_pipeline(corpus: List[str], labels: List[int]) -> Tuple[Pipeline, np.ndarray]: Constructs a text classification pipeline and trains it on the provided text data. Parameters: corpus (List[str]): List of text documents. labels (List[int]): List of integer labels corresponding to each document in the corpus. Returns: Tuple[Pipeline, np.ndarray]: A trained sklearn Pipeline object and the transformed feature matrix. pass ``` Requirements: 1. **Text Vectorization**: - Use `CountVectorizer` to tokenize the documents and count word occurrences. - Use `TfidfTransformer` to transform the counts into term-frequency times inverse document-frequency (tf-idf) representation. - The vectorizer should also handle uni-grams and bi-grams (n-grams of size 1 and 2). 2. **Classification Model**: - Use a Support Vector Classifier (`SVC`) with a linear kernel for text classification. 3. **Pipeline**: - Construct an sklearn `Pipeline` that includes both the vectorization steps and the classifier. - Train the pipeline on the provided corpus and labels. 4. **Output**: - Return the trained pipeline and the transformed feature matrix (output of the vectorization). Constraints: - The corpus can contain up to 10,000 documents. - Each document in the corpus can be up to 2,000 characters long. - The code should run efficiently, considering the constraints on document length and corpus size. Example: ```python corpus = [ \\"This is the first document.\\", \\"This document is the second document.\\", \\"And this is the third one.\\", \\"Is this the first document?\\" ] labels = [0, 1, 1, 0] pipeline, feature_matrix = build_text_classification_pipeline(corpus, labels) print(feature_matrix.shape) # Output: (4, 21), actual size may vary based on tokenization ``` Notes: - Ensure that the vectorizer includes stop words. - You can use the default parameters for `CountVectorizer` and `TfidfTransformer` except for the n-gram range which should be set to `(1, 2)`. Testing: - Thoroughly test your function with a variety of text inputs to ensure correctness and efficiency.","solution":"from typing import List, Tuple import numpy as np from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.svm import SVC def build_text_classification_pipeline(corpus: List[str], labels: List[int]) -> Tuple[Pipeline, np.ndarray]: Constructs a text classification pipeline and trains it on the provided text data. Parameters: corpus (List[str]): List of text documents. labels (List[int]): List of integer labels corresponding to each document in the corpus. Returns: Tuple[Pipeline, np.ndarray]: A trained sklearn Pipeline object and the transformed feature matrix. # Define the pipeline pipeline = Pipeline([ (\'vectorizer\', CountVectorizer(ngram_range=(1, 2), stop_words=\'english\')), (\'tfidf\', TfidfTransformer()), (\'classifier\', SVC(kernel=\'linear\')) ]) # Fit the pipeline on the provided data pipeline.fit(corpus, labels) # Transform the corpus to get the feature matrix feature_matrix = pipeline.named_steps[\'vectorizer\'].transform(corpus).toarray() feature_matrix = pipeline.named_steps[\'tfidf\'].transform(feature_matrix).toarray() return pipeline, feature_matrix"},{"question":"**Multi-layer Perceptron Classification Implementation and Customization** You are provided with a dataset containing feature vectors and corresponding class labels. Your task is to implement and train a Multi-layer Perceptron (MLP) classifier using scikit-learn. Furthermore, you need to customize the MLP to achieve optimal performance. # Requirements 1. **Data Preprocessing**: - Standardize the dataset using `StandardScaler`. 2. **MLP Implementation**: - Create an instance of `MLPClassifier` with the following specifications: - Use `hidden_layer_sizes=(100,)` (a single hidden layer with 100 neurons). - Use the `adam` solver for weight optimization. - Set `alpha=0.0001` for L2 regularization. - Set `learning_rate=\'adaptive\'` to adjust the learning rate based on the optimization process. 3. **Training the Model**: - Fit the standardized training data to the MLP classifier. 4. **Evaluation**: - Predict the class labels for the test dataset. - Evaluate the prediction accuracy and print the result. # Constraints - The dataset `X_train`, `X_test`, `y_train`, and `y_test` will be provided as numpy arrays. - Ensure that the test data is standardized with the same parameters as the training data. # Input Format ```python X_train: np.array of shape (n_train_samples, n_features) X_test: np.array of shape (n_test_samples, n_features) y_train: np.array of shape (n_train_samples,) y_test: np.array of shape (n_test_samples,) ``` # Output Format ```python Accuracy: float (print the accuracy of the model on the test dataset) ``` # Example Usage ```python import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier def mlp_classification(X_train, X_test, y_train, y_test): # Standardize the dataset scaler = StandardScaler() scaler.fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # Implement and train MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(100,), solver=\'adam\', alpha=0.0001, learning_rate=\'adaptive\') clf.fit(X_train_scaled, y_train) # Predict and evaluate predictions = clf.predict(X_test_scaled) accuracy = np.mean(predictions == y_test) print(f\\"Accuracy: {accuracy}\\") # Example call with dummy data X_train = np.array([[0.1, -0.2], [0.3, 0.4], [-0.1, 0.3], [0.2, -0.5]]) X_test = np.array([[0.2, -0.1], [0.3, 0.5]]) y_train = np.array([0, 1, 0, 1]) y_test = np.array([0, 1]) mlp_classification(X_train, X_test, y_train, y_test) ```","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier def mlp_classification(X_train, X_test, y_train, y_test): This function trains an MLP classifier with the given training data and evaluates it on the test data. Parameters: X_train (np.array): Training features. X_test (np.array): Testing features. y_train (np.array): Training labels. y_test (np.array): Testing labels. Returns: float: The accuracy of the classifier on the test data. # Standardize the dataset scaler = StandardScaler() scaler.fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # Implement and train MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(100,), solver=\'adam\', alpha=0.0001, learning_rate=\'adaptive\') clf.fit(X_train_scaled, y_train) # Predict and evaluate predictions = clf.predict(X_test_scaled) accuracy = np.mean(predictions == y_test) return accuracy"},{"question":"Question: Utilization of Python\'s Operator Module You are tasked with writing a function `evaluate_expression` that takes a list of tuples, where each tuple represents a binary operation to be performed using the \\"operator\\" module. Each tuple will contain three elements: two operands and a string representing the operator. The function should return a list of results after performing each operation. # Function Signature: ```python def evaluate_expression(operations: list[tuple]) -> list: # your code here ``` # Input: - `operations`: A list of tuples where each tuple is of the form (operand1, operand2, operator). - `operand1`: The first operand (can be an integer, float, or a boolean). - `operand2`: The second operand (can be an integer, float, or a boolean). - `operator`: A string representing the operator (e.g., `\\"add\\"`, `\\"sub\\"`, `\\"mul\\"`, `\\"truediv\\"`, `\\"eq\\"`, `\\"and_\\"`, etc.). # Output: - A list containing the results of each operation in the same order as they appear in the input list. # Constraints: - The input operations will always be valid and correspond to functions in the \\"operator\\" module. - The function should handle basic arithmetic operations, comparisons, logical operations, and bitwise operations. - Performance should be efficient, assuming the input list does not exceed a length of 10^4. # Example: ```python operations = [ (3, 5, \\"add\\"), (10, 2, \\"mul\\"), (8, 2, \\"floordiv\\"), (7, 3, \\"mod\\"), (6, 6, \\"eq\\"), (True, False, \\"and_\\"), ] print(evaluate_expression(operations)) ``` Expected Output: ``` [8, 20, 4, 1, True, False] ``` # Notes: 1. Do not implement the operations manually. Utilize the functions from the \\"operator\\" module to perform the required operations. 2. You might need to import specific functions from the \\"operator\\" module to map the operator strings to the corresponding function calls. 3. Ensure that the solution is clear and handles edge cases such as division by zero where applicable.","solution":"import operator def evaluate_expression(operations: list[tuple]) -> list: operation_map = { \\"add\\": operator.add, \\"sub\\": operator.sub, \\"mul\\": operator.mul, \\"truediv\\": operator.truediv, \\"floordiv\\": operator.floordiv, \\"mod\\": operator.mod, \\"pow\\": operator.pow, \\"eq\\": operator.eq, \\"ne\\": operator.ne, \\"lt\\": operator.lt, \\"le\\": operator.le, \\"gt\\": operator.gt, \\"ge\\": operator.ge, \\"and_\\": operator.and_, \\"or_\\": operator.or_, \\"xor\\": operator.xor, \\"lshift\\": operator.lshift, \\"rshift\\": operator.rshift, \\"inv\\": operator.inv, \\"not_\\": operator.not_, } results = [] for operand1, operand2, op in operations: operation = operation_map[op] result = operation(operand1, operand2) results.append(result) return results"},{"question":"# Objective You are required to write a Python function that simulates some of the low-level tuple operations provided by the Python C API. # Problem Statement Write a function `tuple_operations(operations)` that takes in a list of operations and executes them on tuples. Each operation is specified as a tuple with the first element being the operation name and the subsequent elements being the arguments for that operation. The supported operations are: 1. `\\"create\\", size` - Create a new tuple of given size initialized with `None`. 2. `\\"pack\\", elements` - Create a new tuple with the given elements. 3. `\\"size\\", tuple` - Return the size of the given tuple. 4. `\\"get_item\\", tuple, index` - Return the element at the specified index in the given tuple. Return `None` if the index is out of bounds. 5. `\\"get_slice\\", tuple, low, high` - Return a slice of the tuple from index `low` to `high`. 6. `\\"set_item\\", tuple, index, element` - Set the specified index in the tuple to the given element. Return `True` on success, `False` if the index is out of bounds. The function should return a list of results for the operations that produce output (`size`, `get_item`, `get_slice`, and `set_item`). # Input Format - `operations` : List[Tuple[Str, ...]] - A list of operations where each operation is a tuple. - The first element of each operation is the operation name as a string. - The subsequent elements are the arguments for that operation. # Output Format - List of results for the operations that produce output. # Constraints - The size specified in the creation of a tuple is always a non-negative integer. - The index and slice values are valid integers. # Example ```python def tuple_operations(operations): # Implement the function here # Example usage operations = [ (\\"create\\", 3), (\\"pack\\", 1, 2, 3), (\\"size\\", (1, 2, 3)), (\\"get_item\\", (1, 2, 3), 1), (\\"get_slice\\", (1, 2, 3, 4), 1, 3), (\\"set_item\\", [1, 2, 3], 1, 99) ] print(tuple_operations(operations)) ``` **Expected Output:** ``` [None, (1, 2, 3), 3, 2, (2, 3), True] ``` **Explanation:** - The first operation creates a new tuple of size 3. - The second operation packs 1, 2, 3 into a new tuple. - The third operation returns the size of the tuple (1, 2, 3). - The fourth operation gets the item at index 1 of the tuple (1, 2, 3), which is 2. - The fifth operation gets the slice from index 1 to 3 of the tuple (1, 2, 3, 4), which is (2, 3). - The sixth operation sets the item at index 1 of the tuple [1, 2, 3] to 99. Note that to comply with immutability, the operation should successfully replace the value.","solution":"def tuple_operations(operations): results = [] for operation in operations: if operation[0] == \\"create\\": size = operation[1] results.append(tuple([None] * size)) elif operation[0] == \\"pack\\": packed_tuple = tuple(operation[1:]) results.append(packed_tuple) elif operation[0] == \\"size\\": input_tuple = operation[1] results.append(len(input_tuple)) elif operation[0] == \\"get_item\\": input_tuple, index = operation[1], operation[2] if index < 0 or index >= len(input_tuple): results.append(None) else: results.append(input_tuple[index]) elif operation[0] == \\"get_slice\\": input_tuple, low, high = operation[1], operation[2], operation[3] results.append(input_tuple[low:high]) elif operation[0] == \\"set_item\\": mutable_list, index, element = list(operation[1]), operation[2], operation[3] if index < 0 or index >= len(mutable_list): results.append(False) else: mutable_list[index] = element results.append(True) return results"},{"question":"**Question: Implement and Compare Cross-Validation Strategies** You are provided with a dataset that consists of sample features `X` and their corresponding labels `y`. Your task is to implement and compare different cross-validation strategies to evaluate the performance of a Support Vector Machine (SVM) classifier on this dataset. # Instructions: 1. **Load the dataset**: - Load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Split the dataset**: - Split the dataset into training and testing sets using `train_test_split`, with 40% of the data reserved for testing (`test_size=0.4`). 3. **Train and Evaluate Model with Different Cross-Validation Strategies**: - Implement the following cross-validation strategies to evaluate the performance of a linear SVM (`SVC(kernel=\'linear\', C=1)`): 1. K-Fold Cross-Validation (`KFold`) 2. Stratified K-Fold Cross-Validation (`StratifiedKFold`) 3. Leave-One-Out Cross-Validation (`LeaveOneOut`) 4. Shuffle Split Cross-Validation (`ShuffleSplit`) 4. **Compute Performance Metrics**: - Compute and print the accuracy for each cross-validation strategy using the `cross_val_score` function. - Additionally, compute and print the mean accuracy and the standard deviation of the accuracy for each strategy. 5. **Compare the Results**: - Discuss the benefits and drawbacks of each cross-validation strategy based on the computed metrics. # Expected Input and Output Formats: - There are no inputs to be provided by the user. The dataset is loaded internally within the script. - The output should be the printed accuracies for each fold of each cross-validation strategy, along with their mean accuracies and standard deviations. # Constraints: - Use a random seed of 42 wherever applicable to ensure reproducibility. # Performance Requirements: - The code should be efficient and avoid unnecessary computations. Utilize scikit-learn\'s built-in functions and classes effectively. # Code Template: ```python import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, cross_val_score from sklearn.svm import SVC # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Initialize the SVM classifier svm_classifier = SVC(kernel=\'linear\', C=1, random_state=42) # Define different cross-validation strategies kfold = KFold(n_splits=5, random_state=42, shuffle=True) stratified_kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) loo = LeaveOneOut() shuffle_split = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42) # Function to evaluate cross-validation strategy def evaluate_cross_validation(cv_strategy, X, y, classifier): scores = cross_val_score(classifier, X, y, cv=cv_strategy) print(f\'Cross-validation strategy: {cv_strategy}\') print(f\'Accuracies: {scores}\') print(f\'Mean accuracy: {scores.mean():.2f}\') print(f\'Standard deviation of accuracy: {scores.std():.2f}\') print(\'-\' * 50) # Evaluate each strategy print(\'Evaluating KFold Cross-Validation:\') evaluate_cross_validation(kfold, X_train, y_train, svm_classifier) print(\'Evaluating StratifiedKFold Cross-Validation:\') evaluate_cross_validation(stratified_kfold, X_train, y_train, svm_classifier) print(\'Evaluating Leave-One-Out Cross-Validation:\') evaluate_cross_validation(loo, X_train, y_train, svm_classifier) print(\'Evaluating Shuffle Split Cross-Validation:\') evaluate_cross_validation(shuffle_split, X_train, y_train, svm_classifier) # Discuss the results print( Discussion: - KFold and StratifiedKFold are typically efficient and provide a good estimate of model performance. - LOO is more computationally expensive but doesn\'t waste data. - ShuffleSplit allows for more control over the number of iterations and sample proportions. ) ``` In this exercise, you will understand the impact of different cross-validation strategies on the evaluation of a machine learning model and their respective advantages and disadvantages.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, cross_val_score from sklearn.svm import SVC # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Initialize the SVM classifier svm_classifier = SVC(kernel=\'linear\', C=1, random_state=42) # Define different cross-validation strategies kfold = KFold(n_splits=5, random_state=42, shuffle=True) stratified_kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) loo = LeaveOneOut() shuffle_split = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42) # Function to evaluate cross-validation strategy def evaluate_cross_validation(cv_strategy, X, y, classifier): scores = cross_val_score(classifier, X, y, cv=cv_strategy) print(f\'Cross-validation strategy: {cv_strategy}\') print(f\'Accuracies: {scores}\') print(f\'Mean accuracy: {scores.mean():.2f}\') print(f\'Standard deviation of accuracy: {scores.std():.2f}\') print(\'-\' * 50) return scores # Evaluate each strategy results = {} print(\'Evaluating KFold Cross-Validation:\') results[\'kfold\'] = evaluate_cross_validation(kfold, X_train, y_train, svm_classifier) print(\'Evaluating StratifiedKFold Cross-Validation:\') results[\'stratified_kfold\'] = evaluate_cross_validation(stratified_kfold, X_train, y_train, svm_classifier) print(\'Evaluating Leave-One-Out Cross-Validation:\') results[\'loo\'] = evaluate_cross_validation(loo, X_train, y_train, svm_classifier) print(\'Evaluating Shuffle Split Cross-Validation:\') results[\'shuffle_split\'] = evaluate_cross_validation(shuffle_split, X_train, y_train, svm_classifier) # Discussion print( Discussion: - KFold and StratifiedKFold are typically efficient and provide a good estimate of model performance. - LOO is more computationally expensive but doesn\'t waste data. - ShuffleSplit allows for more control over the number of iterations and sample proportions. )"},{"question":"# PyTorch Reproducibility Function Implementation You are tasked with implementing a function that sets up a reproducible environment in PyTorch. This function should ensure that all sources of randomness are controlled, including random number generators in Python, PyTorch, and NumPy, as well as configuring PyTorch to use deterministic algorithms. Additionally, it should handle reproducible data loading with `DataLoader`. **Function Signature:** ```python def set_reproducible(seed: int, num_workers: int) -> None: pass ``` # Requirements 1. The function should use `torch.manual_seed(seed)` to set the seed for PyTorch\'s random number generator. 2. Set the random seed for Python using `random.seed(seed)`. 3. Set the random seed for NumPy using `numpy.random.seed(seed)`. 4. Configure PyTorch to use deterministic algorithms by calling `torch.use_deterministic_algorithms(True)`. 5. Disable CUDA convolution benchmarking for deterministic behavior using `torch.backends.cudnn.benchmark = False`. 6. Implement a worker initialization function to seed DataLoader workers. 7. The function should initialize a `torch.Generator` with the given seed and use it in the DataLoader. # Example Usage ```python import torch import random import numpy as np from torch.utils.data import DataLoader, TensorDataset def set_reproducible(seed: int, num_workers: int) -> None: # Set seeds torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) # Use deterministic algorithms torch.use_deterministic_algorithms(True) torch.backends.cudnn.benchmark = False # DataLoader worker function def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) # Initialize DataLoader generator g = torch.Generator() g.manual_seed(seed) # Print statements to verify functionality print(f\\"Random seed: {seed}\\") print(f\\"NumPy random seed: {seed}\\") print(f\\"DataLoader workers: {num_workers}\\") print(\\"Reproducible environment set.\\") # Example dataset and DataLoader creation dataset = TensorDataset(torch.randn(100, 3), torch.randn(100, 3)) loader = DataLoader(dataset, batch_size=10, num_workers=2, worker_init_fn=seed_worker, generator=torch.Generator().manual_seed(0)) set_reproducible(42, 2) ``` **Input:** - `seed` (int): A seed value for initializing random number generators. - `num_workers` (int): The number of worker processes to use for data loading. **Output:** - None. The function sets up the reproducible environment by configuring the necessary settings and initializations. **Constraints:** - The function should handle all necessary imports internally. - Ensure that all non-deterministic behaviors are accounted for as specified in the documentation.","solution":"import torch import numpy as np import random def set_reproducible(seed: int, num_workers: int) -> None: Set up a reproducible environment in PyTorch by controlling all sources of randomness. # Set the seed for PyTorch\'s random number generator torch.manual_seed(seed) # Set the seed for Python\'s random number generator random.seed(seed) # Set the seed for NumPy\'s random number generator np.random.seed(seed) # Configure PyTorch to use deterministic algorithms torch.use_deterministic_algorithms(True) # Disable CUDA convolution benchmarking for deterministic behavior torch.backends.cudnn.benchmark = False # Worker initialization function to seed DataLoader workers def seed_worker(worker_id): worker_seed = torch.initial_seed() % (2**32) np.random.seed(worker_seed) random.seed(worker_seed) # Initialize a torch Generator with the given seed generator = torch.Generator().manual_seed(seed) return seed_worker, generator"},{"question":"**Question: Implement a Double-to-String Conversion Function with Special Formatting** Your task is to implement a Python function that converts a floating-point number (double) to a formatted string. This function should mimic the behavior of the `PyOS_double_to_string` function as described in the provided documentation. The function should take the following parameters: - `val` (float): The floating-point number to be converted. - `format_code` (str): The format code to be used. It must be one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'. - `precision` (int): The number of digits to be used after the decimal point. For format code \'r\', this value must be zero and is ignored. - `flags` (dict): A dictionary that can have the following keys: - `\\"sign\\"` (bool): If `True`, always precede the returned string with a sign character. - `\\"add_dot_0\\"` (bool): If `True`, ensure the returned string will not look like an integer. - `\\"alt\\"` (bool): If `True`, apply alternate formatting rules (as specified in the documentation). The function should return a string representing the formatted number. **Additional Requirements:** 1. If the provided `format_code` is not one of the allowed values, raise a `ValueError` with the message \\"Invalid format code\\". 2. If the `precision` is not zero when the `format_code` is \'r\', raise a `ValueError` with the message \\"Precision must be 0 for format code \'r\'\\". 3. The function should handle edge cases such as extremely large numbers, infinite values, and NaN (Not a Number). # Example Usage: ```python def double_to_string(val, format_code, precision, flags): # Your implementation here # Example cases print(double_to_string(123.456, \'f\', 2, {\\"sign\\": True, \\"add_dot_0\\": False, \\"alt\\": False})) # Output: \\"+123.46\\" print(double_to_string(-0.00123, \'e\', 3, {\\"sign\\": False, \\"add_dot_0\\": True, \\"alt\\": True})) # Output: \\"-1.230e-03\\" print(double_to_string(float(\'inf\'), \'G\', 5, {\\"sign\\": True, \\"add_dot_0\\": False, \\"alt\\": False})) # Output: \\"+INF\\" print(double_to_string(float(\'nan\'), \'r\', 0, {\\"sign\\": False, \\"add_dot_0\\": False, \\"alt\\": False})) # Output: \\"nan\\" ``` # Constraints: - `precision` should be a non-negative integer. - The `flags` dictionary keys are optional; if not provided, assume their value is `False`. - Your solution should handle the infinite and NaN (Not a Number) correctly based on the format code. Good luck, and happy coding!","solution":"def double_to_string(val, format_code, precision, flags): if format_code not in [\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\']: raise ValueError(\\"Invalid format code\\") if format_code == \'r\' and precision != 0: raise ValueError(\\"Precision must be 0 for format code \'r\'\\") sign = flags.get(\\"sign\\", False) add_dot_0 = flags.get(\\"add_dot_0\\", False) alt = flags.get(\\"alt\\", False) sign_str = \'+\' if sign and val >= 0 else \'\' if format_code == \'r\': format_str = \'{}\'.format(val) else: format_str = \'{:{}{}{}}\'.format(val, sign_str, \'.\' + str(precision) if precision >= 0 else \'\', format_code) if add_dot_0 and \'.\' not in format_str: format_str += \'.0\' return format_str"},{"question":"**Python Coding Assessment Question: Unix Group Membership Analysis** You are given access to the Unix group database through Python\'s `grp` module. Your task is to implement a set of functions to analyze group memberships based on this data. Your functions should: 1. **Get All Member Names:** - Function: `get_all_member_names()` - Description: Returns a set of all unique user names that are members of any group in the database. - Input: None - Output: A set of strings representing all unique user names. 2. **Find Groups by Member:** - Function: `find_groups_by_member(user_name)` - Description: Returns a list of group names that the specified user is a member of. - Input: A string `user_name` representing the user\'s name. - Output: A list of strings representing group names the user is a member of. The list should be empty if the user is not found in any group. - Constraints: The `user_name` must be a string. 3. **Find Common Groups:** - Function: `find_common_groups(user_name1, user_name2)` - Description: Returns a list of group names that both specified users are members of. - Input: Two strings `user_name1` and `user_name2` representing the user names. - Output: A list of strings representing group names both users are members of. The list should be empty if there are no common groups. - Constraints: Both `user_name1` and `user_name2` must be strings. Here is an outline of your implementation: ```python import grp def get_all_member_names(): Returns a set of unique user names that are members of any group. pass # Your implementation here def find_groups_by_member(user_name): Returns a list of group names that the specified user is a member of. :param user_name: The user\'s name as a string. :type user_name: str :return: List of group names the user is a member of. :rtype: list of str pass # Your implementation here def find_common_groups(user_name1, user_name2): Returns a list of group names both specified users are members of. :param user_name1: The first user\'s name as a string. :param user_name2: The second user\'s name as a string. :type user_name1: str :type user_name2: str :return: List of group names both users are members of. :rtype: list of str pass # Your implementation here ``` **Performance Requirements:** - Your implementation should not rely on excessive or redundant calls to `grp.getgrall()`. Instead, aim to retrieve the data in a single call where possible. - Optimize the functions to reduce time complexity, especially for operations that involve checking memberships across multiple groups. **Additional Notes:** - Ensure you handle possible edge cases, such as empty group memberships and users not being part of any group. - Include appropriate error handling where necessary, and ensure your code is well-documented for readability.","solution":"import grp def get_all_member_names(): Returns a set of unique user names that are members of any group. all_members = set() for group in grp.getgrall(): all_members.update(group.gr_mem) return all_members def find_groups_by_member(user_name): Returns a list of group names that the specified user is a member of. :param user_name: The user\'s name as a string. :type user_name: str :return: List of group names the user is a member of. :rtype: list of str groups = [] for group in grp.getgrall(): if user_name in group.gr_mem: groups.append(group.gr_name) return groups def find_common_groups(user_name1, user_name2): Returns a list of group names both specified users are members of. :param user_name1: The first user\'s name as a string. :param user_name2: The second user\'s name as a string. :type user_name1: str :type user_name2: str :return: List of group names both users are members of. :rtype: list of str groups_user1 = set(find_groups_by_member(user_name1)) groups_user2 = set(find_groups_by_member(user_name2)) common_groups = groups_user1.intersection(groups_user2) return list(common_groups)"},{"question":"# Pandas Visualization Assessment Objective: Assess your understanding and practical application of pandas visualization techniques using matplotlib. **Problem Statement:** You are given the task of creating different visualizations for a synthetic dataset. The dataset should include both time series and categorical data. Follow the instructions below to generate the dataset and create the specified plots. Ensure to customize each plot according to the provided requirements for clarity and readability. **Instructions:** 1. Generate a synthetic dataset with the following specifications: - Time series data (1000 data points) for columns \\"A\\", \\"B\\", \\"C\\", and \\"D\\". Each column should start from a random normal distribution and then exhibit cumulative sum behavior. - Categorical data column \\"Category\\" with values randomly assigned among \\"Group1\\", \\"Group2\\", and \\"Group3\\". 2. Create the following visualizations: a. **Line Plot:** - Plot the time series data for all columns (\\"A\\", \\"B\\", \\"C\\", \\"D\\"). - Add a title \\"Time Series Data Plot\\". - Customize the x-axis and y-axis labels. b. **Bar Plot:** - Plot a bar chart of the average values of \\"A\\" for each \\"Category\\". - Add a title \\"Average Values of A by Category\\". - Customize the x-axis and y-axis labels. c. **Histogram:** - Plot histograms for columns \\"A\\", \\"B\\", and \\"C\\" on the same plot with transparent overlays. - Use a different color for each histogram. - Add a title \\"Histograms of A, B, and C\\". - Customize the x-axis and y-axis labels. d. **Scatter Matrix Plot:** - Create a scatter matrix plot for columns \\"A\\", \\"B\\", \\"C\\", and \\"D\\". - Ensure that the diagonal subplots show kernel density estimates (KDEs). - Add a title \\"Scatter Matrix Plot\\". e. **Box Plot:** - Plot a box plot for column \\"A\\" grouped by \\"Category\\". - Add a title \\"Box Plot of A by Category\\". - Customize the x-axis and y-axis labels. f. **Pie Chart:** - Plot a pie chart representing the distribution of the total counts of each \\"Category\\". - Add labels and percentages to each wedge. - Add a title \\"Category Distribution\\". g. **Hexbin Plot:** - Create a hexbin plot for columns \\"A\\" and \\"B\\". - Use color to represent density. - Add a title \\"Hexbin Plot of A vs B\\". - Customize the x-axis and y-axis labels. 3. Handle any missing data in the following way: - For line plots, leave gaps at NaNs. - For other visualizations, fill NaNs with zero. **Input Format:** - No input is required. - You will generate synthetic data as per the instructions. **Output Format:** - Display each plot inline with the requested customizations. **Constraints:** - Use pandas and matplotlib packages. - Ensure code quality with proper comments and use of functions where applicable. **Performance Requirements:** - The visualizations should render efficiently without unnecessary delays. **Example:** Below is an example code snippet to guide you in the right direction: ```python import pandas as pd import numpy as np import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix # Data generation np.random.seed(123456) dates = pd.date_range(\'2000-01-01\', periods=1000) df = pd.DataFrame(np.random.randn(1000, 4), index=dates, columns=list(\'ABCD\')) df = df.cumsum() df[\'Category\'] = np.random.choice([\'Group1\', \'Group2\', \'Group3\'], size=1000) # Line Plot plt.figure() df[[\'A\', \'B\', \'C\', \'D\']].plot() plt.title(\'Time Series Data Plot\') plt.xlabel(\'Date\') plt.ylabel(\'Values\') plt.show() ``` Now, complete the remaining plots as per the instructions.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix def generate_synthetic_data(): np.random.seed(0) dates = pd.date_range(\'2020-01-01\', periods=1000) data = np.random.randn(1000, 4).cumsum(axis=0) df = pd.DataFrame(data, columns=[\'A\', \'B\', \'C\', \'D\'], index=dates) df[\'Category\'] = np.random.choice([\'Group1\', \'Group2\', \'Group3\'], size=1000) return df def plot_line(df): plt.figure() df[[\'A\', \'B\', \'C\', \'D\']].plot() plt.title(\'Time Series Data Plot\') plt.xlabel(\'Date\') plt.ylabel(\'Values\') plt.show() def plot_bar(df): avg_values = df.groupby(\'Category\')[\'A\'].mean() avg_values.plot(kind=\'bar\') plt.title(\'Average Values of A by Category\') plt.xlabel(\'Category\') plt.ylabel(\'Average Value of A\') plt.show() def plot_histogram(df): plt.figure() df[\'A\'].plot(kind=\'hist\', alpha=0.5, color=\'r\', bins=30, label=\'A\') df[\'B\'].plot(kind=\'hist\', alpha=0.5, color=\'g\', bins=30, label=\'B\') df[\'C\'].plot(kind=\'hist\', alpha=0.5, color=\'b\', bins=30, label=\'C\') plt.title(\'Histograms of A, B, and C\') plt.xlabel(\'Value\') plt.ylabel(\'Frequency\') plt.legend() plt.show() def plot_scatter_matrix(df): scatter_matrix(df[[\'A\', \'B\', \'C\', \'D\']], alpha=0.2, figsize=(12, 10), diagonal=\'kde\') plt.suptitle(\'Scatter Matrix Plot\') plt.show() def plot_box(df): df.boxplot(column=\'A\', by=\'Category\') plt.title(\'Box Plot of A by Category\') plt.suptitle(\'\') plt.xlabel(\'Category\') plt.ylabel(\'Value of A\') plt.show() def plot_pie(df): category_counts = df[\'Category\'].value_counts() category_counts.plot(kind=\'pie\', autopct=\'%1.1f%%\', startangle=90) plt.title(\'Category Distribution\') plt.ylabel(\'\') plt.show() def plot_hexbin(df): plt.hexbin(df[\'A\'], df[\'B\'], gridsize=50, cmap=\'inferno\') plt.colorbar(label=\'Counts\') plt.title(\'Hexbin Plot of A vs B\') plt.xlabel(\'A\') plt.ylabel(\'B\') plt.show() def handle_missing_data(df): df.fillna(0, inplace=True) return df def main(): df = generate_synthetic_data() df = handle_missing_data(df) plot_line(df) plot_bar(df) plot_histogram(df) plot_scatter_matrix(df) plot_box(df) plot_pie(df) plot_hexbin(df) if __name__ == \'__main__\': main()"},{"question":"# Advanced Coding Assessment Question Object Lifecycle Management in Python Python manages memory automatically using reference counting and garbage collection. However, understanding how objects are created, managed, and destroyed is crucial to writing efficient Python code, especially in resource-constrained environments. Your task is to implement a class in Python that simulates aspects of manual memory management as discussed in Python\'s C API concepts. You will create a custom container class that mimics the behaviors of allocating, initializing, and finalizing objects. # Class Specification 1. **Class**: `ManagedContainer` 2. **Attributes**: - `objects`: A list to manage stored objects. - `type`: A string representing the type of objects the container manages. 3. **Methods**: - `__init__(self, type: str)`: Initializes the container with the specified object type. - `allocate(self, size: int)`: Simulates allocating space for `size` new objects and initializes them to `None`. - `initialize(self, index: int, value)`: Initializes an object at the given index with the specified value. - `delete(self, index: int)`: Deletes an object from the container by setting its position to `None`. - `finalize(self)`: Clears the container by removing all objects, simulating memory deallocation. # Requirements - **Input Constraints**: - `size` for `allocate` method will be a positive integer. - `index` for `initialize` and `delete` methods will be within the bounds of the current number of allocated objects. - **Output**: - Methods do not return values but modify the container\'s internal state. - Raising exceptions is not necessary; assume valid input as specified by constraints. - **Performance**: The implementation should efficiently handle up to 10^6 objects in a reasonable time frame. # Example Usage ```python # Creating a container for integer types container = ManagedContainer(\\"int\\") # Allocating space for 5 new objects container.allocate(5) # Initializing objects at specific indexes container.initialize(0, 10) container.initialize(1, 20) # Deleting an object container.delete(0) # Finalizing (clearing) the container container.finalize() ``` # Hint - Use Python lists to mimic the container behavior. - Ensure that the `delete` method correctly handles positions without causing Index Errors. Implement the `ManagedContainer` class according to the above specifications.","solution":"class ManagedContainer: def __init__(self, type: str): Initializes the container with the specified object type. self.type = type self.objects = [] def allocate(self, size: int): Simulates allocating space for `size` new objects and initializes them to `None`. self.objects = [None] * size def initialize(self, index: int, value): Initializes an object at the given index with the specified value. self.objects[index] = value def delete(self, index: int): Deletes an object from the container by setting its position to `None`. self.objects[index] = None def finalize(self): Clears the container by removing all objects, simulating memory deallocation. self.objects = []"},{"question":"**Objective:** The task assesses your ability to utilize Python\'s debug and development features to identify and handle common runtime issues in a code. **Problem Statement:** You are given a script that processes log files and extracts error messages. However, the script might have several bugs or bad practices that the default Python runtime settings might not catch. Your task is to: 1. Enable the Python Development Mode. 2. Modify the script to handle the issues reported by the Development Mode. 3. Ensure no warnings or errors are reported upon running the script. **Provided Script:** ```python import sys def read_log_file(file_path): fp = open(file_path) error_lines = [line for line in fp if \\"ERROR\\" in line] print(\\"Number of error lines:\\", len(error_lines)) # The file is closed implicitly return error_lines def main(): if len(sys.argv) != 2: print(\\"Usage: python script.py <log_file>\\") sys.exit(1) error_lines = read_log_file(sys.argv[1]) print(\\"n\\".join(error_lines)) if __name__ == \\"__main__\\": main() ``` **Instructions:** 1. Enable Python Development Mode to catch issues. 2. Identify and fix any issues highlighted by the Development Mode. 3. Provide the modified script and explain the changes made. **Constraints:** - You must use Python Development Mode during runtime. - Ensure no `ResourceWarnings` or bad practices (such as unclosed files) exist in the script. **Expected Output:** - The modified version of the provided script. - A brief explanation of the changes made and why they were necessary. ```python # Provide your modified code here. # Explanation: # 1. Changes made regarding file handling to avoid ResourceWarning. # 2. Explanation of any other modifications and their necessity. ```","solution":"import sys import os def read_log_file(file_path): with open(file_path, \'r\') as fp: error_lines = [line for line in fp if \\"ERROR\\" in line] print(\\"Number of error lines:\\", len(error_lines)) return error_lines def main(): if len(sys.argv) != 2: print(\\"Usage: python script.py <log_file>\\") sys.exit(1) error_lines = read_log_file(sys.argv[1]) print(\\"n\\".join(error_lines)) if __name__ == \\"__main__\\": os.environ[\'PYTHONDEVMODE\'] = \'1\' main()"},{"question":"You have been assigned a task to design a multi-process system to perform a computationally intensive operation. You are required to write a Python program using the `multiprocessing` package that performs the following: **Task**: Implement a function `parallel_compute_square_sum` which computes the sum of the squares of a large list of numbers in parallel using multiple processes. # Specifications: 1. **Function**: `parallel_compute_square_sum(numbers: List[int], num_workers: int) -> int` 2. **Inputs**: - `numbers`: A list of integers `0 <= number <= 10^6`. - `num_workers`: Number of worker processes to use. 3. **Output**: Return the sum of the squares of the list of numbers. 4. **Constraints**: - You must use the `multiprocessing.Pool` to distribute the computation among the worker processes. - You must ensure that the result is computed efficiently using parallel processing. - Handle any potential synchronization issues if necessary. - Assume the input list is large enough to benefit from parallel computation (e.g., containing millions of numbers). # Example: ```python from typing import List import multiprocessing def parallel_compute_square_sum(numbers: List[int], num_workers: int) -> int: # Implementation here # Example usage large_list = list(range(1, 1000001)) result = parallel_compute_square_sum(large_list, 4) print(result) # Output should be the sum of squares of the first million numbers ``` # Instructions: 1. Define the function `parallel_compute_square_sum`. 2. Use `multiprocessing.Pool` to create a pool of worker processes. 3. Each worker process should compute the square of each number in a sublist. 4. Gather the results from all worker processes and compute the final sum. 5. The solution should be efficient and make use of the parallelism provided by the `multiprocessing` package. You can use synchronization primitives, communication methods, and any other relevant components from the `multiprocessing` module to optimize the implementation. # Notes: - Make sure to handle the initialization of processes correctly using the `if __name__ == \\"__main__\\"` clause. - Consider edge cases like empty lists.","solution":"from typing import List import multiprocessing def compute_squares(numbers: List[int]) -> int: Helper function to compute the sum of squares of a list of numbers. return sum(number * number for number in numbers) def parallel_compute_square_sum(numbers: List[int], num_workers: int) -> int: Computes the sum of squares of a list of numbers using parallel processing. Args: - numbers: List[int]: The list of numbers to process. - num_workers: int: The number of worker processes to use. Returns: - int: The sum of the squares of the numbers. if not numbers: return 0 with multiprocessing.Pool(processes=num_workers) as pool: # Split the list into chunks for each worker chunk_size = len(numbers) // num_workers chunks = [numbers[i * chunk_size:(i + 1) * chunk_size] for i in range(num_workers)] # Include the remaining items in the last chunk if len(numbers) % num_workers != 0: chunks.append(numbers[num_workers * chunk_size:]) # Calculate the sum of squares of each chunk in parallel results = pool.map(compute_squares, chunks) # Combine the results from each worker return sum(results)"},{"question":"# Question: Implement a Secure User Input Prompt using `termios` You are required to implement a function that securely prompts the user for a username and password in a Unix-like terminal. The function should turn off the terminal’s echoing during password input to ensure that the password is not displayed on the screen. Function Signature ```python def secure_input_prompt(prompt_username: str, prompt_password: str) -> tuple: Prompts the user for a username and password securely. Parameters: prompt_username (str): The prompt message for the username. prompt_password (str): The prompt message for the password. Returns: tuple: A tuple containing the username and password. ``` Requirements 1. The function should take two arguments, `prompt_username` and `prompt_password`, which are strings representing the prompt messages for the username and password, respectively. 2. The function should handle tty attributes correctly to ensure that: - The username is prompted and entered normally with echo enabled. - The password is prompted with echo disabled so that the input is not visible on the screen. 3. The original tty settings must be restored after the password input is completed, regardless of whether the function execution is successful. 4. Handle any exceptions that may occur during setting or restoring tty attributes. Example Usage ```python result = secure_input_prompt(\\"Enter your username: \\", \\"Enter your password: \\") print(\\"Username and password captured successfully:\\", result) ``` Constraints - Your solution should only use standard Python libraries. - Assume the environment is a Unix-like operating system that supports the `termios` module. Performance Requirements - The solution must handle prompt display and user input efficiently without noticeable delay. Example Implementation Skeleton Here is a starting point for your function, using `termios` to manage tty attributes: ```python import sys import termios def secure_input_prompt(prompt_username: str, prompt_password: str) -> tuple: # Prompt for username username = input(prompt_username) # File descriptor for standard input fd = sys.stdin.fileno() # Save the original tty attributes old_attributes = termios.tcgetattr(fd) new_attributes = termios.tcgetattr(fd) # Turn off echoing new_attributes[3] &= ~termios.ECHO # lflag try: # Set new tty attributes termios.tcsetattr(fd, termios.TCSADRAIN, new_attributes) # Prompt for password password = input(prompt_password) finally: # Restore the original tty attributes termios.tcsetattr(fd, termios.TCSADRAIN, old_attributes) return (username, password) # Example usage if __name__ == \\"__main__\\": result = secure_input_prompt(\\"Enter your username: \\", \\"Enter your password: \\") print(\\"Username and password captured successfully:\\", result) ``` Implement and test your solution to ensure it meets all the specified requirements.","solution":"import sys import termios def secure_input_prompt(prompt_username: str, prompt_password: str) -> tuple: Prompts the user for a username and password securely. Parameters: prompt_username (str): The prompt message for the username. prompt_password (str): The prompt message for the password. Returns: tuple: A tuple containing the username and password. try: # Prompt for username username = input(prompt_username) # File descriptor for standard input fd = sys.stdin.fileno() # Save the original tty attributes old_attributes = termios.tcgetattr(fd) new_attributes = termios.tcgetattr(fd) # Turn off echoing new_attributes[3] &= ~termios.ECHO # lflag try: # Set new tty attributes termios.tcsetattr(fd, termios.TCSADRAIN, new_attributes) # Prompt for password password = input(prompt_password) finally: # Restore the original tty attributes termios.tcsetattr(fd, termios.TCSADRAIN, old_attributes) return (username, password) except Exception as e: print(f\\"An error occurred: {e}\\") return (\'\', \'\') # Example usage if __name__ == \\"__main__\\": result = secure_input_prompt(\\"Enter your username: \\", \\"Enter your password: \\") print(\\"Username and password captured successfully:\\", result)"},{"question":"# Question: Unicode String Manipulator Your task is to write a Python function that accepts a string and performs several manipulations to demonstrate the understanding of Unicode objects and character properties with conversions. Function Signature: ```python def unicode_string_manipulator(input_string: str) -> dict: pass ``` Input: - `input_string` (str): A UTF-8 encoded string that can contain characters from any Unicode range. Output: - A dictionary with the following keys: - `\'original\'`: the original input string. - `\'ascii_upper\'`: input string converted to upper case ASCII characters if possible, otherwise leave non-ASCII characters unchanged. - `\'utf16_encoded\'`: UTF-16 encoded version of the input string. - `\'is_identifier\'`: boolean indicating whether the input string is a valid identifier. - `\'ucs4_chars\'`: List of Unicode code points (UCS-4) for each character in the string. Constraints: - The input string can be empty or contain up to 1000 Unicode characters. - Properly handle any non-ASCII and non-printable characters during the operations. Explanation: 1. **Upper case ASCII conversion**: - Convert the string to uppercase, but only if the character is an ASCII character (code point < 128). 2. **UTF-16 Encoding**: - Encode the string to its UTF-16 byte representation. 3. **Identifier Check**: - Check if the string is a valid Python identifier. 4. **UCS-4 Conversion**: - List out the Unicode code points (UCS-4) for each character in the input string. Example: ```python input_string = \\"hello_世界\\" output = unicode_string_manipulator(input_string) print(output) # Expected output: # { # \'original\': \'hello_世界\', # \'ascii_upper\': \'HELLO_世界\', # \'utf16_encoded\': <UTF-16 encoded bytes>, # \'is_identifier\': True, # \'ucs4_chars\': [104, 101, 108, 108, 111, 95, 19990, 30028] # } ```","solution":"def unicode_string_manipulator(input_string: str) -> dict: original = input_string # Convert to upper case ASCII where applicable ascii_upper = \'\'.join(ch.upper() if ord(ch) < 128 else ch for ch in input_string) # Encode as UTF-16 utf16_encoded = input_string.encode(\'utf-16\') # Check if it\'s a valid identifier is_identifier = input_string.isidentifier() # List of UCS-4 code points ucs4_chars = [ord(ch) for ch in input_string] return { \'original\': original, \'ascii_upper\': ascii_upper, \'utf16_encoded\': utf16_encoded, \'is_identifier\': is_identifier, \'ucs4_chars\': ucs4_chars }"},{"question":"# Question: Implement a Custom Autograd Function Objective Implement a custom autograd function in PyTorch that computes the following operation during the forward pass: [ f(x) = (x^2 + 2x) ] and provides the correct gradients during the backward pass. This function should demonstrate an understanding of how to save tensors for backward and how custom gradients are computed. Requirements 1. **Custom Function Class**: - Define a custom autograd function by inheriting from `torch.autograd.Function`. - Implement the `forward` and `backward` methods. 2. **Forward Method**: - Compute the function ( f(x) = x^2 + 2x ). - Save any tensor(s) necessary for the backward pass using `ctx.save_for_backward`. 3. **Backward Method**: - Retrieve the saved tensors. - Compute the gradient of the input tensor, which is the derivative of ( f(x) ): [ frac{df}{dx} = 2x + 2 ] - Return the computed gradient. Function Use - Test your custom function to ensure it works correctly by creating a tensor, passing it through the function during the forward pass, then performing a backward pass to compute the gradient. Input and Output - **Input**: A 1-dimensional tensor `x` with `requires_grad=True`. - **Output**: A 1-dimensional tensor `y` as the result of ( f(x) ); Constraints - Do not use any in-place operations. - Ensure correctness by comparing the custom gradient with the PyTorch automatic gradient. Code Template ```python import torch class MyFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Implement the forward pass result = None return result @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor(s) # Compute the gradient grad_input = None return grad_input # Test the custom function x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = MyFunction.apply(x) y.sum().backward() print(\\"Input:\\", x) print(\\"Output:\\", y) print(\\"Gradient:\\", x.grad) ``` Sample Test Input: ```python x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) ``` Expected Output: ```python Output: tensor([ 3., 8., 15.], grad_fn=<MyFunctionBackward>) Gradient: tensor([ 4., 6., 8.]) ``` Note Ensure to handle `.tensor` methods and practice proper gradient checking for validation.","solution":"import torch class MyFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Compute the forward pass, f(x) = x^2 + 2x result = input ** 2 + 2 * input # Save the input tensor for the backward pass ctx.save_for_backward(input) return result @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor input, = ctx.saved_tensors # Compute the gradient d(f(x))/d(x) = 2x + 2 grad_input = grad_output * (2 * input + 2) return grad_input # Test the custom function x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = MyFunction.apply(x) y.sum().backward() print(\\"Input:\\", x) print(\\"Output:\\", y) print(\\"Gradient:\\", x.grad)"},{"question":"**Objective**: Demonstrate understanding of the `errno` module and error handling in Python. Problem Statement You are required to implement a function that takes an error code as input and returns a dictionary containing two keys: 1. `\\"error_string\\"`: The string name of the error code, as found in the `errno.errorcode` dictionary. 2. `\\"error_message\\"`: The human-readable error message corresponding to the error code, using the `os.strerror` function. Function Signature ```python def get_error_info(error_code: int) -> dict: pass ``` Input - `error_code` (int): A valid error code from the `errno` module. Output - (dict): A dictionary with the following format: ```python { \\"error_string\\": <string_name>, \\"error_message\\": <human_readable_message> } ``` Constraints and Considerations - Use the `errno.errorcode` dictionary to get the string name of the error code. - Use the `os.strerror` function to get the human-readable error message. - Handle the case where the provided error code is not valid by raising a `ValueError` with the message `\\"Invalid error code\\"`. - Ensure the function works efficiently for any valid error code in the `errno` module. Example ```python import errno # Example usage error_code = errno.ENOENT expected_output = { \\"error_string\\": \\"ENOENT\\", \\"error_message\\": \\"No such file or directory\\" } assert get_error_info(error_code) == expected_output error_code = 9999 # Example of an invalid error code # This should raise a ValueError try: get_error_info(error_code) except ValueError as e: assert str(e) == \\"Invalid error code\\" ``` Your implementation will be evaluated based on the correctness and efficiency of your code.","solution":"import errno import os def get_error_info(error_code: int) -> dict: Returns a dictionary containing the string name of the error code and the human-readable error message. If the error code is not valid, raises a ValueError. Args: error_code (int): A valid error code from the errno module. Returns: dict: A dictionary with \\"error_string\\" and \\"error_message\\" keys. if error_code in errno.errorcode: return { \\"error_string\\": errno.errorcode[error_code], \\"error_message\\": os.strerror(error_code) } else: raise ValueError(\\"Invalid error code\\")"},{"question":"# Priority Task Manager using Queue Module In this coding exercise, you will create a task manager system that uses the `PriorityQueue` class from the `queue` module. The task manager should handle tasks based on their priority levels, where each task is assigned a priority number. Lower numbers indicate higher priority. Your implementation should also handle task completion and ensure that tasks are processed in the correct order. Requirements: 1. **Task Class**: - Implement a `Task` class with the following attributes: - `name` (string): The name of the task. - `priority` (int): The priority of the task. - The `Task` objects should be comparable based on priority, so they can be stored in the `PriorityQueue`. 2. **TaskManager Class**: - Implement a `TaskManager` class to manage tasks using the `PriorityQueue` class. - The class should have the following methods: - `__init__(self)`: Initializes the `TaskManager` with an empty `PriorityQueue`. - `add_task(self, task)`: Adds a `Task` object to the priority queue. - `process_task(self)`: Processes and returns the highest priority task. If no tasks are available, it should raise the `queue.Empty` exception. - `get_task_count(self)`: Returns the current number of tasks in the queue. - `is_empty(self)`: Returns `True` if the task queue is empty, otherwise returns `False`. Constraints: - You may assume that priority numbers are unique. - You must handle the `queue.Empty` exception properly when trying to process a task from an empty queue. Example Usage: ```python from queue import PriorityQueue import queue class Task: def __init__(self, name, priority): self.name = name self.priority = priority def __lt__(self, other): return self.priority < other.priority class TaskManager: def __init__(self): self.tasks = PriorityQueue() def add_task(self, task): self.tasks.put(task) def process_task(self): if self.tasks.empty(): raise queue.Empty(\\"No tasks available to process\\") return self.tasks.get() def get_task_count(self): return self.tasks.qsize() def is_empty(self): return self.tasks.empty() # Example usage: task_manager = TaskManager() task_manager.add_task(Task(\\"Task1\\", 2)) task_manager.add_task(Task(\\"Task2\\", 1)) task_manager.add_task(Task(\\"Task3\\", 3)) print(task_manager.process_task().name) # Output: Task2 (highest priority) print(task_manager.get_task_count()) # Output: 2 print(task_manager.is_empty()) # Output: False ``` # Note: You must write code to implement the `Task` and `TaskManager` classes and ensure that the provided example usage works correctly.","solution":"from queue import PriorityQueue import queue class Task: def __init__(self, name, priority): self.name = name self.priority = priority def __lt__(self, other): return self.priority < other.priority class TaskManager: def __init__(self): self.tasks = PriorityQueue() def add_task(self, task): self.tasks.put(task) def process_task(self): if self.tasks.empty(): raise queue.Empty(\\"No tasks available to process\\") return self.tasks.get() def get_task_count(self): return self.tasks.qsize() def is_empty(self): return self.tasks.empty()"},{"question":"**Objective:** Assess your understanding of Python debugging and profiling tools by solving a performance issue in a given piece of code. # Problem Statement You are given an inefficient function called `find_prime_numbers(n)` that is intended to return all prime numbers up to `n`. However, it runs very slowly for large values of `n`. **Required Tasks:** 1. **Identify the Performance Bottleneck:** - Use the `cProfile` module to profile the provided function and identify where the most time is being spent. 2. **Optimize the Function:** - Rewrite the `find_prime_numbers(n)` function to improve its performance. 3. **Verify the Optimization:** - Use `timeit` to measure the execution time of both the original and the optimized functions for `n = 10000`. - Ensure that the optimized function produces the correct result and runs faster than the original. 4. **Trace Memory Usage:** - Use the `tracemalloc` module to trace the memory usage of the optimized function. Display the top 5 lines of code where the most memory is allocated. # Input - An integer `n`. # Output - A list of prime numbers up to `n`. - A comparison of execution times between the original and optimized functions. - The top 5 lines of memory allocations for the optimized function. # Constraints - 2 ≤ n ≤ 10000 # Example ```python import cProfile import timeit import tracemalloc def find_prime_numbers(n): primes = [] for num in range(2, n + 1): for i in range(2, num): if (num % i) == 0: break else: primes.append(num) return primes def optimized_find_prime_numbers(n): # Implement the optimized function here pass # Step 1: Profile the original function: cProfile.run(\'find_prime_numbers(1000)\') # Step 2: Optimize the function: # Place the optimized implementation in \'optimized_find_prime_numbers\' # Step 3: Verify the Optimization: original_time = timeit.timeit(\'find_prime_numbers(10000)\', globals=globals(), number=1) optimized_time = timeit.timeit(\'optimized_find_prime_numbers(10000)\', globals=globals(), number=1) print(f\'Original Time: {original_time}\') print(f\'Optimized Time: {optimized_time}\') # Step 4: Trace Memory Usage: tracemalloc.start() optimized_find_prime_numbers(10000) snapshot = tracemalloc.take_snapshot() top_stats = snapshot.statistics(\'lineno\') print(\\"[ Top 5 Memory Allocation Lines: ]\\") for stat in top_stats[:5]: print(stat) ``` Implement the `optimized_find_prime_numbers(n)` function and ensure it meets the performance and correctness criteria.","solution":"def optimized_find_prime_numbers(n): Returns a list of prime numbers up to n using the Sieve of Eratosthenes. if n < 2: return [] sieve = [True] * (n + 1) sieve[0] = sieve[1] = False for start in range(2, int(n**0.5) + 1): if sieve[start]: for multiple in range(start*start, n + 1, start): sieve[multiple] = False return [num for num in range(2, n + 1) if sieve[num]]"},{"question":"# Pandas Nullable Integer Operations You are working with a dataset that includes columns with integer values, some of which are missing. To better handle these integer columns, you aim to use pandas\' `IntegerArray` with `pandas.NA` for missing values. Task: 1. **Create a DataFrame**: - Construct a pandas DataFrame from the following dictionary: ```python data = { \'id\': [1, 2, 3, 4, 5], \'value\': [10, None, 30, None, 50], \'category\': [\'A\', \'A\', \'B\', \'B\', \'C\'] } ``` - Ensure that the `value` column uses the `Int64` dtype to handle nullable integer values. 2. **Perform Operations and Manipulations**: - Increment all `value` elements by 5. - Set the `value` of rows where the `category` is `\'B\'` to `pandas.NA`. - Cast the `value` column to float and return the DataFrame. 3. **Reduction and GroupBy Operations**: - Calculate the sum of the `value` column, ignoring missing values. - Group by the `category` column and calculate the sum of the `value` column for each group, handling potential missing values. 4. **Output Results**: - Print the modified DataFrame after step 2. - Print the result of the sum operation from step 3. - Print the result of the group by operation from step 3. Implementation: Implement these steps within a function `process_nullable_integers` that returns a tuple containing the modified DataFrame and the results of the sum and group by operations. ```python import pandas as pd def process_nullable_integers(data: dict): # Step 1: Create DataFrame with nullable integer column df = pd.DataFrame(data) df[\'value\'] = pd.array(df[\'value\'], dtype=\\"Int64\\") # Step 2: Perform operations df[\'value\'] += 5 df.loc[df[\'category\'] == \'B\', \'value\'] = pd.NA df[\'value\'] = df[\'value\'].astype(float) # Step 3: Reduction and groupby operations total_sum = df[\'value\'].sum() group_sum = df.groupby(\'category\')[\'value\'].sum() return df, total_sum, group_sum # Sample data data = { \'id\': [1, 2, 3, 4, 5], \'value\': [10, None, 30, None, 50], \'category\': [\'A\', \'A\', \'B\', \'B\', \'C\'] } # Execute function df, total_sum, group_sum = process_nullable_integers(data) print(df) print(total_sum) print(group_sum) ``` Expected Output: ``` id value category 0 1 15.0 A 1 2 NaN A 2 3 NaN B 3 4 NaN B 4 5 55.0 C 70.0 category A 15.0 B 0.0 C 55.0 Name: value, dtype: float64 ``` **Note**: Ensure that you understand how to handle `pandas.NA` and integer operations correctly. The provided function should manage nullable integer types efficiently.","solution":"import pandas as pd def process_nullable_integers(data: dict): Process the given data dictionary to handle nullable integers, perform required manipulations and operations, and return results. # Step 1: Create DataFrame with nullable integer column df = pd.DataFrame(data) df[\'value\'] = pd.array(df[\'value\'], dtype=\\"Int64\\") # Step 2: Perform operations df[\'value\'] += 5 df.loc[df[\'category\'] == \'B\', \'value\'] = pd.NA df[\'value\'] = df[\'value\'].astype(float) # Step 3: Reduction and groupby operations total_sum = df[\'value\'].sum() group_sum = df.groupby(\'category\')[\'value\'].sum() return df, total_sum, group_sum # Sample data function input for testing data = { \'id\': [1, 2, 3, 4, 5], \'value\': [10, None, 30, None, 50], \'category\': [\'A\', \'A\', \'B\', \'B\', \'C\'] } # Execute function df, total_sum, group_sum = process_nullable_integers(data) print(df) print(total_sum) print(group_sum)"},{"question":"**Title: Implement a Custom Slicing Function** **Objective:** To assess your understanding of Python slice objects and your ability to handle edge cases with custom slicing logic. **Background:** Slices in Python allow you to extract portions of sequences such as lists, strings, and tuples. This assignment involves creating a custom slicing function that mimics Python\'s slicing behavior using the information provided about slice objects. **Problem Statement:** Write a function `custom_slice(sequence, start, stop, step=None)` that takes in a sequence (list, string, or tuple) and returns a sliced version of it. You are to handle slicing manually by constructing a slice object, unpacking it, and adjusting the indices as described. **Function Signature:** ```python def custom_slice(sequence: Union[List[Any], str, Tuple[Any]], start: int, stop: int, step: Optional[int] = None) -> Union[List[Any], str, Tuple[Any]]: pass ``` **Input:** - `sequence`: A sequence to be sliced. It can be a list, string, or tuple. - `start`: The starting index of the slice. - `stop`: The stopping index of the slice (not inclusive). - `step`: The step of the slice. It is optional and defaults to `None` (or consider it to be 1 if not provided). **Output:** - A sequence (of the same type as input) that is the result of slicing the input sequence from `start` to `stop` with the given `step`. **Constraints:** - Do not use Python\'s built-in slicing syntax (i.e., `sequence[start:stop:step]`) - Handle edge cases where indices are out of the bounds of the sequence. **Example:** ```python assert custom_slice([1, 2, 3, 4, 5], 1, 4) == [2, 3, 4] assert custom_slice(\\"abcdefg\\", 2, 6, 2) == \\"ce\\" assert custom_slice((10, 20, 30, 40), -3, -1) == (20, 30) assert custom_slice([1, 2, 3, 4, 5], 0, 10, 2) == [1, 3, 5] ``` **Performance Considerations:** The function should efficiently handle different types and lengths of sequences, taking care to work within the constraints of typical slice operations. **Additional Notes:** - Implement and utilize a slice object as mentioned in the documentation for creating new slice objects and adjusting indices. - Ensure proper handling of `None` values for `start`, `stop`, and `step` by defining reasonable defaults. - Verify the corrected and adjusted indices are within the bounds of the original sequence length. **Hint:** You may find it helpful to reference the `PySlice_New`, `PySlice_Unpack`, and `PySlice_AdjustIndices` methods discussed in the provided documentation.","solution":"from typing import Union, List, Tuple, Any, Optional def custom_slice(sequence: Union[List[Any], str, Tuple[Any]], start: int, stop: int, step: Optional[int] = None) -> Union[List[Any], str, Tuple[Any]]: # Determine the proper step value step = 1 if step is None else step # Handle negative indices and normalize them with respect to the sequence length seq_len = len(sequence) start = start if start >= 0 else seq_len + start stop = stop if stop >= 0 else seq_len + stop # Adjust start, stop bounds to be within the sequence length start = max(min(start, seq_len), 0) stop = max(min(stop, seq_len), 0) # Initialize the result result = [] # Perform the custom slicing logic if step > 0: i = start while i < stop: result.append(sequence[i]) i += step else: i = start while i > stop: result.append(sequence[i]) i += step # Return the result in the same type as the input sequence if isinstance(sequence, list): return result elif isinstance(sequence, str): return \'\'.join(result) elif isinstance(sequence, tuple): return tuple(result)"},{"question":"# Audio Processing Challenge Using `audioop` With the `audioop` module, you can manipulate raw audio data for different purposes such as encoding, analysis, and transformation of audio signals. This question requires you to implement an audio processing function that combines several operations provided by this module. Task Write a Python function `process_audio(input_fragment: bytes, sample_width: int) -> bytes` that performs the following operations on an input audio fragment: 1. **Conversion:** Convert the input audio fragment from its current sample width to 16-bit samples using `audioop.lin2lin()`. 2. **Volume Adjustment:** Increase the volume of the converted audio fragment by a factor of 1.5 using `audioop.mul()`. 3. **Reverse Audio:** Reverse the modified audio fragment using `audioop.reverse()`. 4. **Bias Adjustment:** Add a bias of 1000 to each sample in the reversed audio using `audioop.bias()`. 5. **Convert Back:** Convert the final processed audio fragment back to its original sample width using `audioop.lin2lin()`. Function Signature ```python def process_audio(input_fragment: bytes, sample_width: int) -> bytes: pass ``` Input Format - `input_fragment` (bytes): The raw audio data fragment to be processed. - `sample_width` (int): The width of each sample in the input audio fragment, which can be 1 (8-bit), 2 (16-bit), 3 (24-bit), or 4 (32-bit). Output Format - (bytes): The processed audio data fragment with the same length and sample width as the input fragment. Constraints and Specifications - The input and output audio fragments are in raw bytes format. - Ensure the conversion steps maintain the integrity of the audio data. - Handle potential overflow issues where applicable. - Assume the input fragment is non-empty and has a valid sample width. Example Usage ```python input_fragment = b\'x01x02x03x04x05x06x07x08\' sample_width = 2 # Example call to the function output_fragment = process_audio(input_fragment, sample_width) ``` In this example, `output_fragment` should contain the processed audio data according to the steps described above. Notes 1. Use appropriate functions from the `audioop` module to perform the required operations. 2. Ensure the final processed audio fragment is converted back to its original sample width. 3. Test the function with different sample widths (1, 2, 3, and 4 bytes). Good luck! Your implementation will be evaluated based on correctness, efficiency, and adherence to the requirements.","solution":"import audioop def process_audio(input_fragment: bytes, sample_width: int) -> bytes: Processes the audio data fragment by performing several operations such as conversion, volume adjustment, reversing, bias adjustment, and conversion back to original sample width. # Convert the input audio fragment to 16-bit samples fragment_16bit = audioop.lin2lin(input_fragment, sample_width, 2) # Increase the volume of the audio fragment by a factor of 1.5 fragment_louder = audioop.mul(fragment_16bit, 2, 1.5) # Reverse the modified audio fragment fragment_reversed = audioop.reverse(fragment_louder, 2) # Add a bias of 1000 to each sample in the reversed audio fragment_biased = audioop.bias(fragment_reversed, 2, 1000) # Convert the final processed audio fragment back to its original sample width output_fragment = audioop.lin2lin(fragment_biased, 2, sample_width) return output_fragment"},{"question":"# Python Regular Expressions Assessment **Objective:** Implement a series of functions utilizing Python\'s `re` module to manipulate and analyze strings based on specified patterns. **Problem 1: Extracting Dates** Write a function `extract_dates(text)` that takes a string `text` and returns a list of all dates found in the format `DD/MM/YYYY`. * **Input:** - `text` (str): A string containing multiple dates of format `DD/MM/YYYY`. * **Output:** - List of strings: A list containing all the dates in the format `DD/MM/YYYY` found in the input text. * **Example:** ```python text = \\"Today\'s date is 14/09/2023, yesterday was 13/09/2023, and tomorrow will be 15/09/2023.\\" extract_dates(text) ``` *Output:* `[\'14/09/2023\', \'13/09/2023\', \'15/09/2023\']` * **Constraints:** - Use regex to extract the dates. - Assume date format is always `DD/MM/YYYY`. **Problem 2: Validating Email Addresses** Write a function `validate_email(email)` that validates whether an input string `email` is a valid email address. * **Input:** - `email` (str): A string representing an email address. * **Output:** - Boolean: `True` if the email is valid, `False` otherwise. * **Example:** ```python validate_email(\'user@example.com\') # Output: True validate_email(\'userexample.com\') # Output: False ``` * **Constraints:** - Use regex to validate the email. A valid email address contains a local part, an `@` symbol, and a domain part (e.g., `local-part@domain`). **Problem 3: Parsing Log Entries** Write a function `parse_logs(logs)` that parses log entries and returns a list of dictionaries. Each dictionary should contain the log level and the log message. * **Input:** - `logs` (str): A string containing multiple log entries. Each log entry is on a new line and follows the format `[LOG_LEVEL] MESSAGE`. * **Output:** - List of dictionaries: Each dictionary should have two keys: `\\"level\\"` and `\\"message\\"`. * **Example:** ```python logs = \\"[INFO] This is an info messagen[ERROR] This is an error messagen[WARNING] This is a warning message\\" parse_logs(logs) ``` Output: ```python [ {\\"level\\": \\"INFO\\", \\"message\\": \\"This is an info message\\"}, {\\"level\\": \\"ERROR\\", \\"message\\": \\"This is an error message\\"}, {\\"level\\": \\"WARNING\\", \\"message\\": \\"This is a warning message\\"} ] ``` * **Constraints:** - Use regex to parse the logs. - Each log entry is only separated by a newline character. # Function Signatures ```python def extract_dates(text: str) -> list: pass def validate_email(email: str) -> bool: pass def parse_logs(logs: str) -> list: pass ``` # Requirements - Implement all three functions. - Ensure that your regex patterns are accurate and cover the expected formats. - Thoroughly test your functions with various inputs to ensure correctness.","solution":"import re def extract_dates(text): Extracts all dates in the format DD/MM/YYYY from the given text. Args: text (str): The string containing dates. Returns: List of strings: All dates found in the format DD/MM/YYYY. pattern = r\'bd{2}/d{2}/d{4}b\' return re.findall(pattern, text) def validate_email(email): Validates if the given string is a valid email address. Args: email (str): The string representing an email address. Returns: Boolean: True if the email is valid, False otherwise. pattern = r\\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\\" return bool(re.match(pattern, email)) def parse_logs(logs): Parses log entries and returns a list of dictionaries. Args: logs (str): The string containing log entries. Returns: List of dictionaries: Each dictionary contains log level and message. pattern = r\'[(w+)]s+(.*)\' matches = re.findall(pattern, logs) return [{\\"level\\": level, \\"message\\": message} for level, message in matches]"},{"question":"# Problem: Statistical Distribution Calculations In this task, you are required to implement a few helper functions that utilize the `math` module functions to perform calculations related to statistical distributions. You will need to implement the following functions: 1. **normal_cdf(x)** 2. **inverse_normal_cdf(p, tolerance=1e-5)** 3. **binomial_pmf(n, k, p)** 4. **binomial_cdf(n, k, p)** 1. normal_cdf(x) This function calculates the Cumulative Distribution Function (CDF) for a standard normal distribution at a given point `x`. **Input:** - `x` (float): The point at which CDF is evaluated. **Output:** - (float): The CDF value. 2. inverse_normal_cdf(p, tolerance=1e-5) This function calculates the inverse of the Cumulative Distribution Function (CDF) for a standard normal distribution. It finds the value of `x` such that the CDF is equal to the given probability `p`. **Input:** - `p` (float): The probability for which the inverse CDF is calculated. Must be between 0 and 1. - `tolerance` (float, optional): The tolerance level for convergence (default is 1e-5). **Output:** - (float): The value of `x`. 3. binomial_pmf(n, k, p) This function calculates the Probability Mass Function (PMF) for a binomial distribution. **Input:** - `n` (int): Number of trials. - `k` (int): Number of successes. - `p` (float): Probability of success on each trial. **Output:** - (float): The PMF value. 4. binomial_cdf(n, k, p) This function calculates the Cumulative Distribution Function (CDF) for a binomial distribution. **Input:** - `n` (int): Number of trials. - `k` (int): Number of successes. - `p` (float): Probability of success on each trial. **Output:** - (float): The CDF value. # Implementation Details - Use `math.erf` for calculating the normal CDF. - Use a binary search approach for the inverse normal CDF. - Use `math.comb` for calculating binomial coefficients in binomial PMF and CDF. - The functions should handle edge cases and input constraints as specified in the `math` module documentation. # Example ```python import math def normal_cdf(x): return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0 def inverse_normal_cdf(p, tolerance=1e-5): if p <= 0 or p >= 1: raise ValueError(\\"p must be between 0 and 1\\") low_z = -10.0 high_z = 10.0 while high_z - low_z > tolerance: mid_z = (low_z + high_z) / 2 mid_p = normal_cdf(mid_z) if mid_p < p: low_z = mid_z else: high_z = mid_z return mid_z def binomial_pmf(n, k, p): if k < 0 or k > n: return 0.0 combination = math.comb(n, k) return combination * (p ** k) * ((1 - p) ** (n - k)) def binomial_cdf(n, k, p): cum_prob = 0.0 for i in range(0, k + 1): cum_prob += binomial_pmf(n, i, p) return cum_prob # Example Usage print(normal_cdf(0)) # Output: 0.5 print(inverse_normal_cdf(0.95)) # Output: approximately 1.64485 print(binomial_pmf(10, 3, 0.5)) # Output: 0.117 print(binomial_cdf(10, 3, 0.5)) # Output: 0.171875 ``` # Notes: - Ensure all mathematical calculations are precise and consider edge cases. - Your functions should be able to handle large input sizes efficiently.","solution":"import math def normal_cdf(x): Calculates the Cumulative Distribution Function (CDF) for a standard normal distribution at point x. return (1 + math.erf(x / math.sqrt(2))) / 2 def inverse_normal_cdf(p, tolerance=1e-5): Calculates the inverse CDF (quantile function) for a standard normal distribution. Uses a binary search to find the value of x such that the CDF is equal to p. if not 0 < p < 1: raise ValueError(\\"p must be between 0 and 1\\") low_z = -10.0 high_z = 10.0 while high_z - low_z > tolerance: mid_z = (low_z + high_z) / 2 mid_p = normal_cdf(mid_z) if mid_p < p: low_z = mid_z else: high_z = mid_z return (low_z + high_z) / 2 def binomial_pmf(n, k, p): Calculates the Probability Mass Function (PMF) for a binomial distribution. if k < 0 or k > n: return 0.0 combination = math.comb(n, k) return combination * (p ** k) * ((1 - p) ** (n - k)) def binomial_cdf(n, k, p): Calculates the Cumulative Distribution Function (CDF) for a binomial distribution. return sum(binomial_pmf(n, i, p) for i in range(k + 1))"},{"question":"# PyTorch MPS Backend Coding Assessment Objective: Demonstrate your understanding of PyTorch\'s `mps` backend by implementing a function that performs tensor operations on a MacOS device using the Metal programming framework. The function should handle scenarios where the MPS device is not available. Problem Statement: You are given the task to implement the function `perform_mps_operations` that accepts a PyTorch model and a tensor. The function should: 1. Check if the `mps` backend is available. 2. If available, move the tensor and the model to the `mps` device and perform any tensor operation (e.g., element-wise multiplication by 2) on the tensor. 3. Return the resulting tensor. 4. If the `mps` backend is not available, return an exception message explaining why the `mps` backend is not available. Function Signature: ```python def perform_mps_operations(model: torch.nn.Module, input_tensor: torch.Tensor) -> torch.Tensor: ``` Input: - `model`: A PyTorch model (subclass of `torch.nn.Module`). - `input_tensor`: A PyTorch tensor to be moved to the `mps` device. Output: - A PyTorch tensor with the result of the operation if the `mps` backend is available. - An exception message as a string if the `mps` backend is not available. Constraints: - Ensure the function handles both scenarios where the `mps` backend is available and not available gracefully. - Use the provided documentation\'s code snippets as a reference for checking device availability and performing operations. # Example: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(5, 1) def forward(self, x): return self.linear(x) model = SimpleModel() tensor = torch.ones(5) # Assuming \'perform_mps_operations\' is properly implemented result = perform_mps_operations(model, tensor) print(result) ``` # Notes: - You must handle both model and tensor movements to the `mps` device within the function. - Ensure the tensor operation completes without errors when the `mps` device is available and return an appropriate tensor.","solution":"import torch def perform_mps_operations(model: torch.nn.Module, input_tensor: torch.Tensor): Perform tensor operations using the MPS (Metal Performance Shaders) backend if available. Parameters: model (torch.nn.Module): The PyTorch model to move to the MPS device. input_tensor (torch.Tensor): The input tensor to move to the MPS device. Returns: torch.Tensor: The resulting tensor after operations, or a string message if MPS is not available. if not torch.backends.mps.is_available(): return \\"MPS backend is not available.\\" device = torch.device(\\"mps\\") model = model.to(device) input_tensor = input_tensor.to(device) # Perform element-wise multiplication by 2 as an example operation result_tensor = input_tensor * 2 return result_tensor"},{"question":"# Python Coding Assessment Objective You are required to design a robust logging system for a hypothetical application using Python\'s `logging` package. Your task is to implement a function that configures and demonstrates the logging system\'s capabilities, including logger creation, different logging levels, custom formatting, and handler setup. Problem Statement Write a function named `configure_logging_system` that performs the following tasks: 1. **Create a logger**: - Create a logger named \\"application\\" using `logging.getLogger(\\"application\\")`. 2. **Set logger level**: - Set the logger\'s level to \\"DEBUG\\". 3. **Create a console handler**: - Create a `StreamHandler` named `console_handler` that logs messages to the console. 4. **Create a file handler**: - Create a `FileHandler` named `file_handler` that logs messages to a file named \\"app.log\\". Open the file in write mode. 5. **Set handler levels**: - Set the `console_handler`\'s level to \\"WARNING\\". - Set the `file_handler`\'s level to \\"DEBUG\\". 6. **Define a custom formatter**: - Create a custom formatter that includes the timestamp, logger name, logging level, and the log message in the format `\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'`. - Apply this formatter to both `console_handler` and `file_handler`. 7. **Add handlers to the logger**: - Add both `console_handler` and `file_handler` to the \\"application\\" logger. 8. **Log messages**: - Log messages at each level (DEBUG, INFO, WARNING, ERROR, CRITICAL) using the configured logger. - Include a message in each log that indicates its level (e.g., \\"This is a debug message\\"). Expected Function Signature ```python import logging def configure_logging_system() -> None: pass ``` Example When you call the function `configure_logging_system()`, it should configure the logging system as described and log the following messages: ``` 2023-08-01 12:00:00,000 - application - WARNING - This is a warning message 2023-08-01 12:00:00,000 - application - ERROR - This is an error message 2023-08-01 12:00:00,000 - application - CRITICAL - This is a critical message ``` The above messages will appear in the console, while all messages (DEBUG, INFO, WARNING, ERROR, CRITICAL) should be written to the \\"app.log\\" file with the appropriate formatting. Constraints - Assume the required modules (`logging`) are already imported. - Ensure no duplicate messages appear when handlers are used. - Properly handle any exceptions that might occur during logging (e.g., file I/O errors). - Use the `basicConfig` function appropriately if necessary. Additional Requirements - Document your function appropriately, explaining the purpose and functionality of each step. - Ensure your function adheres to best logging practices and demonstrates a clear understanding of the Python `logging` package.","solution":"import logging def configure_logging_system() -> None: Configures the logging system for an application to log messages to both the console and to a file with appropriate logging levels and formatting. # Create a logger named \\"application\\" logger = logging.getLogger(\\"application\\") logger.setLevel(logging.DEBUG) # Create a console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.WARNING) # Create a file handler file_handler = logging.FileHandler(\\"app.log\\", mode=\'w\') file_handler.setLevel(logging.DEBUG) # Define a custom formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) # Log messages at each level logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\")"},{"question":"**Problem Statement:** In this task, you are required to implement a Python function that accepts a directory path and performs the following operations: 1. Recursively compresses all the `.txt` files in the directory using bzip2 compression format. 2. Stores the compressed files in a specified output directory, preserving the original directory structure. Your implementation should leverage the `bz2` module\'s capabilities for file compression. Ensure to handle errors gracefully, such as directory not found, permission issues, etc. **Function Signature:** ```python import os import bz2 def compress_txt_files(input_dir: str, output_dir: str) -> None: pass ``` **Input:** - `input_dir`: A string representing the path to the directory containing the files to be compressed. - `output_dir`: A string representing the path to the directory where the compressed files will be stored. **Output:** - The function doesn\'t return anything. It should create compressed `.bz2` files in the `output_dir`. **Constraints:** - You may assume the input directory contains a significant number of files and nested folders. - `input_dir` and `output_dir` are both valid path strings. - Handle edge cases such as the same input and output directories. **Example:** ```python # Input directory structure: # /input_dir/ # file1.txt # file2.txt # subdir/ # file3.txt compress_txt_files(\'/input_dir\', \'/output_dir\') # Output directory structure: # /output_dir/ # file1.txt.bz2 # file2.txt.bz2 # subdir/ # file3.txt.bz2 ``` **Notes:** 1. Use the `bz2` module for compression. 2. Preserve the sub-directory structure in the output directory. 3. The function should handle errors such as file read/write issues. 4. Provide meaningful log messages for each stage of the operation for debugging and verification purposes.","solution":"import os import bz2 def compress_txt_files(input_dir: str, output_dir: str) -> None: Recursively compresses all .txt files in the input directory and stores them in the output directory, preserving the original directory structure. if not os.path.exists(input_dir): raise FileNotFoundError(f\\"The input directory \'{input_dir}\' does not exist.\\") if not os.path.exists(output_dir): os.makedirs(output_dir) for root, dirs, files in os.walk(input_dir): # Create corresponding directories in the output_dir rel_path = os.path.relpath(root, input_dir) out_dir = os.path.join(output_dir, rel_path) if not os.path.exists(out_dir): os.makedirs(out_dir) for file in files: if file.endswith(\'.txt\'): input_file_path = os.path.join(root, file) output_file_path = os.path.join(out_dir, file + \'.bz2\') with open(input_file_path, \'rb\') as input_file, bz2.open(output_file_path, \'wb\') as output_file: output_file.write(input_file.read()) print(f\\"Compressed {input_file_path} to {output_file_path}\\")"},{"question":"You are given a set of functions that operate on module imports in Python. Your task is to implement a function `custom_import_manager` that provides a custom mechanism for importing, reloading, and managing Python modules. The function `custom_import_manager` should support the following operations based on the given parameters: 1. **Import a Module**: - **Function Signature:** `def import_module(module_name: str) -> Any` - **Inputs**: A string `module_name` representing the name of the module to be imported. - **Outputs**: Returns the imported module or raises an `ImportError` if the module cannot be imported. - **Constraints**: Only import modules using absolute imports. 2. **Reload a Module**: - **Function Signature:** `def reload_module(module_name: str) -> Any` - **Inputs**: A string `module_name` representing the name of the module to be reloaded. - **Outputs**: Returns the reloaded module or raises an `ImportError` if the module cannot be reloaded. 3. **Check if a Module is Imported**: - **Function Signature:** `def is_module_imported(module_name: str) -> bool` - **Inputs**: A string `module_name` representing the name of the module to check. - **Outputs**: Returns `True` if the module is already imported, otherwise `False`. # Implementation Write a class `CustomImportManager` that includes the methods `import_module`, `reload_module`, and `is_module_imported`. Use the functions from the provided documentation to perform these tasks. # Example ```python import sys class CustomImportManager: @staticmethod def import_module(module_name: str) -> Any: pass @staticmethod def reload_module(module_name: str) -> Any: pass @staticmethod def is_module_imported(module_name: str) -> bool: pass # Example usage: manager = CustomImportManager() numpy_module = manager.import_module(\\"numpy\\") print(numpy_module.__name__) # Expected: numpy reloaded_numpy = manager.reload_module(\\"numpy\\") print(reloaded_numpy.__name__) # Expected: numpy is_numpy_imported = manager.is_module_imported(\\"numpy\\") print(is_numpy_imported) # Expected: True ``` # Constraints - All module name inputs will be valid strings. - Assume the modules needed for the imports will be available in the environment the code is executed. - You must implement these methods using only the functions described in the provided documentation. # Notes - Consider edge cases such as attempting to reload a module that has not been imported yet or checking the import status of an unavailable module. - Your implementation should handle and raise appropriate exceptions as needed.","solution":"import importlib import sys from types import ModuleType from typing import Any class CustomImportManager: @staticmethod def import_module(module_name: str) -> Any: try: module = importlib.import_module(module_name) return module except ImportError as e: raise ImportError(f\\"Cannot import module: {module_name}\\") from e @staticmethod def reload_module(module_name: str) -> Any: if module_name not in sys.modules: raise ImportError(f\\"Module {module_name} is not imported and cannot be reloaded.\\") try: module = importlib.reload(sys.modules[module_name]) return module except ImportError as e: raise ImportError(f\\"Cannot reload module: {module_name}\\") from e @staticmethod def is_module_imported(module_name: str) -> bool: return module_name in sys.modules"},{"question":"Advanced Coding Assessment: Managing Weak References # Objective This task is designed to assess your understanding of weak references in Python and how they interact with garbage collection. # Problem Statement You are required to develop a class `CachedObjectManager` that provides a mechanism to cache objects effectively using weak references. Your class should allow the registration of objects and their retrieval by unique identifiers, ensuring that objects are not kept alive solely by being in the cache. # Requirements 1. Implement the class `CachedObjectManager` with the following methods: - `register(self, obj: object) -> int`: Registers an object and returns a unique identifier for it. - `retrieve(self, identifier: int) -> object`: Retrieves the object by its unique identifier. If the object has been garbage collected, return `None`. - `cleanup(self) -> None`: Clean up all the references to objects that have already been collected. 2. Use appropriate weak references to ensure that objects can be garbage collected when no strong references remain. 3. The `register` method should maintain a mapping between the unique identifier and a weak reference to the object. 4. Ensure that your implementation handles the scenario where `retrieve` is called after the object has been garbage collected, correctly returning `None`. # Input and Output - The `register` method takes an object as input and returns an integer identifier. - The `retrieve` method takes an integer identifier as input and returns the corresponding object or `None` if the object has been garbage collected. # Constraints 1. The unique identifiers returned by `register` should be integers. 2. Objects should only be kept alive by strong references outside the cache. 3. Implement necessary error handling to ensure the code handles scenarios like requesting an invalid identifier gracefully. # Example Usage ```python import weakref class CachedObjectManager: def __init__(self): self._cache = weakref.WeakValueDictionary() self._next_id = 0 def register(self, obj: object) -> int: Register an object and return a unique identifier. identifier = self._next_id self._next_id += 1 self._cache[identifier] = obj return identifier def retrieve(self, identifier: int) -> object: Retrieve the object by its unique identifier. return self._cache.get(identifier) def cleanup(self) -> None: Clean up all references to objects that have already been collected. # No action needed since WeakValueDictionary automatically handles cleanup pass # Example Usage manager = CachedObjectManager() obj1 = SomeObject() obj_id = manager.register(obj1) retrieved_obj = manager.retrieve(obj_id) assert retrieved_obj is obj1 # True # After obj1 is deleted del obj1 retrieved_obj = manager.retrieve(obj_id) assert retrieved_obj is None # True ``` In this example, `SomeObject` will be garbage collected when the last strong reference to it is deleted, showing the expected behavior using `WeakValueDictionary`. Ensure to follow these guidelines and test thoroughly. Good luck!","solution":"import weakref class CachedObjectManager: def __init__(self): self._cache = weakref.WeakValueDictionary() self._next_id = 0 def register(self, obj: object) -> int: Register an object and return a unique identifier. identifier = self._next_id self._next_id += 1 self._cache[identifier] = obj return identifier def retrieve(self, identifier: int) -> object: Retrieve the object by its unique identifier. return self._cache.get(identifier) def cleanup(self) -> None: Clean up all references to objects that have already been collected. # WeakValueDictionary automatically handles removal of collected objects, # so additional cleanup is generally unnecessary. pass"},{"question":"# Asynchronous Task Scheduler: Platform-Specific I/O Handlers Objective Create an asynchronous Python function that schedules a task to perform non-blocking I/O operations based on the platform-specific capabilities and limitations. The function should read data from a file and write it to another file using the appropriate event loop and I/O handlers depending on the operating system. Task Write a function `platform_specific_io_handler` that: 1. Accepts two file paths (source and destination). 2. Determines the underlying platform. 3. Uses `asyncio` to read data from the source file asynchronously and write it to the destination file asynchronously. 4. Implements necessary platform-specific adaptations as described below. Platform-Specific Instructions - **Windows**: Utilize `ProactorEventLoop` and appropriate methods for handling file I/O. - **macOS (version >= 10.9)**: Use the default selector event loop. - **macOS (version <= 10.8)**: Replace the default event loop with either `SelectSelector` or `PollSelector` for character device support. - **Other Unix-based systems**: Use `SelectorEventLoop` with `selectors.DefaultSelector`. Input - `source_file_path` (str): Path to the source file to be read. - `destination_file_path` (str): Path to the destination file where data will be written. Output - The function returns `True` if the operation is successful, `False` otherwise. Constraints - The source file and destination file must exist and be accessible with the appropriate permissions. - The function should handle exceptions gracefully and ensure any resources (such as file handles) are properly closed, even in case of an error. Example Usage ```python import asyncio source_path = \'input.txt\' dest_path = \'output.txt\' result = asyncio.run(platform_specific_io_handler(source_path, dest_path)) print(result) # Should print True if file operation is successful ``` Notes 1. Students should consider handling platform detection using the `platform` module. 2. Proper exception handling and resource management using `asyncio`\'s context managers or try-finally blocks are expected. 3. Efficiently handle I/O operations to avoid blocking the event loop.","solution":"import asyncio import platform import selectors async def read_file_async(file_path): loop = asyncio.get_running_loop() with open(file_path, \'rb\') as f: return await loop.run_in_executor(None, f.read) async def write_file_async(file_path, data): loop = asyncio.get_running_loop() with open(file_path, \'wb\') as f: return await loop.run_in_executor(None, f.write, data) async def platform_specific_io_handler(source_file_path, destination_file_path): try: system = platform.system() if system == \'Windows\': asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) else: if system == \'Darwin\': version = platform.mac_ver()[0] if version and tuple(map(int, version.split(\'.\'))) <= (10, 8): selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) else: loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) else: loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) data = await read_file_async(source_file_path) await write_file_async(destination_file_path, data) return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Email Utility Function Implementation Your task is to implement a function that processes a list of email headers and extracts detailed information about the dates and addresses involved. Specifically, you will: 1. Parse the date fields to extract the corresponding `datetime` objects. 2. Parse the address fields to extract tuples of real names and email addresses. 3. Return a structured dictionary with this information. You will use the functions provided in the `email.utils` module. Function Signature ```python from typing import List, Dict, Tuple, Union from datetime import datetime def process_email_headers(headers: List[Tuple[str, str]]) -> Dict[str, Union[datetime, List[Tuple[str, str]]]]: pass ``` Input - `headers`: A list of tuples, where each tuple contains a header field name (e.g., \'Date\', \'To\', \'Cc\') and the corresponding field value (a string). Output - A dictionary containing: - `\'dates\'`: A list of `datetime` objects for each \'Date\' field. - `\'addresses\'`: A list of tuples with real names and email addresses extracted from \'To\' and \'Cc\' fields. Example ```python from email.utils import format_datetime from datetime import datetime dt1 = format_datetime(datetime(2023, 10, 5, 15, 30)) dt2 = format_datetime(datetime(2023, 10, 6, 10, 45)) headers = [ (\'Date\', dt1), (\'To\', \'Alice <alice@example.com>, Bob <bob@example.org>\'), (\'Date\', dt2), (\'Cc\', \'Charlie <charlie@example.net>\') ] result = process_email_headers(headers) ``` Expected `result`: ```python { \'dates\': [datetime(2023, 10, 5, 15, 30), datetime(2023, 10, 6, 10, 45)], \'addresses\': [ (\'Alice\', \'alice@example.com\'), (\'Bob\', \'bob@example.org\'), (\'Charlie\', \'charlie@example.net\') ] } ``` Constraints - The \'Date\' fields are guaranteed to be correctly formatted as per RFC 2822. - The \'To\' and \'Cc\' fields may contain multiple addresses, correctly formatted as per RFC 2822. - Use the `email.utils` functions appropriately to parse and format the required fields. Notes - Ensure your function handles multiple \'Date\' fields and combines addresses from both \'To\' and \'Cc\' fields. - The order of dates and addresses in the output should match their order in the input. Good luck!","solution":"from typing import List, Dict, Tuple, Union from datetime import datetime from email.utils import parsedate_to_datetime, getaddresses def process_email_headers(headers: List[Tuple[str, str]]) -> Dict[str, Union[List[datetime], List[Tuple[str, str]]]]: result = { \'dates\': [], \'addresses\': [] } for name, value in headers: if name == \'Date\': result[\'dates\'].append(parsedate_to_datetime(value)) elif name in (\'To\', \'Cc\'): result[\'addresses\'].extend(getaddresses([value])) return result"},{"question":"You are given a dataset containing information about different employees in a company. The dataset is provided in a CSV file, \\"employees.csv\\", and it has the following columns: - `employee_id`: Unique identifier of the employee. - `name`: Name of the employee. - `age`: Age of the employee. - `department`: Department in which the employee works. - `salary`: Salary of the employee. You are required to perform the following tasks using the `pandas` library: 1. Load the dataset into a pandas DataFrame. 2. Group the employees by their department. 3. For each department, compute the following statistics: - Average salary (`avg_salary`) - Maximum salary (`max_salary`) - Minimum salary (`min_salary`) - Average age (`avg_age`) 4. For each department, identify the employee(s) with the maximum salary. 5. Filter out departments where the average employee age is below 30. 6. Plot a bar chart showing the average salary of each department that passes the filter in step 5. **Expected Input Format:** - A CSV file named \\"employees.csv\\" with the described columns. **Expected Output Format:** - A DataFrame showing the statistics for each department. - A DataFrame showing the employees with the maximum salary in each department. - A bar chart of the average salary for each remaining department. **Constraints:** - The solution should handle datasets of large sizes efficiently. - Use appropriate pandas functions to achieve the tasks efficiently. **Performance Requirements:** - The solution should be optimized for performance, utilizing vectorized operations where possible. **Function Prototype:** ```python def analyze_employees(file_path: str): import pandas as pd import matplotlib.pyplot as plt # Task 1: Load dataset df = pd.read_csv(file_path) # Task 2: Group by department grouped = df.groupby(\'department\') # Task 3: Compute required statistics stats_df = grouped.agg( avg_salary=(\'salary\', \'mean\'), max_salary=(\'salary\', \'max\'), min_salary=(\'salary\', \'min\'), avg_age=(\'age\', \'mean\') ).reset_index() # Task 4: Identify employee(s) with maximum salary in each department max_salary_employees = grouped.apply(lambda x: x[x[\'salary\'] == x[\'salary\'].max()]).reset_index(drop=True) # Task 5: Filter departments where average age is below 30 filtered_stats_df = stats_df[stats_df[\'avg_age\'] >= 30] # Task 6: Plot bar chart of average salary plt.bar(filtered_stats_df[\'department\'], filtered_stats_df[\'avg_salary\']) plt.xlabel(\'Department\') plt.ylabel(\'Average Salary\') plt.title(\'Average Salary by Department (Age >= 30)\') plt.show() return stats_df, max_salary_employees, plt.gcf() ```","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_employees(file_path: str): # Task 1: Load dataset df = pd.read_csv(file_path) # Task 2: Group by department grouped = df.groupby(\'department\') # Task 3: Compute required statistics stats_df = grouped.agg( avg_salary=(\'salary\', \'mean\'), max_salary=(\'salary\', \'max\'), min_salary=(\'salary\', \'min\'), avg_age=(\'age\', \'mean\') ).reset_index() # Task 4: Identify employee(s) with maximum salary in each department max_salary_employees = grouped.apply(lambda x: x[x[\'salary\'] == x[\'salary\'].max()]).reset_index(drop=True) # Task 5: Filter departments where average age is below 30 filtered_stats_df = stats_df[stats_df[\'avg_age\'] >= 30] # Task 6: Plot bar chart of average salary plt.bar(filtered_stats_df[\'department\'], filtered_stats_df[\'avg_salary\']) plt.xlabel(\'Department\') plt.ylabel(\'Average Salary\') plt.title(\'Average Salary by Department (Age >= 30)\') plt.show() return stats_df, max_salary_employees, plt.gcf()"},{"question":"# PyTorch Function Transforms: Custom Function Implementation and Analysis **Objective:** You are required to implement a custom function that utilizes the `torch.func` module\'s transforms as well as analyze and interpret the results. **Problem Statement:** 1. **Function Implementation:** You need to write a function `compute_gradients_and_hessian` that does the following: - Takes as inputs: - A 1-dimensional tensor `x` representing numerical values. - Returns: - The gradient of the function `f(x) = x^3` at `x`. - The Hessian of the same function at `x`. 2. **Usage of Transforms:** Your implementation should leverage `torch.func.grad` and `torch.func.hessian`. 3. **Input and Output Formats:** - Input: ```python x = torch.tensor([...], requires_grad=True) ``` - Output: ```python gradient, hessian ``` Here, `gradient` should be a tensor of the same shape as `x`, and `hessian` should be a 2-dimensional tensor of shape `(len(x), len(x))`. 4. **Constraints:** - Make sure the input tensor `x` is at least of length 1. - Handle the gradients and Hessians computation explicitly using `torch.func.grad` and `torch.func.hessian`. 5. **Example:** ```python import torch from torch.func import grad, hessian def compute_gradients_and_hessian(x): # Define the function f(x) = x^3 def func(x): return (x ** 3).sum() # Compute the gradient using torch.func.grad gradient_fn = grad(func) gradient = gradient_fn(x) # Compute the Hessian using torch.func.hessian hessian_fn = hessian(func) hess = hessian_fn(x) return gradient, hess # Example usage: x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) gradient, hessian = compute_gradients_and_hessian(x) # Expected output: # gradient: tensor([ 3., 12., 27.], grad_fn=<MulBackward0>) # hessian: tensor([[ 6., 0., 0.], # [ 0., 12., 0.], # [ 0., 0., 18.]]) ``` **Note:** The above implementation should compute the gradients and Hessians for the function `f(x) = x^3` correctly. The gradient of `f(x) = x^3` is `3x^2` and the Hessian is a diagonal matrix with entries `6x`. Validate your implementation with some test cases. **Hints:** - The `grad` function differentiates the provided function and returns a function to compute gradients. - The `hessian` function returns a function to compute the Hessian matrix. - Use the results from `grad` and `hessian` functions to validate the correctness of your implementation.","solution":"import torch from torch.autograd.functional import hessian, jacobian def compute_gradients_and_hessian(x): # Define the function f(x) = x^3 def func(x): return (x ** 3).sum() # Compute the gradient using torch.autograd.functional.jacobian gradient_fn = lambda x: torch.autograd.grad(func(x), x, create_graph=True)[0] gradient = gradient_fn(x) # Compute the Hessian using torch.autograd.functional.hessian hess = hessian(func, x) return gradient, hess # Example usage: # x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # gradient, hessian = compute_gradients_and_hessian(x) # print(\\"Gradient:n\\", gradient) # print(\\"Hessian:n\\", hessian)"},{"question":"# Asynchronous Network Communication You are required to implement an asynchronous echo server and client using asyncio. The echo server should be able to handle multiple clients concurrently, echoing back any message it receives. The client should send messages to the server and print out the echoed responses. Requirements: 1. **Echo Server:** - Implement an asynchronous TCP echo server using `asyncio`. - The server should accept connections from multiple clients. - For each client connection, the server should read data, echo it back to the client, and handle disconnection gracefully. 2. **Echo Client:** - Implement an asynchronous TCP echo client using `asyncio`. - The client should be able to connect to the echo server, send messages, and print the echoed responses. - The client should handle disconnection gracefully. Input and Output Formats: - The server does not take any input or produce output directly but should log connections and disconnections for debugging purposes. - The client should have a user interface that allows sending messages to the server. Constraints: - Use the asyncio package to handle all asynchronous operations. - Ensure proper error handling and resource cleanup (e.g., closing sockets, handling interrupted connections). - Use appropriate asyncio methods to manage event loops, create tasks, and handle network communication. Example Interaction: ```plaintext # Server Output Server started on 127.0.0.1:8888 Client connected from 127.0.0.1:12345 Received data: Hello, Server! Echoing back: Hello, Server! Client disconnected from 127.0.0.1:12345 # Client Output Connected to the server at 127.0.0.1:8888 Enter message: Hello, Server! Received echo: Hello, Server! ``` Code Requirements: - Define an `EchoServer` class with methods to start and stop the server. - Define an `EchoClient` class with methods to connect to the server, send messages, and handle the server\'s responses. - Your implementation should use `asyncio.start_server()` for the server and `asyncio.open_connection()` for the client. - Implement proper asynchronous handling of connections, reading, and writing operations. Submit your solution in a single Python file with appropriate comments explaining your code.","solution":"import asyncio class EchoServer: def __init__(self, host=\'127.0.0.1\', port=8888): self.host = host self.port = port self.server = None async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Client connected from {addr}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received data: {message}\\") writer.write(data) await writer.drain() print(f\\"Echoing back: {message}\\") except Exception as e: print(f\\"Error with client {addr}: {e}\\") finally: writer.close() await writer.wait_closed() print(f\\"Client disconnected from {addr}\\") async def start(self): self.server = await asyncio.start_server( self.handle_client, self.host, self.port) addr = self.server.sockets[0].getsockname() print(f\\"Server started on {addr}\\") async with self.server: await self.server.serve_forever() def stop(self): if self.server: self.server.close() class EchoClient: def __init__(self, host=\'127.0.0.1\', port=8888): self.host = host self.port = port async def connect(self): self.reader, self.writer = await asyncio.open_connection(self.host, self.port) print(f\'Connected to the server at {self.host}:{self.port}\') async def send_message(self, message): self.writer.write(message.encode()) await self.writer.drain() data = await self.reader.read(100) print(f\'Received echo: {data.decode()}\') def close(self): if self.writer: self.writer.close() async def main(): # Start server server = EchoServer() server_task = asyncio.create_task(server.start()) # Give server time to start await asyncio.sleep(1) # Connect client client = EchoClient() await client.connect() # Send message from client await client.send_message(\\"Hello, Server!\\") # Close client connection client.close() # Stop server server.stop() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Advanced XML Manipulation and Transformation You are required to implement a Python function using the `xml.etree.ElementTree` module to perform specific manipulations on an XML document. The function should: 1. Parse an XML document from a given string. 2. Add a new attribute `population` to each `country` element with a value based on its `rank`. - Countries with rank ≤ 10 should have a `population` of `\\"small\\"`. - Countries with rank > 10 and ≤ 50 should have a `population` of `\\"medium\\"`. - Countries with rank > 50 should have a `population` of `\\"large\\"`. 3. Remove all `neighbor` elements from each `country` element. 4. Add a new child element `<note>` containing the text `\\"Data processed\\"` to each `country` element. 5. Serialize the modified XML tree back to a string. # Input Format - A single string containing the XML data to be parsed and modified. # Output Format - A string containing the modified XML data. # Constraints - The XML data will always contain the root element `<data>` with multiple child elements `<country>`. - The `<country>` elements will always have a `rank` sub-element containing an integer value. # Example Input ```xml <data> <country name=\\"Liechtenstein\\"> <rank>1</rank> <year>2008</year> <neighbor name=\\"Austria\\" direction=\\"E\\"/> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\"> <rank>4</rank> <year>2011</year> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> <country name=\\"Panama\\"> <rank>68</rank> <year>2011</year> <neighbor name=\\"Costa Rica\\" direction=\\"W\\"/> <neighbor name=\\"Colombia\\" direction=\\"E\\"/> </country> </data> ``` Output ```xml <data> <country name=\\"Liechtenstein\\" population=\\"small\\"> <rank>1</rank> <year>2008</year> <note>Data processed</note> </country> <country name=\\"Singapore\\" population=\\"small\\"> <rank>4</rank> <year>2011</year> <note>Data processed</note> </country> <country name=\\"Panama\\" population=\\"large\\"> <rank>68</rank> <year>2011</year> <note>Data processed</note> </country> </data> ``` # Function Signature ```python def process_xml(data: str) -> str: pass ``` Implement `process_xml` to achieve the desired behavior.","solution":"import xml.etree.ElementTree as ET def process_xml(data: str) -> str: # Parse XML from the given string root = ET.fromstring(data) # Iterate over every country element for country in root.findall(\'country\'): # Get rank value rank = int(country.find(\'rank\').text) # Add population attribute based on rank if rank <= 10: country.set(\'population\', \'small\') elif rank <= 50: country.set(\'population\', \'medium\') else: country.set(\'population\', \'large\') # Remove all neighbor elements for neighbor in country.findall(\'neighbor\'): country.remove(neighbor) # Add new child element <note> with text \\"Data processed\\" note = ET.SubElement(country, \'note\') note.text = \\"Data processed\\" # Serialize the modified XML tree back to a string return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Advanced Pandas Coding Assessment Question Objective: Your task is to implement a set of functions that manipulate and analyze a given time series dataset. You will demonstrate proficiency in working with time series data in pandas, including parsing dates, generating date ranges, resampling, and performing timezone-aware operations. Dataset: You will be working with a CSV file named `timeseries_data.csv`. The file has the following columns: - `date`: A date string in the format `YYYY-MM-DD HH:MM:SS`. - `value`: A numeric value corresponding to each date. Functions to Implement: 1. **load_and_parse_data**: - **Input**: A string `file_path` representing the path to the CSV file. - **Output**: A pandas DataFrame with a `DatetimeIndex` and one column `value`. - **Constraints**: The function should parse the `date` column into datetime objects and set it as the index of the DataFrame. 2. **generate_date_range_series**: - **Input**: Two datetime objects, `start_date` and `end_date`, and a string `frequency` representing the frequency of the date range. - **Output**: A pandas Series with the specified date range as the index, and integer values starting from 1. - **Constraints**: The function should return a Series where the index is a range of dates from `start_date` to `end_date` with the specified frequency. 3. **resample_time_series**: - **Input**: A pandas Series `time_series` and a string `resample_frequency` representing the new frequency. - **Output**: A resampled pandas Series with the mean values. - **Constraints**: The function should resample the input series to the new frequency and return the mean values of the resampled periods. 4. **convert_timezone**: - **Input**: A pandas DataFrame `df` with a `DatetimeIndex` and a string `timezone` representing the new timezone. - **Output**: The input DataFrame with its index converted to the new timezone. - **Constraints**: The function should handle the timezone conversion correctly and return the DataFrame with the updated index. 5. **calculate_moving_average**: - **Input**: A pandas Series `time_series` and an integer `window_size` representing the window size for the moving average. - **Output**: A pandas Series representing the moving average of the input series. - **Constraints**: The function should compute the moving average using a window of the specified size. Example Usage: ```python # Load and parse data df = load_and_parse_data(\'timeseries_data.csv\') # Generate a date range series date_range_series = generate_date_range_series(pd.Timestamp(\'2021-01-01\'), pd.Timestamp(\'2021-01-10\'), \'D\') # Resample the time series resampled_series = resample_time_series(df[\'value\'], \'H\') # Convert the time series to a new timezone df_tz = convert_timezone(df, \'US/Eastern\') # Calculate the moving average of the time series moving_avg = calculate_moving_average(df[\'value\'], 3) ``` **Note**: Please ensure that your implementations handle edge cases and follow best practices for working with pandas.","solution":"import pandas as pd def load_and_parse_data(file_path): Loads and parses the CSV file with a date column into a pandas DataFrame with a DatetimeIndex. df = pd.read_csv(file_path, parse_dates=[\'date\'], index_col=\'date\') return df def generate_date_range_series(start_date, end_date, frequency): Generates a pandas Series with a specified date range as the index and integer values starting from 1. date_rng = pd.date_range(start=start_date, end=end_date, freq=frequency) return pd.Series(range(1, len(date_rng) + 1), index=date_rng) def resample_time_series(time_series, resample_frequency): Resamples the input Series to the new frequency and returns the mean values. return time_series.resample(resample_frequency).mean() def convert_timezone(df, timezone): Converts the DatetimeIndex of the DataFrame to the new timezone. return df.tz_localize(\'UTC\').tz_convert(timezone) def calculate_moving_average(time_series, window_size): Computes the moving average of the input series using a window of the specified size. return time_series.rolling(window=window_size).mean()"},{"question":"# Question: Advanced Producer-Consumer Model with Python Threading **Objective**: Implement an advanced multi-threaded producer-consumer model using Python\'s `threading` module. # Problem Statement: You need to create a class `AdvancedProducerConsumer` which manages multiple producers and consumers. This class should use threading synchronization mechanisms to ensure safe concurrent access to a shared buffer. Specifically, implement the following functionalities: 1. **Initialization**: - The class should initialize with a maximum buffer size. - An `Event` object named `stop_event` should be used to signal all threads to stop their execution gracefully. 2. **Buffer Management**: - The shared buffer must be protected using an `RLock`. - Condition variables should be used to manage the state of the buffer (not full when producing and not empty when consuming). 3. **Producers**: - Producers should generate items and place them in the buffer. - If the buffer is full, producers should wait until an item is consumed. 4. **Consumers**: - Consumers should consume items from the buffer. - If the buffer is empty, consumers should wait until an item is produced. 5. **Start and Stop**: - Methods to start and stop all producers and consumers should be provided. - Stopping should involve setting the `stop_event` and ensuring all threads terminate gracefully. # Requirements: 1. **Class Definition**: ```python class AdvancedProducerConsumer: def __init__(self, buffer_size): pass def start_producers(self, num_producers): pass def start_consumers(self, num_consumers): pass def stop_all(self): pass ``` 2. **Producer and Consumer Logic**: - Producers will produce items (use random sleep intervals to simulate work). - Consumers will consume items (use random sleep intervals to simulate work). 3. **Concurrency Control**: - Use `RLock` for buffer access synchronization. - Use `Condition` to manage full/empty buffer states. - Use `Event` to handle graceful stopping of threads. # Example Usage: ```python if __name__ == \\"__main__\\": pc = AdvancedProducerConsumer(buffer_size=10) # Start producers and consumers pc.start_producers(num_producers=3) pc.start_consumers(num_consumers=3) # Run for a while and then stop all time.sleep(10) pc.stop_all() ``` # Constraints: - Buffer size will be an integer between 1 and 100. - Number of producer and consumer threads will be between 1 and 10. Implement the `AdvancedProducerConsumer` class and ensure it meets the requirements above.","solution":"import threading import time import random class AdvancedProducerConsumer: def __init__(self, buffer_size): self.buffer_size = buffer_size self.buffer = [] self.buffer_lock = threading.RLock() self.not_full = threading.Condition(self.buffer_lock) self.not_empty = threading.Condition(self.buffer_lock) self.stop_event = threading.Event() def producer(self): while not self.stop_event.is_set(): item = random.randint(1, 100) with self.not_full: while len(self.buffer) >= self.buffer_size and not self.stop_event.is_set(): self.not_full.wait() if self.stop_event.is_set(): return self.buffer.append(item) print(f\\"Produced: {item}\\") self.not_empty.notify() def consumer(self): while not self.stop_event.is_set(): with self.not_empty: while len(self.buffer) == 0 and not self.stop_event.is_set(): self.not_empty.wait() if self.stop_event.is_set(): return item = self.buffer.pop(0) print(f\\"Consumed: {item}\\") self.not_full.notify() def start_producers(self, num_producers): self.producers = [] for _ in range(num_producers): producer_thread = threading.Thread(target=self.producer) producer_thread.start() self.producers.append(producer_thread) def start_consumers(self, num_consumers): self.consumers = [] for _ in range(num_consumers): consumer_thread = threading.Thread(target=self.consumer) consumer_thread.start() self.consumers.append(consumer_thread) def stop_all(self): self.stop_event.set() with self.not_full: self.not_full.notify_all() with self.not_empty: self.not_empty.notify_all() for producer in self.producers: producer.join() for consumer in self.consumers: consumer.join() print(\\"All producers and consumers have been stopped.\\")"},{"question":"**XML Parsing with SAX in Python** **Objective**: Implement a custom SAX parser in Python that utilizes the `xml.sax.xmlreader` module to parse a given XML string. The parser should handle specific tags and attributes, and should output relevant data extracted from the XML. # Requirements: 1. **Parse an XML string**: The input will be an XML string. 2. **Handle specific tags and attributes**: Specifically, you should: - Count the number of occurrences of a specific tag (e.g., `<item>`). - Extract and print the value of a specific attribute (e.g., `id`) if present, from each occurrence of the tag. 3. **Use Incremental Parsing**: The parser should process the XML in chunks, simulating streaming data. # Input and Output: - **Input**: An XML string containing multiple `<item>` tags, each with an optional `id` attribute. - **Output**: - Print the count of `<item>` tags. - Print each `id` attribute value on a new line, if present. # Constraints: - The function should use classes and methods from the `xml.sax.xmlreader` module. - The `feed()` method of the `IncrementalParser` class must be used to simulate incremental parsing. - The XML string may be large; consider the performance with regard to memory usage. # Example: ```python Sample XML Input: var xml_data = \'\'\' <root> <item id=\\"123\\">Content</item> <item>Content</item> <item id=\\"456\\">Content</item> <item>Content</item> </root> \'\'\' Expected Output: Number of <item> tags: 4 id: 123 id: 456 ``` # Function Signature: ```python def custom_xml_parser(xml_data: str) -> None: pass ``` # Implementation Details: - Define a class that inherits from `xml.sax.xmlreader.IncrementalParser`. - Implement appropriate methods (`startElement`, `endElement`, etc.) to handle XML parsing. - Use the `feed()` method to process the XML data in chunks. - Output the required information as described. **Good luck!**","solution":"import xml.sax import xml.sax.handler class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.item_count = 0 self.ids = [] def startElement(self, name, attrs): if name == \'item\': self.item_count += 1 id_attr = attrs.get(\'id\') if id_attr: self.ids.append(id_attr) def getCount(self): return self.item_count def getIds(self): return self.ids def custom_xml_parser(xml_data: str) -> None: handler = CustomContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) chunk_size = 1024 for i in range(0, len(xml_data), chunk_size): chunk = xml_data[i:i + chunk_size] parser.feed(chunk) print(f\\"Number of <item> tags: {handler.getCount()}\\") for id in handler.getIds(): print(f\\"id: {id}\\")"},{"question":"# Python Coding Assessment Question Objective: Write a Python function that alternates a terminal file descriptor between raw and cbreak modes every time a character is read from the terminal input. Demonstrate your understanding of Unix terminal control functions provided by the `tty` module. Function Signature: ```python def toggle_terminal_mode(fd: int) -> None: pass ``` # Description: 1. **Input:** - `fd` (int): The file descriptor of the terminal. 2. **Output:** - The function does not return any value but should input/output directly in the terminal. 3. **Behavior:** - The function should: 1. Alternate the terminal mode between raw and cbreak every time a character is read. 2. Use `tty.setraw(fd)` to switch to raw mode. 3. Use `tty.setcbreak(fd)` to switch to cbreak mode. 4. Continue to alternate until a special character (e.g., \'q\') is read to terminate the function. 5. Ensure the terminal mode is returned to its original state before the function exits. Constraints: - The function should only work on Unix-based systems. - Use appropriate error handling for system-specific operations. # Example: ```python import os import sys import tty import termios def toggle_terminal_mode(fd: int) -> None: original_mode = termios.tcgetattr(fd) try: while True: # Read a single character from stdin c = os.read(fd, 1).decode() if c == \'q\': break # Toggle between raw and cbreak modes if tty.tcgetattr(fd) == original_mode: tty.setraw(fd) else: tty.setcbreak(fd) finally: # Reset to the original terminal mode termios.tcsetattr(fd, termios.TCSAFLUSH, original_mode) # Usage example: if __name__ == \\"__main__\\": fd = sys.stdin.fileno() toggle_terminal_mode(fd) ``` This example demonstrates toggling the terminal modes of the standard input file descriptor. Ensure you comprehend working with low-level terminal control functions and handling file descriptors before attempting this task.","solution":"import os import tty import termios def toggle_terminal_mode(fd: int) -> None: Toggles the terminal mode between raw and cbreak every time a character is read. Functions on Unix-based systems only. :param fd: File descriptor of the terminal # Save the original terminal settings original_mode = termios.tcgetattr(fd) try: while True: # Read one character from the file descriptor c = os.read(fd, 1).decode() # Exit on character \'q\' if c == \'q\': break # Toggle between raw and cbreak modes current_mode = termios.tcgetattr(fd) if current_mode == original_mode: tty.setraw(fd) else: tty.setcbreak(fd) finally: # Restore the original terminal settings termios.tcsetattr(fd, termios.TCSANOW, original_mode)"},{"question":"# Thread Synchronization and Lock Management You are required to implement a function that utilizes threading to perform concurrent tasks while safely managing shared resources using locks. **Problem Statement:** Write a Python function `concurrent_sum(numbers, n_threads)` that takes a list of numbers and an integer `n_threads` as input. The function should split the list into `n_threads` parts and use multiple threads to compute the sum of the parts concurrently. The function should return the total sum of all numbers in the list. To ensure thread safety and correct synchronization, use the `_thread` module to manage threads and locks. Follow these steps: 1. Create a lock to manage access to shared resources. 2. Split the input list `numbers` into `n_threads` parts as evenly as possible. 3. Start `n_threads` threads, each calculating the sum of one part of the list. 4. Use the lock to ensure that each thread safely adds its partial sum to a shared total sum variable. 5. Wait for all threads to complete before returning the total sum. # Function Signature ```python import _thread def concurrent_sum(numbers: list, n_threads: int) -> int: pass ``` # Input - `numbers`: A list of integers. - `n_threads`: An integer specifying the number of threads to use. # Output - Returns an integer representing the total sum of all integers in the list. # Constraints - The length of `numbers` will be at least as large as `n_threads`. - `n_threads` will be a positive integer. - All integers in `numbers` are non-negative. # Example ```python numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] n_threads = 3 print(concurrent_sum(numbers, n_threads)) # Output should be 55 ``` # Notes - Use `_thread.start_new_thread` to start the threads. - Use `_thread.allocate_lock` to create the lock for synchronization. - Each thread should securely update a shared variable representing the total sum.","solution":"import _thread def concurrent_sum(numbers: list, n_threads: int) -> int: # List to store sums from each thread thread_sums = [0] * n_threads # Lock for synchronizing access to the shared sum lock = _thread.allocate_lock() def worker(part, index): # Calculate sum of the part part_sum = sum(part) # Safely update the shared sum using the lock with lock: thread_sums[index] = part_sum # Split the list into n_threads parts def split_list(lst, n): avg = len(lst) / float(n) out = [] last = 0.0 while last < len(lst): out.append(lst[int(last):int(last + avg)]) last += avg return out parts = split_list(numbers, n_threads) # Start the threads for i in range(n_threads): _thread.start_new_thread(worker, (parts[i], i)) # Busy wait for threads to complete (simple and naive approach) # This should be improved with synchronization but it ensures threads finish work while any(s == 0 for s in thread_sums): pass # Return total sum return sum(thread_sums)"},{"question":"# Email Message Manipulation using the `email.message.Message` Class Write a Python function `create_and_modify_email(headers, payload, modifications)` that creates an email message using the given headers and payload, and then modifies it based on the provided modifications. Function Signature ```python def create_and_modify_email(headers: dict, payload: str, modifications: dict) -> dict: pass ``` Parameters - `headers`: A dictionary where the keys are the header names and the values are the header values. - `payload`: A string representing the initial payload of the email message. - `modifications`: A dictionary containing the modifications to be made to the email message. The keys in this dictionary can be: - `\\"add_header\\"`: A tuple `(name, value)` representing a header to be added. - `\\"delete_header\\"`: A string representing the name of a header to be deleted. - `\\"replace_header\\"`: A tuple `(name, value)` representing a header to be replaced. - `\\"change_payload\\"`: A string representing the new payload to set. Returns - Returns a dictionary with the following keys: - `\\"headers\\"`: A dictionary of the updated headers after modifications. - `\\"payload\\"`: A string of the updated payload after modifications. Implementation Requirements 1. **Create the email message:** - Initialize a `Message` object. - Set the headers using the `headers` dictionary. - Set the initial payload using the `payload` string. 2. **Perform modifications:** - Add headers using `\\"add_header\\"`. - Delete headers using `\\"delete_header\\"`. - Replace headers using `\\"replace_header\\"`. - Change the payload using `\\"change_payload\\"`. 3. **Return the updated email message:** - Retrieve the headers and payload from the modified email message. - Return them as a dictionary. # Example ```python headers = { \\"From\\": \\"sender@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Subject\\": \\"Test Email\\" } payload = \\"This is the body of the email.\\" modifications = { \\"add_header\\": (\\"CC\\", \\"cc@example.com\\"), \\"delete_header\\": \\"Subject\\", \\"replace_header\\": (\\"From\\", \\"new_sender@example.com\\"), \\"change_payload\\": \\"This is the new body of the email.\\" } output = create_and_modify_email(headers, payload, modifications) ``` Expected `output` would be: ```python { \\"headers\\": { \\"From\\": \\"new_sender@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"CC\\": \\"cc@example.com\\" }, \\"payload\\": \\"This is the new body of the email.\\" } ``` Constraints - Header names are case-insensitive. - Assume that `modifications` dictionary does not have conflicting instructions (e.g., adding and deleting the same header). Notes - Use the `email.message.Message` class for the email message representation. - Ensure that all modifications adhere to the constraints mentioned.","solution":"from email.message import Message def create_and_modify_email(headers: dict, payload: str, modifications: dict) -> dict: msg = Message() # Set the headers for key, value in headers.items(): msg[key] = value # Set the initial payload msg.set_payload(payload) # Perform modifications if \\"add_header\\" in modifications: header_name, header_value = modifications[\\"add_header\\"] msg[header_name] = header_value if \\"delete_header\\" in modifications: del msg[modifications[\\"delete_header\\"]] if \\"replace_header\\" in modifications: header_name, header_value = modifications[\'replace_header\'] msg.replace_header(header_name, header_value) if \\"change_payload\\" in modifications: msg.set_payload(modifications[\\"change_payload\\"]) updated_headers = dict(msg.items()) updated_payload = msg.get_payload() return { \\"headers\\": updated_headers, \\"payload\\": updated_payload }"},{"question":"# Sound File Analyzer **Objective:** Write a Python function that processes a directory of sound files, determines their types and characteristics using the `sndhdr` module, and outputs a summary report. **Function Signature:** ```python def analyze_sound_files(directory_path: str) -> dict: pass ``` **Input:** - `directory_path` (str): A string representing the path to the directory containing the sound files. **Output:** - Returns a dictionary where: - The keys are the file paths of each sound file. - The values are dictionaries with the following structure: ```python { \\"filetype\\": str, \\"framerate\\": int, \\"nchannels\\": int, \\"nframes\\": int, \\"sampwidth\\": Union[int, str] # sample width in bits or \'A\'/\'U\' } ``` **Constraints:** - Assure that the given directory path exists and is accessible. - Handle any exceptions that occur due to invalid files or unsupported file types. - Ignore non-sound files and log an appropriate message for each. **Performance Requirements:** - The function should efficiently handle directories with a large number of files. **Example:** Consider the following directory structure (where files marked as `*.wav`, `*.aiff`, etc., are valid sound files): ``` /sounds sound1.wav sound2.aiff not_a_sound.txt sound3.au ``` The call `analyze_sound_files(\'/sounds\')` should return a dictionary like this: ```python { \\"/sounds/sound1.wav\\": { \\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 100000, \\"sampwidth\\": 16 }, \\"/sounds/sound2.aiff\\": { \\"filetype\\": \\"aiff\\", \\"framerate\\": 0, \\"nchannels\\": 2, \\"nframes\\": -1, \\"sampwidth\\": 16 }, \\"/sounds/sound3.au\\": { \\"filetype\\": \\"au\\", \\"framerate\\": 8000, \\"nchannels\\": 1, \\"nframes\\": 5000, \\"sampwidth\\": 8 } } ``` Please note that non-sound files like `not_a_sound.txt` are ignored in the output. **Implementation Notes:** - Use the `sndhdr` module\'s `whathdr` or `what` function to determine the sound file characteristics. - Use `os.listdir` to iterate over the files in the given directory. - Ensure robust error handling for invalid file formats and permission issues.","solution":"import os import sndhdr def analyze_sound_files(directory_path: str) -> dict: Analyzes sound files in the given directory and returns their properties. sound_file_summary = {} # List all files in the directory for filename in os.listdir(directory_path): file_path = os.path.join(directory_path, filename) # Check if it\'s a file (not a directory) if not os.path.isfile(file_path): continue # Try to identify the file type using sndhdr file_info = sndhdr.what(file_path) if file_info is None: # Log invalid or unsupported file print(f\\"Skipping non-sound or unsupported file: {file_path}\\") continue # Collecting sound file details sound_file_details = { \\"filetype\\": file_info.filetype, \\"framerate\\": file_info.framerate, \\"nchannels\\": file_info.nchannels, \\"nframes\\": file_info.nframes, \\"sampwidth\\": file_info.sampwidth } sound_file_summary[file_path] = sound_file_details return sound_file_summary"},{"question":"# Advanced Python: Implementing and Manipulating Custom Numeric and Sequence Objects Objective Write a Python class that demonstrates a deep understanding of numeric and sequence object manipulation. Leverage type checking, object creation, and fundamental operations as described in the documentation. Problem Statement You are required to implement a class `CustomSequence` that combines numeric and sequence object concepts. This class should allow users to store and manipulate a sequence of numbers with advanced functionalities described below. Class Definition ```python class CustomSequence: def __init__(self, elements: list): Initializes the CustomSequence with a list of elements. If the elements list is not composed entirely of integers or floats, raises a TypeError. pass def add_element(self, element): Adds a new element to the sequence. The element must be an integer or float. Raises a TypeError otherwise. pass def mean(self): Returns the mean (average) of the numeric elements in the sequence. pass def median(self): Returns the median of the numeric elements in the sequence. pass def variance(self): Returns the variance of the numeric elements in the sequence. pass def __len__(self): Returns the number of elements in the sequence. pass def __getitem__(self, index): Returns the element at the specified index. pass def __setitem__(self, index, value): Sets the element at the specified index to a new value. The value must be an integer or float. Raises a TypeError otherwise. pass def __delitem__(self, index): Deletes the element at the specified index. pass def __repr__(self): Returns the string representation of the CustomSequence object. pass ``` Constraints 1. The sequence must only contain integers and floats. Any operation that introduces a non-integer or non-float value should raise a `TypeError`. 2. The mean, median, and variance methods should only operate on the numeric values within the sequence. 3. Implement appropriate error handling for index out-of-bounds scenarios and invalid type operations. 4. You are allowed to use existing Python libraries such as `math` and `statistics` for calculations. Example Usage ```python try: seq = CustomSequence([1, 2, 3.5, 4]) print(seq.mean()) # Output: 2.625 print(seq.median()) # Output: 2.75 print(seq.variance()) # Output: 1.7857142857142858 seq.add_element(5) print(len(seq)) # Output: 5 print(seq[4]) # Output: 5 seq[1] = 7.5 print(seq.mean()) # Output: 4.4 del seq[0] print(seq) # Output: CustomSequence([7.5, 3.5, 4, 5]) seq.add_element(\'a\') # Should raise TypeError except Exception as e: print(e) # Output: TypeError: All elements must be int or float. ``` Notes - Implement all methods and ensure they follow the constraints. - Proper type checking for inputs and operations is crucial. - Handle all edge cases and design the class for easy scalability and maintenance.","solution":"from statistics import mean, median, variance class CustomSequence: def __init__(self, elements: list): Initializes the CustomSequence with a list of elements. If the elements list is not composed entirely of integers or floats, raises a TypeError. if not all(isinstance(elem, (int, float)) for elem in elements): raise TypeError(\\"All elements must be int or float.\\") self.elements = elements def add_element(self, element): Adds a new element to the sequence. The element must be an integer or float. Raises a TypeError otherwise. if not isinstance(element, (int, float)): raise TypeError(\\"Element must be an int or float.\\") self.elements.append(element) def mean(self): Returns the mean (average) of the numeric elements in the sequence. return mean(self.elements) def median(self): Returns the median of the numeric elements in the sequence. return median(self.elements) def variance(self): Returns the variance of the numeric elements in the sequence. return variance(self.elements) def __len__(self): Returns the number of elements in the sequence. return len(self.elements) def __getitem__(self, index): Returns the element at the specified index. return self.elements[index] def __setitem__(self, index, value): Sets the element at the specified index to a new value. The value must be an integer or float. Raises a TypeError otherwise. if not isinstance(value, (int, float)): raise TypeError(\\"Value must be int or float.\\") self.elements[index] = value def __delitem__(self, index): Deletes the element at the specified index. del self.elements[index] def __repr__(self): Returns the string representation of the CustomSequence object. return f\\"CustomSequence({self.elements})\\""},{"question":"**Objective:** Create a Python function that manipulates a given `setup.cfg` file to add or modify configuration options for specified commands. # Problem Statement: You have been provided with the content of a `setup.cfg` file as a string. Your task is to implement a function `update_setup_config(config_str, command, option, value)` that updates or adds the option for the given command in the configuration. If the command section does not already exist, it should be created. If the option exists within the command, its value should be updated; otherwise, the option should be added with the given value. # Inputs: - `config_str` (str): A string containing the contents of the current `setup.cfg` file. - `command` (str): The command section in the configuration file to update (e.g., `build_ext`). - `option` (str): The option to add or update within the specified command (e.g., `inplace`). - `value` (str): The value to set for the specified option (e.g., `1`). # Output: - Returns the updated configuration as a string. # Constraints: - The configuration string will always have a valid syntax as per the described format (commands, options, and values). - The command and option names will always be valid identifiers. # Example: Input: ```python config_str = [build_ext] inplace=0 include_dirs=/usr/include [bdist_rpm] packager=John Doe command = \\"build_ext\\" option = \\"inplace\\" value = \\"1\\" ``` Output: ```python [build_ext] inplace=1 include_dirs=/usr/include [bdist_rpm] packager=John Doe ``` # Instructions: 1. Implement the function `update_setup_config(config_str, command, option, value)` as per the specifications provided. 2. Ensure that your function appropriately handles updating and adding new commands and options. 3. Consider edge cases, such as the command or option not existing and needing to be created. # Notes: - Use standard libraries for handling and manipulating strings and maintaining the format of the configuration file. ```python def update_setup_config(config_str, command, option, value): # Your implementation here pass # Example usage config_str = [build_ext] inplace=0 include_dirs=/usr/include [bdist_rpm] packager=John Doe command = \\"build_ext\\" option = \\"inplace\\" value = \\"1\\" updated_config_str = update_setup_config(config_str, command, option, value) print(updated_config_str) ```","solution":"import configparser import io def update_setup_config(config_str, command, option, value): Updates or adds an option for a given command section in the setup.cfg configuration. Parameters: - config_str (str): Contents of the setup.cfg file. - command (str): The command section to update or add. - option (str): The option to add or update. - value (str): The value to set for the given option. Returns: - str: The updated configuration as a string. config = configparser.ConfigParser() config.read_string(config_str) if not config.has_section(command): config.add_section(command) config.set(command, option, value) with io.StringIO() as output: config.write(output) updated_config_str = output.getvalue() return updated_config_str.strip()"},{"question":"File Descriptor Control and Locking You are required to write a Python function that utilizes the `fcntl` module to perform various file descriptor operations. The function will simulate a simple file locking mechanism using `fcntl.lockf` and demonstrate file control using `fcntl.fcntl`. Function Specification **Function Name:** `file_descriptor_operations` **Parameters:** 1. `filepath` (str): The path to the file on which operations will be performed. 2. `lock_type` (str): The type of lock to acquire. It can be one of the following values: - `\\"shared\\"`: Acquire a shared lock. - `\\"exclusive\\"`: Acquire an exclusive lock. 3. `flags` (list): A list of flags to set on the file descriptor. Each flag is a string and it can be one of the following: - `\\"O_NONBLOCK\\"`: Set the non-blocking mode. - `\\"O_APPEND\\"`: Set the append mode. **Return:** - The function should return a dictionary with the following keys: - `\\"lock_acquired\\"`: A boolean indicating if the lock was successfully acquired. - `\\"flags_set\\"`: A list of flags that were successfully set on the file descriptor. **Function Behavior:** 1. The function should open the file specified by the `filepath` parameter. 2. Based on the `lock_type` parameter, the function should try to acquire the corresponding lock using `fcntl.lockf`. If the lock cannot be acquired, the function should return `False` for `\\"lock_acquired\\"`. 3. Using `fcntl.fcntl`, the function should set the specified flags on the file descriptor. Any failures in setting flags should be ignored, and the successfully set flags should be recorded. 4. Ensure that the file descriptor is properly closed before the function returns. **Constraints:** - The file specified by `filepath` will always exist. - You can assume that the `fcntl` module and required constants are available. **Example Usage:** ```python result = file_descriptor_operations(\\"/tmp/myfile.txt\\", \\"exclusive\\", [\\"O_NONBLOCK\\", \\"O_APPEND\\"]) print(result) # Output might be: # { # \\"lock_acquired\\": True, # \\"flags_set\\": [\\"O_NONBLOCK\\", \\"O_APPEND\\"] # } ```","solution":"import fcntl import os def file_descriptor_operations(filepath, lock_type, flags): Perform file descriptor operations using fcntl. Parameters: - filepath (str): The path to the file on which operations will be performed. - lock_type (str): The type of lock to acquire: \\"shared\\" or \\"exclusive\\". - flags (list): A list of flags to set on the file descriptor. Return: - (dict): Dictionary with keys \\"lock_acquired\\" (bool) and \\"flags_set\\" (list). lock_type_map = { \\"shared\\": fcntl.LOCK_SH, \\"exclusive\\": fcntl.LOCK_EX } flag_map = { \\"O_NONBLOCK\\": os.O_NONBLOCK, \\"O_APPEND\\": os.O_APPEND } result = { \\"lock_acquired\\": False, \\"flags_set\\": [] } try: fd = os.open(filepath, os.O_RDWR) # Acquire lock try: fcntl.lockf(fd, lock_type_map[lock_type]) result[\\"lock_acquired\\"] = True except IOError: result[\\"lock_acquired\\"] = False # Set flags current_flags = fcntl.fcntl(fd, fcntl.F_GETFL) for flag in flags: try: new_flags = current_flags | flag_map[flag] fcntl.fcntl(fd, fcntl.F_SETFL, new_flags) result[\\"flags_set\\"].append(flag) except KeyError: pass finally: if \'fd\' in locals(): os.close(fd) return result"},{"question":"# Question: Create a Complex Scatter Plot with Jitter Using Seaborn Objective You will demonstrate your understanding of seaborn\'s plotting capabilities, specifically the use of the `seaborn.objects` module to create scatter plots with jitter for better visualization of overlapping data points. Problem Statement 1. Load the `penguins` dataset using seaborn. 2. Create two scatter plots: - Scatter Plot 1: `species` on the x-axis and `body_mass_g` on the y-axis with a small amount of jitter applied to the `species` axis. - Scatter Plot 2: `body_mass_g` on the x-axis and `flipper_length_mm` on the y-axis with jitter applied to both axes. Requirements - You must use the seaborn objects module (`seaborn.objects as so`) to create these plots. - The jitter parameters should be tweaked to ensure clear visualization of data points. - The first plot should use the `width` parameter to adjust jitter. - The second plot should independently customize jitter for the `x` and `y` axes. Input and Output Formats There is no specific input format as you will be generating plots directly. The output should be two distinct scatter plots displayed in sequence. Constraints - You must use the seaborn package. - Ensure that the plots handle any possible overlap between the data points effectively using jitter. Example Code For reference, here is an example of how to create a basic scatter plot with jitter using seaborn objects: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset penguins = load_dataset(\\"penguins\\") # Scatter Plot 1 plot1 = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) plot1.show() # Scatter Plot 2 plot2 = ( so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=100, y=5)) ) plot2.show() ``` Your task is to create similar plots, with the specified adjustments to the jitter parameters.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_scatter_plots(): # Load dataset penguins = load_dataset(\\"penguins\\") # Scatter Plot 1: `species` on the x-axis and `body_mass_g` on the y-axis with jitter on x-axis plot1 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.3)) # Adjusting width to control jitter ) plot1.show() # Scatter Plot 2: `body_mass_g` on the x-axis and `flipper_length_mm` on the y-axis with jitter on both axes plot2 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=100, y=5)) # Applying jitter to both axes ) plot2.show()"},{"question":"Your task is to create a setup configuration file (`setup.cfg`) for a hypothetical Python project, demonstrating your understanding of setting default command options for building and distributing the project. The project has the following requirements: 1. The project includes a Python extension module named `project.extension`. When built, the compiled module should be placed directly in the source directory alongside other Python modules. 2. When generating an RPM distribution of the project using `bdist_rpm`, it should use the following details: - Release number: 2 - Packager: Jane Doe - Include the following documentation files: - `CHANGELOG.md` - `README.md` - All files under the `docs` directory You need to create the content of `setup.cfg` that meets these requirements. Expected Output Provide the content of the `setup.cfg` file as a string in your code. ```python def generate_setup_cfg(): Generates the content of a setup.cfg file for the given project requirements. Returns: str: The content of the setup.cfg configuration file. setup_cfg_content = [build_ext] inplace=1 [bdist_rpm] release=2 packager=Jane Doe doc_files=CHANGELOG.md README.md docs/ return setup_cfg_content.strip() # Example Usage print(generate_setup_cfg()) ``` Your implementation should cover the structure, syntax, and necessary options for the configuration file as specified. Constraints - You may assume that the paths and filenames provided are correct and exist in the project structure. - Comments and blank lines may be used for readability but are not required. - The function should return the content as a single string with newlines separating the sections. Evaluation Your solution will be evaluated on: - Correctness: Does the `setup.cfg` meet the specified requirements? - Syntax: Is the configuration file syntax correctly used? - Readability: Is the configuration file easy to understand (proper indentation and line breaks)?","solution":"def generate_setup_cfg(): Generates the content of a setup.cfg file for the given project requirements. Returns: str: The content of the setup.cfg configuration file. setup_cfg_content = [build_ext] inplace=1 [bdist_rpm] release=2 packager=Jane Doe doc_files=CHANGELOG.md README.md docs/ return setup_cfg_content.strip() # Example Usage print(generate_setup_cfg())"},{"question":"<|Analysis Begin|> The documentation provided explains an `array` module in Python, which allows for efficient arrays of numeric values, offering a more memory-efficient alternative to lists for a specific set of data types. The `array` module supports various typecodes for different data types and provides numerous operations for array manipulation, including appending elements, converting to and from different data representations, and manipulating the internal representation of the array. Key elements and methods of the `array` class include: - Initialization with a specified typecode. - Common sequence operations such as indexing, slicing, and concatenation. - Methods for converting to and from bytes and other representations. - Methods for counting elements, finding occurrences, and modifying array contents. Given the detailed overview of the `array` module\'s capabilities, a challenging question can be designed to assess a student\'s understanding of array initialization, element manipulation, and data conversion. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Implement a Python function that utilizes the `array` module to manipulate an array of integers, demonstrating your understanding of various array operations and data conversion techniques. **Problem Statement:** Write a function `process_integer_array(input_list: list, additional_elements: list) -> (bytearray, int, list)` that performs the following operations on an array of signed integers: 1. Initialize an array with type code `\'i\'` using the integers from `input_list`. 2. Append all integers from `additional_elements` to the array. 3. Reverse the array. 4. Convert the array to a bytes representation. 5. Count the number of occurrences of the maximum value in the array after reversing. 6. Convert the array back to an ordinary list. The function should return a tuple containing: - The bytes representation of the array. - The count of the maximum value in the array after reversing. - The final list converted from the array. **Input:** - `input_list`: A list of integers. - `additional_elements`: A list of additional integers to append to the array. **Output:** - A tuple `(bytes_representation, max_count, final_list)` where: - `bytes_representation` is a bytearray created from the array. - `max_count` is the number of occurrences of the maximum value in the reversed array. - `final_list` is the list converted from the array after all operations. **Example:** ```python input_list = [1, 2, 3, 4] additional_elements = [5, 6] result = process_integer_array(input_list, additional_elements) # Expected output is a tuple where: # - bytes_representation is the bytes representation of the array array(\'i\', [6, 5, 4, 3, 2, 1]). # - max_count is 1, since the maximum value in the reversed array is 6 and it appears once. # - final_list is [6, 5, 4, 3, 2, 1], which is the array converted back to a list after all operations. ``` **Constraints:** - All elements in `input_list` and `additional_elements` are integers. - The function should handle large inputs efficiently.","solution":"import array def process_integer_array(input_list: list, additional_elements: list) -> (bytearray, int, list): Processes an array of signed integers: initializes, appends elements, reverses, converts to bytes, counts maximum value occurrences, and converts back to a list. Parameters: - input_list: List of integers to initialize the array. - additional_elements: List of integers to append to the array. Returns: A tuple containing: - A bytearray representation of the modified array. - The count of the maximum value in the reversed array. - The final list converted from the array. # Step 1: Initialize the array with type code \'i\' (signed integers) arr = array.array(\'i\', input_list) # Step 2: Append all integers from additional_elements to the array arr.extend(additional_elements) # Step 3: Reverse the array arr.reverse() # Step 4: Convert the array to a bytes representation bytes_representation = arr.tobytes() # Step 5: Count the number of occurrences of the maximum value in the array after reversing max_value = max(arr) max_count = arr.count(max_value) # Step 6: Convert the array back to an ordinary list final_list = arr.tolist() return bytearray(bytes_representation), max_count, final_list"},{"question":"# XML Data Analysis and Transformation Task **Objective:** To design a function that can parse XML data, analyze it to extract specific information, and transform it as required. **Problem Statement:** You are given an XML document representing a collection of books in a library. Each book has various metadata including title, author, genre, price, publish date, and a short description. You need to write a function `process_library_data(xml_data: str) -> str` that does the following: 1. Parses the provided XML data string and extracts information about each book. 2. Calculates the total number of books and the average price of all books. 3. Removes any book from the collection that has a price greater than a given threshold. 4. For each remaining book, if the publish year is more than 10 years ago from the current year, adds an `updated=\\"true\\"` attribute to the book element. 5. Returns the modified XML document as a string. **Function Signature:** ```python def process_library_data(xml_data: str) -> str: pass ``` **Input:** - `xml_data`: A string containing the XML data of the library books. Example XML data: ```xml <library> <book> <title>Book One</title> <author>Author A</author> <genre>Fiction</genre> <price>30.00</price> <publish_date>2010-09-01</publish_date> <description>Description for Book One</description> </book> <book> <title>Book Two</title> <author>Author B</author> <genre>Nonfiction</genre> <price>50.00</price> <publish_date>2015-06-15</publish_date> <description>Description for Book Two</description> </book> <!-- More book elements --> </library> ``` **Output:** - A string containing the modified XML data after applying the transformations. **Constraints:** - Use `xml.etree.ElementTree` for parsing and modifying the XML data. - Assume valid XML data format. - The threshold price for removing a book will be given by the user. **Example:** ```python xml_data = <library> <book> <title>Book One</title> <author>Author A</author> <genre>Fiction</genre> <price>30.00</price> <publish_date>2010-09-01</publish_date> <description>Description for Book One</description> </book> <book> <title>Book Two</title> <author>Author B</author> <genre>Nonfiction</genre> <price>50.00</price> <publish_date>2015-06-15</publish_date> <description>Description for Book Two</description> </book> </library> print(process_library_data(xml_data)) ``` The function call should return the modified XML data string with transformations applied based on current date and price threshold. **Note:** Make sure your code is efficient and handles large XML data gracefully. Use relevant `xml.etree.ElementTree` methods for parsing, modifying, and generating XML in the required format.","solution":"import xml.etree.ElementTree as ET from datetime import datetime def process_library_data(xml_data: str, price_threshold: float) -> str: current_year = datetime.now().year tree = ET.ElementTree(ET.fromstring(xml_data)) root = tree.getroot() total_books = 0 total_price = 0.0 books_to_remove = [] for book in root.findall(\'book\'): total_books += 1 price = float(book.find(\'price\').text) total_price += price if price > price_threshold: books_to_remove.append(book) continue publish_date = book.find(\'publish_date\').text publish_year = int(publish_date.split(\'-\')[0]) if current_year - publish_year > 10: book.set(\'updated\', \'true\') average_price = total_price / total_books if total_books > 0 else 0.0 for book in books_to_remove: root.remove(book) result_xml = ET.tostring(root, encoding=\'unicode\') return result_xml"},{"question":"**Objective**: Demonstrate understanding of shallow and deep copying, and implement custom copy behaviors in Python classes using the `copy` module. Problem Statement You are required to implement a `Company` class that represents a company entity containing a list of `Employee` objects. The `Employee` class contains details about individual employees, such as name and position. For the `Company` class: - Implement custom shallow and deep copy operations. - Ensure that the shallow copy creates a new `Company` object, but the list of `Employee` objects references the same original `Employee` instances. - Ensure that the deep copy creates a new `Company` object, along with completely independent copies of all `Employee` objects it contains. The class definitions and copy operations should adhere to the following method signatures: ```python import copy class Employee: def __init__(self, name: str, position: str): self.name = name self.position = position class Company: def __init__(self, name: str, employees: list): self.name = name self.employees = employees def __copy__(self): Implement the shallow copy operation. pass def __deepcopy__(self, memo): Implement the deep copy operation. pass ``` Input and Output - **Input**: You do not need to handle input from users. Instead, your primary task is to correctly implement the `Company` class with custom copy methods. - **Output**: Proper functionality should be evident through code demonstration, such as: ```python # Sample usage emp1 = Employee(\\"Alice\\", \\"Engineer\\") emp2 = Employee(\\"Bob\\", \\"Manager\\") company1 = Company(\\"TechCorp\\", [emp1, emp2]) # Perform shallow copy company_shallow_copy = copy.copy(company1) # Perform deep copy company_deep_copy = copy.deepcopy(company1) # Tests to ensure correctness: # 1. company_shallow_copy and company1 should refer to different Company objects, # but contain the same Employee objects (identity comparison). # 2. company_deep_copy should be a different Company object containing new copies of Employee objects (deep comparison). ``` Constraints - Do not change the `Employee` class definition. - Focus on implementing custom shallow and deep copy mechanisms within the `Company` class. Performance - Ensure that the deep copy operation handles the memo dictionary correctly to avoid excessive recursion or duplication of objects already copied. Good luck, and may your copies be flawless!","solution":"import copy class Employee: def __init__(self, name: str, position: str): self.name = name self.position = position class Company: def __init__(self, name: str, employees: list): self.name = name self.employees = employees def __copy__(self): Implement the shallow copy operation. new_company = Company(self.name, self.employees) return new_company def __deepcopy__(self, memo): Implement the deep copy operation. new_employees = [copy.deepcopy(employee, memo) for employee in self.employees] new_company = Company(self.name, new_employees) return new_company"},{"question":"# Advanced Sorting Challenge Objective: You are required to implement a sorting function that sorts a list of custom objects based on multiple attributes. The function should use stable sorting to maintain the order of items with equal keys. Instructions: 1. Define a `Person` class with the following attributes: - `name` (string) - `age` (integer) - `height` (float) 2. Implement the `__repr__` method in the `Person` class to return a string representation in the format: `Person(name, age, height)`. 3. Create a function `complex_sort(persons: List[Person]) -> List[Person]` that: - Accepts a list of `Person` objects. - Sorts the list primarily by `height` in descending order. - If two persons have the same height, sort by `age` in ascending order. - If two persons have the same height and age, maintain their original order (using stable sort). 4. Write a small test suite within the same file to verify that your function works as expected. The test suite should cover: - Basic functionality with a diverse list of `Person` objects. - Edge cases such as an empty list, single element list, and multiple persons with the same attributes. Constraints: - You may assume that the `name` attribute is always unique. - Performance requirements: Optimize your solution to minimize the number of comparisons and ensure it operates efficiently even with large lists. Example: ```python class Person: def __init__(self, name: str, age: int, height: float): self.name = name self.age = age self.height = height def __repr__(self): return f\'Person({self.name}, {self.age}, {self.height})\' def complex_sort(persons: List[Person]) -> List[Person]: # Implement the sorting logic here # Test suite if __name__ == \\"__main__\\": p1 = Person(\\"Alice\\", 30, 5.5) p2 = Person(\\"Bob\\", 25, 6.0) p3 = Person(\\"Charlie\\", 30, 5.5) p4 = Person(\\"Diana\\", 25, 6.0) p5 = Person(\\"Eve\\", 35, 5.0) persons = [p1, p2, p3, p4, p5] sorted_persons = complex_sort(persons) for person in sorted_persons: print(person) ``` In the output, the persons should be sorted first by descending height, then by ascending age, and finally maintain the original relative order in cases of ties.","solution":"from typing import List class Person: def __init__(self, name: str, age: int, height: float): self.name = name self.age = age self.height = height def __repr__(self): return f\'Person({self.name}, {self.age}, {self.height})\' def complex_sort(persons: List[Person]) -> List[Person]: return sorted(persons, key=lambda p: (-p.height, p.age))"},{"question":"PyTorch JIT Compilation Objective You are tasked with demonstrating your understanding of PyTorch\'s Just-In-Time (JIT) compilation utilities, specifically focusing on scripting, tracing, and model persistence. Instructions 1. **JIT Scripting**: - Define a simple neural network model using PyTorch. - Write a function that accepts this model and applies JIT scripting to compile it into optimized TorchScript. 2. **JIT Tracing**: - Create a function that performs a sample inference using the original model\'s forward pass. - Write another function that applies JIT tracing to the model based on the sample inference, converting it into a traced TorchScript form. 3. **Model Persistence**: - Write functions to save and load the JIT-scripted and JIT-traced models to/from disk. Detailed Requirements 1. **Neural Network Model Construction**: - Define a simple three-layer feedforward neural network using `nn.Module`. 2. **JIT Scripting**: - Function **scripted_model(model: nn.Module) -> torch.jit.ScriptModule** - Input: A PyTorch neural network model (an instance of `nn.Module`). - Output: A JIT scripted version of the model. 3. **JIT Tracing**: - Function **sample_inference(model: nn.Module, input_tensor: torch.Tensor) -> torch.Tensor** - Input: - A PyTorch neural network model (an instance of `nn.Module`). - A sample input tensor for the model\'s forward method. - Output: The output tensor from the model\'s forward pass. - Function **traced_model(model: nn.Module, example_input: torch.Tensor) -> torch.jit.ScriptModule** - Input: - A PyTorch neural network model (an instance of `nn.Module`). - An example input tensor (the same as used in sample inference). - Output: A JIT traced version of the model. 4. **Model Persistence**: - Function **save_model(model: torch.jit.ScriptModule, file_path: str) -> None** - Input: - A JIT-compiled TorchScript model (the scripted or traced model). - The file path where the model should be saved. - Output: None (the model file should be saved to disk). - Function **load_model(file_path: str) -> torch.jit.ScriptModule** - Input: The file path from where to load a JIT-compiled TorchScript model. - Output: The loaded JIT-compiled TorchScript model. Constraints and Limitations: - You should use simple tensor inputs for the example input tensor (e.g., random tensors of appropriate dimensions). - Clearly handle exceptions and edge cases, such as invalid file paths for saving/loading models. Expected Performance: - The code should compile and run without errors. - Properly demonstrate understanding by creating, scripting, tracing, saving, and loading the models correctly. Example (for reference only): ```python import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.Linear(20, 5) self.layer3 = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = torch.relu(self.layer2(x)) x = self.layer3(x) return x # Define your functions based on the requirements above # Ensure to test the functions to demonstrate proper functionality ``` Submission: - Write and test all functions as described. - Submit a runnable script or Jupyter notebook that includes all the required implementations and example usage.","solution":"import torch import torch.nn as nn import torch.jit # Define a simple three-layer feedforward neural network using `nn.Module` class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.Linear(20, 5) self.layer3 = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.layer1(x)) x = torch.relu(self.layer2(x)) x = self.layer3(x) return x # JIT Scripting function def scripted_model(model: nn.Module) -> torch.jit.ScriptModule: Returns a JIT scripted version of the model. return torch.jit.script(model) # Sample inference function def sample_inference(model: nn.Module, input_tensor: torch.Tensor) -> torch.Tensor: Returns the output tensor from the model\'s forward pass. return model(input_tensor) # JIT Tracing function def traced_model(model: nn.Module, example_input: torch.Tensor) -> torch.jit.ScriptModule: Returns a JIT traced version of the model based on the sample input. return torch.jit.trace(model, example_input) # Function to save the JIT-compiled TorchScript model to disk def save_model(model: torch.jit.ScriptModule, file_path: str) -> None: Saves the JIT-compiled TorchScript model to the specified file path. model.save(file_path) # Function to load a JIT-compiled TorchScript model from disk def load_model(file_path: str) -> torch.jit.ScriptModule: Loads the JIT-compiled TorchScript model from the specified file path. return torch.jit.load(file_path)"},{"question":"# Question: Bytecode Analysis and Manipulation using the `dis` Module Python\'s `dis` module provides tools for disassembling and analyzing CPython bytecode. In this assignment, you will write a function that analyzes a given Python function\'s bytecode and provides specific insights based on the instructions used within the bytecode. Task Write a function `analyze_bytecode(function)` that takes as input a Python function object and returns a dictionary with the following keys and their corresponding values: - **\\"Instruction Count\\"**: Total number of bytecode instructions. - **\\"Most Common Instruction\\"**: The most frequently occurring bytecode instruction (opname). - **\\"Stack Effects\\"**: A list of tuples containing each instruction\'s opname and its net effect on the stack (positive for push, negative for pop, zero for no effect). Input - `function`: A Python function object whose bytecode needs to be analyzed. Output - A dictionary with the following keys: - `\\"Instruction Count\\"`: An integer representing the total number of bytecode instructions. - `\\"Most Common Instruction\\"`: A string representing the opname of the most common instruction. - `\\"Stack Effects\\"`: A list of tuples, where each tuple contains: - The opname of the instruction. - An integer representing the instruction\'s net effect on the stack. Constraints - You should use the `dis` module for disassembling and analyzing the bytecode. - Assume the input function is a valid Python function. Example ```python import dis def sample_function(x): return x + 1 result = analyze_bytecode(sample_function) print(result) # Possible output (values may vary): # { # \\"Instruction Count\\": 4, # \\"Most Common Instruction\\": \\"LOAD_FAST\\", # \\"Stack Effects\\": [ # (\\"LOAD_FAST\\", 1), # (\\"LOAD_CONST\\", 1), # (\\"BINARY_ADD\\", -1), # (\\"RETURN_VALUE\\", 0) # ] # } ``` Notes - Use `dis.Bytecode` and `dis.stack_effect` functions as needed to compute the required information. - Consider the frequency of instructions to determine the \\"Most Common Instruction\\" correctly. - Ensure zero effects (additional details) are handled accurately for visual comprehension.","solution":"import dis from collections import Counter def analyze_bytecode(function): Analyzes the bytecode of a given function and returns specific details. Parameters: - function: a Python function object to analyze. Returns: - A dictionary with the following keys and values: - \\"Instruction Count\\": Total number of bytecode instructions. - \\"Most Common Instruction\\": The most frequently occurring bytecode instruction (opname). - \\"Stack Effects\\": A list of tuples with each instruction\'s opname and its net effect on the stack. bytecode = dis.Bytecode(function) instructions = list(bytecode) instruction_count = len(instructions) instruction_opnames = [instr.opname for instr in instructions] most_common_instruction = Counter(instruction_opnames).most_common(1)[0][0] stack_effects = [(instr.opname, dis.stack_effect(instr.opcode, instr.arg)) for instr in instructions] return { \\"Instruction Count\\": instruction_count, \\"Most Common Instruction\\": most_common_instruction, \\"Stack Effects\\": stack_effects }"},{"question":"<|Analysis Begin|> The provided documentation explains the abstract base classes available in the `collections.abc` module. These classes enable you to define and verify the interfaces for various types of containers, iterables, sequences, mappings, sets, and more. They support inheritance, registration as virtual subclasses, and direct method presence tests for interface compliance. These abstract base classes (ABCs) help in developing more structured and consistent APIs. There is detailed information on classes available in this module like `Container`, `Iterable`, `Sequence`, `Set`, `Mapping`, and others, including the methods that they should implement either abstract or mixins. A coding question can focus on: 1. Implementing a custom class using these ABCs and overriding certain methods. 2. Testing interface compliance of various class implementations. 3. Designing an efficient data structure by leveraging the mixin methods provided by these ABCs. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** To assess your understanding of Python\'s abstract base classes and their usage in defining interfaces and data structures. **Problem Statement:** You need to implement a custom class `FixedSizeDict` that behaves like a dictionary but has a fixed maximum size. If the number of elements exceeds this fixed size, new elements should replace the oldest ones added. This data structure will be useful in scenarios where you need to maintain a fixed-size cache or store a limited number of items. **Requirements:** 1. Your `FixedSizeDict` class should be a subclass of `collections.abc.MutableMapping`. 2. Implement the following abstract and required methods: - `__getitem__(self, key)` - `__setitem__(self, key, value)` - `__delitem__(self, key)` - `__iter__(self)` - `__len__(self)` 3. Implement any additional methods you feel are necessary, such as for initialization. 4. Add a method `set_max_size(self, size)` to dynamically change the maximum allowed size of the dictionary. **Constraints:** - You may use Python built-in data types and standard library. - The class should handle concurrency (thread-safety) if two threads try to add items simultaneously. - You must ensure that the class is efficient. Aim for time complexity of O(1) for add, remove, and access operations, if possible. **Example Usage:** ```python # Define an instance with a maximum size of 3 fixed_dict = FixedSizeDict(max_size=3) fixed_dict[\'a\'] = 1 fixed_dict[\'b\'] = 2 fixed_dict[\'c\'] = 3 print(fixed_dict) # Should print {\'a\': 1, \'b\': 2, \'c\': 3} # Adding a new element should evict the oldest one fixed_dict[\'d\'] = 4 print(fixed_dict) # Should print {\'b\': 2, \'c\': 3, \'d\': 4} # Accessing an element should not affect the order of the remaining elements print(fixed_dict[\'c\']) # Should return 3 print(fixed_dict) # Should print {\'b\': 2, \'c\': 3, \'d\': 4} # Dynamically change the maximum size fixed_dict.set_max_size(2) print(fixed_dict) # Should print {\'d\': 4, \'c\': 3} ``` **Notes:** - Be sure to handle edge cases such as setting a max_size of 0 or 1, removing non-existent keys, and so on. - Assume all keys are hashable and all values are any type. - Include appropriate docstrings for your class and methods. Complete the class implementation below: ```python from collections.abc import MutableMapping import threading class FixedSizeDict(MutableMapping): def __init__(self, max_size): # Initialize your data structure here pass def __getitem__(self, key): # Implementation code pass def __setitem__(self, key, value): # Implementation code pass def __delitem__(self, key): # Implementation code pass def __iter__(self): # Implementation code pass def __len__(self): # Implementation code pass def set_max_size(self, size): # Implementation code pass # Add any other methods you need # You can add code here to test your implementation. ``` Good luck!","solution":"from collections.abc import MutableMapping import threading from collections import OrderedDict class FixedSizeDict(MutableMapping): def __init__(self, max_size): Initialize a fixed size dictionary. :param max_size: Maximum size of the dictionary. if max_size < 1: raise ValueError(\\"max_size must be at least 1\\") self.max_size = max_size self.data = OrderedDict() self.lock = threading.Lock() def __getitem__(self, key): with self.lock: if key in self.data: return self.data[key] else: raise KeyError(key) def __setitem__(self, key, value): with self.lock: # If the key already exists, we simply update the value if key in self.data: del self.data[key] elif len(self.data) >= self.max_size: # Remove the oldest item self.data.popitem(last=False) self.data[key] = value def __delitem__(self, key): with self.lock: if key in self.data: del self.data[key] else: raise KeyError(key) def __iter__(self): with self.lock: return iter(self.data) def __len__(self): with self.lock: return len(self.data) def set_max_size(self, size): if size < 1: raise ValueError(\\"max_size must be at least 1\\") with self.lock: self.max_size = size # Remove extra elements if new max_size is smaller while len(self.data) > self.max_size: self.data.popitem(last=False) def __repr__(self): return f\\"{self.__class__.__name__}({self.data}, max_size={self.max_size})\\""},{"question":"# HTML Entities Conversion Functions You are tasked with implementing two functions to work with HTML character entities using the `html.entities` module\'s dictionaries. 1. **Function 1: `html_name_to_unicode`** - **Input**: A string containing HTML entity names (e.g., `\\"&lt;&gt;&amp;\\"`) - **Output**: A string where all HTML entity names have been converted to their corresponding Unicode characters (e.g., `\\"<>&\\"`) - **Details**: * You should use the `html.entities.html5` dictionary for the conversion. * Consider and handle both cases where entity names might include or exclude the trailing semicolon. * If an entity name does not exist in the dictionary, leave it unchanged in the output. 2. **Function 2: `unicode_to_html_name`** - **Input**: A string containing Unicode characters (e.g., `\\"<>&\\"`) - **Output**: A string where specified Unicode characters have been converted to their corresponding HTML entity names (e.g., `\\"&lt;&gt;&amp;\\"`) - **Details**: * You should use the `html.entities.codepoint2name` dictionary for the conversion. * Only convert characters that have appropriate HTML entity names defined in the dictionary. * If a character does not have a corresponding HTML entity name, leave it unchanged in the output. # Example Usage ```python print(html_name_to_unicode(\\"&lt;&gt;&amp;\\")) # Output: \\"<>&\\" print(unicode_to_html_name(\\"<>&\\")) # Output: \\"&lt;&gt;&amp;\\" ``` # Constraints - The input strings will be non-empty and contain valid Unicode characters. - Performance is not a primary concern, but the implementation should handle typical length strings efficiently. - You may assume that input strings do not contain malformed entity sequences. Implement these functions to demonstrate your comprehension of the `html.entities` module and your ability to manipulate string data in Python.","solution":"import html.entities def html_name_to_unicode(s): Converts HTML entity names in a string to their corresponding Unicode characters. import re # Regex to find HTML entities pattern = re.compile(r\'&(#?x?[0-9a-zA-Z]+);?\') def replace_entity(match): entity = match.group(1) if entity.startswith(\'#x\'): # Handle hexadecimal entities return chr(int(entity[2:], 16)) elif entity.startswith(\'#\'): # Handle decimal entities return chr(int(entity[1:])) elif entity in html.entities.html5: # Handle named entities return html.entities.html5[entity] else: # If entity is not recognized, return it unchanged return match.group(0) return pattern.sub(replace_entity, s) def unicode_to_html_name(s): Converts Unicode characters in a string to their corresponding HTML entity names. result = [] for char in s: codepoint = ord(char) if codepoint in html.entities.codepoint2name: result.append(\'&{};\'.format(html.entities.codepoint2name[codepoint])) else: result.append(char) return \'\'.join(result)"},{"question":"# Advanced Logging Configuration in Python **Objective:** Your task is to create a custom logging configuration using Python’s logging module. This includes defining custom loggers, handlers, formatters, and filters. The focus will be on creating a well-structured and functional logging system. **Requirements:** 1. **Loggers**: - Create a logger named `app` with a log level of `DEBUG`. - Create a child logger named `app.database` which logs messages with a severity of `INFO` and above. 2. **Handlers**: - Add a console handler to `app` logger that outputs log messages to the console. - Add a file handler to `app.database` logger that writes log messages to a file named `database.log`. 3. **Formatters**: - Create a formatter with the format: `\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'`. 4. **Filters**: - Create a custom filter named `NoHealthCheckFilter` which filters out log messages that contain the keyword `\'health_check\'`. 5. **Integration and Usage**: - Integrate the handlers, formatters, and filters appropriately within the loggers. - Demonstrate logging various messages from both `app` and `app.database` loggers. - Apply the `NoHealthCheckFilter` filter to the file handler of `app.database`. **Constraints:** 1. The console handler should only display logs that have a severity of `WARNING` and above. 2. The file handler should write logs with all severities but filter out messages containing the keyword `\'health_check\'`. **Function Signature:** ```python def configure_logging(): import logging # Create loggers app_logger = logging.getLogger(\'app\') db_logger = logging.getLogger(\'app.database\') # Set log levels app_logger.setLevel(logging.DEBUG) db_logger.setLevel(logging.INFO) # Create handlers console_handler = logging.StreamHandler() file_handler = logging.FileHandler(\'database.log\') # Set console handler level console_handler.setLevel(logging.WARNING) # Create formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # Add formatter to handlers console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Define custom filter class NoHealthCheckFilter(logging.Filter): def filter(self, record): return \'health_check\' not in record.getMessage() # Add filter to file handler file_handler.addFilter(NoHealthCheckFilter()) # Add handlers to loggers app_logger.addHandler(console_handler) db_logger.addHandler(file_handler) # Demonstrate logging app_logger.debug(\'Debugging application\') app_logger.warning(\'Application warning\') db_logger.info(\'Database connected\') db_logger.warning(\'Database warning\') db_logger.info(\'health_check passed\') def main(): configure_logging() # Further logging operations can be put here if needed if __name__ == \'__main__\': main() ``` **Expected Output:** 1. Logs on the console should only show WARNING and above messages. 2. The `database.log` file should contain all logs excluding messages that contain the keyword \'health_check\'. Ensure that comments and documentation within the code explain each section and the role it plays in the logging setup.","solution":"import logging def configure_logging(): # Create loggers app_logger = logging.getLogger(\'app\') db_logger = logging.getLogger(\'app.database\') # Set log levels app_logger.setLevel(logging.DEBUG) db_logger.setLevel(logging.INFO) # Create handlers console_handler = logging.StreamHandler() file_handler = logging.FileHandler(\'database.log\') # Set console handler level console_handler.setLevel(logging.WARNING) # Define formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # Add formatter to handlers console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Define custom filter class NoHealthCheckFilter(logging.Filter): def filter(self, record): return \'health_check\' not in record.getMessage() # Add filter to file handler file_handler.addFilter(NoHealthCheckFilter()) # Add handlers to loggers app_logger.addHandler(console_handler) db_logger.addHandler(file_handler) # Demonstrate logging app_logger.debug(\'Debugging application\') app_logger.warning(\'Application warning\') db_logger.info(\'Database connected\') db_logger.warning(\'Database warning\') db_logger.info(\'health_check passed\') # Should be filtered out from file def main(): configure_logging() # Further logging operations can be put here if needed if __name__ == \'__main__\': main()"},{"question":"**Advanced PyTorch Coding Assessment** You are required to implement a custom neural network in PyTorch using TorchScript. This task will assess your understanding of PyTorch\'s core concepts and its unique TorchScript subset. Specifically, you must manage limitations such as lack of coroutine support, advanced slicing, unpacking, and some operators. # Problem Statement: Implement a custom convolutional neural network (CNN) in PyTorch, script it with TorchScript, and perform a simple forward pass with a given input tensor. # Requirements: 1. **Define a custom CNN class** inheriting from `torch.nn.Module`. 2. **Include initialization for the CNN layers**, with at least: - Two convolutional layers (`torch.nn.Conv2d`). - Two fully connected layers (`torch.nn.Linear`). 3. **Define the forward pass** leveraging these layers. 4. **Script your model using TorchScript**. 5. **Perform a forward pass** with a random input tensor of shape `[1, 3, 64, 64]`. # Additional Constraints: - You are **not allowed** to use any unsupported features listed in the documentation provided (e.g., `async`, `__new__`, `__del__`, context managers like `with`, and certain unsupported operators). - Ensure your code is TorchScript compatible, i.e., can be effectively converted using `torch.jit.script`. # Input: - No direct input is provided. Your task includes creating a random tensor of shape `[1, 3, 64, 64]`. # Output: - Print the shape of the output tensor after the forward pass. ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.jit as jit # Define your custom CNN class class CustomCNN(nn.Module): def __init__(self): super(CustomCNN, self).__init__() # Define layers self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(32 * 16 * 16, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): # Define forward pass x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2) x = x.view(x.size(0), -1) # Flatten the tensor x = F.relu(self.fc1(x)) x = self.fc2(x) return x # Create an instance of the CNN model = CustomCNN() # Script the model scripted_model = jit.script(model) # Define a random input tensor input_tensor = torch.randn(1, 3, 64, 64) # Perform a forward pass output_tensor = scripted_model(input_tensor) # Print the shape of the output tensor print(output_tensor.shape) ``` # Expected Output: ``` torch.Size([1, 10]) ``` # Explanation: - **CustomCNN Class**: This class defines the structure of the CNN with convolutional and fully connected layers. - **Forward method**: This method implements the forward pass using ReLU activation functions and max pooling. - **TorchScript Usage**: The `torch.jit.script` function is used to convert the PyTorch model to TorchScript. - **Random Input Tensor**: A tensor of shape `[1, 3, 64, 64]` simulates a batch of one image with three color channels and 64x64 dimensions. - **Output Shape**: The output tensor\'s shape verifies the network\'s operations, expected to be `[1, 10]` for a batch size of 1 and ten output classes.","solution":"import torch import torch.nn as nn import torch.nn.functional as F import torch.jit as jit # Define your custom CNN class class CustomCNN(nn.Module): def __init__(self): super(CustomCNN, self).__init__() # Define layers self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(32 * 16 * 16, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): # Define forward pass x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2) x = x.view(x.size(0), -1) # Flatten the tensor x = F.relu(self.fc1(x)) x = self.fc2(x) return x # Create an instance of the CNN model = CustomCNN() # Script the model scripted_model = jit.script(model) # Define a random input tensor input_tensor = torch.randn(1, 3, 64, 64) # Perform a forward pass output_tensor = scripted_model(input_tensor) # Print the shape of the output tensor print(output_tensor.shape) def get_output_shape(input_tensor): Apply the scripted model on the input tensor and return the output shape. return scripted_model(input_tensor).shape"},{"question":"# Question: You have been provided with a dataset containing scores of different machine learning models on various tasks. Your objective is to visualize this dataset using the seaborn library to gain insights into the performance of these models. **Dataset**: Load the dataset \'glue\' using the `load_dataset` function from the seaborn library. The dataset contains information about different models, encoders, and their scores on several tasks. Tasks: 1. **Data Preparation:** - Load the \'glue\' dataset. - Pivot the dataset so that \'Model\' and \'Encoder\' are the indices, \'Task\' is the column, and \'Score\' is the values. - Add a new column \'Average\' to the dataset representing the average score of each model, rounded to one decimal place. - Sort the dataset in descending order based on the \'Average\' score. 2. **Visualization:** - Create a horizontal bar plot showing the \'Average\' score for each \'Model\'. - Add text annotations to the bars displaying the average scores using white color, aligning the text to the right. - Customize the text to have a slight offset for better readability. 3. **Advanced Visualization:** - Create a dot plot to compare the \'SST-2\' and \'MRPC\' scores for each model, colored by \'Encoder\'. - Add text annotations above the dots to show the model names. - Further enhance the plot by setting horizontal alignment differently for LSTM and Transformer encoders (left for LSTM and right for Transformer). **Constraints:** - Do not use any other visualization libraries except for seaborn. - Ensure your code is optimized and runs efficiently on large datasets. **Input:** - No explicit input required; the dataset is loaded internally. **Output:** - Two plots generated according to the specifications. Example: ```python import seaborn.objects as so from seaborn import load_dataset # Task 1: Data Preparation glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Task 2: Visualization ( so.Plot(glue.reset_index(), x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Task 3: Advanced Visualization ( so.Plot(glue.reset_index(), x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ) ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Task 1: Data Preparation def prepare_data(): glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) return glue # Task 2: Visualization def plot_average_scores(glue): plot = ( so.Plot(glue.reset_index(), x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) return plot # Task 3: Advanced Visualization def plot_comparison(glue): plot = ( so.Plot(glue.reset_index(), x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ) return plot"},{"question":"# Distributed Multiprocessing with Error Handling in PyTorch Problem Statement You are given a PyTorch-based distributed processing environment. Occasionally, distributed processing tasks may fail due to various issues, and it is crucial to handle these errors gracefully to ensure robust program execution. Your task is to implement a function that runs a distributed processing job across multiple child processes and properly handles errors using the provided error handling classes and methods from the `torch.distributed.elastic.multiprocessing.errors` module. Function Signature: ```python def run_distributed_process(job_function: Callable, num_processes: int) -> List[Any]: pass ``` Inputs: - `job_function: Callable`: A function that represents the task each process will execute. This function may potentially raise errors. - `num_processes: int`: Number of processes to spawn for the distributed job. Outputs: - `List[Any]`: A list of results from each of the processes. If a process fails, return the error message as part of the list. Requirements: 1. **Handle Errors**: Use the provided classes (`ChildFailedError`, `ErrorHandler`, `ProcessFailure`) to appropriately catch and handle errors in child processes. 2. **Run Processes in Parallel**: Ensure that the job function is executed in a truly parallel fashion. 3. **Return Results**: Collect and return results from all child processes, including any error messages if a process fails. Constraints: - Assume `num_processes` will be a positive integer no greater than 8. Example: ```python def example_job(x): if x % 2 == 0: return x * 2 else: raise ValueError(\\"Odd number encountered\\") result = run_distributed_process(example_job, 4) print(result) # Possible output: [0, \'Error: Odd number encountered\', 4, \'Error: Odd number encountered\'] ``` # Hints: 1. `torch.multiprocessing` can be used to create and manage child processes. 2. Look into the `torch.distributed.elastic.multiprocessing.errors` module for handling errors and recording error contexts. 3. You may need to use a queue or similar mechanism to collect results from child processes. This problem assesses your understanding of both PyTorch multiprocessing as well as robust error handling mechanisms in distributed systems.","solution":"import torch.multiprocessing as mp from typing import Callable, List, Any from torch.distributed.elastic.multiprocessing.errors import ChildFailedError, ProcessFailure def process_wrapper(job_function, input, queue, process_id): try: result = job_function(input) queue.put((process_id, result)) except Exception as e: queue.put((process_id, f\\"Error: {str(e)}\\")) def run_distributed_process(job_function: Callable, num_processes: int) -> List[Any]: result_queue = mp.Queue() processes = [] for process_id in range(num_processes): p = mp.Process(target=process_wrapper, args=(job_function, process_id, result_queue, process_id)) processes.append(p) p.start() results = [None] * num_processes for _ in range(num_processes): process_id, result = result_queue.get() results[process_id] = result for p in processes: p.join() return results"},{"question":"You are provided with the well-known `penguins` dataset which is available through `seaborn`. This dataset includes measurements for penguin species observed on various islands. You need to create visualizations using seaborn\'s new `seaborn.objects` interface. # Dataset Description: - `species`: Penguin species (Chinstrap, Adélie, Gentoo). - `island`: Island name (Dream, Torgersen, Biscoe). - `bill_length_mm`: Bill length of the penguin in mm. - `bill_depth_mm`: Bill depth of the penguin in mm. - `flipper_length_mm`: Flipper length of the penguin in mm. - `body_mass_g`: Body mass of the penguin in grams. - `sex`: Sex of the penguin (Male, Female). # Task: 1. Load the `penguins` dataset using `seaborn.load_dataset`. 2. Create a bar plot using `seaborn.objects` that shows the count of each species. 3. Create another bar plot that shows the count of each species separated by island, with counts further grouped by the sex of the penguins (using `so.Dodge()` to separate the bars by sex). # Function Signature: ```python def plot_penguin_data(): pass ``` # Expected Output: The function `plot_penguin_data` should create and display the following plots: 1. A bar plot showing the count of each species. 2. A bar plot showing the count of species on each island, separated by the sex of the penguins (using `so.Dodge()`). # Constraints: - Use `seaborn.objects` for plotting. - Ensure the plots are labeled appropriately. # Example Usage: ```python plot_penguin_data() ``` This should result in two plots being displayed.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def plot_penguin_data(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Plot 1: Bar Plot showing the count of each species plot1 = ( so.Plot(penguins, x=\'species\') .add(so.Bars(), so.Count()) .label(x=\'Species\', y=\'Count\', title=\'Count of Each Penguin Species\') ) plot1.show() # Plot 2: Bar Plot showing the count of species on each island, separated by sex plot2 = ( so.Plot(penguins, x=\'species\', color=\'sex\') .add(so.Bars(), so.Count(), so.Dodge()) .label(x=\'Species\', y=\'Count\', title=\'Count of Species by Island and Sex\') ) plot2.facet(\'island\') plot2.show() plt.show() # To display the plots"},{"question":"Objective: Demonstrate your understanding of pandas plotting capabilities by creating various visual representations of a given dataset. You will need to implement several functions to generate specific types of plots, demonstrating your ability to work with both simple and complex plotting functions in pandas. Dataset: You will use a randomly generated dataset for this task. The dataset consists of three DataFrames, each containing 1000 rows and multiple columns of numerical data. Functions to Implement: 1. **plot_basic_line(df: pd.DataFrame, column: str) -> plt.Figure** - **Description**: Create a basic line plot for the specified column in the DataFrame. - **Input**: - `df` (pd.DataFrame): The DataFrame containing the data. - `column` (str): The column to plot. - **Output**: A `matplotlib` figure containing the line plot. 2. **plot_histogram(df: pd.DataFrame, columns: List[str]) -> plt.Figure** - **Description**: Create overlapping histograms for the specified columns in the DataFrame. - **Input**: - `df` (pd.DataFrame): The DataFrame containing the data. - `columns` (List[str]): A list of columns to plot. - **Output**: A `matplotlib` figure containing the histograms. 3. **plot_boxplot(df: pd.DataFrame, by: str) -> plt.Figure** - **Description**: Create a boxplot visualizing the distribution of values within each column of the DataFrame, grouped by a specified column. - **Input**: - `df` (pd.DataFrame): The DataFrame containing the data. - `by` (str): The column by which to group the data. - **Output**: A `matplotlib` figure containing the boxplot. 4. **plot_scatter_matrix(df: pd.DataFrame) -> plt.Figure** - **Description**: Create a scatter matrix plot of the DataFrame. - **Input**: - `df` (pd.DataFrame): The DataFrame containing the data. - **Output**: A `matplotlib` figure containing the scatter matrix plot. 5. **plot_parallel_coordinates(df: pd.DataFrame, class_column: str) -> plt.Figure** - **Description**: Create a parallel coordinates plot for the DataFrame, colored by the specified class column. - **Input**: - `df` (pd.DataFrame): The DataFrame containing the data. - `class_column` (str): The column indicating the class labels for coloring the plot. - **Output**: A `matplotlib` figure containing the parallel coordinates plot. 6. **plot_hexbin(df: pd.DataFrame, x: str, y: str, gridsize: int = 25) -> plt.Figure** - **Description**: Create a hexagonal bin plot for the specified columns. - **Input**: - `df` (pd.DataFrame): The DataFrame containing the data. - `x` (str): The column to use for the x-axis. - `y` (str): The column to use for the y-axis. - `gridsize` (int): The number of hexagons in the x-direction (default is 25). - **Output**: A `matplotlib` figure containing the hexbin plot. Constraints: - You may assume that the DataFrame `df` always contains valid numerical data for plotting. - Use appropriate titles, labels, and legends to make the plots clear and informative. - Ensure that the code is efficient and follows best practices for handling data and plotting with pandas and matplotlib. Example Usage: ```python import pandas as pd import numpy as np # Generate example data np.random.seed(123456) index = pd.date_range(\\"1/1/2000\\", periods=1000) data1 = pd.DataFrame(np.random.randn(1000, 4), index=index, columns=list(\\"ABCD\\")).cumsum() data2 = pd.DataFrame(np.random.randn(1000, 3), index=index, columns=[\\"X\\", \\"Y\\", \\"Z\\"]).cumsum() data3 = pd.DataFrame(np.random.randn(1000, 5), index=index, columns=list(\\"PQRST\\")).cumsum() data3[\\"Class\\"] = np.random.choice([\\"A\\", \\"B\\", \\"C\\"], size=1000) # Create plots fig1 = plot_basic_line(data1, \\"A\\") fig2 = plot_histogram(data2, [\\"X\\", \\"Y\\", \\"Z\\"]) fig3 = plot_boxplot(data3, by=\\"Class\\") fig4 = plot_scatter_matrix(data1) fig5 = plot_parallel_coordinates(data3, \\"Class\\") fig6 = plot_hexbin(data1, \\"A\\", \\"B\\") # Show plots plt.show() ```","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix, parallel_coordinates def plot_basic_line(df: pd.DataFrame, column: str) -> plt.Figure: Create a basic line plot for the specified column in the DataFrame. fig, ax = plt.subplots() df[column].plot(ax=ax) ax.set_title(f\'Line Plot of {column}\') ax.set_xlabel(\'Index\') ax.set_ylabel(column) return fig def plot_histogram(df: pd.DataFrame, columns: list) -> plt.Figure: Create overlapping histograms for the specified columns in the DataFrame. fig, ax = plt.subplots() for column in columns: df[column].plot(kind=\'hist\', ax=ax, alpha=0.5, bins=30, label=column) ax.set_title(\'Histogram\') ax.set_xlabel(\'Value\') ax.set_ylabel(\'Frequency\') ax.legend() return fig def plot_boxplot(df: pd.DataFrame, by: str) -> plt.Figure: Create a boxplot visualizing the distribution of values within each column of the DataFrame, grouped by a specified column. fig, ax = plt.subplots() df.plot(kind=\'box\', ax=ax, by=by) ax.set_title(\'Boxplot\') ax.set_xlabel(by) ax.set_ylabel(\'Value\') return fig def plot_scatter_matrix(df: pd.DataFrame) -> plt.Figure: Create a scatter matrix plot of the DataFrame. fig, ax = plt.subplots() scatter_matrix(df, ax=ax) plt.suptitle(\'Scatter Matrix\') return fig def plot_parallel_coordinates(df: pd.DataFrame, class_column: str) -> plt.Figure: Create a parallel coordinates plot for the DataFrame, colored by the specified class column. fig, ax = plt.subplots() parallel_coordinates(df, class_column, ax=ax) plt.title(\'Parallel Coordinates\') plt.xlabel(\'Variables\') plt.ylabel(\'Values\') return fig def plot_hexbin(df: pd.DataFrame, x: str, y: str, gridsize: int = 25) -> plt.Figure: Create a hexagonal bin plot for the specified columns. fig, ax = plt.subplots() df.plot.hexbin(x=x, y=y, gridsize=gridsize, ax=ax) plt.title(\'Hexbin Plot\') plt.xlabel(x) plt.ylabel(y) return fig"},{"question":"# Question: You are working on a medical diagnostic tool using a logistic regression classifier to predict the presence of a disease. Your objective is to optimize the model to achieve the best recall rate to minimize the chances of missing any positive cases. In this context, you must tune the decision threshold using scikit-learn\'s `TunedThresholdClassifierCV`. Task: 1. Load a dataset using `make_classification` with 1000 samples, where the class imbalance ratio is 10:90. 2. Train a logistic regression model on this dataset. 3. Use `TunedThresholdClassifierCV` to find the optimum decision threshold that maximizes recall. 4. Evaluate the performance of the tuned model by calculating the recall on the dataset. Instructions: - Use `sklearn.datasets.make_classification` to generate the dataset. - Use `sklearn.linear_model.LogisticRegression` for the logistic regression model. - Use `sklearn.model_selection.TunedThresholdClassifierCV` to tune the decision threshold. - Use `sklearn.metrics.recall_score` to evaluate the model performance. Code Template: ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import recall_score, make_scorer # Step 1: Generate the dataset X, y = make_classification( n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=42 ) # Step 2: Initialize the logistic regression model base_model = LogisticRegression() # Step 3: Define the scorer for recall recall_scorer = make_scorer(recall_score, pos_label=1) # Step 4: Tune the decision threshold using TunedThresholdClassifierCV tuner = TunedThresholdClassifierCV(base_model, scoring=recall_scorer, cv=5) tuner.fit(X, y) # Step 5: Evaluate the performance of the tuned model y_pred = tuner.predict(X) recall = recall_score(y, y_pred, pos_label=1) print(\\"Optimized Recall:\\", recall) print(\\"Best Threshold Found:\\", tuner.best_threshold_) ``` Expected Output: - Print the optimized recall score. - Print the best decision threshold found by the tuner. Feel free to explore different parameters for the `make_classification` function to understand the impact of class imbalance on the decision threshold tuning.","solution":"from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_predict, StratifiedKFold from sklearn.metrics import precision_recall_curve, recall_score import numpy as np class TunedThresholdClassifierCV: def __init__(self, base_model, cv=5, scoring=recall_score): self.base_model = base_model self.cv = cv self.scoring = scoring self.best_threshold_ = 0.5 def fit(self, X, y): cv = StratifiedKFold(n_splits=self.cv) probas = cross_val_predict(self.base_model, X, y, cv=cv, method=\'predict_proba\')[:, 1] precision, recall, thresholds = precision_recall_curve(y, probas) recalls = recall[:-1] precisions = precision[:-1] f1_scores = 2 * recalls * precisions / (recalls + precisions) optimal_idx = np.argmax(recalls) self.best_threshold_ = thresholds[optimal_idx] self.base_model.fit(X, y) def predict(self, X): probas = self.base_model.predict_proba(X)[:, 1] return (probas >= self.best_threshold_).astype(int) # Step 1: Generate the dataset X, y = make_classification( n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=42 ) # Step 2: Initialize the logistic regression model base_model = LogisticRegression() # Step 4: Tune the decision threshold using TunedThresholdClassifierCV tuner = TunedThresholdClassifierCV(base_model, cv=5) tuner.fit(X, y) # Step 5: Evaluate the performance of the tuned model y_pred = tuner.predict(X) recall = recall_score(y, y_pred, pos_label=1) print(\\"Optimized Recall:\\", recall) print(\\"Best Threshold Found:\\", tuner.best_threshold_)"},{"question":"# Advanced XML Parsing with `xml.dom.pulldom` Objective In this exercise, you are required to implement a function that processes an XML document to find and extract specific elements based on given criteria. This will demonstrate your understanding of event-driven XML parsing and node manipulation using the `xml.dom.pulldom` module in Python. Problem Statement You are given an XML document that lists sales items. Each `<item>` element has a `price` attribute. Write a function `extract_high_priced_items(xml_string: str, min_price: int) -> List[str]` that processes the XML string, extracts all items with a price greater than `min_price`, and returns a list of these items as XML strings. Your function should: 1. Parse the provided XML string using the `xml.dom.pulldom` module. 2. Iterate over the events, check for `START_ELEMENT` events with the tag name `item`. 3. For each `<item>` element, compare its `price` attribute with `min_price`. 4. Expand and retrieve the entire `<item>` node if the price exceeds `min_price`. 5. Return a list of XML strings representing these high-priced items. Function Signature ```python from typing import List import xml.dom.pulldom def extract_high_priced_items(xml_string: str, min_price: int) -> List[str]: ``` Input - `xml_string`: A string representing the XML document. - `min_price`: An integer representing the minimum price threshold. Output - A list of strings, where each string is an XML representation of an `<item>` element that has a price greater than `min_price`. Example ```python xml_data = \'\'\' <items> <item id=\\"1\\" price=\\"30\\">Item1</item> <item id=\\"2\\" price=\\"60\\">Item2</item> <item id=\\"3\\" price=\\"45\\">Item3</item> <item id=\\"4\\" price=\\"80\\">Item4</item> </items> \'\'\' print(extract_high_priced_items(xml_data, 50)) ``` Expected output: ```python [ \'<item id=\\"2\\" price=\\"60\\">Item2</item>\', \'<item id=\\"4\\" price=\\"80\\">Item4</item>\' ] ``` Constraints - The XML string provided will be well-formed. - The `price` attribute in `<item>` elements will always be an integer. Notes - Make sure to use `doc.expandNode(node)` to retrieve the entire node content. - Handle the XML parsing and event-driven approach as demonstrated in the `xml.dom.pulldom` module. ```python from typing import List from xml.dom import pulldom def extract_high_priced_items(xml_string: str, min_price: int) -> List[str]: doc = pulldom.parseString(xml_string) high_priced_items = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': if int(node.getAttribute(\'price\')) > min_price: doc.expandNode(node) high_priced_items.append(node.toxml()) return high_priced_items ```","solution":"from typing import List from xml.dom import pulldom def extract_high_priced_items(xml_string: str, min_price: int) -> List[str]: Extracts and returns a list of items with a price greater than min_price as XML strings. Args: - xml_string (str): A string representation of the XML document. - min_price (int): The minimum price threshold for filtering items. Returns: - List[str]: A list of XML strings representing items with price greater than min_price. doc = pulldom.parseString(xml_string) high_priced_items = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'item\': price = int(node.getAttribute(\'price\')) if price > min_price: doc.expandNode(node) high_priced_items.append(node.toxml()) return high_priced_items"},{"question":"**Objective:** You are required to implement a function that creates and manipulates both mutable and immutable sets using the provided APIs. Your function should demonstrate an understanding of the operations that can be performed on sets and frozensets. **Description:** Implement the function `set_operations()` that performs the following steps: 1. Create an empty set and an empty frozenset. 2. Add a list of provided elements to the set. 3. Attempt to add the same elements to the frozenset and handle any errors appropriately. 4. Check if a specific element is in the set and frozenset, returning the results. 5. Remove a specific element from the set and handle the case where the element does not exist. 6. Attempt to remove the same element from the frozenset and handle any errors appropriately. 7. Return the final state of the set and frozenset. **Function Signature:** ```python def set_operations(elements, check_element, remove_element): Args: elements (list): A list of elements to be added to the set and frozenset. check_element: An element to check for presence in the set and frozenset. remove_element: An element to be removed from the set and frozenset. Returns: dict: A dictionary with the following structure: { \\"set\\": (final_set, contains_check_element_in_set, removed_from_set_successfully), \\"frozenset\\": (final_frozenset, contains_check_element_in_frozenset, remove_error) } pass ``` **Constraints:** - `elements` will be a list containing at most 100 elements. - The elements of the list, as well as `check_element` and `remove_element`, will be hashable types. - You must handle any exceptions properly and should not cause the program to crash. **Example Usage:** ```python elements = [1, 2, 3, 4, 5] check_element = 3 remove_element = 2 result = set_operations(elements, check_element, remove_element) print(result) ``` **Expected Output:** The function should return a dictionary providing the final set and frozenset, as well as information on whether the elements were found and removed successfully. For example: ```python { \\"set\\": ({1, 3, 4, 5}, True, True), \\"frozenset\\": (frozenset({1, 2, 3, 4, 5}), True, \\"frozenset is immutable\\") } ``` This question requires the student to demonstrate knowledge of set operations, handle immutable and mutable types correctly, and work with exception handling.","solution":"def set_operations(elements, check_element, remove_element): Args: elements (list): A list of elements to be added to the set and frozenset. check_element: An element to check for presence in the set and frozenset. remove_element: An element to be removed from the set and frozenset. Returns: dict: A dictionary with the following structure: { \\"set\\": (final_set, contains_check_element_in_set, removed_from_set_successfully), \\"frozenset\\": (final_frozenset, contains_check_element_in_frozenset, remove_error) } # Create an empty set and frozenset mutable_set = set() immutable_frozenset = frozenset() # Add elements to mutable set for element in elements: mutable_set.add(element) # Create a frozenset from the elements immutable_frozenset = frozenset(elements) # Check if check_element is in the set and frozenset contains_check_element_in_set = check_element in mutable_set contains_check_element_in_frozenset = check_element in immutable_frozenset # Try removing remove_element from the set removed_from_set_successfully = remove_element in mutable_set if removed_from_set_successfully: mutable_set.remove(remove_element) # Handle errors for frozenset immutability remove_error = None try: # Frozenset does not support remove, but for the sake of demonstration we handle this as an error if remove_element in immutable_frozenset: raise AttributeError(\\"frozenset is immutable\\") except AttributeError as e: remove_error = str(e) return { \\"set\\": (mutable_set, contains_check_element_in_set, removed_from_set_successfully), \\"frozenset\\": (immutable_frozenset, contains_check_element_in_frozenset, remove_error) }"},{"question":"Coding Assessment Question: Wine Classification with scikit-learn In this assignment, you will demonstrate your understanding of scikit-learn by building a classification model using the Wine dataset. You will load the dataset, preprocess it, split it into training and testing sets, build a machine learning model, train it, and evaluate its performance. # Requirements 1. Load the Wine dataset from `sklearn.datasets`. 2. Preprocess the data: - Handle any missing values (if any). - Standardize the data to have zero mean and unit variance. 3. Split the data into training and testing sets (70% training, 30% testing). 4. Build a Random Forest classifier model. 5. Train the model using the training set. 6. Evaluate the model using the testing set with the following metrics: - Accuracy - Precision, Recall, and F1-score for each class. # Specifications Input There are no direct inputs for the function, but you need to load the data within your function. Output Print the following: - Accuracy of the model. - Precision, Recall, and F1-score for each class. Constraints - Use `sklearn` for loading the dataset, preprocessing, model building, training, and evaluation. - Follow standard practices for machine learning model building and evaluation. # Example Here is a template to help you get started: ```python import numpy as np from sklearn.datasets import load_wine from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, classification_report def wine_classification(): # Load the Wine dataset wine_data = load_wine() X = wine_data.data y = wine_data.target # Standardize the dataset scaler = StandardScaler() X = scaler.fit_transform(X) # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Build and train the Random Forest classifier clf = RandomForestClassifier(n_estimators=100, random_state=42) clf.fit(X_train, y_train) # Predict and evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy}\') # Print Precision, Recall, and F1-score report = classification_report(y_test, y_pred, target_names=wine_data.target_names) print(report) # Execute the function wine_classification() ``` In this question, you will demonstrate your ability to use scikit-learn to solve a real-world machine learning problem following best practices.","solution":"import numpy as np from sklearn.datasets import load_wine from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, classification_report def wine_classification(): # Load the Wine dataset wine_data = load_wine() X = wine_data.data y = wine_data.target # Standardize the dataset scaler = StandardScaler() X = scaler.fit_transform(X) # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Build and train the Random Forest classifier clf = RandomForestClassifier(n_estimators=100, random_state=42) clf.fit(X_train, y_train) # Predict and evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy}\') # Print Precision, Recall, and F1-score report = classification_report(y_test, y_pred, target_names=wine_data.target_names) print(report)"},{"question":"**Problem Statement:** You are tasked with creating an interactive drawing program using the `turtle` graphics module. In this task, you will implement a program that allows a user to draw various geometric shapes on the canvas by responding to keyboard events. Your program should support drawing lines, squares, circles, and should clear the screen upon a specific command. # Requirements: 1. The program should use a `turtle` object to draw on the screen. 2. Implement the following keyboard events: - **\\"l\\"**: Draw a straight line. - **\\"s\\"**: Draw a square. - **\\"c\\"**: Draw a circle. - **\\"r\\"**: Clear the screen. - **\\"q\\"**: Quit the program. 3. Use the turtle module\'s built-in methods to handle these actions. 4. Ensure the screen updates appropriately after each event. # Function Signature: You need to implement the following function: ```python def setup_drawing_program(): pass ``` # Description: - Define a function `setup_drawing_program()` that sets up the `turtle` screen, binds the appropriate keys to their respective drawing functions, and runs the turtle\'s main loop to keep the window open for event handling. When the user presses a specific key, the program should respond by drawing the corresponding shape on the canvas or executing the respective action. # Constraints: - Assume the turtle starts at the center of the screen (0, 0) and is facing to the right. - Use default sizes for shapes: - Line Length: 100 units. - Square Side Length: 50 units. - Circle Radius: 30 units. # Example Usage: Upon running the function, the turtle graphics window should pop up, and the user can press \\"l\\" to draw a line, \\"s\\" to draw a square, \\"c\\" to draw a circle, \\"r\\" to clear the screen, and \\"q\\" to quit the program. ```python # Example implementation def draw_line(): turtle.forward(100) def draw_square(): for _ in range(4): turtle.forward(50) turtle.right(90) def draw_circle(): turtle.circle(30) def clear_screen(): turtle.clear() def quit_program(): turtle.bye() def setup_drawing_program(): # Set up turtle screen screen = turtle.Screen() screen.title(\\"Interactive Drawing Program\\") # Set up key bindings screen.onkey(draw_line, \\"l\\") screen.onkey(draw_square, \\"s\\") screen.onkey(draw_circle, \\"c\\") screen.onkey(clear_screen, \\"r\\") screen.onkey(quit_program, \\"q\\") # Listen for key presses screen.listen() # Enter main loop turtle.mainloop() # Run the program if __name__ == \\"__main__\\": setup_drawing_program() ``` Ensure this function runs correctly when executed, responding properly to each key press and performing the specified drawing actions.","solution":"import turtle def draw_line(): turtle.forward(100) def draw_square(): for _ in range(4): turtle.forward(50) turtle.right(90) def draw_circle(): turtle.circle(30) def clear_screen(): turtle.clear() def quit_program(): turtle.bye() def setup_drawing_program(): # Set up turtle screen screen = turtle.Screen() screen.title(\\"Interactive Drawing Program\\") # Set up key bindings screen.onkey(draw_line, \\"l\\") screen.onkey(draw_square, \\"s\\") screen.onkey(draw_circle, \\"c\\") screen.onkey(clear_screen, \\"r\\") screen.onkey(quit_program, \\"q\\") # Listen for key presses screen.listen() # Enter main loop turtle.mainloop() # Run the program if __name__ == \\"__main__\\": setup_drawing_program()"},{"question":"# Advanced Data Visualization with Seaborn\'s FacetGrid Objective Utilize the seaborn `FacetGrid` class to create an advanced, multi-faceted plot to visualize the relationship between various attributes in a dataset. This coding task assesses your ability to effectively use seaborn for complex data visualizations and customize plots as needed. Task Given the `tips` dataset from seaborn\'s in-built datasets, create a faceted grid of scatterplots to visualize the relationship between `total_bill` and `tip` across different days and times, while also splitting by gender (`sex`) and adding a linear regression line. Requirements 1. Load the `tips` dataset using `seaborn.load_dataset`. 2. Initialize a `FacetGrid` with: - `col` as `\\"day\\"`. - `row` as `\\"time\\"`. - `hue` as `\\"sex\\"`. 3. Use the `map_dataframe` method to plot a scatterplot of `total_bill` vs `tip`, and add a linear regression line (`regplot` function) within each facet. 4. Customize the grid: - `height` of each facet to be `4` and `aspect` ratio to be `1`. - Add a legend to distinguish between different genders. - Set axis labels to `Total Bill ()` for x-axis and `Tip ()` for y-axis. - Add titles to each facet as `\\"{col_name} - {row_name}\\"` format. 5. Adjust the layout to ensure there is proper spacing using `tight_layout()`. 6. Save the resulting plot to a file named `\\"tips_facet_grid.png\\"`. Expected Output A plot saved as `\\"tips_facet_grid.png\\"` showing a faceted grid of scatterplots with regression lines, customized axis labels, titles, and a legend. Constraints - Ensure that the seaborn library version is compatible with your implementation. - The plot should be clear and visually informative. Example Solution ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Initialize the FacetGrid g = sns.FacetGrid(tips, row=\\"time\\", col=\\"day\\", hue=\\"sex\\", height=4, aspect=1) # Map the scatterplot along with a regression line g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") g.map_dataframe(sns.regplot, x=\\"total_bill\\", y=\\"tip\\", scatter=False, truncate=False) # Customize the grid g.add_legend() g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") g.set_titles(\\"{col_name} - {row_name}\\") # Adjust the layout g.tight_layout() # Save the plot g.savefig(\\"tips_facet_grid.png\\") ``` Note Ensure to test your resulting plot in a local environment to verify the solution meets all the requirements specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_tips_facet_grid(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Initialize the FacetGrid g = sns.FacetGrid(tips, row=\\"time\\", col=\\"day\\", hue=\\"sex\\", height=4, aspect=1) # Map the scatterplot along with a regression line g.map(sns.scatterplot, \\"total_bill\\", \\"tip\\") g.map(sns.regplot, \\"total_bill\\", \\"tip\\", scatter=False, truncate=False) # Customize the grid g.add_legend() g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") g.set_titles(\\"{col_name} - {row_name}\\") # Adjust the layout plt.tight_layout() # Save the plot g.savefig(\\"tips_facet_grid.png\\")"},{"question":"**Machine Learning with Scikit-learn: Analyzing the Wine Dataset** *Background:* The wine dataset is one of the toy datasets available in scikit-learn. It contains chemical analysis results of wines grown in the same region in Italy but derived from three different cultivars. The dataset includes 13 features such as alcohol content, malic acid, ash, and others. *Objective:* Write a Python function that performs the following tasks using scikit-learn: 1. Load the wine dataset. 2. Perform a train-test split with 80% training data and 20% testing data. 3. Train a k-Nearest Neighbors (kNN) classifier on the training data. Use k=5. 4. Evaluate the classifier on the testing data and return the accuracy score. *Function Signature:* ```python def evaluate_wine_knn(): Trains a k-Nearest Neighbors classifier on the wine dataset and evaluates it. Returns: float: The accuracy score of the classifier on the test data. pass ``` *Constraints:* - Use `sklearn.datasets.load_wine` to load the dataset. - Perform the train-test split using `sklearn.model_selection.train_test_split`. - Train the kNN classifier using `sklearn.neighbors.KNeighborsClassifier` with `n_neighbors=5`. - Calculate the accuracy using `sklearn.metrics.accuracy_score`. *Expected Output:* - The function should return the accuracy score as a float value between 0 and 1. *Example:* ```python accuracy = evaluate_wine_knn() print(f\\"Accuracy: {accuracy:.2f}\\") # Expected Output (example): Accuracy: 0.94 ``` This problem requires students to demonstrate their understanding of: - Loading built-in datasets using scikit-learn. - Splitting datasets into training and testing sets. - Training and evaluating machine learning models using scikit-learn.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def evaluate_wine_knn(): Trains a k-Nearest Neighbors classifier on the wine dataset and evaluates it. Returns: float: The accuracy score of the classifier on the test data. # Load the wine dataset wine = load_wine() X = wine.data y = wine.target # Perform a train-test split with 80% training and 20% testing data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the kNN classifier with k=5 knn = KNeighborsClassifier(n_neighbors=5) knn.fit(X_train, y_train) # Evaluate the classifier on the test data y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Advanced Python Coding Assessment Objective Demonstrate your understanding of module importing, script execution, and metadata extraction using Python’s `importlib` and `modulefinder` modules. Problem Statement You are required to implement two functions: 1. **`find_used_modules(script_path: str) -> List[str]`**: - This function takes the path to a Python script file as input and returns a list of all the modules that the script uses. - Use the `modulefinder` module to identify these modules. 2. **`get_module_metadata(module_name: str) -> Dict[str, Any]`**: - This function takes the name of a module and returns a dictionary containing metadata about the module. Use the `importlib.metadata` module for this task. - The dictionary should include the module\'s name, version, and summary description. Input and Output Specifications - **`find_used_modules(script_path: str) -> List[str]`**: - **Input**: A string representing the path to a Python script file. Example: `\'./example_script.py\'`. - **Output**: A list of strings, each representing a module name used in the script. Example: `[\'sys\', \'os\', \'requests\']`. - **`get_module_metadata(module_name: str) -> Dict[str, Any]`**: - **Input**: A string representing the name of a module installed in your environment. Example: `\'requests\'`. - **Output**: A dictionary with keys `\'name\'`, `\'version\'`, and `\'summary\'`, containing the corresponding metadata of the module. Example: ```python { \'name\': \'requests\', \'version\': \'2.25.1\', \'summary\': \'Python HTTP for Humans.\' } ``` Constraints - Assume that the script file provided as input to `find_used_modules` is a correct and valid Python file. - The module name provided to `get_module_metadata` should be installed in the system where the script is running. - Handle any potential exceptions that might occur during dynamically importing modules or the metadata extraction process and provide appropriate error messages. Example Here is an example of how your functions should work. Script file (`example_script.py`): ```python import sys import os from collections import defaultdict def example_function(): print(\\"This is an example function\\") ``` Function Calls: ```python print(find_used_modules(\'example_script.py\')) # Output: [\'sys\', \'os\', \'collections\'] print(get_module_metadata(\'requests\')) # Output: { # \'name\': \'requests\', # \'version\': \'2.25.1\', # \'summary\': \'Python HTTP for Humans.\' # } ``` Implementation Notes - To avoid over-complicating the code, remember to make use of python’s `importlib` and `modulefinder` as much as possible. - Ensure to write clean and readable code, include appropriate error handling, and add comments where necessary to explain your logic.","solution":"import modulefinder from typing import List, Dict, Any import importlib.metadata def find_used_modules(script_path: str) -> List[str]: Find all modules used by the given script. Parameters: script_path (str): Path to the Python script file. Returns: List[str]: A list of module names used in the script. finder = modulefinder.ModuleFinder() finder.run_script(script_path) return list(finder.modules.keys()) def get_module_metadata(module_name: str) -> Dict[str, Any]: Get metadata information about the specified module. Parameters: module_name (str): The name of the module. Returns: Dict[str, Any]: A dictionary containing metadata about the module. try: distribution = importlib.metadata.distribution(module_name) return { \'name\': distribution.metadata[\'Name\'], \'version\': distribution.version, \'summary\': distribution.metadata.get(\'Summary\', \'No summary available.\') } except importlib.metadata.PackageNotFoundError: return { \'name\': module_name, \'version\': None, \'summary\': \'Module not found.\' }"},{"question":"# Advanced Python Coding Assessment Problem Statement You are required to develop a function that compresses all the files in a given directory into separate gzip files and then decompresses them back into the original format for verification. Your program will help you understand file handling, compression, and decompression in Python using the `gzip` module. Function Signature ```python def compress_and_verify(directory: str, compresslevel: int = 9) -> bool: pass ``` Input - `directory` (str): The path of the directory containing the files to be compressed. - `compresslevel` (int, optional): An integer ranging from 1 to 9 that determines the level of compression (default is 9, which is the maximum level of compression). Output - `bool`: Return `True` if all files are successfully compressed and then decompressed to match the original files, otherwise return `False`. Constraints - The directory will contain only files and no subdirectories. - Ensure that the compression and decompression do not alter the contents of the file. - Handle any exceptions that arise due to invalid files or issues with compression. Example ```python # considering a test directory with files: \'file1.txt\', \'file2.txt\' compress_and_verify(\'test_directory\') # Expected Output: This depends on the correctness of the function. # It should return True if all files are correctly handled. ``` Requirements 1. **Compression**: - Iterate through all the files in the given directory. - Compress each file using `gzip` with the provided compression level. - Save each compressed file with a `.gz` extension in the same directory. 2. **Decompression and Verification**: - Decompress each `.gz` file back to its original format and compare it with the original file to verify integrity. - Clean up the decompressed files after verification. 3. **Exception Handling**: - Handle scenarios where a file might be invalid or cannot be compressed/decompressed. Performance - Ensure the function handles the files efficiently, especially for larger directories with multiple files. Use this task to demonstrate your understanding of file operations, exception handling, and the `gzip` module. Make sure to write clean, readable, and well-documented code.","solution":"import os import gzip import shutil def compress_and_verify(directory: str, compresslevel: int = 9) -> bool: try: # Compress each file in the directory original_files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))] for file in original_files: file_path = os.path.join(directory, file) compressed_file_path = file_path + \'.gz\' with open(file_path, \'rb\') as f_in, gzip.open(compressed_file_path, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) # Verify by decompressing and comparing with the original files for file in original_files: file_path = os.path.join(directory, file) compressed_file_path = file_path + \'.gz\' decompressed_file_path = file_path + \'.decompressed\' # Decompressing the file with gzip.open(compressed_file_path, \'rb\') as f_in, open(decompressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) # Compare the original file and the decompressed file with open(file_path, \'rb\') as original_file, open(decompressed_file_path, \'rb\') as decompressed_file: original_data = original_file.read() decompressed_data = decompressed_file.read() if original_data != decompressed_data: return False # Clean up the decompressed file os.remove(decompressed_file_path) return True except Exception as e: # If any exception occurs, return False return False"},{"question":"**Objective:** Demonstrate your understanding of pandas Sparse data structures by working with dense and sparse data, performing computations, and converting between different formats. **Problem Statement:** You are provided with a large dataset in the form of a dense pandas DataFrame. Your task is to transform this data into a more memory-efficient sparse format. Once the data is in sparse format, you will need to perform certain computations and then convert parts of it back to dense format to analyze the results. **Requirements:** 1. **Convert to Sparse:** - Read a DataFrame from a CSV file (you can use any sample data for testing). - Convert this DataFrame to a sparse format using `pd.SparseDtype`, where `NaN` values are the fill value. 2. **Compute Density:** - Calculate and print the density of the sparse DataFrame. 3. **Mathematical Operations:** - Sum all the values across each column of the sparse DataFrame and return the result as a new sparse Series. 4. **Conversion to Dense:** - Convert the sparse DataFrame to its dense format and display the first 5 rows. 5. **Interaction with SciPy Sparse Matrices:** - Convert the dense DataFrame back to a sparse DataFrame using `DataFrame.sparse.from_spmatrix` method and confirm that the resulting DataFrame has the same density. **Constraints:** - You must use pandas version 1.0 or later. - You should ensure that the operations are efficient in terms of memory usage. **Function Signature:** ```python def sparse_operations(file_path: str) -> pd.Series: Perform operations on dense and sparse DataFrame. Parameters: - file_path (str): The path to the CSV file containing the dataset. Returns: - pd.Series: A sparse Series containing the sum of columns of the sparse DataFrame. pass ``` **Example Usage:** ```python file_path = \'path_to_dataset.csv\' sparse_operations(file_path) # Expected Output: # 0.0012 # Example density (this value may vary) # 0 10.5 # 1 23.4 # dtype: Sparse[float64, nan] # Column_1 Column_2 Column_3 Column_4 # 0 ... ... ... ... # 1 ... ... ... ... # 2 ... ... ... ... # 3 ... ... ... ... # 4 ... ... ... ... # Note: Actual data values will depend on the dataset. ```","solution":"import pandas as pd import numpy as np from scipy.sparse import csr_matrix def sparse_operations(file_path: str) -> pd.Series: Perform operations on dense and sparse DataFrame. Parameters: - file_path (str): The path to the CSV file containing the dataset. Returns: - pd.Series: A sparse Series containing the sum of columns of the sparse DataFrame. # Step 1: Read the dense DataFrame from CSV file df_dense = pd.read_csv(file_path) # Step 2: Convert to sparse format df_sparse = df_dense.astype(pd.SparseDtype(np.float64, np.nan)) # Step 3: Calculate density of the sparse DataFrame num_elements = df_dense.size num_non_nan = df_dense.count().sum() density = num_non_nan / num_elements print(f\\"Density of the sparse DataFrame: {density}\\") # Step 4: Sum all values across each column of the sparse DataFrame sparse_sum = df_sparse.sum(axis=0) # Step 5: Convert the sparse DataFrame to dense format and display the first 5 rows df_dense_converted = df_sparse.sparse.to_dense() print(df_dense_converted.head()) # Step 6: Convert dense DataFrame back to sparse df_from_scipy_sparse = pd.DataFrame.sparse.from_spmatrix(csr_matrix(df_dense_converted.values), columns=df_dense_converted.columns) confirm_density = df_from_scipy_sparse.notna().sum().sum() / df_from_scipy_sparse.size print(f\\"Density after converting back to sparse: {confirm_density}\\") return sparse_sum"},{"question":"# Question **Objective**: Implement and evaluate a machine learning model using cross-validation techniques provided by the scikit-learn library. **Task**: Using the provided dataset, implement a k-fold cross-validation technique to evaluate the performance of an SVM classifier. Additionally, use `GridSearchCV` to find the best hyperparameters for the SVM classifier. # Dataset You can use the Iris dataset provided by scikit-learn for this task. # Requirements 1. **Import necessary libraries**: Import the necessary libraries such as `numpy`, `sklearn.model_selection`, `sklearn.svm`, `sklearn.datasets`, and `sklearn.metrics`. 2. **Load the dataset**: Load the Iris dataset using `datasets.load_iris()` and split the features and labels. 3. **k-Fold Cross-Validation**: - Implement k-fold cross-validation with `k=5` using `cross_val_score`. - Report the mean and standard deviation of the cross-validation scores. 4. **Hyperparameter Tuning with GridSearchCV**: - Use `GridSearchCV` to find the best hyperparameters for the SVM classifier. - Search for the best values of `C` in the range `[0.1, 1, 10, 100]` and kernel types `[\'linear\', \'rbf\']`. - Report the best hyperparameters found and the corresponding cross-validation score. 5. **Evaluation on a hold-out test set**: - Splitting the dataset into a training set (80%) and a hold-out test set (20%) using `train_test_split`. - Train the SVM classifier using the best hyperparameters found. - Report the final accuracy on the hold-out test set. # Constraints - Use a random seed of `42` for reproducibility wherever applicable (e.g., in `train_test_split`). # Performance Requirement - Ensure the code runs efficiently without unnecessary computations or excessive memory usage. # Input No external input is required. Use the Iris dataset provided by scikit-learn. # Output Your code should output: 1. The mean and standard deviation of cross-validation scores. 2. The best hyperparameters found using `GridSearchCV`. 3. The final accuracy of the best model on the hold-out test set. # Example Output ``` Mean cross-validation score: 0.96 Standard deviation of cross-validation score: 0.02 Best hyperparameters: {\'C\': 1, \'kernel\': \'linear\'} Final accuracy on hold-out test set: 0.95 ```","solution":"def svm_cross_validation_and_hyperparameter_tuning(): import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split the dataset into a training set (80%) and a hold-out test set (20%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the SVM classifier svm = SVC(random_state=42) # Perform k-fold cross-validation with k=5 cross_val_scores = cross_val_score(svm, X_train, y_train, cv=5) mean_cv_score = np.mean(cross_val_scores) std_cv_score = np.std(cross_val_scores) # Define the parameter grid for hyperparameter tuning param_grid = { \'C\': [0.1, 1, 10, 100], \'kernel\': [\'linear\', \'rbf\'] } # Use GridSearchCV to find the best hyperparameters grid_search = GridSearchCV(svm, param_grid, cv=5) grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ # Train the SVM classifier using the best hyperparameters best_svm = SVC(**best_params, random_state=42) best_svm.fit(X_train, y_train) # Evaluate the final model on the hold-out test set y_pred = best_svm.predict(X_test) final_accuracy = accuracy_score(y_test, y_pred) # Output the results return { \'mean_cv_score\': mean_cv_score, \'std_cv_score\': std_cv_score, \'best_params\': best_params, \'final_accuracy\': final_accuracy }"},{"question":"Question: Event Scheduler using datetime Module # Objective: Create a function `schedule_event(start_time: str, end_time: str, duration: int) -> List[str]` that takes the following inputs: 1. `start_time`: A string representing the starting time of an event in the format `YYYY-MM-DD HH:MM`. 2. `end_time`: A string representing the ending time of an event in the format `YYYY-MM-DD HH:MM`. 3. `duration`: An integer representing the duration of each scheduled slot in minutes. The function should return a list of scheduled slots based on the given start and end times, with each slot having the specified `duration`. If the duration doesn\'t perfectly fit into the time range, the function should exclude the remaining time that doesn\'t match the full duration. Each slot should be represented as a string in the format `YYYY-MM-DD HH:MM to YYYY-MM-DD HH:MM`. # Constraints: - The `start_time` and `end_time` will always be valid date-time strings. - The `duration` will always be a positive integer and no longer than the total time range. - The time range from `start_time` to `end_time` will be at least as long as the `duration`. # Example Usage: ```python from typing import List def schedule_event(start_time: str, end_time: str, duration: int) -> List[str]: # Implementation here # Example usage: start_time = \\"2023-10-01 08:00\\" end_time = \\"2023-10-01 12:00\\" duration = 60 slots = schedule_event(start_time, end_time, duration) print(slots) # Output: [\\"2023-10-01 08:00 to 2023-10-01 09:00\\", # \\"2023-10-01 09:00 to 2023-10-01 10:00\\", # \\"2023-10-01 10:00 to 2023-10-01 11:00\\", # \\"2023-10-01 11:00 to 2023-10-01 12:00\\"] ``` # Explanation: Given the start time of `08:00` and end time of `12:00` with a slot duration of 60 minutes, the function should generate a list of time slots: - \\"2023-10-01 08:00 to 2023-10-01 09:00\\" - \\"2023-10-01 09:00 to 2023-10-01 10:00\\" - \\"2023-10-01 10:00 to 2023-10-01 11:00\\" - \\"2023-10-01 11:00 to 2023-10-01 12:00\\" # Notes: - Use Python\'s `datetime` module to handle date-related operations. - Ensure to efficiently format and properly handle date-time calculations involving `timedelta`.","solution":"from typing import List from datetime import datetime, timedelta def schedule_event(start_time: str, end_time: str, duration: int) -> List[str]: start_dt = datetime.strptime(start_time, \'%Y-%m-%d %H:%M\') end_dt = datetime.strptime(end_time, \'%Y-%m-%d %H:%M\') duration_td = timedelta(minutes=duration) slots = [] current_start = start_dt while current_start + duration_td <= end_dt: current_end = current_start + duration_td slot = f\\"{current_start.strftime(\'%Y-%m-%d %H:%M\')} to {current_end.strftime(\'%Y-%m-%d %H:%M\')}\\" slots.append(slot) current_start = current_end return slots"},{"question":"Objective To demonstrate understanding of the `configparser` module by implementing functionality to manage nested configurations, dynamically interpolate values using provided templates, and handle various custom parser settings. Problem Statement You are tasked with creating a utility function that generates a configuration file from a nested dictionary. This utility should also support dynamic interpolation within the configuration settings, allowing values to reference other values across different sections. # Specifications 1. **Function Definition**: ```python def generate_config(config_data: dict, file_path: str, interpolation: str = \'basic\', defaults: dict = None) -> None: Generates an INI configuration file with the given configuration data. Parameters: - config_data (dict): A nested dictionary where the keys are section names and values are dictionaries representing key-value pairs within those sections. - file_path (str): Path to the output configuration file. - interpolation (str): Type of interpolation. (\'basic\', \'extended\', or \'none\'). Default is \'basic\'. - defaults (dict): A dictionary of default values applicable across sections. Default is None. Returns: - None ``` 2. **Parameters**: - `config_data`: A nested dictionary representing the configuration data. Sections contain key-value pairs. Example: ```python config_data = { \'DEFAULT\': {\'version\': \'1.0\', \'file_path\': \'/etc/sample\', \'log_level\': \'DEBUG\'}, \'database\': {\'host\': \'localhost\', \'port\': \'5432\', \'user\': \'%(user)s\', \'password\': \'%(password)s\'}, \'user_auth\': {\'username\': \'admin\', \'password\': \'admin_password\'} } ``` - `file_path`: Output path for the configuration file. - `interpolation`: Choose between \'basic\', \'extended\', or \'none\' to handle interpolation of values across sections. - `defaults`: Default values that apply across the entire configuration. 3. **Implementation Constraints**: - Use `ConfigParser` to manage the configuration data. - Handle different types of interpolation based on the `interpolation` parameter. - Include fallback mechanisms for missing values using the `DEFAULT` section. - The resulting configuration must be saved to the specified file path. 4. **Examples**: ```python config_data = { \'DEFAULT\': {\'data_path\': \'/var/data\', \'temp_dir\': \'/tmp\'}, \'section1\': {\'input\': \'%(data_path)s/input.txt\', \'output\': \'%(data_path)s/output.txt\'}, \'section2\': {\'cache\': \'%(temp_dir)s/cache\'} } generate_config(config_data, \'output_config.ini\', interpolation=\'extended\') ``` Expected content of `output_config.ini`: ```ini [DEFAULT] data_path = /var/data temp_dir = /tmp [section1] input = /var/data/input.txt output = /var/data/output.txt [section2] cache = /tmp/cache ``` # Additional Notes: - Ensure the solution handles edge cases, such as missing values during interpolation and user-defined converters. - The function should create the configuration file on the specified path with appropriate error handling for file operations. Evaluation Criteria: - Accuracy of the implementation based on the provided specifications. - Correct and efficient use of the `configparser` module. - Robust error handling and edge cases management. - Clear and readable code with appropriate comments.","solution":"import configparser def generate_config(config_data, file_path, interpolation=\'basic\', defaults=None): Generates an INI configuration file with the given configuration data. Parameters: - config_data (dict): A nested dictionary where the keys are section names and values are dictionaries representing key-value pairs within those sections. - file_path (str): Path to the output configuration file. - interpolation (str): Type of interpolation. (\'basic\', \'extended\', or \'none\'). Default is \'basic\'. - defaults (dict): A dictionary of default values applicable across sections. Default is None. Returns: - None if interpolation == \'basic\': parser = configparser.ConfigParser(defaults) elif interpolation == \'extended\': parser = configparser.ConfigParser(defaults, interpolation=configparser.ExtendedInterpolation()) elif interpolation == \'none\': parser = configparser.ConfigParser(defaults, interpolation=None) else: raise ValueError(\\"Invalid interpolation type. Choose \'basic\', \'extended\', or \'none\'.\\") for section, values in config_data.items(): parser[section] = values with open(file_path, \'w\') as configfile: parser.write(configfile)"},{"question":"Objective: You are required to implement a set of functions to manipulate Python tuples. These functions will mimic some of the C API functionalities for tuples as described in the provided documentation but need to be implemented in standard Python. Description: Implement the following functions: 1. `tuple_check(obj) -> bool` - **Input:** A single argument `obj`. - **Output:** Returns `True` if `obj` is a tuple, `False` otherwise. 2. `tuple_new(length: int) -> tuple` - **Input:** An integer `length` specifying the size of the tuple. - **Output:** Returns a new tuple of specified `length` with all elements initialized to `None`. - **Constraint:** `length` should be non-negative. 3. `tuple_size(tpl: tuple) -> int` - **Input:** A tuple `tpl`. - **Output:** Returns the size of the tuple `tpl`. 4. `tuple_get_item(tpl: tuple, index: int)` - **Input:** A tuple `tpl` and an integer `index`. - **Output:** Returns the item at position `index` in the tuple `tpl`. - **Constraint:** If `index` is out of bounds, raise an `IndexError`. 5. `tuple_set_item(tpl: tuple, index: int, value) -> tuple` - **Input:** A tuple `tpl`, an integer `index`, and a value `value`. - **Output:** Returns a new tuple with the item at position `index` set to `value`. - **Constraint:** If `index` is out of bounds, raise an `IndexError`. 6. `tuple_slice(tpl: tuple, start: int, end: int) -> tuple` - **Input:** A tuple `tpl`, and two integers `start` and `end`. - **Output:** Returns a slice of the tuple from index `start` to `index` `end`. - **Constraint:** If `start` or `end` is out of bounds, they should adhere to Python\'s slicing rules. Example: ```python # Example Usage t = tuple_new(3) print(t) # Output: (None, None, None) print(tuple_size(t)) # Output: 3 print(tuple_check(t)) # Output: True print(tuple_get_item(t, 1)) # Output: None try: print(tuple_get_item(t, 5)) # Raises IndexError except IndexError as e: print(e) # Output: tuple index out of range t2 = tuple_set_item(t, 1, \\"Python\\") print(t2) # Output: (None, \\"Python\\", None) print(tuple_get_item(t2, 1)) # Output: \\"Python\\" print(tuple_slice(t, 1, 2)) # Output: (None,) ``` Submission: Submit the implementation of all the functions defined above. Evaluation: - **Correctness:** The functions must perform the specified operations correctly. - **Edge Cases:** Proper handling of edge cases such as index being out of bounds. - **Efficiency:** The solution should not have unnecessary complexity.","solution":"def tuple_check(obj): Returns True if obj is a tuple, False otherwise. return isinstance(obj, tuple) def tuple_new(length): Returns a new tuple of specified length with all elements initialized to None. if length < 0: raise ValueError(\\"Length must be non-negative\\") return (None,) * length def tuple_size(tpl): Returns the size of the tuple. if not isinstance(tpl, tuple): raise TypeError(\\"Input must be a tuple\\") return len(tpl) def tuple_get_item(tpl, index): Returns the item at position index in the tuple. Raises IndexError if index is out of bounds. if not isinstance(tpl, tuple): raise TypeError(\\"First argument must be a tuple\\") return tpl[index] def tuple_set_item(tpl, index, value): Returns a new tuple with the item at position index set to value. Raises IndexError if index is out of bounds. if not isinstance(tpl, tuple): raise TypeError(\\"First argument must be a tuple\\") if index < 0 or index >= len(tpl): raise IndexError(\\"tuple index out of range\\") new_list = list(tpl) new_list[index] = value return tuple(new_list) def tuple_slice(tpl, start, end): Returns a slice of the tuple from index start to index end. if not isinstance(tpl, tuple): raise TypeError(\\"First argument must be a tuple\\") return tpl[start:end]"},{"question":"**XML Parsing and Data Extraction with `xml.parsers.expat`** Given the following XML document: ```xml <?xml version=\\"1.0\\"?> <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book id=\\"2\\"> <title>Learning XML</title> <author>Jane Smith</author> <year>2018</year> </book> <book id=\\"3\\"> <title>Advanced Python</title> <author>James Brown</author> <year>2021</year> </book> </library> ``` Write a Python function `parse_books(xml_string)` that takes a string containing the XML data and returns a list of dictionaries, where each dictionary represents a book with keys `title`, `author`, and `year`. Use the `xml.parsers.expat` module to parse the XML. **Function Signature**: ```python def parse_books(xml_string: str) -> list: ``` # Input - A single string `xml_string` containing the XML document. # Output - A list of dictionaries, where each dictionary has keys `title`, `author`, and `year`. # Constraints - Each book element in the XML will always contain `title`, `author`, and `year` elements. - XML data string will always be well-formed. - You must use the `xml.parsers.expat` module for parsing the XML. # Example ```python xml_str = \'\'\'<?xml version=\\"1.0\\"?> <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book id=\\"2\\"> <title>Learning XML</title> <author>Jane Smith</author> <year>2018</year> </book> <book id=\\"3\\"> <title>Advanced Python</title> <author>James Brown</author> <year>2021</year> </book> </library>\'\'\' print(parse_books(xml_str)) # Expected Output: # [{\'title\': \'Python Programming\', \'author\': \'John Doe\', \'year\': \'2020\'}, # {\'title\': \'Learning XML\', \'author\': \'Jane Smith\', \'year\': \'2018\'}, # {\'title\': \'Advanced Python\', \'author\': \'James Brown\', \'year\': \'2021\'}] ``` # Notes - Make sure to implement appropriate handler functions for start elements, end elements, and character data. - Carefully manage the state to ensure that data for each book is collected correctly.","solution":"import xml.parsers.expat def parse_books(xml_string: str) -> list: books = [] current_book = {} current_element = \\"\\" def start_element(name, attrs): nonlocal current_element if name == \'book\': current_book.clear() current_element = name def end_element(name): nonlocal current_element if name == \'book\': books.append(current_book.copy()) current_element = \\"\\" def char_data(data): if current_element == \'title\': current_book[\'title\'] = data.strip() elif current_element == \'author\': current_book[\'author\'] = data.strip() elif current_element == \'year\': current_book[\'year\'] = data.strip() parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_string) return books"},{"question":"**Coding Assessment Question** # Objective Implement and compare different regularized regression techniques in `scikit-learn`, focusing on Ordinary Least Squares, Ridge Regression, and Lasso. Additionally, evaluate these models\' performance on a given dataset, determine which model provides the best performance, and report your findings. # Background You are provided with a synthetic dataset for a regression problem. Your task is to fit models using Ordinary Least Squares, Ridge regression, and Lasso, and evaluate their performance using mean squared error (MSE). Also, perform cross-validation to find the best hyperparameters for Ridge and Lasso. # Instructions 1. **Data Preparation**: - Load the dataset from the provided CSV file `data.csv`. This file contains features (`X`) and target values (`y`). 2. **Model Implementation**: - Implement OLS Regression using `sklearn.linear_model.LinearRegression`. - Implement Ridge Regression using `sklearn.linear_model.Ridge`. Use cross-validation to find the best `alpha` from a logarithmically spaced set of values. - Implement Lasso using `sklearn.linear_model.Lasso`. Use cross-validation to find the best `alpha` from a logarithmically spaced set of values. 3. **Performance Evaluation**: - Split the dataset into training (80%) and testing (20%) sets. - Train each model using the training data. - Evaluate the models using the Mean Squared Error (MSE) on the test data. - Compare and report the performance of each model. # Constraints - Use `sklearn.model_selection.train_test_split` for splitting the dataset. - Use `sklearn.model_selection.GridSearchCV` for hyperparameter tuning. - Avoid using any automated pipelines or higher-level abstractions like `sklearn.pipeline.Pipeline`. All steps should be explicitly coded. # Input - A CSV file named `data.csv` containing the data. # Output - MSE values for OLS, Ridge, and Lasso models on the test dataset. - The best hyperparameters (`alpha`) found for Ridge and Lasso. - A comparison report explaining which model performed best and under what circumstances it might be preferable to use one model over the others. # Code Template ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_squared_error import numpy as np # Load the dataset data = pd.read_csv(\'data.csv\') X = data.drop(columns=\'target\') y = data[\'target\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement OLS Regression ols_model = LinearRegression() ols_model.fit(X_train, y_train) y_pred_ols = ols_model.predict(X_test) mse_ols = mean_squared_error(y_test, y_pred_ols) # Implement Ridge Regression with Cross-Validation ridge_alphas = np.logspace(-6, 6, 13) ridge_model = Ridge() ridge_cv = GridSearchCV(ridge_model, param_grid={\'alpha\': ridge_alphas}, scoring=\'neg_mean_squared_error\', cv=5) ridge_cv.fit(X_train, y_train) y_pred_ridge = ridge_cv.predict(X_test) mse_ridge = mean_squared_error(y_test, y_pred_ridge) best_alpha_ridge = ridge_cv.best_params_[\'alpha\'] # Implement Lasso with Cross-Validation lasso_alphas = np.logspace(-6, 6, 13) lasso_model = Lasso() lasso_cv = GridSearchCV(lasso_model, param_grid={\'alpha\': lasso_alphas}, scoring=\'neg_mean_squared_error\', cv=5) lasso_cv.fit(X_train, y_train) y_pred_lasso = lasso_cv.predict(X_test) mse_lasso = mean_squared_error(y_test, y_pred_lasso) best_alpha_lasso = lasso_cv.best_params_[\'alpha\'] # Report Performance print(f\\"OLS MSE: {mse_ols}\\") print(f\\"Ridge MSE: {mse_ridge}, Best Alpha: {best_alpha_ridge}\\") print(f\\"Lasso MSE: {mse_lasso}, Best Alpha: {best_alpha_lasso}\\") # Comparison Report comparison_report = f Comparison Report: ------------------ OLS MSE: {mse_ols} Ridge MSE: {mse_ridge}, Best Alpha: {best_alpha_ridge} Lasso MSE: {mse_lasso}, Best Alpha: {best_alpha_lasso} The model with the best performance in terms of MSE is: {\\"OLS\\" if mse_ols < mse_ridge and mse_ols < mse_lasso else \\"Ridge\\" if mse_ridge < mse_lasso else \\"Lasso\\"}. Ridge regression might be preferable when dealing with multicollinear features, as it imposes an L2 penalty which helps to regularize the coefficients. Lasso is useful for feature selection as it can drive some coefficients to zero, effectively selecting a subset of features. print(comparison_report) ``` # Notes - Ensure that your code is clean and well-documented. - Include inline comments to explain key steps in your implementation. Good luck!","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_squared_error import numpy as np def load_data(file_path): Load the dataset from the given file path. data = pd.read_csv(file_path) X = data.drop(columns=\'target\') y = data[\'target\'] return X, y def split_data(X, y, test_size=0.2, random_state=42): Split the dataset into training and testing sets. return train_test_split(X, y, test_size=test_size, random_state=random_state) def train_ols(X_train, y_train): Train an Ordinary Least Squares (OLS) regression model. model = LinearRegression() model.fit(X_train, y_train) return model def train_ridge(X_train, y_train): Train a Ridge regression model using cross-validation to find the best alpha. alphas = np.logspace(-6, 6, 13) model = Ridge() grid = GridSearchCV(model, param_grid={\'alpha\': alphas}, scoring=\'neg_mean_squared_error\', cv=5) grid.fit(X_train, y_train) return grid def train_lasso(X_train, y_train): Train a Lasso regression model using cross-validation to find the best alpha. alphas = np.logspace(-6, 6, 13) model = Lasso() grid = GridSearchCV(model, param_grid={\'alpha\': alphas}, scoring=\'neg_mean_squared_error\', cv=5) grid.fit(X_train, y_train) return grid def evaluate_model(model, X_test, y_test): Evaluate the model using Mean Squared Error (MSE). y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse def main(file_path): # Load and prepare the data X, y = load_data(file_path) X_train, X_test, y_train, y_test = split_data(X, y) # Train models ols_model = train_ols(X_train, y_train) ridge_cv = train_ridge(X_train, y_train) lasso_cv = train_lasso(X_train, y_train) # Evaluate models mse_ols = evaluate_model(ols_model, X_test, y_test) mse_ridge = evaluate_model(ridge_cv, X_test, y_test) mse_lasso = evaluate_model(lasso_cv, X_test, y_test) # Best alphas for Ridge and Lasso best_alpha_ridge = ridge_cv.best_params_[\'alpha\'] best_alpha_lasso = lasso_cv.best_params_[\'alpha\'] # Output print(f\\"OLS MSE: {mse_ols}\\") print(f\\"Ridge MSE: {mse_ridge}, Best Alpha: {best_alpha_ridge}\\") print(f\\"Lasso MSE: {mse_lasso}, Best Alpha: {best_alpha_lasso}\\") # Comparison Report comparison_report = f Comparison Report: ------------------ OLS MSE: {mse_ols} Ridge MSE: {mse_ridge}, Best Alpha: {best_alpha_ridge} Lasso MSE: {mse_lasso}, Best Alpha: {best_alpha_lasso} The model with the best performance in terms of MSE is: {\\"OLS\\" if mse_ols < mse_ridge and mse_ols < mse_lasso else \\"Ridge\\" if mse_ridge < mse_lasso else \\"Lasso\\"}. Ridge regression might be preferable when dealing with multicollinear features, as it imposes an L2 penalty which helps to regularize the coefficients. Lasso is useful for feature selection as it can drive some coefficients to zero, effectively selecting a subset of features. print(comparison_report)"},{"question":"Problem Statement You are given a list of student records. Each record is a tuple containing three elements: (student_name, grade, ID). Your task is to implement a function `maintain_sorted_records()` that maintains a list of records sorted by `grade` in increasing order. You need to support the following operations: 1. **Add a Record**: `add_record(student_name: str, grade: float, ID: int) -> None` 2. **Find Record with Greatest Grade Less Than Given Value**: `find_record_lt(grade: float) -> Tuple[str, float, int]` 3. **Find Record with Least Grade Greater Than Given Value**: `find_record_gt(grade: float) -> Tuple[str, float, int]` # Implementation Details - Use the `bisect` module functions to maintain the sorted order of records. - Keep the main list of records sorted only by the `grade` element of each tuple. - For `add_record()`, insert the new record in its correct place to maintain sorted order. - For `find_record_lt()` and `find_record_gt()`, search the list of records to find the appropriate record based on the given `grade`. - If no record is found for `find_record_lt()` or `find_record_gt()`, raise a `ValueError`. # Example ```python # Initializing the sorted record list records = [] # Add records maintain_sorted_records.add_record(\\"Alice\\", 85.0, 1) maintain_sorted_records.add_record(\\"Bob\\", 90.0, 2) maintain_sorted_records.add_record(\\"Charlie\\", 80.0, 3) # Find record with greatest grade less than 88.0 record_lt_88 = maintain_sorted_records.find_record_lt(88.0) # Expected output: (\\"Alice\\", 85.0, 1) # Find record with least grade greater than 82.5 record_gt_82_5 = maintain_sorted_records.find_record_gt(82.5) # Expected output: (\\"Alice\\", 85.0, 1) # Adding another record maintain_sorted_records.add_record(\\"Dave\\", 88.0, 4) # Find record with greatest grade less than 88.0 again record_lt_88 = maintain_sorted_records.find_record_lt(88.0) # Expected output: (\\"Alice\\", 85.0, 1) ``` # Constraints - Each student\'s name is a string of up to 100 characters. - Each grade is a floating-point number between 0.0 and 100.0. - Each ID is a unique integer. - The number of student records, `n`, can be up to (10^6) # Function Definitions ```python class MaintainSortedRecords: def __init__(self): self.records = [] def add_record(self, student_name: str, grade: float, ID: int) -> None: # Implement this function def find_record_lt(self, grade: float) -> Tuple[str, float, int]: # Implement this function def find_record_gt(self, grade: float) -> Tuple[str, float, int]: # Implement this function ``` # Notes - Make sure the functions work efficiently within the constraints. - Use appropriate techniques to maintain and search the sorted list. - Test your implementation thoroughly to ensure correctness.","solution":"import bisect from typing import List, Tuple class MaintainSortedRecords: def __init__(self): self.records = [] # This will store the records sorted by grade. def add_record(self, student_name: str, grade: float, ID: int) -> None: Add a new student record while maintaining the list sorted by grade. bisect.insort(self.records, (grade, student_name, ID)) def find_record_lt(self, grade: float) -> Tuple[str, float, int]: Find the record with greatest grade less than the given grade. index = bisect.bisect_left(self.records, (grade, \'\', 0)) if index == 0: raise ValueError(\\"No record found with grade less than the given grade\\") return (self.records[index - 1][1], self.records[index - 1][0], self.records[index - 1][2]) def find_record_gt(self, grade: float) -> Tuple[str, float, int]: Find the record with least grade greater than the given grade. index = bisect.bisect_right(self.records, (grade, \'\', 0)) if index == len(self.records): raise ValueError(\\"No record found with grade greater than the given grade\\") return (self.records[index][1], self.records[index][0], self.records[index][2])"},{"question":"**Objective**: Implement a Python program that uses the `atexit` module to manage the lifecycle of a temporary file and ensure the file is deleted upon program termination. # Description You are required to write a Python program that performs the following tasks: 1. Creates a temporary file named `tempfile.txt`. 2. Writes some initial content to `tempfile.txt`. 3. Registers an `atexit` function to delete `tempfile.txt` upon program termination. 4. The program should also handle cases where `tempfile.txt` does not exist when the `atexit` function is called (e.g., if the file was manually deleted before the program terminated). # Requirements - Define a function `create_temp_file()` that creates `tempfile.txt` and writes the string \\"Temporary file content\\" to it. - Define a function `delete_temp_file()` that attempts to delete `tempfile.txt`. Ensure it handles the case where the file does not exist gracefully. - Register `delete_temp_file()` using `atexit.register()` to ensure `tempfile.txt` is deleted when the program exits. - Demonstrate that `tempfile.txt` is indeed created and deleted properly by printing appropriate messages to the console. # Constraints - Do not use any external libraries other than `os` for file operations and `atexit` for exit handling. - The functions should handle all necessary exceptions gracefully. - The program should run without modification in environments where `tempfile.txt` might already exist or be deleted during execution. # Expected Output The exact output will vary depending on the state of `tempfile.txt`, but an example output might look like this: ```plaintext Creating tempfile.txt and writing initial content... Tempfile.txt created successfully. Running some additional logic... Deleting tempfile.txt as part of atexit handler... Tempfile.txt deleted successfully. ``` # Hints - Use the `os` module to handle file creation, writing, and deletion. - Ensure the `atexit.register()` is called after the definition of `delete_temp_file()`. ```python import os import atexit def create_temp_file(): try: with open(\'tempfile.txt\', \'w\') as temp_file: temp_file.write(\\"Temporary file content\\") print(\\"Tempfile.txt created successfully.\\") except Exception as e: print(f\\"Error creating tempfile.txt: {e}\\") def delete_temp_file(): try: if os.path.exists(\'tempfile.txt\'): os.remove(\'tempfile.txt\') print(\\"Tempfile.txt deleted successfully.\\") else: print(\\"Tempfile.txt does not exist.\\") except Exception as e: print(f\\"Error deleting tempfile.txt: {e}\\") # Register delete_temp_file to run at interpreter shutdown atexit.register(delete_temp_file) # Create the temporary file create_temp_file() # Simulate some additional logic print(\\"Running some additional logic...\\") # The program will end after this, triggering the atexit handlers ```","solution":"import os import atexit def create_temp_file(): try: with open(\'tempfile.txt\', \'w\') as temp_file: temp_file.write(\\"Temporary file content\\") print(\\"Tempfile.txt created successfully.\\") except Exception as e: print(f\\"Error creating tempfile.txt: {e}\\") def delete_temp_file(): try: if os.path.exists(\'tempfile.txt\'): os.remove(\'tempfile.txt\') print(\\"Tempfile.txt deleted successfully.\\") else: print(\\"Tempfile.txt does not exist.\\") except Exception as e: print(f\\"Error deleting tempfile.txt: {e}\\") # Register delete_temp_file to run at interpreter shutdown atexit.register(delete_temp_file) # Create the temporary file create_temp_file() # Simulate some additional logic print(\\"Running some additional logic...\\") # The program will end after this, triggering the atexit handlers"},{"question":"# **Coding Assessment Question** **Objective:** Create a Python function that reads data from a JSON file, processes this data, and writes the processed data to another file with specific formatting requirements. **Problem Statement:** You are provided with a JSON file named `input_data.json` that contains data in the following structure: ```json { \\"employees\\": [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"department\\": \\"HR\\"}, {\\"name\\": \\"Bob\\", \\"age\\": 24, \\"department\\": \\"Engineering\\"}, {\\"name\\": \\"Charlie\\", \\"age\\": 29, \\"department\\": \\"Marketing\\"} ] } ``` Write a function `process_employee_data(input_file: str, output_file: str) -> None` that performs the following tasks: 1. Reads the JSON data from `input_file`. 2. Processes the data to create a formatted string for each employee in the format: `\\"Name: {name}, Age: {age}, Department: {department}\\"`. 3. Writes these formatted strings to a plain text file specified by `output_file`, with each employee\'s information on a new line. 4. Ensure the output file is correctly closed after writing, even if an exception occurs. **Input:** - `input_file` (str): The path to the input JSON file. - `output_file` (str): The path to the output text file. **Output:** - None. The function should perform file I/O operations and write to an output file. **Constraints:** - The input JSON file will always have the specified structure. - You must use formatted string literals (f-strings) for constructing the output strings. **Example:** Given `input_data.json`: ```json { \\"employees\\": [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"department\\": \\"HR\\"}, {\\"name\\": \\"Bob\\", \\"age\\": 24, \\"department\\": \\"Engineering\\"}, {\\"name\\": \\"Charlie\\", \\"age\\": 29, \\"department\\": \\"Marketing\\"} ] } ``` You function call: ```python process_employee_data(\\"input_data.json\\", \\"output_data.txt\\") ``` Should result in `output_data.txt` containing: ```plaintext Name: Alice, Age: 30, Department: HR Name: Bob, Age: 24, Department: Engineering Name: Charlie, Age: 29, Department: Marketing ``` **Notes:** - Use a context manager (the `with` statement) to ensure that files are properly opened and closed. - Properly handle JSON reading and writing. - Make use of f-strings for creating the formatted strings. **Additional Resources:** - [File Handling in Python](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files) - [Formatted string literals](https://docs.python.org/3/tutorial/inputoutput.html#tut-f-strings) - [json module documentation](https://docs.python.org/3/library/json.html)","solution":"import json def process_employee_data(input_file: str, output_file: str) -> None: with open(input_file, \'r\') as infile: data = json.load(infile) employees = data.get(\\"employees\\", []) formatted_employees = [ f\\"Name: {employee[\'name\']}, Age: {employee[\'age\']}, Department: {employee[\'department\']}\\" for employee in employees ] with open(output_file, \'w\') as outfile: for employee in formatted_employees: outfile.write(employee + \'n\')"},{"question":"**Objective**: Design a function that demonstrates advanced understanding of `scikit-learn`\'s dataset generation functions and apply basic machine learning models to these datasets. # Problem Statement You are given a task to generate and visualize different datasets using `scikit-learn`\'s dataset generator functions. Then, you need to apply appropriate machine learning models to these datasets and evaluate their performance. # Function Specification **Function Name**: `generate_and_evaluate_datasets` **Inputs**: 1. `random_state`: An integer to control the random seed for reproducibility. **Outputs**: - A dictionary containing: - Keys as dataset descriptions. - Values as dictionaries with keys `\'model\'` (trained model object) and `\'score\'` (evaluation score). # Requirements: 1. Generate the following datasets: a. Gaussian blobs using `make_blobs` with 3 centers. b. Classification data using `make_classification` with 2 informative features and 1 cluster per class. c. Concentric circles data using `make_circles`. d. Interleaved half-circles data using `make_moons`. 2. Visualize each dataset with a scatter plot where points are colored by class. 3. Apply the following machine learning models to each dataset: a. For `make_blobs`, use a k-means clustering algorithm. b. For `make_classification`, use a logistic regression classifier. c. For `make_circles`, use a support vector classifier (SVC). d. For `make_moons`, use a decision tree classifier. 4. Evaluate the performance of each model using appropriate metrics: a. For clustering (`make_blobs`), use the silhouette score. b. For classification datasets (`make_classification`, `make_circles`, `make_moons`), use accuracy. # Constraints: - Use `random_state` wherever applicable to ensure reproducibility. - The evaluation of the models should be done on the same data they were trained on (this is for simplicity). # Example Usage and Output: ```python results = generate_and_evaluate_datasets(random_state=42) ``` The `results` dictionary might look something like this: ```python { \\"Gaussian Blobs Clustering\\": { \\"model\\": KMeans(...), \\"score\\": 0.75 }, \\"Simple Classification\\": { \\"model\\": LogisticRegression(...), \\"score\\": 0.85 }, \\"Concentric Circles Classification\\": { \\"model\\": SVC(...), \\"score\\": 0.90 }, \\"Interleaved Moons Classification\\": { \\"model\\": DecisionTreeClassifier(...), \\"score\\": 0.95 } } ``` # Solution Template: ```python def generate_and_evaluate_datasets(random_state): # Import required libraries import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_circles, make_moons from sklearn.cluster import KMeans from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import silhouette_score, accuracy_score results = {} # Step 1: Generate datasets # Gaussian Blobs X_blobs, y_blobs = make_blobs(centers=3, random_state=random_state) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"Three blobs\\") plt.show() # Classification X_class, y_class = make_classification(n_informative=2, n_clusters_per_class=1, random_state=random_state) plt.scatter(X_class[:, 0], X_class[:, 1], c=y_class) plt.title(\\"Classification\\") plt.show() # Concentric Circles X_circles, y_circles = make_circles(noise=0.1, factor=0.3, random_state=random_state) plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles) plt.title(\\"Concentric Circles\\") plt.show() # Interleaved Moons X_moons, y_moons = make_moons(noise=0.1, random_state=random_state) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons) plt.title(\\"Interleaved Moons\\") plt.show() # Step 2: Apply models and evaluate them # Gaussian Blobs Clustering kmeans = KMeans(n_clusters=3, random_state=random_state) kmeans.fit(X_blobs) score_blobs = silhouette_score(X_blobs, kmeans.labels_) results[\\"Gaussian Blobs Clustering\\"] = {\\"model\\": kmeans, \\"score\\": score_blobs} # Simple Classification log_reg = LogisticRegression(random_state=random_state) log_reg.fit(X_class, y_class) score_class = accuracy_score(y_class, log_reg.predict(X_class)) results[\\"Simple Classification\\"] = {\\"model\\": log_reg, \\"score\\": score_class} # Concentric Circles Classification svc = SVC(random_state=random_state) svc.fit(X_circles, y_circles) score_circles = accuracy_score(y_circles, svc.predict(X_circles)) results[\\"Concentric Circles Classification\\"] = {\\"model\\": svc, \\"score\\": score_circles} # Interleaved Moons Classification tree = DecisionTreeClassifier(random_state=random_state) tree.fit(X_moons, y_moons) score_moons = accuracy_score(y_moons, tree.predict(X_moons)) results[\\"Interleaved Moons Classification\\"] = {\\"model\\": tree, \\"score\\": score_moons} return results # Example usage results = generate_and_evaluate_datasets(random_state=42) print(results) ```","solution":"def generate_and_evaluate_datasets(random_state): # Import required libraries import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_circles, make_moons from sklearn.cluster import KMeans from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import silhouette_score, accuracy_score results = {} # Step 1: Generate datasets # Gaussian Blobs X_blobs, y_blobs = make_blobs(centers=3, random_state=random_state) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap=\'viridis\') plt.title(\\"Three blobs\\") plt.show() # Classification X_class, y_class = make_classification(n_informative=2, n_clusters_per_class=1, random_state=random_state) plt.scatter(X_class[:, 0], X_class[:, 1], c=y_class, cmap=\'viridis\') plt.title(\\"Classification\\") plt.show() # Concentric Circles X_circles, y_circles = make_circles(noise=0.1, factor=0.3, random_state=random_state) plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=\'viridis\') plt.title(\\"Concentric Circles\\") plt.show() # Interleaved Moons X_moons, y_moons = make_moons(noise=0.1, random_state=random_state) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=\'viridis\') plt.title(\\"Interleaved Moons\\") plt.show() # Step 2: Apply models and evaluate them # Gaussian Blobs Clustering kmeans = KMeans(n_clusters=3, random_state=random_state) kmeans.fit(X_blobs) score_blobs = silhouette_score(X_blobs, kmeans.labels_) results[\\"Gaussian Blobs Clustering\\"] = {\\"model\\": kmeans, \\"score\\": score_blobs} # Simple Classification log_reg = LogisticRegression(random_state=random_state) log_reg.fit(X_class, y_class) score_class = accuracy_score(y_class, log_reg.predict(X_class)) results[\\"Simple Classification\\"] = {\\"model\\": log_reg, \\"score\\": score_class} # Concentric Circles Classification svc = SVC(random_state=random_state) svc.fit(X_circles, y_circles) score_circles = accuracy_score(y_circles, svc.predict(X_circles)) results[\\"Concentric Circles Classification\\"] = {\\"model\\": svc, \\"score\\": score_circles} # Interleaved Moons Classification tree = DecisionTreeClassifier(random_state=random_state) tree.fit(X_moons, y_moons) score_moons = accuracy_score(y_moons, tree.predict(X_moons)) results[\\"Interleaved Moons Classification\\"] = {\\"model\\": tree, \\"score\\": score_moons} return results # Example usage results = generate_and_evaluate_datasets(random_state=42) print(results)"},{"question":"# Question: Visualization with Seaborn You are given a dataset of flight passengers information `flights` which records the number of airline passengers who flew in each month from 1949 to 1960. Your task is to: 1. Convert the `flights` dataset from long-form to wide-form. 2. Create a line plot using seaborn that shows the number of passengers over the years for each month using the wide-form data. 3. Convert the wide-form dataset back to long-form. 4. Create a line plot using seaborn from the long-form dataset showing the number of passengers over the years for each month. 5. Compare both plots and explain any differences or similarities in the aesthetics and information representation. **Input Format:** - None; you will load the dataset internally within your code. **Output Format:** - Two seaborn plots as specified above. - A brief explanation comparing the two plots. **Constraints:** - Ensure your solution handles datasets efficiently in terms of memory usage. - Use appropriate seaborn functions effectively to achieve the desired results. **Notes:** - You are expected to perform any required data manipulations using pandas. - Label the axes and add titles to your plots for better readability. **Example of Implementation:** ```python import seaborn as sns import pandas as pd # Load the dataset flights = sns.load_dataset(\\"flights\\") # 1. Convert the dataset from long-form to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # 2. Create a line plot for the wide-form data sns.relplot(data=flights_wide, kind=\\"line\\") plt.title(\'Wide-form Data: Passengers over Years for Each Month\') plt.show() # 3. Convert the wide-form dataset back to long-form flights_long = flights_wide.reset_index().melt(id_vars=\\"year\\", var_name=\\"month\\", value_name=\\"passengers\\") # 4. Create a line plot for the long-form data sns.relplot(data=flights_long, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") plt.title(\'Long-form Data: Passengers over Years for Each Month\') plt.show() # 5. Comparison print(\\"Comparison:\\") print(\\"The wide-form plot automatically assigns each month\'s data series a separate line without additional configuration.\\") print(\\"The long-form plot requires explicit mapping of data dimensions (year, month, passengers) to plot dimensions (x, y, hue).\\") ``` Provide your solution and comparison explanation below the required code implementations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset flights = sns.load_dataset(\\"flights\\") # 1. Convert the dataset from long-form to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # 2. Create a line plot for the wide-form data sns.relplot(data=flights_wide, kind=\\"line\\") plt.title(\'Wide-form Data: Passengers over Years for Each Month\') plt.xlabel(\'Year\') plt.ylabel(\'Passengers\') plt.show() # 3. Convert the wide-form dataset back to long-form flights_long = flights_wide.reset_index().melt(id_vars=\\"year\\", var_name=\\"month\\", value_name=\\"passengers\\") # 4. Create a line plot for the long-form data sns.relplot(data=flights_long, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") plt.title(\'Long-form Data: Passengers over Years for Each Month\') plt.xlabel(\'Year\') plt.ylabel(\'Passengers\') plt.show() # 5. Comparison comparison_text = Comparison: The wide-form plot automatically assigns each month\'s data series a separate line without additional configuration. The long-form plot requires explicit mapping of data dimensions (year, month, passengers) to plot dimensions (x, y, hue). Both plots convey the same information but in slightly different aesthetic layouts: - The wide-form plot provides a clearer view of month-by-month passenger trends since each line is separately displayed. - The long-form plot requires additional configuration (i.e., mapping the \'hue\') but is usually more flexible and compatible with seaborn\'s functionalities. print(comparison_text)"},{"question":"# Python Coding Assessment Question Context You have been provided with a sample data file that contains a large amount of log data. Your task is to compress this file using the `gzip` module and then ensure that you can read the compressed file back correctly. This exercise will help you demonstrate your understanding of file operations and the `gzip` module in Python. Requirements 1. **Function 1: `compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None`** - This function should take the path of an input file (`input_file`), compress it using the highest compression level (default is 9), and save the compressed data to an output file (`output_file`). - **Input:** - `input_file`: A string representing the file path of the input file to be compressed. - `output_file`: A string representing the file path where the compressed file will be saved. - `compresslevel`: An integer (default is 9) indicating the level of compression. - **Output:** - None (The function should perform compression and save the compressed file). 2. **Function 2: `decompress_file(input_file: str, output_file: str) -> None`** - This function should take the path of a compressed input file (`input_file`), decompress it, and save the decompressed data to an output file (`output_file`). - **Input:** - `input_file`: A string representing the file path of the compressed file to be decompressed. - `output_file`: A string representing the file path where the decompressed file will be saved. - **Output:** - None (The function should perform decompression and save the decompressed file). 3. **Function 3: `verify_compression(original_file: str, decompressed_file: str) -> bool`** - This function should take the paths of the original file (`original_file`) and the decompressed file (`decompressed_file`), compare their contents, and return `True` if they are identical, otherwise `False`. - **Input:** - `original_file`: A string representing the file path of the original uncompressed file. - `decompressed_file`: A string representing the file path of the decompressed file. - **Output:** - A boolean indicating whether the original file and decompressed file have the same contents. Constraints - Ensure that the functions handle exceptions properly, such as when files do not exist. - Do not use any external libraries apart from the ones allowed (`gzip`, `shutil`, `os`, `io`). - Pay attention to file modes (\'rb\', \'wb\', etc.) as required by the `gzip` functions and methods. Example Usage ```python # Compress the file \'example.log\' into \'example.log.gz\' compress_file(\'example.log\', \'example.log.gz\') # Decompress the file \'example.log.gz\' back into \'example_decompressed.log\' decompress_file(\'example.log.gz\', \'example_decompressed.log\') # Verify that the decompressed file matches the original file is_identical = verify_compression(\'example.log\', \'example_decompressed.log\') print(is_identical) # Should print True if the files are identical ``` Make sure to provide function definitions, necessary imports, and any helper functions that you may use.","solution":"import gzip import shutil import os def compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None: Compresses an input file and writes it to an output file using gzip. Args: - input_file (str): Path to the input file. - output_file (str): Path where the compressed file will be saved. - compresslevel (int): Compression level (default is 9). with open(input_file, \'rb\') as f_in: with gzip.open(output_file, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) def decompress_file(input_file: str, output_file: str) -> None: Decompresses a gzip file and writes the contents to an output file. Args: - input_file (str): Path to the compressed input file. - output_file (str): Path where the decompressed file will be saved. with gzip.open(input_file, \'rb\') as f_in: with open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) def verify_compression(original_file: str, decompressed_file: str) -> bool: Verifies that the contents of the original file and the decompressed file are identical. Args: - original_file (str): Path to the original file. - decompressed_file (str): Path to the decompressed file. Returns: - bool: True if files are identical, otherwise False. with open(original_file, \'rb\') as file1, open(decompressed_file, \'rb\') as file2: return file1.read() == file2.read()"},{"question":"You are required to create an animated digital clock using the Turtle graphics module in Python. # Task: Implement a function named `digital_clock()` that displays the current time in HH:MM:SS format using turtle graphics. The clock should update every second to display the current time. # Requirements: 1. The function should create a turtle screen and set it up to a suitable size. 2. The time should be displayed prominently in the center of the screen. 3. Use the `write()` method to render the time. 4. Implement an update mechanism to refresh the time every second. 5. Ensure the turtle pen does not draw any lines when moving to update the time. # Constraints: - Utilize the `time` module to fetch the current time. - The turtle screen should shut down properly when clicked. - Implement the function in an efficient manner to handle the time updates. # Input and Output: - **Input**: No direct input. The function should continuously fetch the current system time. - **Output**: The function displays the time on the turtle screen. # Example usage: ```python import turtle import time def digital_clock(): screen = turtle.Screen() screen.setup(width=600, height=300) clock_turtle = turtle.Turtle() clock_turtle.hideturtle() clock_turtle.penup() clock_turtle.goto(0, 0) def update_time(): current_time = time.strftime(\\"%H:%M:%S\\") clock_turtle.clear() clock_turtle.write(current_time, align=\\"center\\", font=(\\"Arial\\", 48, \\"normal\\")) screen.ontimer(update_time, 1000) update_time() screen.mainloop() digital_clock() ``` # Explanation: - **screen**: Create and set up the turtle screen. - **clock_turtle**: Initialize a turtle object to display the time. - **update_time()**: Fetches the current time, clears the previous time output, and writes the new time. - **ontimer()**: Sets a timer to call `update_time()` every second ensuring continuous updates. - **mainloop()**: Keeps the turtle window open to display the clock. The function `digital_clock` will create an animated digital clock, updating every second to reflect the current time.","solution":"import turtle import time def digital_clock(): Displays the current time in HH:MM:SS format using turtle graphics. The time is updated every second. # Set up the screen screen = turtle.Screen() screen.title(\\"Digital Clock\\") screen.setup(width=600, height=300) screen.bgcolor(\\"black\\") # Initialize the turtle to display the time clock_turtle = turtle.Turtle() clock_turtle.hideturtle() clock_turtle.penup() clock_turtle.color(\\"white\\") clock_turtle.goto(0, 0) # Function to update the time display def update_time(): current_time = time.strftime(\\"%H:%M:%S\\") clock_turtle.clear() clock_turtle.write(current_time, align=\\"center\\", font=(\\"Arial\\", 48, \\"normal\\")) screen.ontimer(update_time, 1000) # Start the time update update_time() # Ensure the turtle screen shuts down properly when clicked screen.exitonclick() if __name__ == \\"__main__\\": digital_clock()"},{"question":"# Function Implementation Question Problem Statement In Python, generic types help in creating functions and classes that can handle different data types while maintaining type safety. Utilizing the `GenericAlias` object and concepts similar to what is described in the provided documentation, your task is to design a function that mimics the behavior of a generic container. You need to implement a `TypeHintGeneric` class that incorporates Python\'s type hinting system to create a generic container. This class should have the following functionality: 1. **Initialization**: Accept any type during initialization. 2. **Type Hinting**: Use generic type hinting to ensure type safety. 3. **Methods**: - `add_item(item)` to add an item to the container. - `get_items()` to return all items in a type-safe manner. - `get_type()` to return the type of items the container is designed to hold. Additionally, write a function `create_generic_container` that creates an instance of `TypeHintGeneric`, accepting a type and a list of items to initialize the container, then returns this instance. Input and Output Formats - **Input**: - The function `create_generic_container` should accept two arguments: 1. `item_type`: The type that the container is meant to hold. 2. `items`: A list of items to initialize the container with. - **Output**: - Return an instance of `TypeHintGeneric` initialized with the provided type and items. Constraints and Requirements - The class should handle any type (e.g., `int`, `str`, custom objects). - The `add_item` method should ensure that added items match the specified type. - You should ensure type safety using Python\'s type hinting and enforce it where necessary. - Performance should be optimal for general use cases. Example ```python from typing import TypeVar, Generic, List T = TypeVar(\'T\') class TypeHintGeneric(Generic[T]): def __init__(self, item_type: T): self.item_type = item_type self.items = [] def add_item(self, item: T): if not isinstance(item, self.item_type): raise TypeError(f\'Item must be of type {self.item_type.__name__}\') self.items.append(item) def get_items(self) -> List[T]: return self.items def get_type(self): return self.item_type def create_generic_container(item_type, items): container = TypeHintGeneric(item_type) for item in items: container.add_item(item) return container # Example Usage: int_container = create_generic_container(int, [1, 2, 3, 4]) print(int_container.get_items()) # Output: [1, 2, 3, 4] print(int_container.get_type()) # Output: <class \'int\'> str_container = create_generic_container(str, [\\"a\\", \\"b\\", \\"c\\"]) print(str_container.get_items()) # Output: [\'a\', \'b\', \'c\'] print(str_container.get_type()) # Output: <class \'str\'> ``` Note - Ensure that your solution works for different types and raises appropriate exceptions for type mismatches. - Use `TypeHintGeneric` and type hinting effectively to complete this task.","solution":"from typing import TypeVar, Generic, List T = TypeVar(\'T\') class TypeHintGeneric(Generic[T]): def __init__(self, item_type: T): self.item_type = item_type self.items = [] def add_item(self, item: T): if not isinstance(item, self.item_type): raise TypeError(f\'Item must be of type {self.item_type.__name__}\') self.items.append(item) def get_items(self) -> List[T]: return self.items def get_type(self): return self.item_type def create_generic_container(item_type, items): container = TypeHintGeneric(item_type) for item in items: container.add_item(item) return container"},{"question":"You are given a Pandas DataFrame that contains data about user activities on a website. The DataFrame has three columns: `user_id`, `login_time`, and `activity_duration`. The `login_time` is a `datetime` object representing the time the user logged in, and `activity_duration` is a `Timedelta` object representing the duration of their activity on the website. Your task is to write a function `analyze_user_activity` that takes this DataFrame and performs the following: 1. Calculate the total activity duration for each user. 2. Identify the user(s) with the maximum total activity duration. 3. Calculate the average activity duration per login for each user. 4. Return a DataFrame with the `user_id`, `total_activity_duration`, `average_activity_duration_per_login`, and a boolean column `max_activity_user` which is `True` for the user(s) with the maximum total activity duration and `False` otherwise. Function signature: ```python import pandas as pd def analyze_user_activity(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Input:** - `df` (pd.DataFrame): A DataFrame with columns: - `user_id` (int) - `login_time` (pd.Timestamp) - `activity_duration` (pd.Timedelta) **Output:** - A DataFrame with columns: - `user_id` (int) - `total_activity_duration` (pd.Timedelta) - `average_activity_duration_per_login` (pd.Timedelta) - `max_activity_user` (bool) **Constraints:** - The DataFrame can have multiple entries (`login_time` and `activity_duration`) for each `user_id`. - Assume the input DataFrame is valid (contains no missing values). **Example:** Given the following input DataFrame: ```python import pandas as pd data = { \'user_id\': [101, 102, 101, 103, 102, 101], \'login_time\': pd.to_datetime([ \'2023-01-01 08:00\', \'2023-01-01 09:00\', \'2023-01-02 08:00\', \'2023-01-02 09:00\', \'2023-01-03 09:00\', \'2023-01-03 10:00\' ]), \'activity_duration\': pd.to_timedelta([ \'1 hour\', \'2 hours\', \'30 minutes\', \'1 hour 30 minutes\', \'1 hour 15 minutes\', \'1 hour\' ]) } df = pd.DataFrame(data) ``` The output should be: ```python user_id total_activity_duration average_activity_duration_per_login max_activity_user 0 101 0 days 02:30:00 0 days 00:50:00 False 1 102 0 days 03:15:00 0 days 01:37:30 True 2 103 0 days 01:30:00 0 days 01:30:00 False ``` **Hint:** Use the `groupby` method and `Timedelta` operations to aggregate durations and calculate the required metrics.","solution":"import pandas as pd def analyze_user_activity(df: pd.DataFrame) -> pd.DataFrame: # Group by user_id and calculate total activity duration and count of logins grouped = df.groupby(\'user_id\').agg( total_activity_duration=pd.NamedAgg(column=\'activity_duration\', aggfunc=\'sum\'), login_count=pd.NamedAgg(column=\'login_time\', aggfunc=\'count\') ).reset_index() # Calculate average activity duration per login grouped[\'average_activity_duration_per_login\'] = (grouped[\'total_activity_duration\'] / grouped[\'login_count\']) # Identify the user(s) with the maximum total activity duration max_duration = grouped[\'total_activity_duration\'].max() grouped[\'max_activity_user\'] = grouped[\'total_activity_duration\'] == max_duration # Drop the login_count column as it is not required in the final output result = grouped.drop(columns=\'login_count\') return result"},{"question":"# File Path Interaction Check You are required to implement a function that determines if a given file path represents an interactive file in the system. This tutorial helps to demonstrate the handling of file descriptors and checks based on the operating system utilities available in the `python310` package. # Function Signature Create a Python function: ```python def is_interactive_file(file_path: str) -> bool: pass ``` # Input - `file_path`: A string representing the path to the file. # Output - Returns `True` if the file is interactive. - Returns `False` otherwise. # Constraints 1. You must use the `Py_FdIsInteractive` function internally to determine if the file is interactive. 2. Assume the file located at `file_path` exists and is accessible for reading. # Example ```python assert is_interactive_file(\'/dev/tty\') == True assert is_interactive_file(\'regular_file.txt\') == False ``` # Notes - You will need to consider using the `os` module to obtain the file pointer (`FILE *fp`) from the file path. - Handle the necessary imports and interfacing with the provided C functions to complete this task. - This function requires understanding of how file descriptors work and how to utilize C functions within Python using the `ctypes` library or similar methods.","solution":"import os import ctypes def is_interactive_file(file_path: str) -> bool: # Load the C standard library libc = ctypes.CDLL(None) # Open the file using the standard C library function `fopen` fopen = libc.fopen fopen.argtypes = [ctypes.c_char_p, ctypes.c_char_p] fopen.restype = ctypes.c_void_p # Check if the file was opened successfully fp = fopen(file_path.encode(\'utf-8\'), b\'r\') if not fp: return False # Function `fileno` gets the file descriptor from the FILE pointer fileno = libc.fileno fileno.argtypes = [ctypes.c_void_p] fileno.restype = ctypes.c_int fd = fileno(fp) # Define `Py_FdIsInteractive` as it is declared in cpython/pylifecycle.h # int Py_FdIsInteractive(FILE *fp, const char *filename) Py_FdIsInteractive = libc.Py_FdIsInteractive Py_FdIsInteractive.argtypes = [ctypes.c_void_p, ctypes.c_char_p] Py_FdIsInteractive.restype = ctypes.c_int # Check if file is interactive result = Py_FdIsInteractive(fp, file_path.encode(\'utf-8\')) # Function `fclose` to close the file fclose = libc.fclose fclose.argtypes = [ctypes.c_void_p] fclose.restype = ctypes.c_int fclose(fp) return result != 0"},{"question":"**Problem Statement:** Using the `seaborn.objects` module, create a function `create_custom_plot` that takes in the following parameters: - `x_values`: A list of integers representing the x-coordinates of the points. - `y_values`: A list of integers representing the y-coordinates of the points. - `x_limits`: A tuple containing the minimum and maximum values for the x-axis, or `None` for default values. - `y_limits`: A tuple containing the minimum and maximum values for the y-axis, or `None` for default values. - `invert_y`: A boolean value indicating whether to invert the y-axis (i.e., reverse the axis). The function should: 1. Create a line plot with the given x and y values, and add markers to the points. 2. Apply the specified limits to the x and y axes. 3. Invert the y-axis if `invert_y` is `True`. 4. Return the created plot object. **Constraints:** - The lengths of `x_values` and `y_values` are guaranteed to be equal and non-zero. - The `x_limits` and `y_limits` tuples contain either two integers representing min and max values, or `None` to indicate that the default limits should be used. **Input:** - `x_values`: List of integers (e.g., [1, 2, 3]) - `y_values`: List of integers (e.g., [1, 3, 2]) - `x_limits`: Tuple of two integers or `None` (e.g., (0, 4) or `None`) - `y_limits`: Tuple of two integers or `None` (e.g., (-1, 6) or `None`) - `invert_y`: Boolean (e.g., True or False) **Output:** - A `seaborn.objects.Plot` object with the specified properties. **Example:** ```python x_values = [1, 2, 3] y_values = [1, 3, 2] x_limits = (0, 4) y_limits = (-1, 6) invert_y = False plot = create_custom_plot(x_values, y_values, x_limits, y_limits, invert_y) plot.show() ``` In this example, the function creates a plot with x-values `[1, 2, 3]` and y-values `[1, 3, 2]`, sets the x-axis limits to `(0, 4)`, the y-axis limits to `(-1, 6)`, and does not invert the y-axis. **Note:** Your solution should utilize the `seaborn.objects` module effectively and demonstrate a good understanding of the package\'s functionalities.","solution":"import seaborn.objects as so def create_custom_plot(x_values, y_values, x_limits=None, y_limits=None, invert_y=False): Create a custom plot using seaborn.objects based on the given parameters. Parameters: - x_values: List of integers representing the x-coordinates of the points. - y_values: List of integers representing the y-coordinates of the points. - x_limits: Tuple containing the minimum and maximum values for the x-axis, or None for default values. - y_limits: Tuple containing the minimum and maximum values for the y-axis, or None for default values. - invert_y: Boolean indicating whether to invert the y-axis. Returns: - A seaborn.objects.Plot object with the specified properties. # Create the basic plot with line and markers plot = ( so.Plot(x=x_values, y=y_values) .add(so.Line(marker=\'o\')) ) # Set x-axis limits if specified if x_limits: plot = plot.limit(x=x_limits) # Set y-axis limits if specified if y_limits: plot = plot.limit(y=y_limits) # Invert y-axis if specified if invert_y: plot = plot.scale(y=(-1, \'reverse\')) return plot"},{"question":"# Question: Implementing a Batch Gradient Calculation Function with vmap You are required to implement a function using PyTorch and `torch.func` that computes the gradient of a given function `f` with respect to its input tensor over a batch of inputs. The function `f` must follow the constraints suitable for transformation using `torch.func`. Function Signature ```python def batch_gradient(f: Callable[[torch.Tensor], torch.Tensor], inputs: torch.Tensor) -> torch.Tensor: Computes the gradient of the function `f` over a batch of inputs. Parameters: f (Callable[[torch.Tensor], torch.Tensor]): The function for which the gradient needs to be computed. This function should take a single tensor input and produce a tensor output. inputs (torch.Tensor): A batch of inputs where the gradient needs to be computed. Should be a 2D tensor of shape (batch_size, input_dim). Returns: torch.Tensor: A tensor containing the gradients for each input in the batch. The output should be of shape (batch_size, input_dim). ``` Requirements and Constraints 1. **Input and Output Formats**: - The function `f` takes a single tensor input and returns a tensor. - `inputs` is a 2D tensor of shape ((text{batch_size}, text{input_dim})). - The output should be a 2D tensor of gradients with the same shape as `inputs`. 2. **Pure functions**: - The function `f` should be a pure function without side effects. 3. **Return All Outputs**: - Any intermediate computations within the function `f` must be returned if needed rather than being stored globally. 4. **Avoid Incompatible Operations**: - Avoid using in-place operations, out= operations, data-dependent control flows, dynamic shape operations, or item() inside the transformed functions. 5. **Handling Randomness**: - Ensure your function avoids random number generation within transformations unless explicitly controlled. Example ```python import torch def quadratic(x): return x.pow(2).sum() inputs = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True) expected_output = torch.tensor([[2.0, 4.0], [6.0, 8.0]]) output = batch_gradient(quadratic, inputs) assert torch.allclose(output, expected_output), \\"The output gradient is incorrect.\\" ``` Write a `batch_gradient` function that fulfills these requirements and constraints.","solution":"import torch from typing import Callable def batch_gradient(f: Callable[[torch.Tensor], torch.Tensor], inputs: torch.Tensor) -> torch.Tensor: Computes the gradient of the function `f` over a batch of inputs. Parameters: f (Callable[[torch.Tensor], torch.Tensor]): The function for which the gradient needs to be computed. This function should take a single tensor input and produce a tensor output. inputs (torch.Tensor): A batch of inputs where the gradient needs to be computed. Should be a 2D tensor of shape (batch_size, input_dim). Returns: torch.Tensor: A tensor containing the gradients for each input in the batch. The output should be of shape (batch_size, input_dim). batch_size, input_dim = inputs.shape gradients = torch.empty_like(inputs) for i in range(batch_size): input_var = inputs[i].clone().detach().requires_grad_(True) output_var = f(input_var) output_var.backward() gradients[i] = input_var.grad return gradients"},{"question":"**Question: Advanced Environment Variable Manipulation using POSIX** **Background:** The \\"posix\\" module provides functionalities for interacting with the operating system\'s environment variables. Although it is recommended to use the \\"os\\" module for portability and additional functionality, understanding the underlying \\"posix\\" module can provide deeper insights into system-level operations. **Task:** Your task is to create a function that modifies the environment variables of the current operating system process based on certain conditions. The function should add, update, or delete environment variables as specified by the input parameters. **Function Signature:** ```python def manage_env_variables(actions: list) -> dict: Manage environment variables according to the specified actions. Args: actions (list): A list of tuples where each tuple contains an action (\'add\', \'update\', \'delete\') and the corresponding environment variable name and value (for \'add\' and \'update\'). Examples: [(\'add\', \'MY_VAR\', \'value1\'), (\'update\', \'MY_VAR\', \'value2\'), (\'delete\', \'MY_VAR\')] Returns: dict: The updated environment variables after performing all actions. ``` **Input:** - `actions`: A list of tuples. Each tuple contains three elements when the action is \'add\' or \'update\' (action, variable name, variable value) and two elements when the action is \'delete\' (action, variable name). **Output:** - The function should return a dictionary representing the updated environment variables. **Constraints:** - The variable names should be strings and should follow environment variable naming conventions (containing only alphanumeric characters and underscores, starting with a letter or underscore). - The variable values should be strings. - The function should only modify the environment variables for the current process. - If an action tries to delete a non-existent variable, it should be ignored. - If an action tries to update a non-existent variable, it should be treated as an addition. **Example:** ```python # Example actions actions = [ (\'add\', \'MY_VAR1\', \'value1\'), (\'update\', \'MY_VAR1\', \'new_value1\'), (\'delete\', \'MY_VAR2\'), (\'add\', \'MY_VAR2\', \'value2\') ] # Example function call updated_env = manage_env_variables(actions) # Example output (actual values will depend on the initial environment) print(updated_env) # {\'MY_VAR1\': \'new_value1\', \'MY_VAR2\': \'value2\', ...} ``` Using this question, students will demonstrate their understanding of environment variables, list manipulation, and system-level operations using Python\'s os module.","solution":"import os def manage_env_variables(actions): Manage environment variables according to the specified actions. Args: actions (list): A list of tuples where each tuple contains an action (\'add\', \'update\', \'delete\') and the corresponding environment variable name and value (for \'add\' and \'update\'). Returns: dict: The updated environment variables after performing all actions. for action in actions: if action[0] == \'add\': _, var_name, var_value = action os.environ[var_name] = var_value elif action[0] == \'update\': _, var_name, var_value = action os.environ[var_name] = var_value elif action[0] == \'delete\': _, var_name = action if var_name in os.environ: del os.environ[var_name] # Return a copy of the current environment variables dictionary return dict(os.environ)"},{"question":"Custom Layer Initialization in PyTorch As a developer, you are tasked with creating a custom neural network layer in PyTorch that utilizes specific initialization methods for its weights and biases. This will assess your understanding of PyTorch\'s initialization functions and how to apply them within a neural network layer. Requirements: 1. Implement a custom layer class `CustomLinear` that inherits from `torch.nn.Module`. 2. The layer should work like a standard linear (fully connected) layer but must initialize its weights using the Xavier uniform distribution and its biases to ones. 3. The layer should have an optional argument to enable bias; when bias is `False`, the layer should omit the bias term. 4. Implement a `forward` method that performs the linear transformation on the input data. Expected Inputs and Outputs: - **Input:** - `input_features`: Integer, the number of features in the input. - `output_features`: Integer, the number of features in the output. - `bias`: Boolean, optional, default to `True`. Indicates whether to include a bias term. - **Output:** - A custom layer instance that can be used in a PyTorch neural network model. Constraints: - You must use `torch.nn.init.xavier_uniform_` for weight initialization. - You must use `torch.nn.init.ones_` for bias initialization. Example Usage: ```python import torch import torch.nn as nn class CustomLinear(nn.Module): def __init__(self, input_features, output_features, bias=True): super(CustomLinear, self).__init__() # Initialize weights and bias here # Your code here def forward(self, x): # Implement the forward pass # Your code here # Example of how to use the custom layer input_features = 10 output_features = 5 layer = CustomLinear(input_features, output_features, bias=True) print(layer) # Expected output includes the initialized weights and bias ``` Performance Requirements: - The layer should be optimized to handle input tensors of varying dimensions efficiently. Good luck, and ensure your implementation adheres to the best practices of PyTorch development!","solution":"import torch import torch.nn as nn import torch.nn.init as init class CustomLinear(nn.Module): def __init__(self, input_features, output_features, bias=True): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.Tensor(output_features, input_features)) self.bias = nn.Parameter(torch.Tensor(output_features)) if bias else None self.reset_parameters() def reset_parameters(self): init.xavier_uniform_(self.weight) if self.bias is not None: init.ones_(self.bias) def forward(self, x): if self.bias is None: return torch.matmul(x, self.weight.t()) else: return torch.matmul(x, self.weight.t()) + self.bias"},{"question":"# PyTorch Random Number Generation and Application Problem Statement You are tasked with implementing a function using PyTorch\'s `torch.random` module. Your function should generate random tensors with specified properties and verify these properties programmatically. After generating the tensors, use them to perform a simple operation that demonstrates their practical use in machine learning. Function Specification and Requirements 1. **Function Name:** `generate_and_apply_random_tensors` 2. **Parameters:** - `shape` (tuple): The shape of the tensor to be generated. - `min_val` (float): The minimum value for the random numbers in the tensor. - `max_val` (float): The maximum value for the random numbers in the tensor. - `operation` (str): The operation to be applied. Could be \'mean\', \'sum\', or \'product\'. 3. **Returns:** - A dictionary with the following keys: - `\'tensor\'`: The generated random tensor of the given shape and within the specified range. - `\'result\'`: The result of the operation applied to the tensor. Details and Constraints 1. The random tensor should be generated using a uniform distribution between `min_val` and `max_val`. 2. The function should validate whether the values of the tensor lie within the specified range `[min_val, max_val]`. 3. The function should perform the specified operation (\'mean\', \'sum\', or \'product\') on the tensor: - `\'mean\'` should return the mean value of all elements. - `\'sum\'` should return the sum of all elements. - `\'product\'` should return the product of all elements. 4. Assume that tensor elements can contain fractions (so floating point precision is expected). Example ```python def generate_and_apply_random_tensors(shape, min_val, max_val, operation): # Your implementation here # Example usage: result = generate_and_apply_random_tensors((3, 3), 0.0, 1.0, \'mean\') print(result) # Example output: # { # \'tensor\': tensor([...]), # 3x3 tensor with values between 0.0 and 1.0 # \'result\': 0.5 # Example mean value of the tensor elements # } ``` The function should be well-documented with comments explaining each step of the process.","solution":"import torch def generate_and_apply_random_tensors(shape, min_val, max_val, operation): Generates a random tensor of the given shape with values uniformly distributed between min_val and max_val, and applies a specified operation on the tensor. Parameters: - shape (tuple): The shape of the tensor to be generated. - min_val (float): The minimum value for the random numbers in the tensor. - max_val (float): The maximum value for the random numbers in the tensor. - operation (str): The operation to be applied. Could be \'mean\', \'sum\', or \'product\'. Returns: - dict: The generated tensor and the result of applying the operation. # Generate random tensor tensor = torch.empty(shape).uniform_(min_val, max_val) # Ensure tensor values lie within [min_val, max_val] assert torch.all(tensor >= min_val) and torch.all(tensor <= max_val), \'Tensor values are out of bounds\' # Apply specified operation result = None if operation == \'mean\': result = torch.mean(tensor).item() elif operation == \'sum\': result = torch.sum(tensor).item() elif operation == \'product\': result = torch.prod(tensor).item() else: raise ValueError(\\"Invalid operation. Must be \'mean\', \'sum\', or \'product\'\\") return { \'tensor\': tensor, \'result\': result }"},{"question":"# Question: WAV File Manipulation You are required to implement a function that reads a WAV file, applies a simple transformation to its audio data, and writes the transformed audio data to a new WAV file. The transformation you will apply is doubling the amplitude of the audio signal. Requirements 1. The function should be named `process_wav_file`. 2. It should accept two parameters: - `input_file_path` (str): The path to the input WAV file. - `output_file_path` (str): The path where the transformed WAV file will be saved. 3. The processing should involve reading the audio data from the input file, doubling the amplitude of each sample, and writing the modified data into the output file. 4. Ensure that the function maintains the same audio format (e.g., sample width, framerate) as the original file. Constraints - The input WAV file can be assumed to be in PCM format. - Handle errors gracefully, such as file not found or incorrect file format. - Performance should be optimized for WAV files of moderate size (up to 10MB). Function Signature ```python def process_wav_file(input_file_path: str, output_file_path: str) -> None: pass ``` Example Given an input WAV file `input.wav`, your function should create a new file `output.wav` with doubled amplitude audio. ```python # Example usage: process_wav_file(\\"input.wav\\", \\"output.wav\\") ``` In the above example, if `input.wav` had a sample data [1, -1, 2, -2], then `output.wav` should have the sample data [2, -2, 4, -4]. Hints - You may use the `wave` module available in Python for reading and writing WAV files. - Ensure that you correctly handle the byte data and preserve the audio format. - Be cautious with overflow when doubling the amplitude; you may need to clamp values to the valid range of the amplitude. Good luck!","solution":"import wave import os import struct def process_wav_file(input_file_path: str, output_file_path: str) -> None: try: with wave.open(input_file_path, \'rb\') as wav_in: n_channels = wav_in.getnchannels() sample_width = wav_in.getsampwidth() framerate = wav_in.getframerate() n_frames = wav_in.getnframes() audio_data = wav_in.readframes(n_frames) if sample_width == 1: # 8-bit audio audio_format = \'B\' max_amplitude = 127 min_amplitude = -128 elif sample_width == 2: # 16-bit audio audio_format = \'h\' max_amplitude = 32767 min_amplitude = -32768 else: raise ValueError(\\"Unsupported sample width\\") # Unpacking the audio data, doubling amplitude, clamping, and repacking frame_data = struct.unpack(\'<\' + audio_format * (n_frames * n_channels), audio_data) new_frame_data = [] for sample in frame_data: new_sample = sample * 2 if new_sample > max_amplitude: new_sample = max_amplitude elif new_sample < min_amplitude: new_sample = min_amplitude new_frame_data.append(new_sample) new_audio_data = struct.pack(\'<\' + audio_format * len(new_frame_data), *new_frame_data) with wave.open(output_file_path, \'wb\') as wav_out: wav_out.setnchannels(n_channels) wav_out.setsampwidth(sample_width) wav_out.setframerate(framerate) wav_out.writeframes(new_audio_data) except FileNotFoundError: print(f\\"File not found: {input_file_path}\\") except wave.Error as e: print(f\\"Error processing WAV file: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# SAX XML Parsing with `xml.sax.xmlreader` **Objective:** Implement a class that uses the `xml.sax.xmlreader.IncrementalParser` to parse a given XML file incrementally and extract specific information. The task will assess your understanding of SAX parsing, event handling, and working with the `xml.sax.xmlreader` module. **Problem Statement:** You need to implement a SAX parser that extracts and prints the names and values of all attributes of each element in an XML document, handling the input data in chunks. Specifically, you are required to: 1. Implement a class `MyIncrementalParser` inheriting from `xml.sax.xmlreader.IncrementalParser`. 2. In the parsing process, use the methods `feed` and `close` to handle the XML content incrementally. 3. Implement methods to handle the start of elements, their attributes, and any errors encountered during parsing. 4. Use an instance of `xml.sax.xmlreader.InputSource` to encapsulate input data. # Requirements: **Class: `MyIncrementalParser`** - Inherits from `xml.sax.xmlreader.IncrementalParser`. **Methods to Implement:** - `feed(data: str)`: Handle a chunk of XML data. - `close()`: Finalize parsing and ensure well-formedness of the document. - Override the `startElement` method to print element names and their attributes. - Implement necessary error handling for the SAX parsing process. **Input:** - A string containing the XML data, provided to the `feed` method in chunks. - Ensure to accurately simulate the input source by calling feed multiple times, followed by a call to `close`. **Output:** - Print the names and attributes of each element in the XML document. **Constraints:** - Handle errors gracefully and print appropriate error messages. - Assume that all XML data provided is valid for the scope of this problem. # Example Usage: ```python import xml.sax.xmlreader class MyIncrementalParser(xml.sax.xmlreader.IncrementalParser): def __init__(self): super().__init__() self.reset() def feed(self, data): super().feed(data) def close(self): super().close() print(\\"Parsing completed successfully.\\") def startElement(self, name, attrs): print(f\\"Element: {name}\\") for attr_name, attr_value in attrs.items(): print(f\\" - Attribute: {attr_name} = {attr_value}\\") def error(self, exception): print(f\\"Error: {exception}\\") # Example of using MyIncrementalParser with incremental data feeding. xml_data = <root><child attr=\\"value\\">Text</child><child>More Text</child></root> parser = MyIncrementalParser() chunk_size = 10 for i in range(0, len(xml_data), chunk_size): chunk = xml_data[i:i+chunk_size] parser.feed(chunk) parser.close() ``` **Notes:** - This example usage feeds the XML data to the parser in chunks of 10 characters. - You must implement the `MyIncrementalParser` class to handle SAX events and errors correctly.","solution":"import xml.sax import xml.sax.xmlreader class MyIncrementalParser(xml.sax.xmlreader.IncrementalParser): def __init__(self): super().__init__() self._parser = xml.sax.make_parser() self._handler = MyContentHandler() self._parser.setContentHandler(self._handler) self._buffer = \\"\\" def feed(self, data): self._buffer += data try: self._parser.feed(self._buffer) self._buffer = \\"\\" except xml.sax.SAXParseException as e: self.handle_error(e) def close(self): try: self._parser.close() print(\\"Parsing completed successfully.\\") except xml.sax.SAXParseException as e: self.handle_error(e) def handle_error(self, exception): print(f\\"Error: {exception}\\") class MyContentHandler(xml.sax.handler.ContentHandler): def startElement(self, name, attrs): print(f\\"Element: {name}\\") for attr_name, attr_value in attrs.items(): print(f\\" - Attribute: {attr_name} = {attr_value}\\")"},{"question":"**Asyncio Synchronization Primitives: `AsyncWriter`** You are tasked with implementing an `AsyncWriter` class that manages access to a shared resource: a file. This class should ensure that multiple asyncio tasks do not access the file simultaneously using the `asyncio` synchronization primitives described in the documentation. # Requirements: 1. **Constructor:** - The constructor should initialize a lock and an event. 2. **write(data) Method:** - This method should take a string `data` and write it to the file sequentially. - Ensure that only one task can write to the file at a time using the lock. - After writing, trigger an event indicating that data has been written. 3. **read() Method:** - This method should asynchronously wait until the event is set. - After being triggered, read the file content and return it. 4. **clear() Method:** - This method should clear the event, allowing for subsequent data writes and reads. # Implementation Details: - All file operations should ensure that tasks do not interfere with each other. - The file should be opened initially, and the class should manage file handles properly to ensure consistency. # Input & Output: - **write(data)**: `data` is a string to be written to the file. - **read()**: Returns the content of the file as a string after the last write operation. - **clear()**: No input or output; simply resets the state for future writes. # Example Usage: ```python import asyncio class AsyncWriter: def __init__(self, filename): self.filename = filename self.lock = asyncio.Lock() self.event = asyncio.Event() async def write(self, data): async with self.lock: with open(self.filename, \'a\') as f: f.write(data) self.event.set() async def read(self): await self.event.wait() async with self.lock: with open(self.filename, \'r\') as f: content = f.read() return content def clear(self): self.event.clear() # Example Usage: async def main(): writer = AsyncWriter(\'test.txt\') await writer.write(\'Hello\') print(await writer.read()) # Should print \'Hello\' writer.clear() asyncio.run(main()) ``` # Constraints: - Ensure that the lock is acquired before writing and reading to avoid race conditions. - The event should properly signal the completion of a write operation. # Performance: - The implementation should efficiently handle concurrent writes and reads without unnecessary blocking.","solution":"import asyncio class AsyncWriter: def __init__(self, filename): self.filename = filename self.lock = asyncio.Lock() self.event = asyncio.Event() async def write(self, data): async with self.lock: with open(self.filename, \'a\') as f: f.write(data) self.event.set() async def read(self): await self.event.wait() async with self.lock: with open(self.filename, \'r\') as f: content = f.read() return content def clear(self): self.event.clear()"},{"question":"# Task You are to implement a Python program that uses the `enum` module to manage a series of file permissions in a secure system. The permissions must be easily combinable using the methods provided by the `IntFlag` class to follow best practices for readability and maintainability. # Requirements 1. **Define an IntFlag Enumeration:** - Create an `IntFlag` enumeration called `Permission`, which includes `READ`, `WRITE`, and `EXECUTE` permissions. 2. **Implement Permission Checking:** - Implement a function `has_permission(user_permissions: Permission, required_permission: Permission) -> bool` that checks if the `user_permissions` include the `required_permission`. 3. **Union of Permissions:** - Implement a function `union_permissions(*permissions: Permission) -> Permission` that takes multiple `Permission` arguments and returns a single `Permission` instance representing the union of all permissions. 4. **Demonstrate Usage:** - Create a sample user permission set combining `READ` and `WRITE`. - Check if the user has `EXECUTE` permission. - Combine `READ`, `WRITE`, and `EXECUTE` permissions and verify them against the sample set for comprehensive validation. # Input and Output - **Function `has_permission`** - *Input*: - `user_permissions`: A flag combining several permissions. - `required_permission`: A single `Permission` to check against the `user_permissions`. - *Output*: - `True` if the `user_permissions` include the `required_permission`, otherwise `False`. - **Function `union_permissions`** - *Input*: - A variable number of `Permission` flags. - *Output*: - A single `Permission` flag representing the combined permissions. # Constraints 1. Only the permissions defined in the `Permission` enum should be used. 2. The solution should be efficient and utilize the functionalities provided by the `IntFlag` class. # Coding Example Implement your solution with the following starting template: ```python from enum import IntFlag class Permission(IntFlag): READ = 1 WRITE = 2 EXECUTE = 4 def has_permission(user_permissions: Permission, required_permission: Permission) -> bool: # Your code here pass def union_permissions(*permissions: Permission) -> Permission: # Your code here pass # Demonstration of usage if __name__ == \\"__main__\\": user_permissions = union_permissions(Permission.READ, Permission.WRITE) print(f\\"User has EXECUTE permission: {has_permission(user_permissions, Permission.EXECUTE)}\\") combined_permissions = union_permissions(Permission.READ, Permission.WRITE, Permission.EXECUTE) print(f\\"Combined permissions include user permissions: {has_permission(combined_permissions, user_permissions)}\\") ``` # Evaluation Your implementation will be evaluated on the following criteria: 1. Correct and efficient use of the `enum` module and `IntFlag` class. 2. Accurate implementation of the permission checking and union functionalities. 3. Clear and readable code with proper comments and documentation. 4. Successful demonstration of implemented functions with the provided examples.","solution":"from enum import IntFlag class Permission(IntFlag): READ = 1 WRITE = 2 EXECUTE = 4 def has_permission(user_permissions: Permission, required_permission: Permission) -> bool: Check if user_permissions include the required_permission. return (user_permissions & required_permission) == required_permission def union_permissions(*permissions: Permission) -> Permission: Combine multiple permissions into a single Permission instance. combined_permission = Permission(0) for permission in permissions: combined_permission |= permission return combined_permission # Demonstration of usage if __name__ == \\"__main__\\": user_permissions = union_permissions(Permission.READ, Permission.WRITE) print(f\\"User has EXECUTE permission: {has_permission(user_permissions, Permission.EXECUTE)}\\") combined_permissions = union_permissions(Permission.READ, Permission.WRITE, Permission.EXECUTE) print(f\\"Combined permissions include user permissions: {has_permission(combined_permissions, user_permissions)}\\")"},{"question":"**Challenging Coding Question on PyTorch Normalization Layers** Objective: Write a PyTorch class that takes an existing neural network model and a specified normalization method, then replaces all BatchNorm layers in the model with the chosen normalization method. Your implementation should support both GroupNorm and BatchNorm with `track_running_stats=False`. # Function Signature ```python def replace_batchnorm(model: torch.nn.Module, method: str, num_groups: int = 1) -> torch.nn.Module: Replaces all BatchNorm layers in a given PyTorch model with the specified normalization method. Args: - model (torch.nn.Module): The original PyTorch model containing BatchNorm layers. - method (str): The normalization method to use. Valid options are \'GroupNorm\' and \'BatchNorm\'. - num_groups (int, optional): The number of groups for \'GroupNorm\'. Defaults to 1. Returns: - torch.nn.Module: The modified model with the specified normalization layers. ``` # Constraints 1. `method` must be either \'GroupNorm\' or \'BatchNorm\'. If `method` is \'GroupNorm\', use `num_groups` to configure it. 2. Your function should maintain the original structure and weights of the model, except for the normalization layers being replaced. 3. If `method` is \'BatchNorm\', ensure that the `track_running_stats` parameter is set to `False`. # Input - A PyTorch model containing multiple layers including BatchNorm layers. - A string specifying the desired normalization method (\'GroupNorm\' or \'BatchNorm\'). - An integer indicating the number of groups for GroupNorm (when applicable). # Output - A modified PyTorch model where all BatchNorm layers have been replaced according to the specified method. # Example ```python import torch import torch.nn as nn # Example model definition class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(3, 32, 3, 1) self.bn1 = nn.BatchNorm2d(32) self.conv2 = nn.Conv2d(32, 64, 3, 1) self.bn2 = nn.BatchNorm2d(64) self.fc1 = nn.Linear(9216, 128) self.bn3 = nn.BatchNorm1d(128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = torch.relu(x) x = self.conv2(x) x = self.bn2(x) x = torch.relu(x) x = torch.flatten(x, 1) x = self.fc1(x) x = self.bn3(x) x = torch.relu(x) x = self.fc2(x) return x # Replace function usage model = SimpleModel() new_model = replace_batchnorm(model, \'GroupNorm\', num_groups=4) # Now, `new_model` should have all BatchNorm layers replaced with GroupNorm with 4 groups. ``` # Notes - Do not change the number of channels or other properties of other layers in the model. - For simplicity, limit your solution to models using `BatchNorm2d` or `BatchNorm1d`. - Ensure that your function is tested on a variety of models, including those from torchvision.","solution":"import torch import torch.nn as nn def replace_batchnorm(model: torch.nn.Module, method: str, num_groups: int = 1) -> torch.nn.Module: Replaces all BatchNorm layers in a given PyTorch model with the specified normalization method. Args: - model (torch.nn.Module): The original PyTorch model containing BatchNorm layers. - method (str): The normalization method to use. Valid options are \'GroupNorm\' and \'BatchNorm\'. - num_groups (int, optional): The number of groups for \'GroupNorm\'. Defaults to 1. Returns: - torch.nn.Module: The modified model with the specified normalization layers. if method not in [\'GroupNorm\', \'BatchNorm\']: raise ValueError(\\"Method must be either \'GroupNorm\' or \'BatchNorm\'\\") def replace_module(module): for name, child in module.named_children(): if isinstance(child, (nn.BatchNorm2d, nn.BatchNorm1d)): if method == \'GroupNorm\': num_channels = child.num_features if isinstance(child, nn.BatchNorm1d) else child.num_features new_layer = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels) elif method == \'BatchNorm\': new_layer = type(child)(child.num_features, eps=child.eps, momentum=child.momentum, affine=child.affine, track_running_stats=False) module.add_module(name, new_layer) else: replace_module(child) replace_module(model) return model"},{"question":"**Objective**: Implement and configure logging using the Unix `syslog` library routines in Python. **Problem Statement**: You are required to write a Python script that logs various messages at different priority levels to the system logger. Additionally, you should configure the logging options to include the process ID in the logged messages and use the mail logging facility. After logging a set of messages, you should reset the logging configuration. **Tasks**: 1. **Configure log options**: - Use `syslog.openlog` to set the logging options: - Include the process ID in the log messages. - Use the mail logging facility. 2. **Log messages at various priority levels**: - Log an informational message stating that the logging process has started. - Log a warning message. - Log an error message. 3. **Reset the logging configuration** by calling `syslog.closelog`. 4. **Log an informational message** after resetting the configuration to demonstrate that it uses the default settings again. **Requirements**: - The script should not produce any uncaught exceptions. - The logged messages should be appropriately tagged with their respective priority levels. - After resetting the configuration, the subsequent log message should not include the process ID and should use the default facility. **Input and Output**: - There are no input arguments for the script. - The output will be the messages logged to the system logger, which can be viewed using system log tools (e.g., `tail -f /var/log/syslog` on Unix systems). **Hints**: - Use `syslog.LOG_INFO`, `syslog.LOG_WARNING`, and `syslog.LOG_ERR` for logging messages at different priority levels. - The `syslog.LOG_PID` option includes the process ID in each log message. - The `syslog.LOG_MAIL` facility directs the log messages to the mail subsystem. **Example**: Here is an example of how your implementation might appear: ```python import syslog def configure_logging(): # Open log with process ID and mail facility syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) def log_messages(): # Log messages with different priority levels syslog.syslog(syslog.LOG_INFO, \'Logging process started\') syslog.syslog(syslog.LOG_WARNING, \'This is a warning message\') syslog.syslog(syslog.LOG_ERR, \'This is an error message\') def reset_logging(): # Close the logging session and reset configuration syslog.closelog() def main(): configure_logging() log_messages() reset_logging() # Log another message to demonstrate reset configuration syslog.syslog(syslog.LOG_INFO, \'Logging after reset\') if __name__ == \'__main__\': main() ``` **Note**: You need system-level permissions to access the system log, and appropriate Unix system configurations are required for the syslog module to function properly.","solution":"import syslog def configure_logging(): Configures syslog to include the process id in the log messages and to use the mail logging facility. syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) def log_messages(): Logs messages at different priority levels. syslog.syslog(syslog.LOG_INFO, \'Logging process started\') syslog.syslog(syslog.LOG_WARNING, \'This is a warning message\') syslog.syslog(syslog.LOG_ERR, \'This is an error message\') def reset_logging(): Resets the logging configuration. syslog.closelog() def log_after_reset(): Logs an informational message after the logging configuration has been reset. syslog.syslog(syslog.LOG_INFO, \'Logging after reset\') def main(): configure_logging() log_messages() reset_logging() log_after_reset() if __name__ == \'__main__\': main()"},{"question":"You have been tasked with simulating a basic inventory system for a small retail store. This system should keep track of product levels and simulate customer purchases over time. You must use the `random` module to generate customer arrival times and the quantities of products they purchase, following specific probability distributions. The simulation will run over a period of one week (7 days). Task Implement a function `simulate_inventory()` that performs the following: 1. **Initial Inventory Setup**: - Start with a list of products, each with an initial stock level. - The inventory is a dictionary where keys are product names and values are integers representing their stock. 2. **Customer Simulation**: - Customers arrive following an exponential distribution with a mean interval of 1 hour (use `expovariate()`). - Each customer purchases products: - The number of different products a customer buys follows a normal distribution with a mean of 3 and a standard deviation of 1 (use `normalvariate()`). - For each product the customer buys, the quantity is uniformly distributed between 1 and 5 (use `uniform()`). - The type of products purchased by a customer is chosen randomly from the inventory (use `choice()`). 3. **Inventory Management**: - Update the inventory levels after each customer purchase. - If the stock of a product reaches zero, it remains zero (no negative stock levels allowed). 4. **Simulation Period**: - Run the simulation continuously for one week (7 days). - Track the time and ensure customer arrivals and purchases are correctly spaced and recorded. 5. **Output**: - At the end of the simulation, output the final inventory levels of each product. Constraints - Use the `random` module functions appropriately as specified. - Assume there are no restocks during the simulation period. - Simulate realistically to reflect a store’s operating hours (e.g., consider that the store operates 24 hours for simplicity). # Example ```python def simulate_inventory(): import random from statistics import mean # Initial inventory setup inventory = { \'apples\': 50, \'bananas\': 40, \'oranges\': 30, \'grapes\': 20, \'pears\': 60 } total_hours = 7 * 24 # Simulating one week current_time = 0 while current_time < total_hours: current_time += random.expovariate(1.0 / 1.0) # mean interval of 1 hour if current_time >= total_hours: break num_products = max(1, int(random.normalvariate(3, 1))) for _ in range(num_products): product = random.choice(list(inventory.keys())) quantity = int(random.uniform(1, 5)) if inventory[product] > 0: inventory[product] = max(0, inventory[product] - quantity) return inventory # Example call to the function print(simulate_inventory()) ``` Notes - Ensure to import the `random` module correctly. - Handle edge cases such as ensuring product quantities and the number of products are within realistic ranges. - You can assume the starting `inventory` is given as shown in the example. # Evaluation Criteria - **Correctness**: The function should simulate the scenario accurately according to the described distributions. - **Readability**: Code should be well-organized and readable. - **Performance**: Simulation should execute efficiently within a reasonable time for the given constraints.","solution":"import random def simulate_inventory(): Simulates an inventory system for a small retail store over a period of one week (7 days). Returns: inventory (dict): Final inventory levels of each product at the end of the simulation. # Initial inventory setup inventory = { \'apples\': 50, \'bananas\': 40, \'oranges\': 30, \'grapes\': 20, \'pears\': 60 } total_hours = 7 * 24 # Simulating one week (7 days * 24 hours) current_time = 0.0 while current_time < total_hours: current_time += random.expovariate(1.0) # mean interval of 1 hour if current_time >= total_hours: break num_products = max(1, int(random.normalvariate(3, 1))) for _ in range(num_products): product = random.choice(list(inventory.keys())) quantity = int(random.uniform(1, 5)) if inventory[product] > 0: inventory[product] = max(0, inventory[product] - quantity) return inventory"},{"question":"Background In sequence models, it is often crucial to ensure that the model does not violate causality—i.e., when generating or predicting the next token in a sequence, it should not consider future tokens. This is typically achieved by applying causal masks to the attention scores. Task You are to implement a simple function using PyTorch that performs the following: 1. Create a causal mask using the provided `causal_lower_right` function from `torch.nn.attention.bias`. 2. Apply this mask to a randomly initialized attention score tensor. 3. Define a class `CausalMaskedAttention`, which contains methods for generating attention scores and applying the causal mask. Specifications - **Input**: - An integer `sequence_length` (length of the sequence). - An integer `d_model` (dimensionality of the model). - **Output**: - A tensor representing the masked attention scores. - **Constraints**: - You must use `torch` for tensor operations. - Your implementation should be efficient and make use of PyTorch\'s built-in operations. - The mask should ensure that each position `i` in the sequence only attends to positions `<= i`. Code Template ```python import torch from torch.nn.attention.bias import causal_lower_right class CausalMaskedAttention: def __init__(self, sequence_length, d_model): self.sequence_length = sequence_length self.d_model = d_model def generate_attention_scores(self): # Initialize random attention scores. attention_scores = torch.rand(self.sequence_length, self.sequence_length, self.d_model) return attention_scores def apply_causal_mask(self, attention_scores): # Generate causal mask using causal_lower_right mask = causal_lower_right(self.sequence_length, self.sequence_length) # Apply mask to attention scores (broadcast if necessary) masked_scores = attention_scores + mask return masked_scores # Example Usage: sequence_length = 5 d_model = 8 ca = CausalMaskedAttention(sequence_length, d_model) attention_scores = ca.generate_attention_scores() masked_scores = ca.apply_causal_mask(attention_scores) print(masked_scores) ``` Notes - You only need to implement the `generate_attention_scores` and `apply_causal_mask` methods. - Ensure your causal mask is correctly generated and applied such that future positions are masked out correctly. Testing - Verify your implementation by checking output shapes and ensuring that positions attending to future tokens are indeed masked.","solution":"import torch class CausalMaskedAttention: def __init__(self, sequence_length, d_model): self.sequence_length = sequence_length self.d_model = d_model def generate_attention_scores(self): # Initialize random attention scores. attention_scores = torch.rand(self.sequence_length, self.sequence_length, self.d_model) return attention_scores def apply_causal_mask(self, attention_scores): # Generate upper triangular mask using triu (1\'s where j >= i, -inf where j < i) mask = torch.triu(torch.ones(self.sequence_length, self.sequence_length), diagonal=1).to(torch.bool) mask = mask.unsqueeze(-1).expand(-1, -1, self.d_model) # Apply mask to attention scores (broadcast if necessary) attention_scores = attention_scores.masked_fill(mask, float(\'-inf\')) return attention_scores # Example Usage: sequence_length = 5 d_model = 8 ca = CausalMaskedAttention(sequence_length, d_model) attention_scores = ca.generate_attention_scores() masked_scores = ca.apply_causal_mask(attention_scores) print(masked_scores)"},{"question":"**Problem Statement: Configuration File Processor** You are tasked with developing a Python function, `process_config(file_path)`, to parse a configuration file in INI format using the `configparser` module. The function should: 1. **Read** the configuration from the given file. 2. **Validate** that certain sections and options exist. 3. **Handle fallback values** for missing options. 4. **Output a specific parameter value** after processing. **Requirements**: - The configuration file must contain the following mandatory sections and their respective options: - Section `[General]` with options: - `app_name` (string): The name of the application. - `version` (string): The version of the application. - Section `[Settings]` with options: - `theme` (string): The theme of the application. - `language` (string): The language setting for the application. - If the `theme` option is not provided in the `[Settings]` section, it should default to `\\"light\\"`. - After reading and validating the configuration, the function should return the value of `theme` from the `[Settings]` section. **Constraints**: - The input file is guaranteed to have a `[Settings]` section, but options within it might be missing. - Any missing `app_name` or `version` in the [General] section should raise a `ValueError` with the message indicating which option is missing. **Function Signature**: ```python def process_config(file_path: str) -> str: pass ``` **Input**: - `file_path` (string): The file path to the INI configuration file. **Output**: - (string): The `theme` value from the `[Settings]` section. **Example**: ```ini # config.ini [General] app_name = MyApp version = 1.0 [Settings] language = en ``` For the above file, calling `process_config(\\"config.ini\\")` should return `\\"light\\"`. **Explanation**: The configuration file does not provide the `theme` option within the `[Settings]` section. The function should use the fallback value `\\"light\\"`. **Note**: Ensure to handle files and exceptions appropriately, making the solution robust for different edge cases.","solution":"import configparser import os def process_config(file_path: str) -> str: Process the configuration file and return the value of \'theme\' from the [Settings] section. Defaults to \'light\' if \'theme\' is not provided. Raises a ValueError if \'app_name\' or \'version\' is missing in the [General] section. if not os.path.exists(file_path): raise FileNotFoundError(\'Configuration file does not exist.\') config = configparser.ConfigParser() config.read(file_path) # Validate [General] section if not config.has_section(\'General\'): raise ValueError(\'Section [General] not found.\') if not config.has_option(\'General\', \'app_name\'): raise ValueError(\'Option app_name is missing in section [General]\') if not config.has_option(\'General\', \'version\'): raise ValueError(\'Option version is missing in section [General]\') # Retrieve [Settings] section values with fallbacks theme = config.get(\'Settings\', \'theme\', fallback=\'light\') return theme"},{"question":"# Custom Visualization with Scikit-learn\'s Plotting API You are required to implement a custom display class for visualizing Precision-Recall Curves using Scikit-learn\'s plotting API. Follow the structure provided in the documentation to create this class. Requirements: 1. Define a class `PrecisionRecallDisplay`. 2. Implement the `__init__` method to initialize necessary attributes like precision, recall, and estimator name. 3. Create class methods `from_estimator` and `from_predictions` to generate the display from a model or from prediction values. 4. Implement the `plot` method to display the precision-recall curve, ensuring it supports visualization customization. 5. Provide support for plotting within a single axis or multiple axes using matplotlib’s GridSpec if required. Input: - For `from_estimator`: - An estimator object (fitted model). - Feature matrix `X` (numpy array). - Target vector `y` (numpy array). - For `from_predictions`: - True values vector `y_true` (numpy array). - Predicted probabilities vector `y_pred` (numpy array). - Estimator name (string). Output: - The `plot` method should return a matplotlib figure object. Constraints: - The precision-recall curve should be computed and plotted correctly. - The plot should be customizable after it\'s created, allowing changes to line properties. Example: ```python import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification import matplotlib.pyplot as plt # Generating synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) estimator = LogisticRegression() estimator.fit(X, y) # Using from_estimator display = PrecisionRecallDisplay.from_estimator(estimator, X, y) display.plot() # Using from_predictions y_pred = estimator.predict_proba(X)[:, 1] display = PrecisionRecallDisplay.from_predictions(y, y_pred, \'Logistic Regression\') display.plot() plt.show() ``` Implement the `PrecisionRecallDisplay` class following the provided requirements.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve from sklearn.base import is_classifier class PrecisionRecallDisplay: def __init__(self, precision, recall, estimator_name): self.precision = precision self.recall = recall self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y, pos_label=1): if not is_classifier(estimator): raise ValueError(\\"The provided estimator is not a classifier.\\") if not hasattr(estimator, \\"predict_proba\\"): raise ValueError(\\"Estimator does not have \'predict_proba\' method.\\") y_pred = estimator.predict_proba(X)[:, 1] precision, recall, _ = precision_recall_curve(y, y_pred, pos_label=pos_label) return cls(precision, recall, type(estimator).__name__) @classmethod def from_predictions(cls, y_true, y_pred, estimator_name): precision, recall, _ = precision_recall_curve(y_true, y_pred) return cls(precision, recall, estimator_name) def plot(self, ax=None): if ax is None: fig, ax = plt.subplots() else: fig = ax.figure ax.plot(self.recall, self.precision, label=f\'{self.estimator_name} (area = {np.trapz(self.precision, self.recall):.2f})\') ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.set_title(\'Precision-Recall Curve\') ax.legend(loc=\'best\') return fig"},{"question":"**Title**: Implement an I/O Event Multiplexer Using `select` **Description**: You are tasked with implementing an I/O event multiplexer using the `select` module in Python. The multiplexer should be able to monitor multiple file descriptors, determine which are ready for reading, writing, or have exceptional conditions, and handle the events accordingly. **Requirements**: 1. **Function Signature**: ```python def io_multiplexer(read_fds, write_fds, except_fds, timeout=None): Parameters: read_fds (list): List of file descriptors (or objects with fileno()` method) to monitor for read-ready. write_fds (list): List of file descriptors (or objects with fileno()` method) to monitor for write-ready. except_fds (list): List of file descriptors (or objects with fileno()` method) to monitor for errors or exceptional conditions. timeout (float or None): Timeout in seconds (floating point). Default is None, meaning wait indefinitely. Returns: tuple: Three lists containing file descriptors that are ready for reading, writing, and have exceptions respectively. ``` 2. **Functionality**: - Use the `select.select()` function to monitor the provided file descriptors. - The function should return three lists corresponding to the subsets of the input lists that are ready for the requested operation (read, write, exception). - If the `timeout` is specified, the function should wait for the specified duration before returning. If it\'s `None`, the function should wait indefinitely. 3. **Constraints**: - The input lists (`read_fds`, `write_fds`, `except_fds`) must contain valid file descriptors or objects with a `fileno()` method. - The function must handle the case where the input lists are empty appropriately. 4. **Performance**: - Aim for a solution with proper use of the `select.select()` interface. - Ensure that the solution handles a reasonable number of file descriptors efficiently. **Example**: ```python import socket import sys # Creating sockets for demonstration sock1 = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock2 = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # Binding and listening on the sockets sock1.bind((\'localhost\', 0)) sock1.listen(5) sock2.bind((\'localhost\', 0)) sock2.listen(5) read_fds = [sock1, sock2] write_fds = [] except_fds = [sock1] # Calling the multiplexer function to check the statuses r_ready, w_ready, x_ready = io_multiplexer(read_fds, write_fds, except_fds, timeout=1.0) print(\\"Read-ready sockets:\\", r_ready) print(\\"Write-ready sockets:\\", w_ready) print(\\"Exception sockets:\\", x_ready) # Close the sockets sock1.close() sock2.close() ``` This question will test the student\'s understanding of the `select` module and handling multiple file descriptors in a time-efficient manner.","solution":"import select def io_multiplexer(read_fds, write_fds, except_fds, timeout=None): Parameters: read_fds (list): List of file descriptors (or objects with fileno()` method) to monitor for read-ready. write_fds (list): List of file descriptors (or objects with fileno()` method) to monitor for write-ready. except_fds (list): List of file descriptors (or objects with fileno()` method) to monitor for errors or exceptional conditions. timeout (float or None): Timeout in seconds (floating point). Default is None, meaning wait indefinitely. Returns: tuple: Three lists containing file descriptors that are ready for reading, writing, and have exceptions respectively. # Use select.select to monitor the file descriptors r_ready, w_ready, x_ready = select.select(read_fds, write_fds, except_fds, timeout) return r_ready, w_ready, x_ready"},{"question":"Objective: Demonstrate understanding of the `imghdr` module and the ability to extend its functionality by implementing custom image type detection logic. Problem Statement: You are tasked with developing a utility to process a list of image files, determine their types, and categorize them based on their formats. In addition to the built-in image types supported by `imghdr`, you need to extend the module to recognize a custom image format identified by a specific header signature. Requirements: 1. **Implement a function `detect_custom_format(h: bytes) -> Optional[str]`:** - This function should take a byte stream `h` as an argument. - It should detect if the byte stream corresponds to a custom image format by checking for a specific header signature. - Return the string `\'custom\'` if the signature matches, otherwise return `None`. 2. **Function `categorize_images(file_paths: List[str]) -> Dict[str, List[str]]`:** - This function takes a list of file paths to image files. - It should determine the type of each image using `imghdr.what()`, augmented with the custom format detection. - Return a dictionary where each key is an image type and the value is a list of file paths corresponding to that image type. Constraints: - The custom image format has a specific signature: The first four bytes of the file are `0x89, 0x50, 0x4E, 0x47`. - The list of file paths provided to `categorize_images` is guaranteed to be valid and the files exist. Example: ```python from typing import List, Dict, Optional import imghdr def detect_custom_format(h: bytes) -> Optional[str]: # Your implementation here pass def categorize_images(file_paths: List[str]) -> Dict[str, List[str]]: # Extend imghdr with custom test imghdr.tests.append(lambda h, f: detect_custom_format(h)) # Your implementation here pass # Testing the functions image_files = [\'image1.png\', \'image2.jpeg\', \'custom_image\'] result = categorize_images(image_files) print(result) # Example output: {\'png\': [\'image1.png\'], \'jpeg\': [\'image2.jpeg\'], \'custom\': [\'custom_image\']} ``` **Notes:** - Make sure to handle the case where files do not match any known format. - The custom format header check should be efficient and correctly detect the custom format.","solution":"from typing import List, Dict, Optional import imghdr def detect_custom_format(h: bytes) -> Optional[str]: Detects the custom image format by checking for a specific header signature. Parameters: h (bytes): Byte stream to check. Returns: Optional[str]: \'custom\' if the signature matches, otherwise None. # The custom image format signature custom_signature = b\'x89PNG\' if h.startswith(custom_signature): return \'custom\' return None def categorize_images(file_paths: List[str]) -> Dict[str, List[str]]: Categorizes images based on their format. Parameters: file_paths (List[str]): List of file paths to image files. Returns: Dict[str, List[str]]: Dictionary categorizing file paths by image type. # Extend imghdr with custom test imghdr.tests.append(lambda h, f: detect_custom_format(h)) # Dictionary to hold categorized images categorized_images = {} for file_path in file_paths: # Determine the image type image_type = imghdr.what(file_path) if image_type is not None: # Add the file_path to the appropriate category if image_type not in categorized_images: categorized_images[image_type] = [] categorized_images[image_type].append(file_path) return categorized_images"},{"question":"Objective This question aims to assess your understanding of the `seaborn` library, specifically the `husl_palette` function, and your ability to apply it effectively for data visualization. Problem Statement You are tasked with creating a visualization for a dataset using custom color palettes generated with the `seaborn.husl_palette` function. The visualization should display a scatter plot with points colored according to a specified continuous variable. Requirements 1. **Input and Output:** - The function `create_custom_scatterplot` should take the following inputs: * `data`: a pandas DataFrame containing the dataset with at least two numeric columns named \'x\' and \'y\', and one categorical column named \'category\'. * `h`: the starting hue (optional, default is 0.01). * `l`: the lightness (optional, default is 0.65). * `s`: the saturation (optional, default is 0.75). - The function should generate a scatter plot, where: * Points are colored based on the `husl_palette` applied to the `category` column. * The x-axis represents the \'x\' column. * The y-axis represents the \'y\' column. - Display the plot using `matplotlib` or any plotting library of your choice that supports seaborn. 2. **Constraints:** - The DataFrame `data` must contain the columns \'x\', \'y\', and \'category\'. - The palette should have one color for each unique value in the \'category\' column. 3. **Example:** ```python import pandas as pd # Sample data df = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5], \'y\': [3, 1, 4, 2, 5], \'category\': [\'A\', \'B\', \'A\', \'B\', \'C\'] }) # Function call create_custom_scatterplot(df, h=0.5, l=0.6, s=0.4) ``` Evaluation Criteria - Correct use of `seaborn.husl_palette` to generate a custom color palette. - Proper implementation of the scatter plot with appropriate coloring. - Clear and accurate visual representation of the data. - Code readability and structure.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatterplot(data, h=0.01, l=0.65, s=0.75): Creates a scatter plot with points colored based on a categorical variable using a custom husl_palette. Parameters: - data: pandas DataFrame with columns \'x\' (numeric), \'y\' (numeric), and \'category\' (categorical). - h: starting hue [0, 1] for the husl palette (default 0.01). - l: lightness [0, 1] for the husl palette (default 0.65). - s: saturation [0, 1] for the husl palette (default 0.75). # Ensure the dataframe contains the required columns if not {\'x\', \'y\', \'category\'}.issubset(data.columns): raise ValueError(\\"DataFrame must contain \'x\', \'y\', and \'category\' columns.\\") unique_categories = data[\'category\'].unique() num_categories = len(unique_categories) # Create a custom husl palette palette = sns.husl_palette(n_colors=num_categories, h=h, l=l, s=s) palette_dict = {category: color for category, color in zip(unique_categories, palette)} # Create the scatter plot plt.figure(figsize=(10, 6)) for category in unique_categories: subset = data[data[\'category\'] == category] plt.scatter(subset[\'x\'], subset[\'y\'], c=[palette_dict[category]], label=category) plt.xlabel(\'x\') plt.ylabel(\'y\') plt.legend(title=\'Category\') plt.title(\'Custom Scatter Plot with husl_palette\') plt.show()"},{"question":"**Title: Implementing and Evaluating Isotonic Regression using scikit-learn** **Objective:** You are required to implement an isotonic regression model using the scikit-learn library\'s `IsotonicRegression` class. The model should fit a non-decreasing function to the given 1-dimensional data. Additionally, you will need to evaluate the model\'s performance using the mean squared error metric. **Task:** 1. Implement the `fit_isotonic_regression` function. 2. Implement the `evaluate_model_performance` function. **Function Specifications:** 1. **`fit_isotonic_regression` Function:** **Input:** - `X_train` (List[float]): The training input data (1-dimensional). - `y_train` (List[float]): The training target values. - `increasing` (Union[str, bool]): Optional. If set to \'auto\', the constraint will be chosen based on Spearman\'s rank correlation coefficient. If a boolean value is provided, it specifies the direction of the ordering constraint (True for non-decreasing, False for non-increasing). **Output:** - A trained `IsotonicRegression` model. **Constraints:** - The lengths of `X_train` and `y_train` must be equal. - The `increasing` parameter must be either \'auto\' or a boolean value. 2. **`evaluate_model_performance` Function:** **Input:** - `model` (`IsotonicRegression`): The trained isotonic regression model. - `X_test` (List[float]): The test input data (1-dimensional). - `y_test` (List[float]): The true target values for the test data. **Output:** - Mean squared error (float) between the model\'s predictions and the true target values. **Constraints:** - The lengths of `X_test` and `y_test` must be equal. **Performance Requirements:** - Your solution should handle edge cases such as: - Very small datasets (less than 5 data points). - Non-unique `X` values in the input data. - The mean squared error should be calculated correctly for the given test dataset. **Implementation Example:** ```python from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def fit_isotonic_regression(X_train, y_train, increasing=\'auto\'): # Implement function here pass def evaluate_model_performance(model, X_test, y_test): # Implement function here pass # Example usage: X_train = [1, 2, 3, 4, 5] y_train = [2, 1, 4, 3, 6] X_test = [1.5, 2.5, 3.5, 4.5] y_test = [1.5, 2.0, 4.0, 5.0] model = fit_isotonic_regression(X_train, y_train, increasing=True) mse = evaluate_model_performance(model, X_test, y_test) print(f\\"Mean Squared Error: {mse}\\") ``` In this task, you will primarily demonstrate your understanding of how to use the `IsotonicRegression` class to fit a model, make predictions, and evaluate its performance using mean squared error.","solution":"from typing import List, Union from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def fit_isotonic_regression(X_train: List[float], y_train: List[float], increasing: Union[str, bool] = \'auto\') -> IsotonicRegression: Fits an isotonic regression model to the training data. Parameters: X_train (List[float]): The training input data (1-dimensional). y_train (List[float]): The training target values. increasing (Union[str, bool]): Specifies the direction of the ordering constraint. Returns: IsotonicRegression: A trained IsotonicRegression model. assert len(X_train) == len(y_train), \\"X_train and y_train must have the same length\\" assert increasing in [\'auto\', True, False], \\"increasing must be \'auto\', True, or False\\" model = IsotonicRegression(increasing=increasing) model.fit(X_train, y_train) return model def evaluate_model_performance(model: IsotonicRegression, X_test: List[float], y_test: List[float]) -> float: Evaluates the performance of the isotonic regression model using mean squared error. Parameters: model (IsotonicRegression): The trained isotonic regression model. X_test (List[float]): The test input data (1-dimensional). y_test (List[float]): The true target values for the test data. Returns: float: The mean squared error between the model\'s predictions and the true target values. assert len(X_test) == len(y_test), \\"X_test and y_test must have the same length\\" y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"# Python Coding Assessment Question **Objective:** To assess your understanding of the `token` module in Python, you are required to implement a function that processes a list of token values. The function will categorize each token into terminal, non-terminal, and end marker categories and return a summary report. **Problem Statement:** Implement a function named `analyze_tokens` that takes a list of integer token values as input and returns a dictionary containing the counts of terminal tokens, non-terminal tokens, and end-marker tokens. # Function Signature ```python def analyze_tokens(token_list: list[int]) -> dict[str, int]: pass ``` # Input - `token_list` (list of int): A list of integer values where each integer corresponds to a token in the `token` module. # Output - Returns a dictionary with the following keys: - `\\"terminal\\"`: The count of terminal tokens in the input list. - `\\"non_terminal\\"`: The count of non-terminal tokens in the input list. - `\\"end_marker\\"`: The count of end-marker tokens in the input list. # Constraints - You can assume the input list contains only valid token values as defined in the `token` module. - The length of the input list will be between 1 and 1000 (inclusive). # Example ```python from token import ENDMARKER, NAME, NUMBER, STRING, NEWLINE, INDENT, DEDENT # Sample input list token_list = [NAME, NUMBER, STRING, ENDMARKER, NEWLINE, INDENT, DEDENT, NAME, ENDMARKER] # Expected output analyze_tokens(token_list) # Output: {\'terminal\': 7, \'non_terminal\': 0, \'end_marker\': 2} ``` # Notes - Use the `token` module described to identify terminal, non-terminal, and end-marker tokens. - Utilize the `ISTERMINAL`, `ISNONTERMINAL`, and `ISEOF` functions to categorize the tokens. **Explanation:** In the example above: - The input list `[NAME, NUMBER, STRING, ENDMARKER, NEWLINE, INDENT, DEDENT, NAME, ENDMARKER]` contains: - 7 terminal tokens (`NAME`, `NUMBER`, `STRING`, `NEWLINE`, `INDENT`, `DEDENT`, `NAME`) - 0 non-terminal tokens - 2 end-marker tokens (`ENDMARKER`, `ENDMARKER`) This should be showcased by the function\'s output. Good luck!","solution":"import token def analyze_tokens(token_list: list[int]) -> dict[str, int]: Analyzes a list of token values and returns the count of terminal, non-terminal, and end-marker tokens. count_terminal = 0 count_non_terminal = 0 count_end_marker = 0 for tok in token_list: if token.ISEOF(tok): count_end_marker += 1 elif token.ISTERMINAL(tok): count_terminal += 1 elif token.ISNONTERMINAL(tok): count_non_terminal += 1 return { \\"terminal\\": count_terminal, \\"non_terminal\\": count_non_terminal, \\"end_marker\\": count_end_marker }"},{"question":"**Problem Statement:** You are provided with a Python version number encoded as a single integer known as `PY_VERSION_HEX`. Your task is to implement a function that decodes this hexadecimal value into its constituent components - major version, minor version, micro version, release level, and release serial. # Function Signature ```python def decode_py_version(hexversion: int) -> dict: Decodes a given Python version number encoded in a single integer into its constituent components. Parameters: hexversion (int): A single integer encoding the Python version number. Returns: dict: A dictionary containing the following keys and their respective integer values: - \\"major_version\\" - \\"minor_version\\" - \\"micro_version\\" - \\"release_level\\" - \\"release_serial\\" ``` # Input - `hexversion` (int): A 32-bit integer representing the Python version number encoded as specified. # Output - A dictionary with the following structure: ```json { \\"major_version\\": int, \\"minor_version\\": int, \\"micro_version\\": int, \\"release_level\\": int, \\"release_serial\\": int } ``` # Example ```python >>> decode_py_version(0x030401a2) { \'major_version\': 3, \'minor_version\': 4, \'micro_version\': 1, \'release_level\': 10, \'release_serial\': 2 } ``` # Constraints - The `hexversion` provided will always be a valid Python version number encoded as per the given specification. - Assume the input integer fits within standard 32-bit representation. # Notes - Use bitwise operations to extract the necessary components from the `hexversion`. **Hint:** You may find it helpful to review the example of how the bytes and bits are aligned and utilize bitwise masking and shifting to extract each component.","solution":"def decode_py_version(hexversion: int) -> dict: Decodes a given Python version number encoded in a single integer into its constituent components. Parameters: hexversion (int): A single integer encoding the Python version number. Returns: dict: A dictionary containing the following keys and their respective integer values: - \\"major_version\\" - \\"minor_version\\" - \\"micro_version\\" - \\"release_level\\" - \\"release_serial\\" major_version = (hexversion >> 24) & 0xFF minor_version = (hexversion >> 16) & 0xFF micro_version = (hexversion >> 8) & 0xFF release_level = (hexversion >> 4) & 0xF release_serial = hexversion & 0xF return { \\"major_version\\": major_version, \\"minor_version\\": minor_version, \\"micro_version\\": micro_version, \\"release_level\\": release_level, \\"release_serial\\": release_serial }"},{"question":"# Custom Importer Implementation Using `importlib` Objective Create a custom importer in Python using the `importlib` module which can dynamically import modules from a specified directory. This custom importer should be able to: 1. Import any module from a given directory path. 2. Handle scenarios where the module might not exist, appropriately. 3. Reload an already imported module if it has been modified on the disk. Detailed Instructions 1. **Function Specification:** Create a class `CustomImporter` with the following methods: - `__init__(self, path: str)`: Initialize the importer with the specified directory path. - `import_module(self, module_name: str) -> ModuleType`: Import the module from the specified path. - `reload_module(self, module_name: str) -> ModuleType`: Reload the specified module if it has already been imported. 2. **Input/Output:** - The `__init__` method takes a single argument `path` which is the directory where the modules are stored. - The `import_module` method takes a single argument `module_name` which is the name of the module to be imported. It returns an imported module object. - The `reload_module` method takes a single argument `module_name` which is the name of the module to be reloaded. It returns the reloaded module object. 3. **Constraints:** - Assume that the modules are simple Python scripts without any dependencies. - The directory path provided to the `CustomImporter` will always be a valid directory. - Python version used should be 3.10 or above. 4. **Example Usage:** ```python # Given directory path path = \\"/path/to/modules\\" # Initialize custom importer custom_importer = CustomImporter(path) # Import module my_module = custom_importer.import_module(\\"my_module\\") # Use function from imported module my_module.some_function() # Reload module reloaded_module = custom_importer.reload_module(\\"my_module\\") ``` 5. **Edge Cases:** - If the module does not exist in the specified path, the `import_module` method should raise an `ImportError`. - If trying to reload a module that hasn\'t been imported yet, raise `ImportError`. Additional Notes: - Use `importlib.util.spec_from_file_location` and `importlib.util.module_from_spec` to load the module from the given file path. - Use `importlib.reload` for reloading the module. Submission Requirements: - Implement the `CustomImporter` class as per the specifications. - Write a sample directory with a sample module and demonstrate the usage of the `CustomImporter` with explained comments.","solution":"import importlib.util import importlib import os import sys from types import ModuleType class CustomImporter: def __init__(self, path: str): self.path = path if path not in sys.path: sys.path.insert(0, path) def import_module(self, module_name: str) -> ModuleType: module_path = os.path.join(self.path, f\\"{module_name}.py\\") if not os.path.exists(module_path): raise ImportError(f\\"Module \'{module_name}\' does not exist in path \'{self.path}\'\\") spec = importlib.util.spec_from_file_location(module_name, module_path) if spec is None: raise ImportError(f\\"Could not load spec for module \'{module_name}\'\\") module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) return module def reload_module(self, module_name: str) -> ModuleType: if module_name not in sys.modules: raise ImportError(f\\"Module \'{module_name}\' has not been imported yet\\") module = importlib.reload(sys.modules[module_name]) return module"},{"question":"You are required to write a Python script that asynchronously executes multiple shell commands concurrently using the asyncio library. Your task is to implement a function `execute_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]` that takes a list of shell commands and returns the results of their executions. # Function Signature: ```python from typing import List, Tuple import asyncio async def execute_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: ``` # Parameters: - `commands` (List[str]): A list of shell commands to be executed. Each command should be a valid string that can be executed in the shell. # Returns: - `List[Tuple[str, int, str, str]]`: A list of tuples, each containing: - The original command (str), - The return code of the command execution (int), - The standard output captured from the command (str), - The standard error captured from the command (str). # Constraints: - You must use `asyncio.create_subprocess_shell` to run the commands. - You must capture both the standard output and standard error of each command. - The function should execute all commands concurrently and wait for all of them to complete before returning the results. - Ensure proper handling of exceptions and return appropriate error messages in case of command failures. # Example: ```python import asyncio from typing import List, Tuple async def execute_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: async def run_command(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() return cmd, proc.returncode, stdout.decode().strip(), stderr.decode().strip() tasks = [run_command(cmd) for cmd in commands] return await asyncio.gather(*tasks) # Example Usage commands = [\\"ls /\\", \\"echo \'Hello, World!\'\\", \\"invalid_command\\"] results = asyncio.run(execute_commands(commands)) for result in results: print(result) ``` Expected Output: ```plaintext (\\"ls /\\", 0, \\"binnbootndevn...\\", \\"\\") (\\"echo \'Hello, World!\'\\", 0, \\"Hello, World!\\", \\"\\") (\\"invalid_command\\", 1, \\"\\", \\"sh: invalid_command: command not found\\") ``` # Notes: - The `execute_commands` function should be fully asynchronous, making use of the async/await syntax and asyncio library features. - The captured standard output and error should be decoded using UTF-8. - Ensure that your implementation avoids deadlocks and efficiently manages subprocesses.","solution":"from typing import List, Tuple import asyncio async def execute_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: Executes a list of shell commands asynchronously and returns their results. Args: - commands: List of shell commands to be executed. Returns: - List of tuples, each containing: - The original command (str) - The return code of the command execution (int) - The standard output captured from the command (str) - The standard error captured from the command (str) async def run_command(cmd: str) -> Tuple[str, int, str, str]: proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() return cmd, proc.returncode, stdout.decode().strip(), stderr.decode().strip() tasks = [run_command(cmd) for cmd in commands] return await asyncio.gather(*tasks)"},{"question":"Coding Assessment Question # Objective Implement a Python function that reads a WAV file, applies a simple audio effect (e.g., reversing the audio), and writes the processed data into a new WAV file. This will assess the student\'s understanding of reading and writing WAV files using the `wave` module as well as manipulating audio data. # Function Signature ```python def reverse_audio(input_wav: str, output_wav: str) -> None: Reads an input WAV file, reverses the audio, and writes the reversed audio to an output WAV file. Args: - input_wav (str): The path to the input WAV file. - output_wav (str): The path to the output WAV file. Returns: - None: The function should write the reversed audio to the output file. ``` # Input - `input_wav` (str): The file path to the input WAV file. The file is guaranteed to be in PCM format. - `output_wav` (str): The file path for the output WAV file. # Output - The function should not return any value. It should write the reversed audio into the specified output WAV file. # Constraints - The input file must be in PCM format. - The output file must retain the same properties (number of channels, sample width, frame rate) as the input file. # Example ```python # Assume \'example.wav\' is an existing WAV file in the current directory reverse_audio(\'example.wav\', \'reversed_example.wav\') # After execution, \'reversed_example.wav\' should contain the reversed audio of \'example.wav\' ``` # Notes 1. You may assume the input WAV file is well-formed and that the path to the output file is valid and writable. 2. Your function should handle both mono and stereo WAV files. 3. Use the `wave` module exclusively for handling the WAV files. # Implementation Details 1. Open the input WAV file using `wave.open()` in read mode. 2. Read the audio frames and reverse them. 3. Open the output WAV file using `wave.open()` in write mode. 4. Write the reversed audio frames to the output file, ensuring the WAV header reflects the correct properties. Implement the function `reverse_audio` that fulfills the above requirements.","solution":"import wave def reverse_audio(input_wav: str, output_wav: str) -> None: Reads an input WAV file, reverses the audio, and writes the reversed audio to an output WAV file. Args: - input_wav (str): The path to the input WAV file. - output_wav (str): The path to the output WAV file. Returns: - None: The function should write the reversed audio to the output file. # Open the input WAV file with wave.open(input_wav, \'rb\') as infile: # Extract audio properties num_channels = infile.getnchannels() sample_width = infile.getsampwidth() frame_rate = infile.getframerate() num_frames = infile.getnframes() # Read the audio frames audio_frames = infile.readframes(num_frames) # Reverse the audio frames reversed_frames = audio_frames[::-1] # Open the output WAV file with wave.open(output_wav, \'wb\') as outfile: # Set the properties outfile.setnchannels(num_channels) outfile.setsampwidth(sample_width) outfile.setframerate(frame_rate) # Write the reversed frames outfile.writeframes(reversed_frames)"},{"question":"# Question: Write a Python function `restrict_recursion_and_invoke(func, max_recurse, *args, **kwargs)` that takes the following inputs: - A callable `func` to be executed. - An integer `max_recurse` which sets the recursion limit. - Any number of additional positional and keyword arguments to be passed to `func`. The function should: 1. First, save the current recursion limit. 2. Then, set a new recursion limit to `max_recurse`. 3. Execute the function `func` with the provided arguments. 4. Restore the original recursion limit. 5. Return the output of `func`. Additionally, if setting the recursion limit to `max_recurse` raises a `RecursionError` due to the current stack depth, the function should handle it gracefully by setting the recursion limit to a higher appropriate value, executing the function, and then resetting to the original recursion limit. It should print an explanatory message when needing to handle such errors. # Constraints: - The function `func` will be a valid callable. - `max_recurse` will be a positive integer. - `*args` and `**kwargs` are optional and can be any inputs that `func` accepts. # Example Usage: ```python def sample_function(x, y): return x + y # Should execute sample_function(3, 4) with a recursion limit of 10 and then restore the original limit result = restrict_recursion_and_invoke(sample_function, 10, 3, 4) print(result) # Output: 7 ``` # Performance Requirements: - The function should manage recursion limits efficiently without causing stack overflow. - It should not alter the execution of `func` except for setting and resetting recursion limits. # Solution Design: To solve this task, students will need to: 1. Use `sys.getrecursionlimit` and `sys.setrecursionlimit` to manage recursion limits. 2. Handle `RecursionError` to reset recursion limits appropriately. 3. Ensure the function returns the correct output of `func` and that all state settings are done carefully and restored after execution.","solution":"import sys def restrict_recursion_and_invoke(func, max_recurse, *args, **kwargs): Executes a function with a specified recursion limit, then restores the original recursion limit. Parameters: - func (callable): The function to be executed. - max_recurse (int): The desired maximum recursion depth. - *args: Positional arguments to pass to `func`. - **kwargs: Keyword arguments to pass to `func`. Returns: - The output of `func`. # Save the current recursion limit original_limit = sys.getrecursionlimit() try: # Set the new recursion limit sys.setrecursionlimit(max_recurse) except RecursionError: # Handle if the new limit causes an error print(\\"Recursion limit too low, adjusting to a higher value\\") sys.setrecursionlimit(max_recurse + original_limit) try: # Invoke the function with the provided arguments result = func(*args, **kwargs) finally: # Restore the original recursion limit sys.setrecursionlimit(original_limit) return result"},{"question":"# Question: Implement a Python C Extension for Integer Conversion Objective: Write a Python C extension that converts an array of Python integers to their corresponding C types and vice versa. You will implement the following functionality: 1. **Convert a list of Python integers to a list of `long` integers.** 2. **Convert a list of `long` C integers back to Python integers.** 3. **Handle errors appropriately when conversion fails.** Details: 1. **Function 1 (PyList_to_CLongList):** Input: A Python list of integers. Output: A new Python list containing the `long` C integer representations of the original list elements. Constraints: - If any element cannot be converted to `long` C integer, raise an appropriate Python Exception. - Retain order of the elements during conversion. 2. **Function 2 (CLongList_to_PyList):** Input: A list of `long` C integers. Output: A new Python list containing the Python integer representations of the original list elements. Constraints: - If any conversion fails from `long` to Python integer, raise an appropriate Python Exception. - Retain order of the elements during conversion. Example: ```python # Example usage: import example_module # Function to convert Python integers to C long integers py_list = [1, 2, 3, 256] c_long_list = example_module.PyList_to_CLongList(py_list) print(c_long_list) # Expected output: [1, 2, 3, 256] # Function to convert C long integers back to Python integers c_long_list = [1, 2, 3, 256] py_list = example_module.CLongList_to_PyList(c_long_list) print(py_list) # Expected output: [1, 2, 3, 256] ``` Performance Requirements: - Ensure the implementation is efficient in terms of both time and space complexity. - Given the arbitrary size of integers in Python, manage large lists effectively. Submission: - Submit the C extension module code. - Provide a Python script demonstrating the functions with appropriate test cases.","solution":"from typing import List def PyList_to_CLongList(py_list: List[int]) -> List[int]: Converts a list of Python integers to a list of C long integers. try: # In Python, int is of arbitrary precision and is mapped to long in C c_long_list = [int(x) for x in py_list] return c_long_list except ValueError as e: raise ValueError(f\\"Conversion error: {e}\\") def CLongList_to_PyList(c_long_list: List[int]) -> List[int]: Converts a list of C long integers to a list of Python integers. try: # Since Python\'s int type handles long integers, directly map them py_list = [int(x) for x in c_long_list] return py_list except ValueError as e: raise ValueError(f\\"Conversion error: {e}\\")"},{"question":"# Question: Tuning Decision Thresholds in Classification Objective: You are tasked with building a binary classification model using scikit-learn. However, instead of using the default classification threshold, you will tune the decision threshold to optimize a specific metric. You\'ll then manually set a threshold and compare the performance. Task: 1. **Data Preparation**: - Use the `make_classification` function from scikit-learn to create a synthetic binary classification dataset with `1000` samples, `20` features, and class weights of `0.1` and `0.9`. 2. **Model Training and Threshold Tuning**: - Train a `LogisticRegression` model on this dataset. - Use the `TunedThresholdClassifierCV` class to tune the decision threshold of the trained model to maximize the `f1_score` with `pos_label` set to the minority class (0). - *Ensure you use cross-validation to avoid overfitting.* 3. **Manual Threshold Setting**: - Manually set the decision threshold of the trained `LogisticRegression` model to a value of `0.3` using the `FixedThresholdClassifier`. 4. **Performance Comparison**: - Compare and print the `f1_score` and `accuracy` of both the tuned model and the manually set threshold model on a separate test set consisting of `200` samples. Requirements: - Implement the task in a function `tune_and_evaluate_thresholds()`. - Inputs: None. - Output: A dictionary with `f1_score` and `accuracy` of the tuned model and the manually set threshold model on the test set. Format: `{\'tuned_model\': {\'f1_score\': float, \'accuracy\': float}, \'manual_threshold_model\': {\'f1_score\': float, \'accuracy\': float}}`. Constraints: - Use `random_state=0` for reproducibility in the `make_classification` and model training processes. - Use a `5-fold` stratified cross-validation for `TunedThresholdClassifierCV`. Example: ```python def tune_and_evaluate_thresholds(): from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, TunedThresholdClassifierCV, FixedThresholdClassifier from sklearn.metrics import make_scorer, f1_score, accuracy_score # Step 1: Data Preparation X, y = make_classification(n_samples=1000, n_features=20, weights=[0.1, 0.9], random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Step 2: Model Training and Threshold Tuning base_model = LogisticRegression(random_state=0) pos_label = 0 scorer = make_scorer(f1_score, pos_label=pos_label) tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=5) tuned_model.fit(X_train, y_train) # Step 4: Performance Comparison for Tuned Model y_pred_tuned = tuned_model.predict(X_test) tuned_f1 = f1_score(y_test, y_pred_tuned, pos_label=pos_label) tuned_accuracy = accuracy_score(y_test, y_pred_tuned) # Step 3: Manual Threshold Setting manual_threshold_model = FixedThresholdClassifier(base_model, threshold=0.3) manual_threshold_model.fit(X_train, y_train) # Performance Comparison for Manual Threshold Model y_pred_manual = manual_threshold_model.predict(X_test) manual_f1 = f1_score(y_test, y_pred_manual, pos_label=pos_label) manual_accuracy = accuracy_score(y_test, y_pred_manual) # Output the results result = { \'tuned_model\': {\'f1_score\': tuned_f1, \'accuracy\': tuned_accuracy}, \'manual_threshold_model\': {\'f1_score\': manual_f1, \'accuracy\': manual_accuracy} } return result ``` **Note**: Ensure to test and validate the implementation, as this is an example outline.","solution":"def tune_and_evaluate_thresholds(): from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold from sklearn.metrics import f1_score, accuracy_score import numpy as np # Synthetic data generation X, y = make_classification(n_samples=1000, n_features=20, weights=[0.1, 0.9], random_state=0) # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Model training model = LogisticRegression(random_state=0) model.fit(X_train, y_train) # Threshold tuning using cross-validation cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0) y_prob = cross_val_predict(model, X_train, y_train, cv=cv, method=\'predict_proba\')[:,1] thresholds = np.linspace(0, 1, 101) f1_scores = [] for threshold in thresholds: y_pred_threshold = (y_prob >= threshold).astype(int) f1 = f1_score(y_train, y_pred_threshold, pos_label=0) f1_scores.append(f1) best_threshold = thresholds[np.argmax(f1_scores)] # Prediction with tuned threshold y_prob_test = model.predict_proba(X_test)[:, 1] y_pred_tuned = (y_prob_test >= best_threshold).astype(int) tuned_f1 = f1_score(y_test, y_pred_tuned, pos_label=0) tuned_accuracy = accuracy_score(y_test, y_pred_tuned) # Set manual threshold at 0.3 manual_threshold = 0.3 y_pred_manual = (y_prob_test >= manual_threshold).astype(int) manual_f1 = f1_score(y_test, y_pred_manual, pos_label=0) manual_accuracy = accuracy_score(y_test, y_pred_manual) result = { \'tuned_model\': {\'f1_score\': tuned_f1, \'accuracy\': tuned_accuracy}, \'manual_threshold_model\': {\'f1_score\': manual_f1, \'accuracy\': manual_accuracy} } return result"},{"question":"# Python `heapq` Coding Assessment Objective: The goal of this task is to implement a function using the `heapq` module to solve a problem that involves merging multiple sorted sequences while maintaining efficiency and correct use of the provided `heapq` functions. Problem Statement You are given a list of N sorted lists. Your task is to merge these lists into a single sorted list using the `heapq.merge()` function. However, you should enhance the default behavior by allowing an optional argument to sort the resultant list in descending order. Write a function `custom_merge(sorted_lists: List[List[int]], reverse: bool = False) -> List[int]` that: 1. Takes a list of sorted lists. 2. Takes an optional boolean argument `reverse` which, if set to `True`, will sort the resultant merged list in descending order. 3. Returns the merged list as a single sorted list. Constraints: - Each input list of integers is already sorted in ascending order. - The total number of elements across all lists is up to 10^6. - The function should be efficient with respect to time and space complexity. Function Signature: ```python from typing import List def custom_merge(sorted_lists: List[List[int]], reverse: bool = False) -> List[int]: pass ``` Examples: ```python # Example 1 sorted_lists = [[1, 4, 7], [2, 5, 8], [3, 6, 9]] print(custom_merge(sorted_lists)) # Output: [1, 2, 3, 4, 5, 6, 7, 8, 9] # Example 2 sorted_lists = [[1, 4, 7], [2, 5, 8], [3, 6, 9]] print(custom_merge(sorted_lists, reverse=True)) # Output: [9, 8, 7, 6, 5, 4, 3, 2, 1] # Example 3 sorted_lists = [[], [1, 3, 5], [], [2, 4, 6]] print(custom_merge(sorted_lists)) # Output: [1, 2, 3, 4, 5, 6] ``` Notes: - Utilize `heapq.merge` for merging the lists. - Ensure the function handles the `reverse` parameter correctly using the optional arguments `key` and `reverse` in `heapq.merge`. Evaluation Criteria: - Correct interpretation and usage of the `heapq` module functions. - Efficiency in handling large lists. - Correct handling of the `reverse` parameter. - Code quality and readability.","solution":"from typing import List import heapq def custom_merge(sorted_lists: List[List[int]], reverse: bool = False) -> List[int]: Merges multiple sorted lists into a single sorted list. Parameters: - sorted_lists (List[List[int]]): A list of sorted lists of integers. - reverse (bool): Whether to sort the merged list in descending order (default is False). Returns: - List[int]: A merged sorted list. merged = list(heapq.merge(*sorted_lists)) if reverse: merged.sort(reverse=True) return merged"},{"question":"**Question: Script and Train a Custom PyTorch Module** **Objective:** Implement a custom neural network module using PyTorch, script it using TorchScript, and perform a forward pass with provided input data. **Description:** You are required to implement a custom PyTorch module (class that inherits from `torch.nn.Module`). The class should: - Contain at least two linear layers. - Use ReLU activation between layers. - Include a method to initialize weights. - Be properly scripted using `torch.jit.script`. - Perform a forward pass when given a tensor input. **Function Signature:** ```python import torch import torch.nn as nn from typing import Any class MyCustomModule(nn.Module): def __init__(self, input_size: int, hidden_size: int, output_size: int): super(MyCustomModule, self).__init__() # Define the layers self.layer1: torch.nn.Linear = torch.nn.Linear(input_size, hidden_size) self.relu: torch.nn.ReLU = torch.nn.ReLU() self.layer2: torch.nn.Linear = torch.nn.Linear(hidden_size, output_size) self.initialize_weights() def initialize_weights(self): torch.nn.init.xavier_uniform_(self.layer1.weight) torch.nn.init.zeros_(self.layer1.bias) torch.nn.init.xavier_uniform_(self.layer2.weight) torch.nn.init.zeros_(self.layer2.bias) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x def script_module(module: MyCustomModule) -> torch.jit.ScriptModule: scripted_module = torch.jit.script(module) return scripted_module # Example usage if __name__ == \\"__main__\\": # Initialize the module model = MyCustomModule(input_size=10, hidden_size=5, output_size=2) # Script the module scripted_model = script_module(model) # Perform a forward pass with dummy data input_tensor = torch.randn(1, 10) output_tensor = scripted_model(input_tensor) print(output_tensor) ``` **Constraints:** 1. Use only modules and functions from the `torch` and `torch.nn` packages. 2. Ensure that the module initializes, scripts, and performs a forward pass without errors. 3. The initial weights must be set using Xavier uniform initialization. **Input:** 1. `input_size` (int): The size of the input layer. 2. `hidden_size` (int): The size of the hidden layer. 3. `output_size` (int): The size of the output layer. **Output:** - The function `script_module` should return a scripted version of the custom module. - The forward pass output should be a tensor of size `[1, output_size]`. **Example Input:** ```python model = MyCustomModule(input_size=10, hidden_size=5, output_size=2) scripted_model = script_module(model) input_tensor = torch.randn(1, 10) ``` **Example Output:** ```python tensor([[ 0.1234, -0.5678]], grad_fn=<AddmmBackward>) ``` **Note:** - The numbers in the output tensor are just placeholders. The actual output will depend on the initialized weights and the input tensor.","solution":"import torch import torch.nn as nn class MyCustomModule(nn.Module): def __init__(self, input_size: int, hidden_size: int, output_size: int): super(MyCustomModule, self).__init__() # Define the layers self.layer1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.layer2 = nn.Linear(hidden_size, output_size) self.initialize_weights() def initialize_weights(self): nn.init.xavier_uniform_(self.layer1.weight) nn.init.zeros_(self.layer1.bias) nn.init.xavier_uniform_(self.layer2.weight) nn.init.zeros_(self.layer2.bias) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x def script_module(module: MyCustomModule) -> torch.jit.ScriptModule: scripted_module = torch.jit.script(module) return scripted_module # Example usage if __name__ == \\"__main__\\": # Initialize the module model = MyCustomModule(input_size=10, hidden_size=5, output_size=2) # Script the module scripted_model = script_module(model) # Perform a forward pass with dummy data input_tensor = torch.randn(1, 10) output_tensor = scripted_model(input_tensor) print(output_tensor)"},{"question":"You are required to create a Python script using the `syslog` module that simulates logging activities for a system undergoing various operations. Your task is to: 1. Open a log with custom settings. 2. Log messages of different priorities. 3. Change the log mask to filter out lower priority messages. 4. Log additional messages after changing the log mask. 5. Close the log. # Detailed Instructions: 1. **Open the Log**: - Use `openlog()` with the identifier `MyApp` and the option to include the process ID (`LOG_PID`). Log to the `LOG_USER` facility. 2. **Log Initial Messages**: - Log a message indicating the start of processing with `LOG_INFO` priority. - Log a message indicating a minor warning with `LOG_WARNING` priority. - Log a message indicating an error with `LOG_ERR` priority. 3. **Change Log Mask**: - Set the log mask to only log messages with priorities `LOG_ERR` and `LOG_CRIT` or higher using `setlogmask()`. 4. **Log Post-Mask Messages**: - Log another informational message with `LOG_INFO` priority. This message should not appear in the log due to the log mask. - Log a critical issue with `LOG_CRIT` priority. This message should appear in the log. 5. **Close the Log**: - Use `closelog()` to reset the syslog module values. # Example Output: Your script should produce log entries on the system logger. While actual entries will depend on the system\'s log viewer, the expected flow of operations will be as follows: - Logging initial messages with various priorities. - Changing the log mask to filter lower priority messages. - Logging post-mask messages, only higher priority messages should appear. - Resetting the logging system. # Constraints: - The solution should only use the `syslog` module for logging activities. - Ensure to handle necessary imports and operations within the Python script. # Solution Template: ```python import syslog def main(): # Open the log with custom settings syslog.openlog(ident=\'MyApp\', logoption=syslog.LOG_PID, facility=syslog.LOG_USER) # Log initial messages syslog.syslog(syslog.LOG_INFO, \'Processing started\') syslog.syslog(syslog.LOG_WARNING, \'A minor warning occured\') syslog.syslog(syslog.LOG_ERR, \'An error occured\') # Change log mask to only log errors or higher priority messages syslog.setlogmask(syslog.LOG_UPTO(syslog.LOG_ERR)) # Log post-mask messages syslog.syslog(syslog.LOG_INFO, \'This informational message should not appear\') syslog.syslog(syslog.LOG_CRIT, \'A critical issue occured\') # Close the log syslog.closelog() if __name__ == \'__main__\': main() ``` # Evaluation Criteria: - Correct usage of `openlog()`, `syslog()`, `setlogmask()`, and `closelog()`. - Proper logging of initial and post-mask messages. - Correct filtering of messages based on the log mask. - Code readability and adherence to Python coding standards.","solution":"import syslog def simulate_logging(): # Open the log with custom settings syslog.openlog(ident=\'MyApp\', logoption=syslog.LOG_PID, facility=syslog.LOG_USER) # Log initial messages syslog.syslog(syslog.LOG_INFO, \'Processing started\') syslog.syslog(syslog.LOG_WARNING, \'A minor warning occurred\') syslog.syslog(syslog.LOG_ERR, \'An error occurred\') # Change log mask to only log errors or higher priority messages syslog.setlogmask(syslog.LOG_UPTO(syslog.LOG_ERR)) # Log post-mask messages syslog.syslog(syslog.LOG_INFO, \'This informational message should not appear\') syslog.syslog(syslog.LOG_CRIT, \'A critical issue occurred\') # Close the log syslog.closelog()"},{"question":"# Question: Parallelized Matrix Operations with PyTorch You are given a large matrix and required to perform parallel matrix operations using PyTorch\'s TorchScript functionality to optimize performance on CPU. The task is to compute the sum of two matrix multiplications in parallel. Specifically, you have two matrices `a` and `b`, and two weight matrices `w1` and `w2`. You need to compute the following expression in a parallelized fashion: [ result = (a cdot w1) + (b cdot w2) ] Implement a TorchScript function to perform this computation using asynchronous execution. Additionally, provide a main function to demonstrate the tuning of intra-op thread settings to find the optimal performance for this operation. Requirements: 1. Implement a TorchScript function using `torch.jit.script` and the `torch.jit._fork` and `torch.jit._wait` primitives. 2. Create a main function that: - Takes four matrices `a`, `b`, `w1`, and `w2` as inputs. - Sets the number of intra-op threads using `torch.set_num_threads`. - Measures and prints the runtime for different numbers of threads. - Plots the runtime against the number of threads to visualize performance. Input: * Matrices `a`, `b` of size (n, m) * Matrices `w1`, `w2` of size (m, p) * `num_threads` - list of integers, representing different numbers of threads to test Output: * Print the runtime for each thread setting. * Plot showing the runtime against the number of threads. Constraints: * ( 1 leq n, m, p leq 1000 ) * Matrices contain real numbers (single precision float). Example: ```python import torch import timeit import matplotlib.pyplot as plt @torch.jit.script def parallel_matrix_operations(a, b, w1, w2): # Parallelized matrix multiplications fut1 = torch.jit._fork(torch.mm, a, w1) fut2 = torch.jit._fork(torch.mm, b, w2) # Wait for results and add them result1 = torch.jit._wait(fut1) result2 = torch.jit._wait(fut2) return result1 + result2 def main(a, b, w1, w2, num_threads): runtimes = [] for t in num_threads: torch.set_num_threads(t) runtime = timeit.timeit( setup=\\"import torch; from __main__ import parallel_matrix_operations\\", stmt=\\"parallel_matrix_operations(a, b, w1, w2)\\", number=100, globals={\'a\': a, \'b\': b, \'w1\': w1, \'w2\': w2} ) runtimes.append(runtime) print(f\'Number of threads: {t}, Runtime: {runtime} seconds\') plt.plot(num_threads, runtimes) plt.xlabel(\'Number of Threads\') plt.ylabel(\'Runtime (seconds)\') plt.title(\'Runtime vs Number of Threads\') plt.show() # Example usage a = torch.randn(1000, 1000) b = torch.randn(1000, 1000) w1 = torch.randn(1000, 500) w2 = torch.randn(1000, 500) num_threads = [1, 2, 4, 8, 16, 32, 48] main(a, b, w1, w2, num_threads) ``` For grading, ensure that the student: 1. Properly implements the TorchScript function with asynchronous execution. 2. Correctly tunes and measures the thread settings. 3. Provides and interprets the runtime plot correctly.","solution":"import torch import timeit import matplotlib.pyplot as plt @torch.jit.script def parallel_matrix_operations(a, b, w1, w2): Computes the sum of two matrix multiplications in parallel. Args: a (Tensor): The first input matrix. b (Tensor): The second input matrix. w1 (Tensor): The weight matrix for the first multiplication. w2 (Tensor): The weight matrix for the second multiplication. Returns: Tensor: The result of (a @ w1) + (b @ w2) # Parallelized matrix multiplications fut1 = torch.jit._fork(torch.mm, a, w1) fut2 = torch.jit._fork(torch.mm, b, w2) # Wait for results and add them result1 = torch.jit._wait(fut1) result2 = torch.jit._wait(fut2) return result1 + result2 def main(a, b, w1, w2, num_threads): runtimes = [] for t in num_threads: torch.set_num_threads(t) runtime = timeit.timeit( setup=\\"import torch; from __main__ import parallel_matrix_operations\\", stmt=\\"parallel_matrix_operations(a, b, w1, w2)\\", number=100, globals={\'a\': a, \'b\': b, \'w1\': w1, \'w2\': w2} ) runtimes.append(runtime / 100) # Average runtime per execution print(f\'Number of threads: {t}, Avg Runtime: {runtime / 100:.6f} seconds\') plt.plot(num_threads, runtimes) plt.xlabel(\'Number of Threads\') plt.ylabel(\'Average Runtime (seconds)\') plt.title(\'Average Runtime vs Number of Threads\') plt.show() # Example usage if __name__ == \\"__main__\\": a = torch.randn(1000, 1000) b = torch.randn(1000, 1000) w1 = torch.randn(1000, 500) w2 = torch.randn(1000, 500) num_threads = [1, 2, 4, 8, 16, 32, 48] main(a, b, w1, w2, num_threads)"},{"question":"# Asynchronous Networked Echo Server with Subprocess Logging Problem Statement You are required to implement a fully functional asynchronous TCP echo server using the `asyncio` library. The server should handle multiple client connections concurrently and echo back any data received from the clients. Additionally, it should log all received messages into a subprocess that appends these messages to a file named `received_messages.log`. Requirements 1. **Echo Server:** - The server listens on `localhost` at port `8888`. - For every incoming client connection, it echoes back any data received. - The server should handle multiple clients concurrently without blocking. 2. **Logging:** - Start a logging subprocess that runs throughout the server\'s lifetime. - Every time the server receives a message from a client, it sends this message to the logging subprocess. - The logging subprocess appends each received message to the `received_messages.log` file, each message on a new line. Expected Input/Output format - **Input:** Client connections sending messages to the server. - **Output:** The server echoes back received messages to the respective clients. The `received_messages.log` file will contain all messages received by the server. Constraints - Use the `asyncio` library with low-level transport and protocol interfaces as documented. - Do not use high-level asyncio functions like `asyncio.start_server()`. - The logging subprocess must use `loop.subprocess_exec()` and handle writing to the file asynchronously. Implementation Steps 1. **Define the Echo Protocol:** Implement the `Protocol` interface to handle connection events and data reception. 2. **Create the Logging Subprocess Protocol:** Implement the `SubprocessProtocol` to manage logging messages received by the server. 3. **Initialize the Echo Server:** Use the `loop.create_server()` method to set up the server and manage client connections. 4. **Send Messages to the Logging Subprocess:** Ensure that every received message is asynchronously sent to the logging subprocess for writing to the log file. Example Code Skeleton ```python import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport # Logic for accepting connections def data_received(self, data): message = data.decode() # Logic for echoing data to the client # Logic for sending data to the logging subprocess def connection_lost(self, exc): # Handle connection lost class LoggingSubprocessProtocol(asyncio.SubprocessProtocol): def __init__(self, exit_future): self.exit_future = exit_future def pipe_data_received(self, fd, data): # Handle received data from pipe def process_exited(self): self.exit_future.set_result(True) # Handle process exit async def main(): loop = asyncio.get_running_loop() # Setup and run the logging subprocess server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` Complete the implementation of the `EchoServerProtocol` and `LoggingSubprocessProtocol` following the provided documentation and example. Ensure to correctly handle asynchronous communication between the server and the logging subprocess.","solution":"import asyncio import subprocess class EchoServerProtocol(asyncio.Protocol): def __init__(self, subprocess_transport): self.subprocess_transport = subprocess_transport def connection_made(self, transport): self.transport = transport print(\'Connection established\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') self.transport.write(data) # Echo back to the client self.subprocess_transport.write(data) # Send data to the logging subprocess def connection_lost(self, exc): print(\'Connection closed\') self.transport.close() class LoggingSubprocessProtocol(asyncio.SubprocessProtocol): def __init__(self, process_exited_future): self.process_exited_future = process_exited_future def pipe_data_received(self, fd, data): if fd == 1: # stdout # Received data from subprocess stdout (if needed for future use) pass def process_exited(self): self.process_exited_future.set_result(True) async def main(): loop = asyncio.get_running_loop() # Setup the logging subprocess process_exited_future = asyncio.Future() subprocess_transport, subprocess_protocol = await loop.subprocess_exec( lambda: LoggingSubprocessProtocol(process_exited_future), \'sh\', \'-c\', \'exec >> received_messages.log 2>&1\', stdin=subprocess.PIPE ) server_coroutine = await loop.create_server( lambda: EchoServerProtocol(subprocess_transport), \'127.0.0.1\', 8888 ) async with server_coroutine: await server_coroutine.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Question: Customizing Seaborn and Matplotlib Themes** You are given a dataset and asked to generate a series of plots using Seaborn. The objective is to demonstrate your ability to set up various Seaborn themes and customize the plots. Please follow the steps below: 1. **Load the Dataset**: Load a dataset of your choice using Seaborn\'s built-in datasets. For this exercise, you may use `sns.load_dataset()`. Example datasets include `\'tips\'`, `\'iris\'`, etc. 2. **Plot with Default Theme**: Create a bar plot using the default Seaborn theme. Plot any categorical variable on the x-axis and a corresponding numerical variable on the y-axis. 3. **Custom Theme with Specific Style and Palette**: Modify the theme to use the `whitegrid` style and the `pastel` palette. Create the same bar plot as in step 2 with this new theme. 4. **Custom Theme Preserving Specific Parameters**: Set the theme to `white` style but preserve the current palette. Create the same bar plot again with this theme setup. 5. **Override Additional Parameters**: Customize the theme further by: - Removing the top and right spines of the plot. - Set the theme to the `ticks` style. - Create the same bar plot again with these additional customizations. **Constraints**: - Your code should be well-documented and clearly indicate the changes in each customization step. **Input and Output**: * Input: Dataset loaded using Seaborn. * Output: Four bar plots demonstrating different themes and customizations. Example code structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load dataset data = sns.load_dataset(\'your_dataset\') # Step 2: Plot with default theme sns.set_theme() sns.barplot(x=\\"your_categorical_variable\\", y=\\"your_numerical_variable\\", data=data) plt.show() # Step 3: Custom Theme with Specific Style and Palette sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") sns.barplot(x=\\"your_categorical_variable\\", y=\\"your_numerical_variable\\", data=data) plt.show() # Step 4: Custom Theme Preserving Specific Parameters sns.set_theme(style=\\"white\\", palette=None) sns.barplot(x=\\"your_categorical_variable\\", y=\\"your_numerical_variable\\", data=data) plt.show() # Step 5: Override Additional Parameters custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"ticks\\", rc=custom_params) sns.barplot(x=\\"your_categorical_variable\\", y=\\"your_numerical_variable\\", data=data) plt.show() ``` Ensure that your plots are displayed correctly and reflect the specified customizations in each step.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_dataset(): Loads the \'tips\' dataset using Seaborn\'s built-in datasets. return sns.load_dataset(\'tips\') def plot_with_default_theme(data): Plots a bar plot using the default Seaborn theme. sns.set_theme() sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=data) plt.show() # For visualization purposes def plot_with_custom_theme_whitegrid_pastel(data): Plots a bar plot with \'whitegrid\' style and \'pastel\' palette. sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=data) plt.show() # For visualization purposes def plot_with_custom_theme_white_preserve_palette(data): Plots a bar plot with \'white\' style but preserving current palette. sns.set_theme(style=\\"white\\", palette=None) sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=data) plt.show() # For visualization purposes def plot_with_custom_theme_ticks_no_spines(data): Plots a bar plot with \'ticks\' style, removing the top and right spines. custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"ticks\\", rc=custom_params) sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=data) plt.show() # For visualization purposes"},{"question":"# Complex Number Operations in Python Implement a class `ComplexNumber` that mimics the behavior of Python\'s built-in complex number type but with additional custom methods. Your class should: 1. **Initialization:** - Initialize with real and imaginary parts provided as arguments. 2. **String Representation:** - Implement the `__str__` method to represent the complex number in the form `a + bi`. 3. **Arithmetic Operations:** - Implement methods `__add__`, `__sub__`, `__mul__`, `__truediv__`, and `__pow__` to add, subtract, multiply, divide, and exponentiate complex numbers respectively. These methods should support operations with both other complex numbers and real numbers. 4. **Magnitude Calculation:** - Implement a method `magnitude` that returns the magnitude of the complex number. 5. **Conjugate Calculation:** - Implement a method `conjugate` that returns the conjugate of the complex number as a new instance of `ComplexNumber`. # Example Usage ```python # Creating complex numbers c1 = ComplexNumber(3, 4) c2 = ComplexNumber(1, 2) # String representation print(c1) # Output: 3 + 4i # Arithmetic operations print(c1 + c2) # Output: 4 + 6i print(c1 - c2) # Output: 2 + 2i print(c1 * c2) # Output: -5 + 10i print(c1 / c2) # Output: 2.2 + 0.4i print(c1 ** 2) # Output: -7 + 24i # Magnitude print(c1.magnitude()) # Output: 5.0 # Conjugate print(c1.conjugate()) # Output: 3 - 4i ``` # Constraints - The input real and imaginary parts will always be integers or floats. - Your implementation should handle division by zero by raising a `ZeroDivisionError`. **Hint:** You can use the `math.sqrt` function to calculate magnitudes. **Performance Considerations:** - Ensure that arithmetic operations are efficient and use constant-time operations wherever possible.","solution":"import math class ComplexNumber: def __init__(self, real, imaginary): self.real = real self.imaginary = imaginary def __str__(self): sign = \'+\' if self.imaginary >= 0 else \'-\' return f\\"{self.real} {sign} {abs(self.imaginary)}i\\" def __add__(self, other): if isinstance(other, ComplexNumber): return ComplexNumber(self.real + other.real, self.imaginary + other.imaginary) elif isinstance(other, (int, float)): return ComplexNumber(self.real + other, self.imaginary) raise TypeError(\\"Unsupported type for addition with ComplexNumber\\") def __sub__(self, other): if isinstance(other, ComplexNumber): return ComplexNumber(self.real - other.real, self.imaginary - other.imaginary) elif isinstance(other, (int, float)): return ComplexNumber(self.real - other, self.imaginary) raise TypeError(\\"Unsupported type for subtraction with ComplexNumber\\") def __mul__(self, other): if isinstance(other, ComplexNumber): real = self.real * other.real - self.imaginary * other.imaginary imaginary = self.real * other.imaginary + self.imaginary * other.real return ComplexNumber(real, imaginary) elif isinstance(other, (int, float)): return ComplexNumber(self.real * other, self.imaginary * other) raise TypeError(\\"Unsupported type for multiplication with ComplexNumber\\") def __truediv__(self, other): if isinstance(other, ComplexNumber): denom = other.real**2 + other.imaginary**2 if denom == 0: raise ZeroDivisionError(\\"division by zero\\") real = (self.real * other.real + self.imaginary * other.imaginary) / denom imaginary = (self.imaginary * other.real - self.real * other.imaginary) / denom return ComplexNumber(real, imaginary) elif isinstance(other, (int, float)): if other == 0: raise ZeroDivisionError(\\"division by zero\\") return ComplexNumber(self.real / other, self.imaginary / other) raise TypeError(\\"Unsupported type for division with ComplexNumber\\") def __pow__(self, power): if isinstance(power, int) and power >= 0: result = ComplexNumber(1, 0) for _ in range(power): result *= self return result else: raise TypeError(\\"Unsupported exponent type for ComplexNumber\\") def magnitude(self): return math.sqrt(self.real**2 + self.imaginary**2) def conjugate(self): return ComplexNumber(self.real, -self.imaginary)"},{"question":"**File Management Utility with Command-Line Interface** You are required to implement a Python script that acts as a file management utility. This script will take command-line arguments to perform various file system operations such as listing files, deleting files, and creating directories. # Requirements 1. **Command-Line Argument Parsing**: - Use the `argparse` module to handle command-line arguments. - The script should support the following commands: - `list`: List all files and directories in a given path. - `delete`: Delete a specified file. - `createdir`: Create a new directory at a specified path. 2. **File System Operations**: - Use the `os` module to perform the file and directory operations. - Ensure the script handles errors gracefully, such as when trying to delete a file that does not exist or creating a directory in an invalid location. # Input Format 1. The script should be run from the command line with the following structure: ``` python file_manager.py <command> <path> ``` - `<command>`: One of `list`, `delete`, or `createdir`. - `<path>`: The path to perform the operation on. # Output - For the `list` command, print all files and directories in the specified path. - For the `delete` command, print a message indicating whether the file was successfully deleted or not. - For the `createdir` command, print a message indicating whether the directory was successfully created or not. # Constraints - The script should handle invalid commands and paths by displaying appropriate error messages. - The script should ensure that only files are deleted and not directories. # Example Usage 1. List files and directories: ``` python file_manager.py list /path/to/directory ``` 2. Delete a file: ``` python file_manager.py delete /path/to/file.txt ``` 3. Create a new directory: ``` python file_manager.py createdir /path/to/new_directory ``` # Evaluation Criteria - Proper use of the `argparse` module to handle command-line arguments. - Correct implementation of file system operations using the `os` module. - Error handling for invalid commands/paths. - Clear and user-friendly output messages. Implement the required `file_manager.py` script based on the above specifications.","solution":"import os import argparse def list_files(path): if not os.path.exists(path): return f\\"Path \'{path}\' does not exist.\\" elif not os.path.isdir(path): return f\\"Path \'{path}\' is not a directory.\\" else: return \'n\'.join(os.listdir(path)) def delete_file(path): if not os.path.exists(path): return f\\"File \'{path}\' does not exist.\\" elif not os.path.isfile(path): return f\\"Path \'{path}\' is not a file.\\" else: os.remove(path) return f\\"File \'{path}\' successfully deleted.\\" def create_directory(path): if os.path.exists(path): return f\\"Directory \'{path}\' already exists.\\" else: os.makedirs(path) return f\\"Directory \'{path}\' successfully created.\\" def main(): parser = argparse.ArgumentParser(description=\\"File Management Utility\\") parser.add_argument(\'command\', choices=[\'list\', \'delete\', \'createdir\'], help=\\"Command to execute\\") parser.add_argument(\'path\', help=\\"Path to perform the command on\\") args = parser.parse_args() if args.command == \'list\': print(list_files(args.path)) elif args.command == \'delete\': print(delete_file(args.path)) elif args.command == \'createdir\': print(create_directory(args.path)) if __name__ == \\"__main__\\": main()"},{"question":"Objective: Create a Python program that demonstrates your understanding of code blocks, variable scope, name resolution, and exception handling. Problem Statement: You need to design and implement a program that performs the following tasks: 1. **Reading Input:** - Define a function `read_user_input` that prompts the user to input two integers and returns them as a tuple. - Ensure that the function handles invalid inputs gracefully by prompting again until valid integers are provided. 2. **Calculations:** - Define a function `perform_calculations` that takes two integers as input and returns a dictionary with the results of addition, subtraction, multiplication, and division. - Ensure that the function handles division by zero and returns an appropriate message in this case. 3. **Main Execution Block:** - Define a main function `main` that executes the following steps: - Uses `read_user_input` to get user input. - Uses `perform_calculations` to perform calculations on the input values. - Prints the results in a formatted manner. - Use the `if __name__ == \'__main__\':` construct to ensure that `main` is executed when the script is run directly. 4. **Global Variable Usage:** - Define a global variable `log` to store messages about the execution flow. - Use the `global` keyword within functions where necessary to update this variable. - After performing the calculations, append a message to `log` about the successful execution or any errors encountered. Requirements: - Functions should have appropriate docstrings. - Handling of exceptions must be done using `try`, `except`, and `finally` where applicable. - Follow the best practices for variable naming and code organization. Example: Below is an example of how the program should work: ```python # Program Execution Example # User input Enter the first integer: 10 Enter the second integer: 5 # Output Results: Addition: 15 Subtraction: 5 Multiplication: 50 Division: 2.0 # Log Execution log: [\'User provided valid input.\', \'Calculations performed successfully.\'] ``` Constraints: - You may assume that the user will enter integer values eventually after invalid attempts. Notes: - Focus on the clarity and organization of your code. - Ensure that name resolution and variable scoping rules are respected, as detailed in the provided documentation.","solution":"log = [] def read_user_input(): Prompts the user to input two integers and returns them as a tuple. Handles invalid inputs gracefully by prompting again until valid integers are provided. while True: try: a = int(input(\\"Enter the first integer: \\")) b = int(input(\\"Enter the second integer: \\")) global log log.append(\\"User provided valid input.\\") return a, b except ValueError: print(\\"Invalid input. Please enter integers only.\\") log.append(\\"User provided invalid input.\\") def perform_calculations(a, b): Performs addition, subtraction, multiplication, and division on the two input values. Returns a dictionary with the results of the calculations. Handles division by zero and returns an appropriate message in this case. results = { \'Addition\': a + b, \'Subtraction\': a - b, \'Multiplication\': a * b } try: results[\'Division\'] = a / b except ZeroDivisionError: results[\'Division\'] = \\"Error: Division by zero is undefined.\\" return results def main(): Main function to execute the workflow of the program. 1. Reads user input. 2. Performs calculations. 3. Prints the results. 4. Appends a log message about the execution flow. a, b = read_user_input() results = perform_calculations(a, b) print(\\"Results: \\") for operation, result in results.items(): print(f\\"{operation}: {result}\\") global log log.append(\\"Calculations performed successfully.\\") print(\\"Execution log:\\", log) if __name__ == \'__main__\': main()"},{"question":"# Advanced String Manipulation and Custom Formatting in Python Problem Statement: You are tasked with implementing a custom string formatter and manipulator using Python\'s `string` module. This will involve creating a class that leverages string constants, custom formatting methods, and template strings. Requirements: 1. **Class `CustomStringFormatter`** - Initialize the class with a dictionary containing template variables. - Implement methods to perform various string manipulations and custom formatting tasks. 2. **Methods**: - **`format_with_template(template_str: str) -> str`**: - Use a `Template` string to perform substitutions using the dictionary provided during initialization. - The method should raise a `KeyError` if a variable in the template string is not found in the dictionary. - **`capitalize_words(input_str: str) -> str`**: - Use the `capwords` function to capitalize all words in the input string. - **`custom_format(format_str: str, *args, **kwargs) -> str`**: - Use the `Formatter` class to format the given string based on `args` and `kwargs`. - Implement additional checks to raise an exception if any of the provided arguments are not used in the format string. Example Usage: ```python # Initialize with a dictionary formatter = CustomStringFormatter({\\"name\\": \\"Alice\\", \\"item\\": \\"book\\"}) # Format with a template string print(formatter.format_with_template(\\"The name wants a new item.\\")) # Output: \\"The Alice wants a new book.\\" # Capitalize words in a string print(formatter.capitalize_words(\\"this is a test string.\\")) # Output: \\"This Is A Test String.\\" # Custom format with positional and keyword arguments print(formatter.custom_format(\\"{0} bought a {item}.\\", \\"Alice\\", item=\\"book\\")) # Output: \\"Alice bought a book.\\" ``` Constraints: - You must not use libraries outside of Python\'s standard library. - The input strings for template substitutions and custom formatting are guaranteed to be valid syntactically. - You should handle errors and exceptions gracefully, ensuring that meaningful error messages are returned. Implement the `CustomStringFormatter` class as specified above.","solution":"from string import Template, capwords, Formatter class CustomStringFormatter: def __init__(self, template_vars): self.template_vars = template_vars def format_with_template(self, template_str: str) -> str: template = Template(template_str) try: return template.substitute(self.template_vars) except KeyError as e: raise KeyError(f\\"Missing key in template variables: {e}\\") def capitalize_words(self, input_str: str) -> str: return capwords(input_str) def custom_format(self, format_str: str, *args, **kwargs) -> str: formatter = Formatter() try: # Will raise KeyError if required keys are missing used_args = set() used_kwargs = set() for _, param_name, _, _ in formatter.parse(format_str): if param_name: if param_name.isdigit(): used_args.add(int(param_name)) else: used_kwargs.add(param_name) if len(args) > 0 and not used_args: raise ValueError(\\"Positional arguments provided but not used in the format string\\") if len(kwargs) > 0 and not used_kwargs: raise ValueError(\\"Keyword arguments provided but not used in the format string\\") formatted_str = formatter.vformat(format_str, args, kwargs) return formatted_str except KeyError as e: raise KeyError(f\\"Missing key in keyword arguments: {e}\\")"},{"question":"Objective: This assessment evaluates your proficiency in handling file input/output operations, JSON serialization/deserialization, and sophisticated string formatting in Python. Problem Statement: You are tasked with developing a function that reads a JSON file containing a list of dictionaries, processes the data, and writes the output to a new file with formatted string content. Function Signature: ```python def process_json_file(input_filename: str, output_filename: str) -> None: pass ``` Input: - `input_filename` (str): The name of the input JSON file. - `output_filename` (str): The name of the output file where the processed data will be written. The input JSON file will be a list of dictionaries, where each dictionary represents a record with the following structure: ```json [ { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"occupation\\": \\"Engineer\\" }, ... ] ``` Output: The function should write to the `output_filename` with each record formatted into a readable string. The format for each line in the output file should be: ``` Name: John Doe, Age: 30, Occupation: Engineer ``` Constraints: - The input file must be handled using the `with open` statement to ensure proper file handling. - Direct usage of `print()` for output is not allowed; use file writing methods. - Ensure the records are formatted using f-strings for readability. - Handle any potential exceptions related to file operations gracefully and log any issues encountered. Examples: Consider the input JSON file `people.json` containing: ```json [ { \\"name\\": \\"Alice Smith\\", \\"age\\": 28, \\"occupation\\": \\"Data Scientist\\" }, { \\"name\\": \\"Bob Johnson\\", \\"age\\": 35, \\"occupation\\": \\"Manager\\" } ] ``` After calling: ```python process_json_file(\'people.json\', \'output.txt\') ``` The `output.txt` should contain: ``` Name: Alice Smith, Age: 28, Occupation: Data Scientist Name: Bob Johnson, Age: 35, Occupation: Manager ``` Notes: - Ensure the output file is properly closed after writing. - Provide comments in your code to explain your logic and approach. - Any assumptions or edge cases handled should be documented in comments. Good luck!","solution":"import json def process_json_file(input_filename: str, output_filename: str) -> None: try: # Read json file with open(input_filename, \'r\') as infile: data = json.load(infile) # Process and write to output file with open(output_filename, \'w\') as outfile: for record in data: name = record.get(\'name\', \'Unknown\') age = record.get(\'age\', \'Unknown\') occupation = record.get(\'occupation\', \'Unknown\') formatted_record = f\\"Name: {name}, Age: {age}, Occupation: {occupation}n\\" outfile.write(formatted_record) except Exception as e: # Log the error to output file in case of any issues. with open(output_filename, \'w\') as outfile: outfile.write(f\\"An error occurred: {str(e)}\\")"},{"question":"# Custom Module Importer In Python, the `imp` module has been deprecated since Python 3.4, and the `importlib` module should be used instead for custom import operations. For this coding challenge, you are required to write a function that mimics the behavior of the `imp` module\'s `find_module` and `load_module` using the newer `importlib` methods. **Task**: Write a function `custom_import(module_name: str) -> Any` that mimics the following sequence: 1. It first searches for the module specified by `module_name`. 2. If the module is found, it returns the module object. 3. If the module is not found, it raises `ImportError`. To achieve this, you will use `importlib.util.find_spec` to find the module and then `importlib.util.module_from_spec` to load it. **Function Signature**: ```python def custom_import(module_name: str) -> Any: ``` **Input**: - `module_name`: A string representing the name of the module to be imported. This could be a top-level module or a submodule. **Output**: - Returns the module object if the module is found and successfully loaded. - Raises `ImportError` if the module cannot be found or loaded. **Example**: ```python # Suppose you want to import the `math` module. mod = custom_import(\'math\') print(mod.sqrt(16)) # Output should be 4.0 # Import a submodule like os.path mod = custom_import(\'os.path\') print(mod.basename(\'/user/home/file.txt\')) # Output should be \'file.txt\' # If the module does not exist, it should raise ImportError try: mod = custom_import(\'nonexistent_module\') except ImportError: print(\\"Module not found\\") # Expected behavior ``` **Constraints**: - You are not allowed to use the `imp` module in your implementation. - Make sure you handle hierarchical module names properly (e.g., `os.path`). Good Luck!","solution":"import importlib.util from typing import Any def custom_import(module_name: str) -> Any: Custom import function to load a module by name using importlib. Args: - module_name (str): The name of the module to import. Returns: - Any: The loaded module object. Raises: - ImportError: If the module cannot be found or loaded. spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module"},{"question":"You need to design a Python script that packages a directory containing Python modules into a distributable format. You will use the `distutils` library to achieve this. Task Write a Python script that: 1. Reads the metadata of a Python package from a configuration file. 2. Packages the Python module directory into a distributable format using the `distutils.core.setup()` function. 3. Ensures that the package includes the necessary files and dependencies specified in the configuration file. Configuration File Create a configuration file named `package_config.json` with the following fields: - `name`: The name of the package. - `version`: The version of the package. - `description`: A short description of the package. - `author`: The author\'s name. - `author_email`: The author\'s email. - `packages`: A list of packages to include. - `install_requires`: A list of dependencies. Example of `package_config.json`: ```json { \\"name\\": \\"sample_package\\", \\"version\\": \\"0.1\\", \\"description\\": \\"A sample Python package\\", \\"author\\": \\"Jane Doe\\", \\"author_email\\": \\"jane.doe@example.com\\", \\"packages\\": [\\"sample_package\\"], \\"install_requires\\": [\\"requests\\"] } ``` Requirements 1. **Input**: Read the configuration file `package_config.json` to get the package metadata. 2. **Package Creation**: Use the `distutils.core.setup()` function to package the directory. 3. **Output**: Generate the distribution files in the current directory. 4. **Error Handling**: Implement error handling to manage missing configuration fields or invalid formats. Constraints - Ensure the script is compatible with Python 3.6+. - The configuration file must be named `package_config.json`. Performance Requirements - The script should handle large packages efficiently. - Ensure the packaging process does not lead to memory bloat by processing files sequentially. Example Given the `package_config.json` as shown above and a directory structure as follows: ``` sample_package/ __init__.py module1.py module2.py ``` Running your script should create a distributable package for `sample_package`. ```bash python package_script.py ``` The above command should generate the necessary distribution files in the current working directory.","solution":"import json import os import sys from distutils.core import setup def read_config(file_path): with open(file_path, \'r\') as file: return json.load(file) def package_directory(config_path=\'package_config.json\'): try: config = read_config(config_path) # Validating config fields required_keys = [\\"name\\", \\"version\\", \\"description\\", \\"author\\", \\"author_email\\", \\"packages\\", \\"install_requires\\"] for key in required_keys: if key not in config: raise ValueError(f\\"Missing required field \'{key}\' in configuration\\") setup( name=config[\'name\'], version=config[\'version\'], description=config[\'description\'], author=config[\'author\'], author_email=config[\'author_email\'], packages=config[\'packages\'], install_requires=config[\'install_requires\'] ) except json.JSONDecodeError: print(\\"Error reading the configuration file. Ensure it is a valid JSON.\\") except FileNotFoundError: print(f\\"Configuration file \'{config_path}\' not found.\\") except ValueError as e: print(f\\"Error in configuration: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") if __name__ == \\"__main__\\": package_directory()"},{"question":"**Question: Implement a Mini Traceback Generator** You are required to implement a mini traceback generator using the `linecache` module. Your task is to create a function `generate_traceback(error_traceback, num_lines)` that accepts an error traceback string and a number `num_lines`. The function should return a formatted traceback string including the specified number of context lines around each error line, fetched using `linecache.getline`. # Function Signature ```python def generate_traceback(error_traceback: str, num_lines: int) -> str: ``` # Input 1. `error_traceback` (str): A multiline string representing the error traceback, where each line contains a file path and line number in the format: ``` File \\"/path/to/file.py\\", line 10, in <module> ``` 2. `num_lines` (int): The number of lines before and after the error line to include in the context. # Output - Returns a string that contains the formatted mini traceback, including the specified number of context lines around each error line. Each section should be formatted as follows: ``` File \\"/path/to/file.py\\", line 10: Context line - num_lines Context line - (num_lines - 1) ... Faulty line Context line + 1 ... Context line + num_lines ``` # Constraints - Ensure the function does not raise any exceptions, similar to the behavior of `linecache`. - Context lines should be retrieved using the `linecache` module. - If a line number is out of bounds, it should not be included in the output. # Example ```python error_traceback = \'\'\'File \\"/path/to/file.py\\", line 10, in <module> File \\"/another/file.py\\", line 5, in <module>\'\'\' print(generate_traceback(error_traceback, 2)) ``` Given the input files, the function should return a formatted string showing the lines surrounding the specified faulty lines. # Notes - Handle edge cases where files might not exist. - You can use the `linecache` module\'s functions to load lines from files and manage the cache appropriately.","solution":"import linecache def generate_traceback(error_traceback: str, num_lines: int) -> str: Generates a mini traceback string with context lines around each error line. lines = error_traceback.strip().split(\'n\') formatted_traceback = [] for line in lines: if \\"File \\" in line: parts = line.split(\',\') file_part = parts[0].strip().split(\' \')[1].strip(\'\\"\') line_part = int(parts[1].strip().split(\' \')[1]) formatted_traceback.append(f\'{line_part}\') # Get context lines context_lines = [] for i in range(line_part - num_lines, line_part + num_lines + 1): context_line = linecache.getline(file_part, i).strip() if context_line: context_lines.append(f\' {context_line}\') file_context = f\'File \\"{file_part}\\", line {line_part}:n\' + \\"n\\".join(context_lines) formatted_traceback.append(file_context) return \\"nn\\".join(formatted_traceback)"},{"question":"on Error and Exception Handling in Python # Objective The objective of this question is to assess your understanding and implementation of exception handling in Python, including handling multiple types of exceptions, raising exceptions, and making use of finally and the with statement for clean-up actions. # Problem Statement You need to implement a function `process_data(file_path: str, data: list) -> float`, which performs the following operations: 1. Reads a floating-point number from a file given by `file_path`. 2. Computes the average of the list of numbers provided in the `data`. 3. Adds the number read from the file to the average computed in step 2. 4. Returns the result. # Detailed Steps - The function should read a single floating-point number from the file specified by the `file_path`. This file is guaranteed to contain exactly one number on the first line. - The function should compute the average of the numbers in the list `data`. - It should handle the following exceptions: - If the file cannot be found, raise a `FileNotFoundError`. - If the content of the file cannot be converted to a floating-point number, raise a `ValueError`. - If the `data` list is empty, raise a `ValueError` with the message \\"Data list is empty\\". - If any item in the `data` list is not a number, raise a `TypeError`. - Ensure that the file is properly closed after reading, regardless of whether an exception occurs or not. - If multiple exceptions could occur, handle them specifically and raise appropriate error messages. # Function Signature ```python def process_data(file_path: str, data: list) -> float: pass ``` # Constraints - You must ensure that the file is closed properly after its usage. - The function must handle exceptions cleanly and provide informative error messages. - Do not use any external libraries except for Python\'s built-in exceptions and file handling capabilities. # Example ```python # Example 1: # Assuming the content of \'number.txt\' is \\"5.2\\" file_path = \'number.txt\' data = [1.0, 2.0, 3.0] try: result = process_data(file_path, data) print(result) # should print 7.2 (average of data is 2.0, and 2.0 + 5.2 = 7.2) except Exception as e: print(e) # Example 2: # Assuming the content of \'number.txt\' is \\"five\\" file_path = \'number.txt\' data = [1.0, 2.0, 3.0] try: result = process_data(file_path, data) print(result) # should raise ValueError indicating the conversion issue except Exception as e: print(e) # Output: \\"Could not convert data to a float.\\" # Example 3: file_path = \'non_existent_file.txt\' data = [1.0, 2.0, 3.0] try: result = process_data(file_path, data) print(result) # should raise FileNotFoundError except Exception as e: print(e) # Output: \\"File not found.\\" # Example 4: file_path = \'number.txt\' data = [] try: result = process_data(file_path, data) print(result) # should raise ValueError indicating the empty data list except Exception as e: print(e) # Output: \\"Data list is empty.\\" ``` # Explanation The function should effectively handle different exceptions related to file reading, data type mismatches, and the emptiness of the data list. The use of try-except blocks and the finally clause or with statement is crucial to ensure resource management and proper exception handling.","solution":"def process_data(file_path: str, data: list) -> float: if not data: raise ValueError(\\"Data list is empty\\") for item in data: if not isinstance(item, (int, float)): raise TypeError(\\"All items in the data list must be numbers\\") try: with open(file_path, \'r\') as file: try: file_value = float(file.readline().strip()) except ValueError: raise ValueError(\\"Could not convert file data to a float.\\") except FileNotFoundError: raise FileNotFoundError(\\"File not found.\\") avg_data = sum(data) / len(data) return avg_data + file_value"},{"question":"You are provided with a dataset and asked to implement Kernel Ridge Regression (KRR) using scikit-learn to predict target values. You should also compare its performance with Support Vector Regression (SVR) on the same dataset in terms of both fitting time and prediction accuracy. # Dataset You will use the provided synthetic dataset: ```python import numpy as np np.random.seed(0) n_samples = 100 X = np.sort(5 * np.random.rand(n_samples, 1), axis=0) y = np.sin(X).ravel() # Add noise to targets y[::5] += 1 * (0.5 - np.random.rand(n_samples // 5)) ``` # Tasks 1. **Implement Kernel Ridge Regression (KRR):** - Train a KRR model on the dataset. - Optimize the hyperparameters of KRR using grid-search. - Report best parameters. 2. **Implement Support Vector Regression (SVR):** - Train an SVR model on the dataset. - Optimize the hyperparameters of SVR using grid-search. - Report best parameters. 3. **Performance Analysis:** - Compare the time taken for fitting both models. - Compare the prediction accuracy of both models using Mean Squared Error (MSE). # Input and Output - **Input:** - `X`: Feature array of shape (n_samples, 1). - `y`: Target values of shape (n_samples,). - **Output:** - Best hyperparameters for both models. - Fitting times for both models. - Mean Squared Error for both models on the provided dataset. # Constraints - Use Radial Basis Function (RBF) kernel for both models. - Utilize scikit-learn’s `GridSearchCV` for hyperparameter optimization. - You are constrained to using scikit-learn for the solution. # Example Code Here is a minimal example to get you started with KRR: ```python from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error # Define the model and parameters krr = KernelRidge(kernel=\'rbf\') param_grid_krr = {\\"alpha\\": [1e-3, 1e-2, 1e-1, 1], \\"gamma\\": np.logspace(-2, 2, 5)} # Perform grid search grid_search_krr = GridSearchCV(krr, param_grid=param_grid_krr, cv=5) grid_search_krr.fit(X, y) # Report best parameters and performance best_params_krr = grid_search_krr.best_params_ y_pred_krr = grid_search_krr.predict(X) mse_krr = mean_squared_error(y, y_pred_krr) print(\\"Best parameters (KRR):\\", best_params_krr) print(\\"MSE (KRR):\\", mse_krr) ``` Translate similar steps for SVR and compare their performance as required.","solution":"import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import GridSearchCV from sklearn.svm import SVR from sklearn.metrics import mean_squared_error from time import time def kernel_ridge_regression(X, y): # Define the model and parameters krr = KernelRidge(kernel=\'rbf\') param_grid_krr = { \\"alpha\\": [1e-3, 1e-2, 1e-1, 1], \\"gamma\\": np.logspace(-2, 2, 5) } # Perform grid search grid_search_krr = GridSearchCV(krr, param_grid=param_grid_krr, cv=5) start_krr = time() grid_search_krr.fit(X, y) end_krr = time() # Report best parameters and performance best_params_krr = grid_search_krr.best_params_ y_pred_krr = grid_search_krr.predict(X) mse_krr = mean_squared_error(y, y_pred_krr) fitting_time_krr = end_krr - start_krr return best_params_krr, fitting_time_krr, mse_krr def support_vector_regression(X, y): # Define the model and parameters svr = SVR(kernel=\'rbf\') param_grid_svr = { \\"C\\": [1, 10, 100, 1000], \\"gamma\\": np.logspace(-2, 2, 5) } # Perform grid search grid_search_svr = GridSearchCV(svr, param_grid=param_grid_svr, cv=5) start_svr = time() grid_search_svr.fit(X, y) end_svr = time() # Report best parameters and performance best_params_svr = grid_search_svr.best_params_ y_pred_svr = grid_search_svr.predict(X) mse_svr = mean_squared_error(y, y_pred_svr) fitting_time_svr = end_svr - start_svr return best_params_svr, fitting_time_svr, mse_svr # Prepare dataset np.random.seed(0) n_samples = 100 X = np.sort(5 * np.random.rand(n_samples, 1), axis=0) y = np.sin(X).ravel() y[::5] += 1 * (0.5 - np.random.rand(n_samples // 5)) # Run KRR best_params_krr, fitting_time_krr, mse_krr = kernel_ridge_regression(X, y) print(\\"KRR - Best parameters:\\", best_params_krr) print(\\"KRR - Fitting time:\\", fitting_time_krr) print(\\"KRR - MSE:\\", mse_krr) # Run SVR best_params_svr, fitting_time_svr, mse_svr = support_vector_regression(X, y) print(\\"SVR - Best parameters:\\", best_params_svr) print(\\"SVR - Fitting time:\\", fitting_time_svr) print(\\"SVR - MSE:\\", mse_svr)"},{"question":"# Support Vector Machines for Classification Tasks In this coding assessment, you are required to implement a function that uses Support Vector Machines (SVM) to classify a given dataset into binary categories. You must implement the function by utilizing the scikit-learn library\'s SVM functionalities. # Objective: To assess your comprehension of scikit-learn’s SVM implementation, parameters\' significance, and model evaluation. # Task: Implement a function `train_evaluate_svm(X_train, y_train, X_test, y_test, kernel_type)`: 1. **Input**: - `X_train`: List of lists, where each sub-list represents the features of a training sample (dimensions: `[n_samples_train, n_features]`). - `y_train`: List of integers, where each integer represents the label of a training sample (dimensions: `[n_samples_train]`). - `X_test`: List of lists, where each sub-list represents the features of a testing sample (dimensions: `[n_samples_test, n_features]`). - `y_test`: List of integers, where each integer represents the label of a testing sample (dimensions: `[n_samples_test]`). - `kernel_type`: A string representing the type of kernel to be used in the SVM (`\'linear\'`, `\'poly\'`, `\'rbf\'`, `\'sigmoid\'`). 2. **Output**: - A tuple containing: - `accuracy`: A float representing the accuracy score of the model on the test data. - `support_vectors`: A list of lists representing the support vectors identified by the trained SVM model (dimensions depend on the number of support vectors and features). 3. **Constraints**: - Assume that the data is already scaled appropriately. - You should use the default values for other SVM parameters not specified in the input. 4. **Performance Requirements**: - The training and evaluation should be done efficiently to handle medium-sized datasets (up to 10,000 samples). # Example ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load dataset and create a binary classification problem data = load_iris() X = data.data y = (data.target != 0).astype(int) # Converting to binary classification (setosa vs non-setosa) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define kernel type kernel_type = \'linear\' # Call the function accuracy, support_vectors = train_evaluate_svm(X_train, y_train, X_test, y_test, kernel_type) print(f\\"Accuracy: {accuracy}\\") print(f\\"Support Vectors:n{support_vectors}\\") ``` # Notes: - Ensure that your implementation is modular and error-handling is included for invalid kernel types. - Evaluate your trained SVM model using scikit-learn’s `accuracy_score` to compute the accuracy on `X_test` and `y_test`. # Hints: - You can use the classes from `sklearn.svm` such as `SVC`. - The `support_vectors_` attribute of the fitted model will give you the support vectors. Write your implementation in Python using the scikit-learn library. The accuracy metric should reflect how well the classifier performs on the test dataset.","solution":"from sklearn.svm import SVC from sklearn.metrics import accuracy_score def train_evaluate_svm(X_train, y_train, X_test, y_test, kernel_type): Trains an SVM model using the provided training data and kernel type, and evaluates it on the test data to return accuracy and support vectors. Parameters: - X_train (list of list of floats): Training features - y_train (list of int): Training labels - X_test (list of list of floats): Testing features - y_test (list of int): Testing labels - kernel_type (str): Type of kernel to be used in SVM (\'linear\', \'poly\', \'rbf\', \'sigmoid\') Returns: - tuple: (accuracy (float), support_vectors (list of list of floats)) # Validating the kernel type if kernel_type not in [\'linear\', \'poly\', \'rbf\', \'sigmoid\']: raise ValueError(\\"Invalid kernel type. Supported types are \'linear\', \'poly\', \'rbf\', and \'sigmoid\'.\\") # Creating and training the SVM model model = SVC(kernel=kernel_type) model.fit(X_train, y_train) # Making predictions on the test dataset y_pred = model.predict(X_test) # Calculating accuracy accuracy = accuracy_score(y_test, y_pred) # Retrieving the support vectors support_vectors = model.support_vectors_.tolist() return accuracy, support_vectors"},{"question":"Title: Unicode String Manipulation and Encoding Objective: To assess the student\'s understanding of handling Unicode strings, encoding and decoding, and employing Unicode properties in Python. Description: You are required to implement a function `process_unicode_string(s: str) -> dict` that performs various operations on a given Unicode string and returns the results in a dictionary. The function should perform the following tasks: 1. **Normalize the Unicode String**: Normalize the input string using the \'NFD\' (Normalization Form D) format. 2. **Convert to Lowercase (Casefold)**: Convert the normalized string to its casefolded version for case-insensitive comparisons. 3. **Encode to UTF-8**: Encode the casefolded string to a UTF-8 byte sequence. 4. **Decode from UTF-8**: Decode the UTF-8 byte sequence back to a Unicode string. 5. **Unicode Properties**: For each character in the decoded string (from step 4): - Get the character\'s Unicode name. - Get the character\'s category. The function should return a dictionary with the following structure: ```python { \\"normalized\\": str, # Normalized string in \'NFD\' format \\"casefolded\\": str, # Casefolded version of the normalized string \\"encoded_utf8\\": bytes, # UTF-8 encoded byte sequence \\"decoded_utf8\\": str, # Decoded string from the UTF-8 byte sequence \\"char_properties\\": list # List of tuples, each with (character, name, category) } ``` Function Signature: ```python def process_unicode_string(s: str) -> dict: ``` Constraints: - The input string `s` can contain any Unicode characters. - The function should appropriately handle empty strings. - The function should ensure that any errors during encoding/decoding are managed gracefully, returning an appropriate representation (e.g., using \'replace\' error handling). Example: ```python input_string = \\"Gürzenichstraße\\" result = process_unicode_string(input_string) # Example output (values will vary): # { # \\"normalized\\": \\"Gürzenichstraße\\", # \\"casefolded\\": \\"gürzenichstrasse\\", # \\"encoded_utf8\\": b\'guxccx88rzenichstrasse\', # \\"decoded_utf8\\": \\"gürzenichstrasse\\", # \\"char_properties\\": [ # (\'g\', \'LATIN SMALL LETTER G\', \'Ll\'), # (\'u\', \'LATIN SMALL LETTER U\', \'Ll\'), # (\'̈\', \'COMBINING DIAERESIS\', \'Mn\'), # ... # ] # } ``` Notes: - Use the `unicodedata` module to get Unicode properties such as name and category. - Implement proper error handling for encoding/decoding operations. - Ensure that the function returns the expected dictionary structure.","solution":"import unicodedata def process_unicode_string(s: str) -> dict: Process the given Unicode string by performing normalization, casefolding, encoding to UTF-8, decoding from UTF-8, and retrieving Unicode properties for each character. # Normalize the input string using \'NFD\' format normalized = unicodedata.normalize(\'NFD\', s) # Convert the normalized string to its casefolded version casefolded = normalized.casefold() # Encode the casefolded string to a UTF-8 byte sequence encoded_utf8 = casefolded.encode(\'utf-8\', errors=\'replace\') # Decode the UTF-8 byte sequence back to a Unicode string decoded_utf8 = encoded_utf8.decode(\'utf-8\', errors=\'replace\') # Retrieve Unicode properties for each character in the decoded string char_properties = [ (char, unicodedata.name(char, \'UNKNOWN CHARACTER\'), unicodedata.category(char)) for char in decoded_utf8 ] return { \\"normalized\\": normalized, \\"casefolded\\": casefolded, \\"encoded_utf8\\": encoded_utf8, \\"decoded_utf8\\": decoded_utf8, \\"char_properties\\": char_properties }"},{"question":"**Python Coding Assessment: Argument Parsing with `argparse`** **Objective:** Demonstrate your ability to implement command-line argument parsing using Python\'s `argparse` module. This will involve creating a command-line utility that processes various types of arguments, including sub-commands, and applies appropriate actions based on the inputs. **Problem Statement:** Implement a command-line utility program using `argparse` that mimics a simplified file management system. The program should support the following functionalities: 1. Add a file with specified content. 2. List all added files. 3. Display the content of a specified file. 4. Remove a specified file. **Detailed Requirements:** 1. **Adding a file**: - Sub-command: `add` - Arguments: - `file_name` (positional): The name of the file to be added. - `content` (optional): The content to be added to the file. If not provided, a default message \\"Default content\\" should be used. When this command is called, it should create an entry for the file with the provided content. 2. **Listing files**: - Sub-command: `list` - No additional arguments. When this command is called, it should list all the file names that have been added. 3. **Displaying file content**: - Sub-command: `show` - Arguments: - `file_name` (positional): The name of the file whose content is to be displayed. When this command is called, it should print the content of the specified file. 4. **Removing a file**: - Sub-command: `remove` - Arguments: - `file_name` (positional): The name of the file to be removed. When this command is called, it should remove the specified file. **Constraints:** - You can maintain the file data using an in-memory dictionary where keys are file names and values are file contents. - Ensure that appropriate error messages are displayed if a user tries to operate on a non-existent file. - The program should support a `--help` option by default to display usage information. **Performance Requirements:** - The program should handle typical command-line operations efficiently. **Example Usage:** ```sh python file_manager.py add hello.txt \\"Hello World\\" python file_manager.py list hello.txt python file_manager.py show hello.txt Hello World python file_manager.py add default.txt python file_manager.py show default.txt Default content python file_manager.py remove hello.txt python file_manager.py list default.txt ``` **Implementation:** Provide the implementation of the `file_manager.py` script based on the above requirements. Ensure your code is clear, well-documented, and follows best practices for argparse usage.","solution":"import argparse import sys # In-memory dictionary to store files and their content files = {} def add_file(args): Add a file with the given name and content. content = args.content if args.content else \\"Default content\\" files[args.file_name] = content print(f\\"File \'{args.file_name}\' added.\\") def list_files(args): List all files. if files: for file_name in files: print(file_name) else: print(\\"No files added.\\") def show_file(args): Display the content of a specified file. file_name = args.file_name if file_name in files: print(files[file_name]) else: print(f\\"File \'{file_name}\' does not exist.\\") def remove_file(args): Remove the specified file. file_name = args.file_name if file_name in files: del files[file_name] print(f\\"File \'{file_name}\' removed.\\") else: print(f\\"File \'{file_name}\' does not exist.\\") def main(): parser = argparse.ArgumentParser(description=\\"File management system\\") subparsers = parser.add_subparsers(title=\\"subcommands\\") # Sub-command: add parser_add = subparsers.add_parser(\\"add\\", help=\\"Add a file with specified content\\") parser_add.add_argument(\\"file_name\\", help=\\"the name of the file to be added\\") parser_add.add_argument(\\"content\\", nargs=\'?\', help=\\"the content to be added to the file\\", default=\\"Default content\\") parser_add.set_defaults(func=add_file) # Sub-command: list parser_list = subparsers.add_parser(\\"list\\", help=\\"List all added files\\") parser_list.set_defaults(func=list_files) # Sub-command: show parser_show = subparsers.add_parser(\\"show\\", help=\\"Display the content of a specified file\\") parser_show.add_argument(\\"file_name\\", help=\\"the name of the file whose content is to be displayed\\") parser_show.set_defaults(func=show_file) # Sub-command: remove parser_remove = subparsers.add_parser(\\"remove\\", help=\\"Remove a specified file\\") parser_remove.add_argument(\\"file_name\\", help=\\"the name of the file to be removed\\") parser_remove.set_defaults(func=remove_file) args = parser.parse_args() if \'func\' in args: args.func(args) else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"<|Analysis Begin|> The provided documentation describes various utilities available in the `sklearn.utils` module of the scikit-learn package. These utilities facilitate tasks related to validation, linear algebra, random sampling, sparse matrix operations, graph computation, testing, and more. The utilities are mostly intended for internal use within scikit-learn but can offer valuable functionality for custom implementations and advanced usage. Here’s a summary of the useful functions and tools mentioned: - **Validation Tools**: Functions like `assert_all_finite`, `as_float_array`, `check_array`, `check_X_y`, `check_random_state`, etc., ensure the validity of inputs used in scikit-learn operations. They also help in converting inputs to appropriate formats and provide means to maintain reproducibility through controlled random state management. - **Efficient Linear Algebra & Array Operations**: Functions such as `randomized_svd`, `fast_logdet`, `safe_sparse_dot`, etc., implement efficient methods for common linear algebra and array operations, often optimized for performance on large datasets or specific data formats like sparse matrices. - **Sparse Matrix Operations**: Some functions, including `mean_variance_axis`, `inplace_csr_row_normalize_l1`, and others, perform efficient computations directly on sparse matrix data, which is crucial for optimizing memory and computational efficiency in various machine learning tasks. - **Graph Algorithms**: Functions like `single_source_shortest_path_length` offer graph computation functionalities which can be used for tasks like shortest path finding in graph-structured data. - **Testing Functions and Helper Utilities**: Functions like `all_estimators`, `all_displays`, and `all_functions` test the consistency of scikit-learn interfaces. Helper utilities like `gen_even_slices`, `safe_mask`, and `safe_sqr` provide efficient ways to handle specific recurring tasks. Based on these functions and their intended use cases, I will design a coding assessment question that requires a well-thought-out solution and demonstrates the student’s understanding of these utilities in practical use. <|Analysis End|> <|Question Begin|> # Coding Assessment: Implementing Custom Data Validation and Processing Pipeline with Scikit-learn Utilities **Objective:** Your task is to implement a custom data validation and processing pipeline using utilities from the `sklearn.utils` module. The pipeline should ensure the validity of inputs and apply specific transformations efficiently, leveraging available functionality from scikit-learn. **Problem Statement:** You are tasked with writing a function `custom_pipeline(data, target, random_state=None)` that performs the following steps: 1. Validates the input `data` and `target` to ensure they follow the expected formats and have consistent lengths. 2. Ensures that the input arrays do not contain NaN or infinite values. 3. Converts all elements in `data` to floats, preserving the input type (array or sparse matrix). 4. Shuffles the data and target arrays in a consistent manner, controlled by the `random_state`. 5. Computes the means and variances along the rows of the sparse input matrix (if `data` is sparse). **Function Signature:** ```python def custom_pipeline(data, target, random_state=None): Perform data validation and processing pipeline. Parameters: - data: array-like or sparse matrix of shape (n_samples, n_features) - target: array-like of shape (n_samples,) - random_state: int, RandomState instance, or None (default) for reproducible results. Returns: - processed_data: array-like or sparse matrix of shape (n_samples, n_features) - processed_target: array-like of shape (n_samples,) - row_means: array of shape (n_samples,) - row_variances: array of shape (n_samples,) ``` **Constraints:** - The input data should be validated using `check_X_y` from `sklearn.utils`. - Use `assert_all_finite` to ensure no NaN or Infinite values are present. - Conversion to float type should use `as_float_array`. - Use `shuffle` from `sklearn.utils` to shuffle the data and target consistently. - Compute row means and variances using `mean_variance_axis` if `data` is a sparse matrix. If `data` is dense, return arrays of zeros for means and variances. **Example:** ```python import numpy as np from scipy import sparse data = sparse.csr_matrix([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]) target = np.array([1, 2, 3]) processed_data, processed_target, row_means, row_variances = custom_pipeline(data, target, random_state=42) print(processed_data.toarray()) print(processed_target) print(row_means) print(row_variances) ``` **Expected Output:** The function should validate the inputs, shuffle them consistently, and compute the row means and variances efficiently if the input data is sparse. **Notes:** - Ensure the function handles both dense and sparse inputs correctly. - Write appropriate docstrings and comments for clarity. - Avoid using functions outside `sklearn.utils` for validation and processing steps where specified.","solution":"import numpy as np from scipy import sparse from sklearn.utils import check_X_y, assert_all_finite, shuffle, as_float_array, check_random_state from sklearn.utils.sparsefuncs import mean_variance_axis def custom_pipeline(data, target, random_state=None): Perform data validation and processing pipeline. Parameters: - data: array-like or sparse matrix of shape (n_samples, n_features) - target: array-like of shape (n_samples,) - random_state: int, RandomState instance, or None (default) for reproducible results. Returns: - processed_data: array-like or sparse matrix of shape (n_samples, n_features) - processed_target: array-like of shape (n_samples,) - row_means: array of shape (n_samples,) - row_variances: array of shape (n_samples,) # Validate input data and target data, target = check_X_y(data, target, accept_sparse=True) # Ensure no NaN or infinite values assert_all_finite(data) assert_all_finite(target) # Convert all elements in data to floats data = as_float_array(data) # Ensure reproducibility random_state = check_random_state(random_state) # Shuffle data and target consistently data, target = shuffle(data, target, random_state=random_state) # Initialize row means and variances if sparse.isspmatrix(data): row_means, row_variances = mean_variance_axis(data, axis=1) else: row_means = np.zeros(data.shape[0]) row_variances = np.zeros(data.shape[0]) return data, target, row_means, row_variances"},{"question":"**Objective:** You are to create a pipeline using the `pipes.Template` class to accomplish a series of text transformations. These transformations include changing text to uppercase, replacing spaces with hyphens, and appending a custom string to the text. **Question:** Using the `pipes` module, create a function `process_text(input_str: str, custom_str: str) -> str` that takes an input string and a custom string, then processes the input string through a pipeline which performs the following operations in order: 1. Converts all characters in the string to uppercase. 2. Replaces all spaces with hyphens. 3. Appends the custom string to the end of the text. **Function Signature:** ```python def process_text(input_str: str, custom_str: str) -> str: # Your code here ``` **Input:** - `input_str`: A string with length between 1 and 1000 characters. - `custom_str`: A string with length between 1 and 100 characters. **Output:** - A string that has been processed through the described pipeline. **Example:** ```python input_str = \\"hello world\\" custom_str = \\"2023\\" output_str = process_text(input_str, custom_str) print(output_str) # Expected: \\"HELLO-WORLD2023\\" ``` **Notes:** - Use `pipes.Template` to build and manage the pipeline. - Use appropriate shell commands to perform the text transformations. - Ensure proper error handling and edge case management. - Document your code clearly, explaining each step and method used. **Constraints:** - The solution must use the `pipes` module as specified. - Assume that the environment where your code runs has a POSIX-compatible shell and the necessary permissions to execute shell commands. **Performance Requirements:** - The function should complete within a reasonable time frame for all input sizes within the specified constraints. **Testing:** - Multiple test cases should be implemented to ensure the function works as expected for various input scenarios.","solution":"import pipes def process_text(input_str: str, custom_str: str) -> str: Processes the input string through a pipeline which performs the following operations in order: 1. Converts all characters in the string to uppercase. 2. Replaces all spaces with hyphens. 3. Appends the custom string to the end of the text. Uses `pipes` module to manage the pipeline. :param input_str: A string with length between 1 and 1000 characters. :param custom_str: A string with length between 1 and 100 characters. :return: A processed string. template = pipes.Template() template.append(\'tr \\"[:lower:]\\" \\"[:upper:]\\"\', \'--\') # Convert to uppercase template.append(\'tr \\" \\" \\"-\\"\', \'--\') # Replace spaces with hyphens template.append(f\'sed \\"s//{custom_str}/\\"\', \'--\') # Append custom string with template.open(\'output.txt\', \'w\') as f: f.write(input_str) with open(\'output.txt\', \'r\') as f: result = f.read().strip() # Clean up the output file import os os.remove(\'output.txt\') return result"},{"question":"# Parsing and Extracting Information from HTML Content You are required to implement a custom HTML parser using Python\'s `html.parser` module. The goal of this parser is to extract and organize information about the structure of an HTML document, such as tags, attributes, and text contents. Your parser should specifically extract meta-information (like meta tags and their attributes) and the content of all paragraphs (`<p>` tags). Requirements: - Create a subclass of `HTMLParser` named `CustomHTMLParser`. - Override the appropriate methods to extract: - All meta tag information (tag names and their attributes). - The content within all `<p>` tags. - Implement a method `get_meta_info()` that returns the meta tag information as a list of dictionaries. - Implement a method `get_paragraph_content()` that returns the content of all `<p>` tags as a list of strings. Input: A string representing the HTML content. Output: Two lists: 1. A list of dictionaries containing the meta tag information. Each dictionary should have the tag name as the key, and its attributes as the value. 2. A list of strings containing the content of each `<p>` tag encountered in the HTML. Example: ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.meta_tags = [] self.paragraphs = [] self.current_data = \'\' self.in_p_tag = False def handle_starttag(self, tag, attrs): if tag == \'meta\': attrs_dict = {} for name, value in attrs: attrs_dict[name] = value self.meta_tags.append({tag: attrs_dict}) elif tag == \'p\': self.in_p_tag = True def handle_data(self, data): if self.in_p_tag: self.current_data += data def handle_endtag(self, tag): if tag == \'p\': self.paragraphs.append(self.current_data.strip()) self.current_data = \'\' self.in_p_tag = False def get_meta_info(self): return self.meta_tags def get_paragraph_content(self): return self.paragraphs # Sample HTML Content html_content = \'\'\' <html> <head> <title>Test Page</title> <meta charset=\\"UTF-8\\"> <meta name=\\"description\\" content=\\"Free Web tutorials\\"> </head> <body> <p>This is a test paragraph.</p> <p>This is another test paragraph.</p> </body> </html> \'\'\' parser = CustomHTMLParser() parser.feed(html_content) print(\\"Meta Information:\\", parser.get_meta_info()) print(\\"Paragraph Content:\\", parser.get_paragraph_content()) ``` Expected Output: ```python Meta Information: [{\'meta\': {\'charset\': \'UTF-8\'}}, {\'meta\': {\'name\': \'description\', \'content\': \'Free Web tutorials\'}}] Paragraph Content: [\'This is a test paragraph.\', \'This is another test paragraph.\'] ``` Constraints: - Ensure the parser handles nested tags and invalid HTML gracefully. - Do not use any additional libraries for parsing except the `html.parser` module.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.meta_tags = [] self.paragraphs = [] self.current_data = \'\' self.in_p_tag = False def handle_starttag(self, tag, attrs): if tag == \'meta\': attrs_dict = {name: value for name, value in attrs} self.meta_tags.append({tag: attrs_dict}) elif tag == \'p\': self.in_p_tag = True def handle_data(self, data): if self.in_p_tag: self.current_data += data def handle_endtag(self, tag): if tag == \'p\': self.paragraphs.append(self.current_data.strip()) self.current_data = \'\' self.in_p_tag = False def get_meta_info(self): return self.meta_tags def get_paragraph_content(self): return self.paragraphs # Sample HTML Content html_content = \'\'\' <html> <head> <title>Test Page</title> <meta charset=\\"UTF-8\\"> <meta name=\\"description\\" content=\\"Free Web tutorials\\"> </head> <body> <p>This is a test paragraph.</p> <p>This is another test paragraph with <em>emphasis</em> and <strong>strong text</strong>.</p> </body> </html> \'\'\' parser = CustomHTMLParser() parser.feed(html_content) meta_info = parser.get_meta_info() paragraph_content = parser.get_paragraph_content() print(\\"Meta Information:\\", meta_info) print(\\"Paragraph Content:\\", paragraph_content)"},{"question":"# Custom Visualization with Multiclass RocCurveDisplay **Objective**: Implement a custom class `MultiClassRocCurveDisplay` that visualizes the ROC curves for multiclass classification problems using the concepts of Scikit-learn\'s plotting API. **Requirements**: 1. **Methods and Attributes**: - `__init__(self, fpr, tpr, roc_auc, estimator_name)`: Initialize with false positive rates, true positive rates, AUC scores, and estimator name for each class. - `@classmethod from_estimator(cls, estimator, X, y, pos_label=None)`: Create the display object from an estimator and data, supporting visualization for multiclass cases. - `@classmethod from_predictions(cls, y_true, y_score, estimator_name)`: Create the display object from true labels and prediction scores. - `plot(self, ax=None, name=None, **kwargs)`: Plot ROC curves with support for multiple axes. 2. **Input and Output Formats**: - `from_estimator` takes an estimator, feature matrix X, true labels y (array-like), and optional positive label for binary class cases. - `from_predictions` takes true labels (array-like) and prediction scores matrix (array-like). - `plot` method optionally takes a matplotlib `ax` object and customization kwargs. 3. **Constraints**: - Support both binary and multiclass classification. - Ensure proper layout and visualization using GridSpec for multiclass cases. 4. **Performance Requirements**: - Efficient computation of ROC metrics for each class. **Example Usage**: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import label_binarize from sklearn.metrics import roc_curve, auc import matplotlib.pyplot as plt # Load iris dataset X, y = load_iris(return_X_y=True) y = label_binarize(y, classes=[0, 1, 2]) n_classes = y.shape[1] # Split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) # Train logistic regression model clf = LogisticRegression(solver=\'lbfgs\', multi_class=\'ovr\') clf.fit(X_train, y_train) # Create and plot multiclass ROC curves roc_display = MultiClassRocCurveDisplay.from_estimator(clf, X_test, y_test) roc_display.plot() plt.show() ``` **Notes**: - Ensure the class methods handle computing ROC metrics for each class. - The plot method should correctly layout the plots for each class in a grid format.","solution":"import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from sklearn.metrics import roc_curve, auc from sklearn.preprocessing import label_binarize class MultiClassRocCurveDisplay: def __init__(self, fpr, tpr, roc_auc, estimator_name): self.fpr = fpr self.tpr = tpr self.roc_auc = roc_auc self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y, pos_label=None): y_score = estimator.predict_proba(X) if len(y_score.shape) == 1: y_score = y_score[:, np.newaxis] y = label_binarize(y, classes=np.arange(y_score.shape[1])) fpr = {} tpr = {} roc_auc = {} for i in range(y.shape[1]): fpr[i], tpr[i], _ = roc_curve(y[:, i], y_score[:, i]) roc_auc[i] = auc(fpr[i], tpr[i]) return cls(fpr, tpr, roc_auc, estimator.__class__.__name__) @classmethod def from_predictions(cls, y_true, y_score, estimator_name): y = label_binarize(y_true, classes=np.arange(y_score.shape[1])) fpr = {} tpr = {} roc_auc = {} for i in range(y.shape[1]): fpr[i], tpr[i], _ = roc_curve(y[:, i], y_score[:, i]) roc_auc[i] = auc(fpr[i], tpr[i]) return cls(fpr, tpr, roc_auc, estimator_name) def plot(self, ax=None, name=None, **kwargs): n_classes = len(self.fpr) if ax is None: fig, ax = plt.subplots(1, n_classes, figsize=(15, 5), constrained_layout=True) if not isinstance(ax, np.ndarray): ax = [ax] if len(ax) != n_classes: raise ValueError(f\\"Number of axes ({len(ax)}) does not match number of classes ({n_classes}).\\") name = self.estimator_name if name is None else name for i in range(n_classes): ax[i].plot(self.fpr[i], self.tpr[i], lw=2, label=f\'ROC curve (area = {self.roc_auc[i]:0.2f})\', **kwargs) ax[i].plot([0, 1], [0, 1], \'k--\', lw=2) ax[i].set_xlim([0.0, 1.0]) ax[i].set_ylim([0.0, 1.05]) ax[i].set_xlabel(\'False Positive Rate\') ax[i].set_ylabel(\'True Positive Rate\') ax[i].set_title(f\'Class {i} ROC curve: {name}\') ax[i].legend(loc=\\"lower right\\") plt.show()"},{"question":"# Objective: Utilize the `pickletools` module to analyze, iterate over, and optimize a given pickled data. This exercise will test your understanding of the `pickletools` module and your ability to manipulate and analyze pickled data effectively. # Task: You are given a pickled string representing a Python list of tuples. Your task is to implement a function `analyze_and_optimize_pickled_data` which does the following: 1. **Disassemble the Pickle**: Use `pickletools.dis` to disassemble the given pickled data and output the disassembly to a string. 2. **Iterate over Opcodes**: Use `pickletools.genops` to iterate over the opcodes of the pickled data. Create a summary dictionary that contains the count of each unique opcode. 3. **Optimize the Pickle**: Use `pickletools.optimize` to generate an optimized version of the pickled string. 4. **Return Results**: Return a tuple of three items: - The disassembled string - The summary dictionary of opcode counts - The optimized pickled string # Input: - `pickled_data` (str): A pickled string representing a Python list of tuples. # Output: - `Tuple[str, Dict[str, int], str]`: A tuple containing the disassembled string, the dictionary of opcode counts, and the optimized pickled string. # Example: ```python import pickle # Example pickled data data = [(1, 2), (3, 4), (5, 6)] pickled_data = pickle.dumps(data) result = analyze_and_optimize_pickled_data(pickled_data) # Expected structure of result # ( # \\"Disassembly string...\\", # {\\"PROTO\\": 1, \\"BININT1\\": 6, \\"TUPLE2\\": 3, \\"BINPUT\\": 3, \\"STOP\\": 1 }, # \\"Optimized pickled string...\\" # ) ``` # Constraints: - You may assume the input pickled data is always valid and represents a Python list of tuples. # Requirements: - Make sure to use the `pickletools` functions as described to achieve the task. - Ensure the function is efficient and clear. ```python import pickletools def analyze_and_optimize_pickled_data(pickled_data: str) -> tuple: # Step 1: Disassemble the pickle disassembly = [] pickletools.dis(pickled_data, out=disassembly.append) disassembled_str = \\"n\\".join(disassembly) # Step 2: Iterate over opcodes opcode_summary = {} for opcode, arg, pos in pickletools.genops(pickled_data): opcode_name = opcode.name if opcode_name in opcode_summary: opcode_summary[opcode_name] += 1 else: opcode_summary[opcode_name] = 1 # Step 3: Optimize the pickle optimized_pickled_str = pickletools.optimize(pickled_data) # Return the results return (disassembled_str, opcode_summary, optimized_pickled_str) ``` Use the example pickled string provided and your implementation to validate the function. Ensure the function works correctly and meets the requirements specified.","solution":"import pickletools def analyze_and_optimize_pickled_data(pickled_data: bytes) -> tuple: # Step 1: Disassemble the pickle import io disassembly_output = io.StringIO() pickletools.dis(pickled_data, out=disassembly_output) disassembled_str = disassembly_output.getvalue() # Step 2: Iterate over opcodes opcode_summary = {} for opcode, arg, pos in pickletools.genops(pickled_data): opcode_name = opcode.name if opcode_name in opcode_summary: opcode_summary[opcode_name] += 1 else: opcode_summary[opcode_name] = 1 # Step 3: Optimize the pickle optimized_pickled_data = pickletools.optimize(pickled_data) # Return the results return (disassembled_str, opcode_summary, optimized_pickled_data)"},{"question":"Resource Management Assessment # Objective: Write a function in Python that dynamically manages the resource limits of a program using the `resource` module. The function should take a resource type, a soft limit, and a hard limit, apply these limits to the current process, and then return the current resource usage. # Function Signature: ```python def manage_resource_limits(resource_type: int, soft_limit: int, hard_limit: int) -> dict: pass ``` # Input: - `resource_type` (int): The type of resource to limit (e.g., `resource.RLIMIT_CPU`). - `soft_limit` (int): The soft limit for the specified resource. - `hard_limit` (int): The hard limit for the specified resource. # Output: - A dictionary containing the following keys and their respective values: - `resource_consumption`: An object returned by `resource.getrusage()` that describes the resources consumed by the current process. # Constraints: 1. The soft limit should not be greater than the hard limit. If `soft_limit > hard_limit`, raise a `ValueError`. 2. The current process should be able to adjust the soft limit if it does not exceed the hard limit. 3. The process should handle the raising of necessary exceptions (e.g., `ValueError`, `OSError`). # Performance requirements: - The function should efficiently set and get resource limits. - Make sure the code handles various edge cases, such as applying limits to resources that are not supported by the underlying system. # Example: ```python import resource def manage_resource_limits(resource_type: int, soft_limit: int, hard_limit: int) -> dict: if soft_limit > hard_limit: raise ValueError(\\"Soft limit cannot be greater than hard limit\\") try: resource.setrlimit(resource_type, (soft_limit, hard_limit)) except (ValueError, OSError) as e: return {\\"error\\": str(e)} usage = resource.getrusage(resource.RUSAGE_SELF) return {\\"resource_consumption\\": usage} # Example usage limits = manage_resource_limits(resource.RLIMIT_CPU, 10, 20) print(limits) ``` In this example, we limit the CPU usage of the current process to 10 seconds soft limit and 20 seconds hard limit. If successful, the function returns the resource usage after applying these limits. If there\'s an error (e.g., setting inappropriate limits), it captures and handles the exception. # Notes: - Make sure to test the function with different resources listed in the `resource` module. - Validate the input limits before applying them. - Comments and documentation lines within the code will be appreciated to understand each step clearly.","solution":"import resource def manage_resource_limits(resource_type: int, soft_limit: int, hard_limit: int) -> dict: Manage resource limits for a given resource type in the current process. :param resource_type: The type of resource to limit (e.g., resource.RLIMIT_CPU) :param soft_limit: The soft limit for the specified resource :param hard_limit: The hard limit for the specified resource :return: A dictionary containing the current resource usage :raises: ValueError if the soft limit is greater than the hard limit if soft_limit > hard_limit: raise ValueError(\\"Soft limit cannot be greater than hard limit\\") try: resource.setrlimit(resource_type, (soft_limit, hard_limit)) except (ValueError, OSError) as e: return {\\"error\\": str(e)} usage = resource.getrusage(resource.RUSAGE_SELF) return {\\"resource_consumption\\": usage}"},{"question":"# Python Coding Assessment Question **Task**: Write a Python script that performs the following tasks: 1. **Command Line Argument Processing**: - The script should accept the following command line arguments using the argparse module: - `-d` or `--directory`: The directory to scan for files (required). - `-p` or `--pattern`: The file pattern to search for (default is `*.txt`). - `-n` or `--num-lines`: The number of lines to display from each file (default is 10). 2. **File Operations**: - Scan the specified directory for files matching the given pattern using the glob module. - For each file found: - If the file is readable, print the first n lines (as specified by the `--num-lines` argument). If the file has fewer than n lines, print the entire content. - Use the os module for necessary file and directory operations. 3. **Error Handling and Performance**: - Handle any potential errors (e.g., directory not found, file not readable) gracefully by printing appropriate error messages to stderr (using the sys module). - Measure the time taken to complete the file scanning and content display using the timeit module and print this timing. **Input Specifications**: - The script should be executed from the command line with the specified arguments. **Output Specifications**: - The script should output the first n lines of each file found matching the pattern in the specified directory. - Error messages should be printed to stderr for any issues encountered. - The script should print the total time taken to complete the operation. **Example**: ```shell python script.py --directory /path/to/dir --pattern *.py --num-lines 5 ``` **Constraints**: - Ensure that the script handles large directories efficiently. - File reading should be done in a memory-efficient manner. **Performance Requirements**: - The script should handle directories with a large number of files and directories efficiently. - Ensure minimal memory usage while reading large files. **Hints**: - Use the os and glob modules for file and directory operations. - Use the argparse module for command line argument parsing. - Use the sys module to handle errors and output to stderr. - Use the timeit module to measure performance. **Deliverables**: - A single Python script file (`script.py`) implementing the above requirements.","solution":"import os import sys import glob import argparse import timeit def read_and_print_file(file_path, num_lines): Reads and prints the first num_lines of the file. try: with open(file_path, \'r\') as file: for i, line in enumerate(file): if i >= num_lines: break print(line, end=\'\') print() # Add a blank line after content from each file except Exception as e: print(f\\"Error reading file {file_path}: {e}\\", file=sys.stderr) def main(args): directory = args.directory pattern = args.pattern num_lines = args.num_lines start_time = timeit.default_timer() # Construct the full path pattern for globbing search_pattern = os.path.join(directory, pattern) files = glob.glob(search_pattern) if not files: print(f\\"No files matching the pattern {pattern} found in directory {directory}\\", file=sys.stderr) for file_path in files: if os.path.isfile(file_path) and os.access(file_path, os.R_OK): read_and_print_file(file_path, num_lines) else: print(f\\"Cannot read file: {file_path}\\", file=sys.stderr) end_time = timeit.default_timer() print(f\\"Time taken to complete the operation: {end_time - start_time:.4f} seconds\\") if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Scan a directory and print the first n lines of each file matching a pattern.\\") parser.add_argument(\'-d\', \'--directory\', required=True, help=\\"The directory to scan for files\\") parser.add_argument(\'-p\', \'--pattern\', default=\'*.txt\', help=\\"The file pattern to search for\\") parser.add_argument(\'-n\', \'--num-lines\', type=int, default=10, help=\\"The number of lines to display from each file\\") args = parser.parse_args() main(args)"},{"question":"# XML Parsing with Custom Handlers **Objective:** Implement an XML parser using the `xml.parsers.expat` module. The parser should be able to read an XML document, handle various XML events, and provide a summary of the parsed data. # Task: 1. **Create a custom XML parser:** - Use the `xml.parsers.expat.ParserCreate()` function to create an XML parser object. 2. **Define handler functions:** - `start_element_handler` to handle the start of elements. - `end_element_handler` to handle the end of elements. - `character_data_handler` to handle character data within elements. 3. **Attach handlers to the parser:** - Set the `StartElementHandler`, `EndElementHandler`, and `CharacterDataHandler` attributes of the parser object to the respective handler functions. 4. **Parse the provided XML document:** - Use the `Parse` method of the parser to process the XML data. 5. **Provide a summary of the parsed data:** - Print the names of elements encountered, number of times each element is encountered, and the character data within each element. # Constraints: - Do not use any external XML parsing libraries. - Use only functionalities provided by the `xml.parsers.expat` module. - Ensure proper error handling using the `ExpatError` exception. # Example Input: ```xml <?xml version=\\"1.0\\"?> <library> <book> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2021</year> </book> </library> ``` # Expected Output: ``` Element: library, Count: 1 Element: book, Count: 2 Element: title, Count: 2, Data: \\"Python Programming\\" \\"Advanced Python\\" Element: author, Count: 2, Data: \\"John Doe\\" \\"Jane Smith\\" Element: year, Count: 2, Data: \\"2020\\" \\"2021\\" ``` # Sample Code Structure: ```python import xml.parsers.expat def start_element_handler(name, attrs): # Implement the logic for start element event pass def end_element_handler(name): # Implement the logic for end element event pass def character_data_handler(data): # Implement the logic for character data event pass def parse_xml(xml_data): parser = xml.parsers.expat.ParserCreate() # Set the handler functions parser.StartElementHandler = start_element_handler parser.EndElementHandler = end_element_handler parser.CharacterDataHandler = character_data_handler try: parser.Parse(xml_data, 1) except xml.parsers.expat.ExpatError as e: print(f\\"Error: {e}\\") # Print the summary of parsed data pass # Implement the summary printing logic # Example XML data to parse xml_data = <?xml version=\\"1.0\\"?> <library> <book> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2021</year> </book> </library> parse_xml(xml_data) ``` # Notes: - Ensure that the handlers collect relevant data. - The summary should clearly show elements\' counts and character data. - Properly handle any exceptions that may occur during parsing.","solution":"import xml.parsers.expat class MyXMLParser: def __init__(self): self.elements = {} self.current_element = None def start_element_handler(self, name, attrs): self.current_element = name if name not in self.elements: self.elements[name] = {\'count\': 0, \'data\': \'\'} self.elements[name][\'count\'] += 1 def end_element_handler(self, name): self.current_element = None def character_data_handler(self, data): if self.current_element and data.strip(): self.elements[self.current_element][\'data\'] += \' \' + data.strip() def parse_xml(self, xml_data): parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = self.start_element_handler parser.EndElementHandler = self.end_element_handler parser.CharacterDataHandler = self.character_data_handler try: parser.Parse(xml_data, 1) except xml.parsers.expat.ExpatError as e: print(f\\"Error: {e}\\") def summarize(self): for element, details in self.elements.items(): data = f\\", Data: \\"{details[\'data\']}\\"\\" if details[\'data\'] else \\"\\" print(f\\"Element: {element}, Count: {details[\'count\']}{data}\\") def main(): xml_data = <?xml version=\\"1.0\\"?> <library> <book> <title>Python Programming</title> <author>John Doe</author> <year>2020</year> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> <year>2021</year> </book> </library> parser = MyXMLParser() parser.parse_xml(xml_data) parser.summarize() if __name__ == \\"__main__\\": main()"},{"question":"# Email MIME Message Serialization You are working on an email client application that needs to generate and serialize email messages. Your task is to implement a function that generates the MIME (Multipurpose Internet Mail Extensions) representation of an email message object using the `email.generator.BytesGenerator` class based on the provided parameters. Function Signature ```python def generate_mime_email(out_fp, msg, mangle_from_=False, max_header_len=None, policy=None, unix_from=False, line_sep=None): Generates the MIME representation of an email message. :param out_fp: A file-like object with a `write` method that accepts binary data. :param msg: The email message object to serialize. :param mangle_from_: Boolean indicating if lines starting with \\"From \\" should be mangled (default is False). :param max_header_len: Integer specifying the maximum length of header lines. If None, uses policy settings (default is None). :param policy: An email policy object to control the generation. If None, uses the policy associated with the message object (default is None). :param unix_from: Boolean indicating if the Unix mailbox envelope header delimiter should be printed (default is False). :param line_sep: String specifying the line separators between lines. If None, uses the policy settings (default is None). :return: None. Writes the serialized email to the provided file-like object. pass ``` Requirements 1. **Initializing `BytesGenerator`:** Create an instance of `BytesGenerator` using the provided parameters. 2. **Message Flattening:** Ensure that the method `flatten` is called with the appropriate parameters to serialize the message. 3. **Headers and MIME Handling:** Ensure that the function accommodates header lines with lengths, content transfer encoding, non-ASCII characters, and MIME parts correctly as specified in the provided parameters. Input 1. `out_fp`: A binary file-like object with a `write` method that accepts binary data. 2. `msg`: An email message object following the structure defined by Python\'s email package. 3. `mangle_from_`: A boolean indicating if lines starting with \\"From \\" should be mangled. 4. `max_header_len`: An integer specifying the maximum length of header lines. 5. `policy`: An instance of an email policy object or None. 6. `unix_from`: A boolean indicating if the Unix mailbox envelope header delimiter should be printed. 7. `line_sep`: A string specifying the line separators between lines. Output - Writes the generated MIME representation of the email message to the provided file-like object `out_fp`. This function has no return value. Constraints - Parameters `mangle_from_`, `max_header_len`, `policy`, `unix_from`, and `line_sep` should directly affect how the MIME message is generated and serialized. - Ensure the function handles typical exceptions that might arise during email serialization, such as incorrectly formed message objects or issues with writing to the file-like object. Example Usage ```python import io from email.message import EmailMessage # Example email message msg = EmailMessage() msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg.set_content(\'This is a test email.\') # Output file-like object output = io.BytesIO() # Generate the MIME email representation generate_mime_email(output, msg, unix_from=True, line_sep=\'rn\') # Verify the generated email contents print(output.getvalue().decode(\'utf-8\')) ```","solution":"from email.generator import BytesGenerator def generate_mime_email(out_fp, msg, mangle_from_=False, max_header_len=None, policy=None, unix_from=False, line_sep=None): Generates the MIME representation of an email message. :param out_fp: A file-like object with a `write` method that accepts binary data. :param msg: The email message object to serialize. :param mangle_from_: Boolean indicating if lines starting with \\"From \\" should be mangled (default is False). :param max_header_len: Integer specifying the maximum length of header lines. If None, uses policy settings (default is None). :param policy: An email policy object to control the generation. If None, uses the policy associated with the message object (default is None). :param unix_from: Boolean indicating if the Unix mailbox envelope header delimiter should be printed (default is False). :param line_sep: String specifying the line separators between lines. If None, uses the policy settings (default is None). :return: None. Writes the serialized email to the provided file-like object. generator = BytesGenerator(out_fp, mangle_from_=mangle_from_, maxheaderlen=max_header_len, policy=policy) generator.flatten(msg, unixfrom=unix_from, linesep=line_sep)"},{"question":"# PyTorch Export IR Graph Manipulation **Objective:** You are required to create and manipulate an Export IR graph of a simple PyTorch model. This task will assess your understanding of the Export IR structure, nodes, and metadata. **Task:** 1. **Define a Simple PyTorch Model**: Create a PyTorch model with a single layer that adds two tensors. The model should take two inputs. 2. **Export the Model to an Export IR Graph**: Use the `torch.export.export` function to convert this model to its Export IR representation. 3. **Analyze the Exported Graph**: - Print the graph to understand its structure. - Ensure that the graph contains the correct nodes and their attributes. 4. **Manipulate the Graph**: - Add a new `torch.sin` operation node that takes one of the tensors and applies the sine function to it. - Adjust the final output to return the result of this new operation. 5. **Verify the Metadata**: Ensure that all nodes, especially the newly added one, have the correct metadata fields. **Input and Output Specifications:** - The input will be two 1-dimensional tensors of the same length. - The output should be a representation of the modified graph in Python code. **Constraints:** - Ensure that placeholders are the first nodes in the graph. - There must be exactly one output node. - The newly added node should correctly reference the appropriate placeholder node. - Metadata fields like `\\"stack_trace\\"`, `\\"val\\"`, `\\"nn_module_stack\\"`, and `\\"source_fn_stack\\"` should be correctly defined for every node. - Follow the FX text format while manipulating the graph. # Example: ```python import torch from torch import nn # Step 1: Define a simple model class AddModel(nn.Module): def forward(self, x, y): return x + y # Step 2: Export the model to Export IR example_args = (torch.randn(3), torch.randn(3)) exported_model = torch.export.export(AddModel(), example_args) # Step 3: Print and analyze the graph print(exported_model.graph) # Step 4: Manipulate the graph # Define a Function to Modify Graph def modify_graph(exported_model): # Create a new graph module with the sine operation graph = exported_model.graph_module.graph for node in graph.nodes: if node.op == \\"output\\": with graph.inserting_before(node): # Add sin operation node sin_node = graph.create_node( \'call_function\', torch.sin, args=(node.args[0][0],), kwargs={} ) node.args = (sin_node,) return graph # Modify the graph modified_graph = modify_graph(exported_model) # Verify the modified graph and its metadata print(modified_graph) # Expected output: The graph should have a sine operation node before the output node ``` **Notes:** - You may use print statements to verify the intermediate steps and the final graph. - Ensure that the modified graph preserves the constraints mentioned above.","solution":"import torch from torch import nn from torch.fx import symbolic_trace # Step 1: Define a simple model class AddModel(nn.Module): def forward(self, x, y): return x + y # Step 2: Export the model to Export IR example_args = (torch.randn(3), torch.randn(3)) model = AddModel() traced_model = symbolic_trace(model) # Step 3: Print and analyze the graph print(traced_model.graph) # Step 4: Manipulate the graph def modify_graph(traced_model): graph = traced_model.graph sin_node = None # Create sine node for node in graph.nodes: if node.op == \'output\': with graph.inserting_before(node): arg_node = node.args[0] sin_node = graph.call_function(torch.sin, (arg_node,)) node.args = (sin_node,) return graph # Modify the graph modified_graph = modify_graph(traced_model) # Ensure that the modified graph is as expected print(modified_graph)"},{"question":"**Python Coding Challenge: Expression Validator** **Objective:** Your task is to implement an expression validator for a subset of Python 3.10 expressions. This validator will check if a given string is a valid expression according to a simplified version of the Python grammar. **Problem Statement:** Write a function `is_valid_expression(expression: str) -> bool` that takes a single string input `expression` and returns `True` if the input is a syntactically valid expression, and `False` otherwise. **Constraints:** 1. The expression can only include the following: - Numeric literals (integers and floats, e.g., `123`, `45.67`) - Basic arithmetic operations `+`, `-`, `*`, `/` - Parentheses for grouping (e.g., `(1 + 2) * 3`) 2. No other types of expressions are considered valid (e.g., assignments, boolean operations, etc.). 3. Expressions do not include variables or function calls. 4. Expressions are single-line strings and do not contain newline characters. **Examples:** ```python assert is_valid_expression(\\"123 + 456\\") == True assert is_valid_expression(\\"(1 + 2) * 3.5\\") == True assert is_valid_expression(\\"2 + (3 - 1)\\") == True assert is_valid_expression(\\"(4 + 5\\") == False # Unmatched parenthesis assert is_valid_expression(\\"7 + * 3\\") == False # Invalid operator sequence assert is_valid_expression(\\"(1 + 2) * (\\") == False # Unmatched parenthesis at the end ``` **Note:** - You are allowed to use Python\'s `re` module for regular expressions if necessary. - Ensure that your function runs efficiently for typical input lengths up to 100 characters.","solution":"def is_valid_expression(expression: str) -> bool: Checks if the given expression is valid, following the specified grammar. import re # Define a regular expression pattern for a valid expression pattern = r\'^[d+-*/().s]+\' # Check if the expression contains invalid characters if not re.match(pattern, expression): return False # Try to evaluate the expression to check for syntax validity try: eval(expression, {\\"__builtins__\\": None}, {}) except: return False return True"},{"question":"# Complex Data Manipulation with Collections Python\'s `collections` module provides several specialized data types that can be incredibly useful for managing and manipulating data efficiently. One such class is `defaultdict`, which simplifies the process of handling dictionary keys that do not yet exist. Problem Statement You are working for a company that processes transaction data from various branches and needs to create reports summarizing the total sales amount for each product across all branches. The transaction data is provided as a list of dictionaries where each dictionary represents a single transaction. Each transaction dictionary contains the following keys: - `\'branch\'`: The branch ID where the transaction took place. - `\'product_id\'`: The ID of the product that was sold. - `\'quantity\'`: The quantity of the product that was sold. - `\'unit_price\'`: The price per unit of the product. Your task is to write a function `aggregate_sales_data(transactions: list) -> dict` that processes this data and returns a dictionary mapping each `product_id` to the total sales amount for that product across all branches. Function Signature ```python def aggregate_sales_data(transactions: list) -> dict: pass ``` Input - `transactions` (List[Dict]): A list of transactions where each transaction is represented by a dictionary. Each dictionary contains the keys `\'branch\'`, `\'product_id\'`, `\'quantity\'`, and `\'unit_price\'`. Output - Returns a dictionary where: - The keys are `product_id` (integer). - The values are the total sales amount (float) for that product across all branches. Constraints - The `transactions` list will have at most `10^5` transactions. - The `product_id` will be an integer between 1 and 10^4. - The `quantity` and `unit_price` will be positive integers or floats. Example ```python transactions = [ {\'branch\': \'B1\', \'product_id\': 101, \'quantity\': 2, \'unit_price\': 50.0}, {\'branch\': \'B2\', \'product_id\': 101, \'quantity\': 1, \'unit_price\': 50.0}, {\'branch\': \'B1\', \'product_id\': 102, \'quantity\': 5, \'unit_price\': 30.0}, {\'branch\': \'B3\', \'product_id\': 101, \'quantity\': 3, \'unit_price\': 50.0}, ] result = aggregate_sales_data(transactions) print(result) # Expected Output: {101: 300.0, 102: 150.0} ``` Notes - Use the `defaultdict` class from the `collections` module to simplify your implementation. - Pay attention to performance as the transaction list can be very large.","solution":"from collections import defaultdict def aggregate_sales_data(transactions): Aggregates the total sales amount for each product across all branches. Args: transactions (list): A list of transaction dictionaries, where each dictionary contains \'branch\', \'product_id\', \'quantity\', and \'unit_price\'. Returns: dict: A dictionary mapping each product_id to the total sales amount. sales_aggregate = defaultdict(float) for transaction in transactions: product_id = transaction[\'product_id\'] quantity = transaction[\'quantity\'] unit_price = transaction[\'unit_price\'] sales_aggregate[product_id] += quantity * unit_price return dict(sales_aggregate)"},{"question":"# Advanced Python Networking with asyncio **Objective:** Demonstrate your ability to use the asyncio library to handle multiple asynchronous tasks, manage network connections, and schedule callbacks efficiently. **Question:** You are tasked with implementing a simple echo server and client using Python\'s asyncio library. The server should accept incoming connections, read messages from the clients, and echo the messages back. Additionally, the server should have a mechanism to shut down gracefully on receiving a specific signal. **Requirements:** 1. **Server Implementation:** - Create and start the server using `asyncio.create_server`. - Use a protocol class or a coroutine to handle client connections. - On receiving a message from a client, send the same message back to the client. - Set up signal handlers to shut down the server gracefully upon receiving `SIGINT` (Ctrl+C) or `SIGTERM`. 2. **Client Implementation:** - Implement a client that connects to the server, sends a message, waits for the echo, and prints the echoed message. - Ensure the client uses `asyncio.open_connection` to manage the connection. 3. **Main Execution:** - Create an `asyncio.run()` function to combine both server and client into a single script for demonstration purposes. - The server should be run in a task and should be able to accept multiple connections. - Use `asyncio.gather()` to run both server and client coroutines concurrently. **Constraints:** - The server should handle multiple simultaneous connections. - Use `asyncio` functions and methods for creating tasks and managing the event loop. - Ensure proper error handling and debugging options are utilized. **Performance Requirements:** - The server should be responsive and handle up to 10 simultaneous client connections without significant latency. - The client should handle network delays and server unavailability gracefully. **Input/Output Formats:** - The client should accept a message string from the user input. - The server should echo back the received message to the client. - Print appropriate debug and error messages to the console. **Example:** ```python # Run your script # Input: \\"Hello, Asyncio!\\" # Expected Output: \\"Echo: Hello, Asyncio!\\" ``` **Hints:** 1. Use `loop.create_server` and `loop.create_connection` methods to handle network operations. 2. Utilize `loop.run_forever`, `loop.stop`, or `loop.run_until_complete` to manage the event loop. 3. Implement signal handling using `loop.add_signal_handler`. *Note: The script should be executed in a Unix-like environment where signal handling can be demonstrated effectively.* --- **Your task is to implement the above requirements in Python using the asyncio library. The following template may help you get started:** ```python import asyncio import signal async def handle_client(reader, writer): try: addr = writer.get_extra_info(\'peername\') print(f\\"Connected to {addr}\\") while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() print(f\\"Closing the connection with {addr}\\") writer.close() await writer.wait_closed() except Exception as e: print(f\\"Error handling client: {e}\\") async def start_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) print(f\'Starting server on {server.sockets[0].getsockname()}\') async with server: await server.serve_forever() def ask_exit(signame, loop): print(f\\"Received signal {signame}: shutting down\\") loop.stop() async def client(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) message = input(\\"Enter message: \\") writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\\"Echo: {data.decode()}\\") writer.close() await writer.wait_closed() async def main(): loop = asyncio.get_running_loop() for signame in {\'SIGINT\', \'SIGTERM\'}: loop.add_signal_handler( getattr(signal, signame), ask_exit, signame, loop ) server_task = loop.create_task(start_server()) client_task = loop.create_task(client()) await asyncio.gather(server_task, client_task) if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Client stopped by user\\") ```","solution":"import asyncio import signal async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connected to {addr}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() print(f\\"Closing the connection with {addr}\\") writer.close() await writer.wait_closed() except Exception as e: print(f\\"Error handling client: {e}\\") async def start_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) print(f\'Starting server on {server.sockets[0].getsockname()}\') async with server: await server.serve_forever() def ask_exit(signame, loop): print(f\\"Received signal {signame}: shutting down\\") loop.stop() async def client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) writer.write(message.encode()) await writer.drain() data = await reader.read(100) response = data.decode() print(f\\"Echo: {response}\\") writer.close() await writer.wait_closed() return response async def main(): loop = asyncio.get_running_loop() for signame in {\'SIGINT\', \'SIGTERM\'}: loop.add_signal_handler( getattr(signal, signame), ask_exit, signame, loop ) server_task = loop.create_task(start_server()) await asyncio.sleep(1) # Ensure server starts before client client_task = loop.create_task(client(\\"Hello, Asyncio!\\")) await asyncio.gather(server_task, client_task) if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Client stopped by user\\")"},{"question":"# Email Parsing and Analysis using the `email.parser` Module **Problem Statement:** The `email.parser` module in Python provides flexible APIs to parse email messages. Your task is to write a Python function that parses a given multi-part email content, extracts specific information from it, and returns a summary. The email content can be provided in two ways: 1. As a complete byte string (e.g., from a file). 2. Incrementally fed chunks of byte data (e.g., from a network socket). Your function should handle both cases by: 1. Parsing the complete byte string email content. 2. Incrementally parsing the email content fed in chunks. **Function Signature:** ```python def parse_email_content(email_bytes: bytes = None, email_chunks: list = None) -> dict: ``` * `email_bytes` (optional): A bytes-like object representing the complete email content. * `email_chunks` (optional): A list of bytes-like objects representing chunks of the email content. **Return:** The function should return a dictionary with the following information extracted from the parsed email: - `subject`: The subject of the email. - `from`: The sender\'s email address. - `to`: The recipient\'s email address. - `is_multipart`: A boolean indicating whether the email is a multi-part message. - `parts_count`: The number of parts if the email is multi-part, 0 otherwise. - `content_defects`: A list of defects found in the email content if any. **Constraints:** 1. Either `email_bytes` or `email_chunks` will be provided, not both. 2. If `email_chunks` is provided, each chunk will be a valid bytes-like object. 3. The parsing should account for possible non-standard compliant emails and handle defects gracefully. **Example Usage:** ```python # Example of full email content email_content_bytes = b\\"From: sender@example.comrnTo: recipient@example.comrnSubject: Test EmailrnrnThis is a test email body.\\" # Example of email content in chunks email_content_chunks = [b\\"From: sender@example.comrn\\", b\\"To: recipient@example.comrn\\", b\\"Subject: Test EmailrnrnThis is a test email body.\\"] # Expected output output = { \\"subject\\": \\"Test Email\\", \\"from\\": \\"sender@example.com\\", \\"to\\": \\"recipient@example.com\\", \\"is_multipart\\": False, \\"parts_count\\": 0, \\"content_defects\\": [] } # Call the function using complete email content result1 = parse_email_content(email_bytes=email_content_bytes) # Call the function using email content in chunks result2 = parse_email_content(email_chunks=email_content_chunks) # Both result1 and result2 should match the expected output ``` **Notes:** 1. Utilize the `email.parser.BytesParser` and `email.parser.BytesFeedParser` as appropriate. 2. Ensure to gracefully handle defects and include them in the `content_defects` field in the output dictionary. 3. If an optional field (like subject, from, or to) is missing in the email content, it should be represented as an empty string in the output.","solution":"import email from email.parser import BytesParser, BytesFeedParser from typing import List, Optional, Dict def parse_email_content(email_bytes: Optional[bytes] = None, email_chunks: Optional[List[bytes]] = None) -> Dict: Parse the given email contents and return a summary of extracted information. :param email_bytes: A bytes-like object representing the complete email content. :param email_chunks: A list of bytes-like objects representing chunks of the email content. :return: A dictionary containing extracted email information. if email_bytes is not None: parser = BytesParser() message = parser.parsebytes(email_bytes) elif email_chunks is not None: parser = BytesFeedParser() for chunk in email_chunks: parser.feed(chunk) message = parser.close() else: raise ValueError(\\"Either email_bytes or email_chunks must be provided.\\") subject = message[\'subject\'] if message[\'subject\'] else \'\' from_address = message[\'from\'] if message[\'from\'] else \'\' to_address = message[\'to\'] if message[\'to\'] else \'\' is_multipart = message.is_multipart() parts_count = len(message.get_payload()) if is_multipart else 0 # Extract defects content_defects = message.defects return { \'subject\': subject, \'from\': from_address, \'to\': to_address, \'is_multipart\': is_multipart, \'parts_count\': parts_count, \'content_defects\': content_defects }"},{"question":"You have been provided with the documentation for the `smtpd` module in Python, which includes various classes to implement SMTP servers. Your task is to create a customized SMTP server that logs all incoming email metadata to a file. The logged information should include sender, recipients, and the subject of the email if it is present in the email headers. # Task 1. Implement a subclass of `smtpd.SMTPServer` named `LoggingSMTPServer`. 2. Override the `process_message` method to extract and log the following metadata from the incoming emails to a file named `email_log.txt`: - Sender\'s email address - Recipients\' email addresses - Subject of the email (if present) # Function Signature ```python class LoggingSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): pass ``` # Input - The **sender\'s email address** should be stored in the `mailfrom` parameter. - The **recipients\' email addresses** should be stored in the `rcpttos` list. - The **email content** should be part of the `data` string. # Output - Your `process_message` method does not need to return any value. - All email metadata should be appended to a file named `email_log.txt`. # Constraints - The `process_message` method should handle the email metadata extraction and file logging. - Ensure that logging the metadata appends to the file rather than overwriting it. # Example Suppose an email is received with the following details: - Sender: `alice@example.com` - Recipients: `bob@example.com`, `carol@example.com` - Subject: `Meeting Schedule` The logged entry in `email_log.txt` should be: ``` Sender: alice@example.com Recipients: bob@example.com, carol@example.com Subject: Meeting Schedule --- ``` # Additional Information - Emails are represented as strings in **RFC 5321** format. - The subject line can be found in the headers of the `data` string, typically prefixed with `Subject:`. # Hints - Use regex or string methods to extract the subject from the email headers. - Open the file `email_log.txt` in append mode to ensure existing logs are preserved. Implement the `LoggingSMTPServer` class below: ```python import smtpd import asyncore import re class LoggingSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): # Extract subject from data using regex subject_search = re.search(r\'^Subject:s*(.*)\', data, re.MULTILINE) subject = subject_search.group(1) if subject_search else \'(No Subject)\' # Log the metadata to a file with open(\'email_log.txt\', \'a\') as log_file: log_file.write(f\\"Sender: {mailfrom}n\\") log_file.write(f\\"Recipients: {\', \'.join(rcpttos)}n\\") log_file.write(f\\"Subject: {subject}n\\") log_file.write(\\"---n\\") # Example usage: if __name__ == \\"__main__\\": server = LoggingSMTPServer((\'localhost\', 1025), None) asyncore.loop() ```","solution":"import smtpd import asyncore import re class LoggingSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): # Extract subject from data using regex subject_search = re.search(r\'^Subject:s*(.*)\', data, re.MULTILINE) subject = subject_search.group(1) if subject_search else \'(No Subject)\' # Log the metadata to a file with open(\'email_log.txt\', \'a\') as log_file: log_file.write(f\\"Sender: {mailfrom}n\\") log_file.write(f\\"Recipients: {\', \'.join(rcpttos)}n\\") log_file.write(f\\"Subject: {subject}n\\") log_file.write(\\"---n\\") # Example usage: if __name__ == \\"__main__\\": server = LoggingSMTPServer((\'localhost\', 1025), None) asyncore.loop()"},{"question":"# Advanced Seaborn Visualization Task **Objective:** Demonstrate your ability to work with the seaborn library to create comprehensive visualizations of datasets. **Dataset:** Use the \'penguins\' dataset provided by seaborn. **Task:** 1. Load the `penguins` dataset using the `sns.load_dataset` function. 2. Create a scatterplot with marginal histograms to visualize the relationship between `bill_length_mm` and `bill_depth_mm`. 3. Color the scatterplot points by species using the `hue` parameter. 4. Add KDE curves for both the bivariate and univariate distributions in the scatterplot, colored by species. 5. Create a hexbin plot for the same variables as an alternative visualization. 6. Customize the hexbin plot by adjusting the size (`height`) and aspect ratio (`ratio`) of the plot. 7. Use the `JointGrid` object to add a KDE plot and a rug plot onto the base hexbin plot. 8. Save the final figure as an image file named `penguins_joint_plot.png`. **Requirements:** - Verify the dataset is loaded correctly and handle any missing values appropriately (e.g., drop NA values). - Ensure your plots are clear, well-labeled, and publication-ready. - Clearly comment your code and explain each step you are performing. - The final image file should be saved correctly in the current working directory. **Input:** None. The dataset is publicly available through seaborn. **Output:** A saved image file (`penguins_joint_plot.png`) showing your custom hexbin plot with additional layers. ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Verify dataset and handle missing values penguins = penguins.dropna() # Step 3: Create a scatterplot with marginal histograms sns.set_theme(style=\\"white\\") scatter_plot = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") # Step 4: Add KDE curves kde_plot = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\") # Step 5: Create a hexbin plot hex_plot = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"hex\\") # Step 6: Customize the hexbin plot custom_hex_plot = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"hex\\", height=8, ratio=3) # Step 7: Use the JointGrid object to add KDE and rug plots g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"hex\\", height=8, ratio=3) g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) g.plot_marginals(sns.rugplot, color=\\"r\\", height=-.15, clip_on=False) # Step 8: Save the final figure plt.savefig(\\"penguins_joint_plot.png\\") ``` This question assesses skills in data visualization, handling datasets, and customizing plots using seaborn, reflecting a comprehensive understanding of advanced plotting techniques.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_plot(): Load the penguins dataset, clean it, and create a complex joint plot. Save the plot as \'penguins_joint_plot.png\'. # Step 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Handle missing values penguins = penguins.dropna() # Step 3: Set the theme sns.set_theme(style=\\"white\\") # Step 7: Use the JointGrid object to create a customized plot g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"hex\\", height=8, ratio=3) # Step 4: Add KDE and rug plots g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) g.plot_marginals(sns.rugplot, color=\\"r\\", height=-.15, clip_on=False) # Step 8: Save the final figure g.fig.savefig(\\"penguins_joint_plot.png\\")"},{"question":"**Question: Implementing a Text-Based Calculator using `curses`** Objective: Create an interactive text-based calculator using the `curses` module. The calculator should support basic arithmetic operations: addition, subtraction, multiplication, and division. It should handle user input for numbers and operations and display the result. # Requirements: 1. **Initialization**: Initialize curses and set up the main screen. 2. **Input Handling**: Capture user input for numbers and operations. 3. **Display**: Display the current input and result on the screen. 4. **Calculation**: Perform the arithmetic operations based on user input. 5. **Error Handling**: Gracefully handle errors (e.g., division by zero) and invalid inputs. 6. **Termination**: Properly terminate the curses application and restore the terminal to its original state. # Expected Input and Output: Input: - Numbers for the calculation (e.g., `12`, `3.5`). - Operations (`+`, `-`, `*`, `/`). - Pressing `Enter` to calculate. - A `C` key to clear the current input and start a new calculation. - A `Q` key to quit the application. Output: - Display the current input string. - Display the result after pressing `Enter`. - Display error messages for invalid calculations. # Constraints: - The program should ensure that inputs are valid numbers and operators. - The calculator should handle multiple operations sequentially (e.g., `12 + 4 - 3`). # Performance Requirements: - The calculator should be responsive to user input with minimal delay. # Implementation: Write a Python function `main(stdscr)` that implements the requirements above. Use `curses.wrapper(main)` to initialize and terminate the curses application properly. ```python import curses def main(stdscr): def refresh_screen(): stdscr.clear() stdscr.addstr(0, 0, \\"Calculator (press Q to quit, C to clear)\\") stdscr.addstr(2, 0, \\"Input: \\") stdscr.addstr(3, 0, current_input) stdscr.addstr(4, 0, \\"Result: \\" + result) if error_message: stdscr.addstr(5, 0, \\"Error: \\" + error_message, curses.A_BLINK | curses.A_BOLD) stdscr.refresh() curses.curs_set(True) stdscr.clear() current_input = \\"\\" result = \\"\\" error_message = \\"\\" while True: refresh_screen() key = stdscr.getch() if key == ord(\'q\'): break elif key == ord(\'c\'): current_input = \\"\\" result = \\"\\" error_message = \\"\\" elif key in (curses.KEY_ENTER, 10, 13): try: result = str(eval(current_input)) error_message = \\"\\" except ZeroDivisionError: error_message = \\"Division by zero is not allowed.\\" except Exception: error_message = \\"Invalid input.\\" result = \\"\\" else: try: char = chr(key) if char.isalnum() or char in \'+-*/. \': current_input += char except ValueError: pass stdscr.clear() stdscr.refresh() curses.wrapper(main) ``` # Explanation: - The `main` function manages the `curses` initialization and the main loop of the application. - The `refresh_screen` function updates the display with the current input, result, and error messages. - The application captures key presses and updates the current input or performs calculations based on the keys pressed. - The calculator performs basic arithmetic operations and handles errors gracefully. - The `\\"q\\"` key quits the application, and the `\\"c\\"` key resets the current input. Write tests to ensure that the calculator handles various input scenarios correctly, such as: - Valid calculations (`12 + 4 - 3`) - Division by zero - Invalid inputs (`12 + `, `abc`)","solution":"# Note: The curses-based application is mainly about interaction, and it cannot be directly unit tested # like a traditional function due to its reliance on real-time keyboard input and terminal display. # Instead, the solution might be used as is, but I will show a simplified function-based approach that # can be unit tested for the core calculation logic. def evaluate_expression(expression): Evaluate a mathematical expression from a string. Handles basic arithmetic operations. try: result = eval(expression) return str(result), \\"\\" except ZeroDivisionError: return \\"\\", \\"Division by zero is not allowed.\\" except Exception: return \\"\\", \\"Invalid input.\\""},{"question":"Objective: The objective of this task is to assess your understanding of fundamental and advanced concepts of scikit-learn, including dataset handling, model training, and evaluation. Problem: Using the `load_diabetes` dataset from scikit-learn, develop a solution that builds and evaluates a regression model. Requirements: 1. **Dataset Loading:** - Load the `diabetes` dataset provided by scikit-learn. 2. **Data Preprocessing:** - Split the dataset into training and testing sets (80-20 split). - Normalize the features to have zero mean and unit variance. 3. **Model Selection:** - Use Ridge Regression as the model. 4. **Training:** - Train the Ridge Regression model on the training set. 5. **Evaluation:** - Evaluate the model on the test set using Mean Squared Error (MSE) and R2 score. - Print the MSE and R2 score for the test set. 6. **Hyperparameter Tuning:** - Use cross-validation to find the best value of the regularization parameter (alpha) for Ridge Regression. Consider alpha values in the range [0.1, 1, 10, 100]. - Print the best alpha value found. Input Format: The function should not require any input from the user. It should load the dataset internally. Output Format: - Print the Mean Squared Error (MSE) on the test set. - Print the R2 score on the test set. - Print the best alpha value chosen through cross-validation. Below is an example structure in Python: ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error, r2_score def diabetes_regression(): # Load the diabetes dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Normalize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Ridge Regression model ridge = Ridge() # Hyperparameter tuning using GridSearchCV params = {\'alpha\': [0.1, 1, 10, 100]} grid_search = GridSearchCV(ridge, params, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train, y_train) # Best parameter alpha best_alpha = grid_search.best_params_[\'alpha\'] # Train the Ridge Regression model with best alpha best_ridge = Ridge(alpha=best_alpha) best_ridge.fit(X_train, y_train) # Predictions on the test set y_pred = best_ridge.predict(X_test) # Evaluate the model mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) # Print the results print(f\'Mean Squared Error on the test set: {mse}\') print(f\'R2 score on the test set: {r2}\') print(f\'Best alpha value: {best_alpha}\') # Run the function diabetes_regression() ```","solution":"from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error, r2_score def diabetes_regression(): # Load the diabetes dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Normalize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Ridge Regression model ridge = Ridge() # Hyperparameter tuning using GridSearchCV params = {\'alpha\': [0.1, 1, 10, 100]} grid_search = GridSearchCV(ridge, params, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train, y_train) # Best parameter alpha best_alpha = grid_search.best_params_[\'alpha\'] # Train the Ridge Regression model with best alpha best_ridge = Ridge(alpha=best_alpha) best_ridge.fit(X_train, y_train) # Predictions on the test set y_pred = best_ridge.predict(X_test) # Evaluate the model mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) # Print the results print(f\'Mean Squared Error on the test set: {mse}\') print(f\'R2 score on the test set: {r2}\') print(f\'Best alpha value: {best_alpha}\') return mse, r2, best_alpha"},{"question":"**Secure User Authentication System** You are required to implement a secure user authentication system using Python\'s `secrets` module. The system should support the following functionalities: 1. **Register User**: Generate a secure password for a new user. The password should be alphanumeric, at least 12 characters long, and contain at least one lowercase letter, one uppercase letter, and two digits. 2. **Generate Temporary URL**: Generate a secure temporary URL for password reset. It must contain a security token which is URL-safe. 3. **Authenticate User**: Verify if the entered password matches the stored password using constant-time comparison to mitigate timing attacks. # Function Signatures ```python def generate_password() -> str: Returns a securely generated password. pass def generate_temp_url() -> str: Returns a temporary URL containing a secure token. pass def authenticate_user(stored_password: str, entered_password: str) -> bool: Returns True if passwords match, else False. pass ``` # Input and Output - `generate_password()` should return a string that is a securely generated password conforming to the specifications. - `generate_temp_url()` should return a string that is a URL containing a secure token of reasonable byte length. - `authenticate_user(stored_password: str, entered_password: str)` should take two string arguments. The `stored_password` is the securely generated password to be stored, and `entered_password` is the password entered by the user for authentication. It should return a boolean indicating if the passwords match. # Constraints - The generated password must be securely random, with a minimum length of 12 characters including at least one lowercase letter, one uppercase letter, and two digits. - The temporary URL must contain a token that is URL-safe and difficult to guess. - Authentication should be performed in a constant-time manner to prevent timing attacks. # Example Usage ```python # Password generation password = generate_password() print(password) # e.g. \'G4iI8kPcB7Tz\' # Temporary URL generation temp_url = generate_temp_url() print(temp_url) # e.g. \'https://example.com/reset=Drmhze6EPcv0fN_81Bj-nA\' # User authentication is_authenticated = authenticate_user(password, \'G4iI8kPcB7Tz\') print(is_authenticated) # True is_authenticated = authenticate_user(password, \'wrongPassword\') print(is_authenticated) # False ``` Write the implementations of the functions `generate_password()`, `generate_temp_url()`, and `authenticate_user()` to fulfill the requirements outlined above.","solution":"import secrets import string import hmac def generate_password() -> str: Returns a securely generated password. characters = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(characters) for _ in range(12)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and sum(c.isdigit() for c in password) >= 2): break return password def generate_temp_url() -> str: Returns a temporary URL containing a secure token. token = secrets.token_urlsafe(16) return f\\"https://example.com/reset={token}\\" def authenticate_user(stored_password: str, entered_password: str) -> bool: Returns True if passwords match, else False. return hmac.compare_digest(stored_password, entered_password)"},{"question":"You are a data scientist at a retail company, and you have been tasked with predicting customer behavior to enhance sales strategies. Your task is to implement a Stochastic Gradient Descent (SGD) classification model to predict whether a customer will make a purchase based on their browsing history. # Dataset The dataset `customer_data.csv` contains the following columns: - `session_duration`: Duration of the browsing session in minutes. - `pages_visited`: Number of web pages visited during the session. - `purchase`: Binary label indicating whether a purchase was made (1) or not (0). The goal is to build a classifier to predict the `purchase` value based on the other two features. # Requirements 1. **Load and preprocess the data:** - Load the dataset using pandas. - Standardize the `session_duration` and `pages_visited` features using scikit-learn\'s `StandardScaler`. 2. **Split the data:** - Split the data into training and testing sets with a ratio of 80% training and 20% testing. 3. **Implement and train the SGD Classifier:** - Initialize an `SGDClassifier` with the following parameters: - `loss=\\"log_loss\\"` for logistic regression. - `penalty=\\"elasticnet\\"` for a combination of L1 and L2 regularization. - `max_iter=1000` for a significant number of iterations. - `tol=1e-3` for the stopping criterion. - `random_state=42` for reproducibility. - Train the classifier using the training data. 4. **Evaluate the model:** - Predict the `purchase` values for the test set using your trained model. - Calculate and print the accuracy, precision, recall, and F1-score of the predictions. 5. **Predict probability:** - Use `predict_proba` to print the probability estimates for the test samples. # Constraints - You must use the `StandardScaler` shortly before fitting the model using `SGDClassifier`. - Ensure to shuffle the dataset before splitting or use the `shuffle=True` parameter. # Input Format The input is provided in a single CSV file `customer_data.csv` in the format described above. # Output Format Output the following metrics as printed statements in the final section of your script: - Accuracy - Precision - Recall - F1-score - Probability estimates for the first 5 test samples # Performance Requirements Your solution should process the dataset efficiently and handle any exceptions related to data loading or model training gracefully, ensuring the script runs end-to-end without issues. # Example Below is an example flow of the solution (you are expected to fill in the actual code): ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Load the data data = pd.read_csv(\'customer_data.csv\') # Preprocess the data ... # Split the data ... # Initialize and train SGDClassifier ... # Evaluate the model ... # Predict probability estimates ... # Output metrics ... ``` **Note:** Ensure your final script is well-commented, explaining each step clearly to demonstrate your understanding.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_and_preprocess_data(file_path): Load and preprocess the customer data from a CSV file. - Standardizes the session_duration and pages_visited features. Parameters: file_path (str): The path to the CSV file. Returns: X (DataFrame): The preprocessed feature set. y (Series): The target variable. # Load the data data = pd.read_csv(file_path) # Separate features and target X = data[[\'session_duration\', \'pages_visited\']] y = data[\'purchase\'] # Standardize features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled, y def split_data(X, y): Split the data into training and testing sets. Parameters: X (DataFrame): The feature set. y (Series): The target variable. Returns: X_train, X_test, y_train, y_test: The split data. return train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True) def train_sgd_classifier(X_train, y_train): Train an SGDClassifier on the training data. Parameters: X_train (DataFrame): The training feature set. y_train (Series): The training target variable. Returns: model: The trained SGDClassifier instance. model = SGDClassifier(loss=\\"log_loss\\", penalty=\\"elasticnet\\", max_iter=1000, tol=1e-3, random_state=42) model.fit(X_train, y_train) return model def evaluate_model(model, X_test, y_test): Evaluate the SGDClassifier on the test data and print metrics. Parameters: model: The trained SGDClassifier instance. X_test (DataFrame): The testing feature set. y_test (Series): The testing target variable. y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) print(\\"Accuracy:\\", accuracy) print(\\"Precision:\\", precision) print(\\"Recall:\\", recall) print(\\"F1-score:\\", f1) # Predict probabilities y_prob = model.predict_proba(X_test)[:5] print(\\"Probability estimates for first 5 test samples:\\") print(y_prob) if __name__ == \\"__main__\\": # File path to the dataset file_path = \'customer_data.csv\' # Load and preprocess the data X, y = load_and_preprocess_data(file_path) # Split the data X_train, X_test, y_train, y_test = split_data(X, y) # Train the classifier model = train_sgd_classifier(X_train, y_train) # Evaluate the model evaluate_model(model, X_test, y_test)"},{"question":"Objective: Write a Python script to: 1. Train both an LDA and a QDA classifier on a given dataset and compare their performances. 2. Use LDA for dimensionality reduction and visualize the results. Dataset: You are provided with a CSV file `data.csv` containing the following columns: - `feature_1`, `feature_2`, ..., `feature_n`: Numerical features. - `label`: Categorical labels for classification. Requirements: 1. **Classification Comparison:** 1. Load the dataset and split it into training and testing sets (80-20 split). 2. Train both LDA and QDA classifiers on the training set. 3. Evaluate and compare their performance on the testing set using metrics such as accuracy, precision, recall, and F1-score. 2. **Dimensionality Reduction with LDA:** 1. After training the LDA classifier, use it to perform dimensionality reduction. 2. Project the feature data into a 2D space using LDA. 3. Generate a scatter plot of the reduced dimensional data, color-coded by their true classes. Implementation Details: - You may use scikit-learn\'s `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis` classes. - Be mindful of proper data preprocessing steps (e.g., handling missing values, scaling features). Input and Output: - **Input:** `data.csv` - **Output:** - Printed evaluation metrics for both classifiers. - A scatter plot displaying the 2D projection of the data after LDA transformation. Constraints: - Ensure your implementation can handle datasets with up to 10000 samples and 50 features. - The performance comparison and plot generation should complete within 1 minute. # Example: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis import matplotlib.pyplot as plt # Load and preprocess the data data = pd.read_csv(\'data.csv\') X = data.drop(\'label\', axis=1) y = data[\'label\'] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train LDA and QDA classifiers lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda.fit(X_train, y_train) qda.fit(X_train, y_train) # Evaluate the classifiers y_pred_lda = lda.predict(X_test) y_pred_qda = qda.predict(X_test) print(\\"LDA Classification Report:\\") print(classification_report(y_test, y_pred_lda)) print(\\"QDA Classification Report:\\") print(classification_report(y_test, y_pred_qda)) # Perform dimensionality reduction using LDA X_lda_2d = lda.transform(X)[:, :2] # Plot the reduced dimensional data plt.figure(figsize=(10, 6)) plt.scatter(X_lda_2d[:, 0], X_lda_2d[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\') plt.title(\'LDA Dimensionality Reduction\') plt.xlabel(\'LD1\') plt.ylabel(\'LD2\') plt.colorbar(label=\'Class\') plt.show() ``` **Note:** This is a basic structure and there may be additional steps required depending on the specifics of the dataset provided.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis import matplotlib.pyplot as plt def load_data(file_path): Load the dataset from a CSV file. return pd.read_csv(file_path) def split_data(data, test_size=0.2, random_state=42): Split the dataset into training and testing sets. X = data.drop(\'label\', axis=1) y = data[\'label\'] return train_test_split(X, y, test_size=test_size, random_state=random_state) def evaluate_model(model, X_train, y_train, X_test, y_test): Train the model and evaluate its performance. model.fit(X_train, y_train) y_pred = model.predict(X_test) return classification_report(y_test, y_pred, output_dict=True) def perform_dimensionality_reduction(lda, X): Perform dimensionality reduction using LDA. return lda.transform(X)[:, :2] def plot_lda_2d_projection(X_lda_2d, y): Plot the 2D projection of the data after LDA transformation. plt.figure(figsize=(10, 6)) plt.scatter(X_lda_2d[:, 0], X_lda_2d[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\') plt.title(\'LDA Dimensionality Reduction\') plt.xlabel(\'LD1\') plt.ylabel(\'LD2\') plt.colorbar(label=\'Class\') plt.show() def main(file_path): # Load and preprocess the data data = load_data(file_path) X_train, X_test, y_train, y_test = split_data(data) # Train and evaluate LDA and QDA lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() lda_report = evaluate_model(lda, X_train, y_train, X_test, y_test) qda_report = evaluate_model(qda, X_train, y_train, X_test, y_test) print(\\"LDA Classification Report:\\") print(classification_report(y_test, lda.predict(X_test))) print(\\"QDA Classification Report:\\") print(classification_report(y_test, qda.predict(X_test))) # Perform dimensionality reduction using LDA X_lda_2d = perform_dimensionality_reduction(lda, data.drop(\'label\', axis=1)) # Plot the reduced dimensional data plot_lda_2d_projection(X_lda_2d, data[\'label\'])"},{"question":"# Coding Assessment: Secure Message Transmission Objective: Implement a secure message transmission module in Python using the cryptographic services provided by `hashlib`, `hmac`, and `secrets`. Your task is to write functions that: 1. **Generate Secure Hash**: Create a secure hash of a given message using the `hashlib` module. 2. **Generate and Verify HMAC**: Create a Keyed-Hash Message Authentication Code (HMAC) for a given message and verify the integrity of the message using the `hmac` module. 3. **Secure Token Generation**: Generate a secure token using the `secrets` module to be included in the secure message transmission. # Functions: 1. **generate_secure_hash(message: str, algorithm: str = \'sha256\') -> str:** - **Input**: - `message`: The message string that needs to be hashed. - `algorithm`: The hashing algorithm to use (default \'sha256\'). Other options include \'sha512\', \'blake2b\', \'blake2s\'. - **Output**: The hexadecimal digest of the hashed message. - **Constraints**: Use the `hashlib` module. Ensure the selected algorithm is supported. 2. **create_hmac(key: bytes, message: str, algorithm: str = \'sha256\') -> str:** - **Input**: - `key`: A byte string used as the key for the HMAC. - `message`: The message string for which the HMAC is generated. - `algorithm`: The hashing algorithm to use for HMAC (default \'sha256\'). - **Output**: The hexadecimal HMAC of the message. - **Constraints**: Use the `hmac` module. The byte string key must be securely random and long enough for the selected algorithm. 3. **verify_hmac(key: bytes, message: str, hmac_to_verify: str, algorithm: str = \'sha256\') -> bool:** - **Input**: - `key`: The byte string key used to generate the HMAC. - `message`: The original message string. - `hmac_to_verify`: The HMAC string to be verified. - `algorithm`: The hashing algorithm used for HMAC (default \'sha256\'). - **Output**: Boolean indicating if the `hmac_to_verify` is valid for the given message and key. - **Constraints**: Use the `hmac` module. 4. **generate_secure_token(length: int = 32) -> str:** - **Input**: - `length`: The desired length of the secure token (in bytes, default 32). - **Output**: A securely generated token (URL-safe base64 encoded string). - **Constraints**: Use the `secrets` module. # Example Usage: ```python # Example inputs message = \\"This is a secret message.\\" key = secrets.token_bytes(32) # Generate secure hash secure_hash = generate_secure_hash(message) print(f\\"Secure Hash: {secure_hash}\\") # Generate HMAC hmac_value = create_hmac(key, message) print(f\\"HMAC: {hmac_value}\\") # Verify HMAC verification_result = verify_hmac(key, message, hmac_value) print(f\\"Verification Result: {verification_result}\\") # Generate Secure Token token = generate_secure_token() print(f\\"Secure Token: {token}\\") ``` # Notes: - Ensure that your functions handle potential errors gracefully. - Validate and document the assumptions and constraints in your code. - Performance is not a primary concern, but your code should be clear and efficient.","solution":"import hashlib import hmac import secrets import base64 def generate_secure_hash(message: str, algorithm: str = \'sha256\') -> str: Returns a secure hash of the given message using the specified hashing algorithm. :param message: The message to hash. :param algorithm: The hashing algorithm to use (default \'sha256\'). :return: Hexadecimal digest of the hashed message. try: hash_function = hashlib.new(algorithm) hash_function.update(message.encode(\'utf-8\')) return hash_function.hexdigest() except ValueError as e: raise ValueError(f\\"Unsupported algorithm \'{algorithm}\': {e}\\") def create_hmac(key: bytes, message: str, algorithm: str = \'sha256\') -> str: Creates an HMAC for a given message using the provided key and hashing algorithm. :param key: A byte string used as the key for the HMAC. :param message: The message string for which the HMAC is generated. :param algorithm: The hashing algorithm to use for HMAC (default \'sha256\'). :return: Hexadecimal HMAC of the message. try: return hmac.new(key, message.encode(\'utf-8\'), algorithm).hexdigest() except ValueError as e: raise ValueError(f\\"Unsupported algorithm \'{algorithm}\': {e}\\") def verify_hmac(key: bytes, message: str, hmac_to_verify: str, algorithm: str = \'sha256\') -> bool: Verifies the HMAC for a given message using the provided key and hashing algorithm. :param key: The byte string key used to generate the HMAC. :param message: The original message string. :param hmac_to_verify: The HMAC string to be verified. :param algorithm: The hashing algorithm used for HMAC (default \'sha256\'). :return: Boolean indicating if the hmac_to_verify is valid for the given message and key. try: expected_hmac = create_hmac(key, message, algorithm) return hmac.compare_digest(expected_hmac, hmac_to_verify) except ValueError as e: raise ValueError(f\\"Unsupported algorithm \'{algorithm}\': {e}\\") def generate_secure_token(length: int = 32) -> str: Generates a secure token of the specified length. :param length: The desired length of the secure token (in bytes, default 32). :return: A securely generated token (URL-safe base64 encoded string). token_bytes = secrets.token_bytes(length) return base64.urlsafe_b64encode(token_bytes).decode(\'utf-8\')"},{"question":"# Python 3.10 I/O Module Assessment Objective: Design a function that reads data from a text file, processes the data, and then writes both the processed and original data to a binary file using efficient buffering techniques. Task: 1. Implement a function `process_and_write_data(text_file, binary_file)` that: - Reads lines from a given text file. - Processes each line by reversing the content. - Writes the original and processed lines to a binary file in the following format: `original_line t reversed_line n`. 2. Both reading and writing operations should use buffering for efficiency. Function Signature: ```python def process_and_write_data(text_file: str, binary_file: str) -> None: pass ``` Input: - `text_file`: A string representing the path to a text file. - `binary_file`: A string representing the path to the output binary file. Output: - The function should write the processed data to the specified binary file with the format described. Constraints: - The text file contains only UTF-8 encoded text. - The file operations must handle large files efficiently. - Proper exception handling for file operations should be demonstrated. Example: Assume `input.txt` contains: ``` Hello World Python 3.10 I/O Module ``` After calling `process_and_write_data(\\"input.txt\\", \\"output.bin\\")`, the `output.bin` should contain something like: ``` Hello World dlroW olleH Python 3.10 01.3 nohtyP I/O Module eludoM O/I ``` Notes: - Make use of the `io` module classes and methods for handling different types of I/O and buffering. - Ensure the solution is efficient and handles large text files without excessive memory usage. Test Cases: To validate the implementation, the following test cases can be considered: 1. A small text file with few lines. 2. A large text file (e.g., multiple MBs) containing enough data to demonstrate buffering efficiency. 3. An empty text file should produce an empty binary file.","solution":"import io def process_and_write_data(text_file: str, binary_file: str) -> None: Reads from a text file, processes the data by reversing each line and writes the original and reversed data to a binary file efficiently using buffering. Args: text_file (str): Path to the input text file. binary_file (str): Path to the output binary file. try: # Reading from the text file using buffering with io.open(text_file, \'r\', encoding=\'utf-8\', buffering=4096) as tf, io.open(binary_file, \'wb\', buffering=4096) as bf: for line in tf: original_line = line.strip() reversed_line = original_line[::-1] output_line = f\\"{original_line}t{reversed_line}n\\" # Writing to the binary file bf.write(output_line.encode(\'utf-8\')) except IOError as e: # Handle file I/O errors print(f\\"An error occurred while reading or writing files: {e}\\")"},{"question":"**Objective:** To assess your understanding of optimizing Python code using scikit-learn, Numpy, and profiling techniques. **Problem Statement:** You are provided with an implementation of a custom k-means clustering algorithm written purely in Python. Your task is to optimize this algorithm for speed using the guidelines provided in the documentation. # Instructions 1. **Initial Implementation:** - Implement a custom k-means clustering algorithm using pure Python and Numpy. - Use the given structure for the KMeans class and complete the `fit` method. 2. **Profiling:** - Profile the initial implementation to identify bottlenecks. - Optimize the code using Numpy’s vectorized operations to avoid explicit Python loops as much as possible. 3. **Further Optimization:** - If further optimization is required, use Cython to optimize the bottleneck function. - Move the Python version into the tests for checking consistency. 4. **Multi-Processing:** - Use `joblib.Parallel` for parallelizing the computation if applicable. # KMeans Class Structure ```python import numpy as np from sklearn.utils.extmath import safe_sparse_dot class KMeans: def __init__(self, n_clusters=3, max_iter=300, tol=1e-4): self.n_clusters = n_clusters self.max_iter = max_iter self.tol = tol self.centroids = None def fit(self, X): Compute k-means clustering. Parameters ---------- X : array-like or sparse matrix, shape=(n_samples, n_features) Training instances to cluster. Attributes ---------- centroids : array, [n_clusters, n_features] Coordinates of cluster centers. n_samples, n_features = X.shape # Initialize centroids randomly from the dataset self.centroids = X[np.random.choice(n_samples, self.n_clusters, replace=False)] for i in range(self.max_iter): # Assign each sample to the nearest centroid labels = [self._closest_centroid(x) for x in X] # Calculate new centroids from the means of the points new_centroids = np.array([X[np.array(labels) == j].mean(axis=0) for j in range(self.n_clusters)]) # Convergence check (if centroids do not change significantly) if np.linalg.norm(new_centroids - self.centroids) < self.tol: break self.centroids = new_centroids return self def _closest_centroid(self, x): Find the closest centroid for a given sample x. Parameters ---------- x : array-like, shape (n_features,) A single sample. Returns ------- closest_index : int Index of the closest centroid. return np.argmin(np.linalg.norm(self.centroids - x, axis=1)) ``` # Expected Input and Output Formats - **Input:** - `X` : 2D Numpy array of shape `(n_samples, n_features)`. - **Output:** - `self` : The fitted instance of the KMeans class. # Constraints and Requirements - You should avoid explicit Python loops as much as possible. - Use profiling tools to identify optimization opportunities. Share your profiling results. - If using Cython, provide both the Python and Cython versions of the bottleneck function. - Ensure that your optimized implementation produces the same results as the initial implementation. # Performance Requirements - The optimized code should significantly reduce the execution time compared to the initial implementation. - Utilize multi-processing for additional performance enhancement if applicable. Submit your completed `KMeans` class with the initial and optimized `fit` methods, along with profiling results and any additional Cython or parallel processing code used.","solution":"import numpy as np import joblib class KMeans: def __init__(self, n_clusters=3, max_iter=300, tol=1e-4, n_jobs=-1): self.n_clusters = n_clusters self.max_iter = max_iter self.tol = tol self.centroids = None self.n_jobs = n_jobs def fit(self, X): Compute k-means clustering. Parameters ---------- X : array-like, shape=(n_samples, n_features) Training instances to cluster. Attributes ---------- centroids : array, [n_clusters, n_features] Coordinates of cluster centers. # Initialization of centroids n_samples, n_features = X.shape self.centroids = X[np.random.choice(n_samples, self.n_clusters, replace=False)] for i in range(self.max_iter): # Assign each sample to the nearest centroid labels = joblib.Parallel(n_jobs=self.n_jobs)( joblib.delayed(self._closest_centroid)(x) for x in X ) labels = np.array(labels) # Calculate new centroids from the means of the points new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(self.n_clusters)]) # Convergence check (if centroids do not change significantly) if np.linalg.norm(new_centroids - self.centroids) < self.tol: break self.centroids = new_centroids return self def _closest_centroid(self, x): Find the closest centroid for a given sample x. Parameters ---------- x: array-like, shape (n_features,) A single sample Returns ------- closest_index : int Index of the closest centroid. return np.argmin(np.linalg.norm(self.centroids - x, axis=1))"},{"question":"Objective Write functions to perform Base64 encoding and decoding, with an option to use URL-safe encoding, and include error handling for different scenarios. Task 1. **Function: `custom_base64_encode`** - **Input:** - `data` (str): The string data to be encoded. - `url_safe` (bool): A flag indicating whether to use URL-safe Base64 encoding. Default is `False`. - **Output:** - (str): The Base64 encoded string. - **Constraints:** - If `url_safe` is `True`, use the URL-safe base64 encoding (`base64.urlsafe_b64encode`); otherwise, use the standard base64 encoding (`base64.b64encode`). 2. **Function: `custom_base64_decode`** - **Input:** - `encoded_data` (str): The Base64 encoded string to be decoded. - `url_safe` (bool): A flag indicating whether to decode using URL-safe Base64 encoding. Default is `False`. - **Output:** - (str): The decoded string data. - **Constraints:** - Raise a `ValueError` with an appropriate error message if the encoded data is incorrectly padded. - If `url_safe` is `True`, use the URL-safe base64 decoding (`base64.urlsafe_b64decode`); otherwise, use the standard base-64 decoding (`base64.b64decode`). Additional Requirements - Ensure that the input data for encoding and the encoded data for decoding are handled correctly (e.g., converting strings to bytes and bytes to strings where necessary). - Implement appropriate error handling for TypeError and binascii.Error exceptions. Example ```python import base64 def custom_base64_encode(data, url_safe=False): if not isinstance(data, str): raise TypeError(\\"data must be a string\\") byte_data = data.encode(\'utf-8\') if url_safe: encoded = base64.urlsafe_b64encode(byte_data) else: encoded = base64.b64encode(byte_data) return encoded.decode(\'utf-8\') def custom_base64_decode(encoded_data, url_safe=False): if not isinstance(encoded_data, str): raise TypeError(\\"encoded_data must be a string\\") byte_encoded_data = encoded_data.encode(\'utf-8\') try: if url_safe: decoded = base64.urlsafe_b64decode(byte_encoded_data) else: decoded = base64.b64decode(byte_encoded_data) except (binascii.Error, ValueError) as e: raise ValueError(\\"Invalid Base64 encoded data\\") from e return decoded.decode(\'utf-8\') # Test Cases print(custom_base64_encode(\\"Hello, World!\\")) # Standard Base64 encoding print(custom_base64_encode(\\"Hello, World!\\", url_safe=True)) # URL-Safe Base64 encoding print(custom_base64_decode(\\"SGVsbG8sIFdvcmxkIQ==\\")) # Standard Base64 decoding print(custom_base64_decode(\\"SGVsbG8sIFdvcmxkIQ==\\", url_safe=True)) # URL-Safe Base64 decoding (raise ValueError due to incorrect padding) ``` Notes: - The function `custom_base64_encode` should correctly handle the conversion of input data to bytes-like objects, and return the encoded data as a string. - The function `custom_base64_decode` should handle errors related to incorrect padding and non-alphabet characters, raising a `ValueError` with an appropriate error message if encountered.","solution":"import base64 import binascii # Custom base64 encode function def custom_base64_encode(data, url_safe=False): if not isinstance(data, str): raise TypeError(\\"data must be a string\\") # Encode string to bytes byte_data = data.encode(\'utf-8\') # Select the encoding type if url_safe: encoded = base64.urlsafe_b64encode(byte_data) else: encoded = base64.b64encode(byte_data) # Convert and return bytes to string return encoded.decode(\'utf-8\') # Custom base64 decode function def custom_base64_decode(encoded_data, url_safe=False): if not isinstance(encoded_data, str): raise TypeError(\\"encoded_data must be a string\\") # Encode string to bytes byte_encoded_data = encoded_data.encode(\'utf-8\') try: # Select the decoding type if url_safe: decoded = base64.urlsafe_b64decode(byte_encoded_data) else: decoded = base64.b64decode(byte_encoded_data) # Convert and return bytes to string return decoded.decode(\'utf-8\') except (binascii.Error, ValueError) as e: raise ValueError(\\"Invalid Base64 encoded data\\") from e"},{"question":"# Python310 Coding Assessment Question **Title: Advanced Arithmetic and Context Management with the Decimal Module** **Objective:** Test the ability to use the `decimal` module for precise arithmetic operations and context management. **Problem Statement:** You are tasked with creating a function that performs a series of precise arithmetic operations using the `decimal` module. The function must handle different contexts, arithmetic operations, and rounding modes. You are also required to validate inputs and handle exceptions appropriately. **Function Signature:** ```python def perform_precise_operations(operations: List[Tuple[str, Any]], initial_context: dict) -> List[Decimal]: pass ``` **Input:** 1. `operations`: A list of tuples where each tuple represents an operation. The first element of the tuple is a string representing the operation type, and the second element is the value(s) needed for the operation. - Supported operations: - \\"add\\" : Addition - \\"subtract\\" : Subtraction - \\"multiply\\" : Multiplication - \\"divide\\" : Division (Handle `DivisionByZero`) - \\"sqrt\\" : Square root - \\"quantize\\" : Round to a given precision 2. `initial_context`: A dictionary containing initial context settings for precision and rounding. Keys include: - \\"prec\\": The precision setting. - \\"rounding\\": The rounding option (e.g., \\"ROUND_HALF_EVEN\\"). **Output:** A list of `Decimal` results for each operation performed in the order provided. **Constraints:** - All operations should be performed with the context specified in `initial_context`. - Validate input values and handle exceptions like `InvalidOperation`, `DivisionByZero`, and `Overflow`. - The function should not directly interact with the user (no print statements for input/output). **Example:** ```python from decimal import Decimal operations = [ (\\"add\\", (Decimal(\'1.1\'), Decimal(\'2.2\'))), (\\"subtract\\", (Decimal(\'5.5\'), Decimal(\'1.1\'))), (\\"multiply\\", (Decimal(\'3.14\'), Decimal(\'2.0\'))), (\\"divide\\", (Decimal(\'9.0\'), Decimal(\'3.0\'))), (\\"sqrt\\", Decimal(\'4.0\')), (\\"quantize\\", (Decimal(\'3.14159\'), Decimal(\'1.000\'))) ] initial_context = { \\"prec\\": 10, \\"rounding\\": \\"ROUND_HALF_UP\\" } results = perform_precise_operations(operations, initial_context) print(results) ``` **Expected Output:** ```python [Decimal(\'3.3\'), Decimal(\'4.4\'), Decimal(\'6.28\'), Decimal(\'3.0\'), Decimal(\'2.0\'), Decimal(\'3.142\')] ``` **Notes:** 1. Make sure to use decimal contexts appropriately. 2. Use appropriate exception handling for invalid operations. **Hints:** 1. Use `decimal.localcontext()` to temporarily change the active context. 2. Handle special values (`NaN`, `Infinity`) as per the `decimal` module\'s specifications. 3. Use methods and functionalities provided in the `decimal` module documentation to implement the operations.","solution":"from decimal import Decimal, localcontext from typing import List, Tuple, Dict, Any def perform_precise_operations(operations: List[Tuple[str, Any]], initial_context: Dict[str, Any]) -> List[Decimal]: results = [] with localcontext() as ctx: ctx.prec = initial_context.get(\\"prec\\", 28) ctx.rounding = initial_context.get(\\"rounding\\", \\"ROUND_HALF_EVEN\\") for operation in operations: op_type, values = operation try: if op_type == \\"add\\": result = values[0] + values[1] elif op_type == \\"subtract\\": result = values[0] - values[1] elif op_type == \\"multiply\\": result = values[0] * values[1] elif op_type == \\"divide\\": result = values[0] / values[1] elif op_type == \\"sqrt\\": result = values.sqrt() elif op_type == \\"quantize\\": result = values[0].quantize(values[1]) else: raise ValueError(f\\"Unsupported operation type: {op_type}\\") results.append(result) except Exception as e: results.append(Decimal(\'NaN\')) # Return NaN for any exception return results"},{"question":"Problem Description: You are tasked to analyze the `diamonds` dataset from seaborn and create a visualization to understand the distribution and estimation of diamond weights across different clarity levels. This visualization should provide insights into the central tendency and spread of diamond weights. Requirements: 1. **Load** the `diamonds` dataset using `seaborn.load_dataset`. 2. **Create** a plot using `seaborn.objects` that shows: - The mean carat of diamonds for each clarity level. - A confidence interval (95%) for the means using bootstrapping. - A comparison plot showing both mean and median diamond weights for each clarity level. - Weighted estimation where the weight variable is the price of the diamonds. 3. **Control** the randomness of the bootstrapping process by seeding it to `42`. 4. **Configure** the plot to clearly display the error bars for both mean and median with standard error. 5. **Present** this as a combined figure with subplots if necessary. Input: - No direct input; you will use the internal seaborn `diamonds` dataset. Expected Output: - A plot showing: - Mean carat with 95% CI as error bars for each clarity level. - Median carat for each clarity level for comparison. - Weighted estimates where the weight is the price of the diamonds. Constraints: - Use only the seaborn and matplotlib packages for visualization. - Ensure readability of the plot by properly labeling axes and legends. # Example Result: A sample visualization should include: - A plot (or subplots) where: 1. X-axis represents `clarity`. 2. Y-axis represents `carat`. 3. Means with 95% CI error bars using bootstrapping (seeding 42). 4. Median lines for each clarity level. 5. Weighted mean estimates, indicating `price` as the weighting factor. Additional Information: To help guide your implementation, you can refer to the seaborn documentation examples provided. # Submission: Submit your solution as a complete python function or script. Ensure that it is well-documented and commented to explain your choices.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import matplotlib.ticker as ticker def plot_diamond_distribution(): # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') # Set the seed for reproducibility np.random.seed(42) # Initialize the aesthetics for the plot sns.set(style=\\"whitegrid\\") # Create a figure with subplots fig, axes = plt.subplots(2, 1, figsize=(14, 10)) # Plot 1: Means with 95% CI for each clarity level sns.pointplot(ax=axes[0], x=\'clarity\', y=\'carat\', data=diamonds, estimator=np.mean, ci=95, join=False, capsize=0.1) axes[0].set_title(\'Mean Carat with 95% CI for each Clarity Level\') axes[0].set_ylabel(\'Mean Carat\') axes[0].set_xlabel(\'Clarity\') # Calculate medians for comparison clarity_levels = diamonds[\'clarity\'].unique() medians = diamonds.groupby(\'clarity\')[\'carat\'].median() median_y = [medians[clarity] for clarity in clarity_levels] # Overlay medians axes[0].plot(clarity_levels, median_y, \'r-\', label=\'Median\') axes[0].legend() # Plot 2: Weighted Means (weighted by price) for each clarity level sns.pointplot(ax=axes[1], x=\'clarity\', y=\'carat\', data=diamonds, estimator=lambda x: np.average(x, weights=diamonds.loc[x.index, \\"price\\"]), ci=\'sd\', join=False, capsize=0.1) axes[1].set_title(\'Weighted Mean Carat (Weighted by Price) for each Clarity Level\') axes[1].set_ylabel(\'Weighted Mean Carat\') axes[1].set_xlabel(\'Clarity\') # Show the plot plt.tight_layout() plt.show()"},{"question":"Advanced `dataclasses` Usage You have been tasked to create a system to manage a library\'s inventory using Python\'s `dataclasses` module. Your goal is to define a `dataclass` for the books in the library, implementing various functionalities to leverage advanced features of `dataclasses`. Requirements: 1. **Define a Dataclass for a Book:** - Fields: - `title`: str - `author`: str - `isbn`: str (should be a unique identifier) - `price`: float - `quantity`: int (should default to 0) 2. **Custom Methods:** - Add a method `total_value()` that returns the total value of the books in stock (`price * quantity`). 3. **Special Field Handling:** - Use a `default_factory` to initialize a field `tags` as an empty list (to store tags related to a book). 4. **Frozen Instances:** - Make the book instances immutable after creation (`frozen=True`). 5. **Keyword-only Parameters:** - Ensure that `price` and `quantity` are keyword-only parameters. 6. **Post-init Processing:** - Implement a `__post_init__` method to validate that the `isbn` follows a specific format (a string of 13 digits). 7. **Utility Functions:** - Implement a function `create_book` that takes in book details as parameters and returns an instance of the book. - Implement a function `update_inventory` that takes a book instance, a new quantity, and returns a new book instance with the updated quantity. 8. **Dictionary and Tuple Conversion:** - Implement functions `book_to_dict` and `book_to_tuple` to convert a book instance to a dictionary and a tuple, respectively. Implementation Details: - Expected input and output formats: - `create_book(title, author, isbn, price, quantity=0)` → returns a `Book` instance. - `update_inventory(book, new_quantity)` → returns a new `Book` instance with the updated quantity. - `book_to_dict(book)` → returns a dictionary representation of the book. - `book_to_tuple(book)` → returns a tuple representation of the book. - Constraints: - `isbn` must be a string of 13 digits; raise a `ValueError` if invalid. - `price` and `quantity` must be non-negative values. - Performance requirements: - The functions should handle typical library inventory sizes efficiently. Example Usage: ```python from dataclasses import dataclass, field, asdict, astuple, replace # Define the Book dataclass @dataclass(frozen=True) class Book: title: str author: str isbn: str price: float = field(kw_only=True) quantity: int = field(default=0, kw_only=True) tags: list[str] = field(default_factory=list) def __post_init__(self): if not self.isbn.isdigit() or len(self.isbn) != 13: raise ValueError(\\"ISBN must be a string of 13 digits.\\") def total_value(self) -> float: return self.price * self.quantity def create_book(title: str, author: str, isbn: str, *, price: float, quantity: int = 0) -> Book: return Book(title=title, author=author, isbn=isbn, price=price, quantity=quantity) def update_inventory(book: Book, new_quantity: int) -> Book: return replace(book, quantity=new_quantity) def book_to_dict(book: Book) -> dict: return asdict(book) def book_to_tuple(book: Book) -> tuple: return astuple(book) # Example book1 = create_book(title=\\"Python 101\\", author=\\"John Doe\\", isbn=\\"1234567890123\\", price=29.99, quantity=10) print(book1.total_value()) # Output: 299.9 print(book_to_dict(book1)) # Output: {\'title\': \'Python 101\', \'author\': \'John Doe\', \'isbn\': \'1234567890123\', \'price\': 29.99, \'quantity\': 10, \'tags\': []} book2 = update_inventory(book1, new_quantity=15) print(book_to_tuple(book2)) # Output: (\'Python 101\', \'John Doe\', \'1234567890123\', 29.99, 15, []) ``` Submission: Submit the implemented `Book` dataclass and the four required functions: `create_book`, `update_inventory`, `book_to_dict`, and `book_to_tuple`. Ensure your code passes all the provided example usages.","solution":"from dataclasses import dataclass, field, asdict, astuple, replace @dataclass(frozen=True) class Book: title: str author: str isbn: str price: float = field(kw_only=True) quantity: int = field(default=0, kw_only=True) tags: list[str] = field(default_factory=list) def __post_init__(self): if not self.isbn.isdigit() or len(self.isbn) != 13: raise ValueError(\\"ISBN must be a string of 13 digits.\\") def total_value(self) -> float: return self.price * self.quantity def create_book(title: str, author: str, isbn: str, *, price: float, quantity: int = 0) -> Book: return Book(title=title, author=author, isbn=isbn, price=price, quantity=quantity) def update_inventory(book: Book, new_quantity: int) -> Book: return replace(book, quantity=new_quantity) def book_to_dict(book: Book) -> dict: return asdict(book) def book_to_tuple(book: Book) -> tuple: return astuple(book)"},{"question":"# SQLite Database Management with Custom Types and Transactions In this task, you will create and manage an SQLite database using Python\'s `sqlite3` module. You need to demonstrate advanced understanding including transaction control, error handling, and custom type adapters. Task 1. **Create a database and establish a connection:** - Create an SQLite database named `company.db` in the current directory. - Connect to this database using `sqlite3.connect()`. 2. **Create a table for employees:** - The table should be named `employee` with the following columns: - `id` (INTEGER, PRIMARY KEY) - `name` (TEXT) - `position` (TEXT) - `salary` (REAL) 3. **Insert data with transaction control:** - Insert the following employees into the table in one atomic transaction: - (\'Alice Johnson\', \'Developer\', 70000) - (\'Bob Smith\', \'Manager\', 80000) - (\'Charlie Davis\', \'CTO\', 120000) 4. **Create and use custom type adapter:** - Define a Python class `Point` that represents coordinates (x, y) and adapt this class to be stored as \'x,y\' string in SQLite. - Insert an additional column `location` (TEXT) in the `employee` table to store the `Point` objects. 5. **Add data including custom type:** - Insert new employees including location data using the adapted `Point` class: - (\'David Brown\', \'Developer\', 75000, Point(100.0, 200.5)) - (\'Eve White\', \'Manager\', 82000, Point(150.3, 250.6)) 6. **Query and handle conversions:** - Read back all employees who are Developers or Managers. - For each queried result, display the employee\'s name, position, and converted `location` as `Point` objects. 7. **Transaction and Error Handling:** - Implement a robust function to wrap these operations in transactions, rolling back if any error occurs. - Raise appropriate exceptions for any integrity errors or operational errors. Constraints - Use parameterized queries to avoid SQL injection. - Handle exceptions using appropriate `sqlite3` exceptions classes. - Implement error handling mechanisms for transaction management, ensuring atomicity and rollback on errors. - Ensure the `Point` class conforms to the protocol for custom adapters. Function Signature ```python def manage_employee_database(): pass ``` Use this function to encapsulate your logic implementing the described tasks. Example Execution ```python manage_employee_database() # Expected: # Should create the database, table, insert data, and print employee details including converted location. ```","solution":"import sqlite3 import re class Point: def __init__(self, x, y): self.x = x self.y = y def __str__(self): return f\\"{self.x},{self.y}\\" @classmethod def from_string(cls, string): x, y = map(float, string.split(\',\')) return cls(x, y) def adapt_point(point): return str(point) def convert_point(s): return Point.from_string(s.decode()) def manage_employee_database(): # Register the adapter and converter sqlite3.register_adapter(Point, adapt_point) sqlite3.register_converter(\\"POINT\\", convert_point) try: connection = sqlite3.connect(\'company.db\', detect_types=sqlite3.PARSE_DECLTYPES) cursor = connection.cursor() # Create employee table cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS employee ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, position TEXT, salary REAL, location POINT ) \'\'\') # Start transaction connection.execute(\'BEGIN\') # Insert employees with transaction control employees = [ (\'Alice Johnson\', \'Developer\', 70000), (\'Bob Smith\', \'Manager\', 80000), (\'Charlie Davis\', \'CTO\', 120000) ] cursor.executemany(\'\'\' INSERT INTO employee (name, position, salary) VALUES (?, ?, ?)\'\'\', employees) # Insert employees with location data employees_with_location = [ (\'David Brown\', \'Developer\', 75000, Point(100.0, 200.5)), (\'Eve White\', \'Manager\', 82000, Point(150.3, 250.6)) ] cursor.executemany(\'\'\' INSERT INTO employee (name, position, salary, location) VALUES (?, ?, ?, ?)\'\'\', employees_with_location) # Commit transaction connection.commit() # Query and handle conversions cursor.execute(\'\'\' SELECT name, position, salary, location FROM employee WHERE position IN (\'Developer\', \'Manager\') \'\'\') rows = cursor.fetchall() for row in rows: name, position, salary, location = row print(f\\"Name: {name}, Position: {position}, Salary: {salary}, Location: {location}\\") except sqlite3.DatabaseError as e: print(f\\"Database error occurred: {e}\\") connection.rollback() except sqlite3.IntegrityError as e: print(f\\"Integrity error occurred: {e}\\") connection.rollback() except Exception as e: print(f\\"An unexpected error occurred: {e}\\") connection.rollback() finally: cursor.close() connection.close()"},{"question":"Coding Assessment Question # Objective To assess your understanding of PyTorch and attention mechanisms, you are required to implement a simple scaled dot-product attention function from scratch. This is a fundamental component often used in transformer models. # Task Write a function `scaled_dot_product_attention` that computes the scaled dot-product attention. The function should be implemented using PyTorch and should conform to the following specifications: # Input 1. `query`: A 3D tensor of shape `(batch_size, num_heads, seq_len_q, depth)` representing the query vectors. 2. `key`: A 3D tensor of shape `(batch_size, num_heads, seq_len_k, depth)` representing the key vectors. 3. `value`: A 3D tensor of shape `(batch_size, num_heads, seq_len_v, depth_v)` representing the value vectors. 4. `mask` (optional): A mask tensor of shape `(batch_size, num_heads, seq_len_q, seq_len_k)` to mask out certain positions. # Output - A 3D tensor of shape `(batch_size, num_heads, seq_len_q, depth_v)`, representing the output of the attention mechanism. # Constraints - All input tensors contain float32 values. - `depth` must be equal for `query` and `key`. - `seq_len_k` and `seq_len_v` must be equal. # Performance Requirements - Your solution should efficiently handle tensors of sizes up to `(64, 8, 128, 64)` for `query`, `key`, and `value`. # Function Signature ```python import torch def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor: pass ``` # Example ```python import torch # Example tensors batch_size = 2 num_heads = 2 seq_len_q = 3 seq_len_k = 3 depth = 4 depth_v = 4 query = torch.rand(batch_size, num_heads, seq_len_q, depth) key = torch.rand(batch_size, num_heads, seq_len_k, depth) value = torch.rand(batch_size, num_heads, seq_len_k, depth_v) mask = torch.randint(0, 2, (batch_size, num_heads, seq_len_q, seq_len_k)) # Scaled dot-product attention output = scaled_dot_product_attention(query, key, value, mask) print(output.shape) # Expected shape: (batch_size, num_heads, seq_len_q, depth_v) ``` # Notes - The scaling factor ( sqrt{depth} ) should be applied. - If `mask` is provided, it should be added to the attention scores before applying the softmax function. - Adding a very large negative value (e.g., `-1e9`) to masked positions before applying softmax can help in ignoring these positions.","solution":"import torch def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor: Calculate the scaled dot-product attention. Args: query: Query tensor of shape (batch_size, num_heads, seq_len_q, depth). key: Key tensor of shape (batch_size, num_heads, seq_len_k, depth). value: Value tensor of shape (batch_size, num_heads, seq_len_v, depth_v). mask: (Optional) Mask tensor of shape (batch_size, num_heads, seq_len_q, seq_len_k). Returns: output: Tensor of shape (batch_size, num_heads, seq_len_q, depth_v). depth = query.shape[-1] scaling_factor = torch.sqrt(torch.tensor(depth, dtype=torch.float32)) # Compute the dot product between query and key attention_scores = torch.matmul(query, key.transpose(-2, -1)) / scaling_factor # Apply mask (if any) if mask is not None: attention_scores += (mask * -1e9) # Apply softmax to get the attention weights attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1) # Compute the output as weighted sum of values output = torch.matmul(attention_weights, value) return output"},{"question":"# Importing and Using a Module from a ZIP Archive # Objective Design a function `import_module_from_zip(zip_path: str, module_name: str) -> str` that imports a specified module from a given ZIP archive and returns a specific attribute value of the imported module. # Details 1. The function must: - Accept a string `zip_path` which is the path to the ZIP archive. - Accept a string `module_name` which is the fully qualified name of the module to be imported. 2. The function must: - Add the `zip_path` to the beginning of the `sys.path`. - Use the `zipimport` module to import the specified module. - Return the value of a specific attribute `__file__` of the imported module. # Constraints - Ensure that `zip_path` is a valid path to a ZIP archive. - The module specified by `module_name` should be present in the ZIP archive. # Example Scenario Suppose the ZIP file \\"modules.zip\\" contains a module named \\"example_module.py\\". ```python import zipfile # Creating a sample zip file for demonstration with zipfile.ZipFile(\'modules.zip\', \'w\') as zf: zf.writestr(\'example_module.py\', \\"my_variable = \'Hello, World!\'n\\") zf.writestr(\'another_module.py\', \\"def greet(): return \'Greetings\'n\\") ``` After creating the function, you should be able to do the following: ```python zip_path = \'modules.zip\' module_name = \'example_module\' output = import_module_from_zip(zip_path, module_name) print(output) # Expected output should be \'modules.zip/example_module.py\' ``` # Implementation Implement the function `import_module_from_zip(zip_path: str, module_name: str) -> str`.","solution":"import sys import zipimport def import_module_from_zip(zip_path: str, module_name: str) -> str: Imports a module from a ZIP archive and returns the path of the module. :param zip_path: Path to the ZIP archive. :param module_name: Fully qualified name of the module to be imported. :return: The value of the __file__ attribute of the imported module. # Add the zip_path to the beginning of sys.path sys.path.insert(0, zip_path) # Create a zip importer importer = zipimport.zipimporter(zip_path) # Import the module module = importer.load_module(module_name) # Return the value of the __file__ attribute of the imported module return module.__file__"},{"question":"# Advanced Asynchronous Programming with asyncio Objective Implement a function that processes a list of CPU-bound tasks efficiently using asyncio. Your function should split the tasks into batches and execute them concurrently in different threads, ensuring that the asyncio event loop remains unblocked. Requirements - **Function Name**: `process_tasks_concurrently` - **Input**: - `tasks`: A list of CPU-bound functions that accept no arguments and return a value after computation. - `batch_size`: An integer representing the number of tasks to process concurrently in a batch. - Each batch of tasks should run in separate threads without blocking the asyncio event loop. - **Output**: - Return a list of results obtained from executing all tasks. Constraints - You must use `asyncio` for managing the event loop. - Use `ThreadPoolExecutor` from `concurrent.futures` to run CPU-bound tasks concurrently. - Ensure that all exceptions are logged and handled appropriately. - Implement efficient batching and ensure that event loop blocking is minimized. Example ```python import asyncio from concurrent.futures import ThreadPoolExecutor import logging async def process_tasks_concurrently(tasks, batch_size): Process tasks concurrently in batches using asyncio and ThreadPoolExecutor. :param tasks: List of CPU-bound task functions. :param batch_size: Number of tasks to process concurrently in a batch. :return: List of results from the tasks. results = [] loop = asyncio.get_event_loop() executor = ThreadPoolExecutor() async def _run_batch(batch): return await asyncio.gather( *[loop.run_in_executor(executor, task) for task in batch] ) for i in range(0, len(tasks), batch_size): batch = tasks[i:i + batch_size] batch_results = await _run_batch(batch) results.extend(batch_results) executor.shutdown(wait=True) return results # Example usage if __name__ == \\"__main__\\": logging.basicConfig(level=logging.DEBUG) def cpu_intensive_task(): # Example CPU-bound task import time time.sleep(1) return \\"Done\\" tasks = [cpu_intensive_task] * 10 # 10 tasks result = asyncio.run(process_tasks_concurrently(tasks, batch_size=3)) print(result) ``` Notes - Ensure you handle `RuntimeWarning` for coroutines that are never awaited or exceptions that are never retrieved. - Use debug mode and logging to identify any potential issues during development.","solution":"import asyncio from concurrent.futures import ThreadPoolExecutor import logging async def process_tasks_concurrently(tasks, batch_size): Process tasks concurrently in batches using asyncio and ThreadPoolExecutor. :param tasks: List of CPU-bound task functions. :param batch_size: Number of tasks to process concurrently in a batch. :return: List of results from the tasks. results = [] loop = asyncio.get_event_loop() executor = ThreadPoolExecutor() async def _run_batch(batch): return await asyncio.gather( *[loop.run_in_executor(executor, task) for task in batch] ) for i in range(0, len(tasks), batch_size): batch = tasks[i:i + batch_size] batch_results = await _run_batch(batch) results.extend(batch_results) executor.shutdown(wait=True) return results"},{"question":"# Custom Copy Operations for Complex Objects Background You are tasked with implementing a custom class that can handle both shallow and deep copies efficiently. You need to demonstrate the nuances of copying mutable objects within this class. The custom class will manage a collection of items that may themselves be mutable, necessitating careful handling of copy operations. Objectives - Implement a class `ComplexObject` with the ability to perform both shallow and deep copies. - Use the `copy` module functions (`copy.copy()` and `copy.deepcopy()`) within your class\'s special methods (`__copy__` and `__deepcopy__`). Specifications 1. **Class Definition**: Create a class `ComplexObject` that contains: - An attribute `items`, a list of items where each item can itself be a mutable object (e.g., dictionaries or objects of another custom class). 2. **Special Methods**: - Implement the `__copy__` method for shallow copying. - Implement the `__deepcopy__` method for deep copying. 3. **Copy Behavior Testing**: - Validate the shallow copy behavior, ensuring that modifications to the original or the shallow copy do not reflect in their counterpart. - Validate the deep copy behavior, ensuring that all nested objects are also copied, maintaining independence from the original. Input-Output Format - **Input**: No direct input. The class methods handle the intrinsic data of the `ComplexObject`. - **Output**: The behaviors of the shallow and deep copies should be asserted through unit tests. Constraints - The `items` list can contain dictionaries with arbitrary nested structures. - Custom objects contained in the `items` list should also support deep copy to demonstrate comprehensive copying processes. Example ```python import copy class ComplexObject: def __init__(self, items): self.items = items def __copy__(self): new_instance = ComplexObject(self.items.copy()) return new_instance def __deepcopy__(self, memo): new_instance = ComplexObject(copy.deepcopy(self.items, memo)) return new_instance # Usage Example original = ComplexObject([{\'key1\': \'value1\', \'key2\': [1, 2, 3]}, {\'key3\': \'value3\'}]) shallow_copied = copy.copy(original) deep_copied = copy.deepcopy(original) # Test shallow copy behavior original.items[0][\'key1\'] = \'new_value\' assert shallow_copied.items[0][\'key1\'] == \'new_value\' assert deep_copied.items[0][\'key1\'] == \'value1\' # Test deep copy behavior original.items[1][\'key3\'] = \'another_value\' assert shallow_copied.items[1][\'key3\'] == \'another_value\' assert deep_copied.items[1][\'key3\'] == \'value3\' ``` Notes: - The primary focus is on understanding the difference between shallow and deep copies and accurately implementing these for complex objects with mutable contents. - Proper usage of the `memo` dictionary in `__deepcopy__` method should be demonstrated to handle recursive object structures correctly.","solution":"import copy class ComplexObject: def __init__(self, items): self.items = items def __copy__(self): # Create a shallow copy of the items list new_instance = ComplexObject(self.items.copy()) return new_instance def __deepcopy__(self, memo): # Create a deep copy of the items list new_instance = ComplexObject(copy.deepcopy(self.items, memo)) return new_instance # Example usage of defined class original = ComplexObject([{\'key1\': \'value1\', \'key2\': [1, 2, 3]}, {\'key3\': \'value3\'}]) shallow_copied = copy.copy(original) deep_copied = copy.deepcopy(original)"},{"question":"**Question: Custom Iterator Implementation and Management** You are tasked with implementing a Python class to customize iteration behavior. You need to demonstrate fundamental and advanced understanding of iterators by implementing a custom iterator class and managing its interactions. This class should follow the standard iterator protocol and include additional custom logic to handle specific iteration control. # Requirements 1. **CustomIterator Class**: - Implement a class `CustomIterator` that: - Takes a list of integers during initialization. - Initializes an internal index to manage the current position in the list. - Implements `__iter__()` and `__next__()` methods to adhere to the iterator protocol. 2. **Constraints**: - In `__next__()`, if the current integer is divisible by 5, skip that number and move to the next. - If the internal index exceeds the length of the list, raise `StopIteration`. 3. **Python Function to Utilize the Custom Iterator**: - Implement a function `iterate_and_collect(iterator)`, which takes an instance of `CustomIterator` and returns a list of all valid integers produced by the iterator. # Input and Output: - **Input**: A list of integers. - **Output**: A list of integers after iteration, excluding those divisible by 5. # Example: ```python class CustomIterator: # Your implementation here def iterate_and_collect(iterator: CustomIterator) -> list: # Your implementation here if __name__ == \\"__main__\\": # Example Usage input_list = [1, 2, 5, 10, 15, 20, 25, 3, 4, 6] custom_iter = CustomIterator(input_list) result = iterate_and_collect(custom_iter) print(result) # Output should be [1, 2, 3, 4, 6] ``` Hints: - Ensure to implement the `__iter__()` and `__next__()` methods in accordance with Python’s iterator protocol. - Use the `next()` function or a loop to retrieve elements from the iterator in `iterate_and_collect()`. This task will test your understanding of custom iteration, handling iteration logic, and correctly implementing Python protocols.","solution":"class CustomIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): while self.index < len(self.data): value = self.data[self.index] self.index += 1 if value % 5 == 0: continue return value raise StopIteration def iterate_and_collect(iterator): Collects all valid integers from the iterator. return [item for item in iterator]"},{"question":"You are required to implement a mini-application for managing a simple inventory system using the `shelve` module in Python. The application should persist the inventory to disk so that updates to the inventory are retained between program runs. # Functional Requirements 1. **Create a Shelf**: - Open or create a shelf file named `\'inventory.db\'` using `shelve.open`. - Use the context manager support provided by the `shelve` module to ensure the shelf is properly closed. 2. **Add Item to Inventory**: - Implement a function `add_item(item_name: str, quantity: int) -> None` that adds a specified quantity of an item to the inventory. If the item already exists, it should update the quantity. - Example: If `add_item(\'apple\', 10)` is called and `\'apple\'` already exists with a quantity of 5, the new quantity should be 15. 3. **Remove Item from Inventory**: - Implement a function `remove_item(item_name: str, quantity: int) -> None` that removes a specified quantity of an item from the inventory. If the quantity becomes zero or negative, the item should be removed completely. - Example: If `remove_item(\'apple\', 5)` is called and `\'apple\'` has a quantity of 10, the new quantity should be 5. 4. **Check Item Quantity**: - Implement a function `get_item_quantity(item_name: str) -> int` that returns the quantity of a specified item. If the item does not exist, return 0. 5. **List All Items**: - Implement a function `list_all_items() -> Dict[str, int]` that returns a dictionary of all items in the inventory with their respective quantities. # Constraints - Key names (item names) are case sensitive. - Quantity values are non-negative integers. - Your implementation should handle potential exceptions, such as IOError during shelf operations gracefully. # Example Usage ```python # Assuming the implementation is correct, following is an example usage: add_item(\'apple\', 10) add_item(\'banana\', 5) print(get_item_quantity(\'apple\')) # Outputs: 10 remove_item(\'apple\', 3) print(get_item_quantity(\'apple\')) # Outputs: 7 list_all_items() # Outputs: {\'apple\': 7, \'banana\': 5} remove_item(\'apple\', 7) print(get_item_quantity(\'apple\')) # Outputs: 0 ``` # Submission Requirements Submit a Python script that includes: - The `add_item` function implementation. - The `remove_item` function implementation. - The `get_item_quantity` function implementation. - The `list_all_items` function implementation. - A brief main section demonstrating the usage of these functions.","solution":"import shelve from typing import Dict def add_item(item_name: str, quantity: int) -> None: with shelve.open(\'inventory.db\', writeback=True) as inventory: if item_name in inventory: inventory[item_name] += quantity else: inventory[item_name] = quantity def remove_item(item_name: str, quantity: int) -> None: with shelve.open(\'inventory.db\', writeback=True) as inventory: if item_name in inventory: if inventory[item_name] > quantity: inventory[item_name] -= quantity else: del inventory[item_name] def get_item_quantity(item_name: str) -> int: with shelve.open(\'inventory.db\') as inventory: return inventory.get(item_name, 0) def list_all_items() -> Dict[str, int]: with shelve.open(\'inventory.db\') as inventory: return dict(inventory) # Example usage: if __name__ == \\"__main__\\": add_item(\'apple\', 10) add_item(\'banana\', 5) print(get_item_quantity(\'apple\')) # Outputs: 10 remove_item(\'apple\', 3) print(get_item_quantity(\'apple\')) # Outputs: 7 print(list_all_items()) # Outputs: {\'apple\': 7, \'banana\': 5} remove_item(\'apple\', 7) print(get_item_quantity(\'apple\')) # Outputs: 0"},{"question":"**Question:** You are provided with a dataset representing the heights and weights of individuals. Your task is to perform a statistical analysis and visualize the data using the Seaborn library to demonstrate your understanding of Seaborn\'s capabilities in adding error bars to plots. **Instructions:** 1. **DataFrame Creation:** Create a Pandas DataFrame `df` with the following two columns: - \'height\': Randomly generated heights (in cm) of 200 individuals, normally distributed with a mean of 170 and standard deviation of 10. - \'weight\': Randomly generated weights (in kg) of 200 individuals, normally distributed with a mean of 70 and standard deviation of 15. 2. **Visualization with Error Bars:** 1. Create a visualization showing the mean height and its standard deviation using Seaborn\'s point plot. 2. Create a visualization showing the mean weight and its standard error using Seaborn\'s point plot. 3. Create a visualization showing the heights with a 95% percentile interval using Seaborn. 4. Create a combined visualization that displays both the original data (using a strip plot) and the mean height along with a 95% confidence interval. 3. **Custom Error Bars:** Define a custom error bar function that shows the minimum and maximum range for the weights and use it in a Seaborn plot. 4. **Regression Analysis:** Perform linear regression to analyze the relationship between height and weight. Visualize the data points along with the regression line and a 95% confidence interval. **Requirements:** - Use Seaborn functions and specify the appropriate `errorbar` parameters. - Ensure that your plots are clearly labeled and easy to interpret. - Provide a brief explanation of the choices you made for each visualization type. **Code Template:** ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Set the theme for Seaborn sns.set_theme(style=\\"darkgrid\\") # Create the DataFrame np.random.seed(42) # For reproducible results df = pd.DataFrame({ \'height\': np.random.normal(170, 10, 200), \'weight\': np.random.normal(70, 15, 200) }) # Visualization 1: Mean height with standard deviation plt.figure(figsize=(10,6)) sns.pointplot(x=\\"height\\", y=\'height\', data=df, errorbar=(\\"sd\\")) plt.title(\\"Mean Height with Standard Deviation\\") plt.show() # Visualization 2: Mean weight with standard error plt.figure(figsize=(10,6)) sns.pointplot(x=\\"height\\", y=\'weight\', data=df, errorbar=(\\"se\\")) plt.title(\\"Mean Weight with Standard Error\\") plt.show() # Visualization 3: Heights with 95% percentile interval plt.figure(figsize=(10,6)) sns.pointplot(x=\\"height\\", y=\'height\', data=df, errorbar=(\\"pi\\", 95)) plt.title(\\"Heights with 95% Percentile Interval\\") plt.show() # Visualization 4: Original data with mean height and 95% confidence interval plt.figure(figsize=(10,6)) sns.stripplot(x=\'height\', y=\'height\', data=df, jitter=True) sns.pointplot(x=\'height\', y=\'height\', data=df, errorbar=(\\"ci\\"), capsize=.2) plt.title(\\"Heights with 95% Confidence Interval\\") plt.show() # Custom Error Bars Function def custom_errorbars(x): return (x.min(), x.max()) # Visualization 5: Custom error bars for weight plt.figure(figsize=(10,6)) sns.pointplot(x=\\"height\\", y=\'weight\', data=df, errorbar=custom_errorbars) plt.title(\\"Custom Error Bars Showing Min and Max Range for Weights\\") plt.show() # Regression Analysis with 95% confidence interval plt.figure(figsize=(10,6)) sns.regplot(x=\'height\', y=\'weight\', data=df, ci=95) plt.title(\\"Regression Analysis of Height and Weight\\") plt.show() ``` **Expected Outcomes:** - A series of plots displaying the requested error bars in different contexts. - Application of both built-in error bar options and a custom error bar function. - Clear representation of the relationship between height and weight with regression analysis and error indicator. **Additional Notes:** - Ensure to comment on the code for better readability. - Make the plots visually distinct and easy to comprehend.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Set the theme for Seaborn sns.set_theme(style=\\"darkgrid\\") # Create the DataFrame np.random.seed(42) # For reproducible results df = pd.DataFrame({ \'height\': np.random.normal(170, 10, 200), \'weight\': np.random.normal(70, 15, 200) }) # Visualization 1: Mean height with standard deviation plt.figure(figsize=(10,6)) sns.pointplot(x=[1]*len(df[\'height\']), y=\'height\', data=df, errorbar=\\"sd\\") plt.title(\\"Mean Height with Standard Deviation\\") plt.xlabel(\\"Uniform group\\") plt.ylabel(\\"Height in cm\\") plt.show() # Visualization 2: Mean weight with standard error plt.figure(figsize=(10,6)) sns.pointplot(x=[1]*len(df[\'weight\']), y=\'weight\', data=df, errorbar=\\"se\\") plt.title(\\"Mean Weight with Standard Error\\") plt.xlabel(\\"Uniform group\\") plt.ylabel(\\"Weight in kg\\") plt.show() # Visualization 3: Heights with 95% percentile interval plt.figure(figsize=(10,6)) sns.pointplot(x=[1]*len(df[\'height\']), y=\'height\', data=df, errorbar=(\\"pi\\", 95)) plt.title(\\"Heights with 95% Percentile Interval\\") plt.xlabel(\\"Uniform group\\") plt.ylabel(\\"Height in cm\\") plt.show() # Visualization 4: Original data with mean height and 95% confidence interval plt.figure(figsize=(10,6)) sns.stripplot(x=[1]*len(df[\'height\']), y=\'height\', data=df, jitter=True) sns.pointplot(x=[1]*len(df[\'height\']), y=\'height\', data=df, errorbar=\\"ci\\", capsize=.2) plt.title(\\"Heights with 95% Confidence Interval\\") plt.xlabel(\\"Uniform group\\") plt.ylabel(\\"Height in cm\\") plt.show() # Custom Error Bars Function def custom_errorbars(data): return np.array([data.min(), data.max()]) # Visualization 5: Custom error bars for weight plt.figure(figsize=(10,6)) sns.pointplot(x=[1]*len(df[\'weight\']), y=\'weight\', data=df, errorbar=custom_errorbars) plt.title(\\"Custom Error Bars Showing Min and Max Range for Weights\\") plt.xlabel(\\"Uniform group\\") plt.ylabel(\\"Weight in kg\\") plt.show() # Regression Analysis with 95% confidence interval plt.figure(figsize=(10,6)) sns.regplot(x=\'height\', y=\'weight\', data=df, ci=95) plt.title(\\"Regression Analysis of Height and Weight\\") plt.xlabel(\\"Height in cm\\") plt.ylabel(\\"Weight in kg\\") plt.show()"},{"question":"**Coding Assessment Question** **Objective:** Implement a simple form-based text user interface using the `curses` module. **Description:** You are required to create a text-based form UI where the user can input their name and age, navigate through the fields using the arrow keys and submit the form using a specific key combination. **Task:** 1. Initialize the `curses` module. 2. Create a form with two fields: - Name (maximum 20 characters) - Age (maximum 3 digits) 3. Provide navigation between fields: - Use the UP and DOWN arrow keys to navigate between fields. 4. Submit the form: - Use the ENTER key to submit the form when on the Age field. - Display the entered data on screen. **Guidelines:** 1. You must use the `curses` library for handling terminal display. 2. Handle any exceptions raised by the `curses` library gracefully. 3. Ensure that the form fields are editable and navigable using the specified keys. 4. Clear instructions should be displayed to the user on how to navigate and submit the form. **Expected Input and Output:** - Input: - Field 1: Name (Alphabets, max 20 characters) - Field 2: Age (Numeric, max 3 digits) - Output: - Display the entered Name and Age on the screen upon submission. **Constraints:** - The Name field should accept only alphabets. - The Age field should accept only numeric values. - The maximum input length for the Name is 20 characters and for the Age is 3 digits. **Performance Requirements:** - The UI should be responsive to key presses. - Avoid screen flickering while navigating or entering data. **Sample Code Structure:** ```python import curses def main(stdscr): # Clear screen stdscr.clear() # Initialize curses curses.cbreak() curses.noecho() stdscr.keypad(True) # Create form try: # Implement the fields and navigation logic here # Draw the form: # Use stdscr.addstr() to display text instructions for the user # Use stdscr.getch() to handle keypress events for navigation and input except curses.error: pass # Handle curses errors if any finally: # Cleanup and return terminal to normal settings curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() # Start the wrapper curses.wrapper(main) ``` You are required to fill in the implementation details inside the `try` block following the specified guidelines.","solution":"import curses def main(stdscr): # Clear screen stdscr.clear() # Disable cursor curses.curs_set(0) # Initialize curses curses.cbreak() curses.noecho() stdscr.keypad(True) # Form fields name = \'\' age = \'\' current_field = 0 try: # Instructions stdscr.addstr(0, 0, \\"Form - Navigate with UP and DOWN arrow keys. Press ENTER to submit.\\") stdscr.addstr(1, 0, \\"Name: \\") stdscr.addstr(2, 0, \\"Age: \\") while True: stdscr.move(1 + current_field, 6) stdscr.clrtoeol() if current_field == 0: stdscr.addstr(1, 6, name) else: stdscr.addstr(2, 6, age) stdscr.refresh() key = stdscr.getch() if key == curses.KEY_UP: current_field = (current_field - 1) % 2 elif key == curses.KEY_DOWN: current_field = (current_field + 1) % 2 elif key == curses.KEY_ENTER or key in [10, 13]: if current_field == 1: if len(age) > 0: break elif key == curses.KEY_BACKSPACE or key == 127: if current_field == 0: name = name[:-1] else: age = age[:-1] else: if current_field == 0 and len(name) < 20 and chr(key).isalpha(): name += chr(key) elif current_field == 1 and len(age) < 3 and chr(key).isdigit(): age += chr(key) # Display entered data stdscr.clear() stdscr.addstr(0, 0, f\\"Name: {name}\\") stdscr.addstr(1, 0, f\\"Age: {age}\\") stdscr.refresh() stdscr.getch() except curses.error: pass # Handle curses errors if any finally: # Cleanup and return terminal to normal settings curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() # Start the wrapper if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# Complex Multi-Part Function Type Hinting: Inventory Management System **Objective:** Design and implement a type-annotated inventory management system that leverages advanced type hinting features from the `typing` module. Your solution should include custom generics, protocols, and precise type hints for function signatures and class definitions. **Requirements:** 1. Define a `TypedDict` called `Product` for representing product details with the following keys: - `id`: a unique identifier of type `int`. - `name`: the product name of type `str`. - `quantity`: the number of units available of type `int`. - `price`: the unit price as a `float`. 2. Implement a generic class `Inventory[T]` where `T` is constrained to `Product`. The class should have the following methods: - `add_product(product: T) -> None`: Adds a product to the inventory. - `remove_product(product_id: int) -> bool`: Removes a product by its ID, returning `True` if successful and `False` otherwise. - `get_total_value() -> float`: Returns the total value of the inventory (sum of quantity × price for all products). 3. Define a `Protocol` called `DiscountStrategy` with a method: - `apply_discount(total: float) -> float`: Applies a discount to the total value and returns the discounted total. 4. Create a `NewType` called `ProductID` based on `int` for representing product identifiers. 5. Implement a function `calculate_discounted_inventory_value` that: - Accepts an `Inventory[Product]` instance. - Accepts a `DiscountStrategy` instance. - Returns the total discounted value as a `float`. Your type hints should accurately reflect the expected input and output types. Ensure that the code is well-documented and leverages advanced type hinting features to guarantee type safety. **Constraints:** - Each product\'s `id` must be unique within the inventory. - Use the provided `typing` module functionalities to ensure type safety. - Implement appropriate error handling and type validations where necessary. - Aim for clarity and maintainability in your code, leveraging type hinting to articulate your design. **Performance Requirements:** - The methods should handle typical usage scenarios efficiently, but optimal performance for extremely large inventories is not a primary concern. Focus instead on correctness and type safety. **Example Usage:** ```python # Define the product structure using TypedDict class Product(TypedDict): id: int name: str quantity: int price: float # Create a NewType for product identifiers ProductID = NewType(\'ProductID\', int) # Protocol for discount strategies class DiscountStrategy(Protocol): def apply_discount(self, total: float) -> float: ... # Generic Inventory class class Inventory(Generic[T]): def add_product(self, product: T) -> None: ... def remove_product(self, product_id: ProductID) -> bool: ... def get_total_value(self) -> float: ... # Function to calculate discounted inventory value def calculate_discounted_inventory_value(inventory: Inventory[Product], strategy: DiscountStrategy) -> float: ... # Example implementations of these components... ``` **Test Cases:** 1. Create an inventory, add multiple products, and check the total value. 2. Implement and apply different discount strategies to the inventory. 3. Verify the behavior of adding and removing products using type-safe methods.","solution":"from typing import TypedDict, Generic, TypeVar, List, Protocol, NewType # Define the product structure using TypedDict class Product(TypedDict): id: int name: str quantity: int price: float # Create a NewType for product identifiers ProductID = NewType(\'ProductID\', int) # Protocol for discount strategies class DiscountStrategy(Protocol): def apply_discount(self, total: float) -> float: ... # Create a type variable constrained to Product T = TypeVar(\'T\', bound=Product) # Generic Inventory class class Inventory(Generic[T]): def __init__(self) -> None: self.products: List[T] = [] def add_product(self, product: T) -> None: self.products.append(product) def remove_product(self, product_id: ProductID) -> bool: for index, product in enumerate(self.products): if product[\'id\'] == product_id: del self.products[index] return True return False def get_total_value(self) -> float: return sum(product[\'quantity\'] * product[\'price\'] for product in self.products) # Function to calculate discounted inventory value def calculate_discounted_inventory_value(inventory: Inventory[Product], strategy: DiscountStrategy) -> float: total_value = inventory.get_total_value() return strategy.apply_discount(total_value)"},{"question":"# Decision Trees Assessment Objective: Implement a decision tree classifier and regressor using scikit-learn. Demonstrate your understanding of setting up trees, managing overfitting, handling missing values, and visualizing the tree structure. Tasks: 1. **Classification with DecisionTreeClassifier** - Load the Iris dataset using `sklearn.datasets.load_iris`. - Split the dataset into training and testing sets. - Fit a `DecisionTreeClassifier` on the training data. - Predict the labels on the test data. - Evaluate the accuracy of the classifier using the `accuracy_score` metric from `sklearn.metrics`. - Use the `plot_tree` method to visualize the trained decision tree. 2. **Regression with DecisionTreeRegressor** - Load the California housing dataset using `sklearn.datasets.fetch_california_housing`. - Split the dataset into training and testing sets. - Fit a `DecisionTreeRegressor` on the training data. - Predict the target variable on the test data. - Evaluate the performance using the `mean_squared_error` metric from `sklearn.metrics`. - Use the `plot_tree` method to visualize the trained decision tree. 3. **Overfitting and Pruning** - Train a `DecisionTreeClassifier` on the Iris dataset and vary the `max_depth` parameter. Evaluate and plot the training and validation accuracy for different depths. - Use `cost_complexity_pruning_path` and `DecisionTreeClassifier(ccp_alpha=...)` to prune the tree. Evaluate the pruned tree\'s performance. 4. **Handling Missing Values** - Introduce missing values randomly into the Iris dataset. - Fit a `DecisionTreeClassifier` using the dataset with missing values. - Predict the labels for the test data containing missing values and evaluate accuracy. Requirements: 1. **Input and Output Formats:** - The classifier and regressor should take the datasets (Iris, California housing) as input. - Output should include the accuracy score for classification and mean squared error for regression. - Visualizations should be provided as plots for the tree structures and evaluation metrics (accuracy, MSE) against different tree depths. 2. **Constraints:** - Use `train_test_split` from `sklearn.model_selection` to split datasets. - Handle missing values using the functionality described in the provided documentation. - Use appropriate metrics for evaluation and ensure visualizations are clear. 3. **Performance Requirements:** - Ensure the models are not overfitted by properly tuning `max_depth`, `min_samples_split`, `min_samples_leaf`. - The model training and prediction should be efficient and handle the datasets within reasonable time limits. Code Template: ```python import numpy as np from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree from sklearn.datasets import load_iris, fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, mean_squared_error import matplotlib.pyplot as plt # Task 1: Classification with DecisionTreeClassifier def classification_decision_tree(): # Load the iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Predict on the test set y_pred = clf.predict(X_test) # Evaluate the classifier accuracy = accuracy_score(y_test, y_pred) print(\\"Classification Accuracy:\\", accuracy) # Visualize the decision tree plt.figure() plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Task 2: Regression with DecisionTreeRegressor def regression_decision_tree(): # Load the California housing dataset housing = fetch_california_housing() X, y = housing.data, housing.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the regressor reg = DecisionTreeRegressor(random_state=42) reg.fit(X_train, y_train) # Predict on the test set y_pred = reg.predict(X_test) # Evaluate the regressor mse = mean_squared_error(y_test, y_pred) print(\\"Regression Mean Squared Error:\\", mse) # Visualize the decision tree plt.figure() plot_tree(reg, filled=True, feature_names=housing.feature_names) plt.show() # Task 3: Overfitting and Pruning def overfitting_and_pruning(): iris = load_iris() X, y = iris.data, iris.target X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) train_scores, val_scores = [], [] depths = range(1, 21) for depth in depths: clf = DecisionTreeClassifier(max_depth=depth, random_state=42) clf.fit(X_train, y_train) train_scores.append(accuracy_score(y_train, clf.predict(X_train))) val_scores.append(accuracy_score(y_val, clf.predict(X_val))) plt.plot(depths, train_scores, label=\'Training Accuracy\') plt.plot(depths, val_scores, label=\'Validation Accuracy\') plt.xlabel(\'max_depth\') plt.ylabel(\'Accuracy\') plt.legend() plt.show() path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas, impurities = path.ccp_alphas, path.impurities clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf.fit(X_train, y_train) clfs.append(clf) clfs = clfs[:-1] ccp_alphas = ccp_alphas[:-1] train_scores = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs] val_scores = [accuracy_score(y_val, clf.predict(X_val)) for clf in clfs] plt.plot(ccp_alphas, train_scores, label=\'Training Accuracy\') plt.plot(ccp_alphas, val_scores, label=\'Validation Accuracy\') plt.xlabel(\'ccp_alpha\') plt.ylabel(\'Accuracy\') plt.legend() plt.show() # Task 4: Handling Missing Values def handle_missing_values(): np.random.seed(42) iris = load_iris() X, y = iris.data, iris.target # Introduce missing values missing_mask = np.random.rand(*X.shape) < 0.1 X[missing_mask] = np.nan X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Predicting and scoring y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Accuracy with missing values:\\", accuracy) classification_decision_tree() regression_decision_tree() overfitting_and_pruning() handle_missing_values() ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree from sklearn.datasets import load_iris, fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, mean_squared_error # Task 1: Classification with DecisionTreeClassifier def classification_decision_tree(): # Load the iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Predict on the test set y_pred = clf.predict(X_test) # Evaluate the classifier accuracy = accuracy_score(y_test, y_pred) print(\\"Classification Accuracy:\\", accuracy) # Returning for unit test return clf, accuracy, X_train, X_test, y_train, y_test # Task 2: Regression with DecisionTreeRegressor def regression_decision_tree(): # Load the California housing dataset housing = fetch_california_housing() X, y = housing.data, housing.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the regressor reg = DecisionTreeRegressor(random_state=42) reg.fit(X_train, y_train) # Predict on the test set y_pred = reg.predict(X_test) # Evaluate the regressor mse = mean_squared_error(y_test, y_pred) print(\\"Regression Mean Squared Error:\\", mse) # Returning for unit test return reg, mse, X_train, X_test, y_train, y_test # Task 3: Overfitting and Pruning def overfitting_and_pruning(): iris = load_iris() X, y = iris.data, iris.target X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) train_scores, val_scores = [], [] depths = range(1, 21) for depth in depths: clf = DecisionTreeClassifier(max_depth=depth, random_state=42) clf.fit(X_train, y_train) train_scores.append(accuracy_score(y_train, clf.predict(X_train))) val_scores.append(accuracy_score(y_val, clf.predict(X_val))) plt.plot(depths, train_scores, label=\'Training Accuracy\') plt.plot(depths, val_scores, label=\'Validation Accuracy\') plt.xlabel(\'max_depth\') plt.ylabel(\'Accuracy\') plt.legend() plt.show() path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas, impurities = path.ccp_alphas, path.impurities clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf.fit(X_train, y_train) clfs.append(clf) clfs = clfs[:-1] ccp_alphas = ccp_alphas[:-1] train_scores = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs] val_scores = [accuracy_score(y_val, clf.predict(X_val)) for clf in clfs] plt.plot(ccp_alphas, train_scores, label=\'Training Accuracy\') plt.plot(ccp_alphas, val_scores, label=\'Validation Accuracy\') plt.xlabel(\'ccp_alpha\') plt.ylabel(\'Accuracy\') plt.legend() plt.show() # Returning for unit test return depths, train_scores, val_scores, ccp_alphas, train_scores, val_scores # Task 4: Handling Missing Values def handle_missing_values(): np.random.seed(42) iris = load_iris() X, y = iris.data, iris.target # Introduce missing values missing_mask = np.random.rand(*X.shape) < 0.1 X[missing_mask] = np.nan X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Predicting and scoring y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Accuracy with missing values:\\", accuracy) # Returning for unit test return clf, accuracy, X_train, X_test, y_train, y_test classification_decision_tree() regression_decision_tree() overfitting_and_pruning() handle_missing_values()"},{"question":"<|Analysis Begin|> The `email.message.Message` class from the Python standard library is designed to represent an email message using the \\"compat32\\" policy, primarily for backward compatibility. This class provides various functionalities to manipulate email headers and payloads. Key aspects of the `Message` class include: - Email messages comprise headers and payloads, where headers adhere to RFC 5322 standards. - The class acts as an ordered dictionary for headers and provides methods to access and manipulate payloads, serialize messages, and walk through message trees. - Payloads can be text, binary, or a list of sub-messages (for MIME container documents). - Methods include `as_string`, `as_bytes`, `is_multipart`, and specialized methods to handle headers and payloads. - Legacy methods (`attach`, `set_payload`, etc.) exist for backward compatibility but are replaced by more modern alternatives in the `EmailMessage` class. Given this detailed functionality, a coding assessment question can focus on implementing a utility that processes email messages, utilizing the various methods provided by the `Message` class. <|Analysis End|> <|Question Begin|> # Email Message Processor You are required to implement a function that processes email messages. The function should perform the following tasks: 1. Flatten the email message to a string format. 2. Extract and print all headers in the email, maintaining their order. 3. Check if the email is a multipart message. If it is, iterate through its subparts and print their MIME types. 4. Extract the payload of the message and print it if it\'s not multipart. If it is multipart, print \\"Multipart message: [number of subparts] parts\\". # Function Signature ```python from email.message import Message from typing import List def process_email_message(email: Message) -> None: pass ``` # Input - `email` (Message): an instance of the `Message` class representing the email. # Output The function should print: 1. The flattened email message as a string. 2. All the headers and their values in order. 3. The MIME type of each subpart if the message is multipart. 4. The payload of the message or a summary if it is multipart. # Example ```python from email.message import Message # Sample email message email = Message() email[\'From\'] = \'sender@example.com\' email[\'To\'] = \'recipient@example.com\' email[\'Subject\'] = \'Test Email\' email.set_payload(\'This is a test email.\') process_email_message(email) ``` Expected Output ``` Flattened Email Message: <flattened email string> Headers: From: sender@example.com To: recipient@example.com Subject: Test Email Payload: This is a test email. ``` In the case of a multipart message, the output might look like: ``` Flattened Email Message: <flattened email string> Headers: From: sender@example.com To: recipient@example.com Subject: Test Email Multipart message: 2 parts Part 1 MIME type: text/plain Part 2 MIME type: application/pdf ``` # Constraints - Assume that the email message will always be well-formed according to RFC 5322. - The function should handle email messages with simple text, binary data, as well as multipart messages. # Additional Notes - Use the `is_multipart()`, `get_payload()`, `get_content_type()`, and `walk()` methods as needed to implement the function. - Ensure the function is robust and can handle different types of payloads gracefully.","solution":"from email.message import Message def process_email_message(email: Message) -> None: # Flatten the email message to a string format email_flattened = email.as_string() print(f\\"Flattened Email Message: {email_flattened}\\") # Extract and print all headers in the email, maintaining their order print(\\"Headers:\\") for header, value in email.items(): print(f\\"{header}: {value}\\") # Check if the email is a multipart message if email.is_multipart(): parts = email.get_payload() print(f\\"Multipart message: {len(parts)} parts\\") for i, part in enumerate(parts, 1): print(f\\"Part {i} MIME type: {part.get_content_type()}\\") else: # Extract the payload of the message and print it payload = email.get_payload() print(f\\"Payload: {payload}\\")"},{"question":"**Problem Statement**: Given a dataset, you are required to implement a function utilizing seaborn\'s new `objects` interface to create a complex plot. The dataset contains information about a collection of products, including their category, price, rating, and number of reviews. Follow the steps below to create a multifaceted, multi-layered plot: 1. **Data Importation**: Load the dataset `products.csv` which contains the columns: `category`, `price`, `rating`, and `num_reviews`. 2. **Facet the plot by `category`:** Create separate facets for each product category. 3. **Add Multiple Layers**: - **Layer 1**: A scatter plot of `price` vs. `rating` where: - The color of the points represents the `num_reviews`. - The size of the points is proportional to the `num_reviews`. - **Layer 2**: A linear fit line to represent the trend of `price` vs. `rating`. 4. **Customization**: - Customize the color scale for `num_reviews` using the \\"viridis\\" color palette. - Set the size scale for `num_reviews` from 2 to 10 units. - Customize the x-axis and y-axis labels to \\"Price ()\\" and \\"Rating\\" respectively. - Apply a theme with a white grid background and dotted grid lines. **Constraints**: - Ensure the plot is rendered correctly within a Jupyter Notebook environment. - The plot should be saved as a PNG file named `products_visualization.png`. **Function Signature**: ```python def create_products_visualization(file_path: str) -> None: pass ``` **Input**: - `file_path` (str): Path to the `products.csv` file. **Output**: - The function should render the plot and save it as `products_visualization.png`. **Example**: Assume the dataset `products.csv` looks like this: ``` category,price,rating,num_reviews Electronics,199.99,4.6,234 Books,15.99,4.2,120 Beauty,45.00,4.8,87 Electronics,99.99,3.9,300 Books,9.99,4.5,85 ... ``` Calling `create_products_visualization(\'products.csv\')` should produce a multifaceted plot with the aforementioned specifications and save it as `products_visualization.png`.","solution":"import seaborn as sns import pandas as pd def create_products_visualization(file_path: str) -> None: # Load the dataset products = pd.read_csv(file_path) # Initialize a FacetGrid by category facets = sns.FacetGrid(data=products, col=\'category\', col_wrap=2, sharex=False, sharey=False) # Add scatter plot layer facets.map_dataframe(sns.scatterplot, x=\'price\', y=\'rating\', hue=\'num_reviews\', size=\'num_reviews\', palette=\'viridis\', sizes=(2, 10), legend=False) # Add linear fit layer facets.map_dataframe(sns.regplot, x=\'price\', y=\'rating\', scatter=False, truncate=False) # Customize labels and theme facets.set_axis_labels(\'Price ()\', \'Rating\') sns.set_theme(style=\'whitegrid\', rc={\\"grid.linestyle\\": \\":\\"}) # Save the plot facets.savefig(\'products_visualization.png\')"},{"question":"**Functional Programming Challenge in Python** **Objective**: Demonstrate your comprehension of the `itertools`, `functools`, and `operator` modules by implementing a function that processes a sequence of elements efficiently. **Problem Statement**: Write a Python function `process_sequence(sequence, n)` that takes in a list of integers `sequence` and an integer `n`. The function should perform the following actions: 1. **Filter** the sequence to include only numbers that are divisible by `n`. 2. **Double** each of the filtered numbers. 3. **Reduce** the resulting numbers using the addition operator to get the sum. To achieve this, you need to: - Use `itertools` to filter the sequence efficiently. - Use `functools.partial` to create a function for doubling the numbers. - Use the `operator` module to perform the addition operation. **Function Signature**: ```python def process_sequence(sequence: List[int], n: int) -> int: ``` **Input**: - `sequence`: A list of integers. Example: `[1, 2, 3, 4, 5, 6]` - `n`: An integer divisor. Example: `2` **Output**: - An integer representing the sum of processed numbers. Example: `20` **Constraints**: - Each element in `sequence` can be any integer. - `n` will be a positive integer greater than 0. - The sequence can have duplicate numbers. - The length of the sequence will be between `1` and `10^5`. **Example**: ```python assert process_sequence([1, 2, 3, 4, 5, 6], 2) == 20 ``` *(Explanation: Only 2, 4, and 6 are divisible by 2. Doubling them results in [4, 8, 12]. The sum is 4 + 8 + 12 = 24.)* **Performance Requirements**: - The function should be efficient with respect to both time and space complexity. Use the `itertools`, `functools`, and `operator` modules to construct your solution.","solution":"import itertools import functools import operator def process_sequence(sequence, n): Process the sequence by following these steps: 1. Filter the sequence to include only numbers that are divisible by n. 2. Double each of the filtered numbers. 3. Reduce the resulting numbers using the addition operator to get the sum. # Step 1: Filter the sequence to include only numbers that are divisible by n filtered_seq = filter(lambda x: x % n == 0, sequence) # Step 2: Double each of the filtered numbers doubled_seq = map(lambda x: x * 2, filtered_seq) # Step 3: Reduce the resulting numbers using the addition operator to get the sum result_sum = functools.reduce(operator.add, doubled_seq, 0) return result_sum"},{"question":"**Coding Question:** # Objective: The goal of this question is to assess your understanding of seaborn\'s advanced plotting capabilities. You will need to manipulate datasets and create multifaceted visualizations using the seaborn library. The datasets to be used are `penguins` and `flights`, which are included in seaborn. You will implement a function that performs these tasks. # Task: Implement a function `create_seaborn_visualization()` that loads the `penguins` and `flights` datasets and generates the following two plots: 1. **Point Plot** for the `penguins` dataset that displays: - x-axis: \\"species\\" - y-axis: \\"flipper_length_mm\\" - hue: \\"sex\\" - Use markers \\"o\\" and \\"s\\" for sexes, and linestyles \\"-\\" and \\"--\\". - Standard deviation as the error bar representation. 2. **Point Plot** for the `flights` dataset that displays: - x-axis: \\"year\\" - y-axis: average number of \\"passengers\\" per year - Color the points by \\"month\\" - Ensure no overplotting by using `dodge`. - Customize the plot to have `capsize` of 0.3, `color` as \\".2\\", and points as \\"D\\" markers. Your function should save these plots as two separate PNG files: `penguins_plot.png` and `flights_plot.png`. # Constraints and Requirements: - Ensure to import any necessary modules (e.g., seaborn, pandas, matplotlib). - Input: None - Output: None # Example: ```python def create_seaborn_visualization(): pass # Your code here # After implementing the function, running the function should generate and save the two plots as specified. create_seaborn_visualization() ``` # Performance Requirements: - The function should execute efficiently and handle the provided datasets without performance issues. This question requires you to understand seaborn\'s plotting functions comprehensively, including data manipulation, customization of plots, and handling categorical variables. Good luck!","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_seaborn_visualization(): # Load the datasets penguins = sns.load_dataset(\'penguins\') flights = sns.load_dataset(\'flights\') # Plot 1: Point Plot for the `penguins` dataset plt.figure(figsize=(10, 6)) sns.pointplot( data=penguins, x=\\"species\\", y=\\"flipper_length_mm\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], ci=\\"sd\\", dodge=True ) plt.title(\'Flipper Length by Species and Sex\') plt.savefig(\'penguins_plot.png\') plt.close() # Plot 2: Point Plot for the `flights` dataset flights_avg = flights.groupby([\'year\', \'month\'], as_index=False)[\'passengers\'].mean() plt.figure(figsize=(10, 6)) sns.pointplot( data=flights_avg, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", dodge=True, capsize=0.3, color=\\".2\\", markers=\\"D\\" ) plt.title(\'Average Number of Passengers per Year by Month\') plt.savefig(\'flights_plot.png\') plt.close()"},{"question":"# Question: You are given a sample PyTorch model and are required to convert it to TorchScript. The model performs a simple feed-forward network operation but has an in-place operation that needs to be handled correctly when using TorchScript tracing. Model Description: The PyTorch model has a simple feed-forward network where the forward pass includes an in-place operation. Here\'s the model implementation: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear1 = nn.Linear(10, 10) self.linear2 = nn.Linear(10, 5) def forward(self, x): x = self.linear1(x) x[0,0] = 0 # In-place operation x = self.linear2(x) return x # Create an instance of the model model = SimpleModel() ``` # Tasks: 1. **Convert the Model Using Tracing**: Use TorchScript tracing to convert the model to TorchScript and handle the in-place operation issue. 2. **Inspect and Print the TorchScript Code and Graph**: Use appropriate methods to inspect and print the generated TorchScript code and graph. # Requirements: - The solution should include the modified model, if necessary. - The TorchScript conversion should handle the in-place operation correctly. - The converted model\'s TorchScript code and graph should be printed and correctly formatted. # Input and Output: - **Input**: None (the model is pre-defined as above) - **Output**: - Modified model code (if any changes are made) - TorchScript code output - TorchScript graph output # Constraints: - You must handle the in-place operation correctly to avoid any warnings or errors during the tracing process. - You should use tracing (`torch.jit.trace`) for the conversion process. # Example Solution: Here\'s an example of how a solution should be structured: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear1 = nn.Linear(10, 10) self.linear2 = nn.Linear(10, 5) def forward(self, x): x = self.linear1(x) # Correct in-place operation issue x = torch.cat((torch.zeros(1), x.view(-1)[1:])).view_as(x) x = self.linear2(x) return x # Create an instance of the model model = SimpleModel() # Convert the model to TorchScript using tracing example_input = torch.rand(2, 10) traced_script_module = torch.jit.trace(model, example_input) # Print the TorchScript code and graph print(traced_script_module.code) print(traced_script_module.graph) ``` # Note: - The example solution is just a guide. Your solution may vary but must meet the requirements and constraints mentioned above.","solution":"import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear1 = nn.Linear(10, 10) self.linear2 = nn.Linear(10, 5) def forward(self, x): x = self.linear1(x) # Correct in-place operation issue x_copy = x.clone() x_copy[0, 0] = 0 x = self.linear2(x_copy) return x # Create an instance of the model model = SimpleModel() # Convert the model to TorchScript using tracing example_input = torch.rand(2, 10) traced_script_module = torch.jit.trace(model, example_input) # Print the TorchScript code and graph print(traced_script_module.code) print(traced_script_module.graph)"},{"question":"**Coding Question:** You are given a dataset containing the monthly average temperatures of two cities over a year. Your task is to visualize the temperature differences between the two cities using a heatmap with a custom diverging color palette. Write a Python function `plot_temperature_diff(data)` that takes a dictionary `data` as input, where each month maps to a tuple containing the average temperatures of the two cities. The function should create and display a heatmap showing the temperature differences between the two cities for each month, using a custom diverging palette. **Requirements:** 1. Use the `sns.diverging_palette` function to create a custom diverging palette. - Use a color range from blue to red. - Set the center color to dark. - Ensure the palette is continuous. - Increase the separation around the center value. - Decrease the saturation and lightness of the endpoints. 2. Use the `ax.annotate` method to display the exact temperature difference values on the heatmap cells. **Input Format:** - `data`: A dictionary with month names as keys and tuples as values. Each tuple contains two float values representing the average temperatures of the two cities. **Output Format:** - The function should display a heatmap. **Function Signature:** ```python def plot_temperature_diff(data: dict): pass ``` **Constraints:** - Ensure the heatmap is properly labeled with month names on the y-axis. - The temperature differences should be calculated as the difference between the temperatures of the two cities for each month. **Example:** ```python data = { \'January\': (5.2, 3.1), \'February\': (6.5, 4.3), \'March\': (9.1, 7.2), \'April\': (12.4, 10.4), \'May\': (15.0, 14.0), \'June\': (18.6, 16.9), \'July\': (20.8, 19.7), \'August\': (20.3, 18.6), \'September\': (17.1, 14.9), \'October\': (12.6, 9.9), \'November\': (7.9, 5.6), \'December\': (4.7, 2.7) } plot_temperature_diff(data) ``` **Expected Output:** A heatmap visualizing the temperature differences between the two cities for each month, with a custom diverging palette and annotated values.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def plot_temperature_diff(data: dict): Plots a heatmap showing the temperature differences between two cities. Parameters: data (dict): A dictionary with month names as keys and tuples as values, where each tuple contains the average temperatures of two cities. Returns: None # Calculate temperature differences months = list(data.keys()) temperature_diffs = [data[month][0] - data[month][1] for month in months] # Create a DataFrame for seaborn heatmap df = pd.DataFrame(data={\'Month\': months, \'Temperature Difference\': temperature_diffs}) df.set_index(\'Month\', inplace=True) # Create custom diverging palette cmap = sns.diverging_palette(240, 10, n=13, s=80, l=70, sep=20, center=\\"dark\\") # Plotting the heatmap plt.figure(figsize=(10, 8)) ax = sns.heatmap(df.T, annot=True, fmt=\\".1f\\", cmap=cmap, center=0, cbar_kws={\'label\': \'Temperature Difference (°C)\'}) # Customize the plot labels ax.set_yticklabels(ax.get_yticklabels(), rotation=0) plt.title(\'Monthly Temperature Differences Between Two Cities\') plt.xlabel(\'\') plt.ylabel(\'\') plt.show()"},{"question":"Objective You are required to design and implement a data processing pipeline using scikit-learn that incorporates multiple transformers to: 1. Preprocess the dataset by handling missing values. 2. Standardize the feature values. 3. Perform dimensionality reduction. Problem Statement You are given a dataset (in the form of a CSV file) with numerical features. The dataset may contain missing values, and it is imperative to preprocess it efficiently. Your goal is to create a scikit-learn pipeline that performs the following steps: 1. **Imputation**: Handle missing values using the mean strategy. 2. **Standardization**: Standardize the dataset (zero mean, unit variance). 3. **Dimensionality Reduction**: Reduce the number of features to 2 principal components using PCA (Principal Component Analysis). You need to implement the function `create_pipeline` that takes no arguments and returns a scikit-learn Pipeline object configured for the above steps. Expected Function Signature ```python def create_pipeline() -> sklearn.pipeline.Pipeline: pass ``` Constraints 1. You must use scikit-learn transformers. 2. You should make use of `Pipeline` to combine the transformers. 3. Handle missing values using `SimpleImputer`. 4. Standardize the dataset using `StandardScaler`. 5. Reduce dimensionality using `PCA`. Instructions 1. Read the documentation and understand the usage of `SimpleImputer`, `StandardScaler`, and `PCA`. 2. Combine these transformations into a single `Pipeline`. 3. Return the configured `Pipeline` object. Example Usage To validate your pipeline, you might load a dataset and fit-transform it using the created pipeline. For instance: ```python import numpy as np import pandas as pd from sklearn.pipeline import Pipeline # Sample dataset generation data = {\'Feature1\': [1.0, 2.0, np.nan, 4.0], \'Feature2\': [np.nan, 1.0, 3.0, 4.0]} df = pd.DataFrame(data) # Create pipeline pipeline = create_pipeline() # Fit-transform the dataset transformed_data = pipeline.fit_transform(df) # Display transformed data print(transformed_data) ``` The output should be a transformed dataset with 2 principal components and no missing values.","solution":"from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA def create_pipeline() -> Pipeline: Creates a scikit-learn pipeline to preprocess the data: - Handles missing values using the mean strategy. - Standardizes the data to have zero mean and unit variance. - Reduces dimensionality to 2 principal components using PCA. return Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)) ])"},{"question":"# Custom HTTP Server Implementation In this task, you are required to implement a simple custom HTTP server using the `http.server` module in Python. Specifically, you will extend the `BaseHTTPRequestHandler` to handle a new custom HTTP method `DO_MAGIC` and return a dynamic content response. Requirements: 1. **CustomMagicHandler class**: - Extend the `BaseHTTPRequestHandler` class. - Implement a method `do_MAGIC(self)` to handle the `DO_MAGIC` request. - Your implementation should parse the request path. - Extract any query parameters present in the request path. - Respond with a JSON object containing the query parameters and their values. 2. **Run the server**: - Create a function `run_custom_server(port)` that sets up and runs the custom server on the given port. - The server should use the `CustomMagicHandler` to handle requests. - The server should continue to run indefinitely. Input: - The `port` parameter for `run_custom_server` which specifies the port on which the server should listen. Output: - The server should respond to `DO_MAGIC` requests by returning a JSON response with the query parameters and their values. Example: When a `DO_MAGIC` request is received at `http://localhost:8000/anything?foo=bar&baz=qux`, the response should be: ```json { \\"foo\\": \\"bar\\", \\"baz\\": \\"qux\\" } ``` Constraints: - You must use the `http.server` module\'s classes. - Ensure the server correctly handles and parses the query parameters. - Use appropriate HTTP status codes and headers for the response. Performance: - The server should be able to handle multiple DO_MAGIC requests concurrently. Starter Code: ```python import http.server import socketserver import urllib.parse import json class CustomMagicHandler(http.server.BaseHTTPRequestHandler): def do_MAGIC(self): # Extract query parameters from the URL query = urllib.parse.urlparse(self.path).query parameters = urllib.parse.parse_qs(query) # Prepare the response response = {key: parameters[key][0] for key in parameters} # Send response status code self.send_response(200) # Send headers self.send_header(\'Content-Type\', \'application/json\') self.end_headers() # Respond with a JSON object self.wfile.write(json.dumps(response).encode(\'utf-8\')) def run_custom_server(port): with socketserver.TCPServer((\\"\\", port), CustomMagicHandler) as httpd: print(\\"serving at port\\", port) httpd.serve_forever() # Example usage: # run_custom_server(8000) ``` Complete the implementation of the `CustomMagicHandler` class and the `run_custom_server` function. You should be able to run your server using the `run_custom_server` function and interact with it using a web browser or tools like `curl` or `Postman`.","solution":"import http.server import socketserver import urllib.parse import json class CustomMagicHandler(http.server.BaseHTTPRequestHandler): def do_MAGIC(self): # Extract query parameters from the URL query = urllib.parse.urlparse(self.path).query parameters = urllib.parse.parse_qs(query) # Prepare the response as a JSON object response = {key: parameters[key][0] for key in parameters} # Send response status code self.send_response(200) # Send headers self.send_header(\'Content-Type\', \'application/json\') self.end_headers() # Respond with a JSON object self.wfile.write(json.dumps(response).encode(\'utf-8\')) def run_custom_server(port): with socketserver.TCPServer((\\"\\", port), CustomMagicHandler) as httpd: print(\\"serving at port\\", port) httpd.serve_forever() # Example usage: # run_custom_server(8000)"},{"question":"# Coding Challenge: Custom Codec Registration and Usage Objective: Create a custom encoding/decoding mechanism in Python 3.10 and demonstrate its registration and usage with error handling. Task: 1. Implement a custom encoder and decoder using Python classes. 2. Register these custom codecs using the functions provided (`PyCodec_Register` and other related functions). 3. Write a function `encode_decode_test` to encode and decode a given input string using the registered codecs and handle any potential errors. Function Specification: 1. **register_custom_codecs(encoder, decoder):** - **Input:** - `encoder`: A callable that serves as the custom encoder. - `decoder`: A callable that serves as the custom decoder. - **Output:** - Register the `encoder` and `decoder` using the appropriate functions. Handle errors if any registration step fails. 2. **custom_encoder(input_string):** - **Input:** - `input_string` (str): The string to encode. - **Output:** - Return an encoded version of the input string. 3. **custom_decoder(encoded_string):** - **Input:** - `encoded_string` (str): The string to decode. - **Output:** - Return the decoded version of the input string. 4. **encode_decode_test(input_string):** - **Input:** - `input_string` (str): The string to encode and decode. - **Output:** - Return a tuple of (`encoded_string`, `decoded_string`). - **Note:** - Use error handling to manage possible encoding/decoding errors. Constraints: - The custom encoder can replace each character with its corresponding ASCII value followed by a space. - The custom decoder can reverse this transformation back to the original string. - Make use of `PyCodec_Register` to register the custom codec and `PyCodec_Unregister` to clean up after tests. Example: ```python def custom_encoder(input_string: str): encoded = \' \'.join(str(ord(char)) for char in input_string) return encoded def custom_decoder(encoded_string: str): decoded = \'\'.join(chr(int(code)) for code in encoded_string.split()) return decoded def register_custom_codecs(encoder, decoder): # Pseudo-code to demonstrate the process. if not PyCodec_Register(encoder): raise ValueError(\\"Failed to register encoder\\") if not PyCodec_Register(decoder): raise ValueError(\\"Failed to register decoder\\") def encode_decode_test(input_string: str): register_custom_codecs(custom_encoder, custom_decoder) try: encoded = custom_encoder(input_string) decoded = custom_decoder(encoded) finally: PyCodec_Unregister(custom_encoder) PyCodec_Unregister(custom_decoder) return (encoded, decoded) # Example Usage encoded, decoded = encode_decode_test(\\"Hello\\") print(encoded) # Expected: \\"72 101 108 108 111\\" print(decoded) # Expected: \\"Hello\\" ``` Note: Specific implementations of `PyCodec_Register` and `PyCodec_Unregister` are abstracted for simplicity, but students should provide the correct usage based on the provided documentation.","solution":"import codecs class CustomCodec: def __init__(self): pass def custom_encoder(self, input_string): encoded = \' \'.join(str(ord(char)) for char in input_string) return encoded, len(encoded) def custom_decoder(self, encoded_string): decoded = \'\'.join(chr(int(code)) for code in encoded_string.split()) return decoded, len(decoded) def lookup_custom(name): if name == \'custom\': codec = CustomCodec() return codecs.CodecInfo( name=\'custom\', encode=codec.custom_encoder, decode=codec.custom_decoder ) return None def register_custom_codecs(): codecs.register(lookup_custom) def encode_decode_test(input_string): # Register the custom codec register_custom_codecs() try: # Encode the input string encoded, _ = codecs.getencoder(\'custom\')(input_string) # Decode the encoded string decoded, _ = codecs.getdecoder(\'custom\')(encoded) except Exception as e: return str(e) return encoded, decoded"},{"question":"**Objective:** Your task is to write a Python function that processes a ZIP file containing multiple text files. The function should read the ZIP file, list all the text files it contains, and then extract the content of each text file, printing its filename along with the first line of its content. **Function Specification:** ```python def process_zip_file(zip_file_path: str) -> None: Reads a ZIP file, lists all text files (.txt) it contains, and prints the filename along with the first line of its content. Args: zip_file_path (str): The path to the ZIP file. Returns: None pass ``` **Input:** - `zip_file_path`: A string representing the path to the ZIP file. **Output:** - The function should print the filename of each text file and the first line of its content. **Requirements:** 1. Ensure to handle any exceptions related to reading the ZIP file. 2. Only process files with a `.txt` extension. 3. Use the appropriate context managers to handle file operations within the ZIP file. **Example:** Consider a ZIP file at path \\"archive.zip\\" containing the following files: - \\"file1.txt\\" (with content: \\"Hello, World!nThis is the first file.\\") - \\"file2.txt\\" (with content: \\"Python is awesome!nLet\'s code.\\") - \\"image.png\\" - \\"document.pdf\\" Upon calling `process_zip_file(\\"archive.zip\\")`, the expected output should be: ``` file1.txt: Hello, World! file2.txt: Python is awesome! ``` **Constraints:** - Assume the ZIP file is not password-protected. - The function should be efficient in terms of memory usage, particularly for large ZIP files.","solution":"import zipfile def process_zip_file(zip_file_path: str) -> None: Reads a ZIP file, lists all text files (.txt) it contains, and prints the filename along with the first line of its content. Args: zip_file_path (str): The path to the ZIP file. Returns: None try: with zipfile.ZipFile(zip_file_path, \'r\') as zf: # List all the file names in the ZIP for file_info in zf.infolist(): # Process only .txt files if file_info.filename.endswith(\'.txt\'): with zf.open(file_info.filename) as file: first_line = file.readline().decode(\'utf-8\').strip() print(f\\"{file_info.filename}: {first_line}\\") except zipfile.BadZipFile: print(\\"Error: Not a valid ZIP file\\") except FileNotFoundError: print(\\"Error: File not found\\") except Exception as e: print(f\\"Error: {e}\\")"},{"question":"# PyTorch Environment Configuration Task In this task, you are required to write a function that configures and initializes a PyTorch environment based on specific requirements using environment variables. Objective Configure a PyTorch script using environment variables for the following requirements: 1. Debug memory allocations by disabling caching in CUDA. 2. Use NVML to check if CUDA drivers are functional before importing PyTorch. 3. Restrict the cuDNN v8 API memory cache limit to 1GiB. 4. Enable non-blocking error handling in NCCL. 5. Avoid using workspace memory in cuBLAS. Function Specification # Function Signature ```python def configure_pytorch(): ``` # Detailed Steps - The function does not take any parameters. - Your function must set appropriate environment variables inside the code to fulfill the requirements. - Use Python\'s `os` module to set these environment variables. - After setting up the environment variables, perform a simple PyTorch operation: initialize a tensor and move it to the CUDA device if available. # Expected Output - The function does not return anything. - Print the PyTorch version and CUDA availability status. - Print the tensor and its device after moving it to CUDA. # Example Usage ```python configure_pytorch() ``` Expected console output should include: ``` PyTorch Version: [Your PyTorch version] CUDA Available: [True/False] Tensor: tensor([1.], device=\'cuda:0\') ``` You can import the required packages for PyTorch and the `os` module. # Constraints and Notes - Assume the machine running this script has an appropriate CUDA-capable device and PyTorch installed with CUDA support. - The modified environment should apply only to the current script execution.","solution":"import os import torch def configure_pytorch(): # Disable caching in CUDA using environment variables os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' # Ensure NVML can be used to check CUDA drivers functionality os.environ[\'CUDA_MODULE_LOADING\'] = \'LAZY\' # Restrict cuDNN v8 API memory cache limit to 1GiB os.environ[\'CUDNN_CACHE_BYTES\'] = str(1024 * 1024 * 1024) # Enable non-blocking error handling in NCCL os.environ[\'NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' # Avoid using workspace memory in cuBLAS os.environ[\'CUBLAS_WORKSPACE_CONFIG\'] = \':4096:8\' # Verify the configurations by performing a simple PyTorch operation # Assume we have PyTorch installed and configured with CUDA try: torch_version = torch.__version__ cuda_available = torch.cuda.is_available() device = torch.device(\\"cuda\\" if cuda_available else \\"cpu\\") tensor = torch.tensor([1.0]).to(device) # Print the required information print(f\\"PyTorch Version: {torch_version}\\") print(f\\"CUDA Available: {cuda_available}\\") print(f\\"Tensor: {tensor}\\") print(f\\"Tensor Device: {tensor.device}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Demonstrate your understanding of seaborn by loading a dataset and creating various customized histograms. Instructions 1. Load the \\"penguins\\" dataset from seaborn. 2. Create a function `custom_histogram` with the following signature: ```python def custom_histogram(dataset: str, x_var: str, y_var: str = None, hue_var: str = None, bins: int = 10, binwidth: float = None, log_scale: bool = False, element: str = \\"bars\\", multiple: str = \\"layer\\", kde: bool = False, cumulative: bool = False): # Your code here ``` This function should generate and display the appropriate histogram based on the parameters provided: - `dataset`: Name of the seaborn dataset to load (e.g., \\"penguins\\"). - `x_var`: The variable to plot along the x-axis. - `y_var` (optional): The variable to plot along the y-axis, if any. - `hue_var` (optional): The variable that defines subsets to plot with different colors. - `bins`: The number of bins to use for the histogram. - `binwidth` (optional): Width of each bin when bins are not mentioned. - `log_scale`: Whether to use a logarithmic scale for the axis, default is False. - `element`: The type of plot element to use; possible values are \\"bars\\", \\"step\\", or \\"poly\\". - `multiple`: How to plot multiple distributions; possible values are \\"layer\\", \\"stack\\", or \\"dodge\\". - `kde`: Whether to add a kernel density estimate, default is False. - `cumulative`: Whether to plot the cumulative counts or frequencies, default is False. 3. Use the function you created to generate the following histograms: - A histogram of \\"flipper_length_mm\\" with 30 bins. - A histogram with \\"flipper_length_mm\\" on the y-axis and \\"species\\" as the hue. - A stacked histogram of \\"flipper_length_mm\\" with 20 bins and hue \\"species\\". - A cumulative density histogram of \\"bill_length_mm\\" with hue \\"island\\" and specified binwidth of 2. Constraints - You may not use any other libraries except seaborn and matplotlib for this task. - Ensure that the function is efficient and handles edge cases appropriately. Example Usage ```python custom_histogram(\\"penguins\\", \\"flipper_length_mm\\", bins=30) custom_histogram(\\"penguins\\", \\"flipper_length_mm\\", y_var=\\"species\\") custom_histogram(\\"penguins\\", \\"flipper_length_mm\\", hue_var=\\"species\\", bins=20, multiple=\\"stack\\") custom_histogram(\\"penguins\\", \\"bill_length_mm\\", hue_var=\\"island\\", binwidth=2, cumulative=True) ``` Each function call should produce a different histogram plot based on the specified parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_histogram(dataset: str, x_var: str, y_var: str = None, hue_var: str = None, bins: int = 10, binwidth: float = None, log_scale: bool = False, element: str = \\"bars\\", multiple: str = \\"layer\\", kde: bool = False, cumulative: bool = False): Generates and displays a customized histogram based on the provided parameters. Parameters: - dataset (str): Name of the seaborn dataset to load. - x_var (str): The variable to plot along the x-axis. - y_var (str): The variable to plot along the y-axis, if any. - hue_var (str): The variable that defines subsets to plot with different colors. - bins (int): The number of bins to use for the histogram. - binwidth (float): Width of each bin when bins are not mentioned. - log_scale (bool): Whether to use a logarithmic scale for the axis. - element (str): The type of plot element to use; possible values are \\"bars\\", \\"step\\", or \\"poly\\". - multiple (str): How to plot multiple distributions; possible values are \\"layer\\", \\"stack\\", or \\"dodge\\". - kde (bool): Whether to add a kernel density estimate. - cumulative (bool): Whether to plot the cumulative counts or frequencies. # Load the dataset data = sns.load_dataset(dataset) # Generate the histogram sns.histplot(data=data, x=x_var, y=y_var, hue=hue_var, bins=bins, binwidth=binwidth, log_scale=log_scale, element=element, multiple=multiple, kde=kde, cumulative=cumulative) # Display the plot plt.show()"},{"question":"# Objective You are tasked to analyze the relationship between the `weight` and `displacement` of cars from the `mpg` dataset available in seaborn and produce a series of seaborn visualizations to illustrate the regression and the residuals, checking for any potential violations of linear regression assumptions. # Requirements 1. **Data Loading & Preparation:** - Load the `mpg` dataset from seaborn. 2. **Visualization Task:** - Create a residual plot for `weight` as the predictor variable (`x`) and `displacement` as the response variable (`y`). - Create another residual plot for `horsepower` as the predictor variable (`x`) and `mpg` as the response variable (`y`). - Modify the second residual plot to fit a higher-order (quadratic) regression model. - Add a LOWESS curve to the quadratic regression residual plot to reveal or emphasize structure. # Constraints - Ensure plots are clearly labeled and include titles. - Use seaborn\'s `set_theme` for consistent styling. - Your functions should handle missing data by dropping any rows with missing values. # Functions to implement ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_mpg_data(): # Step 1: Load the data data = sns.load_dataset(\\"mpg\\") # Step 2: Remove rows with missing values data.dropna(subset=[\\"weight\\", \\"displacement\\", \\"horsepower\\", \\"mpg\\"], inplace=True) # Step 3: Set theme for seaborn sns.set_theme() # Step 4: Create a residual plot for weight vs displacement plt.figure(figsize=(10, 6)) sns.residplot(data=data, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual plot: Weight vs Displacement\\") plt.show() # Step 5: Create a residual plot for horsepower vs mpg plt.figure(figsize=(10, 6)) sns.residplot(data=data, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual plot: Horsepower vs MPG\\") plt.show() # Step 6: Create a quadratic residual plot for horsepower vs mpg plt.figure(figsize=(10, 6)) sns.residplot(data=data, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Quadratic Residual plot: Horsepower vs MPG\\") plt.show() # Step 7: Create a quadratic residual plot with a LOWESS curve plt.figure(figsize=(10, 6)) sns.residplot(data=data, x=\\"horsepower\\", y=\\"mpg\\", order=2, lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Quadratic Residual plot with LOWESS: Horsepower vs MPG\\") plt.show() # Execute function to produce the plots analyze_mpg_data() ``` # Input & Output - **Input:** No direct input to the function since it loads the dataset internally. - **Output:** Display of the required residual plots with appropriate titles and labels. **Note:** Ensure you import necessary libraries such as matplotlib and seaborn before running the function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_mpg_data(): Analyzes the `mpg` dataset and generates residual plots for different predictor-response relationships. # Step 1: Load the \'mpg\' dataset from seaborn data = sns.load_dataset(\\"mpg\\") # Step 2: Remove rows with missing values data.dropna(subset=[\\"weight\\", \\"displacement\\", \\"horsepower\\", \\"mpg\\"], inplace=True) # Step 3: Set theme for seaborn sns.set_theme() # Step 4: Create a residual plot for weight vs displacement plt.figure(figsize=(10, 6)) sns.residplot(x=\\"weight\\", y=\\"displacement\\", data=data) plt.title(\\"Residual plot: Weight vs Displacement\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # Step 5: Create a residual plot for horsepower vs mpg plt.figure(figsize=(10, 6)) sns.residplot(x=\\"horsepower\\", y=\\"mpg\\", data=data) plt.title(\\"Residual plot: Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Step 6: Create a quadratic residual plot for horsepower vs mpg plt.figure(figsize=(10, 6)) sns.residplot(x=\\"horsepower\\", y=\\"mpg\\", data=data, order=2) plt.title(\\"Quadratic Residual plot: Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Step 7: Create a quadratic residual plot with a LOWESS curve plt.figure(figsize=(10, 6)) sns.residplot(x=\\"horsepower\\", y=\\"mpg\\", data=data, order=2, lowess=True, line_kws=dict(color=\\"red\\")) plt.title(\\"Quadratic Residual plot with LOWESS: Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Execute function to produce the plots analyze_mpg_data()"},{"question":"**Coding Assessment Question: Custom Color Palettes and Application in Seaborn** # Objective In this task, you are required to demonstrate your understanding of creating custom color palettes using Seaborn\'s `mpl_palette` function and applying them to Seaborn plots. This task assesses your ability to handle both discrete and continuous colormaps, as well as qualitative colormaps. # Instructions 1. **Custom Continuous Palette:** - Create a continuous colormap palette using the Viridis colormap. - Generate a palette with 10 colors from this colormap using `sns.mpl_palette`. 2. **Qualitative Palette:** - Create a qualitative colormap palette using the Set2 colormap scheme. - Generate a palette with 6 distinct colors from this qualitative colormap using `sns.mpl_palette`. 3. **Scatterplot:** - Using the `iris` dataset (available from Seaborn\'s sample datasets), create a scatter plot of `seaborn.scatterplot()`. - Apply the continuous palette generated in step 1 to color the points based on the `petal_length` attribute. 4. **Barplot:** - Using the `tips` dataset (available from Seaborn\'s sample datasets), create a bar plot of `seaborn.barplot()`. - Apply the qualitative palette generated in step 2 to color the bars based on the `day` attribute. # Specifications - **Input:** - The datasets will be loaded using seaborn\'s sample datasets, specifically the `iris` and `tips` datasets. - **Output:** - One scatter plot visualizing the `iris` dataset with a continuous palette. - One bar plot visualizing the `tips` dataset with a qualitative palette. - **Constraints:** - The palettes should be created strictly using `sns.mpl_palette`. - The visualizations should clearly reflect the intended palette. - Ensure that the plots are labeled appropriately for clear interpretation. # Example Execution ```python import seaborn as sns import matplotlib.pyplot as plt # Load datasets iris = sns.load_dataset(\\"iris\\") tips = sns.load_dataset(\\"tips\\") # Step 1: Create a custom continuous palette continuous_palette = sns.mpl_palette(\\"viridis\\", 10) # Step 2: Create a qualitative palette qualitative_palette = sns.mpl_palette(\\"Set2\\", 6) # Step 3: Create scatter plot plt.figure(figsize=(8, 6)) scatter_plot = sns.scatterplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"petal_length\\", palette=continuous_palette) plt.title(\'Scatter plot with Continuous Palette\') plt.show() # Step 4: Create bar plot plt.figure(figsize=(8, 6)) bar_plot = sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=qualitative_palette) plt.title(\'Bar plot with Qualitative Palette\') plt.show() ``` # Performance Requirements - The code should execute efficiently and generate the plots without significant delays. - Ensure that the color differentiation is clear in both the scatter plot and the bar plot. # Hints - Refer to the seaborn and matplotlib colormaps documentation for additional help and examples. - Test your plots interactively to ensure that palettes are applied correctly before finalizing your code.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load datasets iris = sns.load_dataset(\\"iris\\") tips = sns.load_dataset(\\"tips\\") def create_custom_palettes(): Creates the custom continuous and qualitative palettes using the specified colormap schemes. # Step 1: Create a custom continuous palette continuous_palette = sns.mpl_palette(\\"viridis\\", 10) # Step 2: Create a qualitative palette qualitative_palette = sns.mpl_palette(\\"Set2\\", 6) return continuous_palette, qualitative_palette def plot_scatter_with_continuous_palette(continuous_palette): Creates a scatter plot using the iris dataset and applies the continuous palette. plt.figure(figsize=(8, 6)) scatter_plot = sns.scatterplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"petal_length\\", palette=continuous_palette) plt.title(\'Scatter plot with Continuous Palette\') plt.show() def plot_bar_with_qualitative_palette(qualitative_palette): Creates a bar plot using the tips dataset and applies the qualitative palette. plt.figure(figsize=(8, 6)) bar_plot = sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=qualitative_palette) plt.title(\'Bar plot with Qualitative Palette\') plt.show() continuous_palette, qualitative_palette = create_custom_palettes() plot_scatter_with_continuous_palette(continuous_palette) plot_bar_with_qualitative_palette(qualitative_palette)"},{"question":"Objective You are tasked with creating a function that mimics the basic functionality of the **sdist** command, focusing on including and excluding files based on the provided Unix-style glob patterns. Task Write a Python function named `create_source_distribution` that accepts a list of files and a list of commands. The function should return a list of files that should be included in the source distribution after applying the commands. Function Signature ```python def create_source_distribution(files: list, commands: list) -> list: pass ``` Parameters - `files`: A list of strings where each string represents a file path. - `commands`: A list of strings where each string represents a command to include or exclude files based on patterns. Commands Format - `include pattern1 pattern2 ...` - include all files matching any of the listed patterns. - `exclude pattern1 pattern2 ...` - exclude all files matching any of the listed patterns. - `recursive-include dir pattern1 pattern2 ...` - include all files under `dir` matching any of the listed patterns. - `recursive-exclude dir pattern1 pattern2 ...` - exclude all files under `dir` matching any of the listed patterns. - `global-include pattern1 pattern2 ...` - include all files anywhere in the source tree matching any of the listed patterns. - `global-exclude pattern1 pattern2 ...` - exclude all files anywhere in the source tree matching any of the listed patterns. - `prune dir` - exclude all files under `dir`. - `graft dir` - include all files under `dir`. Constraints - The function should consider commands in the order they are provided. - Patterns follow Unix-style glob patterns. - `files` list should remain unordered. Example ```python files = [ \\"src/module1.py\\", \\"src/module2.py\\", \\"tests/test_module1.py\\", \\"docs/readme.md\\", \\"setup.py\\" ] commands = [ \\"include src/module1.py docs/readme.md\\", \\"exclude docs/readme.md\\", \\"recursive-include tests *.py\\", \\"global-exclude *.md\\" ] output = create_source_distribution(files, commands) # Expected output: [\'src/module1.py\', \'tests/test_module1.py\'] ``` Note - You can use the `fnmatch` module for pattern matching. - Be mindful of the order of commands as they can overwrite previous commands\' effects. Implement the `create_source_distribution` function to achieve the desired behavior based on the given commands.","solution":"import fnmatch import os def create_source_distribution(files: list, commands: list) -> list: included_files = set() excluded_files = set() all_files = set(files) for command in commands: parts = command.split() cmd = parts[0] patterns = parts[1:] if cmd == \\"include\\": for pattern in patterns: included_files.update(fnmatch.filter(files, pattern)) elif cmd == \\"exclude\\": for pattern in patterns: excluded_files.update(fnmatch.filter(files, pattern)) elif cmd == \\"recursive-include\\": dir = parts[1] patterns = parts[2:] for file in files: if file.startswith(dir): for pattern in patterns: if fnmatch.fnmatch(file, os.path.join(dir, pattern)): included_files.add(file) elif cmd == \\"recursive-exclude\\": dir = parts[1] patterns = parts[2:] for file in files: if file.startswith(dir): for pattern in patterns: if fnmatch.fnmatch(file, os.path.join(dir, pattern)): excluded_files.add(file) elif cmd == \\"global-include\\": for pattern in patterns: included_files.update(fnmatch.filter(files, pattern)) elif cmd == \\"global-exclude\\": for pattern in patterns: excluded_files.update(fnmatch.filter(files, pattern)) elif cmd == \\"prune\\": dir = parts[1] for file in files: if file.startswith(dir): excluded_files.add(file) elif cmd == \\"graft\\": dir = parts[1] for file in files: if file.startswith(dir): included_files.add(file) final_included_files = included_files - excluded_files return list(final_included_files)"},{"question":"# Distributed Training with Uneven Inputs **Objective:** Implement a simple distributed training setup using PyTorch, where the dataset is not evenly divisible among the processors. You are required to manage these uneven inputs using the provided classes `Join`, `Joinable`, and `JoinHook`. **Task:** 1. Create a simple neural network using PyTorch. 2. Implement a data loader for a dataset where the data is unevenly distributed. 3. Set up a distributed training environment using PyTorch\'s `torch.distributed` package. 4. Use the `Join`, `Joinable`, and `JoinHook` classes to handle the uneven inputs during the training process. **Instructions:** 1. **Neural Network:** - Define a simple neural network model (e.g., a small CNN or MLP). 2. **Data Loader:** - Create a custom data loader that simulates uneven distribution of data among the available processors. 3. **Distributed Training Setup:** - Initialize the process group for distributed training. - Ensure each process has a distinct subset of the data. 4. **Handling Uneven Inputs:** - Use the `Join`, `Joinable`, and `JoinHook` classes to manage the training loop. Specifically, ensure that the training process can handle cases where different processes may finish processing their data at different times. **Constraints:** - Use PyTorch only. - Assume you have 4 GPUs available for distributed training. - The dataset does not need to be large; a small toy dataset is sufficient to demonstrate the concept. - The focus is on correctly using the `Join`, `Joinable`, and `JoinHook` classes to handle the synchronization of uneven inputs. **Expected Input and Output:** - Input: None directly, but internally you should handle simulated dataset splits. - Output: Training loss printed at the end of each epoch for all processes. **Example:** Assume you have 1000 samples in total and 4 GPUs. Each GPU may receive a different number of samples (e.g., GPU 0 gets 300, GPU 1 gets 250, GPU 2 gets 250, and GPU 3 gets 200). The expected output is the proper synchronization and completion of the training loop with printed loss values even though the data distribution is uneven. **Starter Code:** ```python import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, Dataset # Define a simple neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Create a custom dataset to simulate uneven data distribution class CustomDataset(Dataset): # Implementation of dataset loading with uneven distribution def train(rank, world_size): # Initialize process group # Set up model and optimizer # Create DataLoader with uneven data distribution # Use Join, Joinable, and JoinHook classes to manage training if __name__ == \\"__main__\\": world_size = 4 # Number of GPUs dist.spawn(train, args=(world_size,), nprocs=world_size, join=True) ``` **Performance Requirements:** - Ensure that the solution is scalable and efficient in terms of synchronization among processes. Good luck!","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, Dataset, Subset import numpy as np # Define a simple neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Create a custom dataset to simulate uneven data distribution class CustomDataset(Dataset): def __init__(self, size): self.data = torch.randn(size, 10) self.labels = torch.randint(0, 2, (size, 1), dtype=torch.float) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.labels[idx] # Define a function to split the dataset unevenly def split_dataset(dataset, world_size): # Simulate uneven split lengths = [300, 250, 250, 200] # Example uneven split subsets = [] start = 0 for length in lengths: end = start + length subsets.append(Subset(dataset, range(start, end))) start = end return subsets def train(rank, world_size): # Initialize process group dist.init_process_group(\'gloo\', rank=rank, world_size=world_size) # Set up model and move it to the current device model = SimpleModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) # Set up optimizer optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) criterion = nn.BCEWithLogitsLoss() # Create dataset and unevenly split it dataset = CustomDataset(size=1000) subsets = split_dataset(dataset, world_size) # Set up DataLoader for the current process dataloader = DataLoader(subsets[rank], batch_size=32, shuffle=True) epochs = 5 for epoch in range(epochs): total_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(rank), labels.to(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() total_loss += loss.item() print(f\'Rank {rank}, Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\') # Destroy process group dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = 4 # Number of GPUs (or processes) dist.spawn(train, args=(world_size,), nprocs=world_size, join=True)"},{"question":"Objective Your task is to demonstrate your understanding of memory management and performance profiling using the `torch.mps` package in PyTorch. Problem Statement Write a Python function that performs the following operations: 1. Checks the number of available MPS devices using `torch.mps.device_count()`. If no devices are available, raise an exception with a meaningful error message. 2. Sets a manual seed for the random number generator using `torch.mps.manual_seed(42)`. 3. Allocates a large tensor on the MPS device. 4. Measures the memory used by the current process using `torch.mps.current_allocated_memory()` before and after the tensor allocation. 5. Profiles the tensor allocation and a simple computation (e.g., tensor addition) using the MPS profiler. 6. Releases the cached memory using `torch.mps.empty_cache()` after the computation. 7. Returns a dictionary with: - The number of MPS devices. - The memory before tensor allocation. - The memory after tensor allocation. - The profiling results. Function Signature ```python import torch def manage_mps_memory_and_profile(): # Your code here return { \\"num_mps_devices\\": num_mps_devices, \\"memory_before\\": memory_before, \\"memory_after\\": memory_after, \\"profiler_results\\": profiler_results, } ``` Constraints - Ensure your code handles scenarios where no MPS devices are available. - Use a large tensor (e.g., 10,000 x 10,000) to test memory allocation. - Ensure proper synchronization where necessary. - The function should be able to run on macOS devices that support MPS. Example Output ```python { \\"num_mps_devices\\": 1, \\"memory_before\\": 100000, \\"memory_after\\": 800000000, \\"profiler_results\\": {\\"start\\": \\"Timestamp1\\", \\"stop\\": \\"Timestamp2\\"} } ``` Evaluation Criteria - Correctness: The function must perform the specified operations correctly. - Robustness: The function should handle edge cases and provide meaningful error messages. - Performance: The function should efficiently manage memory and profiling. - Code Quality: The code should be readable, well-organized, and adhere to best practices.","solution":"import torch import time def manage_mps_memory_and_profile(): # Check if MPS is available num_mps_devices = torch.mps.device_count() if num_mps_devices == 0: raise Exception(\\"No MPS devices available.\\") # Set the manual seed torch.mps.manual_seed(42) # Measure memory before tensor allocation memory_before = torch.mps.current_allocated_memory() # Allocate a large tensor on the MPS device tensor = torch.randn(10000, 10000, device=\\"mps\\") # Measure memory after tensor allocation memory_after = torch.mps.current_allocated_memory() # Perform a simple computation (tensor addition) and profile it start_time = time.time() tensor_added = tensor + tensor torch.mps.synchronize() # Ensure synchronization end_time = time.time() profiler_results = { \\"start\\": start_time, \\"stop\\": end_time } # Release the cached memory torch.mps.empty_cache() return { \\"num_mps_devices\\": num_mps_devices, \\"memory_before\\": memory_before, \\"memory_after\\": memory_after, \\"profiler_results\\": profiler_results, }"},{"question":"# Advanced Python Logging Configuration You are tasked with implementing a custom logging configuration for a Python application. Your goal is to set up an efficient logging mechanism that writes logs to multiple destinations with different logging levels and formats, and includes custom contextual information within the logs. Detailed Tasks: 1. **Create a Custom Logging Configuration**: - Configure the logging system to log INFO-level and above messages to a file (`app.log`) with timestamps. - Configure the logging system to log ERROR-level and above messages to the console with a simplified format. - Add a contextual filter that includes extra information (`request_id` and `user_id`) to every log record. You can use random values or predefined ones for the contextual keys. 2. **Implement Contextual Information with a LoggerAdapter**: - Create a `CustomAdapter` subclass of `LoggerAdapter` that adds additional contextual information to log records. 3. **Simulate Logging from Multiple Threads**: - Create a Python script that spawns multiple threads, each thread should generate logs using the configured logger. - Ensure each thread includes unique `request_id` and `user_id` values in its logs. Expected Input and Output: - **Input**: - A Python script that initializes the logging configuration. - Threads that perform operations and generate logs. - **Output**: - Logs should be present in both the console (for ERROR level and above) and the `app.log` file (for INFO level and above). - Logs should include contextual information (`request_id` and `user_id`). Constraints: - Use Python\'s standard `logging` module to set up and configure the logging system. - Ensure thread safety in your implementation. - Avoid using global variables for contextual information. Performance Requirements: - The solution should be efficient and should not block the main thread while logging from multiple threads. Example: Here is an example log format for `app.log`: ``` 2023-10-12 14:55:34,567 - myapp - INFO - [request_id: 1234] [user_id: user_1] - This is an info message ``` Here is an example log format for the console (for ERROR level and above): ``` ERROR: [request_id: 1234] [user_id: user_1] - This is an error message ``` # Implementation Notes: 1. **Initialization Script**: Create a `logging_config.py` file that sets up the logging configuration as described. 2. **Custom Adapter and Filter**: Implement the `CustomAdapter` and the contextual filter inside your script to inject `request_id` and `user_id` into all log records. 3. **Threaded Logging**: Create a script `main.py` that imports `logging_config.py`, sets up multiple threads, and starts logging operations demonstrating proper logging from each thread with contextual info. # Submission: - The `logging_config.py` with the logging setup. - The `main.py` script with the threaded logging simulation. - Ensure all code is well-commented and follows best practices.","solution":"import logging import threading from logging import LoggerAdapter import random import time import sys # Custom Adapter class to include contextual information class CustomAdapter(LoggerAdapter): def __init__(self, logger, extra): super().__init__(logger, extra) def process(self, msg, kwargs): extra = self.extra.copy() if \'extra\' in kwargs: extra.update(kwargs[\'extra\']) kwargs[\'extra\'] = extra return msg, kwargs # Function to configure the logger def configure_logger(): logger = logging.getLogger(\'myapp\') # File handler for INFO level logs and above file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.INFO) file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - [request_id: %(request_id)s] [user_id: %(user_id)s] - %(message)s\') file_handler.setFormatter(file_formatter) # Console handler for ERROR level logs and above console_handler = logging.StreamHandler(sys.stdout) console_handler.setLevel(logging.ERROR) console_console_formatter = logging.Formatter(\'%(levelname)s - [request_id: %(request_id)s] [user_id: %(user_id)s] - %(message)s\') console_handler.setFormatter(console_console_formatter) # Adding handlers to the logger logger.addHandler(file_handler) logger.addHandler(console_handler) logger.setLevel(logging.DEBUG) return logger def thread_logging_function(logger, request_id, user_id): adapter = CustomAdapter(logger, {\'request_id\': request_id, \'user_id\': user_id}) adapter.info(\\"This is an INFO message\\") adapter.error(\\"This is an ERROR message\\") if __name__ == \\"__main__\\": # Configure the logger once at the start logger = configure_logger() # Create and start multiple threads threads = [] for i in range(5): # Starting 5 threads request_id = f\\"req_{random.randint(1000, 9999)}\\" user_id = f\\"user_{random.randint(1, 100)}\\" thread = threading.Thread(target=thread_logging_function, args=(logger, request_id, user_id)) thread.start() threads.append(thread) # Join the threads to ensure complete execution before finishing the script for thread in threads: thread.join()"},{"question":"**Objective:** Implement a function to perform Gaussian initialization of a PyTorch tensor, avoiding any unsupported operations in TorchScript. # Problem Statement You are required to write a function `initialize_tensor(n, m, mean, std)` that initializes an `n x m` PyTorch tensor with values sampled from a Gaussian distribution with the specified mean and standard deviation. However, you must ensure that your code is compatible with TorchScript. Specifically, you should: - Avoid using any of the unsupported functions/methods listed in the documentation. - Ensure the tensor initialization and operations are TorchScript-compatible. # Function Signature ```python import torch from typing import Tuple def initialize_tensor(n: int, m: int, mean: float, std: float) -> torch.Tensor: pass ``` # Input - `n` (int): Number of rows in the tensor. - `m` (int): Number of columns in the tensor. - `mean` (float): Mean of the Gaussian distribution. - `std` (float): Standard deviation of the Gaussian distribution. # Output - Returns a PyTorch tensor of shape `(n, m)` initialized using the specified Gaussian distribution. # Constraints - Ensure the function can be compiled and run with TorchScript. - Avoid using `torch.nn.init.kaiming_normal_` or any other unsupported initialization methods. - You may use operations like `torch.randn` to generate a tensor from a normal distribution but make sure to scale and shift it appropriately to match the given mean and standard deviation. # Example ```python # Sample test n = 3 m = 4 mean = 0.0 std = 1.0 tensor = initialize_tensor(n, m, mean, std) print(tensor) # Expected output: # A 3x4 tensor with values sampled from N(0.0, 1.0) ``` # Notes - Your implementation should ensure compatibility with TorchScript by using only the supported set of operations. - You can make use of `torch.randn(n, m)` to generate a tensor with a standard normal distribution and then transform it to have the desired mean and standard deviation.","solution":"import torch from typing import Tuple def initialize_tensor(n: int, m: int, mean: float, std: float) -> torch.Tensor: Initializes an n x m PyTorch tensor with values sampled from a Gaussian distribution with specified mean and standard deviation. Args: n (int): Number of rows. m (int): Number of columns. mean (float): Mean of the Gaussian distribution. std (float): Standard deviation of the Gaussian distribution. Returns: torch.Tensor: Initialized tensor. # Generate a tensor with values from a standard normal distribution (mean=0, std=1) tensor = torch.randn((n, m)) # Transform to the desired mean and std tensor = tensor * std + mean return tensor"},{"question":"Objective: Demonstrate your knowledge of handling nullable integer data types using the pandas library. You will be required to create a pandas DataFrame that includes nullable integer columns, perform various operations, and ensure the correct data types and handling of missing values. Question: You are given a list of dictionaries representing monthly sales data where some values might be missing (represented as `None`). These dictionaries contain the keys: `month`, `year`, and `sales`. Implement the following functionality: 1. Create a pandas DataFrame from the given list of dictionaries. 2. Ensure that the `sales` column is of a nullable integer type (`Int64`). 3. Add a new column `adjusted_sales` to the DataFrame, which contains the sales value increased by 10% (round to the nearest integer). Handle missing values appropriately by setting them to `pd.NA`. 4. Calculate the total sales for each year and return a DataFrame with `year` and `total_sales` columns, where `total_sales` is of nullable integer type (`Int64`). Your function should have the following signature: ```python def process_sales_data(data: List[Dict[str, Union[int, None]]]) -> pd.DataFrame: pass ``` Input: - `data`: List of dictionaries, where each dictionary has the keys `month` (integer from 1 to 12), `year` (4-digit integer), and `sales` (integer or `None`). Output: - Returns a pandas DataFrame with columns `year` and `total_sales`, where `total_sales` is of nullable integer type (`Int64`). Constraints: - Missing values in `sales` should be handled using `pd.NA`. - The `data` list will contain at least one dictionary. Example: ```python data = [ {\'month\': 1, \'year\': 2020, \'sales\': 100}, {\'month\': 2, \'year\': 2020, \'sales\': None}, {\'month\': 3, \'year\': 2020, \'sales\': 150}, {\'month\': 1, \'year\': 2021, \'sales\': 200}, {\'month\': 2, \'year\': 2021, \'sales\': None}, ] result = process_sales_data(data) print(result) ``` Expected Output: ``` year total_sales 0 2020 250 1 2021 200 ``` Note: - Make sure to handle the missing values properly when calculating `adjusted_sales` and during summation.","solution":"import pandas as pd from typing import List, Dict, Union def process_sales_data(data: List[Dict[str, Union[int, None]]]) -> pd.DataFrame: # Create a DataFrame from the list of dictionaries df = pd.DataFrame(data) # Ensure the \'sales\' column is of nullable integer type (\'Int64\') df[\'sales\'] = df[\'sales\'].astype(\'Int64\') # Add a new column \'adjusted_sales\' with 10% increase, handling missing values with pd.NA df[\'adjusted_sales\'] = (df[\'sales\'] * 1.1).round().astype(\'Int64\') # Calculate the total sales for each year total_sales_per_year = df.groupby(\'year\')[\'sales\'].sum().reset_index() # Ensure the \'total_sales\' column is of nullable integer type (\'Int64\') total_sales_per_year[\'sales\'] = total_sales_per_year[\'sales\'].astype(\'Int64\') # Rename columns appropriately total_sales_per_year.rename(columns={\'sales\': \'total_sales\'}, inplace=True) return total_sales_per_year"},{"question":"# Function Implementation: Audio Device Operations **Objective:** Write a function to perform a sequence of operations on an OSS audio device, ensuring proper setup and error handling. **Question Description:** Implement a function `setup_and_play_audio(device: str, mode: str, format: str, nchannels: int, samplerate: int, data: bytes) -> str` that: 1. Opens an audio device with the specified `device` and `mode`. 2. Configures the audio device with the given `format`, `nchannels` (number of channels), and `samplerate` (sample rate). 3. Plays the provided audio `data`. 4. Properly handles any exceptions that may occur during these steps and closes the device. # Function Signature: ```python import ossaudiodev def setup_and_play_audio(device: str, mode: str, format: str, nchannels: int, samplerate: int, data: bytes) -> str: pass ``` # Input: - `device`: A string representing the audio device filename (e.g., \\"/dev/dsp\\"). If `device` is empty, use the default specified by the module (\\"/dev/dsp\\"). - `mode`: A string indicating the mode with which to open the device; must be one of \'r\', \'w\', or \'rw\'. - `format`: A string representing the audio format (e.g., \\"AFMT_S16_LE\\"). - `nchannels`: An integer indicating the number of channels (1 for mono, 2 for stereo). - `samplerate`: An integer representing the sample rate (e.g., 44100 for CD quality). - `data`: A `bytes` object containing the audio data to be played. # Output: - Returns a string \\"success\\" if the audio data is played successfully. - If any error occurs during the process, return a string describing the error (for example: \\"OSSAudioError: invalid format\\"). # Constraints: - Assume the audio device supports the specified format, channels, and sample rate. - Handle all relevant exceptions that may be raised according to the documentation. # Example: ```python data = b\'x01x02x03x04\' # Example audio data result = setup_and_play_audio(\\"/dev/dsp\\", \\"w\\", \\"AFMT_S16_LE\\", 2, 44100, data) print(result) # Output should be \\"success\\" if there are no errors ``` This task ensures that students understand the initialization and configuration of audio devices, handling exceptions appropriately, and performing basic read/write operations with those devices in the `ossaudiodev` module.","solution":"import ossaudiodev def setup_and_play_audio(device: str, mode: str, format: str, nchannels: int, samplerate: int, data: bytes) -> str: if not device: device = \\"/dev/dsp\\" if mode not in [\'r\', \'w\', \'rw\']: return \\"Invalid mode\\" format_mapping = { \\"AFMT_S16_LE\\": ossaudiodev.AFMT_S16_LE, \\"AFMT_S8\\": ossaudiodev.AFMT_S8, # Add more mappings as required } if format not in format_mapping: return \\"Invalid format\\" try: dsp = ossaudiodev.open(device, mode) dsp.setfmt(format_mapping[format]) dsp.channels(nchannels) dsp.speed(samplerate) if mode in [\'w\', \'rw\']: dsp.write(data) dsp.close() return \\"success\\" except Exception as e: return f\\"OSSAudioError: {e}\\""},{"question":"**Objective**: Demonstrate your understanding of the \\"datetime\\" module in Python by creating, validating, and extracting information from various datetime objects. **Problem Statement**: Write a Python function `process_date_time_info` that performs the following tasks: 1. Creates a `datetime.datetime` object with the given year, month, day, hour, minute, second, and microsecond. 2. Validates if the created object is of type `datetime.datetime`. 3. Extracts and returns the year, month, day, hour, minute, second, and microsecond from the created object. **Function Signature**: ```python def process_date_time_info(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> dict: pass ``` **Input**: - `year` (int): The year for the datetime object. - `month` (int): The month for the datetime object, must be between 1 and 12. - `day` (int): The day for the datetime object, must be between 1 and 31. - `hour` (int): The hour for the datetime object, must be between 0 and 23. - `minute` (int): The minute for the datetime object, must be between 0 and 59. - `second` (int): The second for the datetime object, must be between 0 and 59. - `microsecond` (int): The microsecond for the datetime object, must be between 0 and 999999. **Output**: - A dictionary with keys `year`, `month`, `day`, `hour`, `minute`, `second`, `microsecond` containing their respective values extracted from the created datetime object. **Constraints**: - Use the `datetime` module exclusively for creating and validating the datetime objects. - Handle invalid inputs gracefully by raising appropriate exceptions. **Example**: ```python # Example usage result = process_date_time_info(2023, 10, 5, 15, 30, 45, 123456) print(result) # Expected output: {\'year\': 2023, \'month\': 10, \'day\': 5, \'hour\': 15, \'minute\': 30, \'second\': 45, \'microsecond\': 123456} ``` **Notes**: - Ensure to use the `datetime.datetime` object creation functions provided by the `datetime` module. - Validate the created object using the type-checking macros. Good luck!","solution":"from datetime import datetime def process_date_time_info(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> dict: Creates a datetime object, validates its type, and extracts its components into a dictionary. try: dt = datetime(year, month, day, hour, minute, second, microsecond) if not isinstance(dt, datetime): raise ValueError(\\"Object created is not of type datetime.datetime\\") return { \'year\': dt.year, \'month\': dt.month, \'day\': dt.day, \'hour\': dt.hour, \'minute\': dt.minute, \'second\': dt.second, \'microsecond\': dt.microsecond } except Exception as e: raise ValueError(f\\"Failed to process datetime info: {e}\\")"},{"question":"**Coding Assessment Question:** **Objective:** Design a function that constructs a multipart email message with text and HTML content, and then serializes the message to a string. The function should also add an attachment to the email and ensure proper MIME headers are set. **Function Signature:** ```python def construct_email(subject: str, from_email: str, to_email: str, text_content: str, html_content: str, attachment_file_path: str) -> str: pass ``` **Instruction:** 1. **Input Parameters:** - `subject`: A string representing the subject of the email. - `from_email`: A string representing the sender\'s email address. - `to_email`: A string representing the recipient\'s email address. - `text_content`: A string representing the plain text content of the email. - `html_content`: A string representing the HTML content of the email. - `attachment_file_path`: A string representing the file path of the attachment to be added. 2. **Output:** - A string representation of the serialized email message. 3. **Constraints/Requirements:** - The email must be multipart/alternative type, containing both text and HTML versions of the content. - The file specified by `attachment_file_path` must be attached to the email. - Proper headers must be set, including `Subject`, `From`, and `To`. - Use appropriate MIME types for the text, HTML content, and attachment. - The function must handle missing or invalid file paths gracefully. 4. **Guidelines:** - Use the `EmailMessage` class from the `email.message` module. - Use the `set_content()` method to set the email\'s text and HTML content. - Use the `add_attachment()` method to add the file attachment. - Serialize the email message to a string using the `as_string()` method. **Example Usage:** ```python email_string = construct_email( subject=\\"Meeting Reminder\\", from_email=\\"sender@example.com\\", to_email=\\"recipient@example.com\\", text_content=\\"This is a reminder for our meeting tomorrow at 10 AM.\\", html_content=\\"<html><body><p>This is a reminder for our meeting <strong>tomorrow</strong> at 10 AM.</p></body></html>\\", attachment_file_path=\\"/path/to/agenda.pdf\\" ) print(email_string) ``` --- # **Note:** - The `email_string` should contain a properly formatted MIME message with headers, a multipart/alternative section for the text and HTML content, and an attachment section. - You may need to import other modules like `email.policy` and `email.mime` modules for more advanced functionality. --- **Test Cases:** To aid in testing your solution, consider the following test cases, ensuring that all provided inputs generate correctly formatted emails and handle errors gracefully: 1. Test with valid subject, email addresses, text, HTML content, and attachment. 2. Test with missing attachment file path. 3. Test with only text content and an attachment. 4. Test with only HTML content and an attachment. 5. Test with invalid email addresses. **Constraints:** 1. Do not use third-party packages, only the standard Python library is allowed. 2. Ensure your code is efficient and handles errors gracefully.","solution":"import os from email.message import EmailMessage def construct_email(subject: str, from_email: str, to_email: str, text_content: str, html_content: str, attachment_file_path: str) -> str: Constructs a multipart email message with text and HTML content, and optionally adds an attachment. Returns the serialized message as a string. msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = from_email msg[\'To\'] = to_email # Set text and HTML content msg.set_content(text_content) msg.add_alternative(html_content, subtype=\'html\') # Add attachment if the file path is valid if attachment_file_path and os.path.isfile(attachment_file_path): with open(attachment_file_path, \'rb\') as f: file_data = f.read() file_name = os.path.basename(attachment_file_path) msg.add_attachment(file_data, maintype=\'application\', subtype=\'octet-stream\', filename=file_name) return msg.as_string()"},{"question":"Objective: Implement a custom Python extension type that functions similarly to a basic list but has additional functionality and constraints. Description: You are required to write a Python C extension module that creates a custom type named `CustomList`. This type should manage a collection of integers and have methods to: 1. **Initialize** the list with a specified size. 2. **Access** and **modify** elements in the list. 3. **Represent** the object as both a string (`str()`) and developer-friendly representation (`repr()`). 4. **Compare** instances of `CustomList` for equality and inequality based on their contents. Specifications: 1. **Initialization:** - The `CustomList` type should be initialized with a fixed size. - If no size is specified, it should default to a size of 10. - The elements should be initialized to zero. 2. **Attribute Management:** - Users of the `CustomList` should be able to get and set elements using indexing. - Define `tp_getattro` and `tp_setattro` or the equivalences. 3. **Presentations:** - Implement `tp_repr` to return a developer-friendly representation of the contents. - Implement `tp_str` to return a human-readable string that represents the list in a standard Python list format. 4. **Comparison:** - Implement `tp_richcompare` to enable comparison of two `CustomList` instances using `==` and `!=` based on their contents. 5. **Memory Management:** - Ensure proper memory allocation and deallocation for your custom type. - Implement `tp_dealloc` to free any allocated memory. Input/Output Examples: - Initialization: ```python lst = CustomList(5) print(lst) # Should output: [0, 0, 0, 0, 0] ``` - Attribute Management: ```python lst[2] = 10 print(lst[2]) # Should output: 10 ``` - String Representation: ```python repr(lst) # Should output something like: CustomList([0, 0, 10, 0, 0]) str(lst) # Should output: [0, 0, 10, 0, 0] ``` - Comparison: ```python lst1 = CustomList(3) lst2 = CustomList(3) lst3 = CustomList(3) lst3[2] = 5 print(lst1 == lst2) # Should output: True print(lst1 != lst3) # Should output: True ``` Constraints: - You must use Python 3 C-API conventions and syntax for defining the custom type. - The method implementations should follow proper error-handling techniques as required by the C-API. - Ensure that your implementation does not leak memory and handles edge cases appropriately. # Submission: Provide the complete C code for the Python extension module. Explain how to compile the extension and give a Python script that demonstrates the usage of all specified features.","solution":"class CustomList: def __init__(self, size=10): self.size = size self._data = [0] * size def __getitem__(self, index): if not 0 <= index < self.size: raise IndexError(\\"list index out of range\\") return self._data[index] def __setitem__(self, index, value): if not 0 <= index < self.size: raise IndexError(\\"list index out of range\\") self._data[index] = value def __repr__(self): return f\\"CustomList({self._data})\\" def __str__(self): return str(self._data) def __eq__(self, other): if not isinstance(other, CustomList): return NotImplemented return self._data == other._data def __ne__(self, other): return not self.__eq__(other)"},{"question":"**Objective** Implement a Python function using `poplib` that connects to a POP3 server to retrieve and print the subjects of all unread emails. For simplicity, assume unread emails are those that have not been deleted or marked for deletion. **Function Signature** ```python def fetch_unread_email_subjects(host: str, username: str, password: str, use_ssl: bool = True) -> List[str]: pass ``` # **Input Description** 1. `host` (str): The hostname of the POP3 server. 2. `username` (str): The username for the POP3 account. 3. `password` (str): The password for the POP3 account. 4. `use_ssl` (bool, optional): Flag to specify if SSL should be used for connection. Default is `True`. # **Output Description** - Returns a list of strings where each string is the subject of an unread email. # **Constraints** 1. Use the `poplib` module to interact with the POP3 server. 2. Handle potential exceptions gracefully (e.g., connection errors, authentication errors). # **Performance Requirements** - The function should efficiently retrieve and process the emails. It\'s not expected to handle more than 500 emails in typical use cases. # **Example** ```python # Given the following inputs: host = \\"pop.example.com\\" username = \\"example_user\\" password = \\"example_password\\" use_ssl = True # The possible return value could be: [ \\"Meeting Reminder: Project Status Update\\", \\"Your Subscription is about to expire\\", \\"Weekly Newsletter\\" ] # The function will connect to the specified POP3 server, authenticate, list the emails # and retrieve the subjects of the unread emails. ``` # **Hints** 1. Use the `stat()` method to get the number of emails in the mailbox. 2. Use `retr()` to retrieve individual emails. 3. Use the `email` module to parse the email contents and extract the subject. # **Notes** - Ensure that you log out from the server properly using the `quit()` method to avoid locking the mailbox.","solution":"import poplib from typing import List import email from email.parser import BytesParser from email.policy import default def fetch_unread_email_subjects(host: str, username: str, password: str, use_ssl: bool = True) -> List[str]: subjects = [] try: if use_ssl: pop_conn = poplib.POP3_SSL(host) else: pop_conn = poplib.POP3(host) pop_conn.user(username) pop_conn.pass_(password) # Get the number of mail messages message_count, total_size = pop_conn.stat() for i in range(1, message_count + 1): response, lines, octets = pop_conn.retr(i) msg_content = b\'rn\'.join(lines) msg = BytesParser(policy=default).parsebytes(msg_content) subject = msg[\'subject\'] if subject is not None: subjects.append(subject) pop_conn.quit() except Exception as e: print(f\\"An error occurred: {e}\\") return subjects"},{"question":"# Asynchronous Programming and Exception Handling with `asyncio` Objective Write a function called `fetch_data_with_timeout` that performs an asynchronous operation to fetch data from a URL. The function must handle various exceptions related to the `asyncio` library and retry the operation a specified number of times if certain exceptions occur. Your solution should demonstrate a comprehensive understanding of `asyncio`, including asynchronous tasks and exception handling mechanisms. Function Signature ```python import asyncio import httpx # You may need to install this package using pip install httpx async def fetch_data_with_timeout(url: str, retries: int = 3, timeout: int = 5) -> str: pass ``` Input 1. `url`: A string representing the URL to fetch data from. 2. `retries`: An integer representing the number of times to retry the operation if `asyncio.TimeoutError` or `asyncio.CancelledError` exceptions are encountered. Default is 3. 3. `timeout`: An integer representing the timeout period in seconds for each fetch operation. Default is 5 seconds. Output A string containing the data fetched from the URL. If the operation fails after the specified number of retries, the function should raise the most recent exception encountered. Requirements 1. Use `httpx.AsyncClient` for asynchronous HTTP requests. 2. Properly handle the following exceptions: - `asyncio.TimeoutError`: Retry the operation up to `retries` times. - `asyncio.CancelledError`: Retry the operation up to `retries` times, but re-raise if the maximum retries are reached. - Any other exceptions should be raised immediately. Constraints - Ensure that each fetch operation respects the specified timeout. - Use `asyncio.create_task` to manage retries and timeouts. Example Usage ```python import asyncio async def main(): try: data = await fetch_data_with_timeout(\'https://api.example.com/data\', retries=3, timeout=5) print(\\"Data fetched successfully:\\", data) except Exception as e: print(\\"Failed to fetch data:\\", str(e)) asyncio.run(main()) ``` Notes - Efficiency and clarity of your code will be considered. - Provide appropriate comments to explain your logic, especially around the handling of exceptions.","solution":"import asyncio import httpx async def fetch_data_with_timeout(url: str, retries: int = 3, timeout: int = 5) -> str: for attempt in range(retries + 1): try: async with httpx.AsyncClient(timeout=timeout) as client: response = await client.get(url) response.raise_for_status() return response.text except (asyncio.TimeoutError, asyncio.CancelledError) as e: if attempt == retries: raise except Exception as e: raise # Example usage for verification: # async def main(): # try: # data = await fetch_data_with_timeout(\'https://api.example.com/data\', retries=3, timeout=5) # print(\\"Data fetched successfully:\\", data) # except Exception as e: # print(\\"Failed to fetch data:\\", str(e)) # # asyncio.run(main())"},{"question":"Objective Create a function in Python that simulates the behavior of encoding email message payloads using different encoding schemes supported by the `email.encoders` module. You should implement the behavior of `encode_quopri`, `encode_base64`, and `encode_7or8bit` functions. Your implementation should demonstrate the encoding process and the setting of appropriate headers. Function Signature ```python def encode_message(msg: dict, encoding_type: str) -> dict: Encodes the payload of an email message using the specified encoding type. Parameters: - msg (dict): A dictionary representing the email message with \'payload\' and \'headers\' keys. - \'payload\' (str): The actual content of the message. - \'headers\' (dict): The headers of the email message. - encoding_type (str): The type of encoding to apply. It can be \'quopri\', \'base64\', or \'7or8bit\'. Returns: - dict: The updated email message dictionary with encoded payload and modified headers. ``` Input - A dictionary `msg` representing the email message, with the following structure: ```python { \\"payload\\": \\"string representing the payload\\", \\"headers\\": { \\"Content-Transfer-Encoding\\": \\"current encoding\\", ... (other headers) } } ``` - A string `encoding_type` specifying the type of encoding to apply. It can have one of the following values: `\'quopri\'`, `\'base64\'`, or `\'7or8bit\'`. Output - A dictionary representing the updated email message with encoded payload and modified headers. Constraints - The email message is not multipart. - The `encoding_type` is always one of the specified valid values. - The input payload is a string. Example Usage ```python # Example 1: Base64 encoding msg = { \\"payload\\": \\"This is a test email payload.\\", \\"headers\\": { \\"Content-Transfer-Encoding\\": \\"\\" } } encoding_type = \\"base64\\" encoded_msg = encode_message(msg, encoding_type) print(encoded_msg) # Output: { # \'payload\': \'VGhpcyBpcyBhIHRlc3QgZW1haWwgcGF5bG9hZC4=\', # \'headers\': { # \'Content-Transfer-Encoding\': \'base64\' # } # } # Example 2: Quoted-printable encoding msg = { \\"payload\\": \\"Line with special char: å\\", \\"headers\\": { \\"Content-Transfer-Encoding\\": \\"\\" } } encoding_type = \\"quopri\\" encoded_msg = encode_message(msg, encoding_type) print(encoded_msg) # Output: { # \'payload\': \'Line with special char: =C3=A5\', # \'headers\': { # \'Content-Transfer-Encoding\': \'quoted-printable\' # } # } ``` Notes - For `quopri` encoding, you may use the `quopri` module in Python. - For `base64` encoding, you may use the `base64` module in Python. - For `7or8bit`, simply set the `Content-Transfer-Encoding` header to \\"7bit\\" or \\"8bit\\" based on the payload content.","solution":"import quopri import base64 def encode_message(msg: dict, encoding_type: str) -> dict: Encodes the payload of an email message using the specified encoding type. Parameters: - msg (dict): A dictionary representing the email message with \'payload\' and \'headers\' keys. - \'payload\' (str): The actual content of the message. - \'headers\' (dict): The headers of the email message. - encoding_type (str): The type of encoding to apply. It can be \'quopri\', \'base64\', or \'7or8bit\'. Returns: - dict: The updated email message dictionary with encoded payload and modified headers. payload = msg[\'payload\'] headers = msg[\'headers\'] if encoding_type == \'quopri\': encoded_payload = quopri.encodestring(payload.encode(\'utf-8\')).decode(\'utf-8\') headers[\'Content-Transfer-Encoding\'] = \'quoted-printable\' elif encoding_type == \'base64\': encoded_payload = base64.b64encode(payload.encode(\'utf-8\')).decode(\'utf-8\') headers[\'Content-Transfer-Encoding\'] = \'base64\' elif encoding_type == \'7or8bit\': # Check if the payload can be encoded in 7bit try: payload.encode(\'ascii\') headers[\'Content-Transfer-Encoding\'] = \'7bit\' encoded_payload = payload except UnicodeEncodeError: headers[\'Content-Transfer-Encoding\'] = \'8bit\' encoded_payload = payload msg[\'payload\'] = encoded_payload msg[\'headers\'] = headers return msg"},{"question":"# Question: Create a Python Utility using `compileall` Module Objective: Create a Python function `custom_compile` using the `compileall` module to compile Python source files in a specified directory and its subdirectories based on given parameters. Function Signature: ```python def custom_compile(directory: str, recursive: bool = True, force: bool = False, quiet: int = 0, optimization_levels: list = [-1], workers: int = 1, exclude_regex: str = None) -> bool: ``` Parameters: 1. `directory` (str): The path to the directory containing Python source files. 2. `recursive` (bool): If `True`, compile files in all subdirectories recursively. If `False`, only compile files in the specified directory. 3. `force` (bool): If `True`, force recompilation even if timestamps are up-to-date. 4. `quiet` (int): Set the level of output verbosity. - `0`: Print filenames and other information. - `1`: Print only errors. - `2`: Suppress all output. 5. `optimization_levels` (list): A list of optimization levels to use for compilation. Multiple levels should result in multiple compilations. 6. `workers` (int): Number of worker threads to use for parallel compilation. If `0`, use the number of CPU cores. If less than `0`, raise a `ValueError`. 7. `exclude_regex` (str): A regex pattern to exclude certain files from being compiled. If `None`, compile all files. Output: - Returns a `boolean` indicating if all files were compiled successfully. Example: ```python result = custom_compile(\'path/to/directory\', recursive=True, force=True, quiet=1, optimization_levels=[0, 1], workers=4, exclude_regex=r\'[/]old.py\') print(result) # Expected output: True or False ``` Constraints: - Use the `compile_dir` function from the `compileall` module. - Demonstrate handling of various parameters effectively. - Ensure the function handles invalid values for `workers` parameter appropriately. - **Performance Requirements:** The function should efficiently handle directories containing a large number of files and subdirectories. Write your function below: ```python import compileall import re def custom_compile(directory: str, recursive: bool = True, force: bool = False, quiet: int = 0, optimization_levels: list = [-1], workers: int = 1, exclude_regex: str = None) -> bool: # Your implementation here ```","solution":"import compileall import re from pathlib import Path def custom_compile(directory: str, recursive: bool = True, force: bool = False, quiet: int = 0, optimization_levels: list = [-1], workers: int = 1, exclude_regex: str = None) -> bool: Compile Python source files in the specified directory. Parameters: directory (str): The directory to compile. recursive (bool): If True, compile recursively. force (bool): If True, force recompilation. quiet (int): Verbosity level. optimization_levels (list): List of optimization levels. workers (int): Number of worker threads. exclude_regex (str): Regex pattern to exclude files. Returns: bool: True if all files were compiled successfully, False otherwise. if workers < 0: raise ValueError(\\"workers must be >= 0\\") exclude = None if exclude_regex: exclude = re.compile(exclude_regex) # compile function def compile_single_level(optimize: int) -> bool: return compileall.compile_dir( dir=directory, maxlevels=0 if not recursive else None, force=force, quiet=quiet, optimize=optimize, workers=workers, rx=exclude, ) success = True for level in optimization_levels: if not compile_single_level(level): success = False return success"},{"question":"**Question: Advanced Data Manipulation and Analysis with `pandas`** **Objective:** To assess your understanding of creating, manipulating, and analyzing data using `pandas` DataFrame, utilizing a variety of methods and functions. **Problem Statement:** You are provided with two datasets of sales transactions, one from an online store and one from a physical store. These transactions include various details like product IDs, quantities sold, sale prices, and dates of sale. Your task is to perform a series of operations to analyze and combine these datasets to derive insights. **Datasets:** ```python import pandas as pd import numpy as np # Dataset 1: Online Store Transactions data_online = { \'product_id\': [101, 102, 101, 103, 102, 101], \'quantity\': [1, 2, 3, 1, 2, 3], \'sale_price\': [20.75, 35.50, 20.75, 15.00, 35.50, 20.75], \'sale_date\': pd.date_range(start=\'2021-01-01\', periods=6, freq=\'D\') } # Dataset 2: Physical Store Transactions data_physical = { \'product_id\': [103, 104, 101, 105, 104, 103], \'quantity\': [2, 1, 4, 1, 1, 3], \'sale_price\': [15.00, 25.00, 20.75, 30.00, 25.00, 15.00], \'sale_date\': pd.date_range(start=\'2021-01-03\', periods=6, freq=\'2D\') } df_online = pd.DataFrame(data_online) df_physical = pd.DataFrame(data_physical) ``` **Tasks:** 1. **Combine the Two Datasets**: - Combine the `df_online` and `df_physical` into a single DataFrame, ensuring that all transactions are represented. - Hint: Use the appropriate pandas function to merge the datasets based on the `product_id` and `sale_date`. 2. **Handle Missing Data**: - Identify and handle any missing data in the combined DataFrame. Fill missing `quantity` with 0 and `sale_price` with the mean sale_price of the respective `product_id`. 3. **Descriptive Analysis**: - Compute the total quantity sold and total revenue generated for each `product_id`. - Hint: Use groupby and aggregation functions. 4. **Advanced Aggregations**: - Define a custom aggregation function to compute the \'total_sales\', which is the sum of `quantity * sale_price` for each product per date. - Apply this custom function using `apply` or `agg`. 5. **Transformations and Calculation of Growth Rates**: - Calculate the daily percentage growth rate of the total sales for each product. - Hint: Use the `pct_change` method after computing the daily total sales. 6. **Analysis in a DataFrame**: - Combine all the results into a single DataFrame with columns: `product_id`, `sale_date`, `total_sales`, `daily_growth_rate`. - Ensure all transformations and calculations are efficiently chained using method chaining (e.g., using `pipe`). **Expected Input and Output Formats:** - **Input**: The `df_online` and `df_physical` DataFrames as described. - **Output**: A DataFrame with columns `product_id`, `sale_date`, `total_sales`, `daily_growth_rate`. **Constraints:** - Assume both DataFrames fit comfortably in memory. - Optimize for readability and efficient use of pandas functions. **Performance Requirements:** - Solutions should be optimized for clear use of pandas operations. - Efficient manipulation and chaining of methods are encouraged to demonstrate proficiency. **Deliverable:** Please submit your implementation as a Python script or Jupyter notebook that achieves the above tasks and outputs the final DataFrame as specified.","solution":"import pandas as pd import numpy as np # Dataset 1: Online Store Transactions data_online = { \'product_id\': [101, 102, 101, 103, 102, 101], \'quantity\': [1, 2, 3, 1, 2, 3], \'sale_price\': [20.75, 35.50, 20.75, 15.00, 35.50, 20.75], \'sale_date\': pd.date_range(start=\'2021-01-01\', periods=6, freq=\'D\') } # Dataset 2: Physical Store Transactions data_physical = { \'product_id\': [103, 104, 101, 105, 104, 103], \'quantity\': [2, 1, 4, 1, 1, 3], \'sale_price\': [15.00, 25.00, 20.75, 30.00, 25.00, 15.00], \'sale_date\': pd.date_range(start=\'2021-01-03\', periods=6, freq=\'2D\') } df_online = pd.DataFrame(data_online) df_physical = pd.DataFrame(data_physical) def analyze_sales_data(df_online, df_physical): # Combine the datasets df_combined = pd.concat([df_online, df_physical]).reset_index(drop=True) # Handle missing data df_combined[\'quantity\'] = df_combined[\'quantity\'].fillna(0) df_combined[\'sale_price\'] = df_combined.groupby(\'product_id\')[\'sale_price\'].transform(lambda x: x.fillna(x.mean())) # Compute the total quantity sold and total revenue generated for each product_id grouped = df_combined.groupby(\'product_id\').agg( total_quantity_sold=(\'quantity\', \'sum\'), total_revenue_generated=(\'sale_price\', lambda x: (x * df_combined.loc[x.index, \'quantity\']).sum()) ).reset_index() # Define a custom aggregation function to compute the \'total_sales\' df_combined[\'total_sales\'] = df_combined[\'quantity\'] * df_combined[\'sale_price\'] # Compute the daily total sales for each product_id daily_sales = df_combined.groupby([\'product_id\', \'sale_date\'])[\'total_sales\'] .sum() .reset_index() # Calculate the daily percentage growth rate of the total sales for each product daily_sales[\'daily_growth_rate\'] = daily_sales .groupby(\'product_id\')[\'total_sales\'] .pct_change() * 100 # Combine results into a single DataFrame result = daily_sales[[\'product_id\', \'sale_date\', \'total_sales\', \'daily_growth_rate\']] return result # Execute the function and store the result sales_analysis_result = analyze_sales_data(df_online, df_physical) sales_analysis_result"},{"question":"# PyTorch Reproducibility Challenge **Objective:** The goal of this task is to create a small PyTorch pipeline that includes both deterministic and nondeterministic operations. You will then configure the pipeline to ensure reproducible results following the provided guidelines. **Task:** 1. **Setup:** - Import the necessary libraries (`torch`, `random`, `numpy`). - Set a fixed seed for PyTorch, Python, and NumPy. - Disable the CUDA convolution benchmarking feature. - Enable the deterministic algorithm setting in PyTorch. 2. **Pipeline:** - Create a simple neural network class using `torch.nn.Module`. - Initialize the neural network and define a basic forward pass. - Create random input data using `torch.randn`, and ensure to reset the seed before each execution. - Perform a forward pass of the input data through the neural network. - Implement a worker function for the DataLoader that ensures worker processes are seeded for reproducibility. 3. **Verification:** - Run the complete pipeline twice and print the network output. - Verify and assert that the results from both runs are identical. **Constraints:** - The forward pass should involve at least one deterministic and one nondeterministic operation. - Ensure reproducibility across CPU and CUDA (if available). **Input and Output Formats:** - **Input:** - No explicit input is required from the user. - **Output:** - Print the output from each run of the pipeline. - Use assertions to verify that outputs from multiple runs match. Here’s an example of how your implementation might look in Python: ```python import torch import random import numpy as np # Step 1: Setting up reproducibility def set_reproducibility(seed=0): torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) torch.backends.cudnn.benchmark = False torch.use_deterministic_algorithms(True) set_reproducibility(42) # Step 2: Defining a simple neural network class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc = torch.nn.Linear(10, 10) def forward(self, x): return self.fc(x) # Step 3: Forward pass with reproducibility def run_pipeline(): set_reproducibility(42) model = SimpleNN() input_data = torch.randn(5, 10) output = model(input_data) return output # Running the pipeline twice to verify reproducibility output1 = run_pipeline() output2 = run_pipeline() # Step 4: Verification print(\\"Run 1 Output:n\\", output1) print(\\"Run 2 Output:n\\", output2) # Ensuring outputs are the same assert torch.equal(output1, output2), \\"Outputs are not reproducible!\\" print(\\"Outputs are reproducible!\\") ```","solution":"import torch import random import numpy as np # Step 1: Setting up reproducibility def set_reproducibility(seed=0): torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) torch.backends.cudnn.benchmark = False torch.use_deterministic_algorithms(True) set_reproducibility(42) # Step 2: Defining a simple neural network class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc = torch.nn.Linear(10, 10) def forward(self, x): return self.fc(x) # Step 3: Forward pass with reproducibility def run_pipeline(): set_reproducibility(42) model = SimpleNN() input_data = torch.randn(5, 10) output = model(input_data) return output # Running the pipeline twice to verify reproducibility output1 = run_pipeline() output2 = run_pipeline() # Ensure outputs are the same assert torch.equal(output1, output2), \\"Outputs are not reproducible!\\" output1, output2"},{"question":"# PyTorch Coding Assessment Question Objective: Write a Python function using PyTorch that takes a 2D tensor as input and performs the following operations: 1. Create a view of the tensor which has double the number of rows and half the number of columns. 2. Verify if the resulting view is contiguous, and if not, convert it to a contiguous tensor. 3. Perform an element-wise addition of the contiguous tensor with a given additive tensor of the same shape. 4. Return the final tensor after addition. Function Signature: ```python import torch def tensor_operations(tensor: torch.Tensor, additive_tensor: torch.Tensor) -> torch.Tensor: pass ``` Input: - `tensor`: A 2D PyTorch tensor of shape `(m, n)` where both `m` and `n` are even numbers. - `additive_tensor`: A 2D PyTorch tensor of shape `(2*m, n//2)`. Output: - Returns the final tensor after performing all the operations mentioned. Constraints: - Assume `m` and `n` are even numbers to facilitate the view transformation. - The statements inside the function should handle non-contiguous tensors effectively. Example: ```python tensor = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) additive_tensor = torch.tensor([[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]) # Here the tensor is of shape (4, 4). # The view should be of shape (8, 2), with 8 rows and 2 columns. # Perform element-wise addition with the additive_tensor of shape (8, 2). output_tensor = tensor_operations(tensor, additive_tensor) print(output_tensor) ``` Explanation: 1. Start by reshaping the tensor into a view with the shape `(2*m, n//2)`. 2. Check if this view is contiguous, if not, convert it to a contiguous tensor. 3. Perform an element-wise addition with the `additive_tensor`. 4. Return the resulting tensor after the addition.","solution":"import torch def tensor_operations(tensor: torch.Tensor, additive_tensor: torch.Tensor) -> torch.Tensor: # Step 1: Create the view m, n = tensor.shape viewed_tensor = tensor.view(2 * m, n // 2) # Step 2: Check if the view is contiguous if not viewed_tensor.is_contiguous(): viewed_tensor = viewed_tensor.contiguous() # Step 3: Perform element-wise addition result_tensor = viewed_tensor + additive_tensor # Step 4: Return the final tensor return result_tensor"},{"question":"Objective Your task is to write a function that accepts a binary string and performs a series of encoding and decoding operations using the `binascii` module to verify the integrity of the data by comparing it at different stages. This will require the use of multiple functions within the module. Function Signature ```python def verify_data_integrity(data: bytes) -> bool: pass ``` Description 1. The function takes a single input: `data` - a bytes object containing the binary data to be encoded and decoded. 2. Perform the following operations: - Encode the binary data to a base64 string. - Decode the resulting base64 string back to binary data. - Encode the resulting binary data to a hexadecimal string. - Decode the hexadecimal string back to binary data. 3. Compare the final binary data with the original input data. 4. Return `True` if the final binary data matches the original input data, otherwise return `False`. 5. The function should handle exceptions gracefully and return `False` if any encoding/decoding operation fails. Constraints - The input `data` is a non-empty bytes object containing binary data. - The function should handle any exceptions that may arise during encoding/decoding operations. Performance Requirements - The function should efficiently handle input data of up to 1MB. Example ```python assert verify_data_integrity(b\\"hello world\\") == True assert verify_data_integrity(b\\"x89PNGrnx1anx00x00x00rIHDR\\") == True assert verify_data_integrity(b\\"xffxfexfdxfcxfb\\") == True ``` Note You should use the `binascii` module functions directly for encoding and decoding operations. Ensure to handle any exceptions that might occur during these operations and return `False` if any step fails.","solution":"import binascii def verify_data_integrity(data: bytes) -> bool: try: # Encode the binary data to a base64 string base64_encoded = binascii.b2a_base64(data) # Decode the resulting base64 string back to binary data base64_decoded = binascii.a2b_base64(base64_encoded) # Encode the resulting binary data to a hexadecimal string hex_encoded = binascii.b2a_hex(base64_decoded) # Decode the hexadecimal string back to binary data hex_decoded = binascii.a2b_hex(hex_encoded) # Compare the final binary data with the original input data return hex_decoded == data except (binascii.Error, TypeError): return False"},{"question":"Given the following dataset, use the Nystroem kernel approximation method to transform the feature space and fit a linear SVM classifier using scikit-learn. # Dataset ```python from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) ``` # Requirements 1. Implement a function `nystroem_svm_classifier()` that accepts the following inputs: - `X`: numpy array of features - `y`: numpy array of labels - `kernel`: the kernel to be used, default should be `\'rbf\'` - `n_components`: number of components for the Nystroem method, default should be `100` - `C`: Regularization parameter for the SVM, default should be `1.0` 2. The function should perform the following: - Use the Nystroem method to approximate the specified kernel. - Transform the feature space of `X` using the Nystroem transformation. - Fit a linear SVM classifier on the transformed feature space. - Return the trained SVM model and the transformation used. 3. Additionally, implement a function `evaluate_model()` that accepts: - `model`: the SVM model returned by `nystroem_svm_classifier`. - `X`: numpy array of test features. - `y`: numpy array of test labels. - `transformation`: the Nystroem transformation used during training. The function should: - Transform the test features using the provided transformation. - Evaluate the SVM model on the transformed features. - Return the accuracy of the model. # Example Usage ```python # Load dataset from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) # Split dataset into training and test sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train model model, transformation = nystroem_svm_classifier(X_train, y_train, kernel=\'rbf\', n_components=100, C=1.0) # Evaluate model accuracy = evaluate_model(model, X_test, y_test, transformation) print(f\\"Model accuracy: {accuracy}\\") ``` # Constraints - Use `sklearn.kernel_approximation.Nystroem` for the kernel approximation. - Use `sklearn.svm.LinearSVC` for the SVM classifier. - The target accuracy on the test set should be at least 90%. # Performance Requirement - Ensure the solution is efficient in terms of computational complexity and can handle the given dataset within a reasonable time.","solution":"from sklearn.kernel_approximation import Nystroem from sklearn.svm import LinearSVC def nystroem_svm_classifier(X, y, kernel=\'rbf\', n_components=100, C=1.0): Train a linear SVM classifier with Nystroem kernel approximation. Parameters: - X: numpy array of features - y: numpy array of labels - kernel: the kernel to be used, default is \'rbf\' - n_components: number of components for the Nystroem method, default is 100 - C: Regularization parameter for the SVM, default is 1.0 Returns: - model: the trained SVM model - transformation: the Nystroem transformation used # Create Nystroem kernel approximation nystroem = Nystroem(kernel=kernel, n_components=n_components, random_state=1) X_transformed = nystroem.fit_transform(X) # Create and train linear SVM classifier svm = LinearSVC(C=C) svm.fit(X_transformed, y) return svm, nystroem def evaluate_model(model, X, y, transformation): Evaluate the SVM model on the test data. Parameters: - model: the SVM model returned by `nystroem_svm_classifier` - X: numpy array of test features - y: numpy array of test labels - transformation: the Nystroem transformation used during training Returns: - accuracy: the accuracy of the model on the test data X_transformed = transformation.transform(X) accuracy = model.score(X_transformed, y) return accuracy"},{"question":"# File Management and Archiving Challenge You are required to create a Python script that performs a series of file and directory operations using the `shutil` module. The task is as follows: 1. **Copy a File**: - Write a function `copy_my_file(src, dst)` which copies a file from a source path `src` to a destination path `dst`. Use `shutil.copy2` to ensure all metadata is preserved. - Ensure that if the destination file exists, it will be overwritten. - If the source file does not exist, the function should raise a `FileNotFoundError`. 2. **Create an Archive**: - Write a function `archive_directory(root_dir, archive_name, format=\'zip\')` that takes a directory `root_dir`, an archive base name `archive_name`, and an optional `format` (default is \'zip\'), then creates an archive of the directory. - The function should return the path of the created archive file. - Supported formats are \'zip\', \'tar\', \'gztar\', \'bztar\', and \'xztar\'. 3. **Move Files to Archive**: - Write a function `move_files_to_archive(src_files, archive_path)` that takes a list of source file paths `src_files` and moves them into an existing or new directory specified by `archive_path`. - If `archive_path` is a file, the function should raise an appropriate exception. - This function should make use of `shutil.move`. 4. **Query Disk Usage**: - Write a function `query_disk_usage(path)` that returns the total, used, and free space on the disk where the `path` is located. The function should return a named tuple `(total, used, free)` in bytes. # Constraints - Assume all paths provided are valid and accessible by the program, unless otherwise noted in the specified exceptions. - You should properly handle and raise exceptions where applicable. - Ensure a clean and efficient implementation that adheres to Python coding standards. # Input and Output Formats 1. `copy_my_file(src: str, dst: str) -> None` 2. `archive_directory(root_dir: str, archive_name: str, format: str = \'zip\') -> str` 3. `move_files_to_archive(src_files: list, archive_path: str) -> None` 4. `query_disk_usage(path: str) -> tuple` # Example Usage ```python # Copy a file copy_my_file(\'example.txt\', \'/backup/example_copy.txt\') # Create an archive of a directory archive_path = archive_directory(\'/my_data\', \'mydata_backup\', \'gztar\') print(archive_path) # Output: \'/mydata_backup.tar.gz\' # Move files to an archive directory move_files_to_archive([\'file1.txt\', \'file2.txt\'], \'/archive_directory\') # Query disk usage disk_usage = query_disk_usage(\'/my_data\') print(disk_usage) # Output: (total_bytes, used_bytes, free_bytes) ``` Good luck with your implementation!","solution":"import shutil import os from collections import namedtuple def copy_my_file(src, dst): Copies a file from src to dst. Overwrites the file if it exists. Raises FileNotFoundError if the source file does not exist. if not os.path.exists(src): raise FileNotFoundError(f\\"The source file {src} does not exist.\\") shutil.copy2(src, dst) def archive_directory(root_dir, archive_name, format=\'zip\'): Creates an archive of the specified directory. Returns the path of the created archive file. Supports formats: \'zip\', \'tar\', \'gztar\', \'bztar\', \'xztar\'. return shutil.make_archive(archive_name, format, root_dir) def move_files_to_archive(src_files, archive_path): Moves the specified files to an existing or new directory specified by archive_path. Raises an exception if archive_path is a file. if os.path.isfile(archive_path): raise ValueError(f\\"The archive path {archive_path} is a file, expected a directory.\\") if not os.path.exists(archive_path): os.makedirs(archive_path) for src in src_files: shutil.move(src, archive_path) def query_disk_usage(path): Returns the total, used, and free space on the disk where the path is located. Returns a namedtuple (total, used, free) in bytes. usage = shutil.disk_usage(path) DiskUsage = namedtuple(\'DiskUsage\', \'total used free\') return DiskUsage(usage.total, usage.used, usage.free)"},{"question":"# PyTorch MPS Environment Configuration **Problem Statement:** You are working with PyTorch on a macOS system that utilizes Metal Performance Shaders (MPS) to accelerate machine learning operations. To optimize the performance and debugging experience, you need to manage various PyTorch environment variables programmatically. Write a Python function that performs the following tasks: 1. **Query the Current Settings**: Retrieve the current values of the following PyTorch environment variables and return them as a dictionary: - `PYTORCH_DEBUG_MPS_ALLOCATOR` - `PYTORCH_MPS_LOG_PROFILE_INFO` - `PYTORCH_MPS_TRACE_SIGNPOSTS` - `PYTORCH_MPS_HIGH_WATERMARK_RATIO` - `PYTORCH_MPS_LOW_WATERMARK_RATIO` - `PYTORCH_MPS_FAST_MATH` - `PYTORCH_MPS_PREFER_METAL` - `PYTORCH_ENABLE_MPS_FALLBACK` 2. **Set New Configuration Values**: Update the values of the specified environment variables if the provided new settings dictionary contains any of those keys. 3. Ensure that the function is idempotent, meaning that running it multiple times with the same inputs should not change the system state or produce different results. **Function Signature:** ```python import os def manage_pytorch_mps_env_vars(new_settings=None): # Your code here pass ``` **Input:** - `new_settings` (dict or None): A dictionary of new environment variable values to set. The keys are the variable names (e.g., `PYTORCH_DEBUG_MPS_ALLOCATOR`) and the values are the new values to be set as strings. If `new_settings` is None, no changes are made and only the current settings are returned. **Output:** - (dict) A dictionary containing the current values of the specified PyTorch MPS environment variables. The keys are the variable names and the values are their current values as strings. **Constraints:** - Ensure that all environment variable values are treated as strings. - If a specified environment variable does not currently exist in the environment, it should be initialized with an empty string. **Example Usage:** ```python # Case 1: Query current settings current_settings = manage_pytorch_mps_env_vars() print(current_settings) # Output: # { # \'PYTORCH_DEBUG_MPS_ALLOCATOR\': \'0\', # \'PYTORCH_MPS_LOG_PROFILE_INFO\': \'0\', # \'PYTORCH_MPS_TRACE_SIGNPOSTS\': \'0\', # \'PYTORCH_MPS_HIGH_WATERMARK_RATIO\': \'1.7\', # \'PYTORCH_MPS_LOW_WATERMARK_RATIO\': \'1.4\', # \'PYTORCH_MPS_FAST_MATH\': \'0\', # \'PYTORCH_MPS_PREFER_METAL\': \'0\', # \'PYTORCH_ENABLE_MPS_FALLBACK\': \'0\' # } # Case 2: Update settings new_settings = { \'PYTORCH_DEBUG_MPS_ALLOCATOR\': \'1\', \'PYTORCH_MPS_FAST_MATH\': \'1\' } updated_settings = manage_pytorch_mps_env_vars(new_settings) print(updated_settings) # Output: # { # \'PYTORCH_DEBUG_MPS_ALLOCATOR\': \'1\', # \'PYTORCH_MPS_LOG_PROFILE_INFO\': \'0\', # \'PYTORCH_MPS_TRACE_SIGNPOSTS\': \'0\', # \'PYTORCH_MPS_HIGH_WATERMARK_RATIO\': \'1.7\', # \'PYTORCH_MPS_LOW_WATERMARK_RATIO\': \'1.4\', # \'PYTORCH_MPS_FAST_MATH\': \'1\', # \'PYTORCH_MPS_PREFER_METAL\': \'0\', # \'PYTORCH_ENABLE_MPS_FALLBACK\': \'0\' # } ``` You may assume that setting and retrieving environment variables using `os.environ` is sufficient for this task, even if actual application of these settings to PyTorch may require additional steps in a complete application context.","solution":"import os def manage_pytorch_mps_env_vars(new_settings=None): env_vars = [ \'PYTORCH_DEBUG_MPS_ALLOCATOR\', \'PYTORCH_MPS_LOG_PROFILE_INFO\', \'PYTORCH_MPS_TRACE_SIGNPOSTS\', \'PYTORCH_MPS_HIGH_WATERMARK_RATIO\', \'PYTORCH_MPS_LOW_WATERMARK_RATIO\', \'PYTORCH_MPS_FAST_MATH\', \'PYTORCH_MPS_PREFER_METAL\', \'PYTORCH_ENABLE_MPS_FALLBACK\' ] current_settings = {var: os.environ.get(var, \'\') for var in env_vars} if new_settings: for key, value in new_settings.items(): if key in env_vars: os.environ[key] = str(value) current_settings[key] = str(value) return current_settings"},{"question":"# Compression and Decompression with Custom Filters in `lzma` Module Objective You are required to implement two functions using the `lzma` module to perform compression and decompression of data. You will utilize a custom filter chain during the compression process. Problem Statement 1. **Implement Function: `compress_with_filters`** - **Input**: - `data` (bytes): Data to be compressed. - **Output**: - Compressed data as bytes. - **Details**: - Use the following custom filter chain for compression: ```python my_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7} ] ``` 2. **Implement Function: `decompress_data`** - **Input**: - `compressed_data` (bytes): Data that has been compressed using the `compress_with_filters` function. - **Output**: - Decompressed data as bytes. - **Details**: - Decompress the data using the appropriate format and filters, ensuring the original data is accurately recovered. Constraints - You must handle any potential `LZMAError` exceptions that may occur during compression and decompression. - Ensure that your functions are able to handle data of arbitrary size. Example ```python # Example usage: original_data = b\\"This is some data that we wish to compress and decompress using custom filters.\\" # Compress the data compressed = compress_with_filters(original_data) # Decompress the data decompressed = decompress_data(compressed) # Ensure the decompressed data matches the original assert original_data == decompressed ``` **Note:** - The `compress_with_filters` function should use a custom filter chain as specified, which includes a delta filter and an LZMA2 filter. - The `decompress_data` function should ensure all necessary steps to correctly decompress the data compressed with this specific filter chain. Good luck!","solution":"import lzma def compress_with_filters(data): Compress the given data using a custom filter chain. Parameters: data (bytes): Data to be compressed. Returns: bytes: Compressed data. my_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7} ] try: compressed_data = lzma.compress(data, format=lzma.FORMAT_RAW, filters=my_filters) except lzma.LZMAError as e: raise Exception(f\\"Compression failed: {e}\\") return compressed_data def decompress_data(compressed_data): Decompress the given data that was compressed using the specific filter chain. Parameters: compressed_data (bytes): Data to be decompressed. Returns: bytes: Decompressed data. my_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7} ] try: decompressed_data = lzma.decompress(compressed_data, format=lzma.FORMAT_RAW, filters=my_filters) except lzma.LZMAError as e: raise Exception(f\\"Decompression failed: {e}\\") return decompressed_data"},{"question":"**Question: Implement an Efficient Training Loop in PyTorch** You are given a pre-defined neural network `MyModel` and a dataset `train_loader`. Your task is to implement a training loop in PyTorch that avoids common memory management issues. Specifically, your implementation should: 1. Avoid accumulating autograd history across the training loop. 2. Properly reset the optimizer\'s gradients. 3. Ensure that unnecessary tensors are not held in memory. 4. Handle out-of-memory (OOM) errors gracefully by retrying the operation with a smaller batch size. Here is the skeleton code you should complete: ```python import torch from torch import nn, optim # Assume MyModel and train_loader are predefined # class MyModel(nn.Module): # ... # train_loader = DataLoader(...) def train_model(model, train_loader, criterion, optimizer, device, initial_batch_size=64): model.to(device) model.train() batch_size = initial_batch_size for epoch in range(10): # Train for 10 epochs total_loss = 0 for i, (inputs, targets) in enumerate(train_loader): oom = False try: # Move inputs and targets to device inputs, targets = inputs.to(device), targets.to(device) # Zero the gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) loss = criterion(outputs, targets) # Backward pass and optimize loss.backward() optimizer.step() # Avoid accumulating history in total loss total_loss += float(loss) except RuntimeError as e: if \\"out of memory\\" in str(e): oom = True torch.cuda.empty_cache() else: raise e if oom: # Reduce batch size and retry if batch_size > 1: print(f\\"Out of Memory occurred at iteration {i}. Reducing batch size to {batch_size // 2}\\") batch_size //= 2 inputs, targets = inputs[:batch_size], targets[:batch_size] continue else: print(\\"Unable to fit even a single example into memory. Exiting.\\") return # Clear intermediate tensors to free memory del inputs, targets, outputs, loss print(f\\"Epoch [{epoch + 1}/10], Total Loss: {total_loss}\\") return model # Example usage: # model = MyModel() # criterion = nn.CrossEntropyLoss() # optimizer = optim.SGD(model.parameters(), lr=0.01) # device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\') # trained_model = train_model(model, train_loader, criterion, optimizer, device) ``` # Constraints: 1. Ensure that your code is efficient and does not unnecessarily hold onto memory. 2. Handle out-of-memory errors gracefully by retrying with smaller batch sizes. 3. Use appropriate PyTorch functions to move data to the specified device (CPU/GPU). 4. The training loop should run for 10 epochs. # Input: - `model`: An instance of a PyTorch nn.Module. - `train_loader`: A DataLoader instance providing the training data. - `criterion`: The loss function instance. - `optimizer`: The optimizer instance. - `device`: The device to perform computations on (either \'cpu\' or \'cuda\'). - `initial_batch_size`: The initial batch size for training (default is 64). # Output: The function should return the trained model instance. # Notes: 1. Do not use external libraries other than PyTorch. 2. Ensure that any intermediate variables or tensors that are no longer needed are deleted to free up memory. 3. Use `try-except` blocks to catch out-of-memory errors and handle them appropriately.","solution":"import torch from torch import nn, optim # Assume MyModel and train_loader are predefined # class MyModel(nn.Module): # ... # train_loader = DataLoader(...) def train_model(model, train_loader, criterion, optimizer, device, initial_batch_size=64): model.to(device) model.train() batch_size = initial_batch_size for epoch in range(10): # Train for 10 epochs total_loss = 0 for i, (inputs, targets) in enumerate(train_loader): oom = False try: # Move inputs and targets to device inputs, targets = inputs.to(device), targets.to(device) # Zero the gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) loss = criterion(outputs, targets) # Backward pass and optimize loss.backward() optimizer.step() # Avoid accumulating history in total loss total_loss += float(loss) except RuntimeError as e: if \\"out of memory\\" in str(e): oom = True torch.cuda.empty_cache() else: raise e if oom: # Reduce batch size and retry if batch_size > 1: print(f\\"Out of Memory occurred at iteration {i}. Reducing batch size to {batch_size // 2}\\") batch_size //= 2 inputs, targets = inputs[:batch_size], targets[:batch_size] continue else: print(\\"Unable to fit even a single example into memory. Exiting.\\") return # Clear intermediate tensors to free memory del inputs, targets, outputs, loss print(f\\"Epoch [{epoch + 1}/10], Total Loss: {total_loss}\\") return model # Example usage: # model = MyModel() # criterion = nn.CrossEntropyLoss() # optimizer = optim.SGD(model.parameters(), lr=0.01) # device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\') # trained_model = train_model(model, train_loader, criterion, optimizer, device)"},{"question":"# Python Tokenizer Implementation Objective Implement a Python function to tokenize a given Python source code string according to the lexical rules from Python\'s documentation. Task 1. Write a function `tokenize_python_code(code: str) -> List[str]` that: - Takes a string `code` representing Python source code as input. - Returns a list of strings, each string being a token in the Python code. Requirements 1. Your function should correctly handle: - Single-line comments (ignore everything beyond `#` till the end of the line). - String literals (handle triple-quoted strings, raw strings, and formatted strings). - Numeric literals (integers, floating-point numbers, and imaginary numbers). - Indentation and whitespace. - Keywords and identifiers. - Delimiters and operators. 2. The function should: - Recognize and treat reserved words (like `if`, `else`, `while`) and soft keywords (`match`, `case`) distinctly. - Handle implicit and explicit line joining. - Generate INDENT and DEDENT tokens as described in the documentation. Constraints - The input string represents valid Python code. - You can assume that the input code follows Python 3 syntax. Example ```python code = def example_function(): if x == 10: print(\\"x is ten\\") # This is an example function tokens = tokenize_python_code(code) print(tokens) ``` Expected output: ```python [ \'def\', \'example_function\', \'(\', \')\', \':\', \'INDENT\', \'if\', \'x\', \'==\', \'10\', \':\', \'INDENT\', \'print\', \'(\', \'\\"x is ten\\"\', \')\', \'DEDENT\', \'DEDENT\' ] ``` Notes - The tokenizer should preserve the hierarchical structure of indentation using INDENT and DEDENT tokens. - Comments should be ignored in the output token list. - You may use regular expressions or string manipulation techniques to implement this function. - Pay special attention to edge cases such as nested structures and complex string literals. Good luck!","solution":"import tokenize from io import StringIO from typing import List def tokenize_python_code(code: str) -> List[str]: tokens = [] indents = 0 last_token_type = None # Tokenizing the code reader = StringIO(code).readline for toknum, tokval, _, _, _ in tokenize.generate_tokens(reader): if toknum == tokenize.INDENT: tokens.append(\'INDENT\') indents += 1 elif toknum == tokenize.DEDENT: tokens.append(\'DEDENT\') indents -= 1 elif toknum == tokenize.NEWLINE or toknum == tokenize.NL: continue elif toknum == tokenize.COMMENT: continue elif toknum == tokenize.STRING: tokens.append(tokval) elif toknum == tokenize.NUMBER: tokens.append(tokval) elif toknum == tokenize.NAME: tokens.append(tokval) elif toknum == tokenize.OP: tokens.append(tokval) last_token_type = toknum # Ensure all indents are closed by dedents tokens.extend([\'DEDENT\'] * indents) return tokens"},{"question":"Implementing a Custom Buffer Producer in Python # Objective: In this question, you are required to implement a Python class that acts as a buffer producer. This class should be able to export its internal data buffer to consumers in a way that the Python buffer protocol specifies. # Requirements: 1. Implement a class `CustomBuffer` that manages a byte buffer of a given size. 2. This class should implement a method `get_buffer(self, buf, flags)` to fill the `buf` structure (simulated as a dictionary in Python) based on the requested flags. 3. Ensure that your implementation can handle writable and read-only buffers as specified by the flags. 4. Implement a method to release the buffer correctly, ensuring no memory leaks. # Specifications: - The class `CustomBuffer` should initialize with a given size, creating a byte buffer of that size. - The `bytearray` type should be used to manage the buffer internally. - Provide appropriate error handling if the buffer cannot be created or accessed in the requested format. - Document your code adequately for clarity. # Function Signature: ```python class CustomBuffer: def __init__(self, size: int): pass def get_buffer(self, buf: dict, flags: int): pass def release_buffer(self, buf: dict): pass ``` # Input and Output Formats: Initialization: To initialize an instance of `CustomBuffer` with a specific size: ```python cb = CustomBuffer(1024) # 1024-byte buffer ``` Buffer Request: To request a buffer: ```python buf = {} flags = <appropriate_flags> # Example: 0 for simple read-only buffer cb.get_buffer(buf, flags) ``` The `buf` dictionary should be filled with appropriate buffer information: ```python print(buf) # Expected keys and values based on flags ``` Buffer Release: To release the buffer: ```python cb.release_buffer(buf) ``` Ensure that the buffer\'s internal states are cleaned up properly without memory leaks. # Constraints: - The buffer size must be a positive integer. - The `flags` passed to `get_buffer` should accurately reflect the buffer properties as defined in the provided documentation. # Example: ```python cb = CustomBuffer(256) buf = {} flags = 0 # Simple read-only buffer cb.get_buffer(buf, flags) print(buf) # Expected: {\'obj\': cb, \'buf\': <memory_address>, \'len\': 256, \'readonly\': True} cb.release_buffer(buf) ``` Evaluate the student\'s implementation based on correctness, efficiency, proper use of Python\'s memory management, and adherence to the buffer protocol as described in the provided documentation.","solution":"class CustomBuffer: def __init__(self, size: int): if size <= 0: raise ValueError(\\"Size must be a positive integer\\") self.size = size self.buffer = bytearray(size) self.buffer_info = {\'obj\': self, \'buf\': self.buffer, \'len\': size} def get_buffer(self, buf: dict, flags: int): if not isinstance(buf, dict): raise TypeError(\\"buf must be a dictionary\\") if flags == 0: buf[\'obj\'] = self buf[\'buf\'] = self.buffer buf[\'len\'] = self.size buf[\'readonly\'] = True elif flags == 1: buf[\'obj\'] = self buf[\'buf\'] = memoryview(self.buffer) buf[\'len\'] = self.size buf[\'readonly\'] = False else: raise ValueError(\\"Invalid flag value\\") def release_buffer(self, buf: dict): buf.clear()"},{"question":"# Command-Line Calculator Objective Design and implement a command-line calculator using the `argparse` module. The calculator should support basic arithmetic operations, advanced mathematical functions, and batch processing through input files. Task 1. Design an argument parser that handles the following command-line arguments: - Positional argument `operation`: Specifies the arithmetic operation to perform. - Supported operations: `add`, `subtract`, `multiply`, `divide`, `exp` (exponentiation), `sqrt` (square root). - Positional arguments `x` and `y`: The two operands on which the operations are performed. - `y` is optional and only required for `add`, `subtract`, `multiply`, `divide`, and `exp`. - Optional argument `--file`: Specifies a file containing lines of operations to be performed in batch mode. Each line in the file should contain an operation followed by the required operands (e.g., `add 2 3`). 2. Implement the logic to handle the specified operations: - Addition (`add`): Sum of two numbers. - Subtraction (`subtract`): Difference of two numbers. - Multiplication (`multiply`): Product of two numbers. - Division (`divide`): Quotient of two numbers. - Exponentiation (`exp`): X raised to the power of Y. - Square Root (`sqrt`): Square root of a number X. 3. Ensure that appropriate error handling is implemented for invalid arguments or operations. 4. Handle both single operation mode and batch mode: - In single operation mode (when `--file` is not specified), read the operation and operands from the command-line arguments and print the result. - In batch mode (when `--file` is specified), read the operations and operands from the file and print the results for each line. Example Usages Single Operation Mode: ```shell python calculator.py add 4 5 9 python calculator.py sqrt 16 4.0 ``` Batch Operation Mode: ```shell cat operations.txt add 4 5 subtract 10 3 multiply 6 7 divide 14 2 exp 2 3 sqrt 16 python calculator.py --file operations.txt 9 7 42 7.0 8 4.0 ``` Constraints - You must use Python 3.10 or later. - The script should handle invalid operations and operands gracefully and provide appropriate error messages. - If an invalid operation or operand is encountered in batch mode, continue processing the remaining lines. Function Signature You should implement the function with the following signature: ```python def main(): pass if __name__ == \'__main__\': main() ``` **Input Format** - For single operation mode: Command-line arguments specifying the operation and operands. - For batch mode: A file with each line specifying an operation and operands. **Output Format** - Print the result of the operation(s) to the console.","solution":"import argparse import math def add(x, y): return x + y def subtract(x, y): return x - y def multiply(x, y): return x * y def divide(x, y): if y == 0: raise ValueError(\\"Cannot divide by zero\\") return x / y def exp(x, y): return x ** y def sqrt(x): if x < 0: raise ValueError(\\"Cannot take the square root of a negative number\\") return math.sqrt(x) def process_operation(operation, x, y=None): if operation == \'add\': return add(x, y) elif operation == \'subtract\': return subtract(x, y) elif operation == \'multiply\': return multiply(x, y) elif operation == \'divide\': return divide(x, y) elif operation == \'exp\': return exp(x, y) elif operation == \'sqrt\': return sqrt(x) else: raise ValueError(f\\"Unsupported operation \'{operation}\'\\") def main(): parser = argparse.ArgumentParser(description=\\"Command-Line Calculator\\") parser.add_argument(\'operation\', type=str, choices=[\'add\', \'subtract\', \'multiply\', \'divide\', \'exp\', \'sqrt\'], help=\\"Arithmetic operation to perform\\") parser.add_argument(\'x\', type=float, help=\\"First operand\\") parser.add_argument(\'y\', type=float, nargs=\'?\', default=None, help=\\"Second operand (optional for sqrt)\\") parser.add_argument(\'--file\', type=str, help=\\"File with batch operations\\") args = parser.parse_args() if args.file: try: with open(args.file, \'r\') as file: for line in file: try: parts = line.split() if len(parts) < 2: continue op = parts[0] x = float(parts[1]) y = float(parts[2]) if len(parts) == 3 else None result = process_operation(op, x, y) print(result) except Exception as e: print(f\\"Error processing line \'{line.strip()}\': {e}\\") except FileNotFoundError: print(f\\"File \'{args.file}\' not found\\") else: try: result = process_operation(args.operation, args.x, args.y) print(result) except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Data Analysis and Model Building using scikit-learn** You are provided with access to various real-world datasets through the scikit-learn library. Your task is to demonstrate your understanding of data handling in scikit-learn by performing the following steps: 1. Load the `fetch_california_housing` dataset. 2. Perform preliminary data analysis: - Print the dataset\'s description. - Display the first five rows of the feature data and the target values. - Check for any missing values and report your findings. 3. Split the data into training and testing sets (80% training, 20% testing). 4. Build a linear regression model using the dataset. 5. Evaluate the model: - Report the Mean Squared Error (MSE) on the test set. - Plot the true vs predicted housing values for the test set. # Input and Output Formats **Input:** * No direct input. **Output:** * The dataset description. * The first five rows of feature data and target values. * Report on missing values. * Mean Squared Error of the linear regression model on the test set. * Plot showing true vs predicted values for the test set. # Constraints: - Use `sklearn.model_selection.train_test_split` for splitting the dataset. - Use `sklearn.linear_model.LinearRegression` for building the linear regression model. - Use `sklearn.metrics.mean_squared_error` for calculating the MSE. - Use `matplotlib` or any other plotting library to create the true vs predicted value plot. # Example ```python import matplotlib.pyplot as plt from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error # 1. Load the dataset data = fetch_california_housing() # 2. Preliminary Data Analysis print(data.DESCR) print(data.data[:5]) print(data.target[:5]) # Check for missing values print(\\"Missing values in data:\\", sum(pd.isnull(data.data).sum())) print(\\"Missing values in target:\\", sum(pd.isnull(data.target).sum())) # 3. Split the data X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) # 4. Build the model model = LinearRegression() model.fit(X_train, y_train) # 5. Evaluate the model predictions = model.predict(X_test) mse = mean_squared_error(y_test, predictions) print(\\"Mean Squared Error on test set:\\", mse) # Plot true vs predicted values plt.scatter(y_test, predictions) plt.xlabel(\'True Values\') plt.ylabel(\'Predicted Values\') plt.title(\'True vs Predicted Values\') plt.show() ``` **Note**: You must ensure all required packages (`scikit-learn`, `pandas`, `matplotlib`) are installed in your working environment.","solution":"import matplotlib.pyplot as plt import pandas as pd from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def load_and_analyze_data(): # 1. Load the dataset data = fetch_california_housing() # 2. Preliminary Data Analysis print(data.DESCR) # Print the dataset\'s description print(data.data[:5]) # Display the first five rows of the feature data print(data.target[:5]) # Display the first five rows of the target values # Check for any missing values missing_values_data = pd.isnull(data.data).sum().sum() missing_values_target = pd.isnull(data.target).sum().sum() return data, missing_values_data, missing_values_target def split_data(data): # 3. Split the data into training and testing sets (80% training, 20% testing) X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def build_and_evaluate_model(X_train, X_test, y_train, y_test): # 4. Build a linear regression model using the dataset model = LinearRegression() model.fit(X_train, y_train) # 5. Evaluate the model predictions = model.predict(X_test) mse = mean_squared_error(y_test, predictions) # Plot true vs predicted values for the test set plt.scatter(y_test, predictions) plt.xlabel(\'True Values\') plt.ylabel(\'Predicted Values\') plt.title(\'True vs Predicted Values\') plt.show() return mse"},{"question":"# Python Garbage Collection Management and Debugging As part of a large-scale Python application, you are tasked with managing memory usage efficiently. To that end, you need to understand how garbage collection works in Python and how to control and debug it. Write a function, `manage_garbage_collection(detailed_debug: bool, generation: int) -> dict`, which performs the following tasks: 1. **Enable or Disable Debugging**: Depending on the `detailed_debug` parameter, if `True`, enable debugging to capture detailed information (use `gc.DEBUG_LEAK`). If `False`, disable all debugging. 2. **Manual Garbage Collection**: Perform a manual garbage collection for the specified generation (`generation` parameter). The generation value can be 0, 1, or 2. 3. **Retrieve Garbage Collection Stats**: Retrieve and return garbage collection stats after the collection. The returned dictionary should contain: - The number of objects collected. - The number of uncollectable objects. - The list of currently tracked objects. # Input - `detailed_debug`: A boolean indicating whether to enable detailed debugging information. - `generation`: An integer (0, 1, or 2) indicating which generation to collect. # Output - A dictionary with the following keys: - `\\"collected\\"`: The total number of objects collected in the specified generation. - `\\"uncollectable\\"`: The total number of uncollectable objects. - `\\"tracked_objects\\"`: A list of currently tracked objects after collection. # Constraints - The `generation` value must be one of 0, 1, or 2. If it is not, raise a `ValueError`. # Example ```python # Example input and output result = manage_garbage_collection(detailed_debug=True, generation=1) print(result) # Output might look something like: # { # \'collected\': 10, # \'uncollectable\': 1, # \'tracked_objects\': [<list of tracked objects>] # } ``` # Notes - Use the provided `gc` module functions to accomplish the tasks. - Include proper error handling for invalid generation values. - Ensure the code is robust and efficient for real-world large-scale applications.","solution":"import gc def manage_garbage_collection(detailed_debug: bool, generation: int) -> dict: Manages garbage collection in Python. Parameters: detailed_debug (bool): If True, enables detailed debugging information. If False, disables it. generation (int): The generation to perform manual garbage collection (0, 1, or 2). Returns: dict: A dictionary with keys: \'collected\', \'uncollectable\', \'tracked_objects\'. if generation not in [0, 1, 2]: raise ValueError(\\"The generation value must be one of 0, 1, or 2.\\") # Enable or disable detailed debugging if detailed_debug: gc.set_debug(gc.DEBUG_LEAK) else: gc.set_debug(0) # Perform manual garbage collection collected = gc.collect(generation) uncollectable = len(gc.garbage) tracked_objects = gc.get_objects() # Return the garbage collection stats return { \\"collected\\": collected, \\"uncollectable\\": uncollectable, \\"tracked_objects\\": tracked_objects }"},{"question":"You are asked to implement a custom scaler and normalizer for sparse matrices using the `sklearn.utils.sparsefuncs` utilities. Your implementation should include two classes: `CustomScaler` and `CustomNormalizer`. The `CustomScaler` should scale the columns of a sparse matrix to have unit variance, and the `CustomNormalizer` should normalize the rows to unit L2 norm. # Requirements: 1. Implement two classes: `CustomScaler` and `CustomNormalizer`. 2. Both classes should be designed to process `scipy.sparse.csr_matrix` objects. 3. Use the functions: `sparsefuncs.inplace_csr_column_scale` for `CustomScaler` and `sparsefuncs_fast.inplace_csr_row_normalize_l2` for `CustomNormalizer` from `sklearn.utils`. # Detailed Specifications: Class: `CustomScaler` - **Method**: `fit(self, X: scipy.sparse.csr_matrix) -> CustomScaler` - Calculates and stores the scaling factors (standard deviation) for each column in the sparse matrix `X`. - **Method**: `transform(self, X: scipy.sparse.csr_matrix) -> scipy.sparse.csr_matrix` - Scales the columns using the factors calculated in `fit`. - **Method**: `fit_transform(self, X: scipy.sparse.csr_matrix) -> scipy.sparse.csr_matrix` - Combines `fit` and `transform` in one step. Class: `CustomNormalizer` - **Method**: `fit(self, X: scipy.sparse.csr_matrix) -> CustomNormalizer` - Prepares the internal state (if any) for normalization. - **Method**: `transform(self, X: scipy.sparse.csr_matrix) -> scipy.sparse.csr_matrix` - Normalizes each row of the sparse matrix `X` to unit L2 norm. - **Method**: `fit_transform(self, X: scipy.sparse.csr_matrix) -> scipy.sparse.csr_matrix` - Combines `fit` and `transform` in one step. # Expected Input: - `X`: A `scipy.sparse.csr_matrix` with numeric entries. # Expected Output: - Scaled/Normalized sparse matrix. # Constraints: - You must use the provided utility functions from `sklearn.utils.sparsefuncs`. - Test your implementation with appropriate sparse `csr_matrix` data. # Example: ```python from scipy.sparse import csr_matrix import numpy as np from sklearn.utils import sparsefuncs, sparsefuncs_fast # Assuming the implementation of CustomScaler and CustomNormalizer classes X = csr_matrix([ [1, 2, 0], [0, 0, 3], [4, 0, 6] ], dtype=float) # Using CustomScaler scaler = CustomScaler() X_scaled = scaler.fit_transform(X) print(X_scaled.toarray()) # Using CustomNormalizer normalizer = CustomNormalizer() X_normalized = normalizer.fit_transform(X) print(X_normalized.toarray()) ``` # Note: - This example demonstrates how to use your custom scaler and normalizer with sparse matrices. - Focus on using the provided functions in `sklearn.utils.sparsefuncs` and `sklearn.utils.sparsefuncs_fast` to ensure efficiency in your implementation.","solution":"from scipy.sparse import csr_matrix from sklearn.utils import sparsefuncs, sparsefuncs_fast import numpy as np class CustomScaler: def fit(self, X: csr_matrix) -> \'CustomScaler\': variances = np.var(X.toarray(), axis=0) self.scale_factors_ = np.sqrt(variances) self.scale_factors_[self.scale_factors_ == 0] = 1 # to avoid division by zero return self def transform(self, X: csr_matrix) -> csr_matrix: sparsefuncs.inplace_csr_column_scale(X, 1 / self.scale_factors_) return X def fit_transform(self, X: csr_matrix) -> csr_matrix: return self.fit(X).transform(X) class CustomNormalizer: def fit(self, X: csr_matrix) -> \'CustomNormalizer\': return self # no state to set def transform(self, X: csr_matrix) -> csr_matrix: sparsefuncs_fast.inplace_csr_row_normalize_l2(X) return X def fit_transform(self, X: csr_matrix) -> csr_matrix: return self.fit(X).transform(X)"},{"question":"# Color Space Converter and Analyzer You are required to implement a function that processes a list of colors in RGB format, converts them to YIQ and HLS color spaces, and calculates the average values for each of the respective coordinates (R, G, B, Y, I, Q, H, L, S). Function Signature ```python def color_space_analysis(colors: list) -> dict: pass ``` Input - A list of tuples, where each tuple contains three floating-point numbers representing an RGB color. Each value in the tuple is between 0 and 1 (inclusive). Output - A dictionary with the keys `\'rgb\'`, `\'yiq\'`, and `\'hls\'`. Each key maps to a tuple containing the average values of their respective coordinates: ```python { \'rgb\': (avg_r, avg_g, avg_b), \'yiq\': (avg_y, avg_i, avg_q), \'hls\': (avg_h, avg_l, avg_s) } ``` Constraints 1. The input list will contain at least one color and at most 10,000 colors. 2. All color values are guaranteed to be valid floating-point numbers between 0 and 1 (inclusive). Example ```python colors = [(0.2, 0.4, 0.4), (0.5, 0.5, 0.5), (0.3, 0.7, 0.8)] result = color_space_analysis(colors) ``` Output: ```python { \'rgb\': (0.3333333333333333, 0.5333333333333333, 0.5666666666666667), \'yiq\': (0.41333333333333333, 0.006666666666666666, 0.18666666666666668), \'hls\': (0.5, 0.54, 0.4583333333333333) } ``` Notes - Use the `colorsys` module functions to perform the conversions. - Be mindful of computational efficiency, especially with large input sizes.","solution":"import colorsys from statistics import mean def color_space_analysis(colors: list) -> dict: Processes a list of RGB colors, converts them to YIQ and HLS color spaces, and calculates the average values for each of the respective coordinates. # Unpack the individual RGB components r_values, g_values, b_values = zip(*colors) yiq_colors = [colorsys.rgb_to_yiq(r, g, b) for r, g, b in colors] y_values, i_values, q_values = zip(*yiq_colors) hls_colors = [colorsys.rgb_to_hls(r, g, b) for r, g, b in colors] h_values, l_values, s_values = zip(*hls_colors) return { \\"rgb\\": (mean(r_values), mean(g_values), mean(b_values)), \\"yiq\\": (mean(y_values), mean(i_values), mean(q_values)), \\"hls\\": (mean(h_values), mean(l_values), mean(s_values)) }"},{"question":"Coding Assessment Question # Objective Write a Python function using the `xml.dom.pulldom` module that processes an XML document representing an online store\'s inventory. The function should identify products within a specific price range and return a list of those products\' descriptions. # Function Signature ```python def extract_descriptions(xml_string: str, min_price: float, max_price: float) -> List[str]: pass ``` # Parameters - `xml_string` (str): A string containing XML data representing the store\'s inventory. - `min_price` (float): The minimum price in the range. - `max_price` (float): The maximum price in the range. # Output - Return a list of strings, where each string is the text content of a product\'s description that falls within the specified price range. # Constraints 1. The XML structure includes `product` elements that each contain `price` and `description` sub-elements. 2. Products with prices outside the given range should be ignored. 3. Assume the `price` sub-elements contain valid floating-point numbers and descriptions can contain mixed content. # Example ```python xml_data = <inventory> <product> <name>Widget</name> <price>19.99</price> <description>High-quality widget</description> </product> <product> <name>Gadget</name> <price>45.00</price> <description>Standard gadget with many functions</description> </product> <product> <name>Thingamajig</name> <price>79.99</price> <description>Premium thingamajig with extended warranty</description> </product> </inventory> min_price = 20.00 max_price = 50.00 print(extract_descriptions(xml_data, min_price, max_price)) # Expected output: [\'Standard gadget with many functions\'] ``` # Requirements - Use the `xml.dom.pulldom` module to parse the XML data. - Efficiently process the data, selectively expanding nodes only when necessary. - Handle mixed content in the `description` elements properly. # Note Consider edge cases, such as: - No products within the specified price range. - Empty XML data or missing elements.","solution":"from xml.dom.pulldom import parseString, START_ELEMENT, END_ELEMENT def extract_descriptions(xml_string: str, min_price: float, max_price: float) -> list: Extract and return product descriptions for products within a specified price range. Args: xml_string (str): A string containing XML data representing the store\'s inventory. min_price (float): The minimum price in the range. max_price (float): The maximum price in the range. Returns: List[str]: List of product descriptions within the given price range. descriptions = [] events = parseString(xml_string) in_product = False current_description = \\"\\" current_price = None for event, node in events: if event == START_ELEMENT: if node.tagName == \\"product\\": in_product = True current_description = \\"\\" current_price = None elif in_product and node.tagName == \\"price\\": events.expandNode(node) current_price = float(node.firstChild.nodeValue) elif in_product and node.tagName == \\"description\\": events.expandNode(node) current_description = node.firstChild.nodeValue.strip() elif event == END_ELEMENT and node.tagName == \\"product\\": in_product = False if current_price is not None and min_price <= current_price <= max_price: descriptions.append(current_description) return descriptions"},{"question":"Objective Implement a function that creates a dimensionality reduction pipeline using both Gaussian and Sparse random projections. Additionally, the function should demonstrate the use of inverse transformation to reconstruct the original data and evaluate reconstruction accuracy. Detailed Task 1. Implement the function `dimensionality_reduction_pipeline` that accepts the following parameters: - `X`: Input data as a NumPy array of shape `(n_samples, n_features)`. - `n_components`: Target number of dimensions to reduce to. - `method`: String specifying the projection method to use (`\'gaussian\'` or `\'sparse\'`). 2. Your function should: 1. Perform dimensionality reduction using the specified method (`GaussianRandomProjection` or `SparseRandomProjection`). 2. Apply inverse transformation to reconstruct the original data. 3. Calculate and return the mean squared error (MSE) between the original data `X` and the reconstructed data. 3. Ensure you make use of `compute_inverse_components=True` to make inverse transformation efficient. Constraints - You can assume that `n_samples`, `n_features`, and `n_components` are positive integers. - The `method` parameter will be one of the two valid strings: `\'gaussian\'` or `\'sparse\'`. Performance Requirements - The solution should handle data sizes up to `(10000, 10000)` efficiently. Expected Function Signature ```python import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection from sklearn.metrics import mean_squared_error def dimensionality_reduction_pipeline(X: np.ndarray, n_components: int, method: str) -> float: Reduces the dimensionality of X using the specified method, then reconstructs the data and computes the reconstruction error. Parameters: - X: NumPy array of shape (n_samples, n_features) - n_components: int, target number of dimensions - method: str, either \'gaussian\' or \'sparse\' Returns: - mse: float, mean squared error between the original and reconstructed data if method not in [\'gaussian\', \'sparse\']: raise ValueError(\\"Method must be \'gaussian\' or \'sparse\'\\") # Step 1: Dimensionality reduction if method == \'gaussian\': transformer = GaussianRandomProjection(n_components=n_components, compute_inverse_components=True) else: transformer = SparseRandomProjection(n_components=n_components, compute_inverse_components=True) X_reduced = transformer.fit_transform(X) # Step 2: Inverse transformation X_reconstructed = transformer.inverse_transform(X_reduced) # Step 3: Compute Mean Squared Error mse = mean_squared_error(X, X_reconstructed) return mse ```","solution":"import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection from sklearn.metrics import mean_squared_error def dimensionality_reduction_pipeline(X: np.ndarray, n_components: int, method: str) -> float: Reduces the dimensionality of X using the specified method, then reconstructs the data and computes the reconstruction error. Parameters: - X: NumPy array of shape (n_samples, n_features) - n_components: int, target number of dimensions - method: str, either \'gaussian\' or \'sparse\' Returns: - mse: float, mean squared error between the original and reconstructed data if method not in [\'gaussian\', \'sparse\']: raise ValueError(\\"Method must be \'gaussian\' or \'sparse\'\\") # Step 1: Dimensionality reduction if method == \'gaussian\': transformer = GaussianRandomProjection(n_components=n_components, compute_inverse_components=True) else: transformer = SparseRandomProjection(n_components=n_components, compute_inverse_components=True) X_reduced = transformer.fit_transform(X) # Step 2: Inverse transformation X_reconstructed = transformer.inverse_transform(X_reduced) # Step 3: Compute Mean Squared Error mse = mean_squared_error(X, X_reconstructed) return mse"},{"question":"Objective Your task is to implement a transformation that modifies a given PyTorch model. Specifically, we want to replace all `torch.add` operations with `torch.mul` operations in the model using the `torch.fx` module. This transformation is important for optimizing certain mathematical operations in our model. Problem Statement 1. Write a function `replace_add_with_mul` that takes a `torch.nn.Module` and replaces all `torch.add` operations with `torch.mul` operations. 2. The transformation should maintain the structure of the original model while modifying the specified operations. Your implementation should demonstrate the following: - Acquiring a `Graph` from a given `torch.nn.Module`. - Modifying the `Graph` to replace `torch.add` operations with `torch.mul` operations. - Returning a new `GraphModule` that reflects these changes. Function Signature ```python def replace_add_with_mul(model: torch.nn.Module) -> torch.nn.Module: Transform a given model by replacing all `torch.add` operations with `torch.mul` operations. Args: model (torch.nn.Module): The input PyTorch model. Returns: torch.nn.Module: The transformed PyTorch model with `torch.add` replaced by `torch.mul`. ``` Constraints - Use the `torch.fx` module for symbolic tracing and graph manipulation. - The function must handle any valid `torch.nn.Module` passed as input. - Performance should be considered to ensure the transformation doesn\'t introduce significant overhead. Example ```python import torch import torch.fx from torch import nn class SampleModel(nn.Module): def forward(self, x, y): return torch.add(x, y) model = SampleModel() new_model = replace_add_with_mul(model) # Test the transformed model x = torch.tensor([1.0, 2.0, 3.0]) y = torch.tensor([4.0, 5.0, 6.0]) print(new_model(x, y)) # Expected output: tensor([ 4.0, 10.0, 18.0]), which is x * y ``` Evaluation Criteria - Correctness: The transformed model should produce outputs consistent with the replacement of `torch.add` with `torch.mul`. - Efficiency: The transformation should be carried out without undue delay. - Code Quality: The solution should be clear, maintainable, and follow best practices for symbolic tracing and graph manipulation using `torch.fx`.","solution":"import torch import torch.fx from torch import nn def replace_add_with_mul(model: torch.nn.Module) -> torch.nn.Module: Transform a given model by replacing all `torch.add` operations with `torch.mul` operations. Args: model (torch.nn.Module): The input PyTorch model. Returns: torch.nn.Module: The transformed PyTorch model with `torch.add` replaced by `torch.mul`. # Use symbolic tracing to get a GraphModule traced = torch.fx.symbolic_trace(model) # Create a new graph to apply the transformations new_graph = torch.fx.Graph() # Record the node mappings from the old graph to new graph val_map = {} for node in traced.graph.nodes: if node.op == \'call_function\' and node.target == torch.add: # Replace torch.add with torch.mul new_node = new_graph.call_function(torch.mul, node.args, node.kwargs) else: # Copy the node as is to the new graph new_node = new_graph.node_copy(node, lambda n: val_map[n]) val_map[node] = new_node # Create a new GraphModule using the modified graph new_graph_module = torch.fx.GraphModule(traced, new_graph) return new_graph_module"},{"question":"**Coding Assessment Question** # Title: Advanced Configuration File Processor # Objective: You are tasked with creating a Python function that processes an INI configuration file to extract and validate specific information. Your task will require utilizing the `configparser` module to read and interpret the configuration data. # Problem Description: Write a function `process_config(file_path: str) -> dict` that: 1. Reads an INI file specified by `file_path`. 2. Extracts the following information from the INI file: - Section: `Database` - `host` - `port` - `username` - `password` - Section: `Server` - `host` - `port` - `use_ssl` 3. Validates the extracted values with the following constraints: - `host` (both sections): Must be a valid hostname (you can assume it is valid if it contains at least one dot and no spaces). - `port` (both sections): Must be an integer between 1 and 65535. - `username` (Database): Must not be empty. - `password` (Database): Must not be empty. - `use_ssl` (Server): Must be either `True` or `False`. 4. Returns a dictionary with the extracted and validated information. If the file or any value does not meet the specified constraints, raise an appropriate exception with a clear error message. # Input: - `file_path`: A string representing the path to the INI file. # Output: - A dictionary with the following structure: ```python { \'Database\': { \'host\': str, \'port\': int, \'username\': str, \'password\': str }, \'Server\': { \'host\': str, \'port\': int, \'use_ssl\': bool } } ``` # Constraints: - If any required section or option is missing or if any value does not meet the constraints, raise an appropriate exception with a clear error message. # Example: Assume the INI file at `config.ini` contains: ``` [Database] host = db.example.com port = 3306 username = admin password = secret [Server] host = server.example.com port = 8080 use_ssl = True ``` ```python process_config(\'config.ini\') ``` Should return: ```python { \'Database\': { \'host\': \'db.example.com\', \'port\': 3306, \'username\': \'admin\', \'password\': \'secret\' }, \'Server\': { \'host\': \'server.example.com\', \'port\': 8080, \'use_ssl\': True } } ``` # Note: - You may assume the file exists and is readable. - You should use the `configparser` module from the Python standard library. - Implementations must strictly follow the constraints and format.","solution":"import configparser class ConfigError(Exception): pass def process_config(file_path: str) -> dict: config = configparser.ConfigParser() config.read(file_path) def validate_host(host): if \' \' in host or \'.\' not in host: raise ConfigError(f\\"Invalid host: {host}\\") def validate_port(port): try: port = int(port) if not (1 <= port <= 65535): raise ValueError except ValueError: raise ConfigError(f\\"Invalid port: {port}\\") return port def validate_ssl(ssl): if ssl.lower() in [\'true\', \'false\']: return ssl.lower() == \'true\' raise ConfigError(f\\"Invalid use_ssl value: {ssl}\\") required_sections = { \'Database\': [\'host\', \'port\', \'username\', \'password\'], \'Server\': [\'host\', \'port\', \'use_ssl\'] } result = { \'Database\': {}, \'Server\': {} } for section, keys in required_sections.items(): if section not in config: raise ConfigError(f\\"Missing section: {section}\\") for key in keys: if key not in config[section]: raise ConfigError(f\\"Missing {key} in section {section}\\") # Process Database section db_section = config[\'Database\'] result[\'Database\'][\'host\'] = db_section[\'host\'] validate_host(db_section[\'host\']) result[\'Database\'][\'port\'] = validate_port(db_section[\'port\']) result[\'Database\'][\'username\'] = db_section[\'username\'] if not db_section[\'username\']: raise ConfigError(\\"Database username is empty\\") result[\'Database\'][\'password\'] = db_section[\'password\'] if not db_section[\'password\']: raise ConfigError(\\"Database password is empty\\") # Process Server section server_section = config[\'Server\'] result[\'Server\'][\'host\'] = server_section[\'host\'] validate_host(server_section[\'host\']) result[\'Server\'][\'port\'] = validate_port(server_section[\'port\']) result[\'Server\'][\'use_ssl\'] = validate_ssl(server_section[\'use_ssl\']) return result"},{"question":"Objective Demonstrate your understanding of Python\'s type hinting and the \\"typing\\" module by implementing a function that processes a list of elements with strict type requirements. Problem Statement You are to implement a function named `process_elements`. This function takes a list of elements and processes them according to their type. The function should satisfy the following requirements: 1. The list may contain integers, floats, and strings. 2. For integers, the function should return a list with all even integers doubled. 3. For floats, the function should return a list with all floats rounded to two decimal places. 4. For strings, the function should return a list with all strings converted to uppercase. 5. The function should use appropriate type hints for function parameters and return types. Function Signature ```python from typing import List, Union def process_elements(elements: List[Union[int, float, str]]) -> List[Union[int, float, str]]: pass ``` Input - `elements`: A list of elements of types int, float, and str. Each type of element should be processed according to the rules specified above. - Example: `[1, 2.4567, \'hello\', 4, 3.14159, \'world\']` Output - A list of processed elements, maintaining the order of the input list after processing. - Example: `[1, 2.46, \'HELLO\', 8, 3.14, \'WORLD\']` Constraints - The input list will contain at least one element. - The input list will not contain any nested lists or other data types. - Ensure your function handles empty input lists gracefully. Example ```python assert process_elements([1, 2.4567, \'hello\', 4, 3.14159, \'world\']) == [1, 2.46, \'HELLO\', 8, 3.14, \'WORLD\'] ``` Additional Requirements - Use type hints effectively to ensure type safety. - Avoid using any external libraries; only standard Python libraries and the contents of the `typing` module should be used. Good luck!","solution":"from typing import List, Union def process_elements(elements: List[Union[int, float, str]]) -> List[Union[int, float, str]]: processed_elements = [] for element in elements: if isinstance(element, int): if element % 2 == 0: processed_elements.append(element * 2) else: processed_elements.append(element) elif isinstance(element, float): processed_elements.append(round(element, 2)) elif isinstance(element, str): processed_elements.append(element.upper()) return processed_elements"},{"question":"Below is a challenging question based on the documentation of the \\"pickletools\\" module: # Question Given a pickle file containing pickled data, you are required to implement a Python function that disassembles the pickle, optimizes it by eliminating unused \\"PUT\\" opcodes, and then reassembles it into a new pickle file. Finally, your function should return the newly created pickle file\'s content. Your task is to implement the following function: ```python def optimize_pickle(input_pickle_path: str, output_pickle_path: str) -> bytes: Disassembles, optimizes, and reassembles a pickle file. Args: input_pickle_path (str): The path to the input pickle file. output_pickle_path (str): The path where the optimized pickle file will be saved. Returns: bytes: The content of the new optimized pickle file. pass ``` # Constraints - The `input_pickle_path` will always be a valid path to a file containing pickled data. - The function should handle pickles created with any version of the pickle protocol. - You may assume there is sufficient memory to hold the entire contents of the pickled data in memory. # Example Usage ```python optimized_data = optimize_pickle(\'input.pickle\', \'optimized.pickle\') print(optimized_data) ``` # Additional Requirements - Your solution should make use of the `pickletools` module\'s `dis` and `optimize` functions as described in the documentation. - Ensure that the output pickle file maintains the same data integrity as the input file. - Provide error handling to manage potential issues such as file I/O errors.","solution":"import pickletools import pickle def optimize_pickle(input_pickle_path: str, output_pickle_path: str) -> bytes: Disassembles, optimizes, and reassembles a pickle file. Args: input_pickle_path (str): The path to the input pickle file. output_pickle_path (str): The path where the optimized pickle file will be saved. Returns: bytes: The content of the new optimized pickle file. try: with open(input_pickle_path, \'rb\') as input_file: pickled_data = input_file.read() # Disassemble the pickle data disassembled_data = pickletools.dis(pickled_data) # Optimize the disassembled data optimized_data = pickletools.optimize(pickled_data) # Reassemble and save the optimized pickle to the output file with open(output_pickle_path, \'wb\') as output_file: output_file.write(optimized_data) return optimized_data except Exception as e: raise RuntimeError(f\\"An error occurred during the processing of the pickle file: {e}\\")"},{"question":"**Objective**: Implement and verify data packing and unpacking using the `xdrlib` module. **Task**: Write a Python function called `pack_and_unpack_data`, which takes a list of mixed data types (integers, floats, strings) and does the following: 1. Packs the data using the `xdrlib.Packer` class. 2. Unpacks the packed data using the `xdrlib.Unpacker` class. 3. Ensures the original data matches the unpacked data to verify the integrity of packing and unpacking. **Function Signature**: ```python def pack_and_unpack_data(data: list) -> bool: ``` # Requirements 1. **Function Implementation**: - The `data` list can contain integers, floats, and strings. - Use appropriate methods from `xdrlib.Packer` for packing the data. - Use appropriate methods from `xdrlib.Unpacker` for unpacking the data. - Ensure to handle any exceptions that might occur during packing or unpacking. 2. **Input and Output**: - Input: A list of data items (`data`). - Output: A boolean value (`True` if the data before packing matches the data after unpacking, otherwise `False`). 3. **Constraints**: - Strings in the list will have a maximum length of 256 characters. - The list will contain at most 100 elements. - You must handle different data types appropriately by leveraging the correct packing and unpacking methods. # Example ```python example_data = [1, 2.0, \\"string\\", 3, 4.5, \\"another string\\"] assert pack_and_unpack_data(example_data) == True ``` In this example, the function should pack the provided list and then unpack it to verify the integrity of the data. If the original data matches the unpacked data, the function should return `True`. # Implementation Considerations - Carefully manage the packing of data types using the respective methods. - Ensure to unpack data types in the same order as they were packed. - Properly catch and handle any exceptions that might occur during the packing/unpacking process, such as `xdrlib.ConversionError`. # Notes - The provided documentation of `xdrlib` can be referred to for detailed information on the available methods and their usages. - Focus on maintaining code readability and organization. - Adequate error handling and testing are essential to ensure robustness.","solution":"import xdrlib def pack_and_unpack_data(data): Packs and unpacks the given data using xdrlib, and verifies the integrity of the data. Parameters: - data: list of mixed data types (integers, floats, strings) Returns: - bool: True if original data matches the unpacked data, otherwise False packer = xdrlib.Packer() try: # Packing data for item in data: if isinstance(item, int): packer.pack_int(item) elif isinstance(item, float): packer.pack_double(item) elif isinstance(item, str): packer.pack_string(item.encode(\'utf-8\')) else: raise ValueError(f\\"Unsupported data type: {type(item)}\\") packed_data = packer.get_buffer() # Unpacking data unpacker = xdrlib.Unpacker(packed_data) unpacked_data = [] for item in data: if isinstance(item, int): unpacked_data.append(unpacker.unpack_int()) elif isinstance(item, float): unpacked_data.append(unpacker.unpack_double()) elif isinstance(item, str): unpacked_data.append(unpacker.unpack_string().decode(\'utf-8\')) return data == unpacked_data except (xdrlib.Error, ValueError) as e: print(f\\"Error occurred: {e}\\") return False"},{"question":"**Objective**: Implement a Python function using scikit-learn that loads the Iris dataset, preprocesses the data, trains a machine learning model, and evaluates its performance. **Function Signature**: ```python def evaluate_iris_model(): pass ``` # Instructions: 1. **Loading the Dataset**: - Use the `load_iris` function from `sklearn.datasets` to load the Iris dataset. 2. **Preprocessing the Data**: - Split the dataset into features (X) and target (y). - Split the data into training and testing sets using an 80-20 split. You should use `train_test_split` from `sklearn.model_selection`. 3. **Model Training**: - Train a logistic regression model on the training data. Use `LogisticRegression` from `sklearn.linear_model`. 4. **Model Evaluation**: - Evaluate the trained model on the testing data. Calculate and return the following metrics: - Accuracy - Precision - Recall - F1-Score # Constraints: - You should use the default parameters for `LogisticRegression`. - You should print each metric with a precision of 2 decimal places. # Example: The function should return a dictionary with the following structure: ```python { \\"accuracy\\": 0.90, \\"precision\\": 0.92, \\"recall\\": 0.89, \\"f1_score\\": 0.90 } ``` **Handling Errors**: Ensure your function gracefully handles any errors that may arise during data loading, model training, or evaluation, and prints an appropriate error message. # Note: You should import necessary functions and classes from scikit-learn in your implementation. **Expected Input and Output Formats**: - Input: None - Output: A dictionary containing the evaluation metrics (accuracy, precision, recall, f1_score). Implement the function in the cell below.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def evaluate_iris_model(): try: # Loading the dataset iris = load_iris() X, y = iris.data, iris.target # Splitting the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initiating the model model = LogisticRegression(max_iter=200) # Training the model model.fit(X_train, y_train) # Predicting on the test set y_pred = model.predict(X_test) # Calculating the metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\\"weighted\\") recall = recall_score(y_test, y_pred, average=\\"weighted\\") f1 = f1_score(y_test, y_pred, average=\\"weighted\\") # Printing the metrics result = { \\"accuracy\\": round(accuracy, 2), \\"precision\\": round(precision, 2), \\"recall\\": round(recall, 2), \\"f1_score\\": round(f1, 2) } return result except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage: # print(evaluate_iris_model())"},{"question":"Shared Memory Calculator # Objective You need to write a Python program that utilizes the `multiprocessing.shared_memory` module to perform calculations using multiple processes, sharing data through shared memory blocks. The goal is to calculate the sum of elements in an integer array using two processes to improve performance. # Instructions 1. **Create a shared memory block** to hold an array of integers. 2. **Write a function `process_partial_sum`** that takes the shared memory block, a start index, and an end index as input, calculates the sum of the array elements within this range, and writes the result back into the shared memory block at a specified index. 3. **Use the `multiprocessing` module** to create two processes, each processing a different half of the array. 4. **Aggregate the results** from both processes to compute the final sum of the array. # Requirements - You must use the `SharedMemory` class. - The original array and the results should be stored in shared memory. - Ensure proper cleanup of shared memory resources. # Expected Input and Output - **Input:** An array of integers (e.g., `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`). - **Output:** An integer representing the sum of all elements in the array (e.g., `55`). # Constraints - The input array will have at least 2 elements. - The maximum array length will be 10^6 elements. - Each element in the array will be an integer in the range [-10^6, 10^6]. # Performance - The solution should be efficient. The total runtime should be linear concerning the input array size. # Example ```python from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory import numpy as np def process_partial_sum(shm_name, start, end, result_index): existing_shm = SharedMemory(name=shm_name) array = np.ndarray((end-start,), dtype=np.int64, buffer=existing_shm.buf[start*8:end*8]) partial_sum = array.sum() result_array = np.ndarray((2,), dtype=np.int64, buffer=existing_shm.buf[-16:]) result_array[result_index] = partial_sum existing_shm.close() def main(): input_array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] n = len(input_array) # Create shared memory block for the array and results shm = SharedMemory(create=True, size=(n+2) * 8) # 8 bytes per int64 element, extra 16 bytes for results array = np.ndarray((n+2,), dtype=np.int64, buffer=shm.buf) array[:n] = input_array array[n:] = 0 # Initialize result positions # Create two processes for summing parts of the array mid = n // 2 p1 = Process(target=process_partial_sum, args=(shm.name, 0, mid, 0)) p2 = Process(target=process_partial_sum, args=(shm.name, mid, n, 1)) # Start and join the processes p1.start() p2.start() p1.join() p2.join() # Compute final result final_sum = array[n] + array[n+1] print(f\'Sum of array: {final_sum}\') # Cleanup shared memory shm.close() shm.unlink() if __name__ == \'__main__\': main() ``` **Explanation:** 1. The `main()` function sets up shared memory for the full array and two extra positions for storing the partial sums. 2. Two processes are spawned, each calculating the sum of half of the array and storing their result in shared memory. 3. The results are aggregated to compute the final sum, which is printed. 4. The shared memory is cleaned up properly. Implement the above functionality to understand shared memory usage in Python\'s `multiprocessing.shared_memory` module.","solution":"from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory import numpy as np def process_partial_sum(shm_name, start, end, result_index): This function calculates the sum of the elements of the shared memory array from start index to end index, and writes the result at result_index. existing_shm = SharedMemory(name=shm_name) array = np.ndarray((end-start,), dtype=np.int64, buffer=existing_shm.buf[start*8:end*8]) partial_sum = array.sum() result_array = np.ndarray((2,), dtype=np.int64, buffer=existing_shm.buf[-16:]) result_array[result_index] = partial_sum existing_shm.close() def calculate_array_sum(input_array): This function calculates the sum of the elements of the input_array by dividing the work among two processes and using shared memory. n = len(input_array) if n < 2: raise ValueError(\\"Array should have at least 2 elements\\") # Create shared memory block for the array and results shm = SharedMemory(create=True, size=(n+2) * 8) # 8 bytes per int64 element, extra 16 bytes for results array = np.ndarray((n+2,), dtype=np.int64, buffer=shm.buf) array[:n] = input_array array[n:] = 0 # Initialize result positions # Create two processes for summing parts of the array mid = n // 2 p1 = Process(target=process_partial_sum, args=(shm.name, 0, mid, 0)) p2 = Process(target=process_partial_sum, args=(shm.name, mid, n, 1)) # Start and join the processes p1.start() p2.start() p1.join() p2.join() # Compute final result final_sum = array[n] + array[n+1] # Cleanup shared memory shm.close() shm.unlink() return final_sum if __name__ == \'__main__\': input_array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] print(f\'Sum of array: {calculate_array_sum(input_array)}\')"},{"question":"<|Analysis Begin|> The provided documentation covers the `graphlib` module, which is specifically focused on the `TopologicalSorter` class. This class is used to topologically sort a graph, which is a common requirement when dealing with tasks that have dependencies, such as job scheduling or resolving package dependencies. Key Features: 1. **Graph Representation**: The graph is represented as a dictionary where keys are nodes, and values are iterables of their predecessors. 2. **Methods Provided**: - `add(node, *predecessors)`: Adds a node and its predecessors to the graph. - `prepare()`: Marks the graph as finished for sorting and checks for cycles. - `is_active()`: Checks if more progress can be made in sorting. - `done(*nodes)`: Marks nodes as processed, unblocking their successors. - `get_ready()`: Returns nodes that are ready to be processed. - `static_order()`: Returns nodes in a static topological order without parallelism. 3. **Exception Handling**: Specifically checks for cycles in the graph and raises `CycleError` if any are detected. Given these functionalities, the question should focus on 1. Constructing and manipulating a graph using the `TopologicalSorter` class. 2. Handling cycles and ensuring nodes are processed in correct topological order. 3. Utilizing methods like `static_order()` for convenience. <|Analysis End|> <|Question Begin|> # Coding Assessment: Advanced Graph Operations with `graphlib` You are given a set of tasks and dependencies between them. Your goal is to determine the order in which these tasks should be executed. If there\'s a cycle in the task dependencies, you should identify and handle it appropriately. You will utilize Python\'s `graphlib` module and specifically the `TopologicalSorter` class for this task. Function Signature ```python def schedule_tasks(tasks: List[str], dependencies: List[Tuple[str, str]]) -> List[str]: Determines the order of task execution based on given dependencies using topological sorting. Parameters: tasks (List[str]): List of task names. dependencies (List[Tuple[str, str]]): List of dependencies where each tuple represents (task, dependency). Returns: List[str]: The ordered list of tasks. If a cycle is detected, it should return an empty list. ``` Input - `tasks`: A list of strings, where each string is a unique task. - `dependencies`: A list of tuples, where each tuple `(task, dependency)` means `task` depends on `dependency`. Output - A list of task names (strings) in the order they should be executed. If a cycle is detected, return an empty list. Example ```python tasks = [\\"A\\", \\"B\\", \\"C\\", \\"D\\"] dependencies = [(\\"B\\", \\"A\\"), (\\"C\\", \\"A\\"), (\\"D\\", \\"B\\"), (\\"D\\", \\"C\\")] print(schedule_tasks(tasks, dependencies)) # Output: [\'A\', \'B\', \'C\', \'D\'] or [\'A\', \'C\', \'B\', \'D\'] (both are valid orders) tasks_with_cycle = [\\"A\\", \\"B\\", \\"C\\"] dependencies_with_cycle = [(\\"A\\", \\"B\\"), (\\"B\\", \\"C\\"), (\\"C\\", \\"A\\")] print(schedule_tasks(tasks_with_cycle, dependencies_with_cycle)) # Output: [] ``` Constraints - The `tasks` list will have a maximum of 1000 tasks. - The `dependencies` list will have a maximum of 5000 dependencies. - Tasks and dependencies will always be provided as non-empty lists. - Task names are unique and are strings of at most length 20 consisting only of alphanumeric characters. Notes - Use methods of `TopologicalSorter` to build and manage the task graph. - Handle the `CycleError` exception to detect cycles properly. # Coding Requirements 1. **Construct the graph** using the `TopologicalSorter` class and add nodes and dependencies. 2. **Identify** and **handle cycles** appropriately using `CycleError`. 3. **Optimize for performance** given the constraints. Good luck!","solution":"from typing import List, Tuple from graphlib import TopologicalSorter, CycleError def schedule_tasks(tasks: List[str], dependencies: List[Tuple[str, str]]) -> List[str]: ts = TopologicalSorter() for task in tasks: ts.add(task) for task, dependency in dependencies: ts.add(task, dependency) try: return list(ts.static_order()) except CycleError: return []"},{"question":"# Principal Component Analysis (PCA) Challenge You are provided with a dataset, `large_dataset.csv`, containing a large number of samples and features. Your task is to perform dimensionality reduction using various PCA techniques provided in the scikit-learn library. The goal is to project the dataset onto a lower-dimensional space and evaluate the performance and suitability of each technique. **Dataset** The dataset, `large_dataset.csv`, has `100000` samples and `1000` features. # Tasks 1. **Load the dataset** from the CSV file and display its basic information. 2. **Standard PCA**: - Fit a standard PCA model to the dataset. - Reduce the dimensionality of the dataset to `50` components. - Measure the time taken for the transformation. - Compute the explained variance ratio. 3. **Incremental PCA**: - Implement an Incremental PCA model to process the dataset in batches of `2000` samples. - Reduce the dimensionality of the dataset to `50` components. - Measure the time taken for the transformation. - Compute the explained variance ratio. 4. **Randomized PCA**: - Fit a PCA model using randomized SVD to the dataset. - Reduce the dimensionality of the dataset to `50` components. - Measure the time taken for the transformation. - Compute the explained variance ratio. 5. **Comparison and Analysis**: - Compare the time taken for the transformation by each PCA technique. - Compare the explained variance ratios of the components obtained from each technique. - Discuss the suitability of each method for large datasets and their memory consumption. # Requirements - Your code should be well-documented, and each function should have appropriate comments. - Ensure that the code handles any potential errors, such as missing files or data inconsistencies. - Visualize the explained variance ratio for each PCA technique using a bar chart. # Expected Input and Output Formats Input: - `large_dataset.csv`: CSV file containing the dataset. Output: - Print the time taken for each PCA method. - Print and plot the explained variance ratio for each PCA method. - Provide a written analysis comparing the techniques. ```python import pandas as pd import numpy as np import time from sklearn.decomposition import PCA, IncrementalPCA # 1. Load the dataset data = pd.read_csv(\'large_dataset.csv\') print(data.info()) # 2. Standard PCA start_time = time.time() pca = PCA(n_components=50) X_pca = pca.fit_transform(data) time_taken_standard_pca = time.time() - start_time explained_variance_ratio_standard_pca = pca.explained_variance_ratio_ print(f\\"Standard PCA time taken: {time_taken_standard_pca} seconds\\") print(f\\"Standard PCA explained variance ratio: {explained_variance_ratio_standard_pca}\\") # 3. Incremental PCA start_time = time.time() incremental_pca = IncrementalPCA(n_components=50, batch_size=2000) X_incremental_pca = incremental_pca.fit_transform(data) time_taken_incremental_pca = time.time() - start_time explained_variance_ratio_incremental_pca = incremental_pca.explained_variance_ratio_ print(f\\"Incremental PCA time taken: {time_taken_incremental_pca} seconds\\") print(f\\"Incremental PCA explained variance ratio: {explained_variance_ratio_incremental_pca}\\") # 4. Randomized PCA start_time = time.time() randomized_pca = PCA(n_components=50, svd_solver=\'randomized\') X_randomized_pca = randomized_pca.fit_transform(data) time_taken_randomized_pca = time.time() - start_time explained_variance_ratio_randomized_pca = randomized_pca.explained_variance_ratio_ print(f\\"Randomized PCA time taken: {time_taken_randomized_pca} seconds\\") print(f\\"Randomized PCA explained variance ratio: {explained_variance_ratio_randomized_pca}\\") # 5. Comparison and Analysis (Include plot code) import matplotlib.pyplot as plt labels = [\'Standard PCA\', \'Incremental PCA\', \'Randomized PCA\'] times = [time_taken_standard_pca, time_taken_incremental_pca, time_taken_randomized_pca] variance_ratios = [explained_variance_ratio_standard_pca.sum(), explained_variance_ratio_incremental_pca.sum(), explained_variance_ratio_randomized_pca.sum()] plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plt.bar(labels, times) plt.ylabel(\'Time in seconds\') plt.title(\'Time taken by PCA techniques\') plt.subplot(1, 2, 2) plt.bar(labels, variance_ratios) plt.ylabel(\'Explained Variance Ratio\') plt.title(\'Explained Variance Ratio by PCA techniques\') plt.tight_layout() plt.show() # Provide a written analysis in comments or a markdown cell if using a Jupyter notebook. ``` # Constraints - Ensure that the dataset fits into memory when using Standard PCA and Randomized PCA. - Handle large datasets efficiently with Incremental PCA. - Evaluate the time and variance ratio accurately. # Performance Expectations - The Randomized PCA should be faster than Standard PCA while maintaining a comparable explained variance ratio. - Incremental PCA should be able to handle large datasets and be efficient in terms of memory usage.","solution":"import pandas as pd import numpy as np import time from sklearn.decomposition import PCA, IncrementalPCA import matplotlib.pyplot as plt def load_dataset(filepath): Loads the dataset from a CSV file and returns the DataFrame. try: data = pd.read_csv(filepath) return data except FileNotFoundError: print(f\\"File {filepath} not found.\\") return None def apply_standard_pca(data, n_components=50): Applies Standard PCA on the dataset. start_time = time.time() pca = PCA(n_components=n_components) X_pca = pca.fit_transform(data) time_taken = time.time() - start_time explained_variance_ratio = pca.explained_variance_ratio_ return X_pca, time_taken, explained_variance_ratio def apply_incremental_pca(data, n_components=50, batch_size=2000): Applies Incremental PCA on the dataset. start_time = time.time() ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size) X_ipca = ipca.fit_transform(data) time_taken = time.time() - start_time explained_variance_ratio = ipca.explained_variance_ratio_ return X_ipca, time_taken, explained_variance_ratio def apply_randomized_pca(data, n_components=50): Applies Randomized PCA on the dataset. start_time = time.time() rpca = PCA(n_components=n_components, svd_solver=\'randomized\') X_rpca = rpca.fit_transform(data) time_taken = time.time() - start_time explained_variance_ratio = rpca.explained_variance_ratio_ return X_rpca, time_taken, explained_variance_ratio def plot_comparison(times, variance_ratios): Plots the comparison of time taken and explained variance ratios for different PCA techniques. labels = [\'Standard PCA\', \'Incremental PCA\', \'Randomized PCA\'] plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plt.bar(labels, times) plt.ylabel(\'Time in seconds\') plt.title(\'Time taken by PCA techniques\') plt.subplot(1, 2, 2) plt.bar(labels, variance_ratios) plt.ylabel(\'Explained Variance Ratio\') plt.title(\'Explained Variance Ratio by PCA techniques\') plt.tight_layout() plt.show() def main(): # 1. Load the dataset filepath = \'large_dataset.csv\' data = load_dataset(filepath) if data is None: return # Print basic information about the dataset print(data.info()) # 2. Standard PCA X_pca, time_taken_pca, explained_variance_ratio_pca = apply_standard_pca(data) print(f\\"Standard PCA time taken: {time_taken_pca} seconds\\") print(f\\"Standard PCA explained variance ratio: {explained_variance_ratio_pca}\\") # 3. Incremental PCA X_ipca, time_taken_ipca, explained_variance_ratio_ipca = apply_incremental_pca(data) print(f\\"Incremental PCA time taken: {time_taken_ipca} seconds\\") print(f\\"Incremental PCA explained variance ratio: {explained_variance_ratio_ipca}\\") # 4. Randomized PCA X_rpca, time_taken_rpca, explained_variance_ratio_rpca = apply_randomized_pca(data) print(f\\"Randomized PCA time taken: {time_taken_rpca} seconds\\") print(f\\"Randomized PCA explained variance ratio: {explained_variance_ratio_rpca}\\") # 5. Comparison and Analysis times = [time_taken_pca, time_taken_ipca, time_taken_rpca] variance_ratios = [sum(explained_variance_ratio_pca), sum(explained_variance_ratio_ipca), sum(explained_variance_ratio_rpca)] plot_comparison(times, variance_ratios) if __name__ == \\"__main__\\": main()"},{"question":"Objective You are provided with a data file `sample_data.csv` containing information about various products sold in a store. The file contains columns: `ProductID`, `ProductName`, `Category`, `Price`, `QuantitySold`, and `SaleDate`. Your task is to implement a function that analyzes this dataset to extract various insights. Task Implement the function `analyze_sales_data(file_path: str) -> dict` that performs the following operations: 1. **Load the Data:** - Read the CSV file into a Pandas DataFrame. 2. **Data Cleaning:** - Handle missing values: - Fill missing `QuantitySold` values with 0. - Drop any rows where the `ProductID`, `ProductName`, and `Category` are missing. 3. **Data Transformation:** - Convert the `SaleDate` column to datetime format. 4. **Data Aggregation and Analysis:** - Calculate the total sales for each product (`TotalSales = Price * QuantitySold`). - Find the top 5 categories with the highest total sales. - For each of these top categories, determine the product with the highest sales. 5. **Output:** - Return a dictionary with the following structure: ```python { \'top_categories\': [ {\'category\': \'CategoryName1\', \'top_product\': \'ProductName1\', \'total_sales\': total_sales_1}, {\'category\': \'CategoryName2\', \'top_product\': \'ProductName2\', \'total_sales\': total_sales_2}, # ... up to top 5 categories ] } ``` Constraints - You may assume the `file_path` exists and points to a valid CSV file. - You may assume `Price` and `QuantitySold` columns contain numeric values. Example Given the following `sample_data.csv`: | ProductID | ProductName | Category | Price | QuantitySold | SaleDate | |-----------|---------------|----------|-------|--------------|------------| | 1 | Product A | Cat1 | 10.0 | 5 | 2022-01-01 | | 2 | Product B | Cat2 | 15.0 | 2 | 2022-01-03 | | 3 | Product C | Cat1 | 7.5 | 8 | 2022-01-04 | | 4 | Product D | Cat3 | 25.0 | | 2022-01-10 | Your function should return a dictionary similar to: ```python { \'top_categories\': [ {\'category\': \'Cat1\', \'top_product\': \'Product C\', \'total_sales\': 60.0}, {\'category\': \'Cat2\', \'top_product\': \'Product B\', \'total_sales\': 30.0}, {\'category\': \'Cat3\', \'top_product\': \'Product D\', \'total_sales\': 0.0} ] } ``` Function Signature ```python def analyze_sales_data(file_path: str) -> dict: pass ```","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> dict: # Load the Data df = pd.read_csv(file_path) # Data Cleaning df[\'QuantitySold\'] = df[\'QuantitySold\'].fillna(0) df.dropna(subset=[\'ProductID\', \'ProductName\', \'Category\'], inplace=True) # Data Transformation df[\'SaleDate\'] = pd.to_datetime(df[\'SaleDate\'], errors=\'coerce\') # Data Aggregation and Analysis df[\'TotalSales\'] = df[\'Price\'] * df[\'QuantitySold\'] # Calculate total sales per category and find the top 5 category_sales = df.groupby(\'Category\')[\'TotalSales\'].sum().reset_index() top_categories = category_sales.nlargest(5, \'TotalSales\') top_categories_list = [] for _, row in top_categories.iterrows(): category = row[\'Category\'] category_df = df[df[\'Category\'] == category] top_product = category_df.loc[category_df[\'TotalSales\'].idxmax()] top_categories_list.append({ \'category\': category, \'top_product\': top_product[\'ProductName\'], \'total_sales\': top_product[\'TotalSales\'] }) return {\'top_categories\': top_categories_list}"},{"question":"**Dynamic Graph Tracing with PyTorch** In this task, you will implement a function that utilizes PyTorch\'s Dynamo to create an FX graph for a given model. Your function should also demonstrate an aspect of handling dynamic shapes. # Objective Implement a function `trace_model` that traces a PyTorch model using Dynamo and handles dynamic shapes. # Function Signature ```python import torch from torch import nn from torch._dynamo import optimize def trace_model(model: nn.Module, input_tensor: torch.Tensor): Traces the given model with Dynamo and handles dynamic shapes. Parameters: - model: nn.Module, a PyTorch model to be traced - input_tensor: torch.Tensor, input tensor to pass through the model during tracing Returns: - A list of strings representing the graph of operations traced by Dynamo # Example Models and Usage: class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) model = SimpleModel() input_tensor = torch.randn(2, 10) trace_list = trace_model(model, input_tensor) for op in trace_list: print(op) ``` # Constraints 1. The model can contain linear layers, activation functions, and simple tensor manipulations (like reshaping, permuting). 2. The input tensor may have dynamic batch size or sequence length. 3. Your function should handle dynamic shapes by using symbolic integer tracing as discussed in the documentation. 4. You should ensure the returned trace list excludes any irrelevant operations that are not part of the PyTorch graph. # Notes 1. Use PyTorch to create models. 2. Understand and implement the tracing with Dynamo as per the provided documentation. 3. Handling dynamic shapes is a critical aspect, ensure that the graph is generic enough to accommodate different input shapes. Consider this example: ```python import torch from torch import nn from torch._dynamo import optimize def trace_model(model: nn.Module, input_tensor: torch.Tensor): @optimize(\\"float\\") def compiled_fn(inputs): return model(inputs) symbolic_traced_graph = compiled_fn(input_tensor) fx_graph = symbolic_traced_graph.graph trace_list = [] for node in fx_graph.nodes: trace_list.append(str(node)) return trace_list # Example Models and Usage: class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) model = SimpleModel() input_tensor = torch.randn(2, 10) trace_list = trace_model(model, input_tensor) for op in trace_list: print(op) ``` # Expected Output The output should consist of a list of strings detailing the traced operations of the PyTorch model. Validate the dynamism by altering the tensor input shapes and verify the handling of dynamic shapes.","solution":"import torch from torch import nn from torch.fx import symbolic_trace def trace_model(model: nn.Module, input_tensor: torch.Tensor): Traces the given model with symbolic tracing and handles dynamic shapes. Parameters: - model: nn.Module, a PyTorch model to be traced - input_tensor: torch.Tensor, input tensor to pass through the model during tracing Returns: - A list of strings representing the graph of operations traced # Perform symbolic tracing traced_graph = symbolic_trace(model) # Forward pass to ensure input tensor goes through model _ = traced_graph(input_tensor) # Extract operations as list of strings trace_list = [] for node in traced_graph.graph.nodes: trace_list.append(str(node)) return trace_list # Example Models and Usage Example (not to be executed in this block): # class SimpleModel(nn.Module): # def __init__(self): # super(SimpleModel, self).__init__() # self.linear = nn.Linear(10, 5) # def forward(self, x): # return self.linear(x) # model = SimpleModel() # input_tensor = torch.randn(2, 10) # trace_list = trace_model(model, input_tensor) # for op in trace_list: # print(op)"},{"question":"You are required to write a function that leverages the `glob` module to find and categorize files in a directory based on their extensions. Specifically, you will write a function named `categorize_files_by_extension` that: 1. Takes in a directory path string `dir_path` and a boolean `recursive`. 2. Returns a dictionary where the key is the file extension (e.g., `.txt`, `.gif`) and the value is a list of file paths with that extension. 3. Uses recursive directory traversal if the `recursive` parameter is set to `True`. # Input - `dir_path` (str): A string containing the path to the directory. - `recursive` (bool): A boolean flag indicating whether to traverse directories recursively. # Output - A dictionary with file extensions as keys and lists of file paths as values. # Constraints - The `dir_path` is guaranteed to be a valid directory path. - File paths should be relative to `dir_path`. - If a file does not have an extension, it should be categorized under an empty string key `\\"\\"`. # Example ```python import os # Example directory structure: # test_dir/ # ├── 1.txt # ├── 2.gif # ├── 3.txt # ├── sub_dir/ # ├── 4.gif # Sample usage result = categorize_files_by_extension(\'test_dir\', recursive=True) print(result) # Output: # { # \'.txt\': [\'test_dir/1.txt\', \'test_dir/3.txt\'], # \'.gif\': [\'test_dir/2.gif\', \'test_dir/sub_dir/4.gif\'] # } ``` # Function Signature ```python def categorize_files_by_extension(dir_path: str, recursive: bool) -> dict: pass ``` # Notes - Use the `glob` module for file pattern matching. - Ensure to include files that might not have any extension. - Handle both cases: recursive and non-recursive directory traversal.","solution":"import os import glob def categorize_files_by_extension(dir_path: str, recursive: bool) -> dict: Categorizes files in a directory by their extension. Parameters: dir_path (str): The path to the directory. recursive (bool): Whether to traverse directories recursively. Returns: dict: A dictionary with file extensions as keys and lists of file paths as values. file_pattern = \\"**/*\\" if recursive else \\"*\\" files = glob.glob(os.path.join(dir_path, file_pattern), recursive=recursive) categorized_files = {} for file in files: if os.path.isfile(file): _, ext = os.path.splitext(file) if ext not in categorized_files: categorized_files[ext] = [] categorized_files[ext].append(file) return categorized_files"},{"question":"Objective Demonstrate the ability to use the seaborn.objects interface to create complex plots using loaded datasets, including grouping, aggregation, and customization. Question You are given two datasets: 1. `dowjones` representing Dow Jones stock prices with the columns: `Date` and `Price`. 2. `fmri` representing fMRI data with the columns: `timepoint`, `signal`, `region`, `subject`, and `event`. Your task is to visualize these datasets using seaborn’s objects interface to answer the following requirements: 1. **Dow Jones Dataset**: - Create a line plot of the stock `Price` over `Date`. - Modify the orientation so that the x-axis represents `Price` and the y-axis represents `Date`. 2. **fMRI Dataset**: - Filter the dataset to include only the region `parietal` and the event `stim`. - Create a line plot of `signal` over `timepoint` for each `subject`, with a line color of `0.2` (dark gray) and a line width of 1. - On the same plot, add another line plot where `timepoint` is on the x-axis and `signal` is on the y-axis, but this time: - Group by `region` for `color` and by `event` for `linestyle`. - Include `error bands` in the plot. - Add markers at each data point to indicate sampled values with white edges. # Input None. Datasets are to be loaded using seaborn’s `load_dataset` function. # Output The output should be two plots: 1. A line plot for the Dow Jones dataset. 2. A multi-featured line plot for the fMRI dataset. # Constraints - Use the seaborn.objects interface as demonstrated in the provided documentation. - The plot customization should make use of grouping, aggregation, error bands, and markers. # Example Solution Here is an example of a structured implementation using the seaborn.objects interface: ```python import seaborn.objects as so from seaborn import load_dataset # Load the datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Plot 1: Dow Jones Dataset plot1 = so.Plot(dowjones, \\"Date\\", \\"Price\\").add(so.Line()) plot1_oriented = so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line(), orient=\\"y\\") # Plot 2: fMRI Dataset fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") plot2 = ( fmri_filtered .pipe(so.Plot, \\"timepoint\\", \\"signal\\") .add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") ) p = so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") plot2_combined = ( p .add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) ) # Display the plots plot1.show() plot1_oriented.show() plot2.show() plot2_combined.show() ``` # Note Ensure your solution is thoroughly tested before submission and adheres to all constraints outlined above.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Plot 1: Dow Jones Dataset - Line plot with Stock Price over Date and orientation change plot1_oriented = ( so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\") .add(so.Line(), orient=\\"y\\") ) # Plot 2: fMRI Dataset with various customizations # Filter the dataset first fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Base line plot for each subject plot2_base = ( fmri_filtered .pipe(so.Plot, \\"timepoint\\", \\"signal\\") .add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") ) # Combined plot with additional customization plot2_combined = ( so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line()) .add(so.Band()) .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\")) ) # Display the plots plot1_oriented.show() plot2_base.show() plot2_combined.show()"},{"question":"Objective: Implement a function that generates, compares, and analyzes UUIDs using Python\'s `uuid` module. This function will demonstrate your understanding of UUID generation methods, attributes, and safety checks. Problem Statement: You are required to implement the function `analyze_uuids()`. This function should: 1. Generate five different UUIDs using the following methods: `uuid1()`, `uuid3(uuid.NAMESPACE_DNS, \'example.org\')`, `uuid4()`, `uuid5(uuid.NAMESPACE_DNS, \'example.org\')`, and `uuid.UUID(\'12345678123456781234567812345678\')`. 2. Compare the generated UUIDs and determine which UUID (from the generations) appears first when sorted based on their integer values. 3. For each generated UUID, determine if it was created in a multiprocessing-safe way. 4. Convert the generated UUIDs to their string representations and their decomposed fields. # Function Signature ```python def analyze_uuids() -> str: pass ``` # Expected Output The function should return a formatted string that contains: - The UUIDs in their string representations. - The UUID that appears first when sorted. - The safety status of each UUID. - The decomposed fields of each UUID. # Example Output ``` UUID1: a8098c1a-f86e-11da-bd1a-00112444be1e UUID3: 6fa459ea-ee8a-3ca4-894e-db77e160355e UUID4: 16fd2706-8baf-433b-82eb-8c7fada847da UUID5: 886313e1-3b8a-5372-9b90-0c9aee199e5d UUID from hex string: 12345678-1234-5678-1234-567812345678 First UUID when sorted: 16fd2706-8baf-433b-82eb-8c7fada847da UUID Safety: UUID1: unknown UUID3: unknown UUID4: unknown UUID5: unknown UUID from hex string: unknown UUID Decomposed Fields: UUID1: (281474976710656L, 9999, 9999, 99, 9999, 999999999999L) UUID3: (1885225941, 18888, 15544, 188, 894E, 123456123456L) UUID4: (12515234, 25051, 25055, 82, 800, 82eb8c7fada847daL) UUID5: (992231212341, 12341, 99812, 999, 444231231, 12341231234L) UUID from hex string: (12345678, 1234, 5678, 1234, 567812345678L) ``` **Note:** The actual UUID values, safety status, and decomposed fields will vary based on your implementation environment and the UUID generation. Adjust the output format as necessary to match the exact outputs of your program. Constraints: - Ensure that your function handles UUID generation, comparison, safety check, and field decomposition correctly. - Consider using the attributes `UUID.int`, `UUID.is_safe`, and `UUID.fields` for necessary operations.","solution":"import uuid def analyze_uuids() -> str: # Generate different UUIDs uuid1 = uuid.uuid1() uuid3 = uuid.uuid3(uuid.NAMESPACE_DNS, \'example.org\') uuid4 = uuid.uuid4() uuid5 = uuid.uuid5(uuid.NAMESPACE_DNS, \'example.org\') uuid_from_hex = uuid.UUID(\'12345678123456781234567812345678\') # Collect UUIDs uuids = [uuid1, uuid3, uuid4, uuid5, uuid_from_hex] # Find the first UUID when sorted based on their integer values first_uuid = sorted(uuids, key=lambda u: u.int)[0] # Check if the UUIDs were created in a multiprocessing-safe way safety_status = {uuid1: uuid1.is_safe, uuid3: uuid3.is_safe, uuid4: uuid4.is_safe, uuid5: uuid5.is_safe, uuid_from_hex: uuid_from_hex.is_safe} # Convert UUIDs to their string representations and get their decomposed fields uuid_strings = [str(u) for u in uuids] uuid_fields = {u: u.fields for u in uuids} result = [] result.append(f\\"UUID1: {uuid1}\\") result.append(f\\"UUID3: {uuid3}\\") result.append(f\\"UUID4: {uuid4}\\") result.append(f\\"UUID5: {uuid5}\\") result.append(f\\"UUID from hex string: {uuid_from_hex}\\") result.append(f\\"nFirst UUID when sorted: {first_uuid}\\") result.append(\\"nUUID Safety:\\") for u in uuids: safety = \\"safe\\" if safety_status[u] else \\"not safe\\" result.append(f\\"{u}: {safety}\\") result.append(\\"nUUID Decomposed Fields:\\") for u in uuids: result.append(f\\"{u}: {uuid_fields[u]}\\") return \\"n\\".join(result)"},{"question":"**Problem: Financial Transaction Simulation** You are tasked with creating a financial transaction simulation system. This system tracks users\' transactions, applies taxes, and ensures all calculations are precise to avoid rounding errors typically associated with floating-point arithmetic. Use the `decimal` module to handle all financial calculations accurately. **Function Specification:** Write a function `simulate_transactions(transactions: List[Dict[str, Any]], tax_rate: Decimal) -> List[Dict[str, Any]]` that processes a list of user transactions, applies a specified tax rate, and returns a modified list of transactions with taxes applied and final amounts. **Input:** - `transactions`: A list of dictionaries, where each dictionary represents a transaction containing the following keys: - `user_id` (str): The unique identifier of the user. - `amount` (str): The transaction amount as a string (to maintain precision). - `tax_rate`: A `Decimal` representing the tax rate to be applied (e.g., `Decimal(\'0.05\')` for a 5% tax rate). **Output:** - A list of dictionaries, with each dictionary representing a transaction containing the following keys: - `user_id` (str): The unique identifier of the user. - `original_amount` (Decimal): The original transaction amount. - `tax_amount` (Decimal): The calculated tax amount. - `final_amount` (Decimal): The amount after adding the tax. **Constraints:** - The `amount` in each transaction will always be a valid decimal represented as a string. - The transactions list will contain at least one transaction. - The tax rate will always be a non-negative decimal less than or equal to 1. **Example:** ```python from decimal import Decimal transactions = [ {\'user_id\': \'user1\', \'amount\': \'100.00\'}, {\'user_id\': \'user2\', \'amount\': \'200.50\'}, ] tax_rate = Decimal(\'0.05\') # Expected Output: # [ # {\'user_id\': \'user1\', \'original_amount\': Decimal(\'100.00\'), \'tax_amount\': Decimal(\'5.00\'), \'final_amount\': Decimal(\'105.00\')}, # {\'user_id\': \'user2\', \'original_amount\': Decimal(\'200.50\'), \'tax_amount\': Decimal(\'10.025\'), \'final_amount\': Decimal(\'210.525\')}, # ] result = simulate_transactions(transactions, tax_rate) print(result) ``` **Notes:** - You must use the `decimal` module for all calculations to ensure precision. - Consider edge cases such as zero amounts, varying tax rates, and large transaction amounts.","solution":"from decimal import Decimal from typing import List, Dict, Any def simulate_transactions(transactions: List[Dict[str, Any]], tax_rate: Decimal) -> List[Dict[str, Any]]: Processes a list of user transactions, applies a specified tax rate, and returns a modified list of transactions with taxes applied and final amounts. processed_transactions = [] for transaction in transactions: original_amount = Decimal(transaction[\'amount\']) tax_amount = original_amount * tax_rate final_amount = original_amount + tax_amount processed_transactions.append({ \'user_id\': transaction[\'user_id\'], \'original_amount\': original_amount, \'tax_amount\': tax_amount, \'final_amount\': final_amount }) return processed_transactions"},{"question":"# Custom JSON Encoder and Decoder For this problem, you will need to demonstrate advanced usage of the `json` module by handling a custom complex type (a `ComplexNumber` class) within JSON serialization and deserialization. Task 1. **ComplexNumber Class**: - Define a class `ComplexNumber` that represents a complex number with real and imaginary parts. - It should have the following methods: - `__init__(self, real: float, imag: float)`: Constructor to initialize the complex number. - `__repr__(self)`: Returns a string representation of the complex number in the form `\\"<real> + <imag>j\\"`. 2. **Custom JSON Encoding**: - Create a custom JSON encoder that extends the `json.JSONEncoder` class to handle the `ComplexNumber` class. - When encoding an object of `ComplexNumber`, it should be represented as a dictionary with two keys: `\\"real\\"` and `\\"imag\\"`. 3. **Custom JSON Decoding**: - Create a custom JSON decoder using the `json.loads()` function with the `object_hook` parameter to handle deserialization of `ComplexNumber` objects. - The decoder should be able to recognize the dictionary format generated by the custom encoder and convert it back into a `ComplexNumber` object. 4. **Main Function**: - Write a main function that demonstrates both encoding and decoding of `ComplexNumber` objects using the custom encoder and decoder. Example Usage ```python # Assuming your definitions are correct, the following code should work: # Creating a ComplexNumber object c = ComplexNumber(1.5, -2.3) # Serializing ComplexNumber object to JSON json_str = json.dumps(c, cls=ComplexEncoder) print(json_str) # Output: {\\"real\\": 1.5, \\"imag\\": -2.3} # Deserializing JSON back to ComplexNumber object c_new = json.loads(json_str, object_hook=as_complex) print(c_new) # Output: 1.5 + -2.3j ``` Constraints - The `ComplexNumber` class should only handle real and imaginary parts as floats. - You should ensure that all necessary error handling is in place, e.g., invalid JSON formats. Requirements - Your implementation should include definitions for `ComplexNumber`, `ComplexEncoder`, and the `as_complex` function. - The main function should demonstrate the complete flow from encoding to decoding. ```python # Implement your solution here import json class ComplexNumber: def __init__(self, real: float, imag: float): self.real = real self.imag = imag def __repr__(self): return f\\"{self.real} + {self.imag}j\\" class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, ComplexNumber): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return json.JSONEncoder.default(self, obj) def as_complex(dct): if \\"real\\" in dct and \\"imag\\" in dct: return ComplexNumber(dct[\\"real\\"], dct[\\"imag\\"]) return dct def main(): # Test encoding c = ComplexNumber(1.5, -2.3) json_str = json.dumps(c, cls=ComplexEncoder) print(json_str) # {\\"real\\": 1.5, \\"imag\\": -2.3} # Test decoding c_new = json.loads(json_str, object_hook=as_complex) print(c_new) # 1.5 + -2.3j if __name__ == \\"__main__\\": main() ```","solution":"import json class ComplexNumber: def __init__(self, real: float, imag: float): self.real = real self.imag = imag def __repr__(self): return f\\"{self.real} + {self.imag}j\\" class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, ComplexNumber): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return json.JSONEncoder.default(self, obj) def as_complex(dct): if \\"real\\" in dct and \\"imag\\" in dct: return ComplexNumber(dct[\\"real\\"], dct[\\"imag\\"]) return dct def main(): # Test encoding c = ComplexNumber(1.5, -2.3) json_str = json.dumps(c, cls=ComplexEncoder) print(json_str) # {\\"real\\": 1.5, \\"imag\\": -2.3} # Test decoding c_new = json.loads(json_str, object_hook=as_complex) print(c_new) # 1.5 + -2.3j if __name__ == \\"__main__\\": main()"},{"question":"Objective: You are asked to create a simple text-based GUI interface using the Python `curses` module. This interface should allow a user to navigate through a menu using the arrow keys, select an option, and display corresponding information. Task: Implement a Python program using the `curses` module that performs the following: 1. Initializes the `curses` environment. 2. Displays a menu with the following options: - \\"Option 1\\" - \\"Option 2\\" - \\"Option 3\\" - \\"Exit\\" 3. Allows the user to navigate through the menu using the up and down arrow keys. 4. When the user presses \\"Enter\\" on \\"Option 1\\", \\"Option 2\\", or \\"Option 3\\", display a message in a new window saying \\"You selected Option X\\", where X is the selected option number. 5. Allow the user to exit the program by selecting the \\"Exit\\" option and pressing \\"Enter\\". 6. Properly handle the termination of the `curses` application to restore the terminal state. Input and Output: - **Input:** Arrow keys to navigate, Enter key to select. - **Output:** Terminal GUI with menu and selected option message. Constraints: - The program should handle resizing the terminal window gracefully. - Ensure that the application does not crash on invalid key presses and handles exceptions properly. Example: If the user selects \\"Option 1\\" and presses Enter: ``` ---------------------------------------- | Menu | |--------------------------------------| | > Option 1 | | Option 2 | | Option 3 | | Exit | ---------------------------------------- ``` The new window will show: ``` ---------------------------------------- | You selected Option 1 | | Press any key to return to menu | ---------------------------------------- ``` Performance Requirements: - The program should efficiently handle user input and screen refreshing without noticeable delays. Additional Notes: - Employ the `curses` wrapper function to manage initialization and termination. - Use appropriate `curses` functions for handling text and window operations. - Implement exception handling to close the `curses` environment gracefully.","solution":"import curses def display_menu(stdscr): curses.curs_set(0) # Hide the cursor stdscr.clear() k = 0 current_row = 0 menu = [\'Option 1\', \'Option 2\', \'Option 3\', \'Exit\'] while True: stdscr.clear() h, w = stdscr.getmaxyx() for idx, row in enumerate(menu): x = w//2 - len(row)//2 y = h//2 - len(menu)//2 + idx if idx == current_row: stdscr.attron(curses.color_pair(1)) stdscr.addstr(y, x, row) stdscr.attroff(curses.color_pair(1)) else: stdscr.addstr(y, x, row) stdscr.refresh() k = stdscr.getch() if k == curses.KEY_UP and current_row > 0: current_row -= 1 elif k == curses.KEY_DOWN and current_row < len(menu) - 1: current_row += 1 elif k == curses.KEY_ENTER or k in [10, 13]: if menu[current_row] == \'Exit\': break elif menu[current_row] in [\'Option 1\', \'Option 2\', \'Option 3\']: stdscr.clear() option_selected(stdscr, menu[current_row]) def option_selected(stdscr, option): stdscr.clear() h, w = stdscr.getmaxyx() msg = f\\"You selected {option}\\" stdscr.addstr(h//2, w//2 - len(msg)//2, msg) stdscr.addstr(h//2 + 1, w//2 - len(\\"Press any key to return to menu\\")//2, \\"Press any key to return to menu\\") stdscr.refresh() stdscr.getch() stdscr.clear() def main(stdscr): curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) display_menu(stdscr) if __name__ == \'__main__\': curses.wrapper(main)"},{"question":"Synthesized Data Generation and Analysis You are required to demonstrate your understanding of synthetic data generation using the scikit-learn `datasets` module. You will generate a synthetic dataset, preprocess it, and visualize it effectively. Follow the detailed steps below to complete this task. # Steps: 1. **Generate the Data:** Use the `make_classification` function to generate a synthetic dataset with the following parameters: - `n_samples=1000`: Number of samples. - `n_features=20`: Number of features. - `n_informative=15`: Number of informative features. - `n_redundant=5`: Number of redundant features. - `n_classes=3`: Number of classes. - `flip_y=0.01`: Fraction of samples whose class is randomly assigned. - `random_state=42`: To ensure reproducibility. 2. **Preprocess the Data:** - Standardize the features such that each feature has zero mean and unit variance using `StandardScaler` from `sklearn.preprocessing`. 3. **Visualize the Data:** - Perform Principal Component Analysis (PCA) on the standardized data to reduce it to 2 dimensions. - Plot the 2D projection of the data points, coloring them by their class label. # Functions to Implement: 1. `generate_data()`: - **Input:** No input. - **Output:** Tuple `(X, y)` where `X` is the feature matrix and `y` is the target vector. 2. `preprocess_data(X)`: - **Input:** `X` - The feature matrix. - **Output:** The standardized feature matrix `X_scaled`. 3. `visualize_data(X_scaled, y)`: - **Input:** `X_scaled` - The standardized feature matrix. `y` - The target vector. - **Output:** A 2D scatter plot showing the PCA projection of the data points, colored by class label. # Example Code: ```python from sklearn.datasets import make_classification from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA import matplotlib.pyplot as plt def generate_data(): # Generate the dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, flip_y=0.01, random_state=42) return X, y def preprocess_data(X): # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled def visualize_data(X_scaled, y): # Perform PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Create scatter plot plt.figure(figsize=(10, 7)) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\') plt.colorbar() plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'2D PCA Projection of the Dataset\') plt.show() # Example usage X, y = generate_data() X_scaled = preprocess_data(X) visualize_data(X_scaled, y) ``` # Notes: - Ensure your solution works efficiently with the specified data dimensions. - Make sure your plots are clear and properly labeled. Good luck and happy coding!","solution":"from sklearn.datasets import make_classification from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA import matplotlib.pyplot as plt def generate_data(): Generates a synthetic dataset with specified parameters. Returns: X (numpy.ndarray): Feature matrix. y (numpy.ndarray): Target vector. X, y = make_classification( n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, flip_y=0.01, random_state=42 ) return X, y def preprocess_data(X): Standardizes the feature matrix. Args: X (numpy.ndarray): The feature matrix. Returns: numpy.ndarray: The standardized feature matrix. scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled def visualize_data(X_scaled, y): Visualizes the data by reducing its dimensions to 2 using PCA and plotting it. Args: X_scaled (numpy.ndarray): The standardized feature matrix. y (numpy.ndarray): The target vector. pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) plt.figure(figsize=(10, 7)) scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\') plt.colorbar(scatter) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'2D PCA Projection of the Dataset\') plt.show()"},{"question":"Seaborn Plotting Context Objective Your task is to write a function that visualizes a set of data using different plotting contexts in seaborn. The function should produce two plots side by side for comparison. Description Write a function `compare_plotting_contexts(data, x, y)` that takes in a DataFrame `data` and two strings `x` and `y` representing the column names to be used for the x and y axes, respectively. The function should: 1. Generate a line plot of the data using seaborn\'s default plotting context. 2. Generate a similar line plot, but this time using a predefined plotting context \\"talk\\". 3. Display both plots side by side for comparison. Function Signature ```python def compare_plotting_contexts(data, x, y): pass ``` Input - `data`: A `pandas.DataFrame` containing the data to be plotted. - `x`: A string representing the name of the column for the x-axis. - `y`: A string representing the name of the column for the y-axis. Output The function should display two seaborn line plots side by side. Constraints - Assume `data` contains valid columns `x` and `y`. - Use seaborn for creating the plots. - Use `matplotlib.pyplot` for displaying plots side by side. Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample Data data = pd.DataFrame({ \'Time\': [\'A\', \'B\', \'C\', \'D\'], \'Value\': [1, 4, 2, 5] }) # Function Implementation def compare_plotting_contexts(data, x, y): # Set up the matplotlib figure fig, axes = plt.subplots(1, 2, figsize=(12, 5)) # Plot with default context sns.lineplot(data=data, x=x, y=y, ax=axes[0]) axes[0].set_title(\'Default Context\') # Plot with \'talk\' context with sns.plotting_context(\\"talk\\"): sns.lineplot(data=data, x=x, y=y, ax=axes[1]) axes[1].set_title(\'Talk Context\') plt.show() # Example execution compare_plotting_contexts(data, \'Time\', \'Value\') ``` This function should generate two line plots: one using the seaborn default plotting context and the other using the \\"talk\\" context, displaying them side by side for visual comparison.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def compare_plotting_contexts(data, x, y): Generates two line plots side by side comparing seaborn\'s default context with \'talk\' context. Parameters: data: pandas.DataFrame - DataFrame containing the data to be plotted. x: str - Name of the column for the x-axis. y: str - Name of the column for the y-axis. # Set up the matplotlib figure fig, axes = plt.subplots(1, 2, figsize=(12, 5)) # Plot with default context sns.lineplot(data=data, x=x, y=y, ax=axes[0]) axes[0].set_title(\'Default Context\') # Plot with \'talk\' context with sns.plotting_context(\\"talk\\"): sns.lineplot(data=data, x=x, y=y, ax=axes[1]) axes[1].set_title(\'Talk Context\') plt.show()"},{"question":"**Question: Implement a Custom Sequence Type in Python** Your task is to create a custom sequence type by adhering to Python\'s sequence protocol. The sequence should behave like a list, supporting item access, slicing, and length operations. Additionally, implement custom methods to `reverse` the sequence and to get a `slice sum`. # Requirements: 1. **Initialization**: - The custom sequence should be initialized with an iterable (e.g., list, tuple). - Example: `my_seq = CustomSequence([1, 2, 3, 4])` 2. **Item Access**: - Implement item access using indexing, similar to lists. - Example: `my_seq[0]` should return `1`. 3. **Slicing**: - Implement slicing to return a new sequence from the custom sequence. - Example: `my_seq[1:3]` should return `CustomSequence([2, 3])`. 4. **Length**: - Implement a method to return the length of the sequence. - Example: `len(my_seq)` should return `4`. 5. **Reversal**: - Implement a method `reverse` that returns a new `CustomSequence`, which is the reverse of the original sequence. - Example: `my_seq.reverse()` should return `CustomSequence([4, 3, 2, 1])`. 6. **Slice Sum**: - Implement a method `slice_sum(start, end)` that returns the sum of the items between the indices `start` and `end`. - Example: `my_seq.slice_sum(1, 3)` should return `5` (as it is `2 + 3`). # Constraints: - All elements in the sequence are integers. - The implementation should handle illegal access (i.e., index out of range) appropriately by raising an `IndexError`. - Performance should be in line with typical list operations, ensuring efficient indexing and slicing. # Input and Output: - Initialization input: An iterable of integers. - Access and slicing input: Integer indices or slice objects. - `reverse` and `slice_sum` methods: specific method calls. # Sample Code Structure: ```python class CustomSequence: def __init__(self, iterable): # Implement initialization def __getitem__(self, index): # Implement item access def __len__(self): # Implement length retrieval def __repr__(self): # Implement string representation def reverse(self): # Implement sequence reversal def slice_sum(self, start, end): # Implement slice sum calculation # Example usage: my_seq = CustomSequence([1, 2, 3, 4]) print(my_seq[1]) # Output: 2 print(my_seq[1:3]) # Output: CustomSequence([2, 3]) print(len(my_seq)) # Output: 4 print(my_seq.reverse()) # Output: CustomSequence([4, 3, 2, 1]) print(my_seq.slice_sum(1, 3)) # Output: 5 ``` Your implementation should pass the example usage provided above.","solution":"class CustomSequence: def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): if isinstance(index, slice): return CustomSequence(self._data[index]) return self._data[index] def __len__(self): return len(self._data) def __repr__(self): return f\\"CustomSequence({self._data})\\" def reverse(self): return CustomSequence(reversed(self._data)) def slice_sum(self, start, end): return sum(self._data[start:end])"},{"question":"**Context**: You have been given a dataset containing information about different sessions on an e-commerce platform. The dataset includes session identifiers, user identifiers, page views during the session, the amount spent in that session, and the date and time when the session occurred. You are required to perform various group by operations using pandas to extract meaningful insights from this data. **Dataset Example**: ```python import pandas as pd import numpy as np np.random.seed(42) # for reproducible random results n = 100 # number of sessions data = { \\"session_id\\": range(n), \\"user_id\\": np.random.randint(1, 20, size=n), \\"page_views\\": np.random.randint(1, 100, size=n), \\"amount_spent\\": np.round(np.random.uniform(0.1, 500, size=n), 2), \\"session_date\\": pd.date_range(start=\\"2021-01-01\\", periods=n, freq=\\"H\\") } df = pd.DataFrame(data) print(df.head()) ``` **Tasks**: 1. **Aggregation**: - Group the data by `user_id` and compute the total `page_views` and total `amount_spent` for each user. - Identify the user who spent the most overall, and report the total amount spent by that user. 2. **Transformation**: - For each user, calculate the cumulative amount spent and add it as a new column `cumulative_amount_spent`. 3. **Filtration**: - Filter out users who have less than 5 sessions in total. 4. **Custom Aggregation**: - For each `session_date`, calculate: - The average `page_views`. - The sum of `amount_spent`. - The number of unique `user_id`s that had sessions on that date. - Provide the results as a DataFrame with columns `average_page_views`, `total_amount_spent`, and `unique_users`. 5. **Flexible Apply**: - Define a custom function that adds a new column `session_value` to the DataFrame, which is computed as `amount_spent / page_views`. - Use `apply` method to add this new column to the grouped DataFrame by `user_id`. **Constraints**: - You must use built-in pandas methods for group by operations wherever possible. - Efficiently handle missing values and edge cases. - Ensure your solution is optimized for performance. **Expected Output**: Your script should output the following: - A DataFrame showing the total `page_views` and `amount_spent` for each user. - The `user_id` of the user who spent the most, and the amount they spent. - The updated DataFrame with the `cumulative_amount_spent` column. - A DataFrame with aggregated statistics for each `session_date`. - The updated DataFrame with the new `session_value` column applied. **Code Submission**: Provide your complete Python code to accomplish the tasks above, ensuring it runs without errors and fulfills the requirements.","solution":"import pandas as pd import numpy as np np.random.seed(42) # for reproducible random results n = 100 # number of sessions data = { \\"session_id\\": range(n), \\"user_id\\": np.random.randint(1, 20, size=n), \\"page_views\\": np.random.randint(1, 100, size=n), \\"amount_spent\\": np.round(np.random.uniform(0.1, 500, size=n), 2), \\"session_date\\": pd.date_range(start=\\"2021-01-01\\", periods=n, freq=\\"H\\") } df = pd.DataFrame(data) # Task 1: Aggregation agg_df = df.groupby(\'user_id\').agg({\'page_views\': \'sum\', \'amount_spent\': \'sum\'}).reset_index() # Identifying the user who spent the most max_spender = agg_df.loc[agg_df[\'amount_spent\'].idxmax()] max_spender_user_id = max_spender[\'user_id\'] max_spender_amount = max_spender[\'amount_spent\'] # Task 2: Transformation - Cumulative amount spent df[\'cumulative_amount_spent\'] = df.groupby(\'user_id\')[\'amount_spent\'].cumsum() # Task 3: Filtration - Users with at least 5 sessions session_counts = df[\'user_id\'].value_counts() valid_users = session_counts[session_counts >= 5].index filtered_df = df[df[\'user_id\'].isin(valid_users)] # Task 4: Custom Aggregation by `session_date` custom_agg_df = df.groupby(\'session_date\').agg( average_page_views=(\'page_views\', \'mean\'), total_amount_spent=(\'amount_spent\', \'sum\'), unique_users=(\'user_id\', \'nunique\') ).reset_index() # Task 5: Flexible Apply - Adding \'session_value\' def calculate_session_value(group): group[\'session_value\'] = group[\'amount_spent\'] / group[\'page_views\'] return group df_with_session_value = df.groupby(\'user_id\').apply(calculate_session_value).reset_index(drop=True)"},{"question":"**Coding Assessment Question:** **Objective**: Implement a function to perform multiple set operations based on a sequence of commands provided as input. This will test your understanding of `set` operations in Python, including basic operations like add, discard, pop, and more advanced operations. # Problem Statement: You are to implement a function `process_set_operations(commands: List[str]) -> Set[Any]` that takes a list of command strings and executes them on an initially empty set. # Function Signature: ```python from typing import List, Set, Any def process_set_operations(commands: List[str]) -> Set[Any]: pass ``` # Input: - `commands`: A list of strings where each string is a command. Each command will be one of the following: - `\\"add <element>\\"`: Adds the `element` to the set. - `\\"discard <element>\\"`: Discards the `element` from the set. - `\\"pop\\"`: Removes and returns an arbitrary element from the set. Should raise a `KeyError` if set is empty. - `\\"clear\\"`: Clears all elements from the set. The `element` can be any value that is hashable (int, string, etc.). # Output: - The function should return the final state of the set after all commands have been executed. # Constraints: 1. The commands list will have at most 1000 commands. 2. The `element` provided in the commands is guaranteed to be hashable. 3. The set operations should work efficiently within the allowed constraints. # Example: ```python # Example Commands commands = [ \\"add 1\\", \\"add 2\\", \\"discard 1\\", \\"pop\\", \\"add 2\\", \\"add 3\\", \\"clear\\" ] # Example Function Call result = process_set_operations(commands) # Example Output print(result) # Output: set() -> The set will be empty after \\"clear\\" ``` # Explanation: 1. The command `\\"add 1\\"` adds the element `1` to the set. Set becomes: {1} 2. The command `\\"add 2\\"` adds the element `2` to the set. Set becomes: {1, 2} 3. The command `\\"discard 1\\"` removes the element `1` from the set. Set becomes: {2} 4. The command `\\"pop\\"` removes an arbitrary element from the set, which is `2`. Set becomes: set() 5. The command `\\"add 2\\"` adds the element `2` to the set. Set becomes: {2} 6. The command `\\"add 3\\"` adds the element `3` to the set. Set becomes: {2, 3} 7. The command `\\"clear\\"` removes all elements from the set. Set becomes: set() **Note**: - Ensure that you handle exceptions such as popping from an empty set correctly. - The function should perform the operations in the order they appear in the input list.","solution":"from typing import List, Set, Any def process_set_operations(commands: List[str]) -> Set[Any]: result_set = set() for command in commands: parts = command.split() op = parts[0] if op == \\"add\\" and len(parts) == 2: element = parts[1] result_set.add(element) elif op == \\"discard\\" and len(parts) == 2: element = parts[1] result_set.discard(element) elif op == \\"pop\\": try: result_set.pop() except KeyError: pass elif op == \\"clear\\": result_set.clear() return result_set"},{"question":"# Python Initialization and Configuration You have been given the task to embed Python within a C application. For this purpose, you need to ensure that Python initializes correctly and adheres to the specific configurations required by your application. Task **Write a Python function to simulate the initialization and configuration of Python using the `PyConfig` and `PyPreConfig` structures.** Your function should: 1. Define and initialize both `PyConfig` and `PyPreConfig` structures. 2. Configure the preinitialization settings to enable the UTF-8 mode. 3. Configure the main initialization with the following settings: - Set the program name to `/path/to/my_program`. - Append `/path/to/more/modules` to the module search paths. - Override the executable path with `/path/to/my_executable`. 4. Handle any exceptions or errors that occur during the initialization process. Input The function does not take any parameters. Output The function does not return anything. Instead, it should print messages indicating the success or failure of each initialization and configuration step. In case of an error, it should print the error message and exit with the appropriate exit code. # Example ```python def initialize_python(): # Your code here pass initialize_python() ``` Your implementation should follow the guidelines and methodologies described in the documentation provided. **Note**: You do not need to write C code. Instead, simulate the workflow using Python functions and print statements.","solution":"import sys def initialize_python(): try: print(\\"Initializing PyPreConfig structure...\\") # Simulating PyPreConfig initialization and setting UTF-8 mode pre_config = {\\"utf8_mode\\": True} print(\\"Set UTF-8 mode to\\", pre_config[\\"utf8_mode\\"]) print(\\"Initializing PyConfig structure...\\") # Simulating PyConfig initialization with configurations config = { \\"program_name\\": \\"/path/to/my_program\\", \\"module_search_paths\\": [\\"/path/to/more/modules\\"], \\"executable\\": \\"/path/to/my_executable\\" } print(\\"Set program name to\\", config[\\"program_name\\"]) print(\\"Module search path appended:\\", config[\\"module_search_paths\\"]) print(\\"Override executable path:\\", config[\\"executable\\"]) print(\\"Python initialization and configuration successful.\\") except Exception as e: print(\\"An error occurred during Python initialization:\\", str(e)) sys.exit(1) initialize_python()"},{"question":"# PyTorch Coding Assessment: Tensor Manipulation and Dimension Handling Objective: Demonstrate your understanding of the `torch.Size` class and tensor manipulation in PyTorch by implementing functions that handle tensor dimensions. Task: 1. **Function: `get_tensor_size`** - **Input:** A tensor `t` of arbitrary dimensions. - **Output:** A list containing the size of each dimension of the tensor. - **Example:** ```python t = torch.ones(5, 10, 15) result = get_tensor_size(t) print(result) # Output: [5, 10, 15] ``` 2. **Function: `change_tensor_shape`** - **Input:** A tensor `t` and a tuple `shape` specifying the new shape. - **Output:** A tensor reshaped to the specified dimensions. - **Constraints:** You should handle cases where the reshape is not possible by raising an appropriate error. - **Example:** ```python t = torch.ones(4, 4) new_shape = (2, 8) result = change_tensor_shape(t, new_shape) print(result.shape) # Output: torch.Size([2, 8]) ``` Guidelines: - Make sure your functions handle various edge cases, like tensors with no elements or dimensions of size 1. - Document your code to explain the logic and identify potential pitfalls. - Ensure that any operations you perform are efficient and leverage PyTorch\'s functionalities. ```python import torch def get_tensor_size(t): Returns the size of each dimension of the input tensor. Parameters: t (torch.Tensor): The input tensor. Returns: List[int]: A list containing the size of each dimension of the tensor. # Your implementation here pass def change_tensor_shape(t, shape): Reshapes the input tensor to the specified shape. Parameters: t (torch.Tensor): The input tensor. shape (tuple): The new shape. Returns: torch.Tensor: A tensor reshaped to the specified dimensions. Raises: ValueError: If the reshape is not possible due to incompatible dimensions. # Your implementation here pass # Example usage: if __name__ == \'__main__\': t = torch.ones(5, 10, 15) print(get_tensor_size(t)) # Output: [5, 10, 15] t = torch.ones(4, 4) new_shape = (2, 8) print(change_tensor_shape(t, new_shape).shape) # Output: torch.Size([2, 8]) ``` Submission: Ensure your code is clean, well-documented, and correctly handles edge cases. Submit your Python script with the implemented functions and examples demonstrating their usage.","solution":"import torch def get_tensor_size(t): Returns the size of each dimension of the input tensor. Parameters: t (torch.Tensor): The input tensor. Returns: List[int]: A list containing the size of each dimension of the tensor. return list(t.size()) def change_tensor_shape(t, shape): Reshapes the input tensor to the specified shape. Parameters: t (torch.Tensor): The input tensor. shape (tuple): The new shape. Returns: torch.Tensor: A tensor reshaped to the specified dimensions. Raises: ValueError: If the reshape is not possible due to incompatible dimensions. try: return t.view(shape) except RuntimeError as e: raise ValueError(\\"Reshape not possible due to incompatible dimensions\\") from e"},{"question":"<|Analysis Begin|> The provided documentation for the \\"bz2\\" module in Python covers a comprehensive interface for compressing and decompressing data using the bzip2 algorithm. It details various functions and classes available in the module, their usage, and specific parameters. Key components of the \\"bz2\\" module include: - Functions like `compress()` and `decompress()` for one-shot compression and decompression. - The `BZ2Compressor` and `BZ2Decompressor` classes for incremental compression and decompression. - The `BZ2File` class, which provides a file-like interface for reading and writing bzip2-compressed files. - The `open()` function which simplifies opening bzip2-compressed files. The documentation also provides various usage examples, demonstrating round-trip compression/decompression and file handling with bzip2. Considering the advanced nature of these details, a coding question can be designed around using these functionalities to read, compress, write, and then decompress data to verify integrity. <|Analysis End|> <|Question Begin|> # Compression, Writing, and Verification with bz2 Module You are provided with a large text file containing a set of log data. Your task is to: 1. Read the contents of the file. 2. Compress the data using the bzip2 algorithm. 3. Write the compressed data to a new file. 4. Read the compressed file, decompress it. 5. Verify that the decompressed data matches the original data. # Specifications - The input file path and the output file path should be passed as parameters to your function. - Use the `bz2` module for all compression and decompression operations. - Ensure the data read from the decompressed file exactly matches the original data read from the input file. # Function Signature ```python def compress_and_verify(input_file_path: str, compressed_file_path: str) -> bool: Compress the contents of the input_file_path, write it to compressed_file_path, and verify the integrity by decompressing and checking the contents. Args: input_file_path (str): Path to the input file containing the log data. compressed_file_path (str): Path to write the compressed file. Returns: bool: True if decompressed data matches original data, False otherwise. ``` # Constraints - The function should handle potentially large files efficiently. - Ensure proper file handling (closing files properly). - Use the highest compression level (9) for the bzip2 algorithm. # Example ```python result = compress_and_verify(\\"log_data.txt\\", \\"compressed_log_data.bz2\\") print(result) # Expected output: True if the decompression and verification are successful ``` In this task, demonstrate your understanding of file handling, compression/decompression, and data integrity verification using the `bz2` module.","solution":"import bz2 def compress_and_verify(input_file_path: str, compressed_file_path: str) -> bool: try: # Read the contents of the input file with open(input_file_path, \'rb\') as input_file: original_data = input_file.read() # Compress the data compressed_data = bz2.compress(original_data, compresslevel=9) # Write the compressed data to the output file with open(compressed_file_path, \'wb\') as output_file: output_file.write(compressed_data) # Read the compressed file and decompress the data with open(compressed_file_path, \'rb\') as compressed_file: read_compressed_data = compressed_file.read() decompressed_data = bz2.decompress(read_compressed_data) # Verify that the decompressed data matches the original data return decompressed_data == original_data except Exception as e: # Handle any unexpected exceptions and return False return False"},{"question":"**Question:** # Future Features Metadata Processor You have been provided with Python\'s `__future__` module metadata, which records information about new features introduced in different Python versions. Your task is to write a function that processes this metadata to extract and summarize specific information about these features. Task Write a function `future_features_summary` that takes a list of dictionaries as input where each dictionary represents a feature with the following structure: ```python { \\"feature\\": str, # Name of the feature \\"optional_in\\": tuple, # Version tuple where the feature was optional \\"mandatory_in\\": tuple, # Version tuple where the feature became mandatory (or None if never mandatory) \\"effect\\": str # Description of the feature\'s effect } ``` The function should return another dictionary with the following keys: - `features_by_mandatory_version`: A dictionary where the keys are version tuples and the values are lists of feature names that became mandatory in that version. - `total_features`: The total number of features provided in the input. - `features_never_mandatory`: A list of feature names that never became mandatory (i.e., their `mandatory_in` value is `None`). Example: ```python features = [ { \\"feature\\": \\"nested_scopes\\", \\"optional_in\\": (2, 1, 0, \\"beta\\", 1), \\"mandatory_in\\": (2, 2, 0, \\"final\\", 0), \\"effect\\": \\"PEP 227: Statically Nested Scopes\\" }, { \\"feature\\": \\"unicode_literals\\", \\"optional_in\\": (2, 6, 0, \\"alpha\\", 2), \\"mandatory_in\\": (3, 0, 0, \\"final\\", 0), \\"effect\\": \\"PEP 3112: Bytes literals in Python 3000\\" }, { \\"feature\\": \\"annotations\\", \\"optional_in\\": (3, 7, 0, \\"beta\\", 1), \\"mandatory_in\\": None, \\"effect\\": \\"PEP 563: Postponed evaluation of annotations\\" } ] result = future_features_summary(features) print(result) ``` Expected Output: ```python { \\"features_by_mandatory_version\\": { (2, 2, 0, \\"final\\", 0): [\\"nested_scopes\\"], (3, 0, 0, \\"final\\", 0): [\\"unicode_literals\\"] }, \\"total_features\\": 3, \\"features_never_mandatory\\": [\\"annotations\\"] } ``` Constraints: 1. You can assume the tuples provided for the versions adhere to the structure (major, minor, micro, release level, serial), and the versions are always valid. 2. There will be no duplicate feature names. 3. The input list will contain at least one feature dictionary. Implementing this function will require familiarity with standard Python data structures and control flow mechanisms, along with techniques for manipulating and aggregating nested data.","solution":"def future_features_summary(features): Processes the list of feature dictionaries and returns a summary dictionary. :param features: List of dictionaries representing feature metadata. :return: Dictionary containing the summary of features. summary = { \\"features_by_mandatory_version\\": {}, \\"total_features\\": len(features), \\"features_never_mandatory\\": [] } for feature in features: name = feature[\\"feature\\"] mandatory_version = feature[\\"mandatory_in\\"] if mandatory_version: if mandatory_version not in summary[\\"features_by_mandatory_version\\"]: summary[\\"features_by_mandatory_version\\"][mandatory_version] = [] summary[\\"features_by_mandatory_version\\"][mandatory_version].append(name) else: summary[\\"features_never_mandatory\\"].append(name) return summary"},{"question":"**Python Sequence Protocol Implementation** You are required to implement a custom Python class that mimics the behavior of a sequence object. Your class should appropriately handle sequence operations, focusing on some key methods as described below. This question will test your understanding of Python\'s sequence protocols and your ability to implement class methods. # Class Definition Define a class named `CustomSequence` that mimics the behavior of sequences. Your class should support the following methods: 1. **`__init__(self, data)`**: - Initializes the sequence with an iterable `data`. - Stores the iterable\'s elements in a list. 2. **`__len__(self)`**: - Returns the number of elements in the sequence. - Equivalent to `len(self)`. 3. **`__getitem__(self, index)`**: - Returns the element at the specified index. - Should support negative indexing. - Equivalent to `self[index]`. 4. **`__setitem__(self, index, value)`**: - Sets the element at the specified index to the given value. - Should support negative indexing. - Equivalent to `self[index] = value`. 5. **`__delitem__(self, index)`**: - Deletes the element at the specified index. - Should support negative indexing. - Equivalent to `del self[index]`. 6. **`__contains__(self, value)`**: - Checks if the sequence contains the specified value. - Equivalent to `value in self`. # Constraints - The indices provided will always be valid indices within the range of the sequence. # Example Usage ```python # Initialization seq = CustomSequence([1, 2, 3, 4, 5]) # Length of the sequence print(len(seq)) # Output: 5 # Get item by index print(seq[2]) # Output: 3 print(seq[-1]) # Output: 5 # Set item by index seq[2] = 10 print(seq[2]) # Output: 10 # Delete item by index del seq[3] print(len(seq)) # Output: 4 print(seq[3]) # Output: 5 # Check containment print(5 in seq) # Output: True print(15 in seq) # Output: False ``` Implement the `CustomSequence` class that fulfills the above requirements.","solution":"class CustomSequence: def __init__(self, data): Initializes the sequence with an iterable `data`. self._data = list(data) def __len__(self): Returns the number of elements in the sequence. return len(self._data) def __getitem__(self, index): Returns the element at the specified index. return self._data[index] def __setitem__(self, index, value): Sets the element at the specified index to the given value. self._data[index] = value def __delitem__(self, index): Deletes the element at the specified index. del self._data[index] def __contains__(self, value): Checks if the sequence contains the specified value. return value in self._data"},{"question":"In this task, you will implement a custom neural network initialization function using PyTorch. Your function will initialize the weights of a neural network layer using a combination of different initialization schemes provided by `torch.nn.init`. Function Signature ```python import torch import torch.nn as nn import torch.nn.init as init def custom_layer_init(layer: nn.Module) -> None: Initializes the weights of the given neural network layer using a combination of different initialization schemes. Parameters: layer (nn.Module): The neural network layer to initialize. Returns: None ``` Instructions 1. If the layer is a linear layer (`nn.Linear`), initialize its weights using Xavier uniform distribution (`init.xavier_uniform_`) and its biases to zero (`init.zeros_`). 2. If the layer is a convolutional layer (`nn.Conv2d`), initialize its weights using Kaiming normal distribution (`init.kaiming_normal_`) and its biases to a constant value of 0.1 (`init.constant_(bias, 0.1)`). 3. If the layer is a recurrent layer (`nn.RNN`, `nn.LSTM`, or `nn.GRU`), initialize its weights with orthogonal initialization (`init.orthogonal_`) and its biases to zeros (`init.zeros_`). Constraints - You may assume that the input layer will be one of `nn.Linear`, `nn.Conv2d`, `nn.RNN`, `nn.LSTM`, or `nn.GRU`. - Use appropriate PyTorch functions for initialization. Example Let\'s assume you have the following layers: ```python linear_layer = nn.Linear(10, 5) conv_layer = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) lstm_layer = nn.LSTM(10, 20, 2) ``` After calling `custom_layer_init` on each of these layers, they should be initialized as per the specified rules. Sample Usage ```python custom_layer_init(linear_layer) custom_layer_init(conv_layer) custom_layer_init(lstm_layer) ``` Your task is to fill out the `custom_layer_init` function following the given instructions and constraints.","solution":"import torch import torch.nn as nn import torch.nn.init as init def custom_layer_init(layer: nn.Module) -> None: Initializes the weights of the given neural network layer using a combination of different initialization schemes. Parameters: layer (nn.Module): The neural network layer to initialize. Returns: None if isinstance(layer, nn.Linear): init.xavier_uniform_(layer.weight) if layer.bias is not None: init.zeros_(layer.bias) elif isinstance(layer, nn.Conv2d): init.kaiming_normal_(layer.weight, nonlinearity=\'relu\') if layer.bias is not None: init.constant_(layer.bias, 0.1) elif isinstance(layer, (nn.RNN, nn.LSTM, nn.GRU)): for name, param in layer.named_parameters(): if \'weight\' in name: init.orthogonal_(param) elif \'bias\' in name: init.zeros_(param)"},{"question":"**Question: Recursive Directory Comparison** You are given two directories containing a mixture of files and subdirectories. Your task is to write a Python function that recursively compares these two directories and generates a comprehensive report of the comparison. The report should include: 1. List of files that are identical in both directories. 2. List of files that differ between the directories. 3. List of files that could not be compared. 4. List of files and subdirectories that are unique to each directory. Implement the function using the `filecmp` module from Python\'s standard library. # Function Signature ```python def compare_directories(dir1: str, dir2: str) -> dict: pass ``` # Input - `dir1` (str): The path to the first directory. - `dir2` (str): The path to the second directory. # Output - Returns a dictionary with the following keys: - `\'identical_files\'`: List of file paths that are identical in both directories. - `\'different_files\'`: List of file paths that differ between the directories. - `\'error_files\'`: List of file paths that could not be compared. - `\'unique_to_dir1\'`: List of file paths and subdirectories that are unique to `dir1`. - `\'unique_to_dir2\'`: List of file paths and subdirectories that are unique to `dir2`. # Constraints - Assume that the provided directory paths are valid and accessible. - The comparison should be case-sensitive. # Example ```python result = compare_directories(\'path/to/dir1\', \'path/to/dir2\') # Example output: # { # \'identical_files\': [\'file1.txt\', \'subdir/file2.txt\'], # \'different_files\': [\'file3.txt\'], # \'error_files\': [\'file4.txt\'], # \'unique_to_dir1\': [\'unique_to_dir1_file.txt\', \'subdir/unique_file.txt\'], # \'unique_to_dir2\': [\'unique_to_dir2_file.txt\', \'subdir/unique_file.txt\'] # } ``` # Notes - Use `filecmp.dircmp` for the comparison and recursively traverse subdirectories. - Be sure to handle cases where a comparison might fail (e.g., permission issues). - Structure your implementation to minimize redundant processing and ensure efficiency. ```python def compare_directories(dir1: str, dir2: str) -> dict: from filecmp import dircmp def recursive_compare(dcmp): result = { \'identical_files\': [], \'different_files\': [], \'error_files\': [], \'unique_to_dir1\': [], \'unique_to_dir2\': [] } result[\'identical_files\'].extend(dcmp.same_files) result[\'different_files\'].extend(dcmp.diff_files) result[\'error_files\'].extend(dcmp.funny_files) result[\'unique_to_dir1\'].extend(dcmp.left_only) result[\'unique_to_dir2\'].extend(dcmp.right_only) for sub_dcmp in dcmp.subdirs.values(): sub_result = recursive_compare(sub_dcmp) for key in result: result[key].extend(sub_result[key]) return result dcmp = dircmp(dir1, dir2) return recursive_compare(dcmp) ```","solution":"def compare_directories(dir1: str, dir2: str) -> dict: from filecmp import dircmp def recursive_compare(dcmp): result = { \'identical_files\': [], \'different_files\': [], \'error_files\': [], \'unique_to_dir1\': [], \'unique_to_dir2\': [] } result[\'identical_files\'].extend([dcmp.left + \\"/\\" + file for file in dcmp.same_files]) result[\'different_files\'].extend([dcmp.left + \\"/\\" + file for file in dcmp.diff_files]) result[\'error_files\'].extend([dcmp.left + \\"/\\" + file for file in dcmp.funny_files]) result[\'unique_to_dir1\'].extend([dcmp.left + \\"/\\" + file for file in dcmp.left_only]) result[\'unique_to_dir2\'].extend([dcmp.right + \\"/\\" + file for file in dcmp.right_only]) for sub_dcmp in dcmp.subdirs.values(): sub_result = recursive_compare(sub_dcmp) for key in result: result[key].extend(sub_result[key]) return result dcmp = dircmp(dir1, dir2) return recursive_compare(dcmp)"},{"question":"Objective: You are provided with a sample dataset containing information about various species of flowers. Your task is to create different visualizations using seaborn\'s palette functions to highlight the species category. This exercise will test your ability to use seaborn\'s palette functions and your understanding of data visualization techniques. Requirements: 1. Load the built-in `iris` dataset from seaborn. 2. Create the following visualizations for displaying data, where each species category should have its unique color: - **Histogram** showing the distribution of petal lengths. - **Box Plot** comparing the distribution of sepal widths among species. - **Scatter Plot** comparing petal width and petal length. 3. Use the following seaborn palette functionalities: - Sequential palette using a specified color name. - Sequential palette using a hex code. - HUSL-based palette. 4. Each visualization must: - Include a legend indicating the species. - Have appropriate titles and axis labels. - Showcase distinct colors for each species using the specified palette system. Input: - No user input is required. Output: - Four visualizations (one for each palette type mentioned). Constraints: - Use seaborn for all visualizations. - Ensure the visualizations are clear and the colors distinctly represent each species. - Optimize for readability and aesthetic presentation. Example: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Sequential palette using specified color name palette_name = sns.dark_palette(\\"seagreen\\", 3) sns.histplot(data=iris, x=\\"petal_length\\", hue=\\"species\\", palette=palette_name, element=\\"step\\", stat=\\"density\\", common_norm=False) plt.title(\\"Petal Length Distribution using Seagreen Palette\\") plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"Density\\") plt.legend(title=\\"Species\\") plt.show() # You should implement similar steps for the remaining visualizations using different palettes. ``` Your task is to complete the remaining visualizations using the palette functionalities as specified above. Make sure each plot follows the requirements given.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Sequential palette using a specified color name palette_color_name = sns.dark_palette(\\"seagreen\\", 3) plt.figure() # Create a new figure for each plot sns.histplot(data=iris, x=\\"petal_length\\", hue=\\"species\\", palette=palette_color_name, element=\\"step\\", stat=\\"density\\", common_norm=False) plt.title(\\"Petal Length Distribution using Seagreen Palette\\") plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"Density\\") plt.legend(title=\\"Species\\") plt.show() # Box Plot with sequential palette using a hex code hex_palette = sns.light_palette(\\"#5A9\\", 3) plt.figure() sns.boxplot(data=iris, x=\\"species\\", y=\\"sepal_width\\", palette=hex_palette) plt.title(\\"Sepal Width Distribution using Hex Palette\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Width\\") plt.legend(title=\\"Species\\") plt.show() # Scatter Plot with HUSL-based palette husl_palette = sns.color_palette(\\"husl\\", len(iris[\'species\'].unique())) plt.figure() sns.scatterplot(data=iris, x=\\"petal_length\\", y=\\"petal_width\\", hue=\\"species\\", palette=husl_palette) plt.title(\\"Petal Width vs Petal Length using HUSL Palette\\") plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"Petal Width\\") plt.legend(title=\\"Species\\") plt.show()"},{"question":"In a distributed computing setup, it is crucial to handle errors gracefully to ensure robustness and reliability. Using the `torch.distributed.elastic.multiprocessing.errors` module from PyTorch, you will write code that demonstrates how to record and handle errors during a synthetic distributed training process. # Task 1. Implement a function `distributed_computation(n: int) -> None` that simulates a distributed computation: - The function should spawn `n` worker processes. - Each worker process should simulate performing a task that has a chance of failing (e.g., raise an exception). 2. Implement a custom error handler using `torch.distributed.elastic.multiprocessing.errors.ErrorHandler`: - The error handler should handle any raised exceptions and log the error messages. 3. Use the `torch.distributed.elastic.multiprocessing.errors.record` decorator to record errors in each worker process. # Requirements - **Input:** An integer `n` representing the number of worker processes. - **Output:** None. Your function should print/log error messages if any processes fail. - Your implementation should gracefully handle exceptions raised during the worker processes and log a meaningful error message for each failure using the error handling infrastructure provided by `torch.distributed.elastic.multiprocessing.errors`. # Constraints - Ensure error messages from all processes are captured and logged. - Utilize the functionalities from the provided module documentation (`record`, `ErrorHandler`). # Example ```python import torch.distributed.elastic.multiprocessing.errors as errors class CustomErrorHandler(errors.ErrorHandler): def handle(self, failure): # Custom handling logic print(f\\"Caught error: {failure.error}\\") @errors.record def faulty_worker_process(task_id): import random if random.random() < 0.5: raise RuntimeError(f\\"Task {task_id} failed due to a random error.\\") def distributed_computation(n: int) -> None: from multiprocessing import Process processes = [] handler = CustomErrorHandler() for i in range(n): # Assuming we need to pass the handler to each process p = Process(target=faulty_worker_process, args=(i,), kwargs={\'handler\': handler}) processes.append(p) p.start() for p in processes: p.join() # Run the distributed computation with 4 worker processes distributed_computation(4) ``` In the above example, the `faulty_worker_process` simulates tasks with a 50% failure rate, and the `CustomErrorHandler` captures and logs errors. Make sure your implementation: - Spawns `n` processes. - Handles and logs errors using the provided error handling infrastructure. - Demonstrates clear and accurate error logging for failed processes.","solution":"import torch.distributed.elastic.multiprocessing.errors as errors class CustomErrorHandler(errors.ErrorHandler): def handle(self, failure): # Custom handling logic print(f\\"Caught error: {failure.error}\\") @errors.record def faulty_worker_process(task_id): import random if random.random() < 0.5: raise RuntimeError(f\\"Task {task_id} failed due to a random error.\\") else: print(f\\"Task {task_id} completed successfully.\\") def distributed_computation(n: int) -> None: from multiprocessing import Process processes = [] handler = CustomErrorHandler() for i in range(n): p = Process(target=faulty_worker_process, args=(i,)) processes.append(p) p.start() for p in processes: p.join()"},{"question":"**Complex Plotting with Seaborn and Matplotlib** You are provided with the `diamonds` dataset from Seaborn. Your task is to create a composite plot with the following specifications, demonstrating your understanding of Seaborn\'s `objects` interface and Matplotlib\'s customization capabilities: 1. The main plot should display a scatter plot of `carat` versus `price` using Seaborn\'s `Plot` class. 2. Add a rectangle annotation within this scatter plot to highlight the region where `carat` is between 0.5 and 1.5, and `price` is between 1000 and 5000. 3. Create a subfigure to the right of the main plot displaying a histogram of `price` faceted by `cut` and scaled on the x-axis using logarithmic scale. 4. Ensure that the plots are organized neatly within a single figure with appropriate titles and axis labels. # Input: - None. The `diamonds` dataset should be loaded within your code using `seaborn.load_dataset(\\"diamonds\\")`. # Expected Output: - A single figure containing the specified plots and customizations. # Constraints: - Use Seaborn\'s high-level `Plot` class for the primary scatter plot. - Utilize Matplotlib\'s `Figure` and `subfigures` for organizing the composite plot. - Apply the rectangle annotation and axis transformations using Matplotlib. # Example: Here is a template to help you get started: ```python import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt import matplotlib as mpl from seaborn import load_dataset # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the main scatter plot p = so.Plot(diamonds, x=\'carat\', y=\'price\').add(so.Dots()) # Initialize the figure and subfigures fig = mpl.figure.Figure(figsize=(10, 5), layout=\\"constrained\\") sf1, sf2 = fig.subfigures(1, 2) # Plot the scatter plot on the first subfigure p.on(sf1).plot() # Access the Matplotlib axes and add the rectangle annotation ax1 = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0.5, 1000), width=1.0, height=4000, color=\\"yellow\\", alpha=0.3, transform=ax1.transData ) ax1.add_patch(rect) # Create the faceted histogram with logarithmic scale on the second subfigure so.Plot(diamonds, x=\'price\').add(so.Bars(), so.Hist()).facet(row=\'cut\').scale(x=\'log\').share(y=False).on(sf2) # Display the plot plt.show() ``` This example provides a basic scaffold. Ensure that you fully complete the customization and add titles as required.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt import matplotlib as mpl from seaborn import load_dataset def create_composite_plot(): # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the main scatter plot p = so.Plot(diamonds, x=\'carat\', y=\'price\').add(so.Dots()) # Initialize the figure and subfigures fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10)) # Plot the scatter plot on the first subplot p.on(ax1).plot() # Add the rectangle annotation to highlight the specified region rect = mpl.patches.Rectangle( xy=(0.5, 1000), width=1.0, height=4000, color=\\"yellow\\", alpha=0.3 ) ax1.add_patch(rect) # Set titles and labels for the first subplot ax1.set_title(\\"Carat vs Price Scatter Plot\\") ax1.set_xlabel(\\"Carat\\") ax1.set_ylabel(\\"Price\\") # Create the faceted histogram with logarithmic scale on the second subplot sns.histplot(diamonds, x=\'price\', hue=\'cut\', multiple=\'dodge\', ax=ax2) ax2.set_xscale(\'log\') ax2.set_xlabel(\\"Price (Log Scale)\\") ax2.set_ylabel(\\"Count\\") ax2.set_title(\\"Histogram of Price by Cut (Log Scale)\\") # Adjust layout for better visualization plt.tight_layout() # Display the plot plt.show()"},{"question":"You are tasked with implementing and evaluating a decision tree classifier to predict species from the famous Iris dataset using the scikit-learn library. However, to demonstrate your understanding of various aspects of decision trees, you will need to follow specific guidelines for model creation, handling missing values, and pruning. Task 1. **Data Loading**: Load the Iris dataset. 2. **Data Preparation**: Introduce some missing values into the dataset. 3. **Model Training**: - Train a `DecisionTreeClassifier` model. - Handle the missing values during training. - Implement pruning using the minimal cost-complexity pruning parameter (`ccp_alpha`). 4. **Model Evaluation**: - Evaluate the model performance using cross-validation. - Visualize the decision tree before and after pruning. Detailed Steps 1. **Data Loading**: - Load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Data Preparation**: - Introduce missing values randomly into the dataset. For simplicity, set around 10% of the dataset to NaN. 3. **Model Training**: - Train a `DecisionTreeClassifier` using the dataset, ensuring that it handles missing values appropriately. - Use the minimal cost-complexity pruning parameter (`ccp_alpha`) to prune the tree. 4. **Model Evaluation**: - Evaluate the model using cross-validation (e.g., 5-fold cross-validation). - Visualize the tree before and after applying minimal cost-complexity pruning. Expected Input and Output Format - Input: None (the dataset is loaded directly within the script). - Output: The cross-validation scores as well as the visualizations of the decision trees before and after pruning. Constraints and Notes - Ensure to use the `ccp_alpha` parameter effectively to demonstrate pruning. - Properly handle missing values using the built-in capabilities of the `DecisionTreeClassifier`. - The visualizations can be saved as files or displayed inline if using a Jupyter notebook. Example Here is a skeleton of what your implementation might start like: ```python from sklearn.datasets import load_iris import numpy as np from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score from sklearn.tree import plot_tree import matplotlib.pyplot as plt # Step 1: Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Step 2: Introduce missing values randomly np.random.seed(0) missing_rate = 0.1 mask = np.random.rand(*X.shape) < missing_rate X[mask] = np.nan # Step 3: Train the DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=0) clf.fit(X, y) # Evaluate using cross-validation scores = cross_val_score(clf, X, y, cv=5) print(\\"Cross-validation scores:\\", scores) # Visualize the decision tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True) plt.show() # Step 4: Pruning using minimal cost-complexity pruning path = clf.cost_complexity_pruning_path(X, y) ccp_alphas = path.ccp_alphas clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha) clf.fit(X, y) clfs.append(clf) # Choose the best pruned tree clf = clfs[np.argmax([cross_val_score(clf, X, y, cv=5).mean() for clf in clfs])] plt.figure(figsize=(20,10)) plot_tree(clf, filled=True) plt.show() ```","solution":"from sklearn.datasets import load_iris import numpy as np from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.model_selection import cross_val_score from sklearn.impute import SimpleImputer import matplotlib.pyplot as plt def load_and_modify_iris(): Load the iris dataset and introduce missing values. Returns: X: Features with NaN values introduced y: Target values # Load the dataset iris = load_iris() X, y = iris.data, iris.target # Introduce missing values randomly (10%) np.random.seed(42) missing_rate = 0.1 mask = np.random.rand(*X.shape) < missing_rate X[mask] = np.nan return X, y def train_and_evaluate_decision_tree(X, y, ccp_alpha=0.0): Train a DecisionTreeClassifier, handle missing values and apply pruning. Args: X: Feature data (with potential NaN values) y: Target data ccp_alpha: Minimal cost-complexity pruning parameter Returns: Cross-validation scores # Handle missing values using SimpleImputer imputer = SimpleImputer(strategy=\'mean\') X_imputed = imputer.fit_transform(X) # Train DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) scores = cross_val_score(clf, X_imputed, y, cv=5) clf.fit(X_imputed, y) return clf, scores def plot_decision_tree(clf, title=\\"Decision Tree\\"): Plot the decision tree. Args: clf: Trained DecisionTreeClassifier title: Title for the plot plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=load_iris().feature_names, class_names=load_iris().target_names) plt.title(title) plt.show() # Main execution X, y = load_and_modify_iris() # Train and evaluate without pruning clf_no_prune, scores_no_prune = train_and_evaluate_decision_tree(X, y) print(\\"Cross-validation scores without pruning:\\", scores_no_prune) plot_decision_tree(clf_no_prune, \\"Decision Tree without Pruning\\") # Train and evaluate with pruning using ccp_alpha path = clf_no_prune.cost_complexity_pruning_path(X, y) ccp_alphas = path.ccp_alphas best_scores = 0 best_clf = None # Find the best pruned tree for ccp_alpha in ccp_alphas: clf, scores = train_and_evaluate_decision_tree(X, y, ccp_alpha=ccp_alpha) mean_scores = scores.mean() if mean_scores > best_scores: best_scores = mean_scores best_clf = clf print(\\"Best cross-validation scores with pruning:\\", best_scores) plot_decision_tree(best_clf, \\"Best Pruned Decision Tree\\")"},{"question":"# Custom Weight Initialization in PyTorch **Context:** The `torch.nn.init` module provides various functions to initialize neural network parameters. These initialization methods are essential for neural network training as they can significantly impact the convergence rate and overall performance. Your task is to implement a class that applies custom initialization to a given neural network model. **Task:** You are required to implement a Python class `CustomWeightInitializer` in PyTorch that initializes the weights of a given neural network model using a combination of the initialization functions provided by `torch.nn.init`. **Implementation Requirements:** 1. Implement a class `CustomWeightInitializer` with the following methods: - `__init__(self, model)`: Constructor that takes a PyTorch model (e.g., an instance of `torch.nn.Module`) as input and stores it. - `apply_initialization(self, layers_config)`: Applies custom initialization to the weights of the model based on `layers_config`. 2. The `layers_config` argument to the `apply_initialization` method will be a dictionary where keys are layer types (e.g., `torch.nn.Linear`) and values are dictionaries specifying: - `weight_init`: A string specifying the weight initialization method (from the provided `torch.nn.init` functions, e.g., `xavier_uniform_`). - `bias_init`: A string specifying the bias initialization method (e.g., `zeros_`). 3. For simplicity, assume each layer in the model has both weight and bias attributes. 4. The initialization methods to be used should include at least: - `xavier_uniform_` - `kaiming_normal_` - `zeros_` - `ones_` **Constraints:** - Raise a `ValueError` if an invalid initialization method is provided. - Ensure that your implementation is efficient and does not unnecessarily recompute or reinitialize any components. **Hint:** - You may find the `hasattr` and `getattr` functions useful for dynamically checking and accessing attributes of the model\'s layers. **Example Usage:** ```python import torch import torch.nn as nn import torch.nn.init as init class CustomWeightInitializer: def __init__(self, model): self.model = model def apply_initialization(self, layers_config): for layer in self.model.children(): layer_type = type(layer) if layer_type in layers_config: config = layers_config[layer_type] if hasattr(layer, \'weight\'): weight_init_method = getattr(init, config[\'weight_init\'], None) if weight_init_method is None: raise ValueError(f\\"Invalid weight initialization method: {config[\'weight_init\']}\\") weight_init_method(layer.weight) if hasattr(layer, \'bias\'): bias_init_method = getattr(init, config[\'bias_init\'], None) if bias_init_method is None: raise ValueError(f\\"Invalid bias initialization method: {config[\'bias_init\']}\\") bias_init_method(layer.bias) # Example Neural Network Model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 20) model = SimpleNN() # Layer configuration for custom initialization layer_config = { nn.Linear: { \'weight_init\': \'xavier_uniform_\', \'bias_init\': \'zeros_\' } } initializer = CustomWeightInitializer(model) initializer.apply_initialization(layer_config) # Check the initialization of weights and biases print(model.fc1.weight) print(model.fc1.bias) ``` **What to Submit:** 1. A Python file containing the `CustomWeightInitializer` class. 2. An example script demonstrating the use of your class on a sample neural network model.","solution":"import torch import torch.nn as nn import torch.nn.init as init class CustomWeightInitializer: def __init__(self, model): self.model = model def apply_initialization(self, layers_config): for layer in self.model.children(): layer_type = type(layer) if layer_type in layers_config: config = layers_config[layer_type] # Initialize weights if hasattr(layer, \'weight\'): weight_init_method = getattr(init, config[\'weight_init\'], None) if weight_init_method is None: raise ValueError(f\\"Invalid weight initialization method: {config[\'weight_init\']}\\") weight_init_method(layer.weight) # Initialize biases if hasattr(layer, \'bias\'): bias_init_method = getattr(init, config[\'bias_init\'], None) if bias_init_method is None: raise ValueError(f\\"Invalid bias initialization method: {config[\'bias_init\']}\\") bias_init_method(layer.bias) # Example Neural Network Model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 20) model = SimpleNN() # Layer configuration for custom initialization layer_config = { nn.Linear: { \'weight_init\': \'xavier_uniform_\', \'bias_init\': \'zeros_\' } } initializer = CustomWeightInitializer(model) initializer.apply_initialization(layer_config) # Check the initialization of weights and biases print(model.fc1.weight) print(model.fc1.bias)"},{"question":"# Custom Unicode Error Handler and Codec Usage Objective Implement a custom error handler for handling encoding errors in Unicode strings and use this handler to encode a given text using a specified encoding. Task 1. **Custom Error Handler**: - Implement a custom error handler named `custom_replace_error` which will replace any unencodable characters with the string `\\"[INVALID]\\"`. - Register this error handler under the name `\\"replace_with_invalid\\"` using the `PyCodec_RegisterError` function. 2. **Encoding Function**: - Write a function `encode_text` which takes three arguments: - `text`: A string to encode. - `encoding`: The name of the encoding to use. - `errors`: The name of the error handler method to use (e.g., `\\"replace_with_invalid\\"`). - Use the `PyCodec_Encode` function to encode the text using the specified encoding and error handler method. Constraints and Requirements - Your `custom_replace_error` should properly handle instances of `UnicodeEncodeError`. - If the encoding specified does not exist, your function should return an error message. - Handle exceptions appropriately and ensure the function does not crash on invalid inputs. Input Format - `text` (string): The text to be encoded. - `encoding` (string): The encoding to use (e.g., `\\"utf-8\\"`, `\\"ascii\\"`). - `errors` (string): The name of the error handler to use. Output Format - Return the encoded version of the text as a bytes object. - If an error occurs (e.g., unknown encoding), return an error message as a string. Example ```python def encode_text(text, encoding, errors): # Implementation here. pass # Example Usage encoded_text = encode_text(\\"Some text with € symbol\\", \\"ascii\\", \\"replace_with_invalid\\") print(encoded_text) # Output should be a bytes object with the € symbol replaced. ``` Hints - Use `PyCodec_RegisterError` to register the custom error handler. - Use `PyCodec_Encode` to perform the encoding with the custom error handler.","solution":"import codecs def custom_replace_error(exception): Custom error handler that replaces unencodable characters with \'[INVALID]\'. if isinstance(exception, UnicodeEncodeError): return (\'[INVALID]\', exception.start + 1) else: raise TypeError(\\"Don\'t know how to handle error: %r\\" % exception) # Register the custom error handler codecs.register_error(\'replace_with_invalid\', custom_replace_error) def encode_text(text, encoding, errors): try: encoded_text = codecs.encode(text, encoding=encoding, errors=errors) return encoded_text except LookupError: return \\"Unknown encoding: {}\\".format(encoding) # Example Usage encoded_text = encode_text(\\"Some text with € symbol\\", \\"ascii\\", \\"replace_with_invalid\\") print(encoded_text) # Output should be a bytes object with the € symbol replaced."},{"question":"Seaborn Swarm Plot Visualization **Objective:** Write a function that takes a dataset name (available in Seaborn datasets), a numerical variable, a categorical variable, and several optional customization parameters to generate a comprehensive swarm plot, along with faceting for deeper insights. **Function Specification:** ```python def create_swarm_facet_plots(dataset_name, num_var, cat_var, hue_var=None, palette=None, dodge=False, orient=\'v\', size=5, facet_col=None): Generates a swarm plot with optional faceting. Parameters: - dataset_name (str): Name of the dataset to load from seaborn. - num_var (str): Name of the numerical variable to plot on the x or y-axis. - cat_var (str): Name of the categorical variable to plot on the y or x-axis. - hue_var (str, optional): Name of the variable for color encoding. - palette (str, optional): Name of the seaborn palette to use for hue variable. - dodge (bool, optional): Whether to dodge the hue variable points. - orient (str, optional): Orientation of the plot (\'v\' for vertical, \'h\' for horizontal). - size (float, optional): Size of the plot points. - facet_col (str, optional): Name of the variable to create facets based on. Returns: - None: Displays the generated swarm plot or facet plot. pass # Your implementation here ``` **Input and Output Formats:** - Input: - `dataset_name`: A string representing the name of the dataset to load (e.g., \\"tips\\"). - `num_var`: A string representing the name of the numerical variable (e.g., \\"total_bill\\"). - `cat_var`: A string representing the name of the categorical variable (e.g., \\"day\\"). - `hue_var`: (Optional) A string representing a variable for color encoding. - `palette`: (Optional) A string representing a seaborn palette name for the hue variable. - `dodge`: (Optional) A boolean to dodge the hue variable points within levels. Defaults to False. - `orient`: (Optional) A string for plot orientation; \\"v\\" for vertical or \\"h\\" for horizontal. Defaults to \\"v\\". - `size`: (Optional) A float for the size of the plot points. Defaults to 5. - `facet_col`: (Optional) A string representing the name of the variable to create column facets. - Output: - The function should display the generated swarm plot or facet plot directly using matplotlib\'s display routines. **Constraints:** - The dataset must be one of Seaborn\'s built-in datasets. - Variables (`num_var`, `cat_var`) must exist in the provided dataset. - If `facet_col` is provided, it must be a categorical variable in the dataset. **Example Usage:** ```python create_swarm_facet_plots( dataset_name=\\"tips\\", num_var=\\"total_bill\\", cat_var=\\"day\\", hue_var=\\"sex\\", palette=\\"deep\\", dodge=True, orient=\\"v\\", size=4, facet_col=\\"time\\" ) ``` In this example, the function will: 1. Load the \\"tips\\" dataset from Seaborn\'s repository. 2. Create a swarm plot with \\"total_bill\\" on the x-axis and \\"day\\" on the y-axis. 3. Color points by the \\"sex\\" variable using the \\"deep\\" palette. 4. Dodge points by \\"sex\\" within each \\"day\\" category. 5. Adjust the points to a size of 4. 6. Create separate facets for each value of the \\"time\\" variable. **Notes:** - Ensure to handle any potential errors, such as non-existent dataset names, variables, or wrongly set parameters. - Use appropriate data validation to guide the user in case of incorrect inputs.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_swarm_facet_plots(dataset_name, num_var, cat_var, hue_var=None, palette=None, dodge=False, orient=\'v\', size=5, facet_col=None): Generates a swarm plot with optional faceting. Parameters: - dataset_name (str): Name of the dataset to load from seaborn. - num_var (str): Name of the numerical variable to plot on the x or y-axis. - cat_var (str): Name of the categorical variable to plot on the y or x-axis. - hue_var (str, optional): Name of the variable for color encoding. - palette (str, optional): Name of the seaborn palette to use for hue variable. - dodge (bool, optional): Whether to dodge the hue variable points. - orient (str, optional): Orientation of the plot (\'v\' for vertical, \'h\' for horizontal). - size (float, optional): Size of the plot points. - facet_col (str, optional): Name of the variable to create facets based on. Returns: - None: Displays the generated swarm plot or facet plot. # Load dataset from seaborn if dataset_name not in sns.get_dataset_names(): raise ValueError(f\\"Dataset {dataset_name} not found in seaborn datasets\\") df = sns.load_dataset(dataset_name) # Check if specified columns exist in the dataset for col in [num_var, cat_var, hue_var, facet_col]: if col and col not in df.columns: raise ValueError(f\\"Column \'{col}\' not found in dataset \'{dataset_name}\'\\") # Create the swarm plot if facet_col: g = sns.catplot( data=df, x=num_var if orient == \'h\' else cat_var, y=cat_var if orient == \'h\' else num_var, hue=hue_var, palette=palette, dodge=dodge, size=size, orient=orient, col=facet_col, kind=\\"swarm\\" ) else: plt.figure(figsize=(10, 6)) sns.swarmplot( data=df, x=num_var if orient == \'h\' else cat_var, y=cat_var if orient == \'h\' else num_var, hue=hue_var, palette=palette, dodge=dodge, size=size, orient=orient ) plt.show()"},{"question":"**Question:** You are required to implement a simple chat server and client using Python\'s `asyncio` library to handle multiple clients concurrently. This task will help assess your understanding of coroutines, network IO, and synchronization with asyncio. # Objectives 1. Implement a chat client that can connect to the chat server and send/receive messages asynchronously. 2. Implement a chat server that can handle multiple clients simultaneously, broadcasting received messages from any client to all connected clients. # Instructions 1. **Chat Server Implementation:** - Write an asynchronous function `handle_client(reader, writer)` that reads messages from a client and broadcasts them to all other connected clients. - Use `asyncio.start_server` to start the server and listen for incoming connections. - Maintain a list of connected clients and ensure proper message broadcasting and client disconnection handling. 2. **Chat Client Implementation:** - Write an asynchronous function `chat_client(host, port)` that connects to the chat server, sends messages from standard input, and prints received messages from the server. - Ensure message sending and receiving are handled concurrently. # Requirements 1. **Input/Output for Server:** - The server should run on a specified host and port. - It should handle any number of clients, receiving messages from each, and broadcasting to every other connected client. - The server should handle client disconnection gracefully. 2. **Input/Output for Client:** - The client should connect to the specified server (host and port). - It should read user input from the standard input and send it to the server. - The client should also print any messages broadcasted from the server. # Example Usage Server ```python import asyncio async def handle_client(reader, writer): # Handle client for reading and broadcasting messages pass async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` Client ```python import asyncio async def chat_client(host, port): # Implement client connection, message sending, and receiving pass asyncio.run(chat_client(\'127.0.0.1\', 8888)) ``` # Constraints and Performance - Your solution should handle a minimal latency in message delivery (<500ms) for up to 100 connected clients. - Ensure that proper exception handling is in place for network errors and client disconnections. Good luck, and happy coding!","solution":"import asyncio clients = [] async def handle_client(reader, writer): client = (reader, writer) clients.append(client) addr = writer.get_extra_info(\'peername\') print(f\\"New client connected: {addr}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") for other_reader, other_writer in clients: if other_writer != writer: other_writer.write(data) await other_writer.drain() except asyncio.CancelledError: pass finally: print(f\\"Client {addr} disconnected\\") clients.remove(client) writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() async def chat_client(host, port): reader, writer = await asyncio.open_connection(host, port) async def send_messages(): while True: message = await asyncio.get_event_loop().run_in_executor(None, input, \\"> \\") writer.write(message.encode()) await writer.drain() async def receive_messages(): while True: data = await reader.read(100) if not data: break print(data.decode()) await asyncio.gather(send_messages(), receive_messages()) if __name__ == \\"__main__\\": asyncio.run(chat_client(\'127.0.0.1\', 8888))"},{"question":"**Objective:** Demonstrate your comprehension of pandas string methods by implementing functions to clean and transform textual data. # Problem Statement You are provided with a pandas DataFrame containing information about various products sold in a store. The DataFrame has the following columns: 1. `product_code`: A unique code for each product (string with mixed cases and leading/trailing whitespaces). 2. `description`: A text description of the products which may contain a mix of cases and embedded special characters. 3. `price`: The price of the products in USD (string format with `` and commas). Your task is to perform the following operations: 1. **Clean the `product_code`**: - Strip any leading or trailing whitespaces. - Convert all characters to uppercase. 2. **Standardize the `description`**: - Remove special characters (non-alphanumeric). - Convert all text to lower case. - Replace any sequence of whitespace characters (including tabs and newlines) with a single space. 3. **Convert `price` to a numerical format**: - Remove the `` sign. - Remove any commas. - Convert the resulting string to a floating-point number. 4. **Extract the first three characters of the `product_code`** and create a new column `category`. # Constraints - The input DataFrame will have the columns specified above. - Assume that the input DataFrame is valid and contains no missing values in the columns. # Input - A pandas DataFrame `df` with columns `product_code` (string), `description` (string), and `price` (string). # Output - A pandas DataFrame with the following columns: - `product_code` (cleaned) - `description` (standardized) - `price` (numerical format) - `category` (extracted from the first three characters of cleaned `product_code`) # Function Signature ```python import pandas as pd def clean_product_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Example Input DataFrame ```python data = { \'product_code\': [\' abc123 \', \' Def456 \', \' ghi789 \'], \'description\': [\'Amazing product! @#%\', \' Superb item.\', \'BestttproductnnEVER!\'], \'price\': [\'1,000.00\', \'2,500.50\', \'3,350.75\'] } df = pd.DataFrame(data) ``` Output DataFrame ```python output_data = { \'product_code\': [\'ABC123\', \'DEF456\', \'GHI789\'], \'description\': [\'amazing product\', \'superb item\', \'best product ever\'], \'price\': [1000.00, 2500.50, 3350.75], \'category\': [\'ABC\', \'DEF\', \'GHI\'] } output_df = pd.DataFrame(output_data) ``` # Task Implement the `clean_product_data` function to achieve the desired cleaning and transformation described above.","solution":"import pandas as pd import re def clean_product_data(df: pd.DataFrame) -> pd.DataFrame: # Clean product_code df[\'product_code\'] = df[\'product_code\'].str.strip().str.upper() # Standardize description df[\'description\'] = df[\'description\'].apply(lambda x: re.sub(r\'W+\', \' \', x).strip().lower()) # Clean and convert price to float df[\'price\'] = df[\'price\'].str.replace(\'\', \'\').str.replace(\',\', \'\').astype(float) # Extract category from product_code df[\'category\'] = df[\'product_code\'].str[:3] return df"},{"question":"# Question 1: Advanced Function Inspection Implement a Python function `get_function_frame_info` that mimics the capabilities described by the C API functions `PyEval_GetBuiltins()`, `PyEval_GetLocals()`, `PyEval_GetGlobals()`, `PyFrame_GetBack()`, `PyFrame_GetCode()`, and `PyFrame_GetLineNumber()`. Your function should take another function `target_func` as an argument and execute it. During the execution of `target_func`, it should capture the following information and return it in a dictionary: 1. **Built-ins**: The built-in functions available in the execution frame. 2. **Locals**: The local variables in the current frame after the execution of `target_func`. 3. **Globals**: The global variables available in the execution frame. 4. **Previous Frame**: Metadata from the frame that called `target_func` (if available), including the file name, line number, and code object name. 5. **Code Object**: The code object of `target_func`. 6. **Current Line Number**: The line number currently executing when retrieving the information. # Constraints - The function should handle exceptions in `target_func` gracefully and capture frame information up to the point of the exception. - The function will be executed in a single-threaded environment. # Input - A function `target_func`: The function whose execution frame information needs to be captured. # Output - A dictionary with the following key-value pairs: - `\\"builtins\\"`: Dictionary of built-in functions. - `\\"locals\\"`: Dictionary of local variables. - `\\"globals\\"`: Dictionary of global variables. - `\\"previous_frame\\"`: Dictionary with keys `file_name`, `line_number`, and `code_name`. - `\\"code_object\\"`: The code object of `target_func`. - `\\"current_line_number\\"`: The currently executing line number in `target_func`. # Example ```python def sample_function(): a = 10 print(a) info = get_function_frame_info(sample_function) assert isinstance(info, dict) assert \\"builtins\\" in info and isinstance(info[\\"builtins\\"], dict) assert \\"locals\\" in info and isinstance(info[\\"locals\\"], dict) assert \\"globals\\" in info and isinstance(info[\\"globals\\"], dict) assert \\"previous_frame\\" in info and isinstance(info[\\"previous_frame\\"], dict) assert \\"code_object\\" in info and hasattr(info[\\"code_object\\"], \'co_code\') assert \\"current_line_number\\" in info and isinstance(info[\\"current_line_number\\"], int) ``` # Note - You may use the `inspect` module from Python\'s standard library to assist with capturing frame information. - The structure and additional contents of dictionaries (builtins, locals, globals) will vary based on the state of execution.","solution":"import inspect def get_function_frame_info(target_func): Executes target_func and captures frame information during its execution. frame_info = {} def wrapper_func(): nonlocal frame_info try: target_func() except Exception as e: pass current_frame = inspect.currentframe() outer_frame = current_frame.f_back frame_info = { \\"builtins\\": outer_frame.f_builtins, \\"locals\\": outer_frame.f_locals, \\"globals\\": outer_frame.f_globals, \\"previous_frame\\": None, \\"code_object\\": target_func.__code__, \\"current_line_number\\": outer_frame.f_lineno, } if outer_frame.f_back: prev_frame = outer_frame.f_back frame_info[\\"previous_frame\\"] = { \\"file_name\\": prev_frame.f_code.co_filename, \\"line_number\\": prev_frame.f_lineno, \\"code_name\\": prev_frame.f_code.co_name, } wrapper_func() # Execute the wrapper which captures the frame info return frame_info"},{"question":"**Objective:** To assess your understanding of dynamically executing Python code using high-level interaction functions provided by the Python interpreter. **Task:** You are required to write a Python function `execute_python_code` that dynamically executes Python source code provided as a string. The function should have optional parameters to specify global and local execution contexts. Moreover, it should allow specifying custom compilation flags to influence the code execution. **Function Signature:** ```python def execute_python_code(code: str, globals_dict: dict = None, locals_dict: dict = None, compiler_flags: int = 0) -> object: pass ``` **Input:** - `code` (str): A string containing valid Python source code. - `globals_dict` (dict, optional): A dictionary representing the global execution context. default is `None`. - `locals_dict` (dict, optional): A dictionary representing the local execution context. default is `None`. - `compiler_flags` (int, optional): An integer representing the compiler flags. default is `0`. **Output:** - Returns the result of executing the code as a Python object, or `None` if an exception was raised during the execution. **Constraints:** 1. If `globals_dict` or `locals_dict` is `None`, an empty dictionary should be used by default. 2. The code should handle exceptions gracefully, returning `None` if an error occurs during execution. 3. You may only use standard Python libraries and functions provided in `python310` documentation. **Example Usage:** ```python code = \\"sum(range(10))\\" result = execute_python_code(code) # Expected output: 45 globals_dict = {\\"x\\": 10} locals_dict = {\\"y\\": 5} code = \\"x + y\\" result = execute_python_code(code, globals_dict, locals_dict) # Expected output: 15 code = \\"1 / 0\\" result = execute_python_code(code) # Expected output: None (due to division by zero) ``` **Extra Challenge:** Enhance the function to accept not only plain code strings but also file paths, and read and execute the code from the provided file. ```python def execute_python_code(code_or_path: str, is_path: bool = False, globals_dict: dict = None, locals_dict: dict = None, compiler_flags: int = 0) -> object: pass ``` **Example Usage:** ```python # Assuming you have a file \\"code.py\\" with content \\"x + y\\" globals_dict = {\\"x\\": 10} locals_dict = {\\"y\\": 5} result = execute_python_code(\\"code.py\\", is_path=True, globals_dict, locals_dict) # Expected output: 15 ``` Good luck and happy coding!","solution":"def execute_python_code(code: str, globals_dict: dict = None, locals_dict: dict = None, compiler_flags: int = 0) -> object: Executes the given Python code dynamically and returns the result of the execution. Parameters: code (str): A string containing valid Python source code. globals_dict (dict, optional): A dictionary representing the global execution context. default is None. locals_dict (dict, optional): A dictionary representing the local execution context. default is None. compiler_flags (int, optional): An integer representing the compiler flags. default is 0. Returns: object: The result of executing the code, or None if an exception was raised during the execution. if globals_dict is None: globals_dict = {} if locals_dict is None: locals_dict = {} try: compiled_code = compile(code, \'<string>\', \'eval\', compiler_flags) return eval(compiled_code, globals_dict, locals_dict) except: try: exec(code, globals_dict, locals_dict) return None except: return None"},{"question":"# Custom Deep Copy Implementation You are required to implement a class that can handle custom shallow and deep copying. The class should hold a nested structure of various lists and dictionaries, and you should provide methods to correctly copy these structures, taking into account the differences between shallow and deep copies. Requirements: 1. **Class Definition:** - Create a class called `DeepCopyExample` with the following attributes: - `data` (A dictionary with keys as strings and values as lists of integers). 2. **Methods:** - Implement the `__copy__()` method to provide a custom shallow copy of the object. - Implement the `__deepcopy__()` method to provide a custom deep copy of the object. 3. **Behavior:** - The shallow copy method should create a new instance of `DeepCopyExample` but reference the same lists within the `data` dictionary. - The deep copy method should create a new instance of `DeepCopyExample` with entirely new lists, recursively copied, within the `data` dictionary. Input and Output Formats: - **Input:** No direct input; you will be tested via pre-defined test cases that create instances of `DeepCopyExample` and invoke copying. - **Output:** The behavior of the shallow and deep copies will be tested by modifying the originals and examining the copied instances. Constraints: - You can assume that the `data` dictionary always contains lists of integers. - Design your methods to handle cases of nested lists or dictionaries if applicable. Example: ```python import copy class DeepCopyExample: def __init__(self, data): self.data = data def __copy__(self): # Provide a custom shallow copy implementation new_instance = DeepCopyExample(self.data) return new_instance def __deepcopy__(self, memo): # Provide a custom deep copy implementation new_data = {key: copy.deepcopy(value, memo) for key, value in self.data.items()} new_instance = DeepCopyExample(new_data) return new_instance # Example usage and test cases: original = DeepCopyExample({\'a\': [1, 2, 3], \'b\': [4, 5, 6]}) shallow_copy = copy.copy(original) deep_copy = copy.deepcopy(original) # Modify original data to test copying behavior original.data[\'a\'].append(7) # Check shallow copy - should reflect the change in original assert shallow_copy.data[\'a\'] == [1, 2, 3, 7] # Check deep copy - should NOT reflect the change in original assert deep_copy.data[\'a\'] == [1, 2, 3] ``` Make sure your implementation passes the above tests. Use the `copy` module to assist with the copying operations.","solution":"import copy class DeepCopyExample: def __init__(self, data): self.data = data def __copy__(self): # Provide a custom shallow copy implementation new_instance = DeepCopyExample(self.data.copy()) return new_instance def __deepcopy__(self, memo): # Provide a custom deep copy implementation new_data = {key: copy.deepcopy(value, memo) for key, value in self.data.items()} new_instance = DeepCopyExample(new_data) return new_instance"},{"question":"<|Analysis Begin|> The `site` module in Python 3.10 is involved with configuring the environment in which Python runs. It deals with setting up directories for site-packages and handling custom startup configurations via the `sitecustomize` and `usercustomize` modules. Key aspects include: - Manipulating `sys.path` to include site-specific directories. - Supporting the inclusion of .pth files that can add directories to `sys.path` or execute code. - Providing functions to retrieve site-package directories and user base directories. - A command line interface to get user directories. The documentation provides detailed descriptions of functions like `site.addsitedir()`, `site.getsitepackages()`, `site.getuserbase()`, and `site.getusersitepackages()`. Given the functionality and the purpose of the `site` module, I will design a question that focuses on manipulating the Python environment paths and creating modules that automatically execute on Python startup. <|Analysis End|> <|Question Begin|> **Question: Customizing the Python Environment with the `site` Module** *Objective*: The goal of this exercise is to demonstrate understanding of the `site` module by creating a custom Python environment setup. # Problem Statement You are tasked with setting up a custom Python environment for a project. This involves the following tasks: 1. **Creating a Custom Site Directory:** - Write a function `create_custom_site_dir(path: str) -> None` that takes a directory path as input, creates this directory if it doesn\'t exist, and adds a `.pth` file inside it to include another directory in `sys.path`. 2. **Adding Startup Customizations:** - Write a function `add_sitecustomize(path: str) -> None` that creates a `sitecustomize.py` file in the specified directory. This file should print `\\"Custom site customization loaded\\"` when executed. 3. **Verifying the Setup:** - Write a function `verify_setup(directory: str) -> bool` that verifies whether the custom directory and the `sitecustomize.py` file are correctly set up and whether `sitecustomize.py` prints the correct message on Python startup. # Function Specifications 1. `create_custom_site_dir(path: str) -> None`: - **Input**: - `path` (str): The path where the custom site directory should be created. - **Output**: - This function does not return any value. It should create a directory, if it does not already exist, and add a `.pth` file that includes another directory into the `sys.path`. - **Constraints**: - Use OS-independent code to handle file paths. 2. `add_sitecustomize(path: str) -> None`: - **Input**: - `path` (str): The path to the directory where `sitecustomize.py` should be created. - **Output**: - This function does not return any value. It should create a `sitecustomize.py` file that prints a confirmation message. - **Constraints**: - The file should be created in a platform-independent manner. 3. `verify_setup(directory: str) -> bool`: - **Input**: - `directory` (str): The directory to verify. - **Output**: - Returns `True` if the directory and `sitecustomize.py` are correctly set up and `sitecustomize.py` works as expected. Else, `False`. - **Constraints**: - The function should work independently of the operating system. # Example ```python # Example usage: create_custom_site_dir(\'/path/to/custom/site-dir\') add_sitecustomize(\'/path/to/custom/site-dir\') assert verify_setup(\'/path/to/custom/site-dir\') == True ``` # Notes - Ensure to handle file operations securely and properly close any opened files. - You are not allowed to use third-party libraries for this task. Write the implementation of `create_custom_site_dir`, `add_sitecustomize`, and `verify_setup` functions.","solution":"import os import sys def create_custom_site_dir(path): Creates a custom site directory and adds a .pth file inside it. Args: path (str): The path where the custom site directory should be created. if not os.path.exists(path): os.makedirs(path) # Create .pth file to include additional directory into sys.path pth_file_path = os.path.join(path, \'custom_paths.pth\') with open(pth_file_path, \'w\') as pth_file: # Adding the current directory to sys.path for demonstration purposes pth_file.write(os.getcwd() + \'n\') def add_sitecustomize(path): Creates a sitecustomize.py file in the specified directory. Args: path (str): The path to the directory where sitecustomize.py should be created. sitecustomize_file_path = os.path.join(path, \'sitecustomize.py\') with open(sitecustomize_file_path, \'w\') as sitecustomize_file: sitecustomize_file.write(\'print(\\"Custom site customization loaded\\")\') def verify_setup(directory): Verifies if the custom directory and sitecustomize.py are correctly set up. Args: directory (str): The directory to verify. Returns: bool: True if setup is correct, False otherwise. if not os.path.isdir(directory): return False sitecustomize_file = os.path.join(directory, \'sitecustomize.py\') if not os.path.isfile(sitecustomize_file): return False # Temporarily add the directory to sys.path to check sitecustomize.py execution original_sys_path = sys.path[:] sys.path.insert(0, directory) try: # Reloading sys might not re-execute sitecustomize.py if it is already imported # Using exec in a dedicated empty environment to simulate the custom setup import io from contextlib import redirect_stdout output = io.StringIO() with redirect_stdout(output): exec(open(sitecustomize_file).read(), {}) if \\"Custom site customization loaded\\" not in output.getvalue(): return False finally: sys.path = original_sys_path return True"},{"question":"Objective This question aims to assess your understanding of seaborn\'s aesthetic and scaling features to create customized and visually appealing plots. Problem Statement You are provided with the following data: ```python import numpy as np data = np.random.randn(100, 4) # 100 samples, 4 different variables ``` Write a Python function `customize_plot(data)` that: 1. Plots a boxplot for the given data using `seaborn`. 2. Uses a `darkgrid` theme for the plot. 3. Removes the right and top spines of the plot. 4. Temporarily switches the plot style to `white` theme and plots another plot (e.g., a violin plot) for the same data. 5. Adjusts the context of the second plot to be suitable for a presentation (i.e., \\"talk\\"). 6. Customizes the second plot by changing the font size to 1.5 times the default and the line width to 2.5. Your function should not return anything but should generate the plots inline. Constraints - Ensure you use appropriate seaborn functions for customizing the plot styles, contexts, and other parameters. - Your code should handle the switch of styles and contexts within the same function. Example Here is an example of what the output might look like: ```python import seaborn as sns import matplotlib.pyplot as plt def customize_plot(data): # Step 1: Boxplot with darkgrid theme sns.set_theme(style=\\"darkgrid\\") sns.boxplot(data=data) # Step 2: Remove right and top spines sns.despine() # Show the plot plt.show() # Step 3: Temporarily switch to white theme and contextualize with sns.axes_style(\\"white\\"): sns.set_context(\\"talk\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) sns.violinplot(data=data) # Show the second plot plt.show() # Example usage data = np.random.randn(100, 4) customize_plot(data) ``` Ensure you follow these exact steps and produce the specified plots as described.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def customize_plot(data): Plots a boxplot and violin plot for the given data using seaborn with specified customizations. Parameters: data (numpy.ndarray): Array of data to plot (100 samples, 4 different variables). # Step 1: Boxplot with darkgrid theme sns.set_theme(style=\\"darkgrid\\") sns.boxplot(data=data) # Step 2: Remove right and top spines sns.despine() # Show the first plot plt.show() # Step 3: Temporarily switch to white theme and contextualize with sns.axes_style(\\"white\\"): sns.set_context(\\"talk\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) sns.violinplot(data=data) # Show the second plot plt.show()"},{"question":"**Question: Implement a Custom JSON Serializer and Deserializer for a Complex Python Class** # Objective: To assess the student\'s understanding of the `json` module, particularly custom serialization and deserialization using `JSONEncoder` and `JSONDecoder`. # Problem Statement: You are required to implement serialization and deserialization for a custom Python class `Employee` using the `json` module. The `Employee` class contains attributes that are standard types (strings, integers, lists), as well as some special attributes (complex numbers and datetime). # Class Definition: ```python from datetime import datetime class Employee: def __init__(self, name, age, tasks, hire_date, secret_code): self.name = name self.age = age self.tasks = tasks self.hire_date = hire_date self.secret_code = complex(secret_code) # Store secret_code as complex number def __repr__(self): return (f\\"Employee(name={self.name}, age={self.age}, tasks={self.tasks}, \\" f\\"hire_date={self.hire_date}, secret_code={self.secret_code})\\") ``` # Requirements: 1. **Serialization**: - Implement `EmployeeEncoder` class inheriting from `json.JSONEncoder`. - Override the `default()` method to handle serialization of `Employee` instances. - Complex numbers should be represented as dictionaries with keys `\'real\'` and `\'imag\'` during serialization. - datetime objects should be converted to ISO 8601 strings. 2. **Deserialization**: - Implement `employee_decoder()` function to handle deserialization of JSON objects into `Employee` instances. - Handle conversion of dictionaries representing complex numbers back into `complex` types. - Handle conversion of ISO 8601 strings back into `datetime` objects. # Expected Functions: ```python import json from datetime import datetime class EmployeeEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Employee): # Complete the serialization implementation here pass if isinstance(obj, datetime): return obj.isoformat() if isinstance(obj, complex): return {\'real\': obj.real, \'imag\': obj.imag} return super().default(obj) def employee_decoder(dct): # Complete the deserialization implementation here pass ``` # Input and Output: - Implement the `emp = Employee(\\"John Doe\\", 30, [\\"task1\\", \\"task2\\"], datetime(2021, 5, 21), 5+3j)` instance. - Serialize the `Employee` instance `emp` to a JSON string. - Deserialize the JSON string back into an `Employee` object `emp_new`. # Constraints: - Name the JSON keys for Employee attributes as `name`, `age`, `tasks`, `hire_date`, and `secret_code`. - Ensure that datetime strings are in ISO 8601 format. - Maintain the nested structure of non-primitive attributes. # Example Usage: ```python emp = Employee(\\"John Doe\\", 30, [\\"task1\\", \\"task2\\"], datetime(2021, 5, 21), 5+3j) # Serialize the Employee instance json_str = json.dumps(emp, cls=EmployeeEncoder) print(json_str) # JSON string # Deserialize back to Employee instance emp_new = json.loads(json_str, object_hook=employee_decoder) print(emp_new) # Output should be in the form: # Employee(name=John Doe, age=30, tasks=[\'task1\', \'task2\'], hire_date=2021-05-21T00:00:00, secret_code=(5+3j)) ``` # Notes: - You should implement the missing parts of `EmployeeEncoder` and `employee_decoder` according to the requirements. - You must ensure that all the conversion and error handling is correctly implemented as per the specifications.","solution":"import json from datetime import datetime class Employee: def __init__(self, name, age, tasks, hire_date, secret_code): self.name = name self.age = age self.tasks = tasks self.hire_date = hire_date self.secret_code = complex(secret_code) # Store secret_code as complex number def __repr__(self): return (f\\"Employee(name={self.name}, age={self.age}, tasks={self.tasks}, \\" f\\"hire_date={self.hire_date}, secret_code={self.secret_code})\\") class EmployeeEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Employee): return { \'name\': obj.name, \'age\': obj.age, \'tasks\': obj.tasks, \'hire_date\': obj.hire_date.isoformat(), \'secret_code\': {\'real\': obj.secret_code.real, \'imag\': obj.secret_code.imag} } if isinstance(obj, datetime): return obj.isoformat() if isinstance(obj, complex): return {\'real\': obj.real, \'imag\': obj.imag} return super().default(obj) def employee_decoder(dct): if \'hire_date\' in dct: dct[\'hire_date\'] = datetime.fromisoformat(dct[\'hire_date\']) if \'secret_code\' in dct: dct[\'secret_code\'] = complex(dct[\'secret_code\'][\'real\'], dct[\'secret_code\'][\'imag\']) if all(k in dct for k in (\'name\', \'age\', \'tasks\', \'hire_date\', \'secret_code\')): return Employee(dct[\'name\'], dct[\'age\'], dct[\'tasks\'], dct[\'hire_date\'], dct[\'secret_code\']) return dct # Example Usage emp = Employee(\\"John Doe\\", 30, [\\"task1\\", \\"task2\\"], datetime(2021, 5, 21), 5+3j) # Serialize the Employee instance json_str = json.dumps(emp, cls=EmployeeEncoder) print(json_str) # JSON string # Deserialize back to Employee instance emp_new = json.loads(json_str, object_hook=employee_decoder) print(emp_new)"},{"question":"**Task:** You are required to write a function that performs a series of file system operations. **Function Specification:** ```python def manage_file_system(temp_dir: str, file_names: list, env_var: str, env_value: str) -> list: Manages file system operations including creating directories, files, modifying environment variables, and reading file contents. Args: - temp_dir (str): The base directory where operations are to be performed. - file_names (list): List of file names to be created within `temp_dir`. - env_var (str): The name of the environment variable to be modified. - env_value (str): The value to set for the environment variable. Returns: - list: A list of file contents read from the created files. ``` **Steps your function should perform:** 1. Check if `temp_dir` exists. If not, create `temp_dir`. 2. Create empty files with filenames provided in the `file_names` list inside `temp_dir`. 3. Set an environment variable with the name `env_var` to `env_value`. 4. Write the content of `env_value` into each of the files created in step 2. 5. Read the content back from each file to verify it was written correctly. 6. Return a list containing the content read from each file. **Constraints:** - Assume `temp_dir` is a string representing a valid directory path or a path-like object. - Each entry in `file_names` is a valid string representing the name of a file. - Environment variables should be set in the current environment using `os.environ`. **Example:** ```python # Example Inputs temp_dir = \\"/tmp/mytestdir\\" file_names = [\\"file1.txt\\", \\"file2.txt\\"] env_var = \\"MY_TEST_VAR\\" env_value = \\"Hello, World!\\" # Function Call content_list = manage_file_system(temp_dir, file_names, env_var, env_value) # Example Output print(content_list) # Expected to print: [\\"Hello, World!\\", \\"Hello, World!\\"] ``` **Hints:** - Use `os.makedirs()` or `os.mkdir()` to create directories. - Use `os.environ` to set environment variables. - Use `os.open()`, `os.write()`, `os.read()`, and `os.close()` for file operations. - Use error handling where necessary to handle and report any issues like file access errors or directory creation errors.","solution":"import os def manage_file_system(temp_dir: str, file_names: list, env_var: str, env_value: str) -> list: Manages file system operations including creating directories, files, modifying environment variables, and reading file contents. Args: - temp_dir (str): The base directory where operations are to be performed. - file_names (list): List of file names to be created within `temp_dir`. - env_var (str): The name of the environment variable to be modified. - env_value (str): The value to set for the environment variable. Returns: - list: A list of file contents read from the created files. # Step 1: Check if temp_dir exists, create if not if not os.path.exists(temp_dir): os.makedirs(temp_dir) # Step 2: Create empty files with the filenames provided for file_name in file_names: open(os.path.join(temp_dir, file_name), \'w\').close() # Step 3: Set the environment variable os.environ[env_var] = env_value # Step 4: Write env_value into each file for file_name in file_names: with open(os.path.join(temp_dir, file_name), \'w\') as file: file.write(env_value) # Step 5: Read the content back from each file content_list = [] for file_name in file_names: with open(os.path.join(temp_dir, file_name), \'r\') as file: content_list.append(file.read()) # Step 6: Return the list containing the content read from each file return content_list"},{"question":"Objective Demonstrate your comprehension and ability to utilize the seaborn library for complex data visualization tasks. Problem Statement You have been provided with a dataset containing information about tips received by waitstaff in a restaurant. Your task is to create a comprehensive visual analysis using seaborn, following these specific instructions: 1. Load the seaborn `tips` dataset. 2. Create a **scatter plot** using `relplot` that shows the relationship between the total bill and the tip amounts, distinguishing whether the person was a smoker or not using different colors. Include different marker styles for different days of the week. 3. Display a **linear regression plot** (`lmplot`) for the same relationship (total bill vs. tip amount) and include separate plots for lunch and dinner. Differentiate between smokers and non-smokers. 4. Create a **distribution plot** (`displot`) of total bill values, separated by time (lunch or dinner) and overlaid with Kernel Density Estimate (KDE) plots. 5. Generate a **categorical plot** (`catplot`) to visualize the average total bill amount for each day of the week, grouped by smoker status. Include error bars showing the 95% confidence interval for the mean. 6. Ensure all plots have appropriate titles, axis labels, and legends. Input and Output Formats - **Input**: Use the built-in seaborn `tips` dataset. - **Output**: Multiple plots must be generated as specified above. Constraints - Use seaborn functions to generate the plots. - You must add appropriate titles, axis labels, and legends to all plots. Example An example of a scatter plot: ```python import seaborn as sns import matplotlib.pyplot as plt # Load tips dataset tips = sns.load_dataset(\\"tips\\") # Scatter plot with relplot scatter_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"day\\" ) scatter_plot.fig.suptitle(\\"Scatter plot of Total Bill vs. Tip\\") plt.show() ``` Expectations - Implement the specified visualizations cleanly and effectively. - Utilize seaborn\'s high-level abstraction and customization options. - Demonstrate understanding of seaborn\'s integration with pandas and matplotlib for advanced plotting and customization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_visual_analysis(): tips = sns.load_dataset(\\"tips\\") # 1. Scatter plot using relplot scatter_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"day\\" ) scatter_plot.fig.suptitle(\\"Scatter plot of Total Bill vs. Tip\\") scatter_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") plt.show() # 2. Linear regression plot using lmplot lm_plot = sns.lmplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"time\\" ) lm_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") for ax in lm_plot.fig.axes: ax.set_title(ax.get_title().replace(\'time = \', \'\')) plt.show() # 3. Distribution plot using displot dis_plot = sns.displot( data=tips, x=\\"total_bill\\", hue=\\"time\\", kind=\\"kde\\", fill=True ) dis_plot.fig.suptitle(\\"Distribution of Total Bill values by Time\\") dis_plot.set_axis_labels(\\"Total Bill\\", \\"Density\\") plt.show() # 4. Categorical plot using catplot cat_plot = sns.catplot( data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"bar\\", ci=\\"sd\\" ) cat_plot.fig.suptitle(\\"Average Total Bill per Day by Smoker Status\\") cat_plot.set_axis_labels(\\"Day\\", \\"Average Total Bill\\") plt.show()"},{"question":"Objective To assess your understanding of the `select` module in Python, particularly how to handle multiple file descriptors efficiently using the `select` and `poll` functions. Problem Statement Implement a function `monitor_io(file_descriptors: List[int]) -> Dict[str, List[int]]` that accepts a list of file descriptors. The function should use the `select` function to monitor these file descriptors for readiness for reading, writing, and exceptional conditions. It should return a dictionary with three keys: - `\\"readable\\"`: A list of file descriptors that are ready for reading. - `\\"writable\\"`: A list of file descriptors that are ready for writing. - `\\"exceptional\\"`: A list of file descriptors that have an exceptional condition. Input - `file_descriptors`: A list of integers representing file descriptors. Output - A dictionary with keys `\\"readable\\"`, `\\"writable\\"`, and `\\"exceptional\\"`, each containing a list of file descriptors that are ready for the corresponding I/O operation. Constraints - The list `file_descriptors` will have at most 1000 file descriptors. - You do not need to handle signals or interruptions explicitly. Example ```python def monitor_io(file_descriptors: List[int]) -> Dict[str, List[int]]: # Implementation here # Example usage: fds = [0, 1, 2] # Typically, stdin, stdout, stderr result = monitor_io(fds) print(result) # Output might be something like: # {\\"readable\\": [0], \\"writable\\": [1, 2], \\"exceptional\\": []} ``` # Notes - This question tests your familiarity with non-blocking I/O operations and how to use the `select` module to efficiently monitor multiple file descriptors. - Make sure to handle cases where no file descriptors are ready by returning empty lists for `\\"readable\\"`, `\\"writable\\"`, and `\\"exceptional\\"` in the result dictionary.","solution":"import select from typing import List, Dict def monitor_io(file_descriptors: List[int]) -> Dict[str, List[int]]: Monitors a list of file descriptors for readiness for reading, writing, and exceptional conditions using the select function. readable, writable, exceptional = select.select(file_descriptors, file_descriptors, file_descriptors, 0) return { \\"readable\\": readable, \\"writable\\": writable, \\"exceptional\\": exceptional }"},{"question":"**Task:** Implement a function using PyTorch that takes as input a named tensor and performs a series of operations that demonstrate understanding of name inference rules as described in the provided documentation. # Function Signature ```python import torch def transform_named_tensor(input_tensor: torch.Tensor) -> torch.Tensor: Transform the input named tensor according to the given rules. Parameters: input_tensor (torch.Tensor): A named tensor of shape (N, C, H, W). Returns: torch.Tensor: The transformed tensor with appropriate dimension names. pass ``` # Instructions 1. **Input:** The function will take an input tensor of shape `(N, C, H, W)` with named dimensions. 2. **Operations to perform:** - **Step 1:** Apply absolute value to the input tensor and verify that the dimension names are preserved. - **Step 2:** Sum the tensor over the `C` dimension and check that the `C` dimension name gets removed. - **Step 3:** Perform element-wise addition of the resulting tensor with another tensor of compatible shape but with a missing dimension name. - **Step 4:** Transpose the resulting tensor to swap the `N` and the remaining dimension. - **Step 5:** Add the tensor with another tensor that requires name unification across dimensions. 3. **Output:** The transformed tensor after all operations with its appropriate dimension names. # Constraints and Requirements - Assume that the input tensor will always follow the format and named dimensions as specified. - The operations and their name inference should strictly follow the explanations given in the documentation. - Validate each step of name inference, raising an error if any of the checks fail. # Example ```python # Define input tensor with names input_tensor = torch.randn(3, 3, 3, 3, names=(\'N\', \'C\', \'H\', \'W\')) # The function should execute the series of transformations result_tensor = transform_named_tensor(input_tensor) # The result tensor should have appropriate dimension names as per operations print(result_tensor.names) # Expected names after all operations ``` **Note:** Ensure to test various intermediate states of transformed tensors to verify the correctness of dimension names at each step.","solution":"import torch def transform_named_tensor(input_tensor: torch.Tensor) -> torch.Tensor: Transform the input named tensor according to the given rules. Parameters: input_tensor (torch.Tensor): A named tensor of shape (N, C, H, W). Returns: torch.Tensor: The transformed tensor with appropriate dimension names. # Step 1: Apply absolute value abs_tensor = input_tensor.abs() assert abs_tensor.names == input_tensor.names, f\\"Names mismatch after abs operation: {abs_tensor.names} != {input_tensor.names}\\" # Step 2: Sum over the \'C\' dimension sum_tensor = abs_tensor.sum(dim=\'C\') expected_names = (\'N\', \'H\', \'W\') assert sum_tensor.names == expected_names, f\\"Names mismatch after sum operation: {sum_tensor.names} != {expected_names}\\" # Step 3: Element-wise addition with a compatible tensor compatible_tensor = torch.ones((3, 3, 3), names=(\'N\', \'H\', \'W\')) sum_added_tensor = sum_tensor + compatible_tensor assert sum_added_tensor.names == expected_names, f\\"Names mismatch after element-wise addition: {sum_added_tensor.names} != {expected_names}\\" # Step 4: Transpose to swap \'N\' and one of the remaining dimensions transposed_tensor = sum_added_tensor.align_to(\'H\', \'N\', \'W\') expected_names = (\'H\', \'N\', \'W\') assert transposed_tensor.names == expected_names, f\\"Names mismatch after transposition: {transposed_tensor.names} != {expected_names}\\" # Step 5: Add with another tensor that requires name unification additional_tensor = torch.ones((3, 3, 3), names=(\'height\', \'batch\', \'width\')).rename(height=\'H\', batch=\'N\', width=\'W\') final_tensor = transposed_tensor + additional_tensor assert final_tensor.names == expected_names, f\\"Names mismatch after final addition: {final_tensor.names} != {expected_names}\\" return final_tensor"},{"question":"# Image Type Identification and Extension Task You are provided with an image identification module `imghdr` that can recognize various image formats. Your task is as follows: 1. **Function Implementation**: Create a function `identify_image_type(filepath: str) -> str` that uses `imghdr.what()` to determine the type of an image at the provided file path. - **Input**: A string `filepath` representing the path to the image file. - **Output**: A string representing the identified image format (e.g., \'jpeg\', \'png\', \'gif\') or `\'unknown\'` if the format is not recognized. 2. **Extension Implementation**: Extend the functionality to recognize a new, custom image format. Assume the new format is identified by a specific byte signature at the start of the file: - Byte Signature: `b\'x89NEW\'` - Format Name: `\'newformat\'` Append a custom test function to `imghdr.tests` to recognize this new image format. - **Function to Implement**: `add_new_format_recognition() -> None` This function should append a custom test function for the \'newformat\' image type to `imghdr.tests`. # Constraints 1. Do not use external image processing libraries (e.g., PIL/Pillow). 2. Ensure the solution works for Python 3.10. # Example ```python # Assuming you have an image file \'example.png\' and a file \'example.new\' for the new format # Identify image type print(identify_image_type(\'example.png\')) # Output: \'png\' print(identify_image_type(\'example.new\')) # Output: \'unknown\' before extension, \'newformat\' after extension # Extend image type recognition to include \'newformat\' add_new_format_recognition() # Identify image type again print(identify_image_type(\'example.new\')) # Output: \'newformat\' ``` # Solution Template ```python import imghdr def identify_image_type(filepath: str) -> str: # Implementation of image type identification using imghdr.what() img_type = imghdr.what(filepath) return img_type if img_type is not None else \'unknown\' def add_new_format_recognition() -> None: # Custom test function to recognize the new image format def test_new_format(h, f): if h.startswith(b\'x89NEW\'): return \'newformat\' return None # Append the custom test function to imghdr.tests imghdr.tests.append(test_new_format) # Testing the solution (uncomment to test if running outside of assessment environment) # print(identify_image_type(\'example.png\')) # add_new_format_recognition() # print(identify_image_type(\'example.new\')) ```","solution":"import imghdr def identify_image_type(filepath: str) -> str: Identifies the image type of the file at the given filepath using imghdr. Args: filepath (str): The path to the image file. Returns: str: The identified image format or \'unknown\' if not recognized. img_type = imghdr.what(filepath) return img_type if img_type is not None else \'unknown\' def add_new_format_recognition() -> None: Extends imghdr to recognize a new custom image format with a specific byte signature. def test_new_format(h, f): if h.startswith(b\'x89NEW\'): return \'newformat\' return None # Append the custom test function to imghdr.tests imghdr.tests.append(test_new_format)"},{"question":"Objective Design and implement a Python function that leverages the `sys` module to manage memory usage and handle a specific auditing event. Requirements 1. **Memory Management**: - Implement a function named `manage_memory` that accepts an object and checks its size using `sys.getsizeof()`. - If the size exceeds a given threshold (e.g., 500 bytes), it should clear the internal type cache using `sys._clear_type_cache()` and force garbage collection. - The function should return the size of the object and a message indicating whether the type cache was cleared. 2. **Auditing Implementation**: - Implement a custom audit hook using `sys.addaudithook()`. - The audit hook should log all `sys.addaudithook` events to a file named `audit_log.txt`. - Ensure that the custom audit hook is functional and log an event using `sys.audit()`. Function Signature ```python import sys import gc def manage_memory(obj): Manages memory usage by checking the size of the object and clearing the type cache if the size exceeds a threshold. Parameters: obj (any): The object to check the size of. Returns: tuple: A tuple containing the size of the object and a message. # Your code here def setup_audit_log(): Sets up a custom audit hook to log audit events. Returns: None # Your code here def test_audit_log(): Tests the audit log setup by raising an audit event. Returns: None # Your code here # Example usage: # size, message = manage_memory(some_object) # setup_audit_log() # test_audit_log() ``` Constraints - The function `setup_audit_log` should ensure that the audit hook correctly logs events even if other hooks are present. - The function `manage_memory` should efficiently handle large objects without causing significant performance degradation. Evaluation Criteria - Correct implementation of memory management and type cache clearing. - Proper setup and logging of audit events. - Adherence to Python coding standards and best practices. Example ```python # Example usage if __name__ == \\"__main__\\": dummy_object = \\"a\\" * 1000 size, message = manage_memory(dummy_object) print(f\\"Object Size: {size}, Message: {message}\\") setup_audit_log() test_audit_log() # Check audit_log.txt for logged events. ```","solution":"import sys import gc def manage_memory(obj): Manages memory usage by checking the size of the object and clearing the type cache if the size exceeds a threshold. Parameters: obj (any): The object to check the size of. Returns: tuple: A tuple containing the size of the object and a message. size = sys.getsizeof(obj) threshold = 500 # Bytes message = \\"Type cache was not cleared.\\" if size > threshold: sys._clear_type_cache() gc.collect() message = \\"Type cache was cleared and garbage collection was forced.\\" return size, message def setup_audit_log(): Sets up a custom audit hook to log audit events. Returns: None def audit_hook(event, args): if event == \\"sys.addaudithook\\": with open(\'audit_log.txt\', \'a\') as log_file: log_file.write(f\\"Audit Event: {event}, Arguments: {args}n\\") sys.addaudithook(audit_hook) def test_audit_log(): Tests the audit log setup by raising an audit event. Returns: None sys.audit(\\"sys.addaudithook\\", \\"test_event\\") # Example usage: # size, message = manage_memory(some_object) # setup_audit_log() # test_audit_log()"},{"question":"<|Analysis Begin|> The provided documentation focuses on the concept of **Named Tensors** in PyTorch. Named Tensors allow users to assign explicit names to tensor dimensions, which can help in understanding and debugging tensor operations more easily. Key points from the documentation: - Named tensors provide additional safety by checking the correctness of operations at runtime. - Factory functions like `torch.zeros`, `torch.rand`, and others can create named tensors. - Named tensors can be manipulated using methods such as `align_to`, `refine_names`, `flatten`, and `unflatten`. - The `align_to` and `refine_names` methods are particularly useful for reordering and renaming dimensions respectively. - Named Tensors also allow for more readable code compared to unnamed tensors when performing operations like broadcasting and alignment. Given these key points, a suitable question should require students to demonstrate understanding of: 1. Creating named tensors. 2. Manipulating named dimensions (using methods like `align_to`, `flatten`, `unflatten`, etc.). 3. Performing operations that respect named dimensions. <|Analysis End|> <|Question Begin|> **Question:** As a data scientist, you are working with a complex multi-dimensional dataset and are required to perform various tensor operations while maintaining readability and ensuring the correctness of dimensions. Using PyTorch, you need to create and manipulate named tensors. Implement the following functions: 1. **create_named_tensor**: This function takes in a shape tuple, a list of dimension names of the same length, and a filler value, then returns a tensor filled with this value. 2. **reshape_with_alignment**: This function takes a named tensor and a list of names specifying the desired order of dimensions. The function should return the tensor aligned according to the specified order. 3. **merge_dimensions**: This function flattens specified consecutive dimensions of a named tensor into a single dimension and gives it a new name. 4. **split_dimension**: This function takes a named tensor and splits a specified dimension into multiple dimensions based on provided size constraints. **Function Signatures:** ```python import torch def create_named_tensor(shape: tuple, names: list, value: float) -> torch.Tensor: Creates a named tensor. Parameters: - shape (tuple): The shape of the tensor. - names (list): The list of names to be assigned to each dimension. - value (float): The value to fill the tensor with. Returns: - torch.Tensor: The created named tensor. pass def reshape_with_alignment(tensor: torch.Tensor, desired_order: list) -> torch.Tensor: Reshapes a tensor to align its dimensions according to the specified order. Parameters: - tensor (torch.Tensor): The named tensor. - desired_order (list): The list of names specifying the desired order. Returns: - torch.Tensor: The tensor with dimensions aligned according to desired_order. pass def merge_dimensions(tensor: torch.Tensor, dims_to_merge: list, new_dim_name: str) -> torch.Tensor: Merges consecutive dimensions of a tensor into a single dimension. Parameters: - tensor (torch.Tensor): The named tensor. - dims_to_merge (list): A list of names of dimensions to merge (must be consecutive). - new_dim_name (str): The name of the new dimension. Returns: - torch.Tensor: The tensor with the specified dimensions merged. pass def split_dimension(tensor: torch.Tensor, split_dim: str, new_dims: list) -> torch.Tensor: Splits a specified dimension into multiple dimensions based on provided size constraints. Parameters: - tensor (torch.Tensor): The named tensor. - split_dim (str): The name of the dimension to split. - new_dims (list): A list of tuples with new dimension names and their corresponding sizes. Returns: - torch.Tensor: The tensor with the specified dimension split into new dimensions. pass ``` **Examples:** ```python # Example for create_named_tensor t = create_named_tensor((2, 3), [\'N\', \'C\'], 1.0) print(t) # Expected Output: # tensor([[1., 1., 1.], # [1., 1., 1.]], names=(\'N\', \'C\')) # Example for reshape_with_alignment: t = torch.randn(2, 3, 4, names=(\'A\', \'B\', \'C\')) aligned_t = reshape_with_alignment(t, [\'C\', \'A\', \'B\']) print(aligned_t.names) # Expected Output: (\'C\', \'A\', \'B\') # Example for merge_dimensions: t = torch.randn(2, 3, 4, names=(\'A\', \'B\', \'C\')) merged_t = merge_dimensions(t, [\'B\', \'C\'], \'BC\') print(merged_t.names) # Expected Output: (\'A\', \'BC\') # Example for split_dimension: t = torch.randn(8, names=(\'D\',)) split_t = split_dimension(t, \'D\', [(\'E\', 2), (\'F\', 4)]) print(split_t.names) # Expected Output: (\'E\', \'F\') ``` **Constraints:** - The names of the dimensions must match the length and order of the shapes provided. - The dimensions specified for merging or splitting must exist and be valid. - The resulting tensor after each operation must retain valid and meaningful dimension names. **Performance Requirements:** - The implementation should efficiently handle typical tensor operations to avoid performance bottlenecks.","solution":"import torch def create_named_tensor(shape: tuple, names: list, value: float) -> torch.Tensor: Creates a named tensor. Parameters: - shape (tuple): The shape of the tensor. - names (list): The list of names to be assigned to each dimension. - value (float): The value to fill the tensor with. Returns: - torch.Tensor: The created named tensor. assert len(shape) == len(names), \\"Shape and names must have the same length.\\" return torch.full(shape, value, names=names) def reshape_with_alignment(tensor: torch.Tensor, desired_order: list) -> torch.Tensor: Reshapes a tensor to align its dimensions according to the specified order. Parameters: - tensor (torch.Tensor): The named tensor. - desired_order (list): The list of names specifying the desired order. Returns: - torch.Tensor: The tensor with dimensions aligned according to desired_order. return tensor.align_to(*desired_order) def merge_dimensions(tensor: torch.Tensor, dims_to_merge: list, new_dim_name: str) -> torch.Tensor: Merges consecutive dimensions of a tensor into a single dimension. Parameters: - tensor (torch.Tensor): The named tensor. - dims_to_merge (list): A list of names of dimensions to merge (must be consecutive). - new_dim_name (str): The name of the new dimension. Returns: - torch.Tensor: The tensor with the specified dimensions merged. start, end = tensor.names.index(dims_to_merge[0]), tensor.names.index(dims_to_merge[-1]) assert end - start + 1 == len(dims_to_merge), \\"Dimensions to merge are not consecutive.\\" return tensor.flatten(dims_to_merge, new_dim_name) def split_dimension(tensor: torch.Tensor, split_dim: str, new_dims: list) -> torch.Tensor: Splits a specified dimension into multiple dimensions based on provided size constraints. Parameters: - tensor (torch.Tensor): The named tensor. - split_dim (str): The name of the dimension to split. - new_dims (list): A list of tuples with new dimension names and their corresponding sizes. Returns: - torch.Tensor: The tensor with the specified dimension split into new dimensions. split_size = tuple(size for _, size in new_dims) assert tensor.size(split_dim) == torch.prod(torch.tensor(split_size)), \\"The product of new dimensions must match the original dimension size\\" new_names = [name for name, _ in new_dims] return tensor.unflatten(split_dim, list(zip(new_names, split_size)))"},{"question":"# Bytecode Compilation Task You are given a directory structure `source_dir` containing multiple Python source files and subdirectories. Your task is to write a Python script that uses the \\"compileall\\" module to compile all the Python files in `source_dir` into bytecode files. Implement a function `compile_source_dir` with the following specifications: Function Signature ```python def compile_source_dir(source_dir: str, max_recursion: int = 10, exclude_pattern: str = None, optimization_levels: list = None, use_workers: bool = False) -> bool: ``` # Input * `source_dir` (str): The path to the root directory containing Python source files. * `max_recursion` (int, optional): The maximum number of directory levels to recurse into. Default is 10. * `exclude_pattern` (str, optional): A regex pattern to exclude certain files or directories from being compiled. If `None`, no exclusion is applied. * `optimization_levels` (list, optional): A list of optimization levels to be used (`0`, `1`, and/or `2`). Default is `None`, which means no optimization will be applied. * `use_workers` (bool, optional): If `True`, utilize multiple worker threads for compilation. Default is `False`. # Output * Returns `True` if all files compile successfully, `False` otherwise. # Constraints 1. You should handle any exceptions that might occur during compilation. 2. Output detailed logging information indicating which files were successfully compiled and which were not. 3. If an optional parameter is not provided (`None`), you should handle it gracefully by setting appropriate default behaviors. # Example Usage ```python source_directory = \\"/path/to/source_dir\\" max_recursion_level = 5 exclude_files_pattern = r\'[/][.]svn\' opt_levels = [1, 2] enable_workers = True success = compile_source_dir( source_dir=source_directory, max_recursion=max_recursion_level, exclude_pattern=exclude_files_pattern, optimization_levels=opt_levels, use_workers=enable_workers ) print(\\"Compilation succeeded:\\", success) ``` # Notes 1. You should leverage the `compileall.compile_dir` function within your implementation. 2. Ensure that the `exclude_pattern` is converted into a `re.Pattern` object before use. 3. Use logging or print statements to output the status of each file compilation.","solution":"import compileall import re import logging def compile_source_dir(source_dir: str, max_recursion: int = 10, exclude_pattern: str = None, optimization_levels: list = None, use_workers: bool = False) -> bool: Compiles all Python files in the given source directory into bytecode files. Parameters: source_dir (str): The path to the root directory containing Python source files. max_recursion (int, optional): The maximum number of directory levels to recurse into. Default is 10. exclude_pattern (str, optional): A regex pattern to exclude certain files or directories from being compiled. If None, no exclusion is applied. optimization_levels (list, optional): A list of optimization levels to be used (0, 1, and/or 2). Default is None, which means no optimization will be applied. use_workers (bool, optional): If True, utilize multiple worker threads for compilation. Default is False. Returns: bool: True if all files compile successfully, False otherwise. logger = logging.getLogger() logger.setLevel(logging.DEBUG) handler = logging.StreamHandler() handler.setLevel(logging.DEBUG) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) logger.addHandler(handler) def exclude(match): if exclude_pattern: return re.search(exclude_pattern, match) is not None return False try: success = compileall.compile_dir( dir=source_dir, maxlevels=max_recursion, ddir=None, legacy=False, force=False, rx=re.compile(exclude_pattern) if exclude_pattern else None, quiet=0, optimize=optimization_levels, workers=use_workers ) if success: logger.info(f\'Compiled all files successfully.\') else: logger.error(f\'One or more files failed to compile.\') return success except Exception as e: logger.exception(f\'Compilation failed with exception: {e}\') return False"},{"question":"Dimensionality Reduction and Pipelines in scikit-learn **Objective:** Create a machine learning pipeline that includes dimensionality reduction using Principal Component Analysis (PCA) and evaluate its performance on a given dataset. You will compare this pipeline\'s performance with a baseline model that does not use PCA. **Task:** 1. Load the Iris dataset provided by scikit-learn. 2. Create a baseline machine learning pipeline that includes: - Standard scaling of features. - A supervised learning model (e.g., Support Vector Classifier). 3. Create a second pipeline that includes: - Standard scaling of features. - Dimensionality reduction using PCA. - The same supervised learning model used in the baseline pipeline. 4. Evaluate and compare the performance of both pipelines using cross-validation. 5. Report the cross-validation scores and discuss the impact of PCA on the model\'s performance. **Requirements:** - You must use `sklearn.pipeline.Pipeline` for creating both pipelines. - Use `sklearn.preprocessing.StandardScaler` for scaling the features. - Use `sklearn.decomposition.PCA` for dimensionality reduction. - Use `sklearn.svm.SVC` (Support Vector Classifier) as the supervised learning model. - Use `sklearn.model_selection.cross_val_score` for evaluating the models. - Set `random_state=42` in all applicable components to ensure reproducibility. **Input and Output Formats:** - There are no inputs or outputs for the function. The goal is to write the code in a Jupyter notebook or a Python script. - Your final implementation should print the cross-validation scores for both pipelines and a short discussion on the results. ```python # Sample solution template import numpy as np from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Create the baseline pipeline pipeline_baseline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC(random_state=42)) ]) # Create the PCA pipeline pipeline_pca = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2, random_state=42)), (\'svc\', SVC(random_state=42)) ]) # Evaluate the baseline pipeline using cross-validation scores_baseline = cross_val_score(pipeline_baseline, X, y, cv=5) # Evaluate the PCA pipeline using cross-validation scores_pca = cross_val_score(pipeline_pca, X, y, cv=5) # Print the cross-validation scores print(\\"Baseline pipeline scores:\\", scores_baseline) print(\\"PCA pipeline scores:\\", scores_pca) # Discuss the results ``` **Constraints and Performance:** - Ensure your code is efficient and runs within a reasonable time on a standard machine. - Discuss any patterns you observe in the performance of the pipelines. **Additional Notes:** - Make sure to explain your code with comments and provide a short discussion on the result differences. - Explore different numbers of principal components for PCA if you wish, but remember to keep the setup simple and focused on understanding the core concepts.","solution":"import numpy as np from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Create the baseline pipeline pipeline_baseline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC(random_state=42)) ]) # Create the PCA pipeline pipeline_pca = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2, random_state=42)), (\'svc\', SVC(random_state=42)) ]) # Evaluate the baseline pipeline using cross-validation scores_baseline = cross_val_score(pipeline_baseline, X, y, cv=5) # Evaluate the PCA pipeline using cross-validation scores_pca = cross_val_score(pipeline_pca, X, y, cv=5) # Print the cross-validation scores print(\\"Baseline pipeline scores:\\", scores_baseline) print(\\"PCA pipeline scores:\\", scores_pca) # Discuss the results Baseline pipeline scores are usually higher compared to PCA pipeline scores. Using PCA reduces the dimensionality of the data, which might lead to reduced information for classification. This can result in slightly lower performance. However, in cases where the original data has a lot of redundant dimensions, PCA can help in removing noise and achieving comparable or even better results."},{"question":"Objective You are given a DataFrame containing time series data of stock prices for different companies. Implement functions to compute rolling windows, expanding windows, and exponentially weighted windows, and then apply custom aggregation functions to these windows. Input You are provided with the following data: - A `DataFrame` `prices` with `DatetimeIndex` representing the dates and columns representing the stock prices of different companies. - A window size `N`. # Steps to complete: 1. **Rolling Sum Calculation:** Implement a function `rolling_sum(prices: pd.DataFrame, N: int) -> pd.DataFrame` to compute the rolling sum of the stock prices with a window size `N`. Ensure the window is centered. 2. **Expanding Mean Calculation:** Implement a function `expanding_mean(prices: pd.DataFrame) -> pd.DataFrame` to calculate the expanding mean of stock prices. 3. **Exponentially Weighted Mean Calculation:** Implement a function `ewm_mean(prices: pd.DataFrame, alpha: float) -> pd.DataFrame` to compute the exponentially weighted mean with the given smoothing factor `alpha`. 4. **Custom Rolling Aggregation:** Implement a function `custom_rolling_aggregation(prices: pd.DataFrame, func: Callable) -> pd.DataFrame` that takes a user-defined function and applies it to a rolling window of size `N`. Use the Numba engine for performance optimization. Constraints - The `DataFrame` `prices` will have at least 10 rows and 5 columns. - `N` will be an integer greater than 1 and less than or equal to the number of rows in the `prices`. - `alpha` will be a floating-point number between 0 and 1. - The custom function, `func`, will take a `numpy` array and return a single numerical value. Example ```python import pandas as pd import numpy as np # Example DataFrame dates = pd.date_range(\'2022-01-01\', periods=20, freq=\'D\') data = np.random.randn(20, 5) prices = pd.DataFrame(data, columns=[\'A\', \'B\', \'C\', \'D\', \'E\'], index=dates) N = 4 alpha = 0.2 def custom_func(arr): return np.mean(arr) # Your function calls rolling_result = rolling_sum(prices, N) expanding_result = expanding_mean(prices) ewm_result = ewm_mean(prices, alpha) custom_rolling_result = custom_rolling_aggregation(prices, custom_func) ``` Ensure your functions handle the input correctly and produce the results as described. The function definitions should be as follows: ```python def rolling_sum(prices: pd.DataFrame, N: int) -> pd.DataFrame: pass def expanding_mean(prices: pd.DataFrame) -> pd.DataFrame: pass def ewm_mean(prices: pd.DataFrame, alpha: float) -> pd.DataFrame: pass def custom_rolling_aggregation(prices: pd.DataFrame, func: Callable) -> pd.DataFrame: pass ``` Each function should return a `DataFrame` with the calculated values.","solution":"import pandas as pd import numpy as np from typing import Callable def rolling_sum(prices: pd.DataFrame, N: int) -> pd.DataFrame: Compute the rolling sum of the stock prices with window size N and centered. return prices.rolling(window=N, center=True).sum() def expanding_mean(prices: pd.DataFrame) -> pd.DataFrame: Calculate the expanding mean of stock prices. return prices.expanding().mean() def ewm_mean(prices: pd.DataFrame, alpha: float) -> pd.DataFrame: Compute the exponentially weighted mean of stock prices with the given alpha. return prices.ewm(alpha=alpha).mean() def custom_rolling_aggregation(prices: pd.DataFrame, N: int, func: Callable) -> pd.DataFrame: Apply a custom function to a rolling window of size N with performance optimization. return prices.rolling(window=N).apply(func, raw=True)"},{"question":"**Objective:** You are tasked with creating a custom color palette using seaborn\'s `cubehelix_palette` function and visualizing it using a seaborn plot. This question will test your understanding of the `cubehelix_palette` function and your ability to apply different parameters to achieve the desired color scheme. **Question:** Write a function `create_visualize_palette` that generates a seaborn cubehelix palette with the following specifications and visualizes it using a seaborn scatter plot: 1. The number of colors in the palette should be 12. 2. The palette should be a continuous colormap. 3. The starting point of the helix should be 1.5. 4. The rotation of the helix should be -0.4. 5. Use a gamma value of 0.8. 6. Increase the saturation of the colors to 1.2. 7. The luminance should start at 0.2 and end at 0.8. 8. Reverse the direction of the luminance ramp. The function should generate synthetic data for the scatter plot using the following specifications: - Generate 100 random data points for `x` and `y` ranging between 0 and 10. - Color the points by splitting the data into 12 bins, each corresponding to a color from the palette. **Function Signature:** ```python import seaborn as sns import numpy as np import matplotlib.pyplot as plt def create_visualize_palette(): pass ``` **Expected Output:** A scatter plot with 100 data points and the colors applied from the generated cubehelix palette. # Constraints: - You must use seaborn for color palettes and plotting. - Use the given specifications for the palette. # Example Usage: ```python create_visualize_palette() ``` This should display the scatter plot as described.","solution":"import seaborn as sns import numpy as np import matplotlib.pyplot as plt def create_visualize_palette(): # Generate the custom cubehelix palette palette = sns.cubehelix_palette( n_colors=12, start=1.5, rot=-0.4, gamma=0.8, hue=1.2, # Increasing the saturation dark=0.2, light=0.8, reverse=True ) # Generate synthetic data np.random.seed(42) x = np.random.uniform(0, 10, 100) y = np.random.uniform(0, 10, 100) # Create a colormap for the scatter plot colormap = sns.color_palette(palette, as_cmap=True) # Apply colors based on bins color_bins = np.digitize(y, bins=np.linspace(0, 10, 13)) - 1 colors = [palette[i] for i in color_bins] # Create a scatter plot plt.figure(figsize=(8, 6)) scatter = plt.scatter(x, y, c=colors, s=50, edgecolor=\'k\') plt.xlabel(\\"X axis\\") plt.ylabel(\\"Y axis\\") plt.title(\\"Scatter Plot with custom cubehelix palette\\") plt.colorbar(scatter, ticks=range(12), label=\'Color bins\') plt.show()"},{"question":"**Objective:** Write a function `plot_flight_data` that takes a boolean parameter `wide_format` and performs the following: 1. Loads the “flights” dataset using seaborn. 2. If `wide_format` is `True`, converts the dataset to a wide-form format, else keeps it as long-form. 3. Generates a line plot visualizing the average number of passengers per year, using seaborn. The x-axis should be the years and the y-axis should be the average number of passengers. 4. Include proper labels for the axes and a title for the plot. 5. Return the `FacetGrid` object generated by seaborn. **Input:** - `wide_format` (bool): If `True`, transforms the dataset to wide-form; if `False`, keeps it as long-form. **Output:** - Returns the seaborn `FacetGrid` object containing the plot. **Constraints:** - You are allowed to use seaborn and pandas libraries. - Assume the seaborn library and necessary datasets are preinstalled. - Performance should be optimized for large datasets. Here is a template for the function: ```python import seaborn as sns import pandas as pd def plot_flight_data(wide_format): # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Transform data based on wide_format if wide_format: flights = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Average the data and prepare for plotting if wide_format: flights_avg = flights.mean(axis=1).reset_index(name=\\"passengers\\") grid = sns.relplot(data=flights_avg, x=\\"year\\", y=\\"passengers\\", kind=\\"line\\") else: flights_avg = flights.groupby(\\"year\\").mean(numeric_only=True).reset_index() grid = sns.relplot(data=flights_avg, x=\\"year\\", y=\\"passengers\\", kind=\\"line\\") # Add labels and title grid.set_axis_labels(\\"Year\\", \\"Average Number of Passengers\\") grid.fig.suptitle(\\"Average Number of Passengers per Year\\") # Return the FacetGrid object return grid # Example usage grid = plot_flight_data(True) grid.savefig(\\"wide_format_plot.png\\") grid = plot_flight_data(False) grid.savefig(\\"long_format_plot.png\\") ```","solution":"import seaborn as sns import pandas as pd def plot_flight_data(wide_format): Loads the flights dataset, processes it according to the wide_format flag, and generates a line plot visualizing the average number of passengers per year. Parameters: wide_format (bool): If True, transforms the dataset to wide-form; if False, keeps it as long-form. Returns: sns.FacetGrid: The FacetGrid object containing the plot. # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Transform data based on wide_format if wide_format: flights = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Average the data and prepare for plotting if wide_format: flights_avg = flights.mean(axis=1).reset_index(name=\\"passengers\\") grid = sns.relplot(data=flights_avg, x=\\"year\\", y=\\"passengers\\", kind=\\"line\\") else: flights_avg = flights.groupby(\\"year\\").mean(numeric_only=True).reset_index() grid = sns.relplot(data=flights_avg, x=\\"year\\", y=\\"passengers\\", kind=\\"line\\") # Add labels and title grid.set_axis_labels(\\"Year\\", \\"Average Number of Passengers\\") grid.fig.suptitle(\\"Average Number of Passengers per Year\\") # Return the FacetGrid object return grid"},{"question":"**Problem: Named Tensor Manipulation and Operations** You are given a series of tasks to perform on named tensors in PyTorch. Complete the implementation of the functions below by following the guidelines provided. # Function 1: Create Named Tensor Implement a function `create_named_tensor` that creates a tensor with given shape, filling it with values from a specified range and assigns names to its dimensions. Signature ```python def create_named_tensor(shape: Tuple[int], names: Tuple[str], fill_value: float) -> torch.Tensor: pass ``` Arguments - `shape` (Tuple[int]): A tuple representing the shape of the tensor (e.g., (2, 3)). - `names` (Tuple[str]): A tuple containing the names for each dimension of the tensor (e.g., (\'N\', \'C\')). - `fill_value` (float): A value to fill the tensor with. Returns - `torch.Tensor`: A tensor of the specified shape, filled with the specified value, and with named dimensions. # Function 2: Apply Sign Function Implement a function `apply_sign` that applies the torch sign function to a named tensor, preserving its dimension names. Signature ```python def apply_sign(tensor: torch.Tensor) -> torch.Tensor: pass ``` Arguments - `tensor` (torch.Tensor): A named tensor to which the sign function will be applied. Returns - `torch.Tensor`: The result of applying the sign function to the input tensor, with the same dimension names. # Function 3: Unify Names and Add Tensors Implement a function `unify_and_add_tensors` that adds two named tensors after unifying their dimension names. If the tensor dimensions are not aligned, it should raise a runtime error. Signature ```python def unify_and_add_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: pass ``` Arguments - `tensor1` (torch.Tensor): The first named tensor. - `tensor2` (torch.Tensor): The second named tensor. Returns - `torch.Tensor`: The result of adding the two input tensors. # Constraints - The dimensions of the tensors will be at most 4. - The dimension names will be unique and non-empty strings of at most length 10. # Example ```python # Example usage and expected output # Function 1 t = create_named_tensor((2, 3), (\'N\', \'C\'), 1.0) print(t) # tensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], names=(\'N\', \'C\')) # Function 2 tsign = apply_sign(t) print(tsign) # tensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], names=(\'N\', \'C\')) # Function 3 t2 = create_named_tensor((2, 3), (\'N\', \'C\'), 3.0) result = unify_and_add_tensors(t, t2) print(result) # tensor([[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]], names=(\'N\', \'C\')) ``` Notes - Pay attention to name inference rules when performing operations. - Ensure to test your implementations to handle edge cases and invalid inputs.","solution":"import torch from typing import Tuple def create_named_tensor(shape: Tuple[int], names: Tuple[str], fill_value: float) -> torch.Tensor: Creates a tensor with the given shape, fills it with the specified value, and assigns names to its dimensions. # Create the tensor with the specified shape and fill value tensor = torch.full(shape, fill_value) # Assign names to the dimensions tensor = tensor.refine_names(*names) return tensor def apply_sign(tensor: torch.Tensor) -> torch.Tensor: Applies the sign function to a named tensor while preserving its dimension names. # Apply the sign function result = tensor.sign() # Return the resultant tensor with preserved names return result.refine_names(*tensor.names) def unify_and_add_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Adds two named tensors after unifying their dimension names. If dimensions are not aligned, raises a runtime error. # Ensure dimension names match if tensor1.names != tensor2.names: raise RuntimeError(\\"Dimension names do not match\\") # Perform the addition result = tensor1 + tensor2 return result"},{"question":"**Question**: You are given a dataset `spam.csv` that contains two columns: `text` (the message content) and `label` (either \\"spam\\" or \\"ham\\"). Your task is to implement a text classification pipeline using the Naive Bayes classifiers from the scikit-learn library to classify whether a given message is spam or not. **Instructions**: 1. **Load the Data**: Load the dataset `spam.csv` into a pandas DataFrame. 2. **Preprocess the Data**: - Perform text cleaning (removing punctuation, converting to lowercase, etc.). - Split the data into training and testing sets (80% train, 20% test) using `train_test_split` from `sklearn.model_selection`. 3. **Vectorize the Text Data**: Use `TfidfVectorizer` from `sklearn.feature_extraction.text` to convert the text data into TF-IDF feature vectors. 4. **Train and Evaluate Models**: - Implement and train the following Naive Bayes classifiers on the training data: - `MultinomialNB` - `BernoulliNB` - Predict the labels for the test data using each trained model. - Evaluate the performance of each model using accuracy and display the confusion matrix. **Constraints**: 1. You must use `MultinomialNB` and `BernoulliNB` from `sklearn.naive_bayes`. 2. Ensure reproducibility by setting `random_state=42` in `train_test_split` and `random_state` wherever applicable. **Expected Output**: You should print: - The accuracy of each model on the test data. - The confusion matrix for each model. **Sample Code Structure**: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB, BernoulliNB from sklearn.metrics import accuracy_score, confusion_matrix # Load the data df = pd.read_csv(\'spam.csv\') # Preprocess the data # [Your code here to clean the text data and encode labels] # Split the data X_train, X_test, y_train, y_test = train_test_split(..., random_state=42) # Vectorize the data vectorizer = TfidfVectorizer() X_train_tfidf = vectorizer.fit_transform(X_train) X_test_tfidf = vectorizer.transform(X_test) # Train and evaluate MultinomialNB multinomial_nb = MultinomialNB() multinomial_nb.fit(X_train_tfidf, y_train) y_pred_multinomial = multinomial_nb.predict(X_test_tfidf) accuracy_multinomial = accuracy_score(y_test, y_pred_multinomial) confusion_multinomial = confusion_matrix(y_test, y_pred_multinomial) # Train and evaluate BernoulliNB bernoulli_nb = BernoulliNB() bernoulli_nb.fit(X_train_tfidf, y_train) y_pred_bernoulli = bernoulli_nb.predict(X_test_tfidf) accuracy_bernoulli = accuracy_score(y_test, y_pred_bernoulli) confusion_bernoulli = confusion_matrix(y_test, y_pred_bernoulli) # Print results print(f\\"MultinomialNB Accuracy: {accuracy_multinomial}\\") print(\\"Confusion Matrix for MultinomialNB:\\") print(confusion_multinomial) print(f\\"BernoulliNB Accuracy: {accuracy_bernoulli}\\") print(\\"Confusion Matrix for BernoulliNB:\\") print(confusion_bernoulli) ``` You may assume `spam.csv` is available in the workspace. This task assesses your knowledge of text preprocessing, feature extraction, model training, and evaluation using scikit-learn\'s Naive Bayes classifiers.","solution":"import pandas as pd import string from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB, BernoulliNB from sklearn.metrics import accuracy_score, confusion_matrix def load_data(file_path): Load dataset from a CSV file. return pd.read_csv(file_path) def preprocess_text(text): Preprocess the text by removing punctuation and converting to lowercase. text = text.translate(str.maketrans(\'\', \'\', string.punctuation)) return text.lower() def prepare_data(df): Prepare the dataset by cleaning and splitting into training and testing sets. df[\'text\'] = df[\'text\'].apply(preprocess_text) X = df[\'text\'] y = df[\'label\'] return train_test_split(X, y, test_size=0.2, random_state=42) def vectorize_data(train_texts, test_texts): Vectorize the text data using TF-IDF. vectorizer = TfidfVectorizer() X_train_tfidf = vectorizer.fit_transform(train_texts) X_test_tfidf = vectorizer.transform(test_texts) return X_train_tfidf, X_test_tfidf, vectorizer def train_and_evaluate_model(model, X_train, y_train, X_test, y_test): Train and evaluate the Naive Bayes model. model.fit(X_train, y_train) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix def main(): # Load the data df = load_data(\'spam.csv\') # Preprocess and split the data X_train, X_test, y_train, y_test = prepare_data(df) # Vectorize the data X_train_tfidf, X_test_tfidf, vectorizer = vectorize_data(X_train, X_test) # Train and evaluate MultinomialNB multinomial_nb = MultinomialNB() accuracy_multinomial, confusion_multinomial = train_and_evaluate_model(multinomial_nb, X_train_tfidf, y_train, X_test_tfidf, y_test) # Train and evaluate BernoulliNB bernoulli_nb = BernoulliNB() accuracy_bernoulli, confusion_bernoulli = train_and_evaluate_model(bernoulli_nb, X_train_tfidf, y_train, X_test_tfidf, y_test) # Print results print(f\\"MultinomialNB Accuracy: {accuracy_multinomial}\\") print(\\"Confusion Matrix for MultinomialNB:\\") print(confusion_multinomial) print(f\\"BernoulliNB Accuracy: {accuracy_bernoulli}\\") print(\\"Confusion Matrix for BernoulliNB:\\") print(confusion_bernoulli) if __name__ == \\"__main__\\": main()"},{"question":"**Title**: Implement a Custom WSGI Server Serving Static Files Write a WSGI application that serves static files from a specified directory. You will implement a WSGI server using the `wsgiref.simple_server` module to serve files from a given directory, mimicking basic features of a web server like detecting MIME types and handling 404 errors when files are not found. **Specifications**: 1. Implement a function `static_file_app(environ, start_response)` that serves files from the directory specified by the `base_path` argument. 2. Implement a `create_server` function that initializes a WSGI server using the `make_server` function from the `wsgiref.simple_server` module, with the `static_file_app` application. **Details**: 1. The `static_file_app` function must: - Serve files requested through the WSGI `environ[\'PATH_INFO\']`. - Guess MIME types for the files using the `mimetypes` module. - Respond with a `404 Not Found` status if the requested file does not exist. - Respond with `200 OK` status if the file is found, with the correct MIME type. 2. Use the `FileWrapper` class from the `wsgiref.util` module to wrap the file content for efficient streaming. 3. Provide an optional `index.html` response if a directory is requested. 4. `create_server` should: - Take `host`, `port`, and `app` as arguments. - Use `make_server` from `wsgiref.simple_server` to start the server for the given WSGI application. **Constraints**: - You must not use any web framework (e.g., Flask, Django). Stick to the `wsgiref` utilities provided. - The server should manage concurrency by handling each request separately. - Your solution should be capable of being run as a standalone script and handle keyboard interruptions gracefully. **Example Usage**: ```python if __name__ == \'__main__\': base_path = \'./static\' # Directory to serve files from host = \'localhost\' port = 8000 def static_file_app(environ, start_response): # TODO: Implement serving static files def create_server(host, port, app): # TODO: Implement WSGI server creation # Create and serve the application httpd = create_server(host, port, static_file_app) print(f\\"Serving on port {port}...\\") try: httpd.serve_forever() except KeyboardInterrupt: print(\\"Shutting down.\\") httpd.server_close() ``` **Input Format**: - Directory path for static files to be served. - Host and port for the server. **Output Format**: - HTTP responses serving static files or 404 errors as appropriate. **Evaluation Criteria**: - Correctness: Correct implementation of the WSGI application and server. - Efficiency: Proper use of `FileWrapper` for file streaming. - Robustness: Ability to handle non-existing files and keyboard interruptions gracefully. - Compliance: Adherence to WSGI specifications and usage of `wsgiref` utilities.","solution":"import os import mimetypes from wsgiref.simple_server import make_server from wsgiref.util import FileWrapper def static_file_app(base_path): def app(environ, start_response): path_info = environ.get(\'PATH_INFO\', \'\') file_path = os.path.join(base_path, path_info.lstrip(\'/\')) if os.path.isdir(file_path): file_path = os.path.join(file_path, \'index.html\') if not os.path.exists(file_path): status = \'404 Not Found\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) return [b\'404 Not Found\'] mime_type, _ = mimetypes.guess_type(file_path) mime_type = mime_type or \'application/octet-stream\' status = \'200 OK\' headers = [(\'Content-type\', mime_type)] start_response(status, headers) return FileWrapper(open(file_path, \'rb\')) return app def create_server(host, port, app): httpd = make_server(host, port, app) return httpd if __name__ == \'__main__\': base_path = \'./static\' # Directory to serve files from host = \'localhost\' port = 8000 app = static_file_app(base_path) httpd = create_server(host, port, app) print(f\\"Serving on port {port}...\\") try: httpd.serve_forever() except KeyboardInterrupt: print(\\"Shutting down.\\") httpd.server_close()"},{"question":"**Question: Implement PCA for Dimensionality Reduction** This task involves implementing your own version of the PCA transformation using scikit-learn and applying it on a dataset. The goal is to demonstrate your understanding of dimensionality reduction, variance explanation, and data transformation. # Step-by-Step Instructions 1. **Data Preprocessing:** - Load the Iris dataset from scikit-learn. - Standardize the dataset (mean = 0 and variance = 1 for each feature). 2. **PCA Implementation:** - Implement PCA using scikit-learn’s `PCA` class. - Fit the model on the standardized data and project the data onto the first two principal components. 3. **Variance Explanation:** - Determine the amount of variance explained by each of the first two components. - Print out the explained variance ratio. 4. **Visualization:** - Plot the data points in the new 2-dimensional space using the first two principal components. - Color the points by their respective class labels. # Constraints - Use `scikit-learn` package for implementing PCA. - Input data must be standardized before applying PCA. # Input and Output Formats - **Input:** None (The Iris dataset is to be loaded internally). - **Output:** Explained variance ratios and a 2D scatter plot. # Performance Requirements - Ensure that your implementation is efficient and properly utilizes the scikit-learn library functionalities. # Example: ```python import matplotlib.pyplot as plt import numpy as np from sklearn import datasets from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler # Step 1: Load and Standardize the Data iris = datasets.load_iris() X = iris.data y = iris.target scaler = StandardScaler() X_standardized = scaler.fit_transform(X) # Step 2: Implement PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_standardized) # Step 3: Explained Variance explained_variance_ratio = pca.explained_variance_ratio_ print(f\'Explained variance ratio of first two components: {explained_variance_ratio}\') # Step 4: Visualization plt.figure(figsize=(8, 6)) for label, color in zip(np.unique(y), [\'r\', \'g\', \'b\']): plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], label=iris.target_names[label], c=color) plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.legend() plt.title(\'PCA of IRIS dataset\') plt.show() ``` By completing this task, you will solidify your understanding of PCA and its application in reducing the dimensionality of datasets while retaining the essential structure and information.","solution":"import matplotlib.pyplot as plt import numpy as np from sklearn import datasets from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler def perform_pca(): # Step 1: Load and Standardize the Data iris = datasets.load_iris() X = iris.data y = iris.target scaler = StandardScaler() X_standardized = scaler.fit_transform(X) # Step 2: Implement PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_standardized) # Step 3: Explained Variance explained_variance_ratio = pca.explained_variance_ratio_ print(f\'Explained variance ratio of first two components: {explained_variance_ratio}\') # Step 4: Visualization plt.figure(figsize=(8, 6)) for label, color in zip(np.unique(y), [\'r\', \'g\', \'b\']): plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], label=iris.target_names[label], c=color) plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.legend() plt.title(\'PCA of IRIS dataset\') plt.show() return explained_variance_ratio, X_pca, y, iris.target_names"},{"question":"You are required to implement a function that processes a batch of images represented as tensors and performs basic dimensionality manipulations using PyTorch. This task will assess both your understanding of various PyTorch tensor operations and your ability to handle tensors flexibly. # Problem Statement Write a function `manipulate_tensor` that takes a 4D tensor `input_tensor` of shape `(N, C, H, W)` where: - `N` is the batch size. - `C` is the number of channels. - `H` is the height of the images. - `W` is the width of the images. Your function should: 1. **Reshape** the tensor to combine the batch size and channels into one dimension, resulting in a shape of `(N*C, H, W)`. 2. **Permute** the dimensions to change the order from `(N*C, H, W)` to `(H, W, N*C)`. 3. **Resize** the tensor to have `H//2`, `W//2`, and `N*C` dimensions by taking every second pixel in both height and width dimensions. # Implementation Details - **Input**: - `input_tensor` (torch.Tensor): A 4D tensor of shape `(N, C, H, W)`. - **Output**: - A 3D tensor that has undergone the specified manipulations. - **Constraints**: - `N, C` are positive integers. - `H, W` are even positive integers. - You may assume that the provided tensor is always of the appropriate shape and that `H` and `W` are always even. - Do not use any pre-built high-level functions that directly perform the required transformations. # Function Signature ```python import torch def manipulate_tensor(input_tensor: torch.Tensor) -> torch.Tensor: pass ``` # Example ```python import torch # Example input tensor with shape (2, 3, 4, 4) input_tensor = torch.arange(96).view(2, 3, 4, 4) # Calling the function output_tensor = manipulate_tensor(input_tensor) print(output_tensor.shape) # Expected shape: (2, 2, 6, 6) print(output_tensor) # Example output values ``` # Notes - Ensure that the output tensor maintains the correct dimensions and retains the appropriate content after each transformation. - Use pytest or another testing framework to validate your implementation with different inputs.","solution":"import torch def manipulate_tensor(input_tensor: torch.Tensor) -> torch.Tensor: # Step 1: Reshape the tensor to combine the batch size and channels into one dimension N, C, H, W = input_tensor.shape reshaped_tensor = input_tensor.view(N * C, H, W) # Step 2: Permute the dimensions to change the order from (N*C, H, W) to (H, W, N*C) permuted_tensor = reshaped_tensor.permute(1, 2, 0) # Step 3: Resize the tensor to have (H//2, W//2, N*C) dimensions resized_tensor = permuted_tensor[::2, ::2, :] return resized_tensor"},{"question":"**Objective:** Demonstrate your understanding of the `scikit-learn` dataset generation functions by creating and visualizing various synthetic datasets. # Problem Statement: You are required to write a Python function called `generate_and_plot_datasets` that uses `scikit-learn` to generate multiple datasets and then visualize them. Your function should generate the following datasets and display them in a 2x3 grid of subplots: 1. **Blobs:** - Use `make_blobs` to create a dataset with 4 centers and a standard deviation of 0.6. 2. **Classification:** - Use `make_classification` to create a dataset with: - 3 informative features - 1 redundant feature - 2 clusters per class - 3 classes 3. **Gaussian Quantiles:** - Use `make_gaussian_quantiles` to create a dataset with 2 features and 3 classes. 4. **Moons:** - Use `make_moons` to create a dataset with noise level of 0.2. 5. **Circles:** - Use `make_circles` to create a dataset with noise level of 0.2 and a factor of 0.5. 6. **Regression:** - Use `make_regression` to create a dataset with: - 2 informative features - 100 samples - Noise level of 10 # Requirements: - The function should have the following signature: ```python def generate_and_plot_datasets(): pass ``` - You should use `matplotlib` for visualization and display each dataset in a separate subplot within a 2x3 grid. - Each subplot should have a title corresponding to the dataset type (e.g., \\"Blobs\\", \\"Classification\\", \\"Gaussian Quantiles\\", etc.) - Ensure that the generated datasets are well-separated and clearly distinguishable in the plots. # Example Output: The function should generate and display a plot like the following: ``` +--------------------+-----------------------+-------------------------+ | Blobs | Classification | Gaussian Quantiles | +--------------------+-----------------------+-------------------------+ | Moons | Circles | Regression | +--------------------+-----------------------+-------------------------+ ``` Please make sure your code is clean, well-documented, and follows best practices for Python programming. # Constraints: - Use `random_state=42` for all random generators to ensure reproducibility. - Your solution should be implemented in Python 3.x. - Use only the standard libraries and `scikit-learn` for dataset generation and visualization. # Evaluation: Your solution will be evaluated on the correctness of dataset generation, clarity of the plots, and adherence to the requirements.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles, make_moons, make_circles, make_regression def generate_and_plot_datasets(): fig, axs = plt.subplots(2, 3, figsize=(18, 12)) # Blobs X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42) axs[0, 0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap=\'viridis\') axs[0, 0].set_title(\\"Blobs\\") # Classification X_classification, y_classification = make_classification(n_samples=300, n_features=4, n_informative=3, n_redundant=1, n_clusters_per_class=2, n_classes=3, random_state=42) axs[0, 1].scatter(X_classification[:, 0], X_classification[:, 1], c=y_classification, cmap=\'viridis\') axs[0, 1].set_title(\\"Classification\\") # Gaussian Quantiles X_gq, y_gq = make_gaussian_quantiles(n_samples=300, n_features=2, n_classes=3, random_state=42) axs[0, 2].scatter(X_gq[:, 0], X_gq[:, 1], c=y_gq, cmap=\'viridis\') axs[0, 2].set_title(\\"Gaussian Quantiles\\") # Moons X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42) axs[1, 0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=\'viridis\') axs[1, 0].set_title(\\"Moons\\") # Circles X_circles, y_circles = make_circles(n_samples=300, noise=0.2, factor=0.5, random_state=42) axs[1, 1].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=\'viridis\') axs[1, 1].set_title(\\"Circles\\") # Regression X_regression, y_regression = make_regression(n_samples=100, n_features=2, n_informative=2, noise=10, random_state=42) axs[1, 2].scatter(X_regression[:, 0], y_regression, c=y_regression, cmap=\'viridis\') axs[1, 2].set_title(\\"Regression\\") plt.tight_layout() plt.show()"},{"question":"Objective This question aims to assess your understanding of different methods of parsing and executing Python code from various input forms, as detailed in the Python310 documentation. Problem Statement You are required to implement a Python function named `execute_code` that can dynamically parse and execute Python code from different input forms. The function should support three modes of input: 1. **Complete Program:** Represented as a multi-line string containing valid Python code. 2. **Interactive Input:** Represented as a list of strings, where each string is a valid Python statement. 3. **Expression Input:** Represented as a single string containing a valid Python expression. The function should handle these inputs appropriately, execute the code, and return the result for expression inputs, and for complete programs and interactive inputs, it should return a string \\"Program Executed Successfully\\". Function Signature ```python def execute_code(input_data: Union[str, List[str]], mode: str) -> Union[str, Any]: ``` Input - `input_data` (Union[str, List[str]]): The code input to be executed. Depending on the `mode`, it can be a string or a list of strings. - `mode` (str): The mode of input. Can be one of `\\"complete_program\\"`, `\\"interactive_input\\"`, or `\\"expression_input\\"`. Output - Returns `Union[str, Any]`: For `complete_program` and `interactive_input`, return the string \\"Program Executed Successfully\\". For `expression_input`, return the result of evaluating the expression. Example Usage ```python # Example 1: Complete Program program = def greet(name): return f\\"Hello, {name}!\\" print(greet(\\"Alice\\")) print(execute_code(program, \\"complete_program\\")) # Output: \\"Program Executed Successfully\\" # Example 2: Interactive Input interactive_statements = [ \\"def add(a, b):\\", \\" return a + b\\", \\"\\", \\"result = add(2, 3)\\" ] print(execute_code(interactive_statements, \\"interactive_input\\")) # Output: \\"Program Executed Successfully\\" # Example 3: Expression Input expression = \\"2 + 2\\" print(execute_code(expression, \\"expression_input\\")) # Output: 4 ``` Constraints - Assume the input data is always valid Python code. - You cannot use any third-party libraries for this task. - Consider security implications when using `eval()`. Ensure that only safe expressions are evaluated. Performance Requirements - The function should efficiently handle input sizes up to 10,000 lines of code for complete programs and interactive inputs. - The function should not have significant delays in parsing and executing the code. Note - Pay attention to the different contexts in which code is executed (complete programs, interactive inputs, and expressions) and handle them according to Python syntax rules.","solution":"from typing import Union, List, Any def execute_code(input_data: Union[str, List[str]], mode: str) -> Union[str, Any]: if mode == \\"complete_program\\": exec(input_data) return \\"Program Executed Successfully\\" elif mode == \\"interactive_input\\": interactive_code = \\"n\\".join(input_data) exec(interactive_code) return \\"Program Executed Successfully\\" elif mode == \\"expression_input\\": result = eval(input_data) return result else: raise ValueError(\\"Unknown mode provided\\")"},{"question":"# Unicode Text Analysis and Normalization You are tasked with writing a function that processes a string of text, identifies various properties of the Unicode characters in the text, and normalizes the text to a specified form. The function should also summarize some statistics about the different types of characters present in the text. Function Signature ```python def analyze_and_normalize_text(text: str, normalization_form: str) -> dict: pass ``` Input - `text` (str): A string of Unicode text to be analyzed and normalized. - `normalization_form` (str): A normalization form that can be one of \'NFC\', \'NFKC\', \'NFD\', or \'NFKD\'. Output - Returns a dictionary containing the following keys: - `\'normalized_text\'`: The normalized version of the input text. - `\'char_properties\'`: A list of dictionaries, each containing properties for a character in the input text. Each dictionary should have the following keys: - `\'char\'`: The character itself. - `\'name\'`: The name of the character. - `\'category\'`: The general category of the character. - `\'bidirectional\'`: The bidirectional class of the character. - `\'decimal_value\'` (only if available): The decimal value of the character. - `\'stats\'`: A dictionary summarizing the counts of unique character categories in the text. Constraints 1. The input `text` can have a length of up to `1000` characters. 2. The `normalization_form` must be one of the specified forms (\'NFC\', \'NFKC\', \'NFD\', \'NFKD\'). 3. You should handle cases where a character property (like decimal value) might not be available gracefully. Example ```python input_text = \\"Año 2021\\" normalization_form = \\"NFC\\" output = analyze_and_normalize_text(input_text, normalization_form) ``` Expected Output: ```python { \'normalized_text\': \\"Año 2021\\", \'char_properties\': [ {\'char\': \'A\', \'name\': \'LATIN CAPITAL LETTER A\', \'category\': \'Lu\', \'bidirectional\': \'L\'}, {\'char\': \'ñ\', \'name\': \'LATIN SMALL LETTER N WITH TILDE\', \'category\': \'Ll\', \'bidirectional\': \'L\'}, {\'char\': \'o\', \'name\': \'LATIN SMALL LETTER O\', \'category\': \'Ll\', \'bidirectional\': \'L\'}, {\'char\': \' \', \'name\': \'SPACE\', \'category\': \'Zs\', \'bidirectional\': \'WS\'}, {\'char\': \'2\', \'name\': \'DIGIT TWO\', \'category\': \'Nd\', \'bidirectional\': \'EN\', \'decimal_value\': 2}, {\'char\': \'0\', \'name\': \'DIGIT ZERO\', \'category\': \'Nd\', \'bidirectional\': \'EN\', \'decimal_value\': 0}, {\'char\': \'2\', \'name\': \'DIGIT TWO\', \'category\': \'Nd\', \'bidirectional\': \'EN\', \'decimal_value\': 2}, {\'char\': \'1\', \'name\': \'DIGIT ONE\', \'category\': \'Nd\', \'bidirectional\': \'EN\', \'decimal_value\': 1} ], \'stats\': { \'Lu\': 1, \'Ll\': 2, \'Zs\': 1, \'Nd\': 4 } } ``` This question requires students to understand and utilize various functions from the `unicodedata` module, effectively handle edge cases, and demonstrate knowledge of Unicode text processing and normalization.","solution":"import unicodedata def analyze_and_normalize_text(text: str, normalization_form: str) -> dict: if normalization_form not in (\'NFC\', \'NFKC\', \'NFD\', \'NFKD\'): raise ValueError(\\"Normalization form must be one of \'NFC\', \'NFKC\', \'NFD\', or \'NFKD\'\\") normalized_text = unicodedata.normalize(normalization_form, text) char_properties = [] category_counts = {} for char in text: char_info = { \'char\': char, \'name\': unicodedata.name(char, \\"UNKNOWN\\"), \'category\': unicodedata.category(char), \'bidirectional\': unicodedata.bidirectional(char) } try: char_info[\'decimal_value\'] = unicodedata.decimal(char) except ValueError: pass char_properties.append(char_info) category = char_info[\'category\'] if category in category_counts: category_counts[category] += 1 else: category_counts[category] = 1 return { \'normalized_text\': normalized_text, \'char_properties\': char_properties, \'stats\': category_counts }"},{"question":"# Question: Data Analysis with Pandas You are provided with a dataset representing sales data for a retail store. The dataset contains the following columns: - `Date`: Date of the sale - `Store`: Store identifier - `Product`: Product identifier - `Quantity`: Number of units sold - `Revenue`: Total revenue from the sale You need to implement a function named `analyze_sales` that performs the following operations: 1. **Load the data**: Read the data from a CSV file. 2. **Filter and Create Columns**: - Filter the data to select sales that occurred in 2023. - Create a new column `RevenuePerUnit` which is calculated as `Revenue / Quantity`. - Create another column `HighRevenue` which is `True` if the `Revenue` is greater than 500, otherwise `False`. 3. **Group and Aggregate**: - Group the data by `Store` and `Product`, and calculate the following for each group: - Total `Quantity` sold. - Maximum `RevenuePerUnit`. - Count of `HighRevenue` transactions. - The date of the earliest and latest sale in each group. 4. **Output Format**: Return the result as a pandas DataFrame with the columns: `Store`, `Product`, `TotalQuantity`, `MaxRevenuePerUnit`, `HighRevenueCount`, `FirstSaleDate`, `LastSaleDate`. 5. **Performance consideration**: Ensure your function handles large datasets efficiently. # Input: - The path to the CSV file (string). # Output: - A pandas DataFrame with the aggregated results. # Constraints: - Assume the CSV file is correctly formatted with valid data in the described columns. - The date format in the `Date` column is `YYYY-MM-DD`. # Example: Suppose the CSV file content is: ``` Date,Store,Product,Quantity,Revenue 2023-01-15,StoreA,Product1,5,250.00 2023-01-20,StoreA,Product1,3,600.00 2023-02-05,StoreB,Product2,10,700.00 2022-12-11,StoreA,Product1,4,400.00 ``` The function call `analyze_sales(\\"sales_data.csv\\")` should return a DataFrame: ``` Store Product TotalQuantity MaxRevenuePerUnit HighRevenueCount FirstSaleDate LastSaleDate 0 StoreA Product1 8 200.0 1 2023-01-15 2023-01-20 1 StoreB Product2 10 70.0 1 2023-02-05 2023-02-05 ``` # Implementation ```python import pandas as pd def analyze_sales(file_path): # Step 1: Load the data df = pd.read_csv(file_path) # Step 2: Filter data for the year 2023 df[\'Date\'] = pd.to_datetime(df[\'Date\']) df_2023 = df[df[\'Date\'].dt.year == 2023] # Step 3: Create new columns df_2023[\'RevenuePerUnit\'] = df_2023[\'Revenue\'] / df_2023[\'Quantity\'] df_2023[\'HighRevenue\'] = df_2023[\'Revenue\'] > 500 # Step 4: Group and aggregate grouped = df_2023.groupby([\'Store\', \'Product\']).agg( TotalQuantity=(\'Quantity\', \'sum\'), MaxRevenuePerUnit=(\'RevenuePerUnit\', \'max\'), HighRevenueCount=(\'HighRevenue\', \'sum\'), FirstSaleDate=(\'Date\', \'min\'), LastSaleDate=(\'Date\', \'max\') ).reset_index() return grouped ``` You can download the sample CSV file [here](path-to-sample-csv).","solution":"import pandas as pd def analyze_sales(file_path): # Step 1: Load the data df = pd.read_csv(file_path) # Step 2: Filter data for the year 2023 df[\'Date\'] = pd.to_datetime(df[\'Date\']) df_2023 = df[df[\'Date\'].dt.year == 2023] # Step 3: Create new columns df_2023[\'RevenuePerUnit\'] = df_2023[\'Revenue\'] / df_2023[\'Quantity\'] df_2023[\'HighRevenue\'] = df_2023[\'Revenue\'] > 500 # Step 4: Group and aggregate grouped = df_2023.groupby([\'Store\', \'Product\']).agg( TotalQuantity=(\'Quantity\', \'sum\'), MaxRevenuePerUnit=(\'RevenuePerUnit\', \'max\'), HighRevenueCount=(\'HighRevenue\', \'sum\'), FirstSaleDate=(\'Date\', \'min\'), LastSaleDate=(\'Date\', \'max\') ).reset_index() return grouped"},{"question":"# Question Introduction You are provided with a dataset representing sales data from different cities over multiple years. The dataset is stored in a pandas DataFrame and has the following columns: - `city` (str): Name of the city. - `year` (int): Year of the sales record. - `sales` (int): Sales amount for that particular year. - `category` (str): Category of the product sold. You are required to create visualizations using seaborn to analyze trends in the sales data. Requirements 1. **Convert Data from Wide-form to Long-form and Vice Versa**: - Convert the data to wide-form where each city\'s sales are a separate column. - Convert the data back to long-form. 2. **Visualize Data**: - Create a line plot to visualize the total sales per year for each city using long-form data. - Create a bar plot to show the average sales per category using wide-form data. Input 1. A pandas DataFrame named `sales_data` with columns: `city`, `year`, `sales`, `category`. ```python import pandas as pd data = { \\"city\\": [\\"New York\\", \\"Los Angeles\\", \\"New York\\", \\"Los Angeles\\", \\"New York\\", \\"Los Angeles\\"], \\"year\\": [2019, 2019, 2020, 2020, 2021, 2021], \\"sales\\": [1000, 1500, 1100, 1600, 1200, 1700], \\"category\\": [\\"Electronics\\", \\"Electronics\\", \\"Electronics\\", \\"Electronics\\", \\"Furniture\\", \\"Furniture\\"] } sales_data = pd.DataFrame(data) ``` Expected Output 1. **Wide-form Data**: A wide-form DataFrame where each city\'s sales are in separate columns. 2. **Long-form Data**: A long-form DataFrame from the wide-form data. 3. **Visualizations**: - A line plot showing total sales per year for each city. - A bar plot showing the average sales per category. Implementation Implement the following functions: 1. `convert_to_wide_form(data: pd.DataFrame) -> pd.DataFrame`: Converts the long-form data into wide-form. 2. `convert_to_long_form(data: pd.DataFrame) -> pd.DataFrame`: Converts the wide-form data back to long-form. 3. `plot_total_sales_per_year(data: pd.DataFrame)`: Plots a line plot for total sales per year for each city. 4. `plot_average_sales_per_category(data: pd.DataFrame)`: Plots a bar plot for average sales per category using wide-form data. Constraints - Use seaborn for plotting. - Ensure the x-axis and y-axis labels are appropriately set. - Use different hues for different cities in the line plot. - Handle any missing data appropriately. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def convert_to_wide_form(data: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass def convert_to_long_form(data: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass def plot_total_sales_per_year(data: pd.DataFrame): # Your implementation here pass def plot_average_sales_per_category(data: pd.DataFrame): # Your implementation here pass ``` Provide the implementations for the above functions and visualize the sample data provided.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def convert_to_wide_form(data: pd.DataFrame) -> pd.DataFrame: Converts long-form data to wide-form where each city\'s sales are in separate columns. return data.pivot(index=\'year\', columns=\'city\', values=\'sales\') def convert_to_long_form(data: pd.DataFrame) -> pd.DataFrame: Converts wide-form data back to long-form. return data.reset_index().melt(id_vars=\'year\', var_name=\'city\', value_name=\'sales\') def plot_total_sales_per_year(data: pd.DataFrame): Plots a line plot for total sales per year for each city using long-form data. sns.lineplot(data=data, x=\'year\', y=\'sales\', hue=\'city\') plt.title(\'Total Sales Per Year for Each City\') plt.xlabel(\'Year\') plt.ylabel(\'Sales\') plt.legend(title=\'City\') plt.show() def plot_average_sales_per_category(data: pd.DataFrame, original_data: pd.DataFrame): Plots a bar plot for average sales per category using long-form data. avg_sales_category = original_data.groupby(\'category\')[\'sales\'].mean().reset_index() sns.barplot(data=avg_sales_category, x=\'category\', y=\'sales\') plt.title(\'Average Sales Per Category\') plt.xlabel(\'Category\') plt.ylabel(\'Average Sales\') plt.show()"},{"question":"# Design a Custom Callable Class with Vectorcall Support in Python In this exercise, you are required to demonstrate your understanding of callable objects in Python by implementing a custom class that supports both `tp_call` and vectorcall protocols. The goal is to create a callable class that can sum a list of numbers passed either as positional arguments or keyword arguments using the vectorcall protocol for efficiency. Requirements: 1. Implement a class `SumNumbers` that: - Can be called with a varying number of positional arguments. - Can be called with keyword arguments. - Uses the vectorcall protocol efficiently for handling calls. 2. Adhere to the following structure and logic: - The class should have a `__call__` method that sums the numbers provided as arguments. - The class should override the `tp_call` behavior to sum the numbers. - Implement vectorcall support and ensure it behaves consistently with `tp_call`. 3. Provide a function `sum_numbers` which: - Accepts an instance of `SumNumbers` and a list of numbers. - Calls the instance using the vectorcall protocol. - Returns the sum of the numbers. You are given the following template to start with: ```python class SumNumbers: def __call__(self, *args, **kwargs): total = sum(args) + sum(kwargs.values()) return total # import necessary modules for vectorcall and tp_call support # Implement the necessary logic to support vectorcall here def sum_numbers(callable_instance, numbers): # Implement the function to use vectorcall protocol to call # the instance and return the sum pass # Example usage: # sum_instance = SumNumbers() # result = sum_numbers(sum_instance, [1, 2, 3, 4]) # print(result) # Output should be 10 ``` Constraints: - You may use the CPython C API functions provided in the documentation where necessary. - Ensure that your solution is efficient and follows the proper calling conventions. Input: - `sum_numbers` function will receive an instance of `SumNumbers` and a list of numbers. Output: - The function should return the sum of the numbers provided. Good luck and happy coding!","solution":"class SumNumbers: def __call__(self, *args, **kwargs): total = sum(args) + sum(kwargs.values()) return total def sum_numbers(callable_instance, numbers): # Since Python\'s vectorcall is a CPython-specific optimization not directly accessible in pure Python, # we will simulate it by directly calling the callable instance with unpacked arguments. return callable_instance(*numbers) # Example usage: # sum_instance = SumNumbers() # result = sum_numbers(sum_instance, [1, 2, 3, 4]) # print(result) # Output should be 10"},{"question":"<|Analysis Begin|> The provided documentation describes the Event Loop in the `asyncio` library, focusing on various methods for managing asynchronous events, handling sockets, creating tasks, scheduling callbacks, and working with network connections and subprocesses. It covers core aspects of asyncio such as running and stopping the loop, scheduling tasks, and creating servers and clients using asynchronous patterns. Key functionalities include: 1. **Obtaining the Event Loop**: Methods to get the running event loop or set a new event loop. 2. **Event Loop Methods**: APIs for running/stopping the loop, scheduling callbacks, creating tasks, networking, etc. 3. **Handling callbacks**: Methods for scheduling immediate or delayed callbacks. 4. **Socket operations**: Functions to directly handle socket operations asynchronously. 5. **Running subprocesses**: Methods to manage subprocesses asynchronously. 6. **Network server/client creation**: APIs for creating TCP/UDP servers and clients. 7. **Managing Futures and Tasks**: APIs for creating and managing asynchronous tasks. 8. **Error Handling**: Customizing exception handling in the event loop. Given this rich set of functionalities, the coding assessment can be designed to test comprehension and practical usage of asyncio\'s event loop, scheduling tasks, and managing network operations asynchronously. A suitable question should challenge students to combine these capabilities into a cohesive solution. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Design an asynchronous server that handles multiple clients exchanging messages, demonstrating your understanding of asyncio\'s event loop, task scheduling, and network operations. Problem Statement: Create an asynchronous TCP server using `asyncio` that listens on a specific port for incoming client connections. The server should: 1. Accept multiple client connections simultaneously. 2. Read messages from each connected client. 3. Broadcast each received message to all connected clients except the sender. 4. Handle client disconnections gracefully. Requirements: 1. Implement the server using asyncio functions and event loop methods. 2. Ensure that the server can handle at least 5 concurrent clients. 3. Use appropriate error handling within the server. 4. The server should run until manually interrupted (e.g., via a signal or keyboard interrupt). Input: - No direct input to the function, but your server will read messages (strings) from connected clients. Output: - The server should send the received message to all connected clients except the sender. Constraints: - Messages are strings with a maximum length of 256 characters. - You may assume well-formed input (no need to handle malformed messages). Example: ```python import asyncio class EchoServerClientProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport self.server.all_clients.append(self) def data_received(self, data): message = data.decode() self.broadcast(message) def broadcast(self, message): for client in self.server.all_clients: if client != self: client.transport.write(f\\"{message}\\".encode()) def connection_lost(self, exc): self.server.all_clients.remove(self) self.transport.close() async def main(): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: EchoServerClientProtocol(server), \'127.0.0.1\', 8888) server.all_clients = [] async with server: await server.serve_forever() try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server is shutting down.\\") ``` Instructions: 1. Implement and test the provided function `EchoServerClientProtocol` and the server setup within the `main` function. 2. Ensure that the server can handle broadcasting messages to all connected clients. 3. Test your solution with multiple clients to confirm it meets the requirements.","solution":"import asyncio class EchoServerClientProtocol(asyncio.Protocol): def __init__(self, server): self.server = server self.transport = None def connection_made(self, transport): self.transport = transport self.server.all_clients.append(self) print(f\\"New connection from {transport.get_extra_info(\'peername\')}\\") def data_received(self, data): message = data.decode() print(f\\"Received: {message} from {self.transport.get_extra_info(\'peername\')}\\") self.broadcast(message) def broadcast(self, message): for client in self.server.all_clients: if client != self: client.transport.write(f\\"{message}\\".encode()) def connection_lost(self, exc): print(f\\"Lost connection from {self.transport.get_extra_info(\'peername\')}\\") self.server.all_clients.remove(self) self.transport.close() async def main(): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: EchoServerClientProtocol(server), \'127.0.0.1\', 8888) server.all_clients = [] print(f\\"Serving on {server.sockets[0].getsockname()} ...\\") async with server: await server.serve_forever() if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server is shutting down.\\")"},{"question":"Debugging and Profiling in Python Objective Your task is to write Python code that: 1. Implements a given mathematical function. 2. Uses debugging and profiling tools to identify and optimize performance bottlenecks. 3. Demonstrates the use of profiling to measure and improve the efficiency of your implementation. Problem Statement 1. **Function Implementation**: Implement a function `calculate_exponential_sum(n: int) -> int` that computes the sum of `i^i` for all `i` from `1` to `n` (inclusive). ```python def calculate_exponential_sum(n: int) -> int: pass ``` 2. **Performance Profiling**: Use the `cProfile` module to profile the performance of your function. 3. **Optimization**: Analyze the profiling results to identify any performance bottlenecks and optimize the function. Re-profile the optimized function to demonstrate performance improvements. 4. **Debugging**: Use the `pdb` (Python Debugger) to debug your function implementation by setting breakpoints and inspecting variable values. Document the debugging steps and findings. Input Format - An integer `n` (1 ≤ n ≤ 100,000). Output Format - Return the computed sum from the `calculate_exponential_sum` function. Constraints - Ensure that the implementation is optimized and can handle the maximum constraint efficiently. - Document the profiling and debugging process in a separate `README.md` file. # Example ```python print(calculate_exponential_sum(5)) # Output: 3413 (1^1 + 2^2 + 3^3 + 4^4 + 5^5) ``` # Additional Requirements 1. **Profiling Report**: - Include the profiling report before and after optimization in the `README.md`. 2. **Debugging Documentation**: - Describe the steps taken during debugging and how `pdb` was used, also in `README.md`. Submission - Implement the solution in a file named `solution.py`. - Include a `README.md` with detailed profiling and debugging documentation.","solution":"def calculate_exponential_sum(n: int) -> int: Calculates the sum of i^i for all i from 1 to n (inclusive). return sum(i ** i for i in range(1, n + 1))"},{"question":"# PyTorch JIT Compilation **Objective:** Write a function to optimize a given PyTorch neural network model using TorchScript. This function should take a trained PyTorch model, convert it to TorchScript via tracing, and return the optimized model. **Function Signature:** ```python def optimize_model(model: torch.nn.Module, example_input: torch.Tensor) -> torch.jit.ScriptModule: pass ``` **Input:** - `model`: An instance of a trained PyTorch `torch.nn.Module` representing the neural network. - `example_input`: A sample `torch.Tensor` input that matches the input expectations of the model, used for tracing the model. **Output:** - Returns a `torch.jit.ScriptModule` which is the optimized version of the input model. **Constraints and Requirements:** - You should use the JIT tracing method to convert the PyTorch model to TorchScript. - Handle any necessary preparation or conversion steps needed before tracing. - Ensure that the returned `ScriptModule` maintains the same behavior and performance as the original model. **Example:** ```python import torch import torch.nn as nn # Example model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Instantiate the model and provide example input model = SimpleNet() example_input = torch.randn(1, 10) # Expected usage of the function optimized_model = optimize_model(model, example_input) # Check if the optimized model produces the same output original_output = model(example_input) optimized_output = optimized_model(example_input) assert torch.allclose(original_output, optimized_output, atol=1e-6), \\"The outputs are not the same\\" ``` In the provided `optimize_model` function, students will demonstrate their understanding of how to utilize PyTorch’s JIT compilation feature to optimize model performance for inference.","solution":"import torch def optimize_model(model: torch.nn.Module, example_input: torch.Tensor) -> torch.jit.ScriptModule: Optimize a given PyTorch neural network model using TorchScript. Parameters: - model: An instance of a trained PyTorch `torch.nn.Module` representing the neural network. - example_input: A sample `torch.Tensor` input that matches the input expectations of the model. Returns: - A `torch.jit.ScriptModule` which is the optimized version of the input model. model.eval() # Make sure the model is in evaluation mode traced_script_module = torch.jit.trace(model, example_input) return traced_script_module"},{"question":"**Problem Statement:** Implement a custom `asyncio` server and client application using the low-level transport and protocol APIs. Your server will act as a simple key-value store, and your client will interact with this server to set and get values. # Server Requirements: 1. The server should accept connections and handle requests using a protocol class `KeyValueStoreProtocol`. 2. This protocol will implement the following commands: - `SET key value`: Store the `value` associated with `key`. - `GET key`: Retrieve the value associated with `key`. - `DELETE key`: Delete the entry associated with `key`. 3. The server should start and run indefinitely until manually stopped. # Client Requirements: 1. The client should be able to connect to the server using a protocol class `KeyValueClientProtocol`. 2. The client should support sending the following commands to the server: - `SET key value` - `GET key` - `DELETE key` 3. The client should display the response received from the server. # Implementation Details: - Create two files: `server.py` and `client.py`. - Ensure that your server can handle multiple clients concurrently. - Use asynchronous programming with `asyncio`. - Handle edge cases such as missing keys in the `GET` and `DELETE` commands appropriately. # Input/Output Format: **Server Responses:** - `SET key value` should respond with `OK` if the operation is successful. - `GET key` should respond with the value if the key is found, or `NOT FOUND` if the key is missing. - `DELETE key` should respond with `DELETED` if the key was deleted, or `NOT FOUND` if the key is not found. **Example:** ``` # Client sets a key-value pair SET mykey somevalue Response: OK # Client retrieves the value GET mykey Response: somevalue # Client deletes the value DELETE mykey Response: DELETED # Client tries to get a non-existing key GET mykey Response: NOT FOUND ``` Write the implementation for both `server.py` and `client.py` following these requirements. # Constraints: - You should use Python 3.7 or higher. - Avoid using third-party libraries; only use the standard library. Good luck!","solution":"# server.py import asyncio class KeyValueStoreProtocol(asyncio.Protocol): def __init__(self): self.store = {} def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() command, *params = message.split() response = self.handle_command(command, params) self.transport.write(response.encode()) def handle_command(self, command, params): if command == \\"SET\\" and len(params) == 2: key, value = params self.store[key] = value return \\"OK\\" elif command == \\"GET\\" and len(params) == 1: key = params[0] return self.store.get(key, \\"NOT FOUND\\") elif command == \\"DELETE\\" and len(params) == 1: key = params[0] if key in self.store: del self.store[key] return \\"DELETED\\" else: return \\"NOT FOUND\\" else: return \\"ERROR\\" async def start_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: KeyValueStoreProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(start_server()) # client.py import asyncio class KeyValueClientProtocol(asyncio.Protocol): def __init__(self, message, loop): self.message = message self.loop = loop self.response = \\"\\" def connection_made(self, transport): self.transport = transport self.transport.write(self.message.encode()) def data_received(self, data): self.response += data.decode() def connection_lost(self, exc): self.loop.stop() async def send_command(message): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() transport, protocol = await loop.create_connection( lambda: KeyValueClientProtocol(message, loop), \'127.0.0.1\', 8888 ) try: await on_con_lost finally: transport.close() return protocol.response if __name__ == \\"__main__\\": import sys if len(sys.argv > 1): message = \\" \\".join(sys.argv[1:]) asyncio.run(send_command(message))"},{"question":"**Question: Customizing Plot Styles using Seaborn** You are given a dataset representing the petrol consumption data across various cities and the number of petrol pumps in those cities. Your task is to visualize this data using different styles provided by the `seaborn` library and write code to demonstrate your customizations. # Dataset Assume the data is given as a list of dictionaries: ```python data = [ {\\"city\\": \\"New York\\", \\"petrol_pumps\\": 50, \\"consumption\\": 350}, {\\"city\\": \\"Los Angeles\\", \\"petrol_pumps\\": 40, \\"consumption\\": 300}, {\\"city\\": \\"Chicago\\", \\"petrol_pumps\\": 30, \\"consumption\\": 250}, {\\"city\\": \\"Houston\\", \\"petrol_pumps\\": 35, \\"consumption\\": 270}, {\\"city\\": \\"Phoenix\\", \\"petrol_pumps\\": 20, \\"consumption\\": 180}, ] ``` # Requirements 1. Write a function `visualize_petrol_data(data, style)` that takes the dataset and a style as input. 2. The function should plot a bar chart of the number of petrol pumps in each city. 3. The function should allow the user to specify the plot style using the `sns.axes_style()` function. 4. The function should demonstrate at least three different plot styles (e.g., \\"darkgrid\\", \\"white\\", \\"ticks\\") by using a context manager. # Input - `data`: A list of dictionaries, each containing \'city\', \'petrol_pumps\', and \'consumption\'. - `style`: A string specifying one of the seaborn styles (e.g., \\"darkgrid\\", \\"white\\", \\"ticks\\"). # Output - A bar plot showing the number of petrol pumps per city using the specified style. # Constraints - Ensure the style is applied correctly using the context manager. - Handle the cases where the input style might not be valid by raising a ValueError with an appropriate message. # Performance Requirements - The solution should be efficient and should visualize the plot correctly within a reasonable time for a normal-sized dataset. # Example Function Call ```python data = [ {\\"city\\": \\"New York\\", \\"petrol_pumps\\": 50, \\"consumption\\": 350}, {\\"city\\": \\"Los Angeles\\", \\"petrol_pumps\\": 40, \\"consumption\\": 300}, {\\"city\\": \\"Chicago\\", \\"petrol_pumps\\": 30, \\"consumption\\": 250}, {\\"city\\": \\"Houston\\", \\"petrol_pumps\\": 35, \\"consumption\\": 270}, {\\"city\\": \\"Phoenix\\", \\"petrol_pumps\\": 20, \\"consumption\\": 180}, ] # Visualize with the \\"darkgrid\\" style visualize_petrol_data(data, \\"darkgrid\\") ``` In this prompt, we have provided a detailed description, datasets, constraints, and an example function call to make it self-contained and clear. The question assesses the student\'s ability to use seaborn for styling plots, manage the styles using context managers, and handle invalid styles gracefully.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_petrol_data(data, style): Visualizes petrol pump data with specified seaborn style. Parameters: data (list of dict): The dataset containing petrol consumption data. style (str): The seaborn style to apply to the plot. valid_styles = [\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", \\"ticks\\"] if style not in valid_styles: raise ValueError(f\\"Invalid style. Choose from the list: {valid_styles}\\") cities = [entry[\\"city\\"] for entry in data] petrol_pumps = [entry[\\"petrol_pumps\\"] for entry in data] with sns.axes_style(style): plt.figure(figsize=(10, 5)) sns.barplot(x=cities, y=petrol_pumps) plt.title(\\"Number of Petrol Pumps per City\\") plt.xlabel(\\"City\\") plt.ylabel(\\"Number of Petrol Pumps\\") plt.show()"},{"question":"# PyTorch XPU Device, RNG, and Memory Management Problem Statement In this task, you will write a PyTorch function to perform matrix multiplication on an XPU device, ensuring reproducibility with a fixed random seed and efficiently managing device memory. You are required to implement a function that sets up the device, initializes matrices with random values, performs matrix multiplication on the XPU, and then cleans up the allocated resources. Function Signature ```python def xpu_matrix_multiplication(seed: int, size: int) -> torch.Tensor: Performs matrix multiplication on the XPU with controlled random seed and memory management. Parameters: - seed (int): The random seed value to ensure reproducibility. - size (int): The size of the square matrices to be multiplied. Returns: - torch.Tensor: The product matrix resulting from the multiplication. ``` Input - `seed` (int): A random seed for reproducibility. - `size` (int): The size of the NxN matrices to be multiplied. Output - `torch.Tensor`: The resulting matrix from the multiplication of two randomly initialized NxN matrices on the XPU device. Constraints 1. The function must run on an XPU device if available. 2. The random seed must be set for reproducibility. 3. Device memory must be efficiently managed before and after the operation: - Clear cache before starting computation. - Reset memory stats before and after the operation. 4. Handle any necessary synchronization for streams, if required. Example ```python result_matrix = xpu_matrix_multiplication(seed=42, size=1000) print(result_matrix) ``` This example sets the seed to 42 for reproducibility, multiplies two random 1000x1000 matrices on an XPU device, and returns the resulting product matrix. Additional Information You may find the following `torch.xpu` functions helpful: - `is_available()`: Check if XPU is available. - `set_device()`: Set the device for computation. - `manual_seed()`: Set the random seed for reproducibility. - `empty_cache()`: Clear the cache to free memory. - `reset_peak_memory_stats()`: Reset memory usage statistics. - `memory_allocated()`: Get the current memory allocated on the device. - `synchronize()`: Synchronize streams to ensure all operations are complete. Ensure to handle any potential exceptions that might occur due to device unavailability or memory issues.","solution":"import torch def xpu_matrix_multiplication(seed: int, size: int) -> torch.Tensor: Performs matrix multiplication on the XPU with controlled random seed and memory management. Parameters: - seed (int): The random seed value to ensure reproducibility. - size (int): The size of the square matrices to be multiplied. Returns: - torch.Tensor: The product matrix resulting from the multiplication. # Check if XPU is available, otherwise fallback to CPU device = torch.device(\\"xpu\\" if torch.xpu.is_available() else \\"cpu\\") # Set the random seed for reproducibility torch.manual_seed(seed) # Clear cache and reset memory stats to ensure efficient memory management if device.type == \'xpu\': torch.xpu.empty_cache() torch.xpu.reset_peak_memory_stats() # Initialize random matrices on the specified device matrix1 = torch.randn(size, size, device=device) matrix2 = torch.randn(size, size, device=device) # Perform matrix multiplication result_matrix = torch.matmul(matrix1, matrix2) # Synchronize to ensure all operations are completed if device.type == \'xpu\': torch.xpu.synchronize() # Clean up the cache and reset memory stats after the operation if device.type == \'xpu\': torch.xpu.empty_cache() torch.xpu.reset_peak_memory_stats() return result_matrix"},{"question":"# Python Coding Assessment: Handling Secure Password Entry and User Identification Objective: Implement a Python function that securely prompts the user to enter a password and retrieves their login name. Your implementation should handle scenarios where password input might be echoed, using appropriate exception handling. Function Signature: ```python def secure_login(prompt: str = \'Password: \'): Securely prompts the user for a password and retrieves their login name. Args: - prompt (str): The prompt message shown to the user. Defaults to \'Password: \'. Returns: - Tuple[str, str]: A tuple containing the user\'s login name and the entered password. Raises: - Exception: If obtaining the user’s login name fails or if there is another unexpected error. pass ``` Requirements: 1. **Prompt the User for a Password:** - Use `getpass.getpass()` to prompt the user for a password without echoing. - Display the custom prompt message passed to the function. 2. **Retrieve the User\'s Login Name:** - Use `getpass.getuser()` to get the login name of the user. - If retrieving the login name fails, raise an appropriate exception. 3. **Handle `GetPassWarning`:** - If password input might be echoed, catch the `getpass.GetPassWarning` and print a warning message to the user before proceeding. 4. **Return the Login Name and Password:** - Return the login name and entered password as a tuple. Constraints: - The function should not rely on any external libraries other than the standard library. - Ensure robust handling of exceptions and edge cases. Examples: ```python # Example 1 # Assuming the environment variable \\"USER\\" is set to \\"john_doe\\" output = secure_login(\'Enter your secret password: \') print(output) # Output: (\'john_doe\', \'entered_password\') # Example 2 # If the login name cannot be determined from environment variables or system call, # The function should raise an exception. ``` Use the provided documentation to understand the behavior of `getpass.getpass()` and `getpass.getuser()`, and ensure your implementation adheres to the described functionality.","solution":"import getpass import warnings def secure_login(prompt: str = \'Password: \'): Securely prompts the user for a password and retrieves their login name. Args: - prompt (str): The prompt message shown to the user. Defaults to \'Password: \'. Returns: - Tuple[str, str]: A tuple containing the user\'s login name and the entered password. Raises: - Exception: If obtaining the user’s login name fails or if there is another unexpected error. try: password = getpass.getpass(prompt) except getpass.GetPassWarning: warnings.warn(\\"Warning: Password input may be visible.\\") password = input(prompt) # Fallback to input if getpass fails try: user_login = getpass.getuser() except Exception as e: raise Exception(\\"Failed to retrieve the user’s login name.\\") from e return (user_login, password)"},{"question":"Objective: Assess the students\' understanding of supervised learning and their ability to apply scikit-learn modules to solve a real-world regression problem. Problem Statement: You are given a dataset containing information about houses, including their features and prices. Your task is to build a regression model using scikit-learn to predict the prices of the houses. You will: 1. Preprocess the data by handling missing values, encoding categorical features, and scaling the features. 2. Implement multiple regression models, including Linear Regression, Random Forest Regression, and Support Vector Regression (SVR). 3. Evaluate the performance of the models using appropriate metrics. 4. Select the best model based on the evaluation metrics and justify your choice. Dataset: The dataset `houses.csv` contains the following columns: - `LotArea`: area of the lot (in square feet) - `YearBuilt`: year the house was built - `TotalBsmtSF`: total area of the basement (in square feet) - `GrLivArea`: above ground living area (in square feet) - `FullBath`: number of full bathrooms - `BedroomAbvGr`: number of bedrooms above ground - `TotRmsAbvGrd`: total number of rooms above ground - `SalePrice`: sale price of the house (target variable) - `Neighborhood`: categorical feature representing the neighborhood - `HouseStyle`: categorical feature representing the style of the house Input Format: - A CSV file named `houses.csv`. Output Format: - The trained regression model. - A dictionary containing the RMSE (Root Mean Squared Error) and R2 score for each model. - The name of the best model and the corresponding metrics. Constraints: - Handle missing values appropriately. - Encode categorical features using One-Hot Encoding. - Standardize the features before training the models. - Use a random seed of 42 for reproducibility. Example: **Sample Input:** ``` houses.csv: +----------+----------+-------------+-----------+---------+--------------+---------------+-----------+------------+-------------+ | LotArea | YearBuilt| TotalBsmtSF | GrLivArea | FullBath| BedroomAbvGr | TotRmsAbvGrd | SalePrice | Neighborhood| HouseStyle | +----------+----------+-------------+-----------+---------+--------------+---------------+-----------+------------+-------------+ | 8450 | 2003 | 856 | 1710 | 2 | 3 | 8 | 208500 | CollgCr | 2Story | | 9600 | 1976 | 1262 | 1262 | 2 | 3 | 6 | 181500 | Veenker | 1Story | | 11250 | 2001 | 920 | 1786 | 2 | 3 | 7 | 223500 | CollgCr | 2Story | ... (more rows) +----------+----------+-------------+-----------+---------+--------------+---------------+-----------+------------+-------------+ ``` **Sample Output:** ```python { \\"Linear Regression\\": {\\"RMSE\\": 32000, \\"R2\\": 0.75}, \\"Random Forest Regression\\": {\\"RMSE\\": 27000, \\"R2\\": 0.85}, \\"Support Vector Regression\\": {\\"RMSE\\": 30000, \\"R2\\": 0.80}, \\"Best Model\\": \\"Random Forest Regression\\", \\"Best Model Metrics\\": {\\"RMSE\\": 27000, \\"R2\\": 0.85} } ``` Implementation: You should follow these steps to complete the assignment: 1. Load the dataset from the CSV file. 2. Handle any missing values using an appropriate strategy. 3. Encode categorical features using One-Hot Encoding. 4. Split the dataset into training and testing sets. 5. Standardize the features. 6. Implement the following regression models: - Linear Regression - Random Forest Regression - Support Vector Regression (SVR) 7. Evaluate each model using RMSE and R2 score on the test set. 8. Select the best model based on the evaluation metrics and print the results. Note: - You are expected to write clean, efficient, and well-documented code. - Include any assumptions or decisions made during the implementation in the comments. Good luck!","solution":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor from sklearn.svm import SVR from sklearn.metrics import mean_squared_error, r2_score def load_and_preprocess_data(file_path): # Load dataset df = pd.read_csv(file_path) # Define features and target X = df.drop(\'SalePrice\', axis=1) y = df[\'SalePrice\'] # Identify numerical and categorical columns numerical_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X.select_dtypes(include=[\'object\']).columns # Create preprocessing pipelines numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) X_processed = preprocessor.fit_transform(X) return train_test_split(X_processed, y, test_size=0.2, random_state=42) def build_and_evaluate_models(X_train, X_test, y_train, y_test): models = { \'Linear Regression\': LinearRegression(), \'Random Forest Regression\': RandomForestRegressor(random_state=42), \'Support Vector Regression\': SVR() } results = {} for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) rmse = np.sqrt(mean_squared_error(y_test, y_pred)) r2 = r2_score(y_test, y_pred) results[name] = {\'RMSE\': rmse, \'R2\': r2} return results def select_best_model(results): best_model = min(results, key=lambda k: results[k][\'RMSE\']) return best_model, results[best_model] def main(file_path): X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path) results = build_and_evaluate_models(X_train, X_test, y_train, y_test) best_model, best_model_metrics = select_best_model(results) results[\'Best Model\'] = best_model results[\'Best Model Metrics\'] = best_model_metrics return results # Example usage: # file_path = \'houses.csv\' # print(main(file_path))"},{"question":"# Pandas Windowing Operations: Advanced Data Processing Task You are provided with a sales dataset that contains time-series data of sales transactions from different stores. The goal is to analyze sales trends using advanced windowing operations provided by pandas. Dataset The dataset contains the following columns: - `date`: Date of the transaction (String in `YYYY-MM-DD` format) - `store_id`: Unique identifier for the store (Integer) - `sales`: Number of units sold (Integer) - `revenue`: Revenue generated from the sales (Float) Task 1. **Calculate Store-wise Rolling Sales Trend**: - For each store, calculate the 7-day rolling sum of `sales` and `revenue`. - Save the result in a new DataFrame with columns: `date`, `store_id`, `rolling_sales`, and `rolling_revenue`. 2. **Calculate Weighted Moving Average for Sales**: - Implement a weighted moving average for sales using a Gaussian function (window=7, std=2). - Save the result in a new column named `weighted_sales`. 3. **Custom Rolling Window with Expanding Conditions**: - Define a custom rolling window where for certain conditions of date `2020-12-25` and `2021-01-01`, compute the expanding window sum else use a fixed window of size 3. - Save the result in columns: `date`, `store_id`, `custom_sales_aggregate`, `custom_revenue_aggregate`. 4. **Groupby & Expanding Calculations**: - Group the data by `store_id`, and compute the expanding mean of `sales` and `revenue`. - Save the result in columns: `date`, `store_id`, `expanding_sales_mean`, and `expanding_revenue_mean`. Input - A pandas DataFrame named `df` containing the sales dataset with columns: `date`, `store_id`, `sales`, and `revenue`. - The `date` column is already in datetime format. Output - A dictionary with four keys `rolling_trend`, `weighted_average`, `custom_window`, `expanding_stats` each containing the resultant DataFrame for the respective tasks. Constraints - Ensure that columns for date and store identifiers remain in the resulting DataFrames. - For the custom rolling window, all conditions specified must be efficiently handled with indexing operations. - Use pandas\' windowing functions, and leverage any performance enhancements wherever suitable. Example Usage ```python output = data_processing_pipeline(df) rolling_trend_df = output[\'rolling_trend\'] weighted_average_df = output[\'weighted_average\'] custom_window_df = output[\'custom_window\'] expanding_stats_df = output[\'expanding_stats\'] ``` Function Signature ```python def data_processing_pipeline(df: pd.DataFrame) -> dict: pass ``` This task requires comprehensive usage of pandas windowing operations, custom data manipulations, and applying advanced rolling calculations.","solution":"import pandas as pd def data_processing_pipeline(df: pd.DataFrame) -> dict: # Calculate Store-wise Rolling Sales Trend df = df.sort_values(by=[\'store_id\', \'date\']) df[\'rolling_sales\'] = df.groupby(\'store_id\')[\'sales\'].transform(lambda x: x.rolling(window=7, min_periods=1).sum()) df[\'rolling_revenue\'] = df.groupby(\'store_id\')[\'revenue\'].transform(lambda x: x.rolling(window=7, min_periods=1).sum()) rolling_trend = df[[\'date\', \'store_id\', \'rolling_sales\', \'rolling_revenue\']] # Calculate Weighted Moving Average for Sales df[\'weighted_sales\'] = df.groupby(\'store_id\')[\'sales\'].transform( lambda x: x.rolling(window=7, win_type=\'gaussian\', min_periods=1).mean(std=2) ) weighted_average = df[[\'date\', \'store_id\', \'weighted_sales\']] # Custom Rolling Window with Expanding Conditions df[\'custom_sales_aggregate\'] = df.groupby(\'store_id\')[\'sales\'].transform( lambda x: x.rolling(window=3, min_periods=1).sum()) df[\'custom_revenue_aggregate\'] = df.groupby(\'store_id\')[\'revenue\'].transform( lambda x: x.rolling(window=3, min_periods=1).sum()) df[\'custom_sales_aggregate\'] = df.apply( lambda row: df[(df[\'store_id\'] == row[\'store_id\']) & ( (df[\'date\'] <= row[\'date\']) | (row[\'date\'] == pd.Timestamp(\\"2020-12-25\\")) | (row[\'date\'] == pd.Timestamp(\\"2021-01-01\\")) )][\'sales\'].sum(), axis=1 ) df[\'custom_revenue_aggregate\'] = df.apply( lambda row: df[(df[\'store_id\'] == row[\'store_id\']) & ( (df[\'date\'] <= row[\'date\']) | (row[\'date\'] == pd.Timestamp(\\"2020-12-25\\")) | (row[\'date\'] == pd.Timestamp(\\"2021-01-01\\")) )][\'revenue\'].sum(), axis=1 ) custom_window = df[[\'date\', \'store_id\', \'custom_sales_aggregate\', \'custom_revenue_aggregate\']] # Groupby & Expanding Calculations df[\'expanding_sales_mean\'] = df.groupby(\'store_id\')[\'sales\'].transform(lambda x: x.expanding().mean()) df[\'expanding_revenue_mean\'] = df.groupby(\'store_id\')[\'revenue\'].transform(lambda x: x.expanding().mean()) expanding_stats = df[[\'date\', \'store_id\', \'expanding_sales_mean\', \'expanding_revenue_mean\']] return { \\"rolling_trend\\": rolling_trend, \\"weighted_average\\": weighted_average, \\"custom_window\\": custom_window, \\"expanding_stats\\": expanding_stats }"},{"question":"Objective: Your task is to write a Python function using seaborn that loads a dataset, generates a customized diverging color palette, and creates a heatmap to visualize the data using this palette. Requirements: 1. **Function Implementation:** - Implement a function named `custom_heatmap`. - The function should take the following parameters: ```python def custom_heatmap(dataset: str, start_color: int, end_color: int, center_color: str=\'light\', saturation: int=100, lightness: int=50, sep: int=1, cmap: bool=False) -> None: ``` - `dataset` (str): the name of the dataset to load from seaborn\'s built-in datasets (e.g., \'flights\', \'tips\'). - `start_color` (int): the hue value for the color at one end of the palette. - `end_color` (int): the hue value for the color at the other end of the palette. - `center_color` (str): the color at the center of the palette, either \'light\' or \'dark\'. Default is \'light\'. - `saturation` (int): saturation level of the palette colors (0-100). Default is 100. - `lightness` (int): lightness level of the palette colors (0-100). Default is 50. - `sep` (int): the amount of separation around the center value. Default is 1. - `cmap` (bool): whether to return a continuous colormap instead of a discrete palette. Default is False. 2. **Function Output:** - Load the specified dataset using `sns.load_dataset`. - Generate a diverging palette using the supplied parameters. - Create and display a heatmap of the dataset using seaborn\'s `heatmap` function and the generated palette. Constraints: - The dataset should be a seaborn built-in dataset that is suitable for heatmap visualization (e.g., \'flights\' for a month-to-year grid). - Ensure to handle datasets that might not be suitable for heatmap visualization and output an appropriate message. - Ensure function parameters are validated and handle improper values gracefully. Example Usage: ```python # Example usage custom_heatmap(\'flights\', 240, 20, center_color=\'dark\', saturation=50, lightness=70, sep=30, cmap=True) ``` # Expected Result: The function should output a heatmap visual where the colors transition according to the specified diverging palette parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_heatmap(dataset: str, start_color: int, end_color: int, center_color: str=\'light\', saturation: int=100, lightness: int=50, sep: int=1, cmap: bool=False) -> None: Generates a heatmap visualizing the provided dataset using a customized diverging color palette. # Load the dataset try: df = sns.load_dataset(dataset) except: print(f\\"Dataset \'{dataset}\' not found in seaborn\'s datasets.\\") return # Check if the dataset is suitable for a heatmap if not df.select_dtypes(include=[\'number\']).empty: print(f\\"Dataset \'{dataset}\' contains non-numeric data unsuitable for heatmap.\\") return # Generate diverging palette palette = sns.diverging_palette(start_color, end_color, center=center_color, s=saturation, l=lightness, sep=sep, as_cmap=cmap) # Create and display heatmap sns.heatmap(df.corr(), cmap=palette, annot=True, center=0) plt.show()"},{"question":"Extended Completion Functionality with `rlcompleter` You are tasked with extending the functionality of Python\'s `rlcompleter` module to support additional custom completion patterns. Specifically, you need to implement a new method in the `Completer` class that can provide completions for Python dictionary keys when given a variable that refers to a dictionary. # Requirements 1. **Class Method Implementation**: - Implement a method `complete_dict_key(self, text, state)` in the `Completer` class. - This method should provide completions for dictionary keys when the input `text` is a variable name that refers to a dictionary followed by a `.`. - For example, if `my_dict` is a dictionary with keys `\'apple\'`, `\'banana\'`, and `\'cherry\'`, and the user types `my_dict.`, the available completions should be these keys. 2. **Input and Output**: - `text`: A string representing the current text to be completed. - `state`: An integer representing the current completion attempt (0 for the first, 1 for the second, and so on). 3. **Constraints**: - The solution should handle cases where `text` does not refer to a dictionary gracefully. - Ignore any side effects or modifications to the dictionary during completion. - Raise appropriate exceptions and handle them within the method if necessary. 4. **Performance Requirements**: - The method should perform efficiently for dictionaries with a large number of keys (up to 10,000 keys). # Example Usage ```python import rlcompleter class ExtendedCompleter(rlcompleter.Completer): def complete_dict_key(self, text, state): # Your implementation here pass # Example Usage: ext_completer = ExtendedCompleter() # Text to complete, assume `my_dict` is {\'apple\': 1, \'banana\': 2, \'cherry\': 3} completions = [ext_completer.complete_dict_key(\'my_dict.\', i) for i in range(3)] print(completions) # Expected output: [\'my_dict.apple\', \'my_dict.banana\', \'my_dict.cherry\'] ``` # Instructions 1. Complete the implementation of the `complete_dict_key` method. 2. Ensure your code handles typical edge cases, such as non-dictionary objects and empty dictionaries. 3. Write unit tests to verify the correctness of your implementation.","solution":"import rlcompleter class ExtendedCompleter(rlcompleter.Completer): def complete_dict_key(self, text, state): try: var_name, _ = text.split(\'.\') dictionary = eval(var_name, self.namespace) if isinstance(dictionary, dict): keys = list(dictionary.keys()) if state < len(keys): return f\\"{var_name}.{keys[state]}\\" except (ValueError, AttributeError, NameError, SyntaxError): return None return None # Example usage to test manually: # my_dict = {\'apple\': 1, \'banana\': 2, \'cherry\': 3} # ext_completer = ExtendedCompleter() # ext_completer.namespace = locals() # completions = [ext_completer.complete_dict_key(\'my_dict.\', i) for i in range(3)] # print(completions) # Expected output: [\'my_dict.apple\', \'my_dict.banana\', \'my_dict.cherry\']"},{"question":"As part of improving error handling in your Python application, you need to manage different types of errors gracefully. Your task is to implement a function that processes a list of operations and appropriately handles any errors that might occur using built-in Python exceptions. Additionally, you will create a custom exception to handle specific error cases. Requirements: 1. Implement a function `process_operations(operations: list) -> list`. 2. Each item in `operations` is a tuple with an operation followed by its parameters, e.g., `(\\"divide\\", 4, 2)` or `(\\"concat\\", \\"hello\\", 3)`. 3. Supported operations: - `\\"divide\\"` which divides the first number by the second number. - `\\"concat\\"` which concatenates a string with a non-string element, causing a `TypeError`. - `\\"access\\"` which accesses an element at a specific index from a predefined list `sample_list = [1, 2, 3]`, potentially causing an `IndexError`. - `\\"cause_custom\\"` which will raise a custom exception `CustomError` with a specified message. 4. For each operation, handle the following exceptions: - `ZeroDivisionError` for division by zero. - `TypeError` for invalid concatenation attempts. - `IndexError` for out-of-range list access. 5. Define a custom exception class `CustomError` that inherits from the built-in `Exception` class. 6. Implement a function `raise_custom_error(message: str)` that raises `CustomError` with the provided message. 7. Maintain detailed logs of exceptions and return a list of results or error messages for each operation in the order they were processed. Input: - `operations`: A list of tuples, where each tuple represents an operation and its arguments. Output: - A list of results or error messages corresponding to each operation. Example: ```python operations = [ (\\"divide\\", 4, 2), (\\"divide\\", 4, 0), (\\"concat\\", \\"hello\\", 3), (\\"access\\", 5), (\\"cause_custom\\", \\"This is a custom error\\") ] # Calling the function output = process_operations(operations) # Expected output: # [ # 2, # \\"Error: Division by zero.\\", # \\"Error: unsupported operand type(s) for +: \'str\' and \'int\'.\\", # \\"Error: list index out of range.\\", # \\"Error: This is a custom error.\\" # ] ``` Constraints: - You should provide the exact error messages as mentioned in the example. - Ensure that the function handles different exceptions efficiently. - The length of `operations` list will not exceed 1000 elements. # Function Signature: ```python class CustomError(Exception): pass def raise_custom_error(message: str) -> None: raise CustomError(message) def process_operations(operations: list) -> list: # Your implementation here pass ```","solution":"class CustomError(Exception): pass def raise_custom_error(message: str) -> None: raise CustomError(message) def process_operations(operations: list) -> list: sample_list = [1, 2, 3] results = [] for operation in operations: op_type = operation[0] try: if op_type == \\"divide\\": _, a, b = operation result = a / b results.append(result) elif op_type == \\"concat\\": _, a, b = operation result = a + b results.append(result) elif op_type == \\"access\\": _, index = operation result = sample_list[index] results.append(result) elif op_type == \\"cause_custom\\": _, message = operation raise_custom_error(message) except ZeroDivisionError: results.append(\\"Error: Division by zero.\\") except TypeError as e: results.append(f\\"Error: {str(e)}.\\") except IndexError as e: results.append(f\\"Error: {str(e)}.\\") except CustomError as e: results.append(f\\"Error: {str(e)}.\\") return results"},{"question":"**Command-Line Utility: File Organizer** **Objective:** You are asked to design a command-line utility using `optparse` that organizes files into directories based on their extensions. The utility should support various options to customize its behavior. **Task:** Implement a Python script using `optparse` with the following functionalities: 1. **Input Directory Specification:** - Option: `-i` or `--input` - Argument: Path to the input directory. - This option is required. 2. **Output Directory Specification:** - Option: `-o` or `--output` - Argument: Path to the output directory. - By default, it should use the current directory as the output directory. 3. **Verbose Mode:** - Option: `-v` or `--verbose` - Action: Print detailed information during execution. 4. **File Extension Filter:** - Option: `-e` or `--extension` - Argument: Comma-separated list of file extensions to include (e.g., `-e jpg,png`). - By default, include all file types. 5. **Simulation Mode:** - Option: `-s` or `--simulate` - Action: Simulate the organization without actually moving files (prints the intended actions). **Constraints:** - Use the `optparse` module for parsing command-line arguments. - Validate that the input directory exists. - Implement a custom callback to handle the extension filter and convert it into a list of extensions. - Ensure proper error handling and user guidance via help messages. **Example Usage:** ```bash python organize.py -i /path/to/input -o /path/to/output -e jpg,png -v -s ``` **Input Format:** Command-line arguments as specified in the task. **Output Format:** - If `--verbose` is enabled, print detailed actions. - If `--simulate` is enabled, print the intended actions without performing file operations. **Implementation:** You are to implement the function `main()` which will be the entry point of your script. Ensure that the script behaves correctly for different combinations of options. ```python from optparse import OptionParser import os def extension_callback(option, opt_str, value, parser): extensions = value.split(\',\') setattr(parser.values, option.dest, extensions) def main(): parser = OptionParser(usage=\\"usage: %prog [options]\\") parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input_dir\\", help=\\"Path to the input directory\\", metavar=\\"DIR\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output_dir\\", default=\\".\\", help=\\"Path to the output directory (default: current directory)\\", metavar=\\"DIR\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"Enable verbose mode\\") parser.add_option(\\"-e\\", \\"--extension\\", dest=\\"extensions\\", type=\\"string\\", action=\\"callback\\", callback=extension_callback, help=\\"Comma-separated list of file extensions to include (e.g., jpg,png)\\") parser.add_option(\\"-s\\", \\"--simulate\\", action=\\"store_true\\", dest=\\"simulate\\", help=\\"Simulate the organization without moving files\\") (options, args) = parser.parse_args() if not options.input_dir: parser.error(\\"Input directory is required\\") if not os.path.exists(options.input_dir): parser.error(f\\"Input directory \'{options.input_dir}\' does not exist\\") # Remaining implementation for organizing files based on provided options if options.verbose: print(f\\"Organizing files from \'{options.input_dir}\' to \'{options.output_dir}\'\\") if options.simulate: print(\\"Simulation mode enabled - no files will be moved.\\") if options.extensions: print(f\\"Filtering by extensions: {\', \'.join(options.extensions)}\\") # Simulation or file organization logic here if __name__ == \\"__main__\\": main() ``` Test your script to ensure it handles various scenarios as expected. **Notes:** - Focus on utilizing the `optparse` functionalities effectively. - Aim for clarity and proper error handling. - Ensure the script provides useful help messages for users.","solution":"from optparse import OptionParser import os import shutil def extension_callback(option, opt_str, value, parser): extensions = value.split(\',\') setattr(parser.values, option.dest, extensions) def organize_files(input_dir, output_dir, extensions=None, verbose=False, simulate=False): for root, dirs, files in os.walk(input_dir): for file in files: file_extension = file.split(\'.\')[-1] if extensions and file_extension not in extensions: continue target_dir = os.path.join(output_dir, file_extension) if not simulate and not os.path.exists(target_dir): os.makedirs(target_dir) source_path = os.path.join(root, file) target_path = os.path.join(target_dir, file) if verbose: print(f\\"{(\'Simulate moving\' if simulate else \'Moving\')} file {source_path} to {target_path}\\") if not simulate: shutil.move(source_path, target_path) def main(): parser = OptionParser(usage=\\"usage: %prog [options]\\") parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input_dir\\", help=\\"Path to the input directory\\", metavar=\\"DIR\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output_dir\\", default=\\".\\", help=\\"Path to the output directory (default: current directory)\\", metavar=\\"DIR\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"Enable verbose mode\\") parser.add_option(\\"-e\\", \\"--extension\\", dest=\\"extensions\\", type=\\"string\\", action=\\"callback\\", callback=extension_callback, help=\\"Comma-separated list of file extensions to include (e.g., jpg,png)\\") parser.add_option(\\"-s\\", \\"--simulate\\", action=\\"store_true\\", dest=\\"simulate\\", help=\\"Simulate the organization without moving files\\") (options, args) = parser.parse_args() if not options.input_dir: parser.error(\\"Input directory is required\\") if not os.path.exists(options.input_dir): parser.error(f\\"Input directory \'{options.input_dir}\' does not exist\\") organize_files(options.input_dir, options.output_dir, options.extensions, options.verbose, options.simulate) if __name__ == \\"__main__\\": main()"},{"question":"Given a netrc file containing authentication details for various hosts, implement a function using the python `netrc` package to fetch login details for a specified host. Your function should: 1. Parse the provided netrc file. 2. Return the login, account, and password details for the specified host. 3. Handle cases where the host is not found by returning the details for the \'default\' entry, if available. 4. Properly handle `NetrcParseError` exceptions by returning an informative error message. # Function Signature ```python def get_authenticators(file_path: str, host: str) -> str: pass ``` # Input - `file_path` (str): The path to the netrc file to be parsed. - `host` (str): The host for which to retrieve the authentication details. # Output - `output` (str): The authentication details in the format \\"login: [login], account: [account], password: [password]\\". If the host or default entry is not found, return \\"No entry found for host\\". If a parsing error occurs, return \\"Error: [error message]\\". # Example Given a `my_netrc` file with the following contents: ``` machine host1 login user1 password pass1 default login user_default password pass_default ``` Calling `get_authenticators(\'my_netrc\', \'host1\')` should return: ``` login: user1, account: None, password: pass1 ``` Calling `get_authenticators(\'my_netrc\', \'host2\')` should return: ``` login: user_default, account: None, password: pass_default ``` # Example Code ```python import netrc def get_authenticators(file_path: str, host: str) -> str: try: credentials = netrc.netrc(file_path) auth_details = credentials.authenticators(host) if not auth_details: return \\"No entry found for host\\" login, account, password = auth_details return f\\"login: {login}, account: {account}, password: {password}\\" except netrc.NetrcParseError as e: return f\\"Error: {e.msg}\\" # Example calls print(get_authenticators(\'sample_netrc\', \'host1\')) print(get_authenticators(\'sample_netrc\', \'host2\')) ``` Ensure your implementation correctly handles these cases and returns the appropriate output.","solution":"import netrc def get_authenticators(file_path: str, host: str) -> str: try: credentials = netrc.netrc(file_path) auth_details = credentials.authenticators(host) if not auth_details: auth_details = credentials.authenticators(\'default\') if not auth_details: return \\"No entry found for host\\" login, account, password = auth_details return f\\"login: {login}, account: {account}, password: {password}\\" except netrc.NetrcParseError as e: return f\\"Error: {str(e)}\\""},{"question":"# Question: Enhanced Debugging Function **Objective:** Implement a function named `detailed_debug_log` that wraps around a given function and captures detailed exception information using the traceback module. This function must log the complete traceback and exception details to a specified file whenever an exception is raised during the execution of the wrapped function. **Function Signature:** ```python def detailed_debug_log(func: Callable, *args: Any, **kwargs: Any) -> Any: pass ``` **Input:** 1. `func (Callable)`: The function to be wrapped and monitored for exceptions. 2. `args (Any)`: Positional arguments to be passed to the function. 3. `kwargs (Any)`: Keyword arguments to be passed to the function. **Output:** - The return value of the wrapped function if it executes without exceptions. - If an exception occurs, it should be logged into a file named `error_log.txt`, and the exception should be re-raised. **Requirements:** 1. Use the `traceback` module to capture and format the stack trace and exception details. 2. The log file should capture the following details: - The start of a new log entry. - A timestamp when the exception occurred. - The stack trace of the exception. - The type and message of the exception. 3. Ensure that the logging mechanism appends to the file rather than overwriting it. 4. Re-raise the exception after logging it. **Example Usage:** ```python def faulty_function(x, y): return x / y try: detailed_debug_log(faulty_function, 10, 0) except Exception as e: print(\\"Exception re-raised:\\", e) ``` **Expected Contents of `error_log.txt` after execution:** ``` --- Log Entry --- Timestamp: 2023-10-10 10:10:10 Traceback (most recent call last): File \\"<ipython-input-1-2e0f8a3bbdff>\\", line 2, in <module> detailed_debug_log(faulty_function, 10, 0) File \\"<ipython-input-1-2e0f8a3bbdff>\\", line 4, in detailed_debug_log return func(*args, **kwargs) File \\"<ipython-input-1-2e0f8a3bbdff>\\", line 6, in faulty_function return x / y ZeroDivisionError: division by zero ``` # Constraints: - Do not use external libraries for logging. - Ensure the function implementation is robust and handles any edge cases. - Code readability and appropriate use of the `traceback` module are crucial. This question tests the student’s ability to use the traceback module for detailed error handling, which is a crucial part of developing resilient Python applications.","solution":"import traceback import datetime from typing import Callable, Any def detailed_debug_log(func: Callable, *args: Any, **kwargs: Any) -> Any: try: return func(*args, **kwargs) except Exception as e: with open(\\"error_log.txt\\", \\"a\\") as log_file: log_file.write(\\"n--- Log Entry ---n\\") log_file.write(f\\"Timestamp: {datetime.datetime.now()}n\\") log_file.write(traceback.format_exc()) log_file.write(f\\"{type(e).__name__}: {str(e)}n\\") raise"},{"question":"# Advanced Programming Assessment Objective Write a Python function, `analyze_performance()`, to analyze the performance of another function. The `analyze_performance()` should take as input a target function and its arguments, profile its execution time, memory usage, and generate a detailed report of its behavior including traceback in case of any exceptions. Requirements 1. The function should accept another function `target_func` and its arguments `*args` and `**kwargs`. 2. Use the `cProfile` module to profile the function’s execution time and provide a detailed breakdown. 3. Utilize the `tracemalloc` module to profile memory usage during the function call. 4. Implement fault handling using the `faulthandler` module to capture and print traceback in case the function raises an exception. 5. Generate a combined report with the profiling results and return it as a formatted string. Function Signature ```python def analyze_performance(target_func: Callable, *args, **kwargs) -> str: ``` Input - `target_func`: A callable function to be profiled. - `*args`: Positional arguments for the `target_func`. - `**kwargs`: Keyword arguments for the `target_func`. Output - A formatted string report containing: - A detailed execution time breakdown from `cProfile`. - Memory usage statistics from `tracemalloc`. - Traceback information if an exception occurs during execution. Constraints - The target function should be a Python callable. - The profiling overhead should be minimal. Example ```python # Example of a target function def example_function(n): total = 0 for i in range(n): total += i return total # Analyzing the performance of `example_function` with input 1000 report = analyze_performance(example_function, 1000) print(report) ``` Your task is to implement the `analyze_performance` function as described above.","solution":"import cProfile import pstats import io import tracemalloc import faulthandler import traceback def analyze_performance(target_func, *args, **kwargs): Analyzes the performance of a target function and generates a report. Parameters: target_func (Callable): The function to be analyzed. *args: Positional arguments for the target function. **kwargs: Keyword arguments for the target function. Returns: str: A formatted report containing execution time, memory usage, and exception tracebacks. # Profiling the execution time with cProfile profiler = cProfile.Profile() profiler.enable() # Profiling memory usage with tracemalloc tracemalloc.start() try: # Run and monitor the target function target_func(*args, **kwargs) except Exception as e: # Capture exception traceback exception_trace = traceback.format_exc() else: exception_trace = None finally: profiler.disable() current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() # Create a StringIO buffer to capture the profiler stats s = io.StringIO() ps = pstats.Stats(profiler, stream=s).sort_stats(pstats.SortKey.CUMULATIVE) ps.print_stats() report = f Performance Report: ------------------- Execution Time: --------------- {s.getvalue()} Memory Usage: ------------- Current memory usage: {current / 1024:.2f} KiB Peak memory usage: {peak / 1024:.2f} KiB Exception Traceback: -------------------- {exception_trace or \\"No exceptions raised.\\"} return report"},{"question":"**Question: Implement a Function Utilizing PyNumber Functions** Objective: Given the descriptions provided in the documentation, your task is to implement a function `perform_operations(obj1, obj2)` in Python which performs multiple numerical operations on two Python objects. This function should showcase a variety of operations available in the PyNumber API. # Function Signature ```python def perform_operations(obj1, obj2): :param obj1: First input object, must be compatible with numerical operations :param obj2: Second input object, must be compatible with numerical operations :return: A dictionary containing the results of various operations defined below # Operations to Perform 1. Sum of `obj1` and `obj2`. 2. Difference when `obj2` is subtracted from `obj1`. 3. Product of `obj1` and `obj2`. 4. Integer division of `obj1` by `obj2`. 5. Remainder when `obj1` is divided by `obj2`. 6. Matrix multiplication of `obj1` and `obj2`. 7. Bitwise AND of `obj1` and `obj2`. 8. Bitwise OR of `obj1` and `obj2`. 9. Bitwise XOR of `obj1` and `obj2`. 10. Left shift `obj1` by the number of positions indicated by `obj2`. 11. Right shift `obj1` by the number of positions indicated by `obj2`. 12. Negative of `obj1`. 13. Absolute value of `obj1`. # Example Input/Output ```python # Example >> perform_operations(4, 2) { \\"sum\\": 6, \\"difference\\": 2, \\"product\\": 8, \\"floor_div\\": 2, \\"remainder\\": 0, \\"matrix_mult\\": None, # Could be a well-defined behavior based on input types \\"bitwise_and\\": 0, \\"bitwise_or\\": 6, \\"bitwise_xor\\": 6, \\"lshift\\": 16, \\"rshift\\": 1, \\"negative\\": -4, \\"absolute\\": 4 } Constraints: - `obj1` and `obj2` must be compatible with the operation being performed. - If an operation is not applicable to the objects, handle it gracefully by returning `None`. - Raise appropriate exceptions for invalid operations, where necessary. - Ensure the function is efficient and handles edge cases effectively. # Implementation Details: - Use functions from PyNumber API where applicable. - Ensure proper error handling and type checking as described in the documentation. - Retain a clear and concise coding style for readability and maintenance. Implement the function accordingly.","solution":"def perform_operations(obj1, obj2): Performs various operations on obj1 and obj2 and returns a dictionary of results. results = {} try: results[\'sum\'] = obj1 + obj2 except Exception: results[\'sum\'] = None try: results[\'difference\'] = obj1 - obj2 except Exception: results[\'difference\'] = None try: results[\'product\'] = obj1 * obj2 except Exception: results[\'product\'] = None try: results[\'floor_div\'] = obj1 // obj2 except Exception: results[\'floor_div\'] = None try: results[\'remainder\'] = obj1 % obj2 except Exception: results[\'remainder\'] = None try: results[\'matrix_mult\'] = obj1 @ obj2 except Exception: results[\'matrix_mult\'] = None try: results[\'bitwise_and\'] = obj1 & obj2 except Exception: results[\'bitwise_and\'] = None try: results[\'bitwise_or\'] = obj1 | obj2 except Exception: results[\'bitwise_or\'] = None try: results[\'bitwise_xor\'] = obj1 ^ obj2 except Exception: results[\'bitwise_xor\'] = None try: results[\'lshift\'] = obj1 << obj2 except Exception: results[\'lshift\'] = None try: results[\'rshift\'] = obj1 >> obj2 except Exception: results[\'rshift\'] = None try: results[\'negative\'] = -obj1 except Exception: results[\'negative\'] = None try: results[\'absolute\'] = abs(obj1) except Exception: results[\'absolute\'] = None return results"},{"question":"<|Analysis Begin|> The provided documentation covers details about the `json` module extensively, offering functionalities to encode and decode JSON data efficiently. This includes: 1. Basic functions for JSON operations such as: - `json.dumps()`: Serialize a Python `object` to a JSON formatted `str`. - `json.dump()`: Serialize a Python `object` to a JSON formatted stream. - `json.loads()`: Deserialize a JSON `str`, `bytes`, or `bytearray` instance to a Python `object`. - `json.load()`: Deserialize a JSON document from a file to a Python `object`. 2. Special parameters and functionality extending capabilities such as: - Custom encoding and decoding by subclassing `JSONEncoder` and `JSONDecoder`. - Specialized handling for complex numbers and other objects using `default` method and `object_hook`. - Pretty-printing, compact encoding, circular reference checks, and sorting keys. 3. Command line utilities via `json.tool`, aiding validation and pretty-printing. This rich documentation provides room for crafting questions that require students to demonstrate their understanding of both basic and advanced features of JSON operations in Python, including custom serialization and deserialization. <|Analysis End|> <|Question Begin|> # JSON Encode-Decode Extended Challenge You are required to implement custom JSON serialization and deserialization for a class that handles complex data structures, specifically, considering complex numbers and timestamps. The task involves the following steps: 1. **Custom Class Definition**: - Define a class `ComplexTimedMessage` with attributes: - `message` (string) - a text message - `number` (complex) - a complex number - `timestamp` (datetime.datetime) - a timestamp 2. **Custom Serialization**: - Implement a custom JSON encoder class `ComplexTimedMessageEncoder` that extends `json.JSONEncoder`: - The encoder should serialize `ComplexTimedMessage` objects such that: - `number` is represented as a tuple `[real, imag]`. - `timestamp` is represented as an ISO 8601 string. - Ensure other data types are handled by `JSONEncoder`. 3. **Custom Deserialization**: - Implement a custom JSON decoder class `ComplexTimedMessageDecoder` that extends `json.JSONDecoder`: - The decoder should correctly deserialize JSON strings back into `ComplexTimedMessage` objects. - The `number` tuple should be converted back to a complex number. - The `timestamp` string should be converted back to a `datetime.datetime` object. # Implementation Details - **Input**: JSON string representing `ComplexTimedMessage` object(s). - **Output**: An instance (or list of instances) of `ComplexTimedMessage`. # Constraints - Assume the JSON string is always in the correct format for decoding. - Handle floating-point precision as given by Python\'s built-in `float`. # Example ```python import json from datetime import datetime from json import JSONEncoder, JSONDecoder # Part 1: Class Definition class ComplexTimedMessage: def __init__(self, message, number, timestamp): self.message = message self.number = number self.timestamp = timestamp def __eq__(self, other): return (self.message == other.message and self.number == other.number and self.timestamp == other.timestamp) # Part 2: Custom JSON Encoder class ComplexTimedMessageEncoder(JSONEncoder): def default(self, obj): if isinstance(obj, ComplexTimedMessage): return { \\"message\\": obj.message, \\"number\\": [obj.number.real, obj.number.imag], \\"timestamp\\": obj.timestamp.isoformat() } return super().default(obj) # Part 3: Custom JSON Decoder class ComplexTimedMessageDecoder(JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, obj): if \\"message\\" in obj and \\"number\\" in obj and \\"timestamp\\" in obj: return ComplexTimedMessage( message=obj[\\"message\\"], number=complex(*obj[\\"number\\"]), timestamp=datetime.fromisoformat(obj[\\"timestamp\\"]) ) return obj # Testing Serialization and Deserialization if __name__ == \\"__main__\\": message = ComplexTimedMessage(\\"Hello, world!\\", complex(2, -3), datetime.now()) json_str = json.dumps(message, cls=ComplexTimedMessageEncoder) print(json_str) decoded_message = json.loads(json_str, cls=ComplexTimedMessageDecoder) assert message == decoded_message print(decoded_message.message, decoded_message.number, decoded_message.timestamp) ``` Implement the missing pieces and ensure that the test passes.","solution":"import json from datetime import datetime from json import JSONEncoder, JSONDecoder class ComplexTimedMessage: def __init__(self, message, number, timestamp): self.message = message self.number = number self.timestamp = timestamp def __eq__(self, other): return (self.message == other.message and self.number == other.number and self.timestamp == other.timestamp) class ComplexTimedMessageEncoder(JSONEncoder): def default(self, obj): if isinstance(obj, ComplexTimedMessage): return { \\"message\\": obj.message, \\"number\\": [obj.number.real, obj.number.imag], \\"timestamp\\": obj.timestamp.isoformat() } return super().default(obj) class ComplexTimedMessageDecoder(JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, obj): if \\"message\\" in obj and \\"number\\" in obj and \\"timestamp\\" in obj: return ComplexTimedMessage( message=obj[\\"message\\"], number=complex(*obj[\\"number\\"]), timestamp=datetime.fromisoformat(obj[\\"timestamp\\"]) ) return obj"},{"question":"Task: Implement a Custom Scikit-Learn Estimator # Objective Implement a custom scikit-learn compatible estimator that scales input features and then applies a simple linear regression model. This exercise will test your understanding of the scikit-learn API, including the `fit`, `predict`, `__init__`, and the handling of parameters and attributes. # Task Details 1. **Class Definition** - Define a class `ScaledLinearRegressor` that inherits from both `BaseEstimator` and `RegressorMixin`. 2. **Initializer (`__init__` method)** - The class should accept the following parameters: - `scale_factor`: A floating-point value to scale the input features (default should be 1.0). 3. **Fit Method** - The `fit` method should accept the following parameters: - `X`: np.ndarray of shape (n_samples, n_features), the training input samples. - `y`: np.ndarray of shape (n_samples,), the target values. - Implement the scaling of features using the provided `scale_factor`. - Compute and store the coefficients (`coef_`) and intercept (`intercept_`) of the linear regression model. 4. **Predict Method** - The `predict` method should accept the following parameter: - `X`: np.ndarray of shape (n_samples, n_features), the input samples for prediction. - Return the predicted values based on the linear regression model using the stored coefficients. 5. **Attributes** - Ensure that the class has the following attributes after fitting: - `coef_`: array, shape (n_features,) - `intercept_`: float 6. **Validation** - Validate input data using scikit-learn\'s `check_array` function. - Ensure that the `fit` and `predict` methods work correctly after the model is fitted. # Example Below is an example of using the custom estimator: ```python from sklearn.utils.estimator_checks import check_estimator import numpy as np class ScaledLinearRegressor(BaseEstimator, RegressorMixin): def __init__(self, scale_factor=1.0): self.scale_factor = scale_factor def fit(self, X, y): X = check_array(X) y = check_array(y, ensure_2d=False) X_scaled = X * self.scale_factor # Assume we use a simple linear regression computation (for illustration) self.coef_ = np.linalg.pinv(X_scaled.T @ X_scaled) @ X_scaled.T @ y self.intercept_ = np.mean(y) - np.mean(X_scaled @ self.coef_) return self def predict(self, X): X = check_array(X) X_scaled = X * self.scale_factor return X_scaled @ self.coef_ + self.intercept_ # Example usage X_train = np.array([[1, 2], [2, 3], [3, 4]]) y_train = np.array([5, 7, 9]) regressor = ScaledLinearRegressor(scale_factor=0.5) regressor.fit(X_train, y_train) predictions = regressor.predict(X_train) print(predictions) ``` # Validation Ensure your estimator passes the scikit-learn compatibility checks by running: ```python check_estimator(ScaledLinearRegressor()) ``` # Constraints - Do not use any existing scaling or linear regression functions from scikit-learn in your implementation. - Assume the input data `X` and `y` are always valid NumPy arrays.","solution":"import numpy as np from sklearn.base import BaseEstimator, RegressorMixin from sklearn.utils.validation import check_array class ScaledLinearRegressor(BaseEstimator, RegressorMixin): def __init__(self, scale_factor=1.0): self.scale_factor = scale_factor def fit(self, X, y): X = check_array(X) y = check_array(y, ensure_2d=False) X_scaled = X * self.scale_factor # Calculate coefficients using the normal equation self.coef_ = np.linalg.pinv(X_scaled.T @ X_scaled) @ X_scaled.T @ y self.intercept_ = np.mean(y) - np.mean(X_scaled @ self.coef_) return self def predict(self, X): X = check_array(X) X_scaled = X * self.scale_factor return X_scaled @ self.coef_ + self.intercept_"},{"question":"You are required to implement a function that performs the following steps using PyTorch: 1. Create a tensor from a given list of numbers. 2. Reshape this tensor into a specified shape. 3. Calculate the mean and standard deviation of the tensor. 4. Normalize the tensor by subtracting the mean and dividing by the standard deviation. 5. Perform a matrix multiplication between the normalized tensor and its transpose. 6. Compute the gradient of the sum of all elements in the resulting tensor with respect to the original tensor. # Function Signature ```python import torch def tensor_operations(data: list, shape: tuple) -> torch.Tensor: # Your implementation here ``` # Input - `data`: A list of floats or integers representing the elements of the tensor. - `shape`: A tuple representing the desired shape to which the tensor should be reshaped. # Output - Returns the tensor containing the gradient of the sum of the product tensor with respect to the original tensor. # Example ```python data = [1.0, 2.0, 3.0, 4.0] shape = (2, 2) result = tensor_operations(data, shape) print(result) # This should print the gradient tensor after computing all operations. ``` # Note - Ensure that the operations are performed using PyTorch\'s tensor methods. - You may assume that the number of elements in `data` is such that it can be reshaped into the provided `shape`. # Constraints - The length of `data` is a product of the dimensions in `shape`. # Explanation - Convert the list `data` into a PyTorch tensor with `requires_grad=True`. - Reshape the tensor to the given `shape`. - Compute the mean and standard deviation of the reshaped tensor. - Normalize the tensor. - Compute the matrix product of the normalized tensor and its transpose. - Compute the gradient of the sum of all elements in the resulting tensor with respect to the original tensor using PyTorch’s automatic differentiation functionality.","solution":"import torch def tensor_operations(data: list, shape: tuple) -> torch.Tensor: Perform a series of tensor operations using PyTorch, including reshaping, normalizing, matrix multiplication, and gradient computation. :param data: List of numerical data for the tensor. :param shape: Tuple representing the target shape of the tensor. :return: Tensor containing the gradients. # Step 1: Convert the list to a PyTorch tensor with requires_grad=True tensor = torch.tensor(data, dtype=torch.float32, requires_grad=True) # Step 2: Reshape the tensor to the given shape reshaped_tensor = tensor.view(shape) # Step 3: Calculate the mean and standard deviation mean = reshaped_tensor.mean() std_dev = reshaped_tensor.std() # Step 4: Normalize the tensor normalized_tensor = (reshaped_tensor - mean) / std_dev # Step 5: Matrix multiplication of normalized tensor with its transpose result_tensor = torch.matmul(normalized_tensor, normalized_tensor.t()) # Step 6: Compute the gradient of the sum of the resulting tensor result_sum = result_tensor.sum() result_sum.backward() # Return the gradient of the original tensor return tensor.grad"},{"question":"# Tarfile Manipulation Task **Objective:** Implement a Python function to manage tar archives using the `tarfile` module. The task will assess your understanding of file handling, metadata manipulation, and error handling in tar archives. **Task:** 1. Write a function `create_tar_archive(source_paths, archive_name, compression=None)` that takes a list of file and directory paths (`source_paths`), the name of the tar archive to be created (`archive_name`), and an optional compression method (`compression`). The function should: - Create a tar archive named `archive_name`. - Add all files and directories specified in `source_paths` to the archive. - Support optional gzip, bzip2, and lzma compressions based on the `compression` argument. - Raise a `ValueError` if an invalid compression method is provided. 2. Write a function `extract_selected_files(archive_name, dest_path, file_patterns=None)` that takes the name of a tar archive (`archive_name`), a destination path (`dest_path`), and an optional list of wildcard patterns (`file_patterns`). The function should: - Extract only the files from the archive whose names match the provided patterns to the specified destination path. - Use the \'data\' extraction filter by default to ensure secure extraction. - Use wildcard patterns using the `fnmatch` module. - If no patterns are provided, extract all files. **Constraints:** - You should ensure that no absolute paths are extracted to avoid security risks. - Use appropriate exception handling to manage errors during archive operations. **Performance Requirements:** - The solution should handle typical file counts (< 10,000 files) and sizes (< 1 GB per file) within practical memory and time limits. **Function Signatures:** ```python def create_tar_archive(source_paths: List[str], archive_name: str, compression: Optional[str] = None) -> None: pass def extract_selected_files(archive_name: str, dest_path: str, file_patterns: Optional[List[str]] = None) -> None: pass ``` **Example Usage:** ```python # Creating a tar archive with gzip compression create_tar_archive([\\"file1.txt\\", \\"file2.txt\\", \\"dir1\\"], \\"archive.tar.gz\\", compression=\\"gz\\") # Extracting specific files from the archive extract_selected_files(\\"archive.tar.gz\\", \\"/path/to/destination\\", file_patterns=[\\"*.txt\\", \\"dir1/*\\"]) ``` * The provided code and explanation should demonstrate your understanding of the `tarfile` module functionalities, error handling, and implementation strategies for secure tar archive manipulation.","solution":"import tarfile import fnmatch import os from typing import List, Optional def create_tar_archive(source_paths: List[str], archive_name: str, compression: Optional[str] = None) -> None: Creates a tar archive from the given source paths with optional compression. Parameters: - source_paths: List of file and directory paths to include in the archive. - archive_name: Name of the tar archive to be created. - compression: Optional compression method (\'gz\', \'bz2\', \'xz\'). Default is None (no compression). Raises: - ValueError: If an invalid compression method is provided. mode = \'w\' if compression == \'gz\': mode += \':gz\' elif compression == \'bz2\': mode += \':bz2\' elif compression == \'xz\': mode += \':xz\' elif compression is not None: raise ValueError(f\\"Unsupported compression method: {compression}\\") with tarfile.open(archive_name, mode) as tar: for path in source_paths: tar.add(path, arcname=os.path.basename(path)) def extract_selected_files(archive_name: str, dest_path: str, file_patterns: Optional[List[str]] = None) -> None: Extracts selected files from a tar archive based on provided patterns to a destination path. Parameters: - archive_name: Name of the tar archive to extract files from. - dest_path: Destination path to extract files to. - file_patterns: Optional list of wildcard patterns to match files to extract. Default is None (extract all files). Raises: - FileNotFoundError: If the archive does not exist. with tarfile.open(archive_name, \'r:*\') as tar: members = tar.getmembers() for member in members: if file_patterns: if any(fnmatch.fnmatch(member.name, pattern) for pattern in file_patterns): tar.extract(member, path=dest_path, set_attrs = False) else: tar.extract(member, path=dest_path, set_attrs = False)"},{"question":"**Objective**: Demonstrate your understanding of the `sndhdr` module in Python 3.10 by implementing a function that classifies sound files and creates a summary report. **Problem Statement**: You are given a list of filenames corresponding to various sound files. Your task is to classify these files based on their type, and generate a summary report containing the count of each sound file type and additional statistics (such as the total number of channels and the average sampling rate for each file type). **Function Signature**: ```python def classify_sound_files(filenames: list[str]) -> dict: pass ``` **Input Format**: - `filenames`: A list of strings, where each string is the path to a sound file. **Output Format**: - A dictionary where the keys are sound file types (e.g., `\'wav\'`, `\'aiff\'`, etc.) and the values are dictionaries containing: - `\'count\'`: The number of files of that type. - `\'total_channels\'`: The total number of channels across all files of that type. - `\'average_sampling_rate\'`: The average sampling rate for files of that type. **Constraints**: - If the `sndhdr` functions cannot determine the type of a file, it should not be included in the summary report. - Assume all given filenames are valid paths to sound files. - The summary report should only include sound file types detected by the `sndhdr` module. **Example**: ```python filenames = [\\"sample1.wav\\", \\"sample2.aiff\\", \\"sample3.wav\\", \\"sample4.au\\"] result = classify_sound_files(filenames) print(result) ``` Expected Output: ```python { \'wav\': { \'count\': 2, \'total_channels\': 4, \'average_sampling_rate\': 44100.0 }, \'aiff\': { \'count\': 1, \'total_channels\': 2, \'average_sampling_rate\': 48000.0 }, \'au\': { \'count\': 1, \'total_channels\': 1, \'average_sampling_rate\': 8000.0 } } ``` **Notes**: - To calculate the average sampling rate, sum up all the sampling rates for a file type and then divide by the count of files of that type. - Your solution should handle cases where the sampling rate or the number of channels is zero or unknown by ignoring those values in the calculations.","solution":"import sndhdr def classify_sound_files(filenames: list[str]) -> dict: result = {} for filename in filenames: file_info = sndhdr.what(filename) if file_info is None: continue file_type = file_info.filetype channels = file_info.channels sampling_rate = file_info.framerate if file_type in result: result[file_type][\'count\'] += 1 result[file_type][\'total_channels\'] += channels result[file_type][\'total_sampling_rate\'] += sampling_rate else: result[file_type] = { \'count\': 1, \'total_channels\': channels, \'total_sampling_rate\': sampling_rate } for file_type in result: count = result[file_type][\'count\'] total_sampling_rate = result[file_type][\'total_sampling_rate\'] result[file_type][\'average_sampling_rate\'] = total_sampling_rate / count del result[file_type][\'total_sampling_rate\'] return result"},{"question":"Pattern-Based File Filtering You are given a list of filenames and a list of patterns. Your task is to write a function named `filter_files_by_patterns` that filters the filenames based on the given patterns using Unix shell-style wildcards. # Function Definition ```python def filter_files_by_patterns(filenames: List[str], patterns: List[str]) -> List[str]: pass ``` # Input 1. `filenames` (List[str]): A list of strings representing filenames. (1 <= len(filenames) <= 1000) 2. `patterns` (List[str]): A list of strings representing wildcard patterns. (1 <= len(patterns) <= 100) # Output - Return a list of filenames that match any of the patterns provided. The result should be sorted in lexicographical order. # Constraints - Filenames are case-sensitive (consider using `fnmatchcase`). - You must use the `fnmatch` module functions to solve this problem. - Wildcard characters follow Unix shell-style patterns: - `*` matches everything - `?` matches any single character - `[seq]` matches any character in `seq` - `[!seq]` matches any character not in `seq` # Example ```python filenames = [\\"test.py\\", \\"example_test.py\\", \\"README.md\\", \\"sample.TXT\\"] patterns = [\\"*.py\\", \\"*.md\\"] filter_files_by_patterns(filenames, patterns) # Expected Output: [\\"README.md\\", \\"example_test.py\\", \\"test.py\\"] ``` # Explanation - Both `\\"test.py\\"` and `\\"example_test.py\\"` match the pattern `\\"*.py\\"`. - `\\"README.md\\"` matches the pattern `\\"*.md\\"`. - `\\"sample.TXT\\"` does not match any pattern. # Notes - Make sure to use case-sensitive matching. - The output list must be sorted in lexicographical order.","solution":"from typing import List from fnmatch import fnmatchcase def filter_files_by_patterns(filenames: List[str], patterns: List[str]) -> List[str]: matched_files = [] for filename in filenames: for pattern in patterns: if fnmatchcase(filename, pattern): matched_files.append(filename) break # Move to the next filename once a match is found return sorted(matched_files)"},{"question":"Clustering with Multiple Algorithms in Scikit-learn Objective The objective of this assessment is to demonstrate your understanding of different clustering algorithms available in scikit-learn, how to apply them to a dataset, and how to evaluate their performance using different clustering evaluation metrics. Problem Statement Given the `iris` dataset, you are required to: 1. Apply four different clustering algorithms from scikit-learn: K-Means, Agglomerative Clustering, DBSCAN, and Spectral Clustering. 2. Evaluate and compare the performance of these algorithms using at least three different clustering performance metrics. 3. Provide a function `compare_clustering_algorithms` that takes no input parameters, performs the clustering and evaluation, and returns a dictionary with the performance scores for each algorithm. Requirements 1. Use the following algorithms with their default parameters: - K-Means (`KMeans`) - Agglomerative Clustering (`AgglomerativeClustering`) - DBSCAN (`DBSCAN`) - Spectral Clustering (`SpectralClustering`) 2. Evaluate the performance using at least three of the following metrics: - Adjusted Rand Index (`adjusted_rand_score`) - Adjusted Mutual Information (`adjusted_mutual_info_score`) - Silhouette Score (`silhouette_score`) - Calinski-Harabasz Index (`calinski_harabasz_score`) - Davies-Bouldin Index (`davies_bouldin_score`) 3. The dataset to use is the Iris dataset available in scikit-learn\'s datasets module. Function Signature ```python def compare_clustering_algorithms(): Apply four different clustering algorithms on the iris dataset, evaluate and compare their performance using different clustering metrics. Returns: dict: A dictionary with algorithm names as keys and another dictionary of performance scores as values. pass ``` Expected Output The `compare_clustering_algorithms` function should return a dictionary with the following structure: ```python { \'KMeans\': {\'adjusted_rand_score\': ..., \'adjusted_mutual_info_score\': ..., \'silhouette_score\': ...}, \'AgglomerativeClustering\': {\'adjusted_rand_score\': ..., \'adjusted_mutual_info_score\': ..., \'silhouette_score\': ...}, \'DBSCAN\': {\'adjusted_rand_score\': ..., \'adjusted_mutual_info_score\': ..., \'silhouette_score\': ...}, \'SpectralClustering\': {\'adjusted_rand_score\': ..., \'adjusted_mutual_info_score\': ..., \'silhouette_score\': ...}, } ``` Constraints - You must use scikit-learn\'s implementations of the algorithms and metrics. - The evaluation metrics should be computed on the ground truth labels provided in the Iris dataset. - Handle any exceptions that may arise during clustering and ensure the function returns results for all four algorithms. Example Usage ```python result = compare_clustering_algorithms() print(result) ``` Notes - Ensure to install scikit-learn and other necessary libraries before running your code. - You may use helper functions to structure your code better. - Document your code and provide comments where necessary. Submission Submit your solution as a Python script or Jupyter Notebook.","solution":"from sklearn.datasets import load_iris from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score from sklearn.preprocessing import StandardScaler def compare_clustering_algorithms(): # Load the Iris dataset iris = load_iris() data = iris.data true_labels = iris.target # Standardize the data scaler = StandardScaler() data = scaler.fit_transform(data) # Define the clustering algorithms algorithms = { \'KMeans\': KMeans(n_clusters=3), \'AgglomerativeClustering\': AgglomerativeClustering(n_clusters=3), \'DBSCAN\': DBSCAN(), \'SpectralClustering\': SpectralClustering(n_clusters=3, affinity=\'nearest_neighbors\') } # Dictionary to hold the results results = {} # Evaluate each algorithm for name, algorithm in algorithms.items(): try: # Fit and predict clusters labels = algorithm.fit_predict(data) # Calculate metrics ari = adjusted_rand_score(true_labels, labels) ami = adjusted_mutual_info_score(true_labels, labels, average_method=\'arithmetic\') silhouette = silhouette_score(data, labels) # Store the results results[name] = { \'adjusted_rand_score\': ari, \'adjusted_mutual_info_score\': ami, \'silhouette_score\': silhouette } except Exception as e: # Handle exception if any algorithm fails results[name] = f\\"Error: {str(e)}\\" return results"},{"question":"**Question Title: Analyzing and Visualizing the Penguins Dataset** **Background:** Using visualizations to explore and understand data is crucial for data analysis. This task assesses your ability to use seaborn for effective data visualization using the more recent `seaborn.objects` API. **Objective:** You are provided with the Penguins dataset. Your objective is to create a visualization that effectively communicates the relationship between different numerical features and categorical features of the dataset. **Task:** 1. **Load the Penguins Dataset:** - Load the dataset using the `seaborn.load_dataset` function. 2. **Create a Faceted Plot:** - Visualize the relationship between `bill_length_mm` and `bill_depth_mm`. - The plot should be faceted by the `species` of the penguins. - Differentiate the data points within each facet by the `sex` of the penguins using color coding. 3. **Add Error Bars:** - Overlay the scatter points with error bars. - Use standard deviation (sd) as the measure for the error bars. 4. **Customize the Plot:** - Add appropriate labels and titles to the plot. - Ensure that the plot is clear and informative. **Expected Input and Output:** - **Input:** None (The dataset will be loaded within the script). - **Output:** A seaborn plot displayed inline with faceted scatter plots and error bars. **Sample Code Template:** ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the Penguins Dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create a Faceted Plot plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"sex\\") .facet(\\"species\\") .add(so.Dots(), so.Agg(), so.Dodge()) ) # Step 3: Add Error Bars plot = plot.add(so.Range(), so.Est(errorbar=\\"sd\\")) # Step 4: Customize the Plot plot = plot.label( title=\\"Bill Length vs Bill Depth Faceted by Species\\", x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", color=\\"Sex\\" ) # Display the plot plot.show() ``` **Constraints:** - Ensure you handle any missing values in the dataset appropriately. - The faceting should dynamically handle all unique species in the dataset. Solve this task and provide the code that produces the described plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_penguins(): # Load the Penguins Dataset penguins = load_dataset(\\"penguins\\") # Handle missing values by dropping them penguins = penguins.dropna() # Create a Faceted Plot plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"sex\\") .facet(\\"species\\") .add(so.Dots(), so.Agg(), so.Dodge()) ) # Add Error Bars plot = plot.add(so.Range(), so.Est(errorbar=\\"sd\\")) # Customize the Plot plot = plot.label( title=\\"Bill Length vs Bill Depth Faceted by Species\\", x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", color=\\"Sex\\" ) # Display the plot plot.show()"},{"question":"You are working on a Python script that needs to modify some environment variables and then execute a command in a subprocess that should reflect these changes. Using the `os` module, write a function that updates an environment variable and then runs a given command while passing the updated environment. # Function Signature ```python def update_env_and_execute(command: str, update_vars: dict) -> None: pass ``` # Inputs - `command` (str): The command that needs to be executed as a string. - `update_vars` (dict): A dictionary where keys are the names of environment variables to update and values are the new values for those variables. # Output - The function should not return anything. It should print the output of the command executed with the updated environment. # Constraints - You must use the `os` module for modifying environment variables and executing the command. # Example ```python import os def update_env_and_execute(command: str, update_vars: dict) -> None: # Your implementation here # Example Usage update_env_and_execute(\'echo EXAMPLE_VAR\', {\'EXAMPLE_VAR\': \'Hello World!\'}) ``` The expected printed output of the example should be: ``` Hello World! ``` # Explanation In the example provided, the `update_env_and_execute` function updates the environment variable `EXAMPLE_VAR` to `Hello World!` and then runs the command `echo EXAMPLE_VAR`. The `echo` command should output the value of `EXAMPLE_VAR`, which is `Hello World!`. # Notes - Think about the implications of environment variable updates and how they interact with subprocesses. - Ensure your solution works on both Unix and Windows systems, keeping in mind the portability of the code.","solution":"import os import subprocess def update_env_and_execute(command: str, update_vars: dict) -> None: Updates environment variables and executes given command. Parameters: - command (str): The command that needs to be executed. - update_vars (dict): Dictionary of environment variables to update. Returns: - None # Update environment variables updated_env = os.environ.copy() updated_env.update(update_vars) # Execute the command with the updated environment result = subprocess.run(command, shell=True, env=updated_env, capture_output=True, text=True) # Print the output of the command print(result.stdout.strip())"},{"question":"# Seaborn Swarmplot Coding Challenge Objective Your task is to create a series of swarm plots using the Seaborn library to analyze and visualize the data in various contexts. This assessment requires you to demonstrate an understanding of different plotting options and customization techniques provided by Seaborn. Dataset We will use the built-in dataset `tips` provided by Seaborn. Ensure you load this dataset before starting your visualizations. ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` Tasks 1. **Basic Swarm Plot**: Create a basic swarm plot showing the distribution of `total_bill`. 2. **Categorical Swarm Plot**: Modify the previous plot to split the data by the `day` variable on the y-axis. 3. **Hue Variable**: Further modify the plot from task 2 to add a `hue` dimension using the `sex` variable. 4. **Customized Plot Appearance**: Adjust the plot from task 3 to: - Use the \\"deep\\" color palette. - Set the point size to 5. - Use \'x\' as the marker. 5. **Facet Grid Plot**: Create a multi-facet plot that: - Uses `catplot` with `kind=\\"swarm\\"`. - Shows the relationship between `time` as the x-axis and `total_bill` as the y-axis. - Adds columns based on the `day` variable. - Uses the `sex` variable for the `hue` dimension. - Sets the aspect ratio to 0.7. Constraints - You should complete each task by writing a function named `create_swarm_plots()` that performs all the steps sequentially. - Each plot should be displayed before moving to the next. Output Execute the function and ensure that all plots are rendered correctly. The function does not need to return any values. Example Function Signature ```python def create_swarm_plots(): # load the dataset import seaborn as sns tips = sns.load_dataset(\\"tips\\") # Task 1: Basic Swarm Plot sns.swarmplot(data=tips, x=\\"total_bill\\") # Task 2: Categorical Swarm Plot sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") # Task 3: Hue Variable sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") # Task 4: Customized Plot Appearance sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", size=5, marker=\'x\', palette=\\"deep\\") # Task 5: Facet Grid Plot sns.catplot(data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.7) create_swarm_plots() ``` Write your code inside the given function to complete the tasks.","solution":"def create_swarm_plots(): # Load the dataset import seaborn as sns import matplotlib.pyplot as plt tips = sns.load_dataset(\\"tips\\") # Task 1: Basic Swarm Plot plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\") plt.title(\\"Basic Swarm Plot of Total Bill\\") plt.show() # Task 2: Categorical Swarm Plot plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Categorical Swarm Plot of Total Bill by Day\\") plt.show() # Task 3: Hue Variable plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") plt.title(\\"Swarm Plot of Total Bill by Day and Sex\\") plt.show() # Task 4: Customized Plot Appearance plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", size=5, marker=\'x\', palette=\\"deep\\") plt.title(\\"Customized Swarm Plot of Total Bill by Day and Sex\\") plt.show() # Task 5: Facet Grid Plot g = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.7 ) g.fig.suptitle(\\"Facet Grid Swarm Plot of Total Bill by Time, Day and Sex\\", y=1.05) plt.show()"},{"question":"**Question: Implementing a Secure Password Management System** In this question, you will design a simple password management system using the cryptographic services available in Python. Your task is to implement a class called `PasswordManager`, which should include the following functionalities: 1. **Storing Passwords Securely**: Implement a method `store_password(self, username: str, password: str) -> None` that securely hashes a user\'s password using the `hashlib` module before storing it. Use the BLAKE2b hashing algorithm for this purpose. 2. **Authenticating Users**: Implement a method `authenticate(self, username: str, password: str) -> bool` that checks if the provided password matches the stored hashed password for the given username. Utilize the `hmac` module to ensure that the password verification is secure against timing attacks. 3. **Generating Secure Token**: Implement a method `generate_token(self, username: str) -> str` that generates a secure random token for the user using the `secrets` module. This token might be used for session management or password recovery. # Constraints - You must use the `hashlib` module\'s BLAKE2b algorithm for hashing passwords. - Use the `hmac` module to verify the hashed passwords. - Use the `secrets` module to generate secure random tokens. - Assume that all usernames are unique. # Example Usage ```python pm = PasswordManager() pm.store_password(\\"alice\\", \\"wonderland123\\") pm.store_password(\\"bob\\", \\"builder456\\") assert pm.authenticate(\\"alice\\", \\"wonderland123\\") == True assert pm.authenticate(\\"alice\\", \\"wrongpassword\\") == False assert pm.authenticate(\\"bob\\", \\"builder456\\") == True token = pm.generate_token(\\"alice\\") assert isinstance(token, str) # Ensure token is a string ``` # Implementation Requirements - Carefully handle the storage and retrieval of hashed passwords and tokens. - Ensure that all cryptographic operations are performed securely to avoid common vulnerabilities. - Your implementation should be efficient and make use of appropriate cryptographic practices as described in the provided documentation. **Note**: The actual storage mechanism (e.g., in-memory dictionary, database) for user data is up to you, but make sure it meets the requirements outlined above and ensures secure handling of passwords and tokens.","solution":"import hashlib import hmac import secrets class PasswordManager: def __init__(self): self.user_passwords = {} self.user_tokens = {} def store_password(self, username: str, password: str) -> None: # Hash the password using BLAKE2b hashed_password = hashlib.blake2b(password.encode()).hexdigest() self.user_passwords[username] = hashed_password def authenticate(self, username: str, password: str) -> bool: if username not in self.user_passwords: return False # Retrieve the stored hashed password stored_hashed_password = self.user_passwords[username] # Create a hash of the provided password to compare hashed_password = hashlib.blake2b(password.encode()).hexdigest() # Use hmac.compare_digest for secure comparison return hmac.compare_digest(stored_hashed_password, hashed_password) def generate_token(self, username: str) -> str: # Generate a secure random token token = secrets.token_hex(32) # Generates a token of 64 hex characters self.user_tokens[username] = token return token"},{"question":"Coding Assessment Question # Objective: The objective of this question is to assess your understanding of device and stream management in PyTorch using the `torch.mtia` module. You will implement a function that initializes multiple devices, manages streams, and performs operations for matrix multiplication on different devices. # Question: You are required to write a function named `distributed_matrix_multiplication` that performs the following tasks: 1. Initializes all available MTIA devices. 2. Creates a stream for each device. 3. On each device, performs matrix multiplication of two given tensors in their respective streams. 4. Synchronizes all devices to ensure all operations are complete. 5. Collects the results from all devices and returns them as a list. # Function Signature: ```python def distributed_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> List[torch.Tensor]: pass ``` # Input: - `A` (torch.Tensor): A 2D tensor of shape (N, M). - `B` (torch.Tensor): A 2D tensor of shape (M, P). # Output: - Returns a list of tensors, where each tensor is the result of matrix multiplication `A @ B` computed on different devices. # Constraints: - Both tensors `A` and `B` are guaranteed to be 2D and their shapes are compatible for matrix multiplication. - Assume tensor sizes are within the capability of available MTIA devices. # Example: ```python A = torch.randn(100, 50) B = torch.randn(50, 100) results = distributed_matrix_multiplication(A, B) # results should be a list of tensors with the same result, distributed across available devices. for result in results: print(result.shape) # Should print torch.Size([100, 100]) for each device ``` # Additional Notes: - Make sure to handle MTIA initialization and check device availability. - Utilize the `torch.mtia` module for stream and device management. - Ensure that matrix multiplication operations are performed asynchronously on different devices. Good luck!","solution":"import torch from typing import List def distributed_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> List[torch.Tensor]: # Initialize all available MTIA devices device_count = torch.mtia.device_count() devices = [torch.device(f\'mtia:{i}\') for i in range(device_count)] if not devices: raise RuntimeError(\\"No MTIA devices found\\") # Create a stream for each device streams = [torch.mtia.Stream(device=i) for i in range(device_count)] results = [] for i, device in enumerate(devices): stream = streams[i] with torch.mtia.device(device), torch.mtia.stream(stream): A_device = A.to(device) B_device = B.to(device) result = torch.matmul(A_device, B_device) results.append(result.cpu()) # Wait for all streams to complete for stream in streams: stream.synchronize() return results"},{"question":"Advanced Random Operations in Python **Objective**: Assess the ability to implement and utilize various random generation and sampling techniques using the Python `random` module. Problem Description You are required to write a function to simulate a simplified version of a card game, where the goal is to calculate the probability of drawing a certain combination of cards. This card game uses a standard deck of 52 cards without jokers. The suits are hearts, diamonds, clubs, and spades, and the ranks are from Ace to King. In this game, you will: 1. Deal a specified number of cards from the deck to a player\'s hand. 2. Calculate and return the probability of having at least a certain number of hearts in that hand. Function Signature ```python def calculate_hearts_probability(deck_size: int, hand_size: int, num_hearts: int, trials: int) -> float: Simulate drawing cards from a deck and calculate the probability of drawing a specified number of hearts. Parameters: - deck_size (int): The size of the deck. (Must be 52 for a standard deck) - hand_size (int): The number of cards to draw in a hand. - num_hearts (int): The minimum number of hearts required in the hand. - trials (int): The number of simulation trials to run. Returns: - float: The probability of having at least `num_hearts` hearts in a hand of size `hand_size`. pass ``` Input: - `deck_size`: An integer representing the size of the deck. For a standard deck of cards, this will always be 52. - `hand_size`: An integer representing the number of cards in each hand drawn from the deck. - `num_hearts`: An integer representing the minimum number of hearts required in each hand. - `trials`: An integer representing the number of simulation trials to be conducted. Output: - A floating-point number representing the probability of drawing at least `num_hearts` hearts in a hand of size `hand_size`. Constraints: - `deck_size` is always 52. - 1 ≤ `hand_size` ≤ 52 - 1 ≤ `num_hearts` ≤ `hand_size` - 1 ≤ `trials` ≤ 10000 Example: ```python # Example call probability = calculate_hearts_probability(deck_size=52, hand_size=5, num_hearts=3, trials=1000) print(probability) # Output might be something like 0.157 (The actual value may vary due to randomness) ``` Implementation Details: - Use the `random` module to shuffle the deck and draw cards. - Count the occurrences where the drawn hand meets the required number of hearts. - Estimate the probability based on the number of successful occurrences over the total number of trials. **Note**: The function should handle shuffling and drawing operations efficiently considering the constraints. Make sure to leverage random sampling functions provided by the `random` module.","solution":"import random def calculate_hearts_probability(deck_size: int, hand_size: int, num_hearts: int, trials: int) -> float: Simulate drawing cards from a deck and calculate the probability of drawing a specified number of hearts. Parameters: - deck_size (int): The size of the deck. (Must be 52 for a standard deck) - hand_size (int): The number of cards to draw in a hand. - num_hearts (int): The minimum number of hearts required in the hand. - trials (int): The number of simulation trials to run. Returns: - float: The probability of having at least `num_hearts` hearts in a hand of size `hand_size`. hearts = [\'H\'] * 13 other_suits = [\'D\', \'C\', \'S\'] * 13 # Diamonds, Clubs, Spades full_deck = hearts + other_suits # Construct the full deck of 52 cards successful_trials = 0 for _ in range(trials): random.shuffle(full_deck) hand = full_deck[:hand_size] count_hearts = hand.count(\'H\') if count_hearts >= num_hearts: successful_trials += 1 probability = successful_trials / trials return probability"},{"question":"# Question: **Gaussian Mixture Model Implementation and Evaluation** You are tasked with implementing a Gaussian Mixture Model (GMM) using scikit-learn\'s `GaussianMixture` class. Your goal is to write a Python function that will initialize a GMM, fit it to a given dataset, predict cluster assignments, and evaluate the model using the Bayesian Information Criterion (BIC). Function Signature ```python def gmm_clustering(data, n_components, covariance_type): Fits a Gaussian Mixture Model to the data and evaluates it. Parameters: - data: np.array of shape (n_samples, n_features) The input data to cluster. - n_components: int The number of mixture components. - covariance_type: str Type of covariance to use (one of {\'full\', \'tied\', \'diag\', \'spherical\'}). Returns: - predicted_labels: np.array of shape (n_samples,) The predicted labels for each sample in the data. - bic_score: float The Bayesian Information Criterion (BIC) score for the fitted model. ``` Requirements 1. **Initialize the GMM**: - Use the `GaussianMixture` class. - Set the number of components (`n_components`). - Choose the covariance type (`covariance_type`). 2. **Fit the Model**: - Use the `.fit(data)` method to train the model. 3. **Predict Clusters**: - Use the `.predict(data)` method to get the cluster assignments for the input data. 4. **Evaluate the Model**: - Use the `.bic(data)` method to calculate the BIC score. Constraints - `data` is a numeric numpy array with no missing values. - `n_components` is a positive integer. - `covariance_type` must be one of `\'full\'`, `\'tied\'`, `\'diag\'`, or `\'spherical\'`. Example ```python import numpy as np # Sample data data = np.array([[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0]]) # Number of components and covariance type n_components = 2 covariance_type = \'full\' # Expected function call predicted_labels, bic_score = gmm_clustering(data, n_components, covariance_type) print(\\"Predicted Labels:\\", predicted_labels) print(\\"BIC Score:\\", bic_score) ``` Notes - Ensure you have the `scikit-learn` package installed to use the `GaussianMixture` class. - You should handle any potential exceptions that may arise when fitting the model or predicting cluster assignments. **Your implementation should demonstrate a clear understanding of the concepts provided in the documentation and validate the functionality comprehensively.**","solution":"import numpy as np from sklearn.mixture import GaussianMixture def gmm_clustering(data, n_components, covariance_type): Fits a Gaussian Mixture Model to the data and evaluates it. Parameters: - data: np.array of shape (n_samples, n_features) The input data to cluster. - n_components: int The number of mixture components. - covariance_type: str Type of covariance to use (one of {\'full\', \'tied\', \'diag\', \'spherical\'}). Returns: - predicted_labels: np.array of shape (n_samples,) The predicted labels for each sample in the data. - bic_score: float The Bayesian Information Criterion (BIC) score for the fitted model. # Initialize the GMM gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type) # Fit the GMM to the data gmm.fit(data) # Predict the cluster assignments for each sample predicted_labels = gmm.predict(data) # Calculate the BIC score for the fitted model bic_score = gmm.bic(data) return predicted_labels, bic_score"},{"question":"**Objective**: Assess the student\'s understanding of the `urllib` package in Python, including URL retrieval, error handling, URL parsing, and handling robots.txt files. **Question**: You are tasked with writing a Python script that retrieves data from a given URL, parses the URL to extract components, checks if the URL complies with robots.txt restrictions, and handles possible errors. Follow the steps below to complete the task: 1. **Opening and Reading URLs**: - Write a function `fetch_url(url: str) -> str` that takes a URL as input and returns the content of the page as a string. - If an error occurs while opening the URL (e.g., the URL is not found or there is no internet connection), the function should return a string with an appropriate error message. 2. **Parsing URLs**: - Write a function `parse_url(url: str) -> dict` that takes a URL as input and returns a dictionary with the following components: - `scheme` - `netloc` - `path` - `params` - `query` - `fragment` - Use `urllib.parse.urlparse` for this task. 3. **Checking `robots.txt`**: - Write a function `check_robots(url: str) -> bool` that takes a URL as input and returns `True` if the URL can be fetched according to the site\'s `robots.txt` file, and `False` otherwise. - Use `urllib.robotparser` for this task. **Constraints**: - You may assume that the URL provided is a well-formed URL string. - You may use any built-in library functions provided by `urllib`. **Example Usage**: ```python # Sample URL url = \\"http://example.com/somepage\\" # Fetch URL content content = fetch_url(url) print(content) # Prints the content or an error message # Parse URL parsed_components = parse_url(url) print(parsed_components) # Prints a dictionary with parsed components # Check robots.txt is_allowed = check_robots(url) print(is_allowed) # Prints True if the URL can be fetched according to robots.txt, False otherwise ``` **Expected Output**: Given a URL \\"http://example.com/somepage\\": 1. The `fetch_url` function should return the content of the page at \\"http://example.com/somepage\\". 2. The `parse_url` function should return: ```python { \\"scheme\\": \\"http\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"/somepage\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\" } ``` 3. The `check_robots` function should return `True` or `False` based on the `robots.txt` file at \\"http://example.com/robots.txt\\". This question covers fundamental concepts of URL handling, parsing, and compliance with web standards, making it a comprehensive assessment of students\' understanding and ability to use the `urllib` package.","solution":"import urllib.request import urllib.error import urllib.parse from urllib.robotparser import RobotFileParser def fetch_url(url: str) -> str: try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: return f\\"Error fetching URL: {e.reason}\\" def parse_url(url: str) -> dict: parsed = urllib.parse.urlparse(url) return { \\"scheme\\": parsed.scheme, \\"netloc\\": parsed.netloc, \\"path\\": parsed.path, \\"params\\": parsed.params, \\"query\\": parsed.query, \\"fragment\\": parsed.fragment } def check_robots(url: str) -> bool: parsed_url = urllib.parse.urlparse(url) robots_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\" rp = RobotFileParser() rp.set_url(robots_url) rp.read() return rp.can_fetch(\'*\', url)"},{"question":"**Permutation Feature Importance Assessment** You are given a dataset and asked to train a machine learning model, compute permutation feature importances, and interpret the results. # Task Description 1. Load the provided dataset. 2. Split the dataset into training and testing sets. 3. Train a `RandomForestRegressor` model on the training set. 4. Compute the permutation feature importances on the testing set using R² as the scoring metric. 5. Identify and print the top 3 most important features along with their importance scores and standard deviations. # Input - A CSV file path `data.csv`, which includes a tabular dataset. The dataset\'s target variable is named `target`. # Output - Print the top 3 features in descending order of importance with their mean importance and standard deviation in the following format: ``` Feature: feature_name, Mean Importance: mean_importance, Std Deviation: std_importance ``` # Constraints - Use 10 repeats for the permutation importance calculation. - Ensure that the random state is set for reproducibility. # Example ```plaintext Feature: Feature1, Mean Importance: 0.150, Std Deviation: 0.020 Feature: Feature2, Mean Importance: 0.140, Std Deviation: 0.025 Feature: Feature3, Mean Importance: 0.130, Std Deviation: 0.018 ``` # Python Boilerplate Code ```python import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance # Load the dataset data = pd.read_csv(\\"data.csv\\") X = data.drop(columns=[\\"target\\"]) y = data[\\"target\\"] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model model = RandomForestRegressor(random_state=42) model.fit(X_train, y_train) # Compute permutation feature importance result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42) # Extract importance and identify top 3 features importance_means = result.importances_mean importance_stds = result.importances_std features = X.columns sorted_indices = importance_means.argsort()[::-1] # Print the top 3 features for i in sorted_indices[:3]: print(f\\"Feature: {features[i]}, Mean Importance: {importance_means[i]:.3f}, Std Deviation: {importance_stds[i]:.3f}\\") ``` Feel free to use and modify the provided boilerplate code to complete the task.","solution":"def permutation_feature_importance(csv_path): import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance # Load the dataset data = pd.read_csv(csv_path) X = data.drop(columns=[\\"target\\"]) y = data[\\"target\\"] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model model = RandomForestRegressor(random_state=42) model.fit(X_train, y_train) # Compute permutation feature importance result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42) # Extract importance and identify top 3 features importance_means = result.importances_mean importance_stds = result.importances_std features = X.columns sorted_indices = importance_means.argsort()[::-1] # Print the top 3 features top_features = [] for i in sorted_indices[:3]: feature_info = f\\"Feature: {features[i]}, Mean Importance: {importance_means[i]:.3f}, Std Deviation: {importance_stds[i]:.3f}\\" top_features.append(feature_info) print(feature_info) return top_features"},{"question":"# Advanced Coding Assessment: Implementing and Utilizing Data Classes **Objective:** Demonstrate your understanding and ability to effectively use the dataclasses module in Python by designing and implementing a class using the provided functionalities. Use a mix of simple and advanced dataclass features to solve a real-world problem involving data manipulation and constraints. # Problem Statement You are required to implement a `BookShelf` class to manage a collection of book entries. Each book entry should be represented by a `Book` dataclass. The `Book` dataclass should include the following fields: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `pages`: An integer representing the number of pages in the book. - `publisher`: An optional string representing the publisher of the book (default value should be `None`). Additionally, the `BookShelf` class should provide methods to: 1. **Add a book**: Add a new `Book` instance to the shelf. 2. **Remove a book by title**: Remove a book from the shelf by its title. 3. **List all books**: Return a list of all books, presented as dictionaries. 4. **Search for books by author**: Return a list of books written by a specific author. 5. **Compute the total number of pages**: Calculate the total number of pages across all books on the shelf. Implement the following: 1. Define the `Book` dataclass. 2. Define the `BookShelf` class with the above methods. 3. Include comprehensive type annotations and ensure data integrity using appropriate constraints and error handling. **Constraints:** - Title and author names are non-empty strings. - The number of pages is a positive integer. - The list representation of books should be sorted alphabetically by title. **Sample Input and Output:** ```python # Sample usage of Book and BookShelf classes from dataclasses import dataclass, field from typing import List, Optional @dataclass class Book: # Define the fields title: str author: str pages: int publisher: Optional[str] = None class BookShelf: def __init__(self): self.books: List[Book] = [] def add_book(self, book: Book) -> None: # Add a book to the shelf pass def remove_book_by_title(self, title: str) -> None: # Remove a book by its title pass def list_books(self) -> List[dict]: # List all books as dictionaries pass def search_books_by_author(self, author: str) -> List[Book]: # Search for books by a specific author pass def total_pages(self) -> int: # Compute the total number of pages of all books pass # Example to demonstrate functionality bookshelf = BookShelf() book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", pages=328, publisher=\\"Secker & Warburg\\") book2 = Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", pages=281) bookshelf.add_book(book1) bookshelf.add_book(book2) print(bookshelf.list_books()) # Output: [{\'title\': \'1984\', \'author\': \'George Orwell\', \'pages\': 328, \'publisher\': \'Secker & Warburg\'}, # {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'pages\': 281, \'publisher\': None}] print(bookshelf.search_books_by_author(\\"George Orwell\\")) # Output: [Book(title=\'1984\', author=\'George Orwell\', pages=328, publisher=\'Secker & Warburg\')] print(bookshelf.total_pages()) # Output: 609 ``` # Requirements: - Proper use of `dataclasses`. - Ensuring data integrity with appropriate validation. - Demonstrate functionality through appropriate test cases.","solution":"from dataclasses import dataclass, field, asdict from typing import List, Optional @dataclass class Book: title: str author: str pages: int publisher: Optional[str] = None def __post_init__(self): if not self.title: raise ValueError(\\"Title cannot be empty\\") if not self.author: raise ValueError(\\"Author cannot be empty\\") if self.pages <= 0: raise ValueError(\\"Pages must be a positive integer\\") class BookShelf: def __init__(self): self.books: List[Book] = [] def add_book(self, book: Book) -> None: self.books.append(book) self.books.sort(key=lambda b: b.title) def remove_book_by_title(self, title: str) -> None: self.books = [book for book in self.books if book.title != title] def list_books(self) -> List[dict]: return [asdict(book) for book in self.books] def search_books_by_author(self, author: str) -> List[Book]: return [book for book in self.books if book.author == author] def total_pages(self) -> int: return sum(book.pages for book in self.books)"},{"question":"You are given a dataset in the form of a pandas DataFrame, `df`, containing sales data for a company. The DataFrame consists of the following columns: - `order_id`: Unique identifier for each order (int). - `product_id`: Unique identifier for each product (int). - `order_date`: The date when the order was placed (datetime). - `quantity`: The number of units ordered (int). - `price_per_unit`: The price per unit of the product (float). - `customer_id`: Unique identifier for each customer (int). The DataFrame may contain some missing values (`NaN`). You need to perform the following tasks: 1. **Memory Usage Calculation**: Write a function `calculate_memory_usage` to: - Compute and return the memory usage of `df`, excluding the index, and displaying the memory unit in human-readable format. 2. **Boolean Check**: Write a function `has_large_orders` to: - Check if any order in `df` has more than 100 units (`quantity > 100`) using boolean operations properly. - Return `True` if such orders exist, otherwise `False`. 3. **Missing Values Handling**: Write a function `handle_missing_values` to: - Identify columns with missing values. - If the column is of integer type, replace missing values with 0. - If the column is of float type, replace missing values with the mean of the column. - If the column is of any other type, remove rows containing missing values in that column. - Return the cleaned DataFrame. 4. **Discount Calculation Using UDF**: Write a function `apply_discounts` to: - Define a UDF that calculates a discount of 10% on orders with quantity greater than 50 units. - Apply this UDF to create a new column `discounted_total` reflecting the post-discount total order value. - Return the DataFrame with the new column added. # Input - Pandas DataFrame `df` # Output - For `calculate_memory_usage`: A string representing the memory used by `df` excluding the index, in human-readable format. - For `has_large_orders`: A boolean value indicating whether any order has more than 100 units. - For `handle_missing_values`: A cleaned pandas DataFrame with missing values handled as specified. - For `apply_discounts`: The pandas DataFrame `df` with an additional column `discounted_total`. ```python import pandas as pd def calculate_memory_usage(df): # Implement the logic to calculate memory usage excluding the index. pass def has_large_orders(df): # Implement the logic to check if any order has quantity > 100. pass def handle_missing_values(df): # Implement the logic to handle missing values as specified. pass def apply_discounts(df): # Implement the logic to apply discounts and add the new column. pass ``` # Constraints - Assume that `order_date` is pre-converted to datetime type. - Assume that columns might contain missing values at random. - Ensure the functions are optimized for performance as much as possible.","solution":"import pandas as pd def calculate_memory_usage(df): Compute and return the memory usage of df excluding the index, in human-readable format. mem_usage_bytes = df.memory_usage(deep=True).sum() for unit in [\'B\', \'KB\', \'MB\', \'GB\', \'TB\']: if mem_usage_bytes < 1024: return f\\"{mem_usage_bytes:.2f} {unit}\\" mem_usage_bytes /= 1024 def has_large_orders(df): Check if any order in df has more than 100 units (quantity > 100) using boolean operations. Returns True if such orders exist, otherwise False. return (df[\'quantity\'] > 100).any() def handle_missing_values(df): Handle missing values in each column of df: - Replace missing values in integer columns with 0. - Replace missing values in float columns with the mean of the column. - Remove rows containing missing values in other column types. Returns the cleaned DataFrame. for column in df.columns: if df[column].dtype == \'int64\': df[column].fillna(0, inplace=True) elif df[column].dtype == \'float64\': df[column].fillna(df[column].mean(), inplace=True) else: df.dropna(subset=[column], inplace=True) return df def apply_discounts(df): Define a UDF that calculates a discount of 10% on orders with quantity > 50 units. Apply this UDF to create a new column \'discounted_total\' reflecting the post-discount total order value. Returns the DataFrame with the new column added. def discount(price, quantity): if quantity > 50: return price * quantity * 0.9 return price * quantity df[\'discounted_total\'] = df.apply(lambda row: discount(row[\'price_per_unit\'], row[\'quantity\']), axis=1) return df"},{"question":"**Objective:** Demonstrate your understanding of pandas\' core functionalities including DataFrame/Series creation, data selection/manipulation, merging, and grouping operations. **Problem Statement:** You are given a dataset of employee information in two separate CSV files: `employees.csv` and `salaries.csv`. Your task is to perform various data manipulation and analysis tasks using pandas. 1. Read the `employees.csv` and `salaries.csv` files into pandas DataFrames named `df_employees` and `df_salaries`, respectively. 2. Merge the `df_employees` and `df_salaries` DataFrames on the `employee_id` column with an inner join, and store the result in a DataFrame named `df_merged`. 3. Add a new column named `annual_salary` to the `df_merged` DataFrame. The `annual_salary` should be calculated as the monthly salary from `df_salaries` multiplied by 12. 4. Group the merged DataFrame by the `department` column and calculate the following statistics for each department: - Total number of employees. - Average annual salary. - Maximum annual salary. - Minimum annual salary. - Store the results in a DataFrame named `df_summary`. 5. Identify departments that have fewer than 3 employees and remove these departments from the `df_summary` DataFrame. 6. Save the resulting summaries in a CSV file named `department_summary.csv`. **Constraints:** - Assume the `salaries.csv` file contains columns: `employee_id`, `monthly_salary`. - Assume the `employees.csv` file contains columns: `employee_id`, `name`, `age`, `department`. - All employee IDs are unique. - Monthly salaries are positive integers. **Function Signature:** ```python import pandas as pd def analyze_employee_data(emp_filepath: str, sal_filepath: str) -> None: # You can assume the file paths are correct pass ``` **Expected Output:** - A CSV file named `department_summary.csv` that contains the summarized statistics for each department (as described in step 4), excluding departments with fewer than 3 employees. **Example:** Given the following `employees.csv`: ``` employee_id,name,age,department 1,Alice,34,Engineering 2,Bob,28,Sales 3,Charlie,25,Engineering 4,David,45,HR 5,Eve,30,Engineering 6,Frank,29,Sales ``` And the following `salaries.csv`: ``` employee_id,monthly_salary 1,5000 2,4000 3,5500 4,3000 5,6000 6,4500 ``` `department_summary.csv` should contain: ``` department,num_employees,avg_annual_salary,max_annual_salary,min_annual_salary Engineering,3,66000,72000,60000 Sales,2,51000,54000,48000 ``` **Notes:** - Use pandas functions such as `merge`, `groupby`, `agg`, and file I/O operations like `pd.read_csv` and `DataFrame.to_csv`. - Ensure that your code is clean and well-documented.","solution":"import pandas as pd def analyze_employee_data(emp_filepath: str, sal_filepath: str) -> None: # Read the CSV files into pandas DataFrames df_employees = pd.read_csv(emp_filepath) df_salaries = pd.read_csv(sal_filepath) # Merge the DataFrames on the \'employee_id\' column df_merged = pd.merge(df_employees, df_salaries, on=\'employee_id\', how=\'inner\') # Add the annual_salary column df_merged[\'annual_salary\'] = df_merged[\'monthly_salary\'] * 12 # Group by department and calculate required statistics df_summary = df_merged.groupby(\'department\').agg( num_employees=pd.NamedAgg(column=\'employee_id\', aggfunc=\'size\'), avg_annual_salary=pd.NamedAgg(column=\'annual_salary\', aggfunc=\'mean\'), max_annual_salary=pd.NamedAgg(column=\'annual_salary\', aggfunc=\'max\'), min_annual_salary=pd.NamedAgg(column=\'annual_salary\', aggfunc=\'min\') ).reset_index() # Filter out departments with fewer than 3 employees df_summary = df_summary[df_summary[\'num_employees\'] >= 3] # Save the summary DataFrame to a CSV file df_summary.to_csv(\'department_summary.csv\', index=False)"},{"question":"You are given a dataset of students\' exam results, including their grades, subjects, and participation status. Your task is to write a series of functions to clean, analyze, and transform this data using pandas. Input The input dataset is in a CSV format with the following columns: - `StudentID`: Unique identifier for each student. - `Gender`: Gender of the student. - `ExamYear`: The year in which the student took the exam. - `Subject`: Subject in which the exam was taken. - `Participated`: \'yes\' if the student participated, \'no\' otherwise. - `Passed`: \'yes\' if the student passed the exam, \'no\' otherwise. - `Grade`: Numerical grade the student received. Example input: ```csv StudentID,Gender,ExamYear,Subject,Participated,Passed,Grade 1,M,2020,Math,yes,yes,85 2,F,2020,Math,no,no,0 3,M,2020,English,yes,yes,78 4,F,2019,Science,yes,yes,82 ... ``` Functions to Implement 1. **Data Cleaning**: Write a function `clean_data(df)` that: - Removes rows where `Participated` is \'no\'. - Fills missing values in the `Grade` column with the mean grade of the corresponding `Subject`. - Returns the cleaned DataFrame. 2. **Performance Analysis**: Write a function `exam_summary(df)` that: - Groups the data by `ExamYear` and `Subject`. - Computes the average grade, number of participants, and pass rate (percentage of students who passed) for each combination of `ExamYear` and `Subject`. - Returns a DataFrame with columns: `ExamYear`, `Subject`, `AverageGrade`, `ParticipantCount`, `PassRate`. 3. **Pivot Table**: Write a function `create_pivot(df)` that: - Creates a pivot table with `Gender` as the index and `ExamYear` as the columns. - Values should be `Grade` with the aggregation function being average. - Returns the resulting pivot table DataFrame. Constraints - You can assume the `Participated` column will only contain \'yes\' or \'no\'. - The `Passed` column will only contain \'yes\' or \'no\'. - Grades are numerical and can be missing. Example Function Signatures ```python import pandas as pd def clean_data(df: pd.DataFrame) -> pd.DataFrame: pass def exam_summary(df: pd.DataFrame) -> pd.DataFrame: pass def create_pivot(df: pd.DataFrame) -> pd.DataFrame: pass ``` Example Output Given the dataset provided, the functions should be able to output summarized results and a pivot table that accurately reflect the cleaned and analyzed data, following the specified requirements.","solution":"import pandas as pd def clean_data(df: pd.DataFrame) -> pd.DataFrame: # Remove rows where Participated is \'no\' df_cleaned = df[df[\'Participated\'] == \'yes\'].copy() # Fill missing values in the Grade column with the mean grade of the corresponding Subject mean_grades = df_cleaned.groupby(\'Subject\')[\'Grade\'].transform(lambda x: x.fillna(x.mean())) df_cleaned[\'Grade\'] = df_cleaned[\'Grade\'].fillna(mean_grades) return df_cleaned def exam_summary(df: pd.DataFrame) -> pd.DataFrame: summary = df.groupby([\'ExamYear\', \'Subject\']).agg( AverageGrade=(\'Grade\', \'mean\'), ParticipantCount=(\'StudentID\', \'count\'), PassRate=(\'Passed\', lambda x: (x == \'yes\').mean() * 100) ).reset_index() return summary def create_pivot(df: pd.DataFrame) -> pd.DataFrame: pivot_table = df.pivot_table( values=\'Grade\', index=\'Gender\', columns=\'ExamYear\', aggfunc=\'mean\' ) return pivot_table"},{"question":"<|Analysis Begin|> The provided documentation offers an in-depth exploration of the `Future` object within the `asyncio` package in Python 3.10. The `Future` object allows for interoperability between low-level callback-based code and high-level async/await structures, serving as an awaitable object which coroutines can await upon to obtain results or handle exceptions. Key components of the `Future` object include methods for setting and retrieving results (`set_result`, `result`), setting and checking exceptions (`set_exception`, `exception`), determining the state of the future (`done`, `cancelled`), and managing callbacks (`add_done_callback`, `remove_done_callback`). Additionally, it provides utilities like `asyncio.isfuture`, `asyncio.ensure_future`, and `asyncio.wrap_future` to wrap and ensure objects conform to the Future-like interface. The documentation also highlights some differences between `asyncio.Future` and `concurrent.futures.Future`, focusing on their synchronization, exception handling, and compatibility with synchronous and asynchronous constructs. <|Analysis End|> <|Question Begin|> **Question: Implementing and Utilizing Custom Future Objects** In this coding assessment, you will demonstrate your understanding of `asyncio.Future` by implementing a custom asynchronous task dispatcher using `asyncio` Futures. # Task: 1. **Create a custom Future-based async task dispatcher**: - Implement a `TaskDispatcher` class that: - Accepts asynchronous tasks and schedules them. - Uses Futures to allow other coroutines to await their completion. 2. **Implement the following methods within `TaskDispatcher`**: - `submit(self, coro: Awaitable) -> asyncio.Future` - Schedules the provided coroutine to run. - Returns an `asyncio.Future` object that will hold the result of the coroutine. - `run_tasks(self)` - Starts an event loop to run scheduled tasks. - Continues running until all submitted tasks are completed. # Constraints: - A coroutine should not be submitted or run more than once. - The dispatcher should remain thread-safe. # Performance requirements: - Efficient scheduling and handling of asynchronous tasks using `asyncio`. # Example Usage: ```python import asyncio from typing import Awaitable class TaskDispatcher: def __init__(self): self.tasks = [] self.loop = asyncio.get_event_loop() def submit(self, coro: Awaitable) -> asyncio.Future: fut = self.loop.create_future() self.tasks.append((coro, fut)) return fut async def run_task(self, coro: Awaitable, fut: asyncio.Future): try: result = await coro fut.set_result(result) except Exception as e: fut.set_exception(e) def run_tasks(self): async def main(): tasks_to_run = [self.run_task(coro, fut) for coro, fut in self.tasks] await asyncio.gather(*tasks_to_run) self.loop.run_until_complete(main()) # Example coroutines async def delay_task(seconds: int, result: str): await asyncio.sleep(seconds) return result # Running example dispatcher = TaskDispatcher() future1 = dispatcher.submit(delay_task(2, \'Result 1\')) future2 = dispatcher.submit(delay_task(1, \'Result 2\')) dispatcher.run_tasks() print(future1.result()) # Outputs: Result 1 print(future2.result()) # Outputs: Result 2 ``` In this example, the `TaskDispatcher` instance schedules two asynchronous tasks, runs them, and prints their results. Ensure your implementation handles exceptions within tasks and properly sets them on the corresponding Future object. **Hint:** For concurrency, consider the thread-safe initialization and management of shared data.","solution":"import asyncio from typing import Awaitable class TaskDispatcher: def __init__(self): self.tasks = [] self.loop = asyncio.get_event_loop() def submit(self, coro: Awaitable) -> asyncio.Future: fut = self.loop.create_future() self.tasks.append((coro, fut)) return fut async def run_task(self, coro: Awaitable, fut: asyncio.Future): try: result = await coro fut.set_result(result) except Exception as e: fut.set_exception(e) def run_tasks(self): async def main(): tasks_to_run = [self.run_task(coro, fut) for coro, fut in self.tasks] await asyncio.gather(*tasks_to_run) self.loop.run_until_complete(main()) async def delay_task(seconds: int, result: str): await asyncio.sleep(seconds) return result # Example usage dispatcher = TaskDispatcher() future1 = dispatcher.submit(delay_task(2, \'Result 1\')) future2 = dispatcher.submit(delay_task(1, \'Result 2\')) dispatcher.run_tasks() print(future1.result()) # Outputs: Result 1 print(future2.result()) # Outputs: Result 2"},{"question":"# Topological Sort with Cyclic Graph Handling Problem Statement You are given a set of tasks and their dependencies. Each task is represented as a node, and a dependency between two tasks (nodes) is represented as a directed edge from one node to another. Your goal is to implement a function that takes the task dependencies as input and returns a valid topological order of tasks if possible. If the tasks contain a cycle, the function should return the detected cycle. Input - A dictionary `graph` where keys are task identifiers (strings) and values are sets of dependencies (sets of strings). The graph represents the task dependency structure. Output - If a valid topological order exists, return a list of task identifiers in topological order. - If the graph contains a cycle, return a list representing the detected cycle. Constraints - Each task identifier is a hashable string. - The input graph may have up to (10^4) nodes and edges. - Performance should be efficient, aiming for O(V + E) complexity, where V is the number of vertices (tasks) and E is the number of edges (dependencies). Function Signature ```python def find_topological_order(graph: dict) -> list: pass ``` Example ```python # Example 1: Valid topological order graph = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"} } print(find_topological_order(graph)) # Output: [\'A\', \'B\', \'C\', \'D\'] or any correct topological order # Example 2: Graph with a cycle graph = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}, \\"A\\": {\\"D\\"} } print(find_topological_order(graph)) # Output: [\'D\', \'B\', \'A\', \'D\'] or any valid cycle ``` Notes - You must use the `TopologicalSorter` class from the `graphlib` module. - You can assume the input graph is non-empty. # Detailed Instructions 1. **Initialize** an instance of `TopologicalSorter` with the given graph. 2. **Handle** the `CycleError` exception to detect and return cycles. 3. **Utilize** the `static_order()` method if no cycles are present to get the topological order. 4. Return the appropriate result based on the presence or absence of cycles.","solution":"def find_topological_order(graph: dict) -> list: from graphlib import TopologicalSorter, CycleError sorter = TopologicalSorter(graph) try: order = list(sorter.static_order()) return order except CycleError as e: return list(e.args[1]) # return the detected cycle # Example usage if __name__ == \\"__main__\\": graph1 = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"} } print(find_topological_order(graph1)) # Expected: [\'A\', \'B\', \'C\', \'D\'] or any valid topological order graph2 = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}, \\"A\\": {\\"D\\"} } print(find_topological_order(graph2)) # Expected: [\'D\', \'B\', \'A\', \'D\'] or any valid cycle"},{"question":"**File System Metadata Analyzer** In this assessment, you are required to write a Python function that will analyze files in a given directory. Your task is to implement the following function: ```python def analyze_directory(directory_path: str) -> dict: Analyzes the directory specified by the directory_path and returns a summary of file types and permissions. Args: directory_path (str): The path to the directory to analyze. Returns: dict: A dictionary with the following structure: { \\"directories\\": int, # Number of directories \\"regular_files\\": int, # Number of regular files \\"symbolic_links\\": int, # Number of symbolic links \\"character_special_files\\": int, # Number of character special device files \\"block_special_files\\": int, # Number of block special device files \\"fifo_files\\": int, # Number of FIFO files \\"socket_files\\": int, # Number of socket files \\"door_files\\": int, # Number of door files \\"port_files\\": int, # Number of event port files \\"whiteout_files\\": int, # Number of whiteout files \\"permissions_distribution\\": dict # A dictionary with permission strings as the keys and their counts as values } import os import stat result = { \\"directories\\": 0, \\"regular_files\\": 0, \\"symbolic_links\\": 0, \\"character_special_files\\": 0, \\"block_special_files\\": 0, \\"fifo_files\\": 0, \\"socket_files\\": 0, \\"door_files\\": 0, \\"port_files\\": 0, \\"whiteout_files\\": 0, \\"permissions_distribution\\": {} } # Implement your solution here return result ``` **Requirements:** 1. The function should traverse the given directory and analyze each file inside it. 2. It should detect the type of each file and count how many of each type are found: - Directories - Regular files - Symbolic links - Character special devices - Block special devices - FIFO files - Sockets - Doors - Event ports - Whiteout files 3. For all files (except directories), the function should analyze their permissions and create a distribution dictionary where the keys are permission strings (e.g., \\"-rwxr-xr--\\") and the values are the counts of how often each permission string occurs. 4. Use the `stat` module to interpret file types and permissions. 5. You can assume the input directory exists and it read-only for simplicity. **Example Usage:** ```python result = analyze_directory(\\"/some/directory\\") print(result) ``` The output dictionary should have counts of each file type and distribution of file permissions in the specified directory. # Constraints: 1. Ensure you handle symbolic links properly and do not follow them to avoid infinite loops. 2. Subdirectories should be included in the traversal. # Performance Requirements: - Efficient traversal and analysis of the directory. - Proper handling of different file types using the `stat` module functions.","solution":"import os import stat def analyze_directory(directory_path: str) -> dict: Analyzes the directory specified by the directory_path and returns a summary of file types and permissions. Args: directory_path (str): The path to the directory to analyze. Returns: dict: A dictionary with various file type counts and permissions distribution. result = { \\"directories\\": 0, \\"regular_files\\": 0, \\"symbolic_links\\": 0, \\"character_special_files\\": 0, \\"block_special_files\\": 0, \\"fifo_files\\": 0, \\"socket_files\\": 0, \\"door_files\\": 0, \\"port_files\\": 0, \\"whiteout_files\\": 0, \\"permissions_distribution\\": {} } for root, dirs, files in os.walk(directory_path, followlinks=False): for name in dirs + files: path = os.path.join(root, name) try: st = os.lstat(path) if stat.S_ISDIR(st.st_mode): result[\\"directories\\"] += 1 elif stat.S_ISREG(st.st_mode): result[\\"regular_files\\"] += 1 elif stat.S_ISLNK(st.st_mode): result[\\"symbolic_links\\"] += 1 elif stat.S_ISCHR(st.st_mode): result[\\"character_special_files\\"] += 1 elif stat.S_ISBLK(st.st_mode): result[\\"block_special_files\\"] += 1 elif stat.S_ISFIFO(st.st_mode): result[\\"fifo_files\\"] += 1 elif stat.S_ISSOCK(st.st_mode): result[\\"socket_files\\"] += 1 elif stat.S_ISDOOR(st.st_mode): result[\\"door_files\\"] += 1 elif stat.S_ISPORT(st.st_mode): result[\\"port_files\\"] += 1 elif stat.S_ISWHT(st.st_mode): result[\\"whiteout_files\\"] += 1 if not stat.S_ISDIR(st.st_mode): permissions = stat.filemode(st.st_mode) if permissions not in result[\\"permissions_distribution\\"]: result[\\"permissions_distribution\\"][permissions] = 0 result[\\"permissions_distribution\\"][permissions] += 1 except Exception as e: # Handle any file access exceptions if necessary pass return result"},{"question":"Objective: Demonstrate your understanding of multiclass and multilabel classification problems using scikit-learn, particularly focusing on meta-estimators. Problem Statement: You are provided with a dataset containing text documents, each possibly associated with multiple topics. Your task is to implement a multi-label classification model using the OneVsRestClassifier strategy from scikit-learn. The model should be trained on a sample dataset and evaluated for its accuracy. Instructions: 1. Load the provided sample dataset. 2. Transform the textual data into a suitable numeric format for model training (Hint: Use TfidfVectorizer from sklearn). 3. Implement a OneVsRestClassifier with a base estimator of your choice. 4. Train the model on the training dataset. 5. Evaluate the model\'s performance on the test dataset using accuracy metrics. Dataset: A sample dataset in CSV format named `text_documents.csv` is provided. The dataset has two columns: `text` and `labels`, where `text` contains the textual data and `labels` contains a list of topics associated with the document (comma-separated). Example Input: ``` text_documents.csv: ------------------------------------------------- | text | labels | ------------------------------------------------- | \\"text data 1\\" | \\"topic1,topic2\\" | | \\"text data 2\\" | \\"topic3\\" | | \\"text data 3\\" | \\"topic1,topic3\\" | | \\"text data 4\\" | \\"topic2\\" | ------------------------------------------------- ``` Expected Output: 1. Print the accuracy of your model on the test dataset. 2. Print the classification report including precision, recall, and F1-score. Constraints: 1. You must use the OneVsRestClassifier for the multi-label classification. 2. The base estimator used in OneVsRestClassifier should be one of the classifiers mentioned in the documentation. Code Template: ```python import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.preprocessing import MultiLabelBinarizer from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report # Load the dataset data = pd.read_csv(\'text_documents.csv\') # Transform textual data to numeric data vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(data[\'text\']) # Transform labels to a suitable format for multi-label classification mlb = MultiLabelBinarizer() y = mlb.fit_transform(data[\'labels\'].str.split(\',\')) # Split the dataset into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Implement OneVsRestClassifier with a base estimator (Logistic Regression in this case) classifier = OneVsRestClassifier(LogisticRegression()) classifier.fit(X_train, y_train) # Evaluate the model\'s performance y_pred = classifier.predict(X_test) print(\\"Accuracy: \\", classifier.score(X_test, y_test)) print(\\"Classification Report:n\\", classification_report(y_test, y_pred, target_names=mlb.classes_)) # OPTIONAL: You can visualize the classification report if needed. ```","solution":"import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.preprocessing import MultiLabelBinarizer from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report def train_and_evaluate_model(csv_file_path): # Load the dataset data = pd.read_csv(csv_file_path) # Transform textual data to numeric data vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(data[\'text\']) # Transform labels to a suitable format for multi-label classification mlb = MultiLabelBinarizer() y = mlb.fit_transform(data[\'labels\'].str.split(\',\')) # Split the dataset into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Implement OneVsRestClassifier with a base estimator (Logistic Regression in this case) classifier = OneVsRestClassifier(LogisticRegression()) classifier.fit(X_train, y_train) # Evaluate the model\'s performance y_pred = classifier.predict(X_test) accuracy = classifier.score(X_test, y_test) report = classification_report(y_test, y_pred, target_names=mlb.classes_) return accuracy, report # Use the function to train the model and get the evaluation results # csv_file_path = \'path_to_your_text_documents.csv\' # accuracy, report = train_and_evaluate_model(csv_file_path) # print(\\"Accuracy: \\", accuracy) # print(\\"Classification Report:n\\", report)"},{"question":"**Objective**: Create a well-thought-out application utilizing pandas and its plotting functions to analyze and visualize a dataset. **Question**: You are provided with a dataset containing the daily stock prices of different companies over the past year. The dataset includes columns: `Date`, `Company`, `Open`, `High`, `Low`, `Close`, and `Volume`. Write a function `analyze_and_plot_stock_data(filepath: str)` which takes the file path of the dataset as input and performs the following tasks: 1. **Data Preparation**: - Read the CSV file into a pandas DataFrame. - Ensure the `Date` column is parsed as datetime. - Set the `Date` column as the index of the DataFrame. 2. **Data Manipulation**: - Calculate the daily change for each company\'s stock as a new column `Daily Change`, where `Daily Change = Close - Open`. - For each company, calculate the average closing price across the entire period and store these values in a dictionary with the company names as keys. 3. **Data Visualization**: - Generate a line plot for the daily closing prices of each company. - Create a scatter matrix plot for the `Open`, `High`, `Low`, and `Close` columns. 4. **Output**: - The function should return the dictionary containing the average closing prices. - Save the plots as images (`line_plot.png` and `scatter_matrix.png` respectively). **Input Format**: - `filepath`: A string representing the path to the CSV file. **Output Format**: - A dictionary where the keys are company names and values are the average closing prices for that company. **Constraints**: - You can assume the input file is well-formed and contains valid data. - Make sure the plots are properly labeled and titled for clarity. **Example**: Given a CSV file located at \\"data/stock_prices.csv\\" with the following structure: ``` Date,Company,Open,High,Low,Close,Volume 2023-01-01,CompanyA,100,105,98,102,15000 2023-01-01,CompanyB,150,155,147,152,20000 ... ``` The call `analyze_and_plot_stock_data(\\"data/stock_prices.csv\\")` might return: ```python { \'CompanyA\': 120.5, \'CompanyB\': 134.7, ... } ``` And save the plots as `line_plot.png` and `scatter_matrix.png`. **Performance Requirements**: - The function should be efficient in terms of both time and space complexity. - Aim to leverage pandas vectorized operations and efficient plotting methods. Feel free to discuss any assumptions you make or edge cases you consider in your implementation.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from pandas.plotting import scatter_matrix def analyze_and_plot_stock_data(filepath: str): Analyzes and plots stock data from the given CSV file. Arguments: filepath -- the path to the CSV file containing the stock data. Returns: A dictionary with company names as keys and their average closing prices as values. # Data Preparation df = pd.read_csv(filepath, parse_dates=[\'Date\']) df.set_index(\'Date\', inplace=True) # Data Manipulation df[\'Daily Change\'] = df[\'Close\'] - df[\'Open\'] avg_closing_prices = df.groupby(\'Company\')[\'Close\'].mean().to_dict() # Data Visualization plt.figure(figsize=(14, 7)) for company in df[\'Company\'].unique(): df_company = df[df[\'Company\'] == company] df_company[\'Close\'].plot(label=company) plt.title(\'Daily Closing Prices of Companies\') plt.xlabel(\'Date\') plt.ylabel(\'Closing Price\') plt.legend() plt.savefig(\'line_plot.png\') plt.figure(figsize=(14, 7)) scatter_data = df[[\'Open\', \'High\', \'Low\', \'Close\']] scatter_matrix(scatter_data, alpha=0.2, figsize=(12, 12), diagonal=\'kde\') plt.savefig(\'scatter_matrix.png\') return avg_closing_prices"},{"question":"You are required to implement a function that uses the `mmap` module to perform efficient read and write operations on a large text file. The function should search for a specific string within the file and replace it with another string. # Instructions: 1. **Function Signature**: ```python def replace_string_in_file(file_path: str, search_str: bytes, replace_str: bytes) -> None: ``` 2. **Inputs**: - `file_path` (str): The path to the text file to be modified. - `search_str` (bytes): The string to search for within the file (in bytes). - `replace_str` (bytes): The string to replace `search_str` with (in bytes). 3. **Constraints**: - You may assume that `replace_str` is always of the same length as `search_str`. - Handling of various access modes (e.g., `ACCESS_READ`, `ACCESS_WRITE`) should be taken into consideration. - The function should only modify the file if `search_str` is found. 4. **Output**: - The function does not return anything. It should perform the modifications in place. 5. **Example**: Suppose `example.txt` contains: ``` Hello, this is a test file. This file is for testing the mmap module. ``` Calling `replace_string_in_file(\'example.txt\', b\'test\', b\'exam\')` should modify the file to: ``` Hello, this is a exam file. This file is for examing the mmap module. ``` # Requirements: - You must use Python\'s `mmap` module to perform the file operations. - Ensure the file is properly closed after operations to avoid file corruption. - Handle exceptions appropriately to avoid crashes during file operations. # Hints: - Utilize the `find()` method of the `mmap` object to locate the string. - Use slice notation for replacing the content. - Remember to flush modifications back to the file. # Constraints: - Ensure efficient memory usage, especially for large files. - Assume the file size does not exceed available memory.","solution":"import mmap import os def replace_string_in_file(file_path: str, search_str: bytes, replace_str: bytes) -> None: Function to replace a specific string with another string within a large text file using mmap module. Params: file_path (str): The path to the file to be modified. search_str (bytes): The string to search for within the file (in bytes). replace_str (bytes): The string to replace search_str with (in bytes). Assumptions: - replace_str and search_str are of equal length. file_size = os.path.getsize(file_path) with open(file_path, \'r+b\') as f: with mmap.mmap(f.fileno(), file_size, access=mmap.ACCESS_WRITE) as mm: pos = mm.find(search_str) while pos != -1: mm[pos:pos+len(search_str)] = replace_str pos = mm.find(search_str, pos + len(replace_str)) mm.flush()"},{"question":"# Seaborn Coding Assessment Question Objective: The objective of this question is to assess the student\'s understanding of working with color palettes in seaborn, applying them to visualizations, and retrieving color values. Question: 1. **Retrieve and Customize Palettes:** - Write a function `retrieve_palette(palette_name, n_colors=None, as_cmap=False)` that takes the following parameters: - `palette_name` (str): A string name of the palette to retrieve. Possible values include \\"default\\", \\"pastel\\", \\"husl\\", \\"Set2\\", \\"Spectral\\", \\"flare\\", etc. - `n_colors` (int, optional): The number of colors to return in the palette (use only for specific discrete palettes like \\"husl\\"). Default is `None`. - `as_cmap` (bool, optional): If `True`, return the palette as a continuous colormap. Default is `False`. - The function should return the requested color palette. 2. **Hex Code Representation:** - Write a function `palette_to_hex(palette)` that takes a palette returned by `retrieve_palette` and converts it to hex codes. The function should return a list of hex color codes. 3. **Visualize with Context Manager:** - Write a function `visualize_with_palette(palette_name)` that: - Uses a context manager to set the given `palette_name` as the color palette. - Creates a seaborn scatter plot (`sns.relplot`) with x values as `range(10)`, y values as `[0]*10`, and uses a unique hue for each point. - Displays the plot. Constraints: - The `retrieve_palette` function should raise appropriate errors for invalid palette names. - Handle cases where `n_colors` is provided but is not applicable to the chosen palette. - Ensure that `as_cmap` is only applied to valid palettes. Example: ```python # Sample function calls and expected output: # Retrieve a specific palette with hex codes palette = retrieve_palette(\\"husl\\", 5) hex_codes = palette_to_hex(palette) print(hex_codes) # Output: [\'#f77189\', \'#bddf26\', \'#f68d3e\', \'#5eb871\', \'#8190ff\'] # Visualize using a palette with context manager visualize_with_palette(\\"Set3\\") # Output: A scatter plot with colors from the Set3 palette ``` Input: - `palette_name` is a string, e.g., \\"pastel\\", \\"Set2\\", etc. - `n_colors` is an integer (optional). - `as_cmap` is a boolean (optional). Output: - `retrieve_palette` returns a seaborn color palette or colormap. - `palette_to_hex` returns a list of hex color codes. - `visualize_with_palette` displays a scatter plot with the specified color palette.","solution":"import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import to_hex def retrieve_palette(palette_name, n_colors=None, as_cmap=False): Retrieve a seaborn color palette or colormap. Parameters: - palette_name (str): Name of the palette to retrieve. - n_colors (int, optional): Number of colors to return. Default is None. - as_cmap (bool, optional): If True, return as a continuous colormap. Default is False. Returns: - A seaborn color palette or colormap. if as_cmap: try: return sns.color_palette(palette_name, as_cmap=True) except (ValueError, TypeError) as e: raise ValueError(\\"Invalid palette name for continuous colormap\\") try: if n_colors is not None: return sns.color_palette(palette_name, n_colors) else: return sns.color_palette(palette_name) except (ValueError, TypeError) as e: raise ValueError(\\"Invalid palette name or n_colors\\") def palette_to_hex(palette): Convert a seaborn palette to hex color codes. Parameters: - palette: A seaborn color palette. Returns: - List of hex color codes. return [to_hex(color) for color in palette] def visualize_with_palette(palette_name): Visualize a seaborn scatter plot with the specified color palette. Parameters: - palette_name (str): Name of the palette to set. with sns.color_palette(palette_name): sns.relplot(x=range(10), y=[0] * 10, hue=range(10), palette=palette_name, kind=\'scatter\') plt.show()"},{"question":"# Event Scheduling System You are tasked to implement a small event scheduling system using Python\'s `sched` module. The system should be able to schedule and run events based on specified times and should be capable of maintaining the order of execution even with varying priorities and delays. Requirements: 1. **Function `schedule_events(event_list)`**: - **Input**: A list of dictionaries where each dictionary represents an event with the following keys: - `time`: Absolute time (Unix timestamp). - `priority`: An integer representing priority (lower number means higher priority). - `action`: A string indicating the action to be performed (print the string as action). - `delay`: Delay in seconds for the event from the current time (should be ignored if `time` is specified). - **Output**: A list of executed actions in the order they were executed. 2. **Function `cancel_event(action)`**: - **Input**: A string `action` which is the action of the event to be canceled. - **Output**: Boolean indicating if the event was successfully canceled. 3. **Constraints**: - If both `time` and `delay` are specified, `time` should take precedence. - The `delay` for scheduling should be applied from the time when `schedule_events` is called. - Events with the same time should be executed based on priority. 4. **[Optional]** Performance Requirement: The system should handle up to 1000 events efficiently. Example: ```python import time import random def example_usage(): event_list = [ {\\"time\\": None, \\"priority\\": 1, \\"action\\": f\\"Action {i}\\", \\"delay\\": random.randint(1, 5)} for i in range(1, 6) ] executed_actions = schedule_events(event_list) print(executed_actions) if __name__ == \\"__main__\\": example_usage() ``` Implement the functions `schedule_events` and `cancel_event`.","solution":"import sched import time scheduler = sched.scheduler(time.time, time.sleep) scheduled_events = {} # To keep track of events def schedule_events(event_list): Schedule events based on the given list. Parameters: event_list (list): List of dictionaries where each dictionary contains: - \\"time\\": Absolute time (Unix timestamp). - \\"priority\\": Integer representing priority. - \\"action\\": String indicating the action. - \\"delay\\": Delay in seconds from current time. Returns: list: List of executed actions. executed_actions = [] def action_wrapper(action): executed_actions.append(action) print(action) global scheduled_events scheduled_events = {} scheduler.empty() # Clear previous events for event in event_list: time_value = event[\\"time\\"] delay = event[\\"delay\\"] action = event[\\"action\\"] priority = event[\\"priority\\"] current_time = time.time() if time_value is not None: delay = max(0, time_value - current_time) event_id = scheduler.enter(delay, priority, action_wrapper, argument=(action,)) scheduled_events[action] = event_id scheduler.run() return executed_actions def cancel_event(action): Cancel scheduled event based on action. Parameters: action (str): Action string to identify the event to be canceled. Returns: bool: True if the event was canceled successfully, False otherwise. global scheduled_events if action in scheduled_events: event_id = scheduled_events.pop(action) try: scheduler.cancel(event_id) return True except ValueError: pass return False"},{"question":"**Objective:** You are required to write a program that efficiently manages a list of student records, maintaining them in sorted order based on their scores. The program should handle adding new student records, removing existing records by student ID, and retrieving students by score range. **Problem Statement:** 1. Implement the following functions using the `bisect` module: - `add_student(students, student)`: Add a new student record to the `students` list while maintaining it in sorted order based on their scores. Assume `students` is a list of tuples, where each tuple contains `(student_id, student_name, score)`. - `remove_student(students, student_id)`: Remove a student record from the `students` list based on the student ID. - `get_students_by_score_range(students, low_score, high_score)`: Retrieve a list of student records whose scores fall within the inclusive range `[low_score, high_score]`. **Input and Output Format:** - `add_student(students, student)`: - **Input**: `students` (List[Tuple[int, str, float]]), a sorted list of tuples where each tuple is of the form `(student_id, student_name, score)`. - **student**: Tuple[int, str, float], a single student record to be added to the list. - **Output**: None. The function modifies the `students` list in-place. - `remove_student(students, student_id)`: - **Input**: `students` (List[Tuple[int, str, float]]), a sorted list of tuples where each tuple is of the form `(student_id, student_name, score)`. - **student_id**: int, the ID of the student to be removed. - **Output**: None. The function modifies the `students` list in-place. - `get_students_by_score_range(students, low_score, high_score)`: - **Input**: `students` (List[Tuple[int, str, float]]), a sorted list of tuples where each tuple is of the form `(student_id, student_name, score)`. - **low_score**: float, the lower bound of the score range. - **high_score**: float, the upper bound of the score range. - **Output**: List[Tuple[int, str, float]], a list of student records within the specified score range. **Example:** ```python students = [ (1, \\"Alice\\", 85.0), (2, \\"Bob\\", 92.0), (3, \\"Charlie\\", 78.0) ] add_student(students, (4, \\"David\\", 88.0)) # students => [(3, \\"Charlie\\", 78.0), (1, \\"Alice\\", 85.0), (4, \\"David\\", 88.0), (2, \\"Bob\\", 92.0)] remove_student(students, 3) # students => [(1, \\"Alice\\", 85.0), (4, \\"David\\", 88.0), (2, \\"Bob\\", 92.0)] result = get_students_by_score_range(students, 84.0, 90.0) # result => [(1, \\"Alice\\", 85.0), (4, \\"David\\", 88.0)] ``` **Constraints:** - All student IDs are unique. - The `students` list is initially in sorted order based on scores. - Inserting and removing students should maintain the sorted order. **Performance Requirements:** The `add_student` and `remove_student` functions should be optimized for efficiency using the `bisect` module, adhering to the provided performance insights.","solution":"from bisect import bisect_left, bisect_right def add_student(students, student): Add a new student record to the students list while maintaining it in sorted order based on their scores. student_id, student_name, score = student index = bisect_right(students, (score, student_id)) students.insert(index, (score, student_id, student_name)) def remove_student(students, student_id): Remove a student record from the students list based on the student ID. for i, (score, sid, student_name) in enumerate(students): if sid == student_id: del students[i] break def get_students_by_score_range(students, low_score, high_score): Retrieve a list of student records whose scores fall within the inclusive range [low_score, high_score]. low = bisect_left(students, (low_score, float(\'-inf\'))) high = bisect_right(students, (high_score, float(\'inf\'))) return students[low:high]"},{"question":"# Question: Array Operations and Manipulation in Python Objective: Demonstrate your understanding of the Python `array` module by implementing several functions to manipulate and convert arrays. Problem Statement: You are required to implement a class `ArrayManipulator` that will handle several array operations using the methods provided by the Python `array` module. The class should support creating arrays, adding elements, reversing the array, and converting arrays to different formats. Class Definition: ```python from array import array class ArrayManipulator: def __init__(self, typecode, initializer=None): Initialize an array with the given typecode and optional initializer. :param typecode: A string representing the typecode (e.g., \'i\' for integers). :param initializer: An optional list or iterable containing initial values. self.my_array = array(typecode, initializer if initializer is not None else []) def add_element(self, value): Append a new element to the array. :param value: The value to be added to the array. self.my_array.append(value) def reverse_array(self): Reverse the order of the elements in the array. self.my_array.reverse() def to_list(self): Convert the array to a list and return it. :returns: A list containing the same elements as the array. return self.my_array.tolist() def from_list(self, input_list): Add elements from an input list to the array. :param input_list: A list of elements to be added to the array. self.my_array.fromlist(input_list) def array_to_bytes(self): Convert the array to a bytes object and return it. :returns: A bytes object representing the array data. return self.my_array.tobytes() def bytes_to_array(self, byte_data): Initialize the array with bytes data. :param byte_data: A bytes object with data to initialize the array. self.my_array.frombytes(byte_data) # You can add more methods if needed. ``` Input: - Initialization of `ArrayManipulator` with type code and optional initial values. - Various method calls to manipulate the array. Output: - The resulting array or the converted format (list or bytes) after performing the requested operations. Example Usage: ```python # Create an instance with integer typecode am = ArrayManipulator(\'i\', [1, 2, 3]) # Add elements to the array am.add_element(4) am.add_element(5) # Reverse the array am.reverse_array() # Convert to list list_version = am.to_list() print(list_version) # Output: [5, 4, 3, 2, 1] # Convert to bytes bytes_version = am.array_to_bytes() print(bytes_version) # Outputs byte representation of the array # Reinitialize array from bytes am.bytes_to_array(bytes_version) print(am.to_list()) # Output: [5, 4, 3, 2, 1] ``` Constraints: - Only the type codes provided in the documentation should be used. - Ensure that invalid operations (like adding an incorrect data type) raise appropriate errors. - You can assume that the `initializer`, if provided, is always a valid list or iterable. Implement the `ArrayManipulator` class and test it with various scenarios to ensure all methods work as expected.","solution":"from array import array class ArrayManipulator: def __init__(self, typecode, initializer=None): Initialize an array with the given typecode and optional initializer. :param typecode: A string representing the typecode (e.g., \'i\' for integers). :param initializer: An optional list or iterable containing initial values. self.my_array = array(typecode, initializer if initializer is not None else []) def add_element(self, value): Append a new element to the array. :param value: The value to be added to the array. self.my_array.append(value) def reverse_array(self): Reverse the order of the elements in the array. self.my_array.reverse() def to_list(self): Convert the array to a list and return it. :returns: A list containing the same elements as the array. return self.my_array.tolist() def from_list(self, input_list): Add elements from an input list to the array. :param input_list: A list of elements to be added to the array. self.my_array.fromlist(input_list) def array_to_bytes(self): Convert the array to a bytes object and return it. :returns: A bytes object representing the array data. return self.my_array.tobytes() def bytes_to_array(self, byte_data): Initialize the array with bytes data. :param byte_data: A bytes object with data to initialize the array. self.my_array.frombytes(byte_data)"},{"question":"# Python Module Import Function Analysis You are working with a legacy Python application that integrates C extensions. To facilitate better module management and debugging, you need to analyze and utilize the `PyImport_*` functions effectively. Your task is to create a Python function that mimics the behavior of these C functions. Specifically, you will implement a function using Python’s built-in capabilities to import a module, reload it if necessary, and handle nested imports appropriately. Task Write a Python function `import_module_chain(module_name: str, reload=False) -> object` that: 1. Imports the module specified by `module_name`, which can include nested imports in the format `package.subpackage.module`. 2. If `reload` is set to `True`, reloads the module using the standard library’s `importlib`. 3. Returns the imported (or reloaded) module object. 4. Handles exceptions by printing a relevant error message and returning `None`. Input - `module_name` (str): The name of the module to import. - `reload` (bool): A flag indicating whether to reload the module if it is already imported. Output - Returns the imported or reloaded module object on success, or `None` on failure. Constraints - The function should handle nested module imports correctly, ensuring the full module path is imported. - Use standard library `importlib` for reloading modules. Example ```python mod = import_module_chain(\\"os.path\\", reload=False) print(mod) mod = import_module_chain(\\"json\\", reload=True) print(mod) ``` Implementation ```python import importlib import sys def import_module_chain(module_name: str, reload=False) -> object: Import or reload a module given its name. Args: module_name (str): The name of the module to import. reload (bool): Whether to reload the module if it is already imported. Returns: object: The imported or reloaded module, or None if an error occurred. try: # Import the module module = importlib.import_module(module_name) # Reload the module if reload flag is True if reload: module = importlib.reload(module) return module except ModuleNotFoundError: print(f\\"Module \'{module_name}\' not found.\\") except Exception as e: print(f\\"An error occurred while importing module \'{module_name}\': {e}\\") return None # Test cases if __name__ == \'__main__\': mod1 = import_module_chain(\\"os.path\\", reload=False) print(mod1) mod2 = import_module_chain(\\"json\\", reload=True) print(mod2) ``` Notes - The function uses `importlib.import_module` to import the module and `importlib.reload` to reload it if necessary. - It handles exceptions by printing an error message and returning `None`.","solution":"import importlib def import_module_chain(module_name: str, reload=False) -> object: Import or reload a module given its name. Args: module_name (str): The name of the module to import. reload (bool): Whether to reload the module if it is already imported. Returns: object: The imported or reloaded module, or None if an error occurred. try: # Import the module module = importlib.import_module(module_name) # Reload the module if reload flag is True if reload: module = importlib.reload(module) return module except ModuleNotFoundError: print(f\\"Module \'{module_name}\' not found.\\") except Exception as e: print(f\\"An error occurred while importing module \'{module_name}\': {e}\\") return None"},{"question":"Coding Assessment Question # Objective Write a Python program that simulates a simple task management system using asyncio. Your program should create multiple tasks, manage their execution with timeouts, and use a FIFO queue to distribute tasks among workers. # Requirements 1. **Task Creation**: Implement a function `create_task` that will generate a task that performs a simple action (e.g., printing a message followed by a delay). 2. **Task Management**: Implement a function `task_manager` that will create a number of tasks, start them, and wait for their completion with a specified timeout. 3. **Queue Implementation**: Implement a function `distribute_tasks` that will create a FIFO queue, add tasks to the queue, and use workers to process the tasks from the queue concurrently. 4. **Synchronization**: Implement proper synchronization to ensure tasks are processed in order and handle any possible exceptions. # Functions 1. **`async def create_task(task_id: int) -> None`** - **Input**: `task_id` (integer) indicating the ID of the task. - **Output**: None. - **Behavior**: Prints a message indicating the start of the task, sleeps for a random duration between 1 to 5 seconds, then prints a message indicating the completion of the task. 2. **`async def task_manager(num_tasks: int, timeout: int) -> None`** - **Input**: `num_tasks` (integer) indicating the number of tasks to create, `timeout` (integer) specifying the timeout duration in seconds. - **Output**: None. - **Behavior**: Create `num_tasks` tasks using `create_task`, and use `asyncio.gather` to run these tasks concurrently with the specified timeout. 3. **`async def distribute_tasks(num_tasks: int, num_workers: int) -> None`** - **Input**: `num_tasks` (integer) indicating the number of tasks to create, `num_workers` (integer) specifying the number of worker coroutines. - **Output**: None. - **Behavior**: Create a FIFO queue and enqueue `num_tasks` tasks. Implement worker coroutines that dequeue and process tasks concurrently. 4. **`async def worker(queue: asyncio.Queue) -> None`** - **Input**: `queue` (asyncio.Queue) containing tasks to process. - **Output**: None. - **Behavior**: Continuously fetch tasks from the queue and execute them using `create_task` function. # Example Output ``` Task 1 started. Task 2 started. ... Task 1 completed. Task 2 completed. ... ``` # Constraints - You must use asyncio to manage concurrency. - Each worker should process tasks from the queue until all tasks are completed. - Ensure proper error handling to manage any task cancellations or timeouts. # Performance Requirements - The solution should efficiently manage the creation and completion of multiple tasks concurrently. - The implementation should handle up to 100 tasks and 10 workers without significant performance degradation.","solution":"import asyncio import random async def create_task(task_id: int) -> None: Creates a task that prints a start message, waits for a random duration, and then prints a completion message. print(f\\"Task {task_id} started.\\") await asyncio.sleep(random.randint(1, 5)) print(f\\"Task {task_id} completed.\\") async def task_manager(num_tasks: int, timeout: int) -> None: Manages and runs a specified number of tasks concurrently with a timeout. tasks = [create_task(i) for i in range(num_tasks)] await asyncio.gather(*tasks, return_exceptions=True) async def worker(queue: asyncio.Queue) -> None: Worker coroutine that processes tasks from a queue. while True: task_id = await queue.get() await create_task(task_id) queue.task_done() async def distribute_tasks(num_tasks: int, num_workers: int) -> None: Distributes tasks among workers using a FIFO queue. queue = asyncio.Queue() workers = [asyncio.create_task(worker(queue)) for _ in range(num_workers)] for task_id in range(num_tasks): await queue.put(task_id) await queue.join() # Wait until all tasks are processed for w in workers: w.cancel() # Cancel the workers after the queue is empty def main(): asyncio.run(task_manager(10, 5)) asyncio.run(distribute_tasks(10, 3)) if __name__ == \\"__main__\\": main()"},{"question":"**Question:** Python Development Mode and Resource Management **Objective:** Write a Python script that reads and processes data from a file specified via command-line arguments. Your goal is to ensure the script runs without any ResourceWarnings and correctly handles resource management. **Problem Statement:** 1. Write a Python function `read_and_process_file(filename: str) -> int` that: - Opens the file specified by `filename`. - Counts the number of non-empty lines in the file. - Returns the count of non-empty lines. 2. Write a main script that: - Accepts a filename as a command-line argument. - Calls the `read_and_process_file` function. - Prints the result returned by the function. 3. Ensure that your script closes all opened resources properly and does not produce any ResourceWarnings when run with Python Development Mode enabled. **Input:** - The script should take one command-line argument: - `filename` (str): the name of the file to process. **Output:** - Print the number of non-empty lines in the file specified by `filename`. **Constraints:** - You must use proper resource management techniques (such as context managers) to ensure the file is properly closed. - The function should handle potential I/O errors gracefully. **Example:** ```python # Assume the file \'data.txt\' contains the following lines: # Line 1: \\"Hello World\\" # Line 2: \\" \\" # Line 3: \\"Python-310\\" # Line 4: \\"\\" # Running the script: # python script.py data.txt # Should output: # 2 def read_and_process_file(filename: str) -> int: # Implement your solution here pass if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python script.py <filename>\\") else: filename = sys.argv[1] try: result = read_and_process_file(filename) print(result) except Exception as e: print(f\\"An error occurred: {e}\\") ``` **Additional Requirement:** Run the script using Python Development Mode to ensure it does not generate any ResourceWarnings: ```sh python3 -X dev script.py <filename> ``` **Evaluation Criteria:** - Correct implementation of file reading and processing. - Proper use of resource management to avoid ResourceWarnings. - Handling of I/O errors. - Adherence to input and output specifications.","solution":"def read_and_process_file(filename: str) -> int: Reads the file and counts the number of non-empty lines. Args: filename (str): The name of the file to read. Returns: int: The count of non-empty lines. try: with open(filename, \'r\') as file: non_empty_lines = [line for line in file if line.strip()] return len(non_empty_lines) except IOError as e: print(f\\"An error occurred: {e}\\") return 0 if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python script.py <filename>\\") else: filename = sys.argv[1] result = read_and_process_file(filename) print(result)"},{"question":"# Python Coding Assessment: Manage Subprocesses **Objective**: Implement a Python function that demonstrates the use of `subprocess.run` and `subprocess.Popen` to run shell commands, capture their outputs, handle errors, and manage timeouts. **Problem Statement**: Write a function `execute_commands(commands: list, timeout: int) -> dict` that takes a list of shell commands and a timeout value in seconds. The function should: 1. Execute each command using the `subprocess.run` function if no output redirection is needed. Capture both the standard output and error. 2. If a command requires output redirection (indicated by a `>` symbol), execute it using `subprocess.Popen`. 3. Handle any exceptions that arise due to non-zero exit codes, timeouts, or file not found errors. 4. Return a dictionary where the keys are the commands and the values are tuples containing: - Return code - Standard output - Standard error - Exception message (if any) **Function Signature**: ```python def execute_commands(commands: list, timeout: int) -> dict: pass ``` **Constraints**: - Commands will be simple shell commands (e.g., `ls -l`, `echo \'Hello World\'`, `cat nonexistentfile`). - Commands with output redirection will use the `>` symbol to redirect standard output to a file (e.g., `echo \'Hello World\' > output.txt`). - Timeout will be a positive integer specifying the maximum time (in seconds) allowed for each command to execute. **Example**: ```python commands = [ \\"echo \'Hello World\'\\", \\"ls -l\\", \\"cat nonexistentfile\\", \\"echo \'This will be saved to a file\' > output.txt\\" ] timeout = 5 result = execute_commands(commands, timeout) ``` **Expected Output**: ```python { \\"echo \'Hello World\'\\": (0, \\"Hello Worldn\\", \\"\\", \\"\\"), \\"ls -l\\": (0, \\"...\\", \\"\\", \\"\\"), \\"cat nonexistentfile\\": (1, \\"\\", \\"cat: nonexistentfile: No such file or directoryn\\", \\"\\"), \\"echo \'This will be saved to a file\' > output.txt\\": (0, \\"\\", \\"\\", \\"\\") } ``` **Notes**: - For commands that use output redirection, assume the redirection is always to a file in the current directory. - The standard output and error captured should be as strings. - Handle exceptions gracefully and include any exception messages in the output dictionary.","solution":"import subprocess import shlex def execute_commands(commands: list, timeout: int) -> dict: results = {} for command in commands: try: if \'>\' in command: # Splitting the command for proper redirection handling cmd_parts = shlex.split(command) redir_index = cmd_parts.index(\'>\') cmd = cmd_parts[:redir_index] output_file = cmd_parts[redir_index + 1] with open(output_file, \'w\') as outfile: process = subprocess.Popen(cmd, stdout=outfile, stderr=subprocess.PIPE) _, stderr = process.communicate(timeout=timeout) results[command] = (process.returncode, \\"\\", stderr.decode(), \\"\\") else: result = subprocess.run(shlex.split(command), capture_output=True, text=True, timeout=timeout) results[command] = (result.returncode, result.stdout, result.stderr, \\"\\") except subprocess.CalledProcessError as e: results[command] = (e.returncode, e.stdout, e.stderr, str(e)) except subprocess.TimeoutExpired as e: results[command] = (-1, \\"\\", \\"\\", \\"TimeoutExpired: \\" + str(e)) except FileNotFoundError as e: results[command] = (-1, \\"\\", \\"\\", \\"FileNotFoundError: \\" + str(e)) except Exception as e: results[command] = (-1, \\"\\", \\"\\", \\"Exception: \\" + str(e)) return results"},{"question":"Objective: You are required to implement a Python function that utilizes DTrace/SystemTap markers to monitor and report the execution of specific Python functions in a script. The function should produce an execution call hierarchy report similar to the examples provided in the documentation. Requirements: 1. Implement a Python function `trace_python_script` that: - Accepts the path to a Python script. - Uses the appropriate DTrace/SystemTap commands to trace the execution of all pure-Python functions in the script. - Produces a detailed hierarchy report showing the function call and return sequences along with their respective timestamps. 2. Make sure the function works in both macOS (using DTrace) and Linux (using SystemTap). 3. The report should display: - Timestamp of the function entry. - Function name and line number. - Indentation to indicate call depth. - Timestamp of the function return. 4. Provide error handling for unsupported operating systems or if the DTrace/SystemTap tools are not installed. Expected Input and Output: - **Input**: Path to a Python script (string). - **Output**: Execution call hierarchy printed to standard output (console). Constraints: - Ensure your implementation works efficiently for scripts with large call hierarchies. - You may assume that the Python script files being monitored do not have any syntax errors. - Your solution should not alter the original Python script. Performance Requirements: - Your solution should not introduce significant overhead in the execution of the monitored Python script. Example: For a Python script `example.py` with the following content: ```python def func1(): func2() func3() def func2(): pass def func3(): func4() def func4(): pass if __name__ == \\"__main__\\": func1() ``` When the `trace_python_script` function is called with the path to `example.py`, the expected console output should look similar to: ``` 156641360502280 function-entry:example.py:func1:2 156641360518804 function-entry: example.py:func2:5 156641360546807 function-return: example.py:func2:6 156641360563367 function-entry: example.py:func3:8 156641360591757 function-entry: example.py:func4:11 156641360613100 function-return: example.py:func4:12 156641360642285 function-return: example.py:func3:9 156641360656770 function-return: example.py:func1:3 ``` You may include additional helper functions and data structures to achieve the solution.","solution":"import os import platform import subprocess from datetime import datetime def trace_python_script(script_path): Traces the execution of all pure-Python functions in a script and prints a call hierarchy with timestamps. if platform.system() == \'Darwin\': dtrace_script = f #!/usr/sbin/dtrace -s pidtarget:::entry /execname == \\"{os.path.basename(script_path)}\\" && probefunc == \\"PyEval_EvalFrameEx\\"/ {{ printf(\\"%d function-entry:%s:%s:%dn\\", walltimestamp, copyinstr(arg1), copyinstr(addr ? arg : \\"-\\"), lineno); }} pidtarget:::return /execname == \\"{os.path.basename(script_path)}\\" && probefunc == \\"PyEval_EvalFrameEx\\"/ {{ printf(\\"%d function-return:%s:%s:%dn\\", walltimestamp, copyinstr(arg1), copyinstr(addr ? arg : \\"-\\"), lineno); }} dtrace_command = [\'sudo\', \'dtrace\', \'-q\', \'-w\', \'-s\', \'/dev/stdin\', script_path] proc = subprocess.Popen(dtrace_command, stdin=subprocess.PIPE, text=True) proc.communicate(input=dtrace_script) elif platform.system() == \'Linux\': systemtap_script = f global depth probe process(\\"{script_path}\\").function(\\"do_call\\") {{ entry = strftime(\\"%H:%M:%S.%N\\", gettimeofday_s()) . \\" function-entry:\\" . probefunc() . \\":\\" . @record.return_lineno printf(\\"%sn\\", entry) depth++ }} probe process(\\"{script_path}\\").function(\\"do_call\\").return {{ depth-- exit = strftime(\\"%H:%M:%S.%N\\", gettimeofday_s()) . \\" function-return:\\" . probefunc() . \\":\\" . @record.return_lineno printf(\\"%sn\\", exit) }} with open(\'/tmp/systemtap_script.stp\', \'w\') as f: f.write(systemtap_script) systemtap_command = [\'sudo\', \'stap\', \'/tmp/systemtap_script.stp\'] proc = subprocess.Popen(systemtap_command) proc.communicate() else: raise OSError(\\"Only macOS (DTrace) and Linux (SystemTap) operating systems are supported.\\")"},{"question":"You are asked to design a Python class that utilizes the `py_function` C extension functions discussed in the provided documentation. Your task is to implement a class `FunctionManipulator` that allows programmatic creation and manipulation of Python functions. The class should expose the following methods: 1. `create_function(code_str: str, globals_dict: dict, qualname: Optional[str] = None) -> types.FunctionType`: Creates a new function object from a string of Python code and a dictionary of global variables. The qualified name is optional. 2. `get_code(func: types.FunctionType) -> types.CodeType`: Retrieves and returns the code object associated with the function. 3. `get_globals(func: types.FunctionType) -> dict`: Retrieves and returns the globals dictionary associated with the function. 4. `get_module(func: types.FunctionType) -> Optional[str]`: Retrieves and returns the module name associated with the function. 5. `get_defaults(func: types.FunctionType) -> Optional[tuple]`: Retrieves and returns the default argument values of the function. 6. `set_defaults(func: types.FunctionType, defaults: Optional[tuple])`: Sets the default argument values for the function. 7. `get_closure(func: types.FunctionType) -> Optional[tuple]`: Retrieves and returns the closure associated with the function. 8. `set_closure(func: types.FunctionType, closure: Optional[tuple])`: Sets the closure for the function. 9. `get_annotations(func: types.FunctionType) -> Optional[dict]`: Retrieves and returns the annotations of the function. 10. `set_annotations(func: types.FunctionType, annotations: Optional[dict])`: Sets the annotations for the function. # Example Usage ```python code_str = def example_func(x, y): return x + y globals_dict = {} manipulator = FunctionManipulator() # Create a new function func = manipulator.create_function(code_str, globals_dict) print(func(3, 4)) # Output: 7 # Get and print code object code_obj = manipulator.get_code(func) print(code_obj) # Set new defaults manipulator.set_defaults(func, (1, 2)) print(func()) # Output: 3 # Retrieve and print annotations annotations = manipulator.get_annotations(func) print(annotations) ``` # Constraints - Performance: The solution should efficiently handle typical function sizes and should not have significant overhead for function creation and manipulation. - Correctness: All methods must faithfully interact with the underlying C functions to manipulate Python function objects as intended. # Notes - Assume you have access to the required C extensions and `types` module in Python which provides `FunctionType`, `CodeType`, etc. - Focus on implementing the interface and functionality as described.","solution":"import types class FunctionManipulator: def create_function(self, code_str: str, globals_dict: dict, qualname: str = None) -> types.FunctionType: exec(code_str, globals_dict) func_name = code_str.strip().split(\\"n\\")[0].split(\\" \\")[1].split(\\"(\\")[0] func = globals_dict[func_name] if qualname: func.__qualname__ = qualname return func def get_code(self, func: types.FunctionType) -> types.CodeType: return func.__code__ def get_globals(self, func: types.FunctionType) -> dict: return func.__globals__ def get_module(self, func: types.FunctionType) -> str: return func.__module__ def get_defaults(self, func: types.FunctionType) -> tuple: return func.__defaults__ def set_defaults(self, func: types.FunctionType, defaults: tuple): func.__defaults__ = defaults def get_closure(self, func: types.FunctionType) -> tuple: return func.__closure__ def set_closure(self, func: types.FunctionType, closure: tuple): func.__closure__ = closure def get_annotations(self, func: types.FunctionType) -> dict: return func.__annotations__ def set_annotations(self, func: types.FunctionType, annotations: dict): func.__annotations__ = annotations"},{"question":"# Python Coding Assessment: `threading` Module **Objective**: Assess the ability to leverage the `threading` module in Python to implement a multi-threaded producer-consumer scenario while ensuring proper synchronization. Problem Statement You are required to implement a multi-threaded program using Python\'s `threading` module. The program will involve a producer thread and multiple consumer threads. The producer will fetch data from a data source and place it into a shared buffer, while the consumers will process the data from the buffer. Requirements 1. **Buffer**: - Use a thread-safe `queue.Queue` to implement the buffer. 2. **Producer**: - The producer should fetch data items from a simulated data source (a simple range of numbers). - Place each fetched item into the buffer. - The producer should run infinitely until a stop signal is received. 3. **Consumers**: - Consumers should continuously fetch data items from the buffer and process them. - Each consumer thread should have a unique identifier. - The number of consumer threads should be configurable. 4. **Synchronization**: - Ensure proper synchronization using appropriate synchronization primitives from the `threading` module to avoid race conditions. 5. **Termination**: - Use an `Event` object to signal the producer to stop. - Allow the consumer threads to terminate gracefully after the producer stops. Implementation Details - Implement a function `producer(buffer, stop_event)` that simulates fetching data and placing it into the buffer. - Implement a function `consumer(buffer, consumer_id)` that processes items from the buffer. - Implement a main function to: - Start the producer thread. - Start the specified number of consumer threads. - Allow the producer and consumers to run for a specified duration. - Signal termination and join all threads gracefully. Input and Output - **Input**: - `num_consumers` (int): Number of consumer threads. - `run_time` (int): The duration (in seconds) for which the threads should run before termination. - **Output**: - The output from the consumers processing items (print statements). Constraints - The buffer size should be fixed (e.g., size 10). - Use Python\'s logging module for tracing and debugging messages instead of print statements for thread-safe logging. Sample Code Template ```python import threading import queue import time import logging # Configure logging logging.basicConfig(level=logging.DEBUG, format=\'%(threadName)s: %(message)s\') def producer(buffer, stop_event): while not stop_event.is_set(): data = fetch_data() buffer.put(data) logging.debug(f\'Produced: {data}\') time.sleep(1) # Simulating time taken to fetch data def fetch_data(): # Simulate data fetching logic return def consumer(buffer, consumer_id): while True: try: data = buffer.get(timeout=1) logging.debug(f\'Consumer {consumer_id} processed: {data}\') buffer.task_done() except queue.Empty: break def main(num_consumers, run_time): buffer = queue.Queue(maxsize=10) stop_event = threading.Event() producer_thread = threading.Thread(target=producer, args=(buffer, stop_event)) consumer_threads = [ threading.Thread(target=consumer, args=(buffer, i)) for i in range(num_consumers) ] # Start all threads producer_thread.start() for c_thread in consumer_threads: c_thread.start() # Allow the threads to run for a specified duration time.sleep(run_time) # Signal the producer to stop stop_event.set() # Wait for producer to finish producer_thread.join() # Allow consumers to finish processing remaining items buffer.join() # Waiting for all consumer threads to terminate for c_thread in consumer_threads: c_thread.join() if __name__ == \\"__main__\\": main(num_consumers=3, run_time=10) ``` Submission Instructions Implement the required functions and ensure they meet the specifications. Run your solution to validate its correctness. Your submission should include the complete and executable Python script.","solution":"import threading import queue import time import logging # Configure logging logging.basicConfig(level=logging.DEBUG, format=\'%(threadName)s: %(message)s\') def fetch_data(): # Simulate data fetching logic fetch_data.counter += 1 return fetch_data.counter fetch_data.counter = 0 def producer(buffer, stop_event): while not stop_event.is_set(): data = fetch_data() buffer.put(data) logging.debug(f\'Produced: {data}\') time.sleep(0.5) # Simulating time taken to fetch data def consumer(buffer, consumer_id): while True: try: data = buffer.get(timeout=1) logging.debug(f\'Consumer {consumer_id} processed: {data}\') buffer.task_done() except queue.Empty: break def main(num_consumers, run_time): buffer = queue.Queue(maxsize=10) stop_event = threading.Event() producer_thread = threading.Thread(target=producer, args=(buffer, stop_event)) consumer_threads = [ threading.Thread(target=consumer, args=(buffer, i)) for i in range(num_consumers) ] # Start all threads producer_thread.start() for c_thread in consumer_threads: c_thread.start() # Allow the threads to run for a specified duration time.sleep(run_time) # Signal the producer to stop stop_event.set() # Wait for producer to finish producer_thread.join() # Allow consumers to finish processing remaining items buffer.join() # Waiting for all consumer threads to terminate for c_thread in consumer_threads: c_thread.join() if __name__ == \\"__main__\\": main(num_consumers=3, run_time=10)"},{"question":"# Custom Data Class Implementation You are asked to create a custom data class using Python\'s `dataclasses` module. Your task involves implementing a data class that meets specific requirements and constraints. This exercise will test your understanding of class variables, mutability, inheritance, and custom methods. Requirements: 1. Create a data class `Book` with the following specifications: - `title`: A string representing the book\'s title. - `author`: A string representing the book\'s author. - `publication_year`: An integer representing the year the book was published. - `pages`: An integer representing the number of pages in the book. 2. The `Book` class should include: - A method `is_classic()` that returns `True` if the book was published more than 50 years ago, otherwise `False`. - A method `summary()` that returns a string in the format \\"Title by Author, published in Year, has Pages pages\\". 3. Create another data class `Library` which contains: - `name`: A string representing the library\'s name. - `books`: A list of `Book` objects. 4. The `Library` class should include: - A method `add_book(book: Book)` that adds a `Book` to the `books` list. - A method `remove_book(book: Book)` that removes a `Book` from the `books` list. - A method `get_classics()` that returns a list of all `Book` objects in the library that are considered classics. Constraints: - Use the `dataclasses` module for creating the data classes. - The `books` list in the `Library` class should initialize as an empty list. - Ensure that the methods within both classes handle edge cases appropriately (e.g., attempting to remove a book that is not in the library). Input/Output Example: ```python from datetime import datetime # Creating Book objects book1 = Book(title=\'1984\', author=\'George Orwell\', publication_year=1949, pages=328) book2 = Book(title=\'Brave New World\', author=\'Aldous Huxley\', publication_year=1932, pages=311) book3 = Book(title=\'Modern Python Cookbook\', author=\'Steven F. Lott\', publication_year=2016, pages=750) # Create a Library and add books library = Library(name=\'Central Library\') library.add_book(book1) library.add_book(book2) library.add_book(book3) # Check classic books classics = library.get_classics() print([book.title for book in classics]) # Output: [\'1984\', \'Brave New World\'] # Book summary print(book1.summary()) # Output: \'1984 by George Orwell, published in 1949, has 328 pages\' # Check if a book is a classic print(book1.is_classic()) # Output: True ``` Implement the `Book` and `Library` classes according to the above specifications.","solution":"from dataclasses import dataclass, field from typing import List from datetime import datetime @dataclass class Book: title: str author: str publication_year: int pages: int def is_classic(self) -> bool: current_year = datetime.now().year return current_year - self.publication_year > 50 def summary(self) -> str: return f\\"{self.title} by {self.author}, published in {self.publication_year}, has {self.pages} pages\\" @dataclass class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def remove_book(self, book: Book): if book in self.books: self.books.remove(book) def get_classics(self) -> List[Book]: return [book for book in self.books if book.is_classic()]"},{"question":"# Configuration File Management Task **Objective:** You need to use the `configparser` module to create a utility that can manage configuration settings stored in an INI file format. **Task:** Implement a function `manage_config_file(file_path, section, option, fallback=None)` that performs the following actions: 1. **Reading Configuration**: If the `file_path` exists, it should read the configuration file and print the value of the `option` under the specified `section`. If the section or option does not exist, it should print the `fallback` value (which defaults to `None`). 2. **Modifying/Creating Configuration**: If the `option` does not exist under the specified `section`, add this `option` with a value specified by the `fallback` parameter (if `fallback` is not provided, use the string `\'DEFAULT\'` as the value). 3. **Writing Back to File**: After modifying the configuration, write the updates back to the original `file_path`. **Input Format:** - `file_path` (str): The path to the INI file. - `section` (str): The section in the INI file where the option might exist. - `option` (str): The configuration option whose value needs to be read or updated. - `fallback` (Optional[str]): A default value used if the option does not exist (defaults to `None`). **Output Format:** - Prints the value of the `option` under the specified `section` if it exists, otherwise prints the `fallback` value. **Example Usage:** ```python # Assuming there is a configuration file \'config.ini\' with the following content: # [Settings] # theme = dark # font = Arial # Calling the function when the option exists in the section: manage_config_file(\'config.ini\', \'Settings\', \'theme\') # Output: \'dark\' # Calling the function when the option does not exist: manage_config_file(\'config.ini\', \'Settings\', \'background\', \'light\') # Output: \'light\' # Updates \'config.ini\' to include the new option: # [Settings] # theme = dark # font = Arial # background = light ``` **Constraints:** - The solution should handle both reading and writing of the configuration file. - Ensure proper handling of file I/O operations with error checks. - Assume that `configparser` is pre-imported and ready to use. **Performance Requirements:** - The function should efficiently read, modify, and write to the configuration file with minimal overhead.","solution":"import configparser import os def manage_config_file(file_path, section, option, fallback=None): config = configparser.ConfigParser() if os.path.exists(file_path): config.read(file_path) else: # If file does not exist, create it with open(file_path, \'w\') as configfile: configfile.write(\\"\\") if not config.has_section(section): config.add_section(section) value = config.get(section, option, fallback=fallback) print(value) if not config.has_option(section, option): config.set(section, option, fallback if fallback is not None else \'DEFAULT\') with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"Write a Python program that archives a directory into a tar file, but only includes `.txt` files in the archive. The program should use the `tarfile` module and fulfill the following requirements: - The directory path will be provided as a parameter to the function `archive_txt_files`. - The output tar file should be named `text_files_archive.tar.gz` and saved in the current working directory. - Use gzip compression for the archive. - Ensure the tar file only includes `.txt` files from the specified directory and its subdirectories. - Handle potential exceptions that may occur during the archiving process (e.g., file not found, permission errors) and print appropriate error messages. **Function Signature:** ```python def archive_txt_files(directory_path: str): # Your code here ``` **Example:** ```python archive_txt_files(\'/path/to/directory/\') ``` In this example, if `/path/to/directory/` contains files like `file1.txt`, `file2.pdf`, and a subdirectory with `file3.txt`, the resulting tar archive `text_files_archive.tar.gz` should only include `file1.txt` and `file3.txt`. **Constraints:** - Only include `.txt` files in the archive. - Ensure gzip compression is used. - The archive should retain the directory structure of the included files. - Print appropriate error messages for any exceptions encountered during the process.","solution":"import tarfile import os def archive_txt_files(directory_path: str): Archives all .txt files in the specified directory and its subdirectories into a tar.gz file. try: with tarfile.open(\\"text_files_archive.tar.gz\\", \\"w:gz\\") as tar: for root, dirs, files in os.walk(directory_path): for file in files: if file.endswith(\\".txt\\"): file_path = os.path.join(root, file) tar.add(file_path, arcname=os.path.relpath(file_path, start=directory_path)) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Advanced XML Parsing with Handlers Objective Implement a function that parses a given XML string and extracts information using different handlers provided by the `xml.parsers.expat` module in Python. Task You are required to implement a function `parse_and_extract(xml_data: str) -> dict`. This function will: 1. Parse the given XML string `xml_data` using the `xml.parsers.expat` module. 2. Use various handlers to extract: - The names of all elements in the XML. - The character data within each element. - Any comments in the XML. The output should be a dictionary with three keys: - `\\"elements\\"`: A list of names of all elements. - `\\"character_data\\"`: A dictionary where keys are element names and values are the character data within each element. - `\\"comments\\"`: A list of comments found in the XML. Function Signature ```python def parse_and_extract(xml_data: str) -> dict: pass ``` Example ```python xml_data = <?xml version=\\"1.0\\"?> <parent id=\\"top\\"> <!-- This is a comment --> <child1 name=\\"paul\\">Text goes here</child1> <child2 name=\\"fred\\">More text</child2> </parent> result = parse_and_extract(xml_data) print(result) ``` Expected output: ```python { \\"elements\\": [\\"parent\\", \\"child1\\", \\"child2\\"], \\"character_data\\": { \\"child1\\": \\"Text goes here\\", \\"child2\\": \\"More text\\" }, \\"comments\\": [\\" This is a comment \\"] } ``` Constraints - You can assume that the input `xml_data` will always be a valid XML string. - The XML will not contain nested elements with the same name. - The XML string can be of arbitrary length, but it will fit into memory. Requirements - Use the `xml.parsers.expat` module. - Implement and set the appropriate handlers for start elements, end elements, character data, and comments. - Return the results in the specified format.","solution":"import xml.parsers.expat def parse_and_extract(xml_data: str) -> dict: elements = [] character_data = {} comments = [] current_element = None # Create a parser parser = xml.parsers.expat.ParserCreate() # Define handler for start element def start_element(name, attrs): nonlocal current_element elements.append(name) current_element = name character_data[current_element] = \\"\\" # Define handler for end element def end_element(name): nonlocal current_element current_element = None # Define handler for character data def char_data(data): if current_element: character_data[current_element] += data # Define handler for comments def handle_comment(data): comments.append(data) # Set handlers parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.CommentHandler = handle_comment # Parse the XML data parser.Parse(xml_data) # Clean up character_data to remove empty strings character_data = {k: v.strip() for k, v in character_data.items() if v.strip()} return { \\"elements\\": elements, \\"character_data\\": character_data, \\"comments\\": comments }"},{"question":"Objective: To test your understanding of data preprocessing and transformation using scikit-learn, you are required to create a pipeline consisting of various transformers to preprocess a dataset. Problem Statement: You are provided with a dataset with the following attributes: - `Age`: Numerical feature. - `Income`: Numerical feature. - `Gender`: Categorical feature with two possible values: \\"Male\\" and \\"Female\\". - `Purchased`: Categorical label with two possible values: \\"Yes\\" and \\"No\\". Implement a function `create_preprocessing_pipeline` that does the following: 1. Imputes missing values for numerical features (`Age` and `Income`) with the mean of the respective columns. 2. Scales the numerical features using standard scaling (zero mean and unit variance). 3. Encodes the categorical feature (`Gender`) using one-hot encoding. 4. Encodes the target label (`Purchased`) into binary format. 5. Combine these transformations into a single pipeline. The function should return this pipeline. Input: The function does not take any input parameters. Output: The function should return a scikit-learn `Pipeline` object that combines the described transformations. Example Usage: ```python pipeline = create_preprocessing_pipeline() # Example dataset import pandas as pd data = { \'Age\': [25, 30, 45, np.nan], \'Income\': [50000, 60000, 80000, 70000], \'Gender\': [\'Male\', \'Female\', \'Female\', \'Male\'], \'Purchased\': [\'Yes\', \'No\', \'Yes\', \'No\'] } df = pd.DataFrame(data) X = df.drop(\'Purchased\', axis=1) y = df[\'Purchased\'] pipeline.fit(X, y) X_transformed = pipeline.transform(X) print(X_transformed) ``` The output should be the transformed feature representation of `X` as a numpy array. Constraints: - Aim to use scikit-learn transformers where possible. - Ensure the pipeline is constructed in a manner that it can handle unseen data during inference. Notes: - Utilize `SimpleImputer` for imputing missing values. - Use `StandardScaler` for feature scaling. - Use `OneHotEncoder` for the categorical feature. - Use `LabelEncoder` for the target label.","solution":"from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder import pandas as pd def create_preprocessing_pipeline(): # Numerical features and transformers numerical_features = [\'Age\', \'Income\'] numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Categorical features and transformers categorical_features = [\'Gender\'] categorical_transformer = Pipeline(steps=[ (\'encoder\', OneHotEncoder(drop=\'first\')) # OneHotEncoder with drop=\'first\' to avoid multicollinearity ]) # Creating the column transformer preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Creating the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor) ]) return pipeline"},{"question":"**Objective**: Implement a Python class that simulates custom set operations as described in the provided API documentation, demonstrating your understanding of set and frozenset functionalities and error handling. **Problem Statement**: Create a class `CustomSet` that simulates a set with the following methods: 1. `add(element)`: Adds an element to the set. Raises `TypeError` if the element is unhashable. 2. `remove(element)`: Removes an element from the set if present. Does nothing if the element is not found. 3. `contains(element)`: Checks if the element is part of the set. Returns `True` if found, `False` otherwise. 4. `size()`: Returns the number of elements in the set. 5. `clear()`: Clears all elements from the set. 6. `pop()`: Removes and returns an arbitrary element from the set. Raises `KeyError` if the set is empty. 7. `as_frozenset()`: Returns a frozenset containing the same elements as the set. 8. `is_disjoint(other)`: Returns `True` if the sets have no elements in common, `False` otherwise. **Constraints and Requirements**: 1. Elements added to the set must be hashable. 2. Utilize Python built-in set operations wherever applicable. 3. Implement proper error handling as specified. 4. Optimize for readability and efficiency. ```python class CustomSet: def __init__(self): # Initialize an empty set self._set = set() def add(self, element): # Add element to the set, raise TypeError if unhashable pass def remove(self, element): # Remove element from the set if present pass def contains(self, element): # Check if the element is in the set pass def size(self): # Return the size of the set pass def clear(self): # Clear the set pass def pop(self): # Remove and return an arbitrary element from the set, raise KeyError if empty pass def as_frozenset(self): # Return a frozenset containing the same elements as the set pass def is_disjoint(self, other): # Return True if the sets have no elements in common pass ``` **Example Usage**: ```python cs = CustomSet() cs.add(1) cs.add(2) cs.add(3) print(cs.contains(2)) # True print(cs.size()) # 3 cs.remove(2) print(cs.contains(2)) # False print(cs.size()) # 2 print(cs.as_frozenset()) # frozenset({1, 3}) print(cs.is_disjoint({4, 5})) # True print(cs.is_disjoint({1, 5})) # False cs.pop() print(cs.size()) # 1 cs.clear() print(cs.size()) # 0 ``` Implement this class in Python and ensure all methods work as expected according to the provided documentation.","solution":"class CustomSet: def __init__(self): # Initialize an empty set self._set = set() def add(self, element): # Add element to the set, raise TypeError if unhashable try: self._set.add(element) except TypeError: raise TypeError(\\"Element must be hashable\\") def remove(self, element): # Remove element from the set if present self._set.discard(element) def contains(self, element): # Check if the element is in the set return element in self._set def size(self): # Return the size of the set return len(self._set) def clear(self): # Clear the set self._set.clear() def pop(self): # Remove and return an arbitrary element from the set, raise KeyError if empty if not self._set: raise KeyError(\\"pop from an empty set\\") return self._set.pop() def as_frozenset(self): # Return a frozenset containing the same elements as the set return frozenset(self._set) def is_disjoint(self, other): # Return True if the sets have no elements in common return self._set.isdisjoint(other)"},{"question":"# **Coding Assessment Question** Title: Building and Interacting with a Custom SQLite Database Objective The objective of this assessment is to test your understanding of using the `sqlite3` module in Python to interact with an SQLite database. You will be required to create a database, manage transactions, and handle custom data types efficiently. Problem Statement You have been assigned to develop a Python script that creates and interacts with a SQLite database. Follow the steps below to complete the task: 1. **Database Setup:** - Create a connection to an SQLite database named `custom_movies.db`. Ensure the connection is set to have the isolation level `IMMEDIATE`. - Create a cursor from the connection. - Create a table named `movies` with the following columns: - `title` (TEXT): The title of the movie. - `year` (INTEGER): The release year of the movie. - `score` (REAL): The review score of the movie. - `duration` (INTEGER): The duration of the movie in minutes. 2. **Custom Data Type Handling:** - Create a custom class named `Movie`. - This class should have the attributes `title`, `year`, `score`, and `duration`. - Implement a method in the `Movie` class to conform to SQLite’s `PrepareProtocol` for adapting `Movie` objects. The `Movie` object should be stored in the database as a string in the format `title|year|score|duration`. - Register this custom adapter with sqlite3. 3. **Data Insertion:** - Insert at least three movie entries into the `movies` table using the custom `Movie` data type. 4. **Data Retrieval and Conversion:** - Implement a custom converter function to convert the stored string back into `Movie` objects. - Register this custom converter with sqlite3. - Retrieve and print all entries from the `movies` table as `Movie` objects. 5. **Transaction Management:** - Ensure that the data insertion and retrieval are managed within a transaction. Use proper commit and rollback mechanisms to handle any potential errors during the insertion process. - Print appropriate messages to indicate the success or failure of database operations. Code Requirements - You are required to use the `sqlite3` module and its functionalities to create the database, manage the cursor, handle custom data types, and manage transactions. - Ensure to handle exceptions where appropriate, and provide informative feedback messages. Example Output ``` Movie inserted successfully: \'Inception|2010|8.8|148\' Movie inserted successfully: \'Interstellar|2014|8.6|169\' Movie inserted successfully: \'The Dark Knight|2008|9.0|152\' Successfully retrieved movies from the database: Movie(\'Inception\', 2010, 8.8, 148) Movie(\'Interstellar\', 2014, 8.6, 169) Movie(\'The Dark Knight\', 2008, 9.0, 152) ``` Constraints - You should use parameterized queries or placeholders to prevent SQL injection. - Handle all possible exceptions and ensure the script does not crash on errors. Good luck!","solution":"import sqlite3 class Movie: def __init__(self, title, year, score, duration): self.title = title self.year = year self.score = score self.duration = duration def __repr__(self): return f\\"Movie(\'{self.title}\', {self.year}, {self.score}, {self.duration})\\" def adapt_movie(movie): return f\\"{movie.title}|{movie.year}|{movie.score}|{movie.duration}\\" def convert_movie(movie_str): title, year, score, duration = movie_str.decode(\'utf-8\').split(\'|\') return Movie(title, int(year), float(score), int(duration)) # Register the adapter and converter with sqlite3 sqlite3.register_adapter(Movie, adapt_movie) sqlite3.register_converter(\\"MOVIE\\", convert_movie) # Database Setup conn = sqlite3.connect(\\"custom_movies.db\\", detect_types=sqlite3.PARSE_DECLTYPES, isolation_level=\\"IMMEDIATE\\") cursor = conn.cursor() # Create the table cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS movies ( title TEXT, year INTEGER, score REAL, duration INTEGER ) \'\'\') # Data Insertion movies_to_insert = [ Movie(\'Inception\', 2010, 8.8, 148), Movie(\'Interstellar\', 2014, 8.6, 169), Movie(\'The Dark Knight\', 2008, 9.0, 152) ] try: for movie in movies_to_insert: cursor.execute(\\"INSERT INTO movies (title, year, score, duration) VALUES (?, ?, ?, ?)\\", (movie.title, movie.year, movie.score, movie.duration)) print(f\\"Movie inserted successfully: \'{movie}\'\\") conn.commit() except Exception as e: conn.rollback() print(f\\"Error during data insertion: {e}\\") # Data Retrieval try: cursor.execute(\\"SELECT title, year, score, duration FROM movies\\") rows = cursor.fetchall() print(\\"Successfully retrieved movies from the database:\\") for row in rows: movie_str = \'|\'.join(map(str, row)) print(repr(convert_movie(movie_str.encode(\'utf-8\')))) except Exception as e: print(f\\"Error during data retrieval: {e}\\") finally: conn.close()"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},R={class:"card-container"},q={key:0,class:"empty-state"},F=["disabled"],N={key:0},M={key:1};function O(s,e,l,m,n,o){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")}," ✕ ")):d("",!0)]),t("div",R,[(a(!0),i(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),i("div",q,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),i("span",M,"Loading...")):(a(),i("span",N,"See more"))],8,F)):d("",!0)])}const L=p(z,[["render",O],["__scopeId","data-v-db43d71a"]]),X=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/52.md","filePath":"chatai/52.md"}'),j={name:"chatai/52.md"},Y=Object.assign(j,{setup(s){return(e,l)=>(a(),i("div",null,[x(L)]))}});export{X as __pageData,Y as default};
